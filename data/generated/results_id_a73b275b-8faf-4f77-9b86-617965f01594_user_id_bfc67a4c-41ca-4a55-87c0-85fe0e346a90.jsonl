{"question":"How did copyists modify the Sharafnama text to reflect Safavid political views in the early 17th century?","answer":"In the early 17th century, copyists modified the Sharafnama text in several ways to reflect pro-Safavid and Shi'i views: 1) They deleted marks of praise used for Ottoman sultans, particularly references to them as caliphs or 'holy warriors'. 2) They replaced Safavid sovereigns' names with convoluted honorary titles. 3) They removed traditional religious formulas after names of Sunni figures like Imam Shafi'i and Mu'awiyya, while adding such formulas to Shi'i figures' names. 4) They even rewrote historical events, such as attributing the Safavid defeat at Chaldiran to the 'evil eye'.","context":["A Case Study on Šaraf Xān Bidlīsī’s Šarafnāma (1005-1314/1597-1897) [DYNTRAN WORKING PAPER 23, April 2017]\nby Sacha ALSANCAKLI\nThe Šarafnāma is a well-known history of Kurdish dynasties and ruling houses, written in Persian in 1005-7/1596-99 by Amīr Šaraf Xān Bidlīsī (949-1009/1543-1600), leader of the Rōjikī tribe and prince of Bidlīs, southwest of Lake Van, in northern Kurdistan. It comprises four books on the history of various Kurdish dynasties, preceded by a Muqaddima (“Introduction”) on the origins and manners of the Kurds, and followed by an annalistic Xātima (“Conclusion”) on Ottoman and Safavid history.\nIn the course of my study of the Šarafnāma, I have worked with 36 different manuscripts of the text, covering a time period going from its composition (1005/1597) to the end of the 13th/19th century (the last dated manuscript, ms. 3934 of the Malik library in Tehran, was copied in 1314/1897). This work, focused on the processes of composition, transmission and reception of the text, has led me to question the role played by copyists in these processes. In this preliminary article, I will share a few remarks on the ways in which, through three centuries of manuscript transmission, copyists have relayed and interpreted the text of the Šarafnāma according to cultural context, political necessity and/or professional choices.\nStylistic and structural modifications\nThe first copies of the Šarafnāma were realized under Šaraf Xān’s supervision in the years 1006-7/1596-99. Evidence suggests that, after he wrote the draft manuscript of the work, which is extant (ms. Elliott 332, Bodleian Library of Oxford, completed in Zū al-Ḥijja 1005/August 1597), the Kurdish prince requested a copyist, possibly his court secretary, to produce a more polished version of the text. It appears that Šaraf Xān wished for this manuscript, which seems to have been lost, to be used as a normative model for future copies. Thus, the two extant manuscripts of the Šarafnāma produced in 1007/1599 under the supervision of Šaraf Xān (mss. Hunt. Don. 13, Bodleian Library and Dorn 306, National Library of Russia in St. Petersburg) are based on this “model manuscript”. Šaraf Xān died shortly thereafter, in 1009/1600, in the context of a conflict with the Porte.\nIn 1015/1606, a scribe from Kilīs named Ḥasan b. Nūr al-Dīn was responsible for the first known copy of the Šarafnāma to be produced after the author’s death (ms. Elliott 331, Bodleian Library). The recent nature of this event is hinted at in the colophon of the manuscript, where the author is called “the late Šaraf Xān, Rōjikid prince of Bidlīs” (شرف خان حاکم بتلیس المرحوم الروجکی; f. 327v, l. 21). It seems that this copy was realized at the request of ‘Alī Jānbūlād, ruler of Kilīs and Aleppo and nephew of Ḥusayn Jānbūlād, from the manuscript Hunt. Don. 13 mentioned above, hence the claim by Ḥasan b. Nūr al-Dīn to have made it from the text of an autograph (من خطّ مؤلّفه و مصحّحه و منقّحه). The objective here was apparently to produce a more readable copy of the work than HD 13, which was already in ‘Alī Jānbūlād’s possession.\nTo this end, Ḥasan b. Nūr al-Dīn integrated the numerous marginal annotations present in HD 13 into the text, as well as correcting mistakes in the manuscript, such as the date of Meḥmed III’s accession to the throne. (HD 13, f. 261v, l. 2; E 331, f. 324v, l. 14) On occasions, he also adds bits of information, for example the title of three books by Lāmi‘ī Čelebī. (HD 13, f. 220v, ll. 2-4, and E 331, f. 263v, ll. 17-21) Ḥasan b. Nūr al-Dīn’s manuscript can thus be seen as a continuation of Šaraf Xān’s efforts to produce revised and corrected copies of his book, a feeling that is enhanced by the geographical, political and chronological proximity of E 331 with these earlier manuscripts. As in the case of the copies realized under the supervision of Šaraf Xān, Ḥasan b. Nūr al-Dīn also insists on the legitimacy and textual value of his manuscript, by stressing that his model was an autograph.\nThese are mostly stylistic and structural modifications, as well as minor corrections by Ḥasan b. Nūr al-Dīn, which were meant as improvements on perceived or actual shortcomings in the original text as he knew it. Beyond that, another important way in which copyists acted upon the text was through adapting it to their specific socio-political environment. A classic example of this attitude can be seen in the insertion, by several copyists who were also, probably, court secretaries, of panegyrics to their patrons and sponsors of the copy at the end of the work, in or before the colophon. Among such sponsors, we can mention Abdāl Xān, great-grandson of Šaraf Xān, his own son Šaraf Xān III, or Aḥmad Bēg b. ‘Alī Bēg of the Maḥmūdī. (See mss. Or. 12 of the Royal library, Torino; Or. 1127 of the British Library, London and Farsça 223 of Istanbul University’s library.)\nIn the copies produced at their request, these princes were all presented as independent rulers and, among other laudatory attributes, described as xānī ‘aẓīm al-šānī anūšīrwān (“the glorious and Khusraw-like khān”) or ẓillullāhī (“shadow of God on Earth”). These modifications are thus manifestations of a situation of de facto independence of these principalities at the time of copying, and such a situation is attested in the historical records.\nHowever, such adaptations could go much further, and have an impact on the text as a whole. In the case of the Šarafnāma, the most notable instance of this approach is to be found in the two manuscripts produced by a man named Mīrakī b. Aḥmad Qahramānī Hamadānī, who was probably a court secretary to the Ardalān princes in the early 11th/17th century. It is in this position that he wrote two copies of the Šarafnāma, in 1017/1608 and 1027/1618, the latter at the request of Xān Aḥmad Xān Ardalān, ruler of the principality (mss. of the Golestan Palace, Tehran, and Browne H. 10, Cambridge University Library). Raised at the Safavid court in Iṣfahān and later married to Šāh ‘Abbās’ sister Sayyida Begum, known as Zarrīn Kolāh, Xān Aḥmad Xān remained a close ally of Šāh ‘Abbās to the end of the latter’s rule. (On Zarrīn Kolāh, see Bābānī 1366/1987, p. 42.) It is thus not surprising that the two Šarafnāma manuscripts produced at his behest display a markedly pro-Safavid and Shi‘i stance.\nSome examples of this political viewpoint include the deletion of marks of praise used for the Ottoman sultans, in particular when they refer to their status as caliphs or “holy warriors” (غازی), and the use of convoluted honorary titles in place of the names of the Safavid sovereigns (نواب همایونی, نواب اعلی, etc.); the omission of the traditional religious formulas found after the names of Sunni figures, such as Imām Šāfi‘ī, Mu‘āwiyya, ‘Umar al-Xaṭṭāb or even ‘Abbās, uncle of the Prophet, and conversely, the addition of such formulas to the names of Shi‘i figures; and even the rewriting of certain historical events, like the attribution to the “evil eye” (چشم زخم) of the Safavid defeat at Čaldirān. (See Browne H. 10, ff. 12r, l.3; 14r, l. 5; 24r, l. 21; 39r, ll. 17 and 20; 58v, l. 24; 61r, l. 13; 62v, l. 8; and many other places.)\nNew perspectives on Kurdishness and dynasty politics\nAfter a near-complete hiatus in the 12th/18th century, it is also primarily in the Ardalān region that, from the early 13th/19th century onwards, a resurgence in the production of manuscript copies of the Šarafnāma took place, which later spread to other principalities in south and southeast Kurdistan. This regain of interest in the work was associated with new perspectives on the idea of Kurdishness and connected in new ways to dynasty politics, which also reflected in the work of the copyists. Thus, the enumeration by Šaraf Xān of what he identified as the four major Kurdish groups – Kurmānjs, Lors, Kalhurs and Gūrāns –, preceded by the enigmatic assertion that “Kurmānjs are the best among them” (بهترین ایشان کرماج است), became a hot topic of contention, while it had previously seemed to be rather innocuous. The sentence was the subject of tempering in several copies produced in the (Gūrānī-speaking) Ardalān principality in the 13th/19th century, with a view to advancing an idea opposite to the original, namely that the Gūrāns were, in fact, the best among the Kurds. (See mss. Add. 22698, f. 6r, ll. 1-2; Add. 23532, f. 4v, ll. 19-20, and Or. 4836, f. 3v, ll. 10-11, all kept in the British Library in London.) It is worth mentioning that, at this time, the (Gūrān) Ardalāns and the (Kurmānj) Bābāns were in a continuous situation of rivalry, as is made plain by the experiences and anecdotes related by Claude J. Rich, who visited the region in 1820. (See Rich 1836: I, pp. 80-81, 151-53, 215 and 270.)\nThis new approach towards the Šarafnāma in the 13th/19th century is also reflected in the widespread use of the alternative title Tārīx-i Akrād or Tārīx-i Kurdistān to designate it; suggesting that the work had become known primarily as a “History of the Kurds and Kurdistan”, rather than as the “Book of Honour” of the Kurdish dynasties, a significant shift in focus. This new appropriation of the work by the Kurdish elites came along with an increased tendency to alter and manipulate it in various ways, such as the translation, production of zayls for specific chapters, or cutting and copying of particular sections, presented as stand-alone publications. Although this process had already started in the late 11th/17th century, with the production of two translations in Turkish (in Bidlīs and Pālū) and of two or three zayls (in Pālū, Agīl, and possibly Ġarzan), it became even more common in this period, which is exemplified by the impressive number of historiographical works, directly or indirectly linked with the Šarafnāma, produced in the Ardalān principality in the 13th/19th century.\nThe era of nationalism\nIn the northern Kurdish principalities, where knowledge of the Persian language had become very scarce, there was also a resurgence of copies of the Turkish translations of the Šarafnāma, including one manuscript containing a map of Kurdistan, not entirely congruent with the territory defined by Šaraf Xān, and including the delimitation of the international borders at the time (ms. Tarih 364, Millet library, Istanbul, p. 698). This manuscript is dated 1296/1879, a time by which all the Kurdish autonomous dynasties had been toppled, and a year only before the rebellion of Shaykh ‘Ubaydullāh (1880-81), considered the first Kurdish nationalist revolt. (On this rebellion, see Ateş 2015.) This is the earliest documented use of the Šarafnāma in the context of the emerging Kurdish national question, and the work continued to play a prominent role in the rhetoric of early Kurdish nationalists in the declining Ottoman empire. The history of the editions of the book in the 20th century is also markedly intertwined with political developments in Kurdistan. (On this topic, see Bajalan 2012.)\nAs can be gathered from these short considerations, there were numerous ways to copy a manuscript, and the approach adopted was determined by various factors, such as the degree of professionalism of the copyist, his involvement in the work, the nature of his intended readership, his position towards the ideas developed in the book, etc. While some copyists chose to be as faithful to their model as possible, others felt a responsibility to adapt and interpret the text to better care for the needs and sensibilities of their potential readers. This article also shows that studying the processes of the transmission and reception of manuscripts, in which the figure of the copyist played a central part, might allow us to gain valuable insights into the cultural and political history of some of the less documented regions or time periods, such as, in the case of the Šarafnāma, Kurdistan in the modern era.\nSacha Alsancakli (April 2017)\n- Alsancakli, S., “From Bidlīs to Ardabīl Via Aleppo and Iṣfahān. On the Circulation of a Manuscript of Šaraf Ḫān Bidliī’s Šarafnāma Revised by the Author (1007/1599)”, Eurasian Studies 13 (2015), pp. 133-152.\n- Alsancakli, S. “The Šarafnāma and the Rūjikī rulers of Bidlīs in the 11th/17th century”, DYNTRAN Working Papers, n° 8, online edition, January 2016, available at: http://dyntran.hypotheses.org/902\n- Ateş, S., “In the Name of the Caliph and the Nation: The Sheikh Ubeidullah Rebellion of 1880-81”, pp. 55-118 in Bajalan, Dj. R., and Zandi Karimi, S. (eds.), Studies in Kurdish History: Empire, Ethnicity and Identity, London: Routledge, 2015.\n- [‘Abd al-Qādir b. Rustam] Bābānī, Siyyar al-Akrād, ed. Muḥammad Ra‘ūf Tawakkulī, 1366/1987, Baghdad: Čāpxāna-yi Arjang.\n- Bajalan, Dj. R., “Şeref Xan’s Sharafnama: Kurdish Ethno-Politics in the Early Modern World, Its Meaning and Its Legacy”, Iranian Studies 45/6 (2012), pp. 795-818.\n- [Šaraf al-Dīn Xān] Bidlīsī, Šarafnāma, mss. Hunt. Don. 13, Elliott 331, Elliott 332 (Bodleian Library, Oxford); Browne H. 10 (Cambridge University Library); Or. 1127, Or. 4836, Add. 22698, Add. 23532 (British Library, London); Or. 12 (Royal Library, Torino); Dorn 306 (National Library of Russia, St. Petersburg); Farsça 223 (Istanbul University Library); Tarih 364 (Millet Library, Istanbul).\n- Rich, Cl. J., Narrative of a residence in Koordistan, London: James Duncan, 1836.\nTo quote this publication:\nSacha Alsancakli, “The Role of Copyists in the Transmission of Manuscripts. A Case Study on Šaraf Xān Bidlīsī’s Šarafnāma (1005-1314/1597-1897)”, DYNTRAN Working Papers, no. 23, online edition, April 2017, available at: http://dyntran.hypotheses.org/1826#more-1826"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:06069c5c-ead6-45cc-94ac-9c19944f8514>"],"error":null}
{"question":"I have leukemia and multiple myeloma - can you compare how clinical trials are being used to advance treatment in both conditions?","answer":"Clinical trials are advancing treatment in both conditions in different ways. For ALL (acute lymphoblastic leukemia), current trials are testing various drugs including bortezomib (Velcade) for relapsed/refractory cases, clofarabine for adult patients, and innovative approaches like CAR T-cell therapy where patients' immune cells are engineered to attack cancer cells. For multiple myeloma, clinical trials have focused heavily on maintenance therapy, testing drugs like Revlimid, Velcade, and Ninlaro to prevent cancer recurrence after initial treatment. Studies have shown these maintenance approaches can delay disease progression, though some, like Revlimid, carry risks of secondary cancers.","context":["Taking part in a clinical trial may be the best treatment choice for some acute lymphoblastic leukemia (ALL) patients. Clinical trials are under way for patients at every treatment stage and for patients in remission. Today's standard treatments for cancer are based on earlier clinical trials. The Leukemia & Lymphoma Society continues to invest funds in ALL research.\nClick here to read more about clinical trials.\nCurrent ALL Research and Clinical Trials\nScientists are conducting research strategies and clinical trials that hold the promise of increasing remission and cure rates of ALL patients. Here are examples (and some descriptions) of specific agents under study in clinical trials for ALL. L.\n- Bortezomib (Velcade®) - This drug, approved to treat myeloma and some types of lymphoma, is now being studied in combination with other drugs such as belinostat for the treatment of relapsed or refractory ALL. It is also being studied for treating newly diagnosed pediatric patients with T-cell ALL.\n- Clofarabine (Clolar®) - Already approved to treat pediatric ALL, clofarabine is now showing promising results in studies of adults with ALL. It is also being studied in combination with other drugs such as mitoxantrone in clinical trials for the treatment of children whose ALL is relapsed or refractory.\n- Nelarabine (Arranon®) - This drug, a type of antimetabolite drug, is approved for patients who have relapsed T-cell ALL. It is now being studied in clinical trials in combination with other agents for the treatment of relapsed or refractory T-cell ALL. It is also being evaluated in combination with other drugs as part of an induction regimen for untreated T-cell ALL.\nJanus kinase (JAK) Inhibitor\n- Ruxolitinib (Jakafi®) - Already approved to treat myelofibrosis and polycythemia vera patients, it is being studied in clinical trials in the treatment of pediatric refractory and relapsed ALL. It is also being studied in combination with several chemotherapy drugs in the treatment of children with Ph-like ALL and CRLF2 and JAK mutations.\nSpecial Chemotherapy Combination\n- Augmented Hyper-CVAD - The hyper-CVAD combination (cyclophosphamide, vincristine, doxorubicin and dexamethasone) is a well-established regimen for ALL. The augmented hyper-CVAD formulation was designed in 2011 and it includes intensified doses of vincristine, dexamethasone and asparaginase. Researchers are studying the efficacy of this combination for ALL treatment.\n- Monoclonal antibodies rituximab (Rituxan®), alemtuzumab (Campath®), ofatumumab (Arzerra®) - These drugs are already approved in the treatment of other blood cancers. They are currently being studied for their use in combination with chemotherapy in clinical trials for untreated and relapsed/ refractory ALL.\n- Blinatumomab (Blincyto®) - This drug is a bispecific, anti-CD19, CD3 T-cell engager, approved for the treatment of relapsed or refractory Ph-negative B-cell precursor ALL. It is being studied in current trials for the treatment of refractory and relapsed ALL and also as therapy for older patients with newly diagnosed disease.\n- Inotuzumab ozogamicin - This drug is an anti-CD22 monoclonal antibody that is bound to a toxic drug called calicheamicin. It is being studied, as part of a regimen with combination chemotherapy, in the treatment of relapsed and refractory ALL.\n- Chimeric antigen receptor (CAR) T-cell therapy - This is a type of immunotherapy that consists of engineering patients’ own immune cells first to recognize and then to attack cancerous tumors. This approach has shown very promising results in patients with blood cancers. The T cells are genetically engineered to produce receptors on their surface called “chimeric antigen receptors” (CARs). These receptors recognize and bind to a specific target found on the cancerous cells. Clinical trials are studying the use of CAR T-cell therapy in the treatment of chemotherapy-resistant or refractory ALL in both adults and children. To read more about this treatment, please click here.","by Dr. C.H. Weaver M.D. updated 3/2019\nWhat is maintenance therapy?\nFollowing the primary treatment of multiple myeloma your doctor may recommend additional treatment with “maintenance therapy.” The goal of maintenance therapy is not to cure the cancer but to “maintain” a remission or prevent or delay the cancer's return.\nClinical studies have demonstrated that Revlimid, Velcade, and Ninlaro can delay the progression of multiple myeloma for certain patients but have not been shown to significantly prolong survival.(1-6,8) Revlimid has been associated with an increased risk of new cancers and the FDA recently made a safety announcement regarding the drug.(7)\nRevlimid is an oral medication that can stop or slow the growth of cancerous myeloma cells within the bone marrow. Despite the increased risk of causing new cancers in some patients Revlimid continues to be widely used as maintenance therapy in multiple myeloma. Three double-blind, phase 3, multi-center, randomized trials have evaluated Revlimid maintenance therapy.\nRevlimid After ASCT\nResearchers assigned 614 patients under age 65 to Revlimid or placebo after transplantation.(1) Maintenance therapy with Revlimid improved progression-free survival, with a progression-free survival of 41 months in the Revlimid group compared to 23 months in the placebo group. After a median follow-up of 45 months, more than 70 percent of patients in both groups were alive at 4 years. There was an increased rate of new cancers in the Revlimid group, with 32 new cancers in the Revlimid group and 12 in the placebo group.\nIn a second study, 460 patients age 71 or younger were randomly assigned to Revlimid or placebo after transplantation.(2) Patients in the Revlimid group had a significantly longer time to disease progression compared to those in the placebo group. Estimated median time to progression was nearly doubled for those receiving treatment, from 27 months for the placebo group to 53 months for those receiving lenalidomide. After 65 months median follow-up, the median overall survival has not yet been reached for those receiving lenalidomide and is 76 months for the placebo group. Among those receiving treatment, 25 secondary primary malignancies were observed, compared to 10 in the placebo arm. The study was unblinded at 18 months median follow-up, and 86 patients from the placebo arm who showed no evidence of disease progression chose to cross over to the treatment group.(4)\nThe accompanying NEJM editorial by Dr. Ashraf Badros observed that \"while Revlimid appears to offer benefit as maintenance therapy for multiple myeloma, it does come with risks, namely the increased risk of second primary cancers\" . Furthermore the benefit of progression-free survival in the absence of and improvement in overall survival was called into question.(7)\nRevlimid in ASCT Ineligible Patients\nAmong patients with newly diagnosed multiple myeloma (MM) who are ineligible for high-dose therapy and stem cell transplant, treatment with a combination of Velcade® (bortezomib), melphalan, prednisone, and thalidomide followed by maintenance therapy with Velcade and thalidomide (VMPT-VT) resulted in better response rate and progression-free survival than treatment with Velcade, melphalan, and prednisone (VMP) without maintenance therapy. These findings were recently published in the Journal of Clinical Oncology.\nTo evaluate the addition of a fourth drug to VMP, along with maintenance therapy, researchers in Italy conducted a Phase III clinical trial among more than 500 patients with newly diagnosed multiple myeloma.\nPatients enrolled in this study were not candidates for high-dose therapy plus stem-cell transplantation due to either age or other health problems. Study participants were treated with VMPT followed by maintenance therapy with VT or VMP without maintenance therapy.\n- Patients in the VMPT-VT arm experienced more frequent side effects, including neutropenia (low white blood cell counts), heart problems, and blood clots. Treatment-related deaths occurred in 4% of patients in the VMPT-VT group versus 3% in the VMP group.\n- Three-year progression-free survival was 56% for patients in the VMPT-VT group and 41% in the VMP group.\n- 38% of patients in the VMPT-VT group experienced a complete response compared with 24% in the VMP group.\n- Three-year overall survival was 89% in the VMPT-VT group versus 87% in the VMP group. This difference in overall survival did not meet the criteria for statistical significance, suggesting that it could have occurred by chance alone.\nThe researchers concluded that newly diagnosed MM patients who were not candidates for high-dose therapy plus stem cell transplant experienced a progression-free survival and complete response benefit with VMPT-VT treatment compared with VMP. Individualizing treatment strategies for elderly patients and patients with other health problems is critical to optimizing health outcomes both in terms of survival and quality of life.(9)\nIn a more recent study patients were randomly assigned to one of three groups:152 patients received oral melphalan-prednisone-Revlimid induction followed by Revlimid (MPR-R); 153 patients patients received oral melphalan-prednisone-Revlimid followed by placebo (MPR); and 154 patients received melphalan-prednisone followed by placebo (MP). The median progression-free survival was significantly longer with Revlimid maintenance therapy—the MPR-R group had a median progression-free survival of 31 months, compared to 14 months for the MPR group and 13 months for the MP group. The rate of new cancers was 7% with MPR-R, 7% with MPR, and 3% with MP.(6)\nNinlaro is referred to as a proteasome inhibitor. It produces anti-cancer effects by blocking proteins called enzymes within myeloma cells. By blocking these enzymes, the growth of the cancer cells is reduced or halted.\nNinlaro delays cancer progression when used as maintenance therapy following autologous stem cell transplantation.\nThe TOURMALINE-MM3 clinical trial evaluated the use of Ninlaro maintenance therapy post ASCT. Because nearly one-third of patients will ultimately discontinue Revlimed due to side effects or regarding concerns of causing a second primary cancer. Ninlaro which is taken orally and has requires less-frequent administration could be beneficial for individuals who can’t tolerate Revlimd therapy.(8)\nThe TOURMALINE-MM3 clinical trial evaluated 656 adult patients with newly diagnosed multiple myeloma who had responded to induction therapy prior to undergoing high dose chemotherapy and stem cell transplant. Patients were treated with either weekly Ninlaro or matched placebo and directly compared. The study found that Ninlaro treated patients experienced a 28 percent lower risk of disease progression compared with placebo and a higher rate of conversion from MRD-positive to MRD-negative status. Whether or not Ninlaro prolongs survival has yet to be determined and requires longer follow up.\nVelcade Effective for Induction and Maintenance Treatment of Multiple Myeloma\nVelcade® (bortezomib) is a type of targeted drug known as a proteasome inhibitor. It has been shown to provide benefits in the treatment of multiple myeloma and mantle cell lymphoma.\nA randomized phase III clinical trial evaluated the sustained use of Velcade treatment during induction and maintenance. The study included 827 patients with newly diagnosed symptomatic multiple myeloma who were randomly assigned to receive induction therapy with vincristine, doxorubicin, and dexamethasone (VAD) or Velcade, doxorubicin, and dexamethasone (PAD) followed by high-dose melphalan and autologous stem-cell transplantation. Patients in the VAD group received maintenance therapy with Thalomid® (thalidomide) once per day for two years and patients in the PAD group received maintenance therapy with Velcade once every two weeks for two years.\nComplete response was superior in the patients who received PAD induction—31 percent, compared to 15 percent in the VAD group. Velcade maintenance also produced superior complete response—49 percent, compared to 34 percent in the Thalomid group. After a median follow-up of 41 months, progression-free survival (PFS) was 35 months in the PAD group compared to 28 months in the VAD group. Furthermore, overall survival was better in the PAD group.\nIn high-risk patients presenting with increased creatinine, Velcade significantly improved PFS from a median of 13 months to 30 months and overall survival from a median of 21 months to 54 months.\nThe researchers concluded that Velcade during induction and maintenance improves complete response, progression-free survival, and overall survival.\n- Attal M, Lauwers-Cances V, Marit G, et al. Lenalidomide Maintenance after Stem-Cell Transplantation for Multiple Myeloma. New England Journal of Medicine. 2012; 366:1782-1791.\n- McCarthy PL, Owzar K, Hofmeister CC, et al. Lenalidomide after Stem-Cell Transplantation for Multiple Myeloma. New England Journal of Medicine. 2012; 366:1770-1781.\n- Dimopoulos MA, Gay F, Schjesvold FH, et al. Maintenance therapy with the oral proteasome inhibitor (PI) ixazomib significantly prolongs progression-free survival (PFS) following autologous stem cell transplantation (ASCT) in patients with newly diagnosed multiple myeloma (NDMM): phase 3 Tourmaline-MM3 trial. Abstract #301. Presented at the 2018 ASH Annual Meeting, December 2, 2018; San Diego, CA.\n- Updated analysis of CALGB/ECOG/BMT CTN 100104: Lenalidomide (Len) vs. placebo (PBO) maintenance therapy after single autologous stem cell transplant (ASCT) for multiple myeloma (MM),” is abstract 8523 and will be presented on board no. 340 during the Lymphoma and Plasma Cell Disorders poster session Sunday, May 31, from 8 a.m. to 11:30 a.m. CDT in McCormick Place, S Hall A, and will also be featured in a poster discussion session later that day, from 4:30 p.m. to 5:45 p.m. CDT in McCormick Place E354b.\n- Palumbo A, Hajek R, Delforge M, et al. Continuous Lenalidomide Treatment for Newly Diagnosed Multiple Myeloma. New England Journal of Medicine. 2012; 366:1759-1769.\n- Badros AZ. Lenalidomide in Myeloma — A High-Maintenance Friend New England Journal of Medicine. 2012; 366:1836-1838.\n- FDA Drug Safety Communication: Safety review update of cancer drug Revlimid (lenalidomide) and risk of developing new types of malignancies [FDA Safety Announcement]. U.S. Food and Drug Administration website. Available at:\n- Sonneveld P, Schmidt-Wolf IGH, van der Holt B, et al. Bortezomib Induction and maintenance treatment in patients with newly diagnosed multiple myeloma: Results of the randomized phase III HOVON-65/ GMMG-HD4 trial. Journal of Clinical Oncology. 2012; 30(24).\n- Palumbo A, Bringhen S, Rossi D, et al. Bortezomib-melphalan-prednisone-thalidomide followed by maintenance with bortezomib-thalidomide compared with bortezomib-melphalan-prednisone for initial treatment of multiple myeloma: A randomized controlled trial. Journal of Clinical Oncology [early online publication]. October 12, 2010.\nCopyright © 2018 CancerConnect. All Rights Reserved."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:b2a4183d-43b2-47b2-bda4-9658f14000b3>","<urn:uuid:8eb9450b-162d-477c-8eca-0c214acdabf8>"],"error":null}
{"question":"How do living walls provide versatility in plant selection for both decorative and edible purposes, and what are the technical components needed for proper irrigation of such installations?","answer":"Living walls offer great versatility in plant selection, accommodating everything from decorative perennials and annuals to herbs, peppers and other edible plants. They can even be designed with evergreens for year-round interest in cold climates. For proper irrigation, a system requires several key components: a timer and/or filter attached to the spigot, pressure control valve, distribution lines, control and safety valves, connectors if necessary, and drippers or emitters placed near the plants. These components can be set to work automatically or be controlled manually.","context":["Living Space: Living walls offer visual interest\n07/10/2014 10:52 AM\n07/10/2014 10:53 AM\nPlants are one of the most overlooked decorative accents in the home. But one thing’s for sure: Living walls – large vertical displays of green and/or flowering plants – can’t be ignored. A living wall instantly provides a touch of natural beauty and an attractive focal point for a room or outdoor space. And plants also offer a great way to improve indoor air quality.\nWhether you choose an indoor or outdoor living wall, the results are sure to spark plenty of interest.\nWhat to consider\nOutdoors, green walls can be used to separate various parts of the garden, provide privacy or create a beautiful green backdrop for a patio or other space. Watering and sunlight are not as much of an issue as for similar indoor displays.\nLiving walls, sometimes called vertical gardens, can be created with flowering plants, green plants or vegetables. You can choose anything from decorative perennials and annuals to herbs, peppers and other edible plants. A living wall of herbs would be a great addition for the gourmet cook. Consider a green wall for tomatoes or summer vegetables to make for quick and easy picking. These are especially nice if bending over is difficult for you.\nBefore you build, think about whether your living wall will be something you want to maintain year after year. If so, plant with perennials, but also consider how the wall will look during the winter. For year-round interest in cold climates, you can even plant an entire wall with evergreens.\nGreen walls can be designed in a variety of ways. A kit is the best option for assembling the wall quickly and easily. One handy choice is to use Woolly Pockets. These felt-like pockets can be filled with soil and plants and then hung from screws on a wall ($144, Woollypocket.com). They can be used indoors or out.\nAnother option is to build a tray with a multi-cell vertical planter. Insert a wall planter, such as a Grovert, a polymer unit with 10 individual planting cells, into a wood frame and you’ve got a beautiful green space ($33, verticalgardeningsystems.com).\nOf course, you can always go the do-it-yourself route. Probably the easiest way to build a green wall indoors or out is with vines. Simply plant and wait for them to climb up a trellis or other support. Soon you’ll have a solid green wall that provides privacy and beauty in your home or yard.\nWhen it comes to living walls indoors, a kit is preferable to trying to do the job yourself. Watering is easier, and often the mechanism is built in to the structure. Watering is done from the top of the planters so the moisture trickles down.\nSome wall kits are assemblies of single units and the segments need individual watering. The benefit of these units is that you can create exactly the size unit you want, as opposed to being stuck with a single unit only available in a specific size.\nJoin the Discussion\nCharlotte Observer is pleased to provide this opportunity to share information, experiences and observations about what's in the news. Some of the comments may be reprinted elsewhere on the site or in the newspaper. We encourage lively, open debate on the issues of the day, and ask that you refrain from profanity, hate speech, personal comments and remarks that are off point. Thank you for taking the time to offer your thoughts.","Lush gardens are a wonder to behold. We all love seeing those rows of plants spring up and watch veggies grow big, not to mention seeing the joyous colors of the season in the delicate flower petals we’re lucky enough to enjoy.\nAll of this takes work, of course, but for many gardening is a toil of love. However, even if one loves tending to their garden, some tasks can be grueling for the gardener and the plants. Watering is one of those landscaping tasks that can take up more time and money than a gardener predicts.\nSprinklers and hand-watering seem to be the irrigation choice of every movie character ever seen. The opening shots of a “typical suburban home in summer” scene tends to include a person watering the outdoor greenery with a hose. And who could forget the ill-timed sprinkling system in Caddyshack? As universal as these solutions may seem on the big screen, they aren’t the only ways to irrigate your landscape, and they definitely are not the most efficient.\nDrip irrigation, sometimes called “micro-irrigation” after the tiny water droplets it disperses, is a pipe or hose system with parts called emitters that distribute the water directly to the soil and roots around a plant. The simplest version is a garden hose manufactured with small holes that allow water to seep or lightly spray out. Drip irrigation can save up to 50% of the water typically spent using hand-watering or sprinklers. Many more benefits exist for the homeowner who installs a micro-irrigation system.\nA drip irrigation system:\n• is ready-to-install by the home gardener\n• can be found at many gardening supply stores\n• maintains a good balance of air and water\n• is easily adapted to odd-shaped landscapes\n• has over a 90% water efficiency rate (sprinklers hit 50-70%)\n• may be exempt from restrictions in a water-restriction zone\n• minimizes water runoff by inserting water into the soil\n• avoids over-watering, which can damage and kill plants\n• does not damage delicate new growth\n• can be used in containers and raised beds\n• can be automated with an inexpensive timer found in supply stores\n• is expandable – can be added onto as the garden gets bigger\nParts of a drip irrigation system (in order from the pressurized water source):\n• Timer and/or filter, usually attached to spigot\n• Pressure control valve\n• Distribution lines (the “pipes”)\n• Control and safety valves along the lines\n• Connectors (if necessary)\n• Drippers or emitters (near the plants)\n• Depending on the type of system installed, the timer, valves and emitters can all be set to work and shut off automatically or they can be controlled manually. Some drip irrigation kits come with controllers for the entire system.\n• A bit of gardening logic is required. Emitters must be placed near the roots of the plants. If the emitters are spaced too widely apart, some of the seeds and roots won’t be watered. Knowing the spacing of the rows will assist in emitter placement.\n• Unlike sprinklers, evidence of a drip irrigation system’s work isn’t always so obvious. It can be hard to know if the system is doing its job well. Occasional checks of the soil may be necessary.\n• Filters only work so well. Minerals in the water may collect at different parts of the system. Maintenance includes checking for clogs and mineral buildup. Filtering the water does help avoid clogs. Look for a system that has self-cleaning emitters.\n• Tubes that sit above ground are always a tripping hazard. Don’t run drip irrigators over any pathway or walkway, and avoid laying tubing over any “desire paths” in the landscape. Burying the length of distribution line that must traverse a path is worth the time and effort.\nTry it this year\nDon’t be intimidated. A drip-irrigation system, while not the stuff of movies, is still just as glamorous. And it will be easier to install than you may think. Most kits sold come with detailed instructions for the homeowner to follow, but a person new to drip-irrigation can find more information online. Saving on your water bill also helps save the environment. And that will give us all many more years of gardening to come."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:48b1bf29-ccd3-4268-8f53-79e553289fc3>","<urn:uuid:13d4c4f8-c639-4fcf-bf98-09e22a10c28c>"],"error":null}
{"question":"How do nuclear cataracts and presbyopia differ in terms of their relationship with aging?","answer":"Nuclear cataracts and presbyopia are both age-related conditions but affect different parts of the eye. Presbyopia occurs naturally in people's mid-40s when the lens loses its ability to flex and change with its surroundings, similar to adjusting binocular focus. Nuclear cataracts, on the other hand, develop deep in the center of the lens and cause cloudiness that prevents light from being focused clearly, impairing vision.","context":["Learn More About Eye Exams in Peterborough\nNelms Opticians believes in the importance of educating our customers. Here, you will find the answers to many of our most\nfrequently asked questions about eye care, eye exams, contact lenses and several common eye conditions. Should you not\nfind the answer to your specific question listed on this page, please feel free to contact us or pay our optical centre in\nPeterborough a visit today.\nQ. What is considered an eye emergency?\nA. Please contact us immediately (or go directly to your nearest hospital) if you experience any of the following eye emergencies.\nFlashes of light\nEye injury or pain\nCurtain-like vision obstruction\nSudden haze or blurring\nRed, swollen eyelids\nLoss of side vision\nQ. Do I need a prescription to purchase contact lenses?\nA. Yes, it’s illegal to sell contacts without one. A poorly-fitted contact lens or certain lens materials can cause discomfort, inflammation, infection, poor vision and permanent eye damage due to the lack of oxygen to your eye tissue.\nQ. What is myopia (nearsightedness)?\nA. When you’re nearsighted, it’s difficult to focus on objects in the distance. However, you are able to see objects up close fairly well.\nQ. What is hyperopia (farsightedness)?\nA. When you’re farsighted, close-up objects appear blurry, while objects at a distance are clear.\nQ. What is astigmatism?\nA. Astigmatism is a very common condition involving an uneven or irregular shape of your cornea. Your vision may be distorted both near and far as a result.\nQ. What is presbyopia?\nA. Presbyopia is a natural occurrence for people in their mid-40s and is comparable to adjusting the focus on a pair of binoculars. With age, your lens loses its ability to flex and change with its surroundings.\nQ. What is glaucoma?\nA. With glaucoma, there’s damage to your optic nerve resulting in gradual, irreversible loss of vision. It’s painless and has no symptoms, and if left untreated, can cause blindness. It usually comes with high eye pressure. It can be detected in a routine eye exam.\nQ. What is macular degeneration (AMD)?\nA. Dry macular degeneration (AMD) progresses very slowly and is caused from the deterioration of the macula in the centre of your retina. Wet AMD is severe and is often caused by a sudden loss of central vision. Both can be detected during an eye exam and must be treated immediately.\nQ. What are cataracts?\nA. Cataracts cause the lens in your eye to be cloudy by preventing light from being focused clearly, which impairs your vision. Some are slow growing, but some are fast, causing rapid vision changes. Cataracts are usually age-related and sun-exposure-related. Surgery is performed to remove them once they’re ripe.\nQ. What are dry eyes?\nA. When the quantity and quality of your eyes’ tear layers change it causes your eyes to feel dry and/or irritated. We carry many great eye drops to help alleviate the problem of dry eyes.\nQ. What are eye allergies?\nA. An eye allergy is caused when something from your environment irritates your eyes. This causes your eyes to fight back by releasing histamines, which can trigger itchy, red, swollen, inflamed itchy and watery eyes.\nQ. What should I bring to an eye exam?\nA. When you visit Nelms Opticians for an eye exam, be sure to bring a list of all your prescription medications, vitamins and supplements you’re currently taking. You should also bring your current glasses or contacts or your most recent prescription (contacts must be removed at least 20 minutes prior to the exam). Your eyes may be sensitive to light for a few hours after your pupils are dilated, so you may also wish to bring sunglasses with you.\nQ. Is my eye exam covered by my insurance?\nA. If you’re under 20 or over 65 and have a current Ontario Health Insurance card, your eye exam is covered yearly by this insurance. As these are the years that can have the most changes in sight, it’s highly recommended that you have this yearly exam. For ages 20-64, you’ll need to pay for your eye exam, and if you have a benefit plan that covers this, you can submit your receipt for reimbursement. Most people in this age range have their eyes tested once in every two years. We accept many forms of insurance at Nelms Opticians.\nShould you experience any problems with your eyes at any time, don’t wait, but rather call us to see if investigation is required, and we can direct you to the appropriate person for help. For any emergency, please go directly to the hospital.\nQ. What are transition lenses?\nA. Transition lenses change with the sunlight outside to either a grey or brown colour, making them effective sunglasses.\nQ. What are progressive lenses?\nA. Progressive lenses are bi/trifocal lenses that allow you to wear one pair of glasses for all reading, computer and distance-related activities.","The Complete List of Eye Cataract SymptomsA cataract is a visual impairment where the lens of the eye, which is just behind the iris and pupil, becomes cloudy and opaque. This is the most common cause of visual impairment in people aged 40 and older and is the world’s leading cause of blindness. According to Prevent Blindness America, there are more cases of cataracts than there are cases of diabetic retinopathy, glaucoma, and macular degeneration combined. Over 22 million Americans are diagnosed with cataracts, and the number is expected to exceed 30 million by the year 2020.\nThe three most prevalent types of cataract are:Subcapsular cataract: These occur at the back of the lens. Diabetics taking steroid medications are most at risk for this type of cataract. Nuclear cataract: These develop deep in the center of the lens and are most often a symptom of aging. Cortical cataract: This form of cataract is typified by white opacities that are usually wedge-like in shape. They begin to form in the periphery of the lens and move toward the center like the spokes of a wheel. Cortical cataracts occur in the cortex- the part of the lens surrounding the nucleus.\nThe Symptoms of Cataracts\n- Blurred, cloudy, or dim vision\n- Difficulty seeing at night\n- Doubled vision in one eye\n- Seeing ‘halos’ around lights\n- Colors fade or turn yellow\n- Sensitivity to glare\n- Frequent eyeglass changes\n- Having to use more light to read\nThe Causes of CataractsThe lens inside your eyes is like the lens of a camera. It focuses light coming into the front of the eye onto the light receptors on the back of the eye. It is made of proteins and water primarily. Those proteins are specially aligned so that they do not obstruct the light passing through the eye. But as we age that alignment of the proteins can degrade, causing cataracts. While the root causes of cataracts are not well understood, there are a number of risk factors that have been identified as being associated with an increased likelihood that a person will develop the condition. These risk factors include;\n- Previous eye injury or inflammation\n- Previous eye surgery\n- Ultraviolet radiation exposure\n- Use of corticosteroid medications\n- Use of statin medicines\n- Hormone replacement therapy\n- Alcohol consumption\n- Family history\n- High myopia\nPreventing CataractsNot all the experts agree that cataracts can be prevented at all, but the association of risk factors certainly seem to indicate that there is a possibility of at least reducing or delaying the onset of the condition. Studies have shown that Vitamin E and carotenoids are associated with lowered risk of developing cataracts. So diet, and avoiding risk factors like smoking, excessive exposure to ultraviolet light, and losing weight are all good ways to reduce a person’s chances of getting this eye condition.\nDiagnosis and TreatmentIn order to diagnose cataracts, your doctor will perform a visual acuity test, a slip-lamp examination, or a retinal exam. Most eye surgeons will recommend eye surgery as soon as symptoms develop, or as soon as they begin to degrade the quality of a person’s life. But the success rates with all forms of cataract surgery are very high. The eye is self-sealing and contains the fastest healing tissues in the body, so no stitches will be needed. While the risks are small, the benefits are almost always considered great enough to outweigh the risks. As with any surgery, there is a risk of infection. With cataract surgery, there is also a chance that the patient will suffer a retinal detachment.\nAfter TreatmentMost cataract surgeries take only about ten to twenty minutes to complete, and recovery is usually fast and complete with great improvements to the patient’s vision. Many patients say their vision after cataract surgery is even better than it was when they were a child. After eye surgery, patients can expect to feel some discomfort for three days up to a week, but most patients report feeling that they have recovered completely within one to two days. When your surgery is completed, you will be taken to a recovery room where you will wait for the effects of the sedatives to wear off. You will need someone to drive you home, and most patients will want to rest for a few hours thereafter. You may be able to remove the protective shield your doctor will have provided within a few hours. Follow the advice you are given as, and remove it only after the recommended period of time. Most patients will be prescribed an eye drop solution which is meant to help prevent infection and reduce discomfort. Your doctor will direct you to follow a schedule, the details of which will depend on the type of eye drops you are prescribed. Not every cataract surgery patient receives the same kinds of eye drops as outcomes and expectations vary. To reduce your recovery time, discomfort, and to avoid damaging your eyes during this delicate period of time follow these guidelines.\n- Use your eye drops according to the advice your doctor gives you. You will receive a detailed written schedule which should be clear and easy to follow.\n- Don’t drive for two days.\n- Do not bend over, or perform any physically exerting activities. This is to prevent any internal pressure from pressing against the affected area.\n- Take care when walking, as you may be visually impaired for a period of time, and bumping into objects could damage your eyes.\n- You should not swim or submerge your face in water for one week. You can shower or bathe, as long as you are not submerging your eyes.\n- Avoid any eye irritant, and do not rub your eye. Rubbing the eye is dangerous for anyone, as it can cause a retinal detachment."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:f9200dcc-6d62-4593-9229-8267a0ca65f1>","<urn:uuid:490c87d0-ce07-47b1-8f6a-e92e8c04b450>"],"error":null}
{"question":"What is the relationship between stream temperature and aquatic life, and how do natural stream corridors help maintain healthy water conditions?","answer":"Stream temperature has a critical impact on aquatic life, particularly for species like salmon and trout. These fish require cool water with high oxygen content to survive. When water temperatures exceed 20°C, these fish begin to weaken, and at 25°C they start dying, with some rivers now reaching dangerous temperatures of 30°C. Natural stream corridors help maintain healthy water conditions in several ways. They provide shade through tree cover that keeps water temperatures down, while their vegetation filters pollutants and improves water quality. The riparian corridors also store flood waters, recharge groundwater, and maintain dynamic stability of the stream system. Additionally, natural corridors help filter pollutants and provide essential habitat for various plants and animals, contributing to the overall health of the stream ecosystem.","context":["Once upon a river there were emerald-faceted pools. Their depths flashed gold and silver with the flitting movements of Atlantic salmon and sea trout. Streambanks were sheltered with towering trees. Massive root systems armoured the fertile soils against the ravages of ice and high waters like great fingers holding the earth. Huge trunks and limbs offered cool shade while casting off a shower of leaves, needles and insects.\nThey became important nutrients to the groundwater and direct food for life in the river. As older trees on the banks died and eventually toppled into the stream, their hulks became imbedded in gravel. Sparkling high water plunged over them, reshaping the bottom, maintaining pools and providing shelter for insects, fish and other animals.\nWide and shallow, hot, polluted and with low water levels, many Atlantic rivers today have been transformed into sewers to the sea. Victims of our ignorance and greed, they’ve gradually been degraded to mere drainage ditches. Efforts to deal with this major ecological disaster have proven grossly inadequate. Acclimatised to only the last chapter of a 300-year horror story, the average person today has never read the book and considers such rivers normal.\nThere are solutions. Hope lies in restoring and rehabilitating our waterways.\nSeveral hundred years of land “development” commenced with the arrival of white settlers along eastern North American shores. Most land clearing was done with meagre thought to how rivers function. Land clearers ignored the role played by large living tree roots in holding riverbanks intact. They also neglected to note the subtle importance of the river’s winding or meandering habit, which serves to slow and absorb the energy of the water’s downstream flow.\nConsider the earth’s gravitational pull as felt by downhill skiers. Descending a hill in a straight path generates maximum speed. On the other hand, winding back and forth across and down the slope, a skier’s trip becomes longer, and speed is reduced. Rivers and streams, for reasons of geology, hydrology and other natural forces, usually adopt this meandering, slower way, with a “pool — shallow riffle — and turn” sequence repeated until an interruption, such as a rock outcrop, rearranges the pattern. Turns actually function as energy absorbers.\nPre-settlement rivers and streams in eastern North America tended to be more deep and narrow, and longer due to this meander pattern. They were also gradual in slope. Their channels carried less water because historically, when snow melt poured off hillsides into valleys, rivers topped their banks and spread over the surrounding grassy or wooded flood plain. This vegetation on the valley bottom slowed that water, causing water-born sediments to settle out. In early times valley bottoms acted as giant sponges during floods, soaking up water into underlying gravel seams and organic layers. When droughts occurred, these reservoirs of cool, pure water could seep back into rivers to augment flows and maintain fish habitats.\nThe accumulating soil richness from this process rendered flood plains attractive for agriculture. Farms gradually replaced forests. In the days before electricity, barns were built beside brooks for watering livestock. They are still there, leaching manure. In the woods, logs and pulpwood were cut, hauled and piled along shores over winter periods — to be driven downstream on the high water every spring. Rivers were channelized (straightened) into log “highways” interspersed with dams. By the late 1800s, mill wastes clogged many river channels. Land clearing, drainage projects, channelization and other human endeavours have continued to the present day to erode, flush and straighten rivers, increasing their slope and speed and decreasing their ecological health.\nWithout trees, riverbanks are more vulnerable to high water and ice. Doubling the speed of a river’s flow allows it to erode four times as much bank material and to carry 64 times the amount of material downstream. Widened river channels contain more flood water, capturing the water that once jumped more narrow banks and was then slowed by vegetation and forest cover. Mounds of rapidly eroding bank material now pile up in the channels, diverting flows to the sides, exerting more pressure upon denuded riverbanks during heavy rains or spring run-off. When high water in a fast-flowing, channelized river finally arrives at a turn and slows down, the rubble carried by the current settles to the bottom, eventually plugging the existing channel. This causes increasing pressure on adjacent banks until a new channel bursts through and carves its way across the valley floor, tearing out trees, topsoil and boulders.\nHumans have been clearing land to the water’s edge for years, for farming, forest harvesting, homes, cottages, and businesses like golf courses. Heavy rains that used to soaked slowly into flood plain forests now develop hit-and-run patterns over cleared lands. Flooding becomes more common. River beds were raided in the past for gravels to build such things as the Trans Canada Highway. Rivers without riparian zone (shoreline) protection can suffer yet another consequence. Weak-banked and widened, some become ice factories.\nWhen winter descends during low water conditions, wide, shallow rivers sometimes freeze to the bottom. Imagine what that does for the insect life, young salmon and trout hiding in spaces between rocks. When thawing occurs, water begins to flow over existing ice. As temperatures drop below freezing, new ice layers form on top of the ice. Successive layers accumulate with fluctuating weather until thicknesses of more than two metres sometimes develop! That thick ice is finally wrenched off the bottom after a spell of warm weather.\nAn enhanced form of riverbank bashing then begins. Often these mega-blocks fetch up on bridge abutments, creating ice dams, and flooding neighbourhoods. Newspapers proclaim that nature and the river have run amuck. Humans, albeit unintentionally, have derailed nature’s forces to create the more raging, ice-clogged flood situation.\nFarmers found that having flood plain water close to the ground surface meant poor root growth for planted crops. They ditch or install drainpipes — often with taxpayers subsidies — to flush water out of the valley floor. This exacerbates the effects of both high and low water. Forest clearcutting in watersheds also produces faster, higher runoff after rains. Humans magnify flood effects even more by emptying storm drains directly into streams and rivers, instead of discharging the water onto flood plains, where it could seep in and enter the watercourse gradually.\nThere are a host of other problems. Draining and infilling swamps, marshes, and other wetlands destroys their ability to absorb and moderate flood water, to produce clean water and to release flows during low water conditions. Crop irrigation extracts water during critical summer periods. Runoff from fields frequently contributes a witches’ brew of fertilizers, silt, pesticides and manure. Years of stream bank trampling by cows and other heavy domestic animals renders streams wide, shallow and polluted. Towns extract drinking water and return sewage.\nNot surprisingly, aquatic life has fared poorly. Even headwater streams serve as nurseries for speckled trout. Most older dams were not equipped with functional fish ladders, preventing migratory populations of Atlantic salmon, trout, sturgeon, gaspereau and smelt from reaching spawning grounds. Culverts and bridges are also common impediments. Pools that are so important for adult salmon and trout tend fill in and disappear. Too much distance between pools means that trout and salmon will no longer move up and through the waterway. Acid rain alone has rendered many rivers devoid of salmon in Nova Scotia. Other airborne, heavy-metal pollutants like mercury are seriously affecting fish-eaters like loons. Wide and shallow channels absorb more summertime heat. Cool water contains the extra oxygen which salmon and trout require. As water temperatures exceed 20° Celsius these fish weaken. At 25° C trout and salmon begin to die. Many rivers in New Brunswick and Nova Scotia now reach 30° C.\nDegraded rivers are repairable. Nature is slow to heal them without large dead trees. So human help can make a vital difference. The standard cure for river banks made unstable by humans is to place large rock “rip-rap” along them. This requires the use of heavy machinery, and puts sections of the river in a straight jacket. It’s a high priced antidote for mismanagement. There are softer restoration technologies, proven, more cost effective, and used by fish and game groups and river associations across the Maritimes. These include digger logs (logs placed across a channel in a way that encourages the flow of water to dig a pool), deflectors (placed to trap sediments in the water, while deepening and narrowing the channel), rock sills (to help stop the movement of streambed materials downstream), and tree planting on riverbanks! Restored waterways exist in Atlantic Canada, but funds are scarce and the agents of habitat destruction are still active.\nThe jurisdictional framework for managing lands along waterways tends to be an overlapping quagmire of municipal, provincial/state, and federal government departments. The federal government’s “no net loss” of aquatic habitat policy languishes largely unheeded. Government departments, with agriculture, forestry and other specific mandates, frequently exist to serve clients like farmers, forestry folks, miners, and so on. Each department is run by bureaucrats with scant ecological understanding or background. With economic and accounting blinders on, they tend to serve industrial/business rather than public/environmental interests. The idea of sound, sustainable, ecological underpinnings for their policies is a buzz-word farce. Individual department policies facilitate new development and frequently conflict. As a biologist, I was hired by a provincial government to help volunteer groups restore freshwater habitats. Concurrently, the same government was subsidizing farmers and others to inadvertently destroy fish habitat. One step forward, three steps back. Universally accepted, sensible environmental guidelines for development are a long way off with this chorus line of myopic perspectives and four year mandates.\nIt is possible to farm, selectively harvest forests, build dwellings and roads, live and have recreational pursuits in harmony with rivers, lakes and wildlife. Instead, we took a river like the Cornwallis in Nova Scotia and added the pig-manure-equivalent of sewage from a city of 250,000. That only stopped when hog farming collapsed. We extract water for irrigation and other purposes until, at one point, 120% of the available water was spoken for in permits. What about fish, beaver, otter and others? Water taken for one town returns as treated sewage in volumes that are sometimes equal to the flow in the river. When sampling determined that the river was too contaminated to irrigate strawberries, the solution was to cut the funding for the monitoring!\nImplicated farmers point to towns. Towns, in their turn, blame farmers. When will we wake up? A blessed few look in the mirror, see themselves as part of the problem, and begin to do something positive.\nOur rivers, streams and lakes could use more of these people.\nNova Scotia Naturally is a monthly column by Wildland Writers, a roster of Nova Scotia wilderness experts. This group includes Donna Crossland, David Patriquin, Bob Bancroft, Alain Belliveau, Mark Elderkin, Matt Miller, William Martin and Jamie Simpson.","Every stream is a dynamic hydrological system that is continually altered by the changing character of the watershed. Streams reflect land use by changing course, overflowing, eroding their beds, and depositing sediment. Modification of a stream channel causes channel adjustments such as bank erosion, channel deepening, or sediment deposition, for some distance both upstream and downstream. Streams are also dynamic biological systems comprised of plants and animals. The components of this system are interdependent and are fundamentally linked to habitat in and around the stream.\nInformation obtained from ODNR Ohio Stream Management Guide\nAs described above, streams are dynamic systems, which constantly move and change in their pursuit to reach equilibrium. In their natural state, streams and their associated floodplains provide a variety of important functions including the movement of water and sediment, storage of flood waters, recharge of groundwater, treatment of pollutants, dynamic stability, and habitat diversity. Disturbances to this system, either natural or human-induced, places stress on the system and has the potential to alter structure and/or impair the ability of the stream to perform ecological functions.\nFor more information on streams , contact Gary Norcia, Community Coordinator, by e-mail or phone at (330) 722-9318.\nHow do floods help the stream?\nFloods are a natural process which helps maintain the health of the river. Just as fire is essential in a forest or prairie, flooding is essential to the river ecosystem. Various plant species are adapted to the flooding conditions; thriving on periods of wet or semi-dry conditions. Additionally, floods cue many fish species to begin their spawning migrations.\nFloods also help to form habitat. The energy associated with a flood can be very powerful. Eroding banks and creating side channels and islands. In doing so, they also create places for animals to live, hide, and feed.\nFloods also help plants and animals migrate to new areas downstream. They can also clear away old vegetation, helping the understory grow.\nWhy are floodplains important?\nNatural stream channels have an associated adjacent land area called a floodplain. These adjacent areas are periodically inundated by flood waters and serve a variety of functions.\nStudies have shown that floodplain size is directly related to the overall health of a stream\nFloodplains absorb and store flood waters, reducing velocity and allow for the slow release to the stream. They also improve water quality, plants within the floodplain filter sediments and pollutants. Floodplain trees and plants also help to anchor the river banks preventing erosion and providing shade to reduce water temperatures. Leaves which fall into the water are broken down by aquatic insects and other organisms. Providing the basis for the river's food chain. As stated above, floodplains provide fish and wildlife the places they need to feed and reproduce.\nThe Federal Emergency Management Agency (FEMA) has been involved in management efforts for many years and has mapped many floodplain areas. However, unmapped, headwater or intermittent streams have historically not been viewed as areas of importance. These areas are becoming increasingly more significant as a direct result of their elimination.\nAs more and more land is developed, floodplain encroachment occurs more frequently, resulting in cumulative impacts to the downstream areas.\n|Stream Corridor Restoration and Management\nThis corridor is a valuable ecosystem providing many functions since the beginning of time. Recently, more and more people are recognizing the importance of stream corridors.\nUrbanization, more than any other common land use, damages the quality of streams. With the alteration of the landscape from vegetation to hard surfaces, such as roads and parking lots, the velocity of runoff increases and the filtering capacity decreases. As a result, this increased runoff causes erosion problems leading to undercut banks, flooding, and loss of habitat.\nChannel size is determined by sediment discharge, sediment particle size, stream flow, and stream slope. The figure below was proposed by Lane (1995) and shows this relationship.\nWater flowing through a channel, such as a stream, has the ability to do work (i.e. transport sediment). There are three main modes of sediment transport: 1. solution load -- dissolved material carried in the flow (in effect invisible), 2. suspended load -- finer sediment (silts and clays) suspended by turbulence in the flow, and 3. bed load -- coarser sediment (sand and gravel) that slides, rolls or skips along the stream bed.\nA natural stream channel has a definitive dimension, pattern and profile. David Rosgen developed a classification system which further profiles these stream characteristics. For more information visit the Wildland Hydrology website.\nWith and increased awareness of human-induced effects on the natural stream corridor ecosystem, more and more emphasis has been places on the restoration of these critical areas. Stream corridor restoration can range from very simple to very complex. An array of practices can be used in restoring natural stream functions. Some of these practices include vegetative plantings, log jam removal, fencing (to keep animals out), and a variety of bioengineering practices.\nSome of these practices include vegetative plantings, log jam removal, fencing (to keep animals out), and a variety of bioengineering practices.\nFor more information on management and restoration practices: The Ohio Department of Natural Resources Division of Water has developed a series of Ohio Stream Management Guide fact sheets. These fact sheets are available for on-line viewing and download as PDF files. An Introduction to Stream Management Who Owns Ohio's Streams? Natural Stream Processes Permit Checklist for Stream Modification Projects Restoring Streambanks with Vegetation Trees for Ditches A Stream Management Model Biotechnical Projects in Ohio Tree Kickers Evergreen Revetments Forested Buffer Strips Live Fascines Gabion Revetments Riprap Revetments Live Cribwalls Stream Debris and Obstruction Removal Deflectors Eddy Rocks Large Woody Debris in Streams Gravel Riffles\nFor more information on management and restoration practices:\nThe Ohio Department of Natural Resources Division of Water has developed a series of Ohio Stream Management Guide fact sheets. These fact sheets are available for on-line viewing and download as PDF files.\nAn Introduction to Stream Management\nWho Owns Ohio's Streams?\nNatural Stream Processes\nPermit Checklist for Stream Modification Projects\nRestoring Streambanks with Vegetation\nTrees for Ditches\nA Stream Management Model\nBiotechnical Projects in Ohio\nForested Buffer Strips\nStream Debris and Obstruction Removal\nLarge Woody Debris in Streams\nAdditionally, a technical field guide Stream Corridor Restoration: Principles, Processes and Practices prepared by the USDA Natural Resource Conservation Service is available on-line.\nThere are many benefits to maintaining the natural riparian corridor. Among other things, natural corridors provide a variety of functions including: storing flood waters, filtering pollutants, recharging groundwater, and providing habitat for plants and animals. If you own property containing or adjacent to a stream, our office recommends establishing a management plan.\nAmong other things, natural corridors provide a variety of functions including: storing flood waters, filtering pollutants, recharging groundwater, and providing habitat for plants and animals. If you own property containing or adjacent to a stream, our office recommends establishing a management plan.\nThe following information has been established to inform landowners on the importance of protecting and restoring these vital habitats.\nStream Management Brochure\nCommon Flooding Myth\nCommon Myth about Yard Waste\nIntroduction to Stream Management\nForested Buffers Strips\nBenefits of Riparian Trees\n\"Life at the Water’s Edge: A stream reference manual for the homeowner.\" (contact our office for a copy)\nTaken from Life at the Water's Edge\nOur office assists with the organization of stewardship projects throughout the county, such as stream clean-ups and storm drain stenciling. Below are a list of informational brochures and websites with further information.\n|SOS - Sign Our Streams\nAs human beings, we often feel the need to provide names to those things we cherish most: national monuments, a never-fail 7-iron, our first car, and yes…even our streams and rivers. Did you know there are at least 35 separately named streams and rivers that meander through the county? To draw attention to these invaluable resources and encourage public stewardship of our waterways, the District has a goal of erecting stream name signs at every point a named waterway intersects with a roadway.\nThe District’s SOS project is now well underway, with several stream crossings already sponsored throughout Medina County. Sponsorship fee is $150 (new reduced price). Two signs that display the stream name, artwork and your name or message, will be erected, one on either side of the stream crossing. Artwork for the signs was designed by local high school students, and are unique to each named stream.\nIf you wish to contribute to the efforts aimed at protecting a very special resource, or if you are simply looking for an unusual gift for that person who has everything, consider sponsoring a stream crossing through SOS.\nSponsorship guideline and form\nA wetland is defined as a lowland area saturated by surface or ground water at a frequency and duration sufficient to support hydrophytic (water loving) vegetation. There are many different types of wetlands such as peat bogs, fens, wet prairies or meadows, marshes, swamps, floodplain forests, and vernal pools.\nThree factors are typically used to classify an area as a wetland. All wetlands have:\nWetlands provide a variety of functions and values. One of the greatest economic benefits provided is flood control. Wetlands store excess water and the thick vegetation slows down floodwaters, reducing downstream flooding. Wetlands also provide water quality benefits; they filter sediments and nutrients from surface water. They also provide a variety of products for human use and offer many opportunities for recreation.\nThere are four basic types of wetlands projects: restoration, enhancement, creation, and construction. Restoration involves rehabilitating converted wetlands, enhancement improves a slightly degraded wetland or manages an existing wetland to serve a special function, creation is the establishment of a wetland in a historically upland area, and construction is the building of a wetland to treat nonpoint and point sources of water pollution.\nInformation Provided by USDA SCS Agriculture Information Bulletin and\nBelow is a list of links to information on wetlands construction, recognition, value, and regulations.\nLiving in Harmony with Wetlands\nOhio EPA Wetlands Information\nOhio Wetland Facts (Small Wetlands)\nUS Army Corps of Engineers Buffalo District\nUS Army Corps of Engineers Huntington District\nUSEPA Wetland Website\nUSGS Midwest Wetland Flora\nVolunteer Wetland Monitoring Resource Guide\nWetland Values and Trends\nWetlands Regulation Center\nNational Wetlands Research Center\n|Water Quality/Nonpoint Source Pollution (NPS)\nNPS pollution occurs when rainwater or snow melt carries sediment, organic materials, nutrients, or toxins into rivers, lakes, and streams. During large storms the runoff to surface water and the rate of infiltration to groundwater increases, and so does the rate of NPS pollutant movement. Almost any land use can lead to NPS pollution. The more intensive the land use, the greater the chance of pollution.\nNPS comes from a variety of sources in both urban and rural areas. Examples of NPS pollution include:\n|National Pollutant Discharge Elimination System\nPermits for wastewater have been required through the Clean Water Act since 1972. The USEPA began to regulate stormwater with the inception of the National Pollutant Discharge Elimination System (NPDES) Program. In 1990, NPDES Phase I was established to regulate Municipal Separate Storm Sewer Systems, Industrial Storm Water, and Construction Sites disturbing 5 acres or greater. More recently, NPDES Phase II was established in 1999. This phase added small municipalities (in urbanized areas as defined by census data) and construction activities disturbing 1 acre or greater to the list of permitted entities.\nUnder Phase II, permitted entities must develop a Municipal Storm Water Program (MSWP) aimed at reducing the discharge of pollutants and protecting or improving existing water quality by implementing six minimum control measures. The six minimum measures are:\nCheck out the following links for more information:\n|Before we talk about watershed planning, it is important to understand what exactly a watershed is. A watershed is an area of land from which all water drains to a common location. The watershed is generally named for the lake or river to which it drains. For example, Medina County is split by the continental divide in which water flows to one of two locations; either north towards Lake Erie or south towards the Ohio River. Watersheds come in many shapes and sizes. Smaller watersheds that feed into the same stream, river, lake or ocean are called sub-watersheds of that larger system. Although Lake Erie and the Ohio River are large bodies of water, the areas draining to them are considered sub-watersheds of the Atlantic Ocean and The Gulf of Mexico respectively.\nThe sub-watersheds of Lake Erie and the Ohio River watersheds in Medina County are designated on the map provided.\n\"The health of a stream, river, or lake is a reflection of how its watershed is treated.\" Water does not recognize political boundaries; therefore, activities of one political entity can cause problems to downstream entities. As a result, problems should be looked at on a watershed level. Below are some keys to successful watershed management available from the Conservation Technology Information Center (CTIC).\nThe Watershed Planning Process\nGet to Know Your Watershed\nBuild Local Partnerships\nDetermine Priorities for Action\nConduct Educational Programs\nProvide Landowners with Assistance\nEnsure Implementation and Follow-up\nFor more information of watershed planning visit the following sites:"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:1f6c4f2c-d3b7-4839-86f1-c538aac4cb03>","<urn:uuid:aa941be3-5025-4f14-9d31-953021910963>"],"error":null}
{"question":"I am a model enthusiast. Between the Sword T-38A Talon and Hasegawa TF-104G Starfighter kits in 1/48 scale, which one would be more suitable for a beginner?","answer":"The Hasegawa TF-104G Starfighter kit would be more suitable for a beginner. The Sword T-38A Talon kit requires experienced building skills due to challenging assembly, particularly with parts like intakes that don't fit well and require bodywork. In contrast, the Hasegawa TF-104G is rated as a basic skill level kit and is described as a nice kit overall.","context":["Sword 1/48 T-38A Talon Kit First Look\n|Date of Review||May 2008||Manufacturer||Sword|\n|Subject||Northrop T-38A Talon||Scale||1/48|\n|Kit Number||48004||Primary Media||Styrene/Resin|\n|Pros||First and only styrene kit of this subject in 1/48 scale||Cons||Challenging to assemble|\n|Skill Level||Experienced||MSRP (USD)||$OOP|\nThe T-38 Talon was a trainer variant of Northrop's project N-156 that developed a lightweight, low-cost, single-seat, supersonic interceptor. While the USAF was not very interested in the single-seat aircraft, they lacked a supersonic trainer in the 1950s. The venerable T-33 Shooting Star was the best jet trainer in the inventory and the Air Force recognized that they needed a trainer to bring pilots into the supersonic age.\nThe T-38 first flew in 1959 and would enter service in 1961. Production would run through 1972 with over 1,100 aircraft built. The T-38 was initially used primarily as a training aircraft, taking new pilots out of the T-37 and giving them experience with higher performance aircraft. The aircraft continues in this role today, with a number of them being updated to the T-38C configuration to provide pilots with the look and feel of contemporary service aircraft.\nThe T-38 would soon see another dimension in its capabilities, that of an aggressor/adversary aircraft. The Air Force and Navy used the T-38 as a MiG-21 simulator until more capable F-5 airframes could be obtained. Even after the T-38 completed its aggressor mission, a number of the aircraft were modified into the AT-38B configuration to provide 'fighter lead-in' training to teach new fighter pilots the essential skills of the mission at a significantly lower cost per hour than training on their assigned aircraft.\nAlmost 50 years after the T-38's first flight, the aircraft remains in service with the USAF with no replacements on the horizon. The T-38 also serves with NASA as a proficiency trainer and has provided flight training in a number of international air forces including Germany, Portugal, Taiwan, and Turkey.\nA number of years ago, Fujimi produced the T-38 Talon in '1/48 scale' but the kit actually scales out to 1/50. In addition, the airframe was really an F-5B, so until Sword released this kit, there has not been a styrene rendition of the Northrop workhorse in 1/48th.\nThe kit is molded in dark gray styrene and presented on two parts trees (actually 1 1/4 parts trees) plus a single tree of clear parts. A nice array of resin details accompany this kit which, on opening the box, makes this kit look really nice!\nUnfortunately, the reality of this kit is that it will require some good building skills as some of the parts, like the intakes, do not fit well onto the fuselage halves and will take some skill and bodywork to get everything just right.\nTake a look at those resin parts. If they look familiar, they should, many of them are out of the same patterns used in the Classic Airframes 1/48 F-5 series including the F-5B (review here). That is because same folks that produced this kit also produced the Classic Airframes F-5s (which also share some of the same fit challenges).\nMarkings are provided for two examples:\n- T-38A, 61-3263, Air Training Command\n- T-38A, 61-0836, TAC, Holloman AFB, NM\nThis kit is recommended to experienced builders who can work with resin and limited run styrene-based kits.","Hasegawa 1/48 TF-104G Starfighter Kit First Look\n|Date of Review||November 2006||Manufacturer||Hasegawa|\n|Kit Number||07240/PT40||Primary Media||Styrene|\n|Pros||Nice kit||Cons||See text|\n|Skill Level||Basic||MSRP (USD)||$29.95|\nLess than five years after Chuck Yeager broke the sound barrier, Clarence 'Kelly' Johnson was looking at the first combat experience between jet fighters over the skies of Korea and understood the need for an aircraft that could reach high altitude and affect a high-speed intercept to achieve and maintain air superiority. His revolutionary Model 83 was designed to meet that need and was submitted to the USAF as an unsolicited proposal.\nThe Air Force agreed with the need, but decided to seek other ideas from industry. Designs were submitted by Republic and North American, but a cautious Air Force staff opted for the Lockheed design. The F-104 was born.\nTwo XF-104s were delivered less than two years later, but the first production F-104As would not enter service until early 1958.\nThe F-104 was the first operational aircraft to fly above Mach 2 (twice the speed of sound). Its small airframe enclosed a powerful afterburning J79 engine which could take the F-104 from the ground to 80,000 feet in less than five minutes.\nThe F-104G and its two-seat trainer variant, TF-104G, was produced as a multi-role fighter for operations in Belgium, Germany, Holland, and Italy, with each of these countries producing a combined total of over 1000 airframes. Many of these would find their way into other Air Forces including Greece.\nThere was no doubt in my mind that Hasegawa's 1/48 F-104 series was going to be impressive. In fact, before this kit was released, it was the Hasegawa 1/32 F-104 series that was the best F-104 in any scale. This kit is NOT a scaled down version of that 1/32 scale kit. This is all new design work with scribed detailing and lots of detail that I wish they'd scale up into a new 1/32 version.\nThe kit is molded in light gray styrene, and is impressive with its sharply-scribed detailing. It is presented on eleven parts trees, plus two trees of clear parts. A number of the trees are common to the other 1/48 Starfighter releases, but the fuselage halves, the tree containing the dual cockpit, and one of the clear trees are all different tooling from the previous single-seat releases.\nThe details start in the cockpit with each ejection seat being comprised of nine parts. No seatbelts or harnesses are molded in place, so you'll need to obtain some photo-etch to address these issues. Decals are provided for the side consoles and instrument panels.\nThe afterburner chamber is the nicest I've seen from Hasegawa, with the business end of the J79 protruding into the chamber and a finely molded flameholder/spray ring placed at a scale distance from the turbine face.\nThe cockpit, main wheel wells, and afterburner section are mounted into the fuselage halves before gluing the fuselage together. The new lengthened nose underside with the nose wheel well mounts underneath the nose along with a blanking plate.\nThe tiny wings feature positionable leading edge and trailing edge flaps, as well as positionable ailerons. In addition, the rudder, stabiliator, and speed brakes are also positionable.\nAs this is a trainer variant, the kit only provides the tip tanks and empty underwing pylons.\nThe markings included in this kit are for :\n- TF-104G, 27+73, JBG 31, Luftwaffe\n- TF-104G, 4-36, 4th Stormo/20th Gruppo, Italian AF\n- TF-104G, 4-44, 4th Stormo/20th Gruppo, Italian AF\nI am really glad to see the two-seat F-104G available as there are LOTS of colorful schemes to be had. I am hoping to see the Lockheed demonstrator \"World Starfighter\" appear in the aftermarket world, but I am happy that Superscale has released a set for the Bicentennial TF-104G that was assigned to Luke AFB in 1976. You can see a photo of the aircraft here.\nMy sincere thanks to HobbyLink Japan for this review sample!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:8d1e6d2c-f096-4046-ac4c-05679f4cd2e0>","<urn:uuid:a2461a15-b372-4ac4-805e-ed98eaa63322>"],"error":null}
{"question":"Hey, I'm new to stock trading. What services do stockbrokers offer and what are some shady practices I should watch out for?","answer":"Stockbrokers offer several key services: buying and selling stocks on behalf of clients, providing advisory services based on market expertise, and conducting research on companies and market trends. They can operate under different account types - discretionary (broker makes decisions), advisory (broker gives advice), or execution-only (broker only executes client orders). However, some unethical brokers engage in problematic practices like churning (excessive trading), making unauthorized portfolio changes, providing misleading illustrations about investment products, or failing to disclose all necessary facts about investment opportunities. They might also recommend unsuitable investments or fail to properly explain the risks associated with different investment options.","context":["Who is a Stockbroker?\nA stockbroker is a professional trader who buys and sells shares on behalf of clients. The stockbroker may also be known as a registered representative or an investment advisor. Most stockbrokers work for a brokerage firm and handle transactions for a number of individual and institutional customers.\nBrokerage firms and broker-dealers are also sometimes referred to as stockbrokers. This includes both full-service brokers and discount brokers, who execute trades but do not offer individualized investing advice..\nTypes of brokers\nBased on the types of clients they cater to, stock brokers are usually classified into two distinct categories:\n- Institutional Stock Brokers: They have large institutions and companies as their clients and trade in securities on behalf of them. They offer services such as investment banking, securities services (IPO or secondary offerings), advisory services, and brokerage services to institutional investors.\n- Personal Stockbrokers: Personal stockbrokers offer investment banking, advisory services and brokerage services to small businesses and individual investors.\nServices offered by a Stock market broker\n- Buying : One of the most basic responsibilities of a stockbroker is to buy stocks on behalf of his client; he may do this in different ways, depending on the type of account the client has. In a discretionary account, the stockbroker buys stock for a client based on some prearranged guidelines. In an advisory account, however, the stockbroker only advises a client on what stock to buy, while in an execution account, the stockbroker only buys stock that the client has specifically indicated.\n- Selling: The other responsibility a stockbroker has is selling stock on behalf of a client. Just as in the case of buying stock, the stockbroker can only sell stocks of a client based on the account that a client signed up for. If a client has an execution-only account, the stockbroker can only sell a client’s stock when asked to do so. If a client has an advisory account, a stockbroker can only advise the client to sell his stocks, while if a client has a discretionary account, a stockbroker has some leeway on selling the stocks based on a prearranged guideline.\n- Advisory services: Stock Market brokers possess expertise related to the working of stock market, performance of stocks, market trends, and so on. Besides, they have access to the database and research findings of brokerage firms that they are associated with. Hence, they can provide excellent investment advice to their clients.\n- Research: Competent stockbrokers research accounting, economic and technical analysis of different companies and stocks. Their findings form the basis of their feedback to a client. If a stockbroker believes a client’s stock’s price will drop drastically, he’ll advise him to sell when the price is still high. If, on the other hand, a stockbroker believes the price of a client’s stock will rise significantly, he may advise him to retain the stock.\nEducational Requirements for Stockbrokers\nThe minimum educational qualification required to become a stock broker is a graduation with at least 2 years of experience in a stock broking firm. A sub-broker (the previous stage of being a broker) needs to have passed the class 12th standard to be eligible for his job. Minimum age: 21 years.\nA strong understanding of financial laws and regulations, accounting methods, principles of economics and currency, financial planning and financial forecasting all are useful for working in the field.\nCOMPARATIVE ANALYSIS: ANGEL BROKING Vs ZERODHA\nAngel Broking is an Indian stockbroker firm, founded by Dinesh Thakkar, in 1987. It is a member of the Bombay Stock Exchange (BSE), National Stock Exchange of India (NSE), National Commodity & Derivatives Exchange Limited and Multi Commodity Exchange of India Limited. It is a depository participant with Central Depository Services Limited (CDSL).\nZerodha Broking Limited is an Indian financial services company (a member of NSE, BSE, MCX), founded by Nitin Kamath, in 2010.As of December 2020, Zerodha was the largest retail stockbroker in India by active client base, and contributes upwards of 15% of daily retail volumes across Indian stock exchanges.In June 2020, Zerodha entered the unicorn club with a self-assessed valuation of about $1 billion.\nAngel broking is a public company headquartered in Mumbai whereas Zerodha is a private company headquartered in Bangalore.\nAngel broking is full service broker , which means it is a licensed financial broker-dealer firm that provides a large variety of services to its clients, including research and advice, retirement planning, tax tips, and much more whereas Zerodha is a discount broker which means they carry out buy and sells orders at reduced commission rates, however, they do not offer personal consultations, advice and research to customers.\nBoth of the companies provide services in the financial sector. Angel broking provides services like,equity trading, commodities, portfolio management services,mutual funds , life insurance, health insurance, IPO, depository services and investment advisory however Zerodha provides equity trading, derivative trading, currency trading, commodities, mutual funds and government bonds.\nOther than these some advantages of angel broking are Advisory services ,Trading in all market segments on BSE, NSE, MCX, NCDEX , Widespread Sub-brokers and partners network ,Offers Commodity Trading ,Free Demat Account whilst Zerodha provides , No Brokerage on Equity Delivery and mutual funds , Flat rate for stock Trading ,Own DP services ,Good Brokerage and margin Calculator ,Offers up to 20x leverage on intraday trading ,Offers most advanced online trading tools ,Offers facility to apply online IPO and a Strong Customer Service.\nIf we take a look at the disadvantages, Angel Broking doesn’t offer a 3-in-1 account, charges high Brokerage , not suitable for small and penny stock traders as they charge minimum brokerage of 30 Rs for your trade and also has Hidden charges and Zerodha provides no margin funding, has high charges on call and trade @ Rs50 per call, charges on funds transfer @9Rs per fund transfer, charges for sms alerts, additional charge of ₹50 per executed order for MIS/BO/CO positions ,doesn’t provide stock tips, research reports or recommendations and doesn’t offer Unlimited Trading plans.\nWhy to choose Angel broking?\n- Wide Network with over 8500 Sub-brokers and Franchise partners in 900+ cities in India.\n- Offers Demat account absolutely free.\n- Personalised investment advice through artificial intelligence based investment engine.\n- Provides Top quality research powered by ARQ predictions, which help to pick n invest in best stocks.\n- Call-n-Trade: you can place orders securely over the telephone with the Call N Trade service.\nWhy Do I Choose Zerodha over Angel Broking?\n- Charges flat rate of ₹20 per executed order.\n- Does not charge any brokerage on Equity delivery transactions.\n- Provides a platform to invest in mutual funds.\nInnovative trading platform.\n- No upfront fee, no minimum brokerage and no minimum contract charges.\n- The largest stockbroker by active clients, markerrrrrrrtyuit volume.\n- Zero brokerage Direct Mutual Funds.\n- Offers up to 20x leverage on intraday trading.","Although there are plenty of ethical brokers out there, others choose to break their ethical and legal obligations towards customer, exposing you to loss and the broker and his/her firm to damage liability. Some of the most common examples of this are margin trading, misleading illustration, and unsuitable investments.\nBefore an agent talks you into buying a new product, he or she should be upfront about the benefits and claims associated with this product. Unfortunately, too many investors purchase indexed annuities with the belief that they cannot lose their principal. If a broker has misled you, you may have a legal claim.\nHedge funds are complex vehicles that may lead investors to believe they have little protection. Sometimes a hedge fund manager may be dishonest about his or her qualifications or background, past situations involving theft from hedge funds, or performance of the funds.\nInvestment advisers may engage in one or more of the following problematic behaviors: failing to test for suitability, churning, or making unauthorized changes to a client’s portfolio.\nWhenever an advisers or broker shares an investment opportunity with you, all necessary facts must be provided so that you can make an educated decision. Unethical brokers, though, might try to avoid giving you all the facts, and this may qualify as grounds for a claim against this individual.\nIf any financial institution such as a securities firm engages in misconduct broadly, an investor might bring forth a class action lawsuit. If your losses are large, however, you should think carefully about joining a class action. You may be better off in your own case after consulting with an attorney.\nThere are many different terms involved with buying and selling investments. It’s your brokers job to explain to you the most pertinent facts so that you can make an informed decision. There are varying levels of risk associated with different types of options, but your broker can be liable if he or she makes an unsuitable recommendation that leads to serious losses for you.\nDeciding the investment firm to work with was likely a process that involved you respecting the company name. That’s why it can be a shock when you learn that the broker has given your business to an outside money manager. Without knowing this, you might not realize that you can go another route and avoid all the fees associated with the outside money manager.\nThe potential with a structure product is typically the possible upside, but a broker who leaves out the significant downsides is doing you a disservice. A broker like this will skip over the fact that these products typically have a lack of liquidity, high market and credit risks, and costly expenses. If your broker has recommended a product like this to you and it’s an unsuitable investment, you may have grounds for a claim.\nVariable annuities on their own are not a terrible product, but they are not the right fit for everyone. A broker who has swayed you to purchase one because of the high commissions he/she will receive, however, could be looking at a sales abuse claim.\nIf a broker presents you with an illustration promising a lot of cash value in a product like this, you should be wary. Unfortunately, not all agents selling these universal life insurance policies will be so forthcoming about the reality of smaller returns in these products. If you’ve fallen victim to false claims, you need the advice of an attorney."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:f78162c2-63a2-4096-b99a-3d18cd92a469>","<urn:uuid:f669552d-cafd-4147-a65b-2b03534cee25>"],"error":null}
{"question":"As an astronomy student, I'm trying to comprehend why Sirius appears so bright - is it due to its actual luminosity or its proximity to Earth compared to other stars?","answer":"Sirius appears bright primarily due to its proximity to Earth rather than its intrinsic luminosity. It is only 8.6 light years away from Earth, making it one of our closest stellar neighbors. This is confirmed by examining its absolute magnitude (1.45) which measures its actual luminosity, compared to other stars like Antares which has a much brighter absolute magnitude of -4.7 but appears dimmer from Earth due to its greater distance of 130 parsecs.","context":["Welcome to Dies Caniculares! Ancient civilizations observed that temperatures were hottest in mid-July through mid-August when Sirius, the “Dog Star” rises next to the sun. Sirius means “glowing” in Greek. It is the head of the dog in the constellation Canis Major – the Big Dog. The star glows blue-white on the horizon near the sun at dawn. Sirius is the brightest star in the night sky because it is closer to earth – only 8.6 light years away – not larger in energy or light output. The only objects brighter than Sirius are the sun, moon, Venus, Jupiter, Mars, and Mercury.\nAncient Egyptians who worshipped the sun believed Sirius was a mysterious relative of the sun. They called Sirius the “Nile Star” because it signaled the annual flooding of the land. The Egyptians based their calendar on the rising of Sirius and considered it the most important star of all and a symbol of fertility. The Dog Days were a time of feast and celebration. The Egyptians built the pyramids in alignment with Sirius. Sirius shows up in Freemasonry as the “Blazing Star” prominent in Masonic art due to its association with Egyptian pyramids. Sirius is the star in the Order of the Eastern Star. Other civilizations that marked the Dog Days with feasts and celebration include the ancient Chinese and Japanese who called Sirius the “Wolf Star” and American Indians who called it “Coyote Star.”\nThe Dog Days were dreaded by ancient Greeks and Romans. They thought the period between July 24th and August 24th brought forth evil. During the hot, dry Dog Days it was believed that wine turned sour, the seas boiled, dogs went mad, people contracted disease and fever and were susceptible to hysterics and frenzies. The Romans tried to bargain with the gods for a cooler July and August by sacrificing a red dog in April. Today Europeans do as the Roman aristocracy did – they go on holiday in August.\nSuperstitions die hard. Some people still think that hot weather causes violence during the Dog Days. Criminologists have studied seasonal crime in the U.S. for more than a hundred years and have yet to find any correlation between hot weather and crime. A study that analyzed crime in 2007-2009 in New York City found that the most crimes occurred in September, followed by August, October, July, and December. More than a hundred studies of crime statistics confirm that it’s not the heat, it’s the opportunity for social interaction that drives the crime rate. When teenagers are not in school, the crime rate for personal property crime (burglary, larceny, auto theft) goes up nationwide. During warm weather people leave bicycles in the front yard or leave the garage door open. They may leave their car idling to keep the air-conditioning on, or windows up to cool off a room. Does the rising mercury drive people mad or make them violent? Nationwide the month with the highest homicide rate is December.\nThe National Academy of Sciences issued a study in July 2013 that did conclude there is one aspect of human behavior that is recommended during the Dog Days: Summer is the best time to conceive a child. May is the worst month to conceive because the baby will be born in the winter. Babies born in winter have lower birth rates, weaker immune systems, poorer vision and hearing, slower cognitive development than babies born in spring and summer.\nIt seems the ancient Egyptians had it right when they recognized the Dog Days as a time to celebrate fertility!","The luminosity of an object in space is the amount of energy that it radiates\neach second in all directions.\nLuminosity is also referred to as the absolute\nmagnitude or absolute brightness of an object.\nIt is the real brightness of a celestial object.\nThe apparent magnitude or apparent brightness of an object is a measure of how\nbright an object appears to be to an observer. It is the amount of energy\nfrom an object in space which reaches a square centimeter of a detector each\nsecond. Apparent magnitude is also referred to as flux.\nIt is a measure of how bright a celestial object appears to us. The apparent\nmagnitude of an object depends upon its real brightness and on its distance\nA visible light view of a star cluster.\nNotice how the stars appear to have different brightnesses or\nHillary Mathis & N. A. Sharp, KPNO, AURA, NOAO, NSF\nIf you look up at the night sky on a clear night, you will notice that the stars\nappear to have different levels of brightness - some are bright and some are dim.\nThe apparent brightness of an object is measured in magnitudes. This system\nwas developed over 2000 years ago by the Greek astronomer Hipparchus to rank\nhow bright different stars appeared to the eye.\nIn his magnitude system, the brightest stars were called first magnitude stars,\nand the dimmest were sixth magnitude stars.\nSo, in this system, brighter objects have lower magnitudes than dimmer objects.\nMuch later, when astronomers were better able to measure the brightness of stars\nand other celestial objects, they kept the traditional magnitude scale of\nHipparchus and added magnitudes that go beyond the range of 1 to 6.\nObjects which appear to be much brighter than the stars, such\nas the Sun, Moon and Venus are given negative magnitudes or -26.7, -12.6 and\n-4.4 respectively. With modern telescopes we can measure objects which appear\nas faint as +28 magnitudes.\nEach number on the magnitude scale is about 2.5 times apart in brightness from\nthe next number. For example, first magnitude stars (stars with a magnitude of 1)\nare about 2.5 times brighter than second magnitude stars\n(stars with a magnitude of 2).\nWhen we look at an object in space we see its apparent brightness. If we know\nthe object's distance from us, it is easy to calculate its absolute brightness\nabsolute magnitude = apparent magnitude - 5 × log(distance\nin parsecs) + 5.\nFrom this formula, you can see that if a celestial object is 10 parsecs\naway from us, then its apparent magnitude is equal to its absolute magnitude.\nBoth the apparent and absolute brightness of objects in space will be different\nat different wavelengths, for example the infrared magnitude will not be the\nsame as a visible light magnitude, however, the above formula still applies.\nBelow is a table showing the visible light absolute and apparent magnitudes\nof some well known stars.\n|Sun ||-26.74 ||000000.48 ||4.83\n|Sirius ||-1.44 ||2.64 ||1.45\n|Arcturus ||-0.05 ||11.25 ||-0.31\n|Vega ||0.03 ||7.76 ||0.58\n|Antares ||1.00 ||130.0 ||-4.7\n|Barnard's Star ||9.54 ||1.82 ||13.24"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:f48745d7-1937-4dd9-aaa4-27509bf57021>","<urn:uuid:aa371a76-167c-4353-af84-6155fd90e8b8>"],"error":null}
{"question":"What are the key steps to create a swim lane diagram, and how does it fit into broader process flow management for improving organizational efficiency?","answer":"Creating a swim lane diagram involves several key steps: First, identify a wasteful process to improve and gather 3-5 representatives from involved parties. Initially use manual tools like markers, post-it notes (different colors for each party), and large paper rather than computer software. Draw vertical or horizontal lines to create 'swim lanes' for each group, then map out process steps from left to right using post-it notes under appropriate owners. After group review and adjustment, digitize using tools like Microsoft Visio. As for process flow management, these diagrams are part of a broader framework for visualizing and optimizing workflows. They help organizations identify bottlenecks, redundancies, and areas for improvement in systematic sequences of activities. Process flows are particularly valuable in project management for planning, execution, monitoring, and managing dependencies between tasks.","context":["How To Create a Swim Lane Diagram\nWe live in a world overwhelmed with waste, and many supply chains suffer the effects of it daily. Luckily, there are many tools out there used to target and eliminate that waste. My personal favorite is the process map in the form of a swim lane diagram. It clearly identifies responsible parties and the points in the process at which they are involved. Below, I have created a simple “how to” guide on creating a swim lane diagram.\nIdentify your process.\nIdentify a wasteful process that you wish to improve.\nRound up the herd.\nCollect representatives from all parties involved in the process. It is best to keep the group limited to 3-5 people. When the group is too large it becomes more difficult to reach conclusions and keep everyone engaged.\nSkip the computer.\nWhen creating a process flow for the first time leave the computer at your desk. The best way to get started is with a marker, different colored post-it notes, and a giant sheet (approx 3’x6’) of white paper. A white board/wall can work as well (although make sure you’re process map won’t get erased for a couple of days). The old fashioned way is initially more efficient because most of the time you don’t thoroughly know your process up front. Using post–it notes will allow you to move around steps in the process. Each party involved in the process should have uniquely colored post-it notes so as to visualize who owns which step in the process.\nReview the rules of the road.\nRemember that you are using a lean problem solving tool, which indicates we already possess standard symbols identifying action in the process. A lean process map symbol key is below:\nOn your mark, get set...GO!\nEach group that is associated with the process gets their own section on the diagram. Draw vertical (or horizontal) lines to divide each section - or “swim lane.”\nLay it out.\nStarting from left to right, identify each step in the process. Write each down on a post-it note and place the note under the appropriate owner group. Use the lean symbols to connect each step.\nCheck and adjust.\nReview your process and adjust any post-it notes that are in the wrong place. Make sure the whole group is in agreement.\nBring it to the 21st century.\nNow that you have completed the swim lane diagram, there are many great process tools out there that will help you make it electronic - specifically Microsoft Visio and Excel. Print out the diagram and distribute it to your team members.\nUse what you have learned in this process mapping exercise to remove waste and increase efficiency.\nWritten by Ben Green, Lean Logistics Manager at LeanCor\nRelated articles by Zemanta\n- Constructing Workflow Process Diagrams in Excel 2007 (brighthub.com)\n- Problem Solving Techniques - Problem Solving Training from MindTools.com (mindtools.com)\n- Developing a Process Map for Managing a Project (brighthub.com)\nPosted by LeanCor Supply Chain Group\nLeanCor Supply Chain Group is a trusted supply chain partner that specializes in lean principles to deliver operational improvement. LeanCor’s three integrated divisions – LeanCor Training and Education, LeanCor Consulting, and LeanCor Logistics – help organizations eliminate waste, drive down costs, and build a culture of continuous improvement.Facebook LinkedIn Twitter Google+","Table of Contents\nAny organization that wants to be efficient and productive must comprehend and improve its work processes. Are you seeking to enhance your workflow and optimize productivity? Reevaluating your procedural framework could provide the answers you need.\nBut what exactly is a process flow, and how can you effectively visualize it? In this guide, we will delve into the theory of process flows, their significance, and the techniques to create insightful visualizations that can lead to more streamlined and effective workflows.\nA procedural framework comprises a sequence of actions you and your team undertake to reach your objectives. Creating visual representations of your procedural workflows can unveil redundancies, recurring responsibilities, and areas of constraint.\nProcess flow visualization can accomplished through flow diagram builder or workflow diagrams. Explore the history of procedural workflows, understand what process flow diagrams entail, and discover how they can elevate your team’s effectiveness.\nWhat Is Process Flow?\nProcess flow refers to the systematic sequence of steps or activities that need to be processed to accomplish a specific task or achieve a particular goal within an organization. It provides a structured framework for understanding and visualizing how work is focused, making it easier to identify potential bottlenecks, inefficiencies, and areas for improvement in a given workflow.\nWorkflow diagrams are frequently used to illustrate process flows graphically, and workflow management techniques involve managing them too. For example, let’s consider a general business process: invoice approval. In this scenario, the process flow might involve the following steps:\n- Receipt of Invoice: The process begins when the accounts payable department receives an invoice from a vendor.\n- Verification of Invoice Details: The accounts payable team verifies the accuracy of the invoice, checking for any discrepancies or errors.\n- Approval Request: If the invoice is correct, an approval request is sent to the relevant department or manager, initiating the approval process to seek payment authorization.\n- Manager’s Review: The manager reviews the invoice and checks whether it aligns with the company’s budget and policies.\n- Approval or Rejection: Based on their review, the manager either approves the invoice for payment or rejects it, providing reasons for rejection.\n- Payment Processing: The vendor gets paid according to the established guidelines when the invoice has been approved and processed for payment.\n- Record Keeping: To facilitate auditing and accounting, invoice and payment is maintained in a record.\nIt is a simplified example, but it illustrates a typical process flow. Visualizing this process using a workflow diagram can make it easier for all stakeholders to understand their roles, the sequence of actions, and the dependencies between steps.\nWorkflow management software and practices can further enhance this process by automating tasks, tracking progress, and ensuring efficient execution.\nProcess flows are a fundamental aspect of workflow management, enabling organizations to optimize their operations and achieve greater efficiency in various business processes.\nWhat Is a Process Flow Used For?\nA process flow is a crucial tool used for project management, specifically in business project management. It plays a vital role in planning, executing, and monitoring various aspects of a project.\nLet us explore how process flows in this context and how they help manage project dependencies effectively.\n- Project Initiation: Process flow is used in corporate project management to launch projects. This initial phase involves defining the project’s scope, objectives, and stakeholders. It is where project managers outline the project’s purpose, expected outcomes, and key performance indicators (KPIs).\n- Project Planning: During project planning, the process flow becomes instrumental in creating a comprehensive project plan. It involves defining tasks, allocating resources, estimating timelines, and setting milestones. Project dependencies are identified at this stage to ensure tasks are correctly sequenced. And any interrelated elements are appropriately managed.\n- Project Execution: In the execution phase, the process flow guides the team in carrying out the tasks outlined in the project plan. It helps project managers and team members monitor progress, allocate resources efficiently, and ensure real-time oversight of project dependencies.\n- Project Monitoring and Control: Process flows are also used for monitoring and controlling the project’s performance. It involves tracking progress against the established milestones and KPIs. If any deviations or issues arise, the process flow helps project managers take corrective actions and make necessary adjustments.\n- Project Dependencies Management: Process flows are essential for managing project dependencies throughout the project management lifecycle. These dependencies can be sequential (where one task relies on the completion of another) or resource-related (competing for the same resources). Project managers can ensure that works are in order and correctly executed and that resources are allocated effectively by clearly identifying and addressing dependencies in the process flow.\nProcess flows are an indispensable tool in business project management. They facilitate the planning, execution, monitoring, and control of projects and enable effective management of project dependencies. By using process flows in project management, organizations can increase the likelihood of successful project completion and achieve their business objectives more efficiently.\nWhat Is a Process Flow Diagram?\nA process flow chart diagram is a visual representation that outlines the sequence of steps, activities, and decision points within a process or workflow. Process flow builder provides a clear and structured overview of how a particular process unfolds, making it easier to understand, analyze, and improve. Process flow diagrams are essential in various domains, including project management, business operations, and approval workflows.\nIn an approval workflow, a process flow diagram can illustrate the specific steps involved in seeking and granting approvals. Workflow builder visually maps out who initiates the request, how it moves through various stages of review, and who has the authority to access final approval. This graphical representation helps streamline and manage approval processes efficiently, preventing bottlenecks and delays while ensuring tasks are ordered and properly executed.\nHow Can You Design a Process Flow That Works Well?\nCreating a productive process flow is essential for streamlining operations, improving efficiency, and ensuring consistent execution of tasks. Whether you are managing a business process, a project, or any workflow, here are prime steps to guide you in creating a productive process flow:\n- Define the Objective: Start by clearly understanding the purpose and goal of the process flow. What outcome do you want to achieve? Having a well-defined objective will guide the rest of the process.\n- Identify Key Steps: Break down the process into its constituent steps or tasks. Be thorough and capture all relevant actions. Ensure that each step is essential to achieving the process’s objective.\n- Sequence Tasks: Put the tasks in the correct order so they can completed. Identify any dependencies between tasks before moving to the next steps and ensure complete prerequisites. It is where process automation can play a crucial role in streamlining sequential tasks.\n- Assign Responsibilities: Specify who is responsible for each task. Clear role assignments help ensure accountability and prevent confusion about who should do what.\n- Define Input and Output: For each task, clarify what inputs are required and what outputs should produced. It helps ensure all necessary resources are available and accomplish the desired results.\n- Set Timeframes and Deadlines: Establish timeframes for each task and overall deadlines for completing the entire process. It adds a sense of urgency and helps manage expectations.\n- Review and Optimize: Search for redundant processes, bottlenecks, or stages that can automated to boost productivity and lower mistake rates. Workflow automation software can dramatically improve the speed and accuracy of repetitive tasks.\n- Documentation and Communication: Document the process flow in a clear and accessible format, such as a workflow diagram or written procedures. Share this documentation with relevant team members to ensure everyone understands and follows the process consistently.\n- Testing and Feedback: Before full implementation, test the process flow with a smaller-scale pilot to identify any issues or challenges. Gather feedback from those involved in the process and make necessary adjustments.\n- Continuous Monitoring: Once the process flow is in use, continuously monitor its performance. Track vital metrics, seek users’ feedback, and be open to making refinements as needed.\nIncorporating process automation into your process flow can significantly enhance efficiency by automating repetitive, rule-based tasks, reducing the risk of errors, and speeding up task completion. It is a valuable addition to optimizing and streamlining workflows, making them more effective and efficient.\nProcess flow emerges as the guiding principle in the dynamic world of productivity and efficiency. It acts as your road map for moving from an abstract concept to an actual outcome. Process flow is not just a concept; it is the heartbeat of work, a symphony of tasks orchestrated to perfection.\nBut do not stop at understanding its significance; learn how to visualize it. Dive into the world of flowcharts, diagrams, and visual representations that bring clarity to complexity. With these tools, you unlock the power to spot bottlenecks, streamline workflows, and ensure that every cog in the machine turns with precision.\nIn a world where time is a precious commodity, process flow is your ally in the quest for efficiency. The secret sauce that can transform your endeavors from chaotic to controlled, from erratic to rhythmic. So, learn to visualize process flows and watch your projects, teams, and goals thrive in the light of well-planned success."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:ebcb13ce-b901-4328-9637-cc58deffcbd2>","<urn:uuid:bbffd8e5-ed34-42dd-b823-1b10fa7fb45b>"],"error":null}
{"question":"What caused the Soviet submarine captain to seek defection in The Hunt for Red October?","answer":"The Soviet submarine captain Marko Ramius sought defection after his wife died due to the negligence of a drunken Russian surgeon, whose mistake was ignored because he was the son of a Communist Party high official.","context":["Sean connery in the hunt for red october paramount pictures tom clancy, who died yesterday at age 66 , was renowned for the technical detail of his best-selling military thrillers. The hunt for red october is the debut novel by tom clancy, first published on october 1, 1984 by the naval institute pressit depicts soviet submarine captain marko ramius as he seemingly goes rogue with his country’s cutting-edge ballistic missile submarine red october, and marks the first appearance of clancy’s most popular fictional character jack ryan, an analyst working for the . Tom clancy's the hunt for red october is a thriller that goes into the life of a soviet submarine captain who lost his wife to a drunken russian surgeon this tragic case of negligence was ignored because the surgeon was the son of a communist party high official. Tom clancy is america’s, and the world’s, favorite international thriller author starting with the hunt for red october, all thirteen of his previous books have hit #1 on the new york times bestseller list.\nTom clancy was the author of eighteen #1 new york times-bestselling novelshis first effort, the hunt for red october, sold briskly as a result of rave reviews, then catapulted onto the bestseller list after president ronald reagan pronounced it the perfect yarn clancy was the undisputed master at blending exceptional realism and authenticity, intricate plotting, and razor-sharp suspense. Professionally written essays on this topic: tom clancy rainbow six comparative analysis of the novel and film versions of the hunt for red october. Essay contests cno naval history since the publication of the hunt for red october, tom clancy established an unrivalled position as the world's leading thriller .\nTom clancy was born in maryland in 1947, clancy worked as an insurance broker before writing his first novel, the hunt for red october, in 1984 ten of clancy's books earned no 1 rankings on the new york times' best-seller list. The sort of film that in an earlier era would have been called a thinking man's thriller, the hunt for red october ushered in a new series of hollywood-produced post-cold war adventure films . Author tom clancy in 1985, a year after he published his first book, 'the hunt for red october' clancy's publisher, gp putnam's sons, confirmed that clancy had died on tuesday in baltimore.\nThe hunt for red october summary characters essays and research papers tom clancy the hunt for red october which was one of the best sellers at the time red . The hunt for red october takes place during the cold war, causing much distrust and deceit marko alexandrovich ramius, a lithuanian submarine commander in the soviet navy and son of a prominent soviet politician, intends to defect to the united states with his officers on board the experimental . The hunt for red october airs sunday 11/4, 5pm |4 c – stay tuned for the enhanced dvd tv version 8pm | 7 c tom clancy attributes his success as a novelist to equal parts dogged persistence and deep research you learn to write the same way you learn to play golf, clancy said “you do it, and . The hunt for red october by tom clancy essay sample in the book “the hunt for red october, tom clancy tells us about a man by the name of marko ramius who is in the soviet union’s navy. The hunt for red october is a novel written by tom clancythe story follows the intertwined adventures of soviet submarine captain marko aleksandrovich ramius and cia analyst jack (john) patrick ryan.\nJohn mctiernan directed one of these films, the hunt for red october, based on the similarly titled best-selling novel by tom clancy the hunt for red october, a product of the anti-communist attitudes of the 1980’s, is above all a commentary on morality. In clancy’s novel the hunt for red october, clancy depicts that what someone will do to fight for their freedom tom clancy was born on april 12th 1947 in baltimore, maryland he and his parents, a mail carrier and a credit employee lived a normal life for all of clancy's childhood. Tom clancy bio tom clancy was actually an insurance provider up until eighteen years ago he had only written two things an article about a new missile and a letter to the editor of a magazine when he decided to write the book the hunt for red october. Tom clancy the hunt for red october acknowledgements for technical information and advice i am especially indebted to michael shelton, former naval.\nHunt for red october tom clancy item#: much more common in older books printed on handmade papers with a high rag content than in books printed on manufactured . Immediately download the the hunt for red october summary, chapter-by-chapter analysis, book notes, essays, quotes, character descriptions, lesson plans, and more - everything you need for studying or teaching the hunt for red october. Tom clancy tom clancy was actually an insurance provider up until eighteen years ago he had only written two things an article about a new missile and a letter to the editor of a magazine when he decided to write the book the hunt for red october.\nIn 1984, the us naval institute published a work of fiction, for the first time in its history, tom clancy's novel the hunt for red october. Hunt for red october by: tom clancy it's december 1986 and captain first rank marko ramius, son of a communist party secretary and the soviet union's most trusted and revered submarine commander, is in charge of a new kind of soviet nuclear submarine called red october. The hunt for red october is one of the best submarine films ever made, and a favorite of dads everywhere it was a blockbuster when it came out (the sixth-highest grossing film of 1990), and an ."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:e95ec068-e6af-4872-b1d8-ac9e14e2a9fb>"],"error":null}
{"question":"What's the difference between spot colors and automatic photo fixes for print quality?","answer":"Spot colors are premade inks used instead of or alongside CMYK process inks, requiring separate plates for each color and typically used in 1-3 color jobs. Meanwhile, automatic photo fix features in printers analyze images and adjust multiple settings like contrast, brightness, and gamma. While auto fixes usually improve results, they can sometimes do more harm than good or undo intended effects, so it's recommended to test prints both with and without this feature.","context":["1. Choose between direct printing options. If your combination of printer and camera gives you a choice between printing directly from the camera and from memorywhich includes cards and USB keys in this contextbe sure to experiment with both. The two choices can yield significantly different output quality for the same file, with noticeably different colors and retention of detail based on shading in dark and light areas. It's well worth investing a little time and effort to print several photos both ways to see how great the differences are and which one you like better.\n2. Get familiar with your printer's auto fix feature. Most current dedicated photo printers, and some standard inkjets, include some variation of an automatic fix feature that analyzes the image and may adjust several settings at once. These may include anything from contrast, brightness, and gamma (which changes contrast differently at different levels of brightness), to automatically deciding whether to apply red-eye reduction.\nWith most photos, these automatic fix features improve the final result, but in some cases they do more harm than good, or even undo an effect that you were trying for. Here again, if your printer includes an automatic fix option, it's worth investing a little time and effort printing an assortment of photos both with and without the feature turned on to get a feel for what it does and when you might want to turn it off.\n3. Preview photos for direct printing. If your printer can print directly from memory cards, it may limit you to previewing photos by printing an index sheet or by looking at the images on a built in preview screen. If it gives you both choices however, keep in mind that there are advantages to each, and that you may want to use one or the other at any given time.\nUsing the preview screen is faster, since you don't have to print twiceonce for the index sheet and once for the final printand it costs less, since you don't have to pay for ink or paper to print the index sheet.\nOn the other hand, if you've taken several similar photos with minor variations in settings, for examplea trick professional photographers use to increase the odds that one of the shots has the right settings for the picture to look its bestan index sheet is the preferred approach for deciding which version to print at full size. The printed thumbnails will give you a better sense than the image of the preview screen of how colors will print in the final photo and how well details based on relatively small differences in shading will show.\n4. Get familiar with your printer's editing features. Printers with preview screens often let you edit photos before printing. The editing choices may be limited to a few basics like cropping images or removing red-eye, or they may include options to adjust brightness and contrast, add graphics and frames that are stored in the printer, and more. The process is similar to using the kind of photo kiosk you can find in drugstores, and is almost always designed to be self-explanatory and easy to use. If your printer includes any editing features, they are certainly worth exploring.\n5. Don't fix photos before you see how they really look. Keep in mind that the colors and shading that you see on screen (whether your computer screen or the printer's preview screen) will almost never be an exact matchand are often not even closeto the printed version. (This is true for all sorts of reasons that are way beyond the scope of this article). For photos you care enough about to want the best possible photo with minimal work, it's generally a good idea to do any cropping that you want first, print the photo, and then make any manual adjustments you like based on what the printed version looks like. If the printer or the program you're printing from has an automatic fix option, you might want to try printing the photo both with and without the fix feature before making any manual changes.\n6. Use paper that's appropriate for the task. Better-quality paper yields better-quality prints, but it costs more too. If you're printing a photo to frame and hang on a wall, by all means use the highest-quality paper available for the printer. If you're printing a photo to post on the office bulletin board or stick under a refrigerator magnet, however, consider using plain paper, inkjet paper, or a less expensive photo paper.Next: Tips 7-12 >","WHAT IS OFFSET / LITHOGRAPHIC PRINTING?\nLithography is an offset printing technique. Ink is applied to the printing plate to form the image (such as text or artwork to be printed) and then transferred or “offset” to a rubber blanket. The image on the blanket is then transferred to the substrate (typically paper or paperboard) to produce the printed product.\nTHE DIFFERENCE BETWEEN SHEET FED AND WEB PRINTING?\nSheet-fed offset printing is carried out on single sheets of paper as they are fed to the press one at a time at a very high speed.\nWeb-fed offset printing is carried out on a single, continuous sheet of paper fed from a large roll. The sheet is then cut into individual sheets of desired sizes. They are used to print large quantities (thousands of copies) of magazines, newspapers or catalogs.\nWHAT IS DIGITAL PRINTING?\nDigital printing primarily uses an electrical charge to transfer toner or liquid ink to the substrate it is printed on. It is similar to colour photocopying. It eliminates the need for printing plates by using computer files. It is ideal for lower quantity and customised needs, but can only print in CMYK.\nWHAT ARE CMYK, RGB & SPOT COLOURS?\nC is cyan (blue), M is magenta (red), Y is yellow, and K is black. These are the four inks used in printing and when mixed together form the spectrum of colours seen on your printed product.\nTo reproduce full-color photographic images these four inks are placed on the paper in layers of dots that combine to create the illusion of many more colors. A mistake often made when submitting artwork for 4-color printing is not converting the images to the CMYK colour space. This is needed so that the file can be separated into the four colors and a separate printing plate can be made for each of the colours.\nRGB is the colour system that your computer monitor displays and even when you are watching TV, the pictures are being displayed in a RGB colour mode.\nRGB is an additive colour system. What this means is that the colours are used to form a variety of colours. If you add an equal amount of red light, green light and blue light you will get white light.\nSpot colours are premade inks that can be used instead of, or in addition to CMYK process inks. A spot colour is usually a PMS colour designated by a number and whether or not the ink is to be printed on a specific stock. For example Pantone 123 CVC is used for coated stock and Pantone 281 CVU is used for uncoated stocks.\nWhen printing spot colours each colour needs its own plate on the press. Spot colours are mainly used in one to three colour jobs. If you are using four spot colours you may as well make it a CMYK job (unless the colours are absolutely specific to the content). CMYK printing is far cheaper than four spot colours.\nHOW MANY COLOURS?\n‘Colours’ generally will refer to the number of inks required to produce your print job. You can have as many colours as you like but the number of ‘inks’ will depend on the print process that is required. See “What is CMYK?” and “What is Spot Colour?” to get a better understanding of the ink colours that you may require.\nA common mistake made is that people forget that BLACK is a colour. So if you are ordering a card with black and green it will be 2 colours. (black=1 & green=2). The black could be a process colour and the green would be a PMS spot colour.\nWHAT DO THE STOCK, WEIGHT & SIZES MEAN?\nWHAT DOES STOCK MEAN?\nThis is the paper ‘type’ that your artwork will be printed on. It is best to talk with our staff about the best option to suit your product and desired outcome as there are many options available when selecting the stock. You will need to consider the weight and if you want the paper to be matt or gloss, coated or un-coated or even recycled. Additional embellishment and coatings are also available once the stock has been printed on.\nWHAT DOES THE WEIGHT MEAN?\nThis is the paper your artwork will be printed on.\nThe numerical value refers to how thick the paper is. The measure is in weight, which are grammage or gsm (grams per square meter) values. The larger the number the heavier the stock will be.\nHere are a few descriptions of the paper we offer.\n80gsm – Standard Weight of A4 copy Paper and used for Letterheads\n90gsm – Great for letterheads. A lot more reliable with Laser Printers\n115gsm – Great for flyers and brochures (General use is for advertising mailout flyers)\n150gsm – Great for flyers and brochures\n250gsm – Great for covers\n300gsm – Great for promotional cards\n350gsm – Great for heavy business cards, POS displays, packaging\nWHAT DO THE SIZES MEAN?\nThis is the final trimmed dimension of your product. If you are selecting a product with available folding options such as brochures or flyers, this size is often referred to the flat size. Your finished size will be determined by the folding choice. Below are the most common sizes. You can have your design created to any size you wish, but it may work out more expensive than using those listed.\nBC 90mm x 55mm DL 98mm x 210mm A6 105mm x 148mm\nA5 148mm x 210mm A4 210mm x 297mm A3 297 x 420mm\nA2 420mm x 594mm A1 594mm x 840mm A0 840mm x 1189mm\nEMBELLISHMENTS & COATINGS\nA varnish is a liquid coating applied to a printed surface (for example the outside of a presentation folder) to add a clear glossy, matte, satin, or neutral finish. Machine varnishing is carried out on the printing press, in line with the inks being applied. The varnish is applied directly after the last ink is put on the paper. (It can also be applied some time after printing).\nA varnish increases colour absorption and speeds up the drying process. By ‘locking in’ the ink under a protective coat, the varnish helps to prevent the ink rubbing off when the paper is subjected to handling. Varnishes are used most frequently, and successfully on coated papers.\nAs the name suggests is shiny. A gloss coating can add impact to your print, especially in sales or promotional material, where optimum presentation of images is paramount. This is a good option for brochure printing or flyer printing.\nThis varnish gives the printed surface a non-glossy, smooth look. This type of seal is sometimes considered to ‘soften’ the appearance of a printed image. Small text in a leaflet or booklet is easier to read on a surface coated with matt vanish as the coating scatters the light, reducing glare.\nSILK / SATIN VARNISH\nNaturally enough, this coating represents the ‘middle ground’ between the two above, being neither as glossy as a true gloss, nor as subtle as a matt.\nLaminating is the placing of something between layers of plastic and sealing them with heat and/or pressure, usually with an adhesive. Laminating your printing can prevent it from becoming creased, sun damaged, wrinkled, stained, smudged, abraded and/or marked by grease, fingerprints and environmental concerns. Laminating is available in a MATT (dull) or GLOSS (shiny) finish. There are different weights of laminate available to choose from and you would be best to discuss which option suits your application best with our sales staff.\nThis process involves applying an extra high-gloss varnish (a clear liquid) over the top of a printed area, either to specific areas of a design such as logos in order to highlight them, or to the entire surface of a printed item, resulting in an extremely glossy and luxurious appearance.\nUltra Violet (UV) Varnishing requires the use of special Ultraviolet drying machinery.\nA UV varnish can be applied as either an all-over coating, or as a spot varnish:\nALL-OVER UV VARNISH\nSimply put, this is a UV seal applied all over the printed surface. A gloss UV varnish seal is the most common type of all-over UV varnish, (perhaps because this finish really does achieve a very high gloss effect, more so than with a laminate in many cases) although silk and matt are also available.\nSPOT UV VARNISH\nAs the name suggests, a Spot Varnish is applied to chosen areas (spots), of a printed piece. This has the affect of highlighting and drawing attention to that part of the design, but it also provides the additional visual stimulus of having varied textures on a single printed surface. This adds a lot of interest, and can identify the printing as a premium piece of literature in the perception of the reader.\nOne very effective technique is to apply a UV gloss spot varnish on top of matt laminated printing. This achieves maximum contrast between the highly reflective shiny UV coating and the light-absorbing matt laminate, and can, for instance, create a striking first impression on presentation folders or a brochure cover.\nAdding a powder to the ink being printed on a paper’s surface creates the effect of Raised Print. The printed piece is then passed under heat and literally cooked together. When heated, the ink and powder blend giving a raised effect.\nIs the process of creating a three-dimensional image or design on the paper. It is typically accomplished with a combination of heat and pressure on the paper. This is achieved by using a metal die (female) and a counter die (male) that fit together and actually squeeze the fibers of the substrate. This pressure and a combination of heat actually “irons” while raising the level of the image higher than the substrate to make it smooth. In printing this is accomplished on a letterpress.\nFoil stamping is the application of pigment or metallic foil, often gold or silver, but can also be various patterns or what is known as pastel foil which is a flat opaque color to paper. A heated die is stamped onto the foil, making it adhere to the surface leaving the design of the die on the paper. Foil stamping can be combined with embossing to create a more striking 3D image.\nPresentation folders are printed on flat sheets and then forme cut to shape otherwise we can arrange a new forme to be made to suit your requirements.\nThis is the approximate number of finished pieces for your project. Quantities may vary up to +/-10%.\nDepending on the print process used, frequently an increase in quantity does not reflect in an equivalent proportional increase in cost. Therefore double the quantity may not mean double the price!\nRECEIVING AND PAYING FOR MY ORDER?\nDO YOU DELIVER OR CAN I PICK UP MY ORDER?\nWe deliver Australia wide. Alternatively you could arrange your own pick up or collection. Please advise us when placing your order.\nHOW CAN I PAY?\nOur terms are COD. Full payment must be received before your order will be despatched. We accept Cash, Cheque and Electronic Bank Deposit.\nELECTRONIC ARTWORK SPECIFICATIONS\nAt Offset Solutions we are able to accept files from all major Mac OS & Windows applications. For us to be able to achieve maximum results and minimise delays they must conform to the following specifications.\nFILES FROM THE FOLLOWING APPLICATIONS\nFILE TRANSFER & STORAGE FORMATS\nCD / DVD\nDOCUMENT PAGE SIZES\nBC 90mm x 55mm DL 99mm x 210mm* A6 105mm x 148mm\nA5 148mm x 210mm A4 210mm x 297mm A3 297 x 420mm\nA2 420mm x 594mm A1 594mm x 840mm A0 840mm x 1189mm\n*99x210mm is DL size unless a folding pattern or a design feature dictates otherwise.\nPlease remember to supply us with all the relevant files for output (eg fonts and graphics) and let us know what files are attached and for what product in your order. If mailing your material please include a colour or a black & white proof copy of your file.\nIf you have other files in your transfer besides the files you want us to use, please clearly mark the files that are intended for output.\nPlease make sure that at least 3mm bleed is placed on your artwork where needed, artwork to be 1 up on a single page document. Where possible convert your text to paths, curves or outlines.\nFor large multi page jobs please be aware of the implications of creep. Please check for the required bleed on artwork in such cases.\nPlease ensure that all images supplied are of a resolution of 300dpi at the same size you wish to print.\nWhen using color or color images in your design and layout, be certain to use CMYK values instead of RGB. If you have an image that is RGB, like images that come from your digital camera or scanner, you must convert them to CMYK first before placing into your design.\nWhen using black elements in your 4-color design, it is best to you use a “rich black,” which is a black composed of all four process colors. This gives your black a deeper, darker shade of black on the press. A rich black should be used on larger areas of black to ensure an even, dark coverage, as the second ink colour disguises any inconsistencies. To achieve rick blacks, create a color swatch or assign a process color with the following CMYK combinations: Cyan = 60%, M = 40%, Y = 20%, K = 100%.\nMake sure all colours are nominated CMYK or PMS (Pantone Matching System) depending upon the printing process by which your job is to be printed.\nScreen colours are never accurate; please check your CMYK or PMS specifications. Any files received in any other colour format (i.e.: RGB, Lab, HSB) will still be printed but may not reproduce as to your expectations."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:ec0b99e0-c8f8-43a1-a031-075bb9284864>","<urn:uuid:69506437-8d91-4f2c-9453-e9809bc8f3fa>"],"error":null}
{"question":"As someone new to marine biology, I'd like to understand coral reef health better. What role do marine invasive species play in coral reef degradation, and how does aquaculture help address this problem?","answer":"Marine invasive species can harm coral reefs in several ways, being introduced through ship traffic, aquaculture operations, fishing gear, and aquaria discharge. They can colonize and become dominant in an ecosystem due to loss of natural population controls. A specific example is Sargassum algae, which can negatively impact reefs by shading, limiting space for coral larvae, and transmitting pathogens. Aquaculture helps address these issues by providing an alternative to wild collection, reducing fishing pressures on reef ecosystems. It ensures a steady supply of healthier, parasite-free fishes that are adapted to life in aquariums, and supports sustainable reef-keeping practices. The marine aquarium industry has made significant progress, with recent successes in breeding species like yellow tang, Pacific blue tang, and several species of angelfish.","context":["MASNA Aquaculture & Conservation Position Statement\nAquaculture is the fastest growing sector of food production on the planet, with a reach that extends to nearly every country on the globe. Its products provide a reliable source of protein to a vast number of homes, support the livelihoods of tens of millions worldwide, and generate advancements in the way we produce our medicine, pharmaceutical products, and energy. Looking to the future, aquaculture is expected to play an increasingly important role amidst a growing human population, wild fisheries that have reached or are close to reaching maximum capacity, pollution, and climate change.\nIn recent decades, marine ornamental aquaculture has paralleled advances in our understanding of aquaculture as a whole, fueled largely by emerging technologies developed by the collective community of scientists, industry leaders, and hobbyists that MASNA supports. What once consisted of a dispersed group of hobbyists housing bleached coral skeletons as decoration in fish-only systems, the marine aquarium industry has blossomed to support a diverse group of hobbyists successfully housing a broad array of marine species from diminutive fishes to reef-building corals. As of 2018, 358 species of marine ornamental fishes have been successfully raised in aquaria, and 28 have been added in the last year. This group is mainly comprised of gobies, blennies, and damsels – all species that lay clutches of eggs on the substrate, where they develop and hatch into large, relatively well-developed larvae.\nConversely, species that rely on pelagic spawning like wrasses, tangs, and anthias release smaller eggs directly into the water column, which quickly hatch into small, underdeveloped larvae that immediately require live foods to survive. Given the difficult nature of successfully raising these larvae in captivity, this group of species has received less attention. However, dedicated efforts to advance our knowledge in live feeds production and larval nutrition have made major strides in our ability to culture and provide larval fishes with quality live food items at the commercial scale. Scientists and aquaculturists continue to hone their production methods for copepods, ciliates, and other small prey items that will undoubtedly add to the list of successfully cultured marine ornamental species moving forward. These advances have contributed to several success stories in the past few years. Among them are the yellow tang, Pacific blue tang, melanurus wrasse, Cuban hogfish, borbonius anthias, several species of angelfish, and a handful of butterflyfish.\nAs the marine ornamental industry progresses, it is important to recognize these achievements, what they represent, and encourage the continued development of marine ornamental aquaculture as we grow as a community. From a conservation perspective, success in captive breeding reduces fishing pressures on wild populations, promotes sustainable and ethical reef- keeping, and ultimately ensures the preservation of the marine aquarium hobby for future generations. This is especially important for species that have limited wild ranges like the banggai cardinalfish, those that are already facing the threat of overcollection, and those that are harvested using destructive fishing practices like cyanide application. From an industry perspective, developing these technologies can reduce the cost of providing quality animals to hobbyists, ensure a constant supply of fishes to meet growing market demand, and have the potential to encourage the growth of the marine aquarium industry. From the perspective of the hobbyist, advances in ornamental aquaculture provide a steady supply of healthier, parasite free fishes that are adapted to life in the home aquarium.\nMASNA stands behind the collective efforts of scientists, industry leaders, conservationists, and hobbyists that are instrumental in driving the success of marine ornamental aquaculture. In our celebration of aquaculture for MACNA 2019, we would like to call to action all researchers and practitioners of aquaculture in a concerted effort to close the life cycle of additional species of marine ornamental fishes, particularly those that are popular within the aquarium trade. These include the tangs, wrasses, and anthias, among many others. We firmly believe that if we act together, the marine aquarium industry will continue to grow and exist in perpetuity. This year, join us as we celebrate the many dedicated scientists, hobbyists, and industry leaders in their commitment to furthering ornamental aquaculture.\nBY Tim Lyons, M.Sc., MASNA Director of Conservation for MACNA 2019\nPosition Statement on Sustainable Marine Aquarium Fisheries\nThe Marine Aquarium Societies of North America (MASNA) represents a community of aquarium organizations and thousands of individual aquarists passionate about marine animals and saltwater aquarium keeping. MASNA believes a robust and sustainable marine aquarium trade based on a combination of aquaculture, mariculture and wild collection from sustainable marine aquarium fisheries can be a positive force for reef conservation, education and socioeconomic benefit both at home and abroad. MASNA promotes the understanding that a sustainable marine aquarium fishery can be managed in much the same way a sustainable food fishery is managed, using science-based adaptive management and supply chain transparency in order to demonstrate sustainability.\nMASNA understands the myriad threats to the reef ecosystems after which aquarists frequently model their aquaria, including, but not limited to pollution, development, tourism, global climate change, ocean acidification, and various fisheries activities. While the global impacts of marine aquarium fisheries are comparatively less than most of these other stressors, for example, the annual bycatch from food fisheries is much larger than the entire global harvest for the marine aquarium trade, MASNA believes actively promoting sustainable marine aquarium fisheries is in the best interest of the aquarium trade, the ecosystems and the fishers on which the trade depends.\nMASNA seeks to mitigate the aquarium trade’s impact and promote positive environmental and socioeconomic outcomes by endorsing sustainability. Owing to the reproductive biology of many popular marine aquarium animals, a well-managed marine aquarium fishery, like a well-managed food fishery, may be fished sustainably over time without compromising the population as a whole. Based on these facts, MASNA actively supports science-based fisheries management through its education and outreach efforts. MASNA strongly urges that all marine aquarium fisheries seek out scientific advice where and when available, and if not available, to seek assistance to make scientific advice available. Further, MASNA supports fishers and fisheries managers who adopt precautionary, long-term adaptive management plans resulting from a multi-stakeholder process in which all sides work collaboratively toward long-term ecosystem-based sustainability.\nAccess to sustainably collected marine aquarium animals is essential to the trade given that more than 90 percent of the species commonly kept in aquaria have not yet been bred successfully in captivity. Even in an aquarium trade where many more animals can be bred in captivity, access to sustainably collected aquarium animals from well-managed marine aquarium fisheries will be necessary for the purposes of acquiring broodstock essential to aquaculture and species survival programs. Beyond providing broodstock, MASNA believes supporting sustainable aquarium fisheries, especially in developing island nations, provides real economic incentive to conserve reef ecosystems and employ sustainable fisheries management tools. The marine aquarium trade is also uniquely poised to foster environmentally responsible socio-economic development in remote coastal fishing villages just entering global markets.\nIn summary, MASNA believes all fisheries, aquarium and otherwise, can be managed sustainably. Along with supporting aquaculture and mariculture activities, MASNA also supports well-managed, sustainable wild marine aquarium fisheries because a robust marine aquarium trade plays a vital role in conservation, education, economic growth, and research.\nAPPROVED BY THE MASNA BOARD FEB. 2012","Beyond threats associated with climate and ocean change, coral reefs are also affected by various local and regional threats. These threats may occur alone or synergistically with climate change adding to the risks to coral reef systems.\nOverfishing and Destructive Fishing\nUnsustainable fishing has been identified as the most pervasive of all local threats to coral reefs. ref Over 55% of the world’s reefs are threatened by overfishing and/or destructive fishing. Overfishing (i.e., catching more fish than the system can support) leads to declines in fish populations, ecosystem-wide impacts, and impacts on dependent human communities. Destructive fishing is associated with some types of fishing methods including dynamite, gill nets, and beach seines. These harm coral reefs not just through physical impacts but also through by-catch and mortality of non-target species including juveniles. Read more about threats and management strategies in the Reef Fisheries Toolkit.\nTraditionally, impacts from wastewater pollution have been associated with human health, but the detrimental effects of wastewater pollution on marine life – and the indirect impacts they have on people – cannot be overlooked. Wastewater transports pathogens, nutrients, contaminants, and solids into the ocean that can cause coral bleaching and disease and mortality for coral, fish, and shellfish. Wastewater pollution can also alter ocean temperature, pH, salinity, and oxygen levels disrupting biological processes and physical environments essential to marine life.\nOther sources of pollution to coral reef waters include land-based pollution associated with human activities such as agriculture, mining and coastal development leading to the discharge or leaching of harmful sediments, pollutants, and nutrients. Marine-based pollution associated with commercial, recreational, and passenger vessels can also threaten reefs by discharging contaminated bilge water, fuel, raw sewage, and solid waste, and by spreading invasive species. Learn more in the Wastewater Pollution Toolkit or in the Wastewater Pollution Online Course.\nMore than 2.5 billion people (40% of the world’s population) live within 100 km of the coast, ref adding increased pressure to coastal ecosystems. Coastal development linked to human settlements, industry, aquaculture, and infrastructure can cause severe impacts on nearshore ecosystems, particularly coral reefs. Coastal development impacts may be direct (e.g., land filling, dredging, and coral and sand mining for construction) or indirect (e.g., increased runoff of sediment, sewage, and pollutants).\nTourism and Recreational Impacts\nRecreational activities can harm coral reefs through:\n- Breakage of coral colonies and tissue damage with direct contact such as walking, touching, kicking, standing, or gear contact that often happen with SCUBA, snorkelling, and trampling\n- Breakage or overturning of coral colonies and tissue damage from negligent boat anchoring\n- Changes in marine life behavior from feeding or harassment by humans\n- Water pollution by tour boats through the discharge of fuel, human waste, and grey water\n- Invasive species which can be spread through transportation of ballast water, hull fouling of cruise ships, and fouling from recreational boating\n- Trash and debris deposited in the marine environment\nCoral disease is a naturally occurring process on reefs, but certain factors can exacerbate disease and cause outbreaks. Coral disease outbreaks can lead to an overall reduction in live coral cover and reduced colony density. In extreme cases, disease outbreaks can initiate community phase-shifts from coral- to algal-dominated communities. Coral diseases can also result in a restructuring of coral populations.\nDisease involves an interaction between the coral host, a pathogen, and the reef environment. Scientists are learning more about the causes of coral disease, especially in terms of identifying the pathogens involved. To date, the most infectious coral diseases are caused by bacteria. Transmission of coral diseases can be facilitated in areas of high coral cover ref as well as through coral predation, as predators can act as vectors by oral or fecal transmission of pathogens. ref\nThe causes of coral disease outbreaks are complex and not well understood, although research suggests that important drivers of coral disease include climate warming, land-based pollution, sedimentation, overfishing, and physical damage from recreational activities. ref\nOn coral reefs, marine invasive species include some algae, invertebrates, and fishes. Invasive species are species that are not native to a region. However, not all non-native species are invasive. Species become invasive if they cause ecological and/or economic harm by colonizing and becoming dominant in an ecosystem, due to the loss of natural controls on their populations (e.g., predators).\nPathways of introduction of marine invasive species include:\n- Ship traffic, such as ballast water and hull fouling\n- Aquaculture operations (shellfish aquaculture is responsible for the spread of marine invasive species through global transport of oyster shells or other shellfish for consumption)\n- Fishing gear and SCUBA gear (through transport when moving from place to place)\n- Accidental discharge from aquaria through pipes or intentional release\nSargassum are a type of brown, fleshy macroalgae that can have detrimental ecological and economic impacts on coral reefs when overabundant.\nIn the Indo-Pacific, high percent cover of Sargassum is common on degraded coral reefs and often represents a phase-shift from a coral to algae-dominated reef system. ref Their reproductive biology and morphology make them excellent colonizers of free space and particularly resilient to disturbances such as tropical storms. ref When overabundant, they can negatively impact the reef by shading, limiting space available for coral larvae to recruit, and transmitting pathogens. ref\nIn the Atlantic, two species of floating sargassum, S. natans and S. fluitans, are responsible for causing large mats of algae blooms which are particularly harmful and prevalent on the Caribbean and West African coastlines. ref Floating algae mats are naturally prevalent in the Northern Atlantic and provide many ecological benefits such as habitat, food, and nursery grounds to many species of fish, crustaceans and even sea turtles. ref However, in the last ten years, a shift in oceanic currents has led to an algae invasion in coral reef areas, causing reduced sunlight required by corals and anoxic and hypoxic conditions on reefs, as well as poor conditions on beaches that are detrimental to the tourism industry. ref\nCoral predators (or 'corallivores') are naturally occurring organisms that feed on corals for their polyps, tissue, mucus, or a combination of the above. Such predators typically include echinoderms (starfish, sea urchins), mollusks (snails), and some fish.\nCorallivory is a common process that, under normal conditions, allows for natural turnover in the ecosystem. However, when these predators are overly abundant (e.g., outbreak conditions), they can cause significant declines in coral cover.\nCommon coral predators include:\n- Crown-of-Thorns starfish (COTS), which are found throughout the Indo-Pacific region, occurring from the Red Sea and coast of East Africa, across the Pacific and Indian Oceans, to the west coast of Central America. COTS can be a major driver of coral loss in the Indo-Pacific, particularly under outbreak conditions.\n- Drupella snails, which are commonly found living on corals in reefs throughout the Indo-Pacific and Western Indian Ocean.\n- Coralliophila snails, which are often more problematic for Caribbean reefs, although some species are prevalent in the Pacific."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:718344a8-6a18-48da-b277-9ccfc5764871>","<urn:uuid:6dd87468-19aa-450b-b9cc-46d264a1ee33>"],"error":null}
{"question":"Looking for reliable battery tech - which has better longevity and maintenance needs between Gelion's zinc-bromine batteries and wet-cell UPS batteries?","answer":"Gelion's zinc-bromine batteries come with a seven-year guarantee and are projected to last up to 28 years (12,000 cycles), with automated self-reconditioning that requires no human intervention. In contrast, wet-cell UPS batteries can last up to 20 years but require more frequent maintenance, specifically quarterly attention including measuring temperature, humidity, voltage readings, and adding distilled water to the cells.","context":["The basic cell of Gelion Technologies’ breakthrough made-for-solar energy-storage system has the diameter of a large beer coaster and is about one centimetre thick. Inside this plastic disc is little more than “salty water, some carbon and some special soap”, says Thomas Maschmeyer, Professor of Chemistry at the University of Sydney. It can be stacked and combined into modules to store anything from one kilowatt hour (1 kWh) to a utility-scale, fully containerised 1 MWh — at a projected production cost of less than $100 per kWh when manufactured at scale.\nMaschmeyer says Gelion Technologies will have a utility-scale demonstrator model in the field within the next couple of years, but last week he officially launched the technology by raising the first of a number of solar-plus-Gelion-powered lamp posts on the avenue leading to The Great Hall at Sydney University. The fleet of mobile lighting systems is intended to improve campus sustainability and increase night-time safety on campus, while providing real-world data on the battery’s performance.\nThe specific charge to develop a battery for solar applications began sometime around early 2014, says Maschmeyer who had been considering established battery technologies and the need for “a solution that was cheap enough, durable enough and safe enough” to use in conjunction with solar PV in its many applications. The recyclability of main components was also key to his vision for a sustainable storage technology.\nHe says lead-acid batteries, “while having served the markets for automotive starter motors and UPS systems very well, are not suited for use with solar PV due to inherent technical limitations of the battery chemistry that give rise to issues such as limited depth of discharge and cyclability, as well as toxicity”.\nLithium-ion technology is expected to experience significant supply pressures on materials due to uptake of electric vehicles in coming years; and can have flammability problems. As such, says Maschmeyer, it’s less suited to partnering with solar PV, whether for home use, commercial-industrial use, or in shipping and aircraft applications.\nCheaper non-flammable components\nIn comparison, zinc is widely available and recyclable, and bromine can be extracted from seawater — among its sources are desalination plants, of which it is a byproduct. And these inexpensive constituent materials cannot catch fire.\nThe first patent for zinc-bromine chemistry in relation to batteries was lodged in 1889, but several issues have prevented it from being commercially applied.\n“To cut a long story short,” says Maschmeyer, “the most obvious application was in conjunction with solar PV because zinc-bromine batteries charge reasonably slowly” — over a period of three to four sunlight hours, for example, “and one can then discharge them at the same pace or more slowly, as required.”\nIn the labs of Sydney University, the first experiment testing Maschmeyer’s gel theory looked inauspicious — it comprised “two pieces of carbon paper covered in gel and some silver solder wire, held together by squares of Teflon that were fixed with wooden washing pegs”. The fact that this soggy sandwich did charge and discharge, albeit poorly, justified further work, and by late 2014 the research group had taken out a patent. By early 2015, Gelion was set up as company, in which the University retains a 5% stake.\nIt took a long time to get the gel technology right, says Maschmeyer: “We have a number of components in there, but the upshot is that those components are mobile, with the result that charging and discharging the battery improves overall performance.”\nThe Gelion team has developed “a simple gel in terms of what it looks like, but the thinking behind it and the composition — that’s where the magic is!” says its inventor who was last year awarded the 2018 Eureka Prize for Leadership in Innovation and Science.\nGelion Endure batteries are designed to achieve production costs of below $100 per kWh as scaling of production ramps up. This compares with a current cost for lithium-ion batteries of around $180 per kWh; and is roughly equal to the cost of lead-acid batteries.\nHowever, lead-acid batteries typically only have 50% of their capacity available for charging and discharging, which can effectively double their cost of storage, while Gelion’s zinc-bromine technology offers 100% charge and discharge.\nNo air-conditioning and self-reconditioning\nIn addition, says Maschmeyer, “because Gelion cells are at the low end of lithium-ion technology in terms of energy density — around 120 Wh per kilogram — and they slowly charge and discharge, the system doesn’t generate enough heat to require an air-conditioned operating environment.” It is well served by passive ventilation alone, which further reduces the complexity and cost of deployment.\nSoftware-controlled, remote reconditioning of the batteries — stripping and resurfacing of the electrodes — “occurs in-situ, inside the batteries, without human intervention”, explains Maschmeyer. “One doesn’t need to go in there and physically change batteries and so on. We’ll have a battery management system that does this in a gentle way, so fading is a much smaller issue for us than for other battery technologies”\nIn Gelion’s labs, the batteries are currently being thrashed under accelerated testing, to provide empirical evidence of their durability over 3,000 cycles (seven years) of complete daily charging and discharging. Their durability is likely to be proven over much longer periods — up to 12,000 cycles or 28 years, says Maschmeyer — but to credibly simulate almost three decades of ageing at this point would be a stretch. He says, “We’d rather focus on statements that will be valued and respected by the market.”\nGelion Endure batteries will initially come with a seven-year guarantee. The company expects to have test configurations for lighting, power walls and commercial-industrial applications (200-400 kWh) and a utility-scale module (1 MWh) in the field by mid 2020. It anticipates that subsequent commercial orders will enable scale production to start in 2021.","Uninterruptible power supply systems have become increasingly common for business, home security and computing applications where losses of data due to power interruptions could cost thousands of dollars, compromised data or losses of long-term customers. UPS battery arrays can power anything from emergency lighting to full-service data centers. The best batteries for providing uninterrupted power offer reliability, longevity, safety, consistency and affordability.\nUPS Battery Back-up Systems\nUPS systems provide nearly instantaneous power when a service disruption could cause fatalities, injuries, data losses, inconvenience or costly disruptions of business. You can use a load calculator to determine how much power you need, and you can use serial and parallel configurations for generating higher voltage or higher current.\nFinding the best battery for your needs depends on load characteristics, costs, maintenance requirements and self-discharge rates of the batteries. Most UPS systems use more than one cell, so your costs depend on how many batteries are needed and what kind of housing is required. Arranged serially, battery voltage increases. Parallel configurations increase current. Most systems use valve-regulated lead-acid batteries or sealed batteries because maintenance is minimal and costs are affordable. Although these batteries don’t require adding liquid, maintenance and testing is still important because excess heat and evaporation could reduce the charge and battery life.\nBatteries for UPS systems fall into three categories: online, line-interactive and standby.\nOn-line UPS systems have inverters that convert DC power to AC power running during normal operation. Online systems deliver the highest level of protection for sensitive equipment. The system converts DC power into AC power. The system converts any unused AC power back to DC power to recharge the battery array.\nLine-interactive systems regulate or add power during low- or high-voltage events and short blackouts. These UPS systems protect against surges in power from AC outlets. Cheaper than an online system, this method provides good protection for a lower cost.\nStandby systems are most commonly used for computers to protect from low- and high-voltage situations. The battery power comes online only when AC power fails. The inverter can operate the equipment for some time or allow enough time to save information, safely shut systems down or make alternative power arrangements.\nBattery types for UPS systems\nTypes of UPS batteries include VRLA or sealed batteries and wet or flooded-cell batteries. These batteries are the best types for ensuring uninterruptible power because they require minimum maintenance, provide long-term protection for up to 20 years or cost less. VRLA batteries have shorter lifespans but require less maintenance. Wet-cell batteries last longer but require more maintenance more often. The battery handbook PDF download provides more detailed information about all types of batteries.\n- Predictive maintenance and testing is essential for both types of batteries.\n- You should clean, repair and replace components as needed.\n- You should always use the same type and brand of battery to replace a worn cell, and best practices involve replacing the array as soon as possible if age is responsible for the failure.\n- VRLA batteries require annual maintenance while wet-cell batteries need quarterly attention.\n- Typical battery maintenance includes measuring and recording temperature, humidity and voltage readings.\n- Adding distilled water to wet cells is essential for proper operation.\n- Investigate conditions that could cause battery failure such as excessive cycling, lack of temperature control, poor charging and environmental contaminants.\nKeeping detailed records of maintenance, installation dates and repairs will save money by documenting when replacements are needed. Regular inspections can spot abnormalities or environmental causes of corrosion and defects. Keep a record of the battery manufacturer’s recommended maintenance intervals and replacement times to ensure uninterrupted power in all situations.\nUninterruptible power supply systems depend on getting the right batteries, configuring them properly and maintaining them carefully. You might need to power your server for business, protect sensitive medications that depend on being stored at precise temperatures or run an entire building. During testing, the world’s largest UPS system, The Battery Electric Storage System in Fairbanks, Alaska, generated 46 megawatts of energy, which was enough power to run the city.\nYou might not have such aggressive demands as running a city, but your power needs could be just as important to you. Getting the right batteries for the job, keeping them maintained and replacing them before the charge weakens are essential for safety, security and reliability of any UPS system."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:9ffefb7a-3209-46e5-94c2-bb79cf89699a>","<urn:uuid:3f8aac5b-4d9a-4ac9-b91f-a537f064b92e>"],"error":null}
{"question":"¡Hola! Como CISO de una compañía de alimentos, necesito saber la diferencia entre la frecuencia de auditorías de ciberseguridad vs las pruebas del plan de respuesta a incidentes. ¿Cuál se debe hacer más seguido? 🤔","answer":"Cybersecurity vulnerability assessments or penetration tests should be performed at least once a year, while incident response plans should be tested and updated regularly. Only 30% of companies with IR plans test and update them regularly, which is concerning given the evolving threat landscape. More frequent testing of IR plans is needed since they often become out of date and generic, making them ineffective during a crisis.","context":["In the food manufacturing industry, just as in any other industry, cybersecurity is very important. Your organization should be having cyber vulnerability assessments or penetration tests performed at least once a year. Like any big test you have taken in your life, this sort of assessment can be scary, but if you prepare for it, you can greatly improve the potential of passing the test. As you prepare for the assessment, there are six things you can either implement or do to make the result of this audit better for your organization.\n- Do an inventory of what is connected to your network. You cannot expect to defend devices on your network that you are not aware of. Be sure when you perform this inventory that you include any device that connects to your network. Think past the routers, switches, desktop PCs, laptops and printers. What is connecting to your wireless network? Is your security system or HVAC system connected to the network? Creating a network device inventory can be difficult, but there are tools available to make it easier. Once you have created the initial inventory, your baseline, go back at least monthly to look for new devices or devices that are no longer connected so you can update your inventory.\n- Determine what is running on all of your network devices. In the first step you inventoried the hardware—now we need to inventory what is running on each device. You can use tools such as Nessus to inventory the software on each computer as it scans the network to perform the device inventory. This is the quickest way to complete both of these steps. If there is old or unused software on a device, remove it. You need to document the operating system and application software on each device. This software Inventory should also be included in your baseline and verified/updated on at least a monthly basis.\n- Use the Principle of Least Privilege. This is a very valuable cybersecurity concept. Never give a user or device more rights on the network than they/it need to perform their assigned tasks. Privileges are assigned based on roles or job functions. If a user is unable to download and install applications on their PC or laptop, you reduce the chance of a device becoming compromised. Many hackers, once in a network, move laterally through the network from machine to machine looking for information or vulnerabilities that can be used to give themselves more abilities on the network. If a hacker were to gain access to a user account or system with low privileges, it decreases the amount of damage they could do.\n- Use Secure Configurations. All operating systems, web browsers and many other networked devices have secure configuration settings. One of the problems with doing this is that operating systems alone can have hundreds of settings to choose from. The Center for Internet Security provides benchmarks for just about every conceivable device. The CIS Benchmarks are distributed free of charge in PDF format to propagate their worldwide use and adoption as user-originated, de facto standards. CIS Benchmarks are the only consensus-based, best-practice security configuration guides both developed and accepted by government, business, industry, and academia.\n- Set up a policy and procedure for applying security patches. New vulnerabilities are discovered every day and when these vulnerabilities are found, vendors release updates or patches to mitigate the vulnerability. Exploiting vulnerabilities is what a hacker lives for. An unpatched vulnerability can be almost an open door for a hacker to get into your computer or network. It is mind boggling to hear that some organization was hit with ransomware because they didn’t load a security patch that was released six to 12 months ago. When an application reaches end-of-support, the vendor stops releasing patches, and that should tell you that it is time to upgrade the software to the newest version or find another tool to perform that task. Never use unsupported software on your network. Speaking as an auditor, a fully patched network is impressive.\n- Create an Incident Response Plan. Let’s face it, no matter what you do to protect yourself, something is eventually going to go wrong. Do you have a plan to continue operations if you lose access to your office building? Do your users know what to do if they receive or fall prey to a phishing e-mail? This process starts with performing a risk assessment. Once you have determined the potential risks, you then move on to determining how to mitigate the risks. You will need to create policies and procedures and then train the employees on them, so they know what to do.\nBy performing these six steps you will be protecting and strengthening your networks, your users, and trust me, you will impress the auditor. Also, it should be noted that these are not once and done steps—these are steps that must be repeated sometimes on a daily, if not at least on a monthly, basis.","Incident response plans are, in many ways, like family relics. These written instructions, which detail how firms should adequately detect, respond and limit the effects of an information security incident, are highly valued by some, and yet all too often left gathering dust in the cupboard. To many, they remain untried and untested for years, and thus most are unfit for purpose when that untimely data breach becomes reality.\nExperts say that a robust incident response plan today should include a policy that specifically defines what constitutes an incident, and that this document should provide a specific, step-by-step guide as to how firms respond to an incident. By following these guidelines, organizations hope to limit the damage of an attack, and reduce the high costs and lengthy recovery time that are usually associated with data breaches.\nYet for all of this, incident response plans divide the InfoSec community. eBay was heavily criticized for its failure to respond to its data breach last year, while many IR plans are often considered to be ill-thought out, or ill-equipped, for today’s world. Worse still, some firms don’t even have one.\nFor example, a study from PAC -- polling 200 people from companies with more than 1,000 employees in UK, France and Germany -- found that nearly 40 percent of firms did not have an IR plan. And of those with IR plans, only 30 percent said that they tested and updated these regularly, a concern given the evolving threat landscape.\nIndeed, citing the rise of Advanced Persistent Threat (APT) attacks, growing cyber-criminal activity and under-investment in protection and detection, cryptography expert Bruce Schneier said in 2014 that it was the decade of “response”.\nYet sadly, this is taking time to catch-on; a report from AT&T last month indicated that while 81 percent of IT professionals said their firms did have incident response plans in place, only 31 percent of these believed that these were actually effective. Clearly there is work to be done.\nIR plan faults\nThe problem for most companies is that even if they have IR plans, they’re often not fit for purpose.\nMcKinsey reports that most organizations “don’t truly operationalize their IR plans, which are ineffective due to poor design or implementation, or both.”\nThe firm says that such documentation is often “out of date” and “generic”, and “not useful for guiding specific activities during a crisis.”\nIn addition, the consultancy said that these plans can be created by one department, but not implemented by others. Developing such plans in silos not only damages a businesses’ response, but it can also inhibit sharing relevant knowledge and best practices.\nMcKinsey adds that IR decision-making “is often based on tribal knowledge and existing relationships”, with organizations typically relying on one to two “go-to” people to guide the organization through the crisis. This can result in a single point of failure when the resident expert is not available, or does not have the capacity to lead the response.\nDane Warren, CISO at UK-based product testing company Intertek, tells CSO Online it is difficult to tell why certain incident response plans fail.\n“[It’s] hard to say, without knowing the company. However, it is easy to say that an effective IR plan is paramount is any organization – even if it only used for preparation.”\nBut James Mckinlay, principal security consultant at Praetorian Consulting International and former head of information security at Worldline Global UK&I, tells CSO that often faults are down to departmental issues.\n“Not involving everybody that will be needed, internal communications, external communications, law enforcement, legal, hr, executives, risk director, CISO, client relationships”.\nWhat your plan should look like\nWhen creating your IR plan, InfoSec pros say you should set out to define its purpose, the role each team plays in an incident, as well as the lifecycle of the plan itself. Simulation exercises are also encouraged to stress-test these processes, and to prevent business confusion as to each department’s role and responsibility.\nThis is critical because IR plans must involve multiple departments, including information security, legal and compliance, human resources, and communications. A core team of cross-departmental reps should also be selected to take the lead in responding to incidents. Some say that one executive should be responsible for the plan and its integration across the business.\nWarren believes that security teams can get ahead in this process by understanding the business and where its core assets are.\n“Understand your business, and understand what needs to be protected. Aligning these two things will give you the basis for subsequent activities in an incident response scenario,” says Warren.\n[ ALSO: Incident response matters ]\n“Clearly defined roles and responsibilities are key. Ensuring that people are trained to effectively perform those roles and responsibilities is essential. A clear communication plan, based on the severity, is a must. This may include the general public, customers, internal staff, government officials, and / or suppliers.”\nMckinlay adds that preparation is key, as is having the right leader.\n“To prepare a decent incident response plan requires quite a bit of research and tailoring to the company in question.\n“Testing a plan, like testing Business Continuity Plans or Disaster Recovery Plans, requires either a very smart proactive security leader or an audit finding that dictates a test.\n“Because responsibility for having an incident response plan is likely to fall to the information security manager they have to understand a good one involves a lot of other people and areas outside of IT and security.”\nSANS, meanwhile, believes there are six key phases to developing a successful IR plan:\n- Preparation– Preparing users and IT to handle potential incidents should they happen\n- Identification– Figuring out what is meant by a “security incident”, and which events should be acted upon, and which should be ignored\n- Containment– Isolating attacked systems to prevent further damage\n- Eradication– Finding and eliminating the root cause (essentially removing affected systems from production)\n- Recovery– Permitting affected systems, once fixed, back into the production environment\n- Lessons learned – Writing everything down and reviewing and analyzing with all team members so you can improve future incident response efforts\nTeam and skills\nOther InfoSec professionals highlight the importance of having a good security team; At a London conference last year, SANS instructor Steve Armstrong said an incident response team should almost be like Scooby Doo’s team - with different people bringing different skills.\nArmstrong said that an effective incident response plan should see “geeks that love to geek, leaders that love to lead and managers that love to manage” but he admitted that this isn't always the case. He said that plans often fall down on communication alone.\nArmstrong added that a strong Digital Forensics and Incident Response (DFIR) plan relies on workers sending good intelligence and statistics onto managers, who in turn translate this in business language for company leaders. However, Armstrong warned that any disconnection along the way would see “risk comprehension and funding go away”, with “directors no longer engaged in what’s happening.”\nInstead, he urged CISOs to follow the much-publicized OODA (Observe, Orient, Detect, and Act) loop, which was used by air force, to become more fast and agile. This approach is also favored by Schneier.\nFor example, Armstrong said that a sysadmin or IT security team could observe an intruder on the network, decide a plan of action and then remediate. He urged firms to think about their plan, their communication (for example, how are they going to communicate if their network has become a hostile environment?) and how they can scale up operations. The whole plan, said Armstrong, needs to involve all departments.\nThis, however, requires skilled personnel and Armstrong warned between perceived skills and actual capability.\n“Attackers can see the inefficiencies of your team – they know you're not Bruce Lee. So you've got to make sure you look at the team, look objectively at what they're capable of doing. If they're not [up to speed], look to infill with help, or onsite training.”\nSo take that incident response plan off the shelf; a robust plan is very much achievable, so long as you get the right processes in place, the right people on-board and that you test it regularly to ensure it is fit for purpose. What are you waiting for?"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:b8db8b54-49d7-44b0-85c9-b981ce1cbbb4>","<urn:uuid:cc5da09f-787f-4c94-bcbd-343f702b1fa8>"],"error":null}
{"question":"What are the roles of government agencies and civil society in promoting religious tolerance, and how do international organizations track religious and non-religious discrimination?","answer":"Government agencies and civil society must work together to promote religious tolerance. The U.S. State Department facilitates engagement between citizens, faith-based organizations, and governments, while providing $8 million in funding for civil society programs related to religious freedom. They also offer diplomatic training on religion and foreign policy and sponsor interfaith travel initiatives. To track discrimination, the State Department produces annual International Religious Freedom Reports covering nearly 200 countries. Similarly, the International Humanist and Ethical Union (IHEU) publishes the Freedom of Thought Report, which ranks countries based on their treatment of atheists and non-religious people. These reports serve as valuable tools for policymakers to hold governments accountable for religious freedom violations and discrimination against both religious and non-religious individuals.","context":["Religion and Diplomacy: A Transatlantic Dialogue\nReligion and Diplomacy: A Transatlantic Dialogue\nPrincipal Deputy Assistant Secretary, Bureau of Democracy, Human Rights, and Labor\nMay 14, 2014\nGood morning. It is a pleasure and honor to be with you today. I want to thank The Brookings Institution, William Galston, E.J. Dionne, and Peter Mandaville for this opportunity to talk with you about the nexus between religion and foreign policy. This is an incredibly important topic for us at the Department of State. Recent events are confirming, from the Central African Republic to Crimea, now more than ever, the need to understand religious dynamics and engage with religious leaders. This is not only about promoting religious freedom but how to advance greater respect and acceptance across faiths.\nLet me assure you, religious freedom is a bedrock priority of American foreign policy, alongside democracy promotion. As Secretary of State Kerry stated at his confirmation hearing, religious freedom is “at the core of who we are.” Given the important role religion plays in the lives of so many people, we miss a key opportunity for engagement when we leave out religious leaders and their faith groups—particularly when conflicts arise.\nI understand today’s conference is the second in a series begun in London on Religion and Diplomacy. Today’s meeting is providing the space for this dialogue to continue between government and civil society. What makes it particularly rich is that the Brookings Institution has brought together government officials from multiple countries, including my own, with civil society representatives to strategize on how we might partner to counter rising religious intolerance.\nRegardless of our work affiliations, faith backgrounds or beliefs, I believe our vision is shared: the guarantee of universal religious freedom within peaceful communities free of violent extremism.\nSo where do we stand right now? Three facts before us:\nFact #1: We live in a religious world. According to Pew Forum statistics, 84% of the world’s population claims a religion. It’s clear that religion matters to the majority of the people around the globe. It is no surprise, then, that religious leaders and their communities sometimes hold the key to peaceful resolution and sustained peace.\nOften held as trusted agents by their communities, faith leaders can be authentic voices calling for tolerance and reconciliation. We have seen this in Ukraine where Christian, Muslim, and Jewish leaders are speaking out in support of each other and expressing their shared desire for peace.\nReligious leaders can be the voices of tolerance and acceptance, or the voices inflaming intolerance. A key goal of U.S. engagement with religious communities, therefore, is to urge them to use their leadership responsibly—to promote mutual respect and freedom for their own faith and for others.\nWestern diplomacy has traditionally not emphasized religious dialogue. So we miss important opportunities when we speak only to governments while leaving out religious groups and faith leaders. Interfaith dialogue and religious engagement are therefore key elements in our diplomatic outreach.\nFact #2: Religious intolerance is on the rise. Pew research tells us that the number of countries with religion-related terrorist violence has doubled over the past six years. Furthermore, more than 5 billion people—74% of the world’s population—live in countries with high levels of government restrictions or social hostilities involving religion. Too often, religious minorities are the target of both legal clampdowns and violence.\nAround the world we see conflicts involving religion undermining community well-being and turning neighbor against neighbor. We see this with attacks against Rohingya in Burma, with the recent kidnappings by Boko Haram in Nigeria, with attacks on Christians in some Muslim countries, and with the rise of anti-Semitic, xenophobic parties in Europe.\nAll of us—whether serving in government, civil society, or as members of religious communities—must also stand up and speak out against violence and intimidation carried out in the name of religion. We have a similar obligation to stand up for vulnerable groups facing persecution or discrimination by governments and societal actors.\nFact #3: Religious freedom promotes regional stability. Research shows that where there is religious freedom, there is more stability in communities. In denying religious freedom, governments often undermine their own interests. Crackdowns on religious freedom destabilize communities and suppress economic growth. When governments repress religious expression, when politicians co-opt religious leaders for personal agendas, when public figures fail to denounce religious bigotry, the groundwork is laid for violent extremism to grow.\nPromoting respect for religious freedom and tolerance are therefore essential for peaceful society—but this is not just the job of governments. It’s also the work of civil society to build bridges across religious divides and foster mutual respect. Government and civil society can exponentially increase their effectiveness in building peaceful, democratic societies when they can partner together.\nHow the Department of State is integrating religion in diplomacy.\nPresident Obama and Secretary Kerry have emphasized the importance of engaging religious leaders and communities in advancing development, human rights and conflict mitigation. To this end, the Department is facilitating engagement involving citizens, faith-based organizations, and governments. The Bureau I represent, the Bureau of Democracy, Human Rights and Labor (DRL), is heavily engaged in interfaith collaboration.\nTo give you a brief overview of the DRL bureau, we lead the Department of State’s efforts to promote democracy; protect human rights, including international religious freedom; and advance labor rights around the world. Our mission is to advance universal human rights and fundamental freedoms and to strengthen democratic institutions in pursuit of a more peaceful, prosperous and stable world.\nWithin DRL, we have undertaken a number of initiatives to operationalize religious engagement as part of our foreign policy. While not an exhaustive list, I’d like to note a few of the concrete steps we’ve taken:\n1) The International\nReligious Freedom Report and Country of Particular Concern\nDesignations: Each year we report the status of\nreligious freedom in nearly 200 countries. These reports\nprovide a strong tool to hold governments accountable for\ndeficits in universal religious freedoms that we all should\nenjoy. We also periodically review countries where religious\nfreedom is under the greatest threat. For those nations\nperpetrating particularly severe violations of religious\nfreedom, we recommend that Secretary of State designate them\nas Countries of Particular Concern. Currently these\ncountries include Burma, China, Eritrea, Iran, North Korea,\nSaudi Arabia, Sudan, and Uzbekistan.\n2) Promotion of religious freedom worldwide: Of course our work on religious freedom is not confined to reports alone. We make sure religious freedom concerns are addressed in diplomatic outreach, whether that be urging the government of Pakistan to repeal blasphemy laws or working with European nations and the OSCE to combat rising anti-Semitism and other forms of religious intolerance.\n3) The third area of religious engagement is through the Secretary’s Strategic Dialogue with Civil Society – Religion and Foreign Policy Working Group. Launched in 2011, these strategic dialogues created a formal channel for the Secretary of State to receive recommendations and perspectives from civil society. One of the suggestions coming from the Religion and Foreign Policy Working Group was to establish an official mechanism for faith-based communities to engage with the Department of State. Acting on this recommendation, Secretary Kerry launched the Office of Faith-Based Community Initiatives in 2013. I believe Shaun Casey, who heads up this office, will be speaking with you tomorrow about the incredible work they have been doing.\n4) Diplomatic training in religion and foreign policy. With a careful eye to the First Amendment’s Establishment clause, U.S. diplomats have sometimes steered clear of religious engagement, not wanting to overstep the line separating church and state. To better equip our diplomats with the fluency necessary to engage religious leaders, we now offer twice a year an intensive course on Religion and Foreign Policy at the Foreign Service Institute. As an indicator of the success of this endeavor, the upcoming class is already full. The Bureau of Conflict & Stabilization Operations has also been working to develop new, interactive training modules on religion and conflict mitigation.\n5) Conferences open to government and civil society. Since 2013, we’ve held three roundtables on the following issues. We know how important it is to collaborate with civil society and other government agency colleagues.\n• Working with Civil Society to Advance International Religious Freedom\n• Interfaith Dialogue, Religious Freedom, and Combating Violent Extremism\n• Women Religious Leaders and Conflict Prevention\nGiven that close to 100 people attended each of these roundtables—and over a third of them came from civil society or other US government agencies—I would say there is great interest in such dialogues. That’s also evident from the diversity of attendance here today. These events offer government and civil society a space to dig down further into implementing our shared goals on religious freedom.\n6) Sponsoring interfaith travel. When religious leaders have the opportunity to explore history together, breakthrough insights can occur. For this reason, we see great value in sponsoring interfaith travel. Last year, a group of imams and Muslim scholars along with the Special Envoy to Monitor and Combat Anti-Semitism and the Special Envoy to the Organization of Islamic Cooperation, visited Dachau and Auschwitz. At the close of the trip, they pledged to fight anti-Semitism and hate in their own communities. When leaders unite across faiths to speak up for each other in the face of religious intolerance, it’s an irrefutable message.\n7) Another important track for religious engagement is the Human Rights & Democracy Fund. Recognizing that governments cannot do it alone, DRL currently provides $8 million in funding for civil society programs specifically related to religious freedom. In this way, civil society can effect change at the grassroots level. Sometimes this programming is the only US assistance available to citizens working to improve their societies. Wherever possible, we collaborate with other agencies and bureaus to ensure that our programs are not duplicating other efforts.\n8) 16/18 Process. A final example is the implementation of UN Human Rights Council Resolution 16/18, which offers an affirmative path for combating discrimination and intolerance based on religion or belief. To this end, we facilitate a training program for local officials on cultural awareness regarding religious minorities and enforcement of non-discrimination laws. So far, successful training sessions have been held in Bosnia, Greece, and Indonesia, and we plan to expand the training to other countries soon.\nAs I have stated, we all realize the importance of working together to address religious intolerance and the violence and instability it can breed …it’s the implementation which presents the greatest challenge. The challenges are significant, but I believe they are surmountable. I understand you’ll be looking more closely at implementation and next steps this afternoon. I hope you have a fruitful dialogue and will share these conclusions with me and my staff. We look forward to more such conversations moving ahead.","NEW Report Names Best and Worst Nations for Atheists\nFor Immediate Release\nSarah Henry, (202) 238-9088, firstname.lastname@example.org\nBob Churchill, +44 2074908468, email@example.com\n(Washington, D.C., October 29, 2018) – Today, the International Humanist and Ethical Union (IHEU) launched its 7th annual Freedom of Thought Report at the United Nations General Assembly in New York City. For the first time ever the report contains a full ranking of every country in the world, according to its level of discrimination against atheists, humanists and the non-religious.\nSpeaking at the launch of the report at the United Nations General Assembly in New York, Ahmed Shaheed, UN Special Rapporteur on Freedom of Religion or Belief, said, “The Freedom of Thought Report has become an invaluable source of well-researched and important information for policymakers. The report highlights the range of discrimination that people can face around the world because of their non-religious beliefs, something that many would like to ignore.”\nThe report is published by the world’s leading organization for the non-religious, the International Humanist and Ethical Union (IHEU), and highlights the best and worst countries in which to be an atheist. The first Freedom of Thought report was published in 2012, in response to a meeting between the United States Department of State and the American Humanist Association. The IHEU supports humanists at risk of persecution, and has issued a renewed call for funding and donations to continue their important work.\nThe IHEU and the American Humanist Association (AHA) applaud the “pluralistic” approach to religion in public life in Belgium, Netherlands, and Taiwan, who share the number 1 spot. Nauru, France, Japan, and São Tomé and Príncipe are tied in the following position. Norway, the United States of America, and Saint Kitts and Nevis round out the top 10 list. The penultimate country on the list is Iran, and Saudi Arabia occupies the final position on the list of over 190 nations. This ranking firmly cements Saudi Arabia as the worst nation to live in as an atheist.\nThe appearance of Malaysia and Maldives in the bottom 10 is due to a rise in incidents of anti-atheist rhetoric in the past few years. In the Maldives, alleged atheists have been kidnapped, and secular activists have been disappeared or murdered; citizenship is restricted to Muslims and the previous government created and enforced an autocratic program of Islamization, though this may change following an unexpected defeat in last month’s elections.\nIn Pakistan, last year’s “anti-blasphemy crackdown” features prominently, several alleged atheist bloggers and activists having been arrested and tortured on charges of making posts online that were “insulting” to religion. Pre-trial detention and the slow progress of ‘blasphemy’ cases is a well-known issue in the Pakistani justice system, and some of those arrested as “atheists” early last year remain in prison.\nAndrew Copson, President of the IHEU, noted, “This is a world’s first. For the first time our report will show, with authority and accuracy, the discrimination faced by people around the world because of their non-religious beliefs. This report paints a dark picture, with significant discrimination faced by our non-religious friends and colleagues around the world.\n“At a time of growing nationalism, we continue to see those who are brave enough to criticize and critique conservative religious leaders demonized as ‘unpatriotic’ and ‘subversive’.\nFind the report in full here.\nFind the data behind the report here.\nSupport the work of the International Humanist and Ethical Union here."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:55613e33-3849-4336-96f9-1ef60037d30b>","<urn:uuid:b3322df3-6056-4468-bad7-7e765dcea9b6>"],"error":null}
{"question":"As a housing researcher studying demographics, could you compare homeownership rates between urban and rural areas, and explain how LGBT acceptance varies in these communities?","answer":"In terms of homeownership, rural areas have significantly higher rates at 81% compared to urban areas at 60%. This difference is partly due to rural areas having an older demographic who are more likely to own homes and age in place. Regarding LGBT acceptance, while most cohousing communities pride themselves on LGBT acceptance, the experience varies. In liberal communities, LGBT individuals generally feel safe, though some report feeling invisible due to being a small minority. For instance, at Wasatch Commons in Salt Lake City, a trans man reported feeling accepted during his transition. However, there might be negative reactions if LGBT residents more openly challenged traditional gender norms or practiced polyamory in larger numbers.","context":["Pictured: Aitch Muirhead and Shira Frank, a trans man and his spouse, who live at Wasatch Commons in Salt Lake City.\nThe predominant demographics of American cohousing communities are pretty clear: white, middle or upper middle class income and background, educated, liberal, with a strong tilt towards older folks and older women. There is a small sprinkling of non-white people, but very few are African American, Latino, Native American, or Asian. What are the realistic options for people who are interested in cohousing but don’t fit the typical profile?\nMy experience has been positive, but I have only one mild strike against me, and in a liberal pool, it’s barely a strike. I’m a lesbian.\nBirds of a feather flock together. LGBT folks have created their own intentional communities for decades with need for safety and support (see www.ic.org/wiki/queer-community). In North Carolina, Village Hearth Cohousing (www.villagehearthcohousing.com) is forming for LGBT folks and allies. The current planning group is an all-lesbian group of older women. As the US has become more gay-positive, most cohousing communities pride themselves on LGBT acceptance. My only complaint at Columbia Ecovillage (columbiaecovillage.org), where I live, is that I feel invisible. There are so few of us queer folk and our rich history as a minority subculture is lost in the homogeneity of our largely heterosexual community. Most of us LGBT folks are older, which means we grew up in a more closeted, conservative era and are not on the cutting edge of queer culture.\nAitch Muirhead (theadventuresofaitchalexandar.com), a young transitioning man who lives with his wife at Wasatch Commons (content.csbs.utah.edu/~ehrbar/coho) in Salt Lake City, has felt accepted during his transition from woman to man and the couple are happy to be living in a supportive community. If we queer folk were half our communities, however, and if more of us openly challenged traditional gender and sexuality norms by refusing gender pronouns or practicing polyamory, there would likely be negative reactions or concern. In general, though, cohousing culture seems to be a safe place for those of us queer folk who can afford to join!\nI do otherwise fit the cohousing profile: middle class, educated, liberal, and 64 years old. I am not a person of color nor am I poor. If I were, I would face almost insurmountable barriers to living in cohousing today.\nCohousing communities are increasingly concerned about their homogeneity, but seem pretty much stumped on what to do, particularly when it comes to race and ethnicity. Why do people of color not typically join cohousing planning groups nor purchase or rent in cohousing? When Diane Leafe Christian, a leader in the cohousing movement, queried how communities are responding to this challenge in a National Cohousing Association blog post (www.cohousing.org/node/1672), it looks she didn’t get any replies. She was of the opinion, however, that cohousing communities are “welcoming” to people of color.\nTavi Baker, who works for the Boys and Girls Club of San Francisco and is one of several organizers of the The People of Color Sustainable Housing Network (www.meetup.com/People-of-Color-Sustainable-Housing-Network), would likely disagree. Baker has experience living in cooperative housing and attended meetings of the East Bay Cohousing network to consider cohousing, which is predominantly white. She left, she says, to join other people of color to set up the POC Network and to plan their own communities. “I was tired of being the only one in the room,” Baker says.\nMainstream cohousing organizations tend to reproduce the same unequal power relationships that exist in society, says fellow organizer Deseree Fontenot, who is in the process of getting a masters degree in Social Transformation. POC Network organizers agree that cohousing and other communities must engage and provide a “point of entry” for people of color from their beginnings and must focus not only on sustainability and community, but also on activism and social change—including being willing to share power.\nAnd, they say, people of color will not be drawn when they are a tiny minority in a white cohousing community. Without a critical mass of racial and ethnic minority representation, many interested people of color will not feel truly welcomed, no matter how friendly cohousing planners might be.\nWe liberal white folks are often unaware of our own attitudes and subtle behaviors when it comes to race and class. We typically lack personal connections to and within minority communities. We are often blind to our privileges and are not able to put ourselves in the shoes of someone who is racially or ethnically different, surrounded by white people. And we easily forget that people of color continue to be economically disadvantaged and targeted.\nSome cohousers like Zev Paiss, a former Executive Director of the Cohousing Network, have theorized that people of color may not have the same need for intentional community as white folks do (www.ic.org/wiki/desire-diversity-cohousing-perspective). The POC Network organizers disagree. “There is a rich history of intentional communities developed by people of color,” says POC Network organizer Lina Buffington, an organizational consultant and activist with a Ph.D. in Philosophy. African American farmers were the first to use land trusts in the South. The Black Panthers created a variety of communal housing groups. The MOVE activist community in Philadelphia lived communally. Most of these efforts, she says, were systematically destroyed by the white establishment, but all were engaged in social justice work.\nIn the Bay Area there is clearly a high interest and demand among people of color for intentional community. The need is especially critical in Oakland and the Bay Area in general due to gentrification and extreme increases in housing costs. The POC Network, launched in February 2015 as a Meetup group, already has 140 members with 450 friends on Facebook. The Network’s aim is to create and support the development of POC-centered sustainable communities, with an eclectic, connected network of alternative housing and organizing communities in the area. Options could include cohousing, land trusts, bedroom rentals, large shared houses, accessory dwelling units, and more rural communities with existing or new construction.\nCohousing communities today are typically not focused on issues of poverty, racism, sexism, immigration, LGBT or other community justice issues. In contrast, the POC Network’s projects will emphasize social change activism by and among residents, with organizing centers and mutual networking and support. The leaders are themselves activists, and say that without this focus, people of color are unlikely to be attracted to cohousing communities.\nThe POC Network organizers report that they have drawn a racially diverse group of folks, including a significant number of LGBT people. Although the majority are in their 20s to 40s, the network hopes to draw an intergenerational community of participants. Currently their focus is to build infrastructure by raising funds for staff. Current projects include helping to develop two three-acre communities in East Oakland and El Sobrante using a land trust model. This model would allow communal land use that is affordable for low-income families in perpetuity, protecting residents from the vagaries of the housing market and economy.\nAlthough the busy volunteers who have spearheaded this movement are focused on the Bay Area, they are in communication with a variety of similar efforts throughout the country. They are willing to provide support and consultation to other networks of color. “We are in the infancy stage,” says Buffington, who was preparing to speak to a group in Atlanta who wanted to discuss ideas for a joint communal land purchase. “And we want to connect people to each other whenever we can.”\nI trust that this Network will be a catalyst for communities of color, at least in urban areas with high diversity and activism. I have doubts, however, that the current mainstream cohousing world will begin to attract more people of color without a significant shift to social justice aims and activities, and without better addressing issues of privilege and affordability.\nThe good news is that affordability is a hot topic within the cohousing world. Communities are very focused on environmental sustainability, and build or remodel housing units using green, high-cost technologies. Large properties with significant communal structures require higher prices. Many cohousing communities have various limits on the number of rentals permitted. Cohousing tends to be concentrated in urban areas with high housing costs in general. And because of the concentration of older residents, there is sometimes conservative resistance to non-traditional housing arrangements which may cost less.\nCreative solutions are beginning to emerge, with some good results. Within the constraints of local housing regulations and available affordable housing programs, communities are expanding affordable options, including government-subsidized units, shared housing, and construction and rental of small accessory buildings. At The Commons (santafecohousing.org) in Santa Fe, 11 casitas (little attached homes) were constructed next to their larger, more expensive homes to provide lower-cost rentals. At Sand River (www.sandriver.org) in Santa Fe, a seniors-only cohousing community, a partnership with a local affordable housing program permitted the construction and sale of several lower-cost homes. Troy Land and Gardens (www.communitygroundworks.org/what-we-do/troy-land-gardens) in Madison, Wisconsin uses a land trust model to create a majority of income-restricted homes. And the nonprofit Partnerships for Affordable Housing (www.affordablecohousing.org/home/mission) is working nationally to support the development of affordable cohousing options for low- and moderate-income residents, with a mission that includes empowerment of tenants and greater involvement in local community social justice goals.\nAs a person who has worked in social and empowerment services my whole life, I continue to feel ambivalent about the concentration of whiteness and relative wealth in cohousing. But if I were still living alone in a single family home, would I somehow have more integrity? Not really. I have to be honest that I am flocking with birds of my feather, and that I prefer communal living to solitude, no matter what social justice values I may find missing from my community. It’s my own responsibility to promote these values within and without the fences of my ecovillage, and to help create the kind of community I want to see.\nCynthia Dettman is a lesbian and retired community college counselor who moved in to Columbia Ecovillage in 2014 and has not looked back. She worked as a legal aid attorney and later coordinated empowerment services for low-income women at Mt. Hood Community College. In retirement, she is writing about social justice issues, working on a novel set in South India where she grew up, teaching college success classes, and cooking gourmet meals for her cohousing community.","This is the fourth post in a series about reducing the divide between urban and rural communities.\nTo bridge the gap between urban and rural areas, we first need to understand what sets these communities apart and where they have common ground. Below are a few key stats about urban and rural populations, economies and housing, and the different types of critical challenges these communities face.\nDespite the vast amount of discussion about the urban-rural divide, there is actually little agreement about what these terms mean. The most common definitions are from the Census Bureau — “mostly urban,” “mostly rural” or “completely rural” — and the Office of Management and Budget (OMB) — “metropolitan” or “non-metropolitan.”\nThe problem with these definitions is that more than half of the country’s rural population, as defined by the Census, lives in less densely populated parts of OMB metropolitan areas.\nNo matter how you slice it though, the clear majority of Americans live in urban areas. This share continues to grow as people move from rural to urban regions. Those who live in rural communities tend to be older (with an average age of 51 years, versus 46 in urban communities) and less educated (20 percent with a bachelor’s degree versus 29 percent in urban communities).\nRural and urban unemployment rates have improved in recent years, but labor force participation — the share of the population working or seeking employment — remains below pre-recession levels. Specifically, the gap between urban (63 percent) and rural (59 percent) labor force participation is significant and largely attributed to an aging rural workforce. Additionally, most rural communities still have not recovered the jobs they lost during the recession.\nHowever, not all rural communities are the same, and some are outpacing the growth of urban areas on key economic indicators. For example, many rural areas have higher rates of entrepreneurship, and the National League of Cities’ (NLC) own research found that businesses that export their goods and services are thriving in rural communities. Rural areas in many states are also making outsized contributions to their states’ GDP.\nDespite their differences, affordable housing is a prevalent concern amongst both urban and rural communities. Across the U.S., only 46 units are available for every 100 extremely low-income renter households. The problem is more severe in cities, which typically have only 42 units per 100 low income renters. Rural area housing challenges are compounded by the fact that residents typically have lower median incomes and available affordable housing is often poor quality.\nRural residents have higher homeownership rates (81 percent) than urban residents (60 percent). Younger adults are both more likely to rent and more likely to live in urban areas. Rural areas have an older demographic, whom are both more likely to own their homes and age in place. Lack of mobility within rural housing markets contributes to an overall housing shortage in these communities, limiting business expansion and attraction opportunities.\nOver the years, several other differences have become prominent between urban areas and their rural counterparts:\nIn a world dependent on online communications, broadband access remains a challenge in rural areas. In all states, broadband access is higher in urban areas than in rural ones. While 63 percent of rural Americans say they have a broadband internet connection at home, increased from 35 percent in 2007, there are still many challenges to improving accessibility.\nThe cost of providing services is the most significant hurdle. Even where broadband is available, it is often prohibitively expensive, leading to gaps not only in access, but also in adoption. Lack of internet has widespread consequences, particularly for rural economic and educational opportunities.\nHealth and Opioids\nThere were more than 33,000 opioid-induced deaths in 2015, a fourfold increase from 2000. Although overall opioid mortality rates are higher in urban counties, mortality rates in rural areas have increased more quickly across all regions over the last two decades. Between 1999 and 2016, opioid death rates in rural areas have quadrupled among the 18 to 25 age group, and tripled for females. To treat and prevent opioid addiction, various healthcare services are required. Unfortunately, resources are more limited in rural areas.\nThe lack of improved access to healthcare services also has significant impacts on life expectancy. On average, from 2005-2009, the life expectancy in rural areas was 76.7, compared with 79.1 for urban dwellers, a gap that has widened significantly over the past 50 years. People living in rural areas are also more likely to die from the five leading causes of death, including heart disease, cancer and stroke, than their urban counterparts. In 2014, approximately 71,000 deaths among rural residents were potentially preventable.\nDespite the wide gulf between urban and rural communities, there are a number of challenges they share, from affordable housing to opioid addiction. These common challenges provide opportunities for shared solutions.\nAbout the Author: Rose Kim is a research associate in NLC’s Center for City Solutions."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:988fab29-7f0e-4847-88ef-9179f6d460be>","<urn:uuid:8185aa9c-c26b-44ec-99aa-ee0ffaa16ac2>"],"error":null}
{"question":"What was the Normandy Allies' 2008 trip about, and how did it connect to the strategic importance of LSTs in WWII?","answer":"The Normandy Allies' 2008 trip was their tenth annual journey to visit the 1944 D-Day landing sites, including Utah Beach and various key locations across Normandy. The group, consisting of students and adults, visited sites that were crucial to the invasion that LSTs made possible. While the trip covered the entire landing area from Pegasus Bridge to Utah Beach, the strategic significance of LSTs becomes apparent when considering that these ships were what Churchill called 'the destinies of two great empires.' Without sufficient LSTs, the Allied invasion force would have been much weaker - their availability enabled the addition of Utah Beach to the invasion plan, which proved crucial as the Utah landings resulted in 23,000 troops and 1,700 vehicles pushing almost 5 miles inland with only 200 casualties, compared to the heavier losses at Omaha Beach.","context":["International Experience July 2008\nNormandy Allies 2008: Our 10th International Experience\nby Walter Carter & Marsha Smith\nNormandy Allies took its tenth annual trip to visit the sites of the 1944 landings and battles of World War II. During July 13-24, eight students and thirteen adult travelers participated in the International Experience with a team of four Normandy Allies’ Board members: Marsha Smith, LTC(Ret) Peter Combee, SGM(Ret) Charles Frick, and Walter Carter.\nThe students were welcomed by Norman host families for 4 nights during our time in Normandy—combining history and culture, as Austin commented: The most significant event that happened during the trip for me was being able to learn and understand about the WWII events and the significance behind them, while also experiencing the French culture via their food, habits, living conditions and general differences from the US. All of these events wrapped together made this trip a real learning experience from a historical and cultural standpoint.\nWe were fortunate to have among our travelers WWII veteran Don Robertson from Des Plaines, OH (originally with the 147th Combat Engineers Battalion attached to the 6th Engineer Brigade, and later transferred into the 29th Division).\nDon said of the trip: “[I got] massive amounts of new information and knowledge of what happened around me in 1944. This has been a total explanation of the war.”\nThe other 12 adults included several educators and one lady who lost her husband on D-Day. One adult commented that the trip was “beyond my wildest dreams, a very enjoyable experience.” Another added, “A truly outstanding and educational trip with a good group of people.” The widow’s daughter said, “I believe [Mom] is at peace with the circumstances surrounding her husband’s final interment.”\nOur itinerary covered the landing area from Pegasus Bridge and Sword Beach on the east to Utah Beach and Sainte-Mère-Eglise on the west. We toured major museums at Pegasus, Caen, Bayeux, Juno Beach, Arromanches, Sainte-Mère-Eglise, Utah Beach, and Saint-Lô; walked along the shore and bluffs at Omaha Beach; paid our respects in cemeteries at Bayeux, Colleville, Saint-Lô, and LaCambe; inspected German fortifications at Pointe de Hoc and Longues sur Mer; visited Graignes, where larger German forces finally overwhelmed a small group of American paratroopers and executed a number of prisoners and French villagers who had helped them; listened at Trevières to the stories of French citizens who lived under the German occupation and through the liberation; viewed an exhibit in the Chateau de Colombières on US Army intelligence work carried out there by some of “The Ritchie Boys” (See the wonderful recent film by that title!); and stopped at the Bois de Bretel along Captain Carter Road, where Walter’s father was killed while trying to rescue wounded solders in June 1944.\nWe are grateful to several French citizens gave us invaluable and hospitable assistance in making our appointments and visits: Jacques and Jeannette Chambon, Jean Claude and Nicole Joussard, Bertrand LeJemtel, Joseph Leprieur, Jean-Pierre Richard, Pierre and Collette Labbé, Jean Mignon, Jeanine Vérove, Denis Lesage, Michel Henry, Charles de Maupeou, and (Englishman) Michael Yannaghas.\nWe are also grateful to our team member, LTC(ret) Gene Johnston, who was unable to be with us in person, yet had made many arrangements for our group.\nM. André Heintz (in black sweater on left), former member of the French Resistance, retired university professor and co-author of “If I Must Die…From ‘Postmaster’ to ‘Aquatint'”, gave us an extensive historical account at the Abbaye d’Ardenne, just inland from Juno Beach. Here we paid our respects at the memorial to the Canadian prisoners of war who were executed by German SS troops.\nPrompted by SGM Frick, M. Heintz also told of his own experiences as a member of the Resistance before the landings, and during the bombardment of Caen. We were all moved by his story, and by his humble manner of relaying extraordinary events. As participant Pat Stark commented, “André Heintz was powerful.”\nJoseph Leprieur, Secretaire of the Town Council of Sainte-Mère-Eglise, spoke on behalf of Mayor Lefevre and welcomed Marsha Smith and the Normandy Allies participants to a luncheon at the Town Hall. Several members of the Town Council joined us, as well as representatives from the Airborne Museum who later provided the group with admission to the outstanding museum.\nOne full day of the journey is spent at Utah Beach and Sainte-Mère-Eglise, with additional time at La Fière and Graignes. The airborne assault and the ensuing sacrifices and accomplishments of the 82nd Airborne and the 101st Airborne are a significant part of each Normandy Allies tour.\nOn the July 18 anniversary of the liberation of Saint-Lô, we were honored to attend the commemoration ceremony with Sally Howie McDevitt and members of her family. Sally is the daughter of Major Thomas Howie, “Major of Saint-Lô,” who was killed in action there in July 1944. We were also joined by French students, Lucile and Virginie.\nWe honored the 29th Division by a visit to its monument at Vierville s/Mer, where we recited the Association’s preamble. At St. Jean de Savigny we were welcomed by M. Denis Lesage, President of the Wall of Remembrance Association, and a crowd of community residents. Against a backdrop of flag-bearers with their display of colors, and following the sounds of the French and United States national anthems, Marsha Smith and Walter Carter (Honorary President of the Association) spoke to the assembled crowd, and the eight students recited in unison a poem in French.\nSome special moments:\nFor comments from our participants—see “Trip Feedback”\nFor additional information on our work—request our Annual Report from Marsha Smith. This is available in print-format only; go to our Contact Us page and request to have a copy of our annual report sent to you.","I don't think they even saw each other. The German sailors were shooting at blips on a primitive radar screen, and the British and Americans were just panicking, like chickens with a fox in their hen house. But in that confused melee on a moonless April night ten miles out in Lyme Bay off the English coast, Nazi Germany might have come within one torpedo of winning the Second World War. All they had to do was sink one more of those lethargic targets waddling across their sights at ten knots, and the Allied invasion of France would have been delayed for months, perhaps forever. But how could they know that, when the vital importance of these fat clumsy ships came as a surprise even to the man who inspired their creation? A year earlier Winston Churchill had complained that “ the destinies of two great empires . . . seemed to be tied up in some god-dammed things called LST's”\nThe need for such a ship occurred to Churchill in June of 1940 when 300, 000 British and French soldiers were rescued off the beaches of Dunkurque. The men were saved, but all their trucks and tanks and cannon had to be left behind. So, the need was simple - a vessel which could run up on a beach to directly load or disgorge heavy tanks. Of course in practice things were more complicated. The ship would require a shallow draft to “beach” itself, but a deep draft to remain stable while crossing the open sea. While loading and unloading it had to remain level with its stern afloat and its bow on dry land and pointing “up” the beach. Naval Architect Rowland Baker was ordered to design this floating contradiction. Luckily, he knew enough about ships to be a genius\nLike all engineering problems, the most elegant solution was the source of the problem, in this case sea water – too little or too much. Impact with the land would require the strength of a double hull, while pumping sea water between the hulls would lower the draft, making the ship more stable in the open sea. Selectively pumping that water out of compartments between the hulls would allow the ship to come inshore, while balancing level. All it required was a bit of plumbing, which Baker ingeniously provided. Britain managed to convert three small oil tankers to the new design, but their ship yards were already stretched thin. So, in October of 1941 Baker was sent to Washington on a Lend -Lease shopping spree.\nThe U.S. Bureau of Ships considered the design submitted by Baker, and they assigned John Niedermair to fix it. He made the ship bigger (330 feet long), which allowed it to carry 2,100 tons of equipment, and he added a winch system to the anchor chain to help drag the ship off the beach. He even made the bow doors wider. In early November of 1941, Britain immediately ordered 200 of the new Landing Ship Tanks. And then in December Japan bombed Pearl Harbor, and then Hitler, for some insane reason, declared war on America, as well. And suddenly the US Army and Marine Corps were demanding as many of this ship as possible, when two months earlier none of them had even heard of. And that created a new problem.\nThe U.S. Navy went from 790 active ships in December of 1941, to 6,700 in August of 1945. There was no room in U.S. shipyards for the last minute orders for LST's for our own military, let alone the British. So the decision was to open “cornfield shipyards”, with companies like Chicago Bridge and Iron on the Sennica River in Illinois, Dravo in Pittsburgh and American Bridge in Ambridge, Pennsylvania, and the Missouri Valley Bridge and Iron in Evansville, Indiana. - all on the Ohio River, and all building LSTs. It took them six months to build the first ones. That was incredibly fast for ship building, but not fast enough.\nThe original plan to invade France called for just three divisions in the first wave, and the only American division was to land on the beach code named Omaha. It was clear, that would not be strong enough to guarantee success, but with only 300 LST's in Europe, they could not land more. In September of 1943 the head of the U.S. War Production Board, Donald Nelson, visited London and experienced Churchill's panic first hand.. Nelson cabled his staff that LST's were “most important single instrument of war”, and he added “the need for these ships has been grossly understated”\nOn September 8, 1943 the keel was laid for LST number 507 at the Jefferson Boat and Machine Corporation in Jeffersonville, Indiana, on a bend in the Ohio River. Like all 117 of her sister ships built here – like the 1,000 of her sisters built in America, the 507 was assembled from 30,000 separate parts, and built in sections, which were then welded together, mostly by women earning $1.20 and hour and working 9 hour shifts and a 54 hour work week. Building an LST Ten weeks later the 507 was launched sideways into the river, and after fitting out with her deck guns, she sailed to New Orleans, where she officially joined the U.S. Navy on January 10 – 124 days from laying keel to joining a convoy. She was just one of 62 brand new LST's that reached England that winter.\nThis amazing influx of LST's allowed allied planners to add two divisions to the first wave, and a second American beach, this one to be named Utah. But just inland from Utah beach, on Omaha's right flank, the Germans had flooded the land, leaving only two roads off the beach. That tactical problem could be solved, but the Americans needed someplace to practice, someplace with a lake just inland of a beach. And in December of 1943, three thousand British citizens were evacuated from the south coast of Devon in southwest England, around an area called Slapton Sands. It was a duplicate of the topography in Normandy. It was here that in April of 1944, the allies staged Operation Tiger, a full scale live-fire practice of the Normandy landings.\nAt 7:30 in the morning of Thursday, April 27, 1944, 30,000 men of the U.S. 4th division were to rush ashore on Slapton Sands from, among other ships, 300 LST's. But a glitch had thrown off the time table for the first wave. The gunnery ships were half an hour late, but some of the landing craft stayed on the original schedule. The gunnery officer on the cruiser HMS Hawkins, noted, “they had a white tape line beyond which the Americans should not cross until the live firing had finished. But...they were going straight through the white tape line and getting blown up.” About 300 men were killed or wounded. After that “glitch”, the rest of the first day's operations went smoothly, at least until after midnight, when the follow up units were preparing to practice their part.\nIt was after one in the morning of Friday April 28th, that eight LST's were plowing broad circles at ten knots in the calm, cold waters of Lyme bay, off Slapton Sands. Their crews joked that the ship's initials actually stood for “Large Slow Target” or “Large Stationary Target”. This morning they were about to be proven correct. These eight ships carried quartermasters, engineers, and even a graves registration unit, the house-keeping support without which an army cannot fight for long. Bearing down on them were nine German Schnellbootes, (fast boats), each 120 feet long, armed with four torpedoes and cannon and machine guns and capable of 42 knots – or four times the top speed of the LST's. All of the German boats carried radar. Only one of the LST's did.\nAccording to the official U.S. naval history, issued in 1946, “LST 507, the first attacked, was hit by several torpedoes which failed to explode, then was set afire by a direct torpedo hit. Another struck five minutes later. The enemy craft strafed the decks with machine guns, and fired on men who had jumped into the water. LST 507 began to settle. About the same time, LST 531 was hit and set afire... About 0210(am), LST 289 was hit by a torpedo which destroyed the crew's quarters, the rudder and the rear guns...” Amazingly, with its stern almost blown off, LST 289 was able to make it safely back to port. But after burning for two hours, LST 531 sank. Finally a British destroyer arrived to pick up survivors, and was ordered to sink the wreckage of the pride of Jeffersonville, Indiana, LST 507. She had been in existence for six months, from birth to death. Her remains now lay 200 feet beneath Lyme bay, at 50°29′N, 2°52′W. The cost of that April night was 198 dead American sailors, and 551 dead American soldiers – 749 total, plus wounded. So tight was security surrounding the invasion that all survivors, the wounded and their doctors and nurses were sworn to secrecy, and many of the dead were buried in unmarked graves.\nFor the planners, the loss of three LST's meant that on D-Day, June 6, 1944, there were no LST's in reserve. One more sinking would have meant a weakening of the invasion. That was how close the German sailors came to stopping D-Day. They never knew it, of course. They never saw what kind of ships they were shooting at. On June 6, 1944, the landings on Omaha Beach came perilously close to a disaster. After losing 3,000 casualties, American troops were able to push barely 1 ½ miles inland. Meanwhile, on Utah, the beach added because of the rush the previous summer to produced LST's, 23,000 troops and 1,700 vehicles pushed almost 5 miles inland, at the cost of only 200 casualties.\nAs of May 1st, 1944, the full production of LSTs was assigned to the Pacific. During a war, the key to success, is to not look back. But looking back after the war, it was clear the invasion of Normany was the product of total war - in this case, the genius of a British design, improved by an American, implemented by the entire American economy, including thousands of women who had never before had a job outside the home, and never dreamed they would build a sea going ship, who strained to build enough, to build that one ship more than was needed to give the allies a chance at victory. When you speak of war, it is best to remember not only how many lives it costs, but also the unimagined demands it makes on the nation. Because you can never know in advance, what God-damned thing will be vital this time.\n- 30 -"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:39a8764d-a015-4105-a0e7-9b669b818694>","<urn:uuid:df36c883-db90-4fc1-8a98-ad8b5a0874c2>"],"error":null}
{"question":"As a biblical scholar, I'm comparing different approaches to textual interpretation between Numbers and modern hermeneutics. What are the key differences in how religious texts were applied in Numbers versus the four-step modern application method?","answer":"In Numbers, religious texts and commandments were applied directly and literally - for instance, the priests were given specific instructions for blessing Israel and the Levites had explicit responsibilities for the Tabernacle service. In contrast, the modern four-step method emphasizes first determining the original application, evaluating its specificity (whether it's timeless or time-bound), identifying cross-cultural principles, and then finding contemporary applications that embody those broader principles. While Numbers focused on direct divine commandments for immediate implementation, modern hermeneutics requires careful analysis of cultural context and the extraction of universal principles before application.","context":["How to Apply the Bible to Your Life in Four Steps\nOne hallmark of biblical interpretation is the meant-means distinction: we need to determine what the Bible meant (to the original author and audience, in their context and culture) before understanding what it means (to us in our context and culture).\nAuthors William Klein, Craig Blomberg, and Robert Hubbard echo this hermeneutical rule in Introduction to Biblical Interpretation, Third Edition, a fully updated resource to help students unravel the mysteries of interpreting Scripture.\nOne of those mysteries is the means side of the equation: how to apply the Bible. The authors explain that “all applications must be consistent with the meaning of passages arrived at by means of…sound hermeneutical principles” (609). But how can one make the connection between what a passage meant, as determined through sound hermeneutics, to what it means today?\nThankfully, the authors offer a sturdy four-step method for legitimate application—which we’ve briefly outlined below.\n1.) Determine the Original Application(s)\nStep One begins with the original audience: “How did the biblical authors of a given passage want his hearers or readers to respond? What did the author intend the readers to do?” (611) Klein, Blomberg, and Hubbard offer several more questions:\n- Is there a command to obey?\n- Is there an example to follow or avoid?\n- Is there a promise to claim?\n- Is there a warning to heed or a teaching to act on?\nThis initial step in determining a legitimate application for a given passage recognizes that what it meant back then affects what it means right now. For instance, “obeying the command not to covet a neighbor’s wife or house remains as timely today as it did when Moses received it on Mount Sinai (Exod 20:17)” (611).\nConversely, applying the marketplace evangelism found in Acts 17 as a model for all evangelism may not be appropriate. Given that the kinds of socially acceptable public arenas for considering new ideas simply don’t exist in Western cities, we may need to look for more suitable forms in our day. Step Two helps us here.\n2.) Evaluate Level of Specificity\nOne major application dilemma is understanding whether a biblical principle is timeless or time-bound, transcultural or culture-specific.\nTo help make such a determination the authors offer 10 questions to ask of a given passage:\n- Does it present a broad theological or moral principle or a specific one, which the Bible offers elsewhere?\n- Does the larger context limit the application or promote a more universal application?\n- Does subsequent revelation limit or qualify the application?\n- Is its teaching contradicted elsewhere, showing it was limited to exceptional situations?\n- Are cultural conditions identified or assumed by its authors, making its universal application inappropriate?\n- Is the particular cultural form expressed by it present today, and with the same significance?\n- Is the applicational rationale rooted in creation, in God’s character, or in part of his redemptive plan?\n- Is the command or application at variance with standard cultural norms of its day?\n- Is there an explicit or implicit condition that limits its application?\n- Should we adopt a “redemptive movement” hermeneutic that suggests we move beyond NT teaching?\nWorking through these questions will help you evaluate the level of specificity of a passage’s original application.\n3.) Identify Cross-Cultural Principles\nIn this third step we ask, “Can we deduce a broad principle that a specific biblical text promotes as timeless even if we cannot apply universally without alteration the particular command, example, promise, or warning of the text?” (629).\nThe authors continue with another important part to this step: “If we discern such a principle, we must then devise new illustrations or applications of that principle for new situations” (629).\nFor example, the authors suggest Paul’s teaching on food sacrificed to idols carries the broader principle “freedom for Christians on morally neutral practices while they weigh how their freedom might affect fellow believers” (629). Likewise, for the injunctions against tattoos, “the principle is not to imitate pagan religious practices that call one’s allegiance to Christ into question” (629).\n4.) Find Applications That Embody Broader Principles\nFinally, the authors emphasize that applying the Bible now is entirely concerned with translating principles that led to applications back then:\nKnowing the practice back then that implemented the underlying principle enables us to discern the appropriate practice today that implements that same principle. (632)\nFor instance, giving hearty hugs are appropriate in place of holy kisses. And although we may not need to concern ourselves with meat sacrificed to idols, we should be concerned with drinking alcohol in front of recovering alcoholics.\nWhat this requires is discernment. It also demands greater sensitivity to cross-cultural contexts. “We must learn not only to exegete the Scriptures but also to exegete cultures” (634).\n“This readable and profound book,” says Tremper Longman, “covers all the important topics of interpretation with great skill.”","The Book of Numbers (Greek: Αριθμοί arithmoi meaning \"numbers\") or Bəmidbar (Hebrew: במדבר, literally \"In the wilderness of\") is the fourth book of the Hebrew Bible, and the fourth of five books of the Jewish Torah/Pentateuch. This book may be divided into three parts:\n- The numbering of the people at Sinai, and preparations for resuming their march (1–10:10).\n- An account of the journey from Sinai to Moab, the sending out of the spies and the report they brought back, and the murmurings (eight times) of the people at the hardships by the way (10:11–21:20).\n- The transactions in the plain of Moab before crossing the Jordan River (21:21–36).\nIn Numbers, the priests are instructed to bless the nation of Israel as follows: “May Yahweh bless you, and keep you. May Yahweh let his face shine on you and be gracious to you. May Yahweh show you his face and bring you peace.” This priestly blessing is regularly performed during Jewish services, on Jewish holidays, and sometimes by parents over their own children before the Friday Shabbat meal.\nThe period comprehended in the history extends from the second month of the second year, as measured from the Exodus, to the beginning of the eleventh month of the fortieth year, in all about thirty-seven years and nine months; a dreary period of wanderings. They were fewer in number at the end of their wanderings than when they left the land of Egypt. According to tradition, Moses authored all five books of the Torah. According to the documentary hypothesis, Numbers, with its dry style and emphasis on censuses, derives from the priestly source, c. 550–400 BC, and was combined with the other three sources to create the Torah c. 400.\nThe Hebrew title Bəmidbar, short for bəmidbar Sinai (\"in the desert of Sinai\"), is taken from the first verse, and \"serves to foreground the years of testing in the wilderness that make up the central section of the book (chapters 11–21).\" The English title Numbers is derived from the Greek of the Septuagint, referencing the numbering of the Israelites in the wilderness of Sinai and later on the plain of Moab.\n|Old Testament and Tanakh|\n|Jewish, Protestant, Roman Catholic, Eastern Orthodox, and Oriental Orthodox\n|Roman Catholic, Eastern Orthodox, and Oriental Orthodox|\n|Eastern Orthodox and Oriental Orthodox|\n|Russian and Oriental Orthodox\nNumbering God's people\nGod orders Moses, in the wilderness of Sinai, to take the number of those able to bear arms—of all the men \"from twenty years old and upward,\" the tribe of Levi being excepted, and to appoint princes over each tribe. The result of the numbering is that 603,550 Israelites are found to be fit for military service. Moses is ordered to assign to the Levites exclusively the service of the Tabernacle.\nGod prescribes the formation of the camp around the Tabernacle, each tribe being distinguished by its chosen banner. Judah, Issachar, and Zebulun encamp to the east of the Tabernacle; Reuben, Simeon, and Gad to the south; Ephraim, Manasseh, and Benjamin to the west; and Dan, Asher, and Naphtali to the north. The same order is to be preserved for the march.\nMoses is ordered to consecrate the Levites for the service of the Tabernacle in the place of the first-born sons, who hitherto had performed that service. The Levites are divided into three families, the Gershonites, the Kohathites, and the Merarites, each under a chief, and all headed by one prince, Eleazar, son of Aaron.\nThe Levites who are suited for the service of the Tabernacle—those from thirty to fifty years of age—were then numbered.\nPreparations are then made for resuming the march to the Promised Land. Various ordinances and laws are decreed.\nRecommencement of the journey\nMoses is ordered to make two silver trumpets for convoking the congregation and announcing the recommencement of a journey. The first journey of the Israelites after the Tabernacle had been constructed is commenced, and Moses requests Hobab to be their leader. The people murmur against God and are punished by fire; Moses complains of the stubbornness of the Israelites and is ordered to choose seventy elders to assist him in the government of the people\nThe spies are sent out into the lands and come back to report to Moses. The spies have to see how fertile the ground is, how fortified the cities are and how strong the people are. Joshua and Caleb, two of the spies, argue that the land is abundant and is \"flowing with milk and honey.\" The other spies say that it is inhabited by strong and evil men, which causes the Israelites to want to return to Egypt. The Lord talks to Moses and says he will kill all of the Israelites. Moses pleads with God, saying that others would think badly of God for leading his people to the wilderness and abandoning them there. God speaks to Aaron of having to wander in the wilderness for 40 years.\nMoses is ordered to make plates to cover the altar with the two hundred fifty censers left after the destruction of Korah's band. The children of Israel murmur against Moses and Aaron on account of the death of Korah's men and are stricken with the plague, with 14,700 perishing; Aaron's rod is used to quell the destruction.\nAaron and his family are declared by God to be responsible for any iniquity committed in connection with the sanctuary. The Levites are again appointed to help him in the keeping of the Tabernacle. The Levites are ordered to surrender to the priests a part of the tithes taken by them.\nPreparations for crossing the Jordan\nAfter Miriam's death at Kadesh Barnea, the Israelites blame Moses for the lack of water. Moses, ordered by God to speak to the rock, disobeys by striking it, and is punished by the announcement that he shall not enter Canaan. The King of Edom refuses permission to the Israelites to pass through his land. Aaron dies on Mount Hor.\nThe new census, taken just before the entry into the land of Canaan, gives the total number of males from twenty years and upward as 601,730, the number of the Levites from a month old and upward as 23,000. The land shall be divided by lot. The daughters of Zelophehad, their father having no sons, share in the allotment. Moses is ordered to appoint Joshua as his successor.\nPrescriptions for the observance of the feasts, and the offerings for different occasions are enumerated: every day; the Sabbath; the first day of the month; the seven days of the Feast of Unleavened Bread; the day of first-fruits; the day of the trumpets; the Day of Atonement; the seven days of the Feast of Tabernacles; the day of solemn assembly.\nThe conquest of Midian by the Israelites and the massacre of the Midian population is recounted. The Reubenites and the Gadites request Moses to assign them the land east of the Jordan. After their promise to go before the army to help in the conquest of the land west of the Jordan, Moses grants their request. The land east of the Jordan is divided among the tribes of Reuben, Gad, and the half-tribe of Manasseh.\nThe stations at which the Israelites halted during their forty years' wanderings in the wilderness are enumerated. While in the plains of Moab the Israelites are told that, after crossing the Jordan, they should expel the Canaanites and destroy their idols. The boundaries of the land of which the Israelites are about to take possession are spelled out. The land is to be divided among the tribes under the superintendence of Eleazar, Joshua, and twelve princes, one of each tribe.\nNumbers ends with a summary statement called a colophon, stating the place and circumstances of composition. Colophons were used in literature of the ancient Near East in the second millennium BC and earlier, and their usage was not understood until fairly modern times.\nJulius Wellhausen ascribed most of the composition of Numbers to the Priestly source, and therefore the 6th century BC, with additional material (including the Balaam story) from the Elohist document (c.850 BC) and the Yahwist (c.950 BC); Richard Elliott Friedman gives a similar division in his The Bible with Sources Revealed. Other rationalist scholars, following presuppositions that modify in some way or other the presuppositions of Wellhausen's documentary hypothesis, tend to see all the Pentateuchal books as made up of essentially undateable fragments or accretions, but agree with Wellhausen that the Torah reached its final form no earlier than the 5th century BC.\nBaalam and the Deir Alla inscription\nThe Deir Alla text is an inscription found in modern Jordan (i.e. outside the borders of ancient Israel) dated c. 850–675 BC. It tells a story of \"Balaam Son of Beor,\" a seer apparently famed in the region at this time, and whose prophecies regarding Israel are found in Numbers 22 through 24. The inscription demonstrates that Balaam the Seer was a a figure known in the region in the 1st millenium BC.\n|Books of the Torah|\n|4. Book of Numbers|\n- Book of the Wars of the Lord\n- Inverted nun (only appears twice in the Book of Numbers and seven times in the Book of Psalms)\n- Priestly Blessing\n- Weekly Torah portions in Numbers: Bamidbar, Naso, Behaalotecha, Shlach, Korach, Chukat, Balak, Pinchas, Matot, and Masei\n- What hath God wrought\n- Wilderness of Sin\n- Numbers 6:24-26 (NJB)\n- Priests, in Illustrated Dictionary & Concordance of the Bible, 1986. Wigoder G, Paul S, Viviano B, Stern E, eds., G.G. Jerusalem Publishing House Ltd. And Reader’s Digest Association, Inc. ISBN 0895774070\n- Blessing The Children article at judaism.about.com\n- Harris, Stephen L., Understanding the Bible. Palo Alto: Mayfield. 1985.\n- Gregory Goswell, \"What's in a Name? Book Titles in the Torah and Former Prophets,\" Pacifica 20 (2007), 268.\n- Hermann Hunger, Babylonische und Assyrische Kolophone (Neukirchen: Kevelaer, 1968).\n- Richard Elliott Friedman, The Bible with Sources Revealed. New York: Harpercollins (2005). ISBN 006073065X, 9780060730659\n- Colin Humphreys, The Miracles of Exodus: A Scientist's Discovery of the Extraordinary Natural Causes of the Biblical Stories. New York: HarperCollins (2004): 109. \"Many scholars have come to believe that there are four underlying sources of the Pentateuch. These sources are called J, E, D, and P, and they are usually dated from the tenth to the fifth centuries B.C.\"\n- Book of Numbers article (Jewish Encyclopedia)\n|This page uses content from the English Wikipedia. The original article was at Book of Numbers. The list of authors can be seen in the page history.|\nOnline versions and translations\n- Original language:\n- Jewish translations:\n- Christian translations:\n|Hebrew Bible||Followed by|\n|Christian Old Testament|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:ce4fbeab-f1c4-46d7-ad5d-1b7b5fd39834>","<urn:uuid:125e67eb-17c9-475c-be57-8bf3d3ff0253>"],"error":null}
{"question":"How does the total body water content compare between elderly individuals and young adults, and what makes this difference significant for health?","answer":"Elderly individuals have significantly less body water content compared to younger adults - about 7 liters less for a 150-pound elderly person versus a young person of the same weight. This difference makes elderly people more susceptible to dehydration since they can become dehydrated more quickly. The situation is particularly concerning given that elderly persons may have difficulty getting fluids due to physical impairments and often have a reduced perception of thirst. Prompt recognition of dehydration is critical since it can lead to serious complications like decreased functional ability, falls due to low blood pressure, constipation, infection risk, and even death.","context":["|The Importance of Water\nto oxygen, water is the nutrient most needed for life. A person\ncan live without food for a month, but most people can survive\nonly three or four days without water. All of us start life\nin a watery cradle of amniotic fluid and to this day remain\nwater's creature. Water makes up 85% of the blood, 70% of\nthe muscles and about 75% of the brain, and is present in\nand around each of our cells.\nIn the body, water acts as a solvent, coolant, lubricant,\nand transport agent. It's needed to regulate body temperature,\ncarry nutrients, remove toxins and waste materials, and provide\nthe medium in which all cellular chemical reactions take place.\nFluid balance is vital for body functions and adequate blood\nvolume. A significant decrease in the total amount of body\nfluids leads to dehydration. Dehydration occurs when the amount\nof water you take in is less than the amount you are losing.\nFluids can be lost through the urine, skin, or lungs. Along\nwith fluids, essential electrolytes, such as sodium and potassium,\nare also perilously depleted in a dehydrated individual.\nDehydration and the Elderly\nElderly individuals are at a heightened risk for dehydration\nbecause their bodies have a lower water content than younger\npeople. An elderly person weighing 150 pounds has about seven\nliters less water than a young person of the same weight.\nAs a result of this lower water content, elderly individuals\ncan become dehydrated more quickly.\nBecause of visual, cognitive, or motor impairments, elderly\npersons may have difficulty getting fluids for themselves.\nThey also may have only a muted perception of thirst. Unfortunately,\nmany symptoms of dehydration do not appear until significant\nfluid has been lost. Practitioners in the field know that\ndehydration can happen very rapidly, in less than an eight-hour\nshift. The consequences of dehydration among the elderly can\nbe life threatening and the symptoms are alarmingly swift.\nNursing facility residents are particularly at risk for dehydration.\nAccording to the article \"Preventing Dehydration in the\nElderly\" published in the September 1996 edition of Provider\nmagazine, as many as 75 percent of residents have average\nfluid intakes below 1,500 cc per day, while a minimum recommendation\nis between 1,500 to 2,000 cc of fluid intake per day. These\nresidents may need more opportunities and reminders to drink.\nThose with disabilities may be unable to get a glass of fluid\nwithout assistance. However, disability is not the only risk\nfactor. Residents who are not dependent on staff also need\nencouragement for sufficient fluid intake.\nEven when offered water, residents will refuse the offer\nbecause \"I'm not thirsty\" or \"I've already\nhad too much to drink today.\" Many elderly individuals\nalso exacerbate their risk of dehydration by limiting their\nfluid intake in the incorrect belief that they will prevent\nincontinence and decrease the number of needed trips to the\nbathroom. Residents and staff should be taught that decreasing\nfluid intake does not decrease incontinence, nor does it decrease\ntrips to the bathroom. In fact, as the urine becomes more\nconcentrated, it irritates the bladder and may increase the\nurge to void resulting in frequent small voidings.\nSigns and Symptoms of Dehydration\nIt is important that staff members be able to recognize the\nsigns and symptoms of dehydration to promptly assure correction\nand prevention of complications. Without timely correction,\ndehydration can lead to decreased functional ability, predisposition\nto falls due to orthostatic hypotension, constipation, predisposition\nto infection, and death. According the chapter on \"Dehydration:\nPrevention and Recognition\" published by Long Term\nCare Educator, staff members should be alert to the following\n• Confusion can be an early sign of dehydration.\nCaregivers who work regularly with a resident may be best\nable to detect subtle changes in mental status that are present\nwhen dehydration develops. Dehydration should be one of the\nfirst problems suspected when a resident has a sudden change\nin mental status.\n• Poor skin turgor (elasticity) accompanies\ndehydration, but this can be a difficult sign to assess because\nmost older residents have an age-related reduction in skin\nturgor under normal circumstances. If skin turgor is to be\nused in assessing for dehydration, the best areas to test\nare the skin over the sternum and forehead because these areas\nmaintain better skin turgor than other areas. Generally, skin\nwill feel warm and moist with dehydration.\n• An inspection of the oral cavity can yield useful insight\ninto a resident's hydration status. A dry oral mucosa\nand dry furrowed tongue are good indicators of dehydration.\nStaff should be cautioned that residents who are taking certain\nmedications (such as anticholinergic drugs) may have dry oral\nmucosas in the absence of dehydration.\n• An assessment of vital signs will reveal a decrease\nin blood pressure and increase in pulse\nwhen dehydration is present.\n• A review of bowel elimination patterns can assist in assessing\ndehydration. Recent diarrhea can offer an explanation for\na dehydrated state, while constipation is a\ncommon occurrence when dehydration exists. Likewise, bladder\nelimination patterns can provide useful insights: excess voiding\ncan contribute to dehydration while scanty output\ncan indicate insufficient fluid intake. Concentrated\nurine is a common sign.\n• Because such a large percentage of the body is comprised\nof fluid, weight loss is often present.\nIf any of the above symptoms are noted, steps should be taken\nas soon as possible to further assess the resident for dehydration\nand take corrective action as necessary.\nTaking a Proactive Approach to Preventing Dehydration\nThe best defense against dehydration is prevention. Illinois\nCouncil facilities are utilizing a variety of proactive\nstrategies to identify residents at risk for dehydration,\nmonitor their fluid balance, and establish institutional policies\nand programs to ensure that all residents receive adequate\nhydration. The following are some of recommendations that\nhave been suggested by Illinois Council homes:\n• Routinely monitor residents for signs of dehydration such\nas cracked lips, dry oral mucous, poor skin turgor, and dark\nurine color. Observe the residents' consumption of fluids\nto determine if they have reduced the amount of liquids they\nusually drink. Pertinent observations should be recorded in\nthe nursing notes.\n• Keep a list of residents at high risk of dehydration in\nthe nurses' station and other strategic locations to remind\nothers to monitor residents' fluid intake. Consider placing\na symbol, such as a drop of water, near the beds of those\nresidents at risk of dehydration as a sign for staff members\nto encourage fluid intake.\n• Establish hydration protocols to be instituted immediately\nwhen acute symptoms or illness threaten fluid and electrolyte\n• Schedule fluid administration at least three times a day\nbetween meals. Older people tolerate frequent administration\nof fluid in smaller quantities better than infrequent larger\n• To maintain hydration, note the residents' preferences\nfor type and temperature of fluids, and individualize the\nhydration plan to encourage compliance.\n• Review residents' medications to assess possible impact\nof fluid and electrolyte levels.\n• Leave filled, fresh water pitchers at residents' bedside\nand assure that residents are able to easily reach\npitchers and glasses. Supply straws and special\ndrinking glasses to assist residents as necessary.\n• Staff should offer a variety of fluids at a variety of\ntimes and tell residents what the purpose of this procedure\nis. For example, the dietary staff could add a glass of water\nto each meal. Activities such as discussion or music groups\nshould include the offering of beverages.\n• Arrange for residents to eat meals and have snacks with\nother residents. Residents typically consume more food and\nfluids in a social setting.\n• Offer at least a full glass of fluid with medications.\nStudies have shown that residents tend to drink the entire\namount of fluid offered.\n• CNAs can offer small amounts of fluid every time they interact\nwith a resident during care delivery; i.e., toileting, after\na transfer, after getting dressed, after rehab or therapy\nor range of motion exercises.\n• Instruct residents, staff, and families about the importance\nof hydration. Involve the resident and family in establishing\nand meeting hydration goals.\n• Teach staff to use a direct, positive approach when administering\nfluids. Avoid asking: \"Do you want something to drink?\"\nInstead say: \"Here is some cool, refreshing water for\nyou Mrs. Jones.\" Older people may not feel thirsty and\nmay not recognize their need for fluid.\n• Consider giving residents water bottles (such as those\nused by athletes) to carry with them around the facility.\nOn a regular basis, be sure to fill the residents' water bottles\nwith their favorite cool beverages.\n• Offer residents a variety of delicious beverages on a beverage\ncart. Have a staff member provide drinks from this cart to\nresidents on a daily basis.\n• Sponsor a cocktail hour for residents to enjoy their favorite\nbeverages and socialize. Facilities might consider having\na bar with different bottles and attractive colors where residents\ncan come up and order their drinks.\n• Make drinks more appealing by the use of \"props\"\nthat are colorful and inexpensive, such as lemonade pitchers\nand glasses, cocktail decorations, or a fruit garnish.\n• During activities, use a blender to mix juice combinations\nfor the residents to enjoy such as kiwi and strawberry; vanilla\nand root beer soda; ginger ale in cranberry juice; and orange\nand pineapple juice.\n• Ask the residents for their ideas on delicious drink ideas\nthat can be created during activity programs. Consider the\ncreation of frozen liquids such as lemon ice, popsicles, and\ngelatin desserts. Try developing a \"juice bar\" with\na variety of juices, Italian ices, and snow cones.\n• For residents with disabilities, the occupational therapist\nmay be able to recommend self-help devices such as long flexible\nstraws and spouted or two-handled cups. For residents with\nincreased stiffness, commonly seen in the later stages of\nAlzheimer's disease, a cup with a cut-out on the upper rim\nfor the nose allows the cup to be tilted enough for the resident\n• Ask surveyors if they will offer something to drink to\nresidents they interview during the survey, as part of the\noverall facility efforts to prevent dehydration.\n• When offering fluids, avoid offering beverages that contain\ncaffeine or alcohol since both have dehydrating properties.\n• Educate families about the importance of promoting hydration\nwhile they visit in and/or out of the facility.\n• Take ice water and other beverages on all outings and offer\nat frequent intervals.\n• Before walking through the zoo, museum, park, etc. provide\neach resident with a personal water bottle. These are easy\nto carry and refill and should be marked with the resident's\nname before distribution.\n• If residents refuse fluids on an outing, give them a salty\nsnack. This will increase their tendency to accept a drink.\n• Make sure that water fountains in the facility are low\nenough to facilitate use by residents in wheel chairs. Include\na cup dispenser and wastebasket next to the drinking fountain.\nFor easier use, adaptive \"handles\" can be purchsed\nto make dispensing easier.\nActivity Ideas that Encourage Hydration\nThe activity professional is in a unique position to encourage\nresidents to increase their fluid intake. The social structure\nof interactions used by these professionals offer many opportunities\nto offer \"liquid refreshments\" as a natural part\nof the activity. The following are some simple ideas to add\nto any current activity program:\n• Decorate a cart and stock it with a variety of juices and\nwater when making room visits. Begin the visit by sharing\na beverage of choice. In many instances this will trigger\nfond memories of going to the soda fountain for a cherry Coke\nor sipping lemonade at a church picnic. This same idea can\nbe utilized when staff or volunteers take the \"library\ncart\" from room to room.\n• Incorporate a \"juice break\" into small group\nactivities held on each floor or unit. The residents appreciate\nthis time to rest and socialize with those around them.\n• Use specific beverages as part of a reminiscing group.\nFor example, have the group squeeze lemons and make lemonade.\nYou'll be surprised at the many stories connected with this\n• Have a \"taste-test\" and have the residents guess\nthe flavor of the juice, soft drink, tea, coffee, shake, etc.\nthey are drinking.\n• Hold an afternoon \"tea\" where small groups can\ngather over a cup of tea or other beverages and just visit\nor discuss a specific topic.\n• As the residents wait for the dining room to open, use\nthe opportunity to provide a liquid appetizer such as tomato\njuice, orange juice, V-8 juice, etc.\n• Have a Happy Hour before the evening meal and offer non-alcoholic\ndrinks. Almost any mixed drink can be made without alcohol,\ni.e., a virgin Bloody Mary.\n• Always have water and/or a variety of other beverages available\nwhen on outings or when sitting outside with the residents.\n• Using \"real\" cups and glasses makes beverages\nmore appealing; when living in the community, this is what\nthe residents used.\n• During cooking group, have glasses of water with floating\nslices of lemon and ice cubes at each resident's place when\nthe activity begins. Keep refilling their glasses as the meal\nis prepared and eaten.\nDehydration in nursing home residents is a common and dangerous\nproblem requiring the involvement of the entire interdisciplinary\nteam. Every opportunity to encourage residents to drink should\nbe used, not only by nursing staff, but dietary, activities,\nsocial services, volunteers, and family. By taking a proactive\napproach to preventing dehydration, staff members can make\ngreat progress in reducing unnecessary hospitalizations and\nmaximizing resident health and well-being.","Water accounts for about one half to two thirds of an average person’s weight. Fat tissue has a lower percentage of water than lean tissue and women tend to have more fat, so the percentage of body weight that is water in the average woman is lower (52 to 55%) than it is in the average man (60%). The percentage of body weight that is water is also lower in older people and in obese people. The percentage of body weight that is water is higher (70%) at birth and in early childhood. A 154-pound (70-kilogram) man has a little over 10.5 gallons (42 liters) of water in his body: 7 gallons (28 liters) inside the cells, 2.5 gallons (about 10.5 liters) in the space around the cells, and slightly less than 1 gallon (3.5 liters, or about 8% of the total amount of water) in the blood.\nWater intake must balance water loss. To maintain water balance—and to protect against dehydration Dehydration Dehydration is a deficiency of water in the body. Vomiting, diarrhea, excessive sweating, burns, kidney failure, and use of diuretics may cause dehydration. People feel thirsty, and as dehydration... read more , the development of kidney stones Stones in the Urinary Tract Stones (calculi) are hard masses that form in the urinary tract and may cause pain, bleeding, or an infection or block of the flow of urine. Tiny stones may cause no symptoms, but larger stones... read more , and other medical problems—healthy adults should drink at least 1½ to 2 quarts (about 2 liters) of fluids a day. Drinking too much is usually better than drinking too little, because excreting excess water is much easier for the body than conserving water. However, when the kidneys are functioning normally, the body can handle wide variations in fluid intake.\nDid You Know...\nThe body obtains water primarily by absorbing it from the digestive tract. Additionally, a small amount of water is produced when the body processes (metabolizes) certain nutrients.\nThe body loses water primarily by excreting it in urine from the kidneys. Depending on the body's needs, the kidneys may excrete less than a pint or up to several gallons (about half a liter to over 10 liters) of urine a day. About 1½ pints (a little less than a liter) of water are lost daily when water evaporates from the skin and is breathed out by the lungs. Profuse sweating—which may be caused by vigorous exercise, hot weather, or a high body temperature—can dramatically increase the amount of water lost through evaporation. Normally, little water is lost from the digestive tract. However, prolonged vomiting or severe diarrhea can result in the loss of a gallon or more a day.\nUsually, people can drink enough fluids to compensate for excess water loss. However, people who have severe vomiting Nausea and Vomiting in Adults Nausea is an unpleasant feeling of needing to vomit. People also may feel dizziness, vague discomfort in the abdomen, and an unwillingness to eat. Vomiting is a forceful contraction of the stomach... read more or diarrhea Diarrhea in Adults Diarrhea is an increase in the volume, wateriness, or frequency of bowel movements. (See also Diarrhea in Children.) The frequency of bowel movements alone is not the defining feature of diarrhea... read more may feel too ill to drink enough fluids to compensate for water loss, and dehydration Dehydration Dehydration is a deficiency of water in the body. Vomiting, diarrhea, excessive sweating, burns, kidney failure, and use of diuretics may cause dehydration. People feel thirsty, and as dehydration... read more may result. Also, confusion, restricted mobility, or impaired consciousness can prevent people from sensing thirst or being able to drink enough fluids.\nMineral salts (electrolytes), such as sodium and potassium, are dissolved in the water in the body. Water balance and electrolyte balance Electrolyte Balance read more are closely linked. The body works to keep the total amount of water and the levels of electrolytes in the blood constant. For example, when the sodium level becomes too high, thirst develops, leading to an increased intake of fluids. In addition, vasopressin (also called antidiuretic hormone), a hormone secreted by the pituitary gland (a pea-sized gland at the base of the brain) in response to dehydration, causes the kidneys to excrete less water. The combined effect is an increased amount of water in the blood. As a result, sodium is diluted and the balance of sodium and water is restored. When the sodium level becomes too low, the kidneys excrete more water, which decreases the amount of water in the blood, again restoring the balance.\nMaintaining water balance\nIn the body, several mechanisms work together to maintain water balance. These include\nInteraction of the pituitary gland and kidneys\nThirst is one of the most important mechanisms to maintain water balance. When the body needs water, nerve centers deep within the brain are stimulated, resulting in the sensation of thirst. The sensation becomes stronger as the body’s need for water increases, motivating a person to drink the needed fluids. When the body has excess water, thirst is suppressed.\nAn interaction between the pituitary gland and the kidneys provides another mechanism. When the body is low in water, the pituitary gland secretes vasopressin (also called antidiuretic hormone) into the bloodstream. Vasopressin stimulates the kidneys to conserve water and excrete less urine. When the body has excess water, the pituitary gland secretes little vasopressin, enabling the kidneys to excrete excess water in the urine.\nIn osmosis, water flows passively from one area or compartment of the body to another. This passive flow allows the larger volumes of fluid in the cells and the area around the cells to act as reservoirs to protect the more critical but smaller volume of fluid in the blood vessels from dehydration Dehydration Dehydration is a deficiency of water in the body. Vomiting, diarrhea, excessive sweating, burns, kidney failure, and use of diuretics may cause dehydration. People feel thirsty, and as dehydration... read more ."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:74ca7df2-5361-43d5-8544-881eb7f66f52>","<urn:uuid:fc6c99dd-72bd-4c89-b240-714728f9ef66>"],"error":null}
{"question":"I am a tile installer and need to know the exact process for replacing a damaged tile. What are the steps I should follow?","answer":"To replace a damaged tile, you must first remove the grout surrounding the broken tile, then remove the broken tile itself, and finally remove the adhesive beneath it. Once the area is prepared, you can set a new tile and re-grout it. There are specific tools designed for this work, though some people may prefer to have a professional do the job. It's recommended to save a few tiles from the original installation and note the brand and color designation for the grout for such repairs.","context":["The grout surrounding the broken tile, the tile, and the adhesive beneath the broken tile all must be remove. A new tile is set, and re-grouted. There are tools designed to enable this work. You may wish to have a professional do the job. Because “these things happen”, it is a great idea to save a few tiles from the original installation, and to note the brand and color designation for the grout.\nThere are tile products for practically every surface in your home – floors, walls, counter tops, indoors and outdoors. But, every tile is not right for every setting. Tile is rated based on several factors which tells you whether it is good for floors, or not, is likely to be slippery when wet, or not, etc. Your Hadinger Flooring associate will help you focus your selection on the right source.\nPorcelain is actually a type of ceramic tile. It is made with the mineral feldspar added to the mix, and fired at a higher temperature. The result is a harder tile, with almost zero water absorbency. Porcelain tiles hold up to the heaviest foot traffic, and can be installed outdoors as well as in.\nThere are just a few things to keep in mind. Nothing matches the natural stone look, exactly, but some manufactured flooring comes pretty close. Don’t use natural stone, including marble, for shower floors or surrounds, or any place where they are likely to be subject to prolonged contact with water. This is because natural stone is too porous, and the water will eventually cause staining and problems with the grout. Natural stone can be sealed, to improve stain resistance. Some styles of stone tiles have a highly polished surface and can be slippery. Expect stone tiles to cost more than most ceramic tiles. Watch the potential for sharp edges and corners for you installation.\nAbsolutely. Pay attention to the traffic rating, sometimes called the PEI rating, for each type, to ensure it will perform properly in your space, and pay attention to the thickness of the tiles, to ensure they will produce a consistent surface height. Otherwise, measure your floor space, lay it out on a piece of graph paper, and get creative!\nLet me reassure you that all your dishes will bounce and be fine, except the ones you really care about. Tile is the classic choice for kitchens…beautiful, easy to clean, super durability, stain resistant. Your best bet for avoiding breakage is to include a nice thick area rug (on a slip resistant pad) in the dish-handling area. You’ll love the comfort, too. Don’t overlook the luxury vinyl tile option. It is definitely softer than ceramic, and you might find yourself really appreciating the beautiful styles.\nGrout requires regular cleaning – mopping with a mild soap-and-water solution, then rinsing with clear water. Regular cleaning helps prevent dirt from being ground into the grout. You will find products on the market for sealing grout lines, making them more impervious to water and staining, but there are different points of view about when and where that is a good practice, so consult with your Hadinger Flooring associate about the specifics of your situation.\nInstalling floor tile requires specific skills and tools. Most installations require tiles to be cut to fit, which requires a tile saw. That being said, everybody who has tile-setting skills had to learn at some point. The best advice might be to watch some on-line videos to get a sense of what is involved, ask among your friends to identify a skilled and will assistant, and be sure your Hadinger Flooring associate knows that you are planning to tackle the job as a relative novice. With that information, the associate can make certain that you’ve got everything your installation will require. Good luck!"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:289aa8fb-b135-4002-b8c2-986bdabc8cd1>"],"error":null}
{"question":"What are some basic ways to make a website accessible for users with disabilities, such as those who use screen readers?","answer":"There are three main ways to make a website more accessible: 1) Add alt text and image descriptions to banners and images so screen readers can describe them to vision-impaired users, 2) Implement 'skip links' to allow keyboard-only navigation to jump past menus directly to content, and 3) Design with accessibility principles like proper heading structure, good color contrast, and descriptive link text.","context":["As the number of internet users keeps rising, web accessibility is becoming a more pressing concern. Due to the way some websites are designed, they can be difficult for people with special requirements to access, such as those who use screen readers. Not considering accessibility can lose you a lot of traffic.\nFor this reason, it’s vitally important that you design your affiliate program with accessibility in mind. This ensures that as many people as possible can use it without problems. Not only will this improve your affiliates’ user experience, it can even help to make your program more successful.\nIn this article, we’ll discuss web accessibility and why it’s important. We’ll also show you how to take your first steps towards making your program more accessible for everyone. Let’s get started!\nA Brief Introduction to Web Accessibility\nWeb accessibility is the practice of designing websites that can be used by anyone, regardless of whether they have a disability or other special needs. This is an important topic, especially as the number of internet users increases.\nIf you design any type of site and don’t make it accessible, you’re effectively shutting out a lot of your potential audience. This can be especially harmful for an affiliate program, as you’re limiting the number of potential affiliates who can sign up. As such, it’s important to consider how you can improve the accessibility of your affiliate website.\n3 Ways You Can Make Your Affiliate Program More Accessible\nNow, let’s discuss some of the ways you can improve accessibility for your affiliate program. Bear in mind that this is by no means a comprehensive list, so we recommend that you refer to sources like WebAIM and W3C for more information on designing with web accessibility in mind. However, the following three techniques are a perfect way to get started right now.\n1. Add Image Descriptions to Your Banners\nIncluding alt text for your site’s images is a crucial aspect of web accessibility. Alt text displays when the image itself cannot be loaded for whatever reason. However, it’s also used by screen readers to ‘describe’ the image to vision-impaired users.\nAs such, you’ll want to take care to add alt text to all the images on your site. While you aren’t able to do this for the banner ads you upload to your affiliate program, you can still achieve the same result by adding an image description. You can do this when creating a banner in Affiliate Royale > Links & Banners:\nBy adding a detailed description to the Info field, you can describe the image so users with assistive technology will understand what it contains. This can even be helpful for other affiliates, such as those who aren’t fluent in English and are using automatic translation software.\n2. Implement ‘Skip Links’\nOne of the most important aspects of creating an accessible site is making sure it’s keyboard-friendly. This means that a user should be able to easily navigate your site using only a keyboard (since some people have difficulty using a mouse). In practice, most sites achive this by enabling users to navigate using the tab key to jump between objects on the page.\nOne issue that can result from this setup is that users are required to tab through all your navigational elements just to get to a page’s content. To avoid this hassle, you can implement ‘skip links’, which let people jump ahead to the actual content. This is especially important for your affiliate dashboard, as affiliates may otherwise be stuck in your menus when they just want to access their personal information and resources.\nThere are a number of ways you can add skip links, but one of the easiest is to use a plugin. WP Accessibility is an excellent option, and makes it easy to add skip links to your site’s theme (among other features).\n3. Design Your Program’s Site With Accessibility in Mind\nFinally, in order to create a truly accessible affiliate program, you’ll need to keep the main elements of web accessibility in mind throughout the entire website design process. Let’s quickly run through some of the most important things you’ll want to consider:\n- Use headings correctly. Make sure that you implement headings and use them correctly, as they help assistive software to better understand the layout and content of your site.\n- Consider color contrast. If you use colors that are too similar or clash too much, your site could become difficult to read. This can cause problems for users with colorblindness or photosensitive epilepsy.\n- Use descriptive links. Your links’ anchor text should always clearly describe their purpose. You can do this both when designing your site and your affiliate dashboard, but also when adding descriptions to your affiliate links.\nWhile this can seem like a lot of work, it will help you avoid creating a site that’s difficult for a considerable number of potential affiliates to use. Building an accessible site will benefit both your affiliates and your program in the long run. Plus, most of these techniques can be used to improve an existing site as well as a brand-new one.\nWeb accessibility is becoming a more important consideration by the day, and it’s important to keep it in mind when designing the interface for your affiliate program. Fortunately, WordPress and Affiliate Royale can help you make your site more accessible, which makes the experience of using your program more enjoyable and increases its long-term potential for success.\nIn this article, we’ve shown you a few ways you can improve your affiliate program’s accessibility. These include:\n- Add image descriptions to your banners.\n- Implement skip links.\n- Design your program’s site with accessibility in mind.\nDo you have any questions about how to make your affiliate program as accessible as possible? Let us know in the comments section below!"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:4c095307-e893-4d71-8413-66317d8ea493>"],"error":null}
{"question":"Does Geoff Lawton share the same teaching focus as Garfield Community Farm?","answer":"While both are involved in permaculture education, their teaching focuses differ. Geoff Lawton, as director of the Permaculture Research Institute, emphasizes professional certification through the Permaculture Design Certificate course and focuses on large-scale solutions for degraded or arid landscapes worldwide. In contrast, Garfield Community Farm concentrates on local community education, teaching organic gardening specifically in neglected urban areas, with a primary focus on serving their immediate neighborhood through hands-on learning and community engagement.","context":["There is probably no better way to explain the ethics of permaculture than to discuss a place that exemplifies these ethics. One such place is Pittsburgh’s Garfield Community Farm. Garfield Farm has been designed and managed as a permaculture project since its inception. I have been privileged watch the farm develop and to work there part time each summer over the past several years.\nFirst, about the three permaculture ethics: Care of the earth, care of the people and fairly sharing the excess are at the core of permaculture design. Permaculture co-founder David Holmgren states that these ethics are intended to help guide our transition to a sustainable, regenerative future. ( https://permacultureprinciples.com/ethics/ ) Care of the earth requires us to keep in mind that our design work should strive to heal and promote the health of our local and global environment. We only have one planet. Care of the people helps us to keep in mind that we must design systems that provide for our human needs, food, shelter, energy, access to nature and community in a manner that promotes health and wellness. Sharing the excess may sound political or religious. Perhaps it is, but this ethic is based on the knowledge that accumulation of excess material things by one leads to want in another, and often with it, a degraded environment. By fairly sharing resources, when we are able, we fulfill the first two ethics while building strong communities.\nGarfield Community Farm (http://www.garfieldfarm.com/ ) is a project of Open Door Ministries, a Presbyterian Church in the Garfield neighborhood of Pittsburgh. This urban farm was founded, and is directed by Assistant Pastor John Creasy, and the Board of Directors of the Open Door Church. Day-to-day operations is run by a small core of staff, with much assistance from volunteers from the church and the community. The farm’s stated mission is: We seek to learn, teach, and practice organic gardening and farming in the places that have been neglected and abandoned, in and with the neighborhood of Garfield.\nThis mission is achieved on a 2.5 acre site which was formerly abandoned houses. The farm features a passive solar bioshelter, with a small flock of resident chickens and a couple of pet rabbits; garden beds, poly-tunnel, food forest orchards, perennial landscapes, and community gathering areas.\nCare of the earth is integral to the mission here. Gardening methods and energy systems, such as solar electric panels and passive solar bioshelter, are chosen to be earth friendly. While working to regenerate the farm’s soils with compost and organic mulches, the farm is also working to enhance biodiversity. Habitat for beneficial creatures is maintained between the gardens. A resident Cooper’s hawk and owls patrol day and night for rodents. A great diversity of songbirds share the mulberries and elderberries and help control insect pests and weed seeds. Goldfinch, grey catbird, Baltimore oriole, scarlet tanager, bluebirds, nuthatch and downy woodpeckers are just a few of the species present. Last year, a five-lined skink, a native lizard long thought to be extirpated in Allegheny County, Pennsylvania, was found living in the compost piles at the farm.\nCare of People is indeed the core mission of the farm. Garfield Farm provides access to fresh seasonal produce to an under served neighborhood. Each week during the growing season a produce stand built on a small trailer, known as the mobile market, is set up in several locations to sell farm products. People care is further achieved by educational activities, community engagement, and by giving people an opportunity for a direct connection to nature. The farm hosts several annual events to build and celebrate community, engaging neighbors, volunteers and church members with food, music and conversation.\nThe third ethic, to Share the Excess, is also integral to the farm’s design and activities, and extends from the ethic to care for people. The property spans three city blocks. Farm spaces are open to neighbors and visitors to relax in the pavilion, walk the labyrinth, enjoy nature and sample fruits from trees and bushes. Farm produce sold at the mobile market has a two tier pricing system. Customers have the option to pay the lower, “what you can afford” price, or the standard, “what you can pay” price for higher income customers. Often produce is purchased from a local farmer’s cooperative and is sold at cost to increase the food options offered. The farm’s beekeeper donates most of her honey harvest to the farm for sale as well. Any excess produce, when there is excess after the mobile market sales, is given to a local food bank. The farm also provides the opportunity for volunteers and church members to share their time in good work.\nThis discourse only begins to describe the manifestation of these three ethics at Garfield Community Farm. In a world of changing climates, growing urban populations and diminishing resources, projects like Garfield Community Farm are leading the way to a regenerative future of livable cities, where we all care for the earth, care for the people and share the excess.","called the 'King' or 'Father' of Permaculture\nBill Mollison, who recently died (24 september 2016), is considered to be the ‘father’ of permaculture. He originally coined the word Permaculture by combining the words Permanent and Agriculture, in later years he defined it as ‘Permanent Culture’.\nWith David Holmgren he developed the concept in the mid 70’s which resulted in the book Permaculture One. Published in 1978 it was the first big compiled presentation of the permaculture concepts and the book presented permaculture as an integrated design concept. In 1979 he published Permaculture Two which focusing more on design. His practise as a Permaculture teacher advanced and resulted in Mollison’s book Permaculture – A Designers’ Manual, published in 1988 by his own publishing company Tagari Publishing. This book expands the knowledge into a complete curriculum for designing human habitats in a way that’s ecologically sound and economically viable, through the means of Permaculture. The book is considered the ‘Bible’ for studying Permaculture and is the basic compendium for the Permaculture Design Certificate course (PDC).\nHe had a pragmatic -we've got problems so let's fix it- approach.\nMollisons work is inspired by:\nJoseph Russell Smith (writer of Tree Crops, in which the concept of Permanent Agriculture was introduced)\nP.A. Yeomans (Keyline design)\nMasanobu Fukuoka (Natural farming)\nLinks:Tagari Publishers >>\ncalled 'The Crown Prince' of Permaculture\nGeoff Lawton was a student of founder Bill Mollison. They taught many PDC’s together and Geoff was a close friend to Bill.\nGeoff Lawton is the current director of the Permaculture Research Institute in Australia. Located at Zaytuna Farm in The Channon, the PRI Australia is the intellectual headquarter of permaculture worldwide. It's mission is to ‘create world wide financially and operationally self sufficient permaculture demonstration and education sites’. He emphasizes community building and promotes a future in which mankind is 'living in permaculture abundance’.\nThe PRI Australia website (PRI >>) is the main online portal for permaculture worldwide. He additionally created the Permaculture Global website >>, an online community for practitioners.\nAs a renowned practitioner, teacher and promoter Lawton is seen as the modern front man of what permaculture is about and where it should go. He is the caretaker of the famous Permaculture Design Certificate course, THE standard course for becoming a certified permaculture designer by profession.\nGeoff Lawton initiates and guides many other courses on topics such as earthworks. Since 2013 he runs a yearly online PDC which serves thousands of students world wide.\nAs a teacher-speaker-designer he travels around the world promoting and realizing permaculture solutions, mainly in degraded or arid landscapes, such as the Wadi Rum desert in Jordan. Together with his friend John D. Liu he gained more attention in the mainstream media with their ‘Greening the Desert’ films and philosophy.\nLinks:Geoff Lawton >>\nPermaculture Research Institute >>\ncofounder and philosopher\nDavid Holmgren is the co-originator and co-developer of the permaculture concept. He worked with Mollison in the early days and together they wrote Permaculture One published in 1978. After this period he went into practise for many years. In later times he emerged as a profound permaculture thinker which resulted in the 2002 published book Permaculture Principles and Pathways beyond Sustainability. In this book he compiles his vision and 25 years experience into the formulation of the now famous ’12 principles of permaculture’, each of which he explaines in a chapter.\nHe is an public active designer, speaker and writer. Peakoil, transition and climate change are some of his major topics of interest and he proves to be a visionairy philosopher of great influence in the permaculture world and beyond.\nLinks:Holmgren Design Services >>\nPermaculture Principles >>\ncalled the Mighty, the Glorious, the Amazing...\nSepp Holzer is often considered as the greatest practitionar of Permaculture. Allthough never educated in the field, he was asked by Mollison to call his self taught method of farming (\"Wildkultur\" or \"wild culture\") Permaculture because of the amazing results he accomplished on his farm Krameterhof in Austria. He retired from his farm, started another small homestead 'Holzerhof' and travels around the world creating water retention landscapes and teaching Permaculture and Agro-ecology.\nRecently he does many projects in Russia and eastern Europe.\nLinks:Sepp Holzer in the USA >>\nHolzers Permaculture, Austria >>\nKrameterhof Josef Holzer, Austria >>\ncalled the 'Duke' of Permaculture\nPaul Wheaton is the creator of the largest online permaculture commnity: Permies.com >>. He is often called 'The Bad Boy of Permaculture\" because of his style of presentation. His mission is to \"plant the word Permaculture in every brain\" within his \"Devious plot for World Domination\". He is a permaculture visionairy and an advocate of a very conscientious style of Permaculture, combined with a very commercial view on farming. He is a promoter of Hugelkultur (say hoogle-cool-tour), the Rocket Mass Heater and housing experiments such as his earth sheltered 'Wofati' concept. Recently he obtained 206 acres of woodland near Missoula, now called 'Wheaton's Lab' where he experiments with new concepts.\nLinks:Rich Soil >>\nPredecessors of Permaculture\nJoseph Russell SmithJoseph Russell Smith was a professor of geography at Columbia University and writer of Tree Crops, A Permanent Agriculture in which he describes the value of tree crops for producing food and animal feed on sloping, marginal, and rocky soils as a sustainable alternative to annual crop agriculture.\nMasanobu FukuokaFounder of 'Natural Farming', a natural and philosophical style of farming which attempts to farm in complete accordance with the forces of nature. So no tilling, no prefabricated fertilizers, no biocides or even biological pest control. Some of his techniques are combined rice and barley cultivation and seeding with seed containing clay balls.\nAllthough not a permaculturist himself, his philosophy is very aligned with permculture.\nP.A. YeomansDeveloper of the Keyline design system."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:3d3ee329-4966-471b-abd6-a5a41e91d95e>","<urn:uuid:8fb87d63-9b8b-4e8f-8a3f-fdbd37326d50>"],"error":null}
{"question":"How does access for repairs compare between early-life and end-of-life wind turbines?","answer":"Early-life wind turbines typically have lower operation and maintenance costs and fewer access issues, as maintenance is often covered under warranty periods. In contrast, end-of-life turbines face more significant access challenges and higher maintenance costs, particularly for rotor blades. Most owners don't perform full-scale blade inspections after end-of-warranty, leading to increased damage and higher associated maintenance costs. Additionally, as turbines age, they may require extensive refurbishment or complete decommissioning, which presents additional access and maintenance challenges.","context":["Dr Ralph Rayner, Sector Director, Energy and Environment at BMT Group Ltd explains why there has been so little progress in exploiting this vast renewable resource.\nThe biggest issue associated with exploiting marine renewable energy is the need to operate in a uniquely hostile environment. The huge potential that renewable energy offers is both an opportunity and a challenge because the areas where the greatest energy is available are the areas where it’s most difficult to engineer a means of exploiting that energy resource. The offshore marine environment is quite rightly considered to be one of the most challenging working environments on the planet.\nThe process of exploiting marine renewable energy can be broken down into a sequence of activities and hold-points. At each stage, offshore renewable energy installation developers need to draw on a range of specialist skills and services that may not be readily available in-house or might need to be developed expressly for a particular project.\nWhether this involves measuring wind, tides, tidal currents or temperature distribution to determine the best location for an offshore renewable energy project or preparing environmental and navigational assessments as part of the formal consenting process, the importance of getting it right first time cannot be overstated.\nWhere to site offshore renewable energy\nThe first stage in the process is identifying where the best potential sources of energy can be found. An initial measurement and analysis exercise is required to identify where suitable resources are situated and how these sites map onto relevant coastal locations.\nIn the case of the UK, this involves identifying marine locations which have the best potential for offshore wind, tidal streams, or waves and then looking at how those optimum sites map onto existing infrastructure, grid connections and harbours or ports with the capacity to construct or support the infrastructure.\nIf the met-ocean data used to support the location of an offshore renewable energy project is flawed, it is possible that the facility will never be able to deliver the levels of energy required to make it commercially viable.\nHaving identified suitable resources and their proximity to physical infrastructure, the next stage is to investigate how the use of those areas for marine renewable energy might conflict with other uses.\n|\"...the marine environment is a busy place...\" |\nDespite its apparent vastness, the marine environment is a busy place with fisheries, defence needs and shipping - all key issues that must be considered.\nFurthermore, proposed offshore renewable energy projects must take into account existing structures, submarine cables, oil & gas installations and a whole host of other maritime activities which must be accommodated without undue conflict.\nGovernments across Western Europe have recognised this and have established consenting processes that recognise the conflicting demands on ocean space. Understanding the issues involved and reconciling priorities between different demands is key to determining the suitability of an offshore renewable energy project at concept stage, long before any detailed design or construction takes place.\nIn the UK the consenting process for the first three rounds of offshore wind development and the recent proposals for tidal stream installations in the Pentland Firth have successfully followed this rationale. The proposed tidal barrage across the River Severn where there are many conflicting demands and many environmental impacts associated with major engineering developments in the Estuary, is also being reviewed on the same basis.\nFrom the developer’s perspective, having diligently undertaken the necessary studies and research to determine which sites are both feasible for development from an engineering and commercial point of view, and acceptable for development from an environmental point of view, all the evidence required to pass the formal consenting process should be available.\nHowever, by partnering with a team which has the right expertise and experience in breadth and depth to support the developer through this stage will ensure that a complex requirement does not become unnecessarily onerous.\nDesign and construction – lessons from O&G\nWith a suitable location defined using robust data and with the necessary consents in place, the work of design and subsequently construction can commence. With offshore renewable energy projects now being developed further and further offshore, the experience that the industry has gained building onshore wind farms has less and less relevance, while lessons that companies such as BMT has learned in its work with the offshore oil and gas industry are becoming far more pertinent.\nMassive structures designed to withstand the environmental forces prevalent in offshore locations require large areas of dockside to facilitate fabrication and storage. They also demand equally large pieces of specialist plant to transport and install them. Even the largest jack-up barges and installation cranes have a service wind speed and wave height beyond which they cannot operate.\nConsequently the provision of robust hind-cast and forecast met-ocean data to allow accurate planning and then to define suitable weather windows is fundamental in optimising the construction period.\nOperation and maintenance – the problem of access\nOnce an offshore renewable energy installation has been built and commissioned, the focus turns to how it can be supported in terms of operation and maintenance. There is an ongoing need for up to date met-ocean data so that energy outputs can be estimated to obtain the most favourable prices on spot markets.\nIn terms of maintenance, there is a fundamental difference between operating a plant in the marine environment and operating a plant on land. Generally, land based infrastructure in Europe is readily accessible for most of the year without huge constraints: In the sea this is not the case.\n|\"...the reliability of any mechanical plant installed on an offshore renewable energy installation must be very high if it is going to be commercially viable.\" |\nWind and wave conditions are seasonal but in offshore locations, wind and wave activity can limit access to offshore renewable energy installations at any time throughout the year. Consequently, operational planning issues and the provision of suitable plant and vessels are much more significant in the marine environment than they are on land.\nOperational planning teams will require a wide range of environmental information to ensure optimisation of operation and maintenance. They will also need to safely deploy maintenance crews to offshore renewable energy installations up to 100 miles offshore.\nThis has precipitated the design and development of a brand new class of vessel that fills a gap in the market between unsuitable inshore craft and unacceptably expensive offshore oil and gas support vessels.\nBecause it is difficult to intervene easily at sea, the reliability of any mechanical plant installed on an offshore renewable energy installation must be very high if it is going to be commercially viable.\nIf a wind turbine on land breaks down in the middle of winter, it is still possible to affect a repair. If a wind turbine 50 miles offshore were to breakdown in the middle of winter, repair is much more difficult and might conceivably not be possible for perhaps months, due to wind strength and sea state.\nFor OREIs to be commercially viable a high degree of reliability must be engineered into all of the components of the installation.\nEnd of life-time – then what?\nOne final factor that must also be considered, but has yet to be implemented due to the relative immaturity of the marine renewables sector is that of decommissioning.\nThe challenge currently facing the offshore oil and gas will be repeated. Potentially in 50 or 60 years time there will be thousands and thousands of turbines that have all reached the end of their working lives and will need to be either removed or fully refurbished.\n|\"There is no time to re-learn lessons of the past: We must get it right and get it right first time.\" |\nHowever, with this knowledge and access to the lessons learned by the offshore oil and gas industry, offshore renewable energy installations can be deigned to ensure ease of decommissioning and removal. The challenges associated with removing these structures in the latter part of the 21st century can be considered and addressed almost before they are built.\nThere is certainly a steep learning curve for developers and operators of offshore renewable energy installations to go through. It is not simply a case of taking plant, machinery and working practices that are designed to operate on land and transposing them into the sea.\nThe oil and gas industry learned the hard way that it doesn’t always work and the offshore renewable energy industry need not follow the same path. Developers have the opportunity to work with companies like BMT that have experience in all the skill-sets required to provide through-life consultancy and support for offshore offshore renewable energy projects.\nIn order to support the step-change in design and construction that is so clearly required to meet our obligations under the 2009 Renewable Energy Directive, we must make the best possible use of the wealth of knowledge and experience available in the marine engineering, science and technology sector.\nThere is no time to re-learn lessons of the past: We must get it right and get it right first time.","As today’s operating wind turbine assets age, owners and operators are being straddled with higher operation and maintenance costs than those experienced during the early years of operations; in many cases, more than those assumed within the pro forma. These cost increases are particularly common for wind turbine rotor blades. Even many of those operators who have signed long-term maintenance and service contracts will discover they aren’t immune to these rising costs.\nMost owners and operators do not perform full-scale rotor blade inspections on the operating fleet after end-of-warranty, and are typically only carried out after the field technicians start to report increased signs of damage or when higher associated rotor blade maintenance costs dictate that the inspections be performed to get a better understanding of the conditions of the rotor blades on-site. Once the rotor blades reach this condition, on-site/on-tower repair becomes much more costly and impractical, and other options must be considered. These are commonly:\nA. Allow the turbines to operate with minimal maintenance and plan for near-term re-powering of the site\nB. Re-blading the wind turbines with newer blades of the same or greater length\nC. Refurbishment of rotor blades\nD. Some hybrid solution of b and c\nMost owner/operators do not have the in-house rotor blade knowledge required to perform the comprehensive assessment of their rotor blades in order to understand their current operating condition. For this reason, many third party independent service providers (ISP’s) are brought in to perform these inspections and aid in the decision-making process for addressing issues with the rotor blades. Provided that the condition of the rotor blades is not so deteriorated that options “A” or “B” are the only feasible solutions and on-site repair is no longer an economical option, most companies prefer option “D”.\nOption “D” is generally used due to the ramp-up time associated with the refurbishment process, as most spare rotor blade sets are not normally readily available on-site. At minimum, a lag-time of 4–6 weeks can be expected from the time the rotor blades are removed from the turbine until the refurbishment is completed and the blades are ready to be re-installed. Subsequent rotor blade sets can be expected to be available at much lower lag-times. In order to minimize this downtime cost, new rotor blade sets are purchased, when possible; when not possible, older rotor blades that can be matched into a set are sought and purchased. As is most often the case, these older rotor blades require some degree of refurbishment before being installed, so in many instances these rotor blades are the first to go through the refurbishment process.\nWhat is involved in the refurbishment process?\nFrom the inspection findings, a detailed understanding of the wind farm rotor blades condition is known. When the discovered damage includes defects such as extensive surface coat cracking; flaking; erosion; chord-wise and span-wise cracking in the max chord and transition area; leading edge (LE) erosion; and interior defects affecting a high percentage of the rotor blade population; an off-site refurbishment plan is usually found to be the most cost effective solution—if repair is even economically feasible. In order for an owner and/or operator to proceed with this process, a number of steps need to be taken and items considered:\nStep 1: Project Tender\nIf the work is to be performed by third party contractors, the work should be tendered to multiple potential contractors. Within the tender, a detailed scope of the work to be performed provided with the request that a fixed price for refurbishment be provided. For this reason, most owners and operators work diligently—with outside consultants when required—to establish which of the common defects require repair; and ensure that these defects are repaired under the standard scope of repair. Time and materials rates are also requested for defects requiring repair that are out of the original scope of refurbishment definition. Figure 1\nEstablishing, before submitting the request for tender, which rotor blade repairs need to be included as part of the fixed set refurbishment cost is critical to obtaining a higher level of cost certainty for the project. However, careful consideration must be given to which repairs are and are not critical, as this can greatly influence the overall set refurbishment cost. When replacement rotor blades are available, refurbishment costs above 40–60 percent of new rotor blades costs are generally considered as the upper limit. At a minimum, the following classes of defects require repair:\n1. Interior and exterior structural defects that will have an effect on the safe operation of the rotor blade during the expected operating life\n2. Lack of continuity in lightning protection system down conductor\n3. Blocked drain holes\n4. Missing and damaged aerodynamic elements\nAdditional repairs will influence the overall set refurbishment cost. However, careful consideration must also be given to the following:\n1. The condition of the LE of the rotor blade commonly deteriorates. Based on an assessment of the current condition at the time of refurbishment and the local site conditions, the application of a supplementary LE protective coating is typically performed.\n2. A number of interior and exterior structural defects will be discovered during inspection following operation of the rotor blades. Not all of these defects will have an effect on the safe and continued operation of the rotor blades within the rotor blade design life. Taking a conservative approach when selecting which defects require repair will increase the refurbishment cost, but will likely decrease the long-term operating costs.\n3. Re-application of the final surface coat following completion of repairs is recommended for three main reasons:\na. A very patchwork surface results from the refurbishment process. This will leave the final surface condition of the rotor blade very unaesthetic.\nb. Wear of the surface coating will occur as the rotor blades continue to operate. Re-coating will ensure areas of thin coating are repaired.\nc. A refurbished surface coating will facilitate defect discovery during subsequent rotor blade inspections.\nIf, as is commonly requested, the contractors request access to a sample of the rotor blade population prior to responding to the request for tender, this access should be granted. It can be expected that through this inspection, the individual contractors will gain a better understanding of the rotor blade condition, and be able to provide more accurate pricing. Figure 2\nIt is also important that, as a requirement of responding to the tender, the contractor is able to provide demonstrated knowledge of the specific rotor blade materials and design as well as the general practices within the rotor blade repair industry. If this information cannot be provided, the contractor must be able to describe the processes that will be employed to determine this information and ensure the integrity and continued safe operating ability of the refurbished rotor blades.\nStep 2: Selection of Contractor\nSelection of the contractor to perform the work cannot be awarded solely on a lowest-tendered-quote basis. Careful consideration must be given during this process and individual site visits to assess the individual contractor’s capacity to perform the required work should be performed. Figure 3\nDepending on the scale of the wind farm, the number of years planned for refurbishment and the capacity of the individual contractors responding the request for tender, multiple contractors may be required to complete the refurbishment project.\nStep 3: Development of Standard Repair Procedures\nIf possible, the development of standard repair procedures should be performed prior to commencing with the rotor blade refurbishment process. Individual procedures for all of the common defects specified within the request for tender are required. This is an arduous task, and the required development of 10–20 procedures or more, is not uncommon for the full scale refurbishment process. However, as this generally set-up as a milestone event in the process, the contractor has the required encouragement to perform and complete this task. Figure 4\nAccordingly, standard repair procedures contain at least the following information:\n1. Specific defect condition for which the standard repair procedure applies\n2. Acceptance limits for the individual defects before repair is required\n3. Materials to be utilized\n4. Allowable ambient environmental conditions\n5. Specific and detailed instructions for performing the repair\n6. Reference to quality control check and hold points\nStep 4: Quality Assurance\nA quality system designed to ensure that all inspections, repairs, and documentation are being performed as per project expectations; and where available, defined policy, is required before beginning with the refurbishment project. Once the quality system is implemented, it must be clear that at every stage of the refurbishment process a quality control plan is present and being followed. Insufficient quality control will lead to variability in the refurbished rotor blades end quality, potentially leading to increased operating costs. Periodic audits of this system should be considered as an integral part of the project due diligence. Figure 5\nStep 5: Performance of Standard and Out of Scope Repairs\nAll defects that have a standard repair procedure must be repaired following the prescribed procedure. This ensures repair and refurbished rotor blade final condition consistency. Any and all defects that are to be repaired according to a specific standard repair procedure that are not repaired as per the standard repair procedure should be recorded as a non-conformance.\nOut-of-scope repairs are for defects that require repair without a standard repair procedure. The sub-process for developing, approving, and performance of out of scope repairs must be decided prior to commencing with repair portion of the refurbishment process. As these repairs represent a cost increase to the fixed rotor blade set refurbishment cost, the scope of the repair must be clearly defined so that cost and quality assurance can be tracked and maintained.\nStep 6: Documentation\nAs part of the delivery package for each rotor blade set, the following documentation, at minimum, should be requested and provided:\n1. Interior and exterior inspection findings and defect disposition\n2. Repairs performed and procedures used\n3. Rotor Blade Set Balancing\na. Individual Rotor Blade Mass\n1. Inclusive of mass added and location added at to balance\nb. Centre of Gravity\nc. Individual Rotor Blade Static Moment\nStep 7: Rotor Blade Set Matching\nFollowing the refurbishment of the rotor blades, the deviation between the individual rotor blade static moments is almost always above the allowable limit, necessitating the need for mass addition to balance the rotor blade set. Establishment of what this acceptance limit is should be performed prior to commencing with the refurbishment process. In all instances, if an OEM specified imbalance limit is available, it should be utilized. In the event no OEM specified limit is available, technical rationale for the acceptance limit to be utilized is required. Figure 6\nAs part of the original rotor blade design, sealed compartment(s), which are accessible from the exterior of the blade, are bonded in the rotor blade to allow for the addition of mass in order to match the individual static moments within the rotor blade set. Unfortunately, the records for the mass added to these compartments during original manufacture are not commonly available. Additionally, in many cases these compartment(s) have already been filled to max capacity. For this reason, as part of the refurbishment process, if not already defined, limits must be established for the allowable mass quantity addition and location(s) for balancing rotor blade sets. Uncontrolled mass addition to the rotor blade may lead to the development of non-design operating characteristics, affecting the safe operating condition of the turbine.\nAlthough the rotor blades to be refurbished began as matched rotor blade sets, following refurbishment, all of these blades will not be able to be placed in their original matched sets, due to the mass addition limits defined above. For this reason, a minimum work in progress (WIP) of approximately 3 sets is recommended. With the increased number of blades in WIP, and the common practice of some wind turbine OEM’s to have rotor blades manufactured from different rotor blade OEM’s at the same wind farm, owners and operators must be diligent in ensuring that only like operating characteristic and aerodynamic profile rotor blades are matched and whenever possible, that rotor blades are matched in sets by manufacturer and mold.\nStep 8: Re-Installation on Turbine\nIncoming inspection of the rotor blades prior to re-installation on the rotor hub is a due diligence check that is required. Transportation damage is commonly found, and should be repaired at this time, as the ease of access to the rotor blades and repair will provide long-term cost savings.\nAdditional due diligence checks such as testing of rotor mass imbalance and aerodynamic imbalance should be considered. Failing to perform these due diligence checks may lead to greater than design loading in the rotor blades and through the wind turbine drive train and structure.\nStep 9: Development of Comprehensive and Thorough Long Term Maintenance Plan\nThrough the refurbishment process substantial costs will be incurred. In most cases, these costs could have been minimized through the development of a thorough and comprehensive long term maintenance and inspection program implemented during the original project commissioning. To avoid Einstein’s definition of insanity and repeating the original process and expecting a different end result, it is imperative that a program be developed. If an in-house model is not already available, owners and operators are encouraged to work with the contractor(s) performing the refurbishment and other industry experts to develop a cost effective program that remains sufficiently comprehensive."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:1ffd41bd-c829-4ca0-a25c-7e5197fe80c4>","<urn:uuid:a8f47322-aea4-4695-84d0-b9ee42d564a7>"],"error":null}
{"question":"As an experienced tier looking to expand my skills - what's the main contrast between constructing a basic spinner dressing versus creating a thorax on a nymph when it comes to materials and technique?","answer":"When dressing a spinner, you simply tie fur or feather directly onto the hook shank using cotton thread, with half a dozen trimmed feathers or a couple of pinches of deer hair being sufficient, secured with thread loops and super glue. In contrast, creating a nymph's thorax involves using more textured materials such as chenille, ostrich herl, seal's fur, or angora wool to create an enlarged thorax section. While dressed spinners focus on creating a trailing effect, nymphs are rarely tied with hackles and instead emphasize building up the thorax section with these specialized materials.","context":["Spinners or revolving blade type lures are very effective on a fairly wide range of both fresh and salt water fish just as they are straight out of the pack. There are however ways to make them even more attractive to fish especially timid hard to fool fish in gin clear fresh water. How you ask? Well that’s easy, just as in life, if you want to make an impression you need to Dress for Success. Fortunately when it comes to spinners, dressing for success is fairly easy and relatively cheap, pity we can’t say the same about ourselves.\nSo what is a dressed spinner? Basically it’s just a regular spinner with some fur or feather tied to the hook. The idea is to add to the attractiveness of the lures flash and vibration with a natural wavy tail in the case of feathers and to hide the hook a little in the case of fur (see examples in the picture below). The natural feel of both the feather and the fur also encourage the fish to hold onto the lure a little longer before deciding its not food and spitting it out. This little moment of hesitation on behalf of the fish should see you set the hook more often than not.\nSo how do you dress a spinner? Well the first thing you’ll need is a ‘spinner’ type lure (e.g. Celta); in general any type will do, just make sure it’s a good quality example. Next you’ll need some fly tying material; I generally use Marabou feather and Deer hair in a range of colours and cotton thread to match the chosen colour. Lastly you need some super glue or equivalent product, just make sure it dries clear. All of these products can easily be found at any good tackle store that stocks a decent range of fly gear or from similar stores on the net.\nNow you don’t need to be some legend fly tying guru to dress your spinners, all you need is some patience, perseverance, good lighting sharp scissors and a small vice or other such method for holding the spinner while you tie some fur/feather onto it. A fly tying vice is ideal but if you don’t have one and good ones are fairly expensive, virtually any vice will do, just remember the smaller the better (see pic below).\nI tie the fur/feather directly onto the shank of the hook, how much you use is really up to you but don’t go over board, half a dozen trimmed feathers or a couple of pinches of deer hair should suffice in most cases. Now space the fur/feathers evenly around the hook shank, this is pretty easy if your using a treble hook as the hackle will sit nicely between the hook turns. As with fly tying, you put on a little bit, secure it with cotton thread then add the next bit and secure that until you’ve built up the amount of fur/feather that you want. I then generally put half a dozen or so loops of cotton thread around the whole lot and tie it off, any knot will do but if you know how to do fly tying knots then use them. Finally add a drop of super glue to the knot, this will add to its security and generally make up for any less than perfect knots.\nNow I like to tie a few in a row and well seeing that you’ve gone to all the trouble of getting all this kit together I reckon you may as well do the same. Try and make a few different colours, I generally match the colour of the fur/feather to the colour of the lure, no not silver and gold rather the accent colour on the blade (e.g. Red, Green, and Black etc). You’ll only need a couple of each colour to compliment your tackle box and don’t be afraid to try new colours or new materials, who knows some gold and silver might just work particularly in the salt water, blue might be another idea as well. Come to think of it I might go and try a few of those ideas out right now.\nHappy spinner dressing and remember, Dream it, See it, Catch it.\nWords and Pictures by Brendan Keogh Copyright Brendan Keogh 2013","A box of dry flies, wet flies and nymphs in a variety of colours and sizes is the pride and joy of a game angler. But there is even more satisfaction for the man who has tied them all himself\nTo create even a rough impression of a fly some knowledge of its anatomy is needed. The head (not simulated on all tied flies) is set on a body which is divided into two sec-tions – the upper thorax and larger abdomen. The wings vary in size but can be represented by different hackles made from pieces of feather. Because dry flies float on the surface, lightness and buoyancy in their construction are most important. The tying must be tight and even to prevent waterlogging. Where heavy or porous material would add to the attractiveness, these are given a last minute coating of a suitable oil to help keep the fly afloat.\nFor tying your own flies a small fly-tying vice is essential. Select a model that has a firm base, jaws capable of holding hooks from size 16 up to salmon sizes, and a vice which can be rotated for ample access to the fly. Still smaller hooks can be tied if you buy tiny ‘midge’ jaws which are gripped between the standard jaws. Hackle pliers are needed to hold the hackle firm and to prevent it unwinding when released. Two pairs of scissors with short, pointed blades are required: one for cutting quills, tinsel etc., and one for „ feather fibres and delicate materials. A bobbin holder is essential – one with a long spigot is advisable as it & enables turns of silk to be laid on accurately.\nA cake of cobbler’s wax will also be needed unless you are going to use prewaxed tying silk. Pulling silk quickly through a block of wax will coat it in such a way that it adheres as you apply it to the fly. A good selection of materials, in-cluding pieces of fur, tinsel, hair, feathers on the skin, wool and silk threads should be to hand as, after basic methods have been mastered, experimental and unusual varieties can be created. There are other useful (though non-essential) accessories, including tweezers and a whip-finish tool which completes the tying with a neat knot.\nThe angler is now ready to tie a basic fly. A trout hook (size 14-8) is held in the vice with the shank protruding horizontally and with the point clamped out of sight. An exposed point will catch and fray thread as you work.\nOur basic body shape is constructed from about 10 in of single-ply floss silk. (Floss is often sold with three or four strands plaited together.) Floss is a convenient body substance as it covers thoroughly and quickly.\nThe tying thread you will be using to tie in the different parts and materials of the fly should be attached just behind the eye. Make a few turns towards the bend, then tie in one end of the floss silk with two or three tight turns. Continue winding the tying thread down to the bend.\nThe tail comes next. Three or four fibres torn from a large cockerel or hen hackle are bunched together and tied in tightly at the bend.\nAt the same time as introducing the tail, tie in a length of tinsel, wire or lurex in silver or gold. This will be the rib whose glitter adds greatly to the attractiveness of an artificial. Trim off the waste ends of hackle fibres and wind the tying thread back to where you tied in the floss. The weight of the bobbin keeps the thread under tension, leaving both hands free to apply a coat of varnish to the shank. This fixes the first layer of floss silk.\nWind the floss in touching turns down to the bend and back up to the tying thread. Tie off the floss with three or four tight turns of thread and trim the surplus floss.\nNow you can wind the rib forward using a spiral opposite to that with which the floss was wound on. This way it won’t disappear among the turns of silk. Now tie off the rib as you did the floss and trim any waste.\nWhen you have completed the body, you have the choice of turning your fly into a dry fly with hackle and head or one with wings, hackle and head, a wet fly with wings, hackle and head or one with just hackle and head, or a nymph with thorax, hackle and head. Whichever you choose – it may be best to leave wings until you have become fairly dexterous – the head is added last of all.\nSelecting a wing I Strip the down fibre from the bottom of a suitable feather and separate out a wing section using a dubbing needle. 2 Grasp fibres at tips – pull down and outwards, if they separate, grip the extreme tips, pull outwards, and rub your fingers up and down until the fibres lock together again. For paired wings, select sections from two feathers which are as alike as possible.\nIn dry fly tying, stiff, glossy, water repellant cock hackles are best for buoyancy, though some people use the softer hen hackles in the belief that these present less of an obstruction on the strike.\nTo prepare a feather for hackling, hold it by the tip and run moistened fingers down the fibres so that they stand out at right angles and separate. Then, having stripped the base of the quill, lay the feather at right angles to the hook shank, just forward of the body, and lash the bared quill to the shank with tying silk. The fronded remainder is then held at the tip with hackle pliers and three or four turns made towards the eye.\nBinding a hackle needs a steady hand since the binding (gilt wire in a Palmer) is pulled between separate fibres of the feather: the wire needs to be kept taut and wound on in a spiral opposite to that used in winding the hackle. Any trapped fibres should be teased out with a sharp needle. Wire binding in the Palmer makes for durability and is attractive.\nThe weight of the pliers hanging down from the tip will prevent your work springing undone while freeing your hands to fasten it with turns of the silk. Cut off the tip of the feather. The Palmer is hackled along the whole length of its body. If you were tying a nymph, your artificial would have an enlarged thorax in place of a hackle. A thorax can be furbished out of materials with interesting textures, such as chenille, ostrich herl, seal’s fur or angora wool, but is rarely tied on wet or dry flies. The bodies of the less streamlined flies are dressed or ‘dubbed’ with animal fur – mole for example – which is applied to the waxed silk by rolling a pinch of fur on to the silk between thumb and index finger. The furry silk is then wound on to the shank.\nPreparation and tying in 3 To make hackle stand erect on a dry fly or Palmer, hold the tip, and draw the feather up between the fingers of the other hand. 4 Tie in stripped butt behind the eye, the outside of the feather facing forward. Secure with figure-of-eight turns of silk. Tie in on edge or the fibres will not stand erect.\nI Hold wings firmly along top of shank, so that wingtips just reach the hook-bend. Pass silk up between thumb and near wing and down between forefinger and far wing. Draw silk down firmly. Make two turns towards hook eye. 2 Before releasing, raise wings to the vertical. Take two turns round their base and behind. 3 Separate the wings, passing silk forward between them and around the shank, then back between them and around the shank again.\nThe simplest of dry flies – the Snipe series and Spider patterns – are simply hook, hackle and silk, but with natural colours or by dyeing, this combination affords many possibilities. The addition of wings, however can improve the balance of a dry fly, and makes for more spec-tacular creations. They can be tied in at the same stage as the tail and the fastenings hidden when the body is built up. Alternatively, a space can be left between thorax and hook-eye, and the wings added when the body is complete. There needs to be some silk round the shank before wings can be attached successfully.\nDry flies often sport double wings which also makes them more buoyant. Two feathers are needed from symmetrically identical positions on either side of a bird’s wing or tail fan. Symmetrically matched sections are cut from the centre of these feathers. These diamond-shaped sections have to be torn in half (for the double-wing effect) and eased into rectangles between finger and thumb (without the herls separating). Commercial wing-cutters are available for stamping wing shapes out of cock hackles. The four butt ends are pinched firmly between finger and thumb and introduced at the shoulder, two on either side of the body.\nNow a loop of silk has to be squeezed between finger and feather, passing up between thumb and wing, over the wing and down between index finger and wing of the far side. Do this twice more then pull the silk tight. Three turns of silk must be firmly drawn down without the wings splitting. To finish off, wind the thread in front of and behind the wings, passing it between them to keep them separated. The figures-of-eight will be concealed by any hackle which may be added afterwards.\nUnless a large, varnish head is to be moulded, the fly can be finished off with a neat whip finish near the eye. Vycoat, a modern varnish not prone to cracking, can be dabbed on to represent a glossy black head. This cleverly conceals any waste ends.\nThe varieties of dry fly patterns have occupied many volumes, and the possibilities are not yet exhausted. Nor is their use restricted to fishing for trout and salmon, for such coarse fish as rudd, roach, chub, dace, grayling, perch and pike are all taken on artificial flies."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:f0fb95cf-183b-4454-86c7-3baf22c8ca54>","<urn:uuid:6122f357-d064-4a1c-abbd-594f6c474615>"],"error":null}
{"question":"As a soil scientist, I need to know: what are the main disadvantages of soil with low Cation Exchange Capacity?","answer":"The main disadvantages of low CEC soil are the limited availability of mineral nutrients to plants and the soil's inefficient ability to hold applied nutrients. Plants must expend extra energy searching for minerals instead of using it for growth, flowering, seed production, or root development. Additionally, when soluble mineral salts are applied in large doses, they cannot be held efficiently due to the small cation warehouse.","context":["Cation Exchange Capacity (CEC) – First, what are cations (pronounced CAT-eye-ons)? Simply put, they are positively charged ions. The cations in the soil that concern us the most are calcium (Ca), magnesium (Mg), potassium (K), and hydrogen (H). The capacity of the soil to hold and exchange cations is determined by the amount of clay and/or humus that is present. These two colloidal (negatively charged) substances are essentially the cation warehouse or reservoir of the soil. Sandy soils with very little organic matter (OM) have a low CEC, but heavy clay soils with high levels of OM have a much greater capacity to hold cations.\nThe disadvantages of a low CEC include the limited availability of mineral nutrient to the plant and the soil’s inefficient ability to hold applied nutrient. Plants can exhaust a fair amount of energy (which might otherwise have been used for growth, flowering, seed production or root development) scrounging the soil for mineral nutrients. Soluble mineral salts (e.g. potassium sulfate) applied in large doses to soil with a low CEC cannot be held efficiently because the cation warehouse is too small.\nWater also has a strong attraction to colloidal particles. All functions that are dependent on soil moisture are also limited in soils with low CEC. Organisms such as plants and microbes that depend upon each other’s biological functions for survival are inhibited by the lack of water. Where there is little water in the soil, there is often an abundance of air, which can limit the accumulation of organic matter (by accelerating decomposition) and further perpetuate the low level of soil colloids.\nHigh levels of clay with low levels of OM would have an opposite effect (a deficiency of air), causing problems associated with anaerobic conditions. The CEC in such a soil might be very high, but the lack of atmosphere in the soil would limit the amount and type of organisms living and/or growing in the area, causing dramatic changes to that immediate environment. Oxidized compounds such as nitrates (NO3) and sulfates (SO4) may be reduced (i.e., oxygen is removed) by bacteria that need the oxygen to live, and the nitrogen and sulfur could be lost as available plant nutrients. Accumulation of organic matter is actually increased in these conditions because the lack of air slows down decomposition. Eventually, enough organic matter may accumulate to remedy the situation, but it could take decades or even centuries.\nThe CEC of a soil is a value given on a soil analysis report to indicate its capacity to hold cation nutrients. CEC is not something that is easily adjusted, however. It is a value that indicates a condition, or possibly a restriction that must be considered when working with that particular soil. Unfortunately, CEC is not a packaged product. The two main types of colloidal particles in the soil are clay and humus and neither is practical to apply in large quantities. Compost, which is an excellent soil amendment, is not necessarily stable humus. Over time compost may become humus, but the end product might only amount to 1-10 percent (in some cases, less) of the initial application.\nRemember that each percent of organic matter in the soil is equal to over 450 pounds per 1,000 square feet (20,000 lbs/acre). Compost normally contains about forty to fifty percent OM on a dry basis, and weighs approximately 1,000 pounds per cubic yard (depending on how much moisture it contains). If the moisture level is fifty percent, it would take two cubic yards of compost per thousand square feet to raise the soil OM level one percent (temporarily). Large applications of compost to the surface of the soil, however, can do more harm than good. Abrupt changes in soil layers can inhibit the movement of water and restrict the soil’s capacity to hold moisture. Obviously, building organic matter in the soil is not something that can or should be done overnight. Natural/organic nitrogen sources, in general, will do more than synthetic chemicals to raise or preserve the level of OM, because of the biological activity they stimulate. Colloidal phosphate contains a natural clay and is often used to condition sandy soils with a low CEC. Low phosphorus conditions should be present, however, to justify its use.\nIf a soil has a very low CEC, adjustments can and should be made, but not solely because of the CEC. A soil with a very low CEC has little or no clay or humus content. Its description may be closer to sand and/or gravel than to soil. It cannot hold very much water or many cation nutrients; plants, therefore, cannot grow well. The reason for the necessary adjustment is not the need for a higher CEC, but because the soil needs conditioning. A direct result of this treatment will eventually be a higher CEC. During the process of soil building, the steward must be aware of the soil’s limitations. Soil with a low CEC cannot hold many nutrients, so smaller amounts of fertilizer should be applied more frequently. Feeding a crop growing on soil with a low CEC is analogous to feeding an infant. It doesn’t eat a lot but must be fed often. As the CEC of the soil improves, larger doses of fertilizers can be applied less frequently."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:f87f48e9-cfd4-4086-9d05-12115f368b81>"],"error":null}
{"question":"What are the nutritional implications of refined carbohydrates, and how do they affect athletic performance?","answer":"Refined carbohydrates are nutritionally inferior as they've been processed to remove natural fibers, bran, germ, and essential nutrients like B vitamins, magnesium, iron, phosphorus, manganese, and selenium. This processing leads to blood sugar spikes and poor sugar management, which can contribute to serious health issues like Type 2 diabetes, obesity, chronic inflammation, and heart disease. However, for athletes, particularly before and after workouts, refined carbs can serve a specific purpose. They provide quick energy before exercise to prevent cramping and can boost muscle recovery post-workout by helping buffer protein consumption to maximize muscle-building effects. Still, athletes should primarily rely on slow-absorbing, complex carbohydrates (about 90-95% of their carb intake) for sustained energy release during training and competition, while limiting refined carbs to only 5-10% of daily carbohydrate intake, specifically for post-exercise recovery.","context":["Health Nutrition & Diet What Makes Refined Carbohydrates So Unhealthy? RDs Explain Why They're Not the Most Nutritious Option Certain types of carbs don't serve you like others do—here's how to strike the right balance. By Samantha Lande Samantha Lande Instagram Twitter Website Samantha is a freelance writer who covers health, nutrition, wellness, and has contributed to national and international publications for over a decade. Real Simple's Editorial Guidelines Updated on December 7, 2022 Fact checked by Emily Peterson Fact checked by Emily Peterson Emily Peterson is an experienced fact-checker and editor with Bachelor's degrees in English Literature and French. Our Fact-Checking Process Share Tweet Pin Email Carbohydrates have received a bad rap over the years, but not all carbohydrates are bad for you. In fact, carbs are one of the key macronutrients your body needs everyday to function properly—you just need to know what kind of carbs to reach for. There are whole, unrefined carbohydrates that come from nutritious foods—whole grains, beans, fruits, and vegetables. And then there are refined carbohydrates, often called processed carbohydrates. Refined carbs are the ones to watch out for and eat in limited quantities. Why? Because they contain very few nutrients your body can actually use. How Much Is Too Much Sugar? Here's Where to Cap Your Sugar Intake Every Day What exactly are refined carbs? \"Refined carbs are carbohydrate foods that have been processed to remove natural fibers, bran, germ, and the nutrients in these parts from the grain,\" says Lauren Minchen, MPH, RDN, CDN, nutrition consultant for Aspire, an AI-driven visual diet diary app. \"What's left is the starch and caloric portion of the grain, with a minimal amount of protein.\" Refined carbohydrates generally fall into two categories: refined grains and added sugars. Refined Grains White flour is probably the most recognized refined grain, showing up in everything from bread and pasta to pretzels, doughnuts, snack bars, and cookies. \"Whole grains have three parts: bran, germ, and endosperm,\" explains DJ Blatner, RD, author of The Superfood Swap. \"Refined grains are processed to remove the bran and germs, which removes many nutrients like iron, B vitamins, and fiber.\" Added Sugar This is the other main category of refined carbohydrates, encompassing all of the sugars that don't occur naturally in a whole food, like fruit. \"Added sugar is definitely everywhere, and there are many synonyms for sugar such as cane juice, high fructose corn syrup, glucose, dextrose,\" says Blatner. \"Even brown rice syrup, honey, and maple syrup are considered added sugar.\" Added sugar can be sneaky and appear in salad dressings, sauces, yogurts, and cereals, making it hard to avoid if you're not careful about reading the ingredient labels of packaged foods. What Is Sprouted Bread? And Why You Should Add It To Your Diet Are refined carbohydrates truly bad for you? While undeniably delicious, this kind of carbohydrate sadly isn't the best option for you. \"Refined carbs are void of essential nutrients, like B vitamins, magnesium, iron, phosphorus, manganese, and selenium—all of which are in the bran and germ [that get removed when processed],\" Minchen says. \"In addition, [the lack of] fiber in refined carbs equates to a greater blood sugar spike and risk of poor blood sugar management,\" she adds. This poor sugar management can often lead to more serious issues like Type 2 diabetes, obesity, chronic inflammation, and heart disease. Because refined carbohydrates lack any real nutrition, they aren't very filling or satisfying, and the body digests them rapidly. This can often lead to the need to eat more and difficulty managing diet choices, cravings, and healthy weight. 7 Ways to Break a Sugar Addiction and Curb Cravings for Good How much is OK to consume? Don't panic: You don't need to cut the yummiest foods out of your life completely—but as with all things, moderation is your smartest move when it comes to things like white bread products, white rice, pastas, soda/juices, packaged snacks, and other refined carbs. \"Ideally, refined carbs should be consumed sparingly: up to two to three servings per week for the average person is OK,\" Minchen says. \"For someone with poor blood sugar regulation or diabetes, consuming refined carbs even less often may be recommended.\" One healthy way not to feel like you're missing out on the goodness of carbs is to make sure you're prioritizing whole grains over refined grains. The USDA Dietary Guidelines for Americans suggests we make \"half our grains whole,\" says Blatner. \"That means, for women (30 to 60 years old) the total daily grain target is 5 to 7 ounces per day, and for men, 7 to 10 ounces equivalents per day—with only half of those being refined carbohydrates.\" To put that in perspective, 1 ounce is equivalent to one slice of bread, one cup of cereal, or a half cup of cooked rice or pasta. Be careful with the added sugars. The American Heart Association recommends limiting daily added sugar to 6 teaspoons (25 grams or 100 calories) for women and 9 teaspoons (36 grams or 150 calories) for men. What's the Difference Between Whole Wheat, Whole Grain, and Multigrain Bread? Refined carbs have their occasional perks. Although the bad ultimately outweighs the good, refined carbohydrates do provide quick energy in a pinch.\"Quickly digested energy before a workout is important in order to prevent cramping that can come from eating fiber right before a workout,\" says Minchen, who recommends something like fresh fruit juice or white bread for these circumstances. \"Additionally, eating something quickly digested right after a workout can boost muscle recovery and buffer the protein you consume to help maximize its muscle-building effect.\" Just make sure you avoid any added sugars wherever possible. If you are going to have refined carbohydrates, it's best to find the ones enriched with added vitamins and minerals, advises Blatner. \"But it's always best to choose whole grains,\" she says. How to Eat a Balanced Diet Without Restricting Your Guilty Pleasures Was this page helpful? Thanks for your feedback! Tell us why! Other Submit Sources Real Simple is committed to using high-quality, reputable sources, including peer-reviewed studies, to support the facts in our articles. Read our editorial guidelines to learn more about how we fact check our content for accuracy. Wu D, Guan Y, Lv S, Wang H, Li J. No evidence of increased risk of stroke with consumption of refined grains: A meta-analysis of prospective cohort studies. J Stroke Cerebrovasc Dis. 2015;24(12):2738-2746. doi:10.1016/j.jstrokecerebrovasdis.2015.08.004 Centers for Disease Control and Prevention. Fiber: the carb that helps you manage your diabetes. Frazier TH, DiBaise JK, McClain CJ. Gut microbiota, intestinal permeability, obesity-induced inflammation, and liver injury. JPEN J Parenter Enteral Nutr. 2011;35(5 Suppl):14S-20S. doi:10.1177/0148607111413772 American Heart Association, Added sugars. Academy of Nutrition and Dietetics. Timing your pre- and post-workout nutrition.","Optimizing Carbohydrates for Muay Thai Athletes\nThere has been a recent increase in the popularity of high fat, low carb diets, and as a direct result, carbohydrates have become somewhat demonized within the health and fitness industry.\nCarbohydrates have been described as the cause of cardiovascular and metabolic disease, as well as immune system disorders, and of course, the current obesity epidemic plaguing the nation.\nInterestingly, this is not the entire story.\nWhile a low carbohydrate diet may be beneficial for your everyday office worker as a way to lose weight, I am here to tell you that for anyone trains, competes, and works hard towards athletic endeavors of any kind, low carb diets are detrimental to success.\nCarbohydrates are absolutely essential to the production of energy. This holds particularly true when we discuss energy produced at a high intensity (during anaerobic exercise), such as that seen during a single bout of competition, or multiple bouts back to back.\nIn fact, carbohydrates are so important that it has been recommended that approximately 50% of an athletic individual’s daily energy intake comes from carbohydrates.\nAnd while it is apparent that carbohydrates are essential for energy production, and thus successful sport performance, it goes a little deeper than that. You see, not all carbohydrates are created equal, and the types of carbohydrates that we consume, and at what times, can significantly influence the rate at which we produce energy, and for how long we produce that energy.\nCarbohydrates are one of the three key macronutrients (protein, fat and obviously the topic of today’s discussion, carbohydrates) that we receive from food.\nDespite the fact that carbohydrates can be consumed in different forms (sugar, starches, and fibre), they are all broken down into glucose by the digestive system, which is then absorbed and shuttled around the body to be used for energy.\n(**It is important to note that fibre is the only exception to this rule, as it cannot actually be broken down within the digestive system, and passes through our digestive tract almost completely untouched.**)\nOnce absorbed, glucose is stored in the liver and muscle tissue. Once our energy stores are full, excess glucose can then be converted into fatty acid molecules, which are then also stored to be used for energy at a later time (which essentially describes the process of fat accumulation).\nOnce glucose enters the blood stream, insulin (a key energy storage hormone) is secreted into the blood, which in turn promotes the storage of fatty acids, amino acids (protein molecules), creatine, and glucose into our muscle, adipose, and liver tissues.\nFast and Slow Absorbing Carbohydrates\nNow, while all the carbohydrates that we consume do end up stored as glucose within the body’s tissues, this does not by any means suggest that all carbohydrates are created equal.\nCarbohydrates are found in many different foods, and although it is not as black and white as good carbohydrates from bad carbohydrates, there are a few distinct differences between fast absorbing carbohydrates and slow absorbing carbohydrates.\nSlow Absorbing Carbohydrates\nSlow absorbing carbohydrates are those that we should be consuming most of the time (and as such should make up about 90% of our carbohydrate intake.\nSlow absorbing carbohydrates are known to come from whole food sources, such as vegetables, grains, legumes, and fruit. While these foods undoubtedly have high carbohydrate content, they should by no means be considered bad.\nThese carbohydrates that are found in whole foods consist mostly of complex carbohydrates.\nComplex carbohydrates are starches that are made from very long chains of glucose molecules. This is important, as these complex carbohydrates take quite a long time to be broken down in the gut, and as a result, are absorbed into the blood at an extremely slow rate .\nDue to the slow absorption of these carbohydrates, they only cause a very small insulin response. As a direct result, less glucose is stored within the tissue (and less is converted to fat), and more is available to be used freely for energy.\nWhen it comes to both training and competing, this is integral.\nBy ensuring that the bulk of our carbohydrates come from complex sources, we can guarantee that we have adequate energy available for the performance of high intensity exercise. Furthermore, as these types of carbohydrates are absorbed extremely slowly (hence the name…), they also provide the prolonged release of energy into our bloodstream.\nThis becomes incredibly important during longer bouts of physical activity (after a few rounds of competition, maybe?), as it ensures we can compete at a high intensity for a long duration, without a reduction in performance.\nAs an added bonus, the whole foods providing these complex carbohydrates often also contain an abundance of vitamins and minerals. These essential nutrients can improve health and recovery, and are known to play a number of important roles within the human body .\nBy maintaining a high consumption of these nutrients, we can further improve health, reducing our risk of disease and illness, and further improve our capacity to produce energy, and therefore train and perform efficiently.\nThese slow absorbing carbohydrates are inherently different from fast absorbing carbohydrates, in that they have undergone zero processing (hence the reason they are also often considered whole carbohydrates).\nFast Absorbing Carbohydrates\nOn the other hand, we have fast absorbing carbohydrates.\nFast absorbing carbohydrates are often described as refined carbohydrates, due to the high amount of processing they undergo before they are packaged for consumption.\nRefined carbohydrates come in the form of pasta, sugar sweetened beverages (such as fruit juice and soda), breads, pastries, cereals, and pretty much any type of sweet or candy. They also commonly come in carbohydrate powders, such as dextrose and maltodextrin.\nA simple way to establish whether a food is a fast absorbing carbohydrate or not is to look at how its presented in the grocery store. Ultimately, if it comes in a package, it is most likely a refined carbohydrate.\nThe carbohydrates used in these sorts of foods have been broken down during processing into compounds known as simple sugars. Simple sugars are small chains of glucose molecules held together by very weak bonds (vastly different to complex carbohydrates).\nThese simple sugars are digested and absorbed at a very rapid rate, and as a result, cause an extremely large insulin response .\nNow if consumed regularly, simple sugars can cause a large increase in fat mass over time (due to the role insulin plays in shuttling fatty acids into the fat tissue), which will obviously impact performance negatively.\nBut, they do actually have a place in our diets. When consumed immediately after exercise, these simple sugars can promote recovery by increasing the rate at which amino acids are shuttled into the muscle tissue (amino acids are the building blocks of our muscle cells).\nAs a result, these sugars should make up 5-10% of our daily carbohydrate intake, and should be only consumed after training or competition to promote recovery.\nNot all Carbohydrates are created equal.\nThere are both fast and slow digesting carbohydrates. Slow digesting carbohydrates should make up about 90-95% of the carbohydrates we consume on a daily basis, as they promote sustained energy release, which can improve performance.\nFast digesting carbohydrates should only be consumed after training or competition, as they can enhance recover by increasing the rate at which amino acids are shuttled into the muscle tissue. These carbohydrates should make up only 5-10% of our daily carbohydrate intake, and should only be consumed after exercise.\nBy timing our carbohydrates effectively throughout our days, we can both improve our performance and our rate of recovery!\nHernandez, Teri L., et al. “A higher-complex carbohydrate diet in gestational diabetes mellitus achieves glucose targets and lowers postprandial lipids: a randomized crossover study.” Diabetes Care 37.5 (2014): 1254-1262.\nPrasad, Kedar N. Micronutrients in health and disease. CRC Press, 2016.\nBossetti, Brenda M., et al. “The effects of physiologic amounts of simple sugars on lipoprotein, glucose, and insulin levels in normal subjects.”Diabetes Care 7.4 (1984): 309-312"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:7b18f983-d70b-40a7-95bd-710552295478>","<urn:uuid:746e573c-ca82-4be0-be66-43b91e718ef9>"],"error":null}
{"question":"What are the main components of atmospheric science and how does Weather Command apply them in practice?","answer":"Atmospheric science encompasses several components including meteorology (focusing on weather forecasting), climatology (studying long-term atmospheric changes), and atmospheric chemistry and physics. Weather Command applies these sciences through their professional meteorologists who provide site-specific weather forecasts using multiple tools including radar, satellite data, weather maps, and surface weather observations. They combine theoretical knowledge with practical experience to deliver accurate forecasts 24/7, going beyond simple computer models to serve clients across the Great Lakes Region and throughout the United States.","context":["Frequently Asked Questions\nQ: What are Murray and Trettel, Inc. and Weather Command®?\nA: Murray and Trettel, Inc. is a private meteorological consulting company, founded in 1946 by John R. Murray and Dennis W. Trettel. It is one of the oldest and most respected private meteorological consulting companies in the United States. The company’s Operational Forecast Division is known as Weather Command®.\nQ: How many people does Murray and Trettel, Inc. employ?\nA: Currently, the company has a total of 21 employees on staff, including 17 Professional Meteorologists.\nQ: What does Weather Command® do?\nA: Weather Command’s® team of professional meteorologists provide clients with timely, site-specific weather information and forecasts. The Weather Command® Operational Forecast Facility is staffed 24 hours per day, 365 days per year. Clients have access to telephone consultation with professional meteorologists around the clock.\nQ: For what locations can Weather Command® provide forecasts?\nA: Weather Command’s® primary client base is the Great Lakes Region - Illinois, Wisconsin, Michigan, Indiana and Ohio. However, forecasts are prepared for locations throughout the United States.\nQ: Who uses Weather Command® forecasts?\nA: Any business that can be impacted by the weather is a potential Weather Command® client. The following is a list of the type of clients currently using Weather Command’s® Forecast Service:\n- Electric Utilities\n- Professional Sports Teams\n- Gas Utilities\n- Park Districts\n- Department of Transportation\n- Large Metropolitan Cities\n- Production Companies\n- Public Works Departments\n- Underwater Construction Companies\n- County Highway Departments\n- Marine Shipping Companies\n- Large Corporate Campuses\n- Golf Courses\n- Contract Snow Plowing Companies\n- Car Washes\n- General Contractors\n- Landscape Companies\n- Property Managers\n- Concrete and Asphalt Contractors\n- Steel Companies\nQ: There is plenty of free weather information out there, so why should I pay for it?\nA: The weather that is available on the web and what you hear from the media is a general forecast for a large area. Most of us whose livelihood depends on the weather need something that is more site and time specific. Our clients need to know what kind of weather will happen at a given location at a given time. Thanks to radar and satellite data as well as other tools that we use, to a large extent we are able to do that. Also, you may hear conflicting forecasts on the radio and TV, as well as what you access on the internet, but by using Weather Command® you receive the weather from one reliable source.\nQ: I see where a private weather consulting service will help me, so I should go to the one that charges the lowest fee, right?\nA: Weather forecasts are like most other things. You get what you pay for. Other companies may charge lower fees by using inexperienced forecasters, reformatting the National Weather Service forecasts to make it look like their own, or only use forecasts put out by computer models. At Weather Command®, while we make every possible effort to be financially competitive, we take pride in our forecasts and feel our clients are entitled to the most accurate forecasts possible. Our professional meteorologists, most of whom have over 20 years of experience, develop all of our forecasts. While computer model forecasts are an important tool, by importing our knowledge and experience, the client receives the best possible information.\nQ: I have my own radar, so why can’t I do my own forecasting?\nA: While radar is an important tool, many other things go into a weather forecast. Weather maps, satellite photographs and surface weather observations to name a few. Radar can also be deceiving. When moisture is high up in the atmosphere, it will appear on radar as if it will rain or snow soon. In reality, it will usually evaporate long before it hits the ground. Conversely, when moisture is low level, it may not show up on the radar, but precipitation can still reach the ground.\nQ: Storm warnings are important to me. I don’t sit behind a desk, so how are you going to let me know when a warning is issued?\nA: We’ve got you covered. If we are issuing a storm warning, we'll find you. We will call you on your cell phone, at home, or page you. Warnings are faxed so you have something in writing, but we always make every effort to get in contact with you, to make sure you receive our storm warning. We are watching the weather 24 hours a day, 7 days a week, so you don’t have to.\nQ: I have the warning, what about updates?\nA: Updates are issued as necessary. If there is a significant change in the forecast, you will receive an update. The fact is, meteorology is not an exact science, and as much as we try and get the forecast right the first time, it does not always happen. But we never let a wrong forecast go uncorrected.\nQ: Can you provide historical weather data?\nA: Weather Command® has archived climatological data, dating back to the 1940’s. Data that dates as far back as the late 1800’s is available from Weather Command’s® sources. Clients may request copies of data or written reports, detailing the weather that occurred at specific locations, dates and times.\nQ: Can you provide expert courtroom testimony concerning the weather conditions at the time of an incident?\nA: When weather is an issue in a legal matter, Weather Command’s® forensic meteorologists provide written opinions, depositions, and expert courtroom testimony.\nThe type of legal cases our Forensic Meteorologists have been involved with includes, but is not limited to:\n- Slip and Fall\n- Wind Damage\n- Lightning Strike\n- Rail Accident\n- Car Accident\n- Excessive Heat\nQ: What does Murray and Trettel’s Environmental Applications Division do?\nA: Murray and Trettel’s Environmental Applications Division purchases, installs, maintains and calibrates meteorological monitoring equipment. The equipment we maintain may be located at ground level or on a tower that rises nearly 400 feet above the ground. Murray and Trettel has a staff of qualified field technicians who climb these towers to maintain or calibrate the equipment.\nData is collected periodically from remote sites and reviewed for accuracy. Computers perform the first review of the data and data that may indicate a problem is flagged. Professional meteorologists then review the data to verify that the data is accurate. If a review of the data indicates a problem exists at a site, field technicians are dispatched to perform maintenance to correct the problem.\nValid data recovery is the primary goal. Valid data recovery levels are generally over 98%, far exceeding the minimum criteria set by Federal Agencies.\nMonthly, quarterly and annual data reports are provided to the client. Some clients are required to submit copies of the reports to State or Federal agencies.\nQ: Who uses Murray and Trettel’s Environmental Applications Division services?\nA: Nuclear utilities are Murray and Trettel’s Environmental Applications Division’s largest clients. The Nuclear Regulatory Commission requires that weather be constantly monitored at all nuclear facilities. There are 66 commercial nuclear plants operating in the United States. The Environmental Applications Division provides service to 13 of these plants, located in five states.\nOther Environmental Applications Division clients include natural gas clients, fossil fuel burning electric facilities, a refinery, a large steel plant and a water reclamation district. The Environmental Applications Division has also been involved in atmospheric dispersion modeling.\nQ: Besides maintaining meteorological monitoring equipment, what other services does the Environmental Applications Division offer?\nA: Current environmental projects include calibrating and auditing sulfur dioxide monitoring equipment, auditing opacity monitoring equipment, installing and maintaining thermal and pH probes located in waterways, and providing wind data for a proposed airport.","|Atmospheric chemistry (category)|\n|Tropical cyclone (category)|\n|Global warming (category) · (portal)|\n|Glossary of meteorology|\nAtmospheric sciences are the study of the Earth's atmosphere, its processes, the effects other systems have on the atmosphere, and the effects of the atmosphere on these other systems. Meteorology includes atmospheric chemistry and atmospheric physics with a major focus on weather forecasting. Climatology is the study of atmospheric changes (both long and short-term) that define average climates and their change over time, due to both natural and anthropogenic climate variability. Aeronomy is the study of the upper layers of the atmosphere, where dissociation and ionization are important. Atmospheric science has been extended to the field of planetary science and the study of the atmospheres of the planets of the solar system.\nThe term aerology (from Greek ἀήρ, aēr, \"air\"; and -λογία, -logia) is sometimes used as an alternative term for the study of Earth's atmosphere. Early pioneers in the field include Léon Teisserenc de Bort and Richard Assmann.\nAtmospheric chemistry is a branch of atmospheric science in which the chemistry of the Earth's atmosphere and that of other planets is studied. It is a multidisciplinary field of research and draws on environmental chemistry, physics, meteorology, computer modeling, oceanography, geology and volcanology and other disciplines. Research is increasingly connected with other areas of study such as climatology.\nThe composition and chemistry of the atmosphere is of importance for several reasons, but primarily because of the interactions between the atmosphere and living organisms. The composition of the Earth's atmosphere has been changed by human activity and some of these changes are harmful to human health, crops and ecosystems. Examples of problems which have been addressed by atmospheric chemistry include acid rain, photochemical smog and global warming. Atmospheric chemistry seeks to understand the causes of these problems, and by obtaining a theoretical understanding of them, allow possible solutions to be tested and the effects of changes in government policy evaluated.\nAtmospheric dynamics involves the study of observations and theory dealing with all motion systems of meteorological importance. Common topics studied include diverse phenomena such as thunderstorms, tornadoes, gravity waves, tropical cyclones, extratropical cyclones, jet streams, and global-scale circulations. The goal of dynamical studies is to explain the observed circulations on the basis of fundamental principles from physics. The objectives of such studies incorporate improving weather forecasting, developing methods for predicting seasonal and interannual climate fluctuations, and understanding the implications of human-induced perturbations (e.g., increased carbon dioxide concentrations or depletion of the ozone layer) on the global climate.\nAtmospheric physics is the application of physics to the study of the atmosphere. Atmospheric physicists attempt to model Earth's atmosphere and the atmospheres of the other planets using fluid flow equations, chemical models, radiation balancing, and energy transfer processes in the atmosphere and underlying oceans. In order to model weather systems, atmospheric physicists employ elements of scattering theory, wave propagation models, cloud physics, statistical mechanics and spatial statistics, each of which incorporate high levels of mathematics and physics. Atmospheric physics has close links to meteorology and climatology and also covers the design and construction of instruments for studying the atmosphere and the interpretation of the data they provide, including remote sensing instruments.\nIn the United Kingdom, atmospheric studies are underpinned by the Meteorological Office. Divisions of the U.S. National Oceanic and Atmospheric Administration (NOAA) oversee research projects and weather modeling involving atmospheric physics. The U.S. National Astronomy and Ionosphere Center also carries out studies of the high atmosphere.\nIn contrast to meteorology, which studies short term weather systems lasting up to a few weeks, climatology studies the frequency and trends of those systems. It studies the periodicity of weather events over years to millennia, as well as changes in long-term average weather patterns, in relation to atmospheric conditions. Climatologists, those who practice climatology, study both the nature of climates – local, regional or global – and the natural or human-induced factors that cause climates to change. Climatology considers the past and can help predict future climate change.\nPhenomena of climatological interest include the atmospheric boundary layer, circulation patterns, heat transfer (radiative, convective and latent), interactions between the atmosphere and the oceans and land surface (particularly vegetation, land use and topography), and the chemical and physical composition of the atmosphere. Related disciplines include astrophysics, atmospheric physics, chemistry, ecology, physical geography, geology, geophysics, glaciology, hydrology, oceanography, and volcanology.\nAtmospheres on other celestial bodies\nAll of the Solar System's planets have atmospheres. This is because their gravity is strong enough to keep gaseous particles close to the surface. Larger gas giants are massive enough to keep large amounts of the light gases hydrogen and helium close by, while the smaller planets lose these gases into space. The composition of the Earth's atmosphere is different from the other planets because the various life processes that have transpired on the planet have introduced free molecular oxygen. Much of Mercury's atmosphere has been blasted away by the solar wind. The only moon that has retained a dense atmosphere is Titan. There is a thin atmosphere on Triton, and a trace of an atmosphere on the Moon.\nPlanetary atmospheres are affected by the varying degrees of energy received from either the Sun or their interiors, leading to the formation of dynamic weather systems such as hurricanes, (on Earth), planet-wide dust storms (on Mars), an Earth-sized anticyclone on Jupiter (called the Great Red Spot), and holes in the atmosphere (on Neptune). At least one extrasolar planet, HD 189733 b, has been claimed to possess such a weather system, similar to the Great Red Spot but twice as large.\nHot Jupiters have been shown to be losing their atmospheres into space due to stellar radiation, much like the tails of comets. These planets may have vast differences in temperature between their day and night sides which produce supersonic winds, although the day and night sides of HD 189733b appear to have very similar temperatures, indicating that planet's atmosphere effectively redistributes the star's energy around the planet.\n- Ultraviolet radiation in the solar system By Manuel Vázquez, Arnold Hanslmeier\n- University of Washington. Atmospheric Dynamics. Retrieved on 1 June 2007.\n- Sheppard, S. S.; Jewitt, D.; Kleyna, J. (2005). \"An Ultradeep Survey for Irregular Satellites of Uranus: Limits to Completeness\". The Astronomical Journal. 129: 518. arXiv:astro-ph/0410059. Bibcode:2005AJ....129..518S. doi:10.1086/426329.\n- Zeilik, Michael A.; Gregory, Stephan A. (1998). Introductory Astronomy & Astrophysics (4th ed.). Saunders College Publishing. p. 67. ISBN 0-03-006228-4.\n- Hunten D. M., Shemansky D. E., Morgan T. H. (1988), The Mercury atmosphere, In: Mercury (A89-43751 19–91). University of Arizona Press, pp. 562–612\n- Harvey, Samantha (1 May 2006). \"Weather, Weather, Everywhere?\". NASA. Archived from the original on 8 August 2007. Retrieved 9 September 2007.\n- Knutson, Heather A.; Charbonneau, David; Allen, Lori E.; Fortney, Jonathan J. (2007). \"A map of the day-night contrast of the extrasolar planet HD 189733b\". Nature. 447 (7141): 183–6. arXiv:0705.0993. Bibcode:2007Natur.447..183K. doi:10.1038/nature05782. PMID 17495920. (Related press release)\n- Weaver, D.; Villard, R. (31 January 2007). \"Hubble Probes Layer-cake Structure of Alien World's Atmosphere\". University of Arizona, Lunar and Planetary Laboratory (Press Release). Archived from the original on 8 August 2007. Retrieved 15 August 2007.\n- Ballester, Gilda E.; Sing, David K.; Herbert, Floyd (2007). \"The signature of hot hydrogen in the atmosphere of the extrasolar planet HD 209458b\". Nature. 445 (7127): 511–4. Bibcode:2007Natur.445..511B. doi:10.1038/nature05525. PMID 17268463.\n- Harrington, Jason; Hansen, Brad M.; Luszcz, Statia H.; Seager, Sara (2006). \"The phase-dependent infrared brightness of the extrasolar planet Andromeda b\". Science. 314 (5799): 623–6. arXiv:astro-ph/0610491. Bibcode:2006Sci...314..623H. doi:10.1126/science.1133904. PMID 17038587. (Related press release)\n- Atmospheric fluid dynamics applied to weather maps – Principles such as Advection, Deformation and Vorticity\n- National Center for Atmospheric Research (NCAR) Archives, documents the history of the atmospheric sciences"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:c38fb0df-367c-452a-a97f-2a6d4d9d62e3>","<urn:uuid:527a58c5-de11-42b5-97c8-9e3deb41b063>"],"error":null}
{"question":"Por favor, compare los requisitos de almacenamiento de Sirturo y Anzemet - ¿tienen condiciones especiales de conservación?","answer":"Only Sirturo has specific storage requirements explicitly mentioned in the documents. Sirturo should be stored at 77°F (25°C), kept in its original container, and protected from light. The documents do not mention any specific storage requirements for Anzemet tablets.","context":["\"The food service industry can help prevent norovirus outbreaks.\nMost norovirus outbreaks from contaminated food occur in food service settings, according to a Vital Signs (http://www.cdc.gov/vitalsigns) report by the Centers fo\"...\n(ser toor' oh)\nRead this Medication Guide before you start taking SIRTURO™ and each time you get a refill. There may be new information. This information does not take the place of talking with your healthcare provider about your medical condition or your treatment.\nWHAT IS THE MOST IMPORTANT INFORMATION I SHOULD KNOW ABOUT SIRTURO™?\nSIRTURO™ is an antibiotic prescription medicine used to treat multi-drug resistant tuberculosis (TB) of the lungs in people with limited treatment options. Multi-drug resistant tuberculosis is a serious disease that can result in death and for which there are few treatment choices. More people treated with SIRTURO™ cleared TB from their sputum compared to people who did not receive SIRTURO™.\nIt is important to complete the full course of treatment with SIRTURO™ and your other TB medicines and not skip doses. Skipping doses may decrease the effectiveness of the treatment and increase the likelihood that your TB disease will not be treatable by SIRTURO™ or other medicines.\nSIRTURO™ can cause serious side effects.\n- In one clinical trial, more deaths were seen in people who were treated with SIRTURO™ compared to people who did not receive SIRTURO™.\n- Heart rhythm problems can happen with SIRTURO™.\n- Talk with your healthcare provider about whether SIRTURO™ is right for you.\nWHAT IS SIRTURO™?\nSIRTURO™ is an antibiotic prescription medicine used to treat resistant tuberculosis (TB) of the lungs.\nIt is not known if SIRTURO™ is safe and effective in:\n- people who do not have active TB\n- people who have TB that is not resistant to antibiotics\n- people who have types of TB other than TB of the lungs\n- people who have an infection caused by a bacteria other than TB\n- children under 18 years of age\nBefore you take SIRTURO™, tell your healthcare provider if you:\n- have had an abnormal heart rhythm (ECG) or other heart problems.\n- anyone in your family has or has had a heart problem called “congenital long QT syndrome”.\n- have liver or kidney problems or any other medical conditions, including HIV infection.\n- are pregnant or plan to become pregnant. It is not known if SIRTURO™ will harm your unborn baby.\n- are breastfeeding or plan to breastfeed. It is not known if SIRTURO™ passes into breast milk. You and your healthcare provider should decide if you will take SIRTURO™ or breastfeed.\nTell your healthcare provider about all the medicines you take, including prescription and nonprescription medicines, vitamins, and herbal supplements.\nHOW SHOULD I TAKE SIRTURO™?\n- SIRTURO™ must always be taken with other medicines to treat TB. Your healthcare provider will decide which other medicines you should take with SIRTURO™.\n- Take SIRTURO™ with food. Swallow the tablets whole with water.\n- Take SIRTURO™ exactly as your healthcare provider tells you to take it. Take SIRTURO™ for a total of 24 weeks.\nWeek 1 and Week 2:\nTake 400 mg (4 tablets) 1 time each day.\nWeek 3 to Week 24:\n- Take 200 mg (2 tablets) a day 3 times a week.\n- For example, you may take SIRTURO™ on Monday, Wednesday and Friday every week.\n- Do not take more than 600 mg (6 tablets) SIRTURO™ during a 7 day period.\n- You may need to take your other TB medicines for longer than 24 weeks. Check with your healthcare provider.\n- Do not skip SIRTURO™ doses. If you skip doses, or do not complete the total 24 weeks of SIRTURO™ your treatment may not work as well and your TB may be harder to treat.\n- If you take more SIRTURO™ than you should, talk to a healthcare provider right away.\n- If you miss your SIRTURO™ dose\nduring Week 1 or Week 2:\n- Do not take a double dose to make up for the missed dose. Take the next dose as usual.\nIf you miss your SIRTURO™ dose during Week 3 to Week 24:\n- Take the missed dose as soon as possible and resume the three times a week schedule.\n- Do not take more than 600 mg (6 tablets) in total during a 7 day period. You should take 2 tablets per day, three time a week.\nIf you miss a dose and you are not sure what to do, talk to your healthcare provider.\n- Do not stop taking SIRTURO™ without first talking to your healthcare provider.\nWHAT SHOULD I AVOID WHILE TAKING SIRTURO™?\nYou should not drink alcohol while taking SIRTURO™.\nWHAT ARE THE POSSIBLE SIDE EFFECTS OF SIRTURO™?\nSIRTURO™ may cause serious side effects, including:\n- See “What is the most important information I should know about SIRTURO™?”\n- serious heart rhythm changes (QT prolongation). Tell your healthcare provider right away if you have a change in your heartbeat (a fast or irregular heartbeat), or if you faint.\n- liver problems (hepatotoxicity). Call your healthcare provider right away if you have unexplained symptoms such as nausea or vomiting, stomach pain, fever, weakness, itching, unusual tiredness, loss of appetite, light colored bowel movements, dark colored urine, yellowing of your skin or the white of your eyes.\nThese are not all the possible side effects of SIRTURO™.For more information, ask your healthcare provider or pharmacist.\nCall your doctor for medical advice about side effects. You may report side effects to FDA at 1800-FDA-1088.\nHOW SHOULD I STORE SIRTURO™?\n- Store SIRTURO™ at 77°F (25°C).\n- Keep SIRTURO™ in the original container, and keep SIRTURO™ out of light.\nGENERAL INFORMATION ABOUT THE SAFE AND EFFECTIVE USE OF SIRTURO™.\n- This Medication Guide summarizes the most important information about SIRTURO™.If you would like more information, talk to your healthcare provider. You can ask your pharmacist or healthcare provider for information about SIRTURO™ that is written for health professionals.\nWHAT ARE THE INGREDIENTS IN SIRTURO™?\nActive ingredient: bedaquiline.\nInactive ingredients: colloidal anhydrous silica, cornstarch, croscarmellose sodium, hypromellose 2910, lactose monohydrate, magnesium stearate, microcrystalline cellulose, polysorbate 20, purified water (removed during processing).\nLast reviewed on RxList: 11/3/2014\nThis monograph has been modified to include the generic and brand name in many instances.\nAdditional Sirturo Information\nReport Problems to the Food and Drug Administration\nYou are encouraged to report negative side effects of prescription drugs to the FDA. Visit the FDA MedWatch website or call 1-800-FDA-1088.\nFind out what women really need.","IMPORTANT SAFETY INFORMATION\nDownload Full Prescribing Information\nAnzemet® (dolasetron mesylate) Tablets\nWHAT ARE ANZEMET TABLETS USED FOR?\nAnzemet® (dolasetron mesylate) Tablets are used to prevent nausea and vomiting associated with chemotherapy.\nWHEN SHOULD I NOT TAKE THE DRUG?\nDO NOT USE Anzemet Tablets if you have ever had a bad reaction to any form of Anzemet (tablets or injection) in the past.\nWHAT WARNINGS SHOULD I KNOW ABOUT ANZEMET TABLETS?\nQT, PR, and QRS Interval Prolongation\nThe QT, PR, and QRS Intervals reflect separate measures of time in the heart’s electrical cycle.\nQT Interval: A lengthened QT Interval can be associated with rapid heartbeat and can increase your risk for sudden death. Anzemet Tablets can increase the QT Interval in a dose-dependent fashion. If you are prescribed Anzemet Tablets and have congestive heart failure, very slow heart rate, liver impairment, or if you are older, your doctor may suggest ECG monitoring.\nPR and QRS Interval: Lengthened PR and QRS Intervals are associated with heart block, heart attack, irregular heart beat, and serious slow heart rate in adults and children. Anzemet Tablets can increase these conditions in a dose-dependent fashion and can result in death. If you have any of these conditions, or if you are taking other drugs that impact the PR or QRS Intervals, your doctor will require ECG monitoring.\nSerotonin syndrome occurs when there is excess serotonin in the body. This can have serious, potentially life-threatening and sometime fatal consequences. Anzemet Tablets can cause excess serotonin levels in the body. If you are taking Anzemet Tablets, watch for the following symptoms:\n- Change in your mental status\n- Changes in basic body functions such as blood pressure, heart rate, sweating and digestion\n- Lack of coordination, or overactive reflexes\nIf you are taking Anzemet Tablets and any other serotogenic drugs such as certain antidepressants, then you may be at a higher risk for developing serotonin syndrome. If any of these symptoms occur, stop taking Anzemet and seek emergency treatment.\nSafety and effectiveness in pediatric patients (2 years and older) is based on studies in adults. Safety and effectiveness in pediatric patients under 2 years of age have not been established.\nAnzemet Tablets can be used with children old enough to swallow tablets. For children that do not meet the weight requirements for taking Anzemet 100 mg Tablets or children unable to swallow, Anzemet Injection solution may be mixed into apple or apple-grape juice for oral dosing. (See Anzemet Injection).\nOlder patients are at particular risk for prolongation of the PR, QRS, and QT interval. Caution should be exercised and ECG monitoring should be performed when using Anzemet Tablets in older patients.\nWHAT SHOULD I TELL MY HEALTHCARE PROVIDER?\nIf you are prescribed Anzemet Tablets and have congestive heart failure, very slow heart rate, QT syndrome, liver impairment, have hypokalemia (potassium deficiency in bloodstream) or hypo-magnesemia (magnesium deficiency in bloodstream), take diuretics, anti-arrhythmic drugs or other drugs which lead to QT prolongation, or take high doses of anthracycline, your doctor will monitor you closely.\nYou should tell your doctor if you are pregnant, thinking of becoming pregnant, or nursing.\nAnzemet Tablets have been shown to cause liver cancer in mice at 3,6, and 12 times the recommended doses.\nAnzemet Tablets do not have an effect on fertility and reproduction in rats at up to 9 times the recommended dose.\nHowever there have been no studies in pregnant women, so your doctor will only prescribe Anzemet Tablets if clearly needed.\nIt is not known whether Anzemet Tablets pass through to human milk. If you are nursing or considering nursing, tell your doctor.\nWHAT OTHER MEDICATIONS MIGHT INTERACT WITH ANZEMET TABLETS?\nVery few drugs interact with Anzemet Tablets. However, if you are going to take Anzemet Tablets with any other drugs, make sure you tell your doctor. Your doctor may monitor you if you are taking Anzemet Tablets with certain chemotherapy drugs, drugs that may cause serotonin syndrome, or drugs that affect your QT Interval and/or cause low blood potassium or magnesium.\nWHAT ARE THE SIDE EFFECTS OF ANZEMET TABLETS?\nGet immediate medical help if you notice any of the following side effects:\n- Change in your heart rate\n- Irregular heartbeat, weak pulse, slow breathing\n- Swelling in your hands or feet\n- Headache with chest pain and severe dizziness, fainting, fast or slow pounding heartbeats\n- Urinating less than usual or not at all\n- Agitation, hallucinations, delirium, and coma\n- Muscle stiffness, muscle spasms, overactive reflexes, lack of coordination\n- Seizures, with or without nausea, vomiting, diarrhea\nLess serious side effects may include:\n- Mild headache\n- Tired feeling, mild dizziness\n- Diarrhea, constipation, upset stomach, loss of appetite\n- Chills, shivering, numbness or tingly feeling\n- Fever, sweating\n- Joint or muscle pain\nThe most common side effects reported in patients taking Anzemet Tablets were headache, fatigue, diarrhea, abnormally slow heartbeat (bradycardia), dizziness, pain, abnormally rapid heartbeat (tachycardia), indigestion, and chills/shivering.\nTHIS IS NOT A COMPLETE LIST OF SIDE EFFECTS AND OTHERS MAY OCCUR. TELL YOUR DOCTOR ABOUT ALL MEDICINES YOU USE. THIS INCLUDES PRESCRIPTION, OVER-THE-COUNTER, VITAMIN AND HERBAL PRODUCTS. DO NOT START A NEW MEDICATION WITHOUT TELLING YOUR DOCTOR. YOU MAY REPORT SIDE EFFECTS TO VALIDUS PHARMACEUTICALS LLC AT 1-866-982-5438 (1-866-9VALIDUS).\nThere is no known specific antidote for dolasetron mesylate, and patients with suspected overdose should be managed with supportive therapy. If overdose is suspected, seek emergency medical assistance or call the Poison Help Line at 1-800-222-1222."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:875d637c-00e1-4ede-a0de-495332509181>","<urn:uuid:becca328-c7b7-4bc3-99dd-fc1d6296c12e>"],"error":null}
{"question":"How has Prague's geographical location and economic characteristics influenced its development as a modern European capital, particularly in terms of international trade and tourism?","answer":"Prague's central location in Europe has historically made it an important crossroads of trade and culture. Today, the city maintains very high standards with modern conference venues, hotels, and restaurants. Its development as a major destination is supported by its stable democratic government, pro-market economy, stable currency, and skilled workforce, making it an attractive location for international conferences and business meetings. The city is now the fourteenth-largest in the European Union with approximately 1.3 million inhabitants, while its metropolitan area has nearly 2 million people. Its appeal to tourists is enhanced by its architectural monuments, traditional hospitality, excellent beer, and unique mix of Czech, German, and Jewish cultures.","context":["Prague, in Czech – Praha, is the capital and largest city of the Czech Republic and fourteenth-largest city in the European Union. It is also the historical capital of Bohemia proper. Situated in the north-west of the country on the Vltava river, the city is home to about 1.3 million people while its metropolitan area is estimated to have a population of nearly 2 million. The city has a temperate oceanic climate with warm summers and chilly winters.\nPrague offers a compact city centre, a fascinating centuries – long history with splendid examples of Romanesque, Gothic, Baroque, Renaissance and Art Nouveau architecture.\nThe City of Prague\n“The Mother of Towns”, “The Golden City”, or “The City of a Hundred Spires”, these are just a few of the many attributes that the Czech metropolis nestling above the river Vltava has earned for itself. Prague as a major destination of visitors arriving in the Czech Republic, with its appeal of architectural monuments of all styles, the traditional hospitality of its people and the excellent beer served by Czech pubs, as well as the remarkable mix of Czech, German and Jewish cultures, is considered one of the most beautiful cities, and not just in Europe.\nThe city has a designated UNESCO World Cultural and Natural Heritage area of more than 8 sq km, over 100 theatres, concert halls, galleries etc. For lovers of historical monuments, Prague is, literally, a paradise.\nThe grand Baroque palaces of the Hradčany quarter join forces to form a monumental gateway to Prague Castle, with its magnificent Gothic cathedral consecrated to St Vitus. The Lesser Quarter boasts a profusion of intimate corners and pleasant restaurants; fine burghers houses blend with splendid aristocratic palaces and charming gardens here.\nThe Old Town of Prague offers its visitors a network of twisting medieval lanes and the Old Town Square. When exploring the remains of the former Jewish town, its synagogues, the Old Jewish Cemetery or the extensive collections of the Jewish Museum (one of the most valuable to be found in Europe), you are certain to experience a mysterious spiritual atmosphere. The Charles Bridge, the most beautiful promenade site in Prague, is alive with street artists and musicians.\nMore about Prague\nPrague is a city of very high standards, offering both historical and modern conference venues, hotels, restaurants and places of interest. Thanks to its location in the centre of Europe, Prague has always been an important crossroads of trade and culture. In the course of its thousand-year history, Prague has always been the political, cultural, and business centre of the country.\nThe dominant feature of the city is Prague Castle, which houses the gothic St. Vitus’s Cathedral and which had been the seat of Czech kings since 1087, until 1918 when it became the seat of presidents of the Czechoslovak Republic, and since 1993 it has been the seat of the president of the Czech Republic. Prague has one of the oldest universities in Europe; the Charles University which was founded in 1348.\nYou can also walk around the places that are connected with the lives of more than 100 personalities famous world-wide. These and many more people have lived in Prague: King Charles IV, Rudolph II, J. A. Comenius (Komenský), W. A. Mozart, Franz Kafka, A. Einstein, A. Pick, M. Curie-Sklodowski, Madeleine Albright, Václav Havel, Miloš Forman...\nWith a widely opened pro-market economy, stable democratic government, stable currency and well-known working skills of the people, this country provides a great opportunity to host a conference in order to meet scientists and business people from all over the world.\nMore info at Official Tourist Website for Prague.\nOther important websites"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:01cc7e64-596e-4ac7-88ce-cdbac493b7d3>"],"error":null}
{"question":"Quick comparison - what's the difference between patterns in Steampunk and Art Deco design?","answer":"Steampunk patterns draw primarily from Victorian/Edwardian era mechanical functionality and steam-powered machinery, while Art Deco patterns feature distinctive geometric shapes like zigzags, ziggurats, chevrons, sunbeams, shells and fans. Art Deco patterns were influenced by exotic discoveries like Tutankhamun's tomb and Aztec motifs, whereas Steampunk patterns reflect industrial revolution aesthetics combined with futuristic scientific elements.","context":["What is Steampunk?\nWhat is Steampunk?\nThis is our definition of ‘steampunk’: ‘steampunk’ refers to a fusion of steam-era aesthetics and/or principles with futuristic ideas about human existence in the space-time continuum, informed by the possibilities offered by ongoing scientific endeavour when joined to the imagination. Steampunk is brought to life by the power of dreamers with nostalgic notions, who are supported by a deep knowledge of past eras, deftly knitting together science-inspired visions of the future.\n|Image Credit: Thomas Veyrat|\nLet’s deconstruct it.\n|JW Keter by Photo by Gemnerd 2011|\nSteampunk is a neologism which (arguably) was coined in the 1980’s. It apparently arose from the mind of science fiction author KW Jeter who was searching for an accurate description of his works.\nBut this is an origin story and if you’re interested, you can read about it here. It still does not flesh out the meaning of ‘steampunk’ as it exists now after the pummeling we culture vultures give it. Thanks to us makers and appreciators and cosplayers and consumers, ‘steampunk’ is now a richly contested term. Let’s contest it a little more!\n|Image Credit: Shanna Jones Photography Yatzer Truth Coffee Shop Cape Town. Used under Creative Commons 4.0|\nThe term ‘steampunk’ is largely used as an adjective.\nSteampunk is used to describe speculative worlds, environments, communities and the attributes of people and things that exist within these. Those who adopt steampunk inspired identities often adopt affectations from their favourite eras: manners, mannerisms, speech patterns and communication styles. It adds to the flavor and experience. In delightful ways, it cross-fertilises the past with the digital age.\nTo be classified as ‘steampunk’, primary aesthetic inspiration is drawn from the Edwardian era (1901 – 1915), Victorian era (1837 – 1901) and/or Georgian era (1790 – 1831). At times, the aesthetic elements are buttressed by other elements relating to these eras. These include – but are not limited to – era-inspired mechanical functionality, a sense that time moved at a slower pace yet coincided with a burgeoning taste for faster-than-ever-before land-based travel with all its trappings.\n|‘A Private View at the Royal Academy’ by William Powell Frith|\nLet’s look a little closer at why ‘steam’ was chosen as the first constituent component of the neologism ‘steampunk’. Steam power in and of itself started being harnessed for use as far back as the 1st century AD. It took many iterations and patents by various people over hundreds of years for steam to evolve as a means of propulsion on an industrial scale and to see steam technologies adopted and adapted to service the mass market. You can read more about the steam power timeline here.\nSteam power was gearing up to full strength during the Georgian era. By the Victorian era, we see throngs of people travel on steam trains. Imagine how unbelievable it was for people who had largely travelled by horse and buggy up until that time – it took days punctuated by overnight stops at inns to travel 50 kilometres. These people were then able to travel that same distance by steam train in a single day!\n|Image Credit: Victorian Machine Shop by Les Chatfield, Flickr. Used under Creative Commons 2.0.|\nAnd yet, the mechanisation of labour heralded the decline in what was once skilled labouring (e.g. blacksmithing, millwork) and increased rates of impoverishment amongst the working classes. And as the working classes sought a new life, pioneers were born as they set out to make a new life across the seas and on newly colonised lands. Hence, the ‘Wild West’ aesthetic often infiltrates the steampunk genre. Let’s leave aside the impact of colonisation on first peoples around the planet – just briefly, we shall return to it by the close of this rather long definition.\nBy steam, those Georgians, Victorians and Edwardians were propelled into the future at a rate hithertofore unimaginable. They were time travelers in their own lives.\nNow let’s look at why ‘punk’ is the second part of the word equation. Equally importantly, ‘steampunk’ imaginings are cleverly infused with disruptive, unconventional, bizarre and anti-establishment influences. A handy sobriquet for this cluster of ideas is ‘punk’. Of course, the obvious inspiration was the 1970’s period during which the punk sub-culture arose. Punk sub-culture holds a place in the popular imagination as being a community of loosely associated (or disassociated, disaffected) individuals who embraced many rebellious, shocking and authority-defying ideas. Embedded in the idea of being a punk was the responsibility to offer a serial challenge to societal norms through alternative thought and political systems such as anarchy. Why? Thatcherism and Reagonomics were taking hold across the developed world and a new era of disenfranchised young working class people were created.\nPunks questioned handed down truths of the past. They looked for a different future aimed at liberating them from their concerns. Importantly, key figures in the punk movement voiced their concerns through many artistic channels such as music, art and design. If you want to read about how punk was defined by 25 key punk era influencers, you can read about it here.\nSo what of the modern steampunk-inspired individual and the many and varied accoutrements that comprise a steampunk lifestyle? Why do we say there is a ‘steampunk way of life’?\n|Jules Verne, circa 1878 – photograph restored by Félix Nadar|\nSteampunk is about valuing the era inspired elements and principles of the Georgian, Victorian and Edwardian eras. Its heroes-at-arms are novelists (eg. HG Wells, Jules Verne) and scientists (eg. Nikola Tesla, Thomas Edison). Forward thinking creative and business icons who came of age in the 1970’s punk era left an indelible impression on the steampunk imagination (e.g. David Bowie, Vivienne Westwood) by virtue of their style and unconventional thinking which has led to enduring creations. These collectively have become a monument to the possibilities inherent in questioning and critiquing the status quo.\n|Nikola Tesla sits in front of a spiral coil from a high voltage transformer at his Houston St, New York laboratory in 1896|\n|Image Credit: Portrait of Vivienne Westwood by Biagio Black. Used under Creative Commons 1.0.|\nAnd because we are living in this day and age where science continues to make great leaps forward toward which we eagerly hurtle in search of preserving our planet, our inspirational figures include technological innovators (e.g. Elon Musk and Steve Jobs) and environmentalists (eg. David Suzuki, Rachel Carson). Now is the time where the long ignored voices of first peoples around the globe start assuming their rightful place as millennia of land management protocols and cultural knowledge systems are finally recognised and start informing our ecological survival plan, often hand in hand with new technologies. Steampunk has the capacity, as it evolves, to offer a respectful place of equality within it to First Peoples and the precious offerings they have guarded down the years so as to preserve them for posterity.\n|Image Credit: Elon Musk at the Heisenberg Media Summit, 2013 by Dan Taylor/Heisenberg Media. Used under Creative Commons 2.0.|\n|Image Credit: Rachel Carson, 1940 Fish and Wildlife Service employee photo.|\nThe inspiration drawn from the past is as much about valuing the eras themselves as it is about valuing our ability to question, imagine and create new futures utilising existing, new and emergent technologies and ideas. The steampunk life is disruptive, it is fulsome, it embraces ideas from clever people of conscience across cultures; it races toward the future whilst paying homage to the time periods which birthed it. The steampunk way of life picks up where these eras purportedly ended, uses the latent drives within them which are still evident in era-specific artefacts and ideas, to invigorate new ideas and newly acquired scientific and long buried cultural knowledge about our possible future.\nIt is a future where a body-energy harnessing corset may charge your iPhone.\n|X-rays of women in corsets|\nIt is a future where goggles using VR lenses used at a festival such as Glimmerdark could turn a film installation into an immersive travel experience into the past – Star Trek may have popularised the holodeck concept, but we are on the cusp of creating them.\nScience meets imagination and is married to possibility in the steampunk way of life. It has the potential to create dystopias but it also has the potential to create a better world which acknowledges and celebrates the past, using it to invent our best selves and a sustainable environment to support the next iteration of the planet.","Art Deco Patterns, Colors, and Materials in Interior Design\nConceived in the 1920s, for a hundred years already, Art Deco has kept its position as one of the most renowned and influential styles in architecture and interior design. Bold, sleek, glamorous—the style manifests in luxurious materials, rich colors, decadent details. Anyone in love with the “Great Gatsby” atmosphere, or magic of early Hollywood, would enjoy Art Deco patterns and accents in interior design.\nLet’s look at how to add style and functionality to an open-floor apartment with the use of Art Deco patterns, motifs, and details.\nArt Deco origins\nArt Deco is a child of the Interwar period, absorbing the zeitgeist of post-WWI France and forming a visual representation of its dreams for progress and prosperity. The firm base of Art Deco is woven with other artistic movements of the time: Cubism with its strict geometry and angularity; Constructivism adds impetuosity and boldness of the Machine era; Art Nouveau, its closest predecessor, shares floral and animalistic patterns, softness, providing sensations of a gentle flow.\nThe Roaring ‘20s and ‘30s, no doubt, were reflected in Art Deco as the world changed overwhelmingly fast.\n- Technical advances of the Machine Age brought decisive straight lines and bold color contrasts to architecture and costume.\n- Archeological findings from expeditions to previously inaccessible, exotic countries inspired Art Deco patterns, particularly the Pharaoh Tutankhamun’s tomb from Egypt and the Mayan pyramids and Aztec motifs from Central America.\n- With the rise of cinema, the silver screen images and motifs of early Hollywood not only influenced the lifestyle and aspirations of the public but also spread offscreen as a new form of self-representation and self-construction, including into apartments and interior design.\nThe image of an abundant and profuse lifestyle got put on pause with the devastation of WWII, which brought the austerity of raw functionality to design. At that point, Art Deco froze, substantially unchanged from how we know it today: a clear and distinctive style, easy to implement in modern apartment design and interiors.\nArt Deco materials\nThinking about Art Deco materials, one should keep the main features of the style in mind—bold, luxurious, elegant, decadent. At the rise of the movement, in addition to classic materials such as lacquered wood, steel, and glass—all polished with shining, bright finishes—exotic materials were popular: ivory, crystal, animal skins like those of zebra or shark, tortoiseshell, leather, mirrors. Some materials from distant parts of the globe became reachable due to increased facilitation of travel and were highly demanded.\nGlass and steel were widely used as brand new materials at that time. Along with technological progress, the style grew richer with newly available materials like woods and metals processed in a more sophisticated, cost-effective way and plastic. Still, the ideas of progress and the potential of humankind made some think ecologically and choose more lasting and harmless materials, like steel and glass, and avoid using animal fur or bones as decoration.\nArt Deco patterns\nPatterns are a key element of Art Deco. Their use is very widespread, rich, and impressive. Ornaments and motifs are very characteristic and can hardly be mistaken for any other style. Among the most popular Art Deco patterns are\n- Zigzags, ziggurats, chevrons: mathematically accurate geometrical shapes, repeating self-enclosed motifs, and minaret-like stepped ornaments that resemble the Aztec pyramids, succinct and symmetrical.\n- Sunbeams, shells, fans: resembling all at once the mask of Tutankhamun, the birth of Venus, the fan of a dancer or a peacock—the emanation of chicness, luxury, innocence, and power.\nLet’s consider some ideas on how to implement Art Deco patterns to an apartment interior. In general, materials that easily display patterns include fabrics, furniture upholstery, and wallpapers. A more sophisticated option is to have a partition from steel and/or glass made. A partition itself, as a modest zoning tool that divides an open-floor layout in a functional space, is invaluable. Moreover, decorative technologies allow a partition to be a medium for the style.\nAnd mediums can be mixed! Metals like brass, aluminum, or steel with glass in a variety of finishes can create a unique Art Deco object. Steel patterns like stripes, chevrons, or stepped lines can be applied to glass partitions. Another option is a whole-metal partition with laser-cut ornaments. The number of options is only limited to one’s imagination.\nIn terms of colors, Art Deco loves striking contrasts and subtle shading; rich and vivid colors are widely used. Reds, greens, blues, blacks, pinks—all conform organically with metal, crystal, silver, and mirrors. Also, for a softer effect associated with bedrooms and dining rooms, there are neutral Art Deco color schemes. Pastel tones, beiges, creams go perfectly with lacquered wood or any polished glass surface. A variety of glass finishes are appropriate. Use any of the colors and effects named above to complement a functional glass object in the apartment, from a coffee table to a transom.\nAdditionally, glass partitions are used to solve several practical concerns of an open-concept layout. Frosted and satin glass finishes on a partition add privacy and provide distinct zoning. With tinted glass, one can create rich color accents, while textured glass can subtly add more Art Deco patterns.\nElegant and luxurious, Art Deco features in an apartment create high contrast and boldness in the space. One can enjoy a succinct monochrome palette, plenty of vivid color accents, meticulous geometrical patterns, and roundish or streamlined motifs. Just bringing some Art Deco objects into an apartment can create an expensive, classy look. And Art Deco patterns can create a sexy, streamlined effect.\nBoth a decorative and functional option for an open-floor apartment is steel and/or glass partitions. With the use of glass finishes and laser-cut technology, a partition design can include Art Deco patterns in the preferred color scheme. A partition can become an influential statement in the interior, bold and elegant, a central element for the smaller decorative features to play with."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:fe030e01-fb9f-42b4-8e05-d2e32cbdaf1d>","<urn:uuid:b99ec51b-a12b-4a2b-97e6-f1df6e25cdb8>"],"error":null}
{"question":"How does the optimal pH range for swimming pools compare to the pH of rainwater?","answer":"Swimming pools require a pH range of 7.4 to 7.8, which is slightly alkaline, while rainwater typically has a more acidic pH around 4.0 (similar to tomato juice and classified as acid rain). This means rainwater is significantly more acidic than the optimal swimming pool water.","context":["It's Raining, It's Pouring: Chemical Analysis of Rainwater\n|Time Required||Very Long (1+ months)|\n|Prerequisites||Must be familiar with titration or be able to learn how titration is used in this experiment|\n|Material Availability||Readily available|\n|Cost||Low ($20 - $50)|\nAbstractHere is an interesting project that could be approached from several different scientific angles: Environmental Science, Weather & Atmosphere, Chemistry, or Plant Biology. You can probably think of your own variations to emphasize the scientific area that most interests you.\nThe goal of this project is to assess the water quality of rainwater collected from different geographical areas. The water quality measures used in this project are hardness, pH, and plant growth. Additional measures could be chosen to expand this project.\nAndrew Olson, Ph.D., Science Buddies\nThis project was based on:\n- Allison, J.R., 2003. \"It's Raining, It's Pouring, the Radishes Are Growing: Chemical Analysis of Rainwater for the Nation's Food Production,\" California State Science Fair Project Abstract [accessed May 9, 2006] http://www.usc.edu/CSSF/History/2003/Projects/J0902.pdf.\nCite This PageGeneral citation information is provided here. Be sure to check the formatting, including capitalization, for the method you are using and update your citation, as needed.\nLast edit date: 2018-06-14\nIs the chemistry of rainwater from different geographical regions similar or different? How does rainwater chemistry relate to that of local surface water? How is rainwater chemistry affected by large-scale weather patterns? Does rainwater chemistry affect the growth of plants? These are some of the many questions you could choose to pursue with this project.\nThis project is based on Jonathan Allison's 2003 California State Science Fair entry. Here is how Jonathan summarized his experimental procedure: \"I contacted friends and family from 11 different cities in the United States and asked them if they could help me by collecting rainwater from their city. After they collected it, they shipped it back to me. Then I tested the rainwater for hardness, using the chemical process of titration. Next I tested the rainwater for pH levels. Then I planted radish seeds in potting soil and watered each set of seedlings with rainwater from a different city. I observed, measured and recorded any growth or changes daily for seven days.\" (Allison, 2003)\nWater hardness is a measure of dissolved compounds (e.g., magnesium carbonate, calcium carbonate) in the water. These compounds can precipitate out in boilers and water heaters (scaling). Hard water makes less suds with soap and detergent, so you need to use more soap and detergent to get clothes and dishes clean with hard water. General guidelines for classification of waters are: 0 to 60 mg/L (milligrams per liter) as calcium carbonate is classified as soft; 61 to 120 mg/L as moderately hard; 121 to 180 mg/L as hard; and more than 180 mg/L as very hard (USGS, date unknown).\nFigures 1 and 2 show USGS water hardness data for the continental United States. Figure 1 is a histogram showing the mean hardness data for each of the 344 stations sampled. Figure 2 is a map of the U.S., showing the regional patterns of groundwater hardness. In both cases, the data is from 1975, but the patterns shown have proven to be stable over time.\nFigure 1. Histogram of U.S. groundwater hardness from 344 collection stations (USGS, 1975 data).\nFigure 2. Map of U.S. groundwater hardness from 344 collection stations (USGS, 1975 data).\nAcidity and alkalinity are measured with a logarithmic scale called pH. pH is the negative logarithm of the hydrogen ion concentration:\npH = −log [H+] .\nWhat this equation means is for each 1-unit change in pH, the hydrogen ion concentration changes ten-fold. Pure water has a neutral pH of 7. pH values lower than 7 are acidic, and pH values higher than 7 are alkaline (basic). Table 1 has examples of substances with different pH values (Decelles, 2002; Environment Canada, 2002; EPA, date unknown).\n|pH Value||H+ Concentration\nRelative to Pure Water\n|0||10 000 000||battery acid|\n|1||1 000 000||sulfuric acid|\n|2||100 000||lemon juice, vinegar|\n|3||10 000||orange juice, soda|\n|4||1 000||tomato juice, acid rain|\n|5||100||black coffee, bananas|\n|8||0.1||sea water, eggs|\n|10||0.001||Great Salt Lake, milk of magnesia|\n|11||0.000 1||ammonia solution|\n|12||0.000 01||soapy water|\n|13||0.000 001||bleach, oven cleaner|\n|14||0.000 000 1||liquid drain cleaner|\nFigure 3 shows a map of the average pH of precipitation in the continental U.S. for the year 1992. \"The areas of greatest acidity (lowest pH values) are located in the Northeastern United States. This pattern of high acidity is caused by the large number of cities, the dense population, and the concentration of power and industrial plants in the Northeast. In addition, the prevailing wind direction brings storms and pollution to the Northeast from the Midwest, and dust from the soil and rocks in the Northeastern United States is less likely to neutralize acidity in the rain.\" (USGS, 1997)\nFigure 3. Map of U.S. annual average precipitation pH for 1992. (USGS, 1997).\nMost plants prefer soil that is near neutral pH. There are particular varieties (strawberries, azaleas and rhododendrons, for example) that prefer acidic soil. Soil pH also influences how readily available many soil nutrients are to plants.\nTerms and Concepts\nTo do this project, you should do research that enables you to understand the following terms and concepts:\n- Water hardness\nMore advanced students will also want to understand the following terms and concepts:\n- Wikipedia contributors, 2006. \"Titration,\" Wikipedia, The Free Encyclopedia. Retrieved May 9, 2006, from http://en.wikipedia.org/w/index.php?title=Titration&oldid=51987922.\n- These USGS webpages have information on patterns of water hardness of rivers and acidity in rainwater across the United States.\n- These sites explain the pH scale:\n- Environment Canada, 2002. \"Kids' Corner pH Scale,\" The Green Lane: Acid Rain, Environment Canada website. Retrieved March 14, 2006, from http://www.ec.gc.ca/acidrain/kids.html.\n- Decelles, P., 2002. \"The pH Scale,\" Virtually Biology Course, Basic Chemistry Concepts, Johnson County Community College. Retrieved March 14, 2006, from http://staff.jccc.net/pdecell/chemistry/phscale.html.\n- Review pages on moles:\nPark, J.L., 2004. \"The Mole Table of Contents,\" The ChemTeam, A Tutorial for High School Chemistry. Retrieved May 9, 2006, from http://encyclopedia.kids.net.au/page/mo/Molarity.\n- Allison, J.R., 2003. \"It's Raining, It's Pouring, the Radishes Are Growing: Chemical Analysis of Rainwater for the Nation's Food Production,\" California State Science Fair Project Abstract. Retrieved May 9, 2006, from http://www.usc.edu/CSSF/History/2003/Projects/J0902.pdf.\nNews Feed on This Topic\nMaterials and Equipment\n- This project requires planning ahead. Remember that it will take some time for your volunteers to collect samples and send them to you. You also need to allow time (at least one week) for the plant growth experiment once you have received all of the samples. Start early and make sure your volunteers send their samples in a timely manner!\n- Where to get samples? You will need to obtain rainwater samples from a wide geographical area. Consult the maps in the Introduction for historical patterns of variation. Ask friends and relatives to collect samples for you.\n- How much water do I need? Check your test kit instructions to see how much water is required for each test (usually about 5 ml). You will want to repeat your tests for each sample at least 3 times to assure that your results are consistent. So you'll need a minimum of 30 ml just for testing (best to plan on more). You will also need water for the plant growth experiment. Calculate how much water you will need for plant growth, and add 50 ml for testing purposes. This is how much rainwater each of your volunteers will have to send to you.\n- How should my volunteers collect rainwater samples? Simply putting a jar out on the lawn during a rainstorm is not going to be very efficient. In order to get enough water, your volunteers need a large catchment area. Probably the most straightforward solution is to collect water from the roof, by placing a collection jar underneath a downspout.\n- Make sure your volunteers label the water sample with the date and location from which it was collected.\n- For performing the water quality tests, the simplest method is to use a pre-packaged kit designed for testing aquarium water. There are several different brands available. You should be able to find a choice at a local pet store that sells fish. The kit will say how many water samples it will test. You should be able to find kits to test 50 samples for about $10. The kits you need for this project are:\n- For the plant growth experiment, you will need:\n- Radish seeds, (or other suitable, fast-growing seeds)\n- Small containers (peat pots or seedling trays)\n- Potting soil\n- A measuring device for dispensing water, such as a 10 mL graduated cylinder, which is available from online suppliers such as Carolina Biological, catalog #721610.\nDisclaimer: Science Buddies participates in affiliate programs with Home Science Tools, Amazon.com, Carolina Biological, and Jameco Electronics. Proceeds from the affiliate programs help support Science Buddies, a 501(c)(3) public charity, and keep our resources free for everyone. Our top priority is student learning. If you have any comments (positive or negative) related to purchases you've made for science projects from recommendations on our site, please let us know. Write to us at firstname.lastname@example.org.\n- For the water hardness and pH tests, follow the instructions that come with the water test kit. When titrating samples, it is important to mix the solution well after each drop of test solution is added.\n- For the plant growth portion of the experiment, it is important to keep all of the other growth conditions (sun exposure, soil, temperature, etc.) constant, and to vary only the source of water used for the plants. Be sure to use the same amount of water. Consult the Science Buddies resource, Measuring Plant Growth for methods you can use to quantify differences in growth.\nIf you like this project, you might enjoy exploring these related careers:\n- Does rainwater chemistry in your area vary with weather patterns? Collect samples over several weeks or months, and test the water quality. Keep track of the weather systems that produced the precipitation. Were there variations in the ultimate source of the moisture? Can you correlate these variations with changes in rainwater chemistry?\n- If you live in an urban area, is rainwater chemistry affected by smog? Check the air quality reported in the newspaper for the days that samples were collected. Do you see differences in rainwater chemistry after days with high smog compared to days with cleaner air?\n- For the samples in your study, how does rainwater hardness compare with groundwater hardness? (See Figure 2 in the Introduction, above.) How does the acidity compare to the 1992 U.S. data? (See Figure 3 in the Introduction, above.)\n- Here is a related Science Buddies project you might want to check out:\nAsk an ExpertThe Ask an Expert Forum is intended to be a place where students can go to find answers to science questions that they have been unable to find using other resources. If you have specific questions about your science fair project or science fair, our team of volunteer scientists can help. Our Experts won't do the work for you, but they will make suggestions, offer guidance, and help you troubleshoot.\nAsk an Expert\nNews Feed on This Topic\nLooking for more science fun?\nTry one of our science activities for quick, anytime science explorations. The perfect thing to liven up a rainy day, school vacation, or moment of boredom.Find an Activity\nExplore Our Science Videos\nBuild an Infinity Mirror\nHow to Build a Brushbot\nColorful Melting Ice Ball Patterns - STEM Activity","PH levels in Swimming Pools\nMaintaining a well-balanced pool pH is an important section of children's pool upkeep. Not only does the correct pH protect your material accessories as well as the walls of one's share, the proper pH range is the reason why cycling inside liquid nice in place of an itchy, burning up test. The right range for a swimming pool is a pH of 7.4 to 7.8. If you're testing your share plus the pH registers as also reduced, decide to try the techniques below to improve your pH.\nCheck Your Reagents\nThe very first thing to check as soon as your test indicates a reduced pH is whether or not your test reagents are nevertheless great. Reagents should really be changed annually and may never be combined between test kits. In case the system is old, take to getting a one and evaluating once again.\nInclude Soda Ash\nWhen your pH continues to be reasonable, try incorporating sodium carbonate, generally known as soft drink ash. Never include a lot more than 2 lbs of soft drink ash per 10, 000 gallons of liquid inside share in every solitary therapy. Distribute the soft drink ash on the whole associated with the share area, beginning within the deep end. Make sure the pump is circulating to distribute water after which wait an hour or so or more before testing once again. In the event that pH is still also reduced after an hour, you could add even more soda ash.\nSoda ash may cloud the share water in the beginning, but this would fade-out within each day approximately.\nLook at your Total Alkalinity and Add Baking Soda\nIf the pH inside share continues to drop even after you make an effort to raise it, test thoroughly your total alkalinity. A decreased total alkalinity enables the pH to fluctuate arbitrarily. This number must certanly be within the range of 80 ppm to 150 ppm, so if your test shows that its reasonable, incorporate 1.4 lbs of sodium bicarbonate (baking soft drink) per 10, 000 gallons of liquid within share. This should enhance the complete alkalinity by 10 ppm. Once more, let the liquid flow for an hour or more before assessment once again. Baking soda will raise the pH a little, but it is main effectiveness is in raising total alkalinity. Take care not to add to much cooking soda as it is much harder to lessen total alkalinity than it is to improve it. Soda ash also improve the total alkalinity, so check this quantity after each and every step whenever trying to increase your pH.\nAerate to boost pH\nAnother technique for increasing pH is aeration. When you have any liquid functions, turn all of them in. Point water jets towards the surface or incorporate pipes over jets to direct water towards surface—anything to obtain the water moving around the outer lining. This will drive carbon dioxide from the liquid. Aeration will be to improve the pH inside pool without influencing the full total alkalinity. This procedure can take several times, it is very effective with swimming pools which have otherwise been stubborn in keeping the lowest pH."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:1de76532-160c-492d-8acd-38387fa3fc13>","<urn:uuid:3a588a98-8e35-43dc-a0f8-60aa5ad24870>"],"error":null}
{"question":"How do serving traditions compare between Kentucky burgoo and French beef stews?","answer":"Kentucky burgoo is traditionally served at large gatherings like political rallies, church suppers, and notably at the Kentucky Derby, where it's served after Mint Juleps as a first course in silver cups. French beef stews, on the other hand, are served as main dishes with specific accompaniments - for example, Boeuf Bourguignon is served with buttered noodles or roasted potatoes.","context":["I s it a soup or a stew? A BUR-goo or a bur-GOO?\nAlthough the dish has been a Kentucky crowd-pleaser for more than 150 years, few culinary historians, indeed few Kentuckians, see eye-to-eye on the who, what, when, where, and why of burgoo. And how could they? With so much speculation surrounding burgoo's origin and its exotic name, who's to know what's fact and what's fiction? Get a taste of some hypotheses:\n- Does the word burgoo (and possibly the recipe) originate from the bulgur porridge that sustained sailors on long sea voyages back in the 1700s?\n- Is burgoo a contraction of barbecue and ragoût, two filling but frugal Kentucky dishes prepared in feed-an-army batches that were often served in tandem?\n- Were early burgoos, stirred up in giant iron cauldrons at work camps scattered across the Bluegrass State, such a hodgepodge of game and game birds—\"whatever walked or flew,\" someone joked—they were also called \"road-kill stews\"?\n- Is burgoo a Kentucky spin-off of Brunswick stew, an equally controversial, early, catch-all \"critter\" stew? Virginians and Georgians both claim to have created Brunswick stew and still squabble over bragging rights.\n- Did French chef Gustave Jaubert, while serving Confederate general John Hunt Morgan in 1860, create the first Kentucky burgoo by loading one of those wan work-camp stews with blackbirds? And might \"burgoo\" be how that Frenchman pronounced bird stew?\n- Was the secret ingredient in Jaubert's burgoo, the one that mellowed its unseemly mishmash, a black snake that splattered into the pot from a tree directly above?\nSo many questions with so few answers. What is known is that shortly after the Civil War, Frankfort's Buffalo Trace distillery hired Jaubert to cook for its employees. (Two of his big iron burgoo kettles are still on view at the distillery's Burgoo House.) Before long Jaubert, now called \"the Father of Burgoo,\" was catering events of one sort or another across the state. On November 7, 1897, The New York Times reprinted a Louisville Courier-Journal account of a Jaubert feast that included the serving of both burgoo and barbecue—a culinary tradition that lingers to this day.\n\"The making of good burgoo,\" the article declared, \"is even more difficult than the roasting of the meat and requires more time.…Its ingredients are 400 pounds of beef, six dozen chickens, four dozen rabbits, thirty cans of tomatoes, twenty dozen cans of corn, fifteen bushels of potatoes, and five bushels of onions.\" With ten cooks assisting him, Jaubert slow-simmered 1,000 gallons of burgoo, the story notes. Piece of cake compared to the 6,000 gallons he'd reportedly bubbled up two years earlier for the Grand Army Veterans in Louisville.\n\"Colonel Thomas H. Sherley thought it would be impossible for me to cook on such an immense scale,\" Jaubert said of that event, \"but I told him if he would furnish the provisions I would prepare a meal for one million people. I got through without any trouble.\" Little wonder that Jaubert, though never officially crowned, is considered Kentucky's first \"Burgoo King,\" a title now reserved for the rock-star chefs that headline burgoo cook-offs and festivals in Kentucky as well as in Illinois, Indiana, and West Virginia.\nNo two burgoo meisters— indeed, no two cooks—make burgoo the same way, and all are fiercely protective of their recipes. Some like it hot (no stinting on chile peppers in their renditions), others prefer a burgoo so thick a spoon will stand straight up in the pot, and still others fire up their burgoos at dusk and keep them at a gentle simmer for 24 hours. What most burgoos do have in common, however, are platoon-size proportions as well as some secret something—a hefty splash of bourbon, maybe some innards (kidney or liver, say) for enrichment, or a few trotters, perhaps—and invariably some unexpected, unidentifiable mix of herbs and spices.\nEver since Jaubert, burgoos have been de rigueur at political rallies, church suppers, and family reunions in Kentucky. More important, they're as integral to the Kentucky Derby as the singing of \"My Old Kentucky Home\" and a parade of Thoroughbreds. In 1932, a Bluegrass colt named Burgoo King—yes, Burgoo King—won both the Derby and Preakness, two of the three legs that make up Thoroughbred racing's Triple Crown.\nAccording to Charles Patteson's Kentucky Cooking (1988, HarperCollins), \"Burgoo…midway between a hearty soup and a stew…succeeds the juleps in the guests' cups as a first course.\" At the Kentucky Colonels' barbecue the day after the Derby, those would be silver Mint Julep cups, a far cry from the battered tin mugs used to scoop up burgoo back in work-camp days.\nAnd a final word about the pronunciation of burgoo: The country club and horse-y sets, I'm told, favor BUR-goo, while almost everyone else accents the last syllable.\nBurgoo photo: Kriech-Higdon Photo\nSee More on the Kentucky Derby:\n- Complete Kentucky Derby Package ›\n- The Secrets to a Perfect Mint JulepRecipe Included ›\n- A Guide to Bourbon and Our Favorite Bottles ›","C’est si boeuf\nThree classic French beef stews.\nBeef stew figures heavily in French cooking. A simple stew of beef and carrots is found throughout the country, while the various regions feature similarly prepared stews based on their wines and other local ingredients. Daube Provencale uses anchovy, orange, tomato, and olives, all common in Provence. Boeuf Bourguignon, from Burgundy, must include that area’s famous wine made from the pinot noir grape, and a garnish of pearl onions and mushrooms. Serve with buttered noodles or roasted potatoes.\nBeef Stew with Carrots (Boeuf aux Carottes)\n15 slender carrots (about 2½ pounds)\n3½ to 4 pounds beef chuck roast, trimmed of excess fat and cut into 1½-inch chunks\nSalt and pepper\n2 tablespoons flour\n1 tablespoon olive oil\n4 ounces thick-cut bacon, blanched if desired, cut crosswise into thin strips\n3 large onions, halved and thinly sliced\n4 cloves garlic, minced\n2 teaspoons minced\n3 medium bay leaves\n¼ cup brandy\n½ cup low-sodium chicken broth\n1½ cups dry white wine\n6 tablespoons chopped\nFinely chop 3 carrots and set aside. Cut remaining carrots into pieces about 3 inches long and 1 inch thick and set aside separately. In medium bowl, toss beef with 1 teaspoon each salt and pepper, and flour to coat. In large, heavy-bottomed Dutch oven, heat oil over medium-high heat until it ripples. Reduce heat to medium-low, add bacon, and cook, stirring occasionally, until crisp, about 9 minutes. Remove bacon, drain, and reserve; pour off all but 2 teaspoons of fat into a small bowl and reserve. Return pot to medium-high heat and heat until fat begins to bubble, about 40 seconds. Add half the beef, so that pieces are close together in a single layer but not touching, and cook without moving them until deeply browned on bottom, about 3½ minutes. Turn and cook until second side is deeply browned, 3½ minutes longer; transfer beef to a medium bowl. Add 2 teaspoons reserved bacon fat to pot, and repeat process with remaining beef (reduce heat if drippings begin to burn); transfer to bowl with first batch.\nReduce heat to medium, add 2 teaspoons of reserved bacon fat, and allow to heat for a moment. Add onions, chopped carrots, and ½ teaspoon salt, stir, and cook until onions soften, about 5 minutes. Add garlic, thyme, and bay leaves, and cook, stirring constantly, until fragrant, about 1 minute. Add brandy and chicken broth, increase heat to high, and, using a wooden spoon, scrape bottom of pot until brown film loosens and mixes into liquid. Add wine and reserved beef with accumulated juices, push beef down into liquid, bring to boil, reduce heat to very low, cover, and simmer for 30 minutes. Add large carrot pieces and\n1 teaspoon salt, submerge the carrots, replace cover, and continue to simmer until beef and carrots are tender, about 2 hours longer.\nAdd reserved bacon to pot and continue to simmer for 5 minutes. Remove bay leaves. Taste stew and adjust seasoning with salt and pepper, if necessary. Add 4 tablespoons of parsley, stir to combine, and serve at once, sprinkling each portion with the remaining parsley.\nProvencal Beef Stew (Daube Provencale)\nFollow recipe for Beef Stew With Carrots, making the following changes:\n1) Substitute 4 ounces pancetta, cut into ½-inch pieces, for bacon.\n2) Omit the 12 carrots cut into large pieces and reduce onions to 2, chopping them roughly.\n3) Along with the garlic, thyme, and bay leaves, add 2 tablespoons tomato paste and the zest from 1 large orange, cut into thin strips.\n4) Omit brandy. Substitute medium-bodied red wine, such as cabernet sauvignon, Cotes du Rhone, or zinfandel for the white wine.\n5) Toward end of cooking, add 4 finely chopped anchovy fillets, ½ cup black nicoise olives, pitted and halved, and 1 14½-ounce can diced tomatoes, drained, along with the reserved pancetta, stir to mix, cover the pot, increase heat to medium and cook until olives and tomatoes are heated through, about 5 minutes. Add parsley to pot as above, and serve individual portions garnished with parsley.\nBeef Burgundy (Boeuf Bourguignon) Follow the recipe for Beef Stew with Carrots, making the following changes:\n1) Omit the 12 carrots cut into large pieces and reduce onions to 2, chopping them roughly.\n2) Along with garlic, thyme, and bay leaves, add 1 tablespoon tomato paste.\n3) Omit the ½ cup brandy, increase chicken broth to 1 cup, and substitute 2½ cups Burgundy or pinot noir for the white wine.\n4) Once beef is tender, transfer it to a large bowl and set aside. Strain mixture left in pot, pressing on solids; you should have about 3 cups liquid. Discard solids and return liquid to pot, bring to boil over medium-high heat and cook, stirring occasionally, until sauce is reduced to about 2 cups, about 9 minutes; adjust the heat to low.\n5) While sauce reduces, in a large skillet over medium heat, bring another ¼ cup chicken broth and 2 teaspoons reserved bacon fat to a boil. Add 8 ounces frozen pearl onions and ¼ teaspoon salt, and cook, stirring occasionally, until tender, about 3 minutes. Adjust heat to medium-high and cook, stirring frequently, until liquid evaporates and onions darken slightly, about 3 minutes longer. Add 1 teaspoon reserved bacon fat and 1 pound halved crimini mushrooms, and cook, stirring occasionally, until their liquid evaporates and they brown, about 10 minutes. Add onions and mushrooms to the bowl with the beef. Return skillet to medium-high heat, add ½ cup wine and, using a wooden spoon, scrape bottom of skillet until brown film loosens and mixes into the liquid. Add liquid to the sauce in the pot.\n6) Add beef, onions, mushrooms, reserved bacon, and 1 tablespoon brandy to the sauce in the pot, adjust heat to medium, and cook, stirring occasionally, until heated through, about 10 minutes. Taste stew and adjust seasoning with salt and pepper, if necessary. Add parsley to pot and portions as previously.\nSend comments or suggestions to Adam Ried at email@example.com."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:cb711d58-614b-4179-8931-3d4ed88db0c2>","<urn:uuid:a03cfdb5-b620-4f3e-9316-385f579ea4b7>"],"error":null}
{"question":"Why is Roti Jala called 'Jala' and what does it look like?","answer":"'Jala' is a Malay word meaning 'net' as in fish net. The bread is named this way because of its appearance, which resembles a web of net with randomly scattered holes mingled together.","context":["As for every curry dish, it is best eaten with bread like side dish, mainly because bread soaks up the gravy and it is undeniable that the gravy is what makes any curry or Rendang outstanding in flavor. There are many types of flour dough made products that can be the choice of a curry person and the options are endless from Roti Canai, Murtabak, plain white bread to rice itself. I have chosen Roti Jala for today's dish as I have never made it before despite having bought the mould 5 years ago and I love the turmeric yellow color which goes well with the brown Rendang. 'Jala' is another Malay word that means Net as in fish net and this particular Roti is named so for its particular appearance that resembles a web of net all mingled together with its holes randomly scattered.\nIn the whole process of preparing this dish with its accompanying Roti, I would say the Roti was the one that tested my patience and I found the Roti Jala mould difficult to swirl around and some of its funnel holes simply get clogged after a few rounds of the batter filling. I was about to give up after making the third piece when I suddenly remembered I have a better tool to do the job. I used the usual condiment ketchup bottle that they have at the American Diners to complete the next 10 Roti Jala. It was much easier to handle and squirts out even amount of batter everytime. Still, getting one piece to cook nicely with slight burnt color is not an easy thing to do as each piece has to be made and cooked individually and it takes up a long time to make 12 to feed everyone in my household. Of course, I got the rice cooked too as I wasn't ready to spend another 45 minutes to do another round of 12 more Roti Jala!\nThe results were good and Curry and his father really enjoyed dipping those Roti into the spicy Rendang. Missy E was poking her fingers through all the holes of the Roti and munching away. It will be sometime before she picks up the spicy taste of Rendang though!\nRecipe for Lamb Rendang\n4 cloves garlic, minced\nthumbsize knob of ginger, minced\n2 tsp Chinese 5 spice powder\n15 to 20 dried red chillis\n3 tbsp cumin powder\n2 tbsp coriander powder\n2 tsp galangal powder\n1 tsp tumeric powder\n4 to 5 red shallots, minced\n1 tbsp shrimp paste\n2 tbsp tamarind paste\n4 oz unsweetened dessicated coconut\n2 -1/2 cups of water\n1 can coconut milk\n1)Cut lamb leg into chunks and marinate overnight with garlic, ginger and all the cumin,coriander,galangal, tumeric & 5 spice powder.\n2)Bake/Toast the dessicated coconut in oven till brown and fragrant (350F). Leave to cool.\n3)Heat pot on medium and fry the shallots and dried chillies with oil. Add in the lamb chunks and stir fry for 10 minutes, turning evenly. Add in tamarind paste and shrimp paste an coat evenly. Cook for 5 minutes.\n4)Add in water and simmer for 30 to 45 minutes or till meat is tender and sauce is boiled down to 1/3 of water content.\n5)Add in coconut milk and simmer for 5 minutes.\n6)Add in toasted dessicated coconut by sprinkling to get the dry consistency of your choice.\n7)Serve with boiled rice or Roti of your choice.\nServes: 3 to 5\nRecipe for Roti Jala\nMakes: approximately 12 to 15 Roti Jala (thin)"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:86dc7ace-c08e-4ce7-ac31-c74749afdcc3>"],"error":null}
{"question":"How do urban gardeners and rural farmers differ in pesticide use affecting bees? What expert views exist on this?","answer":"There are conflicting expert views on pesticide use in urban versus rural areas. According to Katia Vincent of Beekind store, city parks, especially areas with eucalyptus trees, are largely safe from pesticides as the blossoms are high out of range of municipal pesticide application. However, Helene Marshall of Marshall's Farm Honey contends that urban areas can be equally unhealthy as farmland, stating that city gardeners also use plenty of pesticides. In rural agricultural areas, the extensive use of pesticides in monocultures is documented as harmful to bee populations, contributing to their declining health and colony collapse disorder.","context":["Photograph by Meg Smith\nDrastic Times: Artist and apiarist Rob Keller uses his bees in his art, keeping them clandestinely when civic laws protest.\nWhy bees thrive in the city, promote life on earth and deserve a guerrilla movement\nBy Alastair Bland\nIn our garden state of almonds, grapes, rice, walnuts, figs, olives and so much more, the bees are starving. According to entomologists, the rising tendency toward monocultures, in which single-plant crops cover many contiguous acres, is limiting bees to a nutritionally deficient diet. The result is a weakened immune system, loss of reproductive capacity and decline in overall vigor of the species.\nBy unusual contrast, big-city bees are thriving. Urban areas offer a wider range of plant species in higher densities than many rural landscapes, pesticides occur less frequently in cities, especially among the treetops, and water is abundant year-round. The environment is safer and cleaner, bee authorities say, and as the much-discussed colony collapse disorder continues to sweep the world, hobbyist beekeeping in cities like San Francisco, Berkeley and Napa could play a significant role in preserving bee populations, strengthening their genetic backbone and assuring a future presence of pollinators, without which the earth would essentially cease turning.\n\"When you come to the reasons that bees are dying, like colony collapse, loss of habitat, stress, loss of genetic diversity and the use of pesticides—all of these things can be addressed to an extent by beekeeping in urbanized areas,\" says Serge Labesque, a beekeeping teacher at Santa Rosa Junior College.\nBut many cities, like Santa Rosa and Napa, have safety ordinances against keeping bees, and the political climate is not entirely apiarian-friendly. Rob Keller, an artist and leading figure in the local beekeeping community, lives in Napa and keeps scores of hives around the North Bay. Keller has observed his rural bee colonies deteriorate, showing particular susceptibility to the problematic Varroa mite. For most of the past year, however, he stashed over a dozen hives in his Napa backyard.\n\"Those colonies of mine were of great genetic material, and I had them here in the city to make it easy on them to get food and to keep their health up,\" he says. \"They were getting a much more varied diet here.\"\nKeller's cherry tree, too, produced a bumper crop this year—strong testament to the beneficial work of bees—but following a neighbor's complaint, Keller was forced by the city to transfer the boxes to a friend's pear orchard south of town.\n\"It's a monoculture out there,\" Keller says. \"They have nothing to forage on. There are grapes, but grapes are self-pollinating.\"\nHybridization between the European honeybee, the bee reared in hives around the world, and the African honeybee, introduced to South America in the 1950s, is another threat faced by honeybees today.\nLabesque believes that a large community of hobbyist beekeepers could serve as a barrier to hybridization between the two subspecies. Hobbyists, he explains, tend to pay closer attention to their hives than large-scale beekeepers, and when aggressive behavior in a colony is observed, beekeepers often respond by \"requeening\" the hive, or replacing the current female breeder with a fresh specimen and thereby resetting the genetic makeup of the colony.\nFeral populations, of course, receive no such treatment and may transform completely into the dangerously aggressive Africanized hybrid, which moved into the southern United States in the late 1990s. \"But urban beekeepers tend to maintain a gentle, good-natured population of bees,\" Labesque assures.\nHelene Marshall, co-owner of Marshall's Farm Honey in American Canyon, says that cities can be just as unhealthy as farmland. \"Gardeners use plenty of pesticides, too,\" she says. \"It's not necessarily safer in the city than in the country.\"\nKatia Vincent, owner of the Beekind store in Sebastopol, disagrees. She points to city parks, where eucalyptus trees in particular may offer a wealth of seasonal blossoms, all high out of range of municipal pesticide application. Moreover, urban gardens are well-watered most of the year, keeping the nectar flow of the blossoms moving. In the feral rangeland, the current drought has curtailed nectar production in many wildflowers, Vincent says, and bees may be hard-pressed this summer to gather sufficient food for the winter.\nThe city of Napa, which has never recorded a serious bee-sting case, prohibits bees on properties of less than one acre in size and requires that legal hives be kept at least 40 feet from neighbors' homes. In short, having legal hives in Napa is scarcely possible and most local beekeepers are no more than a phone call away from being shut down.\nSonoma, Petaluma and Sebastopol are more accommodating toward bees, says Labesque. His hometown of Glen Ellen, too, is a relatively bee-friendly place, but the monotony of the surrounding grape-grown land constitutes a nutritional desert. Biodynamic and organic vineyards often provide cover crops and bee-friendly habitat among the rows, but most vineyards are barren.\nCalifornia's largest monoculture occurs in the Central Valley, where most of the state's 650,000 acres of almonds grow. These vast groves are simply too much for California's resident bees alone, which occupy an estimated half-million commercial hives. To mitigate this deficiency, almond growers bring in hives from as far away Australia and place the boxes among the trees at a density of three per acre.\nSuch intermingling of bees from around the world has encouraged the spread of disease, says Kathy Kellison, who believes one of the answers to alleviating bees' troubles is to foster a population of pollinators in California substantial enough to handle the bulk of the almond industry's needs. As executive director of Partners for Sustainable Pollination (PSP), Kellison is working with state legislators to improve the habitat available for honey bees and other pollinators. The PSP has also submitted a written amendment to the 2008 Farm Bill that would create various incentives for California farmers to provide fallow habitat on their land for bees and native pollinators.\nThe issues facing bees must be taken seriously, Labesque says. \"If we lose our pollinators, we'll lose our ecosystems. The entire environment functions because we have pollinators. Without them, we'll lose most of our food supply. Ground cover will decrease, erosion will increase, plants everywhere will vanish. Pollinators have made everything around us happen. Without them there would be nothing.\"\nFor Keller, beekeeping borders on a personal responsibility. While he negotiates on cordial terms with city officials for the right to bring his bees back to town, he encourages others to take a beekeeping class, invest in the basic equipment and, he smiles, \"go guerrilla.\"\n\"Drastic times call for drastic measures,\" Keller says. \"This is the right thing to do. You can't go to the city and ask, 'Can I please keep bees within the city limit?' They'll say no.\n\"You just do it.\"\nSend a letter to the editor about this story.","Buzzing Rooftops: The Ins and Outs of Urban Beekeeping\nWho will you meet?\nCities are innovating, companies are pivoting, and start-ups are growing. Like you, every urban practitioner has a remarkable story of insight and challenge from the past year.\nMeet these peers and discuss the future of cities in the new Meeting of the Minds Executive Cohort Program. Replace boring virtual summits with facilitated, online, small-group discussions where you can make real connections with extraordinary, like-minded people.\nIn the springtime in Provence, France, the countryside explodes with plush, fluffy looking purple fields. It’s lavender season. Even more miraculous than the aroma of sweet lavender is the sound coming from the fields – they actually buzz with honeybees. When the first taste of lavender honey hit my lips was about the time that my fascination with honeybees and beekeeping began.\nBees on the Rise\nUrban Beekeeping has been on the rise over the past decade. Some reasons for this trend include: the coining of Colony Collapse Disorder (CCD) in 2006 and subsequent efforts to combat it, Michelle Obama placing beehives on the White House lawn in 2009, the legalization of urban beekeeping in New York City in 2010, and the growth of the Local Food Movement.\nIn cities around the globe, urban beekeeping is gaining popularity as a local method of honey production. The Greater London Area has seen a three-fold increase in the number of beekeepers since 2008, with the UK’s National Bee Unit Database tallying 1,237 beekeepers and over 3,500 hives in 2013. A New York Times article highlights the San Francisco Beekeeper’s Association increasing from about 50 SF beekeepers in 2000 to over 400 in 2010. With this rapid increase in urban beehives, scientists are questioning whether or not urban beekeeping is actually a good thing.\nThe “Sweet Spot” of Urban Beekeeping\nA recent study published in The Biologist argues that urban beekeeping has gone too far. Scientists Alton and Ratnieks have found that in London, there are now 25 beehives per square mile of city. Without enough flowers to support a population this size, city bees end up famished. The scientists advise that those looking to help the honeybee population should plant bee friendly flowers instead of managing a hive. In the United States, most cities have enough flowers to support the growing numbers of urban apiarists. However, an uptick in urban beekeeping does mean increasing the population density of one species. This inevitably tips the natural balance of the ecosystem and other factors, such as food supply for bees, need to be considered.\nOn the other hand, urban beekeeping practices in Paris are supporting healthier hives compared to collapsing colonies in rural areas. Urban areas are generally pesticide-free and have a greater variety of flowers compared to monoculture farms that dominate rural landscapes. Additionally, gardens in urban areas are designed to flower all year round, providing a constant food source for bees.\nIn an NBC News article, Henri Clement, the president of the National Union of French Beekeepers, states that urban bees in Paris are producing as much as 5 times the amount of honey as rural bees. The death rate of urban colonies is 3 to 5 percent in Paris, whereas in the country, the death rate of colonies is 30 to 40 percent. With no pesticides in the pollen and greater biodiversity of flowers, cities are counter-intuitively better able to support honeybees than rural landscapes.\nBack in San Francisco, I had the opportunity to shadow an urban beekeeper in the sunny hills of Noe Valley. Our faces covered by nets and smoker in hand, we set out to do hive maintenance. The five hives were positioned in this beekeeper’s back garden with clear flight paths, so the bees don’t end up flying into neighbors’ windows (a requirement in city settings). Honeybees generally tend to fly within a two-mile radius for food. With no shortage of flowers in sight, his hives were dripping with honey. As a member of the SF Beekeeping Association, he will often get calls about hives that have been “rescued” from buildings and are looking for a home, a more sustainable alternative to calling the exterminator.\nUrban beekeeping is a way to reconnect with the natural world that we urbanites have become so separate from. The above-mentioned study from the Biologist, warning against urban beekeeping, has received much press. However, since the study is geographically specific, it should not deter urban dwellers from taking up beekeeping, as long as they consider the larger ecosystem. Urban beekeeping is a way to make urban habitats more habitable. It is a way for city dwellers to actively create the environment they want to live in. An environment that supports beekeeping means more green space, more perennial flowering plants, and the buzzing sound of a healthy bee population, just like the lavender fields of Provence.\nLeave your comment below, or reply to others.\nPlease note that this comment section is for thoughtful, on-topic discussions. Admin approval is required for all comments. Your comment may be edited if it contains grammatical errors. Low effort, self-promotional, or impolite comments will be deleted.\nRead more from MeetingoftheMinds.org\nSpotlighting innovations in urban sustainability and connected technology\nThe development of public, open-access middle mile infrastructure can expand internet networks closer to unserved and underserved communities while offering equal opportunity for ISPs to link cost effectively to last mile infrastructure. This strategy would connect more Americans to high-speed internet while also driving down prices by increasing competition among local ISPs.\nIn addition to potentially helping narrow the digital divide, middle mile infrastructure would also provide backup options for networks if one connection pathway fails, and it would help support regional economic development by connecting businesses.\nOne of the most visceral manifestations of the combined problems of urbanization and climate change are the enormous wildfires that engulf areas of the American West. Fire behavior itself is now changing. Over 120 years of well-intentioned fire suppression have created huge reserves of fuel which, when combined with warmer temperatures and drought-dried landscapes, create unstoppable fires that spread with extreme speed, jump fire-breaks, level entire towns, take lives and destroy hundreds of thousands of acres, even in landscapes that are conditioned to employ fire as part of their reproductive cycle.\nARISE-US recently held a very successful symposium, “Wildfire Risk Reduction – Connecting the Dots” for wildfire stakeholders – insurers, US Forest Service, engineers, fire awareness NGOs and others – to discuss the issues and their possible solutions. This article sets out some of the major points to emerge.\nWhether deep freezes in Texas, wildfires in California, hurricanes along the Gulf Coast, or any other calamity, our innovations today will build the reliable, resilient, equitable, and prosperous grid tomorrow. Innovation, in short, combines the dream of what’s possible with the pragmatism of what’s practical. That’s the big-idea, hard-reality approach that helped transform Texas into the world’s energy powerhouse — from oil and gas to zero-emissions wind, sun, and, soon, geothermal.\nIt’s time to make the production and consumption of energy faster, smarter, cleaner, more resilient, and more efficient. Business leaders, political leaders, the energy sector, and savvy citizens have the power to put investment and practices in place that support a robust energy innovation ecosystem. So, saddle up."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:14a57f8f-fe51-47fc-9251-de960e73d03d>","<urn:uuid:151e5f3d-493b-4691-8c22-917efb9efb5f>"],"error":null}
{"question":"How do marine protected areas address biodiversity conservation, and what role do invasive species play in threatening marine ecosystems?","answer":"Marine protected areas are designated to protect marine biodiversity, with IUCN defining them as areas of intertidal or sub-tidal terrain with associated flora, fauna and cultural features, reserved by law to protect the environment. However, they only make up a fraction of terrestrial protected areas despite oceans covering two-thirds of Earth's surface. Regarding invasive species, they threaten marine ecosystems through multiple pathways including ship traffic (ballast water and hull fouling), aquaculture operations, fishing gear, and aquaria discharge. When invasive species like Sargassum become dominant, they can harm coral reefs by shading, limiting coral larvae recruitment space, and transmitting pathogens.","context":["Life on earth began in the seas which harbors a wider range of biological diversity than land. Despite the fact that a majority of life forms exist in the oceans and seas, and that they are, the biggest provider of human sustenance in many countries and the most common means of goods transport, they are increasingly becoming degraded due to wide neglect and their status as common property.\nIUCN defines marine protected areas as: \"Any area of intertidal or sub-tidal terrain, together with its overlying water and associated flora, fauna, historical and cultural features, which has been reserved by law or other effective means to protect part or all of the enclosed environment,\" MPAs are ocean areas that have been demarcated with limited human activity, or in some cases total prohibition, to conserve and protect the natural marine resources, ecosystems and genetic diversity. In addition to ecological objectives marine protected areas also serve to ensure human welfare, education, recreation and cultural preservation. They are generally governed with more stringent regulations than surrounding areas.\nDespite two thirds of the earth surface is covered by oceans, marine protected areas only makes up a fraction of terrestrial protected areas. Coastal areas remain poorly represented in the national protected area networks of the countries participating in MFF, and many vitally important or threatened coastal ecosystems do not have protected status. Greater representation is required to address these gaps in coverage, and to ensure that critical ecosystems are conserved.\nAs part of the MFF preparatory activities a gap analysis to review existing protected area coverage, identify regionally or nationally under-represented ecosystems, and recommend areas in need of additional protection was conducted. The MFF countries are at different stages in developing their protected area systems, and needs vary accordingly. Even where good intended protected areas have been established in coastal areas, some remain “paper parks”, as there is weak capacity and inadequate funding to manage them effectively. There is also a strong need to support measures to improve management effectiveness, including designing protected areas for resilience linked to climate change, improved integration with tourism, support to participatory management approaches, improving buffer zone management, and identifying sustainable financing mechanisms. MFF has approved its first large project under this PoW,” Evaluating and improving the management effectiveness of Thailand's Marine and Coastal Protected Areas.\nFor details about Actions, Outputs and Results, click [ + ]\n|Programmes of Work||Actions/Outputs||Contribution to results|\n|13. Building national systems of marine and coastal protected areas that contribute to a regional network||\nReturning mangroves to Tanjung Panjang, Indonesia © IUCN, 2018\nMangrove forests worldwide have been vanishing at astonishing rates. Tanjung Panjang, Indonesia, which has lost over 60% of its mangroves in the last 3 decades, exemplifies this trend. The creation of aquaculture in nature reserves has in part led to this decrease in mangrove cover. With the help of IUCN's Restoration Opportunities Assessment Methodology (ROAM), local experts and several NGOs are working with current land users and the local government to restore forest landscapes and strive for a more sustainable future.\nKochi, India 28 Oct 2013\nRepresentatives from more than 12 countries attended a Regional Fisheries Symposium from October 28 to 30 in Kochi, India with the goal of exploring ecosystem-based approaches to protecting fisheries and marine biodivers...","Beyond threats associated with climate and ocean change, coral reefs are also affected by various local and regional threats. These threats may occur alone or synergistically with climate change adding to the risks to coral reef systems.\nOverfishing and Destructive Fishing\nUnsustainable fishing has been identified as the most pervasive of all local threats to coral reefs. ref Over 55% of the world’s reefs are threatened by overfishing and/or destructive fishing. Overfishing (i.e., catching more fish than the system can support) leads to declines in fish populations, ecosystem-wide impacts, and impacts on dependent human communities. Destructive fishing is associated with some types of fishing methods including dynamite, gill nets, and beach seines. These harm coral reefs not just through physical impacts but also through by-catch and mortality of non-target species including juveniles. Read more about threats and management strategies in the Reef Fisheries Toolkit.\nTraditionally, impacts from wastewater pollution have been associated with human health, but the detrimental effects of wastewater pollution on marine life – and the indirect impacts they have on people – cannot be overlooked. Wastewater transports pathogens, nutrients, contaminants, and solids into the ocean that can cause coral bleaching and disease and mortality for coral, fish, and shellfish. Wastewater pollution can also alter ocean temperature, pH, salinity, and oxygen levels disrupting biological processes and physical environments essential to marine life.\nOther sources of pollution to coral reef waters include land-based pollution associated with human activities such as agriculture, mining and coastal development leading to the discharge or leaching of harmful sediments, pollutants, and nutrients. Marine-based pollution associated with commercial, recreational, and passenger vessels can also threaten reefs by discharging contaminated bilge water, fuel, raw sewage, and solid waste, and by spreading invasive species. Learn more in the Wastewater Pollution Toolkit or in the Wastewater Pollution Online Course.\nMore than 2.5 billion people (40% of the world’s population) live within 100 km of the coast, ref adding increased pressure to coastal ecosystems. Coastal development linked to human settlements, industry, aquaculture, and infrastructure can cause severe impacts on nearshore ecosystems, particularly coral reefs. Coastal development impacts may be direct (e.g., land filling, dredging, and coral and sand mining for construction) or indirect (e.g., increased runoff of sediment, sewage, and pollutants).\nTourism and Recreational Impacts\nRecreational activities can harm coral reefs through:\n- Breakage of coral colonies and tissue damage with direct contact such as walking, touching, kicking, standing, or gear contact that often happen with SCUBA, snorkelling, and trampling\n- Breakage or overturning of coral colonies and tissue damage from negligent boat anchoring\n- Changes in marine life behavior from feeding or harassment by humans\n- Water pollution by tour boats through the discharge of fuel, human waste, and grey water\n- Invasive species which can be spread through transportation of ballast water, hull fouling of cruise ships, and fouling from recreational boating\n- Trash and debris deposited in the marine environment\nCoral disease is a naturally occurring process on reefs, but certain factors can exacerbate disease and cause outbreaks. Coral disease outbreaks can lead to an overall reduction in live coral cover and reduced colony density. In extreme cases, disease outbreaks can initiate community phase-shifts from coral- to algal-dominated communities. Coral diseases can also result in a restructuring of coral populations.\nDisease involves an interaction between the coral host, a pathogen, and the reef environment. Scientists are learning more about the causes of coral disease, especially in terms of identifying the pathogens involved. To date, the most infectious coral diseases are caused by bacteria. Transmission of coral diseases can be facilitated in areas of high coral cover ref as well as through coral predation, as predators can act as vectors by oral or fecal transmission of pathogens. ref\nThe causes of coral disease outbreaks are complex and not well understood, although research suggests that important drivers of coral disease include climate warming, land-based pollution, sedimentation, overfishing, and physical damage from recreational activities. ref\nOn coral reefs, marine invasive species include some algae, invertebrates, and fishes. Invasive species are species that are not native to a region. However, not all non-native species are invasive. Species become invasive if they cause ecological and/or economic harm by colonizing and becoming dominant in an ecosystem, due to the loss of natural controls on their populations (e.g., predators).\nPathways of introduction of marine invasive species include:\n- Ship traffic, such as ballast water and hull fouling\n- Aquaculture operations (shellfish aquaculture is responsible for the spread of marine invasive species through global transport of oyster shells or other shellfish for consumption)\n- Fishing gear and SCUBA gear (through transport when moving from place to place)\n- Accidental discharge from aquaria through pipes or intentional release\nSargassum are a type of brown, fleshy macroalgae that can have detrimental ecological and economic impacts on coral reefs when overabundant.\nIn the Indo-Pacific, high percent cover of Sargassum is common on degraded coral reefs and often represents a phase-shift from a coral to algae-dominated reef system. ref Their reproductive biology and morphology make them excellent colonizers of free space and particularly resilient to disturbances such as tropical storms. ref When overabundant, they can negatively impact the reef by shading, limiting space available for coral larvae to recruit, and transmitting pathogens. ref\nIn the Atlantic, two species of floating sargassum, S. natans and S. fluitans, are responsible for causing large mats of algae blooms which are particularly harmful and prevalent on the Caribbean and West African coastlines. ref Floating algae mats are naturally prevalent in the Northern Atlantic and provide many ecological benefits such as habitat, food, and nursery grounds to many species of fish, crustaceans and even sea turtles. ref However, in the last ten years, a shift in oceanic currents has led to an algae invasion in coral reef areas, causing reduced sunlight required by corals and anoxic and hypoxic conditions on reefs, as well as poor conditions on beaches that are detrimental to the tourism industry. ref\nCoral predators (or 'corallivores') are naturally occurring organisms that feed on corals for their polyps, tissue, mucus, or a combination of the above. Such predators typically include echinoderms (starfish, sea urchins), mollusks (snails), and some fish.\nCorallivory is a common process that, under normal conditions, allows for natural turnover in the ecosystem. However, when these predators are overly abundant (e.g., outbreak conditions), they can cause significant declines in coral cover.\nCommon coral predators include:\n- Crown-of-Thorns starfish (COTS), which are found throughout the Indo-Pacific region, occurring from the Red Sea and coast of East Africa, across the Pacific and Indian Oceans, to the west coast of Central America. COTS can be a major driver of coral loss in the Indo-Pacific, particularly under outbreak conditions.\n- Drupella snails, which are commonly found living on corals in reefs throughout the Indo-Pacific and Western Indian Ocean.\n- Coralliophila snails, which are often more problematic for Caribbean reefs, although some species are prevalent in the Pacific."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:42923eae-893c-4c12-ae95-31e23acdfd2c>","<urn:uuid:6dd87468-19aa-450b-b9cc-46d264a1ee33>"],"error":null}
{"question":"What are the specialized educator roles that ASU is preparing teachers for in their new workforce development model?","answer":"ASU is preparing educators for several specialized roles including: novice teachers, specialized teachers, managing teachers, community school connectors, and intrapreneurial school leaders. This represents a shift from the traditional model of one teacher working in isolation to a team-based approach supporting student-centered progressions.","context":["How ASU is Building the Capability and Character of Educators\nLeading the education school at what has been called the most innovative university comes with some pressure to innovate. Carole Basile is in her second year as dean of the Mary Lou Fulton Teachers College at Arizona State University (@asueducation). With about 5,800 students (including 2,400 undergrads), it's one of the largest educator preparation programs in the country.\nDr. Basile and the talented faculty recognized that the innovation economy requires new learning models and that new and transformed schools require a new kind of preparation.\nRecognizing that we can't keep providing inadequate preparation for obsolete learning models, Dr. Basile and the talented faculty have responded with important initiatives to support new learning models and develop the capability and character of the regional education workforce.\nNew Prep for a New Staffing StrategyArizona has a teacher shortage and a performance problem. Low pay, isolating work and crowded classrooms are contributing to a steady decline in students pursuing education degrees.\nBasile concluded that incremental improvements to preparation were an insufficient response--the complex problem requires new learning models and a reconceptualized approach to workforce development. Four core strategies emerged from faculty dialog: developing 21st-century educators, working with schools, building new models and providing use-inspired research.\nCarole realized ASU needed to simultaneously help schools design new approaches and prepare educators to be successful in new school models that cultivate more creativity and less compliance, that are team-based and rely less on isolated contributions and dynamically adjust to personalize learning for students and educators.\nASU developed partnerships with school districts in metro Phoenix area where they host community design labs. ASU faculty help educators construct new personalized learning models.\n\"Imagine,\" says Maddin, \"an educator workforce that creates a pathway for more positions and is no longer just a single teacher in a single classroom, but rather a team of educators, including managing teachers.\"\nHe identified roles that require unique preparation including novice teachers, specialized teachers, managing teachers, community school connectors and intrapreneurial school leaders. The shift from one teacher working in isolation with a group of students to a team supporting student-centered progressions is shown below:\n\"Principals will need to think of the strengths of individual educators, assemble teams of educators, forge community partnerships, budget at the district and school level, increase personalization for students, among other responsibilities.\"\nMaddin points out. \"We are meeting with 15 school districts around the Phoenix metro area on a monthly basis to help craft what the emerging roles within their systems will look like.\" Maddin is certain the people in these roles will need additional skill sets. \"I could imagine specializations within the iLeadAZ Principal Preparation Pathway program for these managing teacher roles, with a lot of the responsibilities a principal would have,\" Maddin says.\n\"There are many more conversations that need to happen among educators, colleges of education and community leaders,\" Maddin says. \"We are eager to have them. Close partnerships will help us make something truly great here and ultimately improve the experience that leaders, teachers and learners have in our schools.\"\nMost prospective teachers participate in the well regarded iTeachAZ program which includes a full-year student teaching residency in a school.\nFor other educators not seeking certification, ASU launched an Education Studies degree program. It now serves 550 students.\nPersonal Transformation for Systems TransformationIn January, ASU was awarded a $12.4 million grant by the Kern Family Foundation to develop and incorporate character education into its teacher and leadership preparation programs.\nMost schools of education are a grab bag of survey courses that always includes a review of pedagogy but almost never considers the character displayed by the educator. But as David Brooks recently observed, \"When you learn about successful principals, you keep coming back to the character traits they embody and spread: energy, trustworthiness, honesty, optimism, determination.\" He added, \"Social transformation follows personal transformation.\"\nBasile said the desire to address character education arose from the conviction that one core purpose of education is to support the development of citizens who have the habits of mind and disposition to maintain civil society and serve the public good.\n\"The great American experiment in self-government has always rested on our ability to educate citizens capable of reasoned argument, respectful discourse and the hard work of balancing individual ambitions and the public good,\" Basile said.\nWhile the college's curricula and programs have emphasized service learning and social responsibility, Basile said the Kern grant allows the college to bring a \"new level of intentionality and structure\" to the work of integrating concepts of character and character development into the systems, norms, curricula and processes of the school.\nAs the first effort of its kind at a college of education, the faculty has engaged in spirited discussions about the meaning of character and the role of a college of education in nurturing it.\n\"We arrived at an approach to character education that aims to prepare our students to be effective educators and caring citizens,\" Basile said. \"Our conversations led to a view of character education based on ideas of equity and reciprocity. It embraces difference, multiple perspectives and the complexity of social life.\"\nRather than offering simple answers, Basile said the ASU program will help educators ask the right questions, \"How to create informed learners?\" and, \"How to promote moral and just citizenship?\"\n\"American parents say that the formation of strong character is their highest aspiration for their children,\" Kern Family Foundation President James Rahn said. \"Ninety percent of Americans say our democracy is only as strong as the virtue of its citizens.\nThe grant seems particularly timely given the rise of artificial intelligence. \"We simply must ask, how will interacting with 'smart' agents affect our interactions with other humans? With this level of immersion, it will be increasingly important to educate the future developers in the humanities,\" said John Balash, Entertainment Technology Center at CMU. \"We must strive to create experiences that have purpose--reinforcing what makes us human,\" added Balash.\nThe increasing importance of developing character particularly among educators drove the Kern Family Foundation to sponsor a preparation program focused as much on mindsets as methods. \"We are confident in ASU's ability to bring this vision to reality and to become the anchor of a national network of colleges of education committed to character education,\" said Rahn.\nThe framework developed by the college identifies four kinds of character: moral, civic, intellectual and performance. It also identifies four social environments that the college's approach to character education will address: interpersonal, university, PK-12 learning environments and the larger communities in which schools reside.\nBasile thinks educators should be able to work within organizations and systems to ask the right questions, navigate uncertainty, and work in teams to design and create solutions to the toughest challenges. They call it creative intrapreneurship. \"We view character as a vital complement to the innovative energy of creative intrapreneurship. It adds purpose to innovation,\" said Baile.\nThe grant will support program materials, research, and an annual convening of scholars, education leaders, nonprofits, policymakers and others to further explore the role of character education in the preparation of teachers and education leaders.\nFor more, see:\n- Education Systems Should Be Based on How Students Develop\n- What Policies and Practices Can Make Learning Personal for All?\n- A Unique Moment in Education Changemaking\n- Shadow a Student: Reinventing the School Experience\n- The Rise of AI: What's happening, What it Means, How to Prepare?\nStay in-the-know with all things EdTech and innovations in learning by signing up to receive the weekly Smart Update. This post includes mentions of a Getting Smart partner. For a full list of partners, affiliate organizations and all other disclosures, please see our Partner page."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:f1559aa5-f426-430e-a473-e550b6384bf1>"],"error":null}
{"question":"How does oxygen-independent photodynamic therapy work?","answer":"Oxygen-independent photodynamic therapy works by using a photo-switchable molecule called GS-DProSw that can be activated by light without requiring oxygen. The molecule is administered in an inactive state and is then activated at the target location using visible light. Once activated, it directly attacks the surrounding tumor tissue. Unlike conventional photodynamic therapy which needs oxygen to create toxic radicals, this method works effectively even in low-oxygen tumor environments.","context":["KIT researchers develop oxygen-independent, photoswitchable molecule and test it successfully in the lab for its effect against tumors\nPhotoswitchable agents might reduce side effects of a chemotherapy. So far, photodynamic therapies have been dependent on oxygen in the tissue. But hardly any oxygen exists in malignant, rapidly growing tumors.\nA group of researchers of KIT and the University of Kiev has now developed a photo-switchable molecule as a basis of an oxygen-independent method. Their successful laboratory tests on tumors are reported in the journal Angewandte Chemie (Applied Chemistry). DOI: 10.1002/ange.201600506.\nPhotodynamic therapy (PDT) in medicine usually uses a substance that reacts to light and converts the oxygen in the tissue into aggressive radicals. These reactive substances are toxic and damage the neighboring cells, such that e.g. tumors are decomposed. As a result of their quicker growth, however, many tumors have a high oxygen consumption. This reduces the concentration of oxygen available in the tissue, which may aggravate conventional PDT.\nResearchers of KIT and the University of Kiev have now developed a new photo-switchable molecule for oxygen-independent PDT. The effect of the GS-DProSw molecule can be \"switched off\" by ultraviolet light prior to therapy. Only upon application is it \"switched on\" in the tumor tissue by visible light and damages the tumor tissue there.\n\"The surrounding organs remain in the dark and are not affected by the active substance,\" Anne S. Ulrich, Professor for Biochemistry and Director of the KIT Institute for Biological Interfaces, explains. \"As a result, side effects are reduced significantly.\"\nFor the first time, this new concept has now been tested on animal models. Once per day, the photoswitchable GS-DProSw molecule was administered. Then, the tumors were irradiated locally with visible light for a period of 20 minutes. After ten days of PDT treatment, the tumors were found to be far smaller than comparative groups not treated with light.\nTo initiate an oxygen-independent reaction in a PDT, the molecule applied has to be of cytotoxic nature. This means that it has to directly attack the tumor tissue irrespective of other reaction partners. A suitable molecule with cytotoxic properties against tumors is the biomolecule gramicidin S (GS), a natural antibiotic. To prevent it from damaging healthy tissue, the research team inserted a photo-switchable diaryl ethene segment into the ring structure.\nAs a result, the GS-DProSw molecule can be switched between two states with the help of light: The agent can be administered in the inactive state and is activated at the desired location by specific irradiation with light. There, it attacks the surrounding tumor tissue and contrary to conventional PDT, it does not require any oxygen for this purpose.\n\"This first proof of functioning represents an important step in fundamental research for the development of anti-tumor agents,\" Ulrich explains. \"But we still have a far way to go: To reliably use this type of photoswitchable molecules for a photodynamic therapy of patients, numerous other studies have to be carried out in cooperation with our partners in Kiev.\"\nOleg Babii, Sergii Afonin, Liudmyla V. Garmanchuk, Viktoria V. Nikulina, Tetiana V. Nikolaienko, Olha V. Storozhuk, Dmytro V. Shelest, Olga I. Dasyukevich, Liudmyla I. Ostapchenko, Volodymyr Iurchenko, Sergey Zozulya, Anne S. Ulrich, and Igor V. Komarov: Direct photocontrol of peptidomimetics: an alternative to oxygen dependent photodynamic cancer therapy. Angewandte Chemie (2016). DOI: 10.1002/ange.201600506\nMonika Landgraf | EurekAlert!\nFinnish research group discovers a new immune system regulator\n23.02.2018 | University of Turku\nMinimising risks of transplants\n22.02.2018 | Friedrich-Alexander-Universität Erlangen-Nürnberg\nA newly developed laser technology has enabled physicists in the Laboratory for Attosecond Physics (jointly run by LMU Munich and the Max Planck Institute of Quantum Optics) to generate attosecond bursts of high-energy photons of unprecedented intensity. This has made it possible to observe the interaction of multiple photons in a single such pulse with electrons in the inner orbital shell of an atom.\nIn order to observe the ultrafast electron motion in the inner shells of atoms with short light pulses, the pulses must not only be ultrashort, but very...\nA group of researchers led by Andrea Cavalleri at the Max Planck Institute for Structure and Dynamics of Matter (MPSD) in Hamburg has demonstrated a new method enabling precise measurements of the interatomic forces that hold crystalline solids together. The paper Probing the Interatomic Potential of Solids by Strong-Field Nonlinear Phononics, published online in Nature, explains how a terahertz-frequency laser pulse can drive very large deformations of the crystal.\nBy measuring the highly unusual atomic trajectories under extreme electromagnetic transients, the MPSD group could reconstruct how rigid the atomic bonds are...\nQuantum computers may one day solve algorithmic problems which even the biggest supercomputers today can’t manage. But how do you test a quantum computer to...\nFor the first time, a team of researchers at the Max-Planck Institute (MPI) for Polymer Research in Mainz, Germany, has succeeded in making an integrated circuit (IC) from just a monolayer of a semiconducting polymer via a bottom-up, self-assembly approach.\nIn the self-assembly process, the semiconducting polymer arranges itself into an ordered monolayer in a transistor. The transistors are binary switches used...\nBreakthrough provides a new concept of the design of molecular motors, sensors and electricity generators at nanoscale\nResearchers from the Institute of Organic Chemistry and Biochemistry of the CAS (IOCB Prague), Institute of Physics of the CAS (IP CAS) and Palacký University...\n15.02.2018 | Event News\n13.02.2018 | Event News\n12.02.2018 | Event News\n23.02.2018 | Physics and Astronomy\n23.02.2018 | Health and Medicine\n23.02.2018 | Physics and Astronomy"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:a085aa8a-3442-4549-9eef-0e84a7463275>"],"error":null}
{"question":"I study religious history. Why were both Hadewijch of Antwerp and Hildegard of Bingen considered revolutionary figures in medieval times? What made them stand out?","answer":"Both women were revolutionary figures for their time but in distinct ways. Hadewijch of Antwerp was exceptional for her intellectual authority and multilingual abilities, being fluent in Latin, French, and Dutch, with scholarly knowledge of classical theologians - unusual for a woman in the 13th century. Hildegard of Bingen was revolutionary in multiple spheres: she held direct authority at a time when female power was virtually unheard of, confronted church authorities demanding reformation, and established the first independent women's monastery separate from male devotees. She was also groundbreaking as a physician, scientist, composer and religious philosopher who intentionally combined science, art and religion in her work.","context":["Friday, July 2, 2021 (Day 32)\nKatie Snipes Lancaster\nPsalm 32 (from Robert Altar’s 2007 translation)\nBe not like a horse,\na mule, without sense,\nthe bit and the reins his adornment—\nto keep him from drawing near you.\nMany are the wicked’s pains,\nbut who trusts in the Lord\nkindness surrounds them.\nRejoice in the Lord and exult,\nO you righteous,\nsing gladly, all upright ones!\nAn Opening Word\nScholars believe that this Psalm was a song. Yes many Psalms are songs, but this one even more so. And the horse/mule, bit/reins metaphor would make for an odd but memorable song lyric.\nThe horse metaphor is true-to-life. As fight-or-flight animals, (wild) horses can injure both humans and fellow horses, pawing when frustrated, stamping when feeling threatened, snapping their jaws, exposing their teeth, flaring their nostrils, pinning their ears back against their neck. The bit and reins restrain and domesticate horses, making us forget that the horse left on its own, is truly a wild animal (and here, the metaphor is extended to mean that the wicked/senseless are akin to wild animals).\nBy contrast God is one to whom we might draw near in a trustworthy way, because in the presence of God, we will be surrounded by kindness. All this is as the Psalmist says, something to sing about. “All you righteous, sing gladly!”\nToday’s mystic Hadewijch of Antwerp is a bit of a mystery. She was from the 13th century but we don’t know the year of her birth or death. Her manuscripts are all we know of her: they were collected by a friend but then lost after his death. They were not rediscovered until medieval scholar Maurice Maeterlinck found them in 1838, and even then were not published as a collection until 1920.\nHadewijch was part of the early Gothic period or what you might call medieval Europe. She was believed to be Flemish, from Antwerp, and due to her style of writing, use of many languages (fluently shifting between Latin, French, and Dutch), and even her tone of authority, most scholars believe she was from a noble background, educated, and exposed to classical theologians like Augustine and biblical scholarship, all of which appear in her writings.\nIn one letter she describes her relationship with God like this:\n“Since I was ten years old I have been so overwhelmed by intense love that I should have died, during the first two years when I began this, if God had not given me other forms of strength than people ordinarily receive, and if he had not renewed my nature with God’s own Being… God, my Love, imparted to me in so many ways at the beginning of my life, gave me such confidence in Godself that ever since that time it has usually been in my mind that no one loved God so intensely as I.”\nFrom this, and from her prayer below, you can tell she had a deep experience of the presence of God from an early age, and experienced God as love, or even called God by just the name “Love,” an intimacy with God that may feel foreign to us, or maybe for some, feels exactly like what we mean when we say the word “God.”\nPrayer from the Mystics: Hadewijch of Antwerp (13th Century)\nO my love!\nFix all your thought\non the Love of God\nwho created you.\nCommend all your being to Love;\nSo shall you heal all torments and pains,\nand you shall fear nothing,\nand shall not flee from adversity in anything.\nYou should trust to Love.","The Middle Ages brought us the Abbess Hildegard von Bingen, the Sybyl (Healer) of the Rhine, considered to be the first German Woman Physician and the mother of German Botany.\nThe Benedictine monastery at Mount St Disibode was to receive the young Hildegard when she was 8 years old. She had been promised to the church by her parents at birth as a servant of God. She eventually became the Abbess and would remain in the monastery until her death at the age of 81.\nHildegard of Bingen (1098-1179) was a mystic, scholar, prophet, composer, moralist, herbalist and scientist. Hildegard was the 10th child of Hildebert and Mechthilde. Born into a family of privilege, she was a sickly child who began experiencing mystical visions from a very young age that continued throughout her life. It was only later in life that she began to record these visions after seeking advice from a person who was highly regarded in the church, St. Bernard of Clairvaux, who encouraged her to write about her visions. She would also gain the respect of Pope Eugene lll who authorized her writings and encouraged her to speak about them in public forums.\nWhen she reached the age of 60 she began to travel to talk to the people about religion and about her beliefs on sickness and cures. She was an advocate for those who were less fortunate in means and in spirit. She proclaimed that each human being deserved the opportunity to develop their individual talents and potential as one of God’s children. As word spread about the mystical healer she was highly sought out for counsel and healing by some of the most influential people of her time.\nHolding authority at a time when it was virtually unheard of for women to be in direct power she was highly acclaimed for her religious and philosophical work. She was also held in high regard for her musical compositions and poetry. She very intentionally and intrinsically wove science, art and religion together, each with its own beauty and power lending insight into the part as well as the whole.\nShe was most certainly a revolutionary visionary as she spoke up against the injustices that she witnessed. Often confronting the authorities of her day to demand change and reformation in the church, she also founded and oversaw for the first time the construction of a new women’s monastery separate and independent of the double monastery housing both female and male devotees.\nFrom Hildegard we receive the word veriditas (pronounced ver id ee tas) meaning greenness and vital energy. Veriditas is the real and visceral energy and spirit of life on earth, and God in all of life. The plant world and every creature in nature is an expression of divine power on earth, veriditas.\nHildegard tended the gardens at Rupertsberg monastery and studied herbs extensively. She of course was not the first person to delve into the world of healing plants and may have very well read the works of those that went before her like Galen, the physician of ancient Rome or Pliny the Elder. She understood the healing properties as well as the magical qualities and employed both in her approach to healing. The three basics of Hildegard’s practice as an herbalist were detoxification, nutrition and natural remedies. This approach is not unlike modern day herbalists. Essentially she sought to purge the body of toxins, resupply the body with nutrients and treat specific disorders with herbs or other remedies found in nature. She stressed harmony between body mind and spirit very much like a naturopath or herbalist in today’s world. She also believed that God was the true healer.\nHildegard’s major works are on theology but she wrote two books on medicine and pharmacology. She wrote about the human body and how to treat many disorders using herbs, describing what she understood as the properties of the herbs that she used in healing from observation and practice. Some of her formulas and remedies remain relevant and are used today.\nShe was a visionary who was propelled from this physical realm on earth to a place of light and understanding which she referred to as God. She was also visionary in her approach to life. She sometimes did the unthinkable and probably suffered a great deal for doing so, but she forged forward and she blazed a trail for others to follow in medicine, religion and morality. Hildegard von Bingen touched many lives in her time on earth and she continues to reach across centuries with an uncanny contemporary view and doctrine. She relayed her visions and knowledge in written form but also used her voice to preach and to sing. She used symbols, mythology, metaphor and imagery that continue to spark our imaginations and senses today."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:a8cc6dc7-a408-45ef-8460-bd61fe37a535>","<urn:uuid:6b1e2ed6-2ea5-4144-840b-29d448e22b01>"],"error":null}
{"question":"Can someone explain the corrective actions for when a wingman gets into a 'sucked' position during combat spread maneuvers?","answer":"When in a sucked position, the wingman should lower the nose, add power, and accelerate until reaching the abeam bearing. Once on the bearing line, they should raise the nose to maintain the bearing and readjust power for 300 KIAS. The wingman can lose as much altitude/airspeed as necessary to regain the bearing line.","context":["TRANSITIONING FROM CLOSE FORMATION TO COMBAT SPREAD\nAfter visually checking the area clear, the lead usually advises the wingman of his heading, altitude, and\nairspeed and then signals the wingman to assume combat spread by pushing his palm out and away. At\nthe signal, the wingman goes to MRT accelerating to a 10-15 knot airspeed advantage and takes a cut\naway from the lead to establish a 10-15 degree heading differential. He varies pitch and angle of bank\n(AOB) to arrive 3/4-1 nm abeam with 1000 feet of vertical separation.\nA common tendency of the wingman while moving into position is to take too great or quick a cut to\ncombat spread, resulting in a sucked position. Patience is the key. If you find yourself looking directly\ndown your 3/9 line or forward, correct immediately by reducing your heading differential and/or varying\nyour rate of climb and/or airspeed to maintain bearing.\nCOMBAT SPREAD STRAIGHT AND LEVEL\nWhile flying combat spread straight and level, the wingman must maintain position, giving priority first to\nthe abeam bearing, second to lateral distance, and third to vertical separation. To determine the abeam\nposition, the wingman looks straight out over his shoulder on a 90-degree relative bearing.\nOnce abeam, the wingman should match the leads airspeed by adjusting power and nose attitude for\nlevel flight. The gouge for straight and level combat spread is approximately 1800 pph to maintain\nTo remain in combat spread, the wingman must employ a continuous inside/outside scan. Look inside to\nscan heading, airspeed, and altitude and outside to check the leads position and scan his primary/\nsecondary lookout areas.\nIf the wingman is sucked or acute, close or wide, the lookout suffers, increasing the sections vulnerability\nto attack. Whenever necessary, trade altitude for airspeed to maintain bearing.\nIf the wingman is sucked, he should lower the nose, add power, and accelerate until arriving on the abeam\nbearing. When on the bearing line, raise the nose to maintain the bearing and readjust power for 300\nKIAS. During this climb the indicated airspeed will be in excess of 300 KIAS. By raising the nose and\nclimbing on the bearing line, the wingman increases his altitude and decreases his airspeed while\nmaintaining position with the lead. The wingman may lose as much altitude/airspeed as necessary to\nregain the bearing line.\nIf the wingmans position is acute, he pulls the nose up and reduces power. Approaching the bearing line,\nhe lowers the nose and resets his power to arrive on the bearing at 300 KIAS.\nIn an acute and wide/close position or a very acute position, the wingman goes to MRT and executes a\nseries of hard turns at 11 units and at least 30 degrees off heading in the direction necessary to regain\nposition. He then returns to the original heading and readjusts power when in position, being careful not to\novercorrect, which may lead to a sucked position.\nIf the wingmans vertical separation is less than 1000 feet but he is on the bearing line with proper distance\nabeam, he adds power and raises the nose to climb while maintaining 300 KIAS. He readjusts power\nwhen back in position. The amount of any correction depends on the amount of the positional error.\nSmall errors require minor maneuvering to finesse the aircraft into proper combat spread. Trading altitude\nfor airspeed is more fuel efficient than adjusting power. Gross errors require more aggressive flying to\ncorrect into proper position."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:234ffe6d-9a3b-4aa9-ae0a-50b44639eb99>"],"error":null}
{"question":"Comparativa technical specs: skin tone monitoring via vectorscope vs gray card exposure metering - pros/cons for pro video/photo workflow?","answer":"The vectorscope YUV monitoring and gray card exposure metering serve different technical purposes. The vectorscope YUV shows an angled skin tone line along the -i axis that appears for all people regardless of skin color, representing blood flow through skin. This is specifically useful for ensuring accurate skin tones in video. Gray card exposure metering, on the other hand, is designed for achieving proper exposure by providing a standard 18% gray reference point, particularly useful when shooting subjects that are predominantly very light or dark. The gray card is more practical for still photography, especially in controlled lighting situations, while the vectorscope is better suited for video color correction workflows.","context":["Achieve proper skin tones by adjusting the white balance in your footage and monitoring the skin tone line in Adobe Premiere Pro.\nThis sample file is an Adobe Stock asset you can use to practice what you learn in this tutorial. If you want to use the sample file beyond this tutorial, you can purchase a license on Adobe Stock. Check out the ReadMe file in the folder for the terms that apply to your use of this sample file.\nShooting video without first properly setting the white balance of the camera often results in scenes with a noticeable color cast. For example, shooting outdoors when your camera is set for typically warmer indoor lighting can yield shots that look cold and tinted slightly blue.\nBecause your eyes can fool you into thinking that colors look normal, rely on the objective tools in the Lumetri Scopes panel instead. Starting in the Editing workspace, make the Project panel active, choose File > New > Adjustment Layer, and accept the default values in the dialog box that appears. Drag the adjustment layer from the Project panel to the Timeline panel, above the clips in your sequence. Trim the layer to overlap all the clips in your sequence that need color adjusting.\nSwitch to the Color workspace and make the Lumetri Scopes panel active. Click the wrench icon at the bottom of the panel and choose Vectorscope YUV and Parade RGB in succession from the menu. (You can display as many scopes as you wish, as well as change their order.) You’ll see right away how the scopes confirm that there’s a color cast in the clip. For example, one color channel may dominate the other two in the Parade RGB scope; or the Vectorscope YUV line, which plots the hue and saturation of every pixel, may skew toward one of the colored boxes in the circle graph.\nSet the playhead on a frame containing something that should appear white and then select the adjustment layer. In the Basic Correction section of the Lumetri Color panel, click the WB Selector eyedropper and then click the white object in the Program Monitor. A white shirt, sign, or piece of paper often does the trick. The Temperature and Tint sliders in the Lumetri Color panel will immediately adjust to show the new white balance setting. The Parade RGB scope in the Lumetri Scopes panel will likewise show more balanced color channels.\nIf your clip contains a face or any exposed skin, you’ll see that the Vectorscope YUV graph shows an angled line that follows the –i axis. This skin tone line appears for all people regardless of their skin color. (It literally represents the color of blood flowing through the skin.) To confirm the skin tone line for your shot, apply the Crop effect to the adjustment layer and then adjust the Left, Top, Right, and Bottom values in the Effect Controls panel until only the person’s skin shows through.\nNow you’ll see a clearly defined skin tone line in the Vectorscope YUV scope that originates at the center and moves along the –i axis toward the line separating Y and R. After you’re happy with your results, delete the Crop effect in the Effect Controls panel.\nWhen you become skilled at reading the scopes in the Lumetri Scopes panel, you’ll find that you can accomplish most of your color correction without even looking at the Program Monitor.\nWith Adobe Stock, you have access to more than 100 million high-quality, royalty-free images including photos, graphics, videos, and templates to jump-start your creative projects. Try Adobe Stock and get 10 free images.","A gray card is a powerful tool used by professional photographers. In this article, we will discuss how to use a gray card in different ways in order to get a proper color and exposure. Gray card can be used to correct color and adjust the white balance in both camera and post-processing. It can also be used to help you get the proper exposure in some situations.\nWhen to use a gray card?\nIt is ideal to use a gray card when for example you are shooting portraits because the lighting remains constant, and you have the time to set up your shot. It is not practical to use a gray card for metering when you have a moving subject or when the light is changing rapidly.\nIt is also useful to use the gray card when you’re dealing with a mixed lighting situation – such as daylight from a window and overhead fluorescent lights – then doing a custom white balance in the camera is a good solution to handle that and get a neutral color to your images. But you have to keep in mind that if you change your camera position or the power and direction of light source then you’ll have to do it again as the color will shift.\nWhat is the best size of the gray card?\nThere are many different sizes available in the market, choosing one to buy depends on the way you are going to use it. If you are going to use it to correct and create a custom white balance in-camera then I’d recommend a slightly larger one because you have to photograph it and crop out everything else. That’s very hard to do with the business card-sized ones.\nIf you only plan on using it for exposure metering and use it for color correction on post-production later then you can go for a smaller one. Also, it will fit nicely in your pocket or camera bag. My recommended types of gray cards are shown at the end of the post.\nHow to use a gray card for exposure metering?\nThe exposure meter system in your camera measures the light reflected off the subject and sets the exposure to make it 18% gray, or middle gray. It reads the light from the entire scene and uses an averaged reading based on an 18-percent gray (also known as middle gray). This gray is a standard value designed to provide a safe and predictable exposure for average subjects.\nIf you look at the histogram, the areas of black are represented on the left and white is on the extreme right – leaving the center for middle gray. You may check my post “How to Use Histogram in Photography” for more information.\nSince these meters assume the subject has a reflectance of about 18% (the same as neutral gray), it sets the exposure accordingly. That’s good for a typical landscape in daylight, but not if something lighter or darker is predominant. Sometimes the camera metering system gets it wrong, for example when you’re shooting a scene with a very dominant white subject, or a really dark one. It’s just getting fooled with the tricky subject matter. The camera is overexposing the black subject and underexposing the white subject to try to make them gray.\nThat is because the camera is measuring and setting to make everything medium or 18% gray. You can use a gray card to help you set the exposure in such situations.\nSteps to take to get your meter reading:\n- Place the gray card where your main subject is going to be.\n- Set your camera to manual shooting mode.\n- Select the ISO and aperture you wish to use for your shot.\n- Set your camera’s metering mode to Spot Metering. This will allow the camera to read a very small area only.\n- Set your focus point to single and choose the center one.\n- Aim your camera at the gray card and press the shutter button halfway down to take a reading.\n- Looking in your viewfinder (eyepiece) adjusts the shutter speed until it gives you a reading of “0” (zero).\n- Review the histogram, you should see one like below.\n- With the same setting, remove the gray card from the scene and take the final shot of the subject.\nHow to use a gray card for making a custom white balance in-camera?\nIf you are shooting photos with a mixed lighting situation, such as daylight from a window and overhead fluorescent lights, then doing a custom white balance in-camera is a good solution to handle that and get a neutral color to your images. The camera settings are slightly different according to camera models, but they are all similar. The followings are the steps of how to make a custom white balance on a Canon camera.\n- Take a photo of only the gray card, zoom or get closer until the card fills the frame, switch to manual focus, focus on the card, and take the shot. You should have something that looks like the photo below.\n- Navigate to your menu until you find “Custom White Balance”\n- It will then ask you to choose an image, select the image of the gray card you have just taken. It may ask you “Use WB data from this image for custom WB” select OK.\n- Set your camera’s White Balance setting to Custom white balance\n- Take your shot.\nHow to use a gray card for color correction in post-production?\nIf you included a shot of your gray card in your computer, you can do a custom white balance in post-production. Both Lightroom and Photoshop have an eyedropper tool you can use to do just this. Don’t forget that a custom white balance in only useful in the same lighting conditions, it is mainly used to do a batch process of a number of photos taken at the same lighting condition. This example uses Adobe Lightroom so the screens and tools might look a little different if you use something else, but the process should be similar.\nOpen the cray card image in Lightroom\nGo to the develop module\nFind, and click on, the White Balance Selector Tool. It’s the eyedropper you see circled below.\nPlace it over an area of your gray card in the image, Try different point until you get the same reading for R, G, and B (that is a neutral gray)\nUse copy and paste buttons, to copy the color settings of this photo. Or select other photos and Sync this color setting to all of them.\nRecommended Gray Cards:\n- Triple re-enforced stitch line – Over 30 stitches per inch ensures steel rims won’t be popping out of the rim tape\n- Collapses to a third of opened size – Close down to a convenient size for portability\n- Pre and Post capture color correction – Can be used to white balance digital cameras or fix color on digitized images\n- Pre and post-capture exposure control – Can help determine the exposure for cameras or fix exposure on digitized images\n- Collapsible/Reversible/Cleanable – Two wipe clean surfaces in one size makes it easy to take with you\n- Can be used with all digital and film cameras – Not limited to only high-end cameras can be used on throwaway film cameras all the way up to large format cameras\nColor Control and Creativity for Photography – from Capture to Edit. Color Checker Passport Photo includes three powerful photographic targets in a portable protective case, enabling you to reduce your image processing time and improve quality control in your Raw or JPEG workflow. Quickly and easily capture accurate color, instantly enhance portraits and landscapes, and maintain color control and consistency from capture to edit. You’ll achieve superior color results in a fraction of the time.\n- Ideal for photographers shooting still images in a Raw workflow\n- Creates professional, custom camera profiles in minutes using the industry standard 24 patches Classic target.\n- Combines 3 photographic color-balance targets into one pocket size protective, a multi-positionable patented case that adjusts to any scene\n- EXCLUSIVE X-RITE TECHNOLOGY! Dual Illuminant profile creation – takes into account two different light sources to create a single profile which can be applied to a variety of lighting conditions\n- Easy to use with Adobe® Lightroom® Plug-In or stand-alone software for use in other image editing software\n- Use Creative Enhancement target to quickly check and evaluate shadow details or highlight clipping; understand and control color shifting or neutralize and create a look with one click.\n- Take one step closer to professional results creating custom, in-camera white-balance. Capturing a consistent white point in a set of images allows for adjustments to different lighting conditions and helps eliminate color casting\n- Supported by several 3rd party software programs\nThanks for reading, I hope you enjoyed the article if you have any questions just post below & I will be happy to answer you.\nYou automatically support us if you order anything through our recommended Amazon links, and we highly recommend them because of their low prices, fast delivery and, the top support, especially when it comes to camera equipment.\nAs an Amazon Associate, the site earns from qualifying Purchase, Most of the “product” links are affiliate links, and you are welcome to check our affiliate Disclosure statement.\nIf you enjoy the site, don’t forget to subscribe, we will only inform you when a new article is posted."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:d0309ece-1f99-4469-81ea-91211aefcbe0>","<urn:uuid:d368bbc7-f102-4683-91fb-7e2b15632b2c>"],"error":null}
{"question":"What is the relationship between Mean Time Between Failures (MTBF) and failure rates in electronic systems versus quantum error correction requirements?","answer":"In traditional electronic systems, MTBF is the inverse of the failure rate (1/λ) and is often used because large positive numbers are more intuitive than very small failure rate numbers. For example, 2000 hours is easier to conceptualize than 0.0005 failures per hour. In contrast, quantum error correction (QEC) requires approximately 1000 physical qubits to support a single logical qubit due to the high failure rates of quantum systems. This high resource requirement is necessary because quantum systems are extremely susceptible to noise and decoherence, making error correction a crucial but resource-intensive process.","context":["What is Quantum Error Correction?\nThe journey to realizing functional quantum computers will be long and it's a path that Q-CTRL is committed to making as easy as possible for you. And by easy, we mean less difficult. Building a universal quantum computer with millions of entangled, coherent quantum bits running complex algorithms is not going to be simple or straightforward.\nFrom noise to error in quantum computing\nHere we’ll get to the heart of why quantum computing is really hard: noise and error.\n“Noise” describes all of the things that cause interference in a quantum computer. Just like a mobile phone call can suffer interference leading it to break up, a quantum computer is susceptible to interference from all sorts of sources, like electromagnetic signals coming from WiFi or disturbances in the Earth’s magnetic field. When qubits in a quantum computer are exposed to this kind of noise, the information in them gets degraded just the way sound quality is degraded by interference on a call. This is known as decoherence.\nCompared with standard computers, quantum computers are extremely susceptible to noise. A typical transistor in a microprocessor can run for about a billion years at a billion operations per second, without ever suffering a hardware fault due to any form of interference. By contrast, typical quantum bits become randomized in about one one-thousandth of a second. That’s a huge difference.\nNow consider a quantum algorithm, executing many operations across a large number of qubits. Noise causes the information in the qubits to become randomized - and this leads to errors in our algorithm. The greater the influence of noise, the shorter the algorithm that can be run before it suffers an error and outputs an incorrect or even useless result. Right now, instead of the trillions of operations that might be needed to run a full-fledged quantum algorithm, we can typically only perform dozens before noise causes a fatal error.\nQuantum Error Correction\nSo what do we do about this?\nCompanies building quantum computers like IBM and Google have highlighted that their roadmaps include the use of “Quantum Error Correction” as they scale to machines with 1000 or more qubits.\nQuantum Error Correction - or QEC for short - is an algorithm known to identify and fix errors in quantum computers. It’s able to draw from validated mathematical approaches used to engineer special “radiation hardened” classical microprocessors deployed in space or other extreme environments where errors are much more likely to occur. QEC is the source of much of the great promise supporting our community's aspirations for quantum computing at-scale.\nIn QEC quantum information stored in a single qubit is distributed across other supporting qubits; we say that this information is \"encoded\" in a logical quantum bit. This procedure protects the integrity of the original quantum information even while the quantum processor runs - but at a cost in terms of how many qubits are required. Overall, the worse your noise is, the more qubits you need.\nDepending on the nature of the hardware and the type of algorithm you choose to run, the ratio between the number of physical qubits you need to support a single logical qubit varies - but current estimates put it at about 1000 to one. That's huge. Today’s machines are nowhere near capable of getting benefits from this kind of Quantum Error Correction.\nQEC has seen many partial demonstrations in laboratories around the world - first steps making clear it’s a viable approach. But in general the enormous resource overhead leads to things getting worse when we try to implement QEC. Right now there is a global research effort underway trying to cross the “break even” point where it’s actually advantageous to use QEC relative to the many resources it consumes.\nHow do we get there?\nQuantum firmware and Quantum Error Correction\nThis is where Q-CTRL comes in. We add something extra - quantum firmware - which can stabilize the qubits against noise and decoherence without the need for extra qubits. Quantum firmware serves as a complement to QEC, such that in combination we can accelerate the pathway to useful quantum computers.\nOne kind of quantum firmware works by something called dynamic stabilization - if you constantly rotate your qubits in just the right way you can make them effectively immune to the noise which would normally randomize them. It sounds a bit like magic, but believe it or not, similar techniques are already used to stabilize the memory in your computer.\nThe techniques are easy to implement and the benefits can be huge - our own experiments have demonstrated more than 10X improvements in cloud quantum computers!\nIn the context of QEC, quantum firmware actually reduces the number of qubits required to perform error correction. Exactly how is a complex story, but in short, quantum firmware reduces the likelihood of error during each operation on a quantum computer. Better yet, quantum firmware easily eliminates certain kinds of errors that are really difficult for QEC, and actually transforms the characteristics of the remaining errors to make them more compatible with QEC. Win-win!\nLooking to the future we see that a holistic approach to dealing with noise and errors in quantum computers is essential. Quantum Error Correction is a core part of the story, and combined with performance-boosting quantum firmware we see a clear pathway to the future of large-scale quantum computers.\nFor a deeper dive into the intersection of QEC and Quantum Firmware.\nDiscover how you can use Fire Opal, our effective error suppression technology for real quantum computers, to improve the success of quantum algorithms without any user intervention, hardware knowledge, or configuration.\nLearn how Quantum Error Correction can enable the quantum computing revolution and the vital role it plays in the future of large-scale quantum computers.\nQuantum firmware bridges the gap between mathematical abstractions of quantum algorithms and practical physical manipulation of imperfect quantum hardware.\nLearn how about the current \"noisy\" era of quantum computing and what it stands to deliver. Uncover what quantum computing in the NISQ era looks like today.\nLearn the fundamentals of how to build quantum algorithms for quantum computing - starting with quantum circuits and logic through to how they're measured.\nLearn the fundamentals of quantum physics for developing quantum computers and explore insights into the basic principles driving quantum innovation.\nLearn how the fragility of quantum hardware lets us detect the undetectable and discover the principles and potential applications for quantum sensing.\nLearn about the foundations of quantum computing and discover the principles for how this technology will power a transformative information age.\nTake the next step on your journey with short articles to help you understand how quantum computing and sensing will transform the world","Failure rate is the frequency with which an engineered system or component fails, expressed in failures per unit of time. It is usually denoted by the Greek letter λ (lambda) and is often used in reliability engineering.\nThe failure rate of a system usually depends on time, with the rate varying over the life cycle of the system. For example, an automobile's failure rate in its fifth year of service may be many times greater than its failure rate during its first year of service. One does not expect to replace an exhaust pipe, overhaul the brakes, or have major transmission problems in a new vehicle.\nIn practice, the mean time between failures (MTBF, 1/λ) is often reported instead of the failure rate. This is valid and useful if the failure rate may be assumed constant – often used for complex units / systems, electronics – and is a general agreement in some reliability standards (Military and Aerospace). It does in this case only relate to the flat region of the bathtub curve, which is also called the \"useful life period\". Because of this, it is incorrect to extrapolate MTBF to give an estimate of the service lifetime of a component, which will typically be much less than suggested by the MTBF due to the much higher failure rates in the \"end-of-life wearout\" part of the \"bathtub curve\".\nThe reason for the preferred use for MTBF numbers is that the use of large positive numbers (such as 2000 hours) is more intuitive and easier to remember than very small numbers (such as 0.0005 per hour).\nThe MTBF is an important system parameter in systems where failure rate needs to be managed, in particular for safety systems. The MTBF appears frequently in the engineering design requirements, and governs frequency of required system maintenance and inspections. In special processes called renewal processes, where the time to recover from failure can be neglected and the likelihood of failure remains constant with respect to time, the failure rate is simply the multiplicative inverse of the MTBF (1/λ).\nA similar ratio used in the transport industries, especially in railways and trucking is \"mean distance between failures\", a variation which attempts to correlate actual loaded distances to similar reliability needs and practices.\nFailure rates are important factors in the insurance, finance, commerce and regulatory industries and fundamental to the design of safe systems in a wide variety of applications.\nFailure Rate Data\nFailure rate data can be obtained in several ways. The most common means are:\n- From field failure rate reports, statistical analysis techniques can be used to estimate failure rates. For accurate failure rates the analyst must have a good understanding of equipment operation, procedures for data collection, the key environmental variables impacting failure rates, how the equipment is used at the system level, and how the failure data will be used by system designers.\n- Historical data about the device or system under consideration\n- Many organizations maintain internal databases of failure information on the devices or systems that they produce, which can be used to calculate failure rates for those devices or systems. For new devices or systems, the historical data for similar devices or systems can serve as a useful estimate.\n- Government and commercial failure rate data\n- Handbooks of failure rate data for various components are available from government and commercial sources. MIL-HDBK-217F, Reliability Prediction of Electronic Equipment, is a military standard that provides failure rate data for many military electronic components. Several failure rate data sources are available commercially that focus on commercial components, including some non-electronic components.\n- Time lag is one of the serious drawbacks of all failure rate estimations. Often by the time the failure rate data are available, the devices under study have become obsolete. Due to this drawback, failure-rate prediction methods have been developed. These methods may be used on newly-designed devices to predict the device's failure rates and failure modes. Two approaches have become well known, Cycle Testing and FMEDA.\n- Life Testing\n- The most accurate source of data is to test samples of the actual devices or systems in order to generate failure data. This is often prohibitively expensive or impractical, so that the previous data sources are often used instead.\n- Cycle Testing\n- Mechanical movement is the predominant failure mechanism causing mechanical and electromechanical devices to wear out. For many devices, the wear-out failure point is measured by the number of cycles performed before the device fails, and can be discovered by cycle testing. In cycle testing, a device is cycled as rapidly as practical until it fails. When a collection of these devices are tested, the test will run until 10% of the units fail dangerously.\n- Failure modes, effects, and diagnostic analysis (FMEDA) is a systematic analysis technique to obtain subsystem / product level failure rates, failure modes and design strength. The FMEDA technique considers:\n- All components of a design,\n- The functionality of each component,\n- The failure modes of each component,\n- The effect of each component failure mode on the product functionality,\n- The ability of any automatic diagnostics to detect the failure,\n- The design strength (de-rating, safety factors) and\n- The operational profile (environmental stress factors).\nGiven a component database calibrated with field failure data that is reasonably accurate , the method can predict product level failure rate and failure mode data for a given application. The predictions have been shown to be more accurate than field warranty return analysis or even typical field failure analysis given that these methods depend on reports that typically do not have sufficient detail information in failure records. Failure modes, effects, and diagnostic analysis\nFailure Rate in the Discrete Sense\nThe failure rate can be defined as the following:\n- The total number of failures within an item population, divided by the total time expended by that population, during a particular measurement interval under stated conditions. (MacDiarmid, et al.)\nAlthough the failure rate, , is often thought of as the probability that a failure occurs in a specified interval given no failure before time , it is not actually a probability because it can exceed 1. Erroneous expression of the failure rate in % could result in incorrect perception of the measure, especially if it would be measured from repairable systems and multiple systems with non-constant failure rates or different operation times. It can be defined with the aid of the reliability function, also called the survival function, , the probability of no failure before time .\n- , where is the time to (first) failure distribution (i.e. the failure density function).\nover a time interval = from (or ) to . Note that this is a conditional probability, where the condition is that no failure has occurred before time . Hence the in the denominator.\nHazard rate and ROCOF (rate of occurrence of failures) are often incorrectly seen as the same and equal to the failure rate.[clarification needed] To clarify; the more promptly items are repaired, the sooner they will break again, so the higher the ROCOF. The hazard rate is however independent of the time to repair and of the logistic delay time.\nFailure rate in the continuous sense\nCalculating the failure rate for ever smaller intervals of time results in the hazard function (also called hazard rate), . This becomes the instantaneous failure rate or we say instantaneous hazard rate as approaches to zero:\nA continuous failure rate depends on the existence of a failure distribution, , which is a cumulative distribution function that describes the probability of failure (at least) up to and including time t,\nwhere is the failure time. The failure distribution function is the integral of the failure density function, f(t),\nThe hazard function can be defined now as\nMany probability distributions can be used to model the failure distribution (see List of important probability distributions). A common model is the exponential failure distribution,\nwhich is based on the exponential density function. The hazard rate function for this is:\nThus, for an exponential failure distribution, the hazard rate is a constant with respect to time (that is, the distribution is \"memory-less\"). For other distributions, such as a Weibull distribution or a log-normal distribution, the hazard function may not be constant with respect to time. For some such as the deterministic distribution it is monotonic increasing (analogous to \"wearing out\"), for others such as the Pareto distribution it is monotonic decreasing (analogous to \"burning in\"), while for many it is not monotonic.\nDecreasing Failure Rate\nA decreasing failure rate (DFR) describes a phenomenon where the probability of an event in a fixed time interval in the future decreases over time. A decreasing failure rate can describe a period of \"infant mortality\" where earlier failures are eliminated or corrected and corresponds to the situation where λ(t) is a decreasing function.\nFor a renewal process with DFR renewal function, inter-renewal times are concave. Brown conjectured the converse, that DFR is also necessary for the inter-renewal times to be concave, however it has been shown that this conjecture holds neither in the discrete case nor in the continuous case.\nIncreasing failure rate is an intuitive concept caused by components wearing out. Decreasing failure rate describes a system which improves with age. Decreasing failure rates have been found in the lifetimes of spacecraft, Baker and Baker commenting that \"those spacecraft that last, last on and on.\" The reliability of aircraft air conditioning systems were individually found to have an exponential distribution, and thus in the pooled population a DFR.\nCoefficient of variation\nWhen the failure rate is decreasing the coefficient of variation is ⩾ 1, and when the failure rate is increasing the coefficient of variation is ⩽ 1. Note that this result only holds when the failure rate is defined for all t ⩾ 0 and that the converse result (coefficient of variation determining nature of failure rate) does not hold.\nFailure rates can be expressed using any measure of time, but hours is the most common unit in practice. Other units, such as miles, revolutions, etc., can also be used in place of \"time\" units.\nFailure rates are often expressed in engineering notation as failures per million, or 10−6, especially for individual components, since their failure rates are often very low.\nThe Failures In Time (FIT) rate of a device is the number of failures that can be expected in one billion (109) device-hours of operation. (E.g. 1000 devices for 1 million hours, or 1 million devices for 1000 hours each, or some other combination.) This term is used particularly by the semiconductor industry.\nThe relationship of FIT to MTBF may be expressed as: MTBF = 1,000,000,000 x 1/FIT.\nUnder certain engineering assumptions (e.g. besides the above assumptions for a constant failure rate, the assumption that the considered system has no relevant redundancies), the failure rate for a complex system is simply the sum of the individual failure rates of its components, as long as the units are consistent, e.g. failures per million hours. This permits testing of individual components or subsystems, whose failure rates are then added to obtain the total system failure rate.\nAdding \"redundant\" components to eliminate a single point of failure improves the mission failure rate, but makes the series failure rate (also called the logistics failure rate) worse—the extra components improve the mean time between critical failures (MTBCF), even though the mean time before something fails is worse.\nSuppose it is desired to estimate the failure rate of a certain component. A test can be performed to estimate its failure rate. Ten identical components are each tested until they either fail or reach 1000 hours, at which time the test is terminated for that component. (The level of statistical confidence is not considered in this example.) The results are as follows:\nEstimated failure rate is\nor 799.8 failures for every million hours of operation.\n- Electrical & Mechanical Component Reliability Handbook. exida. 2006.\n- Goble, William M.; Iwan van Beurden (2014). Combining field failure data with new instrument design margins to predict failure rates for SIS Verification. Proceedings of the 2014 International Symposium - BEYOND REGULATORY COMPLIANCE, MAKING SAFETY SECOND NATURE, Hilton College Station-Conference Center, College Station, Texas.\n- W. M. Goble, \"Field Failure Data – the Good, the Bad and the Ugly,\" exida, Sellersville, PA \n- Finkelstein, Maxim (2008). \"Introduction\". Failure Rate Modelling for Reliability and Risk. Springer Series in Reliability Engineering. pp. 1–84. doi:10.1007/978-1-84800-986-8_1. ISBN 978-1-84800-985-1.\n- Brown, M. (1980). \"Bounds, Inequalities, and Monotonicity Properties for Some Specialized Renewal Processes\". The Annals of Probability. 8 (2): 227–240. doi:10.1214/aop/1176994773. JSTOR 2243267.\n- Shanthikumar, J. G. (1988). \"DFR Property of First-Passage Times and its Preservation Under Geometric Compounding\". The Annals of Probability. 16 (1): 397–406. doi:10.1214/aop/1176991910. JSTOR 2243910.\n- Brown, M. (1981). \"Further Monotonicity Properties for Specialized Renewal Processes\". The Annals of Probability. 9 (5): 891–895. doi:10.1214/aop/1176994317. JSTOR 2243747.\n- Yu, Y. (2011). \"Concave renewal functions do not imply DFR interrenewal times\". Journal of Applied Probability. 48 (2): 583–588. arXiv:1009.2463. doi:10.1239/jap/1308662647.\n- Proschan, F. (1963). \"Theoretical Explanation of Observed Decreasing Failure Rate\". Technometrics. 5 (3): 375–383. doi:10.1080/00401706.1963.10490105. JSTOR 1266340.\n- Baker, J. C.; Baker, G. A. S. . (1980). \"Impact of the space environment on spacecraft lifetimes\". Journal of Spacecraft and Rockets. 17 (5): 479. Bibcode:1980JSpRo..17..479B. doi:10.2514/3.28040.\n- Saleh, Joseph Homer; Castet, Jean-François (2011). \"On Time, Reliability, and Spacecraft\". Spacecraft Reliability and Multi-State Failures. p. 1. doi:10.1002/9781119994077.ch1. ISBN 9781119994077.\n- Wierman, A.; Bansal, N.; Harchol-Balter, M. (2004). \"A note on comparing response times in the M/GI/1/FB and M/GI/1/PS queues\" (PDF). Operations Research Letters. 32: 73–76. doi:10.1016/S0167-6377(03)00061-0.\n- Gautam, Natarajan (2012). Analysis of Queues: Methods and Applications. CRC Press. p. 703. ISBN 978-1439806586.\n- Xin Li; Michael C. Huang; Kai Shen; Lingkun Chu. \"A Realistic Evaluation of Memory Hardware Errors and Software System Susceptibility\". 2010. p. 6.\n- \"Reliability Basics\". 2010.\n- Vita Faraci. \"Calculating Failure Rates of Series/Parallel Networks\". 2006.\n- \"Mission Reliability and Logistics Reliability: A Design Paradox\".\n- Goble, William M. (2018), Safety Instrumented System Design: Techniques and Design Verification, Research Triangle Park, NC 27709: International Society of AutomationCS1 maint: location (link)\n- Blanchard, Benjamin S. (1992). Logistics Engineering and Management (Fourth ed.). Englewood Cliffs, New Jersey: Prentice-Hall. pp. 26–32. ISBN 0135241170.\n- Ebeling, Charles E. (1997). An Introduction to Reliability and Maintainability Engineering. Boston: McGraw-Hill. pp. 23–32. ISBN 0070188521.\n- Federal Standard 1037C\n- Kapur, K. C.; Lamberson, L. R. (1977). Reliability in Engineering Design. New York: John Wiley & Sons. pp. 8–30. ISBN 0471511919.\n- Knowles, D. I. (1995). \"Should We Move Away From 'Acceptable Failure Rate'?\". Communications in Reliability Maintainability and Supportability. International RMS Committee, USA. 2 (1): 23.\n- MacDiarmid, Preston; Morris, Seymour; et al. (n.d.). Reliability Toolkit (Commercial Practices ed.). Rome, New York: Reliability Analysis Center and Rome Laboratory. pp. 35–39.\n- Modarres, M.; Kaminskiy, M.; Krivtsov, V. (2010). Reliability Engineering and Risk Analysis: A Practical Guide (2nd ed.). CRC Press. ISBN 9780849392474.\n- Mondro, Mitchell J. (June 2002). \"Approximation of Mean Time Between Failure When a System has Periodic Maintenance\" (PDF). IEEE Transactions on Reliability. 51 (2): 166–167. doi:10.1109/TR.2002.1011521.\n- Rausand, M.; Hoyland, A. (2004). System Reliability Theory; Models, Statistical methods, and Applications. New York: John Wiley & Sons. ISBN 047147133X.\n- Turner, T.; Hockley, C.; Burdaky, R. (1997). The Customer Needs A Maintenance-Free Operating Period. 1997 Avionics Conference and Exhibition, No. 97-0819, P. 2.2. Leatherhead, Surrey, UK: ERA Technology Ltd.\n- U.S. Department of Defense, (1991) Military Handbook, “Reliability Prediction of Electronic Equipment, MIL-HDBK-217F, 2"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:c0355044-ecac-4637-a513-85de5dd13ef8>","<urn:uuid:49583b26-2c35-46f6-90fd-02a202067e06>"],"error":null}
{"question":"What role does wetness play in the spread of apple scab versus late blight?","answer":"Both diseases require wetness for spreading, but in different ways. Apple scab requires specific wetness periods depending on temperature - for example, 9-14 hours of wetness are needed for infection at 50-60°F, while at higher temperatures (61-75°F) only 9 hours are needed. For late blight, free water must be present on plants for infection to occur, and the pathogen produces sporangia that spread easily in wet or humid conditions through water, wind, and wind-blown rain up to 20km away.","context":["Apple scab is one of the more serious diseases of ornamental crabapples. It is caused by the fungus Venturia inaequalis. Apple scab mainly affects members of the rose family, including nearly all commercial cultivars of apple (Malus spp.), crabapple (Malus spp.), hawthorn (Crataegus spp.), mountain-ash (Sorbus spp.), cotoneaster (Cotoneaster spp.), firethorn (Pyracantha spp.), and common pear (Pyrus spp.).\nCONDITIONS FAVORING DISEASE\nApple scab is most severe during spring and early summer when the humidity is high and the temperature is moderate. Overwintering fungal spores (ascospores) are produced in the diseased leaves on the ground. In most years, the first fungal spores (primary inoculum) mature and are capable of causing infections in the spring at about the time of budbreak (leaf expansion). Fungal spores are expelled into the air following rainfall and continue to be discharged over a period of 1-3 months. The peak period of spore dispersal often occurs near the end of bloom (pink to full-bloom stages). Whether infection occurs or not depends on the period of wetness and the temperature. Fewer hours of wetness are required for infection at high temperatures than at low. For example, at 43 degrees F, a 25-hour period of wetness is required for infection; whereas only a 9-hour wetness period is needed between 61 degrees and 75 degrees F. The severity of disease increases with the duration of wetting. According to Mill’s Chart (W.D. Mills, Cornell University), 9-14 hours of wetness are needed for initial infection to occur in the typical northern Illinois spring temperature range of 50-60 degrees F.\nOnce the fungus has become established on the host, it produces secondary spores (conidia) which help to re-infect new leaves throughout the summer. Conidia are disseminated by splashing rain or irrigation and wind to new leaf or fruit surfaces, and give rise to new lesions. Several “secondary” cycles may occur during the growing season if wet weather prevails during the summer.\nThe best way to prevent apple scab is to plant resistant crabapples. Many species, cultivars, and varieties of Malus are resistant to the scab fungus. Some crabapples especially resistant to apple scab include M. ‘Adirondack’, M. baccata ‘Jackii’, M. ‘Beverly’, M. ‘Dolgo’, M. ‘Donald Wyman’, M. ‘Mary Potter’, M. ‘Molazam’ (Molten Lava), M. ‘Prairifire’, M. ‘Red Jewel’. M. ‘Sutyram’ (Sugar Tyme), M. ‘White Angel’, and M. ‘Zumi Wooster’\nThe Morton Arboretum publication Crabapples for the Home Landscape provides information on selecting crabapples.\nThe apple scab fungus overwinters on fallen leaves and infected twigs so collecting and removing or composting these leaves and twigs will reduce the source of infection. Sanitation practices, such as leaf litter removal and pruning, should be done in the fall or winter before bud break occurs. Earthworms and litterdecomposing microorganisms degrade fallen leaves and help reduce the overwintering population of the scab fungus. Natural leaf decomposition can be accelerated by composting leaves (piles must be mixed regularly and reach temperatures of at least 120 degrees F throughout) and applying nitrogen fertilizer (e.g., urea) to the leaves in the fall.\nFungicide control programs for scab should be integrated with sanitation and other cultural management practices. Apple scab can be effectively managed with fungicides by controlling primary infections. It is important that sprays are applied according to plant development, with the first spray at bud swell and additional sprays at 10-to-14-day intervals. The number of fungicide spray applications required varies with many factors, including weather conditions (rainfall), the susceptibility of the plant, the rate of plant growth development, the fungicide used, and the amount of fungal inoculum present.\nREAD LABEL INSTRUCTIONS ON CONTAINER FOR DILUTION RATES AND METHODS OF APPLICATION\nMancozeb (many products)\nThiophanate-methyl (many products)\nRefer to the Illinois Urban Pest Management Handbook (University of Illinois Cooperative Extension Service) for a complete listing of chemical recommendations. Use pesticides safely and wisely; read and follow label directions\nThe pesticide information presented in this publication is current with federal and state regulations. The user is responsible for determining that the intended use is consistent with the label of the product being used.\nThe information given here is for educational purposes only. Reference to commercial products or trade names is made with the understanding that no discrimination is intended and no endorsement made by The Morton Arboretum.","Susan Smith P. Ag., Ministry of Agriculture\nSolanaceous crops such as potatoes and tomatoes are hosts to late blight, the most destructive disease of potatoes in British Columbia. The pathogen causing late blight, Phytophthora infestans, thrives and produces spores under humid or moist environmental conditions. It causes infection only when free water is present on plants. Our wet weather conditions and cool to moderate temperatures make Lower Mainland BC an area where late blight can progress and spread very rapidly in Solanaceous crops grown both commercially and in home gardens.\nOn potatoes, symptoms of late blight first appear as large, soft, water-soaked spots with a light green halo on the leaves. Symptoms will also commonly develop on stems and leaf petioles which turn brown or black. In humid, wet conditions, a white, fuzzy mould can be seen on the underside of the leaves. As the disease progresses, the whole plant may die back to the ground. If late blight is present at harvest, tubers may become infected resulting in a firm brown rot that starts at the skin and eventually affects the whole tuber. Although symptoms appear slightly differently on tomatoes, the organism causing late blight is the same one seen in potatoes.\nIn addition to persisting in the soil and on infected plant tissues for a period of time, the late blight pathogen can cause infection through structures called sporangia during cool, wet, or humid weather. Sporangia can easily spread to distances of up to 20 km via water, wind, and wind-blown rain and cause infection in healthy Solanaceous plants. For this reason, when late blight is in an area, commercial potato growers are at risk of greater crop losses from disease outbreaks in the field and later when potatoes are in storage.\nPotatoes and tomatoes are popular crops for the home garden and community gardens throughout the Lower Mainland. Late blight is a disease of both potato and tomato that, if not managed for, can easily be spread from gardens to commercial potato fields in the area. This can also be a means of introducing new and more vigorous strains (genotypes) of the late blight pathogen. To minimize and hopefully prevent this from occurring, there are steps that gardeners should be taking to help reduce the pressure of harmful and costly po- tato diseases to local growers.\nIn 2014, the BC Ministry of Agriculture and Agriculture and Agri-Food Canada – through Growing Forward 2, a federal-provincial-territorial initiative – were pleased to fund an education and awareness program for home and community gardeners. The BC Potato and Vegetable Growers Association developed information materials and delivered outreach events to raise awareness of harmful plant pests of commonly grown solanaceous crops. In addition, management tips were provided to reduce the risks to neighbouring commercial potatoes. The brochure developed can be found on the BC Certified Seed Potato Growers website\nPrevention of Solanaceous diseases starts by planting disease-free tomato transplants and potato seed. Grow- ers are strongly encouraged to only plant certi ed seed potatoes and to never plant potatoes grown for tablestock or potatoes purchased for eating. This should be avoided because a potato tuber is a part of its mother plant and can carry its diseases and viruses. Certified seed potatoes (both the mother plant and its tubers) are inspected and tested to ensure that they do not exceed accepted levels of critical diseases. Non-certified potatoes have the potential to carry many economically harmful diseases – and when used as seed, can be the source of problematic viruses that could spread to nearby potato crops. Using certified seed potatoes is a good start for a successful disease-free crop.\nOther important practices for preventing and minimizing the severity of late blight in potatoes and tomatoes include:\n- Minimizing humidity by avoiding overhead irrigation\n- Growing short-season varieties during periods where weather conditions are less favourable to late blight\n- Preventively applying available, registered fungicides suitable for organic production (Note: always read and follow product label directions carefully)\n- Learning to identify the symptoms of late blight\n- Carrying out regular crop monitoring, from plant emergence to harvest, looking for late blight symptoms on the leaves, petioles, and stems of plants. Note: when disease is suspected, avoid spread of inoculum (i.e. spores) to healthy plants. Do not enter clean elds or planted areas with clothes, boots, gloves, pruning, tying equipment, vehicles, etc. that were used in planted areas infected with late blight\n- Removing infected plants and carrying out a thorough clean up of infected plant materials, unwanted tubers, fruits, and volunteer plants. Dispose by either burning or removing in sealed bags. Do not dispose of diseased plant materials in your compost.\n- Practicing crop rotation with non-solanaceous crops to minimize build-up of inoculum in the field.\nFor more information, please contact Susan Smith at the BC Ministry of Agriculture by email: Susan.L.Smith@ gov.bc.ca.\nSusan Smith, P. Ag., is Industry Specialist, Vegetables and Organics with the BC Ministry of Agriculture."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:4bea899f-4537-4558-b1e9-5ea9a8a614a6>","<urn:uuid:ee983639-9bc6-479d-881a-3e3284a832d5>"],"error":null}
{"question":"What was Poland's role in protecting Europe during major conflicts, and how did it affect their relationship with neighboring powers over time?","answer":"Poland played a crucial defensive role for Europe in several major conflicts. They stopped the Ottoman Empire's advance into Europe at the Battle of Vienna in 1683, halted the spread of Communism into western Europe in the 1920 Polish-Bolshevik War, and provided the first resistance against both Nazi and Soviet invasions in 1939. However, these defensive actions often came at a cost to Poland's relationships with neighboring powers. Their position as a buffer zone between East and West led to repeated conflicts with both German and Russian interests. This ultimately culminated in Poland being partitioned between its powerful neighbors in the late 18th century and again being divided between Nazi Germany and the Soviet Union in 1939.","context":["Poland - Guidelines\nAuthors: Biały-Ciszek Beata, Bilan Maciej, Grodzka Daria, Grochal Lucyna, Kunysz Magdalena, Pszczoła Marta, Stinia Mariusz, Wesołowska Halina, Wojtowicz-Dzień Irena, Zaufal Krystyna\nWe have divided our work into two parts. The first part shows what topics from the history of our close and distant neighbours are currently taught in Polish schools. The second part deals with the problems from the Polish history that are important to us - Poles and that we would like to make known to other European countries. When preparing the first part, we came across some difficulties that result from a multitude of history textbooks and syllabuses in Poland. Nevertheless, we tried to show only the primary topics and issues that appear in the history education process in Polish schools.\nIn the second part of our publication, we present selected issues from the history of Poland that Polish teachers consider to be important for the formation of historical identity of Poles and worth making known to Europeans. We are aware of the need to reduce factography and concentrate mainly on key problems. This is why, for the purposes of our work, we have adopted certain criteria for the selection of events: 1) what we would like to communicate about us to others, 2) what common issues we can find in the history of Europe, 3) what is important for the contemporary man, European and citizen. For these reasons, we propose a problem-oriented approach, even though the structure of our syllabuses and textbooks is chronological. We attach special importance to events and phenomena from the 20th century – we pay most attention to them, because they significantly affect the times in which we happen to live. The interpretation of historical facts is another matter. We understand that memories of the same event can be different in various countries and European regions. At the same time, we think that the incorporation of the Polish point of view would become an added value in the multi-perspective teaching of history. Therefore, we concentrate mainly on the factography that we consider useful for the common future and dialogue between European communities, following the rule that truth is the basis of dialogue rather than conflict.\nWe hope that we managed to balance political, economic and cultural aspects and point out universal values, such as freedom, equality, brotherhood and democracy, which are the heritage of European civilisation. Learning the history of Poland from the Polish perspective should help to broaden the knowledge of Europeans in this field and become the basis for a mutual understanding, including the understanding of human attitudes and behaviours in the past and nowadays. This may also become a cause for the elimination of historical stereotypes, myths and misunderstandings between Europeans. We hope that our work encourages readers to look at the history of Poland – the nation which has contributed to the European civilisation for more than ten centuries - also from the perspective of Polish citizens.\nPart I. What history of Poland’s neighbours do we teach in Polish schools?\nNeighbours of Poland over the centuries - Mariusz Stinia.\nHUNGARY – appears in the context of dynastic relations: the Angevins as a Hungarian dynasty on the Polish throne; the Jagiellons as a Polish dynasty on the Hungarian throne. Modern epoch: the reign of Stephen Báthory. 19th century: the lack of sovereign political role of Hungary; the Spring of Nations; Austria-Hungary as a dual state. 20th century: the context of processes occurring in Central & Eastern Europe in the form of maps and tables. Special attention is paid to the Hungarian Revolution of 1956.\nCZECH REPUBLIC – the origin of the Czech state and contacts of Poland with the Czech lands in the Middle Ages: the ethnogenesis of the Slavs; Great Moravia; the alliance with the state ruled by Mieszko I of Poland and the conflict concerning Silesia and Małopolska. Conflicts with Poland in the first half of the 11th century: the territorial development of Bolesław I Chrobry’s monarchy; the role of Saint Adalbert; the temporary annexation of the Czech lands and the long-term annexation of Moravia; the invasion of Poland by Bretislaus I. The last Přemyslids on the Polish throne: attempts to take over the Polish throne by the House of Luxembourg; Wenceslaus II and his reforms in Poland; the House of Luxembourg on the Czech throne and the loss of Silesia. Late period of the Middle Ages: the economic development of the Czech state, the university in Prague; Hussitism. Modern epoch: the Thirty Years’ War; the Spring of Nations in Europe and the Slavic Congress in Prague. World War I; the establishment of Czechoslovakia; the dispute with Poland concerning the Zaolzie region; the Munich conference; the Prague Spring; the peaceful disintegration of Czechoslovakia.\nSLOVAKIA – a part of the Kingdom of Hungary, a part of Czechoslovakia; the Slovak State under Monsignor Jozef Tiso; the Prague Spring; the peaceful disintegration of Czechoslovakia in 1993.\nGERMANY (including Austria until 1804 and monastic states) – the origin of Germany: the role of Charlemagne and his successors, the Treaty of Verdun. Selected aspects from the history of the German dynasty: Otto I, Otto III, Henry II, Henry V, Frederick Barbarossa. The investiture controversy between the Holy Roman Emperor Henry IV and the Pope Gregory VII. Common events from the history of Poland and Germany: wars and conflicts with the Germans during the reign of the Piast and Jagiellon dynasties: conflicts of Mieszko I with German marches, wars of Bolesław I Chrobry, wars of Bolesław III Krzywousty with the German emperor and conflicts of the last Piast and Jagiellonian rulers with the Teutonic Order; cultural and economic influences of Germany, including the colonisation of cities and villages according to the Magdeburg law, the Hanseatic League, Reformation, the role of Martin Luther, the Renaissance north of the Alps. The dynastic policy of the Jagiellons, the Habsburgs, the Hohenzollerns and the Wettins. Partitions of Poland: Polish lands annexed by Prussia and Austria. Cultural issues: Romanticism: J.W. Goethe, F. Schiller, the social thought of Marx, Engels and Nietzsche; the development of science (Freud, Koch) and economic processes: enfranchisement, industrialisation. Nazism; World War II; the Nazi occupation of Poland and other European countries; the Holocaust. The defeat of Germany and changes of borders by the decision of the “Great Three”: compulsory expulsions of Germans after 1945. The partitioning of Germany after World War II (the Iron Curtain, the Berlin Wall) and the uniting process in years 1989-90.\nRUSSIA - Irena Wojtowicz – Dzień, Maciej Bilan.\n- Kievan Rus', Veliky Novgorod, Moscow: the acceptance of baptism, the Kiev expedition of Bolesław I Chrobry in 1018, the reign of Yaroslav I the Wise, the fragmentation of Rus?.\n- The role of Moscow in the unification of the Russian lands in the 14th and the 15th century (release from the Tartar Yoke).\n- Polish-Russian relations in the 16th century: Russia under Ivan the Terrible, Livonia wars during the reign of Stephen Bathory.\n- Conflicts in the 17th century: the Dimitriad wars; the occupation of Moscow by the Polish army; the role of Russia in the formation of the Cossack issue; the war with the Commonwealth.\n- Russia in the epoch of absolutism: reforms by Peter I the Great; the Northern War.\n- Interference in internal affairs of the Commonwealth in the 18th century: the Treaty of the Three Black Eagles, participation in the partitioning of Poland, the war with the Commonwealth in defence of the Constitution of May 3, 1791, the Kościuszko Uprising.\n- Enlightened absolutism in Russia: Catherine II, exact sciences – Lomonosov.\n- Attitude towards Polish lands: Polish national liberation uprisings, Russian rule in the Kingdom of Poland; Russification.\n- Period of Napoleonic wars: participation in anti-French coalitions, the French invasion of Russia in 1812, the Congress of Vienna.\n- Crimea War and its consequences (1853-56).\n- Russia at the beginning of the 20th century: Russian Revolution (1905), World War I (1914-1918), the February Revolution and the October Revolution (1917), the Treaty of Brest-Litovsk, Bolshevik Russia.\n- Soviet Union in the interwar period: the establishment of the Soviet Union (1922), war communism, industrialisation and collectivisation, totalitarianism - Stalinism, the Treaty of Rapallo with Germany (1922), the Ribbentrop-Molotov Pact in August 1939 and the secret protocol establishing the division of Europe.\n- Participation of the Soviet Union in World War II: the Soviet invasion of Poland on 17 September 1939, countries occupied by the Soviet Union, participation in the anti-Nazi coalition,\n- Soviet Union after World War II: participation in decisions of the „Great Three”, deportation of Ukrainians from southeastern Poland, the establishment of the “Eastern block”, Stalinism, the Cold War and the arms race, the perestroika and the collapse of the Soviet Union.\nDENMARK, FINLAND, ICELAND, NORWAY, SWEDEN - Daria Grodzka, Magdalena Kunysz.\n- Normans in the Middle Ages: invasions; Normans’ role in the development of trade, intercultural contacts, the discovery of new routes, the creation of states, e.g. the Duchy of Normandy; the Varangians; Iceland – on the route of Norman’s conquests; Christianisation.\n- Economy in Europe in the 13th century and at the beginning of the 14th century: participation in European trade: trade relations with Novgorod; Hanseatic League)\n- Reformation: Lutheranism\n- Political and economic situation in the 16th century: the union of Denmark and Norway; Iceland as a Danish colony; the Livonia wars; trade with Poland\n- Thirty Years’ War\n- Vasa rulers on the Polish throne in the 17th century: the dynastic policy of Vasa rulers, Polish-Swedish wars.\n- Northern War 1700–1721\n- Enlightenment in Europe: scientific achievements - Anders Celsius, Carl Linnaeus\n- Europe in the second half of the 19th century: Poles’ emigration in search of labour\n- World War I: neutral states, the independence of Finland;\n- World War II: Finnish-Soviet war; Germany’s invasion of Denmark and Norway; the participation of Poles in the defence of Norway, the regime of Vidkun Quisling; Axis powers, states from the anti-Nazi coalition and neutral states; the Battle of the Atlantic.\n- Period after World War II: European integration;\nBELARUS - Daria Grodzka, Magdalena Kunysz.\nBelarus in the Grand Duchy of Lithuania; Belarus as a Soviet republic; the disintegration of the Soviet Union and the declaration of sovereignty; the establishment of the Commonwealth of Independent States.\nBALTIC STATES - Lucyna Grochal and Marta Pszczoła\n- Polish-Lithuanian unions: the Jagiellons on the Polish throne; wars with Teutonic Knights; the state and society; the Republic of the Two Nations\n- Livonia conflict\n- 19th century: Lithuania under Russian rule; Russification; secret organisations\n- 20th century: World War I; the recovery of independence; the Vilnius controversy between Poland and Lithuania;\n- World War II; Lithuania, Latvia and Estonia as Soviet republics; the recovery of independence; European integration\nPart II. What history of Poland do we want in history textbooks and syllabuses of European countries?\nDefence of Europe against threats\n- 1683 - the Battle of Vienna; stopping the advance of the Ottoman Empire into Europe\n- 1830 - the November Uprising; stopping the Russian intervention in Belgium\n- 1920 – the Polish-Bolshevik War; stopping the advance of Communism into western Europe\n- 1939 - resistance against the Nazi invasion on 1 September and against the Soviet invasion on 17 September\n- 15th – 18th century – the noble-class Commonwealth. Parliamentary system. Noble-class democracy.\n- 1791 – Constitution of May 3 as the first European constitution\n- 1918 – Granting of the right to vote to women in Poland\n- Polish national symbols.\n- The period of partitions of Poland. National liberation uprisings.\n- Participation of Poles in fights of other nations for independence\n- Resurrection of the Polish state in 1918. Democratic form of government. Józef Piłsudski.\n- The Nazi and Soviet occupation of Poland. The Polish secret state during World War II. The Warsaw Uprising.\n- The Yalta Conference: Central & Eastern Europe in the sphere of Soviet influence. The change of borders.\n- Resistance of societies of Central & Eastern Europe countries against the supremacy of the Soviet Union.\n- The establishment and role of the Solidarity movement in 1980.\n- The abolishment of Communism in Poland in 1989 as an impulse for the Autumn of Nations in Europe.\nOn the way to united Europe\n- The formation of the Polish state in the 10th century and its incorporation into Christian Europe.\n- The Congress of Gniezno in 1000 as an attempt to restore the unity of Europe.\n- The Republic of the Two Nations as an example of integration in Central & Eastern Europe (Polish-Lithuanian unions).\n- Colonisation in the 13th century – the establishment of municipal and rural self-governments according to the Magdeburg law.\n- Religious tolerance in the Commonwealth of Poland.\n- The Warsaw Confederation in 1573 – peace between religions.\n- The message of Polish bishops to German bishops.\n- Poland in the European Union.\n- The Jagiellonian University, founded in 1364\n- Nicolaus Copernicus\n- Polish winners of the Nobel prize:\n- Maria Skłodowska-Curie (with Piotr Curie - physics) – chemistry\n- Henryk Sienkiewicz – literature\n- Władysław Reymont – literature\n- Lech Wałęsa – Nobel Peace Prize\n- Czesław Miłosz – literature\n- Wisława Szymborska – literature.\n- Polish contribution to the development of European culture:\n- Writers: Adam Mickiewicz, Zbigniew Herbert, Witold Gombrowicz, Ryszard Kapuściński; Artists: Stanisław Wyspiański;\n- Composers: Frederick Chopin, Henryk Mikołaj Górecki, Wojciech Kilar, Krzysztof Penderecki;\n- Thinkers: John Paul II and his message to Poles and the rest of the world.\n- Discoverers and conquerors: Paweł Strzelecki, Henryk Arctowski;\n- Polish film school: Krzysztof Kieślowski, Andrzej Wajda\n- Multiculturalism on the example of towns: Kraków, Gdańsk, Wrocław, Lvov, Vilnius.\n- Ribbentrop-Molotov Pact.\n- Auschwitz-Birkenau. Poles on the list of the Righteous among the Nations - Irena Sendlerowa. Jan Karski.\n- The Katyń Crime. Siberia prisoners. Kolyma\n- Compulsory resettlements after World War II, including Germans, Poles, Ukrainians etc.\n- After 2000 – Poland’s engagement in the global war with terrorism (missions in Afghanistan and Iraq).","|God comes to Poland|\n1000 years of Polish history commenced when a political move brought Poland into being. In 965, an ambitious and insightful Slavic leader Mieszko I married a Bohemian princess and converted to Christianity in order to counter the growing strength of his German neighbors to the west.\nBy inviting the Holy Roman Empire into the region the following year, Mieszko established the Piast dynasty and ensured a separate identity for the lands that soon came to be known as Poland. That diplomatic gain brought political and economic ones as well: Poles mimicked the Church administration, and quickly put to use the new ideas and technology that now flowed in from the west. With one baptizing stroke, Poland took a position which it would hold for centuries: as a buffer zone between the West and the East, its fortunes would rise and fall in a manner unique in European history.\n|Individuality to a fault|\nFollowing its befitting beginning with the blessing of Christianity, Poland soon exhibited another national trait: the individual is king. In 1138, Poland was parceled out among several Piast sons, fragmenting it geographically and politically. As could be expected, the brothers began to fight among themselves, leaving Poland incapable of fighting off outsiders. The state grew weaker and weaker over the next 200 years, losing the Pomeranian region to the north and yielding to the more subtle threat of a germanized Silesia.\nTo counter these losses, Poland invited the Teutonic Knights in to protect its eastern and northern flank. The Knights accepted, set up shop throughout the north, and eventually grew into a threat of their own. In 1308, they took Gdansk, renamed it Danzig, and built it into a thriving trade center. By settling Germans in the area, the Teutonic Order precipitated a tug-of-war over the region which lasted until 1945. Cut off from the sea, fragmented, growing weaker each year, it was not until the early part of the 14th century that the tide turned when Wladyslaw I and his son Kazimierz III reunified and reformed Poland.\nKazimierz III in particular altered Poland, from the ground up.He codified the laws, he reformed the administration, he organized provincial governments, he established Jagiellonian University – one of the first in Europe, and he welcomed the pogrom-plagued Jews from the west. As the old saying goes, Kazimierz “received Poland of wood and left it of stone”.\n|Poland for everyone|\nYet his heir, a nephew from Hungary, did not last long. Louis I was quickly tossed over for his 10 year old daughter Jadwiga who was then wedded to the Grand Duke of Lithuania, Ladislaus Jagiello. Like all royal marriages, the personal union solidified a political one: the Poland-Lithuania alliance of 1386 created a multinational state that would thrive for some time to come. Although Jadwiga died in her early 20’s, Jagiello went on to rule Poland for the next 4 decades and his dynasty lasted until 1572.\nUnder his dynasty, Poland progressed militarily, culturally, and politically. Its military strength grew sufficient to decisively defeat the Teutonic Knights at Grunwald in 1410 – the largest medieval battle in history – weakening their hold over the north and allowing Poland to regain limited power over Danzig. But as that now familiar threat receded, a second developed to the East as Poland became increasingly engaged with the up-and-coming Muscovite czars. The balancing act between the East and the West had begun in earnest, and was to continue until the present day.\nCulturally, Poland welcomed the Renaissance as did the rest of Europe, and its strong ties with Italy left their mark. Even today, Italianate architecture survives in Poland, the reminders of a time passed when Polish nobles learned in Italian universities, and Italian artisans thrived in Polish cities like Zamosc. At that time, a quarter of the population could read and write, a literacy rate which was unmatched in the rest of Europe. Enhancing these intellectual advances, the Reformation left Poland more religiously tolerant than ever. Its already growing number of non-Catholics swelled, making it one of the most diversely populated countries of that time. Politically, Poland also marched ahead. Again giving voice to its faith in the individual, it enfranchised the nobility and established the first representative body in Europe. The Sejm of 1493was further strengthened in 1505 with a constitutionally organized two house system and its power grew sufficient to take the reins when the last Jagiellonian king died without an heir in 1572.\n|Democracy of a sort|\nLeft without an obvious next head of state, Poland took it upon itself to elect one. Thus began the First Republic of Poland which survived until the final partition of 1795. An apparent advance on one level, the Sejm cost Poland on others. In theory, the enfranchised nobility represented 10% of Poland’s population, but in practice, a powerful few controlled them all. Those few solidified their power by weakening the king’s, leaving the strength of the elected position dependent upon the individual that held it. If they didn’t want a strong king, they didn’t elect one.\nThe Republic also ensured that the feudal state survived, and it kept capitalism captive in the mostly German and Jewish dominated cities. Self-interested nobles failed to raise the taxes to support a standing army, and the advances achieved under the Jagiellonian dynasty quickly crumbled away. In the mid-17th century, Poland was invaded and pillaged by Sweden. The resulting war devastated the country and the people.\nBut for a brief time afterwards, Poland re-emerged under the reign of Jan III Sobieski, most noted for his decisive role in the Battle of Vienna. In 1683, the Ottoman Empire had advanced northward, eventually laying siege to Vienna. Considered unbeatable at the time, a Christian Europe was given up for lost until Sobieski showed on the scene. Saving the Austrian Empire cost Sobieski on another front; the Prussians took advantage of the battle-wearied Poland, which barely resisted.\nAlso under the First Republic, Sejm members began to misuse one of their most serious privileges: the right of veto. Assuming that an elected body acted according to the wishes of the people, all decisions were to be unanimous. This meant that any member could veto an act of the Sejm, voiding it for good. The first time such a veto was used, in 1652, the dissenting deputy left and the session ended without final approval of any legislation. An idea with the best of intentions in practice led to the downfall and eventual loss of the country itself. With Sejm business at the mercy of any self-interested member, the resulting fragmented power base left an opening for the land-hungry empires of Prussia-Germany, Austria-Hungary, and Russia.\nEach in their own time began to chip away at the unity of Poland, buying a vote here and there, and eventually bought enough to bring governance to a standstill. By the time Poles woke up, it was a bit too late. The first partition of Poland took place in 1772 when Russia realized it would be easier to cede land to Austria and Prussia than fight for it. The shock did Poland some good, and in the next twenty years a mini-Renaissance briefly revived the country and culminated in the first Constitution in Europe in 1791.\nBut the neighboring powers were not satisfied with their initial land grabs, so in 1793 and 1795 they finished what they started and Poland as a political and geographic entity ceased to exist. It was not to emerge again until the end of World War I.\n|Erased, but not forgotten|\nDisappearing from the map of Europe for over a hundred years forced Poles to rethink the basis of their identity. Each of the three powers ruled their ill-gotten lands differently, but in attempting to achieve the same end — eradicate Polish culture — they ensured failure because what was once a state now became a people.\nInitially, Poles tried to physically regain their sovereignty – one uprising of in 1793 was led by Tadeusz Kosciuszko who also fought for American independence. But the many uprisings failed, and the one bright spark lit by Napoleon went out with his own flame.\nEventually, Poles yielded on the surface, and while many emigrated, others dug in and began channeling their frustrated desires into a new area of expression: the arts. During the Partitions, some of Poland’s greatest literature gave masked voice to their barely-hidden longings, wrapping up dangerous political wishes in subtle symbolism. Adam Mickiewicz’s work became legendary while Henryk Sienkiewicz’s Quo Vadis sparked imaginations everywhere.\n|Independent at last: the Second Republic|\nPoland landed back on the map of Europe when the three empires destroyed one another in World War I. Following the war, its borders shifted somewhat over the next few years, but one early decision proved fatal 20 years later. Given the strongly German population of Danzig and the surrounding Pomeranian region, the Treaty of Versailles established Danzig as a ‘free city’ and east Pomerania as German. But Poland required access to the sea, and was granted it as a thin sliver which isolated these German lands from the Fatherland. Reclaiming them in 1939, Hitler ignited World War Two.\nBut in the interim, Poland forged full-steam ahead to make up for lost time. With a great deal of energy and desire, Poland went about rebuilding its industries and its military. Headed by Jozef Pilsduski, its army successfully repelled the ever-ambitious Soviet Union in 1920 and at the same time expanded its borders eastward to encompass parts of Lithuania and Ukraine. Yet the considerable advances were not enough. Poland resisted but could not repel the dual-pronged invasion of Germany and the Soviet Union in 1939. Once again, it fell prey to the ambitions of its powerful neighbors at a cost higher than any paid previously.\nSkipping over the obvious, some lesser known facts about Poland’s role during World War II may prove illuminating. First, it supported a resistance movement larger than any other in Europe. Sensitized by the Partitions, Poles possibly felt they were fighting for their country in a way no other European could appreciate. Second, the worst of the camps existed and the greatest number of victims were claimed here. This horrific distinction rests on the simple fact that Poland’s long history of religious and cultural tolerance resulted in the largest Jewish population in Europe. In contrast to received opinion, Poles did aid and abet their neighbors and friends, to the degree that such aid was punishable by death. That distinction was also unique to Poland.\nThird, the Soviet Union skillfully played its expansionist card throughout the war. With a mind to move westward, the Russians rounded up Poles and carted them off to the east, or simply shot the more promising types – 4,231 Polish officers – at the Katyn massacre in 1940. On the political front, Stalin established a pseudo-Polish communist party which later served as the backbone for the emergent Polish Worker’s Party. Yet the most troublesome of his antics took place near the end of the war. In 1944, Soviet soldiers sat by while the cream of the Polish military exhausted itself against the Nazis for the better part of three months in the Warsaw Uprising. Defeated, Poles helplessly watched as the now retreating German army systematically destroyed 85% of Warsaw over the next two months. Having been camped across the river for the better part of 6 months, the Soviet army finally crossed it to enter Warsaw in January of 1945. The brutal self-interest behind such a decision is still difficult to accept.\nLast, Poland lost more than any other country involved in the War: 25% of its population, its capital in ruins, its previously diverse population now almost 100% Polish, and its political future determined without so much as one free vote.\n|Occupied again: The People’s Republic|\nCommunism came to Poland, but was never invited. Stalin’s pseudo-Polish ‘Union of Polish Patriots’, headed by Boleslaw Bierut, grabbed power as the retreating Nazis relinquished it. Teaming up with the domestic product, the ‘Polish Worker’s Party’ was formed and headed by Wladyslaw Gomulka. To signal a new era, the communist rulers removed the crown from the Polish eagle. In 1947, a faked election let the world know that Poland ‘chose’ communism, and in 1948 the ruling ‘Polish United Worker’s Party’ (PZPR) was established.Regardless of name, the game was the same. The Soviet Union was determined to maintain its expanded sphere of influence as the Cold War commenced, and it did so by hook and crook for the next 40 years.\nUnlike the Soviet Union proper, Poland did experience some internal independence. For instance, deposed leaders were not assassinated, purges stopped short of outright genocide, and suppression only went so deep. Most importantly, the Church survived and even flourished as a counterpoint to Soviet repression. Things weren’t all bad: Poland did manage to rebuild its war-devastated iron, steel, shipping, and mining industries. But it never regained a decent standard of living and it was that failure, primarily in the form of sky-rocketing food prices, which eventually toppled Soviet rule.\nThe first sign of discontent surfaced in 1956 when Khrushchev opened the door himself by admitting Stalin’s crimes in February of that year. By June, strikes broke out in Poznan, and in October a reform-promising Gomulka was elected without the stamp of Moscow approval. This unheard of defiance elicited a visit from Khrushchev coupled with several armies massing at the Polish border, but Gomulka effectively deflected the threat. The openness and reforms which followed lasted about as long as any decent cynic would expect, and quickly enough, everything went back to normal. After another decade of this, high food prices again sparked unrest in Gdansk in 1970 but this time around the solution proved more dangerous.\nEarlier that year, the chancellor of West Germany, Willy Brandt, visited Poland and opened the east to the west for the first time since the war. Grabbing the opportunity, Party Secretary Gierek began borrowing money, and he borrowed Poland into debt, leaving it far worse off than before. When the food prices announced several years later proved far higher than those which sparked the borrowing in the first place, Gierek was ready.\nBy suppressing all resistance, the government ensured the birth of the eventually fatal union between the workers, the intellectuals, and the Church. Historically, once the students and the workers get together with the clergy, serious counter-trouble starts.\nAnd did, in the early 1980’s. Unrelated, yet so connected, was the election of the Polish-born Pope John Paul II in 1979. Spiritually supported, Poles became bolder. Another price rise sparked yet another strike in Gdansk, and continuing unrest paved the way for some cooperation. The Gdansk shipbuilders wrote out their demands – the 21 Points – and the government agreed to them in August of 1980. With that, the Solidarity Trade Union was born.\nUnfortunately, Poland’s neighbors didn’t take kindly to this development, started complaining to the Kremlin, and the countermeasures began. They ended with martial law in December of 1981: Solidarity was banned, its leaders jailed, and life went back to normal. But not for good: the underlying discontent with life under Soviet hegemony resurfaced again and again, until strikes in 1988 forced the government to negotiate. The Round Table Talks followed in 1989 and soon thereafter the first noncommunist government in Central Europe since WWII was installed."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:dd3d5fbb-49cc-47e5-ba8c-310acc377c23>","<urn:uuid:d763a9ab-3481-459b-a172-686340672fea>"],"error":null}
{"question":"What are the key differences between the autoimmune responses in Juvenile Idiopathic Arthritis (JIA) and Celiac disease in terms of how they affect children's bodies?","answer":"In JIA, the autoimmune response causes chronic synovial inflammation in joints, leading to joint pain, swelling, and restricted range of motion. The immune system attacks the joints, which can result in joint degeneration and loss of articular cartilage. In contrast, in Celiac disease, the immune system has an abnormal reaction specifically to gluten, causing damage to the small intestine's lining. This intestinal damage prevents proper nutrient absorption, leading to malnutrition and growth problems in children.","context":["Arthritis, Juvenile Idiopathic (Rheumatoid)\nChronic synovial inflammation of unknown etiology in at least 1 joint, for at least 6 weeks. Age of onset must be <16 years old.\nJuvenile idiopathic arthritis (JIA) is classified as one of 7 subtypes:\n- Oligoarticular arthritis affects <5 joints during the first 6 months of the disease. Tends to involve large joints, especially the knee. Peak age of onset is 1–6 years; 80% are antinuclear antibody (ANA)-positive:\n- Persistent oligoarticular JIA remains in <5 joints.\n- Extended oligoarticular JIA spreads to involve 5 or more joints. Has worse prognosis than persistent oligoarthritis.\n- Polyarticular juvenile idiopathic arthritis affects ≥5 joints. Can occur at any age: Peak ages of onset are 1–4 years and 7–10 years.\n- Rheumatoid factor–positive (RF+) polyarticular juvenile idiopathic arthritis is like adult-onset idiopathic arthritis that occurs in a child. Often quite aggressive\n- Rheumatoid factor–negative (RF−) polyarticular juvenile idiopathic arthritis is usually less aggressive and easier to control.\n- Systemic-onset idiopathic juvenile arthritis\n- Characterized by high, spiking quotidian or diquotidian fevers and an evanescent pink/salmon-colored macular rash\n- Affected children may also have lymphadenopathy, hepatosplenomegaly, pericarditis, or pleuritis.\n- Arthritis may not appear until weeks to months after the onset of the systemic symptoms.\n- Can occur at any age\n- Enthesitis-related arthritis (ERA)\n- Entheses (e.g., osteotendinous junctions, osteoligamentous junctions) are sites where tendons or ligaments attach to bone.\n- ERA generally affects boys in late childhood or adolescence.\n- Many are human leukocyte antigen-B27–positive.\n- Psoriatic arthritis is associated with psoriasis. It often begins in a few joints and then becomes polyarticular. It often involves small joints of hands and feet, as well as knees. Dactylitis is seen in nearly 50% of patients.\n- Incidence ranges from 1 to 22/100,000/year.\n- Affects ∼70,000–100,000 children in the United States\n- Prevalence ranges from 8 to 150/100,000; varies, but is thought to be ∼1/1,000\n- Girls are affected twice as often as boys, but boys are affected more frequently with ERA.\n- ∼50% of children with JIA have the oligoarticular type.\n- 30% have the polyarticular type.\n- 10% have systemic-onset JIA.\n- Rare in siblings, but many studies have demonstrated increased frequencies of various human leukocyte antigen markers in JIA.\n- Each marker may be associated with a different subtype of JIA:\n- Human leukocyte antigen-DR4: RF+ polyarticular JIA\n- Human leukocyte antigen-DR1: oligoarticular disease without uveitis\n- Human leukocyte antigen-DR5: oligoarticular JIA with uveitis\n- Human leukocyte antigen-B27: ERA\n- Human leukocyte antigen-A2: early-onset oligoarticular JIA\n- Morning stiffness that improves after a warm shower/bath or with stretching and mild exercise is common in JIA. Many young children do not complain of pain, but walk with a limp or refuse to walk down stairs in the morning.\n- Joints often become sore/painful again in the late afternoon or evening.\n- Patients with JIA generally do not complain of severe pain, but rather they avoid using joints that are particularly affected.\n- If a child has severe pain in a joint, especially pain that seems out of proportion to the physical findings, diagnoses other than JIA should be entertained.\n- In systemic JIA, the fever curve is important to document.\n- Between fever spikes, the child is often completely afebrile.\n- The rash is evanescent and patients often have a history of fatigue, malaise, and weight loss.\n- Arthritis must be present, not just arthralgias:\n- In addition to swelling, warmth, and tenderness, there may be restricted range of motion in the affected joints and soft tissue contractures.\n- Enthesitis and sacroiliac tenderness are often seen in ERA.\n- In systemic JIA, the rash, if present, is very suggestive of this disease.\n- Lymphadenopathy and hepatosplenomegaly may be seen in systemic JIA.\n- A careful cardiac and pulmonary examination must be done to look for pericarditis and pleuritis.\nArthritis must be present for at least 6 weeks before a patient can be diagnosed with JIA. Many viral illnesses can produce joint pain and swelling that mimics JIA, but resolves within 4–6 weeks.\nDiagnostic Tests and Interpretation\n- No laboratory finding is diagnostic for JIA.\n- Many patients with JIA, especially the polyarticular and systemic types, have elevated sedimentation rates and anemia.\n- Antinuclear antibody is a useful test in classifying patients with JIA and determining the risk of uveitis. Positive in the following:\n- 80% of oligoarticular\n- 40–60% of polyarticular\n- 15–20% of normal population\n- Rheumatoid factor will be positive in 15–20% of patients with polyarticular arthritis and usually indicates a more aggressive form of arthritis.\n- Radiography is often normal early in JIA.\n- Later, if arthritis persists, bone demineralization, loss of articular cartilage, erosions, and joint fusion may be seen.\n- Monoarticular JIA\n- Septic joint\n- Toxic synovitis\n- Villonodular synovitis\n- Monoarticular or oligoarticular JIA\n- Lyme disease\n- Acute rheumatic fever or poststreptococcal arthritis\n- Viral/postviral arthritis\n- Inflammatory bowel disease\n- Polyarticular JIA\n- Viral or postviral illness (especially parvovirus)\n- Lyme disease\n- Systemic-onset JIA\n- Oncologic process (leukemia, lymphoma)\n- Inflammatory bowel disease\n- Steroids (glucocorticoids):\n- Intra-articular steroids: Triamcinolone hexacetonide injections are often used when patients have only 1 or 2 active joints.\n- Systemic steroids\n- Systemic steroids are often needed to control flares or with the initial presentation of polyarticular or systemic JIA. Because of the many side effects, patients should be weaned off steroids as soon as possible.\n- Glucocorticoids can be given orally (daily or every other day) or as IV pulses (every 1–8 weeks).\n- 1st-line therapy for mild JIA\n- If there is no response to the initial NSAID after 4–6 weeks of an adequate dose, a different one should be tried. Patients will often respond differently to the various NSAIDs.\n- If patients experience GI upset or excessive bruising, COX-2 inhibitors may be used. If arthritis remains active after 2–3 months, a 2nd-line treatment should be added.\n- If NSAIDs are ineffective in controlling the disease, or the patient has moderate to severe arthritis, a 2nd-line agent should be used, such as methotrexate or sulfasalazine.\n- If the arthritis does not respond to NSAIDs, methotrexate is the most common 2nd-line agent for active arthritis in multiple joints.\n- Laboratory values must be monitored closely in these patients, looking for bone marrow suppression or elevation of transaminase levels.\n- Most often used in ERA\n- Biologic agents are often added when patients do not respond adequately to methotrexate or cannot tolerate its side effects, or when the arthritis is severe.\n- Antitumor necrosis factor therapy is frequently used:\n- Etanercept is a receptor for tumor necrosis factor; given SC once or twice a week\n- Infliximab is a chimeric antibody to tumor necrosis factor; given IV every 4–8 weeks\n- Adalimumab is a fully humanized antibody to tumor necrosis factor; given SC every other week\n- IL-1 inhibition may work better than TNF inhibition in systemic JIA.\n- Anakinra is a recombinant IL-1 receptor antagonist. Given as a daily SC injection\n- Canakinumab is given SC once monthly.\n- Rilonacept is an SC injection given weekly.\n- Anti–IL-6 therapy (tocilizumab)\n- An IV medication given every other week.\n- It has been approved for children with systemic-onset and polyarticular JIA.\n- Costimulation blocker. It blocks the interaction of CD28 on T cells with CD80 and CD86 receptors on antigen-presenting cells.\n- It is given IV every 4 weeks.\n- It is currently being tested as an SC injection in children (SC form is approved in adults).\n- An antibody to CD20, which is present on all B cells\n- Approved for use in adult RA but not JIA\n- Medications such as cyclophosphamide or thalidomide are sometimes necessary to control severe systemic-onset JIA.\n- Responses to treatments for juvenile idiopathic arthritis vary tremendously:\n- Some patients may respond to NSAIDs within 1–2 weeks.\n- Others take 4–6 weeks to improve, and some may not respond at all.\n- Steroids usually start to relieve symptoms within a few days.\n- Methotrexate usually takes 4–8 weeks until a benefit is seen.\n- Antitumor necrosis factor therapy can start decreasing symptoms in as little as 1–2 weeks, or it may take up to 3 months.\n- Other 2nd-line agents can take up to 16 weeks until the maximum benefit is seen.\n- The waxing and waning nature of JIA itself adds to the variability of response to treatment.\n- Physical and occupational therapy are important in the management of JIA.\n- The goal is to maintain range of motion, muscle strength, and function.\n- Varies considerably\n- Children with oligoarticular JIA usually do well and often go into remission within a few years of starting treatment. They may have flares, however, even up to 10 years after being symptom-free and off all medications.\n- Patients with polyarticular JIA who are RF+ often develop a severe arthritis that may persist into adulthood.\n- RF− polyarticular patients generally fare better, and many outgrow their disease.\n- 50% of patients with systemic-onset JIA will develop severe chronic polyarticular arthritis.\n- Joint degeneration with loss of articular cartilage\n- Soft tissue contractures\n- Leg length discrepancy\n- Cervical spine dislocation\n- Rheumatoid nodules\n- Growth retardation\n- Oligoarticular JIA, especially with a positive ANA test, is associated with chronic uveitis, which can lead to loss of vision if not detected early with routine slit-lamp eye examinations.\n- May be seen in polyarticular JIA but is less common\n- Pericarditis, pleuritis, and severe anemia may develop in patients with systemic-onset JIA.\n- Macrophage activation syndrome or hemophagocytic syndrome\n- Rare, but potentially lethal complication of systemic-onset JIA, resulting from an overproduction of inflammatory cytokines\n- May present as an acute febrile illness with pancytopenia and hepatosplenomegaly\n- Bone marrow aspiration can be diagnostic.\n- Treatment is often high-dose steroids and high-dose IL-1 inhibitors, or cyclosporine.\n- Andersson Gäre B. Juvenile arthritis—who gets it, where and when? A review of current data on incidence and prevalence. Clin Exp Rheumatol. 1999;17(3):367–374. [PMID:10410275]\n- Ilowite NT. Update on biologics in juvenile idiopathic arthritis. Curr Opin Rheumatol. 2008;20(5):613–618. [PMID:18698187]\n- Patel H, Goldstein D. Pediatric uveitis. Pediatr Clin North Am. 2003;50(1):125–136. [PMID:12713108]\n- Prakken B, Albani S, Martini A. Juvenile idiopathing arthritis. Lancet. 2011;377(9783):2138–2149. [PMID:21684384]\n- Ringold S, Weiss PF, Beukelman T, et al. 2013 update of the 2011 American College of Rheumatology recommendations for the treatment of juvenile idiopathic arthritis: recommendations for the medical therapy of children with systemic juvenile idiopathic arthritis and tuberculosis screening among children receiving biologic medications. Arthritis Rheum. 2013;65(10):2499–2512. [PMID:24092554]\n- Schneider R, Passo MH. Juvenile idiopathic arthritis. Rheum Dis Clin North Am. 2002;28(3):503–530. [PMID:12380368]\n- Weiss J, Ilowite N. Juvenile idiopathic arthritis. Pediatr Clin North Am. 2005;52(2):413–442. [PMID:15820374]\n- 714.30 Polyarticular juvenile rheumatoid arthritis, chronic or unspecified\n- 714.31 Polyarticular juvenile rheumatoid arthritis, acute\n- M08.80 Other juvenile arthritis, unspecified site\n- M08.849 Other juvenile arthritis, unspecified hand\n- M08.879 Other juvenile arthritis, unspecified ankle and foot\n- M08.869 Other juvenile arthritis, unspecified knee\n- 410502007 juvenile idiopathic arthritis (disorder)\n- 410801005 juvenile idiopathic arthritis, enthesitis related arthritis (disorder)\n- Q: Will the patient outgrow JIA?\n- A: Prognosis depends on the type of JIA. In some studies, up to 50% of patients with JIA still had active disease 10 years after diagnosis, but only 15% had loss of function.\n- Q: Will siblings of patients with JIA develop the disease?\n- A: Rarely, but it can occur.\nElizabeth Candell Chalom\n© Wolters Kluwer Health Lippincott Williams & Wilkins\nMedicine Central™ is a quick-consult mobile and web resource that includes diagnosis, treatment, medications, and follow-up information on over 700 diseases and disorders, providing fast answers—anytime, anywhere. Complete Product Information.","Children Diseases and Diet\nCeliac disease is an autoimmune disorder that develops in individuals genetically predisposed to the condition. The digestive disease causes damage to the small intestine, thereby interfering with the absorption of vital nutrients. Children with celiac disease cannot tolerate substances such as gluten, rye and barley. While most often found in wheat, gluten is a protein that can be found in a variety of foods as well as in vitamins, medications and lip balms.\nThe condition has two major components: in addition to the digestive system’s failure to absorb nutrients properly, the child’s immune system has an abnormal reaction to gluten. After consuming the protein, the immune system reacts by damaging or destroying the lining of the small intestine. Not only does this result in malnutrition, but it also prevents the child’s body from absorbing nutrients crucial for healthy growth and development. Without these important nutrients, infants may fail to thrive while children can experience delayed growth, a short physique, dental enamel defects and delayed puberty.\nSymptoms of Celiac Disease in Children\nSymptoms differ from one child to the next. While the disease can affect any part of the body, infants and children most commonly experience digestive symptoms, which may include\n- Abdominal pain and bloating\n- Chronic diarrhea\n- Pale, fatty and foul-smelling stools\n- Weight loss and distended stomach\n- Behavioral changes such as listlessness, irritability and a refusal to eat.\nThe Outlook for Children with Celiac Disease\nWhile it may manifest at any age, the disease can develop in children as early as middle infancy. In children, the symptoms usually appear anywhere between one and five months after the infant as consumed foods containing gluten. For this reason, experts on infant feeding suggest that gluten-containing foods, such as cereal, should not be fed to babies during their first six months.\nDiabetes is an autoimmune disease resulting from the failure of the pancreas to produce insulin properly. Individuals with diabetes have dangerously high blood sugar levels as a result of this inadequate insulin production. High blood sugar levels account for the main symptoms of the disease, such as increased thirst, increased hunger and frequent urination. Normally, the body’s immune system constitutes its primary defense against diseases; however, as with all autoimmune disorders, diabetes causes the immune system to begin attacking the body’s own organs and tissues. In children with diabetes, the pancreatic cells responsible for producing insulin are destroyed.\nType 1 diabetes is the form most commonly found in children and accounts for almost 95 percent of all cases. However, this percentage may be misleading, as childhood diabetes is not a very common disease. While the origin of diabetes is still unclear, it most likely results from a combination of environment triggers and a genetic predisposition to high blood sugar.\nSymptoms of Childhood Diabetes\n- Excessive thirst\n- Frequent urination\n- Weight loss\n- Stomach pains\n- Behavioral problems\nTreatment for Childhood Diabetes\nBecause childhood diabetes requires specialized care, most children are treated in a hospital setting. Insulin therapy is the most effective form of treatment for Type 1 diabetes in kids. Most children require both daily and nightly insulin injections. Young children typically start with just one injection per day, while older children have the option of forgoing injections in place of a continuous insulin pump.\nCystic fibrosis is a genetic disorder that primarily affects the lungs and digestive tract. Because of CF’s effect on the body’s respiratory system, children with this disease are prone to chronic lung infections. The organs and cavities in the human body’s organs and cavities are lined with epithelial cells. In a healthy body, the lungs and digestive organs are covered with a thin coating of fluid and mucus, meant to trap and expel any germs that enter the lungs. However, in children with CF, the diseased gene causes the body’s epithelial cells to create defective proteins called CFTR. Because of these defective proteins, the epithelial cells are unable to control the passage of chloride through the cell membranes. As a result, the mucus coating the lungs and digestive organs becomes thick and sticky, trapping germs instead of clearing them out. These trapped germs result in the repeated lung infections experienced by children with CF. This thick mucus also obstructs the pancreas, preventing the passage of vital digestive enzymes into the intestines. Without these enzymes, the body cannot break down and absorb the nutrients in food, causing children with CF to lose weight regardless of their diet.\nSymptoms of Cystic Fibrosis\n- Frequent lung infections\n- Chronic and persistent cough, sometimes producing phlegm\n- Shortness of breath, wheezing\n- Poor growth and weight gain\n- Frequent slimy, large stools or difficulties with bowel movements\n- Salty-tasting skin\nOutlook for Children with Cystic Fibrosis\nThanks to advancements in both genetics and pharmacology, children with CF can live longer and more comfortable lives. Scientists have spent the past 10 years researching cystic fibrosis. Because of their work, doctors have a better understanding of the disease, allowing them develop new treatments. While children with CF were once condemned to live short lives, usually not making it past the 20 or 30-year-old mark, recent discoveries have lengthened life expectancy to 50 years. There is still hope that researchers may one day find a cure.\nLactose intolerance is a condition in which children cannot properly digest lactose. Lactose is a sugar found in dairy products such as milk, soft cheeses and ice cream. Children who suffer from lactose intolerance are unable to make enough of an enzyme known as lactase. This enzyme is normally produced by the intestines and is necessary for digestion. Without sufficient lactase, any undigested lactose remains in the intestines and results in the symptoms commonly associated with the disorder.\nBecause the symptoms of lactose intolerance are similar as those caused by a variety of digestive problems, diagnosing children with the condition may prove tricky. The first step in evaluating whether a child is lactose intolerant involves gradually removing dairy products from their diet for several weeks to see if their symptoms improve. The next step is to talk with the child’s pediatrician and request that they perform what is called a “lactose breath test.” During this test, a lactose solution is ingested so that the subsequent hydrogen levels in the child’s breath can be measured. As undigested lactose remains in the intestinal tract and ferments, producing hydrogen to be exhaled; thanks to this process, doctors are able to accurately test for lactose intolerance.\nSymptoms of Lactose Intolerance in Children\nSymptoms typically appear anywhere from 30 minutes to two hours after consuming foods that contain lactose. Common symptoms include\n- Stomach cramps\nTreatment for Lactose Intolerance\nUnfortunately, there is no remedy for lactose intolerance. Instead, the condition should be controlled through strict adherence to a restricted, lactose-free diet. You can also buy over-the-counter lactase to give children prior to meals in an effort to aid their body’s digestion. While the condition is bothersome, it doesn’t have to be a life-sentence of the same boring foods. Consult a pediatrician for help, as he can offer suggestions regarding dietary changes and new foods."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:328476a2-51d5-4d1d-8576-b292f609da9c>","<urn:uuid:19879562-a235-47b2-acd4-66e145894d6a>"],"error":null}
{"question":"How is the overall heat transfer rate calculated in heat exchangers?","answer":"The overall heat transfer rate (Q) in heat exchangers is calculated using the equation Q = U×A×ΔT, where U is the overall heat transfer coefficient, A is the overall heat transfer surface area, and ΔT is the mean temperature difference between hot and cold side.","context":["The concept of 'heat exchanger efficiency' is not very well defined in technical literature. Efficiency of any system has to be calculated based on the performance of some ideal system. In case of heat exchangers, we don't have very well defined 'ideal' heat exchangers.\nOverall heat transfer equation\nFor any heat exchanger system, the overall heat transfer rate (Q) is defined as -\nQ = U×A×ΔT\nwhere, U is the overall heat transfer coefficient\nA is the overall heat transfer surface area\nand ΔT is the mean temperature difference between hot and cold side\nIdeal heat exchanger system\nIn most cases the mean temperature difference is a given. The heat transfer rate is mostly governed by the product UA, which is the product of surface area and heat transfer coefficient.\nUA can always be improved by changing the heat exchanger design or just increasing the surface area by getting a bigger heat exchanger. For that reason, we don't have a practical definition for 'ideal heat exchanger'.\nCalculating heat exchanger efficiency\nFor any system, efficiency is normally calculated by comparing the actual performance with ideal performance.\nEfficiency = Actual out put / Output of the ideal system\nSince we don't have an ideal heat exchanger to compare with, we cannot use the traditional concept of efficiency for heat exchangers.\nInstead we can use the value of 'UA' (product of surface area and heat transfer coefficient) to represent the best heat transfer rate that can be achieved for given process conditions (ΔT).\nWhen you are comparing between different heat exchanger designs, it is a good idea to compare their UA values. That should give you a good idea of how much heat transfer would be possible with each of them.\nTo further examine the economic viability, you can also compare the ratio of (UA/cost). That will help you to identify a heat exchanger design that can give you the best heat transfer rate at the lowest cost. This approach will give you the most productive heat exchanger design.\nHeat exchanger effectiveness\nWhen a heat exchanger has been in operation for a while, its performance decreases for a variety of reasons - such as, fouling, scaling, corrosion etc.\nThis reduced performance can also be quantified as the reduced effectiveness of heat transfer. You can compare the current heat transfer rate to the original (rated) heat transfer rate.\nA ratio of (Q/rated duty) will tell you how effectively your exchanger is working.\nShell & tube heat exchanger efficiency\nFor shell & tube heat exchangers, the overall heat transfer rate is defined as,\nQ = U×A×LMTD\nwhere, LMTD is the logarithmic mean temperature difference.\nIn this equation only the definition of mean temperature difference has been made more specific. Otherwise, the heat transfer rate is still governed by the product - UA.\nHere is a list of equations governing the design of a shell & tube exchanger. You can use them to calculate U and A and the product UA. That will help you with selection of the best heat exchanger for your project."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:4ce75d5f-0fbc-4a7c-aafa-8430d08d915d>"],"error":null}
{"question":"When was Kodachrome color film first introduced for amateur use?","answer":"Kodachrome was introduced by Kodak for amateur use in 1935 for 16 mm film and in 1936 for 8 mm film.","context":["1889–1899: Before standardization\nEarly motion picture experiments were performed using a fragile paper roll film, with which it was difficult to view a single, continuously moving image without a complex apparatus.\nEastman Kodak made celluloid film commercially available in 1889; Thomas Henry Blair, in 1891, was its first competitor. The stock had a frosted base to facilitate easier viewing by transmitted light. Emulsions were orthochromatic. By November 1891 William Dickson, at Edison‘s laboratory, was using Blair’s stock for Kinetoscope experiments. Blair’s company supplied film to Edison for five years. Between 1892 and 1893, Eastman experienced problems with production. Because of patent lawsuits in 1893, Blair left his American company and established another in Britain. Eastman supplied Edison with film.\nBlair’s new company supplied European filmmaking pioneers, including Birt Acres, Robert Paul, George Albert Smith, Charles Urban, and the Lumiere Brothers. By 1896 the new movie projector required a fully transparent film base that Blair’s American operation could not supply. Eastman shortly thereafter bought the company out and became the leading supplier of film stock. Louis Lumiere worked with Victor Planchon to adapt the Lumiere “Blue Label” (Etiquette Bleue) photographic plate emulsion for use on celluloid roll film, which began in early 1896.\nFrom 1895, Eastman supplied their motion picture roll film in rolls of 65 feet, while Blair’s rolls were 75 feet. If longer lengths were needed, the unexposed negative rolls could be cemented in a darkroom, but this was largely undesirable by most narrative filmmakers. The makers of Actuality films were much more eager to undertake this method, however, in order to depict longer actions, and created cemented rolls as long as 1000 feet. American Mutoscope and Biograph was the first known company to use this for the Jeffries–Sharkey fight on November 3, 1899.\n1900–1919: Towards the standard picture film\nAs the quantity of film and filmmakers grew, the demand for standardization increased. Between 1900 and 1910, film formats gradually became standardized and film stocks improved. A number of film gauges were made. Eastman increased the length of rolls to 200 feet without major adjustments to the emulsion, retaining a large market share. Lumiere reformulated its stock to match the speed of Eastman film, naming it ‘Etiquette Violette’ (Violet Label). Blair sold his English company to Pathé in 1907 and retired to the US. Pathe began to supplement its operation in 1910 by purchasing film prints, stripping the emulsion from the film base and re-coating it.\n35mm film began to become the dominant gauge because of the popularity of Edison’s and Lumière’s cameras. Consumers usually purchased unperforated film and had to punch it by perforators that were often imprecise, causing difficulty in making prints for the opposite perforation format. In 1908, the perforators began to be made by Bell and Howell. Eastman Kodak used the Bell and Howell’s machine to perforate its films.\nAgfa began to produce motion picture film in 1913, but remained a largely local supplier until World War I boycotts of popular French, American and Italian film stocks allowed the UFA film studio to flourish, boosting Agfa’s orders.\nAll film stocks were manufactured on a nitrate film base, which is highly flammable. Nitrate film fires were difficult to extinguish. A significant number of fatal accidents occurred in theatrical projection booths, where the heat of the projector lamp made ignition most likely.\nAmateur filmmaking (home movies) slowly developed during this period. Kodak developed a heat-resistant ‘safety base’ for home projection. In 1909, tests showed cellulose diacetate to be a viable replacement base, and Kodak began selling acetate-base films the following year in 22 mm widths for Edison’s work on the Home Kinetoscope, which was commercially released in 1912. Eastman Kodak introduced a non-inflammable 35 mm film stock in 1909. The plasticizers used to make the film flexible evaporated quickly, making the film dry and brittle, causing splices to part and perforations to tear. In 1911 the major American film studios returned to using nitrate stock. More amateur formats began to use acetate based film, and several, including Kodak’s own 16 mm format, were designed specifically to be manufactured with safety base.\nKodak released Cine Negative Film Type E in 1916 and Type F (later known as Negative Film Par Speed Type 1201) in 1917. As both of these orthochromatic films were no faster than previous offerings; the improvements were in granularity and sharpness.\n1920s: Diversification of film sensitivity\nFilm stock manufacturers began to diversify their products. Each manufacturer had previously offered one negative stock (usually orthochromatic) and one print stock.\nIn 1920, a variant of Type F film known as X-back was introduced to counteract the effects of static electricity on the film, which can cause sparking and create odd exposure patterns on the film. A resin backing was used on the film, which rendered the film too opaque to allow focusing through the back of the film, a common technique for many cameras of that era. The X-back stock was popular on the east coast of the US. Other manufacturers were established in the 1920s, including American E.I. Dupont de Nemours in 1926 and Belgian Gevaert in 1925. Panchromatic film stock became more common. Created in 1913 for use in color film processes such as Kinemacolor, panchromatic was first used in a black-and-white film for exterior sequences in Queen of the Sea (1918) and originally available as a special order product.\nThe stock’s increased sensitivity to red light made it an attractive option for day for night shooting. Kodak financed a feature in 1922, shot entirely with panchromatic stock, The Headless Horseman, to promote the film when Kodak introduced it as a standard option.\nPanchromatic film stock was expensive and no motion pictures were produced in entirety on it for several years. The cross-cutting between panchromatic and orthochromatic stocks caused continuity problems with costume tones and panchromatic film was often avoided.\nOrthochromatic film remained dominant until the mid-1920s due to Kodak’s lack of competition in the panchromatic market. In 1925, Gevaert introduced an orthochromatic stock with limited color sensitivity and a fully panchromatic stock, Pan-23. In 1926, Kodak lowered the price of panchromatic stock to parity with its orthochromatic offering and the panchromatic stock began to overtake the orthochromatic stock’s market share within a few years. Similar panchromatic film stocks were manufactured by Agfa and Pathé, the shift to panchromatic stocks had largely been completed by 1928, and Kodak discontinued orthochromatic stock in 1930.\nExperiments with color films were made as early as the late 19th century, but practical color film was not commercially viable until 1908, and for amateur use when Kodak introduced Kodachrome for 16 mm in 1935 and 8 mm in 1936.\nBefore 1941, commercially successful color processes used special cameras loaded with black-and-white separation stocks rather than color negative. Kinemacolor (1908–1914), Technicolor processes 1 through 4 (1917–1954), and Cinecolor used one, two or three strips of monochrome film stock sensitized to certain primary colors or exposed behind color filters in special cameras.\nTechnicolor introduced a color reversal stock, called Monopack, for location shooting in 1941; it was ultimately a 35 mm version of Kodachrome that could be used in standard motion picture cameras. Eastman Kodak introduced their first 35mm color negative stock, Eastman Color Negative film 5247, in 1950. A higher quality version in 1952, Eastman Color Negative film 5248, was quickly adopted by Hollywood for color motion picture production, replacing both the expensive three-strip Technicolor process and Monopack.\nClassification and properties\nThere are several variables in classifying stocks; in practice, one orders raw stock by a code number, based on desired sensitivity to light.\nA piece of film consists of a light-sensitive emulsion applied to a tough, transparent base, sometimes attached to anti-halation backing or “rem-jet” layer (now only on camera films). Originally the highly flammable cellulose nitrate was used. In the 1930s, film manufacturers introduced “safety film” with a cellulose triacetate plastic base. All amateur film stocks were safety film, but the use of nitrate persisted for professional releases. Kodak discontinued the manufacture of nitrate base in 1951, and the industry transitioned entirely to safety film in 1951 in the United States and by 1955 internationally. Since the late 1990s, almost all release prints have used polyester film stock.\nThe emulsion consists of silver halide grains suspended in a gelatin colloid; in the case of color film, there are three layers of silver halide, which are mixed with color couplers and interlayers that filter specific light spectra. These end up creating yellow, cyan, and magenta layers in the negative after development.\nDevelopment chemicals applied to an appropriate film can produce either a positive (showing the same densities and colors as the subject) or negative image (with dark highlights, light shadows, and, in principle, complementary colors). The first films were darkened by light: negative films. Later films that produce a positive image became known as reversal films; processed transparent film of this type can be projected onto a screen. Negative images need to be transferred onto photographic paper or other substrate which reverses the image again, producing a final positive image. Creating a positive image from a negative film can also be done by scanning the negative to create a computer file which can then be reversed by software.\nDifferent emulsions and development processes exist for a variety of image recording possibilities: the two most common of which are black and white, and color. However, there are also variant types, such as infrared film (in black and white or false color); specialist technical films, such as those used for X-rays; and obsolete processes, such as orthochromatic film. Generally, however, the vast majority of stock used today is “normal” (visible spectrum) color, although “normal” black and white also commands a significant minority percentage.\nFilm is also classified according to its gauge and the arrangement of its perforations— gauges range from 8 mm to 70 mm or more, while perforations may vary in shape, pitch, and positioning. The film is also distinguished by how it is wound with regard to perforations and base or emulsion side, as well as whether it is packaged around a core, a daylight spool, or within a cartridge. Depending on the manufacturing processes and camera equipment, lengths can vary anywhere from 25 to 2000 feet. Common lengths include 25 feet for 8 mm, 50 feet for Super 8, 100 and 400 feet for 16 mm, 400 and 1000 feet for 35 mm, and 1000 for 65/70 mm.\nA critical property of a stock is its film speed, determined by ASA or its sensitivity to light listed by a measurement on the raw stock which must be chosen with care. Speed determines the range of lighting conditions under which the film can be shot, and is related to granularity and contrast, which influence the look of the image. The stock manufacturer will usually give an exposure index (EI) number equal to the ASA which they recommend exposing for. However, factors such as forced or non-standard development (such as bleach bypass or cross processing), compensation for filters or shutter angle, as well as intended under- and over-exposure may cause the cinematographer to actually “rate” the stock differently from the EI. This new rating is not a change to the stock itself — it is merely a way of calculating exposure without figuring out the compensation after each light reading.\nAnother important quality of color film stock in particular is its color balance, which is defined by the color temperature at which it accurately records white. Tungsten lighting is defined at 3200 K, which is considered “warmer” in tone and shifted towards orange; daylight is defined at 5600 K, which is considered “colder” and shifted towards blue. This means that unfiltered tungsten stock will look normal shot under tungsten lights, but blue if shot during daylight. Conversely, daylight stock shot in daylight will look normal, but orange if shot under tungsten lights. Color temperature issues such as these can be compensated for by other factors such as lens filters and color gels placed in front of the lights. The color temperature of a film stock is generally indicated next to the film speed number — e.g. 500T stock is color film stock with an ASA of 500 and balanced for tungsten light; 250D would have an ASA of 250 and be balanced for daylight.\nWhile black-and-white film has no color temperature itself, the silver halide grains themselves tend to be slightly more responsive to blue light, and therefore will have daylight and tungsten speeds — e.g. Kodak’s Double-X stock is rated 250D/200T, since the tungsten light will give slightly less exposure than an equivalent amount of daylight.\nAll plastic is subject to deterioration through physical or chemical means, and thus, motion picture film is subject to the same deterioration. Cellulose nitrate, cellulose diacetate and triacetate are known to be unstable mediums: improperly preserved film can deteriorate in a period of time much faster than many photographs or other visual presentations.\nCellulose nitrate, because of its unstable chemistry, eventually breaks down, releasing nitric acid, further catalyzing the decomposition. In the final stages of celluloid decomposition, the film turns into a rust-like powder.\nLikewise, tri-acetate stock is also vulnerable to deterioration. Because of the small gauge of the film, owners of home-made films often find that their film can become shrunken and brittle to the point where the film is unwatchable in the space of a few years. In general, decaying acetate film breaks down into acetic acid, and similar to celluloid decomposition, leads to an auto-catylictic breakdown of the base that cannot be reversed. The result of the acetic acid released is a strong odor of vinegar, which is why the decay process in the archival community is known as “vinegar syndrome“.\nModern polyester-based stocks are far more stable by comparison and are rated to last hundreds of years.\nIntermediate and print stocks\nThe distinction between camera stocks and print stocks involves a difference in the recording process. When the work print or edit master has been approved, the Original Camera Negative (OCN) is assembled by a negative cutter using the edited work print or EDL (edit decision list) as a guide. A series of Answer Prints are then made from the OCN. During the Answer Print stage, corrections in the film’s density and color are corrected (timed) to the filmmakers’ tastes. Interpositive (IP) prints are struck from the OCN, checked to make sure they look the same as the custom timed Answer Print, and then each IP is used to make one or more Dupe Negative (DN) copies. The release prints are then generated from the DN(s). Recently, with the development of digital intermediate (DI), it has become possible to completely edit, composite visual effects, and color grade the image digitally at full resolution and bit-depth. In this workflow, the answer print is generated digitally and then written out to the IP stage using a laser film printer.\nDue to the specialized nature of the exposure and the higher degree of control afforded by the film lab equipment, these intermediate and release stocks are specially designed solely for these applications and are generally not feasible for camera shooting. Because intermediates only function to maintain the image information accurately across duplication, each manufacturer tends to only produce one or two different intermediate stocks. Similarly, release print stocks usually are available only in two varieties: a “normal” print or a deluxe print (on more-costly print film like Kodak Vision Premiere) with slightly greater saturation and contrast.\n|Wikimedia Commons has media related to Film stocks.|\n- Direct film\n- Film format\n- Film preservation\n- List of film formats\n- List of motion picture film stocks\n- Color motion picture film\n- Photographic film with emphasis on film for still photography."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:5a20edf4-cee5-4ae6-ad3e-cf1d0f15935e>"],"error":null}
{"question":"I wanna know about equipment needed for these locations - what's different between temple vs firework photography setup?","answer":"For fireworks photography, essential equipment includes a DSLR or micro-four-thirds camera with manual controls, a wide lens (under 50mm), a sturdy tripod, and a remote trigger (cable release or infrared remote). Shooting fireworks requires keeping the camera completely still. For temple photography, the setup is more flexible and mobile - zoom lenses (18-140mm or 18-200mm) are recommended for versatility, and prime lenses like 35mm f/1.8 or 50mm f/1.8 are useful for low light conditions. Tripods are often prohibited or impractical in temples, so photographers must practice handheld shooting at slow shutter speeds (1/15 or 1/30). A lighter, more portable setup is crucial for temple photography due to the need for quick movement and adapting to changing conditions.","context":["The charm about fireworks is that you never really know what you’re going to get. As a result, they can be tricky to photograph, and just as tricky to post-process.\nNow to get good images of fireworks, you’re going to need a camera where you can control the shutter speed, and a means to keep it absolutely still.\nThe choice of camera and lens\nYou will need to choose a camera that has good low light performance, and will allow you to manually adjust the ISO, shutter speed, and aperture. This logically points to an SLR (and given that this article is aimed at an audience that primarily shoots in digital format, I will go as far as to say a Digital SLR), or a micro-four-thirds camera. There are some high end point-and-shoot models that will also give you this sort of control. The key limitation will be their performance in low light and high ISO settings.\nFireworks shows are a special case of landscapes. Hence you’ll want to choose a lens that is fairly wide – typically on the lower side of 50mm.\nKeeping the camera still\nAs you’ll be shooting an illuminated subject that moves against a predominantly still background, you will need to keep the camera still. There are two parts to this.\nFirst, you’ll need a good sturdy tripod. The second part to this (while not essential) is to have a means to remote trigger the camera to prevent vibrations. This can either be a cable release, an infra-red remote, or a built in timer.\nI use a Canon 5D Mark II with a custom firmware installed on it (Magic Lantern) which gives me the functionality of an intervalometer, where I can tell the camera to continuously shoot frames at set intervals indefinitely.\nAs with all forms of photography, I always recommend shooting in RAW. This is so that you can manipulate things such as the white balance, exposure, contrast, highlights and shadows, and white and black points in post-production, which you will not be able to do if you shoot in JPEG.\nNow, there are a lot of theories out there, but my experience has taught me that three key things to consider about capturing good fireworks images are\n- the focus,\n- the exposure time, and\n- the ISO.\nOnce you have these three items sorted, the aperture typically sorts itself out.\nNow a typical fireworks display is really a landscape shot taken in low light. The main subject – the fireworks display – is typically quite far off from where one would be stationed. When it comes to focus, I’ve taken the route of using the camera lens manually focussed at infinity. By doing this, we’re going to take advantage of the camera’s hyper-focal distance. That means that beyond a certain distance (usually, a few metres), everything will be in acceptable focus.\nSelecting the exposure time is tricky. Expose for too long, and you’ll end up blowing out the highlights and ending up with no details. Expose for too little, and you won’t get very much at all. I find that the best exposure times are between 4 to 12 seconds. However, picking out what works best does require some trial and error.\nThe selection of the ISO number is dictated by your camera’s sensitivity to light. Because you’ll be shooting in low light conditions with relatively short exposure times, you will need to shoot at a high ISO number. I usually shoot at around 800 ISO, but you may be limited by your camera’s sensitivity to noise. I recommend choosing the highest ISO number that you feel that your camera will produce good, relatively grain-free images with.\nNow, once you’ve got your camera set, you’ll need to take a few test shots, while varying the aperture till you get an image that you feel is adequately well exposed. You would do this before the fireworks went off and the area was still limit in ambient light. Now, bearing in mind that when the fireworks go off, the light levels will increase, you will need to adjust the aperture (increase the f-number) to accommodate for this increase in light. My rule of thumb is to double the f-number, and then reduce it by half a stop.\nSo, if I had ascertained that I could capture a correctly exposed image of the area before the fireworks went off at f/4, I would set my aperture to f/7.1 (4×2=8; and a half-stop down from f/8 is f/7.1). Similarly, if I found that I could capture a correctly exposed image of the area before the fireworks went off at f/8, I would set my aperture to f/13\nAt this stage, you’re set. Once you have your camera settings programmed in, wait for the fireworks to go off, and then start shooting. Shoot often, and do not move the camera around. The beauty of using a cable release or intervalometer when doing this (in addition to getting good images) is that you will actually be able to enjoy the fireworks display with your own eyes instead of watching it through a view-finder! If you’ve done this, you should be able to pull off images like these from your shoot.\nNow, the images above didn’t come out of camera that way, but have been post-processed. I use Lightroom, Photoshop and Noiseware to post-process my images. I have a set of custom presets which I have built that allow me to apply these adjustments with a single click in Lightroom. The video above shows the exact post-processing sequence that I use to finish off my images as a final product.\nSome Final Thoughts\nWhile I’ve covered the camera techniques and post-processing workflow in the segments above, there are a few things to keep in mind when going out to shoot fireworks that I call the “pre-work”.\n- Scope the location of the fireworks display out a few days in advance, and pick your vantage point so that you have a clear view of fireworks.\n- Check the weather forecast, and specifically, the speed and direction of the wind as this will affect how your images turn out.\n- Ideally, you want the wind blowing across the direction that you will be looking directly out towards. You do not want it blowing towards you.\n- Dress appropriately for the weather. You will be standing around for close to an hour. On cold days, you will want to bundle up. On warm days, you may want to have something to fan yourself with. Always keep some drinking water and a small towel handy.\n- Plan for traffic and come early. Fireworks displays attract crowds, and if you do not get there early, you may struggle to find a place to park, and to secure a vantage point to shoot from.\n- Carry a torch/flashlight. You will be shooting in predominantly dark surroundings. Having a light source is invaluable when you happen to drop something like a lens cap, and are struggling to find it.\n- Above all, don’t spend all your time tinkering with your camera. Do try to enjoy the display.","By : Nikon School Blog | 14 Nov, 2014 |\nFor the photographers in India, the numerous temples offer wonderful opportunities to create amazing frames. The temples provide photographers with almost all they crave for – people, colours, and moments. If you're a photographer and have never been inside any Indian temple, you're surely missing out on ample photographic moments.\nPhotographing temples requires all the abilities of a photographer – technical know-how, instincts, quick thinking and innovations. Here's what to expect when you're out looking for great images inside Indian temples, and a few tips how to capture them the best way.\nLook for Colours\nColours draw your attention the moment you step into any temple complex. Bright reds or nearby shades are usually the dominant ones, but move around and you will find generous splashes of every other colour in the spectrum. Colours make framing so much easier; you just have to include contrasting colours to create attractive frames.\nThe bustling life inside the functioning temples must be in your hunt list while shooting in temples. The red or saffron robed priests, the ash-smeared sadhus, and throngs of people in bright attire can fill up your frame to create amazing visuals. You can use telephoto lenses to single out interesting portraits amongst a crowd of devotees.\nTypical elements worth including in the frame include - smoke from burning fires or incense sticks, offerings of eatables and flowers, religious items like statues or colourful garlands for sale in countless shops, usually present around any major temple complex.\nTemples also showcase the mastery of craftsmen of the past. Try to get closer looks on the sculptures, murals and architecture of the temples.\nInside temples , you need to wait for the right moment to fill your frame. It's much better if you visit during a particular festival or special days. The activity level goes up along with the numbers of priests and devotees at work, increasing your chances of getting the desired images.\nOften the biggest challenge is the light. Constant changes in exposure are needed to deal with the quick changes between bright exteriors and dark shades. Keeping ISO around 400 can deal with movements in low lit areas. Higher values may be required for very dark interiors.\nAll the prominent temples nowadays are under surveillance. Don't try to snoop your way in when a notice prohibiting photography is glaring at you. If you are not allowed to shoot something, just forget it and concentrate elsewhere.\nSimple rule – Aperture priority mode for bright areas and still subjects, like temple exteriors, sculptures, flowers, etc. Quickly shift to Shutter priority for any human element included in your frame.\nUse the auto ISO feature in your camera and limit it to 1600.This way, the camera will be able to adjust ISO if suddenly light goes down.\nZooms are the best, for shifting quickly between wide angles and telephotos. Something like an 18-140 or 18-200 will be very useful, but if you are anticipating challenging light, carry something like a 35mm f/1.8 or a 50mm f/1.8.\nIf portraiture is in your mind, keep a telephoto zoom like the 55-300 or the 70-300 in your bag.\nTravel light as fast movement is crucial. Carrying tripods may be prohibited or impractical, so practice hand held shooting with shutter speeds of 1/15 or 1/30."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:eaac6fa1-61a7-474a-a69d-c6bfc5dd2f78>","<urn:uuid:fa3520c4-7e28-4f49-903f-1195c3c76de4>"],"error":null}
{"question":"Please explain professional boundaries in midwifery and social media engagement guidelines?","answer":"Professional boundaries in midwifery are essential to maintain, with unexpected consequences possible when breached. The Nursing and Midwifery Board of Australia provides specific guidance on professional boundaries. For social media engagement, healthcare practitioners must be extremely cautious - they cannot forward or promote patient testimonials without signed HIPAA consent forms, even if patients voluntarily share their experiences. Healthcare providers must also implement policies explaining that patients shouldn't post personal health information on public social media pages.","context":["Midwifery and the Law\n- : Melbourne VIC 3001\nIncludes: Legal Status of Unborn Child, Auditing, Registration Standards; Prescribing; Refusal of Treatment, Professional Indemnity Insurance, Case Studies and much, much more...\nPotential risks manifest in all areas of health care, including obstetrics and within midwifery practice. Do you have informed knowledge about modern legal and professional aspects of midwifery? Attend this seminar to gain a comprehensive update on these topics. Includes:\n- Laws relating to midwifery, including documentation and duty of care\n- Modern midwifery legal considerations - prescribing, Medicare and professional indemnity insurance\n- What is the legal status of an unborn child?\n- A woman’s right to refuse treatment\n- What are the rights of fathers and grandfathers?\n- How can social media be used safely?\nAttend this seminar and gain an opportunity to discuss a range of contemporary legal, professional and regulatory issues of relevance to midwives. Don’t miss out!\nNeed for Program\nMidwives are regulated health professionals who function in accordance with legislation and common law affecting midwifery practice. They are accountable and responsible for their own actions within their practice; therefore it is essential that midwives have access to ongoing, relevant education to remain up-to-date with legal and professional regulatory issues relevant to their context and conditions of practice.\nPurpose of Program\nThe purpose of this seminar is to provide midwives with a forum to enhance their knowledge of modern law and relevant contemporary professional and regulatory issues as these affect their practice.\nAt the conclusion of this program it is expected that the participants will be able to:\n- Reflect on the laws relevant to the profession and practice of midwifery\n- Review current professional issues affecting midwives, including professional frameworks for competent practice\n- Explore concepts of negligence, liability and professional indemnity as they apply to midwives\n- Discuss the processes that occur when an untoward event takes place within a midwifery context\n- Schedule Day One\n8:30am - Registration and Refreshments\n9:00amLaws Impacting on Midwifery Practice\nAccording to the Code of Professional Conduct for Midwives in Australia, ‘Midwives practise and conduct themselves in accordance with laws relevant to the profession and practice of midwifery’. This session sets the scene for the day and overviews legal obligations under which midwives practise. Includes:/p>\n- Keeping in touch with statutory and common law - a review\n- Laws that currently most impact on the practice of midwifery\n- The concept of 'duty of care' - do you really understand it?\n- What is the relevance of professional codes, standards and guidelines?\n9:45amLegal Areas of Concern\nWhat are the contemporary issues confronting midwives in regard to areas of legal concern? This short discussion will provide an opportunity to raise some questions for exploration.\n10:30am - Morning Tea and Coffee\n11:00amJustifying Your Clinical and Professional Decisions at Law\nThe session will provide a closer examination of neglect and vicarious liability. It will use a sample of recent legal cases involving midwives to demonstrate how these legal concepts work. Includes:\n- The essential facts about neglect and vicarious liability\n- The vital role of accurate and correct documentation\n11:45amModern Midwifery - Medicare, Prescribing, and Professional Indemnity\nAs the delivery of midwifery services change, some interesting issue may arise. We will look at:\n- What are some of the legal considerations if providing Medicare funded care?\n- What are legal obligations associated with prescribing from a formulary for midwives?\n- How effective is Professional Indemnity Insurance for midwives? What are the pitfalls?\n12:30pm - Lunch Break and Networking\n1:30pmThe Legal Status of the Unborn Child\nThere is much debate about the viability, health and ethical situation of unborn children. Midwives may become involved in such considerations and it is advisable to have thought through some of the dilemmas. Includes discussion on:\n- Definition of life and rights of the unborn child\n- Should a midwife intervene if a mother is drinking to excess during pregnancy?\n- Can a mother be sued for damage to her foetus caused by maternal neglect?\n2:00pmThe Woman's Right to Refuse Treatment\nMidwives and the women in their care have a two-way relationship. However, women have a right to refuse treatment. This session looks at the legal issues in upholding the rights of the women and the obligations of the midwife. Includes:\n- If a woman refuses treatment or wishes to cease the relationship, what should you do?\n- What are the rights of the woman?\n- What if the midwife wishes to cease the relationship?\n- Can a mother refuse life-saving treatment if such treatment could cause the death of her baby?\n- Under what circumstances is consent waived?\n- Which processes need to be followed if consent is assumed?\n2:45pm - Afternoon Tea and Coffee\n3:00pmThe Rights of Fathers and Grandparents\nOften fathers and grandparents are 'caught in the crossfire' of personal family traumas. The legal rights of fathers and also grandparents in regard to choice and access-related issues will be discussed.\n- What are the rights of the biological father before and after childbirth?\n- Do grandparents have any rights under the law?\n3:30pmDocumentation, Midwifery and the Law\nThe importance of quality documentation in midwifery from a legal perspective cannot be overstated. This final session will focus particularly on the need to write accurate and timely reports. Using examples from case law, the following will be discussed:\n- What constitutes accurate documentation?\n- How to approach writing incident reports\n- What are reportable deaths and why is the midwife's documentation regarding them so crucial?\n- What would be expected of you if you were called to appear in the Coroner's Court?\n- If this occurs, what legal advice should you seek?\n4:15pm - Close of Day One of ProgramDay Two\n9:00am - Commencement of Day Two\n9:00amRegulation for Midwives – An Update\nAccording to the Code of Conduct for Nurses in Australia, ‘ A midwife is a person who, having been regularly admitted to a midwifery educational programme, duly recognised in the country in which it is located, has successfully completed the prescribed course of studies in midwifery and has acquired the requisite qualifications to be registered and/or legally licensed to practise midwifery.’\nThis session will consider the definition from the National Law perspective as an overview to the regulation of midwives and protection of the public. It will also provide an opportunity to catch up on recent general developments of the Nursing and Midwifery Board of Australia (NMBA).\n9:45amThe Complete Package - Registration Standards and Auditing\nThere are specific standards with which midwives and nurses must comply under the Health Practitioner Regulation National Law Act 2009 (the National Law ) in order to maintain registration. Recently, auditing has been introduced as a means to ensure compliance with the standards. This session will examine the process of auditing those standards and what non-compliance means.\n10:30am - Morning Tea and Coffee\n11:00amCPD and Beyond\nThe Registration Standard for Continuing Professional Development (CPD) specifies certain requirements for CPD. But what is continuing professional development (CPD)? Why is it necessary to develop a learning plan? This session discusses the concept of CPD and its breadth; and some common questions, such as:\n- How do I self-assess competence and learning needs?\n- If I read a journal article about midwifery practice, can this count towards CPD hours?\n- If I manage a project in my workplace does this count as CPD?\n- Is it acceptable for me to undertake CPD requirements at one time each year?\n- What if I just meet the requirements through inservice?\n12:00pmProfessional Boundaries – A Detailed Appraisal\nWhen professional boundaries are breached, unexpected consequences may occur. The NMBA provides a guide on professional boundaries. Why is maintaining a professional relationship/boundary in midwifery so important and what are the consequences of breaching this boundary?\n1:00pm - Lunch Break and Networking\n1:45pmUnprofessional Conduct and Impairment\nAnyone can make a complaint about a midwife’s health, performance or conduct under the National Law . However, it also requires that a registered health practitioner must notify the relevant Board if, in the course of practising their profession, they form a reasonable belief that another registered health practitioner has behaved in a way that constitutes ‘notifiable conduct’.\n- What is notifiable conduct?\n- How does the National Law define unprofessional conduct and impairment?\n- What is the NMBA guideline for mandatory notification?\n- What are the exemptions from mandatory notification?\n- Who is responsible for making a report and when is it mandatory?\n- How to make a report\n- What are the powers of investigators under the National Law?\n2:40pmWhen Would a Midwife be Considered Unprofessional in Their Conduct?\nThis session will be a discussion of issues specific to midwives and will examine examples from health professional case summaries published by AHPRA. It will also look at a compassionate and supportive framework for rehabilitation.\n3:15pm - Afternoon Tea and Coffee\n3:30pmSocial Media and the Health Practitioner\nSocial media use amongst health practitioners, while valuable, may also have negative effects on professional standards depending on how it used. This session will discuss the growing issues and pitfalls of health practitioners using social media and look at some cases of unprofessional conduct and the relevance to guidelines, especially in the context of codes of conduct and ethics.\n4:15pmSummary and Final Questions\n4:30pm - Close of Seminar and Evaluations\nLinda Starr is a qualified general and psychiatric nurse, lawyer and associate professor in the School of Nursing and Midwifery at Flinders University. She was appointed by the Governor of South Australia to be a Justice of the Peace in 2011. Her research interests have been in health law, criminal law, profiling and offender behaviour, forensic practice, forensic nursing, and elder abuse investigation. Her current research for her PhD project is exploring the experience of Registered and Enrolled Nurses and carers in identifying and reporting elder abuse, and the experience of those who receive such reports and investigate the cases. Linda, former Dean of the School of Nursing and Midwifery at Flinders University, is currently the Chair of the State Board of the Nursing and Midwifery Board, Australia, Deputy Chair of the Aged Rights Advocacy Service, President of the Australian Forensic Nurses' Association, and member of the South Coast Health Advisory Council and the Foundation.","Imagine: You manage the social media for a popular medical practice. A patient posts a beautiful comment praising the doctors for how they helped her with her diabetes and blood pressure. It’s a perfect testimony! You proudly forward it to various outlets, hoping to further the online reputation of the medical practice.\nNot so fast.\nAlthough the patient volunteered the information, if you forward it, you are in danger of making a HIPAA violation. In fact, even leaving the post on the page could cause the “HIPAA police” to levy huge fines against you. We’ve compiled some important information on social media and HIPAA to help you navigate the murky waters of patient privacy in a very public forum.\nHIPAA stands for the Health Insurance Portability and Accountability Act. Essentially, this means your medical information is private and protected. Those who can access your private information include doctors, hospitals, those involved with your care, or the police (in the case of an assault, suspected abuse, or a gunshot wound). Insurance companies and workers’ compensation carriers are also exempt from HIPAA. Violations result in fines that can be as high as $11,000 per incident.\nThe writing on the wall\nThe good news: You’re not liable if a patient posts personal medical information on your social media pages. The bad news: If you don’t make a diligent effort to remove these personal posts, you could be found negligent.\nThe laws aren’t always clear, according to David Harlow, a health care lawyer and consultant. Harlow states that private health care information is subject to higher standards, and overlapping regulations can easily cause confusion. Play it safe and have a designated time to “scrub” your social media pages.\nsocial media don’ts\nAre your patients aware they shouldn’t post private information to social media pages? Some don’t realize there’s no such thing as privacy when posting on a public internet page. It’s important that you publish a notice explaining that patients shouldn’t write personal health information on your wall. You may wish to write a blog post about it and link to it from your social media pages. Having a policy placed in a prominent spot demonstrates you’re implementing due diligence.\nPatient testimonials are often powerful and effective stories that can have a dramatic and emotional effect on potential clients. When considering a patient testimonial, it’s critical that you have a signed HIPAA consent form. (An example of one can be found here.) Realize these forms must be housed in a secure location. Often, it’s best to have the medical practice place the form in the patient’s file.\nIf you want to use anonymous testimony, be careful. HIPAA doesn’t consider information truly anonymous unless it has been removed of 18 patient identifiers. Even in large cities, it’s not too difficult for friends and relatives to piece together information to discover the patient’s identity.\nResponding to negative reviews\nNo one wants to have a negative review “hanging” on the internet. However, be careful when responding to the review. If a patient complains that her allergies were not treated properly and the medical practice responds publicly, it is confirming two things: 1) this person is a patient and 2) the patient has a certain medical condition.\nSo how do you avoid this? It’s safe to make general statements in regard to office policies, even though it’s often difficult to keep these messages from sounding “canned.” Essentially, it’s important to acknowledge that the patient has been heard and has a way to address his/her issue. In addition, solicit positive patient reviews to offset the negative reviews, or hire a firm to help you with monitoring social media sites.\nAn example of a response to a patient complaint that does not violate HIPAA could be phrased:\nAt our medical practice, we strive for the highest levels of patient satisfaction. However, we cannot discuss specific situations due to patient privacy regulations. We encourage those with questions or concerns to contact us directly at [phone number].\nYou may also be interested in reading the American Medical Association’s policy on social media. If you’d like help on how to navigate this delicate terrain of patient privacy on social media, contact us! We have years of experience and a HIPAA expert on staff ready to help."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:8be5154b-8f8d-4334-8b89-cc735132ab25>","<urn:uuid:ffa286e1-a3f7-4fd0-8c99-e7c6a877bf94>"],"error":null}
{"question":"As both a flight instructor and simulator enthusiast, I'm curious how flight training differs between military F-35 simulators and commercial 767 simulators. What are the key training capabilities that distinguish these two platforms?","answer":"The F-35 and 767 simulators have distinct training capabilities. The F-35 simulators feature distributed mission training (DMT) allowing up to 12 pilots to train together in networked simulators across different locations, with exact operational flight program software matching the actual aircraft. Meanwhile, the 767 simulator focuses on commercial operations with interactive checklists, automatic helper mode, and extensive systems simulation including dual FMS, triple IRS, and fail operational autoland capabilities. The F-35 emphasizes tactical integration with other military aircraft, while the 767 simulator provides detailed civilian aviation features like weather radar, ground operations, and cabin crew interactions.","context":["F-35 operational flight program simulator. Photo courtesy of Lockheed Martin\nPilots training to fly the F-35 Lightning II will soon have the ability to virtually fly with up to 12 flight simulators networked together across different bases and training facilities as if they were flying within a fully integrated environment.\nLockheed Martin describes the concept as distributed mission training (DMT), which will be a generational leap in fighter jet flight simulator training technology compared to what’s available to F-35 pilots today. Starting in 2019, F-35 pilots training at the U.S. Air Force Weapons School will be introduced to DMT, prior to Lockheed expanding the integration of the technology into other areas of training.\n“The next big step for us is distributed mission training,” John Dixon, F-35 training operations manager at Lockheed Martin told Avionics International.\n“The training is focused on allowing more than four simulators to be networked together, which is what they’re able to do today. DMT enables pilots to train together in a distributed fashion, where F-35 pilots will be able to train as 12-ship together, and we're looking at increasing the blue force package and increasing the scenario to maximize training, so forces are training just like they will fight in an integrated environment,” he said.\nLockheed Martin first received the $8.5 million contract award calling for the introduction of DMT across the F-35 program in March 2018. The contract sees the introduction of DMT occurring globally through May 2021.\nF-35 pilot training software. Photo courtesy of Lockheed Martin\nA goal for introducing the DMT technology is to also allow F-35 pilots to virtually fly with other fighters, tankers and surveillance aircraft from other locations as well. All of the simulators will be connected across a virtual network.\nDixon said F-35 training software has been continuously updated in line with communications, navigation and surveillance upgrades being introduced to the aircraft itself. Over the next 10 years, the fifth-generation fighter jet will see some 60 different software and hardware upgrades, including Raytheon’s upgrade to the distributed aperture system (DAS) to be introduced beginning with Lot-15 F-35s.\n“The simulators we’re using are operational flight program technology, meaning they have the exact same software and code that the aircraft has, there’s no difference. One of the capabilities that we try to ensure the pilots are used to is turning the DAS mode on and looking through the airplane,” said Dixon.\nPhoto courtesy of Lockheed Martin\nF-35 simulators have now all been upgraded to include Block 3F training software as well. That upgrade has additional weapons, communication and expanded sensor mode upgrades among other new functionality.\nWhile Dixon did not provide a firm date on when the new DMT training technology would become available, it will start in the U.S. before expanding to F-35 pilots training in the U.K.\n\"That capability will start to roll out next year, we’re going to start with the Air Force as the lead recipient, and then we’ll continue to follow that with the U.S. services and the U.K.,” said Dixon.","- Officially licensed by the Boeing © Corporation\n- Suitable for beginners - Quick start mode (all systems ready) and tutorials modes\n- Great for advanced users - Complex systems are simulated\n- The most advanced X-Plane Airliner along with other FF Aircraft\n- Full support of in-flight Navigraph charts\n- X-Plane 11 style pop-out EFB with checklist, options, menus, PA announcements and more\n- Full VR support in several modes, native and exclusive FlightFactor mode\n- A very flexible architecture : You choose the set up\n- Different options for many avionics instruments including two types of FMC.\n- Options to composite your own EICAS, EADI and EHSI displays.\n- Most of the options included in the real 767\nInteractive checklists and 'Autohelper' Exclusive feature\n- Full electronic interactive checklist with automatic action detection.\n- Exclusive: An automatic \"helper' mode performs all the actions for you, you just CHECK the items.\n- A tutorial which shows the user what to do and when.\nPerfected Flight model\n- Accurate flight model, as close as it gets to real performance. Tested by real pilots and translated to X-Plane\n- A dynamic and customizable center of gravity that depends on actual cargo and passenger load\nFully Functional Professional FMS and EFIS System\n- Custom Flight Management Computer, integrated with other plane systems.\n- Custom programmed LNAV logic for terminal procedures from updatable database.\n- VNAV-managed climbs and descends.\n- Optimum cruise performance and step climb calculation.\n- Two independent analogue instrument sets for captain and first officer.\n- Two independently simulated EFIS (EADI/EHSI configuration) for captain and first officer.\n- Dual-FMS with two independently working CDUs.\n- Working instrument comparators.\n- Triple IRS and triple symbol generator systems with realistic instrument source switching.\n- Dual air-data computers with custom failure modes and source switching.\n- Independent 2 nav and an ILS receivers.\n- Realistic inertial, radio and GPS position updating, you can see the individual inaccuracies of those systems.\n- Triple-channel autopilot with realistic dependencies.\n- Fail operational and fail passive auto land with mode degradations based on system failures.\n- Load company routes generated by Professional FlightPlanner X (or other compatible programs) directly into the FMC.\n- FMC can be used on external touchscreen or tablet, optimized for the Retina iPad.\nCustom Systems and Failure model\n- Detailed and deep simulation of almost every system in the real aircraft.\n- Custom air and pressure system.\n- Electrical system with all AC and DC busses modeled - see which system depends on which bus.\n- Hydraulic system that uses a little fluid when treated correctly and a lot of fluid if used incorrectly.\n- Multistage custom failure system - over 200 more failures than X-Plane. Exclusive feature\n- Ability to fix failure by following proper procedure.\n- Persistent failure and maintenance system. Aircraft wear and misuse will carry over to your next flight.\nWarning system and radars Exclusive feature\n- Fully functional GPWS with all the modes the real plane has.\n- Fully functional terrain radar, with custom database (just like the real plane), a look-ahead warning system and many other features.\n- Weather radar that works like the real thing. Including tilt and gain functions.\n- Ground clutter, turbulence detection and windshear prediction.\n- Accurate dimensions based on exterior drawings provided by Boeing.\n- Very detailed exterior modelling with high resolution textures.\n- Very high resolution 3D cockpit with every switch functional.\n- Spatial rain simulation with high detail.\n- Very detailed passenger cabin graphics including galleys.\n- Additional graphic features: real working oxygen masks both in cockpit and cabin, dynamic window blinds that react to sunlight etc.\n- Smooth and accurate wingflex.\nSpecial effects with dynamic reflections Exclusive features\n- Multilayer dynamic reflections on all glass objects.\n- Reflective metal and plastic objects in the cockpit.\n- Glossy exterior dynamic reflections Exclusive feature\n- XP weather enhancements like custom windshear.\nAdvanced Custom Sounds Exclusive features\n- A professional sound pack by BSS Studio\n- Several hundred custom sounds.\n- In-cockpit custom sounds.\n- Switches with individual sounds.\n- Many individual system sound inside and outside.\n- Airport environment sounds.\n- Cabin sounds.\n- 3D stereo sound system for engines.\n- In flight cabin announcements.\n- Interactive communication with the cabin crew (reporting misconfigurations and passenger comfort problems).\nOn Screen menus Exclusive features\n- An tablet-like menu popping-up from the cockpit.\n- Custom pages for loading/unloading fuel, cargo and passengers, customizing the CG, calling for pushback and performing maintenance.\n- Ability to customize the plane with winglets, reflection level, wingflex level and set other options to be saved or default.\n- Ground equipment and door pages.\n- Failure monitoring menu.\n- Working push-back truck - Fully controllable with your joystick.\n- Passenger bus and stairs or optional gate configuration (passengers can be loaded from gate instead of bus).\n- Fuel truck, de-Icing truck, GPU, ground-start units both visible and fully functional with airplane systems.\n- Other ground equipment.\nProduced by VMAX. Designed by FlightFactor and StepToSky"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:9324ed02-672e-4a4f-8eaa-74d1fdec9084>","<urn:uuid:9372c872-f684-429c-8249-095fe8912b5e>"],"error":null}
{"question":"La Crosse River适合划船吗? Good for canoeing?","answer":"Yes, the La Crosse River is good for canoeing. It offers 15-1/2 miles of scenic beauty from Sparta to Rockland, meandering through natural wetlands and native prairies. It's spring-fed, ensuring ample water year-round, and features abundant wildlife. The journey takes about two hours, and rental canoes are available in the area.","context":["Boat Cruises and Ferry Rides\nBreweries & Vineyards\nCanoe and Kayak Rivers\nCanoe and Kayak Trip Outfitters\nCasinos and Race Tracks\nCheese Factories & Tours\nFarm Tours & Petting Zoos\nMountainbike & Off-road Trails\nNational Forests & Recreation Areas\nOrchards & Pick-Your-Own\nState Parks, Forests & Recreation Areas\nSummer Camps & Youth Programs\nTrain Rides & Railroad Museums\nWisconsin Canoe and Kayak Rivers\nClick the river's name on the map above, or use the text links below.\nCanoe and Kayak Trip Outfitters\nSee Canoeing Events across the state\nSee Kayaking Events across the state\nAdd a Canoeing or Kayaking Event which is not listed\n- Apple River\n- Outfitters in and around Somerset provide rentals and transportation for this popular tubing river. The first half of the trip offers relaxed family floating. The last half features a wild rapids area with a walkway on the side for the less adventurous.\n- Baraboo River\n- Offers a relaxing outing with no rapids or falls. It flows 100 miles from its headwaters near Hillsboro to its confluence with the Wisconsin River south of Portage. The lower stretches flow through two wildlife areas. Access is available at road crossings.\n- Bear River\n- Tranquil and unspoiled, with good opportunities to see wildlife. Much of the 25-mile course passes through the Lac du Flambeau Chippewa Reservation. Upper portions narrow with steady current through pine forests; lower stretch passes through Powell Marsh narrowing again before joining the Manitowish River as it enters the Turtle-Flambeau Flowage. Low hazard riffles. Several access points off town roads.\n- Black River\n- While the upper portions are considered too rocky for navigation, canoeing is popular on the central stretch through Jackson and Clark counties. From Hall's Creek Landing, 13.5 miles north of Black River Falls, the river flows through quiet forests to a landing above the dam at Black River Falls. Below the dam, put in at the landing on First Street and enjoy a gorgeous 30-mile ride to New Amsterdam. Canoe rentals are readily available along the route. The current is slow and access is available at road crossings.\n- Bois Brule River\n- World-famous for trout fishing, scenery and exciting rapids. Trips can start at Stone's Bridge (Hwy S) and proceed upstream or downstream. Upstream, there are no rapids. Downstream trips can end at landings near Hwy B, the ranger station, or at Hwy 2. The rapids between Hwy B and Hwy 2 are more difficult than those found above Hwy B. Trips beyond Hwy 2 require another day to reach the river's mouth on Lake Superior. Note: A section below the Hwy 2 bridge contains dangerous rapids which no party should attempt unless accompanied by a knowledgeable guide. There are two state forest campgrounds located along the river.\n- Brule River\n- A boundary river between Wisconsin and Michigan with long stretches of slow water and low hazard rapids. Numerous streams flow into the river, and access is available at county and forest service roads. Below Brule River Dam and at the junction where the Michigamme joins the Brule to become the Menominee, the rapid increase in current combined with huge waves can potentially swamp an open craft.\n- Chippewa River\n- Rich in history and scenic beauty, the river is navigable for most of its length to its mouth on the Mississippi. Water levels fluctuate and should be checked locally. Access is available at road crossings. Note: Some rather long portages may be necessary.\n- Flambeau River\n- One of the best whitewater trips in the Midwest. The north fork provides the most stable water flow. Its upper reaches are slower and easier to paddle; the southern sections provide the challenge of rapids and whitewater. The most traveled stretch is from Nine Mile Creek to Ladysmith. Nine Mile Creek is near Hwy 70 where the North Fork flows into the Flambeau River State Forest. There are campsites and landings along the way, so a trip can last a few hours or a week. The Upper Flambeau above Nine Mile Creek and the stretch below Ladysmith to its junction with the Chippewa are also canoeable. Access is available at road crossings, impoundments, and old logging roads.\n- Fox River\n- Flows across central and east-central Wisconsin to Green Bay. The current is slow with large areas of lake and marsh paddling above Lake Butte des Morts. Access is available at many parks along the river. The Fox is navigable below Lake Winnebago, but recreational boat traffic is heavy.\n- Grant River\n- Winds through deep valleys bounded by rocky ledges and high bluffs. One-hour to three-day trips available. Frequent rapids (class I), but minimal portaging. Wide variety of wildlife and timber. Sixteen access points.\n- Kickapoo River\n- Known as \"the crookedest river in the world,\" the Kickapoo is navigable its entire length from just north of Ontario to the Wisconsin River at Wauzeka. This popular river offers access at numerous road crossings. Rentals are available locally. The current is relaxing, but be prepared to make difficult portages around several dams and some dangerous log jams.\n- La Crosse River\n- Meandering through natural wetlands and native prairies on its way to the Mississippi River, the La Crosse River provides the canoe and kayak enthusiast with 15-1/2 miles of scenic beauty from Sparta to Rockland. This spring-fed river allows for ample water supply year 'round. The presence of an abundance of wildlife contributes to this enjoyable two-hour excursion. The official canoe landing in Sparta is located across from Fisherman's Park and a county landing is located east of Rockland at Sixth Drive. Further west, the thirteen miles from the West Salem Dam to Riverside Park in La Crosse travels through tall pine stands, small rapids and the La Crosse River Marsh. Access sites include two county landings and Veterans Memorial Park. Rental canoes are available in the area.\n- Lemonweir River\n- Navigable from western Juneau County (Hwy H) to it's mouth on the Wisconsin River. The current is generally moderate to slow, and portages are necessary at two dams. A number of sloughs near the mouth make it difficult to keep on the main channel.\n- Little Fox River\n- This slow-current waterway meanders from southeastern Wisconsin into Illinois. Access is available at county and state highway crossings. Several dams must be portaged.\n- Lower Wisconsin River\n- The Wisconsin River flows 430 miles across the state from Lac Vieux Desert in northern Wisconsin to its junction with the Mississippi River ar Wyalusing State Park in southwestern Wisconsin. Known as \"the nation's hardest working river,\" it has many power dams and resevoirs, mainly on its upper and middle portions along the lower stretch with beautiful scenery and numerous islands. The southern 92 miles of the river are attractive for family outings as there are no dams to portage. Access and canoe rentals are available at many towns along the way.\n- Lower Wolf River\n- Although it has a steady current and wild appearance, the lower Wolf lacks the rough water characteristics of its upper stretches. Public landings are located in Shawano, Shioton and New London. In the New London area, beginners can rent fiberglass canoes and tubes for family fun on both the Wolf and Little Wolf rivers. A trip down the lower Wolf can be continued through Lakes Poygan and Butte des Morts to Lake Winnebago. The river is well-known for spring walleye and white bass fishing.\n- Manitowish River\n- A river for variety and fun. Flowing from Presque Isle in Vilas County, through Boulder Junction and Manitowish Waters, to the Turtle-Flambeau Flowage in Iron County. The upper portion connects 25 lakes including the famous Manitowish Chain of Lakes. With numerous access points and boat landings, and more than fifty campsites, paddlers can design trips of any length. Downstream the lower portion is steady current flowing through beautiful north country into the flowage. Great opportunities for wildlife viewing, especially eagles. Low to medium-hazard rapids and riffles. Canoe rentals and access maps available locally.\n- Menominee River\n- A boundary river separating Wisconsin and Michigan. The upper stretch offers fast water and difficult rapids. Many rapids and dams must be portaged. Portions of the upper river are extremely dangerous and should only be run by experts in decked boats. The lower river has a slower current with some low-hazard rapids.\n- Milwaukee River\n- Flows along a scenic route into the City of Milwaukee. It features a number of low-level rapids. Several dams must be portaged, During low water, travel can be difficult due to exposed boulders and bars. Access is available at parks and dams. In the city, the adventurous may canoe or kayak from below North Avenue to the mouth of the harbor, but access points are few for small craft (none from the new Riverwalk system).\n- Mississippi River\n- Though not traditionally a canoeing river due to its varied currents and heavy commercial traffic, the river includes two canoe/kayak trails in the Upper Mississippi River National Wildlife and Fish Refuge, La Crosse District. Maps to the Long Lake Canoe Trail near Trempealau and the Goose Island Canoe Trail near La Crosse are available by calling the La Crosse District headquarters at 608-783-8405.\n- Montreal River, West Branch\n- One of the Midwest's most challenging whitewater rivers under high water conditions. Site of the 1994 ICF Junior World Championships. Water levels fluctuate greatly by season. Class II to Class V rapids and numerous falls. Lower portion flows through 300-foot-deep Montreal River Canyon. Take out at Hwy 122 near Lake Superior. Note: This river has not been officially surveyed. For expert paddlers only. Low water conditions make water impassable.\n- Namekagon River\n- Known for camping and fishing, the Namekagon provides lake and river paddling with some low hazard rapids. Low water is the chief obstacle, and levels should be checked locally. Access is available at towns (Cable, Seeley, Hayward, and Trego) and road crossings.\n- Oconto River\n- Can be paddled its entire length. The northernmost section above Chute Pond (south of Mountain) features 15 rated rapids. Water levels must be medium high or above and should be checked locally. Because of difficult whitewater and obstacles, this section should not be attempted by a novice. The river below Chute Pond continues moderately to its mouth in Green Bay.\n- Pecatonica River\n- The Pecatonica River and the west branch of the Pecatonica both offer a mild current and picturesque scenery including farmland, wood lots and marsh. There is no rough water, but it may be necessary to skirt an occasional fallen tree. The current slows below the junction of the rivers near Browntown.\n- Peshtigo River\n- The upper portion flows through beautiful Nicolet National Forest and has some of the most difficult whitewater in the midwest. The river above Caldron Falls Reservoir (Hwy C) has long and difficult rapids that require a high degree of skill. Water levels should be medium high or above. Below Crivitz, the river slows and widens in sharp contrast to its previous character. Reservoirs on the central river provide lake paddling.\n- Pine River (NE Wisconsin)\n- A state-designated wild river, the Pine courses through some of the most primitive areas in the Nicolet National Forest. Spring and early summer are the best times to plunge in as water levels drop significantly in the summer. Several rapids challenge the paddler and a few require portaging. For more information, contact the Nicolet National Forest at 715-362-1300.\n- Pine River (SW Wisconsin)\n- Popular for a relaxing trip. The upper river meanders through farmland while the lower stretch flows through a wildlife area to its mouth on the Wisconsin River. Access is available at Richland Center and a number of county roads.\n- Platte River\n- Navigable from Ellenboro to the Mississippi River. Frequent changes from quiet water to rapids (Class I). One-hour to two-day trips available. Eleven access points.\n- Popple River\n- A state designated wild river, the Popple is a river of contrasts as it flows through the Nicolet National Forest. Long stretches of still waters are interrupted by short, exciting rapids. Best water levels occur in the spring and early summer. For more information, contact the Nicolet National Forest at 715-362-1300.\n- Red Cedar River\n- The Red Cedar runs approximately 85 miles from Lake Chetac to the Chippewa River below Menomonie. The upper 20 miles are relatively shallow and wide with a brisk current and lots of islands -- an ideal float trip for novices and families. The lower 15 miles parallel the Red Cedar State Bike Trail. Fishing is good all along the river. Access is available at numerous county parks. Canoe rentals are offered at many towns along the river.\n- Rock River\n- Meanders through massive wetlands and shallow lakes amidst the rich farmland and picturesque glacial terrain of Dodge, Jefferson and Rock Counties. The east and west branches of the Rock River meet in the famous Horicon Marsh. The river then flows through both Lakes Sinissippi and Koshkonong on its way to the Mississippi River. Access points at many road crossings and in Horicon, Watertown, Fort Atkinson, Janesville, and Beloit. The current is relaxing, but be prepared to portage several dams and an occasional fallen tree.\n- Sheboygan River\n- The river flows from the Sheboygan Broughton Marsh to Lake Michigan. Navigable waters begin at the Marsh and flow well over 25 miles to Lake Michigan. It is necessary to portage several dams and falls. The Pigeon, Mullet and Onion Rivers (all tributaries of the Sheboygan) can be paddled at high water.\n- St. Croix River\n- Designated a \"National Scenic River,\" it combines smooth water and low to medium-hazard rapids and is navigable from its source at Solon Springs to its mouth on the Mississippi River. There is, however, a dam and portage at St. Croix Falls. The lower portion (below St. Croix Falls) is open to commercial traffic.\n- Sugar River\n- Flows gently through forested riverbanks in the rich farmlands of southern Wisconsin. Enjoy three wildlife areas enroute. Two dams must be portaged on the upper river; the Albany Dam has a stairway, while the Decatur Dam can be by-passed to Broadhead by using the mill race waterway. The upper river is open with clearance around log jams. The lower river from the Highway 11 bridge to the state line is navigable, but be prepared to portage fallen trees.\n- Tomahawk River\n- Offers a medium to fast current including two difficult rapids on the lower stretches of the river. Access is available at dams and road crossings.\n- Turtle River\n- A relatively unknown and unspoiled river flowing 27 miles through Iron County lakes and wilderness into the Turtle-Flambeau Flowage at Lake of the Falls. Some Class II to III rapids that may require portaging depending on water levels. Portage at Shay's Dam Falls is required. Numerous public access points and state and county campsites.\n- Upper Wisconsin River\n- The Headwaters of the Wisconsin River flows south from Lac Vieux Desert to Eagle River where it joins the Eagle River chain of lakes and eventually, 430 miles from its source, joins the Mississippi near Prairie du Chien. The Wisconsin drains a third of the state and is interrupted 26 times by hydroelectric dams, the last of which is at Prairie du Sac. Much of the Upper Wisconsin involves flowage paddling, particularly between Wausau and Castle Rock Lake. The river supports abundant wildlife as it winds through forested shores, marshes and productive farmlands. For more information, contact the Wisconsin DNR Service Center in Rhinelander at 715-365-8900.\n- Waupaca Chain O' Lakes\n- A popular area for beginners. Lake paddling is available on any of the 23 connecting lakes in the Chain. For family fun, rent a canoe from area outfitters and enyoy a meandering ride down the Crystal River. The nearby Waupaca River flows gently past farms to its junction with the Wolf River.\n- Wolf River\n- Offers variety, with relatively calm upper stretches to exciting whitewater below Lily. The stretch below Lily is recommended for experienced paddlers and is also popular for whitewater rafting. Area outfitters provide canoe and raft rentals. Access is available at road crossings with possible take-out near Markton and Hwy M before entering the Menominee Indian Reservation. Note: Regulations on paddling beyond Markton should be checked locally.\n- Yahara River\n- Flows from Madison's lakes to the Rock River. Access is available on either Lake Mendoata or Lake Monona, and along the river are road crossings. Locks lead through the series of lakes, and some dams need portaging. The current is slow and additional portages may be necessary during low water.\n- Yellow River\n- Stream and lake paddling with a slow to moderate current and a few gentle rapids. Access is available at numerous road crossings and public landings. Note: Aquatic growth may hinder navigation in mid-summer."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:3128ff1f-e3b8-488f-b480-059067096a2d>"],"error":null}