{"question":"What memory debugging options exist for Android native code, and what performance analysis features does Arm Mobile Studio provide for CPU optimization?","answer":"For memory debugging, Android developers can use Valgrind's Memcheck tool for detecting memory-related errors in C/C++, though AddressSanitizer (ASan) is more commonly used for platform development. Regarding CPU optimization, Arm Mobile Studio's Streamline tool provides native code profiling functionality to locate specific problem areas in application code. It allows investigation of processes, threads, and functions from high-level views down to line-by-line source code analysis, helping developers identify CPU bottlenecks and optimize performance.","context":["This page contains a summary of useful tools and related commands for debugging, tracing, and profiling native Android platform code. The pages within this section contain detailed information on other debugging tools for use during development of platform-level features.\ndebuggerd process dumps registers and unwinds the\nstack. When a dynamically-linked executable starts, several signal handlers are\nregistered that connect to\ndebuggerd64) in the event that signal\nis sent to the process.\nIt's possible for\ndebuggerd to attach only if nothing else is\nalready attached. This means that using tools like\ngdb will prevent\ndebuggerd from working. Also, if\nprctl(PR_SET_DUMPABLE, 0) you can prevent\ndebuggerd from attaching. This can be useful if you wish to\nexplicitly opt out of crash reporting.\nHere is example output (with timestamps and extraneous information removed):\n*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** Build fingerprint: 'Android/aosp_flounder/flounder:5.1.51/AOSP/enh08201009:eng/test-keys' Revision: '0' ABI: 'arm' pid: 1656, tid: 1656, name: crasher >>> crasher <<< signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr -------- Abort message: 'some_file.c:123: some_function: assertion \"false\" failed' r0 00000000 r1 00000678 r2 00000006 r3 f70b6dc8 r4 f70b6dd0 r5 f70b6d80 r6 00000002 r7 0000010c r8 ffffffed r9 00000000 sl 00000000 fp ff96ae1c ip 00000006 sp ff96ad18 lr f700ced5 pc f700dc98 cpsr 400b0010 backtrace: #00 pc 00042c98 /system/lib/libc.so (tgkill+12) #01 pc 00041ed1 /system/lib/libc.so (pthread_kill+32) #02 pc 0001bb87 /system/lib/libc.so (raise+10) #03 pc 00018cad /system/lib/libc.so (__libc_android_abort+34) #04 pc 000168e8 /system/lib/libc.so (abort+4) #05 pc 0001a78f /system/lib/libc.so (__libc_fatal+16) #06 pc 00018d35 /system/lib/libc.so (__assert2+20) #07 pc 00000f21 /system/xbin/crasher #08 pc 00016795 /system/lib/libc.so (__libc_init+44) #09 pc 00000abc /system/xbin/crasher Tombstone written to: /data/tombstones/tombstone_06\nThis can be pasted into\ndevelopment/scripts/stack to get a more detailed unwind\nwith line number information (assuming the unstripped binaries can be found).\nSome libraries on the system are built with\nkeep_symbols to provide usable backtraces directly from\ndebuggerd. This makes\nyour library or executable slightly larger, but not nearly as large as an\nNote also the last line of\ndebuggerd output --- in addition to dumping a\nsummary to the log,\ndebuggerd writes a full “tombstone” to disk. This contains\na lot of extra information that can be helpful in debugging a crash, in\nparticular the stack traces for all the threads in the crashing process (not\njust the thread that caught the signal) and a full memory map.\nFor more information about diagnosing native crashes and tombstones, see Diagnosing Native Crashes\nNative Debugging with GDB\nDebugging a running app\nTo connect to an already-running app or native daemon, use\nCurrent versions of gdbclient just require the process ID (PID). So to debug a process with PID 1234, simply run:\n$ gdbclient 1234\nThe script will set up port forwarding, start the appropriate\ngdbserver on the device, start the appropriate\nthe host, configure\ngdb to find symbols, and connect\ngdb to the remote\nDebugging a native process as it starts\nIf you want to debug a process as it starts, you’ll need to use\ngdbserver64 manually, but that’s easy too:\n$ adb shell gdbserver :5039 /system/bin/my_test_app Process my_test_app created; pid = 3460 Listening on port 5039\nIdentify the app’s PID from the\ngdbserver output, and then in\n$ gdbclient <app pid>\nThen enter continue at the\nNote that to debug a 64-bit process, you'll need to use\nThe error messages from\ngdb if you made the wrong choice are unhelpful\n(along the lines of\nReply contains invalid hex digit 59).\nDebugging processes that crash\nIf you want\ndebuggerd to suspend crashed processes so you can\ngdb, set the appropriate property:\n$ adb shell setprop debug.db.uid 999999 # <= M $ adb shell setprop debug.debuggerd.wait_for_gdb true # > M\nAt the end of the usual crash output,\ndebuggerd will give you\ninstructions on how to connect\ngdb using the typical command:\n$ gdbclient <pid>\nDebugging without symbols\nIf you don’t have symbols, sometimes\ngdb will get confused about the\ninstruction set it is disassembling (ARM or Thumb). The instruction set that is\nchosen as the default when symbol information is missing can be switched\nbetween ARM or Thumb like so:\n$ set arm fallback-mode arm # or 'thumb'\nThe following steps show you how to use Valgrind on Android. This tool suite contains a number of tools including Memcheck for detecting memory-related errors in C and C++.\nAndroid platform developers usually use AddressSanitizer (ASan) rather than valgrind.\n- To build Valgrind, run:\n$ mmma -j6 external/valgrind\n- Set up the temporary directory:\n$ adb shell mkdir /data/local/tmp $ adb shell chmod 777 /data/local/tmp\n- Run the system server with Valgrind:\n$ adb shell setprop wrap.system_server \"logwrapper valgrind\" $ adb shell stop && adb shell start\n- For debug symbols, push unstripped libraries to\n$ adb shell mkdir /data/local/symbols $ adb push $OUT/symbols /data/local/symbols\n- To use Valgrind during boot up, edit\nservice example /system/bin/foo --arg1 --arg2\nservice example /system/bin/logwrapper /system/bin/valgrind /system/bin/foo --arg1 --arg2\nTo see the effects, you need to create a\nboot.imgand reflash the device.\nSee Systrace on developer.android.com for deriving execution times of applications and other Android system processes.","Arm Mobile Studio\nThe Arm Mobile Studio family of performance analysis tools help you evaluate performance on non-rooted Android devices at varying levels of detail throughout your game development workflow:\n- Streamline for deep-dive performance profiling\n- Performance Advisor for quick analytics\n- Graphics Analyzer to debug graphics API calls\n- Mali Offline Compiler to analyze shader programs\nStreamline captures performance data from an Android device as your game runs. Explore the data in charts to see exactly how the device's CPU and GPU resources were used by your game.\nFor CPU bottlenecks, use the native code profiling functionality to locate specific problem areas in your application code. Investigate how processes, threads, and functions behave, from high-level views, right down to line-by-line source code analysis.\nFor GPU bottlenecks, see performance data from the Mali™ GPU driver and hardware performance counters to explore the rendering workload efficiency and quickly identify where to optimize.\nSee the average FPS over the duration of the capture, a breakdown of workload, and the average utilization of the CPU and GPU in the device. These quick metrics are useful to monitor over daily runs, to see how changes to your application affect performance during development.\nA more detailed FPS analysis chart shows how the application performed over time. Where the application is performing well, the background colour of the chart is green. In poorly performing sections, the background colour indicates what’s happening. In the example below, most of the chart below is blue, indicating that the GPU in the device is struggling to process fragment workloads.\nFurther charts provide information about the workload, the properties of your content, and how the functional units within the GPU are utilized. Each chart also shows the FPS, so that you can look for areas of correlation that might indicate a problem.\nPerformance Advisor indicates when it has located a potential problem, and links through to optimization advice on the Arm Developer website, about how to resolve the issue and improve performance.\nGraphics Analyzer enables you to evaluate all the OpenGL ES or Vulkan API calls your application makes, as it runs on an Android device. You can explore the scenes in your game frame-by-frame, draw call-by-draw call, to identify rendering defects, or opportunities to optimize performance.\nStep through the graphics API calls alongside the object geometry and framebuffer output, to evaluate how each draw call impacts the scene. See where different shaders are used in the scene, how many fragments were drawn by each shader, and how many GPU cycles were spent by each shader.\nMali Offline Compiler\nMali Offline Compiler is a command-line tool that enables you to compile shader programs, and analyze how they would perform on a Mali GPU. See an approximate cycle cost breakdown for the major functional units on your target device, to identify ways to optimize your shader programs. Learn how to accelerate your shaders with Mali Offline Compiler.\nGet Arm Mobile Studio\nFor more information about Arm Mobile Studio, visit the Arm Developer Website where you can download it for free. If you are interested in professional edition, which enables Arm Mobile Studio to be used for automatic analysis as part of a continuous integration workflow, please contact Arm."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:7dce8151-5435-4880-a93e-e7811fd1ad41>","<urn:uuid:9717d90c-1dc0-44af-a53c-fd27d2160517>"],"error":null}
{"question":"How does Descartes' view on discovering truth through mathematical reasoning compare to the Religious Studies approach of exploring truth through philosophical inquiry?","answer":"Descartes emphasized mathematical reasoning as a model for achieving certain knowledge, arguing that we must renounce questionable beliefs and follow a rationalistic method based on mathematical principles. In contrast, Religious Studies approaches truth through philosophical inquiry by examining big questions about human behavior, motivation, and existence. The Religious Studies approach, as evidenced in the Philosophy of Religion module, includes exploring various arguments about God's existence and investigating religious language, allowing for multiple perspectives rather than seeking mathematical certainty.","context":["Religion Studies is a subject that is central to world history, society and human life. It is very relevant for the world in which we live, since it informs our values, which are reflected in what we say and how we behave. A study of religion provokes challenging questions about the ultimate meaning and purpose of life, beliefs about God, the self and the nature of reality, issues of right and wrong and what it means to be human. A level Religious Studies lets you study the various philosophies and beliefs that underlie popular religions. It helps you understand the perspectives and motivations of believers and non-believers alike.\nAnyone who is interested in questions about the meaning of life, the historical basis for religion and the ideological foundations of human history and contemporary events will want to study Religious Studies, which investigates such issues in an intellectually serious way. Students of Religious Studies will grapple with ideas, which are often difficult but always interesting.\nReligious Studies is a well-regarded A level subject. The rigorous intellectual discipline that students develop, will assist them in almost any further study or career, since it is a subject which requires students to ask the big questions about human behaviour and motivation. Popular career options are law, journalism, counselling, community development work and charity fundraising.\nThe Religious Studies Department is committed to providing a supportive and purposeful learning environment. We aim to develop inquiring minds that are open to new ideas as well as being able to critically engage with them. As our students explore religion, ethics and philosophy it helps them develop a better understanding of the world around them and what motivates humans to constantly examine life and question the nature of their own existence.\nStudents who choose to study A level Religious Studies will follow the OCR A Level Religious Studies specification. This requires candidates to study three modules: Philosophy of Religion, Religion and Ethics, and Developments in Christian Thought.\nTopics taught include:\nPhilosophy of Religion:\n- Ancient philosophical influences\n- The nature of the soul, mind and body\n- Arguments about the existence or non-existence of God\n- The nature and impact of religious experience\n- The challenge for religious belief of the problem of evil\n- Ideas about the nature of God\n- Issues in religious language.\nReligion and Ethics:\n- Normative ethical theories\n- The application of ethical theory to euthanasia and business ethics\n- Ethical language and thought\n- Debates surrounding the significant idea of conscience\n- Sexual ethics and the influence on ethical thought of developments in religious beliefs.\nDevelopments in Christian Thought:\n- Augustine’s teaching on human nature\n- Death and the afterlife\n- Knowledge of God’s existence\n- The person of Jesus\n- Christian moral principles\n- Christian moral action\n- Religious pluralism, theology and society\n- Feminism, society and theology\n- The challenge of secularism\n- Liberation Theology and Marx\nThere are ten lessons per week and the course is typically taught by two teachers. Students are provided with a core textbook plus additional department handouts and digital resources. Students are regularly assessed through the submission of essays and seminar presentations.\nThe course is examined by three 2 hour exams at the end of the Upper Sixth.\nTrips, activities and clubs\nIn addition to our teaching there are a range of opportunities to support the development of Religious Studies knowledge including:\n- Philosophy Society\n- Lunchtime drop-in exam surgeries.\n- Cross-curricular trip to Auschwitz, with the History department. During the trip, students explore the Problem of Evil and issues of conscience and morality.\n- Miss E Haste BA (Head of Religious Studies)\n- Mr E Cavendish\n- Mr J Skipworth","|Philosophy Pages||Dictionary||Study Guide||Logic||F A Q s|\nLife and Works\n. . Method\n. . Animals\n. . Doubt\n. . Cogito\n. . God\n. . Error\n. . Extension\n. . Dualism\n. . Cartesianism\nThe first great philosopher of the modern era was René Descartes, whose new approach won him recognition as the progenitor of modern philosophy. Descartes's pursuit of mathematical and scientific truth soon led to a profound rejection of the scholastic tradition in which he had been educated. Much of his work was concerned with the provision of a secure foundation for the advancement of human knowledge through the natural sciences. Fearing the condemnation of the church, however, Descartes was rightly cautious about publicly expressing the full measure of his radical views. The philosophical writings for which he is remembered are therefore extremely circumspect in their treatment of controversial issues.\nAfter years of work in private, Descartes finally published a preliminary statement of his views in the Discourse on the Method of Rightly Conducting the Reason (1637). Since mathematics has genuinely achieved the certainty for which human thinkers yearn, he argued, we rightly turn to mathematical reasoning as a model for progress in human knowledge more generally. Expressing perfect confidence in the capacity of human reason to achieve knowledge, Descartes proposed an intellectual process no less unsettling than the architectural destruction and rebuilding of an entire town. In order to be absolutely sure that we accept only what is genuinely certain, we must first deliberately renounce all of the firmly held but questionable beliefs we have previously acquired by experience and education.\nThe progress and certainty of mathematical knowledge, Descartes supposed, provide an emulable model for a similarly productive philosophical method, characterized by four simple rules:\nWhile engaged in such a comprehensive revision of our beliefs, Descartes supposed it prudent to adhere to a modest, conventional way of life that provides a secure and comfortable environment in which to pursue serious study. The stoic underpinnings of this \"provisional morality\" are evident in the emphasis on changing oneself to fit the world. Its general importance as an avenue to the contemplative life, however, is more general. Great intellectual upheavals can best be undertaken during relatively calm and stable periods of life.\nIn this context, Descartes offered a brief description of his own experience with the proper approach to knowledge. Begin by renouncing any belief that can be doubted, including especially the testimony of the senses; then use the perfect certainty of one's own existence, which survives this doubt, as the foundation for a demonstration of the providential reliability of one's faculties generally. Significant knowledge of the world, Descartes supposed, can be achieved only by following this epistemological method, the rationalism of relying on a mathematical model and eliminating the distraction of sensory information in order to pursue the demonstrations of pure reason.\nLater sections of the Discourse (along with the supplementary scientific essays with which it was published) trace some of the more significant consequences of following the Cartesian method in philosophy. His mechanistic inclinations emerge clearly in these sections, with frequent reminders of the success of physical explanations of complex phenomena. Non-human animals, on Descartes's view, are complex organic machines, all of whose actions can be fully explained without any reference to the operation of mind in thinking.\nIn fact, Descartes declared, most of human behavior, like that of animals, is susceptible to simple mechanistic explanation. Cleverly designed automata could successfully mimic nearly all of what we do. Thus, Descartes argued, it is only the general ability to adapt to widely varying circumstancesand, in particular, the capacity to respond creatively in the use of languagethat provides a sure test for the presence of an immaterial soul associated with the normal human body.\nBut Descartes supposed that no matter how human-like an animal or machine could be made to appear in its form or operations,\nit would always be possible to distinguish it from a real human being by two functional criteria.\nAlthough an animal or machine may be capable of performing any one activity as well as (or even better than) we can, he argued,\neach human being is capable of a greater variety of different activities than could be performed by anything lacking a soul.\nIn a special instance of this general point, Descartes held that although an animal or machine might be made to utter sounds resembling human speech in response to specific stimuli,\nonly an immaterial thinking substance could engage in the creative use of language required for responding appropriately to any unexpected circumstances.\nMy puppy is a loyal companion, and my computer is a powerful instrument, but neither of them can engage in a decent conversation.\n(This criterion anticipated the more formal requirements of the Turing test.)\n|History of Philosophy|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"sensitive"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:b849b107-c59c-42d8-bd21-0e68c551bac8>","<urn:uuid:33524e9c-29b6-4b89-b813-8e98302851b1>"],"error":null}
{"question":"I'm a medical student interested in blood testing. How do blood culture tests and arterial blood gas tests differ in their main purposes and sampling procedures?","answer":"Blood culture and arterial blood gas (ABG) tests serve different purposes and require different sampling methods. Blood cultures are used to detect micro-organisms spreading infection through the blood, typically in extremely ill patients with high fever or toxic shock. They require 10-20ml of blood drawn from veins under strictly sterile conditions, with multiple bottles containing special nutrients to grow different types of organisms. In contrast, ABG tests measure oxygen, carbon dioxide, pH, bicarbonate, and oxygen saturation levels in the blood. ABGs require collecting a small blood sample specifically from an artery (usually in the wrist), and the sampling is done by feel since arteries cannot be seen. While blood cultures take several days to yield complete results including antibiotic sensitivity, ABGs provide immediate information about a person's breathing status and are often repeated multiple times during hospitalization for patients in respiratory distress.","context":["WHAT IS IT?\nBlood culture is a test done to detect micro-organisms which may be spreading infection through the blood. Patients needing this test are usually extremely ill.\nHOW IS IT DONE?\nA sample of the patient’s blood is incubated using a special preparation which encourages the growth of any bacteria which may be present. Once enough organisms have been grown, a sample can be examined under the microscope for identification. The bacterium is then exposed to different antibiotics to test which ones it may be sensitive or resistant to.\nThis is not an ordinary blood sample, so certain precautions must be observed, especially in the actual sampling procedure.\nThis blood sample must be taken under sterile conditions. This means that\n- The area around the patient must be clean\n- The doctor/technician must scrub hands as he does for an operation and must wear a mask and sterile gloves\n- Sterile drapes must be placed around the patient's arm, which is swabbed with alcohol or other antiseptic\n- All syringes, needles and blood sample bottles must be sterile\n- Blood (10-20ml)is drawn from the vein in the standard manner, and transferred to special sterile bottles (containing growth mediums) without the needle being touched\n- The bottles are immediately placed in a special container and transferred to the laboratory incubators.\nSeveral different sampling bottles are used, each with special nutrients encouraging the growth of different types of organisms: aerobic, anaerobic, and occasionally fungal. If there is a clinical suspicion of a particular organism, the lab will provide a special culture medium for the test.\nComplete results, including antibiotic sensitivity, may take several days. Whilst waiting for results, the patient may be started on an antibiotic which is considered the most likely to work. If test results confirm the choice, treatment is continued. If the organism cultured proves insensitive to the antibiotic chosen, the correct one can then be selected.\nWHEN IS THE TEST DONE?\nBlood culture should be considered in all patients who are ill with a high fever and rigors, have signs of toxic shock, or whose history puts them at risk of blood-borne infection (for example, patients with abnormal or mechanical heart valves).\nSome patients have obvious sources of sepsis, like an open, infected wound, and this may be swabbed to detect the organism in the wound. While it is likely that the same organism is in the blood and thus causing the illness, this is not necessarily so. Especially if the wound is treated but the patient remains ill, blood culture will be done.\nRESULTS AND PITFALLS\nInadequate skin and site preparation and faulty technique can permit bacteria on the skin to be transferred along with the blood sample into the special bottles. These bacteria will therefore grow, and give a false result. This is usually recognised by the laboratory, because there are many different skin organisms: if the culture yields a variety of organisms, the result is probably due to contaminants.\nIf the test yields a \"pure growth\", that is, a heavy growth of only one type of organism, it is highly likely it is the one causing the patient’s condition. Treating with the appropriate antibiotic then has a good success rate.\nA negative result – no growth after four days – does not necessarily mean there is no organism involved, especially if the patient’s condition does not improve. It could be a consequence of a patient being given antibiotics before the blood was sampled. If the number of organisms in the patient’s blood is low, the chances of growing it are also reduced. This can partly be overcome by using more blood per sample, and by repeating the test several times. Culture-negative endocarditis is a known entity, and must be considered when clinical finding are consistent with infective endocarditis.\nFollow-up blood cultures may be needed (for instance, in patients with mechanical heart valves who have been treated for endocarditis). For this, three consecutive negative cultures are usually required before the patient is considered cured.\n(Dr AG Hall, Health24, January 2008)","“ABG’s” is another one of those medical acronyms that you hear all the time when you have asthma or lung disease. And if you’ve ever been unfortunate enough to be hospitalized for your asthma, no doubt you’ve had one of these tests done on you.\nABG stands for “Arterial blood gases”. An ABG is a blood test that measures the PO2 (oxygen) and PCO2 (carbon dioxide) in arterial blood ( blood that comes from an artery vs a vein). The Ph (the acidity/alkalinity of the blood ), HCO3 (Bicarbonate buffers) and SAO2 (O2 saturation) are also determined.\nThe test is performed by collecting a small sample of blood from a peripheral artery (not a vein) usually in your wrist where you feel your pulse. Because you can’t see arteries, the person drawing the sample has to go entirely by feel, which means it can often take more than one poke to hit the blood vessel ( and yes…it can hurt like crazy too).\nABG’s are usually only obtained on asthmatics who are experiencing severe respiratory distress and are not responding well to treatment. Because ABGs can only indicate a persons breathing status at that particular moment, they are often repeated several times during the course of a hospitalization. Patients who are in critical condition and who require frequent ABGs will usually have a special catheter inserted into the artery (called an Arterial Line). Whenever an arterial blood sample is needed, it can be drawn directly from a special port on the catheter instead of having to poke the patient with a needle every time.\nNormal ABG values would look something like this:\nPh 7.35-7.40 PCO2 35-40 PO2 80-100 HCO3 24 O2 SAT 97-100%\nAll 5 of these parameters are used in evaluating the respiratory status of a patient, but for the sake of this discussion the value we’re most interested in is the PCO2 (carbon dioxide). CO2 is a waste product of cellular metabolism and because we get rid of it by exhaling it out of our lungs, measuring how much CO2 is in our blood gives us a good indication of how well our lungs are doing their job. The faster and deeper we breath, the more CO2 we expel (we call this “Hyperventilation”). The slower we breath, the more CO2 we retain (we call this “Hypoventilation”). During normal breathing, the body maintains just the right level of CO2 (35-40)\nHere’s a scenario of what can happen to your CO2 levels during a severe flare;\nDuring a severe asthma attack it becomes very difficult to breath. As a result, you’re forced to use more breathing muscles than you normally would (what we call accessory muscle use) in order to get the air in and out of your lungs. This extra muscle use causes more CO2 (waste) to be produced. Ever notice that you breath faster during an asthma attack? It’s not only from air hunger. The body’s first line of defense against rising CO2, is to breath faster in order to blow it off and keep the levels within a safe range. However, if the work of breathing gets too severe, the lungs are unable to expel the CO2 fast enough and blood levels continue to rise …we call this “Respiratory failure”. The work of breathing can become so overwhelming, that the person begins to tire out and could eventually stop breathing all together… we call this “Respiratory Arrest”. Hopefully this will never happen to you.\nToo much CO2 can make the blood very acidic (decreased Ph). If the CO2 blood levels get too high or the Ph too low, it can cause damage to the vital organs such as the brain and heart. As a second line of defense the kidneys will hold on to more sodium bicarbonate which helps buffer the extra acidity.Bladder activity is also increased to help get rid of the acidic compounds. The problem with this 2nd line of defense, is that it takes much longer to kick in.\nOne way we can quickly the lower CO2 in someone who’s in respiratory failure, is to blow the CO2 off by mechanical means…either with BIPAP or a ventilator. By placing someone on a ventilator, we can control how much air moves in and out of a persons lungs thereby regulating how much CO2 moves out.\nThe example above, is of course an over simplification of what can occur during a severe attack. There are many other factors involved, but the basic goal in treating a critically ill asthmatic is to open up their airways and normalize their blood gas values.\nAs a Respiratory Therapist Ive done thousands of ABG’s on patients, and as a severe asthmatic myself, Ive had hundreds of them done on me. Here are some of my actually blood gas results between 2005 and 2008 while a patient in the UCSF hospital intensive care unit.\n**Following a more recent hospitalization(4/2013) I wrote a post regarding Bipap and how it effects ABGs**."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:29151e0a-061f-4615-8671-d3093849a078>","<urn:uuid:8c2bf0d0-04db-47a3-aac9-344934b0d275>"],"error":null}
{"question":"What are the preservation techniques used for historic masonry buildings, and what environmental benefits do they offer?","answer":"Historic masonry buildings can be preserved through techniques like Compatible Injection Fill (CIF), which uses cement-based grout pumped at low pressures into cracks and voids to strengthen fragile materials. Advanced computer modeling and finite element analysis are used to evaluate structural behavior and design appropriate interventions. Regarding environmental benefits, modern masonry stains are environmentally friendly, being water-based and containing low VOC content, which helps prevent ozone depletion. These stains are also long-lasting, requiring no maintenance or reapplication for 25 years or more, unlike paint which needs reapplication every 5-7 years, thus reducing resource consumption.","context":["By Pricilla Pendley\nLet’s say you’re working on a restoration project and your customer comes to you with a 20-year-old brick asking if you have a match. What do you say? “Don’t have it. Good luck, try somewhere else”? If that is your approach, chances are your potential customer is going to look for another option, such as stucco or vinyl, to avoid the hassle of looking for an acceptable brick match.\nA brick style or type can become unavailable for a number of reasons. In the case of clay brick, the clay deposits in the ground become depleted after extended mining. Subtle changes in color may be gradual when mining the same pit for a period of months, but over a period of years, the difference can be dramatic. Sometimes it could simply be a matter of the brick company going out of business or the brick itself being discontinued. Therefore, providing an acceptable brick match for an addition or renovation becomes a challenging exercise for design professionals.\nWhen it comes to additions and renovations, let your customer know there is a solution to matching existing brick, block or stone. Specialized masonry and concrete stains applied by skilled and qualified applicators can be formulated to blend new masonry to old, make old look new, or achieve dramatic color changes. When a brick match cannot be found, try instead to obtain a similar size and texture of brick for your addition, renovation or repair. This allows for more effective blending of the new and existing substrates.\nMasonry stains have been around more than 100 years and could be found across Europe for decades before the concept made its way to North America. One day in the early 1970s, Russell Gray, while working for his father at a Canadian brick manufacturer, accidentally spilled acrylic paint on a concrete floor. He was able to remove the paint, but it left a stain on the concrete. This led to the idea to create a product to stain brick to reduce the amount of waste in the brickyard. Gray began testing this concept on leftover brick from various brick lots in his father’s brickyard. This is where he learned that he had a talent for matching brick colors, and that stains provided more precise control over color than paint.\nYears of product testing produced a water-based stain specifically for masonry, and eventually concrete, now known as the NawTone stain product line. This concept was considered revolutionary to masons in North America at the time and quickly became known as “masonry magic.” Nothing else like it was available on the market, and so Nawkaw Corp. was born. Nawkaw is now one of the largest masonry stain application service providers in the world, working to educate architects about the sustainable benefits of stain for masonry projects.\nThe Staining Process\nUnlike paint, masonry stains penetrate into the pores of brick, concrete block, and other concrete materials, leaving the surface texture authentic. This is significant because the surface texture is one of the most appealing features of masonry products. Painting masonry products is not recommended, as it creates a barrier that does not allow moisture to flow outward. Make sure the stain you choose is vapor permeable, as this outward water vapor flow is necessary to prevent potential moisture problems.\nOnce all necessary repair work or renovations are complete, the staining process can begin. After colors have been chosen, certain steps should be taken to ensure that the surface is ready for stain application. Surface cleaning, such as pressure washing and even an acid wash in some instances, is an important preparation step. A professional stain applicator will be able to determine the best steps to prepare your project surface for optimal coverage. In the examples of infill repair shown on these pages, stain disguises the new brick as if the repair had never happened.\nCase Study: University of Dayton\nMany universities, hospitals and commercial developments have utilized stain services as part of their renovation efforts. As buildings age, masonry staining has become a way of blending old brick with new in building repairs. It’s an affordable option over cladding or re-bricking. Masonry staining also allows the flexibility to tie multiple buildings together using similar or the same colors. Even similar accent colors can be applied to help unify the appearance of multi-building complexes.\nThe University of Dayton in Dayton, Ohio, is an example of how color can modernize the appearance of older masonry facilities. This project consisted of decorative, patterned masonry the client wanted to cover. A high-pigment-load stain was used to ensure complete coverage of these areas. The before and after images are dramatic and represent the full possibilities of a professional stain application.\nFinancial and Historical Benefits\nThe economic benefits of utilizing masonry stains are a positive addition to new construction or a remodel. Enhanced curb appeal can upwardly affect property value. Additionally, reduced operating costs can be realized without the need for maintenance and re-application every five to seven years. Unlike paint, a quality masonry stain can withstand 25 years or more of normal weathering and wear.\nThe growth of Main Street revitalization in small towns continues, and staining allows for street enhancements where architects are sympathetic to the traditional style of historic masonry buildings. With much of this restoration work continuing to grow at a rapid pace, staining helps restore the warmth and charm of old brick for residents and visitors alike. With the ability to custom blend colors, many historical colors can be reproduced. Nawkaw Corp. works with architects and contractors across the globe, offering custom color matching and blending in a variety of aesthetic finishes. The wide array of looks that can be achieved through stain is one reason why Nawkaw products and services are specified and recommended on thousands of projects every year.\nOne of Nawkaw Corp.’s prestigious masonry staining projects was the restoration of the Margaret Mitchell House in Atlanta, Ga. This historic site was once home to the famous author of Gone with the Wind and continues to be a popular tourist stop. The project required that the building be colored to mimic the way it looked when Margaret Mitchell rented an apartment there. She wrote the majority of her novel while living there. Nawkaw masonry stains gave the appearance that the renovation had never taken place.\nChoosing a Stain\nWhen choosing a stain, look for a one that is environmentally friendly, water-based and preferably non-flammable. Volatile organic compounds (VOCs), one cause of ozone depletion, can be found in many stain and paint products and are used as a transfer device to deliver the color (as pigments and resins) to the masonry. Low VOC content shows a manufacturer’s commitment to environmental responsibility. You will also want to find a provider who has a good reputation in the masonry community and stands behind their work and products with a solid warranty. Utilizing masonry stain services for your next repair or remodel can save you time, money, and even headaches, leaving you and your customer satisfied with the end result.\n|A new, non-traditional cleaner from PROSOCO removes stains from a variety of masonry substrates in a rinseless, peelable formula.\nEnviro Klean DriKlean is a gentle but powerful rinseless cleaning solution designed for interior spaces where traditional liquid cleaners can’t go. In an easy-to-apply formula via spray, roller or brush, DriKlean safely removes dust, soot, oils and other surface soiling from limestone, sandstone, marble, travertine, plaster, terra cotta, concrete, mortar or brick. The cleaner and soiling easily peels off after drying.\nDriKlean is also free of natural rubber latex, eliminating allergy concerns for users. The new product is a perfect solution for interior restoration applications where rinsing isn’t practical. DriKlean is also low-toxicity and low-odor.\nFor more information, visit www.prosoco.com.","Historic & masonry Structures\nGlobal EXperts in Historic Structures and Preservation\nPublic and private organizations around the globe rely on Atkinson-Noland & Associates when historic structures need assessment, strengthening or repair. Our engineers have evaluated 17th century buildings in Sierra Leone, a 19th century mission in New Mexico, the Brooklyn Bridge and even Mata Ngarau on Easter Island. We provide a full range of testing and analysis, with special expertise in nondestructive evaluation and in situ testing.\nOur conservative, phased approach to projects spares client budgets, while preserving the structural integrity of historic masonry. Drawing upon four decades in research and education, our experts recommend a battery of tests to obtain materials and structural data. Naturally, we start with the least invasive tests and increase the sophistication, as required.\nWith data in hand, we use advanced computer models to provide accurate structural analysis. Extensive knowledge of building materials and construction methods allows us to recommend creative solutions that protect historic structures, using their inherent strength to meet client objectives.\nCompatible injection fill (CIF) is customized to the material properties of the host structure, based on nondestructive evaluation and materials characteristics testing. Technicians pump a fluid, cement-based grout mixture into cracks, voids, or cavities within masonry. Low injection pressures prevent damage, while strengthening already fragile materials.\nA visual condition survey locates and identifies areas of damage and deterioration. Dilapidation, cracks, signs of movement, and other localized failures are mapped and documented on drawings to help determine scope of required repair work. Significant structural cracks are measured for size, location and directionality, where accessible. In addition, sources of deterioration and likely causes of damage are identified, along with repair recommendations, as applicable.\nThis advanced computational analysis technique aids in evaluation of structural behavior, based on accurate numerical simulation of the structural response under load. By adopting reasonable modeling strategies and material properties of the actual structure (as determined through nondestructive investigation and evaluation), finite element (FE) modeling enhances the diagnosis and understanding of damage and visual distress. FE modeling also allows for an in-depth safety evaluation of the structure, contributing to the design and validation of proposed interventions.\nA flatjack is a flexible steel envelope, thin enough to fit within a masonry mortar joint. During testing, the flatjack is hydraulically pressurized and applies stress to the surrounding masonry. Flatjack tests can determine engineering properties of older and historic structures for structural evaluation, including in situ stress (ASTM C1196), masonry compressive modulus (ASTM C1197), masonry compressive strength (ASTM C1197), and mortar shear strength (ASTM C1531).\nInfrared thermography uses a hand-held camera to detect differences in temperature as little as 0.1° F. This technique allows identification of structural features and conditions not otherwise detectable by visible light. Applications include rapid location of grouted cells within concrete block, moisture infiltration, cracks in masonry, and variations in insulation. Unlike other techniques, infrared thermography allows the quick, efficient survey of large areas.\nNondestructive tools such as moisture meters, surface-penetrating radar, and infrared thermography can identify areas of high moisture content and track moisture penetration back to its source. Spray tests are often used to identify moisture leakage pathways and the rate of moisture infiltration. ANA can perform tests in accordance with ASTM C1601, ASTM C1715, ASTM E514, ASTM E1105 and AAMA 501.2.\nThis form of structural analysis assesses a structure’s resistance to earthquake loads. Seismic retrofit solutions are designed to be compatible with the existing materials. In working with historic structures, ANA uses internal strengthening techniques whenever possible to avoid altering the building’s outward appearance.\nStructural analysis characterizes the structural response when subjected to a variety of conditions and actions, such as changing environmental conditions, new load requirements, etc. This assessment involves evaluating a structure under stress, deformation and displacements, and reactions. Nonlinear analysis through finite element (FE) modeling can be used to pinpoint the source of excessive stress leading to damage of the structure (e.g., cracking in tension, crushing in compression) when evaluated under service and ultimate load ranges.\nANA can recommend appropriate repair techniques for a variety of structures, from modern commercial and industrial structures to protected historical monuments. Structural repair services extend from initial assessment to development of rehabilitation schemes, on-site observation, and quality control of repair measures.\nSurface-penetrating radar (SPR), also referred to as ground-penetrating radar or impulse radar, provides valuable information about structural and non-structural building components without causing damage. The radar data reveals voids, construction layers, and the presence of other materials, such as metal inclusions, as well as the thickness of the element."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:e8a62c6e-e3ee-4271-b546-d05850365e5a>","<urn:uuid:c8e030e2-2955-4ce7-b8d5-be45cfc68bee>"],"error":null}
{"question":"What are the two main approaches to addressing climate change impact, and how do green infrastructure solutions support these approaches?","answer":"The two main approaches to addressing climate change are mitigation and adaptation. These play vital roles in tackling climate change impacts like heat waves, reduced water availability, increased flooding, and ecosystem stress. Green infrastructure supports these approaches by providing climate change adaptation and mitigation benefits including disaster prevention, flood alleviation, strengthening ecosystem resilience, creating ecological corridors, improving landscape permeability, and enhancing wildlife habitats.","context":["Sustainability is the \"Development and natural resource use in a way that meets the needs of the present generation, without compromising the ability of future generations to meet their own needs\".\nThe Town of Gawler is embracing sustainability, both at an organisational level and within the wider community. The Council’s Environmental Management Plan outlines five core themes for environmental initiatives, including:\n- Built Environment\n- Resource Management\n- Climate Change\n- Riverine and Natural Environment\n- Community Culture\nWith a number of critical environmental issues becoming increasingly evident globally, taking action on a local level is key in sustaining current and future generations. This is recognised through Councils commitment to leading by example through environmentally responsible corporate practice and fostering environmental stewardship across the community.\nEnergy efficiency is about doing things smarter, with no compromise to comfort. By making suitable changes within the household, you are able to save on costs and reduce greenhouse gas emissions without compromising your lifestyle.\nThere are numerous resources available to help identify where you can make these changes, visit one of the links below for more information and tips. Alternatively, you can drop into the Gawler Administration Centre to collect a copy of the Energy Smart SA \"10 Step Guide to Reducing Your Energy Bills\".\nWith only 1% of the Earths water supply being suitable for drinking, our supply is limited. It is therefore crucial that we use this precious resource wisely, and care for it properly. There are many changes that you can make to save water and costs, including:\n- The installation of water efficient shower heads in your home can use up to 40% less water with each use\n- A standard showerhead uses 15L to 25L or water per minute, using a timer can help you to keep track of your showers so that you are able to set goals and reduce water use\n- There are many ways to collect rainwater, from buckets to rainwater tanks, consider collecting rainwater for use in the garden\n- Purchase water efficient appliances, including washing machines and dishwashers. Generally, front loader washing machines are more water efficient. Look for the Smart Approved Water Mark when considering appliances and be aware the products Water Rating.\nPlant indigenous plants that are resilient to dry conditions in order to use less water in the garden\nFor further information and tips on reducing water use in your home and garden, please visit the SA Water website.\nReduce, Reuse, Recycle...\nBy disposing of your waste thoughtfully, you are contributing to a more sustainable planet. Recycling correctly is easy and has a number of positive effects, including:\n- Reducing the amount of waste that goes to landfill. In turn, this reduces the potential for toxic material to leach into the ground, greenhouse gas emissions as well as helping to manage the demand on land with a growing population\n- Saving Energy, as the manufacturing process of recycled materials uses less than the production of new materials\n- The conservation of resources by reducing the need to extract raw materials from the Earth through mining and forestry\nFor more information on how to Reduce, Reuse, Recycle, please follow the below links.\nFor more information regarding Councils range of household waste services, including recycling, garbage, green waste and hard waste, please visit Waste and Recycling.\nThe ‘Intergovernmental Panel on Climate Change (IPCC) Fourth Assessment Report (AR4)(IPCC 2007)’ states that the continual increase of atmospheric carbon dioxide and other greenhouse gases due essentially to fossil fuel use, leads inexorably to global climate changes. As a result, South Australia is likely to experience continued increases in average temperatures, changes in rainfall (likely reductions in winter and spring), an increase in daily rainfall intensity but longer dry spells between rainfall events, and an increase in evapotranspiration (the combined effects of evaporation and plant transpiration). An increase in very hot days and nights, a reduction in the frequency of frosts and a likely increase in the number of extreme fire danger days are also forecasted.\nImpacts that are likely to result from the above listed changes in climate patterns within the Gawler region include:\n- Heat waves, having health impacts on the community\n- Reduction in water availability, impacting local agricultural production, the maintenance of local amenities and the health of our waterways\n- Increased intensity of rainfall events and subsequent flooding\n- Increased stress on local ecosystems, with loss of biodiversity, loss of habitat and increases in invasions by pest species\nFor these reasons, both mitigation and adaption play vital roles in tackling the issue of climate change. Town of Gawler has made a commitment to \"Develop adaption and mitigation strategies for council and the community to effectively mitigate and respond to the impacts of climate change\".\nFor further information regarding how we are implementing mitigation and adaption measures, please refer to the Town of Gawler Environmental Management Plan. For further information on climate change, follow below links.","Green building method, sometimes known as sustainable building or even green building, is an approach to building and architecture that favors environmentally sound and resource-efficient methods that last through a building’s lifecycle.\nBenefits of green building: 30-50 % water saving, 20-30 % energy saving, conserves natural scarce resources and generates less waste, enhanced air quality and excellent day-lighting and provides healthier space for occupants.\nThe building sector has the greatest potential to reduce CO2 emissions. Currently, 30% of global CO2 emissions and 40% of global resource consumption is a result of constructing buildings.\nOur main goal is to optimise the environmental performance of a building using a life cycle approach, in which the materials and products used in a building are compared and evaluated for best performance at the building level (life cycle assessment).\nOur expertise spans all phases of a project from conceptual design through construction to project hand-over; in addition to continuous operation and maintenance.\nThe built environment has a vast impact on the natural environment, human health, and the economy. By adopting green building strategies, we can maximize both economic and environmental performance. Green construction methods can be integrated into buildings at any stage, from design and construction, to renovation and deconstruction. However, the most significant benefits can be obtained if the design and construction team takes an integrated approach from the earliest stages of a building project.\nGreen infrastructure can be broadly defined as a strategically planned network of high quality natural and semi-natural areas with other environmental features, which is designed and managed to deliver a wide range of ecosystem services and protect biodiversity in both rural and urban settings.\nGreen infrastructure is effective, economical, and enhances community safety and quality of life.\nMore specifically green infrastructure, being a spatial structure providing benefits from nature to people, aims to enhance nature’s ability to deliver multiple valuable ecosystem goods and services, such as clean air or water.\nExperience has shown that investing in green infrastructure can contribute to the recovery of Europe’s economy by fostering innovative approaches and creating new green businesses. Green jobs already represent around 5% of the job market.\nGreen infrastructure and sustainability goals are our specialty, and achieving these goals requires technical knowledge and training in varied fields.\nPotential benefits of green infrastructure can include:\n- Provision of clean water\n- Removal of pollutants from air and water\n- Improved air and water quality\n- Protection against soil erosion\n- Improvement of land quality\n- Enhanced and protected biodiversity and ecosystems\n- Reducing waste streams\n- Conserved and restored natural resources\n- Creation of jobs\n- More energy solutions\n- greener cities\n- Better health and human well-being\n- Diversification of local economy\n- Minimized strain on local infrastructure\n- Improved overall quality of life\nClimate change adaptation and mitigation benefits\n- Disaster prevention\n- Flood alleviation\n- Strengthening ecosystems resilience\n- Ecological corridors\n- Landscape permeability\n- Improved habitats for wildlife\n- Reduced operating costs\n- Improved occupant productivity\n- Optimized life-cycle economic performance"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"sensitive"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:fe3dc869-0588-44b7-8ffd-988c796a68d6>","<urn:uuid:888f4a5f-235e-46c4-907a-3e8da92ff1d6>"],"error":null}
{"question":"How can I prevent my street tree from becoming a safety hazard through proper pruning techniques?","answer":"To keep street trees safe, avoid 'topping' - a discredited pruning method that opens trees to decay. Instead, follow a 5-10 year pruning cycle using certified arborists who conform to nationwide pruning standards. Large pruning cuts make trees less safe, even though homeowners often think reducing canopy size increases safety. Professional pruning may cost a couple hundred dollars but helps maintain the mandated 14-foot vertical clearance above streets while preserving tree health.","context":["The two sycamore trees stand like faithful old sentries in the middle of a quiet block on East Mission Street, near San Jose’s Bernal Park, and at first glance they appear to be identical. Probably planted at the same time, they are about the same size and have a similar wintry grandeur even unadorned by leaves.\nBut one of these trees is in trouble — as are many trees in San Jose’s urban forest. This one could someday become a danger to the neighborhood, and to Christian Bonner, an arborist at San Jose nonprofit Our City Forest, the difference could not be more obvious.\nOne of the trees has been “topped,” a now discredited method of pruning that has opened the tree to decay, and possible ruin. Bonner suspects the tree’s owner isn’t even aware that it’s in peril.\n“People see large canopy trees and they think it’s a potential hazard,” Bonner said. “Then somebody knocks on their door and says, ‘I can prune your tree and make it smaller.’ But when they make those large pruning cuts, they’re actually making the tree less safe.”\nThe Mercury News asked Bonner, an arborist for nine years, to survey several San Jose neighborhoods to gauge the health of San Jose’s urban forest after a 10-ton silver maple last month suddenly toppled on a pickup truck killing 2-year-old Mateo Ortiz.\nMateo’s death — and the debate in its aftermath over who is responsible for San Jose’s street trees — has raised questions about the safety of the trees planted decades ago along the city’s sidewalks.\n“It’s not natural to have as many problems as you see around San Jose,” said Rhonda Berry, CEO of Our City Forest, which plants many of the trees in San Jose. “We have a very neglected urban forest.”\nThe city’s Department of Transportation is considering a proposal to tax homeowners for the upkeep of street trees. But for now, homeowners are responsible.\nBonner was asked to discuss problems homeowners might encounter and to suggest ways to get San Jose’s wooded parking strips thriving again.\nHiring professionals doesn’t always produce a better result. “There are many unscrupulous landscapers, and some unscrupulous arborists, as well,” Bonner said. “If you have an arborist come out to do an assessment on the trees at your home, I’d be willing to wager that nine out of 10 would say, ‘Prune your tree,’ regardless of whether it’s needed or not. They’re businessmen and they’re trying to make a buck.”\nRooting out problems\nTree people like to refer to it as the “urban forest,” a slightly dramatic name for a lush landscape of fruit trees radically changed by development in San Jose over the past century. Willow trees no longer line the streets of Willow Glen, and oak trees have become scarce in Oak Grove.\nWhat’s left is a stand of street trees in a concrete jungle “filled with subdivisions and malls that are named after the trees that were cut down to build them,” Bonner said.\nThe most visible swath of any urban forest is its street trees. They float next to parked cars and give shade to kids riding skateboards. A small fraction of the urban forest is planted in grass strips usually no more than 3 or 4 feet wide.\nBonner found an array of bad choices that had been made about the types of trees planted in those fateful park strips. And he pointed out many “missed opportunities” by the city to improve the health of its estimated 242,000 curbside trees. And though he never criticized City Arborist Ralph Mize, Bonner noted the futility of the office as it’s currently constituted.\n“The city arborist’s office doesn’t have the resources it needs to enforce what’s on the code,” he said. “There’s all kinds of illegal removal, topping and pruning that’s done that they can’t even begin to look at because they have two inspectors for the entire city.”\nEvery neighborhood is dominated by different types of trees, each with their own advantages and challenges. Bonner’s tour took in only a small percentage of the city’s treescape, which means he also only rooted out a few of the problems homeowners confront.\nCruising down the 300 block of Gifford Avenue, Bonner spotted a sycamore — the second-most often planted tree in San Jose, after the Chinese Pistache — whose branches had been crudely shorn, and its canopy “topped.” That technique of removing large branches within the canopy of a tree was a prominent feature of the Yellow Pages advertising of virtually every tree service company in Silicon Valley as recently as a decade ago. Now certified arborists conform to a nationwide pruning standard that forbids topping.\nBut that hasn’t stopped determined homeowners from firing up their chain saws, trying to maintain the 14-foot vertical clearance above the street mandated by city ordinance. “It costs a couple of hundred dollars to hire a tree care company, so they do what they can, and that’s what they can reach when they get on a ladder,” said Bonner, pointing to a huge gap in the branches that was undermining the structural stability of the tree.\nHomeowners sometimes mistakenly assume their large canopy trees are at risk to topple. But trimming them back assures there will be problems. “When they make those large pruning cuts, they’re actually making the tree less safe,” Bonner said.\nHe found an obvious example of this on East Mission Street, where a topped sycamore had opened wounds in its bark, opening the tree to decay. Mushrooms were sprouting from the grass above the roots.\n“Those mushrooms are a clear indicator that extensive decay exists within the trunk of the tree, and probably in the roots themselves,” he said. “If they had left the tree alone, it would have been fine with a five- or 10-year pruning cycle, and it would have cost them a lot less money.”\nThe Rose Garden and Willow Glen, among the city’s highest median-income neighborhoods, also have the most mature tree canopies. That’s no accident. “The nicer neighborhoods have really nice trees,” Bonner said. “There’s a direct correlation, and it’s been proven that a house on a tree-lined street is worth more than the same-size house on a street where there are no trees.”\nThere were other cases of “weed whipper blight,” where exposed tree roots had been damaged by lawn crews. And at one point, a large branch appeared ready to fall on Bonner’s head as he stood in a park strip next to his car. “That’s definitely an imminent hazard,” he said. “If that thing came down on your car, it would probably take the windshield out.”\nAnd with that, he put the key in the ignition and quickly drove away.\nContact Bruce Newman at 408-920-5004.\nNot all tree species are suitable for the narrow strip between city sidewalks and the street. From the list of recommendations Our City Forest provides to residents:"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:57f340d3-929b-4189-ac96-2850d8631673>"],"error":null}
{"question":"Compare Churchill's negotiations on Allied war debts with the later development of international financial governance under Bretton Woods?","answer":"Churchill's approach to Allied war debt negotiations in Paris showed early attempts at coordinated financial diplomacy, where he successfully pushed for a multilateral settlement rather than bilateral debt payments, ensuring British payments to the US would be matched by proportionate payments from other Allied nations to Britain. This multilateral approach to international financial coordination had similarities to, but was less comprehensive than, the later Bretton Woods system. While Churchill focused primarily on war debt settlements among Allied powers, Bretton Woods established a broader international financial architecture that went beyond debt management to include managed currency regimes, balance-of-payments support, and development financing, though it initially emphasized reconstruction over development needs.","context":["The Bretton Woods negotiations are generally described as an “Anglo-American” affair in which the leading officials from the United States and the United Kingdom – Harry Dexter White and John Maynard Keynes, respectively – orchestrated a hegemonic handover of power and fashioned a new set of global rules, centred on the primacy of the US dollar. This helped prevent a return to the economic chaos of the inter-war years, but did little to support the needs and interests of the bulk of the world’s population in developing countries.\nThere is, no doubt, much truth in this description. Yet well over half of the governments invited to Bretton Woods were from developing regions (Helleiner 2016). Moreover, whilst the United States promoted its own strategic economic and political interests at Bretton Woods, an internationalist vision emerging from the New Deal, fashioned in the years preceding the conference, particularly in relations with Latin America, did allow for a more inclusive multilateral dialogue which at least recognized a place for all the participating countries in the conference discussions.\nParticularly active in the discussions were officials from Latin America, China (which had the second largest delegation to the conference) and India (whose delegation was divided equally between British and Indian officials because of its colonial status). The developing countries were in agreement with the broad aims of the conference to support managed currency regimes and provide short-term loans to manage balance-of-payments difficulties. Many of them also saw an opportunity to construct a more development-friendly international financial regime that would accommodate the special needs of commodity exporters and support their efforts to raise standards of living through a State-led industrialization drive. However, incipient North-South lines were also visible during the negotiations, with sharp divisions over whether long-term financing should be private or public, and the relative importance given to reconstruction versus development.\nThe retreat from these more developmental dimensions of the Bretton Woods negotiations began soon after Roosevelt’s death in April 1945, as foreign policy positions crystalised along Cold War lines, and business interests, particularly those in the financial sector, pushed back against the New Deal coalition in the United States. The 1950s witnessed a series of further retreats from a more inclusive multilateral development agenda. Truman’s inauguration speech in 1949 made a point of emphasizing the role of private capital in promoting development, which strengthened the World Bank’s own turn to focusing on domestic reforms to attract private capital and away from providing international public assistance to state-led development programmes. The United States, along with other developed countries, also fended off moves by developing countries at the United Nations to expand its reach into development finance, blocking a proposal for a Special United Nations Fund on Economic Development to offer long-term concessional loans to developing countries, despite a General Assembly vote in its favour.\nAs the 1950s came to a close, with more and more developing countries gaining their political independence, the constraints on their development ambitions arising from an unbalanced international economic order became ever more apparent. In 1962, 36 developing countries from all regions of the world organized a conference in Cairo to discuss their shared economic challenges. The meeting ended with a call to convene a United Nations Conference on Trade and Development (UNCTAD). This was subsequently endorsed by the General Assembly.\nThe first UNCTAD conference, held in 1964 and led by the Argentinian economist Raúl Prebisch, provided some key programmatic elements that developing countries would pursue in the following years: addressing terms-of-trade losses of primary exporters through commodity agreements or compensatory financing; ensuring affordable and reliable financing for development; and promoting a sustainable export-oriented strategy for developing countries that manufactured goods for developed-country markets.\nPrebisch’s report to the Conference addressed all these issues based on three essential premises: the necessity of industrialization, the need to counter external imbalances and the forces that generate them, and the need for different treatment for structurally different economies (UNCTAD, 1964). But he also highlighted the close interdependence of trade and finance in rebalancing the agenda for international cooperation and, in particular, the mutually reinforcing nature of savings and foreign exchange constraints on desired investment and growth targets for developing countries. All this meant that developing countries would need determined political efforts, domestically and internationally, to remove the obstacles to more sustained and inclusive growth.\nThe creation of UNCTAD as a permanent body following the end of the first conference set the stage for a more inclusive trade and development agenda. The purpose was to move beyond policies aimed simply at removing trade barriers to a more positive agenda. In the decade following the conference, UNCTAD advanced this agenda through its efforts to extend supplementary financing, improve the mechanisms of international liquidity, help create commodity agreements, and advocate for tariff preferences, increased flows of official development assistance (ODA), and debt relief. Despite these efforts and the fact that development issues were more vociferously raised at international meetings and discussions, the institutional and other arrangements that determined the functioning of global markets did not fundamentally change.\nFrom the late 1960s, as economic tensions within and between the developed economies began to grow and spread across the global economy, the calls for a new international economic order (a term reminiscent of the call at the first UNCTAD conference by the Group of 77 (G77) for “a new and just world economic order”) grew steadily louder. The growing strains on the Bretton Woods system around the anchoring role of the dollar, the oil price shocks that followed the collapse of the fixed exchange rate system, and the accelerating distributional struggles in the developed countries that accompanied a slowdown in productivity growth, provided further opportunities for developing countries to push for a more inclusive multilateral agenda.\nNegotiations on a New International Economic Order (NIEO) were launched at a special session of the United Nations in 1974. The thrust of the initiative, to break the international constraints on growth in developing countries, had much in common with the earlier efforts of developing countries at Bretton Woods and with reform proposals advanced by UNCTAD. However, the political context of the time encouraged a broader agenda which included regulation and supervision of transnational corporations (TNCs) − and their possible nationalization when required − the promotion of greater economic cooperation among developing countries, and, very explicitly, the strengthening of policy autonomy to manage deeper change in the structures of their economies.\nThe NIEO negotiations were seen at the time as a further challenge to the economic order established at Bretton Woods but can, with hindsight, be better understood as an attempt to revive the multilateral financial system by recovering some of its original ambition. Indeed, the possibility of forging a North-South consensus to rebalance global economic relations, strengthen international cooperation and recover the stability lost with the breakdown of the fixed exchange rate system was a central aim of the Brandt Commission established in 1977.\nThe favourable geopolitical and global economic situation was, however, only short-lived. Beginning in the late 1970s, international economic relations took a very different turn from what had been envisaged in the NIEO, with a more concerted policy backlash in the industrialized countries against the post-war Keynesian policy consensus. The initial response of policymakers in these countries to the breakdown of the Bretton Woods system, two oil shocks, rising labour militancy, a loss of control over inflation and, to some extent, government budget deficits, had been a series of ad hoc adjustments that aimed to contain the threat of “stagflation”.\nHowever, as governments and business groups increasingly viewed redistribution measures and monetary disorder as the root of a wider socio-political malaise, moves to cut welfare provision, control the money supply, liberalize financial flows and use unemployment as a tool of adjustment crystallized into an alternative policy paradigm. That paradigm sought to shift the distribution of income back towards profits through a withdrawal of the State from the active management of the economy and a dismantling of the post-war political and social compromise.\nThe resulting paradigm shift extolled the virtues of smaller government and the benefits of freeing markets from regulatory discipline and oversight. This had its international dimension in a return to beggar-my-neighbour policies and “aid weariness,” combined with capital account liberalization and with corporations seeking greater support from their governments to find new profit opportunities abroad. Moreover, solidarity in the South was beginning to fray as robust growth in some developing countries led them to downplay the threat from structural asymmetries at the international level.\nAs competitiveness trumped employment as the go-to measure of economic success, liberalization moved to the centre of the policy stage with tight monetary policy cast in the sole supportive macroeconomic role. The promise was simple: freed from government intervention, particularly regulation on international capital movements, and wage-price spirals, increased competition would spur entrepreneurship, stimulate investment and bolster wealth creation with the gains trickling down to even the poorest strata of society. The wealth creation would ostensibly spread globally through free trade and heightened capital flows. President Reagan’s refusal in 1981 to give any credence to the Report of the Brandt Commission at a meeting in Cancun effectively ended the North-South dialogue and, with it, any lingering hopes of negotiating an NIEO (Toye and Toye, 2004).\nAt the same time, the economic reality in developing countries was becoming increasingly challenging; as Paul Volcker, Chair of the United States Federal Reserve, pushed interest rates into double figures, a strengthening dollar and falling demand for commodities turned the liquidity strains and financial stresses in developing countries into solvency crises. Mexico’s default in 1982 cast suspicion on other sovereign borrowers and the flight of private capital triggered debt crises across much of the South.\nIn the absence of timely concessional multilateral support, stringent retrenchment measures were inevitable. Structural adjustment programmes, backed by a very different development policy paradigm from the one envisaged in the NIEO, and subsequently christened the “Washington Consensus”, became commonplace in developing countries as a condition for renewed access to multilateral financing and an entry ticket to private capital markets. The damage these programmes caused through dramatic cuts in government spending, rising import costs and exposure to intense international competition resulted in a lost decade for many developing countries, particularly in Latin America and Sub-Saharan Africa, and put an abrupt end to the political solidarity that had underpinned the discussions for a new international economic order.\nThe space for countries to tailor their policies to particular histories, contexts, and institutional structures, recognized at Bretton Woods, was replaced with a one-size-fits-all agenda of so-called “sensible economic policies”. The rapid ascent of financial interests eroded the checks and balances that had previously helped to constrain the more destructive impulses of market forces and channeled their more creative impulses into the kind of productive activities needed for long-term growth. Instead, it encouraged increasingly concentrated forms of market power, shorter investment horizons, and rent-seeking behavior by banks and businesses.\nThe collapse of the Bretton Woods system and the derailing of progressive alternatives paved the way for a new international financial and economic order built on the free movement of capital and a strong ideological faith in the inherent efficiency and stability of markets. Whilst its champions have declared an era of “great moderation”, the reality has been one of persistent instability and rising insecurity characterized by speculative trading, boom and bust cycles, and extreme levels of inequality, in developed and developing countries alike.\nIn the face of these centrifugal forces, the glue holding the system together has been the explosion of private debt along with a pandora’s box of new financial instruments which promised to enhance market flexibility, ensure the smooth management of debt accumulation, and boost stability (Greenspan 2005). The emergence of this lightly regulated and privatized credit system has, instead, allowed the financial sector to transact more and more with itself, creating a complex network of closely interconnected debtor–creditor relations that harbor dangerous levels of fragility and cannot easily be re-engineered for productive investments (private or public) without a fundamental reorganization of the financial system.\nRecurrent banking and financial crises have become endemic in developing countries, linked to sudden surges and stops in capital flows. The end of the boom cycle has not only pushed millions back into poverty but left behind large debt overhangs that delay the recovery of the real economy, sometimes for decades. When this cycle was repeated in the advanced countries, the consequences were global.\nThe response to the global financial crisis of 2008, despite bold pronouncements at the time, failed to rein in the unchecked power of footloose capital and undertake the required reforms to the international financial architecture, particularly with respect to managing sovereign debt. A decade on, the Covid-19 pandemic caused the largest global recession since the end of World War Two and further exposed and intensified the inequities and fragilities of the hyperglobalized world that emerged from the ashes of the Bretton Woods system. It has again demonstrated the incapacity of a liberalized international governance architecture to respond to a global crisis with effective, coordinated, and inclusive global policy and action. If the recovery from the pandemic is to avoid stretching the economic gaps within and across countries to political breaking point, as well as to bring us back from the brink of a climate catastrophe, big changes to that architecture will be needed.\nMuch like the 1970s, a combination of slower growth, economic shocks and political polarization has translated into a crisis of hegemonic leadership at the international level. And like then, geo-political tensions, energy security and the dollarized financial system are at the centre of that crisis. However, progressives in developed countries have, to date, struggled to find a successful reform narrative that links their, albeit limited, political successes at the local level and a growing intergenerational movement around environmental issues to a truly international vision. The concept of a green new deal harbours that possibility but it remains work in progress.\nThe kind of political solidarity in the South that underpinned the push for a NIEO is also missing. Still, there are a growing number of initiatives that challenge the dominant institutions and ideas that emerged with hyperglobalization: the New Development Bank and the Common Reserve Arrangement launched by the BRICS, China`s Belt and Road Initiative and India’s Solar Alliance, and the developing country coalition at the WTO pushing for a TRIPS waiver in response to the pandemic.\nToday’s world can appear bewilderingly complex and deeply interdependent. But in truth, people everywhere desire much the same things: a decent job, a secure home, a safe environment, a better future for their children, and a government that listens and responds to their concerns. They want a different deal from that offered by the sirens of free trade and footloose capital. A new new international economic order is urgently needed.\nKevin Gallagher is the Professor of Global Development Policy at Boston University’s Frederick S. Pardee School of Global Studies and Director of the Global Development Policy Center (GDP Center).\nRichard Kozul-Wright is the Director of the Division on Globalization and Development Strategies at the United Nations Conference on Trade and Development.\nPhoto: Bretton Woods Conference, 1944, UN Photo","“Finance Less Proud and Industry More Content”\nChurchill was active on many fronts, in turn taking on the United States over Allied war debts, the Royal Navy over expenditures, and the Bank of England over a return to the gold standard.\nIn January Churchill went to Paris for a finance conference of the Allied debtor and creditor nations. He had consistently opposed the American position that all inter-Allied debts must be paid bilaterally, regardless of what other nations did. He favored a general settlement. In the event, Churchill prevailed, but it wasn’t easy. He wrote in a letter to Clementine from Paris: “I have had tremendous battles with the Yanks, & have beaten them down inch by inch to a reasonable figure….I think on the whole I have succeeded.”\nIndeed Churchill had succeeded very well. As Martin Gilbert notes: “All the former Allied powers accepted the principle that Britain’s debt payments to the United States should be accompanied by simultaneous, proportionate payments to Britain by France, Belgium, Italy and Japan, Britain’s principal debtors….the £1,000 million which Britain owed America was offset by over £2,000 million which the other former Allies owed to Britain.”\nOn his return from Paris, Churchill resumed his struggle to keep naval expenditures from overwhelming the budget and his plan for an across the-board income tax cut. It wasn’t a fair fight. Churchill knew too much about running a navy. On February 4th, Churchill sent a six-page list of quite specific cost-cutting suggestions to his friend the First Sea Lord, Admiral David Beatty. As Churchill explained in an accompanying note: “It is no good telling me the First Sea Lord cannot do this if he lets it be known that it is his wish. Even when First Lord, as you know, I often found this amount and larger amounts in a few mornings with a blue pencil.” Beatty complained to his wife, “I have to tackle Winston….It takes a good deal out of me when dealing with a man of his calibre with a very quick brain. A false step, remark, or even gesture is immediately fastened upon, so I have to keep my wits about me.”\nChurchill’s eventual agreement, in March, 1925, to the Bank of England’s return to the gold standard at pre-war parity was not easily achieved. At the outset, Churchill prepared a lengthy memorandum in which he challenged Montague Norman, the Governor of the Bank of England: “If we are to take the very important step of removing the embargo on gold export, it is essential that we should be prepared to answer any criticism which may be subsequently made upon our policy.” The British banker, Lord Bradbury, accused Churchill of having “his spiritual home in the Keynes McKenna sanctuary.” Some of Churchill’s letters seem to reflect that view: “The Treasury has never, it seems to me, faced the profound significance of what Mr. Keynes calls ‘The paradox of unemployment amidst dearth.’ The Governor shows himself perfectly happy in the spectacle of Britain possessing the finest credit in the world simultaneously with a million and a quarter unemployed….”\nChurchill eventually made his decision shortly after a dinner and late night “Symposium,” where he brought together Keynes and McKenna with the two principal advocates from the Treasury Department. Keynes’s accurate predictions of deflation and increased unemployment were eventually overcome by the Treasury arguments that inflation was a greater danger."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:afe17666-f51e-408b-90fc-453bfef467e7>","<urn:uuid:7f23607f-b2b9-42cb-8778-e810c5280ecf>"],"error":null}
{"question":"What initiatives did both RIDM and TIFF implement to support women in the film industry?","answer":"RIDM has long prioritized gender parity in their programming, achieving nearly 50% women directors, while TIFF launched the 'Share Her Journey' program, a five-year commitment to increase women's participation both in front of and behind the camera. Both festivals demonstrated their commitment through programming choices - RIDM featured women-directed opening and closing films, while TIFF honored six Canadian women filmmakers at the Birks Diamond tribute.","context":["As the Rencontres internationales du documentaire de Montréal turns 20, the festival's audience continues to evolve.\nDocumentary film is alive and kicking in Quebec, as evidenced by the 20th Rencontres internationales du documentaire de Montréal (RIDM), which presents 142 films from 47 countries, Thursday through Nov. 19.\n“I think 20 years for any festival is just amazing,” RIDM executive director Mara Gourd-Mercado said over coffee with festival programming director Bruno Dequen last week. “Twenty editions for a documentary festival in Montreal is a testament to all the teams that built RIDM into something that has lasted through the years and been growing all the time.”\n“What’s most remarkable is that the festival continues to renew its audience, unlike some other events,” Dequen added. “We could have had a niche audience from the start, which would have been the same, year after year. It’s fascinating to see more and more young people coming.”\nA driving force in the fest’s constant renewal is the growing interest in documentaries in general. Added to that is a thriving documentary scene in Quebec cinema, which feeds RIDM with a steady stream of films and prospective filmgoers.\n“The festival has a double mandate,” Dequen said. “On the one hand, it’s about showing the best or most interesting international films of the year; and it’s a great platform for local artists. We always have a huge selection of Quebec and Canadian films.”\n“The Montreal documentary community feels like RIDM belongs to them,” Gourd-Mercado said, “and it does. They feel comfortable enough to come to us with ideas, and we try to put them forward when we can. That exchange is really important, and has kept us going for 20 editions.”\nLast week, Telefilm Canada announced promising results of its initiative to reach gender parity by 2020 in its funding of films by women and men. Halfway through the first year of the mandate, Telefilm announced that out of approximately 60 features that have been approved for funding so far this year, 44 per cent are directed by women, 46 per cent written by women and 51 per cent produced by women.\nThat’s music to the ears of Gourd-Mercado, who notes that gender parity has long been a priority at RIDM. The fiction film world has lagged behind documentary in terms of films written and directed by women; likewise at primarily fiction festivals, from the Toronto International Film Festival to the Festival du nouveau cinéma, where the proportion of female directors generally lingers around 25 to 30 per cent.\n“It’s great that the institutions are catching up,” Gourd-Mercado said. “This year, we’re at 49 per cent women directors. Our opening and closing films are directed by women. It’s also a reflection of what’s happening in the doc world, where there are so many women producers and directors, it’s almost normal to have parity.”\nAs a programmer at the festival since 2011 and director of programming since last year, Dequen has sought to obtain a wide diversity of films by more than doubling the programming team to its current staff of six, each with their own area of specialization. They include Selin Murat, who specializes in Africa and the Middle East; Inuit filmmaker Isabella Weetaluktuk; Portugal-based Brazilian expat Gustavo Beck, who tracks the European festival circuit; former RIDM programming director Charlotte Selb; and former pre-selection committee member Apolline Caron-Ottavi.\n“There’s strength in numbers,” Dequen said. “The more people you bring in to put their personal touch and input, the more you build something interesting. We’re trying to find the perfect mix. The ideal RIDM film is extremely original in its form, while proposing some political point of view, so you can have discussions about esthetics and the subject matter.”\nThe goal, always, is to bring viewers on a journey. That’s where documentary film has an edge on fiction, allowing viewers to experience real people and places in new ways.\n“People go to RIDM to discover things,” Gourd-Mercado said, “to go, ‘Oh, wow, I never thought of that.’\n“There are no movie stars, so what you see is the content of the film. It’s important to go into the festival with une volonté de découvrir des choses.”\nAT A GLANCE\nThe 20th Rencontres internationales du documentaire de Montréal takes place Thursday, Nov. 9 through Sunday, Nov. 19. For tickets and program information, visit ridm.ca.\n10 films to catch at RIDM\nRIDM programming director Bruno Dequen and executive director Mara Gourd-Mercado offer their highlights from this year’s lineup.\nRoom for a Man, directed by Anthony Chidiac (Thursday, Nov. 16 at 6 p.m., UQÀM’s Pavillon Judith Jasmin Annexe). “It’s a first film by a homosexual director who comes from one of the richest, most conservative families in Lebanon. A very personal, autobiographical essay.”\nTaste of Cement, directed by Ziad Kalthoum (Friday, Nov. 10 at 8 p.m., Cinémathèque québécoise; Friday, Nov. 17 at 7 p.m., Concordia’s Alumni Auditorium in collaboration with Cinema Politica Concordia). “A film by a Syrian refugee, shot in Lebanon with Syrian migrant workers at a huge construction site,” Dequen said.\n“It’s very poetic — a real reflection on war and migration,” Gourd-Mercado added.\nDid You Wonder Who Fired the Gun?, directed by Travis Wilkerson (Saturday, Nov. 11 at 5:30 p.m., Quartier Latin; Tuesday, Nov. 14 at 2:30 p.m., Cinéma du Parc). “The director will be there for the first screening. This is a kind of essay-suspense-search film, very autobiographical — an experimental exploration of racism in America.”\nMaman Colonelle, directed by Dieudo Hamadi (Friday, Nov. 10 at 5:45 p.m., Cinémathèque québécoise; Sunday, Nov. 19 at 1 p.m., Cinéma du Parc). “An observational film from the Congo about a police officer who specializes in women’s and children’s rights, with rare access to the subject matter.”\nTan Pin Pin retrospective: Alternative Singapore Chronicles (Friday, Nov. 10 to Tuesday, Nov. 14, Cinémathèque québécoise). “She’s the most famous documentary filmmaker in Singapore. She uses a different esthetic each time. To Singapore, With Love uses talking head interviews with political activists in exile; while her last film, In Time to Come, is a symphony-essay on her city.”\nManic, directed by Kalina Bertin (Friday, Nov. 10 at 8:30 p.m., Cinéma du Parc; Wednesday, Nov. 15 at 8:30 p.m., Cinémathèque québécoise). “Montrealer Kalina Bertin’s film looks at the history of mental illness in her family, beginning with her father.”\nNowhere to Hide, directed by Zaradasht Ahmed (Saturday, Nov. 11 at 3 p.m., Cinéma du Parc; Monday, Nov. 13 at 7 p.m., Concordia’s Alumni Auditorium in collaboration with Cinema Politica Concordia). “Usually we see refugees at the refugee camp. This is about the whole process before: what makes you take the decision to say, ‘OK, now we have to leave (our country) and become refugees.’ This film really struck something in me.”\nTongue Cutters, directed by Solveig Melkeraaen (Friday, Nov. 10 at 3:45 p.m., Quartier Latin; Sunday, Nov. 19 at 6:30 p.m., Cinéma du Parc). “A film about kids in Norway, where their traditional summer job is to cut the tongues off of fish. It’s about something small, but makes you feel very close to the subject.”\nBirth of a Family, directed by Tasha Hubbard (Friday, Nov. 10 at 7 p.m., Concordia’s Alumni Auditorium in collaboration with Cinema Politica Concordia; Monday, Nov. 13 at 3 p.m., Cinéma du Parc). “A film about the reunion of an Indigenous family that got caught up in the ’60s Scoop, was separated and now they’re finding each other as adults. Super powerful.”\nThe Devil’s Freedom, directed by Everardo González (Friday, Nov. 10 at 6:30 p.m. and Saturday, Nov. 11 at 9:15 p.m., Cinéma du Parc). “A film about violence in Mexico. The director films the perpetrators and the victims with masks on. You hear their stories and all you see is their eyes. You understand that they’re all part of a system built on violence. Fantastic.”","UMA DA CUNHA | 13 SEPTEMBER, 2018\nTIFF2018: Women’s Issues, Gender Disparity Ring Out Loud And Clear\nTIFF dominated by films on the place of women in today’s world\nThis year, the Toronto International Film Festival’s massive programme of close to 350 titles is dominated by films on the place of women in today’s world. Each in the long list of films looks at social and domestic issues that are impacting women, and also at the gender inequality they face.\nTitles Linked Directly to Women\nGirls of the Sun (France, dir. Eva Husson) on a battalion of women fighting to take back their homes from ISIS extremists in Iraqi Kurdistan; Widows (UK, US, dir. Steve McQueen), on four women left in a deadly lurch when their criminally-connected husbands are all killed; The Chambermaid (Mexico, dir. Lila Aviles) on the daily routine and grand ambitions of a young maid working in a high-end Mexican hotel; Her Job (Greece, France, Serbia, dir. Nikos Labot), a compassionate portrayal of a devoted housewife whose brief taste of autonomy as a mall-cleaner where she is a popular, model employee, is threatened by impending lay-offs; Working Woman (Israel, dir. Michal Aviad) follows a mother of three who lands a job as an assistant to a powerful but sexually harrassive realtor, and fights back while her husband struggles to keep his restaurant business; Woman at War (Iceland, France, Ukraine, dir. Benedikt Erlingsson), on Halla, a committed undercover eco-terrorist trying to save Iceland’s natural landscapes from industrialist destruction, torn between the greater good and her own dreams involving a long-desired child available for adoption; and The Good Girls (Mexico, dir. Alejandra Marquez Abella) on Mexico’s 1982 debt crisis, in which a well-heeled socialite influences her husband on how to cope with their changing circumstances.\nLauding Women’s Achievements\nThere are films that extol the remarkable achievement of women, some in the preserve of (traditionally) men, such as … Maiden (UK, dir. Alex Holmes), a documentary on the unprecedented journey of 24-year-old Tracy Edwards and the first all-women sailing crew to enter the Whitbread Round the World Yacht Race; Naziha Arebi's Freedom Fields (Libya, UK, Netherlands, USA, Qatar, Lebanon, Canada), which observes post-revolution Libya through the eyes of an aspiring all-female soccer team struggling to gain mainstream acceptance, and thereby mirroring the challenges women face in contemporary Libyan society; and A Private War (UK, USA, dir. Matthew Heineman), on the all-time celebrated war-correspondent Marie Colvin who was driven to conflict frontlines worldwide to give voice to the voiceless.\nFilms on Gender Disparities\nGender disparities feature as a recurring subject. Leading the list is the Cannes Un Certain Regard winner (FIPRESCI Prize, Best Actor, Caméra d'Or, Queer Palm) Girl (Belgium, dir Lukos Dhont), on a young girl declared as male at birth who struggles to realise her dreams of being a ballerina, all the while desperate for her body to reflect her true identity; Wanuri Kahiu 's Rafiki (Kenya, South Africa, France, Lebanon, Norway, Netherlands, Germany, USA), a precarious love story between two young Kenyan women in a society where homosexuality is banned. Neil Jordan's Greta (USA, Ireland), a psychological thriller about a lonely, mysterious widow whose friendship with a naïve young woman takes on an increasingly obsessive and sinister air; Chanya Button's Vita & Virginia (UK, Ireland), on two uncompromising women: socialite/author Vita Sackville-West and literary icon Virginia Woolf, and the unconventional affair behind one of Woolf’s greatest novels. Also on this list is Mouthpiece (Canada, dir. Patricia Rozema) about an aspiring writer who attempts, following her mother’s death, to reconcile her feminism with her mother’s conformist choices.\nThe Race and Colour Divide\nColour discrimination is another constant preoccupation on TIFF’s screens. There is the film from the US, Monsters and Men, directed by Reinaldo Markus Green, in which when a black man is shot dead by the police, three members of his community are vicitmised into not revealing what they know of the murder or the systematic corruption behind it. Amma Asante's Where Hands Touch (UK), a disquieting, coming-of-age romance about a black German teenager who falls in love with a member of the Hitler Youth. This complex story is about a love so fierce it transcends the most terrible divides conceivable. Avi Nesher's The Other Story (Israel), on two rebellious young women, one fleeing the chaos of secular hedonism for the disciplined comforts of faith, the other desperate to transcend her oppressive religious upbringing for sexual and spiritual freedom, who cross paths unexpectedly in Jerusalem.\nArchiving Women in Film\nTo top it all, TIFF presented Mark Cousin's documentary Women Make Film: A New Road Movie Through Cinema, (UK), a 240-minute paean on the monumental but largely unrecognized efforts of female filmmakers across the years. The film affirms that since the birth of cinema, women directors have made stunning contributions across all genres. Yet most cinema histories tell a male-dominated story. This endeavour is envisaged as an ongoing documentation.\n‘Share Her Journey’\nIntroduced last year, the ‘Share Her Journey’ programme is a five-year commitment that aims to consciously increase the participation of women in film, both in front of and behind the camera.\nNandita Das, at TIFF with her second directorial venture Manto, was on stage at the Share Her Journey rally. Talking of the deep-seated prejudices which impact women’s lives, she said, “We all have multiple identities but for better or for worse, I realise my identity as a woman is a powerful one.”\nToronto-based filmmaker Deepa Mehta, contacted presently in New Delhi, added, “We all need champions when we try to get our work seen and our voices heard. And let’s be honest, it is a battle.” While filmmakers such as Kathryn Bigelow (US) are uncomfortable at being labelled ‘female directors’, Mehta demurred, “I don’t think it creates a divide. If anything, in my experience, the male (enlightened) directors, really welcome it. As for the rest of them, loosen up guys, the more variety we have in our emotional/creative voices, the better the films will be.”\nSix Women Filmmakers Honoured\nTIFF honoured Six Canadian women filmmakers at the sixth annual Birks Diamond tribute to the year’s Women in Film. They were Toronto born writer-director Stella Meghie, screenwriter Susan Coyne, from Quebec, actress Pascal Bussiees and director Jeanne Leblanc, and screen veteran Tanoo Cardinal. The winners, who it appears were earlier given diamonds as their awards, now receive a cash prize.\nIndian Women Filmmakers to the Fore at TIFF\nIn the festival’s 43rd year, of the films selected to be screened, 34 percent (121 films) are directed or co-directed by women. Among these, five by Indian women filmmakers: National Award-winning director Rima Das’ third venture Bulbul Can Sing; Nandita Das, returning to the festival with the formidable Manto; The British-Indian Jayisha Patel’s Circle, a 14-minute documentary that brings to the fore stories of abuse in an Indian, Uttar Pradesh village; and Sandhya Suri with The Fields, a dramatised narrative that delivers a telling tale of a poor agricultural labourer in an Indian village.\nThe selection of Indian films at this year’s TIFF shows Indian women in the forefront of their craft, as well as the content they choose to highlight.\nTranslate this page:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:959b9821-9883-4dc7-a671-ce2ebcb126be>","<urn:uuid:483f2c28-4393-4eb8-abb5-735aef76c332>"],"error":null}
{"question":"I work with ETL testing and want to understand: what's the key difference between ETL testing and test data management (TDM) in terms of their main focus and purpose?","answer":"ETL testing and TDM have different core focuses. ETL testing primarily verifies if data is correctly moved and transformed from source to target systems, checking aspects like data transformation accuracy, mapping validation, and enterprise business intelligence reporting. Meanwhile, TDM focuses on creating and managing non-production test data that mimics production data, with key capabilities like dataset provisioning, data masking, and maintaining virtual database copies that can be easily accessed and modified. While ETL testing is about validating the data movement process itself, TDM is about providing the right test data infrastructure and governance to support various testing needs, including but not limited to ETL testing.","context":["Here are top 4 ETL Testing Tools:\nMost of the software companies today depend on data flow such as large amount of information made available for access and one can get everything which is needed.\nThis is where the concept of ETL and ETL Testing comes into the picture. Basically ETL is abbreviation used for Extraction, Transformation and Loading. Presently ETL Testing is performed using SQL scripting or using spreadsheets which may be time-consuming and error-prone approach.\nIn this article, we will have detailed discussions for several concepts viz. ETL, ETL Process, ETL testing and different approaches used for it along with ETL testing tools.\nAlso read => ETL Testing Tips\nETL Testing Concepts:\n#1) As mentioned previously ETL stands for Extraction, Transformation and Loading which are three database functions where;\n- Extraction: Reading data from database\n- Transformation: Converting extracted data in the required form to store into another database\n- Loading: Writing the data into target database\n#2) ETL is used to transfer or migrate the data from one database to another, to prepare data marts or data warehouses\nFollowing diagram elaborates the ETL Process in precise way\nETL Testing Process:\nETL Testing Process is similar to other testing processes that includes following stages;\n- Identifying business requirements\n- Test Planning\n- Designing test cases and test data\n- Test execution and bug reporting\n- Summarizing reports\n- Test closure\nTypes of ETL Testing\nETL Testing can be categorized into following categories according testing process been followed;\n1) Production Validation Testing:\nIt is also called Table balancing or product reconciliation. It is performed on data before or as it is being moved into production system in correct order\n2) Source To Target Testing:\nThis type of ETL Testing is performed to validate data values after data transformation\n3) Application Upgrade:\nIt is used to check whether the data is extracted from older application or new application or repository\n4) Data Transformation Testing:\nMultiple SQL queries are required to be run for each and every row to verify data transformation standards\n5) Data Completeness Testing:\nPerformed to verify that the expected data should be loaded at destination as per predefined standards\nI would also like to compare ETL Testing with Database Testing but before that let us have a look towards types of ETL Testing with respect to database testing;\n1) Constraint Testing:\nTesters should test whether data is mapped accurately from source to destination, while checking for it testers need to focus on key checks (constraints) such as;\n- NOT NULL\n- Primary Key\n- Foreign Key\n2) Duplicate Check Testing\nSource and target tables contains huge amount of data with frequently repeated values, in such case testers follow some database queries to find such duplication.\n3) Navigation Testing:\nNavigation concerns with GUI of the application. User finds application friendly when he gets easy and relevant navigation throughout entire system. Tester must focus on avoiding irrelevant navigation through user point of view.\n4) Initialization Testing:\nInitialization Testing is performed to check combination of hardware and software requirements along with platform it is installed\n5) Attribute Check Testing:\nThis testing is used to perform for verifying all attributes of source and target system that should be same\nFrom above listing one may consider that ETL Testing is quite similar to Database Testing but the fact is ETL Testing is concerned with Data Warehouse Testing and not Database Testing.\nThere are several other facts due to which ETL Testing differs from Database Testing, let’s have quick look towards it one by one.\n1) The primary goal of Database Testing is to check if the data follows the rules and standards of data model where on the other hand ETL Testing checks if data is moved or mapped as expected\n2) Database Testing focuses on maintaining primary key-foreign key relationship while ETL Testing verifies for data transformation as per requirement or expectation and same at source and target\n3) Database Testing recognizes missing data where as ETL Testing determines duplicate data\n4) Database Testing is used for data integration and ETL Testing for enterprise business intelligence reporting\nThese are some major differences which makes ETL Testing different from Database Testing.\nETL bugs are also of several types such as;\n|Type of bug||Description|\n|Calculation Bugs||Final output wrong due to mathematical error|\n|Input/output Bugs||Accepts invalid values and rejects valid values|\n|H/W bugs||Device is not responding due to hardware issues|\n|User Interface bugs||Related to GUI of an application|\n|Load condition bugs||Denies multiple users|\nHow to create test cases in ETL Testing:\nThe primary goal of ETL testing is to assure whether the extracted and transformed data is loaded accurately from source to the destination. ETL testing includes two documents;\n#1) ETL Mapping Sheets:\nThis document contains information of source and destination tables and their references. Mapping sheet provides help to create big SQL queries while performing ETL Testing.\n#2) Database schema for Source and Destination table:\nIt should be kept updated in mapping sheet with database schema to perform data validation.\nBest ETL Testing Tools List:\nLike automation testing ETL Testing can be also automated. Automated ETL Testing reduces time consumption during the testing process and helps to maintain accuracy.\nGiven below are some ETL Testing Automation Tools that are used to perform ETL Testing more effectively and rapidly.\n#1) Informatica Data Validation\n- Informatica Data Validation provides complete solution for data validation along with data integrity\n- Reduces programming efforts and business risks due to intuitive user interface and built-in operators\n- Identifies and prevents data quality issues and provides greater business productivity\n- Allows 64% free trial and 36% paid service that reduces time and cost required for data validation\nOfficial Link: Informatica Data Validation\n#2) QuerySurge from RTTS\n- QuerySurge is an automated testing tool specifically used for data warehouse testing\n- Verifies, converts and upgrades data through the ETL process\n- Reduces testing time and schedules tests for specific time\n- Builds test scenario and test suits along with configurable reports\n- Commercial tool connects source and target data and also supports real time progress of test scenarios\nOfficial Link: QuerySurge from RTTS\n- iCEDQ is designed for ETL Testing, Data Migration Testing and Data Quality Verification\n- Identifies data integration errors without any custom code\n- Supports rule engine for ETL process, collaborative efforts and organized QA process\n- Commercial tool with 30 days trial provides custom reports with alerts and notifications\nOfficial Link: iCEDQ\n#4) Datagaps ETL Validator\n- ETL Validator is data testing tool specifically for automated data warehouse testing\n- ETL Validator is used to check Data Validity, Data Accuracy and also used to perform Metadata Testing\n- Checks Referential Integrity, Data Integrity, Data Completeness and Data Transformation\n- Commercial tool with 30 days requires zero custom programming and improves business productivity\nOfficial Link: Datagaps ETL Validator\nWhile performing ETL testing several things to be kept in mind by testers, some of them are listed below;\n- Apply suitable business transformation logic\n- Execute backend data-driven tests\n- Create and execute absolute test cases, test plans and test harness\n- Assure accuracy of data transformation, scalability and performance\n- Make sure ETL application reports invalid values\n- Unit tests should be created as targeted standards\nETL Testing is not only tester’s duty but it also involves developers, business analyst, database administrators (DBA) and even users. ETL Testing process became vital because it is required to make strategic decisions at regular time intervals.\nETL Testing is being considered as Enterprise Testing though it requires good knowledge of SDLC, SQL queries, ETL procedures etc.\nIf you want to add any other ETL testing tool in the list let us know.","Comprehensive software reviews to make better IT decisions\nProvision and Mask Your Test Data With the Right Tool\nIt goes without saying that quality assurance (QA) is a critical component in any software development organization. QA establishes the quality standards and tactics for the validation and verification of software products, whether you are running an Agile or waterfall methodology. See Info-Tech’s Build a Strong Foundation for Quality blueprint for more information on QA practices. The insights generated from QA activities are critical to make informed, data-driven decisions for product deployment, enhancements, and product road mapping.\nAutomation, containers, behavior-driven development (BDD), and experienced-based testing are just a few tactics professed by the industry to overcome many of common challenges with QA. While these approaches are helpful, they do not directly address the foundations of effective QA:\n- Clear, justified, and prioritized test strategy, plan, and cases.\n- Production-like, high-fidelity environments to host the testing of your application system.\n- Production-like, high-fidelity data to be used in the testing of your application system.\nMany of today’s testing tactics primarily focus on test design (e.g. BDD), test execution (e.g. automation), and test environment management (e.g. cloud and containers), assuming the right test data is available. However, this is not always the case.\nWhat are the test data management (TDM) challenges?\nThe accuracy and relevance of your test data can make or break the success of your deployed product. Sufficient time and effort should be dedicated to ensuring the data you are preparing supports the test cases you want to execute. Unfortunately, we still see test data management (TDM) as a significant challenge that undermines the speed, rigor, and stability of test automation.\n- 56% of organizations indicated that the lack of appropriate test environment and data as a challenge in applying testing to Agile developments (World Quality Report, 2019-20).\n- 48% of organizations indicated test data and environment availability and stability as a main challenge in the achieving desired level of test automation (World Quality Report, 2019-20).\n- 52% of respondents said their testing teams are dependent on database administrators to get the data they need as a top test data issue while 49% stated it was due to their data spread across multiple databases, and 47% mentioned that it was due to having to maintain the right test data set versions with different test versions (Continuous Testing Report, 2020).\nMany test data challenges can be rooted in the unavailability of on-demand test data due to costs, process handoffs (e.g. laborious sign-offs and wait times), functional siloes (e.g. separate test, data, and operations teams), and application system complexities (e.g. lack of holistic data management governance and distributed systems). These challenges are further exacerbated by the fact that many organizations are generating new test data manually despite the maturity of today’s toolsets. In fact, 69% of organizations employ spreadsheets to manually generate new test data and 59% create data manually with every test run (Continuous Testing Report, 2020).\nTDM solutions can help overcome some of these challenges by automating the manual and error-prone tasks to generate and refresh your test data and by centrally managing your datasets for on-demand access.\nWhat are TDM solutions and how can they help?\nTDM solutions create and manage non-production data that reliably mimics or resembles your production data, so that automation tools and testers can rigorously and accurately verify and validate your application systems. Today’s TDM solutions share several common elements:\n- Dataset Provisioning – Provision test datasets from multiple, heterogeneous data sources through non-disruptive synchronization and cloning methods. Maintain a useable representation of production data based on test requirements.\n- Virtual Database – Create lightweight virtual database copies that are version controlled, easily accessible, secured, compliant with regulations (e.g. GDPR), and can be rolled back and edited.\n- Masking – Replace personal and sensitive data with fictitious yet realistic values using predefined or custom profiling expressions with custom or out-of-the-box masking algorithms.\n- Governance – Secure the visibility, modifiability, and consumption of test data with role-based access control. Indicate when the virtual data should be refreshed with the latest production data, branched virtual data copies, rolled back, shared with other teams and tools, and automated to support continuous integration (CI) and continuous delivery (CD) pipelines.\nWhat are the implementation considerations?\nMuch like any other tool, you will need to evaluate how your TDM solution fits into your overall testing and product delivery environment and strategy:\n- Test Management (TM) Tool Integration – TM tools actively manage, coordinate, and monitor your testing activities and orchestrate the various tasks needed for test automation, provided they have good test data. A key benefit of TM tools is that they provide traceability of test cases, scripts, plans, and defects and trends found within testing back to the original requirements and requests that motivated and justified the testing initiative. See our Choose the Right QA Tools to Validate and Verify Product Value and Stability note for more information.\n- CI/CD Pipelines – Test data management is an integral part of your product delivery pipeline, which can include a number of automation tools that require high fidelity and quality test data, such as test automation, build automation, continuous integration and deployment automation. Therefore, test data management tools must be specifically configured to accommodate on-demand access of up-to-date test data in a consumable format. This requires a close look at the orchestration and requirements of your automation tools and how they trigger the automated pull of test data.\n- Distributed Data Sources – Production data can reside in multiple sources within your distributed application environment. Ensure your target data sources are discoverable and accessible so that data can be consolidated. Evaluate data model changes from each data source and accommodate them when your test data is refreshed with good metadata analysis and data management practices.\n- Data Security and Compliance – Data needs to be protected from unauthorized access. Test data is no different. Most TDM vendors abide to the General Data Protection Regulation (GDPR) and personal identity information protection standards, such as Personal Information Protection and Electronic Documents Act (PIPEDA). However, not every vendor goes about data security and privacy the same way (e.g. dynamic masking through an API, GUI to mask data in CSV files) nor abides to all industry-specific compliance requirements, such as Health Insurance Portability and Accountability Act (HIPAA), Gramm-Leach-Bliley Act (GLBA), and the Safe Harbor requirements and Payment Card Industry Data Security Standard (PCI DSS).\n- Data Governance and Collaboration – Test data is a cross-functional asset. It involves the collaboration among operations, security, developers, testers, and database analysts (DBAs) to provision, subset, mask, and manage test data in such a way that it meets your data quality and management standards, irrespective of where the data comes from and who uses it. The challenge is implementing just-enough oversight and discipline so that teams are not significantly impeded and disempowered to pull and manipulate data as they see fit. Even though TDM may not require the same degree of rigor and control as formal data management practices, some of its key principles can be leveraged to ensure proper data quality, ownership, and approvals are followed. For example, test data owners must keep an active tab on the value and relevance of their data (i.e. test data lifecycle) to determine when test datasets should be tweaked, refreshed, or retired. See our Create a Plan for Establishing a Business-Aligned Data Management Practice blueprint for more details.\nYour TDM solution should not be done in isolation as local improvements can drive down the efficiency and performance of your entire delivery pipeline.\nWho are the players in the space?\nWhile it may be ideal to have a single vendor platform to manage all QA and testing activities, many TM vendors have some test data management features (e.g. Microsoft’s Azure DevOps, Parasoft’s Automated Software Testing Tool Suite, and Sauce Labs’ Continuous Testing Cloud Platform), but they are limited to access management and basic data editing and virtualization. In some cases, these basic capabilities are enough to meet testing requirements. However, some vendors offer complementary products within their portfolio to fulfill the TDM gap (such as Micro Focus’ Data Express and Broadcom’s Test Data Manager) or offer out-of-the-box integration with solutions specializing in TDM (such as Informatica’s Test Data Management).\nNotable TDM vendors include:\n- IBM’s InfoSphere Optim\n- Informatica’s Test Data Management\n- Broadcom’s Test Data Manager\n- Micro Focus’ Data Express\n- Delphix’s DataOps Platform\n- Solix’s Test Data Management\n- IRI’s RowGen\n- Original Software’s Test Data Management\n- DATPROF’s Test Data Management Platform\nWhat are the key features of TDM solutions?\nEach TDM vendor has its own approach to the gathering, masking, and preparing of test data and out-of-the-box compatibility with specific industry standards and regulations. However, they share a common set of table stakes features:\n- Data Discovery, Profiling and Analysis – Find, view, analyze, and observe data mined from specified environments and data sources based on defined filters, data profiles, and data requirements.\n- Data Provisioning – Access, clone, and virtualize production data from targeted single or multiple data sources and store them in a centralized test database that is accessible to teams and tools. Data models and metadata are also centrally stored here to avoid repeated effort.\n- Self-Service Portal – Allow on-demand access to the database. Teams can store, augment, branch, version, and roll back test datasets as they see fit.\n- Data Masking – Provide a comprehensive set of masking and transformation techniques so that test data is consumable and satisfies industry regulations and standards, such as the General Data Protection Regulation (GDPR). This feature includes obfuscating sensitive data (e.g. encryption) or replacing sensitive data with altered, fictious data.\n- Audit Logging – Complete a record of all changes made to entities within the TDM solution. Easily view and filter logs, specify logging levels, and set retention policies.\n- Synthetic Data Generation – Create test data subsets that contain all the characteristics of production data but with none of the sensitive data. The tool offers out-of-the-box test data generation rules that can be repeatedly executed on demand.\n- Test Data Reporting – Aggregated visual reporting of the state, use, and changes of the test data through real-time dashboards with customizable templates.\n- Test Management Integration – Enable real-time integration with third-party TM, environment management, build and deployment management, and configure management solutions through out-of-the-box plugins or customizable REST APIs.\n- Governance – Define, implement, and uphold data classifications, privacy rules, and role-based access rights to govern the TDM process and enforce organizational policies and standards.\n- Cloud and Virtual Data Storage – Host the test database in a cloud or virtual environment that abides to data protection and privacy regulations.\nTDM vendors try to position themselves as key differentiators through unique features in addition to positioning their table stakes features as best in class.\n- Automated Data Comparison – Compare and contrast dataset tables from multiple sources so they can be consolidated and assembled into a usable test dataset. Teams can analyze data used to test an application by comparing the results before and after the application is executed.\n- Capability to Browse and Edit Datasets – Truncate, delete, and insert data into existing test datasets to select and prepare the appropriate subset with the appropriate data volume and quality required for testing.\n- Big Data Support – Support the management and processing of test data generated from big data utilities and tools (e.g. Hadoop).\n- Test Data Designer and Modeler – Define and customize data sources, data models, data management services, and data masking policies and rules for your desired test datasets.\n- Out-of-the-Box Support for Commercial-Off-the-Shelf (COTS) Applications – Access production data stored within COTS applications and within systems managed and hosted by third-party vendors.\nTest data management (TDM) solutions streamline test management workflows by removing the often manual, time-consuming, and laborious tasks of test data provisioning. While it may seem that TDM solutions are nice-to-have compared to other development, testing, and deployment priorities, the impacts of an automated TDM can be significant, especially when the system under test is distributed, large, and diverse. So, consider the following factors when deciding if a TDM solution is valuable in your organization:\n- The degree of fidelity and preparation your tests require for the test data to be compliant and consumable.\n- The complexity of your system under test.\n- The fragmentation, distribution, accessibility, and quality of your production data.\n- The degree of holistic definition and enforcement consistency of data structures, quality, and governance policies.\n- The mandatory regulations regarding the use of production data.\n- The tool’s out-of-the-box integration with other product delivery and test management tools.\nRemember, TDM does not discount the importance of good data management and test management principles and the need to continuously improve your QA practices\nWant to Know More?\n- Build a Strong Foundation for Quality – Instill a quality-first mindset in your product delivery process with a common vision of quality and good QA practices.\n- Choose the Right QA Tools to Validate and Verify Product Value and Stability – Understand the QA tool landscape and their features and implementation considerations.\nCOVID-19 has forced software companies and their suppliers to refocus efforts around prioritizing systems and workflows that are nearly 100% digital in nature. As a result, Info-Tech has observed the quick emergence of six market themes that are highly relevant post COVID-19. This note series will profile key vendors and how they fit into the post-COVID-19 world.\nCOVID-19 has forced software companies and their suppliers to refocus efforts around prioritizing systems and workflows that are nearly 100% digital in nature. As a result, Info-Tech has observed the quick emergence of six market themes that are highly relevant after COVID-19. This note series will profile key vendors and how they fit into the post-COVID-19 world.\nIBM is changing the terms of its ubiquitous Passport Advantage agreement to remove entitled discounts on over 5,000 on-premises software products, resulting in an immediate price increase for IBM Software & Support (S&S) across its vast customer landscape.\nIs it true that everything that can go wrong will go wrong? Don’t bet on it to not.\nWhile Microsoft is not a prominent player in the RPA space now with its Power Automate solution, compared to Blue Prism, UiPath, and Automate Anywhere, its latest acquisition of Softomotive, maker of WinAutomation, demonstrates Microsoft’s dedication to mature and expand its RPA offerings.\nWhen trying to implement Agile as a defined process, Scrum turned BAs or other roles into order takers with the title “product owner.” This undermines the entire value proposition of product management.\nAgile systems delivery (implemented through Scrum) is quickly becoming an accepted norm in IT. But using Scrum successfully in an organization requires a deep understanding of how it works and why. For example, many of our members don’t understand the importance of selecting a Product Owner who has three ears.\nReeling from the pandemic response executed by governments all the over world, companies are accelerating their implementation of low-cost automation. That bodes well for UiPath – a leader in RPA aiming to go public this year.\nThor, the Norse God of Thunder, tells Jane Foster, the woman he’s trying to impress, that on his home world of Asgard, the realm eternal, science and magic are two sides of the same coin. Had Jane been a part of the operations teams at Google (or other mature online service providers), she would have immediately realized we have a similar technology right here on good old Earth. We call the science site reliability engineering (SRE), and service level objectives (SLO) is the magic behind it. SRE is a powerful concept for organizations that are serious about keeping their customers happy. It is therefore important for them to develop well-thought-out SLOs and make certain that management is intellectually equipped to derive valuable business perspectives from them."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:f5951c8d-0a47-4ea3-b8f6-6c8779507550>","<urn:uuid:e86dcf8a-54a0-4bdf-9521-9a147fe4b3ca>"],"error":null}
{"question":"Hola! I'm studying biochemistry and would like to understand how does electrophoresis work para separar proteínas y ácidos nucleicos?","answer":"Electrophoresis uses electrical force to separate and characterize proteins and nucleic acids. Different sized biomolecules migrate through the sample at different rates. The process uses gels (polyacrylamide or agarose) that act as molecular sieves to slow down biomolecule motion and achieve greater separation. For nucleic acids, separation is based on size - as mass increases, both frictional forces and number of charges increase. For proteins, separation can be done by size using SDS electrophoresis, or by charge using isoelectric focusing which relies on a pH gradient in the gel where protein motion stops when the protein becomes neutral at its isoelectric point.","context":["Presentation on theme: \"Electrophoretic Mobility and Electrophoresis (24.10) Electrical force is another way we can cause macromolecules to move – Macromolecules tend to have.\"— Presentation transcript:\nElectrophoretic Mobility and Electrophoresis (24.10) Electrical force is another way we can cause macromolecules to move – Macromolecules tend to have charges associated with them when in solution (e.g., proteins) – Electrical force is proportional to the number of charges on the macromolecule, which is related to the size of the macromolecule A steady-state motion of the molecule is achieved when the electrical and frictional forces balance each other out – Electrophoretic mobility (μ) is similar to sedimentation coefficient, but is not as easily obtained Electrophoresis is the use of electrical force to separate and characterize proteins and nucleic acids Electrophoresis – Different sized biomolecules migrate through the sample at different rates – Mass determinations are accomplished by comparing to a set of standards\nMethods in Electrophoresis (24.10) Gels are used to “slow down” biomolecule motion in order to achieve greater separation – Polyacrylamide or agarose gels increase frictional forces, so they lower μ – Gels act as molecular sieves, so they separate molecules by size Nucleic acids are often separated based on size – As mass increases (more bp added), frictional forces increase but so does the number of charges (phosphates) – As size increases, molecular sieving of gels help to separate nucleic acids – Pulsed field electrophoresis can be used for very large nucleic acid structures, where structures get tangled in the gel ( kbp) Proteins can be separated by size and charge – SDS electrophoresis operates in a similar fashion to gel electrophoresis of nucleic acids – Isoelectric focusing relies on a pH gradient in the gel to separate proteins Isoelectric focusing – Protein motion stops when protein becomes neutral (isoelectric point or pI) – SDS electrophoresis and isoelectric focusing can be coupled together (2-D)\nElementary Chemical Kinetics ( ) Kinetics is the study of how reactions occur – Speed of reaction depends on frequency of productive collisions between molecules (concentration, temperature, nature of productive collision) – Many reactions involve more than one step, so a mechanism is used to explain how the reaction occurs Reaction rates are measured as the speed with which a reactant is consumed or a product is created – Reaction rate is a differential equation since we are looking at a change in concentration in a given amount of time One typically monitors either decay of one reactant or the production of a single productmonitors – Accomplished through absorbance, fluorescence, pH, etc.\nRate Laws and Reaction Mechanisms ( ) We know from experience that reaction rates often depend on concentration of reactants – Rate can be expressed as a product of reactant concentrations of certain orders – Order for each reactant is not necessarily the stoichiometric coefficient (α ≠ a) – Rate constant (k) must contain information about temperature and productive collisions Overall order of the reaction is the sum of the orders for each reactant and must be determined experimentally – The rate can be determined by measuring the change in concentration of a reactant/product over a short range of time (tangent to curve is rate)rate – The order for each reactant can be obtained by changing the concentrations of a single species and monitoring the change in rate (isolation method, method of initial rates) Reaction mechanism is a set of elementary reactions that can be used to explain a rate law – Order of reactants in an elementary rate law is the stoichiometric coefficient – Mechanism is only viable if the sum of elementary rate laws match overall rate law\nElectrophoresis and Molecular Mobilities\nProtein Electrophoretic Mobility and pH\nConcentrations of Product and Reactant During Reaction"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:fdbc0317-da02-44ec-a2f2-e7ca8badf235>"],"error":null}
{"question":"What is the difference in aging and marination requirements between game meat and lamb?","answer":"Game meat is not aged like domestic animals and traditionally requires marination in red wine, oil, herbs, and spices to aid tenderness and mask the 'wild game taste' - though this is becoming less necessary with farmed game animals. In contrast, lamb does not inherently require marination, though it can be marinated for flavor enhancement. For example, lamb chops can be marinated in herbs, garlic, olive oil, salt and pepper for as little as 30 minutes or up to 24 hours simply to enhance flavor and help retain moisture during cooking.","context":["Presentation on theme: \"Veal is the meat of young, usually male calves that are byproducts of the dairy industry. Dairy cows must calve before they produce milk and calves (male)\"— Presentation transcript:\nVeal is the meat of young, usually male calves that are byproducts of the dairy industry. Dairy cows must calve before they produce milk and calves (male) that are not used are used are slaughtered to make veal. Generally these calves are 8-16 weeks old and should not have eaten grass yet (milkfed). Veal is slaughtered and split into two halves across the natural curvature – foresaddle and hindsaddle containing both bilateral portions of the animal. These days although veal at times is split in halves like beef as well.\nShoulder The primal shoulder is similar to the beef primal chuck but contains only 4 ribs (5 in beef). Mainly used for ground meat or cubed for stewing or braising although steaks and chops can be fabricated but are less tender than those of the loin and rib areas. Deboned the shoulder can be roasted and/or stuffed and roasted\nForshank & Breast Located beneath the shoulder and considered as one primal although mostly used separately. The bones of the young calve in the breast area are still soft (cartilaginous) and the breast contains lots of fat and connective tissue. Moist cooking methods like braising breaks down during the cooking process and make the result very flavorful. Deboned the breast can be rolled and stuffed, cut in cubes for stews (blanquette, fricasse) or ground.\nRib The double rib also known as the veal hotel rack is very tender and expensive. The double rib consist of two racks connected by the backbone and is often sold split and can be deboned to produce two veal rib eye. On the inside of the rib is a portion of the veal tenderloin, however mostly the tenderloin is removed whole before separating the foresaddle form the hindsaddle. The rib is cut into chops and the deboned rib eye into steaks which are excellent for grilling, sautéing or braising.\nLoin The veal loin is the continuation of the rib and contains ribs 12 & 13. The loin consist of the loin eye muscle, the rib bones and the tenderloin imbedded on the inside of the two sides. The veal loin is very tender and the tenderloin is the most tender part of the veal. The tenderloin is mostly removed ahead of separating the leg and cut into medallions for dry cooking methods. The rib eye muscle can be cooked on or off the bone as roast or in chops (on or off the bones).\nLeg The primal veal leg consist of both the sirloin and the leg (incl. shank) Although the meat is fairly tender, the leg is mostly fabricated into cutlets and scallops. To fabricate these cuts the leg needs to be broken down into its parts, trimmed of the connecting tissue and cut against the grain of main and further tenderized/flattened with a mallet: Top round Eye round Knuckle Sirloin Bottom round (includes the sirloin) Butt tenderloin (if not removed prior) The hindshank is meatier than the foreshank and roasted or braised whole or sliced across for Osso bucco.\nSeveral organs called offal or innards of beef and veal are used in the food service industry. Beef Heart / tongue / tripe (stomach lining) / liver / kidney / oxtail / Veal Heart / tongue / liver / kidney / oxtail Sweetbreads (the thymus gland only used in young veal and lamb). These glands shrink when the animals get older.\nLamb- Lamb is the meat of sheep generally below 1 year of age Mutton – young female or castrated male sheep older than one year with more than two incisor teeth Hogget – young male or maiden female sheep with no more than two incisor teeth Baby Lamb – milk-fed lamb 6-8 weeks old, non grass/grain eater Spring lamb- milk fed lamb 3-5 months old, non grass/grain eater\nNew Zealand & Australia United States Argentina Europe United Kingdom - Wales and Scotland are particularly famous for their lamb Ireland\nPyrenees lamb 45 days old; 11 to 15 kg weight Exclusively milk fed From the southern French Pyrenees mountains Perigord lamb 90 to 150 days old; 60 days milk fed; 15 to 19 kg From central France region Paulliac lamb: Protected by European law, born and raised in the Gironde area of France Raised by mother milk only, resulting in pink rich tender meat. Kidneys are especially delicate. 75 days old and11 to 15 kg\nPre-Sale lamb From lambs that grazed on the salty herbal borderlands of north-western France, that periodically drenched by seawater. The high consumption of salt results in a more tender meat with juicier muscle tissue.\nLamb has a strong distinctive flavor and needs to be complemented with bold flavored sauces and accompaniments Meat from mutton and hogget is more distinct in flavor and tougher meat due to age and to the maturity of the connecting tissues.\nThe slaughtered lamb is butchered into the primal cuts: Shoulder Breast Rack Loin Leg As the carcass is not split after the slaughtering, some primal cuts of lamb contain both halves (legs for example). Lamb primal cuts are not classified into fore and hind quarters or fore and hind saddle like beef or veal.\nShoulder The primal shoulder contains many small bones and tough muscles whose grains travel in different direction. This fact make it difficult to cook and carve a whole shoulder although the shoulder may be cut into chops or deboned and (slow) roasted or braised. Mostly though lamb shoulder meat is cut for stews or ground for patties\nBreast The primal cut lamb breast includes the breast and the foreshank. The primal breast is located beneath the primal rack and contains the rib tips. Which are cut off to produce the rack. The lamb breast is not often used in the food service industry it can be stuffed and braised. The left over rib tips can be separated and are then called Denver ribs. The lamb foreshanks are meaty and can be braised used for broths or ground.\nRack The primal lamb rack is also known as hotel rack (saddle) and located between the primals shoulder and loin containing eight ribs and a portion of the back bone. The rack is valued for the tender rib eye muscle. The hotel rack is usually split and trimmed so that each set of ribs can easily be cut into chops. The rack can be grilled/broiled or roasted as rack or cut into lamb or single or double chops before cooking. Frenched lamb rack refers to a split rack, trimmed to the bones and cleaned bones ready to use.\nLoin The loin is between the rack and the leg and contains rib number 13, portions of the backbone and the loin eye muscle, tenderloin and flank. Except for the flank the meat is very tender and excellent for dry heat cooking – roasting and grilling/broiling. The loin can be deboned for to produce roasts or boneless chops or cut with bone in into chops. The loin eye can be deboned and cut into medallions or noisettes.\nLeg The lamb leg is separated from the loin by a straight cut leaving what would be the sirloin portion (in beef) as part of the leg. The primal leg is seldom used as is, as it mostly is split into two legs partially deboned or fully deboned. The lamb leg meat is quite tender especially the sirloin end of the leg and suitable for a variety of cooking methods while the shank is less tender and needs to be braised.\nLeg The bone in leg is often roasted for carvery on buffets but can also be made into bone in lamb leg steaks. They can also be deboned and netted for roasting. Special cuts: Fore saddle – the front portion of the carcass separated from the hind saddle on the 12 th or 13 th rib. It includes the primal shoulder, breast, fore shank and rack. Hind saddle – the back portion of the carcass as described above. It includes the loin, leg and kidneys Back – the trimmed rack and loin in one piece Bracelet – the primal hotel rack with connecting breast section\nInnards The lamb brain is a delicacy and can be roasted or sautéed. Lamb kidneys are used in pies as well as they can be sautéed. Lamb liver is seldom used although it can be eaten and is very tender.\nGame are animals hunted for sports or food. Originally game was dependent on seasons and the hunters success but the increased demand of the food service industry has led to farms raising game mainly for food production purposes. Pheasant, quail, and deer are now all available farmed throughout the year. Game meat generally is dark in color and has a strong but pleasant with robust flavor and less fat than meat and poultry. Cooking methods for game meat depend on the age of the animal and the cut used, but in general one can follow the guidelines for meat and poultry.\nFurred or Ground Game Deer, wild boar, bison, moose, elk, hare etc Feathered or Winged Game Quail, pheasant, partridge, wild duck, snipe. grouse Exotic Meats and Reptiles Snakes, kangaroo, crocodile or alligator etc\nFurred or Ground Game These include large animals such as deer, moose, wild boar kangaroo, and elk as well as smaller animals like hare (wild rabbit). There are many more animals that generally speaking are hunted and could be eaten, but are not readily available in the food service industry – Zebra, bear and other big game. Butchering game meat generally follows the same principals as with beef or lamb – primal and sub- primal, but mostly only pre-butchered and precut parts are available for purchase.\nAntelope Farmed in the US, wild in Africa Meat is low on fat but retains moisture when cooked Tender parts can be sautéed or grilled tougher part need to be braised Bison (American Buffalo) The meat is in general cooked like beef Deer The deer family includes elk, mule deer, reindeer, red-tailed and white-tailed deer and is generally known as venison. Farm raised in New Zealand and US Dark very lean meat cooked according to general guidelines\nHare Rabbit the domesticated hare is handled under poultry Hare has dark meat and is quite tough with the exception for the loin Well suited for braising and casseroles Well suited for pates and terrines Wild boar Leaner and darker meat than pork, strong flavored Wild boar is only available through autumn ranch raised is available all year Baby boar (below 6 months) is considered a delicacy Mostly roasted in pieces but also used for sausages and terrines Water Buffalo Used in Asia and butchered the same way as beef Fairly tough meat and combination cooking methods suit the best\nFeathered game includes the wild turkey, pheasants, quail, waterfowls like wild geese and duck, partridge, grouse, doves, woodcocks and more although not all birds hunted are readily sold for food production. Wild birds are not permitted to be sold in the US but in Europe and other countries they are. There is a growing number of farmed wild birds available – quail, pheasants etc.\nPartridge May be roasted whole or cut in pieces and braised 450 g in weight Pheasant Excellent for roasting, stewing or braising and often used for consommés or essences Flavorful tender meat 700-1 kg in weight Quail Good for grilling, roasting (stuffed), broiling or sautéing Very lean and often barded 30-60 g per piece Young quails are also called squabs\nThere are a multitude of other meats that might be specialties in some countries or regions and/or cuisines available for consumption. Kangaroo Snake Alligator / Crocodile Yak These meats in general are not aged and need to be stewed or braised until tender.\nGrading Game meat is not graded as domestic animals or poultry would be and voluntary inspections only confirm the wholesomeness of the producer. Meat inspections are done in accordance with the federal inspection requirements. Marinating Traditionally game has been marinated in red wine, oil, herbs and spice to aid tenderness and to cover up the “wild game taste” With farmed animals these is increasingly not necessary anymore\nYour consent to our cookies if you continue to use this website.","Hello my little lamb chops!\nCooking for two? A romantic dinner perhaps? Consider the lamb chop—delicate, tender, juicy, and easy. Rub with some chopped herbs, garlic, salt and pepper, and olive oil, let sit for a bit, sear all over on high heat, let rest a few minutes, and serve.\n“Lollipops” of Lamb\nLamb rib chops are cut from the “rack of lamb“, the top part of the back attached to the ribs, and have incredibly tender meat.\nOften the chops are French trimmed, where the meat nearest the ends of of the rib bones is scraped away, making for a more elegant presentation. Individual rib chops prepared this way are sometimes called “lamb lollipops” because the meat is attached to one or two rib bones that you can pick up with your hands like a lollipop.\nDouble Rib vs. Single Rib Lamb Chops\nWhile you can cut individual rib chops from a rack of lamb (you’ll need a cleaver and a rubber mallet), it’s easiest to work with chops already cut.\nYou can either buy double rib lamb chops, with two ribs per chop, or single rib chops. Double rib chops yield a thicker piece of meat than single chops, and are more forgiving with cooking time if you like your lamb rare or medium rare.\nThe chops of single rib chops are thinner, and you’ll have to pay closer attention and sear quickly so as to not overcook them.\nWhich to use? If the lamb chops are small (8 ribs to a pound), I might choose double rib lamb chops. If the lamb chops are meatier (8 ribs to 1 1/2 pounds or more), I might use single rib chops.\nOne pound of chops will serve 2 to 3 people, 1 1/2 pounds will serve 3 to 4.\nYou can also use this recipe with lamb loin chops (a tender cut of meat from further down the backbone and not attached to ribs).\nHow to Cook Lamb Chops\nCooking lamb chops on the stovetop couldn’t be easier!\n- Marinate the lamb chops in a mixture of herbs, garlic, olive oil, salt and pepper.\n- Sear the chops on high heat on the stovetop until browned on both sides.\n- If you have thick double-rib chops and you would like them more well done, cover the pan, lower the heat to warm, or put them in the oven for a few minutes.\n- Rest the lamb chops for 3 to 5 minutes before serving.\nWhy Marinate Lamb Chops?\nMarinating the chops ahead of time serves two purposes. It’s an excellent way to pre-salt the meat, which helps the chops retain moisture while they cook. Marinating with herbs and garlic gives the lamb extra flavor.\nThe marinade in this recipe includes fresh rosemary, salt and pepper, garlic, and olive oil. You could easily substitute other herbs or seasonings if you’d like, such as thyme or herbes de Provence.\nMarinate the lamb chops for at least 30 minutes, or up to 24 hours. If you’re working with double rib chops, let them sit at room temperature 30 to 45 minutes before cooking; single rib chops can be kept refrigerated until time to cook.\nBest Temperature for Lamb Chops\nA lamb chop is such a lovely tender cut of meat, you just don’t have to do much to it. In fact, the only thing you really have to take care with is to not overcook it. Lamb is best eaten pink, from rare to medium. Overcooking tender lamb chops can result in dry, less-than-tender meat. That said, if you have eaters who prefer their meat more well done, you can always cook it longer.\nSince rib chops are so small, and cook so quickly, checking for internal temperature of single rib chops with a thermometer can be impractical. For this reason I like to use the finger test to check the doneness of the chops. If you have an instant read thermometer and want to check thicker chops, aim for 125°F for rare, 135°F medium-rare, and 140°F medium.\nWhat to Serve with Lamb Chops\nMy favorite adornment to lamb chops is mint chimichurri, a pesto-like sauce made with parsley, mint, and garlic. You can also make a pan sauce with the drippings by quickly sautéing some shallots in the pan, adding a little broth, water, or red wine, reducing the mixture, then stirring in a little butter at the end.\nWe love lamb chops served with polenta, mashed potatoes, or celery root. For a green, some stir-fried snow peas or boiled and sautéed asparagus or green beans. A light mixed green salad, simply dressed with extra virgin olive oil and vinegar is lovely too.\nLove Lamb? Try These Recipes:\n- Lamb Curry\n- Lamb Stew with Root Vegetables\n- Greek Lamb Burgers with Tzatziki Sauce\n- Spicy Lamb Stew with Chickpeas\nLamb Chops with Rosemary and Garlic Recipe\nDouble rib lamb chops, with two ribs per chop, will yield a thicker piece of meat than single rib chops, and are more forgiving with cooking time if you like your lamb rare or medium rare.\nWith single rib chops, which are thinner pieces, you'll have to pay closer attention, and sear quickly, to not overcook the chops.\nThe flavor of lamb is best when prepared rare or medium rare. Error on less cooking time than you would expect, that way you can cook them further if you want them more well done.\n- 1 pound lamb rib chops\n- 2 tablespoons minced fresh rosemary\n- 2 teaspoons salt\n- 1 teaspoon freshly ground black pepper\n- 1 garlic clove, minced\n- 4 tablespoons extra virgin olive oil, divided\n1 Marinate the lamb chops: In a small bowl, mix the rosemary, salt, pepper, garlic, and 2 tablespoons of the olive oil together. Coat the lamb chops with the mixture, massaging it into the meat with your fingers. If you are working with double rib chops, cover and let stand at room temperature for 30 to 45 minutes.\nIf you are working with single rib chops, and you want the result to be rare, let the chops sit in the rub in the refrigerator, do not let come to room temp or the thin ribs will easily overcook when you sear them in the next step.\nYou can also marinate the chops in the fridge for up to 24 hours. (Allow double rib chops to stand at room temperature 30 to 40 minutes before cooking.)\n2 Sear the lamb chops: Heat the remaining 2 tablespoons olive oil in an oven-proof sauté pan over high heat. When the oil is shimmering hot, sear the chops. Sear double rib chops on all sides about 2 to 3 minutes per side. If you are working with single rib chops, sear only on two sides, and only a minute (or less) on each side if you want the result to be rare or medium rare.\n3 Check for doneness: At this point, if you want your lamb chops rare, they are likely cooked enough.\nIf you would like your chops more cooked, you can put them in a 400°F oven for 3 to 5 minutes, or keep them in the hot pan, lower the heat to warm, and cover the pan for a few minutes.\nNote that rib chops are so small, and cook so quickly, checking for internal temperature with a thermometer can be impractical. For this reason I use the finger test to check the doneness of the chops. That said, if you have an instant read thermometer and want to check thick chops, aim for 125°F for rare, 135°F medium-rare, and 140°F for medium.\n4 Rest the chops: When done, remove the chops from the pan, cover with foil and let rest 3 to 5 minutes before serving.\nHello! All photos and content are copyright protected. Please do not use our photos without prior written permission. Thank you!\nThis post may contain links to Amazon or other partners; your purchases via these links can benefit Simply Recipes. Read more about our affiliate linking policy."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:876d2b60-c1e0-45f2-9a81-38a5e387a23d>","<urn:uuid:049d52eb-997e-4462-8b25-eb1383906060>"],"error":null}
{"question":"How do the cleanroom classification requirements differ between EU GMP Annex 1 and ISO 14644-1 standards in terms of their approach to measuring air cleanliness?","answer":"EU GMP Annex 1 and ISO 14644-1 standards use different approaches for cleanroom classification. Annex 1 defines four air cleanliness grades (A-D) specifically for sterile products manufacturing, while ISO 14644-1 uses a more detailed numerical classification system ranging from ISO 1 to ISO 9, with specific particle concentration limits for different particle sizes (from 0.1 μm to 5 μm). Both standards consider different operating conditions, but ISO 14644-1 recognizes three states for testing: 'as built,' 'at rest,' and 'in operation,' whereas EU GMP focuses primarily on 'at rest' and 'operational' states. These differences in classification approaches can lead to misunderstandings in interpretation and practical implementation, often requiring technical expert consultation.","context":["Air cleanliness plays a decisive role, especially in the manufacture of sterile medicinal products. Specifications on air cleanliness can be found in Annex 1 of the EU GMP Guide, in ISO 14644 and in the FDA Guideline on Aseptic Production.\nBasic requirements for the GMP compliant design of pharmaceutical workrooms are described in the EU GMP Guide Part 1 in Chapter 3. Additional specific requirements for pharmaceutical cleanrooms can be found in Annex 1 of the EU GMP Guide.\nA cleanroom class is not only defined by the air cleanliness grade, but also takes into account a number of other design criteria.\nAnnex 1 defines the air cleanliness grades A–D for the manufacture of sterile products. In ISO 14644 and the FDA Guideline similar air cleanliness classes are described which differ in detail. This often results in misunderstandings in the interpretation and practical implementation, which can be avoided by consulting technical experts.\nThere are no binding room classification specifications for rooms used for non-sterile production. However, such a classification can be derived on the basis of the existing air cleanliness grades.\nThe classification of cleanrooms is confirmed as part of the qualification process. During classification, different operating conditions must be taken into account, which differ with regard to the limit values for particles and germs as well as the associated measurement methods.\nAnnex 1 (2022) requires manufacturers to create a Contamination Control Strategy (CCS). This overarching document shall define the cleanliness zone concepts with all critical control points. It should include effectiveness assessments of all design, procedural, technical and organisational controls and monitoring measures used to manage the contamination risks.\nCompliance with the GMP requirements for clean rooms is achieved through structural design, air handling technology and hygiene measures. These measures are to be defined in advance in the CCS with responsibilities, acceptance criteria, alarm and action limits as well as the procedures and the necessary countermeasures.\nAnnex 1 provides examples for the assignment of various aseptic and sterile manufacturing process steps to the defined air cleanliness grades.\nThe term air handing technology is subdivided into the two categories “air handling technology for rooms” and “process air technology”.\nIn the field of pharmaceuticals manufacturing the basic types of air handling units used are:\nAmbient air is interspersed with different substances of various particle sizes and various types. This mixture of substances is to be removed using air filters to a degree that the specified cleanliness standards in a production are upheld.\nWhen developing the design concept of an air handling system for a pharmaceutical manufacturing facility, the external conditions and situation of the site, the requirements placed on the rooms, the climate factors influencing the production process and the requirements associated with the layout must all be known.\nDuring detailed design of the ventilation system the following criteria must be reflected:\nThe safe, functional and economic operation of an air handling unit in a pharmaceutical environment requires a defined maintenance system and the process/product critical work must be described in the CCS.\nThe maintenance program includes various activities: inspections serve to identify and assess the current state of the equipment, planned maintenance serves to maintain the desired target conditions and overhauling is necessary to reinstate target conditions.\nProduction premises and their associated utilities and infrastructure, including air handling units, are of essential significance to the quality of pharmaceutical products. Thus, their qualification is a requirement.\nThe qualification should be limited to aspects and parameters which have significant impact to product and personal safety according to the risk assessment; for all others, which are required for proper technical functioning of the premises and air handling units, technical acceptance testing according to GEP is sufficient.\nThe basis for demanding qualification projects is provided by a Qualification Master Plan derived from the User Requirement Specification. One differentiates the process into four consecutive qualification stages. The conclusion of each qualification stage is documented via a qualification report. The scope of the qualification activities depends on the complexity of the construction project and the requirements derived from the product characteristics, for example and the air purity. Comprehensive checklists are meant to help define the qualification effort in sufficient scope and detail.\nThe equipment, facilities and operating systems shall be evaluated at a sufficient frequency in order to confirm its qualified status. The possibility should be evaluated that minor changes occur over time. Equipment, facilities, plants and systems are to be evaluated at sufficient intervals in order to confirm their qualified status. The possibility that minor changes occur over time should be considered. If there be a need for re-qualification and this is performed at certain temporal intervals, different from those listed in Annex 1, these intervals are to be justified and the success criteria for requalification are to be defined.\nWater is one of the most important starting materials for the production of pharmaceuticals and has a decisive influence on product quality. It is therefore subject to defined quality criteria. These quality criteria must be monitored and adhered to in routine operation. The monitoring of the operation of a water treatment, storage and distribution system is supported by certain GMP regulations. These rules and the resulting measures aim to maintain the qualified condition and thus continuously ensure the required water quality.\nTo ensure the quality-compliant operation of a water system, the regulatory requirements must be translated into company-specific specifications within SOPs. In addition to the SOPs for operating the water system, a service level agreement for maintaining the system is also important.\nMalfunctions and failures are unavoidable in practice. Appropriate measures must be defined based on risk analyses to control and handle such situations.\nRouging is a surface phenomenon that occurs frequently in water treatment plants. Moderate rouging has no adverse effect on water quality, but appropriate monitoring measures should be established. Various chemical and electrochemical methods are available to remove rouge deposits.\nAnother surface phenomenon is the formation of biofilms, which can have a severe impact on the microbiological quality of the water. Here, too, prevention or measures for timely detection and elimination are of utmost importance.\nRegular sanitisation of the water generation plant and the storage and distribution system is the only way to prevent biofilm formation. It also serves as a quality assurance measure after maintenance. A distinction is made between thermal sanitisation, in which the system is flushed with hot water, cold sanitisation with ozone and chemical sanitisation with H2O2, for example, as a disinfectant.\nIn order to ensure the qualified status of the water system over its entire service life, required maintenance of the system components and calibrations of the measurement equipment must be carried out and documented at defined intervals. In addition, the qualification status of the plant must be checked at regular intervals by means of an inspection and, if necessary, restored by means of appropriate requalification measures.\nIf changes are made to the system, all measures must be planned and released in so-called change requests. All changes must be documented. In the case of critical or quality-relevant changes, a requalification must be carried out for the affected part of the plant.\nThe final decommissioning of the water system is part of the life cycle and must also be planned and documented.\n(Herbert Bendlin, PhD; Fritz Röder)","- What is a cleanroom;\n- Classification of cleanrooms;\n- Cleanrooms: ISO 14644-1 standard limits;\n- Other regulations:\nThe most important characteristic of a cleanroom is forcing the airflow continuously to guarantee the presence of very pure air.\nCleanrooms are ranked by their decreasing concentration level of suspended dust particulates.\nInternationally, the term “cleanroom” designates an atmospheric pressure and particulate contamination controlled room.\nWhen defining and testing the specifications of a cleanroom, the UNI EN ISO 14644-1 regulation is used.\nIn this article, we are going to list the cleanliness classes and the maximum particulate level for each one of them.\nWe are also going to specify which methods are used to collect and elaborate data in the usage of cleanrooms.\nThe quality and cleanliness of the air are the two fundamental elements for a clean room, and they’re measured by controlling of the number of particles within the environment itself.\nDuring the design phase of a cleanroom, the air cleaning classes are defined, based on the needs of the production process that will be carried out. Since 1963 and in the following years, different classification models have been developed:\n- Federal Standard 209D, 209E\n- British Standard 5295\n- EU GMP\n- VDI 2083\nHowever, among all the regulations for clean rooms, the one that today is considered as a reference at a global level is UNI EN ISO 14644-1.\nThe classification of cleanrooms is done by counting microparticles of 0.5 μm in a given volume of air; the fewer particles are suspended in the air, the more purified the cleanroom is. The systems operate at low speeds and force into the room a laminar flow of air, previously filtered by HEPA filters, passing through grids placed on the floor.\nThe degree of cleanliness, hygiene and safety inside a clean room are extremely high. Since the purity of the air is much higher than operating rooms, the staff must wear sterile gowns, shoe covers, bouffant caps, and masks. In rooms where very high or total air purity is required, access is allowed only to properly trained personnel.\nCleanrooms: limits for EN -ISO 14644-1 standards\nThe focal points of the legislation are in the table below. The table defines the maximum concentrations of particles contained in a cubic meter of air, in relation to the different cleanroom classes.\n|CLASS||≥0.1 ΜM*||≥0.2 ΜM*||≥0.3 ΜM*||≥0.5 ΜM*||≥1 ΜM*||≥5 ΜM*||FED STD 209E|\n|ISO 3||1,000||237||102||35||8||Class 1|\n|ISO 4||10,000||2,370||1,020||352||83||Class 10|\n|ISO 5||100,000||23,700||10,200||3,520||832||29||Class 100|\n|ISO 6||1,000,000||237,000||102,000||35,200||8,320||293||Class 1,000|\n|ISO 7||352,000||83,200||2,930||Class 10,000|\n|ISO 8||3,520,000||832,000||29,300||Class 100,000|\n|ISO 9||35,200,000||8,320,000||293,000||Air in the room|\n* max numbers of particulates/m³\nThe UNI EN ISO 14644-1 measuring methods\nThe regulation includes three different states in which a cleanroom can be tested:\n- “As built” : the workers perform the measurements in the finished clean room, but still without machinery and personnel;\n- “At rest”: the measurements are carried out with the working machines, but in the absence of personnel;\n- “In operation”: in this case the measurements must be carried out with machines and personnel present and normally active.\nThe UNI EN ISO 14644-1 standard also establishes some fundamental points for the correct measurement of the parameters:\n- The UNI EN ISO 14644-1 standard also establishes some fundamental points for the correct measurement of the parameters;\n- The volume of air taken for each sampling point shall be large enough to contain at least 20 particles of the largest size considered.\n- In any case, the volume of air must not be less than two litres and the sampling time must be higher than one minute;\n- At least one air sample shall be taken for each point (three if there is only one measuring point).\nThanks to the statistical processing of this series of measurements performed at different points, it will be possible to determine the Cleanroom class.\nAvailable the new FlexiBowl® Version Compatible\nUS FED STD 209E standard\nFederal Standard 209 was published in 1963 in the U.S. as \"Cleanroom and Work Station Requirements, Controlled Environments\". The regulation was revised several times in the following years: vA, 1966; vB, 1973; vC, 1987; vD, 1988; vE, 1992.\nFs 209 classifies Clean Environments according to the maximum number of particles between 0.1μ and 5μ allowed per Volume Unit. The table below shows the limits allowed in its latest version.\n|CLASS||≥0.1 ΜM*||≥0.2 ΜM*||≥0.3 ΜM*||≥0.5 ΜM*||≥5 ΜM*||ISO|\n* max number of particulates/ft³\nAvailable the new FlexiBowl® Version Compatible\nBritish Standard 5295\nThe British Standard 5295 was developed in England in 1989. It is divided into 5 main parts: from Part 0 to Part 4.\n- Part 0 - General introduction and definitions for clean rooms and clean air devices.\n- Part 1 - Specifications for clean rooms and clean air devices.\n- Part 2 - Method of specifying the design, construction and commissioning of cleanroom and clean air devices.\n- Part 3 - Guide to operating procedures and disciplines applicable to cleanrooms and clean air devices.\n- Part 4 - Specifications for monitoring cleanrooms and clean air devices to demonstrate continued compliance with BS 5295.\nPart 1 of British Standard 5295 defines the \"Specifications for cleanrooms and clean air devices\"\nThe Standard contains ten classes of environmental cleanliness. The following table shows the classes indicated in the standard.\nAll classes have particulate counts specified for at least two size ranges to provide adequate confidence in the particle size range for each class.\nSpecifically, it is divided into 10 cleaning classes, identified by the letter C, up to the letter M. If we compare the British Standard with the ISO we will notice that the ISO 1 class is considerably cleaner than the higher class proposed by the British Standard, whose higher grade corresponds approximately to the ISO 3 and ISO 4 class.\nTavola BS 5295 Environmental cleanliness classes\n|Maximum permitted number of particles per m^3 (equal to, or greater than, stated size)||Maximum floor area per sampling position for cleanrooms (m^2)||Minimum pressure difference*|\n|Class of environmental cleanliness||0.3 m m||0.5 m m||5 m m||10 m m||25 m m||Between classified areas and unclassified areas (Pa)||Between classified area and adjacent areas of lower classification (Pa)|\n|E||10 000||3 500||0||NS||NS||10||15||10|\n|G||100 000||35 000||200||0||NS||25||15||10|\n|J||NS||350 000||2 000||450||0||25||15||10|\n|K||NS||3 500 000||20 000||4 500||500||50||15||10|\n|L||NS||NS||200 000||45 00||5 000||50||10||10|\n|M||NS||NS||NS||450 000||50 000||50||10||NA|\nThe GMP (Good Manufacturing Practices) have been written and published in the United States since 1968. Subsequently, the European Union proposed its own revision which took the name of EU-GMP.\nThe legislation defines the classes of environmental containment, or the degrees of cleanliness required for a cleanroom, based on the type of activity carried out. These are rules that illustrate: processing methods, equipment and production management necessary to achieve certain standards.\nAnnex 1 \"Manufacturing of Sterile Medicinal Product\" distinguishes in particular four different degrees of cleanliness:\n- \"A\" Areas: areas dedicated to high-risk operations such as the handling of aseptic components or that promote the growth of microorganisms.\n- \"B\" Areas: for environments adjacent to grade A areas, such as filling and aseptic preparations.\n- \"C\" and \"D\" Areas: for less critical activities during aseptic manipulation. In addition, the GMP regulation also provides for two different employment states: at rest and operational.\nNote that the ISO also refers to the same two states, but adds a third: “as built”.\nThese last three are the same that can be found in the UNI EN ISO 14644-1.\nVDI 2083 “Verein Deutscher Ingenieure” standard\nVDI 2083 is the acronym of \"Verein Deutscher Ingenieure\" or Association of German Engineers. Their standard is a series of German regulations created in 1990 with the aim of:\n- Complete some aspects not yet covered by the ISO standard\n- Explain how standards should be used\n- Offer a deeper level of operational practices.\nThe legislation identifies 6 classes with increasing numbers, starting from class 1, up to class 6. These 6 classes correspond exactly to the structure of the classification proposed in Federal Standard 209.\nAustria and Switzerland have also adopted VDI 2083 to promote the standardisation of cleanroom technology. It addresses some particular issues such as energy and cost efficiency; it also proposes some solutions at a technical and procedural level that guarantee potential energy savings and develops a guideline for the training of operators and cleanroom supervisors."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:da27f586-f735-452b-b8a7-cb6836ef96a6>","<urn:uuid:7e78f27c-ae0e-42e8-846c-06f5463df21e>"],"error":null}
{"question":"What strategic approach should be taken when investing in physical real estate, and which modern investment alternatives minimize direct property management?","answer":"When investing in physical real estate, one should create a comprehensive plan considering interests, financial standing, and lifestyle needs. It's crucial to hire a professional real estate agent, focus on location in relation to amenities and business activity, and thoroughly examine property components beyond staging. For those seeking alternatives with minimal property management, REITs and crowdfunding platforms offer solutions. REITs provide investment in commercial real estate through stock exchange trading with attractive dividends and higher liquidity, while crowdfunding platforms enable investment in real estate with lower initial costs, though these might be less liquid and some platforms are limited to accredited investors.","context":["Important Tips for Any Homebuyer\nNo matter what stage of life you are in, buying a home is a big decision. With a seemingly endless list of choices and a nearly as lengthy list of responsibilities, finding and buying the best home for your needs can seem intimidating. Keep the following 12 tips in mind to make the experience a little easier.\n1. Create a plan.\nConsider what’s important to you—your interests, financial standing, and lifestyle. Considering the big picture is an important step in helping you narrow down what areas you’d like to live in, the communities that are most tailored to your life, and what type of home will work best for you at the present and in the near future. Take notes and create some sort of list, working closely with whomever you will be making the purchase with to ensure that you are on the same page.\n2. Hire a professional.\nNo matter how many homes you’ve purchased, enlisting the help of a licensed, experienced real estate agent will undoubtedly improve your experience. The agent will help you navigate the entire process and understand the legal and market information you will encounter. If you don’t have a particular agent or agency in mind, ask trusted friends and family members for suggestions. You can also attend open houses to meet prospective agents.\n3. Remember that real estate is an investment.\nWhile purchasing a new home can be a deeply emotional experience, it is important to remain grounded and logical in making a final decision. Refer back to the list you made, reviewing what is most important to you and what your family truly needs. Think of your new home as an investment in your future, not a just-for-now purchase.\n4. Obtain loan approval.\nA lot goes into this step, but it will help you understand your financial standing and how much you can realistically afford to put down on a home. Ask your real estate agent for referrals before beginning the loan process. Then, meet with at least three people to keep your loan options open.\n5. Look at tax benefits.\nYou may not be aware of all that you are able to write-off on annually. When you own a home, some of these write-offs include: real estate property taxes, mortgage interest, discount points, and mortgage insurance.\n6. Think location, location, location.\nAsk yourself what area you need to be in. Consider a neighborhood or area’s relation to work, school, local attractions, and other amenities. Also look into what businesses are entering or leaving the area to get a feel for the activity. If you are moving to a new city, this step will require thorough research.\nUse the internet to research all your real estate questions. Make notes on anything you don’t understand and ask your agent for more information or advice.\nDrive by homes, go to open houses, and shop with your agent. Walking through a home and noting what you do or don’t like is often the most efficient way to figure out what you truly want and understand what your money can buy. Keep in mind that finding the perfect match usually takes time.\n9. Look past staging.\nLook at the home’s components and features, not the fancy furniture and décor. Focus on the floor plan, condition, location, yard size, and room placement. Be thorough in your examination of a home before moving forward.\n10. Understand contracts.\nYour agent will do their best to negotiate the best price and terms he/she can. Make sure you understand all the terms of the contract, and ask your agent for clarification if you don’t.\n11. Put in the time.\nMost parts of the home-buying process will require time and research on your end. You can’t rely entirely on your real estate agent to understand your vision and find just what you are expecting.\n12. Trust your instincts.\nThis is likely one of the biggest purchases you will ever make. After all the research and advice, your instincts still might tell you a decision isn’t right. Don’t discount that feeling. For more information regarding homebuyer tips, [ClickHere].","Apartment rentals, REITs, digital real estate as well as crowdfunding platforms are all real estate investment.\nThe investing information provided in this article is for education purposes only. This website does not provide advisory or brokerage services neither does it suggest or suggest investors buy or sell particular stocks, securitie digital real estate or any other investment options.\nThere are several types of real estate investment, but they are generally classified into two classes: Physical investment in real estate, like land commercial and residential properties, and other types of investment that don’t require ownership of physical propertylike REITs and crowdfunding platforms.\nInvesting in traditional, physical real estate may yield high returns, but it is also more expensive upfront and it can have significant ongoing costs. REITs and crowdfunding platforms offer lower barriers for entry, which means that you have the ability to invest in several types of real estate for less than what it costs to invest in even one traditional property. These alternatives to real estate investments also offer the distinct advantage of not needing to leave your house or change into a suit to start investing.\nIf you’re considering investing in real estate There are five types to take into consideration:\nThe public market for publicly traded REITs, also known as publicly traded also known as real estate investment trusts, are businesses that own commercial real estate (think hotels, offices as well as malls). You can purchase REITs’ shares on an exchange. In investing in REITs you invest in the real estate that these companies own and are not subject to the risks of owning real estate directly.\nREITs have to pay at the minimum of 90% of their profits that are tax-deductible to shareholders every year. Investors can therefore receive attractive dividends as well as diversifying their portfolios with real property. REITs that are traded publicly also provide greater liquidity than other estate investments. For instance, if you need money, you could sell your shares through the stock exchange. If you’d like to invest in publicly traded REITs it is possible to do this by opening the use of a brokerage account.\n2. Crowdfunding platforms\nReal estate crowdfunding platforms allow investors access to real estate investments that might yield high returns, but carry significant risk. Some crowdfunding platforms are accessible to only accredited investors being those with assets, or joint net worth , with the spouse, of more than $1 million — excluding the value of their residenceor an annual earnings in the last two years that exceeds $200,000 ($300,000 with the help of a spouse).\n“Keep in mindthat a lot of crowdfunding platforms have a limited experience, and are yet to weather an economic slump.”\nThere are others, too, like Fundrise as well as RealtyMogul which offer investors who aren’t able to meet the minimums — known as non-accredited investors access to investments that they wouldn’t otherwise be capable of investing in. These investments typically take the form of non-traded REITs or REITs, which do not sell on stock exchanges. Since they aren’t publicly traded and aren’t publicly traded, they can be very inliquid, meaning that the funds you invest for at least a few years, and you may not have the option of pulling your money from the investment in case you need it. Remember that many crowdfunding platforms are relatively new with a short history, and have not yet been able to survive an economic slump.\n3. Residential real estate\nReal estate in the residential sector is anyplace where people live or stay, such as single-family properties, condos and vacation houses. Residential real estate investors earn income by acquiring rent (or regular payments for short-term rentals) from tenants in their properties, through the appreciated value their property earns between the time they purchase it, and when they sell it, or both.\nThe investment in residential real estate can take many kinds. It can be as simple as renting out your spare room or as complex as purchasing and flipping a property to earn gain.\n4. Commercial real estate\nCommercial real estate refers to space that is rented or leased by a business. A commercial building that is rented by a single firm and a petrol station, one-stop mall with many distinct restaurants, and leased ones are all examples of commercial real property. If the business does not own the property the business will pay rent to the property’s owner.\nIndustrial and retail real estate can fall under the commercial umbrella. Industrial real estate generally refers to properties where products are made or housed rather than sold, for example, warehouses and factories. Retail spaces are places where people can purchase a product or service like the clothes store. Commercial properties usually have long leases and command more rent than residential properties, which could mean a higher and more stable long-term earnings for the property owner. They may also require larger down payments and management expenses.\n5. Raw land\nIf you build it, will they come? Investors usually buy land for either commercial or residential development.\nHowever, buying land to develop requires some market research, especially in the case of developing the land yourself. This type of investment is most at those who have lots of money to invest and a deep understanding of all things real estate–building codes, flood plains, zoning regulations as well as an understanding of local commercial and residential rental markets.\nWhich investment in real estate is the best?\nIf you’re thinking about purchasing traditional propertyeither commercial or residential properties — taking your time and doing your research doesn’t just mean finding a money for a downpayment. Knowing your local market is important. If there’s little demand for homes or commercial space in your local area or property values start dropping, the investment can quickly become a burden.\nIf you’d like to be more hands-off with the investments you make, then REITs and crowdfunding platforms can be a good way to add real estate to your portfolio with no physical property.\nSome brokerages offer REITs publicly traded as well as REIT mutual funds."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:ae3f8aae-e209-4b39-8df4-f6803e1c6faa>","<urn:uuid:37969f96-f68d-4cfc-9a12-25b71ef910dd>"],"error":null}
{"question":"What steps should I take to properly recycle and avoid contaminating recyclable waste?","answer":"Proper recycling is crucial as contaminated recyclables can end up in landfill. Important steps include: 1) Learn what your local council accepts for recycling - for example, many councils no longer accept pizza boxes 2) Avoid putting incorrect items in recycling bins as they can contaminate an entire truckload of recyclables 3) Report illegal dumping to your local council 4) For items that can't go in regular recycling, check if there are special recycling programs or facilities that accept them. This is especially important now that there are stricter standards for waste processing.","context":["Plastic pollution is a huge problem for marine wildlife.\nAccording to the World Wildlife Fund, 80 per cent of marine pollution is generated by land-based activities and it's having a devastating effect on our oceans.\nJust yesterday a sperm whale that washed up on a beach in Spain was found to have died due to nearly 30 kilograms of trash that blocked its digestive system.\nAnd last month a diver shared video of masses of floating plastic at a popular dive spot in Bali, saying that he had \"never seen anything like\" it before.\nThe problem is complex and requires a number of solutions. But there are some surprisingly simple things that consumers can do to help cut back their plastic pollution and reduce marine debris.\nHere are just a few conscious choices you can make to help.\nReduce your everyday waste\nCleanaway Waste Management says the easiest step you can take to reduce your everyday waste is to eliminate single-use plastic.\nThat includes things like plastic bags, straws and water bottles, which are some of the most common waste collected in the environment.\nHere are a few ways you can go about reducing your plastic consumption:\n- Say 'no' to plastic shopping bags. In fact, every state in Australia — except NSW — has now banned or promised to ban lightweight plastic bags. Woolworths and Coles have also announced plans to phase it out\n- Forgo plastic straws\n- Buy fewer products. For example, the Government's Your Energy Savings campaign says the same cleaner can be used on your mirrors, tiles and shower recess. Same with cosmetic, skin and hair care items — many of which end up in our waterways, and as landfill\n- Purchase a long-life reusable drink bottle or keep cup and keep them in your bag or at work\nTry buying in bulk\nConfused on how this helps?\nWell, it turns out all those single-serve yoghurts, individual packs of washing powder and chocolate wrappers use up quite a bit of packaging.\nBy buying in bulk you're using less packaging. And it also saves you money at the checkout.\nHowever, it's important to point out you should use your common sense with this.\nOnly buy in bulk if you know you are going to use the product — like toilet paper or laundry powder. There's no point buying a 1 litre tub of yoghurt if you don't eat it that often — that will just end up as food waste.\nLimit your use of chemicals\nBasically, what goes down our drains ends up in our waterways and in the ocean.\nThat includes harmful chemicals, such as phosphates, which can cause algal blooms.\nInstead, Your Energy Savings recommends cutting back on the chemicals and using an organic alternative. These include:\n- Baking soda cleans, deodorises, softens water and is a good scouring powder for everything from silverware to sinks\n- Lemon juice is a mild bleach, deodoriser and cleaning agent\n- White vinegar cuts through everything from tarnish to grease, and it's a deodoriser and mild disinfectant. It's also great for cleaning mirrors and windows. Mix half and half with water, and keep it in a labelled spray bottle\n- Washing soda cuts grease and removes stains\nAnd if you do need to buy specialist cleaning products, consider buying those that are plant-based and biodegradable:\n- Use dishwasher detergents that are free of chlorine bleach and lowest in phosphates\n- Use bathroom cleaners that are free of aerosol propellants and antibacterial agents\nFor more tips on green cleaning check out the NSW Office of Environment and Heritage website.\nReuse items or try buying second-hand\nBefore buying new clothes, technology or furniture, consider whether you really need it.\nIn many cases, you probably don't. But for those times that you do, consider getting it second hand.\nAustralians are the world's second largest consumers of textiles but only about 15 per cent of clothes donated to charity are resold within Australia.\nThe rest are sold as industrial rags, sent to landfill, or sent overseas to developing nations.\nAnd in many cases, these clothes can take hundreds of years to break down.\nFor example, clothing made from polyester, which is essentially a plastic, takes up to 200 years to breakdown in landfill.\nBy choosing reused or recycled products, you're helping to create a market for used goods and supporting the recycling industry.\nKnow how to recycle\nA lot of people are still confused about what they can and can't recycle.\nAnd China's recent crackdown on the standard of waste it will import means it's more important than ever to know what can and can't be recycled.\nRecently, the ABC looked at whether people could recycle pizza boxes and many were surprised to find out that \"a lot of councils are saying no to pizza boxes\".\nAnd this can be a problem because the wrong item can \"contaminate\" a recycling bin and reduce the value of your recycling.\nIf there's too much contamination, you risk sending a truck full of recyclables to landfill.\nParticipate in a beach clean-up or do it yourself\nMarine Conservation says people should, \"just remember the Scab Duty maxim — see it, bin it, everyday\".\nIt might not seem like much but every little bit counts — even making sure to throw your rubbish away in a trash can or bring a bag with you.\nWhat else can you do:\n- Look out for rubbish on streets, footpaths, parking lots and storm drains — they can empty into our oceans and waterways\n- You're also encouraged to report instances of illegal dumping to your local council\n- Get involved with your community or participate in an organised clean-up event near you"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:0b334475-56cf-41ea-afe4-8f580ff816df>"],"error":null}
{"question":"What are the EPA regulations for sea burial, and what groundwater protection rules apply to emergency mass burials?","answer":"The EPA requires sea burials to be conducted at least 3 miles from shore, using only materials that easily decompose in water, and must be reported within 30 days. For mass emergency burials with disease risks, groundwater protection zones may extend to source protection zone 2 (SPZ2), and emergency planners should prioritize alternatives like cremation. Any temporary morgue must contain bodily fluids, microbes, substances, and chemicals, and burial sites must not affect the quality or safety of groundwater supplies.","context":["Scattering Cremated Remains at Sea\nPeople are increasingly choosing to scatter cremated remains at sea as a final resting place. It\nprovides a way to remember the person who has passed on in a beautiful and natural setting.\nThere are several reasons why people choose to scatter a loved ones' remains at sea.\nThe Sea Holds an Important Place\nPerhaps the deceased was a surfing enthusiast or a sailor. Or maybe it’s a particular beach\nhouse or vacation spot that was especially beloved. Returning to a place that meant so much\nduring their lifetime often seems like just the right thing to do. For loved ones, visiting that\nspecial spot by the sea can provide a more meaningful connection with the memory of the\nperson who has passed than visiting a grave site.\nAn Affordable and Environmentally Sound Choice\nCompared to traditional burial in a cemetery, scattering cremated remains is considerably less\nexpensive. There is no casket or cemetery plot to purchase. And, unless the deceased had a\nlife insurance policy that covered burial and funeral costs, that expense is left to the survivors.\nFrom an ecological standpoint, scattering at sea is also a wiser choice. It involves the use of\nfewer chemicals, it avoids the consumption of lumber and other material in the construction of a\ncasket, and it doesn’t use up any land.\nA Cathartic Experience for Survivors\nThe loss of someone near and dear to us triggers the grieving process and its five stages: guilt,\nanger, numbness, yearning, and acceptance. Different people may experience the stages in\ndifferent sequences and may go back and forth between them before finally arriving at the\nacceptance stage. The physical act of releasing cremated remains into the sea can be an\nimportant step in the grieving process that will help a loved one get closer to the acceptance\nImportant Things to Know\nAll burials at sea, including the scattering of remains, are regulated by the EPA under the\nMarine Protection, Research and Sanctuaries Act (MPRSA). It states that they cannot be held\nwithin 3 miles of the shore and only materials that will easily decompose in water can be used. It\nalso stipulates that you must report the burial at sea within 30 days.\nThe EPA’s jurisdiction is limited to coastal waters. Scattering cremated remains in inland waters\nsuch as lakes and rivers is subject to local state laws and regulations. For example, the New\nYork State Environmental Protection Agency prohibits scattering cremated remains in fresh\nHow to Scatter Remains at Sea\nConsidering the actual mechanics of scattering cremated remains is important. If you choose to\nrelease the remains from an urn or container, you must take winds into account. A sudden and\nunexpected gust of wind could send the remains back in your direction or onto the ship's deck. If\npossible, choose the side of the ship that is shielded from the prevailing winds.\nAnother way to avoid cremated remains from getting caught by the wind is to release the\nremains using a biodegradable urn to ease the remains into the waters.\nThere are many ceremony and reading options that are suitable for scattering cremated remains\nat sea. Talk to the personal staff at New York Atlantic Funeral Services if you’re looking for\nsome suggestions or ideas.","Understand how to manage cemeteries and burial of human and animal remains, to prevent or limit groundwater pollution.\nBurials must not pollute groundwater. Groundwater can be at risk of pollution from burials where the numbers are sufficient and if the site is in a sensitive or vulnerable area. Measures to prevent or limit pollution must be appropriately considered, given the sensitivity and risks posed.\nThe Environment Agency’s groundwater position statements explain government policy on the burial of human and animal remains.\nHuman and animal burials: minimum groundwater protection\nA burial site must be:\n- outside a source protection zone 1 (SPZ1)\n- at least 250 metres from any well, borehole or spring supplying water for human consumption or used in food production – for example at farm dairies\n- at least 30 metres from any spring or watercourse not used for human consumption or not used in food production\n- at least 10 metres from any field drain, including dry ditches\nDifferent rules apply for:\n- home burials of a person or larger domestic pet animal – the minimum distance is 50 metres from any well, borehole or spring supplying water for human consumption or used in food production purposes, including private water supplies\n- home burials of domestic pets – there are no minimum groundwater protection requirements\n- mass emergency burials with a risk of disease carried by groundwater – the zone may extend to source protection zone 2 (SPZ2)\nAll graves must:\n- have at least 1 metre clearance between the base of the grave and the top of the water table – they should not have any standing water in them when dug\n- not be dug in unaltered or unweathered bedrock\n- not be dug in areas susceptible to groundwater flooding\n- be deep enough so at least 1 metre of soil will cover the top of the coffin, body or animal carcass\nAlways allow for any potential rise in the water table, including seasonal variations and extreme rainfall.\nThe Environment Agency can take action if large numbers of burials, either as a single event or over a period of time, affect or could affect groundwater quality.\nBurials can result in the discharge of hazardous substances and non-hazardous pollutants to groundwater. They are therefore covered by the requirements of the Groundwater Daughter Directive 2006/118/EC as implemented by the Environmental Permitting Regulations.\nThe Environment Agency may serve a works notice under section 161A of the Water Resources Act 1991 and the Anti-Pollution Works Regulations 1999 to prevent or seek remedial action for pollution of controlled waters.\nIn addition to the requirements set out in this guide, you may need to monitor groundwater before burying animal or human remains. Find out what you need to monitor in the cemeteries and burials groundwater risk assessment guidance.\nBurials below the water table\nBurials must not cause pollution and therefore should not take place below the water table. Burials below the water table limit the capacity for attenuation and there must be no direct input of hazardous substances to groundwater. Therefore, some sites with existing planning permission, such as existing cemeteries, may need some form of intervention to control groundwater levels. For example, artificial drainage and abstraction for removal.\nYou must collect any artificially drained groundwater, treat it as contaminated, and dispose of it as foul water. You’ll need an environmental permit to carry out these actions unless you have permission to discharge to mains foul drainage. Contact your local sewerage provider in these cases.\nUntil there is more information about the effect of any new method for managing burials close to, or below, the water table, the Environment Agency will want to see:\n- a hydrogeological assessment of present and future risks\n- plans for continued checks of the site including long-term monitoring\nFor human burials, this includes the use of sealed caskets.\nDisposal of ashes\nYou do not need permission to scatter ashes from a single cremation on your own land, or make any formal record of doing so. You should seek permission from the landowner if you want to scatter ashes on someone else’s land.\nIf you scatter ashes across surface water you should:\n- make sure the effect on the environment and wildlife is minimal and not affect other watercourse users\n- not include casting wreaths or memorabilia as they may harm the environment and wildlife\nCrematoria owners and managers must carry out a site-specific risk assessment if ash is scattered at their sites.\nFind out if you need a permit to bury or spread ash at pet cemeteries.\nHuman home burials\nYou should contact your local council to let them know you are planning a home burial. You may need to speak to the environmental health department.\nYou can find further information on private land burials on the Natural Death Centre website.\nGreen burial sites\nGreen burial sites are often in areas such as woodlands, nature reserves and gardens.\nSite managers and owners must follow minimum groundwater protection requirements.\nContact the Natural Death Centre to find out how to arrange a green funeral.\nManage existing cemeteries to limit environmental impact. For example, use methods such as artificial drainage to reduce the risk and meet the minimum requirements where possible.\nNew cemeteries and extensions\nAny new cemetery or extension to an existing site, including grave plot reuse and ‘lift and deepen’ methods, must:\n- comply with minimum groundwater protection requirements\n- pose no unacceptable risk to groundwater used for drinking water and food production purposes\nAs a minimum you must do a tier 1 risk assessment to evaluate the potential harm to groundwater from pollution.\nLocal councils control new cemetery and extension applications through planning laws, and the Environment Agency is a statutory consultee for potential groundwater pollution.\nThe Town and Country Planning Act and Regulations (various dates) have provisions allowing the control of development and land use, including cemeteries. Planning conditions may be set to protect groundwater.\nThe Environment Agency considers sites with the potential for 100 burials a year or more to be high risk. These sites will need detailed evidence to show both:\n- sufficient depth to the water table or that natural formations offer protection\n- proposed engineering and management methods to prevent unacceptable groundwater pollution\nYou may also have to carry out regular monitoring to ensure the risk of groundwater pollution stays acceptable. How often, and what checks, depends on:\n- cemetery size and rates of use\n- results of the risk assessment\n- hydrogeological characteristics\n- ongoing results of the monitoring\nThe Environment Agency expects you to limit your cemetery’s environmental impact, such as phasing burials to reduce the concentration of substances and organisms.\nArrangements for human burials in emergencies\nDuring an emergency situation, such as a health epidemic or disaster, it’s a priority to keep sources of drinking water safe from contamination.\nEmergency planners should always try to use alternatives to burial for body disposal, such as cremation. Plans for using existing cemeteries, or land reserved for new cemeteries, must not affect the quality or safety of groundwater or any other water supplies.\nAny temporary morgue or mortuary must be able to contain bodily fluids, microbes, substances and chemicals. This may involve sealing drains for safe collection of liquids for later removal by a specialist contractor.\nDuring such emergency situations, if groundwater is at risk, then the minimum groundwater protection requirement of being outside an SPZ1 may be extended to SPZ2.\nDifferent rules apply for:\nIf you’re burying wild animals (except wild game) you must follow the minimum groundwater protection requirements.\nYou do not need permission to bury domestic pets.\nYou can bury small domestic pet animals such as a dog or a cat on your own land, for example in your back garden. There are no minimum groundwater protection requirements.\nIf you want to bury a larger pet animal such as a pet horse follow the minimum groundwater protection requirements.\nFor these larger pet burials contact your local council’s animal health office. Your local council may ask for a map marking the burial place or they may have additional requirements .\nOwners and managers of pet cemeteries should comply with:\n- minimum groundwater protection requirements\n- the Animal By-products (Enforcement) (England) Regulations 2013\n- the voluntary code of practice of the Association of Private Pet Cemeteries and Crematoria\nYou must also register your pet cemetery with the Animal and Plant Health Authority. Find out where you can build pet cemeteries and how to register them.\nLivestock and wild game\nYou must not bury on-site any animal kept as livestock or that’s wild game. This includes animals at farms, zoos and similar places. You can dispose of them by:\n- commercial incineration and rendering\n- landfill – the site must have the correct environmental permit for animal carcasses\nUnder normal circumstances, the burial of fallen stock is prohibited by the Animal By-products (Enforcement) (England) Regulations 2013. A relaxation from this rule (a ‘derogation’) applies in the Isles of Scilly.\nFind out the rules on burying or burning fallen stock so you can safely dispose of dead animals.\nAnimal burials in emergencies\nThe government may relax the laws preventing on-site burials of animals kept as livestock and wild game during extreme events. For example, during a widespread outbreak of foot and mouth disease. You must consider the risk to groundwater – the minimum groundwater protection measures still apply.\nEmergency conditions and time pressures mean only quick, simple risk assessments are possible. This limits burial permissions. Farm managers must have no other means available for disposal before considering burial.\nAnimal burials: pollution risk\nThe potential for disease transmission may mean the burial exclusion zone applies to both SPZ1 and SPZ2. The risk of pollution is site-specific and depends on a number of issues.\nAnimal carcass type and number\nLarge volumes of carcasses pose a greater hazard, especially in areas close to principal aquifers. These may have to go to existing landfill sites with permits to handle animal waste.\nRisk of contamination and spread of pathogens\nIf this is a concern then you’ll need to limit use of groundwater for drinking water, food production purposes and livestock watering.\nBurial method and proposed site\nBurial in unlined pits under emergency conditions will affect groundwater quality.\nAreas with permeable deposits may result in a greater risk to the underlying groundwater. Areas of low permeability present a higher risk that contaminated water will build up and present a hazard to surface water.\nDepth to the water table\nYou should allow for any potential rise in the water table. There must be no direct input of hazardous substances to groundwater and non-hazardous pollutants must be limited to avoid pollution.\nCurrent and potential use of groundwater\nAs well as the risk to any current use of groundwater, over time the burial is likely to remain an active source of contamination so this may limit future use of groundwater. You need to avoid causing pollution to groundwater resources in future.\nEmergency animal burial by weight\nThere are different requirements for the emergency burial of animal carcasses based on the animals’ weight.\nLess than 2 tonnes\nYou do not need permission from the Environment Agency for burials of less than 2 tonnes, but you should follow the minimum groundwater protection requirements.\nYou can carry out more than 1 burial a year, providing:\n- no single burial exceeds 2 tonnes\n- the burial sites are at least 500 metres apart\n- the total weight of all carcasses buried is no more than 8 tonnes\nBetween 2 and 8 tonnes\nContact your local Environment Agency office for burials between 2 and 8 tonnes. It will work with you to:\n- assess the risk of groundwater pollution\n- decide if burial is safe\n- advise if you need an environmental permit\nOver 8 tonnes\nIf the burials exceed 50 tonnes you will also need to show comprehensive plans for engineered containment and site management during and after burial.\nFor more information on emergency burials read Section M of the groundwater protection position statements.\nYou may have to follow the groundwater monitoring rules for animal carcass burials."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:73f434df-69c0-4986-add3-bd17b9d65a72>","<urn:uuid:8f71ef90-0e51-43f1-88ff-e7db75d015e9>"],"error":null}
{"question":"How do fabric selection considerations differ between dollhouse miniatures and theatre acoustics?","answer":"For dollhouse miniatures, key considerations include pattern size (using a one-inch window test), fabric weight (preferring light-weight materials like cotton, wool, or silk blend), wrinkle retention for pleats, and stain resistance for glue application. In contrast, for theatre acoustics, fabric selection focuses on sound absorption properties, with factors like fiber type and weave affecting how much sound is absorbed across different frequencies. Theatre fabrics like velour and wool serge are chosen for their acoustic performance, while lightweight materials like tulle or muslin are used for high-frequency absorption in wall coverings.","context":["If your inspiration for decorating dollhouse miniatures comes from interior style and design journals or Television display, do not overlook the scale in which you are performing. It appears self-evident, but so quite a few of us reduce sight of that actuality, when we gaze at all the superb materials presented to us.\nFollowing are some tips to keep targeted on what is most vital: you are exploring for a ideal fabric to use in a dollhouse miniature. Now please never say, “Duh!” just nevertheless.\nI strongly feel that when we emphasis mainly on coloration, and don’t retain the “technicalities” of pattern dimension and the bodyweight of the fabric in brain at the beginning of the design and style approach, we threat falling in really like with an inappropriate content. The color is magnificent, but most likely the sample is as well substantial, or the material also stiff and major. But it can be these kinds of a gorgeous shade! Proper, then we test to drive this materials into our challenge and the next phase is usually to begin over.\nThe good thing is, we can “audition’ materials before we buy them.\nBrick Mortar Shops\nEducators inform us we all master in 3 distinct approaches: visual, auditory and kinetic – touching. The trick for instructors is to figure out which the a few is the major portal to the brain or each of their fees. We have a kinetic learner in the household. When he encounters some thing new, he claims, “Permit me see!” grabs the object. This darling is kept out of great glassware and porcelain shops.\nThe good news is for miniaturists, fabric merchants give us the opportunity to hone our kinetic capabilities, without having anxiety of breakage.\nDeciding upon The Ideal Pattern\nA single trick is to reduce a just one inch sq. out of a piece of stiff paper or a plastic card. I choose a plastic card for the reason that its easy to keep in my wallet.\nScan the bolts of material in the rack and pull numerous that may possibly be appropriate. Remember, you are thinking of color, pattern and pounds, all at the exact time. To zero in on pattern, pass the a person inch window over a cloth. This expands your options due to the fact even substantial flowered prints may have regions like stems, buds and leaves that could be beneficial to your layout.\nThis video from Joanne’s Minis gives a good demonstration of the Just one Inch Window procedure: http://youtu.be/zXd38Jm4bpI\nWrinkles Are Superior.\nIf you need to have pleats on curtains, dresses or home furnishings skirts, the fabric must maintain a crease, Scrunch the substance in your hand and see if it wrinkles. If it does, it is a prospect.\nStains Usually are not Good.\nSoaked a modest spot with some saliva and see if it stains. This will be vital if you want to use glue everywhere and don’t want it to exhibit.\nFraying, Occasionally Very good.\nExamine out the reduce close of the cloth to see if it frays. You you should not want to be sewing tiny seams and have it fray aside. On the other hand, you want it to fray a bit, if a fringe is in your prepare.\nExcess weight Regulate.\nPay attention to the body weight of the material. If it is hefty, it may perhaps be too thick for miniature function. I come to feel at ease doing the job with cotton, light-pounds wool, cotton and silk blend, rayon and some other gentle-fat fabrics – if they behave the way I want. Normal quilting cotton or identical resources have the features I like for most jobs\nOn the internet Searching\nNo nearby keep can contend with the variety of materials out there online. And you need to have not be overcome by the selection alternatives. A “lengthy tail search term search” gives you sufficient regulate on what is introduced to you.\nAs an example, start with “cloth little prints.” Slender it down by incorporating “cotton” or “large weave.” Use as several vital words as you can, just before the search engine will get totally baffled and very little but irrelevant options are provided.\nWorking with Each On-line and Regional Stores\nListed here is a recent knowledge I experienced. Personalized manufactured curtains are a preferred product in my online shop, I obtained an get for pleated curtains in shade of grey that aqua toss pillows would appreciate. The community JoAnns experienced absolutely nothing handy same at Jay’s Materials. On the internet shopping was upcoming.\nVery first I went to quite a few attempted-and-genuine sites and made use of the inner backlinks to look through. Still no luck, so I went to my favorite browser and entered this extended tail key phrase string in the lookup subject: “dollhouse curtain fabric brocade cotton grey” and received inbound links to a few alternatives. The descriptions of the materials appeared excellent. I did a monitor print of every and emailed them to the client. She produced her option, I built and shipped curtains. All is effectively.\nIt would have been much easier, fewer time-consuming, ergo far more lucrative if I could have discovered what I preferred at a regional fabric retailer. They provide to a mass sector, and the miniature artisan gets lost in that demographic. In the conclude, there is often a way. From time to time we just have to discover new points.","Acoustical applications of fabrics\nMain Curtain and Window Curtains at Baruch College Photo: Todd Kaplan\nRose Brand has partnered with our friends at Stages Consultants to create a blog series about the acoustical applications of fabrics. In this 3-part series, we’ll discuss some of the more common approaches for using fabrics in performance spaces and also the things to consider when choosing a fabric for your project.\nWhy Are Acoustics Important?\nOne of the basic goals of room acoustics is controlling reverberation, or the persistence of sound in a room after the source is silenced. While sometimes the goal is to create “live” spaces with longer reverberation times for a concert hall, more often you are looking for ways to make a room’s acoustics less reverberant, especially in theatre and studio spaces. Adding sound absorbing material to a room will reduce its reverberation time and the perceived loudness of sound. Shorter reverberation times enhance sound clarity, improve speech intelligibility, and reduce loudness. This allows better communication between performers and audiences at comfortable volumes. It also improves source localization, including surround sound effects.\nFabric is among the most versatile and effective sound absorption options available. Each fabric has different characteristics, such as type of fiber and weave that determine how much sound it absorbs and whether it provides even sound absorption from low (bass) to high (treble) frequencies, or whether it is only effective over a limited range. The way a fabric is used can also change its acoustic performance. A flat panel of fabric against a hard wall is different from a curtain with 50% or 100% fullness against the same wall. A fabric panel with backing, or one that’s set off from a surface with an airspace, will also perform differently.\nTypical Use in Theatres\nTypical fixed fabric elements in theatres include audience seating, wall panels, wall coverings, and a portion of draperies. Portions of draperies and dedicated acoustic elements are devices that may be extended in the room or removed depending on scenic needs and desired acoustical effect.\nAcoustic Panels and Wall Coverings\nWhen you think about acoustic wall panels, you may imagine a thick sound absorptive panel with an open-weave fabric covering like you see in many office conference rooms. Acousticians love these panels — low-cost, reliable performance, and most clients can find panels they like. However, in performance spaces you’re not always looking for maximum absorption or the same acoustic performance over every square foot of wall space. A larger variety of fabrics and in different applications is helpful in tuning a performance space.\nUsing fabric to cover walls is a great way to increase the absorption in a room or even cover a reflective panel. Attaching lightweight materials like a tulle or muslin will make walls more absorptive than paint at high frequencies, but have little impact on mid and low-frequencies. Heavier and thicker fabrics start to provide more substantial high frequency absorption and, depending on how they’re used, they can begin to affect mid and low frequencies, even without a porous backing.\nSection of Upholstered Wall\nCurtains and Banners\nWhile we’ve occasionally motorized a set of acoustic wall panels to allow them to be extended or retracted within an auditorium, most often we use fabric as tracked curtains or vertical banners to provide a range of absorption in a room and vary reverberation and loudness. Velour and Wool Serge fabrics are most effective curtain and banner options, and can be doubled or combined with a backing fabric like Canvas or Commando Cloth to achieve the acoustic performance needed in a particular room.\nWindow Curtains at Baruch College Photo: Todd Kaplan\nFabrics with Acoustic Absorption Data Available from Rose Brand\nIn Part 2, we’ll take a more careful look at Rose Brand products and how they perform in different configurations.\nStages Consultants provides world class acoustics and theatre design consulting for performing arts buildings. We bring to every project the knowledge, creativity, design skills and leadership offered by only the most experienced members of our profession - with the individual client oriented service that a small firm can deliver best. You can learn more about them and their services at www.stagesconsultants.com."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:4435e0e7-984a-4df1-a084-7c872282a641>","<urn:uuid:39f8dd73-8ba9-4000-9d44-5bd8b9ea19d5>"],"error":null}
{"question":"Could you compare the artistic venues of Sir Peter Blake's CCA Art Bus and Andy Warhol's Factory in terms of their social and educational purposes?","answer":"The CCA Art Bus and The Factory served quite different purposes. The Art Bus is a mobile educational facility that visits schools, communities, and festivals, specifically targeting schools in underprivileged areas and those specializing in learning difficulties. It provides free access to visual arts and printmaking education, offering gallery space and satellite connections to interact with artists. In contrast, The Factory was Warhol's art and experimental film studio that functioned as a social hub where artists, musicians, and actors gathered. It was characterized by a non-stop party atmosphere where ideas, rock and roll, drugs, sex, and art intermingled, and where Warhol and his 'Superstars' produced over 300 experimental and pornographic films.","context":["The CCA Art Bus is a mobile work of art designed by Sir Peter Blake, it is also a mobile art gallery. Sir Peter designed the bus in a groovy pop art style, with the aim of creating a rock 'n' roll tour bus for art and artists. The CCA Art Bus visits schools, communities, festivals, galleries and art events. Recently seen at The Lord Mayor's Show.\nThe eye-catching design of the bus has proven to make it fascinating to children (including the grown-up ones!) of all ages. Sir Peter's exuberant design includes motifs that are infinitely familiar. The use of these childhood characters in the design of the bus not only grabs the attention of the young, but hopefully also opens their eyes to the fact that ‘Art' does not have to be exclusive or dryly academic, but can be all-embracing, fun and practical.\nThe Art Bus provides access to the visual arts to those pupils who may not normally have that opportunity. The bus primarily targets schools that specialise in learning difficulties as well as those in underprivileged inner city areas.\nThe Art Bus is used as an educational facility to give children an introduction to the visual arts and more specifically printmaking. The bus drives to school and the children climb aboard to experience the art of some of the UK's finest contemporary artists first hand by seeing their work in the bus' gallery space. They will also have an opportunity of a question and answer session with one of these artists live from their studio via the bus' satellite links. The children will learn about printmaking and have the chance to design their own model Art Bus.\nPlease note that all our educational visits are free, however in order to keep the costs we incur to a minimum we try to incorporate school/college visits into trips to galleries/art fairs/festivals that we are planning. So unless the Art Bus is scheduled to be in your area it may not be possible to arrange a visit. If requesting a visit from the CCA Art Bus to your school, please bear in mind (when considering access) that the bus is 4.5 m tall, 9 m long and 3m wide. We must also have a suitable and secure place to park, as well as access to mains electricity.\nFor more information about education on the CCA Art Bus or organising a school visit please contact\nThe Art Bus can be hired on a daily basis. Please be aware that because the bus is a valuable work of art the purposes for which it can be hired are strictly limited.\nPlease note that the CCA Art Bus IS NOT AVAILABLE for party hire.\nPlease see below all the fabulous press coverage about the Art Bus.\nSir Peter Blake parked his Pop Art bus in the Virgin Media Lounge at the V Fesitival. Blake, 77, who created the Sgt Pepper cover, tells us that he won't be designing the sleeve for Robbie Williams's comeback\nSir Peter Blake might have been in the news recently for feeling sidelined by the Tate galleries, but hopefully having his own double-decker art bus will help to ease the perceived snub.\nThe CCA Galleries Art Bus was launched with the help of Helen Mirren, Kevin Spacey, Harry Hill and Sir Peter Blake. Brian Alexander went along for a very surreal ride.","Fame, Mass Media, Consumerism and Death\nBorn Andrew Warhola in Pittsburgh, Pennsylvania in 1928, Andy Warhol was destined to change the world of art forever.\nUpon his graduation, he moved to New York where he worked as an illustrator for publications such as The New Yorker, Vogue , and Harpers Bazaar. He also created window displays for several prominent retail stores at this time. It is perhaps during these years that he developed his keen sense of style and realized the power of image and media manipulation. Throughout the 1950s, Warhol was one of New York City’s leading commercial artists, and he received numerous awards and accolades for his work.\nHe held his first solo show at The Hugo Gallery in 1952 and a group show at The Museum of Modern Art in 1956.\nIn 1961, Warhol created his first series of silkscreens with images of Campbells Soup Cans. The Pop Art Movement thrived on presenting seemingly banal, everyday objects and giving them a monumental importance, Warhol was simply making society aware of it’s own obsessions. The silkscreen process enabled silk-screen mass-produced multiple images with a seemingly endless array of color and compositional variances. Consumerism, one of Warhol’s central themes, was evident in many of the works produced at this time. Coca-Cola bottles, Brillo Boxes and Dollar Bills took on a life of their own. The silkscreen technique and the iconic treatment of Products as Art made Andy a star.\nWarhol’s next thematic breakthrough was the Death and Disaster Series. Works depicting car accidents, Electric Chairs, and racial Riots. The heavily manipulated photographs, repeated over and over again, imply through their multiplicity that society is merely a silent witness to everyday horrors and that death, is simply another aspect of life to be reckoned with. The public’s reaction to these works was not exactly all-embracingly positive and at the advice of Henry Geldzahler, Warhol’s Art Dealer, he produced a less threatening series of Flower Prints.\nIn the years between 1962 to 1964, Andy altered his concentration and celebration of iconic images to include famous personalities and focused on the allure and mystique of Fame. It was at this point that he created the now legendary Series of Marilyns, Jackies, and Elvis paintings, at his studio known as The Factory.\nBy this time, Andy Warhol had become a world famous artist. He held exhibits at The Institute of Contemporary Art in Philadelphia, The Leo Castilli Gallery, and as far away as The Moderna Museet in Stockholm. He produced works at an amazing rate and baffled many with his uncanny ability to choose images that literally became instant icons. Warhol erased the lines between Fine and Commercial Art and forced the world to consider a new perspective that it, subconsciously, had already embraced.\nFAMOUS WARHOLIAN QUOTES:\n“ When you think about it, department stores are kind of like museums.“\n“ In the Future, everyone will be famous for fifteen minutes.“\n“ I love Los Angeles. I love Hollywood. They’re beautiful. Everybody’s plastic, but I love plastic.I want to be plastic.“\n“ I want to be a machine.“\nThe Factory was Andy’s art and experimental film studio, where he and his entourage of self-proclaimed „Superstars“ produced over 300 experimental and pornographic films. The Studio was far more than simply an artist’s atelier. It was THE meeting place for artists, musicians, and actors. The atmosphere was a non-stop party where ideas, rock and roll, drugs, sex and art mingled. In 1968, Warhol was shot two to three times by a fanatical woman,Valerie Solanis, who claimed at her arrest that „He had too much control over my Life.“ The truth of the matter was that he had ignored her and her radical organization, SCUM (Society for Cutting Up Men). This near fatal attack changed Warhol and his Art. His artistic response to this episode, The Skulls and The Shadow Series reflected an interplay between printing and Painting.\nMoving away from the repitition of Iconic Figures, Andy’s work focused on singular Portraits of the Rich and affluent. The silkscreen was still utilized but with a far more expressionistic quality and singularity. Some of his subjects were Mick Jagger, Michael Jackson, Liza Minelli and the like.\nIn the early Seventies Warhol began publishing Interview magazine. he also wrote the autobiographical The Philosophy of Andy Warhol (from A to B and back again). He continued to produce numerous Portraits of celebrities and members of the European elite. The phenomenally priced portraits photographed in Europe, were often produced by Warhol’s assistants at the factory with Andy’s long distance artistic „direction“. His subjects in the late eighties, Mao-tse-Tung and The Endangered Species Series continued to confound,delight and shock art lovers with his always new and ever expanding catalog of colorful images.\nDuring the last years of his life, Warhol began a series of collaborations and promotions with a whole new generation of artists, among them, Jean-Michel Basquiat, Keith Harring, and Francisco Clemente.\nInterestingly, these were all younger contemporaries of Andy’s that were carrying on his tradition of artistic revolution. On February 22 ,1987, Warhol succumbed to heart failure, and as a consequence of a badly executed gall bladder operation. The assassination attempt of 1968 had finally taken it’s toll on the physically fragile artist. In 1989, an exhibit was organized by The Museum of Modern Art, encompassing the largest retrospective exhibit of his works to that date. in May of 1994, the Andy Warhol Museum opened in Pittsburgh, Pennsylvania.\nAndy Warhol’s influence on 20th Century Art cannot be denied. His perception, exploration and experimentation in the field of Visual Arts is unmatched. There is hardly an Artist today that is not touched in some way by his thematic and cultural accomplishments and vision. In accordance with his will, he provided a considerable endowment Fund for Art Education and Patronage, The Andy Warhol Foundation for the Visual Arts, Inc.\n“ Andy Warhol looks a scream, hang him on my wall. Andy Warhol, Silver Screen, can’t tell them apart at all.“\n– David Bowie, Hunky Dory, 1971\nWebsites of Interest:\nThe Andy Warhol Museum\nThe Andy Warhol Foundation for The Visual Arts\nAndy warhol Prints\nThis Article Text © 2005 by John Keaton . All Rights Reserved.Immobilienmakler Heidelberg Makler Heidelberg\nSource by John Keaton"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:3f349faf-ea47-4a9c-91b1-dcf0003b21f9>","<urn:uuid:7ff33fb8-25a0-41d7-9d2b-94f867f07340>"],"error":null}
{"question":"Hi! I'm researching social hierarchies. How does the caste stratification among Muslims in India compare to the traditional Hindu caste system? Los musulmanes en India también tienen castas?","answer":"While both communities have hierarchical social structures, there are some key differences. In the Hindu system, there are 4 main varnas (Brahmin, Kshatriya, Vaishya, Shudra) plus Untouchables. The Muslim community in India is broadly divided into 3 categories: Ashraf (noble/high-born), Ajlaf (low-born/wretches), and Arzal. While Islamic ideology promotes egalitarianism, hierarchical norms emerged among Indian Muslims due to regional adaptations and pre-conversion customs. The Muslim stratification is primarily based on descent (foreign vs local converts) and traditional occupations, with marriages typically confined within these groups. However, unlike the Hindu system which is explicitly based on concepts of purity and pollution, the Muslim hierarchical structure emerged more from historical and social constructs despite Islam's egalitarian teachings.","context":["Article in PDF (Download)\nMuslim Backward Classes – A Sociological Perspective – by Dr. Azra Khanam, Teacher & Researcher, Dr K. R. Narayanan Centre for Dalit and Minorities Studies, Jamia Millia Islamia University, New Delhi. Published by Sage Publications\nMy book is a humble attempt to understand the sociological perspectives on Muslim OBCs as every complex society has individuals who are backward and have low position in the status of hierarchy, the backwardness, as understood in the Indian context, has a number of distinctive features. Here, backwardness is viewed as an attribute not of individual but of certain clearly-defined social segments the membership is generally acquired by birth and which entitles to that social segments certain advantages and concessions specially conferred by the government. The other backward classes constitute a category of people who are for the most part officially listed in a variety of context. The distinctive problems of the backward classes derive from their status ascribed to them by traditional Indian society and by the subsequent forces of history which are in any ways unique to Indian society. Muslim constitutes the second largest religious minority community in India. Census million. After sixty years of independence and positive economic growth, it has been proved by various reports that Muslim community is lagging behind in a number of development indicators as compared to other religious communities including Hindus Christians and Sikhs. Muslims are the followers of Islam and the Islamic ideology espoused the principle of egalitarianism and suggests the equality of all believers. But despite a mark egalitarian pronouncement of Islam some sort of hierarchical norms emerged among the Muslims due to group distinction as adapted to regional situations.\nIndian society is divided on the basis of caste which has been an organizing principle of Hindu social order. Though its rigidity and contours changed greatly through the different historical periods and empirical studies which initially took the form of decennial census adduced considerable evidences that castes (or caste like groupings, which is a much later categorization) existed among Muslims and could be identified through a hierarchy of status orders that has several significant attributes: source of descent so that those claiming to be the descendants of the Prophet or one of his Companions enjoyed precedence over local converts and association with an occupation leading to each caste confining marriages to its members. Using evidences form decennial censuses, Gaus Ansari has argued that Muslims in India are divided in to three broad categories and further these categories are divided in to number of groups for which he chose to designate as castes.\nJustice Sawant clearly expressed his point of view in the famous nine judges Constitution Bench Indira Sahni versus Union of India landmark, judgment, 474. As far as Islam is concerned, Islam also does not recognize caste or stratification; however, among the Muslims in fact there are Ashraf and Ajlaf i.e. high born or low born.\nThe census report 1901 also shows that The Mohammedan themselves recognize two main social stratas- Ashraf and Ajlaf. Ashraf means noble and includes all undoubted descendants of foreigners and converts from high group and all convert from lower rank are known by the contemptuous term Ajlaf, wretches or mean people, they are also called Itar, base as Rasil’, a corruption of Rizals, ‘worthless’. The encyclopedia Americana and (International Edition) and encyclopedia Britanica also describe this division among the Muslims community: Ignoring the caste based categories means one is not recognizing already established and acknowledged empirical ground reality. The situation is here baffling. The very existence of caste based division questions that where did these words Ashraf, ajlaf, and arzal come from? Terms like kamtar (inferior), kameen or razeel are socially constructed. Therefore the division on the basis of caste and stratification in Muslim community is a result of their pre-conversion customs and a tradition, which is, socially constructed reality with long historical orientations.\nThe OBCs are legal constitutional category that has been striving to obtain acceptance of the identity of an independent social category in the post-independence period. The term itself was not used in the Indian constitution until the amendment of Article 338 where it comes to be used for the first time. Until then, even legal-constitutional designation-wise, the OBCs had only a status either as a part of “Weaker Section” (Article 46) or socially, and educationally backward classes [SEBCs: Article 15 (4)] or backward Classes of citizens [BCCs: Article16 (4)]. Also the inferior status accorded to them is visible in the nature of provisions made for the other two clearly defined BC categories of SC/STs. All the provisions in the later case are mandatory. In case of OBCs, all the provisions have been left to the discretion and convenience of the government of the day at central and state levels.\nIn India, Backward Classes is closed status groups and is a more nebulous category. They are mentioned in the constitution only in most general terms. Muslim groups currently bracketed under the category of OBC come essentially from the non-ashraf section of the Muslim population they are the converts from the middle or the lower castes Hindus and are identified with their traditional occupation. India has a constitutional commitment under Article 249, that is, to adopt a special treatment policy for the Backward Classes in order to bring them on par with the rest of the population. Such reverse discrimination is, of course important for social, economic, and political mobility for the backward classes.\nSince independence India has achieved significant growth and development. But all religious communities and social groups (henceforth socio-religious communities-SRCs) have not shared equally the benefits of growth process. Among these, the Muslim community is the largest minority community which is seriously lagging behind in terms of the most of the human development indicators. When we discuss the overall condition of minorities in India we find that a problem of group representation is particularly miserable. The book ‘Muslim Backward Classes: A Sociological Perspective’ is a humble attempt to explore and analyze the social profile of Muslim backward classed and it tries to understand sociological dimensions among Muslim Backward Classes in Pihani Block District Hardoi. This book not only depicts the socio-cultural economic and educational status of Muslim backward classes but also analyze the status of Muslim Backward classes in national and international contexts. The present book tries to define a community not only in term of their religion but also in terms of their social status though identified on the basis of religion but analysed in terms of their social status. Therefore, it encapsulates the overall representation of Muslims in general and Muslim Backward Classes in particular.\nThis piece of research reflects on certain important point some points are as follows:\n1- It is quit misleading to place the theoretical formulations in understanding the community only on the basis of religious precepts and not the social practices, and therefore, placing community as homogenous on the basis of Islamic egalitarianism, ignoring the very fact of empirically acknowledged reality which shows that Muslim community is as much divided and stratified as Hindu society is. Therefore, it is pertinent to understand the structure of the community from sociological point of view in order to locate the community in broader social structure.\n2- Muslim OBCs is constitutionally recognized category which is very significant from policy perspective.\n3- Muslim OBCs constitute more than 80% of the total Muslim population, therefore, the issue of their marginalization and rate of participation in various development indicators become more vibrant when we have such enthusiastic targets of MDGs to be achieved by 2015.\nThe whole book is spread over nine chapters :\nFirst chapter is about the ‘Introduction’ which includes explanation of the problem of Muslim OBCs chosen for the study.\nSecond chapter contains review of literature, literature related to caste and social stratification among Muslims in India that reflects the caste and class debate. OBCs category constitutes converted Muslims; they brought pre conversion customs and rituals with them. Here the great tradition of Islam has been parochializd. Our country is highly stratified on the basis of caste and some elements of it also prevalent among Muslims and Christian communities. These communities are divided in various groups having their own occupational affiliation and endogamy. This occupational identity also provides caste and class identity. Since occupations are hierarchically arranged so the occupational groups. People belonging to the OBCs category constitute peculiar groups based on occupation. Same is true in terms of Muslim OBCs, they are divided in occupation based groups having their peculiar socio-cultural, and economic trends. These trends provide them separate identity and specific location in social system. Further it includes, the literature regarding economic and educational backwardness of the community. Therefore efforts have been made to review the available literature on the topic. Introduction of the field of the study, and the part of methodology has also been incorporated in it.\nThird chapter ‘Backward Classes: An Explanation’ deals with the comprehensive explanation about the meaning and origin of the term Backward Classes, it also provide thorough account about the history and origin of the term and how this term has been discussed and explained in Constitution of India.\nFourth chapter ‘Historical Perspectives of the Muslim Community in India’ depicts about various historical factors responsible for the Muslim backwardness in India. It also gives the viewpoints of the eminent scholars on the relative backwardness of the Muslim community.\nIn fifth chapter ‘Stratification among Muslims in India: A Caste, Class Debate’ author has tried to analyze the caste, class debate among the Muslim community in India. In addition to this second part of this chapter deals with the Category of OBCs among Muslims, their present status and the state intervention for their overall upliftment.\nSixth chapter is about ‘Sociological Dimension among Muslim OBCs’ is the most significant one which presents field based data and formulate empirical base to the study. It is consisting quantitative data. It explores the socio-cultural, economic, educational status of Muslim OBCs, level of general awareness, political participation, and exposure to mass media and status of female among the Muslim OBCs. Sample of 500 respondents have been selected from eleven different locations of the area Therefore, present section of the book is solely based on primary data.\nChapter seven consists “Qualitative interpretation of Internal Dynamics among Muslim OBCs” which endow with qualitative information, the author has applied Sociometric test to analyse the level of affinity and distance among various occupational groups which has been shown in the form of diagram. On the basis of this test it is pertinent to say here that although the notion of purity and pollution among Muslims is absent but certain degree of distance and affinity is found among them on the basis of the cleanliness of occupation, because occupations are hierarchically arranged, so are the people who are following these occupations. Therefore, the level of social intercourse is also guided by their occupational requirement, which also reflect their interaction which is hierarchically constructed. Second part of the present chapter contains case studies (respondent’s life situation) for in-depth inquiry in to the problem.\nChapter eight captioned ‘Millennium Development Goals and Muslim OBCs’ evaluates the status of Muslim OBCs in term of the attainment of the Millennium Development goals in India. After analyzing the condition of Muslim OBCs in India The author reaches to the conclusion that the socio-economic and educational condition of Muslim OBCs poses a question that whether India will be able to achieve the UN Millennium Development Goals or not. Last chapter is about the Conclusion which includes the compilation of all arguments regarding concept of term of backward classes, backwardness of Muslim community in India, caste and stratification among Muslims and the category of Muslim OBCs.\nMuslim community in India is backward. The representation of Muslim OBCs in PSUs (Public Sector Undertakings) is abysmally low. Majority of the Muslim OBCs are living in Kucha house. Land holding among Muslim OBCs is very low even if they having land they are having very small pieces of land. This may also be a reason of high number of Nuclear families. Because agriculture is not the main economic activity but petty traditional occupations are the major sources of income. Therefore the low level of land holdings is resulting in the form of high level of nuclear families. This chapter further includes the findings which reflect the overall status of female in education and economy that is quite discouraging.\nPandey Rajendra .1997. Minorities in India Protection and Welfare, New Delhi: A.P. H. Publications\nMondal S. R. 1998. “Dynamics of Social Stratification among Muslims in Rural west Bengal” in M. K. A. Siddiqui (ed.) Muslims in Free India, New Delhi: Institute of Objective Studies\nAnsari Ashfaq Husain, 2007, Basic Problems of OBC and Dalit Muslims, New Delhi: Serial Publications\nVerma H. S. 2005. “The OBCs and the Dynamics of Social Exclusion in India” in H. S. Verma (ed.) The OBCs and the Dynamics of Social Exclusion in India, New Delhi: Serial Publications\nSocial Economic and Educational Status of Muslim Community of India: AReport November 2006 Prime Minister’s High Level Committee (Cabinet Secretariat, Government of India), New Delhi,p. 193\nDr. Azra Khanam has been teaching and doing research in Dr K. R. Narayanan Centre for Dalit and Minorities Studies, Jamia Millia Islamia, New Delhi since 2008. Dr Khanam did her PhD from Aligarh Muslim University. She taught as a guest lecturer at Women’s College, Aligarh Muslim University during 2003–04 and 2007–08. She has published several articles in reputed Indian journals","CASTE SYSTEM IN INDIA\nAuthor: Shikha Mishra\nThe word Caste is derived from the Spanish word ‘Caste’, the literal meaning of the term is ‘lineage’. It also signifies race or kind. Being such a fancy phenomenon, the word caste is difficult to define, so different scholars gave different definitions.\n1. According to Sir Herbert Risely, caste may be a collection of families, bearing a standard name, claiming a typical descent, from a mythical ancestor, human and divine, professing to follow the identical hereditary calling and regarded by those that are competent to present an opinion as forming one homogeneous community.\n2. According to C.H. Cooley defined when a category is somewhat strictly hereditary, we may call it a caste.\n3. D.N. Majumdar and T.N. Madan have said that caste could be a closed group.\n4. A.W. Green defined caste as a system of stratification during which mobility up and down the status ladder, a minimum of ideally might not occur.\nCASTE SYSTEM IN INDIA\nBRAHMIN: Priests, Academics\nKSHATRIYA: Warriors, Kings\nVAISHYAS: Merchants, Landowners\nSHUDRAS: Commoners, Peasants, Servants\nUNTOUCHABLES: Outcastes, Street sweepers\nWhenever we discuss class structure in India, the primary thing that strikes our mind is the above-mentioned categories.\nIn Indian terms, caste is thought to be ‘jati’, which implies a community or a tribe to which an individual belongs. However, the four varieties of castes, which are mentioned above is termed as ‘varna’, which is defined by a person’s work, the existence of this class structure that prevails in Indian society results in discrimination and injustice which ends up in inequality. This happens in India because the varna of an individual is determined by his/her birth which is completely wrong. It shouldn't support birth and also the same is written in books like Bhagavad Gita.\nIn chapter 18 of Bhagavad Gita, it's mentioned that “all the various qualities of labour of the varied castes in society namely the Brahmins, Kshatriyas, Vaishyas and Shudras are determined by the innate modes of their nature. Consistent with the varna class structure, Brahmins are considered to be the class of the society which generally includes seers, teachers, scientists, etc. After Brahmins comes the Kshatriyas, which incorporate the defendants of the society like cops, soldiers and so on. Below Kshatriyas comes the Vaishyas or the producers like industrial workers, etc. After Kshatriyas is that the Shudras, which service providers are generally like cobblers, etc. the bottom within the table are untouchables or Harijans, which include outcastes, street sweepers, etc.\nCASTE SYSTEM IN OTHER COUNTRIES\nNepal, China, Japan, Korea, Rome, Pakistan, Hawaii, Africa, and other countries of South Asia face the practice of class structure. Some parts of the US also face discrimination but that's supported race and colour. It's also said that even the traditional Siberians, Persians, etc have their sort of class structure. Some systems resembling caste are found at the moment in Burma, Massai, Polynesia and Somali of East Horn. However, the class structure, which we understand today with all its peculiarities, is in India alone. The class structure, the joint family system and also the village system of life – are often considered the three basic pillars of the Indian system and therefore the practice of class structure as a style of stratification is somewhat peculiar to India. There's no comparable institution elsewhere within the world.\nEVILS OF THE CLASS STRUCTURE\nThe class structure that's about 3500 years old is unacceptable within the society and not practised anymore. The existence of class structure promotes hatred and results in differences between different sections of the society. It creates a hierarchical division within the society because it is split into several small groups. A way of highness and lowness or superiority and inferiority is related to this gradation.\nBrahmins which are being placed at the highest of the hierarchy are thought to be pure and supreme. On the opposite hand, the degraded caste or the untouchables (Harijans) have occupied the opposite end of the hierarchy.\nUnacceptable rules are made between all the groups. Restriction on food habits is imposed by the class structure. People belonging to the topmost class i.e. Brahmins will eat only pakka food and folks belonging to lower castes like untouchables are allowed to eat kachcha food. Other restrictions have been introduced years ago. Restriction on marriages was inter-caste marriage wasn’t allowed which implies Brahmins couldn't marry Shudras or vaishyas and contrariwise.\nThe class structure puts restrictions on the range of social relations also. As an example, a lower caste man isn't allowed to touch a person belonging to upper caste. This has resulted within the practice of untouchability within the traditional caste society, some lower caste people suffered from certain civil disabilities also.\nThey were made to measure within the outskirts of the villages i.e. far-flung from the people belonging to higher castes. Restriction on occupational choice was also followed. Weaving, shoemaking, sweeping, washing clothes, etc. were considered to be performed by people belonging to lower castes only. However, learning, teaching, priesthood, etc were considered to be superior work and usually were performed by Brahmins only.\nArticle 15 of the Indian Constitution prohibits the discrimination of citizens of India on the idea of faith, race, caste, sex or place of birth. It states that nobody or state is allowed to discriminate against any citizen of the country on the grounds, which are mentioned above. One and all of the country are equal within the eyes of the law.\n· Archana Chaudhary, Bloomberg, October 25, 2019, https://www.bloomberg.com/quicktake/india-s-caste-systemDonal Johnson, Jean Johnson, Asia society, https://asiasociety.org/education/jati-caste-system-india\n· New World Encyclopaedia, https://www.newworldencyclopedia.org/entry/Caste_system"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:2b0c919f-d377-49d6-afbe-c3eaeb705069>","<urn:uuid:908a214e-079c-47a4-9120-2f7fae6fafed>"],"error":null}
{"question":"What similarities exist between TeamSTEPPS Step 5 and DMAIC's Measure phase for evaluating effectiveness?","answer":"Both approaches emphasize data collection and measurement, but with different frameworks. TeamSTEPPS Step 5 focuses on testing intervention effectiveness through pre- and post-intervention studies, requiring specific measures, target ranges, data collection methods, and timeline planning. DMAIC's Measure phase quantifies the problem by measuring current performance, using tools like Process Flowchart, Data Collection Plan, Benchmarking, and Process Sigma Calculation. Both methodologies require clear measurement strategies to evaluate success, though TeamSTEPPS is specifically oriented toward healthcare team interventions while DMAIC applies to general business processes.","context":["Quick Reference Guide to TeamSTEPPS Action Planning\nStep 1: Create a Change Team\nObjective: To create a team of leaders and staff members with the authority, expertise, credibility, and motivation necessary to drive a successful TeamSTEPPS Initiative.\n- Ensure key leadership representation: senior leadership, frontline leadership.\n- Identify an Executive Sponsor.\n- Ensure key expertise representation: clinical/technical, medical teamwork, process improvement and trending techniques.\nStep 2: Define the Problem, Challenge, or Opportunity for Improvement\nObjective: To specifically state the problem, challenge, or opportunity for improvement that will be targeted by your TeamSTEPPS intervention; and to identify the involved process.\n- Identify a problem, challenge, or opportunity that you feel could be improved with enhanced medical teamwork.\n- Identify the process during which the target problem, challenge, or opportunity occurs by stating what the process is, who is involved, and when and where it occurs.\nStep 3: Define the Aim(s) of Your TeamSTEPPS Intervention\nObjective: To succinctly state in measurable terms exactly what you hope to achieve with the TeamSTEPPS Intervention.\n- For each aim, state in one sentence in measurable terms what you hope will be achieved, who will be involved (whose behavior will change), and when and where the change will occur.\n- Ideally, define a team process aim, a team outcome aim, and a clinical outcome aim.\nStep 4: Design a TeamSTEPPS Intervention\nObjective: To design a TeamSTEPPS Intervention that will address your target problem, challenge, or opportunity and achieve your stated aims.\n- Flowchart or map the process during which the problem, challenge, or opportunity occurs. Write the process steps as they currently occur, identifying who is doing what, when, with what tools.\n- Study the process to identify risk points where things could go wrong and lead to a recurrence of the target problem.\n- Identify team strategies and tools (e.g., brief, huddle, debrief, STEP, SBAR, and I PASS the BATON) that would eliminate the risk points and prevent the problem from recurring.\n- Review the evidence base, brainstorm, and elicit input from key personnel to design your TeamSTEPPS Intervention. State what team tools and strategies will be implemented and who will use them, when, and where.\n- Flowchart the redesigned process as it would look with the intervention in place to identify potential benefits and negative effects.\n- Evaluate your intervention using the TeamSTEPPS Intervention Checklist, and then modify it if needed.\nStep 5: Develop a Plan for Testing the Effectiveness of Your TeamSTEPPS Intervention\nObjective: To develop a method to determine if your TeamSTEPPS intervention achieved your aims. Did it work?\nFor each aim, create a testing plan, including:\n- Change Team member responsible for data collection, analysis, and presentation.\n- A measure and target ranges for the measure.\n- Study design (usually pre- and postintervention study).\n- Sample (study group) or data source with comparison group.\n- Methods for data collection, analysis, interpretation, and presentation.\n- Timelines for baseline and for postintervention data collection and analysis.\n- Resources required.\nStep 6: Develop an Implementation Plan\nObjective: Part A: To develop a plan for training your staff in the medical teamwork knowledge, attitude, and skills necessary to successfully implement your TeamSTEPPS intervention. Part B: To develop a plan for putting your TeamSTEPPS intervention into place.\nPart A: Develop a Plan for Medical Team Training\n- Identify your Instructors, trainee audience(s), and their specific training requirements. Determine who needs to be trained on what team knowledge/skills and by when in order to achieve your aims.\n- Develop a training plan for each trainee audience, including who will attend, what will be taught, when and where sessions will occur, and how training will be conducted (e.g., method of presentation, tools, supplies).\n- Use TeamSTEPPS training materials that will best meet your audiences' training needs.\nPart B: Develop an Implementation Plan for the TeamSTEPPS Intervention\n- Ensure you have collected all baseline data before implementing the intervention.\n- Identify the person(s) responsible for implementation.\n- Determine how you will implement your intervention in order to achieve your aims. Identify who will use what team strategies and tools, when, and where.\n- Create an implementation timeline.\nStep 7: Develop a Plan for Sustained Continuous Improvement\nObjective: To develop a plan for continuous process improvement with your TeamSTEPPS intervention, including plans for ongoing assessment of the effectiveness of the intervention, sustainment of positive changes, and identification of opportunities for further improvements.\n- Develop a monitoring plan for ongoing assessment of intervention effectiveness, including measures and target ranges; data source; methods for data collection, analysis, and use for continuous improvement; and person(s) responsible.\n- Develop a plan for sustaining and spreading positive changes, including rewards, feedback, integration, ongoing teamwork coaching, and sharing of lessons learned.\nStep 8: Develop a Communication Plan\nObjective: To create a communication plan targeting major stakeholders that will generate initial and ongoing support for your TeamSTEPPS initiative and promote the maintenance and spread of positive changes.\n- Identify persons or groups whose support will be important for achieving your intervention aims and for maintaining positive changes. Consider organization and frontline leaders, staff directly involved in the intervention, patients, and other units affected by the intervention.\n- Develop a communication plan for each identified group, including your goals for communication, who will get the information, what information you will communicate, and when and how you will communicate it (e.g., reports, presentations, Emails).\n- Identify Change Team member(s) responsible for implementation and oversight.\nStep 9: Putting It All Together: Write the TeamSTEPPS Action Plan\nObjective: To generate a written Action Plan, based on steps 1 through 8, which will function as your \"How-To Guide\" for every component of your TeamSTEPPS initiative.\nIf you completed each of the worksheets for steps 1 through 8, you have already written your TeamSTEPPS Action Plan. Ensure that your final Action Plan includes all of the following:\n- Identification of the Change Team.\n- Identification of the problem, challenge, or opportunity that will be targeted by the TeamSTEPPS initiative.\n- Stated aims of your TeamSTEPPS intervention.\n- Detailed description of your intervention.\n- A plan for testing the effectiveness of your intervention.\n- An implementation plan for both medical team training and for your intervention.\n- A monitoring plan for ongoing assessment of the effectiveness of your intervention.\n- A communication plan to generate support for the TeamSTEPPS initiative and to promote maintenance and spread of positive changes.\n- Resources required.\nStep 10: Review Your TeamSTEPPS Action Plan With Key Personnel\nObjective: To generate support and elicit ideas from major stakeholders and to identify barriers to program implementation.\n- Ask key stakeholders to review your Action Plan and to provide input. Request that they identify any potential problem areas and offer solutions.\n- Modify your Action Plan based on their input, as appropriate.\nTeamSTEPPS Action Planning At-A-Glance","It’s not an exaggeration to say that DMAIC is a method for solving almost any kind of business problem. This may seem like hyperbole, until you sees exactly what’s involved in this exacting methodology.\nThe name itself is, as one could guess, an acronym: D is for Define; M is for Measure; A is for Analyze; I is for Improve: and C is for Control. Thus, each letter stands for a phase you take in trying to solve a problem. Each phase, incidentally, is broken down into different steps.\nThis way of solving problems is far from academic. It has been tested long enough in the real world to be shown to be highly effective in resolving a wide range of initially bewildering business problems. However, with that being said, the dmaic process is most suitable for following types of business problems:\n- · One that is an obvious problem, rather than one where there is some disagreement that things need to be improved.\n- · One that is worth solving, rather than focusing on fixing things that aren’t broken. A problem is considered worthy of time and attention if the solution will potentially increase revenues, slash costs, or improve efficiency.\n- · One where data can be collected; a quantifiable problem rather than a philosophical or qualitative problem.\nAlthough DMAIC appears linear and sequential in theory, it’s not exactly a step-by-step approach in practice. Since problem-solving is often a process of discovery and insights, iteration is usually necessary for the problem to be resolved.\nSuppose, for example, you are trying to keep your cloud services secure. When you get to the Analysis phase, you might discover that you don’t have enough information on all the biggest threats that could affect security. You then iterate back to the earlier stage, Measure, where you identify and collect the data you forgot or that you didn’t realize you needed.\nLet’s take a look at each DMAIC phase and the tools you could use for that stage:\nD or Define Phase: This is the stage where you get clear on the nature of your project and your customers. What are your project goals? Who are your internal and external customers? What are your deliverables? At the Define Phase, your choices of tools include: Project Charter, Process Flowchart, SIPOC Diagram, Stakeholder Analysis, DMAIC Work Breakdown Structure, CTQ Definitions, and Voice of the Customer Gathering.\nM or Measure Phase: Here is where you will quantify the problem. You need to measure current performance so that you can measure the process. At the Measure Phase, your choices of tools include: Process Flowchart, Data Collection Plan/Example, Benchmarking, Measurement System Analysis/Gage R&R, Voice of the Customer Gathering, and Process Sigma Calculation.\nA or Analyze Phase: You now analyze and figure out the root cause or causes of the problem or defects. At the Analyze Phase, your choices of tools include: a Histogram, Pareto Chart, Time Series/Run Chart, Scatter Plot, Regression Analysis, Cause and Effect/Fishbone Diagram, 5 Whys, Process Map Review and Analysis, Statistical Analysis, Hypothesis Testing (Continuous and Discrete), and Non-Normal Data Analysis. It’s a lot of tools, but remember, you don’t have to use them all; you will probably only need a few tools to analyze the particular problem you’re working on resolving.\nI or Improve Phase: You are now ready to see how you can improve the process by eliminating the issues that are causing the setbacks in the business issue. At the Improve Phase, your choices of tools include: Brainstorming, Mistake Proofing, Design of Experiments, Pugh Matrix, QFD/House of Quality, Failure Modes and Effects Analysis (FMEA), and Simulation Software.\nC or Control Phase: Finally, you are now at the point where you work out how to control future processes. At the Control Phase, your choices of tools include: Process Sigma Calculation, Control Charts (Variable and Attribute), Cost Savings Calculations, and Control Plan.\nAfter reviewing what goes on at each stage, you’re now in a position to appreciate the reason why the DMAIC methodology works as well as it does. Since it is so structured and rigorous, there are different degrees of Six Sigma expertise measured by belts, from white belt to black belt and then beyond to master black belt. Besides understanding all the steps to be taken at each phase, each tool has to be understood clearly enough to put to good use when needed."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:f8489b6d-35c7-4343-84d6-0bfa15c1744b>","<urn:uuid:e3ef5fe8-8799-4dee-ac0b-ecbcd848d566>"],"error":null}
{"question":"What similarities exist between Terry Riley's collaborations with Kronos Quartet and Zimmermann's work with Piano Duo Takahashi|Lehmann in terms of compositional transformation?","answer":"Both musical relationships demonstrate how compositions can be transformed through different interpretations and arrangements. Riley wrote 13 string quartets specifically for Kronos Quartet over their long association, while Zimmermann's Monologues performed by Piano Duo Takahashi|Lehmann exists in two significantly different versions - one for orchestra and one for two pianos. In both cases, the composers created new reflections on musical material through different arrangements. Zimmermann's piano version is considered a new composition that integrates numerous quotations and explores the transformation of historic music, similar to how Riley's work with Kronos Quartet involved creating new compositions that built upon existing musical traditions.","context":["The third audite recording of the PianoDuo Takahashi|Lehmann presents transcriptions of pre-existing works as well as new compositions permeated with musical quotes and allusions. Drawing on musical history, existing works are approached in diverse ways, from using almost original material through to dissolving the classical concept of time and epochs.more\n\"Das Duo Takahashi-Lehmann spielt dieses anspruchsvolle Programm mit bestechender Klarheit und, besonders in den langsameren Stücken, sehr stimmungsvoll.\" (Pizzicato)\nMusical Transformations for Piano Duo\nThe latest release of the Piano Duo Takahashi | Lehmann - their third recording for audite - presents transcriptions of pre-existing works as well as new compositions permeated by musical quotations and allusions to the works of other composers. Drawing on musical history, existing works are approached in diverse ways, from using almost original material through to dissolving of the classical understanding of time and tradition.\nGyörgy Kurtág's arrangements of Bach's chorale preludes and Max Reger's transcription of the Fifth Brandenburg Concerto transfer a piece from one medium (orchestra or organ) to another (piano duo) without changing the original composition; Kurtág chose sacred works, Reger a secular concerto. Kurtág's translations for two pianists were the result of a contemplative immersion in Bach's pieces; Reger managed to retain the concerto character whilst creating a transparent opus for two players at one piano.\nBrahms' Haydn Variations and Bernd Alois Zimmermann's Monologues also exist in alternative versions for orchestra, written by the composers themselves. In Zimmermann's case, the two versions differ to such a degree that the second one for two pianos, recorded here, should be considered a new composition - a new, second reflection on the idea of the work and the musical material. Bernd Alois Zimmermann integrated numerous quotations, thus displaying the process of transformation of historic music in his Monologues, as well as showing the tension between sacred and secular music. For Zimmermann, \"music about music\" does not just mean tracing the long arc of time stretching from Bach's era to our present day, but also separates the traditional understanding of time and epochs out of its linearity.\nThe Piano Duo Takahashi | Lehmann once again illuminate the multi-faceted cosmos of piano music for four hands at one or two pianos. With their familiar insightful and virtuoso piano technique, the two players manage to combine scholarly programming with passionate expressiveness. Their music-making, both sophisticated and touching, makes even seemingly abstract works accessible to their audience.\nThis is Piano Duo Takahashi-Lehmann’s third disc for Audite. The first, Originals and Beyond (Audite 97.706), contained arrangements for piano fourMehr lesen\nThis disc would be worth getting if all it contained were the wonderful performance of Brandenburg Concerto 5 in Reger’s piano duet version. Add theMehr lesen\nNorie Takahashi and Björn Lehmann’s piano duo has been around since 2009 and already has two previous releases on the Audite label: “OriginalsMehr lesen\nDas Piano Duo Takahashi-Lehmann legt seine nunmehr dritte CD-Produktion bei Audite vor. Sie enthält Transkriptionen bereits existierender Werke sowieMehr lesen\nhr2-kultur - der CD-Tipp. Ein Klavierduo teilt sich entweder die 88 TastenMehr lesen","Terrence Mitchell Riley (born June 24, 1935) is an American composer and performing musician associated with the minimalist school of 20th century music, of which he was a pioneer. Influenced by jazz and Indian classical music, his music became notable for its innovative use of repetition, tape music techniques, and delay systems.\nRiley in 2004\n|Birth name||Terrence Mitchell Riley|\n|Born||June 24, 1935|\nColfax, California, US\n|Genres||Minimalism, avant-garde, tape music|\n|Instruments||Electric organ, tape machine, saxophone, keyboards|\n|Years active||1950s - present|\n|Associated acts||Kronos Quartet, La Monte Young, John Cale, Pandit Pran Nath, Stefano Scodanibbio, Rova Saxophone Quartet, Pauline Oliveros, ARTE Quartett|\nRiley is best known for works such as his 1964 composition In C and 1969 album A Rainbow in Curved Air, both considered landmarks of minimalism and important influences on experimental, rock, and electronic music. Since the 1970s, he has collaborated extensively with chamber ensemble the Kronos Quartet.\nBorn in Colfax, California in 1935, Riley began performing as a solo pianist during the 1950s. That decade, he studied composition at San Francisco State University, the San Francisco Conservatory, and University of California, Berkeley, studying with Seymour Shifrin and Robert Erickson. He was involved in the experimental San Francisco Tape Music Center, working with Morton Subotnick, Steve Reich, Pauline Oliveros, and Ramon Sender. Throughout the 1960s he also traveled frequently in Europe, taking in musical influences and supporting himself by playing in piano bars.\nHis most influential teacher was Pandit Pran Nath (1918–1996), a master of Indian classical voice who also taught La Monte Young, Marian Zazeela, and Michael Harrison. Riley made numerous trips to India over the course of their association to study and accompany him on tabla, tambura, and voice. In 1971 he joined the Mills College faculty to teach Indian classical music. Riley also cites John Cage and \"the really great chamber music groups of John Coltrane and Miles Davis, Charles Mingus, Bill Evans, and Gil Evans\" as influences on his work.He was awarded an Honorary Doctorate Degree in Music at Chapman University in 2007.\nRiley began his long-lasting association with the Kronos Quartet when he met their founder David Harrington while at Mills. Over the course of his career, Riley composed 13 string quartets for the ensemble, in addition to other works. He wrote his first orchestral piece, Jade Palace, in 1991, and has continued to pursue that avenue, with several commissioned orchestral compositions following. He is also currently performing and teaching both as an Indian raga vocalist and as a solo pianist.\nRiley's music is usually based on improvising through a series of modal figures of different lengths. Works such as In C (1964) and the Keyboard Studies demonstrate this technique. The first performance of In C was given by Steve Reich, Jon Gibson, Pauline Oliveros and Morton Subotnick. Its form was an innovation: The piece consists of 53 separate modules of roughly one measure apiece, each containing a different musical pattern but each, as the title implies, in the key of C. One performer beats a steady pulse of Cs on the piano to keep tempo. The others, in any number and on any instrument, perform these musical modules following a few loose guidelines, with the different musical modules interlocking in various ways as time goes on.\nIn the 1950s Riley was already working with tape loops, a technology still in its infancy at the time; he would later, with the help of a sound engineer, create what he called a \"time-lag accumulator\". He has continued manipulating tapes to musical effect, in the studio and in live performances throughout his career. An early tape loop piece titled Music for the Gift (1963) featured the trumpet playing of Chet Baker. It was during Riley's time in Paris, while composing this piece, that he conceived of and created the time-lag accumulator technique. He has composed using just intonation as well as microtones. In New York City in the mid-1960s he played with his longtime friend La Monte Young, as well as with John Cale and tabla player Angus MacLise, who were founding members of The Velvet Underground. Riley is credited as inspiring Cale's keyboard part on Lou Reed's composition \"All Tomorrow's Parties\", which was sung by German actress Nico and included on the album The Velvet Underground and Nico, recorded in 1966.\nRiley's famous overdubbed electronic album A Rainbow in Curved Air (recorded 1968, released 1969) inspired many later developments in electronic music. These include Pete Townshend's organ parts on The Who's \"Won't Get Fooled Again\" and \"Baba O'Riley\", the latter named in tribute to Riley as well as to Meher Baba. Charles Hazlewood, in his BBC documentary on Minimalism (Part 1) suggests that the album 'Tubular Bells' by Mike Oldfield was also inspired by Riley's example.\nRiley's collaborators have included the Rova Saxophone Quartet, Pauline Oliveros, the ARTE Quartett, and, as mentioned, the Kronos Quartet. His 1995 Lisbon Concert recording features him in a solo piano format, improvising on his own works. In the liner notes Riley cites Art Tatum, Bud Powell and Bill Evans as his piano \"heroes\", illustrating the importance of jazz to his conceptions.\n- 1963: Music for The Gift (Organ of Corti 1, 1963)\n- 1964: In C, Columbia MS7178\n- 1965: Reed Streams, Mass Art Inc. M-131\n- 1967: You're No Good, recorded in 1967 but unreleased until 2000 (Cortical Foundation / Organ of Corti, 2000)\n- 1968: Germ, with Gérard Frémy & Martine Joste (Spalax CD 14542, 1998). Includes a Pierre Mariétan track.\n- 1969: A Rainbow in Curved Air, CBS 64564\n- 1971: Church of Anthrax, with John Cale (CBS)\n- 1972: Happy Ending (Les Yeux Fermés film soundtrack), Warner Bros. Records France 46125; Les Yeux Fermés & Lifespan, for solo electric organ; two soundtracks (2007 reissue)\n- 1972: Persian Surgery Dervishes, Shanti 83502\n- 1975: Le Secret de la Vie (Lifespan film soundtrack), Philips France 9120 037\n- 1975: Descending Moonshine Dervishes, Kuckuck Records\n- 1978: Shri Camel, CBS Masterworks M3519, for solo electronic organ tuned in just intonation and modified by digital delay\n- 1983: Songs for the Ten Voices of the Two Prophets, for two Prophet 5 synthesisers, Kuckuck Records\n- 1984: Terry Riley: Cadenza on the Night Plain, a collaboration with the Kronos Quartet\n- 1984: Terry Riley and Krishna Bhatt: Terry Riley and Krishna Bhatt Duo, a collaboration with Krishna Bhatt\n- 1985: No Man's Land\n- 1986: The Harp of New Albion, for piano tuned in just intonation\n- 1987: Chanting the Light of Foresight, with Rova Saxophone Quartet in just intonation\n- 1989: Salome Dances for Peace, for the Kronos Quartet\n- 1995: Lisbon Concert, solo piano concert, recorded live July 16, 1995 Festival dos Capuchos, Teatro São Luis, Lisbon, Portugal., New Albion Records\n- 1997: Lazy Afternoon Among the Crocodiles, experimental album recorded with contrabassist Stefano Scodanibbio.\n- 1998: Piano Music of John Adams and Terry Riley, performed by Gloria Cheng\n- 2001: Moscow Conservatory Solo Piano Concert, recording of a live performance on 18 April 2000\n- 2002: Sun Rings for the Kronos Quartet\n- 2002: Atlantis Nath, hand-numbered signed edition of 1000 copies\n- 2004: The Cusp of Magic, with the Kronos Quartet, composed for his seventieth birthday, an ode to the rite of Midsummer Eve\n- 2008: Banana Humberto, piano concerto with Paul Dresher Ensemble\n- 2008: The Last Camel in Paris, live solo electric organ performance in Paris, 1978\n- 2010: Two Early Works, the first-ever recordings of two of Riley's early compositions, performed by the Calder Quartet\n- 2012: Aleph\n- 2015: G Song, Kronos Quartet, in honor of his eightieth birthday\n- 2019: The Lion's Throne, with singer Amelia Cuni, recorded live (Sri Moonshine Music, SMM008)\n- 1970 – Corridor. Film by Standish Lawder.\n- 1975 – Lifespan. Film by Alexander Whitelaw feat. Klaus Kinski, Tina Aumont and Hiram Keller. Soundtrack released as Le secret de la vie in France, on Philips LP 9120 037 (1975).\n- 1976 – Crossroads. Film by Bruce Conner.\n- 1976 – Music With Roots in the Aether: Opera for Television. Tape 6: Terry Riley. Produced and directed by Robert Ashley. New York, New York: Lovely Music.\n- 1986 – In Between the Notes...a Portrait of Pandit Pran Nath, Master Indian Musician. Produced by Other Minds, directed by William Farley.\n- 1995 – Musical Outsiders: An American Legacy – Harry Partch, Lou Harrison, and Terry Riley. Directed by Michael Blackwood.\n- 2008 – \"A Rainbow In Curved Air\" features in the in-game soundtrack of Grand Theft Auto IV. It can be found when listening to the fictional radio station, \"The Journey\".\n- 2017 – Hochelaga, Land of Souls. Film by François Girard.\n- Carl, Robert. 2009. Terry Riley's in C. Oxford University Press. ISBN 978-0-19-532528-7\n- [Anonymous] (2002). Album notes for The Who: The Ultimate Collection by The Who, 12. MCA Records.\n- Potter, Keith (2000). Four Musical Minimalists: La Monte Young, Terry Riley, Steve Reich, Philip Glass. Music in the Twentieth Century series. Cambridge, UK; New York, New York: Cambridge University Press.\n- Edward Strickland, \"Terry Riley\". Grove Music Online (subscription access).\n- Meigh-Andrews, Chris, 2006. A History of Video Art.\n- Hooper, Greg (June–July 2006). \"Hear and now: Terry Riley in Australia\". RealTime. Australia (73): 33.\n- \"Terry Riley's benefit performance for Old First Concerts\". Examiner.com. 2012-03-24. Retrieved 23 April 2012.\n- Ankeny, Jason. \"Biography\". AllMusic. Retrieved 16 October 2017.\n- \"Like a Rainbow in Curved Air: Terry Riley\". Bluefat.com.\n- \"The 10 Best Moments Of All Tomorrow's Parties\". 16 May 2011.\n- Honigmann, David. \"In C, Barbican, London – review\". Financial Times. Retrieved 28 August 2016.\n- Meigh-Andrews, Chris (2006). A History of Video Art. New York, NY and Oxford, UK: Berg (Oxford International Publishers). pp. 94–95. ISBN 978-1-84520-219-4.\n- Holmes, Thomas B. Electronic and Experimental Music, Taylor & Francis (2008) p. 132, 362 ISBN 978-0-415-95781-6\n- This album also produced the name of psychedelic band Curved Air. The Who: The Ultimate Collection (Media notes). The Who. MCA Records. 2002. p. 12.CS1 maint: others (link)\n- Hazlewood, Charles. \"Tones, Drones and Arpeggios: The Magic of Minimalism\". BBC Website. Retrieved 13 March 2018.\n- Collins, Dan (November 19, 2009). \"Terry Riley: Droning Dark Darkness\". L.A. Record.\n- \"Terry And Gyan Riley: Together IN C\". Npr.org.\n- Hersh, Howard (10 January 1993). \"A Composer on the Edge : Minimalist Terry Riley, on a journey of spiritual and artistic discovery, is deeply moved by the concept of artist-as-madman\" – via LA Times.\n- O'Neal, Sean. \"Terry Riley turns an R&B ditty into 20 minutes of madness\". Avclub.com. Retrieved 26 April 2017.\n- \"Terry Riley Discography\". AllMusic. Retrieved 26 April 2017.\n- \"Germ (6), Terry Riley, P. Mariétan* - Keyboard Study 2 / Initiative 1 (+ Systèmes)\". Discogs.\n- \"Shri Camel - Terry Riley | Songs, Reviews, Credits\". AllMusic.\n- \"Terry Riley: Cadenza on the Night Plain - Kronos Quartet, Terry Riley | Songs, Reviews, Credits\". AllMusic.\n- \"Terry Riley, Krishna Bhatt - Terry Riley Krishna Bhatt Duo\". Discogs.\n- \"Terry Riley - No Man's Land\". Discogs.\n- \"The Harp of New Albion - Terry Riley | Songs, Reviews, Credits\". AllMusic.\n- \"Terry Riley: Chanting the Light of Foresight - Rova Saxophone Quartet | Songs, Reviews, Credits\". AllMusic.\n- \"Salome Dances for Peace - Kronos Quartet | Songs, Reviews, Credits\". AllMusic.\n- \"Stefano Scodanibbio - Discography\".\n- \"Piano Music of John Adams & Terry Riley - Gloria Cheng | Songs, Reviews, Credits\". AllMusic.\n- \"Terry Riley – Moscow Conservatory Solo Piano Concert (CD, Album) at Discogs\".\n- \"Sun Rings, for string quartet,… | Details\". AllMusic.\n- \"Atlantis Nath - Terry Riley | Songs, Reviews, Credits\". AllMusic.\n- \"The Cusp of Magic - Kronos Quartet | Songs, Reviews, Credits\". AllMusic.\n- \"Banana Humberto - Terry Riley | Songs, Reviews, Credits\". AllMusic.\n- \"Terry Riley: The Last Camel in Paris - Terry Riley | Songs, Reviews, Credits\". AllMusic.\n|Wikimedia Commons has media related to Terry Riley.|\n- Official home page\n- 2014 in-depth interview on Innerviews\n- Terry Riley at AllMusic\n- Terry Riley discography at Discogs\n- Free scores by Terry Riley at the International Music Score Library Project (IMSLP)\n- Terry Riley on IMDb\n- Terry Riley page on Ubu Web (audio and video)\n- Davidson, Robert. \"Short Biography of Terry Riley.\" 1999.\n- Elision Fields, Riley's management and CD label\n- A Concert in Honor of Terry Riley on his 50th Birthday, featuring the Kronos Quartet .\n- Epitonic.com: Terry Riley featuring tracks from The Book of Abbeyozzud and The Light of Foresight (with Rova)\n- Art of the States: Terry Riley In C (1964)\n- Golden, Barbara. \"Conversation with Terry Riley.\" eContact! 12.2 — Interviews (2) (April 2010). Montréal: CEC.\n- Leopizzi, Marco. \"Terry Riley — Il guru del minimalismo.\" Interview from 1 June 2008. Musicaround.net. (in Italian)\n- Terry Riley (16 February 2001). \"Obsessed and Passionate About All Music\". NewMusicBox (Interview). Interviewed by Frank J. Oteri (published 1 June 2001). (includes video)\n- Interview with Terry Riley.\n- on YouTube"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:d6bb486e-e6c7-4b9f-81ab-6d9c754ee56e>","<urn:uuid:024bb07f-d537-4fb4-a862-5c59fbe3cff3>"],"error":null}
{"question":"Hi! Me interesa mucho indigenous cultures. Can you compare how the Taino people in Puerto Rico and the Ancestral Puebloans show their cultural legacy through buildings or structures?","answer":"The Taino cultural legacy in Puerto Rico can be seen through caves with extensive petroglyphs and pictographs. Meanwhile, the Ancestral Puebloans left more elaborate architectural structures including pit-houses (subterranean wood and earth structures), pueblos (above-ground stone-masonry structures), cliff dwellings (compact masonry-walled pueblos in cliffs), and kivas (round, semi-subterranean ceremonial structures). Both cultures' structures serve as important historical evidence of their presence, though the Ancestral Puebloans left more extensive architectural remains while the Taino legacy is more heavily represented through cave art and continuing family traditions.","context":["Smithsonian Forums on the Living Indigenous Legacies of the Dominican Republic and Puerto Rico\nThe Taino presence in the Dominican Republic and Puerto Rico can be found in the people and the cultures according to scholars conducting research in both places.\nScholars from the Caribbean and the National Museum of the American Indian (NMAI) held forums and conducted research in both the Dominican Republic and Puerto Rico recently to explore indigeneity or the indigenous presence in both of these cultures.\nIn January, the Indigenous Legacies of the Caribbean Project (ILCP) of the NMAI sponsored a forum on Thursday, January 17th at the Archaeological Museum in San Juan de la Maguana, a Dominican town well known on the island for its indigenous heritage. The title of the event was \"The Indigenous Legacy of the Dominican Republic.”\nIn his account of the visit, Dr. Jose Barreiro, Assistant Director of Research at the NMAI, noted why the town of San Juan de la Maguana was chosen for the event.\n“In a country of presumed extinction of indigenous identity and culture, San Juan de la Maguana…stands out for its concentration of people who profess and relish the indigenous heritage of Quisqueya and the Caribbean, broadly identified as Taíno,” Barreiro wrote.\nAt the forum in San Juan de la Maguana, Barreiro presented information about the ILCP’s research and plans for a future exhibition entitled “Consciousness of Taino: Caribbean Indigeneity.” Another member of the ILCP team, Ranald Woodaman of the Smithsonian’s Latino Center also came to gather research for the project and participate in the forum.\nScholars such as epidemiologist Tony De Moya and Anthropologist Glenis Tavares of the National Museum of the Dominican Man then presented studies and evidence of indigenous heritage in the local area and throughout the country. During the discussions held after the presentations, many local residents asked questions about how to conduct oral history interviews as well as descriptions of some of “the Indian roots that undergird Afro-Dominican…music, religious practice,” Barreiro stated. He did note that there were audience members who believed in the “total extinction” of the indigenous presence in the Dominican Republic but that most of the Forum audience was sympathetic and supportive of the ILCP.\nFollowing the event in the museum, the ILCP team traveled to the municipal center of the town where Mayor Hanoi Sanchez stated that San Juan de la Maguana was “the capital of aboriginal culture” of the country.\nIn visits near the town, Barreiro and Woodaman were shown a local indigenous ceremonial area.\n“A local group including Dr. Sobieski de Leon guided us to the Plaza of Anacaona, known locally as the Corral de Indios. This is a sacred space in the old cacicazgo, a large circular ceremonial field, with a stone—the Stone of Anacaona—at the center. It was fascinating to me that the stone is identified as having been in place for more than five hundred years since the massacres that were committed at this exact site. A local prayer woman (oradora), blending Catholic saints and \"world alive\" practice, normally cares ceremonially for the stone,” Barreiro said.\nThe presence of indigenous traditions in families and regions were among the topics explored when the ILCP team traveled to Puerto Rico a few weeks after the visit to the Dominican Republic.\nOn Tuesday, February 19th, the NMAI and the Center for Advanced Studies of Puerto Rico and the Caribbean held a forum entitled “The Indigenous Legacy in Puerto Rico” at the Center in San Juan, the island’s capital. The Center is devoted to graduate level studies in history, economics, anthropology and other fields involving Puerto Rico and its role in the Caribbean.\nAlong with Barreiro, who gave an overview of the ILCP, the presenters at the forum included the Puerto Rican Archaeologist Dr. Osvaldo Garcia who spoke about the Smithsonian’s Caribbean Indigenous Collection, Dr. Juan Martinez Cruzado, the Puerto Rican geneticist who conducted the indigenous mitochondrial DNA Survey of the island who has done further studies there and throughout Latin America, and Professor Jalil Sued Badillo, a noted historian and author of two books involving Taino history in Puerto Rico.\nAfter the forum, the ILCP team visited a number of Taino historical sites in Puerto Rico, including a number of caves with extensive petroglyphs and pictographs. Accompanying Barreiro was Professor Juan Manuel Delgado a historian who over 30 years has collected oral history interviews of Puerto Ricans with indigenous heritage. Delgado, along with Woodaman, Garcia and Martinez Cruzado, are participants in the ILCP.\nThrough Delgado, Barreiro met with Alice Cheverez and her family in the mountains near Morovis, on the western side of the island. Barreiro described part of his visit to the family and their indigenous aspects, noting that Alice had \"classic Taino physique and facial features\".\n\"We had driven for over three hours out of Mayagüez to visit her family. Puerto Rico around San Juan is heavily urbanized but go east or west out of the capital, pass up the mountains to the central and some coastal regions, and you can still meet some families of distinguishable indigenous legacy and lineage,\" he stated.\n\"The Chéverez are a large, extended indo-Boriken family still living in these precious mountains. Their place has the feel of the old campesino (jibaro) homestead – hanging hamocks, animals walking loose, barefoot children playing,\" Barreiro continued. \"The family is reminiscent of large multi-family, indo-Cuban homestead caserios found in the Cuban mountains. More than a single nuclear family at the end of a long and winding road of verdant hills, the Chéverez are a multi-family lineage. Mapping preliminarily with Alice on her family’s extensions, we could count ten families with several children each just among her siblings, while the extended genealogies of a large chain of uncles and aunts and their children’s families through three living generations, took our quick kinship count to some two hundred people...I encouraged Alice and the family to develop a count of their relations.\"\n\"There is a Taino revival and a great continuing interest in things indigenous in Puerto Rico,\" Barreiro asserted. \"The revival is as intense as it is contested, but nevertheless real and extensive. Major historians and archeologists sustain vigorous research agendas, pushing the edges of knowledge and interpretation of a substantial and growing Taino material and archival wealth. We were fortunate to meet up with a few from this distinguished circle during our recent journey through the island.\"","Throughout the southwest, ancient people left their mark but mysteriously abandoned their villages between 750 and 1350 AD/Current Year (CE). The ruins are beautiful reminder of their heritage.\nRecently, the term Anasazi has been replaced by Ancestral Puebloans, referring to people who lived in the Colorado Plateau roughly 2000 to 700 years ago. Why? Anasazi is a Navajo word that translates to “enemy ancestors”, and the peoples were not necessarily enemies.\nThe Dwellings of Puebloan People\n- Pit-houses were the earliest Ancestral Pueblo residences. Subterranean wood and earth structures with roof entryways were cool in the summer and warm in the winter.\n- Pueblos, above ground stone-masonry structures, later replaced pit-houses.\n- Cliff Dwellings were compact, masonry-walled pueblos located in cliffs. Ladders used to access the dwellings were pulled up for protection.\n- Kivas are round, semi-subterranean structures used for ceremonial purposes or other large gatherings.\nVisiting the Ruins\nPalatki Heritage Site, Sedona.\nCliff dwellings and rock art from the Sinqua people from 1150 to 1350 AD/CE. Sinqua means “without water”.\nPalatki is a Hopi word for Red House; however, the Hopi had no specific name for this site. It was named by an archaeologist.\nSince its discovery in the 1900’s, about 70 to 90% of the structure has disappeared due to looting and misuse. It has since been designated a World Heritage Site.\nMontezuma’s Well, Sedona\nAn oasis/sinkhole with cliff dwellings for the Sinaqua in the 1100’s. Despite less than 13″ of annual rainfall, the well is replenished daily with 1.5 million gallons from an underground spring percolating through miles of rock.\nDespite the name, the site has nothing to do with Monetzuma or the Aztecs, the name is based on an inaccurate assumption by those who discovered the ruins.\nMontezuma Castle National Monument, Sedona.\nBuilt 90 feet up a cliff face, possibly to protect from flooding, this is a five-story structure with 20 rooms for 30 to 50 Sinaqua. 1100-1450 AD/CE. (Montezuma is a misnomer).\nTuzigoot National Monument, Sedona.\nTuzigoot is a word that means “crooked water”. The pueblo ruins are two to three stories with 110 rooms that housed 250 Sinaqua.\nWupatki National Monument, north of Flagstaff.\nWupatki is Hopi for “Tall House”: By 1182, 85 to 100 Puebloans lived in Wupatki Pueblo with 100 rooms, a community room and a ball court. The community room was likely used for trading and interaction with the larger community. Within a day’s walk, a population of several thousand lived in separate villages. Wupatki, the largest building within 50 miles, and the nearby hamlets were mysteriously abandoned circa 1250 AD/CE.\nPlayball!: There are over 200 ball courts in southern Arizona, common from 750 to 1200 AD. The Wupatki people intermingled with their neighbors between villages, (a days walk).\nThe Blowhole – A crevice in the earth’s crust that connects to an underground passage in the village. Archaeologists have no idea why. There was a nice warm updraft that I appreciated on a freezing day in May.\nWukoko a neighboring hamlet\nWukoko Pueblo, an elegant hamlet 3 miles from Wupatki Pueblo, is built on a giant rock and stands three stories tall. Pieces of wood beam remain.\nThe Sad Truth\nThe Visitor Center displays testimonials from Navajo families that called this place home, but were evicted due to competition and conflict with ranchers, the railroad, and later the National Park Service.\nCoombs Excavation Site, Boulder, UT,.”Anasazi” State Park\nA village of almost 100 rooms occupied between 1050 and 1175 AD/CE. The excavation site shows evidence of early pit houses and masonry pueblos. The people departed in 1175 AD and the village was burned, possibly by the inhabitants. One artifact on display, the Atlatl (Spear Thrower) is 2,000 years old.\nPetroglyphs are carvings or etchings in the rock. Pictographs are painted figures.\nPalatki Rockart (1150 to 1350 AD), Sedona. The rock art predates the ruins and are thousands of years old.\nNewspaper Rock, Near Needles/Canyonlands UT. Petroglyph panel recording 2000 years of early human activity. Etchings on the rock date from BC time to 1300 AD/CE by Ute, Navajo, Europeans, Americans (and some recent high school grads).\nPetroglyphs Fremont Cliffs Capitol Reef. 600 to 1300 AD/CE\nResources and Information\nSubscribe to receive emails for new posts\nAt A Glance: Click Here for Archived USA Posts – Click Here for Arizona Posts\n6 thoughts on “AZ-The Ancients, Native Americans”\nSo awesome! I have seen the sign for Montezuma’s Castle several times but never stopped. Next time I am in the area I will definitely visit! Thanks for sharing!!!\nI will definitely go!!!\nInteresting images those Petroglyphs Fremont Cliffs Capitol Reef. 600 to 1300 AD/CE.\nLove this post! Seeing these dwellings together is awe-inspiring mixed with some sad truths. Thanks for the gorgeous pics and history lesson. xo\nComments are closed."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:835a8b4c-838f-427e-9d1c-50b5570c83f1>","<urn:uuid:700bc875-7701-4267-8885-ac096f530d13>"],"error":null}
{"question":"Which river ecosystem characteristics distinguish the Indus Dolphin from the Brahmaputra river system?","answer":"The Indus Dolphin has adapted to life in muddy rivers and is functionally blind, using echolocation to navigate and hunt prey like prawns, catfish and carp. They are confined to a 750-mile stretch of the Indus River system. In contrast, the Brahmaputra carries less water and silt in Tibet due to cold and dry conditions, but causes widespread devastation due to floods in Assam during the rainy season where it carries a larger volume of water.","context":["Why in news? NWIC has been setup as a single window source of updated data on water resources.\nNational Water Informatics Centre (NWIC):\nNational Water Informatics Centre (NWIC) has recently been created by Ministry of Water Resources, River Development and Ganga Rejuvenation at New Delhi.\nNWIC would be a repository of nation-wide water resources data and would work as a Subordinate Office under the Ministry of Water Resources, River Development and Ganga Rejuvenation.\nThe Centre would be headed by a Joint Secretary level officer.\nThe management of water resources is a highly complex and tedious task that involves expertise of multidisciplinary domains and depend on historical and real time reliable data and information.\nFor this, the first requirement is to develop, maintain and update regularly a comprehensive Water Resources Information System (WRIS) in public domain for awareness and involvement of all concerned for effective integrated water resources management.\nThis is also prerequisite for scientific assessment, monitoring, modelling and Decision Support System (DSS) and Integrated water resource Management.\nIn this back drop NWIC is expected to:\nprovide a ‘Single Window’ source of updated data on water resources & allied themes\nprovides value added products and services to all stake holders for its management and sustainable development\nempower, inform and enrich every citizen with upto date and reliable water data (other than classified data) and information through web based India Water Resources Information System (India-WRIS) on a GIS platform in Public Domain\ndevelop value added products and services for all aspects of integrated water resources management serving the nation through research, capacity building, linkages, outreach and better governance in water resources sector\nBesides the centre will also collaborate with leading research institutes nationally as well as internationally to provide technical support to other central and state organizations dealing with water, emergency response for hydrological extremes.\nFACT # 2\nWhy in news? Punjab government along World Wildlife Fund (WWF)-India are conducting first organised census of Indus Dolphins.\nIndus river dolphins are one of only four river dolphin species and subspecies in the world that spend all of their lives in freshwater.\nThey are believed to have originated in the ancient Tethys Sea.\nWhen the sea dried up approximately 50 million years ago, the dolphins were forced to adapt to its only remaining habitat i.e. rivers.\nOnly 1,816 exist today in the lower parts of the Indus River in Pakistan and India.\nNumbers declined dramatically after the construction of irrigation systems.\nMost dolphins are confined to a 750 mile stretch of the river and divided into isolated populations by six barrages.\nThey have adapted to life in the muddy river and are functionally blind.\nThey rely on echolocation to navigate, communicate and hunt prey including prawns, catfish and carp.\nIt listed by the IUCN as endangered on its Red List of Threatened Species. Indus Dolphin was also found in Sutlej decades back, but river pollution has caused its extinction in river.\nThe first organised census will be conducted over period of five days in 185 km stretch between Talwara and Harike Barrage in Beas River in Punjab.\nIt is where riverine fresh water Indus Dolphins are confined.\nIt will be conducted two joint teams of Department of Forests and Wildlife Preservation, Punjab and WWF-India.\nIts main objective is to establish accurate population of dolphins in order to plan conservation of species.\nFACT # 3\nWhy in news? Union government has started a project to strengthen Namami Gange Programme through GIS Technology\nThe National Mission for Clean Ganga(NMCG) is the implementation wing of National Ganga Council which was set up in October 2016 under the River Ganga (Rejuvenation, Protection and Management) Authorities order 2016.\nThe order dissolved National Ganga River Basin Authority.\nThe aim is to clean the Gangaand its tributaries in a comprehensive manner.\nNitin Gadkariis the present Minister for Ministry for Water Resources, River Development and Ganga Rejuvenation, Government of India.\nMain pillars of the Namami Gange Programme are:-\nSewerage Treatment Infrastructure\nIndustrial Effluent Monitoring\nGIS technology for Namami Gange programme:\nThe project includes use of Digital Elevation Model (DEM) technology which ensures accurate data collection, an important aspect for river basin management planning.\nDEM technology will enable the identification of entire topography of an area, making it easy for policy makers to analyse the available data.\nThe use of GIS technology for Namami Gange programme will also ensure decentralization, that is the data collected by the government would be easily shared with the local public through geo portals and mobile apps.\nThe technology will also enable people to send their feedback up to the national level.\nThe high resolution GIS enabled data will help in regulating the proposed protected and regulatory zones along the banks of river.","96) What do Himalayan rivers do in their upper course?\nAnswer: The Himalayan rivers perform intensive erosional activity in their upper courses and carry huge loads of silt and sand.\n97) What do Himalayan rivers do in the middle and lower course?\nAnswer:In the middle & lower courses these rivers form meanders, ox-bow lakes and many other depositional features in their flood plains. They also have well developed deltas.\n98) Which river has the largest basin in India?\n99) How dendritic pattern is formed by the river?\nAnswer:The dendritic pattern develops where the river channel follows the slope of the terrain. The stream with its tributaries resembles the branches of a tree, thus, the name dendritic is given.\n100) How is Trellis Pattern developed by a river?\nAnswer:A river joined by its tributaries, at approximately right angles, develops a trellis pattern. A trellis drainage pattern develops where hard and soft rocks exist parallel to each other.\n101) How is rectangular drainage pattern formed?\nAnswer: A rectangular drainage pattern develops on a strongly jointed rocky terrain.\n102) When does a Radial pattern of drainage develop?\nAnswer: The radial pattern develops when streams flow in different directions from a central peak or dome like structure.\n103) Why are Peninsular rivers called seasonal?\nAnswer: A large number of peninsular rivers are seasonal as their flow is dependent on rainfall and even the large rivers have reduced flow of water in the dry season.\n104) Name the rivers which originate in Central Highlands & flow towards the west.\nAnswer:Rivers Narmada and Tapi.\n105) What is a river system?\nAnswer:A river alongwith its tributaries may be called a river system.\n106) From where does river Indus originate?\nAnswer:River Indus originate in Tibet, near Lake Mansarowar.\n107) Name the tributaries which join Indus in Kashmir.\nAnswer:The Zaskar, the Nubra, the Shyok and the Hunza are the tributaries which join river Indus in the Kashmir region.\n108) Which are the main tributaries of river Indus?\nAnswer:The Satluj, the Beas, the Ravi, the Chenab and the Jhelum are the main tributaries of river Indus.\n109) What is the total length of river Indus?\nAnswer:It is about 2,900 kms.\n110) In which states of India, Indus Basin is located?\nAnswer:In India in the states of Jammu & Kashmir, Himachal Pradesh and Punjab, the Indus basin is located.\n111) What are the regulations of Indus Water Treaty of I960?\nAnswer:According to the regulations of the Indus Water Treaty(1960), India can use only 20 percent of the total water carried by Indus river system.\n112) From which place does river Ganga originate?\nAnswer:The head waters of the Ganga, called the 'Bhagirathi' is fed by the Gangotri Glacier & joined by the Alaknanda at Devaparyag in Uttarakhand.\n113) Name the main tributaries of the river Ganga.\nAnswer: The Yamuna, the Ghaghara, the Gandak and the Kosi are the main tributaries of river Ganga.\n114) Which rivers rise from Nepal Himalaya?\nAnswer:The Ghaghara, the Gandak and the Kosi rise in the Nepal Himalaya.\n115) Which tributaries join Ganga from peninsulas uplands?\nAnswer: The Chambal, the betwa and the son are the tributaries.\n116) How is Sunderban Delta formed?\nAnswer: The waters of two mighty rivers ? the Ganga and the Brahmaputra flows into the Bay of Bengal and the delta formed by these rivers is known as Sunderban Delta. It is world's largest delta.\n117) How is Sunderban Delta named?\nAnswer: The Sunderban delta has derived its name from Sundari trees which grow well in marshland.\n118) What is the total length of the river Ganga?\nAnswer:The total length of the river Ganga is 2,500 km.\n119) Which place is located on water divide of river Ganga and the Indus?\nAnswer: Plains of Ambala.\n120) From where does river Brahmaputra originate?\nAnswer: The Brahmaputra rises in Tibet, east of Mansarovar lake very close to the sources of the Indus and the Satluj.\n121) From where does river Brahmaputra enter India?\nAnswer:On reaching the Namcha Barwa, Brahmaputra takes a 'U' turn and enters India in Arunachal Pradesh through a deep gorge.\n122) Which tributaries join Brahmaputra in Assam?\nAnswer: It is joined by the Dibang, Lohit, Kenula to become Brahmaputra.\n123) By what name is Brahmaputra known in Tibet and Bangladesh?\nAnswer: In Tibet it is known as Tsang-Po and in Bangladesh it is called Jamuna.\n124) Why does river Brahmaputra carry less water in Tibet?\nAnswer: In Tibet, the river carries a smaller volume of water & less silt as it is a cold and a dry area.\n125) How does Brahmaputra river cause damage in Assam during rainy season?\nAnswer: Every year during the rainy season, the river overflows its banks, causing widespread devastation due to floods in Assam.\n126) Name the major Peninsular rivers.\nAnswer:The Mahanadi, the Godavari, the Krishna and the Kaveri.\n127) Which two peninsular rivers form-estuaries?\nAnswer: Narmada and Tapi.\n128) Which Peninsular rivers form delta?\nAnswer:The Mahanadi, the Godavari, the Krishna & the kaveri.\n129) At what place Narmada river rises?\nAnswer:The Narmada rises in Amarkantak hills in Madhya Pradesh.\n130) How does river Narmada create picturesque locations?\nAnswer:The 'Marble Rocks' near Jabalpur where Narmada flows through a deep gorge and the 'Dhuadhar falls' where the river plunges over steep rocks are some of the example.\n131) From which states Narmada river Hows?\nAnswer:River Narmada flows in Madhya Pradesh and Gujarat.\n132) From where does River Tapi originate?\nAnswer:The Tapi rises in the Satpura range in Betui district of Madhya Pradesh.\n133) Which are the main west flowing rivers of Western Ghats?\nAnswer:Sabarmati, Mahi, Bharathpuzha and Periyar.\n134) Which river is the largest Peninsular river?\nAnswer: The Godavari, it's length is about 1500 km.\n135) The Godavari rises from which place?\nAnswer:The Godavari rises from the slopes of Western Ghats in the Nasik district of Maharashtra.\n136) In which states Godavari's basin lies?\nAnswer:Maharashtra, Madhya Pradesh, Odisha and Andhra Pradesh.\n137) Name the tributaries of river Godavari.\nAnswer:The Purna, the Wardha, the Pranhita, the Manjra, the Wainganga and the Penganga.\n138) Which river is called 'Dakshin Ganga'?\nAnswer:Godavari is called 'Dakshin Ganga' because of its length and the area it covers.\n139) From where the Mahanadi river rises?\nAnswer: The Mahanadi rises in the highlands ofChhattisgarh.\n140) What is length of river Mahanadi?\nAnswer: It is about 860 km.\n141) From which states river Mahanadi passes?\nAnswer: River Mahanadi passes from Maharashtra, Chhattisgarh, Jharkhand and Odisha.\n142) From which region River Krishna rises?\nAnswer:River Krishna rises from a spring near Mahabaleshwar.\n143) What is the length of river Krishna?\nAnswer: The Total length of river Krishna is about 1400 km.\n144) Name the major tributaries of River Krishna.\nAnswer:The Tungabhadra, the Koyana, the Ghatprabha, the Must and the Bhima.\n145) In which states the basin of river Krishna lies?\nAnswer: The basin of river Krishna is shared by Maharashtra, Karnataka and Andhra Pradesh.\n146) From which place river Kaveri originates and at which place it merges?\nAnswer:The Kaveri rises in Brahmagri range of the Western Ghats and it reaches the Bay of Bengal in South of Cuddalore in Tamil Nadu.\n147) What is the total length of river Kaveri?\nAnswer:The Total length of river Kaveri is about 760 km.\n148) Name the tributaries of river Kaveri?\nAnswer:Its main tributaries are Amravati, Bhavani, Hemavati and Kabini.\n149) Which regions of India are drained by river Kaveri?\nAnswer:Its basin drains parts of Karnataka, Kerala and Tamil Nadu.\n150) What is 'Shivasamudram'? What is its use?\nAnswer:The river Kaveri makes the second biggest waterfall in India. It is known as Shivasamudram. The hydroelectric power generated from the falls is supplied to Mysore, Bangalore and the Kolar Gold Field.\n151) How are lakes formed?\nAnswer:There are some lakes which are the result of the action of glaciers and ice sheets, while the others have been formed by wind, river action and human activities.\n152) How ox-bow lakes are formed?\nAnswer:A meandering river across a flood plain forms cut-offs that later develop into ox-bow lakes.\n153) How lagoons are formed?\nAnswer: Lagoons are salt water lakes which are formed with spits and bars in the coastal areas, for example, the Chilka Lake.\n154) How lakes become seasonal?\nAnswer:Lakes in the region of inland drainage are sometimes seasonal. For example, Sambhar lake of Rajasthan.\n155) What are fresh water lakes?\nAnswer: Fresh water lakes are mostly found in the Himalayan region. They are formed when glacier dug out a basin, which was later filled with snowmelt water.\n156) Name some of the fresh water lakes of India.\nAnswer: The Wular lake. The Dal lake, Bhimtal, Nainital, Loktak and Barapani.\n157) How man-made lakes are formed?\nAnswer: The drainage of rivers for the generation of hydroelectric power also led to the formation of lakes called man-made lakes. For example. Guru Gobind Sagar Lake (Bhakra Nangal Project).\n158) What is the importance of lakes?\nAnswer:Lakes moderate the climate of surroundings, maintain the aquatic ecosystem, enhance natural beauty, help develop tourism and provide recreation.\n159) How rivers help in an agricultural country like India?\nAnswer:Rivers help in irrigation, navigation, hydro-power generation to help the farmers in their production and marketing.\n160) How growing demand of water from rivers, affects the quality of water?\nAnswer:More and more water is drained out of the rivers for growing domestic, municipal, industrial and agricultural demands of water, which reduces their volume. On the other hand, a heavy load of untreated sewage & industrial effluents are emptied into the rivers, affecting the quality of water.\n161) What does 'drainage' mean? What is a 'drainage basin'?\nAnswer: (i) The term 'drainage' means the river system of an area.\n(ii) Some small streams flowing from different directions come together to form the main river, which ultimately drains into a large water body. The area drained by a single river system is called a 'drainage basin'.\n162) What is meant by 'water divide'? Give an example.\nAnswer:(i) Any elevated area such as a mountain or a plateau that separates two drainage basins is known as a 'water divide'. (ii) For example, Ambala is located at the water divide between the Indus and Ganga rivers. It does not receive water from either of the two rivers.\n163) From where does the river Indus originate and which tributaries join the main river?\nAnswer: (i) Several tributaries such as the Zaskar, Nubra, Shyok and the Hunzajoin the river Indus in Kashmir.\n(ii) The Satluj, Beas, Ravi, Chenab andJhelum rivers join together to enter the Indus near Mithankot in Pakistan.\n164) From where does the river Ganga originate and which tributaries join it?\nAnswer:The river Ganga originates from the Gangotri glacier as 'Bhagirathi'. Then it is joined by Alaknanda at Devaprayag in Uttarakhand. The Ganga is joined by many rivers from the Himalayas such as Yamuna, Ghaghara, Gandak and the Kosi rivers. The river Yamuna originates from the Yamunotri glacier, but joins the river Ganga at Allahabad. The main tributaries which come from the peninsular uplands are the Chambal, the Betwa and the Son.\n165) Prepare a short note on the 'Sunderban Delta'.\nAnswer:The Sunderban Delta is the world's largest and the fastest growing delta. Filled by various tributaries, the river Ganga reaches West Bengal. This is the northernmost part of the Ganga Delta. From here, the Bhagirathi-Hooghly (a distributary) flows southwards towards the deltaic plains and the Bay of Bengal. The main stream flows southwards into Bangladesh and is joined by Brahmaputra, known as Meghna. This mighty river (with waters from Ganga and Brahmaputra) forms the deltaic plains and then flows into the Bay of Bengal. It derives its name from the Sundari tree which grows in marshland.\n166) How does the Brahmaputra river enter India? Which are its tributaries?\nAnswer: (i) The Brahmaputra rises in Tibet, near the Mansarovar lake. Then it flows eastwards, parallel to the Himalayas. On reaching the Namcha Barwa, it takes a U-turn and enters India into Arunachal Pradesh through a deep gorge. (ii) Its tributaries are - the Dibang, Lohit and Kenula.\n167) From where does the river Narmada originate? How is the Narmada basin formed?\nAnswer:The Narmada river rises in the Amarkantak hills in Madhya Pradesh. From here, it flows towards the west in a rift valley. In Jabalpur, it passes through a deep gorge of'marble rocks'. It also forms the 'Dhuadhar falls'. It passes through the states of Madhya Pradesh and Gujarat.\n168) Prepare a short note on the river Tapti.\nAnswer:(i) The Tapti rises in the Satpura ranges in Betui district of Madhya Pradesh.\n(ii) It also flows in a rift valley parallel to the Narmada river.\n(iii) Its basin covers parts of Madhya Pradesh, Gujarat and Maharashtra.\n169) Give a brief description of 'Dakshin Ganga'.\nAnswer: (i) The Godavari river is the largest Peninsular river.\n(ii) It rises from the slopes of the Western Ghats in the Nasik district of Maharashtra.\n(iii) Its length is 1,500 km and it drains into the Bay of Bengal.\n(iv) Its drainage basin is also the largest among the peninsular rivers.\n(v) Its tributaries are?the Puma, Wardha, Pranhita, Manjra, Wainganga and the Penganga. (vi) This basin covers parts of Maharashtra, MP, Odisha and Andhra Pradesh.\n170) What do you know about the Mahanadi river?\nAnswer: (i) The Mahanadi rises in the highlands of Chhattisgarh.\n(ii) It flows through Odisha to reach the Bay of Bengal.\n(iii) The length of the river is about 860 km.\n(iv) Its drainage basin is shared by the states of Maharashtra, Chhattisgarh, Jharkhand and Odisha."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:440b6c10-2d65-46a3-bdfa-279a0901e6a3>","<urn:uuid:acf81c87-a54f-4722-a965-264b3dce0037>"],"error":null}
{"question":"What is the success rate of Repair Cafes in fixing items, and what key benefits do they offer to communities?","answer":"Repair Cafes have a high success rate of 67-75% in fixing broken items, as demonstrated by data from locations like North Saanich and Port Coquitlam. These cafes offer several key benefits to communities: they help build a more sustainable world by keeping items out of landfills, they save people money on repairs, they teach valuable repair skills, and they create opportunities for community connection. For example, the North Saanich Repair Cafe brings together an average of 18 volunteers per event, including coordinators, fixers, and administrators. The cafes also fight against throwaway culture and help people understand how their products work.","context":["In early September, the article “Libraries and the Art of Everything Maintenance” (Megan Cottrell, American Libraries, 9/1/2017) was the Library Link of the Day. The article featured a few public libraries that partnered with organizations such as Repair Cafe and Fixit Clinic to encourage the repair of broken items, and teach people how to repair their own things.\nThere is so much to love about this idea. Together, libraries and Repair Cafe/Fixit Clinic:\n- help build a more sustainable world\n- fight the “throwaway” culture of obsolescence\n- encourage an interest in how things work\n- teach useful skills\nFor the past several years, libraries have been talking about Makerspaces – and in some cases, carving out space and buying 3D printers. While I think that 3D printers are amazing for specific purposes (like making teeth), I’m afraid a lot of them are used for churning out cheap plastic junk. They may serve as an introduction to design and robotics, which is not to be discounted…but I think the repair cafe/fixit clinic idea is so much more useful. After all, learning a skill comes easier when you have a purpose: learning a coding language, for example, will probably be a wasted effort unless there’s something you want to make with it.\nIn this scenario, a broken item – lamp, toaster, necklace, scooter – provides motivation for learning, the library provides space and coordinates the event, and the Repair Cafe or Fixit Clinic provides the volunteers (who may bring the tools of their trade with them). In the AL article, Cottrell writes, “The goal of the U-Fix-It Clinic [is] allowing people to repair broken items instead of throwing them away, but also inspiring them to learn more about the products they consume and how they work. The event is part of a larger movement across the globe working to help keep broken items out of landfills and revive the lost art of repair.”\nKnowing how things work – and how to go about fixing them – is empowering; it’s useful knowledge. In a piece for WGBH, “‘Fixit Clinics’ Help People Revive Their Broken Items,” Tina Martin interviewed the founder of Fixit Clinics, MIT grad Peter Mui, who said, “There’s a sense that [people] don’t have a choice when something breaks, there’s no repair people left anymore to fix this stuff.”\nMui wrote a guest blog post on ifixit.org, saying, “Once people start repairing, they start asking questions like ‘What went wrong?’, ‘Can it be fixed?’, and ‘How might it have been designed differently to avoid breaking in the first place?’ That last question is where we’re ultimately going with Fixit Clinic: to encourage products designed with maintenance, serviceability, and repairability in mind.”\nAs the things we use on a daily basis have become more complex (sometimes by necessity, sometimes not), design has become more opaque. I often think of Don Norman’s The Design of Everyday Things while I’m working at the reference desk, explaining the “hamburger menu” to a patron, or helping them locate the miniscule, hidden power button on our new laptops. They often apologize, and I tell them it’s not their fault – it’s poor design. But as more and more of our things have microchips inside them, instead of parts we can see and tinker with, we’ve forgotten how to open things up and explore; we’ve given up on figuring out how things work – or why they stop working.\nThe mentality behind the Repair Cafe and the Fixit Clinic addresses these problems in a tremendously useful way. The Repair Cafe “About” page explains, “We throw away vast amounts of stuff….The trouble is, lots of people have forgotten that they can repair things themselves or they no longer know how. Knowing how to make repairs is a skill quickly lost. Society doesn’t always show much appreciation for the people who still have this practical knowledge, and against their will they are often left standing on the sidelines.”\nThe Fixit Clinic’s mission has similar themes: “Fixit Clinic conveys basic disassembly, troubleshooting, and repair skills using peoples’ own broken things as the vehicle. By sharing these skills while transferring them to others we teach critical thinking through the lens of our relationship to consumption and sustainability. We strive to demystify science and technology so that we can ultimately make better policy choices as a society.”\nA community learning experience that brings people together to share skills and tools, and repair items that would otherwise end up in landfills and be replaced with new things: this is a perfect program for libraries to host. The Cambridge Public Library has partnered with the Repair Cafe in Cambridge already; I’d love for our library to do this as well, and I’m keeping the idea on the back burner. (The front burners are already occupied: I’ve just launched a cookbook club this fall, which is wonderful but a lot of work. If only we had more staff…)","Before you head to one of ElectroRecycle’s 230+ collection sites across B.C. to recycle a broken small appliance or power tool – visit a Repair Cafe first!\nWhat is a Repair Cafe?\nRepair Cafes are community gatherings where visitors bring in broken household items to be fixed for FREE, or for a small donation. At the event, knowledgeable volunteers give advice and help to fix things like small appliances, power tools, furniture, electronics, bicycles, jewelry, decorations, clothing, and toys. When you visit a Repair Cafe, you and an expert repairer will roll up your sleeves to troubleshoot the fix together. It’s an ongoing learning process.\nWhy is a Repair Café worth visiting?\nRepair cafes help to reduce the amount of waste in the environment. Rather than throwing away a broken item, you can bring it to the cafe and have it fixed instead. This helps to extend the life of the item and prevent it from ending up in a landfill.\nRepairing an item can be cheaper than buying a new one. Visiting a repair cafe can help you save money on repairs and reduce your overall expenses.\nLearn new skills\nRepair cafes often have volunteers who are skilled at fixing a variety of items. By bringing your broken item to the cafe, you can learn how to fix it yourself and gain new skills that you can use in the future.\nMeet new people\nRepair cafes are community spaces where people can come together and work on fixing items. It’s a great opportunity to meet new people and connect with others who are passionate about reducing waste and protecting the environment. For example – at the North Saanich Repair Cafe, each event comes together through the help of an average of 18 volunteers. North Saanich’s volunteers are event and volunteer coordinators, set-up and take-down crews, website experts, email responders, data sorters and managers, advertisers, administrators, community facilitators, parts shoppers, and of course – expert fixers!\nWhat is the likelihood the item will be fixed?\nLets take a look at some real life data from the North Saanich Repair Cafe, which happens once a month on Vancouver Island. In early 2023, ElectroRecycle partnered with the Repair Cafe to collect items that couldn’t be fixed and offer an extra recycling location for the community.\nThe Repair Cafe saw fixes to things like vacuums, sewing machines, a number of DVDs and lamps, lots of dull blades, a portable heater, and a couple of clocks as well as clothing and jewelry. The more unusual fixes were a compost bucket (to make compost tea) and a school’s paper cutter.\nSince 2020, more than 1000 different items have been presented for repair at the North Saanich Repair Cafe. And on-average 67 – 75% of the items are fixed by their amazing volunteers!\nThis number is consistent with numbers found by other Repair Cafes as well (like the Port Coquitlam Repair Cafe) which reports that approximately 70% of their items brought in for repair go home fixed!\nElectroRecycle’s role at Repair Cafés\nOur recycling program’s main objective is to keep small appliances and power tools out of landfill. When possible, we attend alongside Repair Cafes to promote repair and collect small appliances and power tools beyond repair for recycling. In addition, we aim to spread awareness about the variety of electrical items (400+ different types!) that our program accepts. Above all, we want British Columbians to know how to recycle these items, ensuring proper disposal at their end-of-life.\nB.C. Repair Cafes in 2023\nHere’s a round-up of Repair Cafe’s we’ve heard are happening this year in British Columbia. Additionally, a database of Repair Cafes being held globally can be found at RepairCafe.org.\nPlease note that ElectroRecycle is NOT the host of these Repair Cafes, and ElectroRecycle may be in attendance at some but not all of the below listed Repair Cafes to collect recycling. Remember to check our events page regularly to discover the Repair Cafes we are attending and other collection events we have scheduled.\nHowe Sound Repair Cafes\nSquamish Repair Cafe – Squamish Repair Cafe info, Dates TBD.\nInterior Repair Cafes\nKamloops Repair Cafe – Hosted by Transition Kamloops. Saturday June 17 at Kamloops Yacht Club (1140 River Street, Kamloops). Check the Kamloops Repair Cafe Facebook page for additional dates.\nKelowna Repair Cafe – Hosted by the Regional District of Central Okanagan (RDCO), check RDCO’s website for dates.\nSalmon Arm Repair Cafe – Located at and hosted by Shuswap Makerspace, 220 Shuswap St., NE. Follow Shuswap Makerspace on Facebook for dates.\nKootenays Repair Cafes\nFernie Heritage Library (FHL) Repair Cafes – Hosted by the FHL Makerspace, check FHL Makerspace’s webpage for date announcements.\nThe STEAM Truck – Based in the West Kootenays, the STEAM truck is available for any educational hands-on activity with a focus on Science, Technology, Engineering, Arts and Math. It’s event initiatives include pop-up Repair Cafes so follow its Facebook page or visit thesteamtruck.ca to keep up to date on it’s latest plans.\nKimberley Fix it Fair – Hosted by Wildsight’s Youth Climate Corps. Over time Beer Works Patio. Saturday June 17. 12pm – 3pm. Event Info Here.\nLower Mainland Repair Cafes\nMaple Ridge Repair Cafe – Locations vary. Dates: June 17, July 15, September 16, October 21, November 18. Check the Ridge Meadows Recycling Society website for more Info\nMission Repair Cafe (BRIM) – Located at the Mission Library, 33247 2nd Ave, Mission. Dates: Third Saturday of each month (except December). 10:30am – 1:30pm. More Info on BRIM’s website and Facebook Page.\nPort Coquitlam Repair Cafe – Port Coquitlam Public Works Yard (1737 Broadway Street – entrance off of Cameron Ave). First Saturday of each month. 10am – 2pm. More Port Coquitlam Repair Cafe Info\nVancouver Repair Cafes – Locations vary, organized by the Society Promoting Environmental Conservation (SPEC), check SPEC’s website for more information. Dates: June 17, July 22, and more dates TBD. Pre-registration highly recommended.\nWhite Rock/ South Surrey Repair Cafe – Located at Alexandra Neighborhood House, Crescent Beach, Surrey. Monday, October 9th and Sunday, November 26. 1pm – 4pm. More Info\nSunshine Coast Repair Cafes\nSechelt Repair Cafe – Located at Sechelt Library. Second Saturday of the month. 10:30am – 2:30pm. More Info\nGibsons Repair Cafe – First and last Saturdays of the month. More Info\nVancouver Island Repair Cafes\nCampbell River Repair Cafe – Visit the City of Campbell River Recreation’s Facebook page for event updates. Occasional events are held but not on a set schedule.\nFairfield Repair Cafes – Located at Fairfield Gonzales Community Hall, 1330 Fairfield Road, Victoria. October 14, 2023. 2pm – 4pm. Website Info, Facebook Event Info\nCourtenay – Lake Trail Community Education Society has held Repair Cafes in the past and may have upcoming ones. Visit their website for event updates.\nMetchosin Repair Cafe – Check the Metchosin Arts & Cultural Centre Association’s event page for future dates.\nNorth Saanich Repair Cafes – Located at St John’s United Church, 10990 W Saanich Rd, North Saanich. Monthly (except for December when there is no Cafe). 10am – 1pm. More Info\nSooke Repair Cafes – TBD. Hosted by Zero Waste Sooke, check their Facebook page for updates\nLet us know if you’d like a Repair Cafe in your community added to this list or would like us to attend your upcoming Repair Cafe in British Columbia. Please contact us via social media or email at email@example.com."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:7f92f9e2-93fa-401b-a0df-642efc7af52b>","<urn:uuid:96ea539f-b632-4ba0-82dd-0af15f9195af>"],"error":null}
{"question":"I'm researching educational tech transformation cases - what digital learning infrastructure did the University of Iceland implement pre-COVID that helped during remote teaching? Such as LMS etc","answer":"Before COVID-19, the University of Iceland had implemented several key digital learning tools: they joined edX (the online course platform created by Harvard and MIT), introduced a new learning management system, and established a digital exam platform. These pre-pandemic investments in IT and electronic teaching infrastructure enabled them to effectively handle the transition to online learning when the university closed due to COVID-19.","context":["Jan 2021 Interview with Jón Atli Benediktsson, Rector of the University of Iceland, Iceland\nPrisma Reports (PR): As rector since 2015, please tell us about the evolution of the university during your time at the helm and its major milestones and achievements?\nJón Atli Benediktsson (JB): The University of Iceland was established in 1911 and is the oldest and largest university in the country with about 15,000 students – some 70% of all university students in Iceland. We are an international research university and have five schools covering almost all subjects: humanities, social sciences, engineering and natural sciences, health sciences and education. We have all levels of degrees: bachelor’s, master’s and PhD.\nIn the recent past we have focused strongly on research and innovation, and awarded doctoral degrees have increased substantially. The university plays a vital role in Icelandic society, being a large and established institution in the Icelandic context. Because Iceland is small, a student body of 15,000 individuals is 5% of the population, so is significant.\nOur 2016-2021 strategic plan set high goals, called UI 21. The plan has four main chapters: teaching, research, service to society and human resources. We strive for excellence with respect to all our resources and have been successful in implementing this plan, which is an important achievement.\nPR: Please tell us more about this strategic plan and the doors it opened for the university?\nJB: There was a big focus on teaching and learning, in particular digital means for teaching. For example, the University of Iceland was invited to join edX – an American massive open online course provider created by prestigious education providers Harvard and the Massachusetts Institute of Technology (MIT) – during this period which is also a recognition of our accomplishments. Being a partner of edX also helped us strengthen our international contacts, which is vital.\nIt was an important milestone for the university to join edX, but even more importantly, we put a lot of effort into improving the teaching environment with the introduction of a new learning management system and a digital exam platform. When the university closed due to COVID-19, we were able to handle the online situation. If we had not invested significantly in IT and an electronic teaching environment, our situation would have been dire.\nThe University of Iceland also took an important step in international collaboration when we joined the Aurora Network of Universities – a network of nine strong research universities that have shown great social responsibility in their countries. We joined the Aurora Network as a founding member in 2016 and recently the Aurora Network with a few other universities under the name of Aurora Alliance, were selected as a European University Alliance.\nThis initiative is a key pillar of the European Education Area which attempts to strengthen the global competitiveness of European universities through the joining of forces across international borders.\nPR: You also operate a science park so how is that helping your students learn key new skills?\nJB: The science park is a community of university partners and partners from industry. The science park is important in terms of innovation, has been growing and has already attracted important companies. The first company established in the area was deCode Genetics, a pivotal player in biotech and genetic research; they are a global leader in their field.\nThen we have a pharmaceutical company, Alvotech, and a computer gaming company, CCP Games. We also have startup incubators located in the science park. We work closely with all the companies located in the science park as they are a part of the university community.\nPR: Tell us a bit more about your strategy to continue fostering symbiotic relationships with international institutions and plans to expand this network?\nJB: We have more than 400 agreements with partner universities and continue to be open towards agreements with universities in other nations because international collaborations are extremely important. We particularly like to work with top universities, with many of whom we now have agreements.\nMost of our partner universities are in Europe, but we have seen growth in the last few years in agreements with universities in Asia and the South Pacific, especially Japan and China, but also places like Singapore and Australia. We are always open to partnerships in new regions.\nPR: The University of Iceland is known as a research focused university. Researchers have conducted studies on various aspects of the pandemic. Can you tell us more about this?\nJB: There are several, but the best known here in the Icelandic society is the prediction model that the government has been using for estimating the number of COVID-19 infections. That model came from research by the University of Iceland, statisticians and public health experts.\nIn collaboration with the National University Hospital of Iceland, which is not a part of the university but a very close collaborator – in my words the largest lecture room of the university – our researchers have designed an algorithm to design the flow of how patients are treated.Overall, we have more than 40 research and innovative projects on COVID-19 at the University of Iceland. Also, deCode Genetics, which is a very close collaborator of ours, has been a very significant player here in testing and analysis of the pandemic, and has published several important articles with the University of Iceland co-authorship on the pandemic.\nPR: Any final words for readers of Foreign Policy?\nJB: We welcome international collaboration and international students at the University of Iceland. Our country is an exciting place to visit so I would encourage people to look towards us in the future, whether as potential faculty or staff, or as potential student or a visitor. We strive to be successful."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:be99680c-88cb-4dea-a366-2b7697a638cd>"],"error":null}
{"question":"How do knowledge deficiencies impact both religious understanding and vitamin absorption?","answer":"Knowledge deficiencies can significantly impact both religious understanding and vitamin absorption. In religious contexts, lack of proper knowledge can lead to misdirected zeal and incorrect religious practices, as demonstrated by historical examples where people had fervent religious devotion but lacked proper understanding of their faith. Similarly, in health contexts, lack of knowledge about vitamin D can lead to deficiency, as people may be unaware of the importance of sun exposure, proper diet, and supplements for maintaining adequate vitamin D levels, which is crucial for calcium absorption and overall health.","context":["For I bear them record that they have a zeal of God, but not according to knowledge.\nJump to: Alford • Barnes • Bengel • Benson • BI • Calvin • Cambridge • Chrysostom • Clarke • Darby • Ellicott • Expositor's • Exp Dct • Exp Grk • Gaebelein • GSB • Gill • Gray • Haydock • Hastings • Homiletics • ICC • JFB • Kelly • KJT • Lange • MacLaren • MHC • MHCW • Meyer • Newell • Parker • PNT • Poole • Pulpit • Sermon • SCO • Teed • TTB • VWS • WES • TSK\nEXPOSITORY (ENGLISH BIBLE)A zeal of God, but not according to knowledge.—It would be difficult to find a more happy description of the state of the Jews at this period. They had “a zeal for God.” “The Jew,” said Josephus, “knows the Law better than his own name . . . The sacred rules were punctually observed . . . The great feasts were frequented by countless thousands . . . Over and above the requirements of the Law, ascetic religious exercises advocated by the teachers of the Law came into vogue. . . . Even the Hellenised and Alexandrian Jews under Caligula died on the cross and by fire, and the Palestinian prisoners in the last war died by the claws of African lions in the amphitheatre, rather than sin against the Law. What Greek,” exclaims Josephus, “would do the like? . . . The Jews also exhibited an ardent zeal for the conversion of the Gentiles to the Law of Moses. The proselytes filled Asia Minor and Syria, and—to the indignation of Tacitus—Italy and Rome.” The tenacity of the Jews, and their uncompromising monotheism, were seen in some conspicuous examples. In the early part of his procuratorship, Pilate, seeking to break through their known repugnance to everything that savoured of image-worship, had introduced into Jerusalem ensigns surmounted with silver busts of the emperor. Upon this the people went down in a body to Cæsarea, waited for five days and nights in the market-place, bared their necks to the soldiers that Pilate sent in among them, and did not desist until the order for the removal of the ensigns had been given. Later he caused to be hung up in the palace at Jerusalem certain gilded shields bearing a dedicatory inscription to Tiberius. Then, again, the Jews did not rest until, by their complaints addressed directly to the emperor, they had succeeded in getting them taken down. The consternation that was caused by Caligula’s order for the erection of his own statue in the Temple is well known. None of the Roman governors dared to carry it into execution; and Caligula himself was slain before it could be accomplished.\nJustice must be done to the heroic spirit of the Jews. But it was zeal directed into the most mistaken channels. Their religion was legal and formal to the last degree. Under an outward show of punctilious obedience, it concealed all the inward corruption described by the Apostle in Romans 2:17-29, the full extent of which was seen in the horrors of the great insurrection and the siege of Jerusalem.Acts 26:5; Philippians 3:5, and he well knew the extraordinary exertions which they put forth to obey the commands of the Law.\nA zeal of God - A zeal for God. Thus, John 2:17,\" The zeal of thine house hath eaten me up;\" an earnest desire for the honor of the sanctuary has wholly absorbed my attention; compare Psalm 69:9; Acts 21:20, \"Thou seest, brother, how many thousands of Jews there are which believe, and they are all zealous of the law;\" Acts 22:3, \"And was zealous toward God as ye all are this day.\" Zeal for God here means passionate ardor in the things pertaining to God, or in the things of religion. In this they were, doubtless, many of them sincere; but sincerity does not of itself constitute true piety; John 16:2, \"The time cometh that whosoever killeth you will think that he doeth God service.\" This would be an instance of extraordinary zeal, and in this they would be sincere; but persecution to death of apostles cannot be true religion; see also Matthew 23:15; Acts 26:9, \"I thought that I ought to do,\" etc. So many persons suppose that, provided they are sincere and zealous, they must of course be accepted of God. But the zeal which is acceptable is what aims at the glory of God, and which is founded on true benevolence to the universe; and which does not aim primarily to establish a system of self-righteousness, as did the Jew, or to build up our own sect, as many others do. We may remark here, that Paul was not insensible to what the Jews did, and was not unwilling to give them credit for it. A minister of the gospel should not be blind to the amiable qualities of people or to their zeal; and should be willing to speak of it tenderly, even when he is proclaiming the doctrine of depravity, or denouncing the just judgments of God.\nNot according to knowledge - Not an enlightened, discerning, and intelligent zeal. Not what was founded on correct views of God and of religious truth. Such zeal is enthusiasm, and often becomes persecuting. Knowledge without zeal becomes cold, abstract, calculating, formal; and may be possessed by devils as well as human beings. It is the union of the two - the action of the man called forth to intense effort by just views of truth and by right feeling - that constitutes true religion. This was the zeal of the Saviour and of the apostles.\nthat they have a zeal of—\"for\"\nGod, but not according to knowledge—(Compare Ac 22:3; 26:9-11; Ga 1:13, 14). He alludes to this well-meaning of his people, notwithstanding their spiritual blindness, not certainly to excuse their rejection of Christ and rage against His saints, but as some ground of hope regarding them. (See 1Ti 1:13).For I bear them record, i.e. I must testify this of them, or of many of them,\nthat they have a zeal of God; that they have a fervent desire to maintain the law of God, with all the Mosaical rites and ceremonies, as thinking thereby to promote the glory of God.\nBut not according to knowledge; i.e. true and right knowledge. Though it be a warm, yet it is a blind zeal. They know not the will of God, or what that righteousness is which he will accept. They know not for what end the law and worship of God, under the Old Testament, was instituted. They knew not that Christ, in, and by whom, that law is fulfilled.\nbut not according to knowledge: it was not well regulated, it proceeded on mistaken principles, and moved in a wrong way, in persecuting the church of God, in doing things contrary to the name of Christ, in putting to death his ministers and members, thinking that hereby they did God good service; which arose from their ignorance of their Father, and of the Son: though they had a zeal of God, they knew neither God nor Christ aright; they did not know God in Christ, nor Jesus to be the true Messiah; they understood neither law nor Gospel truly, and fancied the Gospel was contrary to the law, and an enemy to it; and therefore in their great zeal opposed it, and the professors of it; they were zealous of the law, and of doing the commands of it, but knew not the true nature, use, and end of the law; as appears by what follows.For I bear them record that they have a zeal of God, but not according to knowledge.\nEXEGETICAL (ORIGINAL LANGUAGES)Romans 10:2. Reason assigned why ἡ εὐδοκία … εἰς σωτηρίαν.\nζῆλον Θεοῦ] zeal for God. Comp. Acts 21:20; Acts 22:3; Galatians 1:14; John 2:17; 1Ma 2:58. This their zeal makes them worth that interest of my heart.\nοὐ κατʼ ἐπίγνωσιν] knowledge is not that, according to the measure of which they are zealous for God. We must here again (comp. on Romans 1:28) note the composite expression; for the Jews were not wanting in γνῶσις generally, but just in the very point, on which it depended whether their γνῶσις was the right and practically vital ἐπίγνωσις.Romans 10:2. Their good qualities compel his affection. ζῆλον θεοῦ ἔχουσιν: they have a zeal for God, are intensely (though mistakenly) religious. Cf. Galatians 1:14. An unbelieving Jew could interpret his opposition to the lawless gospel of Paul as zeal for the divinely-given rule of life, and his opposition to the crucified Messiah as zeal for the divinely-given promises. It was God’s honour for which he stood in refusing the Gospel. ἀλλʼ οὐ κατʼ ἐπίγνωσιν: this religious earnestness is not regulated by adequate knowledge. For ἐπίγνωσις see Ephesians 4:13, Php 1:9, Colossians 1:9-10; Colossians 2:2, 1 Timothy 2:4, 2 Timothy 2:25; it is especially used of religious knowledge, and suggests attainment in it (ἄρτι γινώσκω ἐκ μέρους, τότε δὲ ἐπιγνώσομαι, 1 Corinthians 13:12).2. For] The connexion is, that they seem to be, but are not, in the way to salvation; and that this stirs up his affectionate and anxious longing that they may find it.\nrecord] witness; as one who so intimately knows them and their state of conscience and will.\nzeal of God] So lit. The genitive implies that the zeal is in close connexion with, and directed towards, Him. So “faith of God” (Gr. of Mark 11:22). Jewish jealousy for the Law, Temple, Scriptures, &c.—eagerness to proselytize—hatred of Christian renegades—is all implied here; all being connected, rightly or mistakenly, with the true God, and intended, more or less, to “do Him service.”—Observe that St Paul gives them full credit for sincerity, and yet does not look on their sincerity as a ground of safety. His true generosity had in it no false “liberalism.” The Jews (like himself of old, 1 Timothy 1:13,) acted “ignorantly in unbelief;” but their “ignorance,” in face of offered knowledge, was their crime; and so their misguided zeal was indirectly sinful.\nknowledge] Lit. full knowledge. (German, Erkenntniss.) Same word as Romans 1:28, Romans 3:20. The word is appropriate, for it was just the full knowledge of the true God as God in Christ which they lacked. Their knowledge of God impelled them to persecuting zeal exactly because it was not full knowledge. (See Acts 13:27.) So it is with all persecution in the name of the true God.Romans 10:2. Ζῆλου Θεοῦ, a zeal of God) Acts 22:3, note. Zeal of God, if it is not against Christ, is good.—οὐ κατʼ ἐπίγνωσιν, not according to knowledge) An example of Litotes [expressing in less strong terms a strong truth] i.e. with great blindness; it agrees with the word, ignorant, in the next verse. Flacius says: The Jews had and now have a zeal without knowledge; we on the contrary, alas! to our shame, have knowledge without zeal. Zeal and ignorance are referred to at Romans 10:19.\n Γὰρ, for.) Therefore even in those, who are not in a state of grace, something at least may be found which may induce those, who rejoice in the Divine favour, to intercede for them.—V. g.Verses 2, 3. - For I bear them record that they have a zeal of God. For ζῆλον Θεοῦ, meaning zeal for God, cf. John 2:17; Acts 22:3; Galatians 1:14. The word ζῆλος was commonly used for the religious ardour of the Jews at that time (cf. Acts 21:20, Πάντες ζηλωταὶ τοῦ νόμου ὑπάρχουσι), and there was a faction among them called distinctively Ζηλωταὶ, to which Simon Zelotes (Luke 6:15; Acts 1:13) is supposed to have belonged originally. St. Paul's mention of the religious zeal of the Jews of his day is apposite in this place. In Romans 9:1-5, where he was about to speak of their rejection from the inheritance of the promises, he appropriately dwelt on their ancient privileges; here, where he has in view their own failure to respond to God's purpose for them, he as appropriately refers to their undoubted zeal, which he regrets should be misdirected. But not according to knowledge. For being ignorant of (ἀγνοοῦντες, in explanation of οὐ κατ ἐπίγνωσιν preceding) God's righteousness, and seeking to establish their own (righteousness, repeated here, is ill supported), they have not submitted themselves to the righteousness of God. For the meaning of God's righteousness, opposed to man's own righteousness, see on Romans 3:19, 20; also on Romans 1:17, and Introduction.\nRev. witness. \"He seems to be alluding to his conduct of former days, and to say, 'I know something of it, of that zeal'\" (Godet).\nZeal of God (ζῆλον Θεοῦ)\nLinksRomans 10:2 Interlinear\nRomans 10:2 Parallel Texts\nRomans 10:2 NIV\nRomans 10:2 NLT\nRomans 10:2 ESV\nRomans 10:2 NASB\nRomans 10:2 KJV\nRomans 10:2 Bible Apps\nRomans 10:2 Parallel\nRomans 10:2 Biblia Paralela\nRomans 10:2 Chinese Bible\nRomans 10:2 French Bible\nRomans 10:2 German Bible","Vitamin D is an essential nutrient that is crucial for our overall health and well-being. It plays a vital role in maintaining strong bones, boosting immunity, regulating the absorption of calcium and phosphorus, and protecting against diseases. Despite its importance, many people suffer from vitamin D deficiency, which can lead to a range of health problems. In this article, we will discuss the symptoms, causes, and treatments of vitamin D deficiency.\nWhy is Vitamin D so Important?\nVitamin D is a crucial nutrient for maintaining good health and well-being. Despite its importance, many people are deficient in this nutrient, which can lead to a range of health problems. In this article, we will explore why vitamin D is so important and what role it plays in our bodies.\nRegulates Calcium and Phosphorus Absorption\nOne of the most important functions of vitamin D is to regulate the absorption of calcium and phosphorus, two minerals that are essential for strong bones. Vitamin D helps the body absorb these minerals from the gut and distribute them to the bones, where they are stored. If the body doesn’t get enough vitamin D, calcium and phosphorus levels may drop, leading to weak and brittle bones, a condition known as osteomalacia.\nBoosts Immune System\nVitamin D plays a crucial role in boosting the immune system, helping the body fight off infections and diseases. The nutrient activates the immune system’s T-cells, which are responsible for recognizing and destroying harmful pathogens. It also helps regulate the production of cytokines, proteins that help coordinate the immune response.\nReduces Risk of Chronic Diseases\nStudies have shown that vitamin D may help reduce the risk of chronic diseases, such as heart disease, diabetes, and certain types of cancer. The nutrient is believed to play a role in regulating cell growth and division, and may also help protect against oxidative stress and inflammation, two factors that contribute to the development of chronic diseases.\nImproves Mood and Mental Health\nVitamin D is also important for maintaining good mental health and a positive mood. The nutrient is involved in the production of neurotransmitters, such as serotonin, which play a role in regulating mood and emotions. Low levels of vitamin D have been linked to depression, anxiety, and other mental health conditions.\nSymptoms of Vitamin D Deficiency\nVitamin D deficiency can present itself in a number of ways, and the symptoms may not be obvious at first. Some of the most common signs of vitamin D deficiency include:\n- Fatigue and tiredness\n- Muscle weakness\n- Pain in bones and muscles\n- Depression and mood swings\n- Decreased bone density\n- Impaired wound healing\n- Increased risk of infections\nCauses of Vitamin D Deficiency\nThere are several factors that can contribute to vitamin D deficiency, including:\n- Lack of sun exposure: Vitamin D is produced by the body when the skin is exposed to the sun’s UV rays. If you spend most of your time indoors or live in a place with limited sun exposure, you may be at risk of vitamin D deficiency.\n- Dark skin: Melanin, the pigment that gives skin its color, reduces the skin’s ability to produce vitamin D from sun exposure. This means that people with darker skin are at a higher risk of deficiency.\n- Aging: As we age, our skin becomes less efficient at producing vitamin D, which can lead to a deficiency.\n- Obesity: Vitamin D is stored in fat, and obese individuals may have lower levels of vitamin D in their body.\n- Malabsorption: Certain medical conditions, such as Crohn’s disease and celiac disease, can interfere with the body’s ability to absorb vitamin D, leading to a deficiency.\nTreatments for Vitamin D Deficiency\nThe treatment for vitamin D deficiency depends on the severity of the deficiency and the underlying cause. Here are some of the most common treatments:\n- Sun exposure: Spending time in the sun is the best way to get vitamin D. When your skin is exposed to the sun’s UV rays, it produces vitamin D. Aim to spend 10-15 minutes in the sun each day, especially between 10 AM and 3 PM, when the sun’s rays are the strongest.\n- Supplements: If you can’t get enough vitamin D from food and sun exposure, consider taking a supplement. Vitamin D supplements are available in different forms, including tablets, liquids, and gummies. Your doctor can recommend a supplement that’s right for you.\n- Diet: Fatty fish, such as salmon, mackerel, and tuna, are good sources of vitamin D, as are egg yolks and mushrooms. Some dairy products, such as milk and yogurt, are fortified with vitamin D, and can help increase your levels of the nutrient.\n- Fortified foods: Many foods, such as breakfast cereals and dairy products, are fortified with vitamin D, which can help increase your levels of the nutrient.\nIn conclusion, vitamin D deficiency can lead to a range of health problems, including bone weakness, fatigue, and depression. If you suspect you have a deficiency, it’s important to speak with your doctor, who can diagnose and treat the problem. By following a healthy diet, getting regular sun exposure, and taking vitamin D supplements, you can help keep your levels of the nutrient in check and maintain good health.\n- What are the symptoms of Vitamin D deficiency?\nThe symptoms of Vitamin D deficiency can include fatigue, muscle weakness, joint and muscle pain, and depression. In severe cases, Vitamin D deficiency can also lead to osteoporosis and increased risk of fractures.\n- What causes Vitamin D deficiency?\nVitamin D deficiency can be caused by a number of factors, including a lack of exposure to sunlight, a diet that is low in Vitamin D, certain medical conditions that limit the body’s ability to absorb Vitamin D, and certain medications that can interfere with Vitamin D metabolism.\n- How is Vitamin D deficiency treated?\nVitamin D deficiency is usually treated with supplements, either in the form of oral tablets or injections. The recommended daily dose and duration of treatment will depend on the individual’s unique needs and will be determined by their healthcare provider. It is also important for individuals with Vitamin D deficiency to make lifestyle changes, such as increasing their exposure to sunlight and consuming a diet that is rich in Vitamin D, to help prevent future deficiencies."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:92fb137a-90d2-42f1-9f30-66c880d11b52>","<urn:uuid:7b91b940-5deb-488e-a808-a8af0db6f6c0>"],"error":null}
{"question":"How do Malaysian scaffolding regulations handle violations, and what are the typical consequences of safety breaches in construction?","answer":"Malaysian scaffolding regulations handle violations through various penalties including stop-work orders, fines, business license revocation, and criminal prosecution for injuries or deaths caused by non-compliance. Common violations include inadequate design or construction of scaffolding platforms, lack of proper fall protection equipment, insufficient maintenance and inspection, use of untrained workers, and use of substandard materials. These violations can result in serious consequences including project delays, additional costs for employers, and enforcement actions by DOSH. Similarly in the construction industry broadly, safety breaches can lead to fatal and non-fatal injuries, with statistics showing high rates of incidents including falls from height, being trapped by collapsing structures, and being struck by objects, as well as long-term health conditions like COPD and occupational deafness.","context":["The introduction focuses on Malaysian scaffolding and emphasises the value of adhering to the rules set forth to guarantee worker safety on building sites.\nConstruction and maintenance projects require scaffolding, which can be hazardous if improperly erected and maintained. Since workers who utilise scaffolding must be protected from accidents and injuries, Malaysia has strong rules in place.\nAfter that, the introduction goes on to say that the essay will offer a thorough explanation of how to adhere to Malaysia's scaffolding requirements. The manual will go over the standards and recommended procedures for guaranteeing security and avoiding penalties.\nEmployers can guarantee the safety of their workers when utilising scaffolding by adhering to the rules and best practices mentioned in the article. This not only shields employees from mishaps and injuries but also assists businesses in avoiding fines and other consequences for breaking the rules.\nOverall, the introduction emphasises the necessity of adhering to Malaysia's scaffolding standards and offers readers a guide to assist them in doing so while maintaining compliance and ensuring safety on construction sites.\nII. Recognizing Malaysia's Scaffolding Regulations\nMalaysian legislation and rules protect those who use scaffolding at work.\nThe Occupational Safety and Health Act (OSHA) of 1994 is the main law that governs workplace safety in Malaysia.\nEmployers must offer a safe working environment for all of their employees, including those who use scaffolding, according to OSHA regulations.\nOSHA regulations violations might result in penalties or legal action.\nThe organisation in charge of applying OSHA laws in Malaysia is the Department of Occupational Safety and Health (DOSH).\nThe Ministry of Human Resources oversees DOSH, which makes sure that employers follow OSHA's rules and regulations.\nTo safeguard the safety of workers who use scaffolding in their job, DOSH also publishes guidelines and standards of practice for scaffolding.\nOverall, the introduction emphasises how crucial it is to adhere to OSHA standards as well as the rules and codes of conduct established by the DOSH when using scaffolding in Malaysia. In order to avoid workplace accidents and injuries, businesses must put their employees' safety first.\nIII. Best Scaffolding Techniques in Malaysia\nWorkers must receive the appropriate training from their employers on how to use scaffolding, safety precautions, and hazard recognition.\nPersonal Protective Equipment (PPE): While working on the scaffold, employees must put on the proper PPE, such as safety harnesses, helmets, and safety shoes.\nFall Protection: To prevent workers from falling, employers must provide adequate fall protection equipment, such as safety nets or harnesses.\nEnsure that Only Trained and Authorized Personnel Use the Scaffold: Employers must safeguard the site to prevent unauthorised access to the scaffold.\nIV. Common Offenses and Sanctions\nThe cautions that violating Malaysia's scaffolding laws could result in serious penalties, such as fines or jail time. A stop-work order or closure order may be issued by the Department of Occupational Safety and Health (DOSH) until the infractions are corrected. Delays and additional expenses for the employer may result from this.\nList some of Malaysia's most frequent infractions of the scaffolding laws. These consist of:\nInadequate design or construction could result in an unsafe scaffolding platform, which could eventually fail or collapse.\nLack of proper fall safety equipment, such as harnesses and guardrails, results in this infringement when workers are not given access to them.\ninadequate maintenance and inspection: Scaffolding that hasn't been properly inspected and maintained may have structural problems or present other risks.\nIncompetent workers: Only trained and certified workers should install and disassemble scaffolding. Accidents and injuries can result from failing to do this.\nUse of subpar materials: The stability and safety of the scaffolding can be jeopardised if materials are used that do not adhere to the necessary safety standards.\nThese frequent infractions serve as a reminder of the value of adhering to the rules and industry standards to maintain safety on Malaysian building sites. Employers may safeguard their workforce and stay out of trouble with pricey fines and other penalties by avoiding these offences.\nV. Scaffolding Types\nScaffolding comes in a variety of forms, and each type is created to address a particular necessity. The following scaffolding styles are most frequently used in Malaysia:\nSeparate Scaffolding - This kind of scaffolding can be utilised for both interior and exterior construction projects because it is constructed separately from the building.\nScaffolding by Putlog - In putlog scaffolding, one side of the scaffolding is supported by the building's framework. Then, using putlogs or horizontal tubes supported by the building, the scaffolding is erected.\nScaffolding for bird cages - As the name implies, birdcage scaffolding is a style of scaffolding that resembles a birdcage. It is frequently employed for indoor tasks like painting or installing ceilings.\nFloating Scaffolding - On tall buildings, external construction is done using suspended scaffolding. It enables workers to reach confined spaces and is intended to dangle from the building's roof.\nScaffolding for cantilevers - Similar to suspended scaffolding, cantilever scaffolding is fixed to the building structure on one end as opposed to dangling.\nVI. Safety Requirements for Scaffolding\nWhen employing scaffolding, the public's and employees' safety should come first. Some crucial safety prerequisites that must be addressed include the following:\nInstallation of scaffolding requires knowledgeable, skilled workers.\nEvery piece of scaffolding needs to be checked both before and after each use.\nHard helmets, safety shoes, and harnesses are among the personal protection equipment (PPE) that employees are required to wear.\nThe base on which scaffolding is built must be sturdy and capable of supporting the necessary load.\nToeboards and guardrails must be put in place to avoid falls.\nScaffolding needs to be free of trash, equipment, and supplies.\nWorkers must avoid placing an excessive amount of weight on the scaffolding.\nVII. Regulatory Structure\nThe Factories and Machinery Act of 1967 and the Occupational Safety and Health Act of 1994 (OSHA 1994) are the main laws in Malaysia that control scaffolding regulations (FMA 1967). It is the duty of the Department of Occupational Safety and Health (DOSH) to enforce these laws.\nEmployers are obligated to provide a safe working environment, including scaffolding, in accordance with OSHA 1994. According to the statute, all scaffolding must be set up and utilised in a way that doesn't endanger people's health or safety.\nThe Malaysian Standard MS 1462:2006, which describes the design, installation, and use of scaffolding throughout the nation, is required compliance for all scaffolding under FMA 1967.\nPenalties for Failure to Comply\nRegulations regarding scaffolding violations may have substantial repercussions, such as:\norders to halt work\nPenalties and fines\nLicense revocation for businesses\nCriminal prosecution for harm or death brought on by non-compliance\nIn conclusion, scaffolding is an important component of the construction sector, and adhering to Malaysian rules is essential for both worker and public safety. You can stay in compliance with Malaysia's scaffolding laws thanks to the detailed information in this guide.\nWe discussed the most typical scaffolding types utilised in Malaysia, necessary safety precautions, and the legal framework. In order to allay any worries or confusion regarding scaffolding compliance, we also addressed some frequently asked questions.\nIf you work in the construction industry and want to scaffold, renting might be your best bet. Scaffolding alternatives are available from rental companies at reasonable pricing. By renting scaffolding, you may avoid the expense of buying it all together while still using high-quality equipment that complies with safety rules.\nKeep in mind that following scaffolding standards is not only required but can also have serious repercussions. You may prevent accidents and injuries and guarantee a safe working environment by adhering to the rules and making sure that all scaffolding is installed, maintained, and utilised correctly. In order to maintain safety on building sites, whether you choose to buy or rent scaffolding, be sure you are adhering to the rules.","Health and safety in the construction industry: what are the major risks?\nThe construction industry has the second-highest rate of fatalities across all industries in the UK according to the Health and Safety Executive (HSE) website statistics for 2019. Sadly, 30 people lost their lives during 2019 due to incidents on the construction site. With another 54,000 non-fatal injuries reported across the year, there is clearly a way to go when managing health and safety in construction to prevent further deaths and long-term conditions.\nThe construction industry is the European Union’s (EU) largest industry employer. A workforce of 18 million contributes to 9% of the EU’s gross domestic profit (GDP). This large and diverse workforce provides an invaluable service to society and the prosperity of each nation. So, why are there still so many incidents on construction sites and how can health and safety regulations work to protect our workers?\nTargeting the big five\nFive main categories of incidents are most common on construction sites:\n- Falls from height – which account for 49% of fatalities and 18% of non-fatal injuries (a number that is 10% higher than the statistics for all other industries)\n- Slips, trips or falls on the same level – accounting for 25% of non-fatal injuries\n- Being trapped by something collapsing or overturning – responsible for 14% of fatalities\n- Being struck by a moving, falling or flying object – the cause of 10% of fatalities and 12% of non-fatal injuries\n- Injured while handling, lifting or carrying – making up 20% of non-fatal injuries\nBesides these main issues, construction workers are at an increased risk from other serious and life-changing conditions. Conditions such as chronic obstructive pulmonary disease (COPD), occupational deafness, hand-arm vibration syndrome (HAVS) and asbestosis are serious and life-altering conditions that can be caused, or aggravated, by conditions on-site.\nHealth and safety legislation should be at the core of all construction businesses and every project, it should be a top priority for senior managers and it should be embedded into every plan, decision and activity. Read on for ideas on how to manage health and safety in construction, to ensure that your staff are safe, give confidence to your clients, maintain a flawless health and safety record and save time and money through introducing safety measures.\nAssess and avoid\nRisk assessments should be the first stage of all construction projects. Health and Safety teams should be involved in early planning to ensure that the health of workers is embedded in the project.\nAvoidance is the first key step. Is an activity deemed as dangerous? Can it be carried out in an alternative way that would avoid the danger? Can working at height be removed altogether or can fabrication be carried out on the ground and the structure lifted to height for fixing? Are hazardous chemicals essential to work, or can alternatives be found?\nIf time constraints or pressure from third parties will impact on the health and safety of your staff, these issues should be flagged to all stakeholders. If the risk can’t be removed, then safe working conditions should be made available through the correct equipment or protocols. If the situation can’t be made safe, then the group must work to change the working parameters. Time or client pressures should never impact the safety of workers on site. No project is that important.\nHealth & Safety: Do you have the skills on-site?\nOnce all stakeholders have understood and accepted the risk parameters for a project, plans must be put in place to reduce these risks. The main aim, and the measure of success for all health and safety teams, is for a project to run with zero reported incidents.\nChecking the qualifications and experience of all workers and supervisors is key. Workers must be able to perform tasks confidently and competently to reduce the risk to themselves and others. Construction Skills Certification Scheme (CSCS) card checks are a must in the UK and a full log of each card, every contractor and their competency to carry out a task must be documented. Where gaps in knowledge or experience are identified, training should be provided or new staff must be brought in to complete the task. If this can’t be done, then the task must be changed to accommodate the experience of the workers on-site, even if this does add extra time or cost.\nEnsure that qualified Health, Safety and Environment (HSE) officers are known to all workers and that all staff know how and when to report a dangerous situation. Whether you have a dedicated HSE officer on-site, site office staff logging incidents or an app-based solution everyone can access, it should be easy for anyone to report an incident.\nMake protective equipment a habit\nOnce you have the right workers with the correct experience, they must have access to the correct personal protective equipment (PPE) to carry out the job safely. Every person on site should understand the risks to themselves and those around them and know which equipment they need to complete the task safely. Wearing the necessary equipment should become second nature and this only happens if everyone adheres to the rules, if the equipment is missing, there should be a quick and easy way for staff to request it.\nManagers should model the behaviours expected of their teams. Everyone should be free to stop work and demand equipment if it is not available and to request that others in their team do the same.\nHealth and safety regulations become a habit when the culture enables team members to express their views openly. This open and honest approach rewards workers who show behaviours that protect their own safety and those of their colleagues.\nThe right tools for the job\nThis theme carries through to the suitability of equipment too. All equipment, from pneumatic drills and saws to large earthmoving machinery, should be assessed for its suitability and safety. Does the equipment pose a risk to the worker? Can a different piece of equipment give the same result with lower risk? If we do have to accept a risk, does the equipment minimise the risk in the best way? What other checks do we need to ensure zero incidents whilst using this equipment?\nChecks don’t stop when the equipment is in use. Maintenance schedules are essential to ensuring the safety of equipment and safeguarding those using it. A robust service and maintenance routine should be developed, assigned to an individual or team and monitored throughout the project, with any checks or maintenance work documented.\nIdentify issues, make changes and demonstrate the impact\nSoftware really can be a health and safety professional’s best friend. A good software package will allow easy data collection from multiple team members in multiple locations. Everyone should be able to easily report a health and safety concern, record standard health and safety data and complete assessments. The best software allows for collection through app-based systems, enabling any smartphone to become a data collection device.\nOnce data is recorded, software should help you to easily identify hazards and other issues, show where changes were made, demonstrate the impact of these changes and allow for easy reporting to senior management. This comprehensive and closed-loop data flow is essential for demonstrating regulatory compliance and for providing evidence during inspections.\nBy moving to a continuous improvement culture, issues that are flagged and dealt with on one project can be carried through as health and safety requirements for the next project. This process ensures that the same problem isn’t repeated and that everyone learns from previous mistakes.\nThe PlanRadar app is the perfect tool for completing comprehensive digital health and safety assessments through straightforward templates. Fire assessments and other specific templates can be designed, saved and adapted for future projects. And, most importantly, where plants require no transmitting appliances, PlanRadar can be used in an offline mode, automatically syncing information as soon as the device is reconnected to a network. This functionality can be essential to COSHH assessments.\nWork hard, but rest hard too\nWhen time pressures are mounting and the threat of late fines are looming, it’s tempting to cut corners and push to get a project done, but this should never be at the expense of health and safety. If changing plans impact on people, equipment or procedures, then risk assessments should be re-evaluated.\nAlthough contractors should expect high standards of work and output from their workers, rest is important too. Workers need to be highly alert to their surroundings and to help keep everyone safe; this is especially important when working at height, with hazardous material or in confined spaces. Workers should be given adequate time to rest and relax and not be pushed to work overly long hours, even if they request the overtime. Distraction and tiredness lead to mistakes and they can be serious and even life-threatening.\nIf in doubt, shout\nBusinesses that excel at managing health and safety in construction have safety at the very core of their work and everyone on-site sees the benefit of the precautions, follows the rules and enforces them within their teams.\nCommunication is key here. When an individual understands why a rule is in place or why something has changed, they are more likely to comply and enforce compliance from others. When you have zero incidents during a project, reward the team for achieving this. If a near-miss has occurred, communicate this to the team, help them to understand why it happened and how further incidents can be avoided. Be clear about the changes you’ve made to make ensure the incident doesn’t occur again and how others can contribute to this adjustment.\nHealth and safety in the construction industry is the responsibility of everyone on site. Put simple procedures in place so that construction workers can quickly and easily report an issue and instil a culture of responsibility. If a trip hazard is identified, encourage workers to remove it. If someone is in danger, encourage others to act. Dealing with an issue when it arises can help reduce immediate incidents that may follow.\nHealth and safety rules should be repeated at daily briefings and reinforced throughout the day. There are many ways to remind workers of their health and safety obligations, perhaps through posters displayed around the site or reminders on software tools.\nSeveral of the bigger construction companies have powerful slogans to help educate and engage all team members on-site.\nBalfour Beatty’s award-winning golden rules highlight core expectations and they are simple and easy to remember:\nMeanwhile, Amey’s Target Zero encourages workers to speak out and talk about health and safety issues so that problems can be identified and dealt with quickly.\nWe have all the tools to achieve zero incidents and to make the construction site a safe place for all workers. By taking a thorough approach to identifying risk, developing a culture of open and honest communication and ensuring workers have access to the safest tools for the job, we can reduce injuries on-site and achieve zero incidents."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:439df54c-eb7d-4e59-91b0-ef719d79dc9d>","<urn:uuid:2aab1c69-8b42-4bc3-b9a1-8f308e254433>"],"error":null}
{"question":"How does the 10-minute cooking rule for fish compare to the guidelines for handling Salmonella-prone foods?","answer":"The 10-minute cooking rule for fish specifies cooking fish for 10 minutes per inch of thickness at 450 degrees, turning halfway through, with modifications for frozen or foil-wrapped fish. This is one specific method to reach the safe internal temperature of 145°F. In contrast, Salmonella prevention guidelines are more comprehensive, requiring not just proper cooking temperatures but also proper cleaning (washing hands and surfaces), avoiding cross-contamination (separate cutting boards), and proper chilling (refrigerating within 2 hours). While the 10-minute rule is specific to fish preparation, Salmonella prevention guidelines apply to all types of raw foods of animal origin.","context":["Salmonella Questions and Answers\n- What is Salmonella?\n- What is salmonellosis?\n- What are the symptoms of salmonellosis?\n- Are there long-term consequences?\n- How do people get salmonellosis?\n- What foods are most likely to make people sick?\n- Are chickens labeled \"Kosher,\" \"free-range,\" \"organic,\" or \"natural\" lower in Salmonella bacteria?\n- What is FSIS doing to prevent Salmonella contamination?\n- How can consumers prevent salmonellosis?\nSalmonella bacteria are the most frequently reported cause of foodborne illness. In order to reduce salmonellosis, a comprehensive farm-to-table approach to food safety is necessary. Farmers, industry, food inspectors, retailers, food service workers, and consumers are each critical links in the food safety chain. This document answers common questions about the bacteria Salmonella, describes how the Food Safety and Inspection Service (FSIS) of the U.S. Department of Agriculture (USDA) is addressing the problems of Salmonella contamination on meat and poultry products, and offers guidelines for safe food handling to prevent bacteria, such as Salmonella, from causing illness.\nQ. What is Salmonella?\nA. Salmonella is a gram-negative, rod-shaped bacilli that can cause diarrheal illness in humans. They are microscopic living creatures that pass from the feces of people or animals to other people or other animals.\nThe Salmonella family includes over 2,300 serotypes of bacteria which are one-celled organisms too small to be seen without a microscope. Two serotypes, Salmonella Enteritidis and Salmonella Typhimurium are the most common in the United States and account for half of all human infections. Strains that cause no symptoms in animals can make people sick, and vice versa. If present in food, it does not usually affect the taste, smell, or appearance of the food. The bacteria live in the intestinal tracts of infected animals and humans.\nSalmonella bacteria have been known to cause illness for over 100 years. They were discovered by an American scientist, Dr. Daniel E. Salmon.\n[Top of Page]\nQ. What is salmonellosis?\nA. Salmonellosis is an infection caused by the bacteria Salmonella. According to the Centers for Disease Control and Prevention (CDC), salmonellosis causes an estimated 1.4 million cases of foodborne illness and more than 400 deaths annually in the United States. The Surveillance Report from the Food Diseases Active Surveillance (FoodNet) for 2007, identified Salmonella as the most common bacterial infection reported.\nFoodNet is a collaborative project among CDC, the 10 Emerging Infections Program sites (EPIs), USDA, and the U.S. Food and Drug Administration (FDA). One of the objectives of FoodNet is to measure effectiveness of a variety of preventive measures in reducing the incidence of foodborne illness attributable to the consumption of meat, poultry, and other foods.\n[Top of Page]\nQ. What are the symptoms of salmonellosis?\nA. Although in some people salmonellosis could asymptomatic, most people experience diarrhea, abdominal cramps, and fever within 8 to 72 hours after the contaminated food was eaten. Additional symptoms may be chills, headache, nausea, and vomiting. Symptoms usually disappear within 4 to 7 days. Many people with salmonellosis recover without treatment and may never see a doctor. However, Salmonella infections can be life-threatening especially for infants and young children, pregnant women and their unborn babies, and older adults, who are at a higher risk for foodborne illness, as are people with weakened immune systems (such as those with HIV/AIDS, cancer, diabetes, kidney disease, and transplant patients).\n[Top of Page]\nQ. Are there long-term consequences?\nA. Persons with diarrhea usually recover completely, although it may be several months before their bowel habits are entirely normal. A small number of persons who are infected with Salmonella may develop pains in their joints, irritation of the eyes, and painful urination. This is called Reiter's syndrome. It can last for months or years and can lead to chronic arthritis that is difficult to treat.\n[Top of Page]\nQ. How do people get salmonellosis?\nA. Salmonella lives in the intestinal tract of humans and other animals, including birds. Salmonella is usually transmitted to humans by eating foods contaminated with animal feces. Salmonella present on raw meat and poultry could survive if the product is not cooked to a safe minimum internal temperature, as measured with a food thermometer. Salmonella can also cause foodborne illness (salmonellosis) through cross-contamination, e.g., when juices from raw meat or poultry come in contact with ready-to-eat foods, such as salads.\nFood may also become contaminated by the unwashed hands of an infected food handler who might or might not be showing symptoms. Salmonella can also be found in the feces of some pets, especially those with diarrhea. People can become infected if they do not wash their hands after contact with these feces. Reptiles are particularly likely to harbor Salmonella. People should always wash their hands immediately after handling a reptile, even if the reptile is healthy.\n[Top of Page]\nQ. What foods are most likely to make people sick?\nA. Any raw food of animal origin, such as meat, poultry, milk and dairy products, eggs, seafood, and some fruits and vegetables may carry Salmonella bacteria. The bacteria can survive to cause illness if meat, poultry, and egg products are not cooked to a safe minimum internal temperature as measured with a food thermometer and fruits and vegetables are not thoroughly washed. The bacteria can also contaminate other foods that come in contact with raw meat and poultry. Safe food handling practices are necessary to prevent bacteria on raw food from causing illness.\n[Top of Page]\nQ. Are chickens labeled \"Kosher,\" \"free-range,\" \"organic,\" or \"natural\" lower in Salmonella bacteria?\nA. FSIS does not know of any valid scientific information that shows that any specific type of chicken has more or less Salmonella bacteria than other poultry.\n[Top of Page]\nQ. What is FSIS doing to prevent Salmonella contamination?\nA. The Food Safety and Inspection Service is the public health regulatory Agency in the USDA responsible for the safety of the nation's commercial supply of meat, poultry and egg products. As part of this responsibility, FSIS issued the \"Pathogen Reduction; Hazard Analysis and Critical Control Point (PR/HACCP) Systems, Final Rule\" in 1996. This rule sets Salmonella performance standards for establishments slaughtering selected classes of food animals or those producing selected classes of raw ground products to verify that industry systems are effective in controlling the contamination of raw meat and poultry products with disease-causing bacteria, like Salmonella.\nFSIS inspectors make sure the establishments are meeting the standards by collecting randomly selected product samples and submitting them to an FSIS laboratory for Salmonella analysis. FSIS requires all plants to reduce bacteria by means of the PR/HACCP system.\n[Top of Page]\nQ. How can consumers prevent salmonellosis?\nA. Bacteria on raw foods of animal origin do not have to cause illness. The key to preventing illness at home, in a restaurant, at a church picnic, or anywhere else is to prevent the bacteria from growing to high levels and to destroy the bacteria through cooking to a safe minimum internal temperature. Follow these guidelines for safe food preparation:\nCLEAN: Wash Hands and Surfaces Often\n- Wash hands with warm soapy water for 20 seconds before and after handling food and after using the bathroom, changing diapers, and handling pets.\n- Wash utensils, cutting boards, dishes, and countertops with hot soapy water after preparing each food item and before you go on to the next item.\n- Consider using paper towels to clean kitchen surfaces. If you use cloth towels, wash them often in the hot cycle of your washing machine.\nSEPARATE: Don't Cross-contaminate\n- Separate raw meat, poultry, and seafood from other foods in your grocery shopping cart and in your refrigerator.\n- If possible, use one cutting board for fresh produce and a separate one for raw meat, poultry, and seafood.\n- Always wash cutting boards, dishes, countertops, and utensils with hot soapy water after they come in contact with raw meat, poultry, and seafood.\n- Never place cooked food on a plate that previously held raw meat, poultry, or seafood.\nCOOK: Cook to Safe Temperatures\nUse a clean food thermometer when measuring the internal temperature of meat, poultry, casseroles, and other foods to make sure they have reached a safe minimum internal temperature:\n- Cook all raw beef, pork, lamb and veal steaks, chops, and roasts to a minimum internal temperature of 145 °F as measured with a food thermometer before removing meat from the heat source. For safety and quality, allow meat to rest for at least three minutes before carving or consuming. For reasons of personal preference, consumers may choose to cook meat to higher temperatures.\n- Cook all raw ground beef, pork, lamb, and veal to an internal temperature of 160 °F as measured with a food thermometer.\n- Cook all poultry to a safe minimum internal temperature of 165 °F as measured with a food thermometer.\n- Stuffed poultry is not recommended. Cook stuffing separately to 165 °F.\n- Egg dishes, casseroles to 160 °F.\n- Fish should reach 145 °F as measured with a food thermometer.\n- Bring sauces, soups, and gravy to a boil when reheating.\n- Reheat other leftovers thoroughly to at least 165 °F.\nCHILL: Refrigerate Promptly\n- Keep food safe at home, refrigerate promptly and properly. Refrigerate or freeze perishables, prepared foods, and leftovers within 2 hours (1 hour if temperatures are above 90 °F).\n- Freezers should register 0 °F or below and refrigerators 40 °F or below.\n- Thaw food in the refrigerator, in cold water, or in the microwave. Foods should not be thawed at room temperature. Foods thawed in the microwave or in cold water must be cooked to a safe minimum internal temperature immediately after thawing.\n- Marinate foods in the refrigerator.\n- Divide large amounts of leftovers into shallow containers for quick cooling in the refrigerator.","|Cooked to perfection, fish is at its flavorful best and will be moist, tender and have a delicate flavor. In general, fish is cooked when its meat just begins to flake easily when tested with a fork and it loses its translucent or raw appearance. Like most foods, fish should be thoroughly cooked. The U.S. Food and Drug Administration (FDA) suggests cooking fish until it reaches an internal temperature of 145 degrees.|\nOne helpful guideline is the 10-minute rule for cooking finfish. Apply it when baking, broiling, grilling, steaming and poaching fillets, steaks or whole fish. (Do not apply the 10 minute rule to microwave cooking or deep frying.)\nPractice makes perfect and cooking fish properly is all in the timing. Here's how to use the 10-minute rule:\n- Measure the seafood product at its thickest point. If the fish is stuffed or rolled, measure it after stuffing or rolling.\n- At 450 degrees, cook it 10 minutes per inch thickness of the fish, turning the fish halfway through the cooking time. For example, a 1-inch fish steak should be cooked 5 minutes on each side for a total of 10 minutes. Pieces of fish less than 1/2-inch thick do not have to be turned over.\n- Add 5 minutes to the total cooking time if you are cooking the fish in foil or if the fish is cooked in a sauce.\n- Double the cooking time (20 minutes per inch) for frozen fish that has not been defrosted.\nWhole fish, whole stuffed fish, fillets, stuffed fillets, steaks and chunks of fish may be baked. Use pieces of similar size for even cooking. It's best to bake fish in a preheated, 450 degrees oven following the 10-minute rule; bake uncovered, basting if desired.\nTIP: For a quick and delicious dinner, bake fish on a bed of chopped vegetables. Try a mixture of onions, celery and carrots or a combination of mushrooms, onions and peppers.\nSteaks, whole fish, split whole fish and fillets lend themselves well to broiling. Place fish, one-inch thick or less, two to four inches from the heat source. Place thicker pieces five to six inches away. Baste frequently with an oil-based marinade. Using the 10-minute rule, cook on one side for half the total cooking time, basting once or twice, then turn the fish over to continue broiling and basting.\nThis technique lends itself well to meatier or steak fish such as salmon, halibut, swordfish, tuna and whole fish. Preheat an outdoor gas or electric grill. If using a barbecue grill, start the fire about 30 minutes before cooking. Let it burn until white hot then spread coals out in a single layer. Adjust the grill height to 4 to 6 inches above the heat.\nTo grill fish, a moderately hot fire is best for cooking seafood. Always start with a well oiled grid to prevent the delicate skin of the fish from sticking. Support more delicate pieces of fish in a hinged, fish-shaped wire basket for easier turning or handling.\nFrequently baste steaks and fillets while grilling to prevent them from drying out. Marinating fish an hour before grilling also helps keep it moist.\nUse indirect heat for whole fish by banking hot coals on either side of the barbecue or preheat gas or electric grill. Oil fish well and place in an oiled fish basket. Cook, covered, 10 to 12 minutes per inch of thickness, turning halfway through cooking time.\nUse a shallow dish to allow maximum exposure to the microwaves. Arrange fillets with the thicker parts pointing outward and the thinner parts, separated by pieces of plastic wrap, overlapping in the center of the dish. Cover dish with plastic wrap and vent by turning back one corner. Allow 3 minutes per pound of boneless fish cooked on high as a guide. Rotate the dish halfway through the cooking time. Rolled fillets microwave more evenly and are less likely to over-cook than flat fillets, which may have thin edges.\nPoach fish in simmering liquid such as fish stock, water with aromatic herbs/vegetables, or a mixture of wine and water. In a large skillet, saute pan or fish poacher, ring the liquid to a boil. Add the fish and return to boiling. Quickly reduce to a simmer-the liquid should barely bubble. Cover and begin timing the fish according to the 10-minute rule. The remaining liquid can be used to make a sauce for fish if desired.\nSautéing or Pan-frying\nAn excellent method for fillets and pan-dressed fish like trout, tilapia and catfish.\nTIP: Dip the fish into seasoned flour, cornmeal or bread crumbs just before sautéing. Heat a small amount of olive oil or butter in a skillet large enough to hold the fish. When the pan is very hot, place the fish into the skillet. Saute for half the total time as determined by the 10-minute rule, turn over and complete cooking.\nWhole fish, chunks, steaks and stuffed fillets steam well. To steam fin fish, fill a large sauce pan with one inch of water. Place the fish on a steamer rack and put the rack in the pan. The water should not exceed the height of the rack. Cover tightly and bring the water to a boil. Using the 10-minute rule, steam until thoroughly cooked.\nThis cooking method is a very fast technique, so it's important to have all ingredients in uniform size and ready for cooking.\nUsing a wok or large skillet, coat the bottom and sides with vegetable oil. Add the fish and stir-fry, tossing gently to coat on all sides, until about three quarters cooked, approximately two to four minutes. Remove to a warm platter.\nStir-fry a selection of sliced vegetables (i.e. carrots, onions, bell peppers, zucchini, and mushrooms) in addition to a light sauce if desired. Return the fish to the wok or skillet and cook one to two minutes more. Serve immediately.\nUse enough water to cover, plus 1 tbs. of salt. Boil medium size for 3-5 minutes or\njumbo 8-10 minutes. Depending on the size, it takes from 3 to 5 minutes to boil or steam 1 pound of medium size shrimp in the shell.\nUse enough water in a large pot to cover lobsters and add 1 tablespoon of salt. All cooking times are from the time the water begins boiling after lobsters are placed in pot.\nSize (lb) / Time\n- 1 - 1 ¼ 15 minutes\n- 1 ½ - 2 lb. 17-20 minutes\n- 2 – 3 lb. 20-24 minutes\n- 3 – 6 lb. 24-28 minutes\n- 6 – 7 lb. 28-30 minutes\n- 8 lb. And over 4 minutes per pound\nBake at 350 degrees"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:d8b89c01-5b86-428a-831d-5e7df5cd2e8a>","<urn:uuid:847b37a7-7925-4f96-8cba-b44e9eb54d01>"],"error":null}
{"question":"¿Qué impactos tiene el juicio de criminales de guerra en la justicia global, y cómo se relaciona esto con los efectos ambientales de las nuevas tecnologías en las comunidades?","answer":"The prosecution of war criminals impacts global justice by bringing perpetrators to account and establishing precedents, as seen in cases like the Eichmann trial where Israel sought justice for Holocaust crimes. Similarly, environmental justice is impacted by new technologies, as demonstrated by crypto-asset mining operations which can cause local noise and water impacts, electronic waste, and air pollution that disproportionately affect neighboring communities that are often already burdened with other environmental issues. Both situations raise questions about protecting vulnerable communities and ensuring accountability for harmful actions.","context":["Adolf Eichmann Trial Discussion Questions | Classroom Activities\n1. Why did Eichmann agree to be extradited to Israel to stand trial?\n2. Did Eichmann believe he had a reasonable chance of being found innocent by the Court?\n3. Once in Israel, why did Eichmann cooperate in the judicial process?\n4. Once the trial was underway and Eichmann realized that the Court was not likely to be sympathetic to his plight, why did he continue to cooperate?\n5. Why did the United Nations vote to accept Israel’s right to keep Eichmann and put him on trial?\n6. How might people with various backgrounds likely have viewed this trial?\n7. Was this a “show trial” — that is an activity designed more for public relations than to obtain justice? Were the results of the trial successful from either a criminal justice standpoint or a public relations standpoint?\n8. Is there any value in continuing to search for Nazi war criminals and bringing them to trial, when most are feeble old men who are unlikely to cause any more harm to anyone?\n9. If you had been a teacher in pre-war Germany and Adolf Eichmann had been in your class, what types of lessons/courses would you feel might have been effective in order to have affected how he participated in mass murder?\n10. Was Eichmann convincing in his argument that he was dispassionately just”following orders”? Had this argument been accepted, would Eichmann have been found not guilty? What was the defense strategy, and what were alternative strategies? Was there any possibility of Eichmann being found “not guilty”?\n11. Were these three judges capable of presiding over a fair trial? Why did the Israeli government seek to try Eichmann themselves rather than turn him over toan international court?\n12. How would the trial have turned out had Eichmann been trieda t Nuremberg?\n13. Discuss the sentence Eichmann received. Would there have been any value in having him sentenced to life imprisonment?\n14. Compare and contrast the differences between the values which we feel are important to those who live in a democracy and those values which were important to Eichmann and his fellow Nazis.\n15. What were some of the pros and cons relating to the Israel Court agreeing to have the entire trial televised?\n16. What were the pros and cons of all three of the judges being Jews who were natives of Germany?\n17. Where in the world is genocide occurring today? What is thew orld community doing to stop it and bring the perpetrators to justice?\n18. Why, after all of this public education concerning the Holocaust, does genocide recur? What strategies can you devise to minimize outbreaks of genocide?\n19. Could the United States ever become a left-wing or right-wing dictatorship? Why or why not? If it did, what would be the principal sources of resistance?\n20. Why did the Nazis keep such detailed records of their genocide?\n21. After the end of the war, the U.S. government encouraged the immigration of hundreds of German scientists, many with Nazi backgrounds and questionable activities during the war, to the United States. Discuss whether this was appropriate.\n22. Was the kidnapping of Adolf Eichmann by the Israel government legal? Was it justified? What would have happened if the Israel government had provided information to the Argentina government about Eichmann and requested his extradition to Israel? What risks did Israel take in capturing Eichmann?\n23. Today’s newspapers are filled with a controversy concerning the assets of Holocaust victims which found their way into Swiss banks, and the efforts of the Jewish community to return those assets to their rightful owners. Does Switzerland bear responsibility for relinquishing these assets after more than 50 years? Does the U.S. government have the responsibility for returning land to Native Americans which may have been illegally confiscated three hundred years ago?\n24. Had Eichmann not had such an obsession with killing Jews,how effective could he have been in saving Jews from extermination?\nby Gary Grobman\nNote: Material in all capital letters is copyrighted by other individuals/organizations.","FACT SHEET: Climate and Energy Implications of Crypto-Assets in the United States\nClimate change is one of the most pressing problems confronting our nation and our world, and President Biden has taken bold steps to address it with legislation and policy. Among the President’s commitments are: protecting communities from pollution, reducing greenhouse gas emissions by 50% by 2030, achieving a carbon pollution-free electricity grid by 2035, and reaching net-zero greenhouse gas emissions no later than 2050.\nTo achieve these ambitious goals, we must ensure that emerging technologies contribute to a net-zero, clean energy future. The use of digital assets based on distributed ledger technology (DLT) is expanding. Digital assets are a form of value, represented digitally. As an emerging technological innovation, digital assets have provided some benefits and value for some residents and businesses in the United States, and have the potential for future benefits with emerging uses.\nCrypto-assets are digital assets that are implemented using cryptographic techniques. Crypto-assets can require considerable amounts of electricity usage, which can result in greenhouse gas emissions, as well as additional pollution, noise, and other local impacts to communities living near mining facilities. Depending on the energy intensity of the technology and the sources of electricity used, the rapid growth of crypto-assets could potentially hinder broader efforts to achieve U.S. climate commitments to reach net-zero carbon pollution.\nIn March, in Executive Order 14067 on Ensuring the Responsible Development of Digital Assets, President Biden made clear that the responsible development of digital assets includes reducing negative climate impacts and environmental pollution. The Executive Order directed the White House Office of Science and Technology Policy (OSTP), in coordination with other federal agencies, to produce a report on the climate and energy implications of crypto-assets in the United States. OSTP assembled an interdisciplinary team of experts to assess and extend existing studies with new analysis, based on peer-reviewed studies and the best available data.\nToday, OSTP published its report, examining the challenges and opportunities of crypto-assets for the United States’ clean energy and climate change goals, and providing a set of recommendations to further study and track impacts of the sector, develop potential performance standards, and provide tools and resources to reduce negative impacts. This report’s assessment and recommendations align with federal actions that reduce greenhouse gas emissions to protect public health and welfare, grow a clean energy economy with good-paying jobs, and improve environmental justice.\nCrypto-Assets Can Be Energy-Intensive, and the United States Has a Major Crypto-Asset Sector\nFrom 2018 to 2022, annualized electricity usage from global crypto-assets grew rapidly, with estimates of electricity usage doubling to quadrupling. As of August 2022, published estimates of the total global electricity usage for crypto-assets are between 120 and 240 billion kilowatt-hours per year, a range that exceeds the total annual electricity usage of many individual countries, such as Argentina or Australia. This is equivalent to 0.4% to 0.9% of annual global electricity usage, and is comparable to the annual electricity usage of all conventional data centers in the world.\nNearly all crypto-asset electricity usage is driven by consensus mechanisms: the DLT used to mine and verify crypto-assets. The dominant consensus mechanism is called Proof of Work (PoW), which is used by the Bitcoin and Ethereum blockchains. Bitcoin and Ether combined represent more than 60% of total crypto-asset market capitalization. The PoW mechanism is designed to require more computing power as more entities attempt to validate transactions for coin rewards, and this feature helps disincentivize malicious actors from attacking the network. As of August 2022, Bitcoin is estimated to account for 60% to 77% of total global crypto-asset electricity usage, and Ethereum is estimated to account for 20% to 39%.\nThe energy efficiency of mining equipment has been increasing, but electricity usage continues to rise. Other less energy-intensive crypto-asset ledger technologies exist, with different attributes and uses. Switching to alternative crypto-asset technologies such as Proof of Stake could dramatically reduce overall power usage to less than 1% of today’s levels.\nThe United States is estimated to host about a third of global crypto-asset operations, which currently consume about 0.9% to 1.7% of total U.S. electricity usage. This range of electricity usage is similar to all home computers or residential lighting in the United States. Crypto-asset mining is also highly mobile. The United States currently hosts the world’s largest Bitcoin mining industry, totaling more than 38% of global Bitcoin activity, up from 3.5% in 2020. Despite the potential for rapid growth, future electricity demand from crypto-asset operations is uncertain, demonstrating the need for better data to understand and monitor electricity usage from crypto-assets.\nCrypto-Assets Can Have Significant Environmental Impacts\nGlobal electricity generation for the crypto-assets with the largest market capitalizations resulted in a combined 140 ± 30 million metric tons of carbon dioxide per year (Mt CO2/y), or about 0.3% of global annual greenhouse gas emissions. Crypto-asset activity in the United States is estimated to result in approximately 25 to 50 Mt CO2/y, which is 0.4% to 0.8% of total U.S. greenhouse gas emissions. This range of emissions is similar to emissions from diesel fuel used in railroads in the United States.\nBesides purchased grid electricity, crypto-asset mining operations can also cause local noise and water impacts, electronic waste, air and other pollution from any direct usage of fossil-fired electricity, and additional air, water, and waste impacts associated with all grid electricity usage. These local impacts can exacerbate environmental justice issues for neighboring communities, which are often already burdened with other pollutants, heat, traffic, or noise. The growth of energy-intensive crypto-asset technologies, when not directly using clean electricity, could hinder the ability of the United States to achieve its National Determined Contribution under the Paris Agreement, and to avoid the most severe impacts of climate change. Broader adoption of crypto-assets, and the potential introduction of new types of digital assets require action by the federal government to encourage and ensure responsible development. This includes minimizing negative impacts on local communities, significantly reducing energy intensity, and powering with clean electricity.\nDistributed Ledger Technologies May Help with Climate Monitoring or Mitigation\nDLT may have a role to play in enhancing market infrastructure for a range of environmental markets like carbon credit markets, though other solutions might work as well or better. The potential benefits of DLT would need to outweigh the additional emissions and other environmental externalities that result from operations to merit broader use, relative to the markets or mechanisms that DLT displaces. Use cases are still emerging, and like all emerging technologies, there are potential positive and negative use cases yet to be imagined. Responsible development of this technology would encourage innovation in DLT applications while reducing energy intensity and minimizing environmental damages.\nKey Recommendations of the Report\nTo help the United States meet its climate objectives, crypto-asset policy during the transition to clean energy should be focused on several objectives: reduce greenhouse gas emissions, avoid operations that will increase the cost of electricity to consumers, avoid operations that reduce the reliability of electric grids, and avoid negative impacts to equity, communities, and the local environment.\nTo ensure the responsible development of digital assets, recommendations include the following actions for consideration:\n- Minimize greenhouse gas emissions, environmental justice impacts, and other local impacts from crypto-assets: The Environmental Protection Agency (EPA), the Department of Energy (DOE), and other federal agencies should provide technical assistance and initiate a collaborative process with states, communities, the crypto-asset industry, and others to develop effective, evidence-based environmental performance standards for the responsible design, development, and use of environmentally responsible crypto-asset technologies. These should include standards for very low energy intensities, low water usage, low noise generation, clean energy usage by operators, and standards that strengthen over time for additional carbon-free generation to match or exceed the additional electricity load of these facilities. Should these measures prove ineffective at reducing impacts, the Administration should explore executive actions, and Congress might consider legislation, to limit or eliminate the use of high energy intensity consensus mechanisms for crypto-asset mining. DOE and EPA should provide technical assistance to state public utility commissions, environmental protection agencies, and the crypto-asset industry to build capacity to minimize emissions, noise, water impacts, and negative economic impacts of crypto-asset mining; and to mitigate environmental injustices to overburdened communities.\n- Ensure energy reliability: DOE, in coordination with the Federal Energy Regulatory Commission, the North American Electric Reliability Corporation and its regional entities, should conduct reliability assessments of current and projected crypto-asset mining operations on electricity system reliability and adequacy. If these reliability assessments find current or anticipated risks to the power system as a result crypto-asset mining, these entities should consider developing, updating, and enforcing reliability standards and emergency operations procedures to ensure system reliability and adequacy under the growth of crypto-asset mining.\n- Obtain data to understand, monitor, and mitigate impacts: The Energy Information Administration and other federal agencies should consider collecting and analyzing information from crypto-asset miners and electric utilities in a privacy-preserving manner to enable evidence-based decisions on the energy and climate implications of crypto-assets. Data should include mining energy usage and fuel mix, power purchase agreements, environmental justice implications, and demand response participation. OSTP could establish a National Science and Technology Council subcommittee to coordinate with other relevant agencies to assess the energy use of major crypto-assets.\n- Advance energy efficiency standards: The Administration should consider working with Congress to enable DOE and encourage other federal regulators to promulgate and regularly update energy conservation standards for crypto-asset mining equipment, blockchains, and other operations.\n- Encourage transparency and improvements in environmental performance: Crypto-asset industry associations, including mining firms and equipment manufacturers, should be encouraged to publicly report crypto-asset mining locations, annual electricity usage, greenhouse gas emissions using existing protocols, and electronic waste recycling performance.\n- Further research to improve understanding and innovation: For improved analytical capabilities that can enhance the accuracy of electricity usage estimates and sustainability, the National Science Foundation, DOE, EPA and other relevant agencies could promote and support research and development priorities that improve the environmental sustainability of digital assets, including crypto-asset impact modeling, assessment of environmental justice impacts, and understanding beneficial uses for grid management and environmental mitigation. Research and development priorities should emphasize innovations in next-generation digital asset technologies that advance U.S. goals in security, privacy, equity, and resilience, as well as U.S. climate goals."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:9ae3add6-ab89-47b6-89a0-fdbdee72d3ad>","<urn:uuid:c2b3e9e1-0ef3-465d-9a23-92fdbe2309a6>"],"error":null}
{"question":"I'm designing mechanical systems and need to understand what linkages are. Could you provide a clear definition and some common examples where they're used?","answer":"Linkages are mechanical systems made of bars and joints that move in a plane to accomplish a purpose or create patterns/curves. They are commonly found in engines, car suspensions, steam shovel mechanisms, and even in human knees.","context":["By Glen Whitney for the Museum of Mathematics\nThere is perhaps no topic that has brought the worlds of math and practical construction so close together as the study of linkages. These are mechanical systems of bars and joints (and sometimes plates and hinges and slides, but we’re going to stick with bars and joints for a while…) which are free to move in a plane (or in space, but again, two dimensions will keep us plenty busy) and whose motion accomplishes some purpose or creates some pattern or curve. We constantly benefit from linkages. They appear in engines, in your car’s suspension, the mechanism of a steam shovel, and in your knee, just to mention a few places.\nToday, we’re proud to present the MoMath Linkage Kit so that you, too, can explore the wonderful world of mathematical linkages. You can download the PDF file of the pieces in the kit via this link. You then have a variety of ways to actually fabricate the pieces. You can print the diagram out on the largest, heaviest cardstock you can manage to print on, and then use brass paper fasteners to make the joints, or if your print is too small, staple the pieces so that one leg of the staple goes through the center of the holes and the other side of the staple is off the paper. Note that you then may have to break off the other leg of the staple to get full rotation of your linkage. Also, if you are making paper copies of the linkages, note that all of the small sticks and the washers to the right of the page can be safely ignored.\nOr, if you have access to a laser cutter or other automatic cutting device (anyone want to water-jet these out of bronze for us?), you can cut these pieces from a layer of other material. Note the kit is sized for an 18″ x 32″ sheet, 0.23″ thick (that was the actual thickness of the nominal quarter-inch acrylic we were using here at MoMath Labs). If you cut out the pieces in a material (like acrylic) that has a bit of give, when you line up the holes in two of the bars, you can insert one of the small sticks from the top right of the diagram and it will push through the holes and thanks to the slight flares, lock in place and produce a smoothly rotating joint. There are also locking sticks for three- and four-layer joints, and spacers that you may need to keep each bar in a plane, when you are building some of the more complex linkages with a thick material. If you are using a sheet of a different thickness, you may need to adjust the length of the locking sticks, and quite possibly their width and the size of all of the pivot holes (since the thickness of the material becomes one of the cross-sectional dimensions of the locking sticks).\nOh… and instructions for making the linkages? You’ll find those right here in Math Mondays, as we give you a tour of (a small part of) what can be accomplished with linkages over the next few weeks. Ready to start building? Let’s begin with a very familiar linkage that you’re quite likely to have used at some point.\nBut first, some general instructions: Each bar is labeled with its length, from center to center of the holes near the ends, in consistent units relative to all the other bars (1 unit = 3/8 inch if you print the Kit to full scale, incidentally). Some bars have additional holes. When there is a need to refer to one of these additional holes, it will be labeled by the distance in units from one end of the bar. For example, it should not be hard to find in the Kit the 60-bar with four holes, a 0-hole, 30-hole, 45-hole, and 60-hole. When a bar has been identified with a letter, like C, the directions will just refer to its 45-hole (say) as C45. Sometimes the directions will instruct you to make a link with a pen. If you fabricate at full size, a commonplace felt-tip pen squeezes snugly into the linking holes. At other scales, you will have to experiment — maybe a gel pen, or the tip of a Sharpie, or a thick lead from a compass or other drawing tool, etc. Anyhow, anything that will make the mechanical linkage and leave a mark as it moves will do.\nHere’s your first recipe:\nIngredients: Four 60-bars with 30-holes (A, B, C, and D); two 30-bars (E and F); six linking sticks; two pens\nDirections: Link A30 and B30. Link A60 to C0, B60 to D0, and C30 to D30. Link C60 to E and D60 to F. Link the free ends of E and F with a pen.\nWhen that’s done, you should have a linkage that looks like this:\nYou will note that you can move that pen quite a ways out by bringing the two free ends of the the bottom 60-bars together. That’s the mode in which this linkage is used, for example, as the mount of an extending mirror. But we are going to see what this linkage does as a drawing tool. So fix one of the free ends so it can pivot. (Slide it onto a fixed vertical rod, tape a spacer down and use a linking stick, or just have a friend hold it in place…) and put a pen in the hole at the other free end. Now as you draw with that pen (as seen in the opening image).\nThe other pen moves and draws in response:\nAnd here are the original and resultant drawings side by side in their proper orientations, so you can see the relationship. Note the magnification, and the change in direction: the scissors jack moves perpendicular to the direction of input motion, a fact you can appreciate if you’ve ever changed a tire on the roadside.\n(Also note the replica is not quite perfect, of course, as there is a bit of “slop” in the links constructed in this way.)\nNext time, we’ll start to build a repertoire of linkages, from the simplest on up.\nSee all of our Math Monday columns"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:4a6edd0c-e9e1-4406-ba2c-e6179a609350>"],"error":null}
{"question":"What safety features are prioritized in the design of modern school laboratories compared to restaurant ventilation systems?","answer":"Both environments emphasize different but complementary safety features. School laboratories require slip-resistant flooring that passes pendulum tests, centralized systems for shutting down electricity, gas, and water, and dedicated chemical storage cupboards. Restaurant ventilation systems, on the other hand, focus on fire suppression systems (particularly in multi-floor commercial buildings), grease traps in ventilation hoods to capture oil, and make-up air systems to maintain balanced air pressure. Both settings prioritize proper ventilation - laboratories to prevent fume circulation and restaurants to remove smoke, grease, and harmful gases.","context":["It is undeniable that the environment of an educational institution has a great impact on teaching and learning. As such, today’s classrooms are optimised for a greater degree of student collaboration and engagement. The stereotypical image of a student sitting at his or her desk for hours one end is becoming a thing of the past in many places of learning.\nHowever, the design of interactive areas like science laboratories in schools and colleges is often overlooked during the planning stage. Optimisation of these spaces is crucial, from the very start, as they provide a safe area for students to explore and investigate.\nWhen designing school science laboratories, take note of the following elements:\nComplementing Practical and Theoretical Learning\nScience calls for a combination of practical and theoretical learning. It’s essential to build an environment where these two learning disciplines balance one another so that students can learn efficiently and effectively.\nWhen designing school laboratories, the classroom space must complement both practical and theoretical learning even as the students go about dong varied tasks. Furniture must adapt to support a range of learning activities, from solo experiments to group work.\nInvesting in the right laboratory equipment and materials adds major value to the research experience and minimises costs associated with maintenance, repair and replacement. Make sure to collaborate with reliable suppliers who provide industry-leading and durable equipment.\nIntegration of Technology\nThe integration of technology in the laboratory gives students access to advanced learning tools and the latest updates in their field of study. Devices like tablets and laptops also help students transition between working in the laboratory to completing homework wherever they are.\nEnvironmental factors help students fulfil their potential and improve their learning. These include:\nWhite is the standard colour palette for laboratories, as it allows students and teachers to see what’s on their bench when they’re working with chemicals. To avoid monotony, choose bright colours like light green and orange to help stimulate thinking.\nLight and Learning Space\nNatural light and access to outdoor spaces minimise the instances of nature deficit disorder and help improve concentration. Sufficient space is essential for students and teachers to move around safely and easily.\nLaboratory flooring needs to pass a pendulum test to ensure that it is slip-resistant. Vinyl is an ideal flooring material, as it is resistant to most chemicals and can be easily cleaned.\nA workbench suffers the most wear in the laboratory, as they are where experiments are conducted, but they can last for years when maintained regularly. Choose benches made from composite stone, granite or solid grade laminate, since they are more resistant to water, chemicals, hot water and abrasion than other surfaces.\nIn compliance with safety standards, laboratories should be well-ventilated to avoid fumes circulating into non-science areas. Since emergencies can happen in labs, a central system to immediately shut down electricity, gas and water must be accessible to students and teachers. Chemicals should be stored in dedicated cupboards, clearly labelled and separated from other equipment.\nLike other learning areas, the science laboratory is essential in honing skills like observation, communication, collaboration and critical thinking. One key to success is designing a safe science laboratory that optimises learning and collaboration.","Ventilation is Essential for the Myrtle Beach Restaurant\nMyrtle Beach restaurants are home to some of the best southern cooking in the region. From fried chicken and Calabash-style flounder, to crispy hush puppies and golden french fries, Grand Strand restaurants (and Southern kitchens in general) run on plenty of hot grease. One of the downsides of this type of cooking is that the hot oil and grease particles in the air can make your kitchen ventilation work overtime.\nAll restaurants are familiar with routine hood vent cleanings as this is the area that typically becomes dirtiest the fastest. But essential ventilation maintenance is more than just a hood cleaning, and a clean and properly maintained ventilation system can improve the overall effectiveness of heating and cooling as well.\nWhat Do Restaurant Ventilation Systems Do?\nImprove air quality: ventilation systems in commercial kitchens improve air quality by removing particulates from the kitchen and exiting them to the outdoors. Smoke, grease and harmful gases from cooking can be health hazards, and ventilation improves working conditions by making the kitchen much more tolerable.\nReduce smells and odors: ventilation also helps to remove strong odors from the cooking area. Strong odors, if not exited to outdoors, may enter the dining room, mixing into an unpleasant environment for diners. Ventilation systems keep air moving, pushing bad air out and cycling new air in.\nKeep kitchens cooler: ventilation systems help keep commercial kitchens cooler by expelling hot, contaminated air from areas around ovens, stovetops and grills. A ventilation system will remove hot air and help circulate cooler air, creating a more comfortable working environment.\nRemove grease: Depending on how much grease is used in your kitchen, it’s important to use a ventilation system designed for removing the levels of grease present in your kitchen. Inefficient ventilation systems will leave greasy residues on surfaces and create an unpleasant work environment.\nRestaurant Ventilation Equipment\nVentilation Hood: Installed over kitchen appliances, hoods exhaust heat and grease from the cooking area to the outdoors. This part of the system uses grease “traps” to capture and contain oil as the greasy air exits.\nExhaust Fan: Exhaust fans help with air circulation and are installed in places needing strong air flow.\nMake-Up Air System: All of the air exited the kitchen must be replaced with fresh “make-up” air. This part of your system delivers outdoor air to keep air pressure balanced. Without balanced air pressure, you’ll notice doors suctioning closed, back-venting, and poor indoor air quality.\nFire Suppression System: Fire suppression is important for preventing the spread of fire, particularly in multi-floor commercial buildings such as hotels. If your restaurant is in a space that requires fire suppression, Cooper ventilation experts can make sure your system meets all fire suppression requirements.\nCall Cooper: The “V” is for Ventilation\nCooper Mechanical Services provides heating, ventilation, air conditioning and refrigeration expertise to Myrtle Beach restaurants. We also handle electrical contracts of all sizes, so if you are looking for a restaurant or food service upgrade or renovation, our teams are available to offer turn-key service for all HVACR and electrical systems."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:4e10aea3-9ebf-4eb8-8b86-1ad228ae3c6d>","<urn:uuid:7552c50a-e95b-4b71-be4c-199039e405cf>"],"error":null}
{"question":"What are the benefits of intellectual property licensing for innovators, and what cybersecurity risks emerge when implementing AI-based licensing systems?","answer":"Licensing intellectual property allows inventors to profit from their innovations by selling knowledge to imitators while potentially maintaining monopolistic advantages, especially when the pool of potential imitators is large. However, implementing AI-based licensing systems carries cybersecurity risks - machine learning tools may create a false sense of security due to supervised learning vulnerabilities, and hackers could potentially compromise licensing algorithms through techniques like SQL injection, affecting the quality of business decisions.","context":["Waiting to imitate: on the dynamic pricing of knowledge\nWe study the problem of an inventor who brings to the market an innovation that can be legally copied. Imitators may 'enter' the market by copying the innovation at a cost or by buying from the inventor the knowledge necessary to reproduce and use the invention. The possibility of contracting affects the need for patent protection. Our results reveal that: (i) Imitators wait to enter the market and the inventor becomes a temporary monopolist; (ii) The inventor offers contracts which allow resale of the knowledge acquired by the imitators; (iii) As the pool of potential imitators grows large, the inventor may become a permanent monopolist.\nIf you experience problems downloading a file, check if you have the proper application to view it first. In case of further problems read the IDEAS help page. Note that these files are not on the IDEAS site. Please be patient as the files may be large.\nAs the access to this document is restricted, you may want to look for a different version under \"Related research\" (further below) or search for a different version of it.\n|Date of creation:||Oct 2009|\n|Date of revision:|\n|Contact details of provider:|| Postal: |\nPhone: 44 - 20 - 7183 8801\nFax: 44 - 20 - 7183 8820\n|Order Information:|| Email: |\nPlease report citation or reference errors to , or , if you are the registered author of the cited work, log in to your RePEc Author Service profile, click on \"citations\" and make appropriate adjustments.:\n- Nancy Gallini and Suzanne Scotchmer., 2001.\n\"Intellectual Property: When Is It the Best Incentive System?,\"\nEconomics Working Papers\nE01-303, University of California at Berkeley.\n- Nancy Gallini & Suzanne Scotchmer, 2002. \"Intellectual Property: When Is It the Best Incentive System?,\" NBER Chapters, in: Innovation Policy and the Economy, Volume 2, pages 51-78 National Bureau of Economic Research, Inc.\n- Nancy Gallini & Suzanne Scotchmer, 2003. \"Intellectual Property: When is it the Best Incentive System?,\" Levine's Working Paper Archive 618897000000000532, David K. Levine.\n- Nancy Gallini & Suzanne Scotchmer, 2002. \"Intellectual Property: When Is It the Best Incentive System?,\" Law and Economics 0201001, EconWPA.\n- Gallini, Nancy & Scotchmer, Suzanne, 2001. \"Intellectual Property: When Is It the Best Incentive System?,\" Department of Economics, Working Paper Series qt9wx2c2hz, Department of Economics, Institute for Business and Economic Research, UC Berkeley.\n- Katharine E. Rockett, 1990. \"Choosing the Competition and Patent Licensing,\" RAND Journal of Economics, The RAND Corporation, vol. 21(1), pages 161-171, Spring.\n- Arora, Ashish & Fosfuri, Andrea, 1999.\n\"Licensing the Market for Technology,\"\nCEPR Discussion Papers\n2284, C.E.P.R. Discussion Papers.\n- B. Douglas Bernheim, 1984. \"Strategic Deterrence of Sequential Entry into an Industry,\" RAND Journal of Economics, The RAND Corporation, vol. 15(1), pages 1-11, Spring.\n- Ashish Arora & Andrea Fosfuri & Alfonso Gambardella, 2004. \"Markets for Technology: The Economics of Innovation and Corporate Strategy,\" MIT Press Books, The MIT Press, edition 1, volume 1, number 0262511819, June.\n- Gallini, Nancy T, 1984. \"Deterrence by Market Sharing: A Strategic Incentive for Licensing,\" American Economic Review, American Economic Association, vol. 74(5), pages 931-41, December.\n- repec:oup:restud:v:52:y:1985:i:1:p:99-106 is not listed on IDEAS\n- Arundel, Anthony, 2001. \"The relative effectiveness of patents and secrecy for appropriation,\" Research Policy, Elsevier, vol. 30(4), pages 611-624, April.\n- Wesley M Cohen & Richard R Nelson & John P Walsh, 2003.\n\"Protecting Their Intellectual Assets: Appropriability Conditions and Why U.S. Manufacturing Firms Patent (Or Not),\"\nLevine's Working Paper Archive\n618897000000000624, David K. Levine.\n- Wesley M. Cohen & Richard R. Nelson & John P. Walsh, 2000. \"Protecting Their Intellectual Assets: Appropriability Conditions and Why U.S. Manufacturing Firms Patent (or Not),\" NBER Working Papers 7552, National Bureau of Economic Research, Inc.\n- Drew Fudenberg & Jean Tirole, 1991. \"Game Theory,\" MIT Press Books, The MIT Press, edition 1, volume 1, number 0262061414, June.\n- Anton, James J & Yao, Dennis A, 1994. \"Expropriation and Inventions: Appropriable Rents in the Absence of Property Rights,\" American Economic Review, American Economic Association, vol. 84(1), pages 190-209, March.\nWhen requesting a correction, please mention this item's handle: RePEc:cpr:ceprdp:7511. See general information about how to correct material in RePEc.\nFor technical questions regarding this item, or to correct its authors, title, abstract, bibliographic or download information, contact: ()The email address of this maintainer does not seem to be valid anymore. Please ask to update the entry or send us the correct address\nIf references are entirely missing, you can add them using this form.","Your company has started to use Artificial intelligence (AI), but are you effectively managing the risks involved? It’s a new growth channel with the potential to boost productivity and improve customer service. However, particular management risks need to be assessed in cybersecurity. Start by considering AI trends to put this risk in context.\nWhy Is AI an Emerging Cybersecurity Threat?\nArtificial intelligence is a booming industry right now with large corporations, researchers, and startups all scrambling to make the most of the trend. From a cybersecurity perspective, there are a few reasons to be concerned about AI. Your threat assessment models need to be updated based on the following developments.\nEarly Cybersecurity AI May Create a False Sense of Security\nMost machine learning methods currently in production require users to provide a training data set. With this data in place, the application can make better predictions. However, end-user judgment is a major factor in determining which data to include. This “supervised learning” approach is subject to compromise if hackers discover how the supervised process works. In effect, hackers could evade detection by machine learning by mimicking safe code.\nAI-based Cybersecurity Creates More Work for Humans\nFew companies are willing to trust their security to machines. As a result, machine learning in cybersecurity has the effect of creating more work. WIRED magazine summarized this capability as follows: “Machine learning’s most common role, then, is additive. It acts as a sentry, rather than a cure-all.” As AI and machine learning tools flag more and more problems for review, human analysts will need to review this data and make decisions about what to do next.\nHackers Are Starting to Use AI for Attacks\nLike any technology, AI can be used for defense or attack. Researchers at the Stevens Institute of Technology have demonstrated that fact. They used AI to guess 25% of LinkedIn passwords successfully after analyzing 43 million user profiles in 2017. In the hands of defenders, such a tool could help to educate end users on whether they’re using weak passwords. In the hands of attackers, this tool could be used to compromise security.\nThe Mistakes You Need to Know About\nAvoid the following mistakes, and you’re more likely to have success with AI in your organization.\n1. You Haven’t Thought Through the Explainability Challenge\nWhen you use AI, can you explain how it operates and makes recommendations? If not, you may be accepting (or rejecting!) recommendations without being able to assess them. This challenge can be mitigated by reverse engineering the recommendations made by AI.\n2. You Use Vendor-provided AI Without Understanding Their Models\nSome companies decide to buy or license AI from others rather than building the technology in house. As with any strategic decision, there’s a downside to this approach. You can’t trust the vendor’s suggestions that AI will be beneficial blindly. You need to ask tough questions about how the systems protect your data and what systems AI tools can access. Overcome this challenge by asking your vendors to explain their assumptions about data and machine learning.\n3. You Don’t Test AI Security Independently\nWhen you use an AI or machine learning tool, you need to entrust a significant amount of data to it. To trust the system, it must be tested from a cybersecurity perspective. For example, consider whether the system can be compromised by SQL injection or other hacking techniques. If a hacker can compromise the algorithm or data in an AI system, the quality of your company’s decision making will suffer.\n4. Your Organization Lacks AI Cybersecurity Skills\nTo carry out AI cybersecurity tests and evaluations, you need skilled staff. Unfortunately, there are relatively few cyber professionals who are competent in security and AI. Fortunately, this mistake can be overcome with a talent development program. Offer your cybersecurity professionals the opportunity to earn certificates, attend conferences, and use other resources to increase their AI knowledge.\n5. You Avoid Using AI Completely for Security Reasons\nBased on the previous mistakes, you might assume that avoiding AI and machine learning completely is a smart move. That might’ve been an option a decade ago, but AI and machine learning are now part of every tool you use at work. Attempting to minimize AI risk by ignoring this technology trend will only expose your organization to greater risk. It’s better to seek proactive solutions that leverage AI. For instance, you can use security chatbots such as Apollo to make security more convenient for your staff.\n6. You Expect too Much Transformation from AI\nGoing into an AI implementation with unreasonable expectations will cause security and productivity problems. Resist the urge to apply AI to every business problem in the organization. Such a broad implementation would be very difficult to monitor from a security point of view. Instead, take the low-risk approach: apply AI for one area at a time, such as automating routine security administration tasks, and then build from there.\n7. Holding Back Real Data from Your AI Solution\nMost developers and technologists like to reduce risk by setting up test environments. It’s a sound discipline and well worth using. However, when it comes to AI, this approach has its limits. To find out whether your AI system is truly secure, you need to feed it real data: customer information, financial data, or something else. If all this information is held back, you’ll never be able to assess the security risks or productivity benefits of embracing AI.\nAdopt AI with an Eyes Wide Open Perspective\nThere are certainly dangers and risks associated with using AI in your company. However, these risks can be monitored and managed through training, proactive management oversight, and avoiding these seven mistakes."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:2aef4313-792d-4943-8a99-d1ed3b1b8ee5>","<urn:uuid:75c20593-63bc-45e3-ac40-3a0820eb44e8>"],"error":null}
{"question":"Interested in archery and optics - can you compare how beam propagation matrices (ABCD) and arrow aerodynamics both deal with misalignment problems?","answer":"Both domains handle misalignment differently. In optical systems, misalignment analysis uses 3x3 ABCD matrices where additional parameters E and F are derived from misalignment parameters Δ1(2), allowing the calculation of beam propagation through cascaded, individually misaligned elements. In archery, misalignment due to wind is handled through equipment optimization - using micro-diameter arrows with higher mass and FOC (Front of Center) weight to reduce oscillation and improve flight control, while some archers use 'bubbling' technique with sight levels to compensate for drift.","context":["Presentation on theme: \"Gaussian Beam Propagation Code ABCD Matrices Beam Propagation through a series of parabolic optical elements can be described by the use of ABCD matrices.\"— Presentation transcript:\nGaussian Beam Propagation Code ABCD Matrices Beam Propagation through a series of parabolic optical elements can be described by the use of ABCD matrices Examples: Matrices for a mirror,lens, dielectric interface\nApplication on a ray defined by position and slope Curved dielectric interface The ABCD matrix algorithm can be applied on a propagating ray as well as on a propagating gaussian beam\nSiegman, LASERS, Chapt. 15, Ray Optics and Ray Matrices\nGaussian Paraxial Wave Optics The ABCD matrix can also be applied to transform the so called q Parameter of a Gaussian beam R radius of phase front curvature w spot size defined as 1/e^2 radius of intensity distribution\nTransformation of the q parameter by an ABCD matrix The q parameter is given by\nRay Matrix System in Cascade M1M1 M2M2 M3M3 Total ray matrix\nGaussian Duct A. E. Siegman, LASERS A gaussian duct is a transversely inhomogeneous medium in which the refractive index and the absorption coefficient are defined by parabolic expressions n(r) r\nand n 2 parabolic refractive index parameter α 2 parabolic gain parameter Parabolic parameters n 2 and α 2 of a gaussian duct\nABCD Matrix of a Gaussian Duct With the definition the ABCD matrix of a gaussian duct can be written in the form\nIn LASCAD the concept of the Gaussian duct is used to compute the thermal lensing effect of laser crystals. For this purpose the crystal is subdivided into short sections along the axis, and every section is considered to be a Gaussian duct.\nA parabolic fit is used to compute the parabolic parameters for every section. Example: Parabolic fit of the distribution of the refractive index\nWith the ABCD matrices of mirrors, lenses, internal dielectric interfaces, and Gaussian ducts most of the real cavities can be described. To compute the eigenmodes of a cavity the q parameter must be self-consistent, that means it must meet the round-trip condition.\nThe round-trip condition can be used to derive a quadratic equation for the q parameter. All these computations are simple algebraic operations and therefore very fast.\nGaussian Optics of Misaligned Systems With 2 x 2 ABCD Matrices only well aligned optical systems can be analyzed. However, for many purposes the analysis of small misalignment is interesting. This feature has not been implemented yet the LASCAD program, but it is under development, and will be available within the next months.\nAs shown in the textbook LASERS of Siegman the effect of misalignments can be described by the use of 3x3 matrices Here E and F are derived from the parameters Δ 1(2) describing the misalignmet of the element\nThese 3x3 Matrices also can be cascaded to describe the propagation of a gaussian beam through any sequence of cascaded, and individually misaligned elements. is the total ABCD Matrix is the total misalignment vector which depends on the individual misalignments and the individual ABCD matrices","Archers who hunt out west know that executing shots under windy conditions is more the rule than the exception. I once took a shot at a Wyoming pronghorn in a stiff, 30-mph crosswind. The shot seemed true, yet I missed my target by a full 14 inches! I also remember a follow-up shot on a mule deer at 55 yards when the wind was whipping with intermittent gusts. I had to aim about two feet off-center to send the arrow through the buck’s chest. When the wind blows that hard, a well-gauged aim is critical, but so is your use of specialized tackle to help counter the wind’s forcing your arrow off course.\nUnfortunately, being a precise shooter and knowing how to aim in the wind is more art than science, since wind speed is usually unknown and variable. Also, everyone’s setup is a little different, too, when considering arrow diameter and speed, and fletching and broadhead type — so, no handy arrow-drift chart exists for all archers’ setups. With that said, let’s focus on the important factors that do remain in our control.\nStart With The Arrow\nDiameter and Mass: A lot has been written about the benefits of using a micro-diameter arrow to counter the wind’s effects. By using a shaft with less surface area, a reduction in aerodynamic drag will occur, and the arrow will exhibit less drift and fly more accurately.\nMass weight and arrow velocity are other elements of aerodynamics. A heavier, smaller-diameter arrow improves momentum and reduces the shaft’s deceleration rate. In other words, it won’t slow down as much when compared to a lighter, larger-diameter shaft. This allows for a higher ballistic coefficient and improved downrange punch.\nMicro-sized arrows with sufficient mass such as Easton’s FMJ Injexion, Gold Tip’s Kinetic, and Carbon Express’ Maxima Red SD are all top choices for improved shooting in the wind. I prefer shafts that weigh close to 10 grains per inch using a 400-spine shaft. This strikes a nice balance between speed, energy, and aerodynamics.\nIn addition, “skinny” arrows spin faster in flight (due to the rotational axis of the shaft) when compared to larger-diameter shafts using the same fletching and helical. This premise is based on centrifugal force. As an arrow spins quickly and its fletching builds up air friction, it becomes stable and will no longer wobble. With small-diameter arrows, you can lessen the drag or air resistance caused by larger vanes without jeopardizing arrow stability or control.\nFor this reason, micro-diameter arrows go well with ultra-compact vanes and less helical, both of which improve speed and aerodynamics.\nBroadhead Weight: A shaft’s front of center (FOC) weight also ties in with this accuracy gain. By using a heavier broadhead, you can increase the shaft’s FOC weight and effectively transfer the center mass or balance point of the arrow further toward the tip. This extends the fletching’s leverage effect over the shaft, which accentuates centrifugal force and flight control.\nFurthermore, increased FOC weight also pulls or directs the arrow better through the wind. This helps minimize oscillation to the back end of the arrow and improves flight performance. For this same reason, it gives the arrow increased straight-line energy when the broadhead cuts into game — a win-win proposition.\nA simple switch from 100-grain to 125-grain broadheads will boost FOC by two to three percent. However, for bowhunters looking to increase point weight by 50 percent or more, brass inserts are the obvious answer and are available from most arrow manufacturers. When using Easton’s 4mm Injexion arrows, I often use two inserts in addition to using a 100-grain broadhead. I prefer 100-grain broadheads for the Deep Six inserts, because there are more broadhead models to choose from. Overall, this allows me to boost point weight to 140 grains, giving the arrow better control in the wind.\nWith fletching, I prefer the smallest profile that allows for consistent flight properties, whether using a mechanical or ultra-compact, fixed-blade broadhead. Again, this will minimize drag and lessen the effects of arrow drift. I’ve achieved superior results using Arizona Archery’s Pro Max vanes (4.9 grains each) with a profile of 1.7 x .46 inch. I use a four-fletch setup, which produces about the same amount of steering control as three larger Arizona Archery Max Hunter vanes (7 grains each and a profile of 2.1 x .58 inch).\nPrep The Bow\nNext on the list is the bow. For windy day shooting, fast bows are an advantage because they propel the arrow at a higher velocity. Speed is good, because it cuts down on the time the arrow is subjected to air forces and wind drift. However, I won’t spend too much time on this topic, since the fastest bows don’t automatically qualify as the best bowhunting bows. Why? Some of them can be temperamental to shoot. Bottom line here is to shoot a forgiving yet highly efficient bow to promote solid all-around consistency. Any new compound with an ATA speed rating between 330-340 FPS is an excellent choice.\nPerhaps more important are the accessories you place on the bow. For best results, choose quivers, sights, arrow rests, and stabilizers that have the least amount of surface area. This will keep the bow more streamlined and less susceptible to air forces as you aim. A bow quiver is the biggest culprit here. Practice removing the quiver prior to taking a shot. Of course, a quick-detach quiver is needed in this case. If not, try removing all the arrows in the quiver and then shooting. A cluster of arrows and fletching can act as a wind sail, causing severe “bouncing” to occur and ruining your aim. Some bowhunters prefer a back or hip quiver for this reason. Please experiment heavily to find what works best for you for your specific style of hunting.\nWind Speed & Aiming Techniques\nWind speed: In a perfect world, a bowhunter would tape to the bow limb a chart with specific wind speeds and arrow-drift amounts, use a Kestrel Meter to acquire the wind rate prior to the shot, reference the chart accordingly, and then aim “off-center” the predetermined amount to lethally strike his or her target.\nThe problem is, it takes a significant amount of time to create this wind chart, since it must be created by using your specific bow, arrow velocity, and shaft setup. This could take months, if not years, to acquire.\nFor this reason, most archers are forced to shoot in the wind by “feel,” and lots of practice in various windy conditions. Also, if the wind is blowing exceedingly hard, say 40 mph with intermittent gusts, you should not take a shot. It’s unethical.\nShooting by feel could also mean using techniques to help you gauge the wind’s angle and speed. Some archers prefer to do this with a piece of yarn or twine attached to the stabilizer. Depending on how taut the string suspends in the air, you can estimate the wind’s speed. Other archers prefer to toss blades of grass at waist level to assess wind speed and angle. Use what works best for you.\nAiming: Besides aiming off-target a significant amount, archers can use a secondary method commonly used by outdoor tournament archers. This is known as “bubbling,” and it’s done by using the bubble level on the sight and canting the top limb a slight amount in the direction of the wind to acquire a “quarter,” “half,” or “full bubble” out of center to compensate for arrow drift and impact. With lots of practice, this method works surprisingly well.\nArchers who use this method like the idea of keeping the pin on the target, rather than holding way off-center. However, skeptics of this technique say “bubbling” can disrupt normal shooting form, and it also requires using the same bubble on every bow.\nShooting in the wind is an expert’s game that requires dedicated practice under various types of windy conditions to ensure consistent results. By improving your arrow’s speed and aerodynamics, reducing the bow’s surface-area features, and getting a feel for different arrow-impact patterns in various wind speeds, you can deliver on that important shot and show the wind who’s the real boss."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:55f79743-ac40-4fbe-972d-e4c981f81424>","<urn:uuid:b9d8475b-e806-4190-b46a-96e9251348fa>"],"error":null}
{"question":"How do both direct and indirect forms of marine pollution affect coastal waters, and what solutions exist to address the SF Bay's water quality issues specifically?","answer":"Marine pollution occurs through both direct discharges (from vessels, municipal/industrial wastewater, and dumping) and indirect sources (which account for 1/2 to 2/3 of pollutants) including sediments, nutrients, pathogens from agricultural lands. For the specific case of SF Bay, the key solution identified is investment in improved water treatment facilities to remove excessive nutrients from wastewater. Organizations like Bay Keeper and the San Francisco Estuary Institute are working on research and advocacy to address this, while groups like EarthJustice pursue legal action against regulatory bodies to enforce stronger pollution controls.","context":["- Nicolette Beck\nA Quick Guide to the Red Tide: Harmful Algae Blooms in the SF Bay\nby Nicolette Beck, Philanthropic Advisor\nYou may have heard the news this summer of toxic red algae blooms in the SF Bay. You may have personally seen the effects: thousands and thousands of dead fish washing up on shores or floating limply in the water. You might also be wondering what is up and what we can do to help.\nAlgal blooms have their role in a healthy aquatic ecosystem, but lately, there has been an overabundance of one particular variety: Heterosigma akashiwo. It has been depleting oxygen in the water, causing fish and other sea life to suffocate and drown. The blooms also block out light from the water so the sea grass and other types of seaweed die as well.\nScientists believe that this overabundance is caused by excessive nutrients in the water, which primarily comes from our treated sewage. Other sources of nutrients that feed Harmful Algal Blooms (HABs) can be artificial fertilizers for agriculture and landscaping.\nThe good news is that the Red Tide has subsided in the SF Bay– for now. The bad news is that it will happen again if we don’t change our ways, and the impacts on sea life will continue to be devastating.\nWe looked into it, and the best solution is for the SF Bay Area to invest in water treatment facilities to remove these excessive nutrients from our wastewater.\nWays to help:\nWrite to your political officials, urging them to invest in water treatment facilities. There is a pre-written letter at this link here to write to the SF officials: https://baykeeper.org/actionalert/algalbloom\nSign this letter to the SF Bay Regional Quality Control Board, urging them to invest in more nutrient pollution research and to reduce such pollution. https://baykeeper.org/action-alert/algae-blooms-new-normal\nYou could also donate to the following charities:\nBay Keeper - www.baykeeper.org\nBay Keeper is an environmental charity that works to protect the SF Bay and its watershed. They\nuse research, litigation, advocacy, and policy work to stop polluters and protect the SF Bay’s\necosystem. From their website: “Baykeeper uses science, advocacy, and law to hold polluters\naccountable and stop destructive activities in San Francisco Bay and throughout its watershed.”\nEarthJustice - www.earthjustice.org\nEarthJustice is a charity made of lawyers who sue and advocate on behalf of the Earth. Though not\nspecific to the SF Bay area, EarthJustice is currently suing the EPA for not doing enough to stop\nHarmful Algal Blooms in Florida, leading to thousands of manatee deaths as well as the deaths of\nother sea life. If they win it will have implications for the EPA’s responsibility in preventing this all\naround the country. This is a great organization to support. About their lawsuit here:\nSan Francisco Estuary Institute - https://www.sfei.org\nSFEI is currently doing the research for the The SF Water Board to learn more about the harmful\nalgal blooms. From their website: “The San Francisco Estuary Institute (SFEI) is one of California’s\npremier aquatic and ecosystem science institutes. Our mission: provide scientific support and tools\nfor decision-making and communication through collaborative efforts. We provide independent\nscience to assess and improve the health of the waters, wetlands, wildlife and landscapes of San\nFrancisco Bay, the California Delta and beyond. SFEI’s 50 scientists and experts provide data,\ntechnology and tools that empower government, civic and business leaders to create cost-effective\nsolutions for complex environmental issues--from cleaner water and sustainable communities to\nclimate change. We have three primary programs: Clean Water, Resilient Landscapes, and","State of the planet2 - Marine environmental polution\n\"It does not matter where on Earth you live; everyone is utterly dependent on the existence of that lovely, living saltwater soup. There's plenty of water in the universe without life, but nowhere is there life without water. The living ocean drives planetary chemistry, governs climate and weather, and otherwise provides the cornerstone of the life-support system for all creatures on our planet, from deep-sea starfish to desert sagebrush. That's why the ocean matters. If the sea is sick, we'll feel it. If it dies, we die. Our future and the state of the oceans are one.\"\n• Sea Change A Message of the Oceans\nSylvia Earle, 19\nIn the beginning it was all water ,Human civilization itself start along all coat of the world , be it river , lake, ocean - and all of the world are linked to the ocean -not only linked together but also almost everything we use eventually get in touched with the water- under cycle water in the ecosystem - all activities we do on land eventually get washed buy the rain and pass through the ground and have encounter with the water table that merge to the inland water which then run into the ocean- the ocean get evaporated , for cloud which eventually turn to rain - all In the name of supporting human\nHuman benefit from marine and coastal ecosystem and activities\n• Coastal tourism =161 billion American dollars\n• Trade and shipping =155 billion American dollars\n• Offshore oil and gas = 132 billion American dollars\n• Fisheries = 80 billion American dollars\nTherefore it is important to be careful and maintain balance in dealing our activities The popular media attention is concentrated on loss of life and property. There is little prospect for preventing many of the disasters from occurring although much could be done to reduce their severity. Many impacts could be mitigated through better vulnerability and risk assessment, predictive modeling, information dissemination, and policy development.\nMajor source of pollution are:\n- air pollution\n- dredge disposal and contaminated sediments .\n- endangered and threatened species\n- landbased water pollution\n- oil pollution - regulatory compliance\n- ship/port generated waste - partnerships\nEffect from industry and household that run of into the river • Oil pollution • Chemical pollution • Harmful substances in package form • Sewage • Ballast water • Garbage • emission • Dumping of wastes liquid,solid)\nMain source of marine pollution:\nMarine Pollution I -Point form polution - Oil Pollution • ,Toxic Contaminants ,• Marine Debris • Mining and Dumping •\nMarine Pollution II - Nonpoint Pollution, • Sewage ,• Alien Species • Watershed Issues\nMain source from ships is in form of:\n•Operational - Through socio - economic impacts to marine ecology, habitat, and coastal infrastructures are affected though operational activities that results from: Oil spill, Emission, Ballast Water, Garbage, contamination, Antifouling Dredging activities.\n• Accidental risk - marine accident that could result to oil spills which then, end up degrading our environment about 400-300 thousands of oil entered the world ocean, collision with marine mammal, which then cause propeller injuries through : Grounding ,Stranding, Loss of oil, Hazardous cargo, Noxious liquid, collision with marine mammals.\n• Design - Risk associated with environmental issue n ship and in ship designing are : In the context of ship design the impacts areas are: Shipping Trends, Channel Design Criteria, Ship Maneuverability, Ship Controllability, and Use of Simulators in Channel Studies. Since world II many nations built port but forget about maintaining them while shipyard continues to build larger ships. Physical dimension and ratio of ships to channel has got impact in today's ship controllability.\nDischarge could be:\n• Intentional and unintentional discharge (oil, garbage, antifouling paint, air emission, on indigenous species from ballast water • Environmental damage and pollution due to port activities • Disturbance of marine environmental (collision and noise) • Emission from scraping of ships at the end of their life cycle Direct Discharges • Direct discharges are defined here to include releases from vessels, discharges of municipal and industrial wastewater via pipelines, and dumping of waste materials, such as dredged material, into ocean waters\nIndirect discharge • One to two-thirds of pollutants contributing to the degradation of coastal and marine waters are from indirect sources, and include sediments, nutrients, pathogens, and toxic compounds. Pollutants from agricultural and pasture lands include sediments, fertilizers, pesticides, herbicides, and animal wastes which contain bacteria and nutrients\nAccidental Releases- • Oil spill and bunkering fuel, Emmsion)Sox,Nox,CFC &VoC,Antifouling toxins,Ballast water discharges,Noise,Watse disposal at sea,Dredging @dispersal of soil Impacts • Habitat Destruction (overview) • Loss of Wetlands • Tourism and Recreation • Deforestation • Fresh Water Alterations • Fishing Issues (overview) • Over fishing • Ecosystem Changes • Bombs, Poison, Scrapes • By catch • Global Change Climate Change • Ozone Depletion • Coastal Development • Population\nOther impacts are -\nthe introduction of pathogens to coastal waters - alteration of water tables - modification of nutrient cycles or soil fertility - increased erosion - interference with navigation - a reduction in sport and commercial fishing yields - negative impacts on recreational boating and beach use .\nThe introduction of non-indigenous species often results in unexpected ecological, economic, and social impacts to the coastal and marine environment. Predation and competition by non-indigenous species has resulted in the eradication of some native populations and the drastic reduction of others, thereby altering local food webs. This process is often compounded by the exploitation of commercial fish. Overpopulation of some non-indigenous species has resulted in the degradation and loss of wetland vegetation and other submerged aquatic vegetation as a result of overgrazing.\nBecause industrialized society depends on petroleum products to maintain its accustomed standard of living, large volumes of petroleum are transported each day in the coastal and marine environment. Spills and leaks cause the formation of tar balls, oil slicks, and tar mats, and can impact the micro-layer, the benthos, the coast, and marine life.\nSustainability capacity building, efficiency optimization of development, practice and operations that meets the needs of the present generation without compromising the ability of future generation to meet their need\nGood environmental quality is essential for sustaining coastal and marine ecosystems, commercial and recreational fisheries, and economic growth in coastal communities.\nIt is also an important means of providing natural protection against rising sea levels and storm damage. The health of coastal and marine ecosystems is affected by water quality, and in turn, water quality is dependent upon ecosystem health. If one is impaired, the other is threatened.\nDespite their value and the programs designed to protect them, many coastal waters are being degraded at an alarming rate in addition to this, other advantages are: • compliance with all applicable environmental laws and regulations; • No significant adverse environmental impacts; • Wastes treated or destroyed on board to the extent practicable; • No inappropriate dependence on shore facilities for waste off-load and disposal; • Minimal energy consumption; • Minimal logistical costs for waste management; and • Minimal use of hazardousmaterials.\n• Low exhaust emission diesel engine achieves a 25% reduction in air emissions\n• Expected to result in reduced maintenance, better engine performance, and decreased fuel consumption\n• Hull of boat was coated with a Teflon-based coat that contains no toxic chemicals • Holding tank for waste prevents discharges\n• Port's attempt to reduce emissions from marine vessel engines.\n• Particular features of existing tug engines were modified to keep combustion temperatures as low as possible; this optimizes engine efficiency and produces fewer emissions.\n• With a Teflon coat, hulls are easy to clean, contain no toxics, and no longer need to be painted, reducing pollution from waste products generated during painting.\n• Surface sediments contaminated with metals, PAHs, PCB, and other organics\n• All clean material deposited in Mass Bay Disposal Site - innovation/uncommon\nothers - transferable - response to EPA or government initiative - significance and breadth of benefits - effectiveness and results - acknowledgement by others - Beneficial disposal of dredged material - Treating waste as resources and put them under recycling\nStrategies for shipboard control\n• Shipboard and waste emission outline -treatment and elimination - Pollution Prevention (P2) or Pollution Control-this is backbone of the thrust in achieving clean ship. Pollution Prevention Use fewer environmentally harmful substances and generate less waste on board.\nPollution Control: Increase treatment, processing, or destruction of wastes on board. The basic P2 principles follow:\nEliminating the use of environmentally harmful chemicals, such as ozone-depleting substance (ODSs), toxic antifoulant hull coatings, and other hazardous materials, may be the best approach for some potential of :\n• Sulfur reduction in bunker fuel\n• Nitrogen reduction to choice of propulsion system\n• On board Cataleptics system like charlatanic converter, water injection, emulsion\n• Operationally sped reduction and use of shore power connection has\nWhat is expected from youWe're all responsible for this mess, and it will take all of us to stop it from getting worse. It's time to completely rethink how we as a society use (or abuse) plastic. Here are some things that you can do right now:\nEvery time you see litter, pick it up and dispose of\nReduce, Reuse, Recycle - you've heard it before, but\nnow you know what happens when you don't. Be conscious of all\nthat you buy, and be sure to avoid products with excessive\npackaging, especially in disposable products.\nDemand more and better recycling facilities in your\nTake part in local stream, river and beach cleanups -\nor organize one yourself. Though these don't solve the\nproblem, they are very effective at drawing attention to the\ngreater problem offshore.\nIf you live near the ocean, or a river that drains\ninto it, your storm drains are probably washing garbage right\nout to sea. Be conscious of this and any other potential\nsources of marine litter in your area. Demand that these are\n- Be very conscious of your ecological footprint. Encourage change though your decisions and do no accept the current paradigm of use and waste.\nFeature environmental technology\n- Ozone safe substances- 200-Ton Air-Conditioning Plant Conversion Kit -The CG-47and DDG-51 plants have been successfully converted to the ozone-friendly refrigerant HFC- 236fa conversion kit .\n-Solid waste - Solid-Waste Pulpers -The\npulper (especially the large pulper) is the machine into which\nyou dump tremendous quantities of paper, cardboard, or food\nwaste.The waste mixes with seawater to form slurry, which is\nthen discharged overboard. Studies show an immediate\n100,000-to-1 dilution when discharged into the wake of a ship.\nShips equipped with a pulper can dispose off their paper,\ncardboard, and food waste just about anywhere and at anytime-at\nsea including MARPOL areas.\nLiquid waste - OWS and Bilge water Polishers: Many bilge cleaners the Navy uses today contain long-lasting emulsifying agents, which produce stable oil-in-water emulsions that shipboard OWSs cannot effectively process. The popular media attention is concentrated on loss of life and property.\nThere is little prospect for preventing many of the disasters from occurring although much could be done to reduce their severity. Many impacts could be mitigated through better vulnerability and risk assessment, predictive modeling, information dissemination, and policy developmeent .\n\"... [M]an's fingerprint is found everywhere in the oceans. Chemical contamination and litter can be observed from the poles to the tropics and from beaches to abyssal depths...But conditions in the marine environment vary widely. The open sea is still relatively clean...In contrast to the open ocean, the margins of the sea are affected by man almost everywhere, and encroachment on coastal areas continues worldwide...If unchecked, this trend will lead to global deterioration in the quality and productivity of the marine environment.\" The State of the Marine Environment, 1989; Group of Experts on the Scientific Aspects of Marine Poll comments will appreciated -"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:ee2fe746-d29a-4463-bf76-94cecfb8eb32>","<urn:uuid:3c66f1d5-b4e8-4e49-8d6d-90b988085123>"],"error":null}
{"question":"As a European defense analyst, I'm interested in understanding how NATO's strategic priorities have evolved in response to environmental versus military threats since 2010. What are the key differences in how NATO approaches these challenges?","answer":"NATO approaches environmental and military threats quite differently. For environmental threats, NATO focuses primarily on adaptation and consequence management rather than prevention, since it is not the first responder to climate change. Its environmental efforts include developing protection guidelines, conducting disaster response exercises through EADRCC, and improving military energy efficiency through the Green Defence Framework. In contrast, regarding military threats, particularly since Russia's 2014 annexation of Crimea, NATO has shifted to emphasize its core mission of collective defense and deterrence. While environmental security is seen as important, it remains secondary to military effectiveness, with NATO being careful not to let environmental initiatives detract from its primary defense role. The Alliance recognizes that both environmental and military security are existentially important but maintains that they should not be pitted against each other.","context":["Image courtesy of Lisa Ferdinando/DVIDS.\nEnvironmental change1 is increasingly recognised as one of the major factors that will shape the global security environment. According to most experts, rising global temperatures will lead to rising sea levels and cause more extreme weather events, such as storms, flooding, droughts and wildfires.2 The firestorms that engulfed parts of Australia in late 2019 and early 2020, burning an area the size of Belgium and Denmark combined, and severely decimating that continent’s wildlife, were a stark reminder of the force of these changes.\nWhile the causal relationship between environmental change and conflict is difficult to establish, there have been arguably several conflicts where environmental change has acted as a trigger, notably Darfur and Somalia. Even the beginning of the Arab Spring has been related to environmental change: unrest erupted because of increasing food prices, which in turn were the result of several bad harvests attributed to climate change.3 In general, there is a widely held assumption that environmental change could lead to food and water shortages, pandemic diseases, mass migration, and humanitarian disasters.\nEnvironmental changes will also influence the way in which military forces conduct their missions. For example, the military could be called upon more often to provide humanitarian assistance and disaster relief. Other consequences for the military are the vulnerability of coastal installations to rising sea levels and the impact of floods, wildfires, and more extreme temperatures on military exercises and supply chains. Finally, as climatic changes open up regions hitherto largely closed to human activity (e.g. the Artic) the military will need to operate in these challenging environments.\nNATO is not the first responder to climate change. This role is played by other international bodies, in particular those who can set limits on CO2 emissions. However, as the premier transatlantic security and defence organization, NATO would seem an appropriate forum for discussing the security implications of environmental change. After all, NATO offers a seamless continuum of political consultation and decision-making, military planning and military implementation.\nIn addition, through its various committees and agencies NATO is covering many non-military, security-related subjects that range from intelligence sharing to civil emergency planning, and from environmental to medical issues. Moreover, as a democratic Alliance NATO is answerable to public concerns, in particular if these concerns have a genuine security dimension.\nNATO’s environmental security “acquis”\nIn NATO’s 2010 Strategic Concept, Allies for the first time acknowledged that “[k]ey environmental and resource constraints, including health risks, climate change, water scarcity and increasing energy needs will further shape the future security environment and have the potential to significantly affect NATO planning and operations”.4 This formulation already hinted at two distinct dimensions of environmental security: as a “threat multiplier” that generates new security challenges or aggravates existing ones, and as a phenomenon that impacts on the nature and conduct of NATO’s military operations.5 Both dimensions lead to different conclusions. While the first puts the onus on prevention and mitigation, the second puts the emphasis on (military) adaptation. Given NATO’s nature as a political-military alliance, its emphasis will inevitably lie on adaptation. However, as a closer look at NATO’s environmental security activities reveals, the Alliance’s agenda also encompasses some preventive elements. Three major areas can be identified: Strategic Awareness, the Military Dimension, and Cooperative Security.\nStrategic Analysis. The security implications of environmental change in terms of climate-induced geopolitical, economic and military shifts are already dealt with in Allied consultations and intelligence-sharing. Moreover, certain NATO Committees in the Civil Emergency Planning domain have been dealing with, inter alia, extreme weather conditions, pandemics, and disaster relief. Also, the Secretary General’s Policy Planning Unit and the Emerging Security Challenges Division are promoting a dialogue with climate experts. NATO’s Allied Command Transformation is conducting work related to the security implications of environmental change through its regular Strategic Foresight Analysis and the Framework for Future Alliance Operations. Finally, the NATO Strategic Direction-South Hub is also undertaking studies on environmental change, notably with a focus on the African continent.\nPolicies and Standards on Environmental Protection. Since the 1970s NATO developed environmental protection guidelines and standards, resulting in an overarching policy (MC469, agreed in 2003). The implementation of this policy is supported by a number of NATO standards, for example on waste management, water treatment and best practice for camps. Furthermore, NATO’s “Policy on Power Generation for Deployed Force Infrastructure” has a strong focus on saving fossil fuel. The rationale of these efforts is both military and political: mission success requires ensuring the public acceptance of deployed NATO forces in the host country. This requires these forces to demonstrate a genuine commitment to the well-being of the local population, including by protecting their environment.\nEducation, Training, and Exercises. The need to prepare for operations in a changing environment is also reflected in NATO’s education and training efforts. Some training tools already exist, for example the Norway-based NATO Centre of Excellence (CoE) for Cold Weather Operations. ln addition to CoEs, national education and training facilities as well as the NATO School in Oberammergau are conducting environmental protection courses that sensitise soldiers and civilians with the stringent requirements for operating deployed camps, protecting cultural property, etc. NATO’s education and training efforts are constantly being adapted to changing needs and requirements. If considered politically and/or militarily desirable, training courses could be devised that focus specifically on climate-related threats and responses.\nLikewise, NATO’s Euro-Atlantic Disaster Response Coordination Centre (EADRCC) is regularly conducting consequence management field exercises involving dozens of Allies and partner countries. The scenarios, some of which are based on environmental challenges faced by the host nations, are designed to strengthen the ability of teams from different nations to cooperate across a wide range of relief operations. These include urban search and rescue, emergency medical teams, as well as detection, protection and decontamination teams. Compared with traditional military exercises, they are smaller (up to 2,000 personnel) and performed in close cooperation with the United Nations Office for the Coordination of Humanitarian Affairs (UN OCHA), which retains the primary role in the coordination of international disaster relief operations.\nThe military dimension\nOperational Planning and Defence Procurement. As Allies acknowledged in the Strategic Concept, climate change and other developments could impact NATO’s operational planning. The Pentagon has repeatedly noted that rising sea levels may impact the execution of amphibious landings; changing temperatures lengthening the arid season could impact operation timing windows; and the increased frequency of extreme weather could limit surveillance and reconnaissance measures.6 Environmental change can also affect military bases (e.g. a rise in the sea level could render important hubs like Diego Garcia unusable). More hostile climatic conditions could also affect both the service life and the maintenance requirements of military equipment (for example, adding dust filters to protect the engines of military vehicles will limit their range and performance). Finally, changing climatic conditions could also lead some Allies to invest in a different force structure, e.g. emphasising helicopters, coast guard vessels and amphibious vehicles over other equipment.\nEnergy Efficiency in the Military. The high fuel demand of combat forces can diminish their performance, increase their vulnerability, and may require the diverting of combat forces to protect supply lines. Hence, in-creased energy efficiency could offer benefits in terms of combat power and agility. NATO’s own work in this regard focuses on reducing the consumption of fossil fuel in deployed force infrastructure (i.e. camps), resulting in more autonomy, a lesser logistical burden and a smaller environmental footprint.7 In early 2014, Allies agreed the “Green Defence Framework”, which sought to bring various internal work strands in NATO closer together in order to create synergies and improve NATO’s “green” profile.8 In addition, questions related to national energy efficiency measures have been included in NATO’s Defence Planning Capability Survey. All these measures are undertaken with military effectiveness in mind. However, there is widespread agreement that a reduced energy footprint of certain military activities offers the additional advantage of demonstrating to a broader public that the military is not indifferent to environmental concerns.\nConsequence Management. While humanitarian relief missions are not considered a core task of NATO, the Alliance has been involved in such missions on several occasions, starting in 1991 to protect the Kurdish population in Northern lraq (“Provide Comfort”); in the aftermath of a severe earthquake in Pakistan in 2005 (where NATO delivered almost 3,500 tons of relief supplies); and after Hurricane Katrina hit the US in 2005. The Euro-Atlantic Disaster Response Coordination Centre also coordinated national assistance on many other occasions (floods, mudflows, wildfires) in Allied and partner countries (e.g. Albania, Bulgaria, Turkey, Algeria, Moldova). Noticeably, thus far, the NATO Response Force, which had initially been conceived as the flagship of NATO’s military transformation, has mostly been deployed in humanitarian relief efforts.\nEnvironmental security as a partnership tool. The partnership dimensions of addressing environmental security risks are several-fold. First, as environmental change affects many partner countries, they will be interested in scientific cooperation with NATO on mitigation measures, but also on consequence management, training and education. Past examples of such scientific cooperation encompassed measures on flood control in Ukraine and preventing desertification in Egypt. Second, helping partner countries build the capacity and resilience to better manage environmental impacts could become a legitimate element in NATO’s Defence Capacity Building approach. Finally, as humanitarian relief oper-ations involve many other international organizations and NGOs, they provide a strong rationale for deepening NATO’s ties with all prospective actors, thereby underlining NATO’s inclusive and comprehensive approach to security.\nScientific Cooperation. NATO’s Science for Peace and Security (SPS) Programme has a long tradition of addressing environmental concerns. Its predecessor, the “Committee on the Challenges of Modern Society”, looked at the challenge of environmental degradation as far back as the late 1960s. Today, SPS brings together scientists from Allied and partner countries, has supported numerous workshops and multi-year projects linked to security issues arising from key environmental and resource constraints, as well as disaster forecasting and the prevention of natural catastrophes, and defence-related environmental issues. Allies’ interest in supporting projects on environmental security has waned in the recent past. Still, SPS remains an important resource for NATO to support workshops and training courses with military and civilian experts addressing geophysical, meteorological and atmospheric/ space-weather phenomena that have an effect on military capabilities.\nThe delicate path ahead\nAs this non-exhaustive list of NATO’s activities demonstrates, the Alliance is already coping with the consequences of environmental change on various levels. Hence, giving this dossier greater emphasis and visibility appears feasible. This could entail, first and foremost, revisiting the aforementioned “Green Defence Framework”, seeking to operationalise its various suggestions in areas such as energy efficiency measures and the sharing of best practices, and perhaps even considering “the applicability of ‘green’ standards and principles across the NATO HQ, NATO Command Structure and NATO Agencies, and […] the applicability of setting up ‘green’ accounting and benchmarks to measure progress”.9 Allies could also support more scientific research projects through NATO’s Science for Peace and Security Programme, seek to elevate the role of environmental security in its dialogue and cooperation with partner countries, enhance NATO’s presence at climate-related events, and initiate a more robust public diplomacy effort. However, simply producing more public statements on the importance of environmental security will not be enough. Successfully raising NATO’s visibility in this domain will depend on a number of important factors.\nFirst, as environmental change touches upon many Allied sensitivities, great care needs to be taken to establish and sustain consensus on this subject. One reason why the 2010 Strategic Concept did not lead to a stronger focus on environmental security was the hesitation of many Allies to become engaged in a subject that could upset the balance of interest in certain regions (e.g. the High North) or risks degenerating into a controversial debate about national environmental or energy policies. In the same vein, Allies’ interest in using the SPS Programme to support scientific projects related to environmental security proved rather uneven in the past, with some Allies questioning whether NATO was the appropriate framework for sponsoring such efforts. In short, without proper handling, these diverging views could quickly re-surface. Hence, all Allies must be reassured that a more visible NATO role in climate and environmental issues will not be detrimental to their national (security) interests.\nA plausible narrative\nSecond, given NATO’s nature as a political-military Alliance, for any NATO narrative on environmental security to be credible, it should be focussed predominantly on consequence management and less on prevention – where NATO’s role is comparatively small. While this may clash with the public sense of urgency about the need to slow down or even arrest environmental change, NATO’s major contribution to security remains in employing its military competence as a “force for good”, be it through deterring major war or offering humanitarian assistance after natural disasters. This contribution to international peace and stability is what makes NATO unique. Hence, any attempt to give NATO more visibility in the environmental field must take care not to send conflicting messages. With public expectations set on mitigation and prevention, Allies must not allow a view to take hold that they have concluded that mitigation efforts will fail and that NATO therefore had to prepare for the worst. At the same time, neither should they give the impression that a stronger focus by NATO on environmental security means moving away from its core business of military deterrence and defence.\nThird, given the highly emotional and at times outright apocalyptic nature of the current debate on environmental change, Allies must resist any temptation to cater to the wilder shores of this debate. After all, irrespective of public expectations regarding reduced greenhouse emissions, the military (notably air forces) will remain a major polluter.10 Stressing NATO’s “greening” efforts is a genuinely positive message, all the more so as it would tie in with the logic of other initiatives, such as the EU’s new “Green Deal”, which aims for the Union to become climate-neutral by 2050. However, its military focus will not allow NATO to formulate similarly ambitious goals. Militaries will have to focus on operational effectiveness, with environmental concerns playing a growing, yet secondary role. Overselling NATO’s contribution to environmental security would run the risk of breeding disappointment or even outright resentment, with hardcore climate activists denouncing such efforts as mere political window-dressing. In short, in discussing environmental security NATO must not be apologetic. Environmental and military security are both of existential importance; they must not be pitted against each other.\nConclusion: setting the stage for a more visible role in environmental security\nThe international discussion on environmental change is now becoming a legitimate part of the security debate. While NATO is not going to be a major factor in the environmental debate, its elevated role in international security demands that it be more than merely a dispassionate observer of that debate. If NATO can demonstrate that in implementing its core mission of deterrence and defence it is conscious of environmental concerns, and that its national militaries have understood the need to make their own contribution, the stage could be set for a more visible role in environmental security. Even if modest, such a more visible role in environmental security would help align NATO with a challenge that a growing number of people are regarding as a major security concern.\nThis article was published under a Creative Common “Attribution-Non Commercial-NoDerivs” Licence. (CC BY-NC-ND)\n1 Although the term “climate change” is more commonly used, it is politically charged, as it is closely connected with the question of whether it is a man-made phenomenon. Hence, this paper mostly uses the term “environmental change”, as it is also more comprehensive.\n2 See International Military Council on Climate and Security, The World Climate and Security Report 2020, February 2020.\n3 See S. Johnstone and J. Mazo, “Global Warming and the Arab Spring”, Survival, April/May 2011, pp.11-17. 4 NATO’s Strategic Concept, 2010, para.15.\n5 See also T. H. Lippert, NATO, climate change and international security: a risk governance approach, Palgrave MacMillan, 2019.\n6 See the numerous examples listed in: Office of the Under Secretary of Defense for Acquisition and Sustainment, “Report on Effects of a Changing Climate to the Department of Defense”, Washington, DC, January 2019.\n7 For a selection of documents and other information on Allies’ and NATO’s work on “smart energy” see the “Smart Energy LibGuide” at http://www.natolibguides.info/smartenergy\n8 Green Defence Framework, February 2014, http://www.natolibguides. info/ld.php?content_id=25285072. Given the novelty of some of the issues addressed in that paper, its rather hesitant tone should not come as a surprise. However, it was Russia’s annexation of Crimea in March 2014 and NATO’s subsequent re-emphasis on collective defence that prevented a more systematic pursuit of the paper’s various innovative elements.\n9 Green Defence Framework, 2014, para.10.\n10 For a typical example see T. Lorincz, “NATO is a threat to the climate”, Ricochet, 29 December 2019.\nAbout the Author\nMichael Rühle is Head, Energy Security Section, in the Emerging Security Challenges Division in NATO’s International Staff.\nFor more information on issues and events that shape our world, please visit the CSS website.","NATO: Towards a new Strategic Concept\nThe new threats and challenges the Euro-Atlantic area is facing pose the question of whether it is the right moment for a new Strategic Concept to be developed.\nThe Strategic Concept is NATO’s authoritative strategic document, which describes the Alliance’s goals, challenges and means to meet its security goals and requirements. Strategic Concepts are updated more or less every 10 years to take account of changes in the global security landscape and to ensure that the Alliance is prepared to fulfil its tasks. This article briefly analyses the changed threats and challenges NATO faces since the publication of the current Strategic Concept (2010) and argues why it should develop a new document anytime soon.\nThe current Strategic Concept\nThe Strategic Concept published at the Lisbon summit in 2010 reflects the experiences and lessons learnt from 9/11 and the fight against Islamic terrorism, especially the NATO-led ISAF mission in Afghanistan. The three NATO essential core tasks underlined in it, are the principles of collective defence, crisis management and cooperative security. Plus, it emphasizes solidarity between allies, the importance of transatlantic consultation and the need to engage in reform processes. Among the threats to be faced by the Alliance from 2010 onwards, the Strategic Concept includes the proliferation of ballistic missiles and nuclear weapons, terrorism, cyber-attacks and various environmental problems. When promoting security, a key element is cooperation, which must take place at different levels and areas, such as at the reinforcement of arms control, through disarmament and non-proliferation efforts and by enhancing partnerships with different actors and in different regions of the world, among others.\nWhat’s new since 2010?\nToday, seven years after the current Strategic Concept was published, there are several main reasons to argue that the security environment has changed. The first of them is Russia’s changed role and behaviour. In 2010, NATO aimed for a true strategic partnership with Russia. Some of the members from Eastern Europe took a critical stance on Russia, but the majority agreed on the idea of a common European security order that included it. However, with the illegal annexation of Crimea in 2014, Putin did not only alter the basis for such a cooperative relation but turned against the European security order itself. It is also important to note that Russia’s changed behaviour does not only apply to Ukraine. Nuclear threats against Western neighbours, test flights with nuclear bomber aircrafts or President Putin’s announcement that Russia could overrun the Baltic States within a matter of days have contributed to the crisis in Eastern Europe as well.\nAnother reason why the security environment has changed is the instability in the Middle East and North of Africa (MENA) region. Countries that are usually defined as belonging to MENA include Afghanistan, Algeria, Egypt, Iraq, Jordan, Syria or Yemen, among others. Given continuous civil wars and conflicts taking place in the North of Africa and the Middle East, the rush of migrants has been increasing since 2010. Plus, major cities in the Euro-Atlantic area have suffered terrorist attacks from the so called “Islamic State” (IS) of Iraq and the Levant. NATO works very hard to provide stability on the Eastern flank, but it must also address the needs of member states on the Southern flank in order not to get obsolete, and to be able to fulfil its primary task, guaranteeing security for its members. In this context of MENA, it is argued that an article 5 threat would be possible in a scenario of a hypothetical devastating attack by IS against, for example, a NATO member such as Turkey (if the consequences were analogous to a military aggression).\nIt is also important to consider the European Union’s new situation after the United Kingdom’s decision to leave. The UK still wants to be involved in Euro-Atlantic security affairs –being a properly functioning NATO central to May’s view of British security-, and so does the EU, which has now lost its strongest military power. For this reason, the European Union would be more willing than ever before to cooperate with NATO. The EU has already started to catch up on security matters, for instance with the creation of a defence fund to streamline military research, development and procurement. This and other actions would make the European Union a more attractive partner for NATO.\nImportant also is the election of Donald Trump as the new President of the United States. After his dubious and polemic declarations during the electoral campaign and his first months in the presidency, Trump is implementing all US commitments to NATO, and he has even asked the Congress to increase the budget of the European Reassurance Initiative (supporting the US security presence in Europe) from $3.4 billion to $4.8 billion. It can thus be said that, so far and in spite of his unpredictability and discontent with the current state of the 2% initiative, Trump’s administration is one of continuity when it comes to security policy, as was his predecessor’s.\nFurther challenges to be considered are developments in the Asia-Pacific region, such as the economic and military growth of China or the presence of nuclear powers, such as North Korea, India or Pakistan. In a region where also the United States is exerting its influence, the potential for violent conflict is real.\nIs it the right time for a new Strategic Concept?\nAll the above mentioned are, in addition to issues that should be reflected on a new Strategic Concept, reasons to actually write and publish a new document. However, there are some reasons to believe that this might not be the best time to do so. The first of them is that there is always a risk with getting a document started. Once the drafting begins, the old document must be replaced by a new one, otherwise NATO’s credibility and unity would be doubted. Secondly, the relationship between member states and the trust between them is particularly complicated at the moment. This is mainly due to some allies’ lack of faith and trust on Trump’s USA, the different strategic priorities of member states and the already existing East-South gap, Turkey’s recent developments, etc. This kind of uncertainty is argued not to create the circumstances in which NATO member states could agree on a new strategy. The main argument here is that a debate around a new Strategic Concept would increase tensions, and it would weaken allied solidarity instead of strengthening it. The final major counterargument to a new Strategic Concept is that since the Alliance already reacted to major changes at the Wales and Warsaw summits, through the publication of elaborate declarations and working papers, the revision of the Strategic Concept is not as urgent as it might seem, given that these documents can for now be used as strategic guidelines.\nNew Strategic Concept- better than do nothing?\nThe arguments fearing a new Strategic Concept are right and important to take into account. However, in the light of such changes and new challenges, it seems essential to adapt NATO’s strategic foundations to the new situation. Avoiding a debate for the sake of avoiding disagreement or looking weak should not stop an organization that has often been characterised as the institutionalization of transatlantic dispute, and which still has survived for almost 70 years now. Developing a new Strategic Concept is argued to be better than not doing anything because inaction can also be harmful for NATO, since it would send a signal of insecurity and self-doubt. In this context, it would be desirable to have a policy debate on the future tasks of the Alliance that would lead to a generally accepted new Strategic Concept. The North Atlantic Council recently announced that Secretary General Jens Stoltenberg will continue at the head of NATO until 2020, which means he will be in charge of leading the transition towards a new Strategic Concept, if this is to take place in the next few years.\nAbout the author: Rosario Soler is a Bachelor student of International Studies at Universidad Carlos III de Madrid and was a Research Assistant Intern at the IIR."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:c393af76-3013-49d5-919b-39a6c9f5c59f>","<urn:uuid:b5e9d007-760b-4575-bea7-d4bd46032941>"],"error":null}
{"question":"Je suis un étudiant en jazz - I'm comparing different jazz musicians' teaching approaches. How do Kirk Whalum and Kenny Burrell contribute to jazz education?","answer":"Both musicians have made significant contributions to jazz education, but in different ways. Kirk Whalum's educational work focuses on his Gospel According to Jazz documentary series, which teaches how instrumental music can 'help make visible the invisible' and introduces people to spiritual concepts through music. Kenny Burrell has had a more formal academic role as a professor at UCLA, where he developed the first regular college course on Duke Ellington in the United States called 'Ellingtonia' (started in 1978). He also founded and directs UCLA's Jazz Studies Program, serves as a professor of music and ethnomusicology, and conducts workshops on guitar and jazz studies. Additionally, he is the founder and President Emeritus of the Jazz Heritage Foundation, working to promote jazz as a classical art form.","context":["Kirk Whalum is a Grammy-winning jazz artist who recently released The Gospel According to Jazz IV, the fourth in a documentary and music series that dates back to 1998.\nPrimarily a saxophonist, Whalum has had a long and distinguished career in the jazz world, including touring for seven years with Whitney Houston. He contributed the saxophone to Houston’s monster hit “I Will Always Love You”.\nWhalum’s newest documentary features a number of original compositions performed by his band that included many of his family members. His band consists of people who have toured with everyone from Stevie Wonder to Bruno Mars to Billy Currington. Whalum and the band also play a few covers including a stirring version of the Foo Fighter’s “My Hero” sung by Sheléa (currently on tour with Wonder).\nRecorded at the Christian Cultural Center in Brooklyn, the GATJ IV is a two disc, 19-song CD, and a DVD which intersperses the music with interviews with Whalum. We talked to Whalum about his career, his inspiration for the GATJ series, and the differences between being the frontman and playing in a band.\n– 1 –\nSB: So what was the inspiration for the Gospel According to Jazz series?\nKW: This series is something that started out of a big disappointment. I was signed to Columbia Records for 12 years which was a big deal. I realized how much it meant to me when I got dropped from the label. I was about to have my pity party when my wife said to me, “What can you do today that you couldn’t do yesterday when you were a Columbia Recording artist?”\nIt was a complete switch of my paradigm. And right away I knew what I wanted to do. I wanted to do a series that features musicians that are not known as Gospel musicians, whether they were R&B or Jazz or whatever, playing Gospel music. Fast-forward many years and I won a Grammy for this series, and that’s very serendipitous because it came out of such a big disappointment.\n– 2 –\nSB: How much work goes into making a documentary like this?\nKW: It is a year’s worth of work. For me it’s conceptualizing the music that takes a long time. I write and arrange most of the music, and even when we cover songs I spend a lot of time reimagining those songs for this concept. As far as the directing, it’s a lot of dialogue about what this is about. It kind of falls in between the cracks. Is it Gospel? Is it Jazz? Is it both? If you listen you can see that it’s kind of an artistic statement with a spiritual core.\n– 3 –\nSB: So what is the central message of the Gospel According to Jazz?\nKW: The central message of part IV is God’s radical hospitality. God has welcomed all of His creation through this sacrificial act that the Savior Jesus has done. The door is now open, it’s no longer a matter of whether you work hard enough or if you’re good enough — that’s the Good News.\nThe message of the whole series has to do with instrumental music and how powerful it is. It can help make visible the invisible, to give a sonic imagery to He who is indescribable. That’s where the whole series is coming from, to introduce people to the idea that God is speaking through the language of music.\n– 4 –\nSB: You’ve played in a band with some very famous people, most notably Whitney Houston, but you’ve also been the frontman of your own band. Which do you prefer and how do you approach your job differently depending on your role?\nKW: I appreciate being able to work for people, to be able to add a spice to the gumbo. For Whitney I was the guy that was able to bring a certain spirit and soul and gospel kind of feel to what she was doing. That is a very fun thing to do. It’s important not to overdo things or make it about you because it’s not about you.\nThere are times for me to be out front. Frankly, half of the time I want to tell others to take the lead. It’s nice to have a band that I can pass the ball to and get out of the way. Having great special guests is beautiful.\n– 5 –\nSB: As someone who is playing an instrument and only communicating with the audience through that medium, how do you make sure you are keeping the audience engaged?\nKW: I’m constantly amazed that someone would get in their car and come hear me play. There’s a part of me that always pinches myself. This communication we do with these instruments is not just making noise, it’s a dialogue. My advice is to be authentic to who you are. Whatever influences that make you who you are and where you came from are important. It’s tempting to want to tailor the music to a given audience. You have to do some of that, but you should try your best to play it like you hear it.","After 40 years as a jazz professional, appearing on several hundred albums as leader and sideman, Kenny Burrell is among the handful of guitar greats who have forever changed the role of their instrument.\nStaunch musical integrity and discriminate taste coupled with matchless technique have made the guitarist nonpareil among his peers. \"My goal is to play with good tone, good phrasing and to swing,\" says Burrell, \"I strive for honesty in playing what I feel.\"\n\"Master instrumentalist and composer,\" \"virtuoso,\" \"historic figure of American guitar.\" \"Ellington's favorite guitar player\" - this is a typical sampling of the critical praise routinely bestowed on Burrell, who pioneered the guitar-led trio with bass and drums in the late Fifties. Although he has since worked in countless other formats, from big band to three guitars plus rhythm to solo, he has remained constant in his quest to get the most out of a natural, low-volume, acoustic sound. \"My audience has developed so that they come to listen and are quiet,\" he explains. \"Thus I can work in a limited volume range and explore all the subtleties that can happen, which is my favorite part of the music.\"\nAside from his performing and recording schedule, Kenny has been a teacher at the University of California at Los Angeles (UCLA) for many years. Included in his teaching schedule is a special course that he developed on the music and life of Duke Ellington called \"Ellingtonia\". Started in 1978, it was the first regualr college course on Ellington taught in the United States. In addition he is also the founder and director of the Jazz Studies Program at UCLA where he is a professor of music and ethnomusicology. He is also a lecturer and director of workshops on guitar and Jazz studies, founder and President Emeritus, of the Jzz Heritage Foundation, and all around crusader for the recognition of jazz as a classical art form.\nKenny Burrell is also a prolific composer whose work is more and more in demand. Kenny is composer of the 1998 Grammy Award winning song \"Dear Ella\", performed by Dee Dee Bridgewater. His compositions have been recorded by many other great artists such as Ray Brown, Jimmy Smith, Grover Washington Jr., John Coltrane, June Christy, Frank Wes and Stevie Ray Vaughn. More recently, he recieved a commission grant from Meet the Composer, Inc. to write an original, extended composition for the Boys Choir of Harlem which premiered at New York's Lincoln Center, and in 1997 was recorded for Concord Records.\nKenny is a man who has garnered the respect of the entire jazz world. \"He's one of jazz's most gracious gentlemen,\" says pianist Mike Wofford, \"an educator and spokesperson for the entire tradition of American Jazz, Kenny is truly a goodwill ambassador for our music, and more importantly, a representative of the best in our society.\"\nBorn in Detroit, Kenny Burrell was raised in a musical family. His mother, who sang in the Second Baptist Church choir, also played the piano around the house. His father was fond of the banjo and the ukulele. \"He was just the kind of guy who could pick up string instruments and do something with 'em.\" Kenny recalls, - \"It kinda rubbed off on us.\"\nKenny, who credits Charlie Christian, Oscar Moore, and Django Reinhardt as influences, as well as such bluesmen as T-Bone Walker and Muddy Waters, played on his first major recording session in Detroit in 1951 with a Dizzy Gillespie combo that included John Coltrane, Milt Jackson, and Percy Heath. Even though the young guitarist was keeping heavy company, including that of such other up-and-coming Detroiters as Tommy Flanagan, Yusef Lateef, Pepper Adams, and Elvin Jones, he remained in Detroit to study at Wayne State University, from which he earned a B.A. in music composition and theory in 1955. He also studied classical guitar with Joseph Fava during that period and continues to employ finger-style and other techniques.\nA six-month tour in 1955 with the Oscar Peterson Trio helped to set Burrell's sight on the Big Apple. The following year, he and Flanagan drove to New York City and were promptly drafted into the major league of jazz. Burrell not only became the city's most indemand Jazz guitarist, recording with his own groups and with Coltrane, Billie Holiday, Thad Jones, Kenny Dorham, Paul Chambers, Jimmy Smith, Gene Ammons - and many others, but played on pop sessions with the likes of Tony Bennett, James Brown and Lena Horn and worked in the pit bands of such Broadway shows as Bye Bye Birdie and How to Succeed in Business without Really Trying.\nIn all, Burrell has recorded more than ninety albums as a leader. This body of work has received much critical acclaim.\nSince the mid-Sixties, the guitarist has been leading his own group plus working in \"All-Star\" settings and has performed with college bands and orchestras. He has also performed with professional orchestras such as the Detroit Symphony and the Buffalo Philharmonic.\nThough his combos vary in personnel, size and instrumentation, integrity and invention have constantly guided his music. \"My inspiration comes from the message Duke gave - you are unique, be yourself, put out that thing that is you, then use your work ethic and produce great music.\"\nKenny Burrell has been the recipient of many awards and has been voted \"Best Guitarist\" numerous times by music fans and critics worldwide. Recently he recieved this honor for the second time from the Jazz Times International Readers Poll.\nHis music and recordings have recieved much international recognition including the \"Prix de Disc\" from Switzerland. He has also received many academic honors including a Doctorate of Human Letters, and the 1997 Ellington Fellowship awarded by Yale Universtiy. He was voted \"favorite Jazz Musician\" by listeners of KLON Jazz Station in Los Angeles in 1996 and was inducted into the KLON Jazz Hall of Fame. He served on the awards panel for the National Endowment for the Arts and was the National Chairperson for guitars for the National Association of Jazz Educators. He has been dubbed America's \"guitar laureate\" by the Detroit Free Press.\nSome quotes of fellow musicians on Kenny Burrell:\n\"Kenny Burrell is overall the greatest guitarist in the world and he's my favorite.\" - B.B. King\n\"Burrell is the grand master of jazz guitar.\" - Dizzy Gillespie\n\"There is no finer guitarist than Kenny Burrell\" - George Benson\n\"Kenny Burrell that's the sound I'm looking for.\" - Jimi Hendrix\n\"Kenny Burrell is a great musician and his music has helped to make me what I am today.\" - Stevie Wonder\n\"Kenny Burrell is one of my Favorite guitarists\" - Pat Metheny"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:0982b346-1bc6-4e38-922b-cb28c6d9880c>","<urn:uuid:a89c3e04-170f-44a4-b448-72d4f8803358>"],"error":null}
{"question":"What difference between PVC and thermosetting wire insulation for automotive?! Need understand material properties pls!","answer":"PVC and thermosetting wire insulations have several key differences. PVC (thermoplastic) insulation can be melted with heat and is created through extrusion. It's easier to manufacture, less expensive, and can be extruded in very thin walls, but will melt when subjected to heat. PVC automotive wire is typically rated to 80-105°C. In contrast, thermosetting insulation (like cross-linked types) will not soften or melt when exposed to heat. It's more expensive and requires a cure process when extruded, but offers better low temperature properties and higher temperature potential (rated to 125°C). Thermosetting materials are also more forgiving when exposed to overloads.","context":["What types of insulation are on automotive wire?\nThere are two main categories of automotive wire – PVC and Cross-Linked. The biggest difference between the two categories is temperature range. Cross-linked automotive wire can withstand much higher temperatures than PVC automotive wire.\nThe three main types of PVC automotive wire are:\n- GPT - used for general circuit wiring and rated to 80 °C\n- TWP - lead-free, thin wall automotive wire rated to 105 °C\n- HDT - heavy wall automotive wiring rated to 80 °C\nPVC is insulation is extruded, which is created by heating PVC and then extruding it through a die on the stranding. This insulation can be melted with a heat source, changing the form.\nThe three most common types of cross-linked automotive wire are:\n- GXL – thin wall, most common type, works with most standard automotive connectors, rated to 125 °C\n- SXL – standard wall, rated to 125 °C\n- TXL – extra thin wall, best for applications that require minimal size and weight, rated to 125 °C\nCross-linked insulation is created by extruding the material through a tube, under heat and pressure, in order to 'cross-link' or change the molecules of the insulation to another state.\nClick here to watch our video on automotive wire or read the transcript!\nHow do you determine the Gauge (AWG) of the automotive wire?\nMake a small cut about 1/2\" long and remove the insulation on the automotive wire. Then you will need to count the individual strands of copper. Next use a micrometer and measure one of the strands. Also count the total number of strands that are present in the automotive wire. Look at the following information to determine the gauge of your automotive wire.\n- 7/.028 = 20 (7 strands that measure .028 each equals 20 gauge)\n- 16/.030 = 18 AWG\n- 19/.029 = 16 AWG\n- 19/.027 = 14 AWG\n- 19/.025 = 12 AWG\n- 19/.023 = 10 AWG\n- 19/.021 = 8 AWG\n- 37/.021 = 6 AWG\nWhat are some of the different types of battery cable?\nBattery cable is large automotive cable. Like smaller types of automotive wire, it is available in PVC and cross-linked forms. One type of PVC battery cable is SGT cable. It is rated to 80°C. SGT can be used in starters or battery grounds.\nCross-linked battery cables can also be used in starter and battery ground applications, but they are more resistant to heat, abrasion, and aging than PVC cable. Two types of cross-linked battery cable are SGX and STX. They are rated to 125°C. Of the three types of battery cable, STX has the thinnest wall, making it a popular choice for automotive applications with limited space.\nHow can I customize my automotive wire?\nAWC offers several customization services for automotive wire. Both PVC and cross- linked automotive wire can be printed with custom text or company logos. They can also be striped. Up to three stripes, called tracers, can be added to the cable’s jacket. Both striping and printing make your automotive wire easy to identify, saving you time. PVC automotive wire may also be dyed for easy identification.\nWhat are other common types of automotive wire and cable and what kinds of applications are they used for?\nAside from the automotive primary wire and battery cable mentioned above, trailer cable, automotive brake cable, ignition wire, fusible link wire and SRML wire (high temperature motor lead wire) are all considered automotive wire and cable. Trailer cable can be used on trailers and trucks as well as in other applications where resistance to weather, oil, and grease is necessary. SRML wire is flexible and fire resistant and can be used as motor lead wire or as lead wire for high temperature electrical equipment applications. Automotive brake cable is used for electric brakes in cars, trucks, and trailers. Common applications for primary wire include general circuit wiring and wiring in engine compartments while battery cable is intended for use in starters and battery ground circuits. Car speaker wire is designed for use in radios, music systems, public address systems, and other low voltage applications.\nWhat are some specifications and standards I should consider when choosing automotive cable?\nCommon standards for the automotive industry include those created by the Society of Automotive Engineers (SAE), the American Society for Testing and Materials (ASTM), Underwriters Laboratories (UL), and the Recreational Vehicle Industry Association (RVIA). All of these organizations develop standards for the automotive industry to encourage the manufacture and use of safe and high quality automotive wire and cable. More important specifications to consider are those set by individual car manufacturers. For instance, all of Allied’s primary wire meets both Ford and Chrysler specifications.","Popular Jacket Types\nThermoplastic vs. Thermosetting\nTHERMOPLASTIC: A material which will soften, flow, or distort when subjected to sufficient heat and pressure. These compounds are heated and extruded over conductor. Likewise, the insulation on the finished product can be re-melted or soften if exposed to heat.\n- Easy to manufacture\n- Normally less expensive\n- No cure required\n- Will melt when subjected to heat\n- Can be extruded in very thin walls\nTHERMOSETTING: A material which will not soften, flow, or distort when subjected to heat and pressure. Once extruded over conductor, these compounds will not re-melt, however, they can be burnt or deteriorate due to heat.\n- Will harden and age when overheated\n- Forgiving when exposed to overloads\n- Better low temperature properties\n- Higher temperature potential\n- Usually more expensive\n- Requires a cure process when extruded\n- Not extruded smaller than 22 AWG in CV processes. Irradiated products can be extruded in smaller sizes.\nPOLYVINYL CHLORIDE PVC, sometimes referred to as vinyl or polyvinyl chloride, consists of three types of vinyl compounds - standard, semi-rigid and irradiated. Depending upon the formulation the rated temperature may vary from -55 C to 105 C. Typical dielectric constant values can vary from 2.7 to 6.5\nSTANDARD PVC, rated for 1000 volts or less, is used for hook-up, computer and control wires. Different compounds are used for 60C, 80C, 90C and 105C service along with commercial and military applications.\nSEMI-RIGID PVC (SRPVC) is much tougher than standard vinyl. It has greater resistance to abrasion and cut-through and offers more stable electrical properties.\nIRRADIATED PVC has improved resistance to abrasion, cut-through, soldering and solvents. Irradiation changes the vinyl from a thermoplastic to a thermosetting material.\nPOLYETHYLENE (PE) is a very good insulation as it offers a low dielectric constant, a stable dielectric constant over all frequencies and a very high insulation resistance. In terms of flexibility, polyethylene can range from stiff to very hard depending on molecular weight and density. Low density is the most flexible, while high density and high molecular weight formulations are very hard. Moisture resistance is excellent, however, both types are flammable. Brown and Black formulations have excellent weather resistance. The dielectric constant is 2.3 for solid insulation and 1.5 for cellular (foamed) designs.\nRULAN is a flame retardant polyethylene which contains additives to inhibit the rate of burning. These additives have only a slight effect on physical or electrical properties of the insulation.\nPROPYLENE (Solid and Cellular) is similar in electrical properties to polyethylene. This material is primarily used as an insulation. Typically, it is harder than polyethylene, making it suitable for thin wall insulations. UL maximum temperature ratings may be 60C or 105C. The dielectric constant is 2.59 for solid and 1.55 for cellular (foamed) designs.\nKYNAR has great mechanical strength, superior resistance to abrasion and cut-through and substantially reduced cold-flow which makes it an excellent back plane wire insulation. Kynar is self-extinguishing, radiant resistant and rated at 135C.\nTEFZEL (ETFE) is rated at 150C, has very good electrical properties, chemical inertness, high flex life and exceptional impact strength. Tefzel can withstand an unusual amount of physical abuse and is self-extinguishing. Tefzel is a registered trademark of DuPont Corporation.\nHALAR (ECTFE) has a specific gravity of 1.68, the lowest of any fluorocarbon. Its dielectric constant and dissipation factor at 1 Mhz are 2.6 and 0.013 respectively. Halar chars, but does not melt or burn when exposed to direct flame and immediately extinguishes on flame removal. Its other electrical, mechanical, thermal and chemical properties are almost identical with Tefzel's. Its temperature rating is -70C to 150C. Halar is a registered trademark of Ausimont Corporation.\nTEFLON (FEP) is extrudable in a manner similar to PVC and polyethylene, allowing for long wire and cable lengths. FEP has excellent electrical characteristics, chemical inertness and a service temperature of 200C. Teflon is a registered trademark of DuPont Corporation.\nTEFLON (TFE) is extrudable in a hydraulic ram type process. Lengths are limited due to the amount of material in the ram, thickness of the insulation, and preform size. TFE must be extruded over a silver or nickel coated wire, with ratings at 260C and 200C respectively. Teflon is a registered trademark of DuPont Corporation.\nPFA is the latest addition to DuPont's Teflon resins. Like the others it has outstanding electrical properties, high operating temperature (250 C), resistance to virtually all chemicals and flame resistance. PFA is a registered trademark of DuPont Corporation.\nTHERMOPLASTIC RUBBER (TPR) has properties similar to those of vulcanized (thermosetting) rubbers. The advantage is that processed like thermoplastics, it is extruded over the conductor. Like many conventional rubber materials, TPR is highly resistant to oils, chemicals, ozone and other environmental factors. It has low water absorption and excellent electrical properties, and is very flexible with good abrasion resistance.\nCHLOROSULFONATED POLYETHYLENE (CSPE), better known as Hypalon, is sometimes used as a 105C rated motor lead wire insulation, but most often as a jacketing compound. Hypalon has excellent tear and impact strength, excellent abrasion, ozone, oil, and chemical resistance and good weathering properties. This material also has low moisture absorption, excellent resistance to flame and heat, and good dielectric properties. Hypalon is a registered trademark of DuPont Corporation.\nSILICONE is a soft insulation which possesses a typical temperature range from -80C to 250C. It has excellent electrical properties plus ozone resistance, low moisture absorption, weather resistance and radiation resistance. Silicone typically has low mechanical strength and poor scuff resistance. While silicone rubber burns slowly, it forms a non-conductive ash which, in some cases, can maintain the integrity of the electrical circuit.\nETHYLENE PROPYLENE RUBBER (EPR) is a chemically cross-linked, thermosetting high temperature rubber insulation. It has excellent electrical properties combined with outstanding thermal stability and flexibility. EPR's resistance to compression, cutting, impact, tearing and abrasion is good and is not attacked by acids, alkalis and many organic solvents. It is also highly moisture resistant. EPR has temperature ratings up to 150C.\nCROSS-LINKED POLYETHYLENE (XLP) is a material which has greater resistance to environmental stress cracking, cut-through, ozone, solvents and soldering than either low or high density polyethylene. Sometimes designated as XLPE. Can be cross-linked either chemically or irradiated.\nSEMI-BUTYL RUBBER (SBR) is flexible and offers good heat and moisture resistance at an economical cost. It must be jacketed for mechanical and chemical protection. SBR is suitable for 75C max temperature ratings.\nNote: ratings are based on average performance of general purpose compounds. Any given property can usually be improved by the use of selective compounding"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:58a8c33e-3f36-4694-b91a-08dd525cc59d>","<urn:uuid:73b57f56-251f-4b74-bd68-6318aa3e4931>"],"error":null}
{"question":"¿Cuál es la conexión entre el tumor suppressor p53 y the long non-coding RNA Gm15290 in non-small cell lung cancer progression?","answer":"The p53 tumor suppressor and Gm15290 lncRNA play opposing roles in non-small cell lung cancer (NSCLC). While Gm15290 promotes cell invasion and proliferation in NSCLC cells, p53 acts as a crucial tumor-thwarting gene that helps prevent lung cancer by blocking PDL1 and protecting immune responses. p53 is frequently damaged or under-expressed in 70% of lung cancers, while Gm15290 shows increased levels in NSCLC tissues compared to normal tissues.","context":["Long non-coding RNAs (lncRNAs) have already been involved with occurrence and progression of multiple cancers. marketed cell apoptosis. Subsequently, we explored the fundamental mechanism by which Gm15290 promoted cell invasion and proliferation. The result of RNA cross types bioinformatic tool uncovered that Gm15290 possibly interacted with tumor suppressor which shown an opposite appearance design in the cell lines and a solid negative correlation using the degrees of Gm15290 in NSCLC sufferers (r2 = 0.9677, and increased the proteins degrees of target genes, including mimic could antagonize the marketing aftereffect of Gm15290 in cell invasion and proliferation. was transcribed through the web host gene homeobox C4 on Chromosome 12 in individual . Several research have uncovered the tumor suppressive function of in a few parenchymatous tumors, including hepatocellular carcinoma and pancreatic ductal adenocarcinoma [23,24]. It had been confirmed that could focus on multiple oncogenes straight, suppress their appearance, and inhibit their mediated tumor metastasis and development. In today’s research, we explored the function of Gm15290, a quite uncovered lncRNA recently, in the invasion and proliferation of NSCLC cells. The known degrees of Gm15290, in the NSCLC tissue weighed against adjacent normal tissues and in the human normal lung epithelial cell line compared with NSCLC cell lines, were detected. Then, different concentrations of pcDNA-Gm15290 expression vector and Gm15290 siRNA were respectively transfected into A549 NSCLC cells LY2835219 reversible enzyme inhibition to uncover its exact role in cell proliferation and invasion. Moreover, we found that the role of Gm15290 in NSCLC progression was related to mimic were designed, synthesized, and validated effective by Ribobio Company (Guangzhou, China). For transfection, the cells were seeded into six-well plates at the density of 105/cm2. On reaching 70% of confluence, the pcDNA-Gm15290, Gm15290 siRNA, and mimic were individually transfected or co-transfected into the A549 cells with Lipofectamine 3000 (Invitrogen) according to the manufacturers instructions. Cell proliferation, apoptosis, and invasion analysis Cell proliferation was evaluated using the Cell Counting Kit-8 (CCK-8; Sigma, St. Louis, MO) assay. The cells were incubated for 24, 48, and 72 h before adding 200 l of CCK-8 reagent to each well and incubated at 37C for 2 h. Cell proliferation was measured by absorbance at 450 nm wavelength using a microplate reader (Bio-Rad, Hercules, CA). Cell apoptosis was detected with a PI/AnnexinV Cell Apoptosis Detection Kit (Sigma). Following transfection for 48 h, 106 cells (in 1 ml medium) were washed with cold PBS and centrifugated at 1000 rpm for 5 min. The cells were resuspended by 10 l of AnnexinV-FITC solution that followed by a 15-min incubation on ice. Then, the cells were transferred into the detection tube with 500 l of PBS and 5 l of PI solution. After another 2 min, the cells were analyzed by a flow cytometry (Bio-Rad). The percentage of early apoptotic cells (AnnexinV+PI?) was calculated. Cell invasion was detected with the transwell cell invasion assay. Briefly, the assay was performed with a Matrigel (Sigma) coated on the upper surface of the transwell chamber (Corning, Lowell, MA). The cells that had migrated through the membrane were fixed with methanol and stained with crystal violet. Photographs of three randomly LY2835219 reversible enzyme inhibition selected fields of the stained cells were taken, and cell numbers were counted by a Countess Automatic Cell Counter (Invitrogen). Real-time LY2835219 reversible enzyme inhibition quantitative PCR Total RNA was LY2835219 reversible enzyme inhibition isolated using TRIzol reagent (Invitrogen). Real-time qPCR reactions were carried out in a 25-l system using SYBR Premix Ex Taq (TaKaRa), 0.4 mM of each primer, and 200 ng of cDNA template. Specific primers for Gm15290, 18S RNA mature, bound by Gm15290 The biotinylated DNA probe complementary to Gm15290 and negative control probe were designed and synthesized by Invitrogen and dissolved in 500 l of binding buffer (0.5 Rabbit polyclonal to ARFIP2 M NaCl, 20 mM Tris-HCl, pH 7.5, and 1 mM EDTA). The probes were incubated with streptavidin-coated magnetic beads (Sigma) at room temperature for 3 h to obtain probe-coated magnetic beads. Cell lysates were incubated with probe-coated beads, and the RNA complexes pulled down were eluted and extracted for the following Northern blot analysis. The RNA complexes were run on a 15% polyacrylamide-urea LY2835219 reversible enzyme inhibition gel and transferred to positively charged nylon membranes (Millipore) followed by cross-linking through UV irradiation. The membranes were subjected to hybridization with 3-digoxigenin-labeled probes overnight at 4C. The probe and U6 RNA probe were labeled with digoxigenin using a 3-End Digoxigenin Labeling Kit (Roche). The detection was performed using a Digoxigenin Luminescent Detection Kit (Roche) according to the manufacturers instructions. Pull-down assay with biotinylated miRNA A549 cells were transfected with 50 nM of wild type biotinylated or.","Tumor-suppressor p53 regulates protein that stifles immune attack on cancer\nMD Anderson News Release November 20, 2015\nResearch makes microRNA case for blocking PDL1; related drug in phase I clinical trials\nMD Anderson News Release 11/20/2015\nA crucial tumor-thwarting gene protects an immune attack against lung cancer by blocking the key to an off switch on T cells, the customized warriors of the immune system, a team led by researchers at The University of Texas MD Anderson Cancer Center reports in the Journal of the National Cancer Institute.\n“Identifying this role for tumor-suppressing p53 provides both a potential biomarker for response to important new cancer immunotherapy drugs and a possible new therapeutic pathway for treatment,” said James Welsh, M.D., associate professor of Radiation Oncology at MD Anderson and senior author.\nThis preclinical research shows an experimental drug currently in phase I clinical trials can replace the immunity-protecting role lost when p53 fails.\nThe p53 gene is damaged, missing or under-expressed in 42 percent of common cancers and 70 percent of lung cancers. It’s by far the most common mutation in cancer. Lung cancer is the leading cause of cancer death in the United States, with an estimated 221,200 new diagnoses and 158,040 deaths in 2015, according to the National Cancer Institute.\nScientists have long known that p53 plays a central role in cancer control by regulating a process that forces abnormal cells to repair themselves and, failing that, to kill themselves.\nWelsh and colleagues found that p53 also blocks a protein called PDL1 that tumor cells can wield to halt immune attack. Like a key, PDL1 connects with and activates a checkpoint molecule called PD1 found on the surface of T cells that shuts down those killer white blood cells. Two PD1-inhibiting drugs, pembrolizumab (Keytruda) and nivolumab (Opdivo) were approved this year for treatment of metastatic lung cancer. Both drugs help a significant fraction of patients, but not all.\np53 launches miR-34a to thwart PDL1\nFirst author Maria Angelica Cortez, Ph.D., instructor of Experimental Radiation Oncology, identified the mechanism by which p53 blocks expression of PDL1.\n“The interaction is specific: p53 activates the micro RNA miR-34a, which in turn directly blocks expression of PDL1,” said Cortez. “If you lose p53 function, then miR-34a is lost and PDL1 is over-expressed.”\nUnlike messenger RNAs produced by genes that lead to production of specific proteins, micro RNAs do not code for proteins but instead regulate other genes.\nWhile p53 had been linked to other aspects of immune response, the JNCI paper is the first to connect it to immune evasion by tumors and regulation of PDL1.\nThe team conducted a series of experiments in cell lines, miRNA target-predicting databases and tumor samples from non-small cell lung cancer patients. Then in a mouse model of NSCLC they showed that MRX34, alone or with radiation therapy, reduced PDL1 expression, preventing T cell exhaustion.\nMRX34, a first-in-class miR-34-based cancer therapy being developed by Mirna Therapeutics in Austin, Texas, packages a synthetic version of natural miR-34a in a fatty nanoparticle called a liposome. The drug is in phase I clinical trials for advanced solid tumors, liver cancer and hematological malignancies at MD Anderson and other cancer clinics.\nThe researchers analyzed tumor samples from The Cancer Genome Atlas of 181 patients with NSCLC. Expression of p53 and PDL1 were inversely correlated, when one was high, the other was low and vice-versa. Tumors with p53 mutated had higher levels of PDL1 and lower levels of miR-34a.\nHigh levels of p53, miR-34a increase survival\nPatients with either low PDL1 and high p53 expression or with high p53 and miR-34a levels had longer median survival than those with low expression of p53 and miR-34a and higher PDL1.\nForced expression of miR-34a in NSCLC cell lines suppressed PDL1. Injecting MRX34 into lung cancer tumors in mice increased levels of miR-34a and reduced levels of PDL1. The team also showed that miR-34a binds to a specific site on the PDL1 gene to block its expression.\nThe researchers randomized mice to control, MRX34, radiation therapy or a combination of MRX34 and radiation. The treatment arms all led to increased numbers of T cells infiltrating the tumor, reduced numbers of T cells positive for the PD1 checkpoint, and slowed tumor growth, with the combination having the strongest effects.\nStudies under way include a retrospective analysis of the clinical outcome of patients treated with PD1 inhibitors to see whether p53 or miR-34a status at the initial biopsy predicted response.\nWhile patients with PDL1 in their tumors have a higher response rate to PD1 checkpoint inhibitors, a fraction of patients without the biomarker also respond to these drugs. So additional biomarkers are sought to further guide treatment, Cortez said.\nIn the lab, the team is combining MRX34 with a PD1 inhibitor to see if the combination improves tumor response, Welsh said.\nCo-authors with Welsh and Cortez, are Cristina Ivan, Ph.D., and George Calin, M.D., Ph.D., of Experimental Therapeutics; David Valdecanas, Xiaohong Wang, Ph.D., and Yuping Ye, M.D., of Experimental Radiation Oncology; Ritsuko Komaki, M.D., Daniel R. Gomez, M.D., and Sunil Krishnan, M.D., of Radiation Oncology; Luiz Araujo, M.D., David Carbone, M.D., Ph.D., and Konstantin Shilo, M.D., of Ohio State University; Heidi Peltier, Kevin Kelnar, Desiree Martin, and Andreas G. Bader, Ph.D., of Mirna Therapeutics, Inc., Austin, Texas; and Dipak Giri, Ph.D., of Sipaumdi Pathology Consultancy, Pearland, Texas.\nThis research was funded by grants from the Doctors Cancer Foundation, The Lung Cancer Research Foundation, MD Anderson’s cancer center support grant from the National Cancer Institute of the National Institutes of Health (CA016672), Mr. Tamotsu Mabuchi, the family of Mr. Mohammed A. Hamed, Mr. and Mrs. Peter Goodwin, and the Orr Family Foundation to MD Anderson Cancer Center’s Thoracic Radiation Oncology program, an MD Anderson Knowledge Gap award, the U.S. Department of Defense and the E. L. Wiegand Foundation. Calin is an Alan M. Gewirtz Leukemia & Lymphoma Society Scholar and is supported in part NCI grants (1UH2TR00943-01 and 1 R01 CA182905-01). Mirna Therapeutics has received a commercialization grant from the Cancer Prevention and Research Institute of Texas."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:224fd12e-6263-49e9-889a-4ed8d75ba9d6>","<urn:uuid:8c181a86-8bb9-4116-9d16-a133bd49764b>"],"error":null}
{"question":"I work in the food industry - can citric acid and benzoic acid both be used as food preservatives? 🤔","answer":"Yes, both citric acid and benzoic acid are used as food preservatives. Citric acid (E330) acts as a preservative because its acidic pH prevents many bacteria from thriving. It is commonly used in jams, jellies, candy, canned foods and meat products. Similarly, benzoic acid (E210) is used in food preservation, particularly in dried and pickled food products, as it helps curb the growth of fungi and bacteria. Benzoic acid is especially important in the growing packaged food industry, where it helps extend shelf-life of products.","context":["1. Food Acids\nFood acids are added in order to make the flavours of the food to be 'sharper' or to have a 'stronger taste'. Food acids also act as preservatives and antioxidants.\nFood acids do occur naturally. These acids in natural food products give them a distinct flavour or tinge. Some examples of naturally occurring acids in foods are citric acid, malic acid and tartaric acid.\n*Naturally Occurring Food Acids\na. Citric Acid - Citrus Fruits (Lime, Lemon, Orange)\nb. Malic Acid - Apple\nc. Tartric Acid - Grapes, Pineapples, Potatoes, Carrots\nd. Acetic Acid - Vinegar\ne. Oxalic Acid - Tea, Cocoa, Pepper\nf. Tannic Acid - Tea\ng. Caffeotannic Acid - Coffee\nh. Benzoic Acid - Cranberries, Prunes, Plums\ni. Butyric Acid - Decomposition of Butter\nj. Lactic Acid - Milk Digestion\n(*taken from HERE)\nThe Production of Citric Acid as Food Additive\nNaturally, citric acid is found in citrus fruits especially lemons, limes and oranges. It is interesting to note that citric acid is produced by going through the citric acid cycle. Mold and bacteria also produce citric acid!\nCitric acid is used for its sour flavour, the ability to preserve foods and acts as a pH buffer. Thus, citric acid is added to many manufactured food products.\nIn 1917, an American food chemist by the name of James Currie, discovered that the mold, Aspergillus niger was able to produce citric acid by metabolizing sucrose or glucose. And this method was efficient and cheap! Thus it became a profitable business.\nCitric acid is used as a flavour enhancer in beverages. It is used in soft drinks, teas, and juices to create a slightly tart flavour.\nThe pH which is acidic also functions as a preservative as many bacteria are unable to thrive in an acidic environment. Thus it is suitable to be used in jams, jellies, candy, canned foods and meat products.\nCitric acid can be used in dry foods too as it can be produced in powder form.\nThe E number of citric acid is E330.\nThe Production of Malic Acid as Food Additive\nMalic acid contributes to the sourness of green apples. It is also present in grapes and in most wines. In rhubarb, the taste of malice acid is very sharp and clear.\nIt is used as an artificial vinegar flavour in 'Salt & Vinegar' flavoured potato chips.\nThe E number for malic acid is E296.\nTo read more about malic acid, click on LINK.\nThe Production of Tartric Acid as Food Additive\nNaturally, tartaric acid occurs in grapes, bananas and tamarinds.\nIn baking, tartaric acid is combined with baking soda to form baking powder, which is a leavening agent.\nTartaric acid is used as a food additive in sour-tasting sweets.\nIn the pharmaceutical industry, tartaric acid combined with citric acid is used to improve the taste of oral medication.\n*It is very interesting to note that tartaric acid is a muscle toxin which inhibits the production of malic acid and when used in high doses causes paralysis and death! (*taken from HERE)\nThe E number for tartaric acid is E334.\nThe Production of Acetic Acid as Food Additive\nThis acid has a very pungent smell and a distinctive sour taste.\nIn the food industry, acetic acid is labelled as E260. It is used as an acidity regulator and as a condiment. It is also used in pickled food.\nTo read more on acetic acid, click on LINK.\nThe Production of Tannic Acid as Food Additive\n*Tannic acid is the commercial form of tannin which is the basic ingredient in the chemical staining of wood. This occurs naturally in oak, walnut and mahogany.\nIn food, tannic acid is used in processing beer. It is also used as an aroma compound in soft drinks and juices. In the wine industry, it is used as a colour stabiliser and taste enhancer.\nInterestingly, it currently does not hold an Enumber as it is not considered as a food additive but as a food ingredient.\nIn the pharmaceutical industry, tannic acid is used to produce albumin tannate which is used as an anti-diarrhoea agent. It is also used in some anti-histamines to either act as a stabiliser or as a slow-release.\n(*take from HERE)\nThe Production of Benzoic Acid as Food Additive\nBenzoic acid is found naturally in berries. It is even produced by certain species of animals!\nThis acid is used in dried and pickled food products.\n*In the pharmaceutical industry, benzoic acid is combined together with salicylic acid to treat skin irritation and inflammation caused by burns, insect bites, fungal infarctions and eczema.\n(*taken from HERE)\nThe E number of benzoic acid is E210.\nTo read more about citric acids in food production, click on LINK and LINK.","Global Benzoic Acid Market: Brief Account\nThe global benzoic acid market will witness a strong demand during the forecast period of 2018 – 2026 due to it wide variety of applications in food and beverage industry. Benzoic acid is a colorless and crystalline aromatic carboxylic acid. Benzoic was initially extracted from resin of tree that belonged to the Styrax genus. Benzoic acid is extensively used for preserving food products in food and beverage industry. It helps in curbing the growth of fungi and bacteria. Benzoic acid is also used as a chemical reagent. Surge in demand for packaged food and beverages due to rise in working class people and rapid urbanization have fueled the demand for benzoic acid. Increase in working hours and need for nutrient rich food have made consumers to opt for packaged food instead of homemade meals.\nGlobal Benzoic Acid Market: Trends and Opportunities\nThe global benzoic acid market is riding on the high demand for packaged food and beverages throughout the world. Benzoic acid is used to preserve these food products thus increasing the shelf-life of these products. Benzoic acid is also used for preserving acidified food products like pickles, sparkling drinks, and fruit juice, as the ability to produce desired results depends on the pH of food. Rising demand to substitute phthalate plasticizers for different applications is expected to boosts the global benzoic acid market growth.\nApart from food and beverage industry, benzoic acid also finds its application in plastics, pharmaceutical, chemical, and other industries. Benzoic acid can also finds its application for animal feed, dye intermediates, and medical purposes. Benzoic acid is used for treating fungal skin diseases such as athlete’s foot, tinea, ringworm, etc. All these factors are anticipated to boosts the global benzoic acid market growth. Furthermore, benzoic acid plays an integral role in manufacturing cosmetics products. Benzoic acid is an import ingredient in the toothpastes, deodorants, mouthwashes, etc. In these cosmetic products, benzoic acid acts as an anti-microbial agent. Apart from these, the cheap price and easy availability of benzoic acid is likely to favor the global benzoic acid market growth.\nGlobal Benzoic Acid Market: Regional Outlook\nThe global benzoic acid market is segregated into Europe, Latin America, Middle East and Africa, North America, and Asia Pacific. Of these, Asia Pacific has seen a gradual rise in consumption of benzoic acid over past few years, owing to rise in per-capita disposable income along with gradual shift towards packaged foods. The major contributor in this region are China and India. North America is anticipated to expand at a robust CAGR during the assessed period.\nGlobal Benzoic Acid Market: Companies Mentioned\nSome of the key players in the global benzoic acid market are Chemcrux Enterprises ltd, Emerald Kalama Chemical, Shri Hari Chemicals, Huangashi Taihua Industry, and Novaphene.\nThe report offers a comprehensive evaluation of the market. It does so via in-depth qualitative insights, historical data, and verifiable projections about market size. The projections featured in the report have been derived using proven research methodologies and assumptions. By doing so, the research report serves as a repository of analysis and information for every facet of the market, including but not limited to: Regional markets, technology, types, and applications.\nThe study is a source of reliable data on:\n- Market segments and sub-segments\n- Market trends and dynamics\n- Supply and demand\n- Market size\n- Current trends/opportunities/challenges\n- Competitive landscape\n- Technological breakthroughs\n- Value chain and stakeholder analysis\nThe regional analysis covers:\n- North America (U.S. and Canada)\n- Latin America (Mexico, Brazil, Peru, Chile, and others)\n- Western Europe (Germany, U.K., France, Spain, Italy, Nordic countries, Belgium, Netherlands, and Luxembourg)\n- Eastern Europe (Poland and Russia)\n- Asia Pacific (China, India, Japan, ASEAN, Australia, and New Zealand)\n- Middle East and Africa (GCC, Southern Africa, and North Africa)\nThe report has been compiled through extensive primary research (through interviews, surveys, and observations of seasoned analysts) and secondary research (which entails reputable paid sources, trade journals, and industry body databases). The report also features a complete qualitative and quantitative assessment by analyzing data gathered from industry analysts and market participants across key points in the industry’s value chain.\nA separate analysis of prevailing trends in the parent market, macro- and micro-economic indicators, and regulations and mandates is included under the purview of the study. By doing so, the report projects the attractiveness of each major segment over the forecast period.\nHighlights of the report:\n- A complete backdrop analysis, which includes an assessment of the parent market\n- Important changes in market dynamics\n- Market segmentation up to the second or third level\n- Historical, current, and projected size of the market from the standpoint of both value and volume\n- Reporting and evaluation of recent industry developments\n- Market shares and strategies of key players\n- Emerging niche segments and regional markets\n- An objective assessment of the trajectory of the market\n- Recommendations to companies for strengthening their foothold in the market\nNote: Although care has been taken to maintain the highest levels of accuracy in TMR’s reports, recent market/vendor-specific changes may take time to reflect in the analysis."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:42607634-c8b6-4687-83ec-d2b45dac53b6>","<urn:uuid:be3b346f-b274-49f4-93a2-a8898a63ca24>"],"error":null}
{"question":"How do small firms differ from large firms in their ability to participate in global value chains in North Africa versus theoretical trade predictions?","answer":"Small and large firms show notable differences in their global value chain participation, contrasting with theoretical predictions. While trade theories suggest that all firms should specialize according to comparative advantage when countries open to trade, the evidence from North Africa shows that firm size significantly impacts participation patterns. Small North African firms mostly serve local markets and struggle to join global value chains, while larger firms show more complex international linkages and are more likely to be two-way traders (both importing and exporting). Large firms are also more likely to possess internationally recognized quality certifications and be foreign-owned, which are crucial for GVC participation. This differs from theoretical predictions of uniform specialization, as size-based constraints like certification costs and meeting quality standards create barriers particularly affecting smaller firms' ability to participate in global production networks.","context":["In the past twenty years, production has been increasingly unbundled and shared across different countries at different levels of development. This fragmentation was triggered by falling transport costs, a reduction in trade barriers, the ICT revolution and the ‘servicification’ of the economy. Trade, especially of intermediate goods, and foreign direct investments have increased substantially, with important shifts in their composition and in global patterns of specialization (Koopman et al., 2014). This expansion of global production networks has increased the opportunity for small and medium-sized enterprises (SMEs) in developing and emerging economies to enhance their competitiveness.\nThanks to the division into individual tasks, SMEs do not necessarily need to develop the domestic capacity to perform all major production steps nor the expertise to export; they now have the opportunity to better exploit their comparative advantage by entering a value chain as (specialized) suppliers or as subcontractors, even several levels down from the ultimate buyer (Humphrey & Schmitz, 2002). The tasks involved can be highly complex, spanning from manufacturing to logistics and transportation (Baldwin & Venables, 2013).\nThese developments, together with the increasing complementarities between goods and services, imply that any successful trade policy should explicitly consider the goods-services nexus, and ‘think value chains,’ as clearly pointed out by Hoekman (2014). Another important implication is that the evolution of global market shares, in many cases, is no longer a suitable indicator of a country’s competitiveness and there is a need for new statistics to account for firms’ activity at the global level.\nRecent studies show that shifting the focus from traditional market shares (gross exports) to shares in value added does not change the overall picture much; however, the story behind the gains or losses in market share may differ quite substantially (Benkovskis & Wörz, 2015). Against this background, Giovannetti et al. (2015), analyzing the Italian case, argue that joining international supply chains may trigger an increase in productivity and competitiveness for smaller and less productive firms, by providing incentives and opportunities to upgrade their technical capabilities.\nParticipation in a supply chain and cooperation within a network of upstream and downstream partners can also enhance interfirm and intrafirm information flows and learning possibilities, and introduce new business practices and more advanced technology, in turn enhancing growth. Hence, to increase country and business competitiveness, the reallocation of resources from less productive activities to new and more connected ones is crucial.\nThis note discusses some of the potential benefits of supply chain participation for SMEs. The discussion is based on new empirical evidence for North African firms, which represent a relevant but still understudied case. While there are large potential gains for the area, firms have not been able, so far, to fully integrate into international production networks. In what follows, we examine internationalization modes of North African firms and discuss certain policy implications related to the new patterns of value-added trade.\nGlobal Value Chains: New Opportunities for North African Firms\nSince the mid-nineties, several developing and emerging countries, especially in Asia, have been able to exploit the new opportunities offered by the participation in global value chains (GVCs). Unlike Asian countries, and China in particular, North Africa has not been able, so far, to intercept the main changes in trade patterns nor enter massively into global networks. Despite a relatively good geographic and logistics positioning, most North African firms, especially the smaller ones, have mainly remained ‘local,’ producing at home and for the domestic market. Their involvement in GVCs is still limited and mostly in low value-added phases.\nAlternative indicators of value chain participation suggest that North African involvement is rather limited (Del Prete et al., 2016a). Factors often cited for their limited participation are inadequate infrastructures, a lack of reliable information, and a high degree of uncertainty in contract enforcement. This, in particular, can generate distrust between parties from different countries, especially on the North and South shore of the Mediterranean Sea, limiting their willingness to engage in transactions where suppliers must often customize their production to the specifications of particular buyers.\nBut as China and other Asian countries move up the value chain, North African countries may have the opportunity to become the next hub of labour intensive production and expand technological sectors. Participation in international supply chains provides chances to diversify production and trade, and is associated with learning, technology transfers, and, often, knowledge spillovers, much needed in North Africa.\nUnlike Asian countries, North Africa has not been able, so far, to intercept the main changes in trade patterns nor enter massively into global networks. Despite a relatively good geographic and logistics positioning, most North African firms have mainly remained ‘local,’ producing at home and for the domestic market\nIn what follows, we discuss the supply chain involvement of firms from selected North African countries included in the original World Bank Enterprise Survey database. The dataset provides information on a panel of 930 manufacturing firms in Egypt and Morocco active in 2004 and 2007, and certain characteristics, such as size, ownership, trading status and performance (see Del Prete et al., 2016b, for a detailed description).\nA first analysis of the data suggests that, like firms from other countries, North African firms are characterized by different modes of internationalization, involving different levels of complexity between domestic and foreign status.\nCHART 1 Internationalization Mode and Size\nChart 1 shows that, not surprisingly, the share of traders tends to increase with the firms’ size (small firms have from 5 to 19 employees, medium between 20 and 99 and large above 100). Smaller firms are more likely to only serve the local market, while as size increases, international linkages become more complex, as suggested by the higher share of two-way traders, i.e. import-export firms. This confirms a well-known finding of the trade literature, according to which larger firms perform relatively better, are more prone to paying the sunk costs of internationalization and have a higher propensity to reach further afield and more developed markets (Mayer & Ottaviano, 2008).\nChart 2 shows that larger firms are also more likely to be foreign-owned and possess internationally recognized quality certifications (e.g. ISO 9000 or 14000, or HAPC).\nCHART 2 Certifications, Ownership and Size\nImportantly, internationally recognized certifications and standards are increasingly regarded by multinationals as necessary features that guarantee and signal the ability of the firm to meet the quality levels typically required in vertically fragmented production processes (Beghin et al., 2015; Del Prete et al., 2016b). This is becoming more and more relevant given the increasingly complex buyer-supplier relations in the exchange of customized inputs, which entail firms operating on a global scale to develop a very high level of coordination along the chain. In order to participate in global production networks, firms have a clear incentive to afford the cost and apply for internationally recognized certifications, and sometimes this is not even an option. Nadvi (2008) argues that compliance with international standards is now a sine qua non condition for entry into globalized networks, and this is even more true for firms in developing countries, whose production is often considered of lower quality. Hence, certified international traders, especially in developing countries, are likely to be operating in a value chain, which allows us to identify GVC participation through certifications.\nNorth African countries may have the opportunity to become the next hub of labour intensive production and expand technological sectors. Participation in international supply chains provides chances to diversify production and trade\nNot surprisingly, summary statistics show that the share of internationalized firms is always higher among certified firms. On the one hand, quality certifications tend to be strongly associated with internationalization, as 84% of certified firms are also international traders. On the other hand, certified firms are only 22% among traders, while 97% of domestic firms are uncertified and possibly not (yet) involved in GVCs. Overall, almost all certified firms are also internationalized, although certifications seem to capture a specific feature characterizing only some of the traders (Table 1).\nTABLE 1 Certification and Internationalization Mode\nThe fact that GVC participation requires firms to meet specific standards is already a sign that certified firms are likely to perform better than others. Moreover, when firms join a global production network, they can further benefit from increased specialization, economies of scale and knowledge spillovers, thus improving their performance. This is shown in Chart 3, in which the productivity distribution of firms in GVCs is shifted to the right compared to that of other firms, suggesting higher levels of productivity.\nCHART 3 Productivity and GVC\nDifferent countries, sectors and firms participate in GVCs to different degrees and in different ways. Recent findings for developing countries that are not yet fully integrated into GVCs, such as North Africa, show that there are very few and specific successful examples of beneficial participation, namely, the aeronautic industry for Airbus, and the automotive, textiles, agri-food and phosphate industries (Del Prete et al., 2016a). The evidence at the firm level is in line with these stylized facts, and shows that smaller firms may greatly benefit from GVCs, but may also find it more difficult to setup the required capacity, especially in terms of standards compliance.\nTwo main policy implications can be drawn. Regardless of a firm’s position in the chain, minimum quality and reliability requirements must be met. The buyers’ sourcing strategies are constantly revised to improve these elements of their supply chains. The complexity and heterogeneity of quality standards and certifications has become a major barrier, in particular for SMEs in developing and emerging countries, adding a significant cost to trade. Upstream firms supplying intermediate inputs to several destinations may have to duplicate production processes to comply with conflicting standards, or to incur burdensome certification procedures multiple times for the same product (Miroudot et al., 2013). In this area, international regulatory cooperation (convergence of standards, certification requirements and mutual recognition agreements) can alleviate the burden of compliance and enhance firms’ competitiveness.\nGVCs therefore need appropriate policies. On the one hand, information gaps have to be reduced by promoting environments that facilitate information exchange between firms\nFor GVCs to have a positive impact on firms’ productivity and countries’ competitiveness, adequate preparation is required. Human capital development can be tailored to the needs of particular segments of the value chain; specialized skills, often associated with industries such as information technology, electronics and pharmaceuticals, but also needed for some phases in more traditional industries, are a prerequisite for involvement in high value-added stages. Policies designed to support education and technical training therefore represent an important tool to increase the gains from global production.\nWhen the proper environmental, regulatory and endowment conditions are met, GVCs can become an important means for linking developing countries to global production and trade, potentially supporting export propensity for SMEs, with possible positive consequences on employment and growth. GVCs also trigger a change in the existing links between currency depreciation and exports: in the presence of backward linkages, it is likely for a depreciation to increase the cost of imported intermediate inputs used in final-good production, thus lowering the competitive gain of the exchange rate difference. With forward linkages, on the other hand, a depreciation increases the competitiveness of downstream producers, which stimulates demand for their goods. Hence, when a firm is in a GVC, the way exchange rate fluctuations affect competitiveness is different to how they would be affected traditionally.\nGVCs therefore need appropriate policies. On the one hand, information gaps have to be reduced by promoting environments that facilitate information exchange between firms (suppliers) in the industry or across industries. Some information gaps can be addressed by improving visibility through certifications, as suggested by the fact that most of the certified firms are also traders and are likely to be involved in GVCs. It is also known that most firms in international production chains screen potential suppliers for compliance with relevant standards. On the other hand, there is the need to decrease the regulations and tariffs that are magnified with multiple crossings of borders. Available policies aimed at internationalization and GVC participation also need to directly involve SMEs. Skill upgrading programmes are often not perceived as facilitating involvement into global markets as the effect tends to be indirect. Finally, there is a need to develop local infrastructure and business environments, and help local clusters to become part of regional and global value chains.\nBaldwin, R., and Venables, A. J. “Spiders and snakes: Offshoring and agglomeration in the global economy.” Journal of International Economics, 90(2), 245–254, 2013. http://doi.org/10.1016/j.jinteco.2013.02.005\nBeghin, J., Maertens, M., and Swinnen, J. F. M. “Non-Tariff Measures and Standards in Trade and Global Value Chains.” Annual Review of Resource Economics, 7, 2015.\nBenkovskis, K., and Woerz, J. “‘Made in China’ How Does It Affect Our Understanding of Global Market Shares?”, ECB Working Paper 1787, April, 2015.\nDel Prete, D., Giovannetti, G., and Marvasi, E. “Global Value Chains: new evidence for North Africa.” Working Paper University of Firenze N. 07/2016, 2016a.\nDel Prete, D., Giovannetti, G., and Marvasi, E. “North African Countries and Firms in International Production Networks.” EUI Working Paper RSCAS 2016/26, 2016b.\nGiovannetti, G., Marvasi, E., and Sanfilippo, M. “Supply chains and the internationalization of small firms.” Small Business Economics, 44(4), 845–865, 2015. http://doi.org/10.1007/s11187-014-9625-x\nHoekman, B. “Supply chains, mega-regionals and multilateralism: a road map for the WTO.” Robert Schuman Centre for Advanced Studies Research Paper No. RSCAS, 27, 2014.\nHumphrey, J. and Schmitz, H. “How does insertion in global value chains affect upgrading in industrial clusters?” Regional Studies, 36(9), 1017–1027, 2002.\nKoopman, R., Wang, Z., and Wei, S.-J. (2014). “Tracing Value-Added and Double Counting in Gross Exports.” American Economic Review, 104(2), 459–494, 2014. http://doi.org/10.1257/aer.104.2.459\nMayer, T., and Ottaviano, G. I. “The happy few: The internationalisation of European firms.” Intereconomics, 2008, 43(3), 135-148.\nMiroudot, S., Rouzet, D., and Spinelli, F. “Trade Policy Implications of Global Value Chains.” OECD Trade Policy Paper, 2013.\nNadvi, K. “Global standards, global governance and the organization of global value chains.” Journal of Economic Geography, 8(3), 323–343, 2008.","In general, international trade theories predict that once countries open up to trade outside their borders they will specialize in goods for which they have comparative advantage. Early theories of trade explained comparative advantage as being driven by relative productivity differences (as explained by David Ricardo) or by relative abundance of factors of production (as explained by Eli Heckscher and Bertil Ohlin). More recent theories incorporate monopolistic competition and firm level analysis in their models to allow countries (or firms within a country) to specialize in varieties of goods in order to explain what economists refer to as intra-industry trade—when two countries trade among themselves products within the same industry, such as cars, for example (Helpman and Krugman 1985, Melitz 2003). All in all, most trade theories would predict that a country transitioning out of autarky will respond to incentives to specialize in either goods or non-perfectly substitutable varieties of goods. This results from international competition driving less productive firms out of the market (and with them their products) and only those firms that are productive enough to compete in world markets surviving. Thus, countries that are more exposed to international trade would specialize further.\nYet, data shows a strong correlation between income levels and diversification of export baskets. The evidence is not always definitive. Figure 1 explores the relationship between both openness to trade and export concentration (i.e., specialization) and income per capita. The left panel uses data for the year 2010 to show that, on average, countries with higher openness to trade (measured as exports as a share of GDP) tend to be richer. The right panel shows that countries that are less concentrated—as measured by the Herfindahl-Hirschman Index of concentration using four-digit export categories—tend to be richer. Notice that a negative correlation between export concentration and income per capita holds in spite of severe outliers, such as natural resource rich countries like Venezuela, Libya, Kuwait, and Saudi Arabia, in the upper right part of the graph.\nIn light of this confounding evidence, what is it that explains the path to development? Is it diversification or specialization? This note explores this question by reviewing the existing literature and attempting to close the gap between the two contradictory sets of evidence.\nThe question on the role of diversification or specialization on economic growth and development has been widely explored in the economic literature. The seminal paper by Imbs and Warcziag (2003), referenced as IW here onward, presents a stylized fact that has become widely accepted in the literature. Using several datasets to measure the concentration of production across different countries and years, IW show that as countries grow, specialization is non-linear. In particular, they show that the specialization pattern follows a U-shaped curve: at low levels of income, countries are highly concentrated, then they diversify and at the higher levels they tend to concentrate again. One of the strengths of their methodology is that their results can be interpreted as a within-country process. These results have been confirmed using export data by Cadot et al. (2011).\nFigure 1: Exports and concentration\nNote: The left panel shows a scatterplot of openess to trade (exports as share of GDP) against PPP-adjusted GDP per capita (in logs). The right panel shows a scatterplot of exports concentration—measured as the Herfindahl-Hirschman Index—against PPP-adjusted GDP per capita. Data sources come from the World Bank’s World Development Indicators, Penn World Tables and trade data from U.N. Comtrade, with corrections by Hausmann et al. (2011).\nYet, the work summarized in Hausmann et al. (2011) shows that countries in the highest level of income and development, much on the contrary, tend to have highly diversified export baskets. That is, the results suggest that, in fact, there is no re-specialization pattern among countries in the highest levels of income.\nThus, these opposing statements beg the question: what is the path to growth and development? Is it concentration or diversification of a country’s export basket? Both paths could be explained by different theories. On one hand, even after countries diversify and grow, they become more integrated in the global economy, allowing them to reconcentrate in a particular set of goods fulfilling the gains from trade that come from comparative advantage. However, in a general equilibrium setting, global integration also implies diffusion of knowledge and this could also result in productivity gains that are reflected in diversification, not concentration (see Bahar et al., 2014; Bahar and Rapoport, forthcoming). Yet, how come there could be such discrepancy in the data?\nIt might be due to measurement peculiarities. In particular, I focus on two aspects of measurement of the data. The first has to do with outliers in the concentration versus income relationship which might be driving the results of re-specialization for richer countries. In particular, the presence in the sample of natural resource rich countries, which tend to have high incomes and also very highly concentrated in their export baskets.\nThe second regards the disaggregation level used in the data. It might well be possible, that at lower levels of disaggregation the re-specialization pattern in richer countries is more prominent, whereas it is not the case when looking at highly disaggregated data. That is, rich countries might specialize in certain aggregated sectors (e.g., electronics or chemicals), but within such sectors these countries remain highly diversified. This is something that can be tested using only different levels of sectoral aggregation in the same analysis. In the next section I test whether taking into account these two issues can bridge the gap between the contradicting evidence in the literature.\nRevisiting Imbs and Warcziag (2003)\nI start by revisiting the evidence presented by Imbs and Warcziag (2003). To do so, I replicate their results using more recent data. I limit the analysis to exports reported in the Harmonized System (HS) categorization, which goes up to six digits in terms of disaggregation, since 1996 until 2011. I also use real income per capita (PPP adjusted) reported by the Penn World Tables 9.0 (Feenstra et al., 2015). The final dataset has data for 114 countries, which is based on the sample of exporter countries used in Hausmann et al. (2011), excluding former Soviet Union countries given the volatility of their data in during the 1990s. I replicate IW methodology for the analysis (see Appendix Section A for more details). When it comes to concentration indexes, I limit the results of this note to the Herfindahl–Hirschman index (HHI) only, but results are robust for a number of different indexes.\nFigure 2 visualizes the relationship between HHI and income per capital using exports data at the four digits level, following the IW methodology. The results are qualitatively consistent with those presented by IW: poor countries are highly concentrated, middle income countries are more diversified, and at high levels of development diversification stops and there is a re-specialization pattern.\nFigure 2: Concentration (HHI, 4 digits) and Income per capita\nNote: This figure estimates the non-linear relationship between export concentration (using the Herfindahl-Hirschman Index based on four-digit HS categorization) and PPP-adjusted income per capita. The estimation follows the technique described in Imbs and Warcziag (2003) and it is explained in Appendix Section A.\nNext I explore the robustness of these results to two measurement peculiarities: the exclusion of natural resource rich countries and the level of disaggregation of the data used to compute concentration.\nNatural resource rich countries\nNatural resource rich countries tend to be outliers in the income per capita versus concentration of exports relationship (Bahar and Santos, 2016). How would these results be affected when excluding this set of countries? I explore this question by reestimating the relationship between concentration and income per capita, following the same methodology as in IW, excluding from the sample countries for which the natural resource rents are, on average, at least 10 percent of their GDP throughout the years of the sample (using data from the World Development Indicators). These countries are Angola, United Arab Emirates, Bolivia, Chile, Cameroon, Congo, Algeria, Ecuador, Egypt, Ethiopia, Gabon, Ghana, Guinea, Indonesia, Iran, Kuwait, Lao, Liberia, Libya, Mongolia, Mozambique, Mauritius, Malaysia, Nigeria, Norway, Oman, Papua New Guinea, Qatar, Saudi Arabia, Syria, Trinidad and Tobago, Uganda, Venezuela, Vietnam, Yemen, and Zimbabwe.\nFigure 3 compares the concentration (using four digit export data) versus income relationship for all countries and for non-natural resource (NNR) countries in the data.\nFigure 3: Concentration and income (All and NNR countries)\nNote: This figure estimates the non-linear relationship between export concentration (using the Herfindahl-Hirschman Index based on 4-digit HS categorization) and PPP-adjusted income per capita. The estimation follows the technique described in Imbs and Warcziag (2003) and it is explained in Appendix Section A. The green line estimates the relationship for all countries and the orange line estimates the relationship for non-natural resource rich countries.\nThe first thing that should be noticed is that, for all levels of income, NNR countries are on average less concentrated (a result that is consistent with the findings of Bahar and Santos, 2016). However, more importantly, the re-specialization pattern, even if still present, is somewhat less pronounced for NNR countries only (orange line). Also the “bump” in the relationship for all countries in the 10K to 20K area in the income per capita range virtually disappears, hinting that it was driven by middle income natural resource rich countries. More generally, without establishing the significance of these results, it is important to notice that the robustness of these results are dependent on the sample of countries used. Yet, I still find some pattern of re-specialization in NNR countries. In the next subsection, I explore whether the disaggregation level of the export data can play a role in explaining it.\nDisaggregation level of export data\nThe level of disaggregation of exports data can play an important role in explaining the documented patterns. Why? Because even if re-specialization occurs at higher levels of economic development, it might be only at highly aggregated sectors. For instance, East Asian countries are concentrated in a few clusters, electronics being the most prominent one, but within such clusters there is wide diversification. Thus, the limitations of data (as well as the conceptualization of what a sector actually is) might bias the interpretation of the results.\nI explore this by using in the same analysis concentration indexes computed using different levels of data disaggregation. Figure 4 presents the relationship between concentration and income per capita following IW’s methodology (lines represent moving averages, for visualization purposes) for the HHI computed using export figures disaggregated at the two, three, and six digit levels, for all countries.\nFigure 4: Concentration and income, different disaggregation levels\nNote: This figure estimates the non-linear relationship between export concentration (using the Herfindahl-Hirschman Index based on two, three, and six digit HS categorization) and PPP-adjusted income per capita. The estimation follows the technique described in Imbs and Warcziag (2003) and it is explained in Appendix Section A. For visualization purposes, the lines are moving averages.\nThe results show that the re-specialization pattern at higher levels of income is much more pronounced the more aggregated data is used to compute the index. This also holds true for NNR countries, as can be seen in Figure 5.\nFigure 5: Concentration and income, different disaggregation levels (NNR countries)\nNote: This figure estimates the non-linear relationship between export concentration (using the Herfindahl-Hirschman Index based on two, three, and six digit HS categorization) and PPP-adjusted income per capita, for non-natural resource rich countries. The estimation follows the technique described in Imbs and Warcziag (2003) and it is explained in Appendix Section A. For visualization purposes, the lines are moving averages.\nWhat are the implication of these results? It implies that while rich countries tend to concentrate in particular sectors, in terms of export varieties, they remain highly diversified, much more than poor countries. As more disaggregated data is being used to compute the level of concentration, the re-specialization pattern documented by IW disappears. Thus, it might be the case that the re-specialization pattern that has been documented by IW is, in fact, that the process of growth is associated with the development of highly diversified clusters of economic activity.\nLooking at the bigger picture\nA more careful analysis of the numbers ratifies the above visualizations. Table 1 summarizes the level of concentration in different percentiles of the income distribution: the 1st percentile, the median and the 99th for the HHI computed using two, four, and six digits of aggregation of exports data. The table also presents the ratio of the concentration level in the 99th to the 1st percentile of the income distribution. The column titled “All” uses all countries in the dataset while the column titled “NNR” uses only non-natural resource rich countries.\nTable 1: HHI and income\nWhen looking at exports concentration computed using two digits categories, countries in the 1st percentile of the income distribution have, on average, an HHI of 0.23. Countries in the median of the distribution have an HHI of 0.135, while countries in the 99th percentile of the distribution have an HHI of 0.185, implying there is re-specialization at this level. The level of concentration for the countries in the 99th percentile of the income distribution is about 53 percent of that in the 1st percentile. A similar pattern occurs with NNR countries, where the re-specialization in the upper end of the income distribution reaches levels that are about 84 percent of the concentration in the bottom of the distribution.\nHowever, as more disaggregated exports data is used to compute the HHI, the ratio of the concentration of income levels in the 99th percentile to income levels in the 1st percentile drops. At the four digit levels, the ratio is 51.7 percent for all countries and 49.5 percent for NNR countries, and at the six digit level these numbers become 50 percent and 45.7 percent, respectively. Thus, the evidence here suggests that, as more disaggregated data is used to compute concentration, the re-specialization pattern documented in the data becomes progressively less pronounced.\nWhat is the relationship between structural transformation and economic development? This short note explores this question by revisiting some of the stylized facts that have been established in the literature. While economic theory suggests that specialization is a result of openness to trade, it is less clear what is the general equilibrium outcome of integration in the global economy. Naturally, the relationship between development and diversification is highly endogenous.\nDiversification of a country’s export basket might be both a cause and a consequence of the process of economic growth and development. The evidence in this note, however, suggests that after taking into account measurement peculiarities, re-specialization is not necessarily the norm among high income countries. Thus, in the presence of market failures that hinder diversification, there might be a place for public policy to overcome those failures. While the policies governments can implement to diversify their economies must be customized to address the particular context that defines each country, state or city, understanding the patterns in the data is crucial to devising them. Policies to boost diversification must be based on solving the market failures that hinder the emergence of new productive sectors. If market failures cannot be identified, then these policies are doomed to fail, as they did in the previous century in different emerging markets.\nBahar, Dany, Ricardo Hausmann, and Cesar A. Hidalgo. “Neighbors and the evolution of the comparative advantage of nations: Evidence of international knowledge diffusion?” Journal of International Economics 92, 1: (2014) 111–123.\nBahar, Dany, and H. Rapoport. “Migration, Knowledge Diffusion and the Comparative Advantage of Nations.” The Economic Journal (Forthcoming).\nBahar, Dany, and Miguel A. Santos. “Natural Resources and Export Concentration: On the Most Likely Casualties of Dutch Disease.” CID Working Paper Series, May.\nCadot, Olivier, Céline Carrère, and Vanessa Strauss-Kahn. “Export Diversification: What’s behind the Hump?” Review of Economics and Statistics 93, January: (2011) 590–605.\nFeenstra, Robert C, Robert Inklaar, and Marcel P. Timmer. “The Next Generation of the Penn World Table.” American Economic Review 105, 10: (2015) 3150–3182.\nHausmann, Ricardo, César A Hidalgo, Sebastián Bustos, Michele Coscia, Sarah Chung, Juan Jímenez, Alexander Simoes, and Muhammed A. Yildirim. The Atlas of Economic Complexity: Mapping Paths to Prosperity. Cambridge, MA, 2011.\nHausmann, Ricardo, and Bailey Klinger. “The Structure of the Product Space and the Evolution of Comparative Advantage.” CID Working Paper Series, 146.\nHeckscher, Eli, and Bertil Ohlin. Heckscher-Ohlin Trade Theory. Cambridge: MIT Press, 1991.\nHelpman, Elhanan, and Paul Krugman. Market Structure and International Trade. Cambridge, MA: MIT Press, 1985.\nHidalgo, César A, and Ricardo Hausmann. “The building blocks of economic complexity.” Proceedings of the National Academy of Sciences of the United States of America 106, 26: (2009) 10,570–5.\nHidalgo, César A, Bailey Klinger, AL Barabási, and Ricardo Hausmann. “The product space conditions the development of nations.” Science (New York, N.Y.) 317, 5837: (2007) 482–7.\nImbs, Jean, and Romain Wacziarg. “Stages of diversification.” American Economic Review 93, 1: (2003) 63–86.\n See also Hidalgo et al. (2007), Hausmann and Hidalgo (2009), Hausmann and Klinger (2006)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:12709f1a-49e9-40d1-8e25-fef9dfed18fd>","<urn:uuid:458a80ac-2e1b-46ef-846c-a5ff517ae18e>"],"error":null}
{"question":"What are the minimum building characteristics that legally require an ERRCS system installation?","answer":"Buildings that are over 50,000 square feet, with 3 or more stories and underground parking require an ERRCS system installation. This requirement exists because materials like brick, concrete, wood, low e-glass, and metal block inhibit radio signals and cause bad reception.","context":["The importance of ERRCS in early construction phases\nWhat is ERRC in construction?\nWhen thinking about constructing a new building, keep in mind that accidents happen every day. Installing an ERRC system in the early stages of construction allows solid communication between first responders and your future occupants when disaster strikes. This blog will discuss the importance of an ERRC system in construction and the requirements it needs.\nWhat is an ERRCS?\nAn Emergency Responder Radio Communication System, or ERRCS, is an in-building communication system that is used to enhance critical communications inside and outside construction.\nAn ERRCS is crucial to managing evacuation, improving critical communication, and boosting signal systems so that people and first responders remain safe in times of crisis.\nEven though an ERRCS system uses smaller antennas (Distributed Antenna Systems or DAS) that allow first responders to stay in contact with each other using two-way radio signals, it differs from a DAS system.\nDAS Systems engineers install ERRCS in your building by joining a Bi-Directional Amplifier (BDA) system to the donor antenna. The antenna receives signals that operate on private frequencies via first responder networks.\nThe BDA uses cable or fiber and receives external radio signals to connect to a DAS and amplify the signals of buildings located in areas with poor coverage.\nHaving an ERRCS in every residential structure is mandated by the NFPA and Authority Having Jurisdictions (AHJ). It is based on International Fire Code (IFC), section 510, and the National Fire Protection Association (NFPA), code 72.\nCommon ERRCS requirements\nWhen planning to install an ERRC system in your building, there are a couple of requirements you need to be aware of, such as:\nIt is tricky for big large constructions and building complexes to keep a strong signal strength. Buildings over 50,000 square feet, with 3 or more stories and underground parking require an ERRCS system. Materials like brick, concrete, wood, low e-glass, and metal block inhibit the signal and cause bad reception. DAS Systems strives to provide proper RF mapping to make sure all areas in the building have good coverage and comply with the IFC regulations.\nTo regulate, preserve, and repair Federal Communications Commission (FCC) licensed radiotelephone transmitters, you need special training and an FCC General Radiotelephone Operator License (GROL), which DAS Systems engineers have.\nIn case of an emergency, building tenants need to stay safe and vacate the premises quickly and carefully. Installing Signal boosters or DAS in vulnerable areas of your construction ensures that all radio frequencies first responders need remain amplified in your building.\nIt’s pivotal for constructions to have the standard signal strength used in critical communication, which is a 95% majority of minimum signal strength of -95dBm. In case the standard ERRC signal strength isn’t reached, buildings need to install a DAS or a signal booster to be up-to-code.\nPublic safety compliance revolves around ERRC testing and re-certification. DAS system offers annual testing and maintenance of your ERRCS system and ensures that your building is always up to standard. We work closely with you and alongside public authorities to design bespoke ERRCS systems and get you a Certificate of Occupancy, guaranteeing to satisfy all your building safety needs.\nHaving battery backups is crucial in case any power outages happen in the building. Sometimes during an emergency, the power of the building needs to be cut for the occupants’ safety; that’s why having a reliable battery backup to keep the ERRC system running is a matter of life and death.\nHow to know if my construction complies with local public safety building codes?\nWhen in doubt about whether or not your construction meets safety requirements, it’s good to run a test. Working with DAS Systems provides you with an ERRC testing that evaluates the signal strength in all the areas of your building and establishes which ones require signal augmentation. Furthermore, we verify every requirement of the IFC and NFPA codes and recommend improvements.\nWhen planning on starting a construction project, it’s always wise to involve a provider like DAS early in the construction design phase. This way ensures that the building’s layout includes an ERRCS that helps limit casualties and keep people safe. But worry not; our team of DAS experts can also retrofit an ERRC system to existing buildings.\nBefore designing an ERRCS system, our team employs predictive mapping and modeling to make sure that the entire area has access to top-notch coverage. Reach out to us for all your ERRC DAS needs to ensure that people in your building will get out unscathed the next time an emergency occurs."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:f7c4439b-3ec2-49ab-a6c6-76f09e5c6675>"],"error":null}
{"question":"Please compare the purpose of Clearing caches in Composr versus System Redundancy in disaster recovery. What is the main goal of each? Can you explain this in list format?","answer":"The main goals of these two mechanisms are different:\\n\\nClearing caches:\\n- Purpose: To resolve problems caused by outdated database or file changes that may be obscured by caching\\n- Used when: Unable to access Admin Zone or experiencing severe problems\\n- Methods: Can be done via upgrader tool or decache.sh command\\n\\nSystem Redundancy:\\n- Purpose: To prevent complete information flow stoppage by using multiple sources, devices, or connections\\n- Used when: Implementing disaster recovery planning\\n- Goal: Eliminate single points of failure that could stop business operations","context":["Composr Tutorial: Disaster recovery\nWritten by Chris Graham (ocProducts)The Composr 'error log' screen points to this tutorial (as well as the 'critical error' screen). If you think you have found a bug, please report it. The rest of this tutorial deals with potential disaster situations – an error in the error log does not imply a disaster has happened.\nThis tutorial will go through some solutions to 'disastrous' scenarios that could happen on an unstable web server configuration. It does not cover all that could go wrong by any means, as there are literally an infinite number of things that could go wrong if Composr is running in an unreliable environment.\nI am going to assume that you have access to phpMyAdmin for the purposes of this tutorial. phpMyAdmin is a user-friendly web-based frontend to MySQL, and is available from most hosting control panels. For more information, see the Manually editing your database with phpMyAdmin tutorial.\nIf you need a quick data rescue, consider using ocProducts support (it will be handled more quickly if you already have support credits in your account). Or, your preferred third-party developer.\nTable of contents\nComposr Tutorial: Disaster recovery\n- Viewing the error log\n- Clearing caches\n- Repairing a corrupted copy of Composr\n- Repairing corrupted MySQL tables\n- Fixing a theme\n- Changing an account password\n- Changing the master password\n- Unbanning an IP address\n- Resetting page ownerships\n- Changing Installation Options\n- Removing Facebook from an account\n- Problems with addons\n- Disabling URL Schemes\n- Closing the site\n- See also\nViewing the error logYou may view the Composr error log even if Composr won't start. Just look at the contents of the data_custom/errorlog.php file manually (don't run it, open it in a text editor). The error log often contains clues to what is causing a problem.\nClearing cachesChanges in the database or to files may be obscured by caching. If you have a problem so severe that you cannot get into the Admin Zone, you can clear caching either:\n- from the upgrader tool (accessible via http://yourbaseurl/upgrader.php)\n- opening up a command prompt to your base directory and running sh decache.sh (Linux/Mac only) – this works if the upgrader won't run; it doesn't immediately clear database caching, but it adds a flag for Composr to do that automatically next time the site is loaded\nRepairing a corrupted copy of ComposrThe upgrader tool (accessible via http://yourbaseurl/upgrader.php) contains a very useful 'integrity checker' which can find out-dated and corrupt files. You can use this tool at any point, regardless of whether you have recently upgraded or not.\nMore details on the integrity checker are in the Performing an upgrade tutorial.\nRepairing corrupted MySQL tablesInstructions are provided in the Manually editing your database with phpMyAdmin tutorial.\nIf you don't have phpMyAdmin, whatever database management tool should have an equivalent feature as table repair is a standard MySQL mechanism.\nIf you don't have any visual database management tool, you should look into getting one. However if you are technical you can use the command line using the MySQL REPAIR TABLE <tablename> command, or the myisamchk program to batch fix all *.MYI files while having the MySQL server temporarily offline.\nFixing a themeIf you've set up a theme, that is activated, and you've edited it in such a way that you can no longer realistically use the Admin Zone, you'll need to rescue your theme.\nIf you still are logged in as admin or the default theme is accessible from guest accounts, use the 'Safe mode' link you were asked to bookmark during the Setup Wizard – this will allow you to use your website, using the default theme. If you have forgotten the link, it's any link to your website with &keep_theme=default added to the end of the URL.\nIf you are not logged in as admin, connect to your web server using FTP, then rename all the folders inside the themes folder except default and admin. For instance, if you have folders my_theme, another_theme, default, and admin, rename the first two to my_theme.old and another_theme.old, but leave default and admin alone. This renaming will essentially temporarily erase your themes and force Composr to use the default theme regardless of permissions. Now change the theme selection on your site back back to default, rename your theme folders back to their prior names (remove the .old suffix you added), fix your theme, and then restore your theme selection.\nChanging an account passwordIf you've misset a Composr account password and for whatever reason can't use the 'lost password' feature to reset it (perhaps because of an invalid/non-existent e-mail address for that account, or a faulty/non-existent mail server, or a protected admin account), then you'll want to manually reset it.\nObviously this needs a greater level of access than ordinary members would have. We could either use:\n- Database access\n- Disk access\nVia the databaseFor this, I am assuming that you are using Conversr . If you are using another forum then the method will be similar, but will be done upon the data of that forum.\n- Enter phpMyAdmin\n- Browse the contents of the Composr f_members table (named cms_f_members if you used the default table prefix).\n- You will see rows for all usernames. Click the little edit icon and you'll see an editing screen.\n- There are numerous fields, but we only need to change three:\n- m_pass_hash_salted should be given the new password. It is essential that you also choose 'MD5' from the list to the left of the input field, so that the password gets 'encrypted' (Composr assumes an encrypted password and if it is not then it will not successfully log in).\n- m_pass_salt should be blanked out.\n- m_password_compat_scheme should be set to the literal value 'md5'.\n- Scroll to the bottom of the form and click 'Go'.\nYou should now be able to log in to Composr using your new password.\nVia the diskIf you need to 'hack' your way in to the site using your FTP access, you can place a temporary backdoor.\nGo to myIPaddress.com: What is my IP address? How do I find my IP address? and copy your IP address, then put it into your _config.php file:\n$SITE_INFO['backdoor_ip'] = '220.127.116.11'; // Enable to a real IP address to force login from FTP access (if lost admin password)\nYou will then be logged in as the first admin user there is. Additionally membership bans, probation, and flood control, will not apply when using the backdoor (so it is useful if your account was locked somehow, e.g. by a malicious other staff member).\nIt should go without saying that you should only use this 'backdoor' for a short space of time and not disclose your IP address to third parties when doing it. The developers make no claims to how secure this backdoor might be and you open it up entirely at your own risk.\nChanging the master passwordIf you need to log in to the upgrader, config editor, or code editor, you'll need the master password.\nIf you have forgotten it, to change it just edit the _config.php file:\n$SITE_INFO['master_password'] = '...';\nYou should then encrypt your password via changing the password within the http://yourbaseurl/config_editor.php script.\nUnbanning an IP address\nIf you don't know your IP address then you can find it out from myIPaddress.com: What is my IP address? How do I find my IP address?.\nResetting page ownershipsIf you delete or demote an admin, you may find lots of pages now show Comcode permission errors (even default ones, which are assigned to the first site admin automatically).\nTo fix this, edit the Comcode page and assign a new submitter via the edit form.\nTo do it en-masse, you can run this in Commandr (this example assumes you are reassigning from member #5 to member #2):\nChanging Installation OptionsIf your server base configuration has changed (such as your database password, for example) then you need to update Composr's _config.php config file to reflect this. You can either do this directly, or you can do it via launching the http://yourbaseurl/config_editor.php script installed at the base of your website (you will need your master password to do this).\nRemoving Facebook from an accountIf you have associated an account with Facebook, you can remove it by doing a password reset on that account.\nProblems with addonsIf you find an addon is crashing your site, the Safe Mode link (http://yourbaseurl/index.php?keep_safe_mode=1) can help. It should let you into your site to uninstall the addon causing the problem.\nThe above requires for to be logged in at admin, which you may not be or be able to do. If you can't then you can force Safe Mode on using the http://yourbaseurl/config_editor.php script. Load up the http://yourbaseurl/config_editor.php script, login, look for the \"Whether Composr is to be forced into Safe Mode, meaning no custom files will load and most caching will be disabled\" option, tick (check) the option, and then save the configuration. Turn the option back off after you've solved your problem.\nDisabling URL SchemesURL Schemes are subject to your webhost supporting rewrites and not doing any strange redirects or filter rules. It is possible you could enable them, and then find they don't work and that you can't get back at the configuration to disable them. We try and detect them working before we allow the option to be changed, but the detection may not be perfect. Or, they may work, then your host may change something, causing them to stop working.\nYou can use the http://yourbaseurl/config_editor.php script in order to turn them off, overriding the main configuration setting. Load up the http://yourbaseurl/config_editor.php script, login, look for the \"Whether to block the URL Scheme (mod_rewrite)\" option, tick (check) the option, and then save the configuration.\nClosing the siteIf you can't access the Admin Zone then the upgrader tool (accessible via http://yourbaseurl/upgrader.php) can close the site while you're working on problems. If that won't work reliably you can create a closed.html file in your base directory which will automatically be displayed to all users accessing Composr.\nIf failover-mode is configured you can activate it manually from within the http://yourbaseurl/config_editor.php script.\nPlease rate this tutorial:\nHave a suggestion? Report an issue on the tracker.","Understanding the lingo of disaster recovery and business continuity planning is essential to ensuring a firm is fully knowledgeable during the planning process and prepared should an incident occur. Here at Eze Castle Integration we are regularly defining key DR terms for our hedge fund clients. Since we fancy ourselves experts on all things hedge fund DR related, we have have developed this handy list of common DR definitions.\nA component of Disaster Recovery that deals with the restoration of business system software and data, after the operating system environment has been restored or replaced.\nBusiness Recovery Team\nA pre-identified group of individuals that is reponsible for maintaining and executing the recovery process.\nA system of planning for, recovering and maintaining both the IT and business environments within an organisation regardless of the type of interruption. In addition to the IT infrastructure, it covers people, facilities, workplaces, equipment, business processes, and more. Be sure to read our articles on the difference between DR & BCP and preparing for a worst case scenario.\nBusiness Impact Analysis\nA collection of information on a wide range of areas from recovery assumptions and critical business processes to interdependencies and critical staff that is then analysed to assess impact a disaster may have.\nThe process of restoring and maintaining the data, equipment, applications and other technical resources on which a business depends.\nHigh availability describes a system’s ability to continue processing and functioning for a certain period of time - normally a very high percentage of time, for example 99.999%. High availability can be implemented into a firm's IT infrastructure by reducing any single points of failure using redundant components.\nHot Sites versus Remote Sites\nA disaster recovery hot site is a remote physical location where you can maintain copies of all of your critical systems, such as trading applications, data, and documents. A remote site provides a secondary instance or replica of your IT environment—without physical desks and office infrastructure—that you and your firm’s employees can securely access and use remotely, through standard Internet connections from anywhere.\nWant to know more? We have a whole article on the difference between hot sites and remote sites.\nA computer system or application that is essential to the functioning of your business and its processes.\nProduction or Primary Site\nIn the context of a primary and secondary site, the primary site contains the original data that cannot be recreated.\nRecovery Time Objective (RTO)\nThe RTO is the duration of time and service level within which a business process must be restored after a disruption in order to avoid unacceptable losses. RTO begins when a disaster hits and does not end until all systems are up and running.\nRecovery Point Objective (RPO)\nThe RPO is the point in time to which a firm must recover data as defined by the organisation. In other words, the RPO is what an organisation determines is an “acceptable loss” in a disaster situation. The RPO dictates which replication method will be required (i.e. nightly backups, snapshots, continuous replication).\nA system of using multiple sources, devices or connections so that no single point of failure will completely stop the flow of information.\nThe identification and prioritisation of potential business risk and disruptions based on severity and likelihood of occurrence.\nSecondary Site (or DR Site)\nThe secondary site contains information and applications that are built from the primary repository information. This site is activated should the primary site become unavailable.\nWhat's missing from the list? Help us expand this DR dictionary.\n- Disaster Recovery Guidebook\n- eBook: Preparing for the Worst: Disaster Recovery and Business Continuity Planning for Investment Firms\n- Blog Article: Common Disaster Recovery Misconceptions\nPhoto credits: Muffet on Flickr\n- Expert Tips for Launching a Hedge Fund in a New Environment\n- Answering the FCA's Dear CEO Letter on Outsourcing with Some Practical Steps\n- Reflecting on What We're Thankful For This Thanksgiving\n- Finding Your One-Stop Shop: The Benefits of Choosing an All-Inclusive IT Provider\n- Three Ways Your Cloud Provider Can De-Stress Your Life\n- business continuity planning\n- cloud computing\n- data loss prevention\n- disaster recovery\n- eze castle milestones\n- hedge fund due diligence\n- hedge fund marketing\n- hedge fund operations\n- hedge fund regulation\n- help desk\n- high frequency trading\n- launching a hedge fund\n- privacy compliance\n- project management\n- real estate\n- startup & relocation\n- trends we're seeing\n- videos and infographics"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:0cccc161-d2e1-4bd3-b248-30954f45aab0>","<urn:uuid:d91a672e-ed64-4b9e-80ea-dbfa5f37dabe>"],"error":null}
{"question":"As a research analyst studying space technology costs, what are the budget differences between the RemoveDebris mission and CleanSpace One project?","answer":"The RemoveDebris mission is budgeted at 15.2 million euros (approximately $18.7 million) and was partially funded by the European Commission with additional funding from 10 participating companies. In comparison, the CleanSpace One project costs around CHF10m (£6.9m) to design, build and launch. Both projects emphasize cost-effectiveness as crucial for commercial viability, with RemoveDebris specifically aiming to prove that debris removal can be relatively inexpensive for commercial companies and governments operating under budget limitations.","context":["A proposed satellite designed to clean up space debris could help prevent damage to the hundreds of craft orbiting the Earth.\nResearchers at Swiss technology institute EPFL have announced the launch of the CleanSpace One project to build the first in a family of craft that can track down and grab old satellites and pull them back to Earth, reducing the risk of orbital collisions.\nSpace debris is a growing concern for authorities, as the abandoned satellites, rocket stages and other rubbish around the Earth that threaten to collide with live craft are increasingly breaking into thousands of fragments, which can also cause serious damage.\n‘Space agencies are increasingly finding it necessary to take into consideration and prepare for the elimination of the stuff they’re sending into space,’ said EPFL’s Swiss Space Center director Volker Gass.\n‘We want to offer and sell a whole family of ready‐made systems, designed as sustainably as possible, that are able to de‐orbit several different kinds of satellites.’\nEPFL is already developing an ultra-compact motor to enable the CleanSpace craft to adjust its trajectory after launch so that it can match the orbital plane of its target.\nWhen the target, which will be travelling at 28,000km/h at an altitude of 630–750km, is within range the clean-up satellite will use a gripping mechanism inspired by a plant or animal example to grab hold of the debris.\nFinally CleanSpace One will come out of orbit, pulling the target with it through the Earth’s atmosphere where both craft will burn up on re-entry.\nNASA is monitoring 16,000 objects larger than 10cm in diameter travelling around the Earth at speeds of several kilometres per second, primarily in low earth orbit — less than 2,000km in altitude.\nCleanSpace One will cost around CHF10m (£6.9m) to design, build and launch, and will result in one of Switzerland’s two small ’picosatellites’ being pulled out of orbit.\n‘A de-orbiting satellite has never been attempted before,’ EPFL spokesman Michael Mitchell told The Engineer. ‘Our goal is to prove to the scientific and industrial community that the global idea is feasible, of which we are very confident.\n‘The very first prototype needs to have a simple design that is sure to work. Once this has been proven, other options — multiple de-orbits, retrieving old satellites back to earth, bigger devices — are sure to follow.’\nWho would be willing to pay for clean-up satellites has yet to be determined, but Mitchell stressed that the goal of the project was to prove that it was feasible.\nThe European Space Agency (ESA) has adopted recommendations that require satellites to be de-orbited after 25 years.\nAnd the rising cost of satellite insurance, which is currently around $20bn (£12.7bn) per craft, could also force satellite operators to act to reduce existing space debris.\nBut Surrey University’s Dr Vaios Lappas, who is developing sails to drag future satellites out of orbit, told The Engineer that without regulation or a very cost-effective solution, industry would be hesitant to add de-orbiting systems to satellites.\nOn the topic of technology to pull down existing space debris he said: ‘I don’t think it will happen if it’s on a commercial model. It’s a very interesting engineering and research topic but I don’t see that working unless it is heavily subsidised by big space agencies.’\nDr Hugh Lewis, who is developing space debris simulation models at Southampton University, said there was a growing consensus on the removal of debris and space companies saw it as a potential market.\n‘Asking governments to pay for it is one route by which we can get this done but that’s not necessarily the best way,’ he told The Engineer.\n‘Another way to do it is to ask all spacecraft operators to pay into a pot from which you dip into to pay for removal.\n‘The other way is through insurance, so that you provide the incentive to remove space junk. If you can reduce the risk to your spacecraft you can pay a lower premium, then it’s worth going up to remove objects that will enable that to happen.’","European engineers who developed a small satellite hitching a ride to the International Space Station aboard a SpaceX supply ship Monday are gearing up for a first-of-a-kind experiment to examine ways to snare a chunk of space junk and tug it back to Earth.\nDeveloped in a public-private partnership, the RemoveDebris mission will test the utility of nets and harpoons to capture tumbling objects in orbit, repurposing devices commonly used in fishing to pluck debris out of orbit and bring them into Earth’s atmosphere to burn up.\nGuglielmo Aglietti, principal investigator for the RemoveDebris mission, calls the project a “proof-of-concept.”\nThey crux of the mission, Aglietti said in an interview, is to prove that cleaning up space junk can be relatively inexpensive — something that could be affordable by commercial companies, or governments operating under budget limitations.\n“We want to learn as much as possible,” said Aglietti, who is also director of the Surrey Space Center, a research institute affiliated with the University of Surrey and Surrey Satellite Technology Ltd., a British manufacturer of small satellites. “Even if some experiment doesn’t go exactly as planned, provided we get all the data, it’s still a positive outcome.”\nThe RemoveDebris satellite will launch in a container inside a SpaceX Dragon cargo craft set for launch at 4:30 p.m. EDT (2030 GMT) Monday from Cape Canaveral. The commercial supply ship is carrying more than 5,800 pounds (2.6 metric tons) of food, provisions and experiments to the space station’s six-person crew.\nRemoveDebris accounts around 220 pounds, or 100 kilograms, of the Dragon’s cargo load.\nBut the small spacecraft, developed by SSTL in the United Kingdom, punches above its weight. The RemoveDebris mothership contains two CubeSats, a net and a harpoon, a laser ranging instrument, and a “dragsail” designed to unfurl behind the main satellite and hasten its fall back into Earth’s atmosphere using aerodynamic resistance.\nAssuming the Dragon resupply mission takes off Monday, the cargo capsule is due to reach the space station early Wednesday. Astronauts will unpack the RemoveDebris satellite, along with tons of other equipment, in the following weeks.\nSome time this spring, the crew will take the one-meter cube-shaped spacecraft from its shipping container, remove shields used to protect it during launch, then place it on a sliding tray inside the Japanese Kibo lab module airlock.\nFixed to a NanoRacks carrier, the spacecraft will transfer through the airlock to the Japanese lab’s outside science deck, where the Kibo module’s robotic arm will grab it and move to a predetermined position for release.\nAglietti said RemoveDebris is currently slated for deployment from the space station in late May, but the schedule is not yet confirmed.\n“The mission will really start once we are deployed out of the space station, hopefully at the end of May or the beginning of June,” he said.\nRemoveDebris will be the biggest satellite launched from the space station. That has placed the mission under extra scrutiny from NASA managers, who want to ensure the satellite poses no hazard to the orbiting outpost or its crew.\nThe launch of RemoveDebris was supposed to happen last year, but officials bumped it to a later SpaceX cargo flight.\n“It took a little bit longer to make sure that all the right boxes were ticked,” Aglietti said.\nThe demo craft’s high-flying experiments will not begin until the satellite is well away from the space station, Aglietti said. In fact, the satellite will remain in a dormant mode for around a half-hour after its release, before switching on to begin checkout procedures.\nGround controllers at SSTL’s campus in Guildford, England, will put the spacecraft through four primary experiments.\n“Basically, it has a net, a harpoon and a dragsail on-board,” said Jason Forshaw, the RemoveDebris mission’s project manager at SSTL. “The concept is it’s going to go up there, and it’s going to eject small little satellites that will be used as artificial space junk.”\nOne of the CubeSats, about the size of a loaf of bread, will inflate a balloon to mimic the dimensions of a bigger piece of tumbling space junk. Flying a short distance away, RemoveDebris will release a net to envelop the CubeSat — named DebrisSat 1 — which will be cut loose to re-enter the Earth’s atmosphere.\n“The net, as a way to capture debris, is a very flexible option because even if the debris is spinning, or has got an irregular shape, to capture it with a net is relatively low-risk compared to … going with a robotic arm, because if the debris is spinning very fast, and you try to capture it with a robotic arm, then clearly there is a problem,” Aglietti said. “In addition, if you are to capture the debris with a robotic arm or a gripper, you need somewhere you can grab hold of your piece of debris without breaking off just a chunk of it.”\nAnother CubeSat, named DebrisSat 2, will separate from the RemoveDebris mothership to test out tracking and ranging lasers and algorithms. The RemoveDebris satellite will use DebrisSat 2 to test out close-up navigation technology needed for an orbiting garbage collector to approach an out-of-control piece of space junk.\nThe LIDAR instrument “can observe debris, and figure out all the parameters of what this debris is doing in order to plan your capture,” Aglietti said. “We have a normal camera, and then a LIDAR, which uses lasers to illuminate the object and figure out what the object is doing, and try to quantify the parameters, not just looking and seeing it, but also trying to see the spin rate, for example.”\nThe third RemoveDebris experiment will test the functionality of a harpoon, which would be used to fire at a dead satellite and spear it, allowing the junk to be maneuvered out of orbit for a fiery re-entry.\nBut RemoveDebris will not test the harpoon on an actual satellite. The technology is still untried in space, and there are legal concerns about using it to lasso someone else’s spacecraft without permission.\n“Maybe it’s a bit more risky because you have to hit your debris in a place that is suitable to be captured by the harpoon,” Aglietti said. “Clearly, you have to avoid any fuel tanks … That would produce some undesired effects.”\nInstead, RemoveDebris will extend an arm with a target for the harpoon on the end, then fire the projectile on a tether.\n“We have harpoons, we have nets,” Forshaw said in a TEDx talk last year. “These all seem like simple concepts, and they are. They’ve been used for thousands of years underwater to capture things such as sea creatures. However, taking technologies that are mature on Earth, in the oceans, and actually bringing them up there into space and seeing (if) these concepts work for the first time — nobody has ever used a net or a harpoon for these purposes in space before.”\nFinally, RemoveDebris will open up an expandable sail to act like an airbrake or spoiler, generating drag from collisions with air molecules in the rarefied outer atmosphere. At the space station’s altitude of around 250 miles (400 kilometers), the dragsail will bring the RemoveDebris satellite back into the denser layers of the atmosphere, where it will burn up.\n“All the elements of the mission should be de-orbited very quickly,” Aglietti said. “Clearly, for a mission like ours, we don’t to further contribute to the problem of space debris. We want to make sure that all the pieces we are putting up there are going to come down pretty quickly. For us, a launch from the International Space Station is particularly good because it’s in such a low orbit, that in any case, even if some of the experiments do not work out as planned, it doesn’t matter because everything is going to come down and burn up in the atmosphere.”\nBudgeted at 15.2 million euros — $18.7 million at today’s currency exchange rates — the RemoveDebris mission was partially funded by the European Commission. The rest of the project was paid by the 10 companies involved in the demonstration, including SSTL, Airbus Defense and Space, and Ariane Group.\n“Since the beginning of the space era, orbital debris has progressively been building up and there are now almost 7,000 tons of it around the Earth,” said Martin Sweeting, SSTL’s executive chairman. “It is now time for the international space community to begin to mitigate, limit and control space junk, and I am very pleased that the RemoveDebris consortium is leading the way with an innovative ADR (Active Debris Removal) mission which I hope will be a precursor to future operational ADR missions.”\nIn-space collisions have happened before.\nIn 2009, a commercial Iridium communications satellite collided with a deactivated Russian military craft, destroying both objects and creating thousands more pieces of space junk.\n“Satellites that get old also have residual fuels on them,” Forshaw said. “Sometimes these fuels mix, so satellites are remarkably good at exploding by themselves.”\n“One of the core questions is who is responsible for all of this,” Forshaw said. “Who is responsible for keeping space tidy? There, space law is complicated. Every single item in space, whether it be a full satellite or a piece of glass, is actually owned by somebody. You can’t take away their property without their permission. Besides, if a tiny little fragment hit your satellite, you wouldn’t even know who did it.”\nAs companies like OneWeb, SpaceX and others build out planned “mega-constellations” of hundreds and thousands of communications satellites, the space debris problem will remain top of mind for many in the industry.\nOneWeb and SpaceX say they will steer their planned broadband communications satellites back into Earth’s atmosphere once their missions are complete, but some of the spacecraft could become stranded if they suffer unexpected failures.\n“There are really two solutions: Either we ensure things launched into space have the ability to come back down themselves, and/or we launch missions up there to actually capture some of this space junk and bring it back down to Earth, where it will burn up in the Earth’s atmosphere,” Forshaw said.\n“Once the whole campaign is finished, and the (RemoveDebris) satellite is de-orbited, it would be great if companies offered this as a service, and there will be bigger missions when they will go and capture a real piece of debris using some of the technologies we have demonstrated,” Aglietti said.\nOne company established to remove space debris out of orbit is Astroscale, headquartered in Singapore. Astroscale is developing a commercial space debris capture experiment in partnership with SSTL.\nThe European Space Agency also has a mission concept called e.Deorbit, which would launch in 2024 to rendezvous with Envisat, a defunct Earth observation satellite that failed suddenly in 2012. Envisat is the size of a double-decker bus, and experts anticipate it will remain in space for up to 150 years, posing a hazard to other satellites in the same region of space nearly 500 miles (800 kilometers) above Earth.\nThe e.Deorbit mission, if approved by ESA member states, would bring Envisat down in a controlled manner. The RemoveDebris mission will check to see if some of the fundamental parts of such an endeavor will work.\n“At the end of the day, everything boils down to funding,” Aglietti said. “We all agree, in the space sector, that it is a good idea to start to remove larger pieces of debris, which are the ones that cause the major threat. The problem is just financial.\n“That was why we’re testing cost-effective technologies,” he said. “In my opinion, the stumbling block is the cost … If the cost to do it is exorbitant, then people will prefer to take the risk that their new satellite is going to be hit by a piece of debris. If we manage to lower the cost of the missions, then this is much more likely to happen.”\nEmail the author.\nFollow Stephen Clark on Twitter: @StephenClark1."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:30515ec1-9a6c-443f-82c2-716b28b61d33>","<urn:uuid:768cca71-2429-4f46-87e5-306bc2c25aa3>"],"error":null}
{"question":"I'm trying to understand climate goals - what's the difference between the warming limits advocated by small island nations versus what we're currently experiencing?","answer":"Small island nations, through the Alliance of Small Island States (AOSIS), strongly advocate for limiting global warming to 1.5°C above pre-industrial levels. Currently, we have experienced nearly 1.1°C of warming since the Industrial Revolution. This difference is critical because while warming of 1.5°C will have significant impacts, small island nations would still have adaptation options available as sea level rise would remain below 1 meter. However, at 2°C of warming, impacts are expected to exceed adaptation limits for Small Island Developing States and result in irreversible and dangerous levels of climate change.","context":["Introduction to special issue\nSmall Island Developing States (SIDS) are positioned as particularly vulnerable to the effects of climate change due to their small size, geographical features and concentration of infrastructure, economic activities and population in coastal zones (Mertz et al., 2009; MacPherson & Akpinar-Elci, 2013, Nurse et al. 2014).\nThe extreme exposure and vulnerability of SIDS to sea level rise, increased intensity of extreme events, changes in temperature and precipitation and resultant impacts on economic and social structures (Gamble et al., 2010; Scott, 2012, Hernández-Delgado, 2015), stands in stark contrast with the negligible contributions of SIDS to global greenhouse gas emissions, the drivers of climatic change. Sea level rise and coastal erosion are particularly disturbing impacts of climate change as they threaten the very existence of many small, low elevation islands (Albert et al., 2016; Storlazzi, Elias & Berkowitz, 2015).\nIn addition, observed and projected impacts on coastal ecosystems, especially on coral reefs, severely threaten livelihoods in island regions and are projected to cause high economic damages (Chen, Chen, Chu & McCarl, 2015). Small islands also have especially sensitive fresh water supply systems and water stress is likely to pose a serious threat (Karnauskas, Donnelly & Anchukaitis, 2016; Terry & Chui, 2012).\nThese dire projected effects of climate change on SIDS has led the Alliance of Small Island States (AOSIS) to advocate strongly for a cap of 1.5⁰C on global warming above pre-industrial levels (AOSIS, 2015; Benjamin and Thomas, 2016).\nCurrent levels of warming at nearly 1⁰C have already had observable impacts on a global scale including declines in marine fisheries and food production and increases in sea level rise and flooding (UNFCCC, 2015). While warming of 1.5⁰C will result in significant additional impacts, it is likely that there will still be adaptation options for SIDS as sea level rise is projected to remain below 1 meter and terrestrial and marine species important for SIDS have higher likelihoods of survival (UNFCCC, 2015). With a warming of 2⁰C, impacts are thought to exceed the limits of adaptation for SIDS and result in irreversible and dangerous levels of climate change (UNFCCC, 2015).\nWhile there is a significant body of work focused on climate change and SIDS, there is a lack of literature that focuses specifically on the 1.5⁰C temperature limit and its implications for SIDS. The upcoming IPCC special report on 1.5°C represents an unique opportunity to address this important literature gap and this special issue aims to facilitate a timely and comprehensive collection of new contributions to this matter that will feed into the IPCC 1.5°C report.\nFor this special issue, we welcome submissions from variety of disciplines across both social and natural sciences that address the issue of 1.5⁰C and SIDS. Original research articles and commentaries are welcomed for submission. For commentaries, we explicitly invite contributions that revisit already existing analysis to specifically address the 1.5°C question.\nSubmissions that focus on a particular geographic region, i.e. Pacific, Caribbean, etc. or on a particular country or community within SIDS are also welcome. Submissions that address any of the below questions or any question related to 1.5⁰C and SIDS will be well received.\n- What are the projected biophysical and/or socioeconomic impacts of 1.5⁰C for SIDS?\n- What are the avoided impacts and reduced risks at 1.5⁰C compared to higher levels of warming?\n- How is vulnerability and adaptation of SIDS affected at 1.5⁰C of warming?\n- What are existing methods of adaptation and are these methods applicable at 1.5⁰C?\n- What are limits to adaption for SIDS and will they be reached at 1.5⁰C or above?\n, University of The Bahamas\n, Climate Analytics\nPolicy and Technical Expert, Climate Change,\nDeadlines for submissions\nFull papers submission: September 2017\nProposed special issue published date: March 2018\nAlbert, S. et al. (2016). Interactions between sea-level rise and wave exposure on reef island dynamics in the Solomon Islands. Environmental Research Letters, 11 (5), 54011.\nAOSIS. (2015) ‘Submission by AOSIS on the Outcome of the Structured Expert Dialogue and the 2013-2015 Review.\nBenjamin, L. and Thomas, A. (2016). ‘1.5⁰C to stay alive? AOSIS and the long term temperature goal in the Paris Agreement. IUCNAEL E-Journal\nChen, P.-Y., Chen, C.-C., Chu, L. & McCarl, B. (2015). Evaluating the economic damage of climate change on global coral reefs. Global Environmental Change, 30, s. 12–20.\nGamble, D.W., Campbell, D., Allen, T.L., Barker, D., Curtis, S., McGregor, D. and Popke, J. (2010) ‘Climate Change, Drought, and Jamaican Agriculture: Local Knowledge and the Climate Record’, Annals of the Association of American Geographers, 100 (4), 880–893.\nHernández-Delgado, E. A. (2015). The emerging threats of climate change on tropical coastal ecosystem services, public health, local economies and livelihood sustainability of small islands: Cumulative impacts and synergies. Marine Pollution Bulletin, 101 (1), s. 5–28.\nKarnauskas, K. B., Donnelly, J. P. & Anchukaitis, K. J. (2016). Future freshwater stress for island populations. Nature Climate Change, (April), s. 1–7. doi:10.1038/nclimate2987\nMertz, O., Halsnaes, K., Olesen, J.E. and Rasmussen, K. (2009) ‘Adaptation to climate change in developing countries’, Environmental Management, 43, 743–752.\nMacpherson, C. and Akpinar-Elci, M. (2013) ‘Impacts of climate change on Caribbean life’, American Journal of Public Health, 103 (1), e6.\nNurse, L. A. et al. in Clim. Chang. 2014 Impacts, Adapt. Vulnerability. Part B Reg. Asp. Contrib. Work. Gr. II to Fifth Assess. Rep. Intergov. Panel Clim. Chang. (Barros, V. R. et al.) 1613–1654 (Cambridge University Press).\nScott, D. (2012) ‘The vulnerability of Caribbean coastal tourism to scenarios of climate change related sea level rise’, Journal of sustainable tourism, 20 (6), 883–898.\nStorlazzi, C. D., Elias, E. P. L. & Berkowitz, P. (2015). Many atolls may be uninhabitable within decades due to climate change. Nature Scientific Reports, 5:14546, s. 1–9. doi:10.1038/srep14546\nTerry, J. P. & Chui, T. F. M. (2012). Evaluating the fate of freshwater lenses on atoll islands after eustatic sea-level rise and cyclone-driven inundation: A modelling approach. Global and Planetary Change.\nUNFCCC (2015) ‘Report on the structured expert dialogue on the 2013-2015 review’","Breaking Down the IPCC Report and What States Can Do\nSeptember 16, 2021\nIPCC Report Overview\nThe Intergovernmental Panel on Climate Change, or IPCC, is the world authority on climate science. Comprised of leading scientists from all over the world, the body releases a new comprehensive report on the state of climate science approximately every seven years, with interim reports on more specialized topics. The most recent report from August, Climate Change 2021: The Physical Science Basis, builds on the 2013 report.\nMany of the report’s findings are not new. However, the evidence and anticipated severity of human-caused climate change is increasingly stronger. The historic heatwaves, wildfires, hurricanes, and flooding that have overwhelmed the US in just the past year are precursors to increasingly worse climatic disruption. The report’s findings are an alarm bell.\nLimiting warming to 1.5°C is possible, but time is of the essence.\nSince the Industrial Revolution, humans have emitted enough heat-trapping climate pollutants to warm the world by approximately 1.1°C. In order to limit warming to 1.5°C – a goal stipulated by the Paris Agreement – we must cut carbon pollution in half by 2030 and completely cease emissions by 2050. The Earth’s climate is complex, so there is a lag time between when greenhouse gas pollutants are released and when we start experiencing the associated warming. We are currently on a path to overshoot the target, but if we cease emissions immediately and improve marine and terrestrial ecosystem health so they can better sequester carbon, we may be able to return to below 1.5°C later this century. Some projections show a potential to hit 1.5° of warming by 2030, so the time for steep emissions reductions is now.\nWithout immediate action, we run the risk of activating irreversible tipping points.\nThe longer we burn fossil fuels and increase emissions, the more likely we are to trigger catastrophic climate tipping points. Climate tipping points are high-impact events where critical earth systems collapse, leading to irreversible cascading effects. Examples of tipping points include Antarctic ice sheet collapse, which would trigger rapid sea level rise, or the Amazon rainforest turning from a carbon sink to a carbon emitter, which would greatly speed up warming. Already, the Atlantic Meridional Overturning Circulation appears to be weakening; if it slows significantly or even stops, weather patterns and water cycles across continents will shift abruptly.\nCurbing methane is key to limiting warming.\nMethane is more than 80 times more potent than carbon dioxide over a 20-year time horizon, but it is short-lived in the atmosphere. That means that tackling methane emissions can have an immediate impact in slowing warming. Methane is a precursor pollutant to ozone, so stopping methane emissions would also improve air quality. NCEL recently held a webinar on how states could reduce methane pollution.\nStates Have the Tools to Decarbonize\nThe good news is that there are climate policy solutions out there that states are already implementing. Despite the chaos caused by the COVID-19 pandemic, in 2020 and 2021 state legislatures passed increasingly ambitious bills on climate change. For example, Massachusetts, Rhode Island, and Virginia passed net zero emissions targets to decarbonize the state economy by 2050 or earlier. Washington state passed a comprehensive carbon pricing bill to support the implementation of its own net-zero emissions target passed in 2019. Maine became the first state to legislatively divest from fossil fuels. And, in weeks since the IPCC report came out, New York committed to zero-emissions vehicles and Illinois passed a bill to transition to no-carbon energy by 2050.\nThe threat of global climate change requires comprehensive solutions, and this is an all-hands-on-deck moment. Every state legislator can have a role in climate solutions, whether they serve on the transportation committee or the bonding committee or anything else. A helpful overview of climate policy solutions by sector is available here."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:49ca088c-46c8-467f-9c81-e807acba1b4a>","<urn:uuid:ea535e6b-4cd0-4c7b-8759-8ce18138f471>"],"error":null}
{"question":"Hi! I'm studying Jewish communities in Europe. Can you tell me if both the Jewish communities in Zamość and Augsburg were affected by Nazi persecution during WWII? What happened to their synagogues?","answer":"Yes, both Jewish communities were severely impacted by Nazi persecution during WWII. In Zamość, out of 12,000 Jews (45% of the city's population) in 1939, only 5,000 managed to escape the Holocaust by crossing the Bug River. The remaining Jews were imprisoned in the Zamość Ghetto and transported to the Bełżec death camp. The Zamość Synagogue was damaged, particularly its northern parts, vandalized, looted, and converted into a carpenters' workshop by the Nazis. In Augsburg, by late 1941, the remaining 170 Jews were forced into a ghetto, with 129 being sent to Piaski in Poland in April 1942 and the rest mostly to the Riga ghetto and Theresienstadt. The magnificent Augsburg Synagogue, which had been dedicated in 1917, was burned down by the Nazis in 1938.","context":["|Zamość Old Synagogue|\n|Location||9-11 Zamenhofa Street\n|Width (nave)||11.6 x 12.2 metres|\nZamość Synagogue, (Polish: Synagoga Dawna w Zamościu), was built between 1610 and 1618 Zamość in the Polish-Lithuanian Commonwealth. The synagogue had functioned as a place of worship until World War II, when the Nazis turned the interior into a carpenters’ workshop. Recently restored to the Polish Jewish community, the building is going to be renovated and rearranged in order to serve the current citizens of Zamość.\nThe first Jews settled in Zamość in 1588, eight years after the founding of the town by kanclerz Jan Zamoyski. They were Sephardim coming from the Ottoman Empire and Venice and consequently established the northernmost Sephardi community in Eastern Europe. It was the Sephardim that built the first synagogue in Zamość in the 1590s as a wooden structure. In 1610, after restrictions prohibiting Jews from building synagogues from stone were rescinded, the current brick building was erected, taking eight years to complete. The original Sephardi community ceased to exist in the 1620s when it assimilated into the fledging Ashkenazi community, following an economic crisis caused by the accumulation of bad debts by Polish debtors. Ashkenazi Jews had begun settling in Zamość at the beginning of the 17th century having been attracted by the commercial significance of the town. The influx of Ashkenazi Jews increased in the 1640s, especially by refugees fleeing the anti-Jewish massacres perpetrated by the troops of Bohdan Khmelnytsky during the Ukrainian revolt against Polish rule.\nToday only 3 Jews live in Zamość. In 1939 there were over 12,000 who made up 45% of the city's population. Of these only 5,000 managed to escape the Holocaust by crossing the Bug River, which in 1939 became the border with the Soviet Union. The Nazis imprisoned those remaining in a ghetto (the Zamość Ghetto), from which they were transported to the Bełżec death camp. During the Holocaust the synagogue suffered major damage, especially to the northern parts that were destroyed by the Germans. The synagogue was vandalized and looted and then used as a carpenters’ workshop. During 1948–1950 it was rebuilt in the communist period and from 1958 until early in the 20th century the building served as a public library. A second restoration of the building was conducted during 1967-1972.\nCurrently next to the building of the synagogue is the former office of the community, dating from the 18th century with additions from the 19th century, and the cheder. After the Second World War it was transformed into a hotel. The 18th century building of the former Mikveh, renovated in the 19th century, is located in the cellars at 3 Zamenhofa Street (previously ul. Żydowska - “Jewish Street”).\nThe synagogue was one of the first properties to be officially returned to the Jewish community by the Polish government in 2000 and in 2004 the public library which used the building moved to another location. In 2009 a major reconstruction of the synagogue was underway under the auspices of the Warsaw-based Foundation for the Preservation of Jewish Heritage in Poland. A permanent exhibit will feature a \"virtual tour\" of the many Jewish shtetls that existed in this region before the Holocaust. In addition to being available for prayer services, the restored main prayer hall of the synagogue will be used for lectures and concerts.\nThe other synagogue in Zamość is at 32 Gminna Street in the Nowa Osada district. It was erected in 1872 and extended during 1909–1913. In 1948 it was turned into a kindergarten.\nThe town of Zamość was built and designed as a renaissance “citta ideale” or “ideal city” by the Italian architect Bernardo Morando for chancellor Zamoyski (the Old City quarter of Zamość has been placed on the UNESCO list of World Heritage Sites). The Old Synagogue is a prominent example of late Polish Renaissance or Mannerist style in harmony with the general urban design. The prayer hall represents the core of the building and during the middle of the 17th century two low porches for women were added to the north and south elevations. Similar to that found in other Polish synagogues, the floor was lowered in order to increase the height of the interior. This was due to restrictions preventing a synagogue being built higher than a church.\nDuring the 18th century, a modest entrance hall was added on the west side of the prayer hall. At the same time a second floor was built over the original women's prayer rooms. At some stage the exterior walls were extended upwards, with fortress style parapets, concealing the roof. The synagogue was last renovated during the period 1967–1972 when the building received a new roof parapet and exterior decoration including decorative painting, the original of which was removed during the 18th century. The work followed an early seventeenth-century engraving and the appearance of other local buildings. Since that time no major works took place in the synagogue.\nThe vaults of the synagogue, both in the main hall and in the porches, are richly decorated with stucco in the so-called “Kalish-Lublin” style. Floral motifs including a stylized Tree of Life, crowns, and rosettes are also to be found. The walls used to bear very rich paintings and numerous Hebrew inscriptions. One of the only features remaining that indicates its former use as a Jewish house of prayer is the Aron Kodesh on the Eastern wall which dates from the first half of the 17th century. The lavishly decorated stone carved frame depicts motifs of ritual vessels used at the Temple in Jerusalem and a Torah crown. The octagonal iron bimah was located at the center of the prayer hall and was a gift by Shmuel Barzel in 1787. The prayer hall also boasted a number of majestic candelabras. Today nothing remains either of the bimah or of the candelabra.\nZamość Synagogue Revitalization Project\nThe Foundation for the Preservation of Jewish Heritage in Poland (FODZ)  together with the local authorities, NGOs including the World Monuments Fund and the Israeli Organization of Zamość Jewry, aim to establish in the synagogue a cultural center that will provide housing and support for various local initiatives, as well as the Museum of Jews from the Zamość area. The Foundation for the Preservation of Jewish Heritage in Poland website states:\nTogether with city’s authorities and local non-governmental organizations, we want to renovate the Zamość synagogue and make it a vibrant cultural centre which will serve all the people from Zamość and its environs. The synagogue will also house a Museum of Jews from Zamość and the Zamość region. The Museum is going to present the history of penetration and mutual enrichment of Polish and Jewish culture in the Zamość region. It will also introduce those Jews, who contributed to the intellectual, religious and cultural history of the region. Unfortunately the building of the Zamość synagogue is in a very bad condition and urgently requires a complex restoration. The cost of the restoration works is estimated for a couple of millions PLN; the Foundation is taking steps in order to obtain sufficient funds for renovation works, but it is a tremendous challenge.\nThe assigning of new functions to the building, including use as an art gallery, concert and theatre hall, has been deemed necessary to attain funds for necessary conservation works, although this has proved a controversial move. Another organisation, the Yaacov Magid of Dubno Fund (YMDF), established in 2001, names that one of its aims is to “restore the splendid interior of the synagogue to its former glory”. Angered by the way the FODZ has utilised the site in order to raise funds, they approached the UN representative of Agudath Israel of America to request intervention in protecting the holy site from “unholy purposes”.\nIn September 2009, restoration work was begun at the hands of the FODZ. The bulk of the funding for the restoration came from the European Economic Area and Norway Grants, which was established by Iceland, Liechtenstein and Norway to support various social and economic projects throughout Europe, as well as from the World Monuments Fund.\n- The Zamość Synagogue Revitalization Project\n- Beth Hatefutsoth Museum\n- Adam Mickiewicz Institute: Traces of the Past, Zamość\n- Survey of Historic Jewish Monuments in Poland, Samuel Gruber and Phyllis Myers, Report to the Presidents Commission for the Preservation of America's Heritage Abroad, Jewish Heritage Council World Monuments Fund, Nov. 1995, p. 46\n- Freund, Michael. Renovation begins on medieval Polish synagogue, Jerusalem Post, (September 5, 2009)\n- Adam Mickiewicz Institute: Traces of the Past, Zamość\n- Zamość Synagogue Revitalization Project\n- Rosell, Dina (2006-12-14). \"Unholy purposes\". Jewish Tribune, London. p. 3.\nThe synagogue has fungus growing on the inside which could easily be stopped but is causing severe damage. To add insult to injury the synagogue was used for an art exhibition by Zamość council in 2005 and had an Australian painter paint awful bright garnish colours all over the ancient walls. This is truly criminal!\n|Wikimedia Commons has media related to Zamość Synagogue.|\n- Computer simulation of the synagogue renovation project\n- “Have You Killed and Also Taken Possession” Eva Bar-Ze'ev, May 2001.\n- Steven Pinker's Photos of Poland, Zamość Synagogue\n- Steven Pinker's Photos of Poland, Zamość Synagogue entrance\n- Wonders of Zamosc - Synagogue\n- Zamojska synagoga dzisiaj, May 2005\n- Wall drawings for the Zamość Synagogue, David Tremlett, 2006\n- Zamość 4th Cultural Gathering, 30/06/06 – 02/07/06","AUGSBURG, city in Bavaria, Germany; a free imperial city from 1276 to 1806. Documentary evidence of Jews living in Augsburg dates from 1212. Records from the second half of the 13th century show a well-organized community, and mention the Judenhaus (1259), the synagogue and cemetery (1276), the ritual bathhouse, and \"dancehouse\" for weddings (1290). The Jews were mainly occupied as vintners, cattledealers, and moneylenders. The Augsburg municipal charter of 1276, determining the political and economic status of the Jewish residents, was adopted by several cities in south Germany. Regulation of the legal status of Augsburg Jewry was complicated by the rivalry between the episcopal and municipal powers. Both contended with the emperor for jurisdiction over the Jews and enjoyment of the concomitant revenues. Until 1436 lawsuits between Christians and Jews were adjudicated before\na mixed court of 12 Christians and 12 Jews. In 1298 and 1336 the Jews of Augsburg were saved from massacre through the intervention of the municipality. During the *Black Death (1348–49), many were massacred and the remainder expelled from the city. The emperor granted permission to the bishop and burghers to readmit them in 1350 and 1355, and the community subsequently recovered to some extent. Later, however, it became so impoverished by the extortions of the emperor that the burghers could no longer see any profit in tolerance. In 1434–36 Jews in Augsburg were forced to wear the yellow *badge . The community, then numbering about 300 families, dissolved within a few years; by 1340 the last Jews had left Augsburg. The Augsburg town council paid Albert II of Austria 900 gulden to compensate him for the loss of his *servi camerae . Thereafter Jews were only permitted to visit Augsburg during the day on business. They were also granted the right of asylum in times of war. From the late 16th century Jewish communities existed in the close-by villages Pfersee, Kriegshaber, and, temporarily, Oberhausen.\nIn the late Middle Ages the Augsburg yeshivah made an important contribution to the development of the *pilpul method of study and analysis of the Talmud. The variant of the pilpul method evolved in Augsburg is referred to as the \"Augsburg ḥillukim.\" The talmudist Jacob *Weil lived in Augsburg between 1412 and 1438. While some Hebrew pamphlets were printed in Augsburg by Erhard Oeglin as early as 1514 on the initiative of the apostate J. Boeschenstein, a Hebrew press was established in 1532 by Ḥayyim b. David Shaḥor, the wandering printer from Prague, together with his son Isaac and son-in-law Joseph b. Yakar who had learned printing in Venice. Between that year and 1540 nine books appeared including Rashi's Pentateuch commentary (1533); an illustrated Passover Haggadah (1534); Jacob b. Asher's Turim (1536); a Melokhim Buch, in Yiddish (1543); a maḥzor; and a siddur. In 1530 *Joseph Joselmann of Rosheim convened a synod of German community representatives in Augsburg, the seat of the Reichstag (see *Germany ). An organized Jewish community was again established in Augsburg in 1803. Jewish bankers settled there by agreement with the municipality in an endeavor to redress the city's fiscal deficit. In practice, the anti-Jewish restrictions in Augsburg were eliminated in 1806, with the abrogation of the city's special status and its incorporation into Bavaria; however, the new Jewish civic status was not officially recognized until 1861. In 1871 Augsburg was the meeting place of a rabbinical assembly dealing with liturgical reform. The Jewish population increased from 56 in 1801 to 1,156 in 1900. It numbered 1,030 in 1933. In 1938, the magnificent synagogue, dedicated in 1917, was burned down by the Nazis. In late 1941, after emigration and flight to other German cities, the last 170 Jews were herded into a ghetto, with 129 of them sent to Piaski in Poland in April 1942 and the rest mostly to the Riga ghetto and Theresienstadt. In the immediate postwar period, a camp was established in Augsburg to house displaced Jews. A few weeks after the liberation, services were resumed in the badly damaged synagogue by survivors of the Holocaust and Jewish soldiers of the U.S. Army, and the community was eventually reestablished. The synagogue was restored and rededicated in 1985. As a result of the immigration of Jews from the Former Soviet Union, the number of community members rose from 199 in 1989 to 1,619 in 2003.\nR. Gruenfeld, Ein Gang durch die Geschichte der Juden in Augsburg (1917); R. Strauss, Regensburg and Augsburg (1939), includes bibliography; H. Rinn (ed.), Augusta 955–1955 (Ger., 1955); M. Steinschneider, in: ZGJD, 1 (1887), 282–7; German Jewry (Wiener Library, Catalogue, series 3, 1958), 35; A.M. Habermann, in: KS, 31 (1955/56), 483–500; Monumenta Judaica, 2 vols. (1963–64); Germ Jud, 1 (1963), 14–16; 2 (1968), 30–41; A.M. Habermann, Ha-Sefer ha-Ivri be-Hitpatteḥuto (1968), 127 ff.; A. Marx, Studies in Jewish History and Booklore (1944), 329 ff. ADD. BIBLIOGRAPHY: M.N. Rosenfeld, Der juedische Buchdruck in Augsburg in der ersten Haelfte des 16. Jh. (1985); H. Kuenzl, in: Judentum im deutschen Sprachraum (1991), 382–405; P. Boettger, in: Denkmaeler juedischer Kultur in Bayern (1994), 75–90; S. Muetschele, \"Juden in Augsburg 1212–1440\" (Diss., 1996); S. Ullmann, Nachbarschaft und Konkurrenz (1999); J. Spokojny, in: Geschichte und Kultur der Juden in Schwaben, 2 (2000), 413–21.\n[Zvi Avneri /\nStefan Rohrbacher (2nd ed.)]\nSource: Encyclopaedia Judaica. © 2008 The Gale Group. All Rights Reserved."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"content_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:ce5e227d-ab02-445f-91e8-4738b520b8f2>","<urn:uuid:567b7188-3736-42d3-b17a-492dc25c6b07>"],"error":null}
{"question":"What causes hereditary ovarian cancer and what treatment options are available for carriers?","answer":"Hereditary ovarian cancer is caused by inherited faulty genes, primarily BRCA1 and BRCA2, which normally help prevent cancer. Other rare genes that increase ovarian cancer risk include Lynch syndrome genes (MLH1, MSH2, MSH6, PMS2) and RAD51C/RAD51D. For treatment and risk management, carriers have several options: PARP inhibitors like Lynparza for those with BRCA mutations, and preventive surgery (oophorectomy) which not only reduces ovarian cancer risk but can also lower breast cancer risk in BRCA carriers. Treatment decisions are influenced by genetic test results, and carriers may be eligible for specific clinical trials targeting hereditary cancers.","context":["Up to 20% of epithelial ovarian cancers are thought to be the result of inheriting a faulty gene from either your mother’s or father’s side of the family.\nMany women who have ovarian cancer or who have a relative with ovarian cancer want to find out if the cancer may be hereditary.\nWhat are genes?\nGenes are made up of DNA. They act as ‘chemical instructions’ that tell our body’s cells what to do. Genes determine things such as what colour hair and eyes we have and how tall we are. Genes also provide the recipe for building all the chemical substances in our bodies and allow the cells to function normally. Humans have between 20,000 to 25,000 pairs of genes in every cell of their bodies. One copy of each pair of genes is inherited from each of our parents. Sometimes changes can occur in genes that may stop the gene from working as it should. These changes are often called a ‘faulty gene’ or ‘mutation’. In some cases, these faulty genes can lead to disease.\nOvarian cancer in the family:\nMost women diagnosed with ovarian cancer do not have a history of ovarian cancer in their family. However, some women’s family history of ovarian cancer can increase their risk of developing the disease. You may have a family history by chance or because you have inherited a faulty gene that increases your risk of developing cancer. You are considered to have a strong family history of cancer if you have two or more close relatives on the same side of the family (father’s or mother’s relatives) who have or had cancer and one of the following applies:\n- The family members have all had ovarian cancer or different cancers (e.g. bowel or breast) that can be caused by the same faulty gene. The cancers were diagnosed when the relatives were younger than age 50;\n- You have a family member who has had genetic testing confirming they have a faulty gene;\n- You have Ashkenazi Jewish ancestry (who have a higher incidence of BRCA mutations than the general population); or\n- Your own cancer was diagnosed at an early age, or you have had more than one type of cancer.\nInheriting an ovarian cancer gene:\nOvarian cancer caused by inheriting a faulty gene is called ‘hereditary cancer’. Having a personal or family history of ovarian, breast, colon or endometrial cancer may mean you have inherited an increased risk of developing ovarian cancer.\nGenerally, the more relatives from the same side of the family who have had these related cancers, the greater your risk of having a hereditary cancer. However, it is still possible to inherit a faulty gene without having a family history of these cancers.\nInheriting a faulty BRCA1 or BRCA2 gene accounts for most cases of hereditary ovarian cancer. These genes are named for their connection to breast cancer (BReast CAncer genes 1 and 2) but can also be associated with other cancers, including cancer of the fallopian tube, peritoneum, prostate, pancreas and breast cancer in men.\nThe BRCA1 and BRCA2 genes normally help to prevent cancer, but when a woman inherits a faulty version of either gene, she is less protected against cancer. Women who inherit a faulty BRCA1 gene have about a 44% risk of developing ovarian cancer, while women who inherit a faulty BRCA2 gene have approximately a 18% risk of developing ovarian cancer up to the age of 80. This compares to less than 2% risk for women in the general population.\nMany women who have a faulty BRCA1 or BRCA2 gene do not have a known family history of ovarian or breast cancer. This can happen for many reasons including:\n- Other family members may have inherited a faulty gene but not have developed ovarian or breast cancer;\n- There may be few females in a family, making it difficult to see a pattern of family history; and/or\n- There may be little know about the family history (eg. a women is adopted)\nAlthough rare, inheriting faulty genes other than BRCA1 and BRCA2 can also increase your risk of developing ovarian cancer. Some of the other genes known to increase the risk of developing ovarian cancer include:\n- Lynch syndrome (also known as hereditary nonpolyposis colorectal cancer syndrome (HNPCC)) is a bowel cancer predisposition syndrome linked to faults in the genes MLH1, MSH2, MSH6 and PMS2. Women who inherit a faulty copy of one of these genes have up to a 15% risk of developing ovarian cancer, however, the risk varies depending on the specific faulty gene inherited.\n- RAD51C and RAD51D faulty genes (very rare).\nThere are also other rare genetic links to ovarian cancer, and ongoing research may uncover genetic links not currently known. Inheriting one of these gene faults increases a person’s risk of developing ovarian cancer but doesn’t mean that they will develop ovarian cancer.\nGuide to genetic testing and hereditary ovarian cancer\nLearn more by downloading our Ovarian Cancer Australia Genetic Testing Booklet","Treatment decisions for people with hereditary breast cancer\nHereditary cancers are different than sporadic cancers in ways that can affect treatment choices. Some treatment decisions that may be influenced by genetic test results are listed below. If you are concerned that your cancer may be caused by an inherited mutation, you should contact a genetics expert to see if genetic testing is right for you.\n- PARP inhibitors for metastatic breast cancer: Lynparza and Talzenna are targeted therapies known as PARP inhibitors. Both agents have received FDA approval for treating\nmetastatic breast cancer caused by a BRCA mutation. The National Comprehensive Cancer Network (NCCN) added Lynparza as a preferred single agent treatment for people with Her2-negative, metastatic breast cancer who carry a BRCA1 or BRCA2 mutation. You can read more about PARP inhibitors here.\n- Choice of breast surgery: Because of the very high risk for a second (or third) breast cancer diagnosis, women who are diagnosed with breast cancer who test positive for an inherited mutation often choose bilateral mastectomy (surgical removal of both breasts) rather than lumpectomy and radiation. Mutation carriers who undergo mastectomy are less likely to develop a second breast cancer since the at-risk breast tissue has been removed. Our mastectomy section provides more information about surgical options.\n- Participation in treatment clinical trials: Some research studies are exploring new treatments to specifically to treat hereditary breast cancer. If you are interested in the possibility of participating in a clinical trial, it is best to express your interest when you are first diagnosed or have a recurrence and before you start treatment. Using our Research Search Tool, you can find clinical trials enrolling patients with hereditary breast cancer.\n- Oophorectomy vs. medication to induce menopause:\nBRCA mutation carriers are at increased risk for ovarian cancer. In young women with ER-positive breast cancer, treatment sometimes includes injections to shut down the ovaries' production of estrogen. Another option may be an oophorectomy (surgical removal of the ovaries), which lowers the risk for ovarian cancer as well. Oophorectomy may lower the risk for new breast cancers in BRCA carriers who have not have bilateral mastectomies.\n- Tamoxifen, aromatase inhibitors, or other hormonal therapies:\nTamoxifen is a drug used to treat ER-positive breast cancer. Aromatase inhibitors are drugs prescribed to post-menopausal women with breast cancer to reduce estrogen production by their fat cells and adrenal glands. These drugs are used to help prevent breast cancer recurrence in women with ER-positive or hormone-receptor positive cancers. Research on these drugs suggests that they not only lower the risk for subsequent breast cancers in women who have already been diagnosed, but also in those at risk who have not ever been diagnosed with breast cancer.\n- Use of chemotherapy agents:\nSome research studies show that women with\nBRCA1 mutations tend to develop more aggressive breast cancers than those in women who develop sporadic breast cancer. A small study suggests that women with BRCA1 mutations who received any chemotherapy had better outcomes than women who did not receive chemotherapy. Other research suggests that BRCA-positive individuals diagnosed with triple-negative breast cancer may respond particularly well to platinum-based chemotherapy.\nIf you are a breast cancer survivor making decisions about genetic testing can be confusing and you may want additional guidance or support. FORCE's Peer Navigation Program provides expert reviewed resources and 1:1 personalized peer support by specially trained volunteers who have experienced the very challenges you face."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:9aa6d7eb-6f25-40cc-adcb-2c39912075b6>","<urn:uuid:a0d39458-531e-422e-93e9-6ce962ef7c26>"],"error":null}
{"question":"What protected species inhabit Saaremaa's nature reserves and Pacific Remote Islands, and what conservation measures exist?","answer":"In Saaremaa's Viidumäe Nature Reserve, protected species include the endemic Rhinanthus osiliensis and 59 rare plant species under protection. The Pacific Remote Islands Marine National Monument houses 22 species of protected seabirds, sea turtles, reef sharks, three threatened coral species, and some of Earth's largest coral colonies. Conservation measures include designation of protected areas in both regions - Saaremaa's nature reserves are protected under conservation legislation, while the Monument is managed by U.S. agencies and prohibits all commercial extraction to preserve its pristine ecosystems.","context":["Saaremaa recreation area\nSaaremaa Recreation Area encompasses the islands of Muhu and Saaremaa and the small islands and islets near them. The fragile and unique wildlife of the islands, rich in relicts, provides excellent and varied opportunities for nature recreation. In order to share in everything, we recommend you to start your visit at RMK Mustjala nature center and information point.\nCamping areas, hiking trails and other places to go\nMihkel Rand´s dendrarium (picture) is located in Pöide parish and is made by village blacksmith Mihkel Rand. Thirty years of work have resulted in a collection of over 100 trees and bushes. The arboretum is protected under nature conservation legislation.\nKoigi hiking trail is located in Pöide parish and is 4,8 km long. There is a parking area, information board and metal boardwalk on tyres. The trail starts 800 m from the parking area. Koigi is the only bog in Saaremaa with wonderful pools and hollows.\nTriigi campsite is on the beautiful sandy beach in Leisi municipality on the Triigi peninsula. There is a parking area, 3 campfire sites, picnic tables, woodshed, information board and a dry toilet. Nearby are sights like Karja Church, Angla windmills and Sassi dinner house.\nMeiuste campsite is a nice resting place with sandy beach. It is located in Leisi parish and it has 3 campfire sites, picnic tables, 2 dry toilets and 2 woodsheds.\nTuhkana beach campsite is located in Leisi and it has a nice sandy beach for swimming. There is a parking area for 20 cars or 4 buses. Also picnic tables, 2 outdoor fireplaces, information board and a dry toilet in the parking area. Tuhkana is known as the best sandy swimming beach in Saaremaa. 500 m to the beach, access only along the boardwalk. Volleyball court on the beach. Good mushroom and wild berry forests around.\nLaugu forest keeper´s cottage(picture) was built for the military forest ranger in 1856. This old, 32-metres long farmhouse has been in service of Estonian forestry for 150 years, regardless of the political regime. Today the building serves as the cultural heritage and nature education centre of RMK Saaremaa Recreation Area, in which visitors learn the history of the Estonian forestry and explore old forestry techniques. There is a big parking area, picnic tables and a fire ring.\nNinanuki forest hut is located in Triigi peninsula and there is a parking area for 5 cars. The hut is 500 metres from the parking place. There is a table and benches and a fireplace in the hut. The hut has room for 4 people. Outside there is a picnic table, dry toilet, woodshed and an outdoor fireplace. Near the hut is a beautiful sandy beach and a pine forest. Use of the forest hut is free of charge for visitors.\nKalja hiking trail introduces the karst area in Mustjala parish. There are 7 large sinkholes, the largest of which is 18 m in circumference and 5.5 m in depth. It is an interesting natural phenomenon during the spring flood. Near the karst area the interesting old Selgase dolomite quarry and the Küdema doline (sinkhole) can be found. Marked hiking trail (blue markings), with descriptions of points of interest.\nKonati hiking trail (1 and 3 km) introduces forest management, cultivated pine forests, protected forests and old indigenous trees. Parking by the Mustjala road or by the Konati campsite. At the start of the trail by the road there is an information board and instructions. The trail is signposted and the points of interest marked. Possible to choose the shorter or longer trail: 1 km or 3 km. You can walk the trail on your own or with a guide.\nKalasma hiking trail and campsite is located in Mustjala and is independently passable, marked with yellow ribbons and points of interest information. The track is 5.4 km long and introduces the coast and its development. Campsite has picnic tables, a shelter, ampfire site, dry toilet and a woodshed.\nVeere camping sites are located on Pidula highway. There are nice campfire rings on the seashore with benches and information boards. Also dry toilets and woodsheds.\nVeere observation platform(picture) offers a good view of the sea, especially in the evening, when boats arrive at the harbour. The nearby sights of interest include the Harilaid peninsula with the tilted lighthouse of Kiipsaare, the interesting wooded meadow at Tagamõisa, Kihelkonna church,Mihkli Farm Museum, Viidumäe Nature Reserve and Loona manor, which houses the centre of the Vilsandi National Park and RMK information point. Near the platform is an information board and stairs to the sea. No campfires allowed.\nViidumäe study trail starts from the center of Viidumäe Nature Reserve Centre, which is located 28 km from Kuressaare. The trail is 1.6 km long. Viidumäe Nature Reserve is home to over 660 species of vascular plants, among which 59 rare species are under protection in Estonia. The most famous of these is the endemic Rhinanthus osiliensis, a species of rattle. The escarpment is surrounded by a belt of springs, 70 to 80 according to the count. In some places crooked oak trees grow in the understory of pine forests. Bogs and fens cover about 10% of the nature reserve. Parking for 5 cars or a tour bus at the start of the trail. Also information board, arrow signs and points of interest.\nWhere to go next\nDejevo bunker and camping area is located near the lake Karujärv. It is a fixed up military underground building, which can accommodate up to 60 people. Outside is a large camping area with a barbecue area, benches, tables, and three dry toilets. In the forest is a 9 km long marked trail for cycling and hiking; skiing in the winter. Great place for meetings, reunions and other events. The nearest swimming beach Karujärv is 1.5 km from the camping area. Nearby Kärla Church and Suure Tõllu kerisekivi.\nHarilaiu resting place is located in Kihelkonna parish, 50 km from Kuressaare. There is also a hiking trail that leads to the lighthouse Kiipsaare. There is a woodshed, outdoor fireplace, picnic table, camping area for tents and a dry toilet. Motorised vehicle traffic is prohibited!\nKulpri camping site is located in Kihelkonna parish, on island Vilsandi. It is known as a beautiful sandy beach perfect for camping or swimming. The camping site has two bench tables, fireplaces, information stand, a dry toilet and a woodshed. The beach near the site is public and local people use it for swimming.\nElda camping area is located in Lümanda parish, Atla village. On both sides of the campsite there are Silurian cliffs and ivy on them. A beautiful view on Vilsandi National Park aquatorium and the most western islet of Estonia. A good point for watching seals, birds and butterflys. There is parking area for 10 cars, information boards, covered bench tables, covered beds for fire, shed and an outhouse. Camping area belongs to the Vilsandi National Park. Please visit the Elda cliff on foot, walking along the seashore or taking the paths leading to the top of the peninsula across privately owned lands.\nKäkisilma camping area is located in Lümanda parish, Kuusknõmme village. It is a seaside camping site with a pineforest. Nearby parking area and 10m high wooden observation tower.\nLoode oak stand study trail (picture) is in Nasva village, about 3 km to the south-west from the centre of Kuressaare. The oak belonging to the oldest generation of the Loode oak stand, dedicated to the linguist Johannes Aavik, was measured to be 450-500 years old in October 2000. The oldest generation of the oak stand is presumably formed of trees that remained from the shipbuilding campaign initiated by Peter I. The start of the trail is accessible by any means of transport. Parking for 2-3 buses or 10 cars. Information boards are in the beginning of the trail and on the trail. Several rare species grow in the oak stand. 14 orchid species have been found there.\nKeskranna resting place is located in Kuressaare-Sõrve highway, parking lot is located on the left side of the road. Nearby is a long and beautiful sandy coastline, pine forest and an interesting flora of rare plant species. Footpath leads to the sea. The place has an outhouse, bench and two clothing cabins.\nJärve recreational area is located in the area of Salme, in Järve village. The area has a long golden sand beach and a pine forest with interesting plants. It is forbidden to make a campfire and drive a car in the area.\nSopi forest hut is 30 km from Kuressaare, on Kuressaare-Sääre road. This is the earliest known forest keeper’s place in Saaremaa, dating from 1795, when Campenhausen prepared the first forest management plan for state forest and appointed Kiviste Andres as the forest keeper of Sopi forest. Around the hut there is old-growth forest with interesting erratic boulders, former anti-tank lines, old stone farm fence; the access road is built on brushwood. The hut can be used all year round. The hut can accommodate 3 people. There is a table, benches and fireplace in the hut. Dry toilet, woodshed and outdoor fireplace. No drinking water available.\nMore sights: Lõo-Kaugatoma alvar, Sõrve lighthouse, Ohessaare cliffs, Stebel coastal defense battery, Lõo anti-tank line ja 1 km long Lindmetsa hiking trail. Also Viieristi hiking trail that starts from Sääre highway and leads to spring named Võluallikas. Viieristi terrace is mostly covered with pines.\nMore information about the south-western Saaremaa is available at Mändjala Nature House, which is located by the Suur Katel Bay in RMK Saaremaa Recreation Area.","Pacific Remote Islands Marine National Monument\nUnited States Department of the Interior\nThe Tentative Lists of States Parties are published by the World Heritage Centre at its website and/or in working documents in order to ensure transparency, access to information and to facilitate harmonization of Tentative Lists at regional and thematic levels.\nThe sole responsibility for the content of each Tentative List lies with the State Party concerned. The publication of the Tentative Lists does not imply the expression of any opinion whatsoever of the World Heritage Committee or of the World Heritage Centre or of the Secretariat of UNESCO concerning the legal status of any country, territory, city or area or of its boundaries.\nProperty names are listed in the language in which they have been submitted by the State Party\nThe Pacific Remote Islands Marine National Monument (PRIMNM) is located south of the Hawaiian Island archipelago in a remote area of the Pacific Ocean. The Monument area is approximately 370,000 square nautical miles (1,270,000 square kilometers), or nearly twice the size of the State of Texas. It ranges from Wake Atoll in the northwest to Jarvis Island in the southeast, and also encompasses Baker and Howland Islands, Johnston and Palmyra Atolls, and Kingman Reef. It includes outstanding examples of pristine coral reef and deep sea ecosystems. It is managed by the U.S. Fish and Wildlife Service, the National Oceanic and Atmospheric Administration and the Department of Defense.\nBaker Island 1 N 558207 21403\nHowland Island 1 N 542473 89677\nJarvis Island 4 M 389087 9958617\nWake Atoll 58 Q 673382 2132592\nJohnston Atoll 2 Q 656234 1850416\nPalmyra Atoll 3 N 823505 651726\nKingman Reef 3 N 785785 706296\nJustification of Outstanding Universal Value\nThe seven atolls and islands included within the Pacific Remote Islands Marine National Monument are farther from human population centers than any other U.S. area. They represent one of the last frontiers and havens for wildlife in the world, and comprise the most widespread collection of coral reef, seabird, and shorebird protected areas on the planet. This description focuses on natural heritage values, but the site also has important cultural heritage values that may be included in a future nomination.\nCriterion (vii): The Monument is home to pristine coral reefs and thriving fish and bird populations. The Monuments sparkling waters, colorful corals and abundant wildlife create a unique area of natural beauty.\nCriterion (viii): Significant geological features of the Monument include over 165 submerged seamounts, mountains rising from the seabed that do not reach the sea surface. These seamounts provide habitat for colonies of deepwater corals that are thousands of years old, and provide an opportunity for identification and discovery of many species not yet known to humans, with possibilities for research, medicines, and other important uses. In addition, much of the Monument is located in the Prime Crust Zone (PCZ) of mineral-rich crusts. The Monument designation protects the Pacific Remote Islands from all commercial extraction, including mining. The site provides crucial baseline information that may be used in the future to protect additional areas from mining and mitigate mining impacts in areas outside the Monument.\nCriterion (x): Many threatened, endangered, and depleted species thrive in the Monument, including at least 22 species of seabirds, Pisonia forest, sea turtles, pearl oyster, giant clams, reef sharks, coconut crabs, groupers, Napoleon (Maori) wrasse, humphead parrotfish, dolphins, whales, three threatened coral, and some of the most ancient and largest coral colonies and communities on Earth. The Monument provides foraging habitat for several of the world’s largest remaining colonies of sooty terns, lesser frigatebirds, red-footed boobies, red-tailed tropicbirds, and other seabird species.\nThe low coral islands and atolls of the Monument are the crests of ancient coral reef caps and massive underlying volcanoes. Coral cover and biodiversity is much higher than in Hawai’i and Florida. From these protected waters, we can gain knowledge that can be applied elsewhere to improve coral reef management in more populated areas. Beyond the shallow fringing reefs and terraces, the slopes of the extinct volcanoes drop off sharply to the deep floor of the equatorial Pacific Ocean. The Monument protects near-pristine deep sea areas and open-ocean ecosystems overlaying a diverse seafloor topography and array of benthic and pelagic habitats that support biological communities, including high-density deep-sea coral and sponge communities.\nStatements of authenticity and/or integrity\nPRIMNM protects a remote, largely pristine coral reef ecosystem. Current threats that could affect the global significance of the Monument include climate change (such as coral bleaching and ocean acidification), invasive species and Illegal, Unreported, and Unregulated (IUU) Fishing. Unlawful activities pose ecological threats and resource integrity concerns. Monument managing partners are working together to address these threats and concerns by exploring various technologies that would allow for remote monitoring and enforcement.\nComparison with other similar properties\nThe PRIMNM possesses a fish biomass nearly twice the size of Papahanaumokuakea Marine National Monument and 16 times that of the main Hawaiian Islands. Kingman Reef has the greatest known fish biomass and proportion of apex predators of any coral reef ecosystem that has been scientifically studied in the world. Kingman is also one of the world’s most pristine coral reefs and will likely serve as a baseline for future studies of coral reef and microbial communities."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:989d0fca-19e0-42ac-824a-b21ae2804932>","<urn:uuid:b702d9f9-5a69-4ba1-82a9-d5be5d50df01>"],"error":null}
{"question":"What innovative sustainable technology is Adidas developing for eco-friendly shoes? 阿迪达斯在环保鞋方面有什么创新技术？","answer":"Adidas has developed a prototype sneaker called the 'Adidas Future craft Bio fabric' made from a material called 'Bio steel'. This material is 15% lighter than traditional silk strands and is completely biodegradable. The shoe is designed to dissolve when it comes into contact with the digestive enzyme proteinase. Under the right conditions, the shoes will completely degrade in 36 hours. However, this shoe was never made available to the public.","context":["Top 10 Shoe Brands\nA shoe is a type of footwear that is designed to keep the human foot safe and comfortable. It can also be used for fashion. Environment plays a very important role when it comes to the human shoe. The shoe is made according to the environment, climate, and surroundings, and it can adapt to various constraints of climatic conditions, hazards such as sharp rocks and temperature variations. Steel-toe boots, essential footwear on industrial work sites, are examples of footwear worn as a safety device.\nShoes have traditionally been constructed of leather, wood, or canvas, but rubber, plastics, and other petrochemical-derived materials are also used widely. The shoe industry generates approx. $200 billion in a year. Because the materials are difficult to disassemble, recycle, or reuse, 90 per cent of shoes end up in landfills. If you want to buy any shoes, consider their brands, durability, material, style, design, packaging, pricing, and comfort level. Footwear is made for various purposes and needs, including sports, formals, casual wear, and stylish. Several well-known shoe firms have used cutting-edge technology to create cutting-edge goods that are extremely stylish and comfortable.\nThere are many different styles of shoes available in the market from different companies. The majority of shoes are made for certain purposes. Boots, for example, are usually made for labour or severe outdoor use. Athletic shoes are made for certain sports like running, walking, and other activities. Some shoes are meant to be worn on more formal occasions, while others are worn on a more casual basis. There are also shoes made specifically for particular forms of dancing. Orthopaedic shoes are unique sorts of footwear made for those who have specific foot problems or requirements. For example, there are many animals like dogs and horses that may wear different types of shoes to protect their feet.\nCertain types of shoes can fit into several categories this also depends on the activity for which they were made. Cowboy boots, for example, are considered boots but can also be worn as dress shoes for more formal events. Hiking boots have many of the same protective qualities as boots, but they also have the flexibility and comfort of many athletic shoes. Flip-flops are generally informal footwear, although they've been seen on formal events like White House visits.\nThe top ten most popular shoe brands are as follows:\nDesign, fit, comfort, material are few requirements that a shoe must fulfil before reaching a buyer's hands. All shoes have a long and illustrative 40000-year history that may be difficult to believe. What began as a product created to fulfil a very basic function has now evolved into an art that is still functional. The most well-known shoe brands are constantly competing to outdo one another with the latest and most advanced shoe-making technologies to provide their clients with shoes that meet all of their needs.\nNike is an American multinational corporation that designs, develops, manufactures, and markets footwear, clothes, equipment, and accessories in the whole world. The company is located in the Portland metropolitan area, near Beaverton, Oregon. The company's revenue in the coming year exceeds US$37.4 billion. The upper is made of Nike's famous Flyknit material, which looks and feels like a sock and stretches like one. The shoes are designed to be tight fitting. Furthermore, Nike is one of the world's most forward-thinking shoe companies.\nAt the same time, they tend to stick to a few popular models for a long time (i.e., the more than 30 years history of the Nike Pegasus). They are continually introducing new technology and new names and classifications that go along with them. For example, the Nike React foam is a unique blend of cushioning (soft while compressing on impact with the ground) and responsiveness (the ability to quickly going back to its original shape).\nSince August 2021, Reebok International Limited, a British-American footwear and apparel business, has been a subsidiary of American management firm Authentic Brands Group. Adidas, the German sporting goods corporation, held the brand from 2005 to 2021. Reebok manufactures and sells apparel and footwear for fitness, running, and CrossFit. It is the current sponsor of Spartan Race and was once the official footwear and gear sponsor of CrossFit. Fitness, running, CrossFit gear, and clothes and footwear are designed, manufactured, distributed, and sold by Reebok. T-shirts, hoodies, and jeans are among the things available in the clothing range.\nFor a long time, Reebok was overshadowed by major brands like Nike in the running world. Still, it has recently invested more money in its running technology, resulting in some genuinely outstanding shoes. If you look over Reebok's current lineup, you'll notice a lot of interesting technologies and innovative ideas. In 2017, the business introduced their Pebax-based Floatride Foam, which provides an excellent return of energy while significantly lighter than EVA or TPU-based foams. This foam allows the Floatride Run Fast to provide outstanding cushioning without adding weight, resulting in a stunning speed shoe that earned Gear of the Year award for 2018.\nAdidas is a German multinational firm that designs and manufactures shoes, clothes, and accessories. It was founded and is based in Herzogenaurach, Germany. It is Europe's largest sportswear producer and the world's second-largest after Nike.\nAdidas' identification mark has been employed as a marketing tool on the company's clothing and shoe designs. Adidas purchased the branding from Finnish sports firm Karhu Sports in 1952 for the equivalent of 1,600 euros and two bottles of whiskey, and it became so popular that Dassler dubbed the company \"The three stripes company.\" Adidas teased a sneaker built from ocean debris in November 2016. The shoe is made of a material called \"Bio steel.\" The \"Adidas Future craft Bio fabric\" is the name of the sneaker. The employed substance is 15% lighter than traditional silk strands and is completely biodegradable. When the shoe comes into touch with a high quantity of the digestive enzyme proteinase, which occurs naturally, the shoe begins to dissolve. The shoes will degrade in 36 hours if this happens. The shoe was never made public.\nKering is a global leader in the fashion and accessory industries. Gucci, Bottega Veneta, Volcom, and Puma are among the powerful luxury, lifestyle, and sports brands it generates. Puma, the company's footwear, apparel, and accessories line, is outstanding and varied.\nPuma has continually expanded over the last few decades to produce a diverse product portfolio, including Puma Suede, Basket, Roma, Easy Rider, and many others. Co-branded sneakers with industry heavyweights like Ducati and Ferrari are part of Puma's motorsports range. Usain Bolt, the world's fastest man, endorses the brand's highly advanced footwear. Puma announced its return to basketball in 2018 after an almost 20-year hiatus. Jay-Z has been named Creative Director of Puma Basketball by the company.\nBata India, a subsidiary of the Bata Shoe Organization, is India's largest retailer and premier footwear producer. Bata Shoe Company Private Limited was established as a modest operation in Kon Nagar (near Calcutta) in 1932 and was incorporated in 1931 as Bata Shoe Company Private Limited. The foundation stone for Bata's first structure, currently known as the Bata, was laid in January 1934, and the whole property was doubled in size in the years that followed. Bhatnagar is the name given to this township. It was also the first manufacturing plant in the Indian shoe business to be certified according to ISO: 9001.\nIn 1973, the company changed its name to Bata India Limited and became public. Bata India has grown to become the country's leading footwear store. It has a retail network of over 1375 outlets that no other footwear manufacturer can match in reach and coverage. The stores are conveniently located and may be found in all metros, mini-metro stations, and towns.\nBata's sophisticated new locations, complemented with a higher-quality product line, are designed to provide customers with a superior shopping experience. The company also runs a vast non-retail distribution network that serves millions of clients through over 30,000 dealers through its urban wholesale segment.\n6. New Balance\nThis Boston-based global brand was founded in 1906 by William J Riley and is situated in the United States. It is one of the world's largest shoe manufacturing enterprises. It offers a wide range of style, comfort, sports, and casual footwear as a premium brand.\nThe organisation employs a variety of integrated technologies as well as other new ideas. This brand is associated with sports such as soccer, basketball, tennis, golf, and many more and casual footwear. Dale Steyn, Matt Reid, Trayvon Bromell, and Mathew Wade are just a few celebrities who have signed endorsement agreements with New Balance.\nSkechers is a billion-dollar company that is pushing the shoe industry to new heights. This brand was established in 1992 by Robert Greenberg. This brand ranks among the top three most important brands in the United States.\nThis brand specialises in both lifestyle and athletic footwear. For its diverse customer base, this well-known company offers about 900 styles and distinguished patterns. Camilo Cabello, Kelly Brook, and Brook Henderson are just a few of the celebrities who have endorsed the company.\nWoodland is a homegrown brand that has gone global thanks to its high-quality cloth. According to their name, the shoes have a wood-like fashionable and sporty design that people of all ages and personalities can wear.\nThe versatility of the design and material utilised in producing these shoes is what makes them so long-lasting and practical. It's no surprise that they've been around for a long time, and the variety of styles, colours, and designs make them a wardrobe must-have.\nFounded in 1966 by James Van Doren, Gordon C Lee, and Paul Van Doren, Vans is a well-known shoe brand. It functions as a subsidiary of VF Corporation.\nIt is an American-based firm that produces a wide range of skateboarding footwear. For the younger age, the main focus of this brand is elite athletes. Its E-commerce business allows customers to customise shoe styles on an online platform.\n10. Peter England\nIt appears that all big fabric companies have decided to service the country in the shoe sector. Then there's Peter England, who boasts of designs that meet international standards and are of the finest quality. A must-have for men's casual and dressy footwear.\nWhether you're wearing jeans or a suit, you'll want a pair of flexible shoes that complement your overall appearance, and Peter England is the man for the job. The fashionable new colour textures from this brand will certainly offer attention to your overall fashion statement. Peter England's shoes fully support the natural mobility of the body and aid inefficient movement. Their shoe models are well-designed and reasonably priced, keeping in mind the latest trends. It is unsurprising that Peter England is regarded as India's top formal shoe brand and will continue to be so for a long time. Contrast colour, stylish, comfortable, cushioned footbed are just a few of the qualities of this shoe."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:0a0d17cb-110f-4dbf-ae1a-120a118f00e4>"],"error":null}
{"question":"How does the presence of African-origin mitochondrial DNA in Ukrainian cattle breeds compare with the heterozygosity patterns of κ-casein genes in Russian cattle breeds?","answer":"In Ukrainian cattle, three genotypes of Ukrainian Whitehead (FJ014303, FJ014298, FJ014294) belong to the T1a mtDNA haplogroup of African origin. In comparison, Russian cattle breeds show varying patterns of κ-casein heterozygosity, with some herds displaying significant excess of heterozygotes. While both genetic markers indicate diversity, the distribution patterns differ - the African mtDNA presence represents distinct maternal lineages, whereas the κ-casein heterozygosity shows high within-breed variation but low differentiation between breeds, as evidenced by low Fst values (0.0025-0.0431).","context":["GENEBANK ANALYSIS: SINGLE NUCLEOTIDE POLYMORPHISMS OF ANIMALS MITOCHONDRIAL GENOME UKRAINIAN GRAY AND UKRAINIAN WHITEHEAD CATTLE BREEDS\nExamination of variation in mitochondrial DNA (mtDNA) control region sequences has been pivotal in the elucidation of bovine phylogeography. Initial studies have demonstrated a deep bifurcation in bovine mtDNA phylogeny, which indicates a predomestic divergence between the two major taxa of cattle, humped zebu (Bos indicus) and humpless taurine (Bos taurus). Subsequent genetic investigations have yielded further inference regarding origins within the B. taurus lineage. B. taurus mtDNA sequences fall into one of five ancestral star-like haplotypic clusters, which are geographically distributed. Just one of these clusters, T3, predominates in Western Europe. Symmetrically, diversity within Africa is composed almost exclusively of members of a separate haplotypic cluster, T1, which is rarely detected elsewhere. The almost mutually exclusive geographic distribution of these two haplotypic clusters allows geographical exceptions to be securely identified as secondary introductions.\nWe investigated a comparative analysis of mitochondrial genome sequences for different breeds of cattle (Bos taurus, Bos indicus) with global genetic bank. Mitochondrial DNA sequences from bovine animals (Bos taurus) breeds Ukrainian Whitehead and Ukrainian Gray freely available on the global genetic bank (http://www.ncbi.nlm.nih.gov/Genebank/). Local alignment of sequences for mitochondrial genome of different cattle breeds was performed using the program MEGA 4.0. For the detection of nucleotide replacements used mitochondrial DNA sequence of Bos taurus Hereford breed (Anderson S. at al., 1982) as a reference (accession number V00645).\nHere we report the analysis results of testing for 9 genotypes Ukrainian Gray mitochondrial DNA sequences showed that one animal (GQ129208) has haplotype Bos indicus, other belongs to haplogroup T1 with European origin mtDNA. Analysis of single nucleotide replacement in one of the hypervariable regions mtDNA (position number 16019-16339) shows, that among 10 submitted genotypes of Ukrainian Whitehead the 3 of them (FJ014303, FJ014298, FJ014294) relating to T1a mtDNA haplogroup of African origin, which characterized by replacement of T to C at position 16255. Also have been two animals (FJ014301, FJ014295) with single nucleotide replacements with relatives to Bos indicus mtDNA haplogroup.\nWe performed alignment with reference sequences (Bos_taurus_v00654.1) and comparative nucleotide sequences analysis of another hypervariable D-loop (position number 1-240) mtDNA with 5 Ukrainian Whitehead genotypes and 5 Ukrainian Gray genotypes represented in genetics bank. Among the Ukrainian Whitehead genotypes (FJ014298, FJ014297, FJ014296, FJ014295, FJ014294) all were polymorphic that characterizes large differentiation these animals for maternal and describe deep heterogeneous parent population of studied group. We determined one animal with genotype FJ014295 was significantly different by the number of segregation sites. The analyzed sequences (FJ014290, FJ014289, FJ014288, FJ014287, FJ014286) of 5 Ukrainian Gray genotypes showed no polymorphism in hypervariable D-loop (position number 1-240) mtDNA.\nThe mtDNA analysis of different species of animals allowed to distribute their mtDNA belonging to European, African and Asian haplogroups. The technique, which allows to differentiate the animals represented by their belonging to the respective haplogroups. The process that gave rise to different genotypes in one lineage is clearly of fundamental importance in understanding intraspecific mitochondrial polymorphism and evolution in mammals.\nСomprehensive study genetic material provide more opportunities to optimize costs in-situ conservation of different cattle breeds, to optimize methods and techniques which used in ex-situ conservation programmes of National gene bank of animal genetic resources.\n2. Henetyko-selektsiynyy monitorynh u molochnomu skotarstvi / [Zubetsʹ M.V., Burkat V.P., Yefimenko M. YA. ta in.]; nauk. red. V. P. Burkata. — K. : Ahrarna nauka, 1999. – 88 s.\n3. Pochernyayev K.F., Hetya A.A. Ustanovlennya porodnosti svyney z vykorystannyam polimorfizmu mitokhondrialʹnoho henomu. Rozvedennya i henetyka tvaryn 2007. Vyp.41. – S. 233–239.\n4. Stolpovskiy YU.A. Kontseptsiya i printsipy geneticheskogo monitoringa dlya sokhraneniya in situ porod domestitsirovannykh zhivotnykh / YU.A. Stolpovskiy // Sel'skokhozyaystvennaya biologiya. – 2010. – № 6. – S. 3–8.\n5. Achilli A., Olivieri A., Pellecchia M. et al. Mitochondrial genomes of extinct aurochs survive in domestic cattle // Current Biology. – 2008. – V.18. – Р. 157–158.\n6. Avise J. C., Arnold J., Ball R. M., Bermingham E., Lamb T., Neigel J. E. et al. Intraspecific phylogeography: The mitochondrial DNA bridge between population genetics and systematics // Annu. Rev. Ecol. Syst. – 1987. – V.18. – P.489–522.\n7. Anderson S., De Bruijn M.H., Coulson A.R., et al. Complete sequence of bovine mitochondrial DNA. Conserved features of the mammalian mitochondrial genome // Journal of Molecular Biology. – 1982. – V.156. – P. 683–717.\n8. Cymbron T., Loftus R.T., Malheiro M.I., Bradley D.G. Mitochondrial sequence variation suggests an African influence in Portuguese cattle // Proceedings of the Royal Society of London, Series B, Biological sciences. – 1999. – V.266. – P. 597–603.\n9. Ginja C., Penedo C.T., Melucci L. et al. Origins and genetic diversity of New World Creole cattle: inferences from mitochondrial and Y chromosome polymorphisms // Animal Genetics. – 2010. – V. 41, № 2. - Р. 128–141.\n10. Kantanen J., Edwards C.J., Bradley D.G. et al. Maternal and paternal genealogy of Eurasian taurine cattle (Bos taurus) // Heredity. – 2009. V.103. – P.404–415.\n11. Loftus R.T., MacHugh D.E., Bradley D.G. et al. Evidence for two independent domestications of cattle // Proceedings of the National Academy of Sciences of the United States of America. – 1994. – V.91. – P.2757–2761.\n12. Lai S.J., Liu Y.P., Liu Y.X. et al. Genetic diversity and origin of Chinese cattle revealed by mtDNA D-loop sequence variation // Molecular Phylogenetics and Evolution. – 2006. – V.38. – P.146–154.\n13. Mannen H., Kohno M., Nagata Y. et al. Independent mitochondrial origin and historical genetic differentiation in North Eastern Asian cattle // Molecular Phylogenetics and Evolution. – 2004. – V.32. – P. 539–544.\n14. Miretti M.M., Pereira H.A., Jr. Poli M.A. et al. African-derived mitochondria in South American native cattle breeds (Bos taurus): evidence of a new taurine mitochondrial lineage // Journal of Heredity. – 2002. – V.93. – P.323–330.\n15. Troy C.S., MacHugh D.E., Bailey J.F. et al. Genetic evidence for Near-Eastern origins of European cattle // Nature. – 2001. – V.410. – P.1088–1091.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.","The frequencies of the κ-casein gene (CSN3) alleles and genotypes have been determined in five Russian cattle breeds (Bestuzhev, Kalmyk, Russian Black Pied, Yaroslavl, and Yakut breeds) by means of PCR-RFLP analysis using two independent restriction nucleases (HinfI and TaqI) and by allele-specific PCR. Typing alleles A and B of CSN3 is of practical importance, because allele B is correlated with commercially valuable parameters of milk productivity (protein content and milk yield) and improves the cheese yielding capacity. The frequencies of the B allele of CSN3 in the breeds studied vary from 0.16 to 0.50; and those of the AB and BB genotypes, from 0.27 to 0.60 and from 0.02 to 0.23, respectively. The Yaroslavl breed had the highest frequencies of CSN3 allele B and genotype BB (0.50 and 0.23, respectively). The frequencies of the B allele and BB genotype in other breeds studied varied from 0.25 to 0.32 and from 0.03 to 0.09, respectively. In none of the breeds studied have the observed and expected heterozygosities been found to differ from each other significantly. However, the observed genotype distributions significantly differ from the expected one in some herds (in most such cases, an excess of heterozygotes is observed). Two herds of the Yaroslavl breed dramatically differ from each other in the heterozygosity level: a deficit (D = −0.14) and an excess (D = 0.20) of heterozygotes have been observed at the Mikhailovskoe and Gorshikha farms, respectively. In general, however, the heterozygosity of the Yaroslavl breed corresponds to the expected level (D = 0.04). Analysis of breeds for homogeneity with the use of Kulback’s test has shown that all cattle breeds studied are heterogeneous, the CSN3 diversity within breeds being higher than that among different breeds, which is confirmed by low F st values (0.0025–0.0431). Thus, a DNA marker based on CSN3 gene polymorphism is extremely important for breeding practice as a marker of milk quality; however, it is inapplicable to marking differences between breeds or phylogenetic relationships between cattle breeds because of the high diversity with respect to this locus within breeds.\nRussian Journal of Genetics – Springer Journals\nPublished: Jan 24, 2007\nIt’s your single place to instantly\ndiscover and read the research\nthat matters to you.\nEnjoy affordable access to\nover 18 million articles from more than\n15,000 peer-reviewed journals.\nAll for just $49/month\nQuery the DeepDyve database, plus search all of PubMed and Google Scholar seamlessly\nSave any article or search result from DeepDyve, PubMed, and Google Scholar... all in one place.\nGet unlimited, online access to over 18 million full-text articles from more than 15,000 scientific journals.\nRead from thousands of the leading scholarly journals from SpringerNature, Elsevier, Wiley-Blackwell, Oxford University Press and more.\nAll the latest content is available, no embargo periods.\n“Hi guys, I cannot tell you how much I love this resource. Incredible. I really believe you've hit the nail on the head with this site in regards to solving the research-purchase issue.”Daniel C.\n“Whoa! It’s like Spotify but for academic articles.”@Phil_Robichaud\n“I must say, @deepdyve is a fabulous solution to the independent researcher's problem of #access to #information.”@deepthiw\n“My last article couldn't be possible without the platform @deepdyve that makes journal papers cheaper.”@JoseServera"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:8a232d40-5fb6-4fca-b0ee-4ee5ab6f4fe0>","<urn:uuid:b705dee9-9c76-4e10-8bfb-fae3ee1e8e12>"],"error":null}
{"question":"Does Keith Dow's advocacy for disability inclusion align with Moltmann's vision of divine-human relationships?","answer":"Yes, there is clear alignment between these perspectives. Dow advocates for full participation and belonging of people with disabilities in faith communities, challenging exclusionary practices. This corresponds with Moltmann's vision where God demonstrates 'encountering kindness' and 'vulnerable love,' creating relationships of friendship rather than domination. Moltmann emphasizes that divine freedom manifests through making space for all people, while Dow similarly works toward communities 'where everybody belongs' - both envisioning inclusive fellowship that values all persons regardless of ability.","context":["Faith Today Podcast with Karen Stiller\nLISTEN TO PODCAST March 2, 2022.\nKeith Dow is our guest and manager of organizational and spiritual life at Christian Horizons, an organization that works together with people who experience disabilities to accomplish their goals and nurture communities where everybody belongs.\nLetting Go & Learning Anew\nWATCH ON YOUTUBE. From Presence to Participation Webinar. March, 2022.\nMutual Book Interview: Brian Brock and Keith Dow\nWATCH ON YOUTUBE. September 2021\nDr. Brian Brock and Dr. Keith Dow discuss intersecting points of their most recent books, the ethics of caregiving, and disability theology. This virtual event was hosted by the Institute on Theology and Disability, with support from Baylor University Press, on 15 September 2021.\nToward Accessible Faith & Flourishing: Reconsidering Greek Intellectualism in Western Christian Theology\nREAD ARTICLE (Open Access). 2021. Journal of Disability and Religion.\nMuch of what we believe about the “good life” can be traced to Ancient Greek philosophy. In Western Christian theology, who is seen as living a life of human flourishing is significantly shaped by Greek intellectualism. In our churches and communities, we must resist the forces that undermine the lives of witness and contribution of people with disabilities.\n- READ How MAiD Aids Ableism (2021)\n- READ COVID in a Lifeboat (2020)\n- READ Curing our Moral Virus of Loneliness (2020)\n- READ The Crash of Sacred Disruption (2018)\n- READ Baby Steps for Gerber (2018)\n- READ Putting Christ in Perspective (2017)\n- READ Stories Yet to Tell (2017)\nSimplicity, Purity of Heart, and the Gift of Limits\nREAD ARTICLE (Open Access). 2020. Conrad Grebel Review.\nDrawing on Anabaptist history and core values and “Purity of Heart is to Will One Thing” by Søren Kierkegaard, this paper highlights the unique virtue and contribution of people with intellectual disabilities in our communities and encourages churches to learn from the witness of people it has tended to marginalize.\nChristian Wholeness with Dr. Keith Dow\nLISTEN TO THE PODCAST. 2020. The Healthy Spirituality Podcast with Mike Friesen.\nRenew Course (Audio): Caring for you as you care for others\nWATCH INTRO VIDEO (YouTube).\nJoin us as we explore what it looks like to practice resilient care in difficult times. A three-hour course broken into nine podcast sessions, unpacking practices of self-care to combat compassion fatigue in caregiving.\n- READ GUIDE| 2019 | Asset-Based Community Development at a Glance\nAt the Intersection of Theology and Disability\nLISTEN TO THE PODCAST 2018. A Podcast of Discipleship with Stephen J. Bedard.\nOverview of PhD work, “TED Talk”\nWATCH VIDEO (YouTube). 2019 Institute on Theology and Disability.\nEvery Church Can be Incarnational\nWATCH VIDEO (Vimeo). 2018. Inclusion Fusion Conference.\nCall, Encounter, and Response: Loving my neighbour with intellectual disabilities\nVIEW DISSERTATION 2019. PhD Dissertation, Vrije Universiteit, Amsterdam.\nTheological ethics of care dissertation under the supervision of Hans Reinders and Thomas E. Reynolds.\nSuspending the Ethical: Autonomy, Disability, and Shalom\nREAD ARTICLE (Open Access). 2013. Peace Studies Journal Vol 6: Issue 4.\nKeith Dow argues that “modern Western culture teleologically suspends the ethical in its death-making treatment of persons with disabilities”. He employs the term death-making coined by Wolf Wolfensberger and asks what response peacemaking has to give to a society blind to its own death-making practices.\nKierkegaard’s Ethic: The Other by Faith\nREAD THESIS (2009). Thesis completed toward the fulfillment of MA Phil, Dominican University College Ottawa.\nThis thesis examines the authorship of Søren Kierkegaard and the role of faith in establishing ethical responsibility to the human other. Focusing on the concepts of self, despair, sin, faith, neighbour love and preferential love, this thesis demonstrates that Kierkegaard’s authorship posits a love- and faith-based ethics missed by his critics, Martin Buber in particular.","Moltmann, Jürgen. The Trinity and the Kingdom: The Doctrine of God. 1st HarperCollins paperback ed. San Francisco, CA: HarperSanFrancisco, 1991.\nJürgen Moltmann (born 8 April 1926) is a German Reformed theologian who is Professor Emeritus of Systematic Theology at the University of Tübingen. Moltmann is a major figure in modern theology and was the recipient of the 2000 Grawemeyer Award in Religion, and was also selected to deliver the prestigious Gifford Lectures in 1984-1985. He has made significant contributions to a number of areas of Christian theology, including systematic theology, eschatology, ecclesiology, political theology, Christology, pneumatology, and the theology of creation.\nInfluenced heavily by Karl Barth‘s theology, Hegel‘s philosophy of history, and Ernst Bloch‘s philosophy of hope, Moltmann developed his own form of liberation theology predicated on the view that God suffers with humanity, while also promising humanity a better future through the hope of the Resurrection, which he has labelled a ‘theology of hope’. Much of Moltmann’s work has been to develop the implications of these ideas for various areas of theology. While much of Moltmann’s early work was critiqued by some as being non-Trinitarian, during the latter stages of his career Moltmann has become known for developing a form of Social Trinitarianism. His two most famous works are Theology of Hope and The Crucified God. Moltmann also served as a mentor to Miroslav Volf.\nMoltmann tries to bypass two major cul-de-sacs of monotheism that have dominated Western Theology. The first is that which understands God as supreme substance and the uncreated creator who established a stable and ordered universe of which the human is one substance among many. This originated in Greek Antiquity, was passed to the Middle Ages (Aquinas) and still forms the core of Roman Catholic theology. The second cul-de-sac is that which understands God to be the absolute subject. This flows from the special tradition of the Old Testament, was carried through medieval nominalism, and matured in nineteenth century idealist philosophy. In Western theology it followed the lineage of Augustine, Descartes, Kant, Schleiermacher, Bultmann, Fichte, and Hegel. The universe, and thus any knowledge, can only be known through the single subject of the human perciever. Therefore, God must be the absolute subject that is engaged in the process of self-revelation to all the other single subjects in the cosmos.\nMoltmann notes that both of these theological systems claim to be Trinitarian, but begin with the one-ness of God and move to the three-ness of God as only secondary in nature. On the one side, God is one substance (essence) and three persons. On the other, God is one subject expressed in three modes of being. In both cases God is ultimately monotheistic and the Trinity is deemed irrelevant.\nMoltmann suggests that we should begin with the three-ness of God as revealed in the narrative of scripture and then move back to the one-ness of God in the tri-unity of the three unique persons of the narrative—the Father, the Son, and the Holy Spirit. It is the fellowship and the relationality between the persons of the Trinity that is the essence of God and that opens up God to the world. Moltmann calls this the social Trinity.\nThis is important work for the missional church conversation. Moltmann’s social Trinity establishes a theological basis for the church as an open system that moves toward the world—the Other—with hope of reconciliation and a future vision of peace. The church is not sent from a single subject to another subject to coerce or convince that subject to come in. Rather, the church is constituted by the relationality of God to embrace the other and move toward a preferred and future promise of hope and restoration in the glory of God in the Kingdom of love.\nThe present book is an attempt to start with the special Christian tradition of the history of Jesus the Son, and from that to develop a historical doctrine of the Trinity. Here we shall presuppose the unity of God neither as homogenous substance nor as identical subject. Here we shall enquire about that unity in the light of this trinitarian history and shall therefore develop it too in trinitarian terms. The Western tradition began with God’s unity and then went on to ask about the triunity. We are beginning with the trinity of the Persons and shall then go on to ask about the unity. What then emerges is a concept of the divine unity as the union of the tri-unity, a concept which is differentiated and is therefore capable of being thought first of all.\nIn distinction to the trinity of substance and to the trinity of subject we shall be attempting to develop a social doctrine of the Trinity. We understand the scriptures as the testimony to the history of the Trinity’s relations of fellowship, which are open to men and women, and open to the world. This trinitarian hermeneutics leads us to think in terms of relationships and communities; it supersedes the subjective thinking which cannot work without the separation and isolation of its objects.\nHere, thinking in relationships and communities is developed out of the doctrine of the Trinity, and is brought to bear on the relations of men and women to God, to other people and to mankind as a whole, as well as on their fellowship with the whole of creation. By taking up panentheistic ideas from the Jewish and the Christian traditions, we shall try to think ecologically about God, man and the world in their relationships and indwellings. In this way it is not merely the Christian doctrine of the trinity that we are trying to work out anew; our aim is to develop and practise trinitarian thinking as well.\nWhich of these freedoms corresponds to God’s freedom? The triune God reveals himself as love in the fellowship of the Father, the Son and the Holy Spirit. His freedom therefore lies in the friendship which he offers men and women, and through which he makes them his friends. His freedom is his vulnerable love, his openness, the encountering kindness through which he suffers with the human beings he loves and becomes their advocate, thereby throwing open their future to them. God demonstrates his eternal freedom through his suffering and his sacrifice, through his self-giving and his patience. Through his freedom he keeps man, his image, and his world, creation, free—keeps them free and pays the price of their freedom. Through his freedom he waits for man’s love, for his compassion, for his own deliverance to his glory through man. Through his freedom he does not only speak as Lord, but listens to men and women as their Father.\nThe New Testament talks about God by proclaiming in narrative the relationships of the Father, the Son and the Spirit, which are relationships fellowship and are open to the world.\n“His kingdom is the kingdom of fatherly and motherly compassion, not the kingdom of dominating majesty and slavish subjection.”\nAt this stage in the history of the Son the Trinity means:\n⁃ The Father sends the Son through the Spirit.\n⁃ The Son comes from the Father in the power of the Spirit.\n⁃ The Spirit brings people into the fellowship of the Son with the Father.\nThe Father is crucifying love, the Son is crucified love, and the Holy Spirit is the unvanquishable power of the cross.”\nThe form of the Trinity which is revealed in the giving up of the Son appears as follows:\n⁃ The Father gives up his own Son to death in its most absolute sense, for us.\n⁃ The Son gives himself up, for us.\n⁃ The common sacrifice of the Father and the Son comes about through the Holy Spirit, who joins and unites the Son in his forsakenness with the Father.\nWe ought not to interpret Jesus’ resurrection in merely eschatological terms. In its innermost process it is Trinitarian too. This makes the express use of the Son’s name necessary in these contexts. Which form of the Trinity can be perceived at this stage in the history of the Son?\n⁃ The Father raises the Son through the Spirit;\n⁃ The Father reveals the Son through the Spirit;\n⁃ The Son is enthroned as Lord of God’s kingdom through the Spirit.\nWhereas until his resurrection we were able to perceive in the history of Jesus the sequence: Father—Spirit—Son, we now encounter the sequence Father—Son—Spirit. What does this mean?\nIt means that in the sending of the Spirit the Trinity is an open Trinity. Through the sending of the creative Spirit, the Trinitarian history of God becomes a history that is open to the world, open to men and women, and open to the future. Through the experience of the life-giving Spirit in faith, in baptism, and in the fellowship of believers, people are integrated into the history of the Trinity. Through the Spirit of Christ they not only become participators in the eschatological history of the new creation. Through the Spirit of the Son they also become at the same time participants in the Trinitarian history of God himself. That is the profounder reason why acknowledgment of the trinity was developed in the context of baptism first of all.\nWhat Trinitarian order can we perceive in the eschatological consummation?\nIn the sending, delivering up and resurrection of the Christ we find this sequence:\nIn the lordship of Christ and the sending of the Spirit the sequence is:\nBut when we are considering the eschatological consummation and glorification, the sequence has to be:\nWe shall start from the assumption that the relationship between God and the world has a reciprocal character, because\nthis relationship must be seen as a living one….\nJust as God goes out of himself through what he does, giving his world his own impress, so his world puts its impress on God too, through its reactions, its aberrations and its own inititatives. It certainly does not do so in the same way; but that it does so in its own way there can be no doubt at all. If God is love, then he does not merely emanate, flow out of himself; he also expects and needs love; his world is intended to be his home. He desires to dwell in it.\nA new divine presence is experienced in the experience of the Spirit. God does not simply confront his creation as creator. He is not merely, as the incarnate One, the representative and advocate for men and women. In the Spirit God dwells in man himself. The experience of the Spirit is therefore the experience of the Shekinah, the divine indwelling. The Shekinah is a divine presence which was otherwise only experienced in the Temple, in worship on the Lord’s day. But now men and women themselves, in their own bodies, already become the temple of the Holy Spirit (1Cor. 6.13-20). In the end, however, the new heaven and the new earth will become the ‘temple’ of God’s indwelling. The whole world will become God’s home. Through the indwelling of the Spirit, people and churches are already glorified in the body, now, in the present. But then the whole creation will be transfigured through the indwelling of God’s glory. Consequently the hope which is kindled by the experience of the indwelling Spirit gathers in the future, with panentheistic visions. Everything ends with God’s being ‘all in ll’ (1 Cor. 15.28 AV). God in the world and the world in God—that is what is meant by the glorifying of the world through the Spirit. That is the home of the Trinity. If the world is transformed and glorified into this through the Holy Spirit, then creation can only be conceived of in trinitarian terms, if it is to be understood in Christian terms at all.\n“Time is an interval in eternity, finitude is a space in infinity, and freedom is a concession of the eternal love. God withdraws himself in order to go out of himself. Eternity breathes itself in, so as to breathe out the Spirit of life.”\nThe Father creates the world out of his eternal love through the Son, for the purpose of finding a response to his love in time, in the power of the Holy Spirit, which binds together what is in itself different.\nIn creation all activity proceeds from the Father. But because the Son, as Logos, and the Spirit, as energy, are both involved—each in its own way and yet equally—creation must be ascribed to the unity of the triune God. In his creative love God is united with creation, which is his Other, giving it space, time and liberty in his own infinite life.\nIf follows from this that the Son of God did not become man simply because of the sin of men and women, but rather for the sake of perfecting creation…Christ is the ‘true man’ in this perverted and inhumane world. It is therefore in fellowship with him that believers discover the truth of human existence.\nFreedom in the light of hope is the creative passion for the possible. Unlike lordship, it is not merely directed towards what already exists. Nor, like love, is it only directed towards the fellowship of existing people. It is directed towards the future, in the light of the Christian hope for the future of the coming God. The future is the kingdom of not yet defined potentialities, whereas the past represents the limited kingdom of reality. Creative passion is always directed towards a project of a future of this kind. People want to realize new possibilities. That is why they reach forward with passion. In hope, reason becomes productive fantasy. People dream the messianic dream of the new, whole life that will at last be truly alive. They explore the future’s possibilities in order to realize this dream of life. This future dimension of freedom has long been overlooked, theologically too, because the freedom of the Christian faith was not understood as being participation in the creative Spirit of God.\nIn obeying God’s command a person feels himself to be the Lord’s servant. In faith in the gospel he sees himself as being the child of his heavenly Father. As God’s friend he talks to God in prayer, and his prayer becomes a conversation with his heavenly friend.\nFreedom itself is indivisible and all-comprehensive. That is why every partial freedom presses forward to total freedom and to the freedom of the whole creation. The thirst for freedom cannot be quenched by any partial satisfaction. It knows no limits. That is why even the freedom of God’s friends is not yet complete freedom. In history it is the best of all possible freedoms in our relationship to God. But even this points beyond itself to the freedom that only achieves its complete and perfect bliss in God in the kingdom of glory. When God is known face to face, the freedom of God’s servants, his children and his friends finally finds its fulfillment in God himself. Then freedom means the unhindered participation in the eternal life of the triune God himself, and in his inexhaustible fullness and glory. ‘Our hearts are restless until they find rest in thee’, said Augustine. And when we think of freedom we may surely say: ‘Our hearts are captive until they become free in the glory of the triune God.’\n Jürgen Moltmann, The Trinity and the Kingdom: The Doctrine of God, 1st HarperCollins paperback ed. (San Francisco, CA: HarperSanFrancisco, 1991), location 405.\n Ibid., loc. 899.\n Ibid., loc. 1000.\n Ibid., loc. 1097.\n Ibid., loc. 1171.\n Patriarch Philareth of Moscow in ibid., loc. 1281.\n Ibid.loc. 1350\n Ibid., loc. 1382.\n Ibid., loc. 1439.\n Ibid., loc. 1482-1499.\n Ibid., 1572.\n Ibid., loc. 1660.\n Ibid., loc. 1693.\n Ibid., loc. 1742.\n Ibid., loc. 3114.\n Ibid., loc. 3160.\n Ibid., loc. 3176."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:302519c3-dc02-4a59-9b49-dbf0ce85cd5c>","<urn:uuid:eabfdcb5-bdba-4624-a552-2e6754ee354d>"],"error":null}
{"question":"How are major transportation systems addressing environmental and efficiency challenges through technological innovation?","answer":"Transportation systems are implementing various technological solutions to address these challenges. Indian Railways is developing GPS-based tracking systems and integrated digital platforms like COA to optimize train movements and prevent delays between fast and slow trains. On the environmental front, major delivery companies in the US are adopting electric vehicles - Amazon has ordered 100,000 electric delivery trucks and UPS has ordered 10,000 electric trucks - as part of their commitment to carbon neutrality. These initiatives help comply with anti-idling laws in cities like New York and Philadelphia while improving operational efficiency.","context":["As passengers we may have experienced many digital platforms such as the e-ticketing and national train enquiry system (NTES) of the Indian Railways. However, the use of digital technologies at the Indian Railways is much broader and deeper than that. It has developed a highly functional platform by seamlessly integrating multiple applications together, evolving them with emergence of new technologies for managing the core and the passenger facing functions. It can match any world-class corporate application especially for a scale represented by the Indian Railways. We talked to senior officers in CRIS to obtain an idea of the operational and planned application landscape.\nAnkur: Let’s start with Control Office Application (COA). Would you please throw some light on what is the COA of the Indian Railways?\nCRIS: Running of trains is different from running of road vehicles. Trains take time to accelerate and need a long distance to stop. Locomotive Pilots (Train Drivers) therefore need continuous guidance from the 75+ Control Offices via trackside signals to send instructions on whether it should stop, slow down, or move at a particular signal. For this, the entire Railway route is divided into Block Sections of about 2 to 5 Km length each. 15 to 20 Block Sections make up a Line Section of 150 – 200 Km, which is controlled by a Section Controller on one Section Control Board. The signals in each Block Section are set by the Section Controller to ensure that at a time only one train can travel within one Block Section, so that there is no chance of two trains running too close to each other.\nPresently, the location of the trains is captured by telephonic communication with the Station Master of each station along the way. The Section Controller sitting in his Control Office watches the movement of each train in his section and plans its movement for the next 15 minutes to 2 hours to provide the most efficient train running. For this purpose, his main tools are the Control Chart and the movement forecast for the trains.\nAnkur: How does the COA help in controlling train movement?\nCRIS: Trains of various speeds move on the rails. There are high speed and intermediate speed passenger trains which need to stick to the schedule (time-table). There are non-scheduled freight trains. There may be a situation that a fast train is following a slow train and may get delayed because of the latter. Or in order to allow a fast moving train to move fast, the slow moving trains may be kept waiting for more time than necessary. Both these situations need to be avoided. Manual charts were prepared by the section officers to visualize and forecast the movement in the next 15 minutes to 120 minutes and make the optimum decisions. Now COA does this task automatically. The data on running of the trains is captured using the telephonic system and fed into COA and the application helps in forecasting the movement of trains. It enables the control officers in making efficient decisions.\nAnkur: What are other initiatives at IR and how do they benefit the stakeholders?\nCRIS: There are multiple stakeholders, especially when the COA is integrated with many other applications and platforms at the Indian Railways. Examples include Freight Operations Information System (FOIS), National Train Enquiry System (NTES), and Integrated Coaching Management System (ICMS).\nThe key stakeholders of the control office applications are all the people and organizations who need information on the movement of trains. It includes the section controllers, Chief Controllers, and Operations Managers who need to make decisions related to efficient train movement; passengers who need to know whether the train is running on time or is delayed and if delayed by how much time, the present position of the train etc.; Freight customers who need to know when they can expect their consignments to arrive, and other information systems of the railways such as FOIS, ICMS etc. The COA also is integrated with the NTES. Earlier the passengers had to go to the enquiry counters to know the movement status of a train and in some cases they had to wait on the platform for hours. Now the passengers can check the status on the NTES mobile app and plan accordingly.\nAnkur: There is massive use of GIS / GPS technology in the governance. Are there any plans to use GIS/GPS or related technologies for capturing real-time information on the movement of trains in the Indian Railways?\nCRIS: That’s a good question. We, in fact, are working to use GPS for capturing the real-time movement information of the trains and locomotives. This information will be communicated to the central application using a suitable communication system. This system will replace the current dependence on telephonic systems to capture the location of a train. There will be a big process automation with this. No manual data entry will be required. This will make the forecasting even more accurate.\nI must also add that our approach has always been to keep scope for emerging and new technologies to be leveraged by our architecture. Be it speech recognition, IoT, RFID, or Analytics, we continue to leverage new technological developments.","Electric vehicles (EVs) hold a lot of promise for the private sector — especially as consumers, who are increasingly aware of the relationship between emissions and climate change, are starting to demand eco-friendly delivery options. EV adoption, however, has been slowed down by a few different challenges — the US’s poor EV charging infrastructure in particular.\nNow, however, we’re beginning to see signs that major businesses are willing to buy into EVs, despite potential road bumps.\nHere are the businesses that are leading the way when it comes to EV adoption.\nAmazon and UPS Lead Way on EV Adoption\nTwo delivery giants — Amazon and UPS — have begun to aggressively add EVs to their delivery fleets.\nEarlier this year in January, Amazon ordered 100,000 electric delivery trucks from EV manufacturer Rivian, as well as 10,000 electric delivery rickshaws for their operations in India. Then, around the end of the month, UPS announced that it had ordered 10,000 electric trucks from the UK-based manufacturer Arrival Ltd., and would soon be teaming up with self-driving car manufacturer Waymo for a pilot test of self-driving delivery vehicles.\nThe moves are part of broader pushes towards carbon neutrality and self-driving delivery by the two companies. Last year, Amazon announced the company’s plan to be 100 percent carbon-neutral by the year 2040. UPS already offers carbon-neutral and carbon-offset delivery options.\nThe moves also come as more cities around the U.S., including New York and Philadelphia., have begun to adopt anti-idling laws that allow the city to fine companies over idling delivery vehicles.\nSome cities have even developed apps that allow citizens to report idling vehicles based on that vehicle’s DOT number — making these policies even more costly for delivery companies. Because electric vehicles produce no emissions, they’re typically free from being fined — meaning savings for businesses that adopt EVs for city deliveries.\nThe announcements are both historic. While other companies have announced EV purchases — like Lyft, which plans to deploy 200 EVs in Denver as part of its rental vehicle program there — there’s been nothing near scale of these announced by Amazon and UPS.\nWhile neither UPS nor Amazon has plans to go fully electric any time soon, the purchases are a welcome sign for the EV industry. Coupled with similar positive signals from the individual consumer side of the industry, they likely demonstrate that despite early growing pains, EVs may be on track for widespread adoption in the near future.\nChallenges Facing Further EV Adoption\nHowever, there still remain significant barriers that may slow or prevent full EV adoption, primarily the weak EV charging infrastructure in the US and limited number of charging stations — although this, too, seems like it’s starting to change.\nChargePoint, in coalition with the National Association of Truck Stop Operators (NATSO) has formed the National Highway Charging Collaborative, which plans to install new charging stations at more than 4,000 highway-side locations in the U.S., in order to increase the availability of EV charging stations in rural areas.\nAt the same time, legislative support for stronger EV infrastructure is beginning to build. In February, Democratic lawmakers in the House of Representatives announced a new bill that would create a nationwide EV charging network within the next five years.\nUpgrades to existing infrastructure would likely encourage further adoption. They may also be especially beneficial for businesses like Amazon and UPS, as both companies regularly make deliveries to rural parts of the country — areas that don’t always have the charging infrastructure needed to support EVs.\nThe Future for EVs in Business\nEV adoption in the private sector, which has lagged in the past, seems to be accelerating. Two major delivery companies have now announced that they will be adding significant numbers of EVs to their delivery fleets, with more likely to come in the near future as both pursue low-carbon delivery options.\nWhile challenges remain that may slow down EV adoption — primarily the nation’s weak EV charging infrastructure — the purchases are likely a good sign for the industry and the future of EVs in the private sector."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:c498d1b9-29cc-47f0-9bc1-1f96ba6258f2>","<urn:uuid:a3b2914f-bfcd-4107-a76d-db9ca5d880cd>"],"error":null}