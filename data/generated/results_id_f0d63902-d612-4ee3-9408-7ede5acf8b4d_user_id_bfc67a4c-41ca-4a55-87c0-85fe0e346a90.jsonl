{"question":"¿Cuál fue la diferencia principal between the Western and Eastern Front during World War I en términos de warfare style y distancias? Please compare both fronts considering their geographical characteristics.","answer":"The key difference was that while the Western Front developed into a fixed line of opposing trenches stretching 460 miles from the North Sea to Switzerland, the Eastern Front covered a much larger area of over 1,000 miles north-to-south and hundreds of miles east-to-west. On the Western Front, there were 25,000 miles of trenches that barely moved for over two years, creating a tactical stalemate. In contrast, on the Eastern Front a solid trench system never materialized because neither side had enough manpower to cover such vast distances in depth. This resulted in more of a war of maneuver on the Eastern Front, where attackers could penetrate 50-60 miles before being stopped, something unimaginable on the heavily trenched Western Front.","context":["The Beginning of the War for Britain\nBritain in 1914\nBritain at the start of World War One, was a much different place than today. Britain in 1914 ruled the largest empire in history, through industrial might, commercial prowess and maritime supremacy. Britain through its colonies, dominions, protectorates and mandates, controlled about a quarter of the World’s land surface and some 435 milion people, or 20% of the world’s population. Half of these people were Hindus. The legal, linguistic and cultural influence of Britain was worldwide and immense.\nBritain was the wealthiest country in the world and had the largest and most powerful navy. Britain built 50% of the world’s tall ships, and 40% of the all the world’s merchant fleet flew the British flag. Only Germany produced more than Britain economically. Unchallenged at sea, Britain adopted the role of global policeman. Alongside the formal control it exerted over its own colonies, Britain’s dominant position in world trade meant that it effectively controlled the economies of many countries, such as China, Argentina and Siam. British imperial strength was underpinned by new technologies, like the steamship and telegraph, which allowed it to control and defend the empire.\nBritain’s population was about 46 million and it was considered very overcrowded. From 1900, one in twenty British citizens emigrated to the colonies for a better life. In 1912, 300,000 people left Britain. The majority moved to the United States, Australia and Canada.\nLife in Britain could be very poor. About 80% of the British people were ‘working class’ and 90% rented their homes, rather than owned them. One percent of Britiain’s richest people owned 70% of the wealth. The average weekly wage was only £1.40. Life expectancy for a wealthy man was 55 years. Most people in poorer parts of Cities were lucky to live beyond 30 years old. Beer was 2p a pint.\nIt was only compulsory to attend school until the age of 12. Many children left school early to work and support their families. Only 6% of children remained at school over the age of 16. Five million women worked, mostly as maids, cooks and servants, but they did not have the vote and there were no female Members of Parliament. Only half of men could vote. The First World War would change Britain.\nEurope in 1914\nIf Britain was the worlds’ Superpower, Europe in 1914 was the most wealthy and powerful continent on earth. While past civilisations might have built great cities, invented gunpowder or algebra, nothing could compare to Europe’s material and technological culture. Europe was integrated more then, than it had ever been, intrinsically linked by flows of goods, money and people. Europe was an engine room of enterprise and innovation, densely interconnected and criss-crossed by railway lines and telegraph wires. Europe represented the summit of interdependence, with each country relying on its neighbours for resources, markets or access to the rest of the world. The GDP of Europe before the First World War was not equalled again until 1970. Empire was Europe’s supreme product. Imperialism expressed National prestige and superiority and was seen as a legitimate way of organising and improving the world. Little thought was given to the exploitation of indigenous people. Even Europe’s smaller nations like Denmark, Portugal, Belgium or the Netherlands, had empires in the Caribbean, south east Asia or central Africa. Only Austria and Hungary had no colonial empire. However, the slaughter and barbarism of the First World war ended Europe’s credibility as a civilising force. Europe had squandered its wealth fighting the war and many of its nations were in debt. The war and the peace that followed, would change and challenge Europe’s dominance. While London and Paris remained the capital cities of the world largest empires, the Versaille Treaty, signalled a new world order of Nation States running affairs, rather than Empires. The German Kaiser would flee to exile in Holland, his empire would become the German Republic. The German capital city would be moved from Berlin to Weimar. The Austrian- Hungarian empire, built up over centuries, would after four years of war, be carved into the new states of Czechoslovakia. Both Austria and Hungary would be reduced in territory. Austria which was almost completely German speaking, would be forbidden from uniting with Germany again. The once grand imperial capital of Vienna would become an oversized capital of a much smaller Austria. In eastern Europe’, Poland re-emerged as a new country. The Balkans became Yugoslavia. Russia wracked by revolution and civil war, became a Soviet Union of states. The Tsar and his family were shot. The Russian capital was moved from Petrograd to Moscow. At the end of the Great war, the Ottoman Empire was dismantled and a new country called Turkey was created. Arab territories fell under the political influence or direct control of Britain and France, with London replacing Constantinople as the ultimate master of Jerusalem. The Great War was the beginning of the end for Europe’s predominance in world affairs, and for its claim to civilisational superiority. The only countries to emerge from the Great war, stronger, richer and more influential, were Japan and the United States.\nThe British Expeditionary Force (BEF)\nUnder the 1839 Treaty of London, Britain was duty bound to protect Belgium. When Germany invaded Belgium on 4th August 1914, and refused a British ultimatum to withdraw, Britain mobilized the BEF. To a meticulously planned, pre war timetable, the BEF arrived in Belgium within 12 days to halt the German advance. This mobilisation of the BEF was phenomenal. One division alone, contained 19,000 men, 5,600 horse, 75 guns, and 650 wagons. It occupied in column, ten miles of road and required 90 trains to transport. Britan transferred 4 divisons in just 12 days, with the 2nd Corps operating near La Bassee, and 3rd Corps, based at St Omer, in just the first 3 days.\nContrary to popular belief, the BEF were not Britain’s best troops. The majority, were relatively recent enlistments, with a high proportion having less than two years service. The more experienced soldiers, were reservists who had left the army years ago, and their standard of fitness and miltary knowledge varied considerably. However, the quality of Britain’s military training was good and the majority of its Officers and NCO’s were experienced and battle hardened. The British Battalion system was also an excellent way of bonding troops into an effective fighting force.\nThe Four Divisions of the British Expeditionary Force (BEF), sent to France in 1914, numbered approximately 100,000 men. Although outnumbered by the Germans by at least ten times, the BEF halted the German advance from 23rd August 1914, at Mons, Le Cateau, and then along the River Marne.\nThese first three weeks of the war were a critical period, in which the German plans to end the war at a stroke were stopped. While the BEF was successful in holding the Germans at bay, much of the Army was destroyed in the fighting. Between 5th August and 30th November 1914, the BEF suffered 86,237 casualties. These represented a large core of the Professional British Army. To replace these numbers and commanders, Britain had to quickly recruit and train New Armies.\nThereafter, the ‘Western Front’ developed into a line of opposing trenches, stretching 460 miles, from the North Sea to Switzerland. There were 25,000 miles of trenches, enough to wrap around the entire planet – front line trenches, reserve and support trenches, connected by communication trenches. This ‘front line’ barely moved for more than two years. Both sides were evenly matched and although they produced deadly new weapons, like tanks, aircraft, and gas attacks, to break the deadlock, these were soon countered, to create a tactical stalemate The civilian population from France and Flanders was evacuated and the occupying forces settled into the long, grinding routine of trench warfare. The Germans were quick to seize the high ground and dug their trenches in deep, taking advantage of the better soil conditions. This gave them a great defensive advantage over the attacking allies.\nIn the first two weeks of October 1914, the BEF was moved from the central sector of the front to Ypres and Flanders. This move shortened its lines of communications which ran through Dunkirk, Calais and Boulonge. Britain was able to protect these ports which were vital to its own supplies and reinforcements, and to the Royal Navy’s command of the Channel. The Battle of Neuve Chappell between the 10-13th March 1915, was an indication of things to come. Britain’s Royal Flying Corps was used for the first time to take observation photos of the German defences. This innovation allowed the BEF artillery to accurate target the Germans and blow a six mile hole in the German front line. However, British telephone cables had been broken by the German shell fire and poor communications with the artillery meant that the BEF infantry could not advance until the barrage stopped. This delay alowed the Germans to regroup. When the BEF 1st Army, eventually attacked, the Germans were ready for them. Some 200 German troops from the Jager 11 Battalion, armed with two Maxim machine guns, firing 600 bullets a minute, held 9,000 British and Indian Troops, at bay for 90 minutes. They could not be moved, as the British artillery was running low on shells. A counterattack by the 16,000 Germans was similarly mowed down by machine guns, but pushed the British back to almost where they began. The battle killed 21,000 men, some of the BEF’s best troops (Ten Victoria Crosses were awarded), but achieved tactically nothing It did however expose the futility and costly stalemate of trench warfare,- massive artillery bombardments, followed by calamitous infantry assaults and costly counter attacks, with little gain. A pattern to be repeated across the entire front line, for next three years.\nOver the next four years the BEFs strength rose to 50 Divisions, and 12 overseas Commonwealth Divisions. This comprised troops from Canada, Australia, New Zealand, South Africa, India, Newfoundland and the British West Indies.\nThe British and Commonwealth forces fought a number of bloody battles in the defence of the Ypres salient. Other major battles, like the Somme, Messines, Cambrai, Arras and Passchendaele took place across the entire front line during the course of the war.\nIn March 1918, following the earlier capitulation of Russia in the East, Germany with over a million extra troops, began their great offensive on the Western Front aiming to finish the war. Their sweeping advances reclaimed all of the ground the Allies had won before. However, by August 1918, the Germans were a spent force. They had lost many of their best troops in the offensive. The pitted battlefields meant their supply lines were overstretched and slow. German troops became hungry, exhausted and disillusioned. While the German offensive had made large advances, their land gains were not strategic, and did not threaten the allied ports and supply lines. Stiff allied resistance, boosted by fresh American troops arriving on the Western Front, eventually halted the German attacks. The Allies then retook the offensive and through August and September 1918, counter attacked across the old battlefields and beyond.\nThe British Army during the First World War was the largest military force, that Britain had ever put into the field up to that point. Over the course of the war 5,399,563 men served with the BEF, the maximum strength being 2,046,901 men. On the Western Front, the BEF ended the war as the strongest fighting force, more experienced and bigger than the American Army and with better morale than the French Army.\nThe cost of victory was high. The official “final and corrected” casualty figures for the British Army, including the Territorial Force, were issued on 10 March 1921. The losses for the period between 4 August 1914, and 30 September 1919, included 573,507 “killed in action, died from wounds and died of other causes” and 254,176 missing (minus 154,308 released prisoners), for a net total of 673,375 dead and missing. Casualty figures also indicated that there were 1,643,469 wounded.\nPhotos: Courtesy of The Imperial War Museum Collection\n1. Belgian civilians cheering a force of British marines on their arrival at Ostend in August 1914.\n2. British troops resting in a Belgian village, 13 October 1914\n3. Soldiers of the 2nd Battalion, Scots Guards in the hastilly constructed trenches near Zandvoorde, October 1914.\n4. View of No Man’s Land towards the German trenches, which ran along the line of trees; La Boutillerie, winter 1914-1915. 7th Division.","I try to make history readable and interesting, warts and all. We must look to the past to understand the present and confront the future.\nNeglected By The West\nIn the English-speaking world, the Eastern Front during World War One is generally ignored in favor of the Western Front fought in France and Belgium. This is unfortunate since the Eastern Front in Eastern and Central Europe was every bit as horrendous as the war in the West. The Western Front cannot be fully understood without appreciating the effect the war in the East had on it.\nWhat Was Different About The Eastern Front?\nThe fighting on the Eastern Front was mainly between the Central Powers (the German and Austro-Hungarian Empires) and the Russian Empire. Later, Bulgaria and the Ottoman Empire joined the Central Powers and Romania joined Russia. There were several factors which changed the nature of fighting on the Eastern Front when compared to the Western Front:\nThe Eastern Front covered a far larger area, stretching at times for over 1,000 miles, basically north-to-south and hundreds of miles east-to-west. A solid trench system similar to the Western Front never materialized because neither side had the manpower to cover such a distance in depth. This resulted in more of a war of maneuver, whereby attackers might penetrate 50 or 60 miles before being stopped.\nThe Russian Empire\nRussia's infrastructure was poor. Although Russia initially fielded a huge and well-trained army, her factories could not keep up with demand, and, even when they finally geared up around 1916, there weren't enough roads and railroads to keep the army supplied most of the time.\nThe Austro-Hungarian Empire\nThe Empire of Austria-Hungary was in decline. Many of her soldiers came from provinces and states that yearned for freedom and thus had little loyalty to the empire. This, combined with poor leadership, resulted in low morale.\nThe German Empire\nThe German Army was trained to fight a war of maneuver, and had strong leaders and a good infrastructure for supply. This enabled them to succeed even when outnumbered.\nOn August 17, 1914, Russia launched its full-scale offensive against Germany by entering East Prussia in the northern part of the front. The Russians were decisively beaten at the Battle of Tannenberg and withdrew.\nFurther south, Russia had much more success against Austria-Hungary, driving the Austrians back across the Carpathian Mountains and occupying the Austro-Hungarian province of Galacia.\nAt the beginning of 1915, the Austrians were unable to do much against the Russians in Galacia. So Germany took over command of the entire Eastern Front and shifted troops to bolster their southern neighbors. In two weeks, the German and Austrian troops launched a major offensive in May and drove the Russians back more than 200 miles from the Carpathian Mountains—an unimaginable feat on the Western Front. The Russians had to make a strategic withdrawal, partly due to the deficiencies of supplies and ammunition, before they managed to make a stand, now back in their own territory. The Central Powers had captured Russian Poland, Lithuania and most of Latvia, and parts of Russian Ukraine.\nBy 1916, things improved for the Russians, who were then better supplied. While Germany was occupied in the west by their massive offensive against the French at Verdun and then fighting for her life against the British Somme offensive, Russia attacked the Austro-Hungarians and, once again, drove into Galacia. In addition, to the south of the Eastern Front, Romania entered the war on the side of the Allies, extending the Eastern Front hundreds of miles south. Instead of first setting up adequate defenses, Romania immediately attacked the west, dreaming of regaining the Transylvanian region of Austria-Hungary. Germany, Austria-Hungary, Bulgaria and the Ottoman Empire counterattacked against Romania, which collapsed, and the Central Powers gained control of her vast coal and wheat fields.\nRead More From Owlcation\nLate 1916 also saw mutinies and revolts in several countries as soldiers became disillusioned with the war, the way it was conducted and the unimaginable loss of life. Russia, especially, edged closer to revolution.\n1917 was the year of the Russian collapse. Her armies mutinied, the Tsar abdicated, and a provisional government tried to hold things together. A final Russian offensive was tried, but the soldiers wouldn't stand for it, and open civil war swept Russia as the Germans continued to advance. In November, the Communist Bolsheviks took control and began negotiations with the Germans and fighting stopped in December.\nOn March 3, 1918, the Treaty of Brest-Litovsk was concluded, officially ending the war on the Eastern Front. As far as concessions to Germany, its terms didn't survive the year, but it did affirm the independence of Finland, Lithuania, Latvia, Estonia and Ukraine. Poland was not included, which caused riots and animosity of Poles toward the Central Powers. This freed up substantial German soldiers to transfer to the Western Front to support the massive German Spring Offensive but still tied up a million Germans till the end of the war. The Spring Offensive made spectacular gains in France, but the arrival of American soldiers eventually offset any German advantage in numbers.\n- The Russians lost from 1.8 million to 2.3 million soldiers killed and from 3.8 to 5.0 million wounded. About 500,000 civilians died in the fighting.\n- Romania lost about 250,000 soldiers killed and 120,000 wounded, with 120,000 civilians killed in the fighting.\nCentral Powers Casualties\nCasualty figures for the Central Powers are not broken down by which front they occurred in, so these are total casualties:\n- Austria-Hungary lost 1.1 million soldiers killed and 3.6 million wounded. About 120,000 civilians died in the fighting.\n- Bulgaria lost about 87,000 soldiers killed and 150,000 wounded.\n- Germany lost about 2.1 million soldiers killed and 4.2 million wounded. Only about 1,000 civilians died in the fighting.\n- The Ottoman Empire lost about 770,000 soldiers killed and 400,000 wounded.\n© 2012 David Hunt\nsherman tank on May 12, 2018:\nI love ww1, stupos\nJohnwin A. Ferasan on February 08, 2017:\nThank you for this information about eastern front because I'm using for my assignment in History. I learned a lot things that I do not know about WW1. This so useful and interesting to me. This is so awesome. Thank you very much!!!\nfootball01 on January 24, 2017:\nThis is a great site, I am using it for a project and I have learned a lot of info ya'll.\nDavid Hunt (author) from Cedar Rapids, Iowa on June 02, 2012:\nAnd thanks for the awesome comment, DS. I'm glad you enyoyed it and got something out of it. Sometimes Hub \"lengths\" restrict us from writing near-book-length articles and I think that's generally a good thing. It forces me to keep focus without going overboard (I hope). Thanks again for all the vote ups.\nDS Duby from United States, Illinois on June 02, 2012:\nI loved this article, I have always had a vast interest in world history, but sadly I don't know nearly enough about our more recent centuries focusing a little to much on ancient history and mythology. This hub brought me much interest and enthusiasm in learning more about the twentieth century, Thank you. Voted up awesome, interesting and useful.\nDavid Hunt (author) from Cedar Rapids, Iowa on May 27, 2012:\nThanks for reading and commenting, JKenny and Judi Bee. I felt the same way myself, having no \"personal\" involvement with the Eastern Front (my two grandfathers served in France) and being culturally more attuned to the Western Front. I think part of it is all the \"foreign\" names and cultures we have to learn just to be able to follow the narrative. I imagine that's why many don't follow World War One anyway-- \"Where's Ypers? How do you pronounce it? This is too hard to follow\".\nJudi Brown from UK on May 27, 2012:\nGreat overview Harald - I know very little about the war on the Eastern Front. For my part my ignorance is largely due to there being no \"personal\" involvement, my family all being mainly on the Western front.\nVoted up etc.\nJames Kenny from Birmingham, England on May 27, 2012:\nA very interesting article Harald, and a very useful one too. You're right, about it being neglected; we often hear about Verdun, the Somme and Ypres. But we hardly know anything about the Eastern Front, so thank you for writing this. Voted up and shared."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:899e2d84-9a2c-4fc5-97a6-b0aced6f8eb8>","<urn:uuid:d35ad503-9e06-4dc0-a32f-7c9bcc0974e5>"],"error":null}
{"question":"What were the political circumstances and legislative processes that led to the official adoption of the spiritual in South Carolina versus the Missouri Waltz as state songs?","answer":"The spiritual became South Carolina's state music in 1999 through a bipartisan effort, with Governor Jim Hodges signing a bill initiated with support from both Democratic Senator Darrell Jackson and Republican Representative Chip Campsen. The bill passed unanimously after legislators heard recordings of spirituals performed by Burke High School Blue Notes. In contrast, the Missouri Waltz became the state song in 1949 following a suggestion by Representative Floyd Snyder after the song gained popularity due to President Harry Truman playing it at the White House. However, the Missouri Waltz required amendment of some lyrics considered racist before its official adoption. Notably, President Truman himself was ambivalent about the song, stating he 'could take it or leave it' and even criticizing its musical quality.","context":["Do you remember the first music you heard as a child? Growing up on my family’s farm in the Pee Dee, I recall “Swing Low Sweet Chariot,” sung by my nanny, Doretha. The sweet sound of her voice became a part of me forever.\n“Swing Low” is a spiritual, and, to honor Black History Month in February, I wanted to pay tribute to my state’s heritage by preserving this music and making it the official South Carolina state music – so I set out to accomplish that goal.\nWhile I was in graduate school, I wrote a major research paper on African-American religious music in the Antebellum South that revealed the strong link between the Palmetto State and spirituals. Mrs. Lula Mitchell Holmes of St. Helena Island not only helped me get a good grade on my paper but also inspired me to do something to preserve this music, culture and heritage. A descendant of slaves, she performed the “Gullah shout,” which begins with a spiritual.\nWhy make the spiritual our state music? There are many reasons, but the most significant I found is that many spirituals originated in South Carolina.\nIn 1999, Gov. Jim Hodges signed a bill making the spiritual the official state music. I had the privilege of initiating the bill, with the help of Sen. Darrell Jackson, a black Democrat from Columbia, and Rep. Chip Campsen, a white Republican from Charleston County. At the time, one historian was quoted as saying, “The spirituals are as defining a music of South Carolina as the blues are of Mississippi.”\nSpirituals were passed down orally for many years. My research revealed they were first committed to writing in South Carolina on St. Helena Island. Many songs are indigenous to the South Carolina coast, but slave narratives indicate they were sung in several areas of the state. Clearly, South Carolina, more than any other state, can call the spiritual its own.\nThe late Booker T. Washington best described spirituals as “the spontaneous outbursts of intense religious fervor … having their origin chiefly in the camp meetings, the revivals and in other religious gatherings.”\nThe day before I was to testify before a Senate subcommittee chaired by Sen. Jackson, I went to Burke High School and asked Vivian E. Jones, then director of the Burke High School Blue Notes, to record a few spirituals to play for the senators. I listened to their beautiful voices as they sang three songs for me and ultimately for the legislators, who listened to them intently as I played them on my boom box.\nAfter hearing the songs, the legislators voted unanimously, right in front of me, to pass the bill. I am grateful to them for their support, but, ultimately, I thank God for my upbringing, which spawned the idea to make the spiritual our official state music.\nThe bill is a tribute to the African-American citizens of our state and their contributions to our culture and heritage, and it makes sure this music will never be forgotten or lost. I envision this bill enhancing and promoting African-American history in our colleges and museums, and I hope it will further benefit education through music scholarships. Hopefully, the International African-American Museum planned for Charleston will incorporate the spirituals throughout.\nBy Matthew Butler Chandler","Missouri History:: Missouri State Song\nWhat is Missouri's State Song?\nThe \"The Missouri Waltz\" was adopted as the official state song on June 30, 1949.\nHush-a-bye, ma baby, slumbertime is comin' soon;\nRest yo' head upon my breast while Mommy hums a tune;\nThe sandman is callin' where shadows are fallin',\nWhile the soft breezes sigh as in days long gone by.\nWay down in Missouri where I heard this melody,\nWhen I was a little child upon my Mommy's knee;\nThe old folks were hummin'; their banjos were strummin';\nSo sweet and low.\nStrum, strum, strum, strum, strum,\nSeems I hear those banjos playin' once again,\nHum, hum, hum, hum, hum,\nThat same old plaintive strain.\nHear that mournful melody,\nIt just haunts you the whole day long,\nAnd you wander in dreams back to Dixie, it seems,\nWhen you hear that old time song.\nHush-a-bye ma baby, go to sleep on Mommy's knee,\nJourney back to Dixieland in dreams again with me;\nIt seems like your Mommy is there once again,\nAnd the old folks were strummin' that same old refrain.\nWay down in Missouri where I learned this lullaby,\nWhen the stars were blinkin' and the moon was climbin' high,\nSeems I hear voices low, as in days long ago,\nOrigin of Song:\nIt is difficult to determine the origin of the \"Missouri Waltz.\" Historians generally agree that the tune was first printed around 1912 by Frederick Knight Logan of Oskaloosa, Iowa. About 1000 copies were published in Chicago and distributed to various music dealers and orchestra leaders. The lyrics did not appear with the tune until later.\nMost versions of the song's origins agree that Logan picked up the song from orchestra leader John Valentine Eppel of Fort Dodge, Iowa. According to one version, Eppel learned the melody from an African American man in Missouri who had been taught the tune by his mother. Around Moberly, Missouri, residents say that the original composer was Dab Hannah, an African American piano player, but in Oskaloosa, some say that Henry Clay Cooper, an African American dance instructor, gave the melody to Logan. Another version claims that gifted piano player Edgar Lee Settle of New Franklin, Missouri, obtained the tune from the DiArmo sisters, a musical team on his theatrical circuit, who in turn, had been given it by an old African American man from the South. Settle's brother claimed that Settle composed the piece, which he called the \"Graveyard Waltz,\" and was playing it one evening when John Valentine Eppel heard it and used it with his orchestra.\nIn 1914, the Forster Publishing Company of Chicago secured the rights to the melody from Logan and, with lyrics composed by Jim Shannon, it appeared in 1915 as the \"Hush-a-Bye Ma Baby\" song with \"Missouri Waltz\" printed as a substitute in parentheses.\nBecoming the State Song:\nThe \"Missouri Waltz\" gained in popularity after Harry Truman became President of the United States in 1945. He played the song on the piano at the White House and, in so doing, enhanced its popularity. There were unsuccessful attempts to get the president to record the song.\nIn 1949, the year after Truman's unexpected victory over Thomas Dewey, Representative Floyd Snyder (D-Independence), suggested that the \"Missouri Waltz\" be given official status as state song. Due to some of the lyrics, which were considered racist, the song was amended and became the official state song on June 30, 1949.\nWhen the song legislation was being considered, reporters contacted the White House, asking whether the song was really his favorite. The following reply was published by the White House: \"President's attitude towards the song? He can take it or leave it. Is it really his favorite? No. Does he play it often? No. Is Margaret ever heard singing it? No. What is the President's reaction to song's adoption by Missouri as state song? See answer to first question.\"\nAlthough the song is often associated with Harry Truman, the president did not claim it as his favorite song. In fact, he had this to say about it in a television interview: \"If you let me say what I think, I don't give a ... about it, but I can't say it out loud because it's the song of Missouri. It's as bad as 'The Star Spangled Banner' as far as music is concerned.\"\nSheet music for the state song\nColeman-Topi, Debbie. \"Giving credit for Missouri's song.\" Kansas City Star, December 21, 1986, p. 15B.\nWatson, Bob. \"The Missouri Waltz.\" The Statesman, March 1997, pp. 14-15.\nWolfe, James F. \"Amended waltz became state song.\" The Joplin Globe, January 3, 1993."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:da14f02e-7b0e-412f-9390-b4e87e98d75c>","<urn:uuid:239ff025-9026-49f8-9b77-cbf4057d0a84>"],"error":null}
{"question":"What percentage of rainwater becomes runoff in urban vs. natural areas, and how can buildings mitigate this?","answer":"In areas with natural ground cover, only 10% of rainwater becomes runoff, while the rest is absorbed or evaporates. In contrast, urban areas can have up to 55% of rainfall become stormwater runoff. Buildings can help mitigate this through features like those implemented at Sidwell Friends School, including green roofs that retain rainwater, constructed wetlands for water recycling, and solar chimneys for passive ventilation. The building also uses a reclaimed cedar 'skin' that selectively admits and rejects environmental elements based on inhabitants' needs, with an innermost layer that sheds most rainwater while remaining open to air movement.","context":["How can we transform an outdated facility into an interior and exterior teaching landscape that educates students about the environment?\nThe Master Plan for the Sidwell Friends School, which we completed in 2001, outlines a phased transformation of the disparate collection of buildings on its Washington, DC, and Bethesda, Maryland, campuses into a physical demonstration of the school's values, founded in Quaker principle.\nThe Stewart Middle School, one of the buildings in the Master Plan, was an awkwardly-sited, undersized, 1950s facility that needed updating and expansion. All water falling on the site was shuttled into poorly repaired underground piping and directed into the municipal storm sewer system. We saw an opportunity to transform the facility into a true teaching landscape—not only inside its classrooms, but also on site—with rooftop gardens and a constructed wetland in the school's quadrangle as a metaphor for the school's commitment to sustainability.\nWe retained rainwater on a green roof, directing overflow down into a spillway, where it could make its way into a pond, rain garden, and cistern. Sewage is recycled in the wetland for grey water use in the building. A central energy plant was designed to serve the entire campus, allowing greater control of energy resources and providing a demonstration of responsible energy use to students.\nThe reclaimed cedar “skin” on the building was selected for its ability, like biological skin, to selectively admit and reject the environment based upon the needs of the inhabitants. The innermost layer sheds most rain water but remains open to the movement of air. The outermost layers are organized to shield sunlight based upon orientation of the sun. Very high-grade western red cedar from salvaged fermentation barrels was used to make the solar shading. To expedite construction time, the cladding was fabricated off site as integrated assemblies.\nToday, teachers have integrated the building into the curriculum, educating students about materials and systems, from the origin of cork flooring and bamboo casework to the impact of switching a light on or off. A building dashboard provides real-time computation of the building's resource use, raising awareness of each person's role in reducing carbon footprint. Environmental science students measure and compare nitrogen and phosphorus levels of the wetland, and they learn the valuable role that wetlands play in purifying water. The vegetated roof of the addition has been developed as an outdoor classroom for container gardening, and a photovoltaic array has been installed to generate a portion of the building's electricity.\nSolar chimneys—designed for passive ventilation, operating without additional energy—provide another learning opportunity about the sun as a primary, renewable source of energy. Sunlight heats air within the glass chimney tops, creating a convection current that draws cooler air into the building through north-facing open windows. Portals in the shaft ways allow students to witness the operation of the ventilation system. Wind chimes ring to indicate the movement of air.\nThe new school has sparked communal social action and inspired changes in school operations, including a new emphasis on local, organic foods in the cafeteria. It has engaged students to make the connection between the building's systems and the world outside the building—wholly consistent with Quaker values of environmental stewardship.","No matter the size of the city, problems arise from uncontrolled storm water runoff. Development and inadequate drainage systems increasingly compound problems associated with moderate to significant rainfall. Storm water runoff from these rainfall events accumulate in many areas of our city, causing nuisance flooding and possible threats to public health and safety.\nIn addition to drainage issues, the rain that falls on our streets, houses, driveways, parking lots, buildings and other impervious surfaces run off and carry pollutants such as oil, gasoline, pet waste and heavy metals. Storm water runoff from lawns and other green spaces can carry pesticides, herbicides and fertilizers. Over time, these pollutants accumulate in the waterways causing significant damage to our creeks, streams and lakes.\nThe EPA now considers pollution from all different sources, including urban storm water pollution, to be the most important source of contamination in the nation’s waters. Uncontrolled urban runoff also contributes to hydrologic and habitat modification, two important sources of river impairment identified by the EPA.\nNational Pollutant Discharge Elimination Systems Phase II\nOn October 29, 1999, the EPA issued new storm water regulations that require communities with populations under 100,000 to control water pollution caused by storm water runoff. Known as the EPA Phase II storm water rule, these regulations require cities such as McMinnville to implement a storm water management program that will reduce pollution associated with storm water runoff to the maximum extent practicable.\nThe City of McMinnville is required to meet and enforce the regulations of the NPDES Phase II permit program as authorized by the Clean Water Act. The NPDES permit program controls waste pollution by regulating point sources that discharge pollutants into waters of the United States. Water pollution degrades surface waters making them unsafe for drinking, fishing, swimming, and other activities.\nUnder the NPDES Phase II program, the Storm Water Management Department is responsible for overseeing the proper maintenance, construction and inspection of storm drainage systems within the city limits. Land development directly affects watershed functions. When development occurs in previously undeveloped areas, the resulting alterations to the land can dramatically change how water is transported and stored. Residential and commercial development create impervious surfaces and compact soils that filter less water, which increases surface runoff and decreases groundwater infiltration. These changes can increase the volume and velocity of runoff, the frequency and severity of flooding and peak storm flow.\nFREQUENTLY ASKED QUESTIONS\nWhat is stormwater runoff?\nStormwater runoff is the water that flows off roofs, driveways, parking lots, streets, and other hard surfaces during rainstorms. Rather than being absorbed into the ground, it pours into ditches, culverts, catch basins, and storm sewers. It does not receive any treatment before eventually entering the community’s streams and lakes.\nWhat problems does it cause?\nStormwater can carry harmful nonpoint source pollutants, cause flooding, erode topsoil and stream banks, and destroy marine life habitats. In an area with natural ground cover, only 10% of rainwater becomes runoff. The rest is absorbed or evaporates. In urban areas, up to 55% of rainfall can become stormwater runoff.\nWhy are the stormwater and sewer systems separate?\nUnlike wastewater, which is treated before it is released back into the environment, stormwater goes directly into a community’s ponds, streams, and lakes. Because stormwater comes in large amounts at unpredictable times, treating it as wastewater would be very expensive.\nWhat is nonpoint source pollution?\nNonpoint source pollution is water pollution that is difficult to trace to a specific discharge point. Because it comes from many diverse sources, it is hard to control. Examples of common nonpoint source pollutants include fertilizers, pesticides, sediments, oils, salts, trace metals, and litter. They come from farms, yards, roofs, construction sites, automobiles, and streets.\nWhat is impervious surface area?\nAny surface that does not readily absorb water and impedes the natural infiltration of water into the soil. Common examples include roofs, driveways, parking areas, sidewalks, patios, decks, tennis courts, concrete or asphalt streets, crushed stone, and gravel surfaces.\nHow does the city determine impervious surface area?\nFor single-family homes, a statistical sampling is taken using Geographical Information System (GIS) including aerial survey data. Each is measured and an average impervious surface area is determined. For businesses and other institutions, the city measures the impervious area using aerial survey data.\nWhat can I do to reduce pollution in stormwater runoff?\nCreating natural areas on your property can help reduce the quantity of stormwater runoff. Disposing of wastes properly, using the minimum amount of chemicals on your yard, and keeping your car well-maintained can reduce the amount of pollution that you add to stormwater runoff.\nADDITIONAL THINGS WE CAN DO TO HELP\n- Report any non-emergency type pollution problem to your local government offices. For emergency pollution problems, such as major spills, call 911.\n- Never dump anything down a storm drain inlet. They flow directly to our lakes and streams.\n- Keep your leaves and grass clippings out of the streets so that they do not end up washing into the storm drain inlets.\n- Wash your vehicles on your lawn or at a car wash facility instead of in your driveway.\n- Keep your automobiles and your gas-powered lawnmowers or blowers well-tuned so that they are not dripping toxic fluids or emitting toxic fumes.\n- Do not use chemicals on your lawn before it is expected to rain, and try using organic or slow-release products, which are better for your lawn and for the environment.\n- Be conservative with pesticides and herbicides (weed killers) and try natural alternatives. Call your local Extension Service to find out more about natural pesticides.\n- Make sure your air conditioners are in good working order and not leaking harmful chemicals.\n- Install early closing toilet flappers and water-conserving showerheads.\n- Lawns need less than an inch of water per week. If it rains an inch, do not water. Try using a rain gauge.\n*** TO REPORT ILLICIT DISCHARGES INTO STORM SEWER SYSTEMS ***\nGet started with our Citizenserve Public Portal now.\nCONSTRUCTION STORMWATER DOCUMENTS\n· City of McMinnville Storm Water Ordinance\n· Land Disturbance Application\nVISUAL STREAM ASSESSMENTS\n· Oakland Branch VSA\nBENTHIC MACROINVERTEBRATE SAMPLING\n· Coming in March 2020\n· Rain Garden Information\n· Storm Water & Erosion Control Training & Manuals\n· Water Footprint Calculator This tool will estimate your total usage at home and virtually (e.g. in your\nfood, the miles you drive in your car, etc.)\nONLINE PAMPHLETS & INFORMATION\n· Landscaping Best Management Practices\n· Food Service Best Management Practices\n· Automotive Best Management Practices\n· Heavy Equipment Best Management Practices\n· Detention Pond Best Management Practices\n· Home Construction & Remodeling Best Management Practices\n· Storm Drains Are For Rain\nCity of McMinnville\nCommunity Development Department\n101 East Main St./P.O. Box 7088\nMcMinnville, TN 37111"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:eb945972-2264-4141-90b2-c8dc45153176>","<urn:uuid:cfc3abdc-fb15-466c-b71d-97ebb1f1ca81>"],"error":null}
{"question":"What's the difference between prescription HCG and those homeopathic HCG products I see online for dieting?","answer":"Prescription HCG is FDA-approved only for fertility treatment and certain hormone treatments, and is available only in injection form with a prescription from a medical professional. In contrast, homeopathic HCG products marketed for weight loss are illegal and unapproved by the FDA. These products come in forms like oral drops, pellets, and sprays, and can be found online and in retail stores. The FDA has issued warning letters to companies selling these homeopathic HCG products because they are unsafe, make unsupported weight loss claims, and cannot be legally sold as homeopathic medications since HCG is not listed in the Homeopathic Pharmacopoeia of the United States.","context":["HCG Diet Products Are Illegal - US Food & Drug Administration\nWednesday, March 7th 2012\nIf you are considering using HCG, read this first.\nAnyone who has ever been on a diet—and there are many of us—knows that there are sensible ways to lose weight. These include balanced diets, exercising and realistic goals.\nAnd then there are reckless ways to shed pounds—fads and diet aids that promise rapid weight loss, but often recommend potentially dangerous practices. These include HCG weight-loss products marketed over-the-counter (OTC) that are identified as \"homeopathic\" and direct users to follow a severely restrictive diet.\nThe Food and Drug Administration (FDA) is advising consumers to steer clear of these \"homeopathic\" human chorionic gonadotropin (HCG) weight-loss products. They are sold in the form of oral drops, pellets and sprays and can be found online and in some retail stores.\nFDA and the Federal Trade Commission (FTC) have issued seven letters to companies warning them that they are selling illegal homeopathic HCG weight-loss drugs that have not been approved by FDA, and that make unsupported claims.\n(For the list of manufacturers, distributors and products—and more information about FDA’s concerns about HCG—visit www.fda.gov/hcgdiet.)\nHCG Makes Big Claims\nHCG is a hormone that is produced by the human placenta during pregnancy.\nProducts that claim to contain HCG are typically marketed in connection with a very low calorie diet, usually one that limits calories to 500 per day. Many of these popular HCG products claim to “reset your metabolism,” change “abnormal eating patterns,” and shave 20-30 pounds in 30-40 days.\n“These products are marketed with incredible claims and people think that if they're losing weight, HCG must be working,” says Elizabeth Miller, acting director of FDA’s Division of Non-Prescription Drugs and Health Fraud. “But the data simply does not support this; any loss is from severe calorie restriction. Not from the HCG.”\nHCG is approved by FDA as a prescription drug for the treatment of female infertility, and other medical conditions. It is not approved for weight loss. In fact, the prescription drug label notes there “is no substantial evidence that it increases weight loss beyond that resulting from caloric restriction, that it causes a more attractive or ‘normal’ distribution of fat, or that it decreases the hunger and discomfort associated with calorie-restricted diets.”\nHCG is not approved for OTC sale for any purpose.\nA Potentially Dangerous Diet\nLiving on 500 calories a day is not only unhealthy—it’s hazardous, according to FDA experts. Consumers on such restrictive diets are at increased risk for side effects that include gallstone formation, an imbalance of the electrolytes that keep the body’s muscles and nerves functioning properly, and an irregular heartbeat.\nShirley Blakely, a nutritionist at FDA’s Center for Food Safety and Applied Nutrition, echoes concerns about such restrictive diets. They can be dangerous, she says, and potentially fatal.\nVery low calorie diets are sometimes prescribed by health care professionals for people who are moderately to extremely obese as part of medical treatment to lessen health conditions caused by obesity, like high blood pressure. But even then, strict—and constant—medical supervision is needed to ensure that side effects are not life threatening, says Blakely.\nWithout medical oversight, consumers on very low calorie diets may not be getting enough vitamins, minerals and—most critically—protein.\n“In general, the reference (average) calorie level is 2,000,” says Blakely. “If you want to lose weight, reduce your daily intake by 500 calories. Over the course of a week, that equals 3500 calories, which is the loss of a pound. Gradual weight loss is the way to do it.”\nStory Started Decades Ago\nMiller explains that HCG was first promoted for weight loss in the 1950s. “It faded in the 1970s, especially when it became apparent that there was a lack of evidence to support the use of HCG for weight loss,” she says.\nThe diet has become popular again and FDA and FTC are taking action on illegal HCG products. “You cannot sell products claiming to contain HCG as an OTC drug product. It’s illegal,” says Brad Pace, team leader and regulatory counsel at FDA’s Health Fraud and Consumer Outreach Branch. “If these companies don’t heed our warnings, they could face enforcement actions, legal penalties or criminal prosecution.”\nElisabeth Walther, a pharmacist at FDA, explains that the agency does not evaluate homeopathic drug products for safety or effectiveness, and is not aware of any scientific evidence that supports homeopathy as effective. However, those that meet certain conditions set by FDA can be marketed. A reference document called the Homeopathic Pharmacopoeia of the United States lists active ingredients that may be legally included in homeopathic drug products.\n“HCG is not on this list and therefore cannot be legally sold as a homeopathic medication for any purpose,” Walther says.\nFDA advises consumers who have purchased homeopathic HCG for weight loss to stop using it, throw it out, and stop following the dieting instructions. Harmful effects should be reported online to FDA’s MedWatch program or by phone at 800-FDA-1088 (800-332-1088) and to the consumer’s health care professional.\nThis article appears on FDA's Consumer Updates page, which features the latest on all FDA-regulated products.\nConsumer Updates - FDA. Dec. 6, 2011\nQuestions and Answers on HCG Products for Weight Loss\n1. What action are the Food and Drug Administration (FDA) and the Federal Trade Commission (FTC) taking against human chorionic gondatropin (HCG) drug products marketed for weight loss?\nFDA and FTC are issuing seven joint warning letters to firms marketing over-the-counter (OTC) HCG drug products that are labeled as homeopathic for weight loss. The firms are receiving these letters because they are violating the Federal Food, Drug, and Cosmetic Act and the Federal Trade Commission Act by selling unapproved new drugs and misbranded drugs that make unsubstantiated claims about weight loss.\nThese unapproved “homeopathic” HCG drug products are marketed OTC on websites and in retail stores, and can be in the form of oral drops, pellets, and sprays. FDA has not evaluated these products for safety or effectiveness.\nFDA and FTC will monitor the firms’ responses to the warning letters and take further action as needed. Firms that do not correct the violations may face enforcement action, possible legal penalties, or criminal prosecution.\n2. What is HCG?\nHuman chorionic gonadotropin (HCG) is a hormone produced by the human placenta and found in the urine of pregnant women. HCG is FDA-approved for the treatment of select cases of female infertility and hormone treatment in men. FDA-approved HCG products are only available in injection-form and require a prescription from a licensed medical professional.\nThere are no FDA-approved HCG products for weight loss.\n3. Why is FDA concerned about the use of homeopathic HCG drug products marketed for weight loss?\nCurrently, there are no FDA-approved HCG drug products for weight loss. HCG has not been demonstrated to be effective therapy in the treatment of obesity. There is no substantial evidence that HCG increases weight loss.\nAdditionally, the labeling for the “homeopathic” HCG products states that each product should be taken in conjunction with a very low calorie diet (VLCD). Consumers on a VLCD are at increased risk for side effects including gallstone formation, electrolyte imbalance, and heart arrhythmias. A VLCD should only be used under proper medical supervision.\n4. Which homeopathic HCG products and manufacturers/distributors are affected?\nThe following table provides the products and manufacturers/distributors that are the subjects of this action. This is not an all inclusive list of illegal “homeopathic” HCG products currently on the market. At this time, all drug products claiming to include “homeopathic” HCG are illegally marketed.\nManufacturer/Distributor Name Product Name\nHCG Diet Direct, LLC HCG Diet Homeopathic Drops\nHCG 1234 LLC (The hCG Drops LLC) Homeopathic HCG\nHCG Platinum LLC; RightWay Nutrition HCG Platinum\nHCG Platinum X-30\nHCG Platinum X-14\nNutri Fusion Systems LLC HCG Fusion 30\nHCG Fusion 43\nwww.resetthebody.com Homeopathic Original HCG\nwww.theoriginalhcgdrops.com Homeoapthic HCG\nHcg-miracleweightloss.com HCG Extra Weight Loss Homeoapthic Drops\nNatural Medical Supply Alcohol Free hCG Weight Loss Formula\n5. Are prescription HCG products safe and effective for weight loss?\nNo. FDA-approved uses for prescription HCG products include female fertility and select hormonal treatment in males, and FDA has not approved prescription HCG for any other uses, including weight loss.. Current scientific evidence does not support the claim that HCG is safe and effective for weight loss or for the treatment of obesity. In fact, FDA labeling for the approved HCG drug products requires the following statement about the use of HCG for weight loss:\n“HCG has not been demonstrated to be effective adjunctive therapy in the treatment of obesity. There is no substantial evidence that it increases weight loss beyond that resulting from caloric restriction, that it causes a more attractive or ‘normal’ distribution of fat, or that it decreases the hunger and discomfort associated with calorie-restricted diets.”\nFDA is aware that healthcare professionals sometimes prescribe prescription injectable HCG for unapproved uses such as weight loss. FDA understands that sometimes approved products are used to treat conditions that the products were not approved for (i.e., “off-label” uses). The “off-label” use of products usually presents greater uncertainty about both the risks and benefits because less information is available on safety and effectiveness. Unexpected adverse events may occur in this context. FDA has received reports of serious adverse events associated with the use of HCG injections for weight loss including cases of pulmonary embolism, depression, cerebrovascular issues, cardiac arrest, and death.\n6. What should consumers do?\nFDA advises consumers who are using “homeopathic” HCG for weight loss to stop using the product, to stop following any labeled dieting instructions, and to discard the product. Consumers who suspect they have experienced adverse effects as a result of the use of HCG drug products for weight loss should contact a licensed health care professional immediately.\nSourced from the FDA's website http://www.fda.gov/ForConsumers/ConsumerUpdates/ucm281333.htm\nFootnote from Ideal Health:\nThe following products are all useful for Fat Loss and Muscle Gain:\nCell Food Natural Burn Formula\nDoctor's Best Best Acetyl-L-Carnitine HCI\nTummyTone - CLA\nClean Lean Protein Vanilla\nIsoWhey Weight Management - Madagascan Vanilla®\nGarcinia Cambogia and Ultimate Colon Cleanse + Detox\nSuper African Mango 1200\nRelated health information can be found here:\nAspartame - NutriSweet, Neurotoxin\nCarbohydrate and Protein Content of Foods\nNeed a performance boost? Try Carnitine to improve fat burning and reduce muscle fatigue\nWater, the Elixir of Life\nWhat is cholesterol?\nRelated articles can be found here:\nGet moving gym-free!\nHoodia - A Natural Supplement that kills the appetite and attacks obesity!\nMetabolism: The Fat and Thin of It\nNot Only to Fight Obesity and Depression - Rhodiola - More Powerful than Ginseng!\nThe Mood Food Connection\nIf you need help or advice, you are welcome to email our naturopathic team with your health question.\nDisclaimer: The health information presented here has been written for the New Zealand health consumer. It is of a general nature and is only intended to provide a summary of the subjects covered. The information is not intended to be comprehensive or to provide medical advice to you. While all care has been taken to ensure the accuracy of the information, no responsibility or liability is accepted, and no person should act in reliance on any statement contained in the information provided. All health ailments should be treated by a qualified health professional.\nPrevious news itemSleeping Pills Raise Risk of Death - Study\n29 Feb 2012\nNext news itemListening To Music Can Reduce Chronic Pain And Depression By Up To A Quarter\n24 Apr 2012","Is HCG injections safe for weight loss?\nNo on both counts. In fact, the Food and Drug Administration (FDA) has advised consumers to steer clear of over-the-counter weight-loss products that contain HCG. HCG is human chorionic gonadotropin, a hormone produced during pregnancy. As a prescription medication, HCG is used mainly to treat fertility issues.\nHow does HCG make you lose weight?\nIts promoters claim that taking hCG can reduce feelings of hunger and support weight loss by redistributing body fat from the thighs, stomach, and hips. According to the FDA, popular diet products containing hCG state that they reset the body’s metabolism and fix “abnormal eating habits.”\nWhy is HCG banned?\nThe United States Food and Drug Administration has stated that over-the-counter products containing HCG are fraudulent and ineffective for weight loss. They are also not protected as homeopathic drugs and have been deemed illegal substances.\nHow long do HCG injections last?\nIf you are using the Pregnyl brand of HCG, throw away any mixed medicine that you have not used within 60 days after mixing. If you are using the Novarel brand of HCG, throw away any mixed medicine that you have not used within 30 days after mixing. Store Ovidrel prefilled syringes in the refrigerator.\nHow much weight can you lose on the HCG diet in 2 weeks?\nHow much weight will I lose with the hCG diet? Most hCG dieters report a loss of 1 to 2 lbs a day; at the very least, . 5 pound a day, and at the most, 3 + lbs a day. 1 lb is generally lost in the first day.\nIs HCG a fat burner?\nSince 1975, the FDA has considered the use of HCG for weight loss to be fraudulent and requires labels for HCG to state: HCG HAS NO KNOWN EFFECT ON FAT MOBILIZATION, APPETITE OR SENSE OF HUNGER, OR BODY FAT DISTRIBUTION. HCG HAS NOT BEEN DEMONSTRATED TO BE EFFECTIVE ADJUNCTIVE THERAPY IN THE TREATMENT OF OBESITY.\nWhat are the side effects of hCG?\nThe most common side effects men experience when hCG injections are used include:\n- growth of male breasts (gynecomastia)\n- pain, redness, and swelling at the injection site.\n- stomach pain.\nHow do I keep the weight off after the hCG diet?\nMaintaining Weight After hCG\n- Stay hydrated. Dehydration is a major factor in feeling hungry. …\n- Don’t be afraid to eat. After aggressive calorie-restriction it can be difficult to feel confident with a higher-calorie diet. …\n- Support regeneration and energy with a daily multivitamin. …\n- Be patient.\n15 мая 2018 г.\nIs there a shot to lose weight?\nSaxenda® is an FDA-approved, prescription injectable medicine that, when used with a low-calorie meal plan and increased physical activity, may help some adults with excess weighta who also have weight-related medical problems (such as high blood pressure, high cholesterol, or type 2 diabetes), or obesity,b to lose …\nIs HCG being banned?\nHuman chorionic gonadotropin, or HCG, is one of these drugs that will now be deemed a biologic product. This means that, starting on March 23, 2020, pharmacies will no longer be able to compound with HCG, and we will be retiring all PCCA formulas that include it at that time.\nHow long does it take for HCG to work?\nIt takes about 2 weeks for your hCG levels to be high enough to be detected in your urine using a home pregnancy test. A positive home test result is almost certainly correct, but a negative result is less reliable. If you do a pregnancy test on the first day of your missed period, and it’s negative, wait about a week.\nCan you still buy HCG?\nThe Food and Drug Administration (FDA) is advising consumers to avoid human chorionic gonadotropin (HCG) weight-loss products. These products are typically sold in the form of oral drops, pellets and sprays, and can be found online, at weight loss clinics and in some retail stores.\nIs hCG better than testosterone?\nThe best treatment option for you depends on your age and interest in fertility. For men who already have as many children as they want, Bio-identical Hormone Replacement Therapy with testosterone is best. For men who want to preserve their fertility, hCG is the better option.\nWhat happens after taking hCG injection?\nOvulation occurs 36-40 hours after the HCG injection. Eggs will release in this timeframe if they have not been retrieved. This is adequate time for planning any form of treatment."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:e320906a-aa57-4de9-bcb0-3a5cbbe06f4d>","<urn:uuid:303a0243-777e-4e3e-aa23-ec3fe00690b1>"],"error":null}
{"question":"How do skepticism and Descartes' ontological argument differ in their approach to proving absolute truth?","answer":"Skepticism and Descartes' ontological argument take contrasting approaches to absolute truth. Skepticism requires all new information to be well supported by evidence and may even doubt the reliability of one's own senses, with some skeptics suspending judgment in investigations. In contrast, Descartes' ontological argument attempts to prove absolute truth through the concept of God as a 'supremely perfect being', arguing that God must exist because existence is a necessary attribute of perfection. While skepticism questions and doubts claims taken for granted, Descartes tries to establish certain truth by proving God's existence as a foundation for all knowledge.","context":["Individual differences |\nMethods | Statistics | Clinical | Educational | Industrial | Professional items | World psychology |\nPhilosophy Index: Aesthetics · Epistemology · Ethics · Logic · Metaphysics · Consciousness · Philosophy of Language · Philosophy of Mind · Philosophy of Science · Social and Political philosophy · Philosophies · Philosophers · List of lists\nSkepticism (or scepticism) has many definitions, but generally refers to any questioning attitude of knowledge, facts, or opinions/beliefs stated as facts, or doubt regarding claims that are taken for granted elsewhere. The word may characterise a position on a single matter, as in the case of religious skepticism, which is \"doubt concerning basic religious principles (such as immortality, providence, and revelation)\", but philosophical skepticism is an overall approach that requires all new information to be well supported by evidence. Skeptics may even doubt the reliability of their own senses. Classical philosophical skepticism derives from the 'Skeptikoi', a school who \"asserted nothing\". Adherents of Pyrrhonism, for instance, suspend judgment in investigations.\n- (a) an attitude of doubt or a disposition to incredulity either in general or toward a particular object;\n- (b) the doctrine that true knowledge or knowledge in a particular area is uncertain; or\n- (c) the method of suspended judgment, systematic doubt, or criticism that is characteristic of skeptics (Merriam–Webster).\nIn philosophy, skepticism refers more specifically to any one of several propositions. These include propositions about:\n- (a) an inquiry,\n- (b) a method of obtaining knowledge through systematic doubt and continual testing,\n- (c) the arbitrariness, relativity, or subjectivity of moral values,\n- (d) the limitations of knowledge,\n- (e) a method of intellectual caution and suspended judgment.\n- Main article: Scientific skepticism\nA scientific (or empirical) skeptic is one who questions beliefs on the basis of scientific understanding. Most scientists, being scientific skeptics, test the reliability of certain kinds of claims by subjecting them to a systematic investigation using some form of the scientific method. As a result, a number of claims are considered \"pseudoscience\" if they are found to improperly apply or ignore the fundamental aspects of the scientific method. Scientific skepticism does not address religious beliefs, since these beliefs are, by definition, outside the realm of systematic, empirical testing/knowledge.\n- Main article: Philosophical skepticism\nIn philosophical skepticism, pyrrhonism is a position that refrains from making truth claims. A philosophical skeptic does not claim that truth is impossible (which would be a truth claim). The label is commonly used to describe other philosophies which appear similar to philosophical skepticism, such as academic skepticism, an ancient variant of Platonism that claimed knowledge of truth was impossible. Empiricism is a closely related, but not identical, position to philosophical skepticism. Empiricists see empiricism as a pragmatic compromise between philosophical skepticism and nomothetic science; philosophical skepticism is in turn sometimes referred to as \"radical empiricism.\"\nPhilosophical skepticism originated in ancient Greek philosophy. The Greek Sophists of the 5th century BC were for the most part skeptics. Pyrrhonism was a school of skepticism founded by Aenesidemus in the first century BC and recorded by Sextus Empiricus in the late 2nd century or early 3rd century AD. One of its first proponents was Pyrrho of Elis (c. 360-275 B.C.), who traveled and studied as far as India and propounded the adoption of \"practical\" skepticism. Subsequently, in the \"New Academy\" Arcesilaus (c. 315-241 B.C.) and Carneades (c. 213-129 B.C.) developed more theoretical perspectives, by which conceptions of absolute truth and falsity were refuted as uncertain. Carneades criticized the views of the Dogmatists, especially supporters of Stoicism, asserting that absolute certainty of knowledge is impossible. Sextus Empiricus (c. A.D. 200), the main authority for Greek skepticism, developed the position further, incorporating aspects of empiricism into the basis for asserting knowledge.\nGreek skeptics criticized the Stoics, accusing them of dogmatism. For the skeptics, the logical mode of argument was untenable, as it relied on propositions which could not be said to be either true or false without relying on further propositions. This was the regress argument, whereby every proposition must rely on other propositions in order to maintain its validity (see the five tropes of Agrippa the Sceptic). In addition, the skeptics argued that two propositions could not rely on each other, as this would create a circular argument (as p implies q and q implies p). For the skeptics, such logic was thus an inadequate measure of truth and could create as many problems as it claimed to have solved. Truth was not, however, necessarily unobtainable, but rather an idea which did not yet exist in a pure form. Although skepticism was accused of denying the possibility of truth, in fact it appears to have mainly been a critical school which merely claimed that logicians had not discovered truth.\nIn Islamic philosophy, skepticism was established by Al-Ghazali (1058–1111), known in the West as \"Algazel\", as part of the orthodox Ash'ari school of Islamic theology, whose method of skepticism shares many similarities with Descartes' method.\nRené Descartes is credited for developing a global skepticism as a thought experiment in his attempt to find absolute certainty on which to base the foundation of his philosophy. Descartes discussed skeptical arguments from dreaming and radical deception. David Hume has also been described as a global skeptic. However, Descartes was not ostensibly a skeptic and developed his theory of an absolute certainty to disprove other skeptics who argued that there is no certainty.\n- ↑ See R. H. Popkin, The History of Skepticism from Erasmus to Descartes (rev. ed. 1968); C. L. Stough, Greek Skepticism (1969); M. Burnyeat, ed., The Skeptical Tradition (1983); B. Stroud, The Significance of Philosophical Skepticism (1984). Encyclopedia2.thefreedictionary.com\n- ↑ \"Philosophical views are typically classed as skeptical when they involve advancing some degree of doubt regarding claims that are elsewhere taken for granted.\" URM.edu\n- ↑ Merriam–Webster\n- ↑ \"Philosophical skepticism should be distinguished from ordinary skepticism, where doubts are raised against certain beliefs or types of beliefs because the evidence for the particular belief or type of belief is weak or lacking...\" Skepdic.com\n- ↑ \"...the two most influential forms of skepticism have, arguably, been the radical epistemological skepticism of the classical Pyrrhonian skeptics and the Cartesian form of radical epistemological skepticism\" UTM.edu\n- ↑ Liddell and Scott\n- ↑ Sextus Empiricus, Outlines Of Pyrrhonism, Translated by R. G. Bury, Harvard University Press, Cambridge, Massachusetts, 1933, p. 21\n- ↑ Skeptoid.com: What is skepticism?\n- ↑ Scepticism – History of Scepticism\n- ↑ Najm, Sami M. (July–October 1966), \"The Place and Function of Doubt in the Philosophies of Descartes and Al-Ghazali\", Philosophy East and West (Philosophy East and West, Vol. 16, No. 3/4) 16 (3–4): 133–141, doi:10.2307/1397536\n- A Greek-English Lexicon, Henry George Liddell and Robert Scott, revised and augmented throughout by Sir Henry Stuart Jones, with the assistance of Roderick McKenzie, Clarendon Press, Oxford, UK, 1940. Online, perseus.tufts.edu.\n- Richard Hönigswald, Die Skepsis in Philosophie und Wissenschaft, 1914, new edition (ed. and introduction by Christian Benne and Thomas Schirren), Göttingen: Edition Ruprecht, 2008, ISBN 978-7675-3056-0\n- Keeton, Morris T., \"skepticism\", pp. 277–278 in Dagobert D. Runes (ed.), Dictionary of Philosophy, Littlefield, Adams, and Company, Totowa, NJ, 1962.\n- Runes, D.D. (ed.), Dictionary of Philosophy, Littlefield, Adams, and Company, Totowa, NJ, 1962.\n- Webster's New International Dictionary of the English Language, Second Edition, Unabridged, W.A. Neilson, T.A. Knott, P.W. Carhart (eds.), G. & C. Merriam Company, Springfield, MA, 1950.\n- Butchvarov, Panayot, Skepticism About the External World (Oxford University Press, 1998).\n- Daniels, M.D., D.; Price, PhD, V. (2000), The Essential Enneagram, New York: HarperCollins\n- Sextus Empiricus, Outlines of Pyrrhonism, R.G. Bury (trans.), Prometheus Books, Buffalo, NY, 1990.\n- Richard Wilson, Don't Get Fooled Again - The skeptic's guide to life, Icon Books, London, 2008. ISBN 978-184831014-8\n- Skeptical Inquiry at the Open Directory Project\n- \"Most Scientific Papers are Probably Wrong\", NewScientist, 30 August 2005\n- \"In the Name of Skepticism: Martin Gardner's Misrepresentations of General Semantics\", by Bruce I. Kodish, appeared in General Semantics Bulletin, Number 71, 2004.\n- Classical Skepticism by Peter Suber\n- \"Outstanding skeptics of the 20th century\" – Skeptical Inquirer magazine\n- \"CSICOP and the Skeptics\" – critical essay by paranormal believer George P. Hansen\n- \"Nonsense (And Why It's So Popular)\" – course syllabus from The College of Wooster.\n- Template:CathEncy – A Christian (Catholic) account of scepticism\nTemplate:Template group Template:Use dmy datesar:شكوكية an:Scepticismo az:Skeptisizm br:Amgredoni bg:Скептицизъм ca:Escepticisme cs:Skepticismus da:Skepticisme de:Skeptizismus et:Skeptitsism es:Escepticismo eo:Metodologia Dubo eu:Eszeptizismo fa:شکگرایی fr:Scepticisme (philosophie) gl:Escepticismo ko:회의주의 hi:संशयवाद hr:Skepticizam id:Skeptisisme ia:Scepticismo is:Efahyggjahe:ספקנות kk:Скептицизм la:Scepticismus hu:Szkepticizmus nl:Scepticismeno:Skeptisisme uz:Skeptitsizmpt:Ceticismo ro:Scepticism ru:Скептицизм sq:Skepticizmi simple:Skepticism sk:Skepticizmus (filozofia) sl:Skepticizem sr:Скептицизам sh:Skepticizam fi:Skeptismi sv:Skepticism ta:ஐயுறவியல் te:సంశయవాదంuk:Скептицизм ur:ارتیابیت vi:Chủ nghĩa hoài nghi zh:怀疑论\n|This page uses Creative Commons Licensed content from Wikipedia (view authors).|","Descartes’ Ontological Argument: A Critique\nPosted by moleboi\nDuring the seventeenth century the French rationalist René Descartes constructed a series of sixth short essays known as ‘The Meditations’, in which Descartes proposed to demolish all his beliefs in order to make way for certain truth upon which he could build all further knowledge upon. Descartes points out in ‘The Meditations’ that the only certain truth must be of the metaphysical, thus this must be the root for all knowledge. Since certain truth, for Descartes at least, comes from metaphysics then it knowledge must have a metaphysical cause, hence Descartes spends some time throughout ‘The Meditations’ trying to prove that it is necessary for God to exist since God is the only metaphysical entity we can rely on to be non-deceiving and therefore use as the root system for the tree of knowledge. Descartes uses a set of three arguments to prove God’s existence; however these arguments turn out to be partly unsuccessful as we can only prove that God exists as an idea.\nDescartes’ first causal argument claims we can prove God exists because any idea we hold must be true as we cannot prove false that we have the idea of it and no idea can be any less true than another since they are all ideas of equal existence within our minds. However since we hold minds of a finite capacity then we can only conjure up ideas of finite things, such as hippogriffs or phoenixes, anything greater than our finite capacity must have been placed within our minds by a greater entity. Since God is of infinite nature then it would be impossible for us to devise such an idea by ourselves and so the idea of God must come from another entity, also the idea must come from something of equal capability to the object in question, so God being infinite must have been the idea of an infinite mind and since God is the only infinite source we can hold the idea of the only answer we can rationally deduce is that God created himself, thus it is necessary that God exists in order for us to hold the idea of him.\nThe second casual argument begins by Descartes asking how he came into existence of which he poses three ways; himself, his parents or some first principle. He dismisses himself from the equation by stating that if he created himself he must hold an infinite power to do so since only an infinite power can create itself due to its nature of being infinite but during the first argument he had already claimed his was of finite nature. He then moves on to refute the claim that it must be his parents which caused his existence, since they would need to have been created by their parents who were in turn created by their parents and so in creating a state of infinite regress unless some first principle proves to be the ultimate cause. This proves, in Descartes eyes, that he must have been created by some first principle, which he names as God, and since he was created by this first principle it is necessary that it exists as something that does not exist could not create something that does, thus God exists through necessity.\nIn the fifth meditation Descartes puts forward a third and final argument, known as his ontological argument, which goes as follows: whenever Descartes imagines the idea of God he always imagines God as having a “supremely perfect being”, thus by this definition God must hold all attributes that entail perfection, including omnipotence, unconditional honesty, benevolence and also existence. Since God is ultimately perfect then God must exist since by definition God is perfect and by perfect we mean existing. Descartes then completes his argument by using a set a set of analogies to prove that the nature of a substance cannot be separated from the substance, therefore both nature and substance are mutually dependent. One of the analogies is of a geometric nature as Descartes takes the idea of a triangle, by nature a triangle has three angles and “that its three angles are equal to two right angles”. The second analogy made is between a mountain and a valley, both are mutually dependant since there cannot be a mountain without a valley. From this line of argument Descartes concludes that God exists because he relies on his existence on order to maintain his perfection, just as the existence relies on God being perfect otherwise existence would not be necessary.\nSorell in page sixty-eight of ‘Descartes a Very Short Introduction’ states the arguments employed by Descartes in proving God’s existence are nothing more than circular argument, a form of sham-reasoning, since Descartes claims that he can only prove God’s existence by using clear and distinct premises, but Descartes also claims that he can only come across clear and distinct premises after he has proven God’s existence. This being the case then there are only two possible outcomes. One being that there are no clear and distinct premises so we shall never be able to prove God exists, because the only premises that we cannot hold any degree of doubt about are those of clear and distinct nature. Hence we can only ever doubt that God exists, to be certain either way it impossible. The other possibility is that there are clear and distinct premises and so we can prove that God exists but the certainty of the used premises do not rely on God’s existence, consequently God’s existence is not necessary for our capability of holding certain truth about anything, this refutes Descartes claim that God is the foundation needed for his metaphysics which is to be the root system to his metaphoric ‘tree of knowledge’. French scientist Pierre Petit also pointed out that it is difficult to accept Descartes claim that all humans hold an idea of God and God was necessary for certainty about all other knowledge, since even “the most dedicated atheists” could hold with absolute certainty the Earth and Sun existed without needing God to exist. Following Sorell’s and Petit’s claims we can rationally conclude that either God doesn’t exist, or if he does we cannot be certain and it is also unnecessary that he exists.\nFurther criticisms of Descartes’ arguments come from Jorge Secada. Secada says the first casual argument “assumes knowledge of God’s essence before it [the argument] can draw its inference. As with the ontological argument, it too follows the essentialist path.” By this Secada means Descartes is guilty of assigning the attribute of existence to God before actually proving that God exists in order to hold said attribute. This is a dangerous position to put oneself in since if it turns out that God doesn’t exist then he has misplaced the attribute since only existents, or substances as Descartes calls them, are capable of holding attributes, consequently if God does not exist then Descartes reasoning would fall down allowing refutation for the remainder of Descartes arguments. Secada also raises a concern with Descartes claim that only infinite entities could create themselves out of nothing and all infinite entities are perfect, but Descartes acknowledges his imperfection therefore he cannot be infinite. What Secada puts forward is the possibility that Descartes was infinite only decided to relinquish some of his perfections in order to create himself as a finite entity, having done this it was seem logical that Descartes would forget about his previous infinite self due to the newly acquired finite memory he had given himself. Even though this does propose that God is unnecessary it does raise the question why choose to be something less perfect? A plausible answer is we would do so because we had a reason for doing, only this reason remains unknown to us so it cannot be the case and therefore we can go someway to refute Secada on this occasion and accept Descartes’ proposal that we are incapable of creating ourselves, and thus some first principle must be our creator.\nThere is another concern we can make with Descartes arguments based on the claim that God is a “supremely perfect being”, if God is, as he is claimed to be, perfect then he would hold the attribute of being unconditionally benevolent, however this is not the case since we can see injustice happening in the world all the time, in fact the contemporary poet Churchill speaks about one particular type of injustice in his poem ‘Home’ which is to do with bullying. We can see this in the lines “watching outside nervously. Will they? Won’t they? Crash! Another window gone.” An unconditionally benevolent God would not allow such a social injustice to occur and therefore God cannot be unconditionally benevolent. The point of this being that if it is possible that God is not unconditionally benevolent then it is also possible that God fails to hold other attributes that are necessary for perfection since God would have been proved to be imperfect. One attribute Descartes assigns to his concept of a perfect God is existence, but since God might possibly be imperfect then it is plausible to claim that it is possible that God fails to hold the attribute of existence in which case we can conclude that God does not exist. Although this conclusion is shaky as it is founded upon hypothetical reasoning, which has not been proven to be either true or false, thus we cannot use it as sound evidence against Descartes but merely as inductively forceful or unsound evidence.\nHowever there is some support for Descartes assertions from the cosmological argument for the existence of God as put forward by Saint Thomas Aquinas in his Five Ways. Aquinas’ First Way states that “there must be a first cause of all change, since nothing changes itself” and that we cannot have a series of causes leading to an infinite regress. This supports Descartes second causal argument as far as it proves that Descartes must have been caused by a first principle and not his parents, as the latter suggestion leads to infinite regress which is an illogical position. However what this first principle is remains unnamed, Aquinas only argues that it is what we call God he does not claim that it is God. Hence the first principle does not have to be God but some other infinite entity, perhaps even Anaximander’s apeiron. In fact if we look back upon what has preceded this section of the argument then we know that the first principle exists but it is nowhere mentioned that it must be perfect, only that God is perfect and it may be the case that God is not the first principle, in which case the apeiron might be the first principle since as it does not need to hold benevolence as a attribute allowing room for injustices to occur. Therefore, following Aquinas we can say that a first principle exists and that it is often called God but the existence of any such God is still questionable as the first principle may actually be something other than God.\nTo conclude it seems unlikely that God exists since we are capable of doubting God’s existence based on the injustice argument mentioned above and the dubious reasoning Descartes uses as highlighted by Sorell, not only this but following Sorell and Petit we can also see that God is unnecessary and since Descartes and Aquinas both claim that all existents require a necessary reason for existing then we have no reason for God to exist. On the other hand we should not completely dismiss Descartes arguments since there is certainty about the necessity for a first principle, and as Aquinas points out this is often called God, so what Descartes refers to as God in ‘The Meditations’ is not God as such but just a name for the first principle. Thus as long as God is used as a name for the first principle and we do not assume God is an actual entity then we say that God exists as an idea within our minds and any external God is unnecessary and cannot exist.\n- Burns. E & Law. S, ‘Philosophy for AS and A2’, Routledge, 2004\n- Descartes. R, ‘Discourse on Method and The Meditations’, Penguin Classics, 1968\n- Gaarder. J, ‘Sophie’s World’, Phoenix House, 1995\n- Secada. J, ‘Cartesian Metaphysics’, Cambridge University Press, 2000\n- Skirry. J, ‘Descartes a Guide for the Perplexed’, Continuum, 2008\n- Sorell. T, ‘Descartes a Very Short Introduction’, Oxford University Press, 2000\n- Wise. P et al, ‘POP! The Power of Poetry’, Young Writers, 2006\n Pg. 123, Burns. E & Law. S, ‘Philosophy for AS and A2’, Routledge, 2004\n Pg. 143, Descartes. R, ‘Discourse on Method and The Meditations’, Penguin Classics, 1968\n Pg. 52, Sorell. T, ‘Descartes a Very Short Introduction’, Oxford University Press, 2000\n Pg. 150, Secada. J, ‘Cartesian Metaphysics’, Cambridge University Press, 2000\n Pg. 123, Burns. E & Law. S, ‘Philosophy for AS and A2’, Routledge, 2004\n Pg. 477, Wise. P et al, ‘POP! The Power of Poetry’, Young Writers, 2006,\n Pg, 109, Burns. E & Law. S, ‘Philosophy for AS and A2’, Routledge, 2004"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:7818fad3-5ce7-48c5-9317-423018291d4f>","<urn:uuid:f4669096-88af-4963-8e17-d6bb83988a90>"],"error":null}
{"question":"Could you explain how epoxy floor maintenance affects its longevity, and what slip-resistance considerations should be taken into account when cleaning?","answer":"Regular maintenance is crucial for epoxy floors, requiring daily sweeping and weekly deeper cleaning to maintain longevity. While the floors are durable, daily traffic can cause chipping or cracking over time, potentially requiring reapplication after several years of heavy use. Regarding slip resistance, there's a trade-off between cleanability and safety - the more slip-resistant a floor is (higher R rating), the more difficult it becomes to clean due to the larger and more angular aggregate used in the coating. This is particularly noticeable with extreme slip resistance ratings like R12, where cleaning equipment like mops can be damaged by the aggressive surface profile.","context":["If you’re considering installing an epoxy floor in your kitchen or bathroom, consider some of the pros and cons of this type of flooring. Here’s a look at stain resistance, durability, color options, and cost. It’s best to speak with a professional before making the final decision. Once you’ve made the decision, you can start planning the installation process. Listed below are the main benefits of installing this type of floor.\nFor a great stain-resistance flooring solution, you should have the floor thoroughly prepared before the epoxy coating application begins. This preparation process involves the use of media blasting, diamond grinding, jackhammers, chipping guns, and other materials. In order to ensure good adhesion, de-greasing may be required before the epoxy can be applied properly. Installation procedures vary according to the type of system you choose, but they commonly include screeding, troweling, broadcasting, and grinding. All of these procedures are discussed before the installation process begins.\nEpoxy floor installation is a cost-effective alternative to traditional materials like carpets. Because of the seamless nature of the epoxy surface, it requires less ventilation than other types of flooring. Traditional materials release particulates into the air. Moreover, a quality epoxy provides a stain-resistant surface that’s both smooth and glossy. These qualities make it an excellent choice for high-traffic industrial areas. In addition to enhancing aesthetics, stain-resistant epoxy floors are perfect for areas where people are prone to spills and liquids.\nMoreover, epoxy floors are highly durable and scratch-resistant. These floors can hold heavy items without losing their luster. Aside from that, epoxy flooring is also chemical and shock-resistant. It won’t absorb chemicals like antifreeze and acid, and it won’t spread electricity. Despite the many benefits of epoxy floors, it is important to note that regular maintenance and cleaning are still necessary to keep them looking new.\nCompared to other types of flooring, epoxy floors are easy to maintain. They can withstand heavy items being dropped on them. However, it’s important to keep epoxy floors free of dust and debris, and it’s best to keep them spotless for a few weeks. You can deep clean if the floor is heavily stained or soiled. This is particularly important in areas where heavy objects are dropped. This ensures that your floors will be clean for years to come.\nWhen you install an epoxy coating, you can easily connect your walls and floors. It will create a seamless transition between the two. You’ll also get a more hygienic environment in these areas. And if you have a cafeteria or a gym, you can install a seamless wall system around the facility. If you have a gym or a spa, you can choose epoxy coatings for the walls.\nAn epoxy floor installation is among the toughest commercial and industrial floors. Only pure concrete can compete with epoxy. Untreated concrete is porous and invites microbial infestation. An epoxy floor coating is also highly durable and long-lasting, lasting as much as 30 years. Over time, this means that an epoxy coating is a more cost-effective solution. And while epoxy may initially seem to be expensive, the long-term savings are worth it.\nThe durability of an epoxy floor installation depends on how much traffic and to use it receives. In general, a daily sweep is sufficient, but a deeper cleaning should be performed at least once a week. A floor deteriorates if it develops cracks or localized breakdown of the coating. Eventually, the floor will need a new coat of epoxy to prevent damage from occurring. For this reason, it is advisable to wear stable shoes when walking on a concrete floor.\nAfter epoxy flooring is installed, it takes several days for the surface to fully harden. The curing process should be undertaken in conditions that will not cause excessive moisture, as this will result in a stronger finish. The curing process will take up to 30 days. However, it is important to note that the drying time is critical as the epoxy will wear down and scratch easily if it is exposed to high humidity levels. If a floor is not dry enough for 24 hours, the floor can be damaged by moisture trapped beneath the floor finish.\nThe preparation work associated with epoxy floor installation is extensive. The concrete slab must be thoroughly cleaned, with oil, grease, and solvents removed from it. Properly applied epoxy will allow for the desired bond between the concrete slab and the sealant. It may take several attempts to clean the floor, but the process can be done relatively easily with hand tools. If any imperfections are present, the contractor can repair them. So, hiring a professional to install an epoxy floor is important; check out the Precision Epoxy Fort Myers listing.\nAlthough the longevity of an epoxy floor installation is generally uncompromising, daily traffic can cause it to chip or crack over time. A floor that receives daily traffic is likely to require reapplication in a few years if it is heavily used. If it does crack and requires reapplication, it may be time to look for another solution. Fortunately, Colorado has some of the best quality epoxy coatings available. We specialize in basement, garage, and commercial floor coatings.\nIf you want to create the ideal environment, choosing the right epoxy floor color is critical. Choose a color that blends in with the environment. If you are a woodworker, select a color that resembles sawdust to hide any dust that may accumulate on the floor. This way, your garage will appear cleaner and less cluttered. Light-colored soil should be white, or a light color that matches it.\nMetallic-colored concrete sealers reflect light, giving off blinding rays. Light colors make the space more prominent and reduce the need for additional lighting. Depending on the type of color, you can select a variety of different colors for your epoxy floor. However, you should know that darker colors can show dirt and require more frequent cleaning. However, light colors are easier to maintain. Metallic pearl effect epoxy is an excellent choice for creating dynamic colors.\nIn addition to industrial applications, epoxy paint is a great choice for garages and basement floors. Basements can get quite humid, so epoxy paint is a sensible choice instead of carpet. In addition to a beautiful floor, a washable area rug will also provide warmth, softness, and sound dampening. If you’re considering remodeling your basement, choosing a color-rich epoxy floor is an excellent way to begin. You can even choose to make the space radon-proof!\nDecorative color flakes are another option for enhancing your space. These are available in an endless number of sizes and textures, and you can customize the design and color scheme to enhance the environment. Flake color flakes are usually installed in conjunction with resinous flooring systems, so you can achieve the optimal appearance. Furthermore, they will blend in with the surrounding decor. With so many colors available, you’ll have no trouble finding the right match for your space.\nPolycuramine is another option. This is a material that is twenty times stronger than epoxy and dries much faster. It can be driven on within 24 hours. Another option is to paint over the old epoxy. A polyamine coating can be used as a top coat over a different color. If you don’t have time to hire an epoxy coating company, you can also opt for a DIY version.\nThe cost of epoxy floor installation can vary widely, depending on the area of the floor you want to be covered. For a 400-square-foot basement, for example, you may spend anywhere from $3,200 to $3,400. A discount of up to 10% will be given for projects of 600, 900, and 1,200 square feet. For large projects of over 2,500 square feet, you will receive a 25% discount. You can use the following tips to minimize the cost of epoxy flooring installation.\nBefore determining the cost, it’s essential to consider the four main variables. By doing this, you will be better prepared to answer the contractor’s questions and understand the range of prices for different Epoxy Flooring options. By preparing yourself, you’ll know exactly what you should expect from your new floor. It’s also important to understand the process of installation and the different materials used. When choosing a flooring system, you should consider the type of floor you’ll have in mind – basic, medium, and superior – and your budget.\nThe types of epoxy floor installation available vary in cost and quality. If you’re looking for a low-cost option, consider choosing a metallic epoxy floor system. It’s a more affordable option and can deliver outstanding performance for many years. It’s also important to check the Better Business Bureau for any complaints. DIY projects can also be a good idea, and you can often purchase do-it-yourself epoxy floor kits at paint stores.\nAmong the least expensive flooring options, an epoxy floor can cost as little as $2.50 per square foot, and the price can go as high as $8 per square foot for a smaller job. However, it’s worth considering that epoxy flooring is among the best all-around value for the money when it comes to resinous flooring. However, it is important to note that epoxy is durable and resistant to stains, but it’s not impervious to objects falling on it. If anything does happen to your epoxy floor, you can simply lay another layer of epoxy to fix the damage.\nThe cost of epoxy floor installation is directly proportional to the level of preparation work required by the concrete floor. The surface must be completely smooth and clean before the application of epoxy. In Portland, the prep work will take a day. If the concrete is in poor condition, the job will require more time and materials to ensure the best possible result. The installation process can last between eight and ten years, but proper care is essential to avoid costly repair costs.","Understanding P and R Slip Rating in Epoxy Floor Coating\nUnderstanding P and R Slip Rating in Epoxy Floor Coating\nEpoxy resin flooring stands out from other flooring solutions for numerous reasons when it comes to picking new flooring systems for industrial usage, garage floor coating or any other outdoor surfaces. Epoxy flooring, also known as resinous flooring, is a versatile, long-lasting, environmentally friendly and aesthetic option for any surface. Epoxy flooring is appealing because of its resilience to high levels of wear and tear, making it one of the longest lasting flooring solutions.\nSafety of Flooring Solutions\nAdults walk 3 to 4 kilometres each day on average, taking approximately 6-8,000 individual steps. That’s nearly 2,750,000 individual steps in a year. With that many steps, most adults should be very skilled at walking, yet hundreds of individuals slip, trip, and fall every day, with some suffering serious injuries. People don’t plan on slipping or tripping and falling in public. Building owners do not anticipate consumers sliding, stumbling and falling when visiting their establishment. Every fall, trip and slip accident have a tale to tell and there are a lot of similar variables. One of them is the type of flooring solution used and the variable of slip resistance.\nWhat is Slip resistance?\nThe relative force that prevents the shoe or foot from sliding over the pathway surface is known as slip resistance. Slip resistance is determined by several elements, including the surface of the pathway, the bottom of the footwear and the presence of foreign objects between them. In this article, we will talk about the slip resistance rating of epoxy floors.\nWhat is the P rating in epoxy floor coating?\nWhen the flooring is moist, the P3 grade implies a moderate danger of sliding. This slip rating indicates that the product may be used on stairs without adding additional traction. P2 is the industry standard and signifies a high danger of sliding on damp flooring. A Pendulum Friction Tester and a rubber slider are used in the Wet Pendulum Test. The rubber slider material of the revised Standard has been changed to better simulate a worn and polished heel. This means that achieving greater levels of slip resistance in the test is more difficult than in prior editions of the Standard.\nWhat is the R rating in epoxy floor coating?\nSo, what precisely is an ‘R Rating’ on the floor? Architects, floor specifiers, builders and you may use the R9 to R13 Rating for Shod Feet (and an ABC Rating for Bare Feet) to measure the slip resistance of a surface before purchasing flooring. These R ratings refer to the non portable DIN Standard* Floor RAMP Test (the ‘R’ in Ramp Test). The Ramp Test involves attaching a certain floor type to a ramp and applying oil (For Shod Feet) to the ramp. The ramp is then elevated and the human test subject goes backwards and forward in little increments until they slide on the floor while wearing boots.\nHow Important is P and R rating in Epoxy Floor Coating?\nStandards Australia HB 197:1999 provides guidance for these, which has been recommended since 1999. A variable angle ramp walking test is used to rate the slip resistance of flooring, which ranges from R9 (low slip resistance) to R13 (high slip resistance). When determining the degree of slip resistance required, there is a trade off to be made. Cleaning your epoxy floor system becomes more difficult as it becomes more slip resistant, yet your floor is not slippery. Your floor becomes easier to clean as the slip resistance is reduced. For some applications, a less aggressive slip resistance with a higher emphasis on cleanability is required. The more aggressive the floor’s slide resistance, the more difficult it is to clean. This is due to the bigger and more angular aggregate that is disseminated onto the coating of an aggressive slip resistant component. The more angular the floor surface’s profile, the more difficult it is to clean. When a more extreme slide resistance, such as R12, is required, problems arise cleaning equipment, such as a mop, will be ripped to bits.\nIn an epoxy floor system, there are three approaches to regulate slip resistance:\n# 1 Rate of topcoat coverage\n# 2 Aggregate is added to the epoxy resin\n# 3 The natural finish of the topcoats\nAggregates, in general, are varied sizes and forms of oxide or silica sand; topcoat finish is the natural floor finish without aggregate, and rate of material coverage is computed by square metre per litre attained during application.\nIn locations where there is a lot of foot activity, you should choose slip resistant paint. In most cases, floor treatments are performed when the hard surfaces become slick when wet. To provide appropriate slide resistance in both dry and wet conditions, the use of epoxy flooring is highly recommended.\nHow does Epoxy Flooring Compare to other Flooring Solutions?\nEpoxy flooring, particularly epoxy mortar and flake epoxy floors, is meant to minimise sliding and skidding. Most varieties of epoxy flooring are anti slip, as mentioned in the previous section. Epoxy mortar and flake epoxy flooring are the ones that resist slippage the best. If you maintain the floor sufficiently dry, you should be able to walk back and forth without losing your balance or slipping all day. Epoxy flooring is a form of flooring that is commonly used in garages and other outdoor areas. Although many types of epoxy flooring are anti slip, if they are wet with water or oil, it is still possible to slip and fall. You may walk on your epoxy flooring safely any day by strategically installing synthetic mats and runners or purchasing a big containment mat.\nEpoxy flooring is less slick and more resistant to skidding than other smooth flooring materials. Epoxy coatings are also available with slip resistant versions, making them an excellent alternative for areas where slips and falls are a concern."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:57680291-20c4-43ab-ae13-9d0104fd7c53>","<urn:uuid:c8544c02-4ece-4195-9050-9d309f12cb8e>"],"error":null}
{"question":"What research facilities and educational materials are available about Byron and the Greek Revolution, and how are these events being commemorated in contemporary publications? 📚","answer":"The Messolonghi Byron Research Center offers extensive research facilities including a library of 2000 books in English and Greek, devoted to critical theory, letters, and poetry, with resources available to scholars, students, and enthusiasts. The Center is located in Messolonghi, Greece, accessible by a three-hour bus trip from Athens. Contemporary commemoration of these events is represented by publications like Sons of Chaos, a graphic novel released to honor the 200-year anniversary of the Greek War for Independence, which tells the story of 1821 through a detailed historical narrative focusing on key figures of the revolution.","context":["Director of International Relations for the Messolonghi Byron Research Center\nProfessor Peter W. Graham, Virginia Polytechnic Institute and State University\nAs Director of International Relations for the Messolonghi Byron Research Center, I’d like to welcome you to the site itself, its growing collection of research materials, and its varied range of programs and conferences. Envisioned by Messolonghiots and chiefly made possible by a collaboration of local, regional, and national efforts, the Center is supported and enhanced by the generosity of a truly international community of Byronists, Romanticists, and Philhellenists. The Messolonghi Byron Research Center extends a warm invitation to everyone who might like to consult its books, view its prints and other artifacts, or simply learn more about Byron, his circle, or the Greek War of Independence here in the town where Byron died, a historic place made sacred by the Exodos of 1826, when the besieged citizens of Messolonghi heroically sacrificed their lives in the cause of freedom.\nDirector of Hellenic Studies, for the Messolonghi Byron Research Center\nRoderick Beaton, Koraes Professor of Modern Greek & Byzantine History, Language & Literature, King’s College London\n● ‘Historical poetics’ of the novel from ancient to modern times\n● Greek national identity in the nineteenth century\n● Byron and the Greek Revolution\nRoderick Beaton graduated from Cambridge with a first degree in English Literature and a PhD in Modern Greek. He came to King’s in 1981 as Lecturer in Modern Greek Language and Literature, and in 1988 was appointed to the Koraes Chair. He has several times served as Head of Department and has been actively involved with the Centre for Hellenic Studies since its establishment in 1989 and in the Comparative Literature programme.\nFrom October 2009 to September 2012 he has been appointed to a Major Leverhulme Fellowship, to work on a project entitled Byron’s War: The Greek Revolution and the English Romantic Imagination.\nThe Messolonghi Byron Research Center is a library dedicated to the scholarship of Lord Byron and other Romantic Era poets. The center has a growing library of 2000 books in English and Greek, devoted to critical theory, letters, poetry, and many other textual sources. Scholars, students, and enthusiasts are encouraged to use the extensive resources of the library while visiting the historical places of Lord Byron’s last days. Messolonghi, Greece is a convenient three hour bus trip from Athens, Greece. This includes a scenic trip on the Peloponnesian peninsula and the new suspension bridge “Harilaos Trikoupis” connecting Rio and Antirio. The area is rich with historic and academic treasures and is a wonderful place to study Byron and Greece.\nIf you are a scholar or a student and you would like to use the Messolonghi Byron Research Center, please contact Rodanthi-Rosa Florou for details on available resources and arranging a visit. Please visit this website again to see developments regarding the Messolonghi Electronic Library and Digital Archive Project, or M.E.L., an on-line source for textual and digital materials devoted to early nineteenth century critical theory on Lord Byron and other Romantic Era poets.","Prana: Direct Market Solutions is a marketing and sales services company founded by Atom Freeman, formerly of Valiant Entertainment and most recently of ComicHub.\nIn cooperation with IDW Publishing and Levendis Entertainment, they have announced a first-of-its-kind program to promote the IDW graphic novel release, Sons Of Chaos by Chris Jaymes and Alejandro Aragon from earlier in the summer.\nThe creators of Sons of Chaos and Prana DM will be creating social media campaigns for each Direct Market Retailer stocking Sons Of Chaos targeting fans who might not usually find their local comic shop but are the target market for this graphic novel.\nLevendis will be matching the wholesale cost of each copy of Sons Of Chaos purchased by brick & mortar comics shops during this program with advertising dollars in social media ads pushing fans into the stores during the holiday season. Details were released earlier today on the ComicsPRO Members message board along with several other online retailer boards.\n“When we set out to tell the story of the 1821 Greek War of Independence, working with independent comic shops was part of what I was excited about after spending so much of my childhood living in them. And honestly, becoming friends with so many of those retailers has been one of the best parts of this experience,” said Sons of Chaos writer/creator Chris Jaymes. “The point of this program is to support those who have supported us and who give so much of their lives to keeping this industry alive.”\nYou can read the first forty-four pages below:\nSONS OF CHAOS HC GN\n(W) Chris Jaymes (A) Alejandro Aragon\nOversized, panoramic graphic novel Sons of Chaos, exposes the quiet agenda of the Ottoman Empire’s most brutal dictator, and his fascination with a young Greek boy that led to a war that would define the Western World.\nAn immersion into the moments we never see, and the self-serving motivations that convince a nation that violence is warranted, and that war is necessary.\nIn honor of the 200 Year anniversary of the Greek War for Independence, Sons of Chaos presents the story of 1821 through the eyes of Marcos Botsaris, the son of a respected Greek leader taken prisoner as a child and raised within the dungeons of history’s most infamous Ottoman Pasha, known as the “Napoleon of the East”– Ali Pasha of Ioannina. Over the next ten years, the bond formed between them would define history.\nThe Greek War for Independence was a conflict that quietly influenced the entire world and participants ranged from the London Stock Exchange to celebrities such as Lord Byron, as well as average impassioned Americans willing to transport themselves across the Atlantic to fight alongside the Greeks.\nThis conflict was the pinnacle of what we now know as the Romantic Period and yet, it’s a war that few know ever existed outside of the Greek and Turkish cultures; a war that stimulated the fall of the Ottoman Empire and shaped Western Civilization as we now know it, and in a sense is being fought today under a different heading amongst today’s political world leaders.\nHundreds of years of Ottoman rule gave the Greeks a reason to fight. Marcos Botsaris gave them a leader.\nIn Shops: Jul 17, 2019"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:a362af64-c352-4afa-a55a-cfb0d64db1d6>","<urn:uuid:93861164-e810-4782-99ce-5878201c9226>"],"error":null}
{"question":"Yo fam, what's worse for liver - eating lots of HFCS or regular sugar?","answer":"Neither high fructose corn syrup (HFCS) nor regular sugar is worse for the liver - they are equally concerning when consumed in excess. When either sweetener is consumed, fructose goes directly to the liver for processing. While the liver can handle small amounts of fructose without difficulty, consuming large amounts of either HFCS or regular sugar can overwhelm the liver's processing ability. Both sweeteners contain similar amounts of fructose (HFCS has 42-55% fructose, while regular sugar has 50%), and excessive consumption of either can potentially lead to insulin resistance, fatty liver, and obesity.","context":["Carbohydrates on a keto or low carb diet\n- What are carbs?\n- How are carbs processed in the body?\n- Video: Carbohydrates on a low carb diet\n- How are carbs used by the body?\n- What are the benefits of restricting carbs?\n- Do I need a minimum amount of carbs?\n- The best carbs to eat on a keto or low carb diet\n- What are “net carbs”?\n- How many carbs should I eat per day?\nWhat are carbs?\nThere are two basic types of carbs in food: starches and sugars.\nStarches are made up of long chains of individual glucose (sugar) units that are linked together.\nThe diagram below is a simplified depiction of the structure of starch.\nStarchy foods generally don’t taste sweet.1 However, because starch is just a long chain of sugar (glucose) molecules linked together, once it’s digested in the gut, it’s absorbed into the blood as pure glucose, which raises blood sugar levels.2\nExamples of foods high in starch\nSugars are much shorter chains compared to starches. In fact, sometimes they are just a single glucose or fructose molecule. However, in food they’re usually two sugar molecules linked together, such as sucrose (glucose and fructose) or lactose (glucose and galactose).\nBelow are simplified depictions of a sucrose and a lactose molecule.\nSugars are found in whole foods such as many plants and dairy products, but aside from fruit and root vegetables like carrots and beets, these foods don’t taste very sweet. Most vegetables, nuts and seeds only have tiny amounts of sugar.\nExamples of whole foods that contain sugar\n- Nuts and seeds\n- Milk, yogurt and kefir\nProcessed and packaged foods often contain added sugars. Food manufacturers typically add refined sugar or high-fructose corn syrup to their products, although they sometimes use honey or other “natural” sugars that are considered healthier. But sugar is sugar, and your body processes all of it the same way.3\nExamples of added sugars\n- Refined white sugar and all other sugars: brown sugar, raw sugar, beet sugar, coconut sugar, turbinado sugar, etc.\n- High-fructose corn syrup\n- Maple syrup\n- Agave nectar\nTo learn more about sugars and other sweeteners on a keto or low carb diet, check out our keto sweeteners guide.\nHow are carbs processed in the body?\nStarches and two-unit sugars like sucrose and lactose are too big for your body to absorb. Therefore, after you eat carbs, your body produces enzymes that break them down into single sugar units that can be absorbed.\nThese single sugar units are handled by the body in different ways. To understand the effect of carbs on the body, it’s useful to know how glucose and fructose are absorbed.\nOnce glucose enters your bloodstream, it causes your blood sugar to rise immediately.4 This prompts your pancreas to produce insulin, the hormone that allows glucose to move out of your blood and into your cells. How much your blood sugar goes up – and how long it stays elevated – depends on a number of factors, including how many carbs you eat, how much insulin you produce, and how sensitive your cells are to insulin.\nOn the other hand, fructose doesn’t raise blood sugar the way glucose does.5 Instead, it goes straight to the liver, where it is converted to glycogen for storage.\nYour liver can handle small amounts of fructose found in whole foods without difficulty.6 However, consuming processed foods and beverages high in fructose can overwhelm your liver’s ability to process it properly. High fructose intake on a regular basis may potentially lead to insulin resistance, fatty liver, and obesity.7\nAgave nectar and other high-fructose “healthy” alternative sweeteners are often marketed as being “low glyemic index” because they don’t raise blood sugar as much as white sugar does. But they may possibly be an even worse choice than white sugar when it comes to your weight and health due to fructose’s adverse effects.8\nImportantly, all digestible carbs, or net carbs, are absorbed and (with the exception of fructose) can raise blood sugar — whether they come from whole or refined grains, fruits, vegetables, or sugar itself.9\nHow does the body use carbs?\nOnce the carbs you’ve eaten are digested and absorbed, the glucose they provide can be used as an energy source by all the cells in your body, including those in your muscles, heart, and brain.\nGlucose that isn’t immediately needed by these cells can be stored in your liver and muscles as glycogen (long chains of glucose, similar to starch in food). However, there’s a limit to the amount that can be stored. Once your glycogen storage sites are full, any additional glucose from the breakdown of excess carbs will be converted to fat and stored in your body, including your liver.10\nWhat are the benefits of restricting carbs?\nA keto or low carb diet provides several benefits, especially for people who want to get their blood sugar under control or lose weight:\n- Lower levels of blood sugar and insulin.11\n- Elimination of carb cravings.12\n- Powerful appetite control.13\n- Ability to go for many hours without eating due to feeling full and satisfied.14\nDo I need to eat a minimum amount of carbs?\nThe short answer is no. In fact, you technically don’t need to eat any carbs at all.\nWhen carbs are restricted, your body switches to using fat and ketones rather than sugar as its main energy source.15 Aside from your red blood cells and a small portion of your brain and kidneys, which do require glucose, your cells can use fatty acids or ketones as fuel.16 You can learn more about this in our complete guide to ketosis.\nYour body is actually capable of making glucose for any cells that need it, even if you don’t consume any carbs. This is because your liver can convert amino acids (found in protein) and glycerol (found in fatty acids) into glucose. This process is known as gluconeogenesis.\nIn their 2005 textbook “Dietary Reference Intakes for Energy, Carbohydrate, Fiber, Fat, Fatty Acids, Cholesterol, Protein, and Amino Acids,” the U.S. Food and Nutrition Board of the Institute of Medicine states:\n“The lower limit of dietary carbohydrate compatible with life apparently is zero, provided that adequate amounts of protein and fat are consumed.”17\nInterestingly, there are nine essential amino acids found in protein and two essential fatty acids, but there is no such thing as an “essential” carbohydrate.\nHowever, there are valuable nutrients in many low carb foods that contain some carbs, such as vegetables, nuts, and seeds. These foods also provide fiber, flavor and texture, which can enhance your eating experience.\nBest of all, including them in your diet will still allow you to experience the benefits of a low carb or keto lifestyle.\nThe best carbs to eat on a keto or low carb diet\nBy choosing your carbs wisely you should still be able to keep your blood sugar within healthy limits, while nourishing your body with important vitamins and minerals. Adding some carbs to your diet may also make your low carb lifestyle more sustainable, fun, colorful and varied.\nHere are some of the best sources of carbs on a keto or low carb diet:\n- Leafy greens\n- Macadamia nuts\n- Pumpkin seeds\nWhat are “net carbs”?\n“Net carbs” refer to the amount of carbs a food contains after subtracting the fiber.\nIt’s generally accepted that the fiber in whole foods isn’t digested and absorbed.18 However, not all experts on carb-restricted diets agree on this point.\nAdditionally, in people with type 1 diabetes, fiber may distend the stomach and trigger the release of hormones that raise blood sugar.19 Therefore, you can either subtract the fiber carbs in whole food to get the ‘net carbs’ or count total carbs, depending on your personal preference and tolerance.\nHere is an example of how to calculate net carbs: 100 grams (3.5 ounces) of cauliflower contains 5 grams of total carbs, 2 of which come from fiber.\n5 grams of total carbs minus 2 grams of fiber = 3 grams of net carbs.\nOn the other hand, many processed low carb foods display labels indicating their “net carbs,” which reflect their total carbs minus added fiber and sweeteners known as sugar alcohols. Studies have shown that some of these additives can be partially absorbed and raise blood sugar levels.20 Therefore, the term “net carbs” on packaged foods may be very misleading.\nWhen calculating net carbs, only subtract fiber from whole foods. In any case, we recommend sticking with whole foods and avoiding processed and packaged “low carb” products.\nHow many carbs should I eat per day?\nNot everyone needs the same carb restriction for optimal health. Healthy, physically active, and normal-weight individuals may not necessarily have to restrict carbs at all, especially if they choose minimally-processed sources most of the time.21\nHowever, for people with a range of health issues or weight problems, it’s often beneficial to keep carb consumption relatively low. Generally speaking, the lower the carbs, the more effective for weight loss and for metabolic health problems like type 2 diabetes.22\nAt Diet Doctor, we define three different levels of carb restriction as follows:\n- Ketogenic: less than 20 grams of net carbs per day\n- Moderate low carb: 20-50 grams of net carbs per day\n- Liberal low carb: 50-100 grams of net carbs per day\nTo learn more about these levels and how to choose the one that’s best for you, be sure to check out our helpful guide, How low carb is low carb?\nTop posts by Franziska Spritzler\nCarbohydrates on a keto or low carb diet - the evidence\nThe guide contains scientific references. You can find these in the notes throughout the text, and click the links to read the peer-reviewed scientific papers. When appropriate we include a grading of the strength of the evidence, with a link to our policy on this. Our evidence-based guides are updated at least once per year to reflect and reference the latest science on the topic.\nAll our evidence-based health guides are written or reviewed by medical doctors who are experts on the topic. To stay unbiased we show no ads, sell no physical products, and take no money from the industry. We're fully funded by the people, via an optional membership. Most information at Diet Doctor is free forever.\nShould you find any inaccuracy in this guide, please email firstname.lastname@example.org.\nHowever, when starches are made industrially or packaged in food products, they are often combined with sugar which will make them tase sweet ↩\nStudies have shown that starchy foods like rice and bread can raise blood sugar as much as sweet foods:\nJournal of Insulin Resistance 2016: It is the glycemic response to, not the carbohydrate content of food that matters in diabetes and obesity: the glycemic index revisited [case series; weak evidence] ↩\nIn one study, when people with impaired glucose tolerance were assigned to eat honey, white sugar, or high-fructose corn syrup for two weeks each, they experienced similar increases in blood sugar, triglycerides, and inflammatory markers regardless of which sugar they’d consumed:\nJournal of Nutrition 2015: Consumption of honey, sucrose, and high-fructose corn syrup produces similar metabolic effects in glucose-tolerant and -intolerant individuals [randomized crossover trial; moderate evidence] ↩\nAmerican Journal of Clinical Nutrition 2008: Fructose consumption and consequences for glycation, plasma triacylglycerol, and body weight: Meta-analyses and meta-regression models of intervention studies [systematic review of randomized trials; strong evidence] ↩\nIn studies, overweight and obese adults who consumed high-fructose beverages for 10 weeks gained weight and experienced a worsening of insulin resistance and heart disease risk factors:\nThe Journal of Clinical Investigation 2009: Consuming fructose-sweetened, not glucose-sweetened, beverages increases visceral adiposity and lipids and decreases insulin sensitivity in overweight/obese humans [randomized controlled trial; moderate evidence]\nEuropean Journal of Clinical Nutrition 2012: Consumption of fructose-sweetened beverages for 10 weeks reduces net fat oxidation and energy expenditure in overweight/obese men and women [randomized controlled trial; moderate evidence]\nSome, although not all, reviews on dietary fructose conclude that consuming it on a regular basis may lead to metabolic health issues:\nInternational Journal of Obesity 2004: Effect of carbohydrate overfeeding on whole body macronutrient metabolism and expression of lipogenic enzymes in adipose tissue of lean and overweight humans [non-controlled study; weak evidence]\nThe American Journal of Clinical Nutrition 2012: Effect of short-term carbohydrate overfeeding and long-term weight loss on liver fat in overweight humans [non-controlled study; weak evidence] ↩\nThese studies demonstrate that a low carb diet reduces both blood sugar and insulin levels:\nThe European Journal of Clinical Nutrition 2017: The interpretation and effect of a low carbohydrate diet in the management of type 2 diabetes: a systematic review and meta-analysis of randomised controlled trials [strong evidence]\nAmerican Journal of Clinical Nutrition 2010: Lack of suppression of circulating free fatty acids and hypercholesterolemia during weight loss on a high-fat, low carbohydrate diet [randomized controlled trial; moderate evidence]\nAnnals of Internal Medicine 2005: Effect of a low carbohydrate diet on appetite, blood glucose levels, and insulin resistance in obese patients with type 2 diabetes [non-randomized study; weak evidence] ↩\nNutrition X 2019: Effects of differing levels of carbohydrate restriction on mood achievement of nutritional ketosis, and symptoms of carbohydrate withdrawal in healthy adults: A randomized clinical trial [randomized controlled trial; moderate evidence]\nThe physiology of ketone production and utilization has been well described in the medical literature. Below are 3 review articles that expand into more detail about the process\nInstitute of Medicine of the National Academies 2005:Dietary reference intakes for energy, carbohydrate, fiber, fat, fatty acids, cholesterol, protein, and amino acids [textbook chapter; ungraded] ↩\nA portion of the processed fiber known as isomaltooligosaccharide (IMO) can be absorbed in the intestine like non-fiber carbs, which may raise blood sugar:\nEuropean Journal of Clinical Nutrition 2003: Comparison of digestibility and breath hydrogen gas excretion of fructo-oligosaccharide, galactosyl-sucrose, and isomalto-oligosaccharide in healthy human subjects [non-controlled study; weak evidence]\nMaltitol, the most common sugar alcohol in packaged low carb treats, has the highest glycemic index and insulin index of all sugar alcohols, and approximately 50% is absorbed in the digestive tract:\nEuropean Journal of Clinical Nutrition 1994: Digestion and absorption of sorbitol, maltitol and isomalt from the small bowel: a study in ileostomy subjects [randomized controlled trial; moderate evidence]\nGastroentérologie Clinique et Biologique 1991: Clinical tolerance, intestinal absorption, and energy value of four sugar alcohols taken on an empty stomach [randomized controlled trial; moderate evidence] ↩\nIn several studies, diets providing less than 50 grams of carbs per day have produced excellent weight loss and blood sugar results:\nJournal of medical Internet research 2017:An online intervention comparing a very low carbohydrate ketogenic diet and lifestyle recommendations versus a plate method diet in overweight individuals with type 2 diabetes: A randomized controlled trial [randomized trial; moderate evidence]\nNutrition & Metabolism 2008: The effect of a low carbohydrate, ketogenic diet versus a low-glycemic index diet on glycemic control in type 2 diabetes mellitus [randomized trial; moderate evidence] ↩","Is high fructose corn syrup (HFCS) worse than sugar?\nThere currently is no good evidence to suggest that one is worse than the other; either they are both inert or they are both evil. The difference between them is too small to matter in moderate consumption, and in excess both are harmful to health\nOriginal Article at Examine.com\nHigh Fructose Corn Syrup (HFCS) structure\nHigh fructose corn syrup (HFCS) is a liquid blend of both glucose and fructose, where both molecules float in solution (as monosaccharides or lone sugar molecules) rather than being bound to each other; it is commonly used in food products due to being cheap to produce and having a slightly sweeter perception than a similar dose of sucrose. The fructose content, which generates most of the sweetness, varies between 42-55% in liquid solutions (with a more rare form of HFCS used in hard candies being up to 90% fructose).\nSucrose is known as a disaccharide (two sugar) of glucose and fructose, it is essentially glucose connected to fructose in a 1:1 ratio. Because of this ratio, sucrose can be seen as 50% glucose and 50% fructose.\nDue to the presence of the sucrase (invertase) enzyme in the intestines, sucrose is broken apart into free glucose and fructose prior to intestinal absorption; this results in both glucose and fructose being detectable in the blood after sucrose ingestion, and no differences between sugars aside from the relative amounts of fructose (which are minor).\nStructurally speaking, table sugar (sucrose) and HFCS are very similar and confer both of the same sugars in somewhat similar ratios. The difference between sucrose and the higher end of fructose content (55%) is practically insignificant with moderate or moderately high consumption\nAssuming a worst case scenario, HFCS can be 55% fructose; for 100g (400kcal) of ingested sugar this would confer an extra 5g of fructose relative to the same amount of calories from sucrose.\nIt appears that in practical situations the extra fructose load is too insignificant to be practically relevant, and overconsumption of HFCS to a degree where the fructose may be practicall relevant is associated with overconsumption of sugar in general.\nA ‘cannot see the forest for the trees’ phenomena exists when looking at the possible influence of the already small fructose difference, as ingesting enough HFCS to make a practical difference results in sugar overconsumption and sugar overconsumption per sewould be obesogenic\nBoth sugar and HFCS have been found to have the same effect on satiety and leptin, a major regulator of the feeling of fullness. This is due to the interactions with satiety differing between glucose and fructose (the two monosaccharides that make up both sucrose and HFCS) but sucrose and HFCS have similar ratios of the two and an approximately equal fructose load exists.\nIn at least one double blinded intervention, four groups of persons all subject to a caloric deficit (500kcal deficit) had equal rates of weight loss despite up to 10% and 20% of total calories coming from HFCS (two groups, one at each percentage level) and sucrose (same). These levels were chosen to mimic the 25th and 50th percentile of average American intake, respectively.\nAlthough fructose and glucose have varying effects on appetite regulation, the difference between sucrose and HFCS is minimal to the point of nonexistent due to them both having similar fructose and glucose concentrations\nThe metabolic response by the body, in reference to mostly leptin and insulin, appears to be the same between sucrose and HFCS when both sugars are given in similar oral doses with no gender influence as the lack of difference has been noted in both healthy males and females.\nCurrently, it has been concluded (expert panel) that HFCS and sucrose do not have different influences on body composition and obesity (both being of comparable innocence or blame, depending on context).\nIndependent of whether or not sugar in general influences obesity and weight gain, sucrose and HFCS have no significant differences in their effects on the body\nAt the time of this FAQ entry, only one study in existence has found a difference between dietary HFCS and sucrose in an in vivo model.\nOne could take two approaches to debunking this study; either take the high-ground and mention that since the definition of ‘statistical significance’ relies on the fact that a study’s results are 95% chance due to the variable in question and 5% due to chance indicates that there will be some studies that show false results, due to chance (in this situation, one should refer to the entire body of literature to see what the consensus is). The low road could also be taken, and attack the study design using the terms ‘mockery of science’.\nSaid study was conducted by a Psychology department, and numerously contradicted itself. The below graph is replicated from said study:\nIn experiment 1, groups fed HFCS for 12 hours were heavier than those fed Sucrose for 12 hours. This is the comparison from which the conclusions were drawn, with no regard to the 24 hour HFCS group that was thinner than the 12 hour sucrose group.\nExperiment 2 shows contradictory results to experiment 1, and experiment 3 replicates the two 12 hour groups and shows much lesser difference, with a change in which group is heavier (with sucrose causing more obesity than HFCS).\nIn no experiment were calories controlled.\nAt this point in time, only one other non-epidemiological study has found differences with high-fructose corn syrup and creatine in regards to serum levels of fructose; however, the HFCS group appear to ingest more overall fructose prior to the experiment.\nCurrently, the most well cited and only evidence in a living system to find worse effects of HFCS relative to sucrose appears to be a poorly conducted study with contradictory results; it appears to be overblown by media interpretation\nNowhere should it be taken away that HFCS is blameless in adult obesity. It does cause obesity when overconsumed and is constantly correlated with increases in obesity although the degree of which it contributes is debated.\nWhat should be taken away is that the issue of sucrose versus HFCS is one where we are trying to find the lesser of two evils, and both seem to be about the same. Additional calories in the form of sugars do tend to increase the risk of obesity when not controlled for, but beyond this the only differences lie in fructose content. Since sucrose and HFCS are very similar in regards to how much fructose they contribute, they are essentially equivalent.\n- Sugars and satiety: does the type of sweetener make a difference?\n- The effects of four hypocaloric diets containing different levels of sucrose or high fructose corn syrup on weight loss and related parameters\n- Forshee RA, et al A critical examination of the evidence relating high fructose corn syrup and weight gain . Crit Rev Food Sci Nutr. (2007)\n- Stanhope KL, et al Twenty-four-hour endocrine and metabolic profiles following consumption of high-fructose corn syrup-, sucrose-, fructose-, and glucose-sweetened beverages with meals . Am J Clin Nutr. (2008)\n- Soenen S, Westerterp-Plantenga MS No differences in satiety or energy intake after high-fructose corn syrup, sucrose, or milk preloads . Am J Clin Nutr. (2007)\n- Melanson KJ, et al Effects of high-fructose corn syrup and sucrose consumption on circulating glucose, insulin, leptin, and ghrelin and on appetite in normal-weight women . Nutrition. (2007)\n- Bocarsly ME, et al High-fructose corn syrup causes characteristics of obesity in rats: increased body weight, body fat and triglyceride levels . Pharmacol Biochem Behav. (2010)\n- Le MT, et al Effects of high-fructose corn syrup and sucrose on the pharmacokinetics of fructose and acute metabolic and hemodynamic responses in healthy subjects . Metabolism. (2011)\n- Bray GA, Nielsen SJ, Popkin BM Consumption of high-fructose corn syrup in beverages may play a role in the epidemic of obesity . Am J Clin Nutr. (2004)\n- Moeller SM, et al The effects of high fructose syrup . J Am Coll Nutr. (2009)\n- Duffey KJ, Popkin BM High-fructose corn syrup: is this what’s for dinner . Am J Clin Nutr. (2008)\n- Bray GA Fructose: should we worry . Int J Obes (Lond). (2008)\n- Dekker MJ, et al Fructose: a highly lipogenic nutrient implicated in insulin resistance, hepatic steatosis, and the metabolic syndrome . Am J Physiol Endocrinol Metab. (2010)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:14449ed5-51cb-41d8-b82a-63bd6ab54a3a>","<urn:uuid:dfad0c11-129e-4fed-805e-2931fbbd45b9>"],"error":null}
{"question":"When comparing historical maritime conflicts, how do the Dutch-Portuguese battles for control of Maluku's spice trade in the 16th-17th centuries differ from the WWII naval presence in Majuro? Are there any surviving historical structures from these respective periods?","answer":"The Dutch-Portuguese conflict in Maluku was a prolonged commercial war focused on controlling the spice trade, particularly cloves and nutmeg. It resulted in significant local casualties, with the Dutch eventually establishing monopoly through violent means including the Hongi expeditions. Physical remnants include multiple Dutch forts like Fort Orange in Ternate and Forts Belgica and Nassau in Bandaneira. In contrast, Majuro's WWII presence was part of a military conflict, with evidence remaining primarily as underwater wrecks. These include a Grumman Duck aircraft, an F6F Hellcat fighter at 115 feet depth, and a B-24 Liberator bomber in 12 feet of water. While Maluku's remnants are primarily land-based fortifications, Majuro's historical structures are mostly submerged military vehicles and aircraft.","context":["Maluku is the only Indonesian province in which land makes up just 10 percent of the area's total surface. In many places the surrounding seas could be thousands of meters deep. Maluku is a transition zone between the Asian and Australian fauna and flora, and also between the Malay-based cultures of western Indonesia and those of Melanesia.\nIts approximately 1000 islands support a population of less than 1.7 million people. The average population density figure is 19 people per-square kilometer, but the distribution is uneven. Air and sea transportation are the main means which link the islands together. The province has 32 seaports and 20 airports, and only about 160 km of roads. However, good roads on many of the islands provide easy access to the often remote places of tourists interest.\nA great variety of endemic plant and animal species are found in the rugged forest-covered and mountainous hinterlands of most of the islands. A few of the best known are the Racker-tailed king fisher, the red-crested Moluccan cockatoo, and various brilliantly-colored lorikeets and parrots.\nMost of Maluku sits astride one of the world's most volatile\nvolcanic belts. The region has known more than 70 eruptions in the last 400 years. Tremors and volcanic eruptions are by no means rare events at present. Many islands, in fact, look from a distance like volcanic cones rising right out of the sea.\nFormerly known as the Moluccas, these islands are the original Spice Islands which in the 16th and 17th centuries lured the major seafaring nations of Europe to come to trade and to establish their power and influence in this part of the East.\nChinese annals of the Tang dynasty from around the middle of the 7th century A.D make mention of a land named Mi-li-ku. The 14th century Javanese manuscript Nagarakertagama mentions the name Maloko, meaning the island of Ternate, part of this province, which in the 17th century was known to the Portuguese as Moluquo.\nIt was Nicoli de Conti, however, who in 1440 revealed the existence of the Spice Islands to the Europeans. Using his information, Fra Maura drew his world map, and soon the race to the East began. In 1511, the Portuguese built their first fort in the area on the island of Ternate and established their monopoly of the clove trade.\nThe Spanish also came, but posed little trouble to the Portuguese. The Dutch, who arrived in 1599, on the other hand, proved to be their toughest contestants in the quest for Maluku's treasures. Armed conflicts broke out, taking a toll not only among the two rival European powers, but also among the local populations. To make it short, the Dutch finally emerged as winners and established their trade monopoly with iron hand. Whole villages were razed to the ground and thousands of islanders died in the so called Hongi expeditions launched by the Dutch to maintain their trade monopoly, especially on the island of Banda.\nThe British occupied Maluku for a brief period during the Napoleonic war between England and France. Dutch rule was restored in 1814, leading to e new rebellion under Matulessi which the Dutch suppressed with difficulty. The compulsory cultivation of spices was abolished in Maluku only in 1863.\nTraces of that turbulent period in Maluku's history can still be found on a number of islands. However, Maluku's great attraction for present-day visitors is its sea gardens and beaches and the beauty of the land. Music and dances and hybrid culture in general, are among the province's strong touristic drawing cards. Fish and other sea products are nowadays Maluku's major sources of revenue, but nickel, oil, manganese and various timber also contribute to the province's wealth.\nAmbon, the provincial capital of Maluku which is built on a hillside overlooking the bay, has a number of interesting sites of historical and cultural interests. Among them are the remnants of some old forts built by the Dutch East Indies Company during the heydays of the spice trade and the Museum Siwa Lima with its collection of local arts and crafts.\nMore ruins of forts are found such as the Dutch one at Lima and those of the Portuguese at Hila, which are almost entirely hidden underneath the contorted roots of a giant Banyan tree.\nThe ANZAC War Cemetery near Ambon town is the site of services held every year to commemorate the Allied soldiers who died in the region during world War 11. Ambon is at the Maluku end of the annual yacht race between Darwin, Australia and Ambon. The race usually takes place in August.\nCoral Sea Gardens\nGood beaches with coral reefs just off the shore are found around Pombo island and at Hunimoa Beach on Ambon. A popular recreation beach on the same island is Natsepa.\nTernate, an island off the west coast of Halmahera in northern Maluku, was once the seat of an important kingdom which prospered from the spice trade. The Portuguese, the Spanish and the Dutch vied with each other for influence on this island. A stronghold of Islam in the otherwise predominantly Christian province of Maluku, Ternate nevertheless carries the clear imprints of both its pre-Islamic past and its period of contact with the West, especially the Portuguese.\nThe old sultan's palace in Ternate town is now a museum. In the vicinity are the ruins of old Portuguese, Spanish and Dutch forts. The remnants of the Dutch Fort Orange are right in town.\nAbout five kilometers west of the town, on the slope of a 1,715 meter tall volcano in the middle of the island, is Afo, with its giant clove tree, said to be more than 350 years old.\nMorotai Island, just off Halmahera's northern arm, was an important air-base during world War II, first for the Allies and later for the Japanese until its recapture near the end of the war. The ghosts of war still linger in this area, where many wrecks of aircraft and rusting guns lie abandoned in the bushes.\nThe Banda group, about 160 kilometers southeast of Ambon, consists of three larger islands and seven smaller ones, perched on the rim of Indonesia's deepest sea, the Banda Sea. Near the island Manuk, the water reaches a depth of more than 6,500 meters. Of the three biggest islands Banda, Banda-Neira and Gunung Api, the first two are covered with nutmeg trees and other vegetation. The third however, is entirely bare and highly volcanic. The last eruption of Mt. Api occurred only a few years ago. The seas around Banda are the site of the famous Maluku sea gardens with their bright corals and colorful fish darting through the crystal-clear waters. Facilities for sightseeing, snorkeling and skin diving are available, as well as clean, comfortable cottages.\nBanda saw some of the bloodiest episodes of Maluku's past history during the 17th century. In 1609, the Dutch East Indies\nCompany (VOC) dispatched Verhoeff to the islands to obtain the contested spice trade monopoly at any cost. Confronted by a superior power, the people of Banda were forced to allow the company to establish a fort, but in that same year Verhoeff was killed together with 45 of his men. The Company retaliated, but peace was not restored. In 1619, V.O.C. Governor-General Jan Pieterszoon Coen arrived at the head of a penal expedition and exterminated the entire population of Banda. The land was divided into lots, called \"perken, and given to former company employees, the \"perkiniers\", who were obliged to grow nutmeg and sell them at predetermined prices to the company. Slaves did the actual work in the fields. The old \"perkenier houses\", or what is left of them, and old churches still retain a peculiar colonial character to the port town of Bandaneira today. Two old forts Belgica and Nassau, are inside the town limits. Others are found elsewhere on the islands. See also the former Dutch Governor's mansion, the Museum of History in Neira, and the huge nutmeg plantation nearby.","|Home||Photo Gallery||Sportfishing||Diving||Travel Tips||Links||Comments|\n|Diving||Majuro Diving||Arno Diving|\n|Dive Sites around Majuro|\nYou can spend years on Majuro and still find dive sites that you haven't\nhad a chance to visit. The lagoon is loaded with variety from coral\npinnacles to deep walls to wrecks -- there is something for everyone.\n|A few minutes from the Yokwe Divers dock lies The Bridge, a favorite for many divers in Majuro. Plate coral splattered with clusters of Black Coral and Tridacna clams embedded in the coral base, drops down a steep wall that exceeds 130 feet. Whitetip and Silvertip sharks cruise the area and Napoleon wrasse hover in the shallow areas. Expect to see schools of Red Snapper and brightly colored Angel Fish as well as Helfrecht's Dartfish at this popular dive site.|\n|Aneko Island has both shallow and deep water coral heads that are incredibly large. From 12-90 feet, you will find anemone, cleaner shrimp, resting turtles and a deep water coral garden.|\nKalalen Pass is a favorite location\nfor drift dives. Diving depths run from 30-130 feet at the Pass\nand steep coral walls drop into the crystal clear water.\nPelagic species cruise the currents in search of food and you\ncan expect to see sea turtles, rays, several species of sharks\nand sea turtles on a typical day. Silvertip sharks over 8 feet\nin length have been seen at this location. Kalalen Island,\nlocated adjacent to the Pass, has a lagoon side reef that boasts\nboth hard and soft corals and divers exploring the coral will be\nthrilled as they discover butterfly fish, triggerfish and the\nelusive octopus hiding in the coral gardens. Further east,\nin areas called Second Island, a gradual slope down the\ncoral head drops to a sandy bottom at around 120 feet.\nWhitetip sharks share this area with shrimp gobys and Grass eels\nare commonly sighted.\nThe North Shore outer reef on the ocean side of Kalalen Island is a pristine gradual slope populated by thousands of table corals, anemone, and tens of thousands of tropical reef fish. Schools of fusiliers rain down from the surface as you glide toward the transition from slope to near vertical wall. Sharks, rays, dogtooth tuna, and turtles are also seen here regularly.\nEven further east, Fourth Island offers a popular site for easy second dives as well as for beginning divers because of the extremely calm conditions usually found in this area. Bring your camera because in addition to schools of Kiribati Red Snapper and thousands of tropical reef fish, you can see 3 different species of anemone and anemone fish including the Marshallese Three Striped Clownfish.\nIncoming tides at The Aquarium may offer one of the most exciting dives of your trip. Ranging from 60-130+ feet on the outer reef wall of Kalalen Channel, this natural \"horse shoe\" shaped feature creates an area where tidal flow is compressed, concentrating the flow of rich, open ocean sea water as it enters Majuro Lagoon. This is the place to see Horse Eye Jacks, Black & White and Red snapper, Barracuda, and all manner of reef fish numbering in the MANY tens of thousands. On the sandy ocean floor, you will see sleeping reef sharks and Sting-rays, Gray reef, White-tip, and Black-tip sharks. Schools of Rainbow Runner, Napoleon Wrasse, and huge schools of fusiliers are also common here.\n|Bokolap Island offers an exciting experience on a dive site ranging from 12-120 feet in depth. Beautiful coral heads, 4 species of anemone, clownfish, Harlequin shrimp, 3 species of lionfish, colorful nudibranchs and more fish than you can imagine are here for the viewing. A WWII U.S. torpedo plane sits at 115 feet at this location. A Grumman Avenger is also located in this area at 120 ft. depth. Downed by anti aircraft fire, this Avenger crash-landed on the ocean-side of Bokolap Island, washed over the reef, and sunk inside the lagoon where it rests today. The tail section lies up the rubble slope and is home to a family of three striped Marshallese clown-fish.|\n|If you are looking for extreme visibility, in excess of 140 feet, you will hope for the mild weather conditions that will allow you to dive The Riviera. This northern reef location runs across nearly two miles of untouched coral reef. A drift along the reef will reveal sharks sleeping on the bottom - within your visibility but over the recreational diving limits. This area drops to over 130 feet and is populated by schools of huge red snapper, Mantas and Spotted Eagle Rays and coral reef in every color imaginable.|\n|Just northwest of the Uliga Wall is another dive that is accessible only in the best weather conditions. Known as Shark Street, this is a deep reef on the northeast outer reef of Majuro Lagoon. Divers have reported sightings of 25 or more sharks on a single dive. Thick forests of black coral and schools of Napolean Wrasse make this a thrill for anyone.|\n|Shore Dives are popular with local divers and good shore diving sites can be found on the southern reef wall a few miles west of the Yokwe Divers shop. Weather conditions and local knowledge are important when attempting shore diving which may require entry through breaking waves and a possibility of strong currents. The south shore wall ranges from 20 to over 130 feet in depth and you will be rewarded with schools of Dogtooth Tuna and Grey Reef Sharks. Gorgonian Sea Fans ride the current on the vertical walls and drop-offs of these area - commonly referred to as Mile 14, Mile 15 and Mile 17.|\n|Wreck Diving gives you the best of everything. Explorers enjoy the mystery of the wrecks and the incredible variety of marine life that can be found in every nook and cranny of a sunken structure. Photographers are thrilled by the shafts of light that penetrate through holes and open decks into the dark interiors. In a nation with so much WWII history and a heritage of water transportation as a way of life, you can expect to find a multitude of underwater structures including ships and airplanes - some intentionally sunk and functioning as artificial reefs, others sometimes referred to as \"natural wrecks\".|\n|A sunken freighter, The Kabilok, once sailed between the outer islands and Majuro, hauling copra and supplies. She lies on her side on an 80' sandy bottom in Majuro Lagoon. A favorite for night dives and underwater photographers, the Kabilok offers safe, interesting penetration into the open cargo hold and is home to colorful sponges, whip coral, and tropical fish of many species. On night dives, beautiful batfish and puffers take refuge in and around the wreck.|\n|Ejit Island (The Parking Lot) at 10-120 feet is the location of a U.S. military dumpsite. A small coral pinnacle marks the spot where Jeeps, Trucks, a Navy Tug boat, and a landing craft were sunk at the end of WWII. The relics, now artificial reefs, are home to colorful sponges, corals, and tropical fish of many species. This area is a favorite for photography and exploration.|\n|The Grumman \"Duck\", just a few miles from the Yokwe Divers dock sits inverted on the bottom. It seems to have crashed on approach to Majuro's WWII carrier re-supply airfield, which was adjacent to this site. Used primarily for search & rescue and reconnaissance, there are said to be fewer than 10 surviving Ducks left in the world. This aircraft is also home to hundreds of fish, sponges, and corals. The \"Duck\" is in excellent condition and steeped in the history of this area of operation during WWII.|\nJust 500 meters from our dock at the Marshall Islands Resort, an\nF6F Hellcat sits in 115 of water. It was pushed overboard from\none of the five aircraft carriers that were on Majuro Lagoon in late\n1944. The Grumman F6F Hellcat was the U.S. Navy's primary fighter\nbrought into service to battle against the Japanese ZERO. The control stick, rudder pedals, and throttles\nare still intact and the wings are folded back in pre-flight storage\nposition. Hundreds of tropical fish, sponges, oysters, and\ncorals have made their homes there.\nIn the same general area there are other wrecks to be explored. A short 5 minutes from our dock lies the Ratak-Ralik. A dive of 60 - 92 feet will place you on this 120' freighter that sank in the late 1980's. The engine room is accessible and very open penetrations are made through the wheelhouse and the hold. Expect to see thousands of fish on this dive. Within 200 yards the Evangeline offers an upright 85' wreck with a wheelhouse and exposed hatch into the engine room.\n|Majuro's latest find, the Cenpac, was discovered in January 2003. The Cenpac is a 150' refrigerator ship that was used to haul copra and also passengers and supplies between Majuro and the outer islands. It sank in 135 feet of water about ten years ago. Schools of Spade Fish and Giant Sweetlips call the wreck home.|\n|Midway on the western atoll between Laura & Rong-Rong is a B-24 Liberator, resting in 12 feet of water. A popular site for snorkeling, this classic American bomber was damaged during a bombing run from Kiribati prior to the U.S. occupation of the Marshall Islands. The pilots brought her down on the reef top at low tide. The pilots were captured but the plane remained and was scuttled by the Japanese soldiers who were stationed here at the time. Although the fuselage has broken apart and been buried in the surrounding sand, more than 2/3 of the wing structure is still intact with all four engines and props. The belly machinegun turret is now the host of corals and fish.|\n|A DC-3 has become a very interesting artificial reef, covered in corals and fish and located near shore at Anemonit Island. Penetrations are easily and safely made through the open aft section where the plane was dismantled. The area is dotted with other natural coral heads teeming with life and is a favorite of novice wreck and reef divers.|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:e2329f83-7c82-43b5-85ba-cb1018444cfa>","<urn:uuid:5cc866e2-6b29-445e-820a-d9cfb7fcdc3b>"],"error":null}
{"question":"What are the key differences between diagnosing glaucoma and schizophrenia using eye examination techniques?","answer":"In glaucoma diagnosis, doctors primarily measure intraocular pressure using tools like the Goldmann Applanation tonometer, Schiotz tonometer, or ICare tonometer, evaluate optic nerve appearance, and conduct visual field testing. In contrast, schizophrenia diagnosis using eye examination relies on measuring electrical activity from the retina using a handheld device called RETeval, which flashes lights of various intensities while recording retinal electrical responses through a skin electrode. While glaucoma diagnosis focuses on structural and pressure changes, schizophrenia diagnosis through eye examination looks for reduced electrical activity in multiple retinal cell layers.","context":["Summary: According to researchers, a hand held device that records electrical activity from the retina may be a useful tool in diagnosis schizophrenia.\nSource: Rutgers University.\nA portable device common in optometrists’ offices may hold the key to faster diagnosis of schizophrenia, predicting relapse and symptom severity and assessing treatment effectiveness, a Rutgers University study finds.\nIn the study, published in the May 2018 issue of the Journal of Abnormal Psychology, researchers used RETeval, a hand-held device developed to record electrical activity from the retina, to replicate and extend prior studies showing that people with schizophrenia had abnormal electrical activity in the retina. This was the first time a portable device was used for these tests. The results show the device accurately indicated reduced electrical activity in the retina in multiple cell layers in the participants who had schizophrenia, including in cell types that had not been studied before in this disorder.\n“Schizophrenia is a devastating disorder, probably the most disabling disorder long term. Although we know quite a bit about it, it’s still not that well understood,” said Steven Silverstein, professor of psychiatry at Rutgers Robert Wood Johnson Medical School and director of research at Rutgers University Behavioral Health Care (UBHC), who designed the study. “Our study should help generate further research into developing a test that clinicians – like psychologists, psychiatrists or nurses – can use in their offices to diagnose, treat and monitor the condition of people with schizophrenia.”\nLooking at biomarkers in the eye as a way to understand psychiatric disorders is a new field of study.\n“Since the retina is part of the nervous system, what is happening in the retina is likely reflective of what is occurring in the brain,” Silverstein said. “For example, we know that certain changes in the retina, like thinning tissue [due to cell loss] or weakening electrical activity, occur alongside loss of brain tissue and reduced brain activity in patients with neurological disorders like multiple sclerosis and Parkinson’s disease. We and other researchers are now investigating whether retinal changes are related to brain structure and function changes in schizophrenia.”\nIn the just-published study, the researchers evaluated 50 participants: 25 with schizophrenia and 25 with no diagnosed psychiatric disorder. In the test, the participants closed one eye and placed the other against the RETeval device, which flashed 10 to 20 white or colored lights of various intensity against a white or colored background. A tiny skin electrode was placed on the skin under the eye to record the retina’s electrical activity. The participants were tested in normal light and after sitting in the dark for 10 minutes to assess activity in different types of retinal cells. Most individual tests were completed within two minutes.\n“Since many of our participants were experiencing severe psychiatric symptoms, such as hallucinations and delusions, we wanted to use a test that was as noninvasive and quick as possible,” Silverstein said.\n“While the portable device clearly distinguished people with schizophrenia from those without a psychiatric diagnosis, it’s too soon to call this a diagnostic tool,” said lead author Docia Demmin, a graduate assistant in UBHC’s Division of Schizophrenia Research and a doctoral student in Rutgers Department of Psychology. “However, since every prior study has found that people with schizophrenia exhibit reduced retinal wave forms and slowed retinal responses, our research shows that we closing in on an accurate test that is faster, less invasive, inexpensive and more accessible to patients.”\nAbout this neuroscience research article\nSource: Patti Verbanas – Rutgers University Publisher: Organized by NeuroscienceNews.com. Image Source: NeuroscienceNews.com image is credited to Nick Romanenko / Rutgers University. Original Research:Abstract for “Electroretinographic anomalies in schizophrenia” by Demmin, Docia L.; Davis, Quentin; Roché, Matthew; and Silverstein, Steven M. in Journal of Abnormal Psychology. Published May 2018. doi:10.1037/abn0000347\nCite This NeuroscienceNews.com Article\n[cbtabs][cbtab title=”MLA”]Rutgers University “Eye Function May Be Key to Schizophrenia Diagnosis.” NeuroscienceNews. NeuroscienceNews, 30 May 2018. <https://neurosciencenews.com/eye-function-schizophrenia-9180/>.[/cbtab][cbtab title=”APA”]Rutgers University (2018, May 30). Eye Function May Be Key to Schizophrenia Diagnosis. NeuroscienceNews. Retrieved May 30, 2018 from https://neurosciencenews.com/eye-function-schizophrenia-9180/[/cbtab][cbtab title=”Chicago”]Rutgers University “Eye Function May Be Key to Schizophrenia Diagnosis.” https://neurosciencenews.com/eye-function-schizophrenia-9180/ (accessed May 30, 2018).[/cbtab][/cbtabs]\nElectroretinographic anomalies in schizophrenia\nFlash electroretinography (fERG) has been used to identify anomalies in retinal cell function in schizophrenia. Several consistent findings have now emerged, but several potentially important parameters have not yet been investigated. In this study, we recorded light- (photopic) and dark-adapted (scotopic) fERG data from 25 schizophrenia patients and 25 healthy control subjects to (1) determine if past key findings on abnormal photoreceptor and bipolar cell signaling could be replicated; (2) for the first time, examine retinal ganglion cell functioning using the photopic negative response of the fERG; (3) also for the first time, determine responsiveness of schizophrenia patients to a flickering stimulus, as an additional method to isolate cone photoreceptor function; and (4) determine if schizophrenia-related changes in the fERG could be detected using a portable hand-held ERG device. In both photopic and scotopic conditions, schizophrenia patients demonstrated weakened photoreceptor and bipolar cell activations that were most pronounced in response to the most intense stimuli. A reduced cone response to a flicker stimulus and attenuation in ganglion cell activity were also observed in the schizophrenia group. In general, groups did not differ in implicit time of retinal cell responses. These findings (1) replicate and extend prior studies demonstrating reduced photoreceptor (both rod and cone) and bipolar cell functioning in schizophrenia; (2) indicate that retinal ganglion function abnormality can also be detected using fERG; and (3) indicate that these anomalies can be detected using a portable testing device, thereby opening up possibilities for more routine administration of ERG testing.","Glaucoma: The least the general practitioner should know\nA A Stulting,1\nMB ChB, MMed (Ophth), FCS(SA) (Ophth), FRCOphth\n(UK), FEACO (Hon), FCMSA (Hon), FACS, FICS;\nMB ChB, MMed (Ophth), PhD (HPE)\n1 Professor and Head, Department of Ophthalmology, University of the Free State, Bloemfontein, South Africa\nHead, Clinical Simulation Unit, School of\nMedicine, Faculty of Health Sciences, University of the Free\nState, Bloemfontein, South Africa\nto: A Stulting (firstname.lastname@example.org)\n• What is the definition of glaucoma?\n• What is the classification of glaucoma?\n• How is glaucoma diagnosed?\n• What is the management of glaucoma?\n• What is the role of the GP in the management of\nGlaucoma has been called the ‘silent thief of the night’\nbecause patients may lose their peripheral vision slowly over a\nperiod of time with no symptoms in many cases. In South Africa,\nthe prevalence of glaucoma in people older than 40 years of age\nis between 4.5% and 5.3%. In whites 1 in 40 people over the age\nof 40 years will develop glaucoma (2%). The prevalence in\nAfrican-Americans and African-Caribbeans over 40 years of age is\n4 times higher.1\nSeventy million people world-wide suffer from glaucoma.\nDefinition of glaucoma\nGlaucoma is defined as a progressive, bilateral neuropathy of the optic nerve. It is characterised by:\n• raised intra-ocular pressure (IOP) – (normal IOP = 12 - 20 mmHg)\n• optic nerve head damage (cupping of the optic disc)\n• visual field loss.\nGlaucoma can be classified as either congenital or acquired (Table 1).\nTable 1. Classification of glaucoma 2\n• Primary open-angle glaucoma\n• Secondary open-angle glaucoma\n• Pre-trabecular glaucoma\n• Trabecular glaucoma\n• Post-trabecular glaucoma\nAqueous humor is produced by the ciliary body of the eye and it flows through the trabecular meshwork to ensure that the eye is firm so that it can function optically. (Place your finger gently on your eye and then you will feel that the eye is not soft, nor hard, but firm.)\nWhen the trabecular meshwork is obstructed by pigment (e.g. after trauma), red blood cells (e.g. after blunt trauma causing hyphema), inflammatory cells (e.g. after uveitis) or lens material (phacolytic glaucoma), the aqueous humor cannot pass through the trabecular meshwork and the intra-ocular pressure increases.\nPre-trabecular aqueous obstruction may occur if a membrane develops that covers the trabecular meshwork.\nIn some cases the aqueous humor passes through the iris and sometimes through the uveo-scleral route.\nPost-trabecular glaucoma occurs when the trabecular meshwork itself is normal but aqueous outflow is impaired as a result of elevated episcleral venous pressure. This may occur in the following conditions:\n• carotid-cavernous fistulae and dural shunts\n• Sturge-Weber syndrome\n• obstruction of the superior vena cava.\nGrading angle width\nThe grading of angle width is an essential part of the assessment of any patient with glaucomatous or potentially glaucomatous eyes. The main aims are to evaluate the functional status of the angle, its degree of closure and the risk of future closure.3\nThe following angle structures are identified:\n• Schwalbe’s line is the most anterior structure, appearing as an opaque line. Anatomically it represents the peripheral termination of Descemet’s membrane and the anterior limit of the trabecular meshwork.\n• The trabecular meshwork extends from Schwalbe’s line to the scleral spur. Gonioscopically, it has a ground-glass appearance.\n• The scleral spur. Gonioscopically, it is\nsituated just posterior to the trabecular meshwork and appears\nas a narrow, dense, often shiny, whitish band. It is the site of\nattachment of the longitudinal muscle of the ciliary body.\nThe anterior chamber angle can be classified in the following grades:3\n• Grade 4 (35 - 45 degrees): the widest angle (characteristic of myopia and aphakia)in which the ciliary body can still be visualised with ease. It is incapable of closure. (Remember, grade 4 = 4 structures can be seen: Schwalbe’s line, trabecular meshwork, scleral spur, ciliary body.)\n• Grade 3 (25 - 35 degrees): the angle is such that at least the scleral spur can be identified. It is also incapable of closure. (Remember, grade 3 = 3 structures can be seen: Schwalbe’s line, trabecular meshwork, scleral spur.)\n• Grade 2 (20 degrees) is a moderately narrow angle. Angle closure is possible. (Remember, grade 2 = 2 structures can be seen: Schwalbe’s line and the trabecular meshwork.)\n• Grade 1 (10 degrees) is a very narrow angle in which only Schwalbe’s line can be identified. The risk of angle closure is high. (Remember, grade 1 = 1 structure can be seen, only Schwalbe’s line.)\n• Grade 0 (0 degrees) is a closed angle due\nto irido-corneal contact. (Remember, grade 0 = 0 structures\ncan be seen in the angle).\nPrimary angle-closure glaucoma is a condition in which\nelevation of IOP occurs as a result of obstruction of aqueous\noutflow by partial or complete closure of the angle by the\nperipheral iris. Secondary angle-closure glaucoma occurs when\nposterior forces push the peripheral iris against the trabecular\nmeshwork or when anterior forces pull the iris over the\ntrabecular meshwork (e.g. the contraction of inflammatory\nThe diagnosis of glaucoma\nIt is important to enquire about the following risk factors:\n• age: older than 40 years (remember: 1 out of every 40 patients above the age of 40 years will develop glaucoma)\n• race: black people have a higher risk of glaucoma.\n• a positive family history is also a risk factor\nfor developing glaucoma.\nRaised IOP (more than 21 mmHg)\nThe IOP is measured with a tonometer. The most commonly used tonometer is the Goldmann Applanation tonometer (Fig. 1a), which is attached to a slit-lamp. An area of 3.06 mm diameter is flattened by a prism after local anaesthetic has been instilled in the eye.\nThe Schiotz tonometer (Fig. 1b) uses the principle of indentation tonometry, in which the extent of corneal indentation by a plunger of known weight is measured.4 This is the recommended instrument in general practice.\nPneumotonometers, such as the Air-Puff tonometer are popular with optometrists as contact is not made with the subject’s eye and topical anaesthesia is not required. The central part of the cornea is flattened by a jet of air. The time required to sufficiently flatten the cornea relates directly to the level of IOP. Its main disadvantage is that it is accurate only within the low-to-middle range.\nThe ICare tonometer is a new, small hand-held instrument. A\nvery light probe makesmomentary contact with the cornea and,\nbecause only a very small force is applied to the cornea, a\ntopical anaesthetic is not required. It can even be used in\nbabies and small children (Fig. 1c).\nOptic nerve evaluation\nIt is important to know the appearance of the normal optic disc because then the abnormal glaucomatous disc will be more readily diagnosed.\nThe normal optic disc is characterised as follows (Fig. 2):\n• colour: orange-yellow\n• vessels: come out centrally out of the optic disc\n• cup:disc ratio: usually up to 0.4 (the C/D ratio indicates the diameter of the cup expressed as a fraction of the diameter of the disc)\n• edges: well-defined and clearly visible\n• shape : round or oval\n• size: myopes (larger), hypermetropes (smaller).\nThe glaucomatous optic disc is characterised as follow (Fig. 3):\n• colour: white\n• vessels: displaced towards the nasal side\n• cup:disc ratio: more than 0.4\n• edges: well-defined and clearly visible\n• disc haemorrhages\n• bayonetting sign (characterised by double angulation of a blood vessel over the edge of the disc): with neuroretinal rim loss between the outer edge of the cup and the optic disc margin, a vessel entering the disc from the retina may angle sharply backwards into the disc and then turn towards the original direction\n• neuroretinal rim: thinning and paleness\n• laminar dot sign (seen in the base of the optic disc when the neuroretinal rim recedes)\n• excavated disc (‘bean pot’).\nVisual field testing\nA visual field machine, for example, a Humphrey’s perimeter, is\nused. Various field defects can be documented, for example a\nnasal step of Rönne, a temporal nerve fibre defect, a Bjerrum\narcuate scotoma or a Seidel scotoma.\nThe angle of the anterior chamber can be assessed by performing\ngonioscopy. The examiner, usually the ophthalmologist, uses a\ngonio-mirror lens to grade the angle from 1 to 4 (see above).\nManagement of congenital glaucoma (Fig. 4) 5\n• Symptoms: photophobia, lacrimation, blepharospasm.\n• Signs: buphthalmos (calf’s eye), corneal diameter ˃12 mm, corneal clouding, white optic disc.\n• Treatment: surgery (goniotomy or trabeculotomy).\nManagement of open-angle glaucoma\nTreatment to lower the IOP includes\nmedical therapy, laser surgery or surgery.\n• Topical aqueous suppressants: beta-blockers, adrenergic agents, alpha-2 agonists, carbonic anhydrase inhibitors\n• Systemic aqueous suppressants: carbonic anhydrase inhibitors, e.g. Diamox\n• Outflow facilitators: topical miotics, prostaglandin agonists.\nAdministration of eye drops\n• The drops should be placed in the inferior fornix of the eye.\n• It may be helpful to use a mirror when pulling the eyelids apart.\n• Use the ‘no touch’ technique to maintain good hygiene.\n• One drop, correctly placed, is sufficient.\n• Keep the drops in the fridge: the patient will feel when the cold drop goes into the eye.\n• Keep the drops for an average period of 30 days to ensure maximun efficacy.\n• If instilling different drops, wait at least 5 minutes between placing the drops in the eye to prevent dilution of the drops.\n• Close the punctum so that the drops can stay in\nthe eye for a longer period.\n• argon laser trabeculoplasty (ALT)\n• selective laser trabeculoplasty (SLT)\n• diode laser trabeculoplasty (DLT).\nArgon laser trabeculoplasty (ALT) (Figs 5a-e)\nThe mechanism of ALT is such that the treated area of trabecular meshwork may shrink, causing stretching of adjacent areas, which leads to increased outflow facilities.\nDespite favourable results, laser therapy has not replaced medications as primary therapy in patients with primary open-angle glaucoma (POAG). This was partly due to loss of efficiency over time and the introduction of more effective glaucoma medications, namely prostaglandin analogues.\nThe role of laser trabeculoplasty is limited and it is used either as adjunctive therapy or as an intermediate step between failed medical therapy and surgical intervention.\nSelective laser trabeculoplasty (SLT) 9\nSLT is a relatively new procedure which uses a 532 nm Q-switched Nd:YAG laser. This selectively targets melanin pigment in the trabecular meshwork cells, without damaging the non-pigmented structures. It may be safer than ALT as there is no thermal tissue damage. The main advantage is that treatment can be repeated, the laser is portable and the initial results show that it is probably as effective as ALT. This technique may have a place in treating patients in the rural areas of South Africa where compliance or follow up may be a problem.\nDiode laser trabeculoplasty (DLT) 10\nDiode laser ablation lowers the IOP by destroying part of the secretory ciliary epithelium, thereby reducing aqueous secretion. More than one treatment is usually required for adequate pressure control.\nFig. 5e. The sketch on the left (A) represents the trabecular meshwork with the openings in the meshwork (drawn as squares), while the intended areas of treament are shown as X. The sketch on the right (B) shows that the openings in the trabecular meshwork enlarge at areas away from the fibrotic areas caused by the laser treatment. Aqueous humor can now flow more easily through the enlarged openings in the trabecular meshwork.\nSurgical treatment involves either trabeculectomy with or\nwithout mytomicin-C (MMC) (Figs 6 and 7) or valves (Molteno,\nClosed-angle glaucoma and its management\n• rapidly progressive impairment of vision\n• periocular pain and congestion\n• nausea and vomiting may occur in severe cases\n• transient blurring and haloes around lights.\n• circumcorneal redness (in contrast with conjunctivitis, where the redness is mainly in the fornices)\n• corneal oedema\n• increased IOP\n• the pupil is vertically oval and fixed in the\nsemi-dilated position and is unreactive to both light and\n• Initial medical therapy:\n• carbonic anhydrase inhibitors (Diamox 500 mg followed by oral acetazolamide 250 mg 4 times daily.\n• hyperosmotic agents (oral glycerine given by the ophthalmologist).\n• Peripheral Nd: YAG laser iridotomy to re-establish the communication between the anterior and posterior chambers of the eye by making an opening in the peripheral iris.\n• A prophylactic laser iridotomy must be performed on the fellow eye to prevent an acute attack in the future.\n• Sometimes a surgical peripheral iridectomy must be performed if the iridotomy is too small and not functioning.\n• Trabeculectomy is reserved for those patients\nwho fail to respond or where the angle is chronically closed.\nThe role of the general practitioner in glaucoma\n• Take a good history from the patient:\n• Do you have glaucoma?\n• Any family member with glaucoma?\n• Tell your patients that every person older than 40 years should test their IOP annually.\n• Make sure that your patients have regular measurements for IOP.\n• Make sure that your glaucoma patients go for regular visual field testing.\n• Do regular direct ophthalmoscopy on your patients to detect glaucomatous optic nerve changes.\n• Monitor the therapy and make sure that you know the side-effects of the drops your glaucoma patients are taking, namely:\n• beta blockers: postural hypotension, bronchospasm\n• Diamox: potassium depletion\n• topical allergy (Fig. 8).\n• Repeat prescribing glaucoma drops – to be reviewed 6-monthly.\n• Inform and reassure every glaucoma patient.\n• Refer patients to the ophthalmologist for re- assessment, e.g. side-effects of medication, post-glaucoma surgery or red eye.\n• Remember that glaucoma is a medical disease\nthat should be under the control of an ophthalmologist.\n1. Fraser S, Wormald R. Epidemiology of glaucoma. In: Yanoff M, Duker JS, eds. Ophthalmology, 2nd ed. St Louis: Mosby, 2009,1413.\n2. Kanski JJ, McAllister JA, Salmon JF. Glaucoma. A Colour Manual of Diagnosis and Treatment, 2nd ed. London: Butterworth-Heinemann, 1996:3-5.\n3. Kanski JJ, McAllister JA, Salmon JF. Glaucoma. A Colour Manual of Diagnosis and Treatment, 2nd ed. London:Butterworth-Heinemann, 1996:17-19.\n4. Kanski JJ, Bowling B. Clinical Ophthalmology: A Systematic Approac., 7th ed. Ediinburgh: Elsevier Saunders, 2011:313-315.\n5. Bruce Shields M. Textbook of Glaucoma, 4th ed. Baltimore: Williams and Wilkins, 1998:195-205.\n6. Cioffi GA, Duncan J, Girkin CA, et al. Basic and Clinical Science Course, Section 10. Glaucoma. San Francisco: AAO, 2009-2010:188.\n7. Cioffi GA, Duncan J, Girkin CA, et al. Basic and Clinical Science Course, Section 10. Glaucoma. San Francisco: AAO, 2010-2011:167-183.\n8. Kanski JJ, Bowling B. Clinical Ophthalmology A Systematic Approach, 7th ed. Edinburgh: Elsevier Saunders, 2011:387-388.\n9. Kanski JJ, Bowling B. Clinical Ophthalmology A Systematic Approach, 7th ed. Edinburgh: Elsevier Saunders, 2011:388.\n10. Kanski JJ, Bowling B. Clinical Ophthalmology A Systematic Approach, 7th ed. Edinburgh: Elsevier Saunders, 2011: 390.\nFull text views: 4607"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:47811b3c-2039-4eca-9119-a858ff6159ab>","<urn:uuid:06ef6185-df79-4fd9-804f-e1a159b16479>"],"error":null}
{"question":"Can both Tableau Clustering and Multiclass Decision Jungle automatically determine the optimal number of groups/clusters in the data?","answer":"Yes, both algorithms can automatically determine optimal groupings, but in different ways. Tableau Clustering automatically generates the number of clusters from the data, starting with 2 clusters by default, though users can manually adjust this number. Similarly, Multiclass Decision Jungle's algorithm will determine what it thinks is the optimal number of clusters based on the data, while also allowing users to modify parameters such as the number of decision DAGs and maximum width/depth of the graphs.","context":["I’ve been an avid user of Tableau for years. I pretty much can’t solve a quantitative problem these days without using Tableau to help me visually explore my data and iterate through ideas and hypotheses. But, some problems require more heavy lifting in Tableau than a viz can handle simply. Today, we will discuss Tableau Clustering and why it is useful in creating better analysis of data.\nWhat is Tableau Clustering?\nTableau has recently begun adding more statistical tools that provide powerful ways of visualizing and exploring data. Tableau clustering is one of the newest features in Tableau 10. It puts advanced statistics into your hands with just a few clicks.\nTableau Clustering allows you to easily identify statistically similar groups. In plain English, based on attributes you tell Tableau, it will go through and determine similarities and create look-a-like groups. You can then drill into those for more detail or compare how each group behaves relative to each other.\nLike we discussed above, the ability to segment data into useful groups or bins is as important as ranking and identifying your top and bottom values. It’s a must for any data analyst. Tableau Clustering takes that ability to a whole new level. You don’t need code or need to be a trained statistician to access it.\nTableau Clustering excels at visually seeing the relationships between data. For instance, we might wonder, “How do these 6 things interact together and what results do they produce?” What if we wanted to add Measures instead of Dimensions? For example, purchase patterns (Sales) and amount we actually make (Profit) and return or discount patterns (Discount, Returns).\nTableau clustering allows us to add this additional information. This helps us move beyond simple segments to advanced, incorporating data on behavior patterns and actions (Measures), as well as attribute information like Region or Marketing Channel (Dimensions).\nWhy is Tableau Clustering Useful?\nGetting better insights faster enables us to take more action. Being able to take action that makes an impact makes you a hero; it makes you the person with all the answers. That’s an awesome feeling and it’s what Tableau clustering enables us to achieve. The ability to find hidden insights with Tableau’s easy drag and drop functionality is a major step in getting to action faster.\nTableau Clustering Examples\nHere are some example vizzes of how people have used Tableau clustering to create segments and find insights they couldn’t get to easily:\nMarketing pro Chris Penn used the Tableau clustering tool to find insights about his own blog that were obscured with traditional methods of visualization. Namely, drilling into what topics of social media posts drove new users, large number of reshares, or were stagnant:\nChris Wood gives an insightful interactive analysis of at risk youth in the Washington, D.C. school district, also explaining how he used Tableau clustering to do so.\nTableau Clustering Uses\nCheck out more Tableau clustering applications at work below.\n1. Customer segmentation\nSay you have a group of customers that logs in very infrequently, never calls support, started with low monthly recurring revenue, but spent tons on upgrades over time. That’s an odd group with tremendous organic growth and low costs, even though initial revenues were low. Tableau clustering can find groups like this.\n2. Market research\nHow do we determine different groups in the market and create products and marketing messages that resonate with those people? For example, a bank found a group of entrepreneurs that was using equity from their homes via a 2nd mortgage to fund their startups. Knowing that led to a whole new line of products for the bank that resonated much stronger with that group.\n3. Customer surveys\nWhat Tableau clusterings crop up among satisfied customers, what clusters crop up among unsatisfied customers? Are the unsatisfied customers also utilizing your excellent support services?\n4. Matching or recommendation algorithms\nNetflix: For example, based on movies that have a Strong Female Protagonist, Witty Humor, and British Actors, we recommend all movies based on every Jane Austen book ever.\nPosition the cell towers so that all customers receive optimal signal strength based on addresses, usage patterns, roaming, subscriptions, peak times, traffic patterns and roads, etc.\nSay you’re a police chief trying to maximize your officer time with limited budget. You need to schedule patrols at peak times in the most crime-likely areas, again based on any number of factors, like time of day, weather, income and education levels, past crime events, types of crime, known gang locations, etc.\nI personally use Tableau clustering all the time in my daily analytics work and I find that it has unrivaled abilities in telling a story about groups of data. Stay tuned for part 2 where I cover how to create your own Tableau clustering charts.\nHow to Create a Tableau Clustering\nLet’s jump in and create a Tableau clustering chart from the Superstore Dataset that shows the relationship between sales and profits with highlighting of other fields such as marketing channel or product category. We start with a view with these fields pulled out:\nA. Tableau clustering creation\n- First, click ‘Show Me’ at the top right and choose the ‘Scatter Plot’ option to get this into more of a useful format. Then, you’ll see that Marketing Channel and Region are on the Shapes and Color shelves, respectively.\n- After that, set it to ‘Entire View’ from the drop down menu at the top.\n- Then, let’s add several more ‘Dimensions’. Add Product Category, Customer Segment, and Product Subcategory to the Detail shelf.\n- Subsequently, click on the ‘Shapes’ card to set each of the marks to Filled from the drop down menu labeled ‘Select Shape Palette.’ Then, choose Assign Palette and click Ok\n- Next, click on the Analytics tab at the top left, above your Dimensions.\n- The next step in Tableau clustering is clicking Cluster and dragging it out. Be sure to place it on top of the Cluster box that appears.\n- Also, notice that 2 clusters are generated automatically from the data.\nB. Play with potential Tableau clusterings\n- Let’s play with the number of potential Tableau clusterings. Change the number from Automatic to 5. Then, you should see the different colors.\n- Go over to top right where the data highlighter shows the different Tableau clusterings. Click each one in succession to highlight that segment on the scatter plot. Are you seeing some interesting groups, like a group of high sales, low profit?\n- Click on the down arrow on each pill that you put on the Detail shelf and select “Show Highlighter.”\n- These should appear on the right-hand side. Click through these to see if there are any interesting insights that emerge. For example, under the Marketing Channel highlighter, choosing “SEO” or “Social Media” reveals some interesting insights. Or choosing “Google Adwords” reveals an interesting outlier.\nWith this advanced Tableau clustering chart created, stay tuned for part 3 where I cover how to interpret, explain, and visually fine-tune Tableau clustering charts.\nC. Tableau clustering visual fine tuning\nWe can easily build an awesome Tableau clustering chart, but there is some visual fine-tuning to do before we can hit a home-run with our boss. Therefore, we have created an easy guide to Tableau clustering so data interpretation and explanation becomes easier to do. Before we jump in, let’s take a look at some of the numbers under the hood.\nD. Tableau clustering interpretation and analysis\nHow do I understand each of my Tableau clusterings beyond just eyeballing it?\n- First, click the down arrow on the Clusters pill which should be on your Color shelf.\n- Then, choose Describe Clusters.\n- Lastly, a window will appear with a lot of information about how this was created. You want to pay attention to the following:\nE. Tableau clustering variables\n- Variables of Tableau clustering – these are the measures that you are crunching to find look-a-likes (i.e. group similar customers by sales and profit)\n- Level of detail – these are the dimensions that you’re incorporating into the Tableau clustering (i.e. show me look-a-like customers by sales and profit, by analyzing customer segment, marketing channel, product category, etc. and finding commonalities across all of those).\n- Number of clusters – these are the distinct groups or segments that the algorithm found\n- Clusters – you need to scroll down to find these.\n- Number of Items – shows how many data points are in each cluster (these could be your bars or the circles on a scatter plot)\n- Centers – this is the average value within each cluster. You’ll see the obvious differences.\n- It’s OK to have Tableau Clustering of different sizes as data may group more strongly at one end then another, but you want each cluster to have enough data points to be meaningful.\n- If it only has one or two, consider excluding those from the view as they might be outliers skewing your results, or consider changing the number of clusters.\n- Note: Most of the cluster centers will appear in scientific notation, which is frustrating. If you click the Copy to Clipboard button and paste it into Excel, you can format the numbers so you know correctly what they represent.\nF. Cleaning up Tableau clusterings\nNow, Let’s clean the clusters up with a trick to rename them with the added bonus of being able to use them in other charts and analyses. (Note that once you complete this step you cannot view the previous underlying numbers, so make sure you have copied the numbers or taken a screenshot.) This is the final product:\n- Hold down the Ctrl key and then click on the Clusters pill on the color shelf, and then drag this over into Dimensions.\n- Now, double click the Clusters pill you just dragged into Dimensions and rename it to “Sales & Profit Clusters.” This is now a field that we can reuse again later, which will be very helpful in analyzing certain segments of customers.\n- Click the down arrow on the renamed pill and choose Edit Group.\n- Right click on Cluster 1 and choose Rename. Type “Low Sales, Low Profit.”\n- Follow the same procedure for Cluster 2 (note that they may not be in numeric order!). Rename is to “High Sales, Low Profit.”\n- Rename Cluster 3 to “Top Performers.”\n- Then, rename Cluster 4 to “Mid-tier Sales, Low Profit.”\n- Also, rename Cluster 5 to “Medium Sales, Medium Profit.”\n- Now drag the updated “Sales and Profit Clusters” pill and replace the existing Clusters field on the color shelf. You can do this by placing this pill directly on top of the other one. Or, by dragging the current field on Color off and replacing it with the new. Follow along with the GIF below to see it completed up to this point (Click to see it full screen):\nG. Changing colors in Tableau Clustering\nNow, let’s change the color scheme in Tableau clusterings, so that our colors convey a little more meaning.\n- On the legend, click the drop down arrow at the top right, and choose Edit Colors.\n- Set the color palette to Superfishel Stone in the drop down menu.\n- Now choose the “Top Performers” segment and the click on the dark green pill.\n- Repeat this procedure and change “Low Sales, Low Profit” to the orange color. Change “High Sales, Low Profit” to red. Change “Mid-tier Sales…” to the light olive color. Change “Medium Sales” to the aqua color.\n- Choose OK.\nWe now have some statistically valid segments that we can reuse and that are highlighted with meaningful titles that indicate the next step. For example, “High Sales, Low Profits” leads us to the very obvious “why” question. We can then drill down deeper to see what else surfaces from these data points that indicate actions we need to take.\nHow do I explain Tableau clustering to other people…?\n…and get the “thumbs up” from your boss?\nUse the following tips:\nExplain Tableau Clustering In English\nFind members of a potential group (could be customers, could be cities, could be anything you’re trying to group on) that are as similar to each other as possible, and as dis-similar as possible to the next group. We want each group to be as unique and distinct as possible, while we want each member of a particular group to be as similar as possible.\nExplain Tableau Clustering Quantitatively\nFor a given number of clusters or look-a-like groups (denoted by the letter “K”), the algorithm partitions the data into that many clusters or groups. The algorithm will determine what it thinks is the optimal number of clusters for you, based on your data. But you can easily change that to see if new patterns emerge. Each Tableau Cluster has a center (centroid) that is the average value of all the points in that cluster. Each cluster is a valid statistical grouping that will update dynamically as data values change or as new data is added.\nShare an Example of Tableau Clustering\nLet’s say you have information about four Domino’s pizza chains, and a list of customer addresses. But those customer addresses aren’t tied to any particular Domino’s location. You’d have to manually sort through the addresses and compare them on Google Maps to determine which location they should order from. Tableau clustering does this automatically. It would crunch through the data and then determine which neighborhoods are around each Domino’s location. You’d have four clusters. This is essentially what Google does when you search for “pizza near me,” by the way.\nWhat have you used Tableau clustering for and how did it allow you to find more insight? Let us know in the comments! To learn more about Tableau, check out our course offerings.","Multiclass Decision Jungle\nSupport for Machine Learning Studio (classic) will end on 31 August 2024. We recommend you transition to Azure Machine Learning by that date.\nBeginning 1 December 2021, you will not be able to create new Machine Learning Studio (classic) resources. Through 31 August 2024, you can continue to use the existing Machine Learning Studio (classic) resources.\n- See information on moving machine learning projects from ML Studio (classic) to Azure Machine Learning.\n- Learn more about Azure Machine Learning.\nML Studio (classic) documentation is being retired and may not be updated in the future.\nCreates a multiclass classification model using the decision jungle algorithm\nApplies to: Machine Learning Studio (classic) only\nSimilar drag-and-drop modules are available in Azure Machine Learning designer.\nThis article describes how to use the Multiclass Decision Jungle module in Machine Learning Studio (classic), to create a machine learning model that is based on a supervised learning algorithm called decision jungles.\nYou define the model and its parameters using this module, and then connect a labeled training data set to train the model using one of the training modules. The trained model can be used to predict a target that has multiple values.\nMore about decision jungles\nDecision jungles have the following advantages:\nBy allowing tree branches to merge, a decision DAG typically has a lower memory footprint and a better generalization performance than a decision tree, albeit at the cost of a somewhat higher training time.\nDecision jungles are non-parametric models, which can represent non-linear decision boundaries.\nThey perform integrated feature selection and classification and are resilient in the presence of noisy features.\nFor more information about the research behind this machine learning algorithm, see Decision Jungles: Compact and Rich Models for Classification (downloadable PDF).\nHow to configure Multiclass Decision Jungle Model\nAdd the Multiclass Decision Jungle module to your experiment in Studio (classic). You can find this module under Machine Learning, Initialize Model, and Classification.\nDouble-click the module to open the Properties pane.\nResampling method, choose the method for creating multiple trees, either bagging or replication.\nBagging: Select this option to use bagging, also called bootstrap aggregating.\nEach tree in a decision forest outputs a Gaussian distribution by way of prediction. The aggregation is to find a Gaussian whose first two moments match the moments of the mixture of Gaussians given by combining all Gaussians returned by individual trees.\nReplicate: Select this option to use replication. In this method, each tree is trained on exactly the same input data. The determination of which split predicate is used for each tree node remains random, so diverse trees are created.\nSpecify how you want the model to be trained, by setting the Create trainer mode option.\nSingle Parameter: Use this option when you know how you want to configure the model.\nParameter Range: Use this option if you are not sure of the best parameters, and want to use a parameter sweep.\nNumber of decision DAGs: Indicate the maximum number of graphs that can be created in the ensemble.\nMaximum depth of the decision DAGs: Specify the maximum depth of each graph.\nMaximum width of the decision DAGs: Specify the maximum width of each graph.\nNumber of optimization steps per decision DAG layer: Indicate how many iterations over the data to perform when building each DAG.\nAllow unknown values for categorical features: Select this option to create a group for unknown values in testing or validation data. The model might be less precise for known values, but it can provide better predictions for new (unknown) values.\nIf you deselect this option, the model can accept only values that were present in the training data.\nConnect a labeled dataset, and one of the training modules:\nIf you set Create trainer mode to Single Parameter, use the Train Model module.\nIf you set Create trainer mode to Parameter Range, use the Tune Model Hyperparameters module. With this option, the algorithm iterates over multiple combinations of the settings you provided and determines the combination of values that produces the best model.\nIf you pass a parameter range to Train Model, it uses only the first value in the parameter range list.\nIf you pass a single set of parameter values to the Tune Model Hyperparameters module, when it expects a range of settings for each parameter, it ignores the values and uses the default values for the learner.\nIf you select the Parameter Range option and enter a single value for any parameter, that single value you specified is used throughout the sweep, even if other parameters change across a range of values.\nRun the experiment.\nAfter training is complete:\n- To use the model for scoring, connect it to Score Model, to predict values for new input examples.\nFor examples of how decision forests are used in machine learning, see the Azure AI Gallery:\n- Compare Multiclass Classifiers sample: Uses several algorithms and discusses their pros and cons.\nThis section contains implementation details, tips, and answers to frequently asked questions.\nFor more information about the training process with the Replicate option, see:\n- Decision forests for computer vision and medical image analysis. Criminisi and Shotton. Springer 2013\nIf you have limited data or want to minimize the time spent training the model, try these recommendations:\nLimited training set\nIf the training set contains a limited number of instances:\n- Create the decision jungle using a large number of decision DAGs (for example, more than 20)\n- Use the Bagging option for resampling.\n- Specify a large number of optimization steps per DAG layer (for example, more than 10,000).\nLimited training time\nIf the training set contains a large number of instances and training time is limited:\n- Create the decision jungle that uses a smaller number of decision DAGs (for example, 5-10).\n- Use the Replicate option for resampling.\n- Specify a smaller number of optimization steps per DAG layer (for example, less than 2000).\n|Resampling method||Any||ResamplingMethod||Bagging||Choose a resampling method|\n|Number of decision DAGs||>=1||Integer||8||Specify the number of decision graphs that can be created in the ensemble|\n|Maximum depth of the decision DAGs||>=1||Integer||32||Specify the maximum depth of the decision graphs to create in the ensemble|\n|Maximum width of the decision DAGs||>=8||Integer||128||Specify the maximum width of the decision graphs to create in the ensemble|\n|Number of optimization steps per decision DAG layer||>=1000||Integer||2048||Specify the number of steps to use to optimize each level of the decision graphs|\n|Allow unknown values for categorical features||Any||Boolean||True||Indicate whether unknown values of existing categorical features can be mapped to a new, additional feature|\n|Untrained model||ILearner interface||An untrained multiclass classification model|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:d509d4d3-3a2e-4d95-bb2b-78c759d6d2d3>","<urn:uuid:4c3ffd17-5a66-49ce-b9a6-c74181484596>"],"error":null}
{"question":"How do weighted moving averages and ARIMA models compare in handling time series data?","answer":"Weighted moving averages and ARIMA models handle time series data differently. Weighted moving averages use weights from 0.0 to 1.0 to indicate the subjective importance placed on past or recent data, with higher weights emphasizing more recent data in creating smoothed time series. ARIMA, on the other hand, combines both autocorrelation and moving averages, computing the autocorrelation function while minimizing calculation errors. It specifically accounts for cases where the average changes over time and integrates both aspects to predict trends with associated probabilities.","context":["In this article, we will discuss 2 types of forecasting models – the weighted and unweighted moving average model. I’ll also discuss Measure of Forecasting Accuracy.\nUnweighted Moving Average\nThe Moving Average model is in class of “naive” models, because it takes a data set with variation and creates another data set with less variation, or a smoothed data set. The Moving Average model takes the average of several periods of data; the result is a dampened or smoothed data set; use this model when demand is stable and there is no evidence of a trend or seasonal pattern.\nMoving average routines may be designed to remove the seasonal and random noise variation within a time series. If the moving average routine is used repeatedly on each newly-generated series, it may succeed in removing most of any cyclical variation present. What is left of the original series after early smoothings to remove seasonal and random or irregular components is a successor series retaining some combination of trend and cyclical behavior. If no trend or cyclical behavior are present in the time series, the smoothings may leave a successor series which plots as a nearly horizontal line against time on the horizontal axis. Assuming the presence of trend and cyclical behavior in the original series, the moving average process provides a method of isolating it.\nThe smoothing effect of the moving average model provides for a “cleaner” data set, which may or may not help in estimating the future level of a variable.\nThe formula for the Moving Average Model is below:\nLet’s suppose the data set below:\nThe Month and Demand columns shows the time series for the month. The 3rd column shows the 3 period moving average, calculated as follows:\n((119 + 72 + 113) / 3 = 101)\nFollowing the same formula above, walk across the time series in 3 week periods in order to build the smoothed series, the new time series with less variation.\nForecasting Measures of Accuracy\nThe 4th column shows an Error column calculated as follows:\n((Demand – 3 Week MA) = Error)\nThe 5th column shows the Absolute Error, which is just the absolute number of the items in the Error column.\nThe 6th column shows a Mean Forecast Error, or MFA. This shows the direction of the forecasting error. The rule here is that if the number is negative, the the model tends to over-forecast. If the number is positive, then the model tends to under-forecast. The MFA is calculated as follows:\nSum of Error / N = MFA\nUsing the data set above,\n[((-19 + -7 + 39 + 13 + 8 + -30 + -10 + -12 + -1) / 9) = -2]\nSince the MFA is negative, then this tells us that the direction of the forecast error; in this case, the data and using this model tends to over-forecast.\nThe last column shows a Mean Absolute Deviation, which gives us an indication of the size of the error. In other words, since MFE for our data set is negative, pointing to an over-forecast, the MAD tells us that the average size of that over-forecast is ~15. To calculate MAD, do the following:\nAbsolute Error / N = MAD\nIn other words,\n[((19 + 7 + 39 + 13 + 8 + 30 + 10 + 12 + 1) / 9) = 15]\nThe smoothed time series above coupled with MFA and MAD gives us more context around our point estimates and forecast accuracy. This enables the firm to better plan for the future and form strategies around labor, capacity, inventory, service levels, and other pertinent items important to the firm.\nWeighted Moving Average\nFollow the steps for the Moving Average model above. Weights on this model indicates the subjective importance we wish to place on past or recent data. Weights can be from 0.0 to 1.0; the higher the weight, then the higher importance we are placing on more recent data; similarly, for lower weights.\nIn forming your moving average time series, just add a weight multipier to form the new, smoothed-out time series. All remaining steps in the Moving Average Model are the same.","A big issue in cloud computing is knowing when you should upstart more VMs or switch to a more powerful virtual machine in order to process user requests efficiently. Monitoring system utilization is very important for detecting if VM utilization is too high to guarantee stable and high performing IT services. But how can one determine if upscaling of a VM-infrastructure is required? Part of the answer lies in trend detection algorithms. This article describes two of the most popular ones that can be applied to VM-infrastructures.\nAutocorrelations and moving averages\nIf a series of measurements is correlated to the time of measurement, it is said that the series is “autocorrelated”. If you measure VM utilization several times you might discover that utilization will increase or decrease from time to time. A (linear) regression of measurement values will reveal growth trends. If such a trend appears, the average utilization is increasing, it is a “moving average”. The movement of the average causes the regression to produce errors, because regression models are computed on constant average values. Therefore one has to consider the errors produced by the moving average of measured values.\nMoving average and autocorrelation can be combined in the “AutoRegressive Integrated Moving Average” (ARIMA) model. The ARIMA model has two advantages: on one side the autocorrelation function of a set of values is computed, on the other side the errors that are produced by performing this calculation are minimized. ARIMA integrates aspects of autocorrelation and moving average. Therefore it is a quite feasible model to predict trends.\nWhen the ARIMA is applied to VM utilization one can predict (with a certain probability) that some threshold of utilization will be reached in the future. Defining acceptance criteria for probabilities of growth trends and for reaching a threshold in the future is a major steps towards determine the “ideal” point in time when an upscaling of a VM-infrastructure is required.\nTwo things must be done:\n- Define threshold values for VM utilization metrics that tell when a VM is overutilized. One could e. g. say that if mean CPU-utilization of the last 5 minutes is over 90%, the VM with that CPU is inacceptably overutilized and therefore such a value is athreshold for VM utilization.\n- Define a threshold for ARIMA growth trends that result in VM overutilization (which is the threshold for VM utilization). For this purpose you have to measure values for VM utilization metrics and repeatedly calculate growth trends following the ARIMA model. If such a calculation results in reaching a threshold for VM utilization, an upscaling of VM utilization is required.\nWith threshold values for VM utilization metrics and ARIMA growth trends one can construct an event management system that catches problems of VM overutilization by repeatedly measuring metrics and calculating growth trends.\nThe advantages of the ARIMA model are:\n- It gives an extrapolated estimation of the future growth trend and tries to assign a value to predicted VM utilization.\n- It takes the fact that average VM utilization changes over time into account by repeatedly calculating a moving average.\nThe drawbacks of the ARIMA model are:\n- The model states a prediction which appears to be “exact” to the statistically inexperienced viewer, but in fact there is only a probability that the future values will be most likely in the neighbourhood of the predicted values. Given any ARIMA prediction it is still possible that growth trends will discontinue in the future. Therefore predicted values can never be seen as “guaranteed” future values.\nAnother model which can be used to predict upscaling utilizes Shewhart control charts. These charts are used in business process management for controlling process quality based on statistical measurements. The idea behind control charts is the following: we have to take n repeated samples of i measurements and then calculate the range and the average of the each sample. The ranges are then put as data points in an “R-chart” and the averages are filled in an “X-chart”. Then we calculate the average μ and the standard deviation σ of all n data points in the R- and the X-chart. Then we do the following: we define some upper and lower bound for the data points which are considered as “natural” process limits and check if there are data points lying above or below these “control limits”. The upper and lower control limit (UCL and LCL) are proportional to the standard error which is σ divided by the square root of n. As a rule of thumb the UCL is defined as the average of all data points plus two times the standard error, while the LCL is the average minus two times the standard error. By calculating the UCL and LCL for the X- and R-chart, we can check if there are data points below or above the UCL.\nControl charts assume that if all data points lie within the UCL and LCL limits, the process will most likely continue as it is. It is said then that the process is “in control”. The interesting thing about control charts is that data points which lie outside the UCL or LCL can be used as indicators of process changes. If multiple points lie above the UCL, a growth trend can be indicated.\nWhen control charts are applied to VM utilization one must first define the sample size i and the number of data points n. Let us say that we want to measure average CPU utilization of the last 5 minutes. One could e. g. measure CPU utilization at 20 random points (i=20) in the time interval between 0 and 5 minutes. Then one can calculate the average of the sample as well as the range which is the difference between the maximum and minimum of the 20 values. As a result we get one data point for the X-chart and one for the R-chart. Then one should take n samples to populate the X- and R-charts. If we chose n=5, we can then compute the standard deviation, standard error and average of all samples. This values can be used to define the UCL and LCL for the process. As a next step we must define a decision criterion for when do we say that a process will result in a growth or decline trend. We could e. g. say that if 2 or more points lie above the UCL, a growth trend will occur in the future.\nThe upscaling is necessary, when either a process contains 2 or more data points above the UCL and the average is near some critical threshold (where the low performance VM reaches its maximum capacity) or when a process is in control but the UCL lies above the critical threshold. In both cases an upscaling is necessary, either because the next data points will probably lie above the threshold as a result of some growth trend or because the future data points can reach the threshold even when the process continues as it is.\nControl charts are a quite simple means to predict process changes. They have the following advantages:\n- Control charts are a relatively reliable indicator for future growth trends and can therefore indicate possibly critical growth trends very early.\n- They do not bias viewers towards giving “exact” predictions for future VM utilization values.\nDespite these advantages, control charts also have the following drawbacks:\n- They need a lot of parameter estimations (e. g. choice of n or i). If those parameters are not chosen well, control charts lead to many “false alarms” that indicate overutilization when there is none.\n- Control charts can predict growth trends, but they do not tell anything about the strength of the growth trend. Therefore they tend to either overestimate small process changes or underestimate large changes. They are insensitive to trend sizes.\nBoth models, the ARIMA and the control charts have some advantages and some drawbacks. Like many tools they are just as good as the person that uses them. Often it is advisable to test both tools first and then decide which instrument should be used for VM utilization prediction. But predicting future growth trends is still more an art than a craft. Therefore it can not be decided which method is “better”, but it is clear that both of them are better than do nothing about VM performance measurements."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:40eba19d-e791-4b10-ba99-c812f535f8d2>","<urn:uuid:37fff8a2-247f-49a1-9e51-d4bcf5894627>"],"error":null}
{"question":"Could you explain how memes function as both a form of entertainment and a tool for censorship resistance, particularly in cases of political criticism?","answer":"Memes serve a dual purpose as both entertainment and political resistance. On the entertainment side, they are comedic expressions that follow specific patterns and share storytelling characteristics, often building upon established cultural patterns. However, they can also function as powerful tools against censorship and political control. This is exemplified by the Winnie the Pooh memes targeting China's President Jinping - these memes use humor to challenge authority while providing critical commentary. When faced with such criticism through memes, authorities often respond with censorship measures, as seen in China's banning of Pooh-related content and the film Christopher Robin. This creates a dynamic where memes become a creative form of activism that challenges established frameworks, while censorship attempts to suppress such forms of expression and criticism.","context":["In recent years, memes have become a much used and discussed medium on the internet. Memes are expressed mannerisms, traits or ideas that are often reproduced through a specific pattern. They can be used to describe a vast set of expressions, be it videos, gifs, or edited pictures and texts. The term meme was coined by Richard Dawkins in his book The Selfish Gene (1976, 192), where it is described as a tool for forwarding culture from one generation to the next through learned and repeated behavior. While the meaning and use of the term still contains a large spectrum of possibilities, most memes share a comedic, entertaining and storytelling character.\nFrom the perspective of sociologist Erving Goffman (1967, 5–11), memes can in their nature be a response to our interactions with everyday life. While we constantly shape our social performance based on the people we interact with, we strive to maintain a coherence in our presentation. Most importantly, we avoid being met with humiliation or embarrassment, since it challenges the face we maintain for our performance. Losing face is equivalent to losing your stance, pride and conviction as you know it.\nPerformance- wise, memes follow and are influenced by one another, aiming to build upon specific patterns that have been established for meme culture. If a meme is performed, it is often readapted using the same script, or keying. Keyings in this sense can be seen as significant phrases or actions used to give meaning to the social interaction unfolding (Goffman, 1974, 43-45). If we look at a meme as a performance, a key point in that performance serves as the meme’s bottom line. For example, remaking the “distracted boyfriend / Man looking at another woman” meme requires the three main characters of the original meme, the “you know I had to do it to em” meme needs the person standing with their hands clenched together. These crucial elements serve as keys that convey the story the meme builds on.\nPhoto credit: Twitter / @Hong Kong Free press\nPhoto credit: Weibo\nPhoto credit: Reuters/Twitter\nWhat all these memes share is having pictures of Jinping recreated with Winnie The Pooh. The recreation emphasizes having Pooh posing in the same way as Jinping, often down to body language and facial expressions. Additional characters being mimicked in the pictures are common, but not necessary to recreate the meme, as the middle picture shows only Jinping in his car. The keyings in these pictures are Pooh himself- without the bear, the memes would lose their meaning. But Why Pooh? What is going on in this frame?\nFirstly, the keying is not only recreating Jinping, but rather his role as the president. Presenting the pictures side by side creates an inevitable comparison between the two characters, asking the audience to spot the difference. Is it Jinping the lovely bear or Pooh, the ruler of the communist party? The role of the president does not only seem to be swapped, however, but recast entirely. This can be interpreted through the positioning of the pictures; all three pictures show the president first and Pooh second. The order portrays the president as the original and Pooh as the follow-up. The meme’s performance tells us “here is the president”, while the keying states that “he is actually, or might just as well be Winnie The Pooh”.\nSecondly, the meme is comparing Jinping’s performance as president to Pooh. It’s not just the role as president, it’s how you do it that is shown through Pooh. Initially, Pooh is performing the exact same actions as the president. But Pooh’s characteristics create a humorous contrast; His round design and relaxed expression portray him as adorable, carefree and harmless. Instead of a big, eccentric black vehicle, Pooh drives a plucky, green toy car. Pooh is cute and giddy in his own right, but these traits paint a sharp contrast with the preestablished knowledge we have regarding the role of a president. The performance of a president is expected to be anything but carefree and nonserious, but rather stern, authoritative and determined. Mimicking the exact same actions as the president, Pooh traits in relation to Jinping turns the performance into a mockery.\nThe meme is telling the audience that it does not take the president seriously. And as it puts the president the face of ridicule, it challenges Jinping’s facework. With the meme becoming popular, shared and recreated, the audience are both reestablishing their image of the president based on the meme’s performance, as well as using it as an outlet to criticize him. This has inevitably lead to the president taking action to not lose his face; The Chinese government has repeatedly censored pooh-related memes on social media and stated that they are a threat to the dignity of their president. The above picture of Pooh and the president in their respective cars serves as the most censored picture of 2015. Winnie The Pooh has since been included in the governments blacklist, and as of 2018, the newly released Disney film Christopher Robin has not been allowed any screenings in China.\nWhether the President of China has lost his face or is working to maintain it might still be up for debate. However, it’s clear that through challenging his facework, the Winnie The Pooh memes have become a creative and insightful tool for critical thought regarding his presidency. Memes are as such a comedic reaction to our everyday-life, but also a form of activism to challenge the framework around it.\nAino Jauhiainen, kirjoittaja opiskelee kriminologiaa Helsingin yliopistossa\nGoffman, E. (1974). Frame analysis: An essay on the organization of experience. Harvard University Press.\nGoffman, E. (1967). Interaction ritual: essays on face-to-face interaction. New York: Anchor Books.\nDawkins, Richard (1989), The Selfish Gene (2 ed.), Oxford University Press, ISBN 978-0-19-286092-7,","- This article is about limits to free speech in publication and discussion. For other uses, see Disambiguation.\nCensorship is the systematic use of group power to broadly control freedom of speech and expression, largely in regard to secretive matters. Sanitization (cleaning or decontamination) and whitewashing (from whitewash) are almost interchangeable terms that refer to particular acts or campaigns of censorship or omission which seek to \"clean up\" the portrayal of particular issues and facts which are already known, but which may conflict with a presented point of view.\nIn a modern sense, censorship consists of any attempt to suppress information, points of view, or method of expression such as art, or profanity. Censorship is commonly used by social groups, organized religions, corporations and governments. There are also groups which specifically oppose censorship.\nTable of contents\nTypes of censorship\nCensorship can be explicit, as in laws passed to prevent information from being published or propagated (as in Australia, China, or Saudi Arabia where certain Internet pages are not permitted entry), or it can be implicit, taking the form of intimidation by government or even by popular censure, where people are afraid to express or support certain opinions for fear of losing their lives, or their jobs, position in society, or, in academia, their academic credibility. In this latter form it is similar to McCarthyism. These two forms (explicit and implicit) can be generalized to represent laws and government authority (explicit) and social forces or social persuasion (implicit).\nState Secrets and controversial history\nWartime censorship is carried out with the intention of preventing the release of information that might be advantageous to the enemy. Typically it involves obfuscation of times or locations, or delaying the release of information (e.g. the objective of an operation) until it is of no possible use to enemy forces. Mention of weapons and equipment (especially if newly introduced) is another favourite area for censorship. The moral issues here are somewhat different as release of the information carries a high risk of increased casualties among one's own forces and possibly loss of the overall conflict.\nA well-known example of sanitization policies comes from the USSR under Stalin, where publicly used photographs were often altered to remove people whom Stalin had ordered executed. Though past photographs may have been remembered or kept, this deliberate and systematic alteration of history in the public mind is seen as one of the central themes of Stalinism and totalitarianism. Hitler was considered a pioneer in creating propaganda material, all of which was, by definition, sanitized for the improvement of the Nazi image. Censorship is a form of sanitization. Specifically, censorship refers to a socially accepted policy of eliminating material rejected as harmful.\nThe content of school textbooks is often the issue of debate, as their target audience is young people, and the term \"whitewashing\" is the one commonly used to refer to selective removal of critical or damaging evidence or comment. The reporting of military atrocities in history is extremely controversial, as in the case of Nanjing Massacre, the Holocaust (or Holocaust denial), and the Winter Soldier Investigation, regarding the Vietnam War. Also, the theory of evolution has been questioned by many since it contradicts the beliefs of their religion and some textbooks were even requested to include disclaimers at the beginning of the book about it.\nIn each society the representation of its own flaws or misconduct is typically downplayed in favor of a nationalist or patriotic view. In the context of high-school level education, the presentation of facts and history greatly influences the interpretation of contemporary thought, opinion, and socialization. The legitimate argument for censoring the type of information shown is based on the inappropriateness of such material for certain younger age groups. The use of the \"inappropriate\" distinction is controversial, as well, as it can be used to enforce wider politically-motivated censorship.\n\"Censorship\" comes from the ancient Roman word \"censor\". The first reference to the term \"whitewash\" dates back to 1762 by a Boston Evening Post article. In 1800 the word was first used in a political context, when a Philadelphia Aurora editorial said that \"if you do not whitewash President Adams speedily, the Democrats, like swarms of flies, will bespatter him all over, and make you both as speckled as a dirty wall, and as black as the devil.\"\nThe term \"sanitization\" is a euphemism commonly used in the political context of propaganda to refer to the doctoring of information that might otherwise be perceived as incriminating, self-contradictory, controversial, or damaging.\nCensorship, unlike acts or policies of sanitization, refers to a publicly set standard, not a privately set (or government-enforced but unannounced) standard. Censorship does not attempt to cover up material made by an organization, but rather to restrict or abolish defined types of material produced by private citizens.\nCensorship is regarded as a typical feature of dictatorships and other authoritarian political systems. Democratic nations usually have far less institutionalized censorship, and instead promote the importance of freedom of speech.\nSome thinkers understand censorship to include other attempts to suppress points of view or ideas such as negative propaganda, media manipulation, spin, disinformation or \"free speech zones\". These methods, collectively, tend to work by disseminating misleading information or by preventing other ideas from obtaining a receptive audience.\nOthers point out the suppression of access to the means of dissemination of ideas by governmental bodies such as the FCC in the United States of America, the CRTC in Canada, or a newspaper that refuses to run commentary the publisher disagrees with, or a lecture hall that refuses to rent itself out to a particular speaker, or an individual refusing to finance that lecture.\nPrevention and bypassing\nData havens and decentralized peer-to-peer file sharing systems such as Freenet can be used to prevent censorship. A recent phenomenon for avoiding censorship and speaking directly to members of society is culture jamming, where individuals or non-conforming groups use large-scale corporate techniques to attack implicit domination and censorship through trivial or deliberately irrelevant messages. More traditionally, mass protests are a method for resisting unwanted impositions.\nInterestingly, the censorship of swear-words in the United States seems not always to extend to non-American pronunciations. Instead of shit, the Scots and Northern English variant shite may apparently be used, as may fook for fuck. (Note: this was witnessed on broadcast television in early 2004, before the FCC levied several highly-publicized fines.)\nCensorship around the world\n- Censorship in Australia\n- Censorship in Egypt\n- Censorship in France\n- Censorship in Germany\n- Censorship in Iraq\n- Censorship in Malaysia\n- Censorship in the Republic of Ireland\n- Censorship in Singapore\n- Censorship in Taiwan\n- Censorship in the Russian Empire\n- Censorship in the United Kingdom\n- Censorship in the United States\n- Censorship in Tuva\n- Internet censorship in the People's Republic of China\nOther types of censorship\n- Biblical censorship\n- Censorship by organized religion\n- Censorship in cyberspace\n- Censorship under communist regimes\n- Censorship under fascist regimes\n- Government censorship\nCensorship of media\n- Al Menconi\n- Bleep censor\n- Book burning\n- the Censored Eleven (banned Looney Tunes and Merrie Melodies cartoons)\n- Cindy's Torment\n- Entertainment Software Rating Board\n- Fahrenheit 451\n- Index Librorum Prohibitorum of The Roman Catholic Church\n- International Freedom of Expression eXchange\n- Joe Lieberman\n- John Stuart Mill\n- Lady Chatterley's Lover\n- Media controversy\n- MPAA rating system\n- Prior restraint\n- Pro-censorship lobbying\n- Production Code\n- Project Censored\n- Super Bowl XXXVIII controversy\n- Thomas Bowdler\n- TV Parental Guidelines\n- Video game controversy\n- Volkz.Net Against Censorship T-shirt (Not for profit organization)\n- Censorship of Curriculum Materials\n- The Right To Read: Censorship in the School Library\n- Challenges to and Censorship of School Guidance Materials\n- 'Ten things wrong with the media effects model' article by Prof David Gauntlett\n- National Coalition Against Censorship\n- International Freedom of Expression eXchange\n- Olympic Watch (Committee for the 2008 Olympic Games in a Free and Democratic Country) on censorship in China\n- Abbott, Randy. \"A Critical Analysis of the Library-Related Literature Concerning Censorship in Public Libraries and Public School Libraries in the United States During the 1980s.\" Project for degree of Education Specialist, University of South Florida, December 1987. [ED 308 864]\n- Burress, Lee. \"Battle of the Books.\" Metuchen, NJ: The Scarecrow Press, 1989. [ED 308 508]\n- O'Reilly, Robert C. and Larry Parker. \"Censorship_or Curriculum Modification?\" Paper presented at a School Boards Association, 1982, 14 p. [ED 226 432]\n- Hendrikson, Leslie. \"Library Censorship: ERIC Digest No. 23.\" ERIC Clearinghouse for Social Studies/Social Science Education, Boulder, Colorado, November 1985. [ED 264 165]\n- Hoffman, Frank. \"Intellectual Freedom and Censorship.\" Metuchen, NJ: The Scarecrow Press, 1989. [ED 307 652]\n- Marek, Kate. \"Schoolbook Censorship USA.\" June 1987. [ED 300 018]\n- National Coalition against Censorship (NCAC). \"Books on Trial: A Survey of Recent Cases.\" January 1985. [ED 258 597]\n- Small, Robert C., Jr. \"Preparing the New English Teacher to Deal with Censorship, or Will I Have to Face it Alone?\" Annual Meeting of the National Council of Teachers of English, 1987, 16 p.\n- (arguing that the English teacher should get advice from school librarians in preparing to encounter three levels of censorship:\n- rejection of adolescent fiction and popular teen magazines as having low value,\n- experienced colleagues discouraging \"difficult\" lesson plans,\n- outside interest groups limiting students' exposure. [ED 289 172])\n- Terry, John David II. \"Censorship: Post Pico.\" In \"School Law Update, 1986,\" edited by Thomas N. Jones and Darel P. Semler. [ED 272 994]\n- List of websites with known sanitization policies\nCleans-up mistakes made in speeches by US President George W. Bush, it also contains little contradictory information of current administration policies, and has deleted any reference to controversial Corporate accounting scandal figure Ken Lay, among others.\nIn a different example of sanitization, the U.S. State Department website will display material only when it supports administration policies. For example the website contains in full, the UN Security Council resolutions, which support the administration in its views of Iraq, but will not show such UN resolutions against Israel or the US.\n- In ancient Rome, censorship was the office or function of a censor.\n- The utensil for incense is a censer;\n- A device or organ that senses its environment is a sensor.\n- Whitewash is a type of inexpensive paint made from slaked lime (Calcium hydroxide, or Ca(OH)2) and chalk (whiting). Other additives have historically included milk, flour, salt, glue, water glass and soap. Whitewash cures through a reaction with carbon dioxide in the atmosphere to form calcite."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:15017874-1242-4dd8-9750-245138dfdf52>","<urn:uuid:3474ecfe-908a-424f-a353-55cfc2593bc5>"],"error":null}
{"question":"What percentage of California Urban Water Agencies' operating costs are fixed and don't decrease with reduced water usage?","answer":"About 80-90% of these agencies' operating costs are fixed, meaning they don't go down when water use declines. These fixed costs must be collected to operate their systems sustainably over the long term to pay for the infrastructure that brings water to the taps.","context":["In the midst of the pandemic and recession, the cost of delivering safe drinking water continues to rise across California, creating a crisis of affordability for water users and a revenue problem for water suppliers. We talked to Robert Shaver—board chair for the California Urban Water Agencies (CUWA) and general manager of the Alameda County Water District—about how the state’s largest public water agencies are thinking about this issue.\nPPIC: How big of an issue is water affordability for customers of CUWA agencies?\nROBERT SHAVER: Our 11 member agencies serve 26 million people and our research shows that about a third of our customer base falls into the state’s definition of low income (at or below 200% of the federal poverty level). These folks have a tough time making ends meet generally, and water is an added concern. They can minimize their bill by reducing water use, but that only goes so far. Water rates have been trending upward over many years, and given that in California access to safe drinking water is a human right, affordability is a big deal for our member agencies.\nPPIC: Why are rates rising? How can large agencies mitigate costs?\nRS: Water rates have been going up faster than the consumer price index for several decades. The reasons are many. There is a big need to update aging infrastructure—a lot of our systems were built in the 1950s and ‘60s and are reaching the end of their useful life. Agencies are also making investments to reduce the impacts of future shocks to their systems—earthquakes, for example, or power shutoffs, which have become more common as wildfire risk has grown. We’re also making investments to deal with new contaminants such as PFAS (also known as “forever chemicals”). And we have to ensure the reliability of existing water supplies, which are increasingly threatened by climate change and sea level rise. All of these things put pressure on water rates.\nOur agencies have tried various things to mitigate costs, and we always look for ways to be more efficient before raising rates. Many agencies have turned to renewable energy to reduce electricity costs, and we use new technologies to help us operate more efficiently. Our agencies have comprehensive conservation programs to reduce customers’ water use, which not only lowers their bills but also helps us put off the need to develop new supplies, and reduces the use of electricity and treatment chemicals.\nBut one challenge is that about 80–90% of our agencies’ operating costs are fixed—meaning that they don’t go down when water use declines. We have to collect enough revenue to operate our systems sustainably over the long term so we can pay for the infrastructure that brings water to the taps.\nPPIC: Are there ways to change the structure of water rates to provide relief to low-income customers, while not undermining suppliers’ financial health?\nRS: There are several ways to structure water rates—for example, fixed vs. volumetric charges, tiered vs. uniform rates, water budgets, etc. A few other models are being discussed, such as a “zero” rate tier that gives a minimum allotment of water for free or very cheap to low-income households. But ultimately, our rates must be tied to the cost of service, and no rate structure perfectly fits the issue of supplying water to the high number of low-income residents in our service areas. There are pros and cons to every rate structure, and no silver bullet solutions.\nOur agencies are exploring other creative approaches, such as putting some of the fixed costs onto property taxes instead of water bills, which could assist some low-income renters.\nThis pandemic has created an extraordinary situation for our agencies. We’re waiving late fees, expanding customer assistance programs, and allowing customers to self-certify eligibility for assistance. But these temporary changes may not be financially viable over the long term. In normal times, we work with non-paying customers to develop payment plans. CUWA agencies also have discount programs for low-income customers, but Proposition 218 restricts us on expanding these programs, so there’s only so much we can legally do.\nPPIC: How can the state or federal governments help in efforts to address affordability?\nRS: Water sales make up the vast majority of our revenue—more than 80% for most, and up to 97% for some of our agencies. So ensuring payment will always be important. What complicates the issue is that we can’t easily determine if nonpayment is a choice or a necessity—we need better data on this distinction. We could partner with the state and federal governments to get needed data, and maybe work with the state to determine who should be eligible for low-income assistance. Given Prop. 218 restrictions on local rate structures, it could be helpful to have another source of funding to fill the affordability gap.\nI’m hopeful that this crisis will prompt imaginative ways to deal with this issue. California has a history of being able to solve many big problems. So even though the affordability challenge continues to grow, I’m choosing to remain optimistic."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:0415c4e7-5aea-4a50-80d2-8ef3b255c11e>"],"error":null}
{"question":"Looking at historical military conflicts, how did the effectiveness of LARP weapons compare to the Springfield Trapdoor in actual combat scenarios? #MilitaryHistory","answer":"These items cannot be directly compared in terms of combat effectiveness since LARP weapons are purely for entertainment/game purposes, requiring specific safety features and weapon checks to prevent injuries. In contrast, the Springfield Trapdoor was a real military weapon that saw extensive combat use in major conflicts, including the Indian Wars (like the Battle of Little Bighorn), the fighting in Cuba at the Battle of Santiago, and the war in the Philippines at Caloocan. It was known as a dependable, hard-hitting, all-weather weapon that served as the US Army's standard arm for a quarter century.","context":["LARP Weapons and You\nAs the world of Last Hope is a dangerous one, most players should expect to carry a weapon with them. While you are under no obligation to fight when you are at a game (and oftentimes, the best option is to simply flee from combat if you’re outmatched), it’s always helpful to have something to defend yourself with. There are a staggering number of weapons available, however, so the choices can sometimes be a bit staggering – there are a LOT of companies and styles out there. This is intended as a resource for players looking to purchase their first weapon or someone considering expanding their armoury.\nThings to keep in mind\nFirst off, remember that we are a low-fantasy game. This means that weapons are meant to be a little more utilitarian, and you should avoid weapons which are overly decorated or fantasy-oriented. Another thing that you should remember is that you should try to have a weapon that fits with your character – if a weapon seems rather incongruous or out-of-place in the hands of a character, you may be asked to explain where you got it – this allows for a bit of leeway in terms of gear, but prevents things like Ulven warriors carrying jewel-covered scimitars or Vandregonian nobility with stone clubs.\nA few other points to keep in mind with your weapons:\n- If a weapon requires Herald training or approval before use, you must have gone through the training in order to use it. Just because you have purchased a weapon does not mean that the staff is obligated to allow you to use it – there are specific safety concerns with some of these weapons, and failing to adhere to the safety guidelines or training will likely result in a Herald asking you to put the weapon away.\n- Take the time to maintain your weapons! Don’t just leave them in your car – Calimacil weapons, while practically indestructible, tend to harden a bit if left in extreme temperatures, and latex degrades. After an event, wipe down your weapons, check them for defects, and (if they are made of latex) utilize silicon spray to maintain the latex.\n- Expect to go through weapon checks at every event you attend. Just because a weapon has passed before doesn’t mean that it will always pass – entropy is the master of us all, including LARP weapons. There will be a clearly-marked area for weapon checks, and if a weapon fails, the Herald doing a weapon check will explain why. If you take the time to maintain your weapons, then it is much more likely that they will continue to pass in every game.\n- Homemade weapons are generally inspected somewhat more stringently. We are working on guidelines for making your own weapons, but be aware that it takes quite a bit of time and effort to make a weapon that matches the quality (both in appearance and safety) of commercially-available weapons; this is not intended as a slight against anyone’s foamsmithing skills.\n- If you have questions about the appropriateness of a weapon, talk with a Herald. We like seeing new equipment and ideas, and (if it’s needed) we can post a few examples of passing/failing weapons from the various manufacturers.\nChoosing a weapon for your character\nThink about what you can comfortably wield, how it fits in with your overall costume design, and what you’re willing to spend when looking at weapons. Rather than limiting each class to only using certain weapons, Last Hope tries to encourage players to wield what they think their character would use – while certain skills to utilize weapons (such as two-handed weapons, utilizing two weapons, or using bows) cost more experience for certain classes, it would make no sense to say that a mage can’t use a two-handed sword because “Mages don’t use them”. Instead, think about how your character got their training – are they self-taught? Did they study under a swordmaster? Did they learn in the military? Or did you pick it up in-character over the course of the game?\nHere are some general suggestions for players – feel free to ask “why” we’ve made these recommendations in terms of the various races and weapons.\n- The Ulven tend to use simpler weapons, and prefer axes, spears, and straight swords. If a weapon has adornment, it is not the focus of the weapon – the Ulven see their weapons as useful tools for survival, not objects to have art lavished on them. If a weapon has decoration on it, then it most likely has a story – take the time to think about it!\n- Syndar tend towards staves, bows, and more ‘graceful’ weapons, such as curved swords. Decoration and embellishment is relatively common, although Feral Syndar tend to follow the Ulven pattern a little more closely. Expect a well-made weapon to have a long story, usually with connection to some named smith.\n- Humans have the widest variety of weapons, ranging the gamut from utilitarian to nearly priceless blades covered in delicate filigree. Most of the kingdoms have their own guidelines, but with the prevalence of trade, you can expect to find quite a few interesting examples.\nCompanies and Manufacturers\nThere are a staggering array of companies. Here is some basic information for people looking to sort out the various brands.\nA Canadian company, these weapons are fairly unique in that, rather than using latex-skinned foam, they use a proprietary foam mixture that has no latex in it. Quite a few players in Last Hope use these weapons – they are incredibly durable, made in a variety of styles, and tend to have a slightly more realistic weight than some of the other blades on the market.\nUnfortunately, they are also quite expensive. Their basic swords tend to start at around 80$, and they can easily reach over 200$ in price for a single weapon. Additionally, they tend to have quite a bit more “bite” than other LARP weapons, especially their two-handed weapons and blunt weapons. Generally, you will need to be careful of control when using a Calimacil sword.\nThere are a few weapons with specific concerns or special rules.\n- Currently, we are still reviewing the suitability of their line of stab-safe rapiers. The main concerns is the size of the tip – they are smaller than an eye socket, and so we are waiting to make the decision until we have thoroughly reviewed it and seen what training is required for safe thrusting. They are still legal weapons (although Asmoth is a little bit too high-fantasy: covering or painting may be required) if used for only cutting – if you are caught thrusting without Herald approval, you will not be allowed to use that weapon in the future.\n- Their Martial Arts line utilizes much denser foam, and so are unsuitable.\n- If you are planning on using any of their pole weapons (Telescoping Spear or Halberd), talk with a Herald.\nA German manufacturer, these are rather high-quality latex weapons. Some of their line tends towards the fantastic – ask a Herald if you have any questions. We’ve had no problems with them thus far, although some of the axe heads tend to be a little bit stiff at first.\nThey also manufacture a wide variety of shields – as above, make sure that the design you’ve chosen fits in with your character concept and the low-fantasy feel of the world. In general, the Medieval and Mercenary collections are your best bet in terms of design, the Gaelic and Ancient collections are fairly acceptable (although their spears are not thrust-approved), and the Chaos and Elven collections tend to be somewhat variable – the Bone swords, for example, are not acceptable for player characters.\nA Danish company, they produce two lines that are appropriate for Last Hope: Epic Armoury and Ready For Battle. Both of them are perfectly acceptable – there are a few weapons which are a little bit too high fantasy, but otherwise, they are excellent weapons. In general, most of the gear from their Dark Moon line is not appropriate, although there are some modern/sci-fi games in the area that they would be excellent for.\nIf you are looking to get started, the Ready for Battle line is a good starter weapon – they’re a little cheaper than other latex weapons, they work just fine, and there are a number of good designs available. Otherwise, they make a wide variety of good weapons – regular maintenance is recommended. At the moment, there’s an axe floating around that’s been used in semi-regular practice for two years that is just showing some minor signs of wear, so they’re quite durable.\nAn English company that does a mix of commercial designs and custom work, their gear ranges in the solid middle to high-end of latex weaponry. There are a few US distributors of their weapons, but their selection is somewhat limited.\nIf you’re not looking to get custom work done, then most of what they have available is simple straight swords. Most of their medieval/Renaissance-looking weapons are just fine. The latex seems to be a little bit harder than that of Iron Fortress or Forgotten Dreams, and their longer weapons have a little bit more whip than some people are used to, but they are also extremely well-balanced and make some gorgeous-looking swords. At the moment, no one in Last Hope has any experience with their custom work.\nAn excellent starter brand – while Eagle Flex is one of the cheapest “entry level” brands, they are also relatively good quality – their swords hold up well, handle decently, and come in a variety of styles. They make a few axes and daggers, as well as shields, all of which hold up rather well.\nA Canadian company that specializes in high-end latex weaponry. While they maintain some stock, most of the appeal lays in the customization available on their weapons – when ordering, you can select from a good number of options to customize a pre-existing design. The detail and handling of these weapons is fantastic, although they are rather pricey for latex weapons, easily exceeding the price of most Calimacil gear for the larger weapons.\nIf you are ordering a custom weapon, expect a wait time of about a month for delivery. As another note, they do not offer many items over 44″ in length, most likely due to construction techniques, so don’t expect to find a cleaving sword from them. They have a few weapons which are a bit high-fantasy, but the overall line is fantastic.\nMostly a custom shop based in the UK, they do some fantastic-looking work. At the moment, they do not have an overly-large selection of pre-made weapons, and the shipping costs to the United States can be pricey, but their work is quite beautiful, and they show a willingness to make a wide variety of weapons (Check out the LARP-safe Ship’s Wheel!)\nAt the moment, no one in Last Hope has any experience with these weapons. It is likely that they will pass, as they are in use at Empire, but until someone buys one for us to check, we can’t really comment on that.\nSo how do I choose?\nWhen looking at a LARP weapon, there are three things to consider:\n- How much am I willing to spend?\n- What is the overall look that I’m going for?\n- How do I want the weapon to handle?\nAs a general rule, higher-quality/more elaborate weapons will cost more money – a dagger is less expensive than a polearm, unless you really manage to get some odd custom work done. Simple weapons are often best when you’re just starting out – a lot of people in Last Hope got their start with the basic Eagle Flex Norman Sword, and you still see quite a few of them.\nThe handling is a major area to consider, however – different brands handle differently. In general, latex is quite a bit lighter than Calimacil’s blend – the Calimacil blades handle a little bit more ‘realistically’, but they’re also slightly slower to respond than the latex weapons. This is a double-edged sword, as they are better for making stable parries, but move a little more slowly when you’re attacking.\nSo why can’t I just make my own weapon/use a boffer?\nTheoretically? You can make your own weapons – they’re just much harder to pass, take a lot more time to get them looking good enough to pass the visual inspection, and it can end up being even more expensive than some of the custom companies if you’re not really sure what you’re doing at first. We’re beginning to experiment with the creation of some latex gear, but that is currently in its infancy.\nAs for the boffer question, it’s a matter of appearances. We’re looking to make a really immersive game for everyone involved, and the latex weapons just tend to look better. While it’s entirely possible to make boffers that pass the visual inspection (I would recommend looking up Wynar’s Fine Beating Implements for an example of that), it’s not very common, and we would like to avoid the immersion-breaks of the typical duct-tape boffer.\nWhat about ranged weapons?\nArrows are something that go through a stringent safety check before use, so they’re a little more regulated. At the moment, there are three main companies that make LARP-safe arrows or arrowheads.\nLive Action Products\nA domestic retailer located in Sheboygan, WI, they make modular arrowheads in a variety of colours. The foam is waterproof enough that covers are not required, although they are recommended, because the foam on the arrowheads can get pretty badly chewed-up by foliage, protrusions, and other sharp things if you’re not careful.\nAs with all modular arrowheads, it is required that you either lock the head to the shaft using an adhesive or artificially reduce the width of the slot by using duct tape – this is to ensure that the head does not accidentally unscrew in flight.\nA German company, they make two types of arrowheads – low-velocity open-cell arrows, and their standard, rounded head. At the moment, the low-velocity heads are the only ones approved for standard bows. The rounded heads are approved for use in IDV’s home-manufactured crossbows, all of which currently pass in Last Hope. Be warned, however – their crossbows tend to have some variability in performance, so you may need to check with a Herald before using one in a game.\nAnother German company that manufactures crossbows, arrowheads, and other archery equipment. We have no experience with their equipment, although we have heard complaints from some German LARPers that Hammerkunst foam is slightly more rigid than IDV’s. We have no basis to test this, but should anyone ever bring arrows manufactured by this company to an event, expect significant testing before approval.","This is a review by Randy L “Harv” Harvey of the Osprey Publishing LTD\nbook The “Trapdoor” Springfield – From the Little Bighorn to San Juan Hill\nby author John Langellier, and illustrators Steve Noon and Alan Gilliland.\nBODY OF THE TEXT\n** The Springfield trapdoor breech-loader began service as a cost-cutting conversion using the mountains of surplus rifle-muskets left over after the American Civil War. Adopted in 1865, for the next quarter-century both rifles and carbines based on this concept incrementally improved over time, and served as standard arms for the US Army. This dependable, hard-hitting, all-weather, single-shot weapon was carried by the US Infantry and cavalry on hundreds of frontier engagements from the Great Plains to the Mexican border and beyond. When the United States entered the world stage as an international power during the war with Spain in 1898, the tried-and-true veteran had its swansong. Written by a historian who has spent a half-century researching and writing about the post-Civil War US Army, this study weaves the narrative of the Springfield trapdoor’s development and impact into the larger story of westward expansion in the United States.**\n** Quoted from the back cover of the book.\nOsprey Publications Ltd has released The “Trapdoor” Springfield – From the Little Bighorn to San Juan Hill\nas Number 62 in their Weapon series\n. It is a softcover book with 80 pages. Included with the text are black and white photographs and color photographs, color illustrations, a cut-away view illustration, original military manual illustrations, an original schematic drawing, personal quotes, detailed captions and more. It has a 2018 copyright and the ISBN is 978-1-4728-1970-3\n. The book details the development, use and impact of the “Trapdoor” Springfield carbine in US Military service.\n- A breech-loader for the United States Army\n- From frontier constabulary to emerging global power\n- Conquering the Wild West\nThe text in the book is nicely written and well detailed. Author John Langellier did an excellent job of researching the Trapdoor Springfield and this is very obvious with the amount of details provide in his writing. Langellier discusses early trapdoor rifles and well as the Springfield. He details various models and the Trapdoor Springfield, improvements that were made, faults and benefits, new initiatives as well as later models and the type of rifles that were used in replacing the Trapdoor Springfield. Langellier provides very specific details about certain parts of the trapdoor such as the various model breechblocks, hammer and strike plates, sights, stocks and triggers. Another bit of information I found interesting are the step by step details an individual would need to follow in order fire the weapon. As well as the rifle itself Langellier also provides information on various accoutrements and accessories such as various bayonet types, cartridge boxes, cartridge belts, maintenance tools and even marksmanship badges. As well as specific rifle information Langellier provides excellent information on the various conflicts where the rifle was used such as the fighting in Cuba where the battle of Santiago is discussed and the war in the Philippines such as the fighting at Caloocan. A great deal of information is provided about the Indian Wars. Some of the various conflicts discussed are the Great Sioux War, the Wagon Box Fight, the Battle of the Little Big Horn, the Red River War, the Nez Pierce War, the fighting against the Apache and more. Included in the text are quotes from various individuals. One of them I found interesting was from an infantryman, William Murphy, stationed along the Bozeman Trail who spoke of the US Army’s tight-fisted budget when it came to issuing ammunition to the troops. The soldiers were given an ammunition allotment and if they lost ammunition, or used too much ammunition to target practice with, they were responsible for buying more from the Army to maintain their required allotment. Murphy was quoted as saying, “The government charged twenty-five cents per cartridge if the men were short” which was pricey for the soldiers considering that a private made roughly thirteen dollars a month in pay. As I read through the text I didn’t notice any spelling or grammatical errors. Grammar and spelling might not be an important factor to everyone however it is something that I take notice of and pass on my findings. I feel that if the text is well written then it shows that the author has taken the time to be a professional with their writing. Anyone wanting to add an excellent reference and history book on the “Trapdoor” Springfield to their personal library will be pleased with this very informative and interesting book.\nPlease refer to the scans that I have provided so that you can judge the text for yourself.\nThere are a total of 18 black and white photographs and 59 color photographs. The photographs range from wide angle photographs to close-up detailed photographs. Several of the photographs are of period illustrations. The majority of the photographs are of rifles as one can assume but there are also period photographs of the various carbines in use by the US Military. The majority of the photographs are clear and easily viewable, however a few have an out of focus look to them and some appear to be too dark, and others appear too light. This is typical for the discussed periods of history and consideration needs to be given to the fact that some of the photographs are several years old and the quality of the photographs is of no fault of the author and do not take anything away from the book. I appreciate the fact that there are several photographs of just the weapons themselves as opposed to photographs that feature the weapons in a broad generalized military photograph. In my opinion it makes it much easier to study the various weapons and their details. Author John Langellier stuck to the title of the book and chose subject specific photographs and did not include photographs that strayed from the main subject of the book. The majority, if not all, of the photographs will prove to be a wealth of information to the firearm enthusiast due to the details they contain.\nSome of the various rifles shown and discussed are:\n- Springfield Model 1873 carbine\n- Springfield Model 1870 carbine\n- Springfield.50 caliber Model 1865 Cadet rifle\n- Springfield Model 1879 carbine\n- Springfield Model 1886 full stock\n- Springfield Model 1890 carbine\n- Springfield Model 1884 carbine\n- Springfield Model 1892 30-40 Krag-Jargensen rifle\n- Model 1871 Ward-Burton rifle\nSome of the various carbine bayonets shown and discussed are:\n- Model 1869 trowel bayonet\n- Detachable triangular socket bayonet\n- Practice fencing bayonet\n- Sliding ramrod bayonet\nPlease refer to the scans that I have provided so that you can judge the photographs for yourself.\nThere are 4 color illustrations by illustrators Steve Noon and Alan Gilliland. The illustrations are of:\n- The Trapdoor Exposed.\n- A cut-away view showing the internal workings of a .45-70-405 Springfield Model 1873 carbine.\n- The Wagon Box Fight.\n- The illustration shows the conflict between the Plains Indians and the 18th US Infantry that took place on August 2, 1867.\n- Reno’s Skirmish Line, 1876\n- The illustration shows the initial skirmish line of troopers from the US 7th Cavalry during the fighting at the Battle of the Little Bighorn on June 25th, 1876.\n- The 20th Kansas Volunteer Infantry at Caloocan, 1899.\n- The illustration shows members of the 20th Kansas Volunteer Infantry engaging Filipino forces on February 11, 1899. (Refer to attached scan)\nThe cut-a-way view illustration was done by illustrator Alan Gilliland and the battle scene illustrations were done by illustrator Steve Noon.\nPlease refer to the scan that I have provided so that you can judge the illustrations for yourself.\nThere are 4 notes included in this volume and they are:\n- Editor’s Note\n- Artist’s Note\nThe captions are well written and explain the accompanying photographs and illustrations in great detail eliminate any doubt as to what is shown. The captions go into very specific detail as to weapons and the specific model shown, individual names, dates, location and conflict shown and other such pertinent information. I was very impressed by John Langellier’s captions as they are very helpful to the reader due to their detailed content as opposed to other captions I have seen that are very brief and lack detail.\nPlease refer to the scans that I have provided so that you can judge the captions for yourself.\nThis book was provided to me by Osprey Publishing Ltd. Please be sure to mention that you saw the book reviewed here when you make your purchase.\nAs with the other Osprey Publishing weapons series titles I was impressed with this volume. This is a very nice reference book that contains many close-up detailed subject specific photographs and illustrations and well detailed captions that all provide excellent information about the “Trapdoor” Springfield rifle. I would have no hesitation to add other Osprey Publishing titles to my personal library nor would I hesitate to recommend this book to others as it will be a welcome addition to one’s personal military reference library.\nUS $20.00 / UK £12.99 / CAN $27.00\nWinchester An American Legend\nThe Official History of Winchester Firearms and Ammunition from 1849 to the present\nChartwell Books, Inc.\nWarriors at the Little Bighorn 1876\nApache Warrior Versus US Cavalryman 1846-86\nSharpshooting Rifles of the American Civil War\nColt, Sharps, Spence, and Whitworth\nLittle Big Horn 1876\nCuster’s Last Stand\nThe Old West\nSearch inside The “Trapdoor” Springfield – From the Little Bighorn to San Juan Hill on the Osprey web site:\nLook inside The “Trapdoor” Springfield – From the Little Bighorn to San Juan Hill on the Amazon web site:\nhttps://www.amazon.com/Trapdoor-Springfield-Little-Bighorn-Weapon/dp/1472819705/ref=sr_1_1?ie=UTF8&qid=1529707572&sr=8-1&keywords=The \"Trapdoor\" Springfield\nLook inside the Kindle Edition of The “Trapdoor” Springfield – From the Little Bighorn to San Juan Hill on the Amazon web site:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:253a2044-6f11-4b71-ba86-335f035ddafd>","<urn:uuid:27daf464-4648-4831-92e1-b3617bf512f5>"],"error":null}
{"question":"How do eggshell powder and whey protein powder compare as dietary calcium supplements, considering their preparation methods and effectiveness?","answer":"Eggshells and whey protein differ in their calcium supplementation properties. Eggshells contain calcium carbonate, which is the main ingredient in many calcium supplements, and can be prepared by grinding clean eggshells into a fine powder for adding to shakes or smoothies. The shells contain 750-800 mgs of calcium each. Whey protein, while being a high-quality protein source, is derived through a different process involving cheese-making, where the liquid whey goes through specialty filters and ion exchange towers to create the final powder. While whey protein is primarily valued for its complete protein content and branched chain amino acids, it's not specifically noted for calcium content. Eggshell powder represents a more direct calcium supplement option, while whey protein's benefits are primarily protein-related.","context":["We eat them for breakfast, we hard-boil them for salads and we add them to our cake and cookie batter. Eggs are inexpensive and versatile. Yet, did you know you may be throwing away a useful part of the egg every time you eat one? The egg’s shell has almost as many uses as the egg’s contents.\nAs you continue to explore ways to re-purpose items in your home, here are some ideas you may not have considered for getting even more out of your eggs.\n1. Household cleaner. Household cleaner. You can make an easy and non-toxic abrasive cleanser by grinding some eggshells and mixing them with soapy water in a clean bottle or jar. Use the concoction to scrub your dirty pots and pans or your kitchen sink\n2. Facial mask. Grind clean, dried eggshells into a powder with a mortar and pestle. Now combine the shell powder with some whisked egg whites to create a skin-tightening facial mask. Let it dry on your face and then rinse with water. You can also use eggshells to help ease minor skin irritations. Add clean and dry eggshell pieces to apple cider vinegar and let the shells soak in the vinegar for two or three days. Then dab the mixture on irritated or itchy skin with a cotton ball.\n3. Drain cleaner. Keep a container of clean, dry ground egg shells near your sink to sprinkle periodically into your sink strainer. The fragments will help trap bits of food and, as they slowly break down, they will help clean your pipes of debris as well.\n4. Seedling starter. Place clean rinsed and dried egg shell halves into an empty egg carton. Poke a small drainage hole in the bottom of each shell. Next fill shells three-fourths of the way with potting soil. Add seeds. As your seedlings grow, you can transfer the shell pot to the ground or to a larger pot without transplanting. The shell is biodegradable.\n5. Garden fertilizer. An eggshell contains 750 to 800 mgs of calcium and other nutrients that will enrich your garden soil. Calcium is especially effective in combatting blossom-end rot in tomatoes. Before transplanting your tomato plants, place crushed egg shells in the bottom of the hole you have dug.\n6. Garden pest repellent. Slugs, snails and cutworms do not like to crawl over eggshells, so by placing eggshells in your garden, you can deter these soft-shelled pests without using toxic sprays or pellets. Deer also dislike the smell of eggs and can be kept away with the use of egg shells around your plants.\n7. Coffee taste enhancer. Before brewing, add crushed eggshells to your ground coffee. It helps take away any bitter taste.\n8. Mineral supplement. Egg shells contain calcium carbonate, which is the main ingredient of many antacids and calcium supplements. Try grinding clean egg shells into a fine powder and adding to shakes or smoothies to get an extra boost of calcium. You can also get the benefit of added minerals in your soups and stews, by adding crushed egg shells to your stock or broth.\n9. Dog Remedy. If Fido is prone to diarrhea, save clean dried egg shells to crush and sprinkle on your pet’s food. The stomach problems are usually cleared up within a day.\n10. House plant helper. Give your indoor plants the benefit of minerals found in egg shells by keeping crushed egg shells in a lidded jar to use in your watering can.\n11. Cat deterrent. If neighborhood cats tend to use your garden as a litter box, you can use crushed eggshells as a safe means of keeping them away. They don’t like to walk on it.\n12. Laundry detergent booster. Keep your whites from turning gray by placing a handful of clean, dry shells along with a couple of slices of lemon into a cheesecloth bag into your washing machine before laundering your clothing.\nAnd we will make it a true baker’s dozen by mentioning that scientists at Ohio State University have explored the use of egg shells as fuel for hydrogen cars. Since egg shells are rich in absorbent calcium carbonate, the patented process uses ground egg shells to filter out carbon dioxide, which is a by-product of hydrogen production.\nSince we’re on the subject of eggs, here are some other interesting facts about chicken eggs:\n- The average hen lays anywhere between 250 and 275 eggs per year.\n- Egg yolk color is determined by a hen’s diet. Yolk color will be more vibrant if the hen is fed grain with more yellow and orange plant pigments.\n- An egg shell, which contains pores that allow oxygen in and carbon dioxide and moisture out, comprises about 10 percent of an egg’s weight.\n- The egg white contains about 57 percent of an egg’s protein.\n- According to the Egg Safety Center, egg whites indicate freshness of the egg. A fresh egg has a cloudy white, while clear whites indicate an egg is aging. The Center warns against consuming egg whites that are pink or iridescent.\n- Spots of blood sometimes seen in an egg come from small ruptured blood vessels in the yolk. It does not indicate a problem with the egg, the Egg Safety Center says.\n- The color of the hen indicates the color of her eggs. According to the American Egg Board, there are no significant differences between different colored eggs.\nEgg shells are an often-overlooked but amazingly useful way to move further off the grid.","What is whey protein\nWhey protein is a high quality protein powder naturally made found in cow’s milk. Milk has two proteins: Casein (approximately 80%) and Whey Protein (approximately 20%). Whey protein is more soluble than casein and also has a higher quality rating. It is often referred to as the best protein as it is the most nutritious protein available and provide optimum nutrition for quick absorption into muscle.\nHow is whey protein made?\nA: Whey protein is a natural co-product of the cheese making process. Listed below is a brief description of the steps involved in making pure whey protein isolate.\n1. Fresh milk is tested, approved by Quality Assurance experts and pasteurized.\n2. The casein, or “curd”, and a portion of the milk-fat are separated out to make cheese.\n3. The remaining liquid whey goes through a series of fine, specialty filters to separate the whey protein from the lactose and other ingredients in the liquid whey.\n4. Concentrated liquid whey enters an ion exchange tower to further concentrate and purify the whey protein. Ion exchange is a gentle process and does not denature, or “break down”, the whey protein.\n5. Next, the product enters a drying tower to remove water.\n6. The final step is to package the pure whey protein isolate powder into various size containers for use\nWhy does the body need whey protein nutrition?\nA: Protein is an important nutrient needed by everyone of a daily basis. It is made up of essential and non-essential amino acids, which are the “building blocks” for healthy bodies. Protein has a number of different roles in the body including the following:\n* Repair body cells\n* Build and repair muscles and bones\n* Provide a source of energy\n* Control many of the important processes in the body related to metabolism\nIs whey protein good for athletes and people who exercise?\nBranched Chain Amino Acids A: Whey protein is a high quality, complete protein, with all the essential amino acids. Whey protein is also the richest known source of naturally occurring branched chain amino acids (leucine, isoleucine and valine). These are important for active individuals, individuals who exercise and professional athletes. The body requires higher amounts of branched chain amino acids during and following exercise as they are taken up directly by the skeletal muscles versus first being metabolized through the liver, like other amino acids. Low BCAA levels contribute to fatigue and they should be replaced in one-hour or less following exercise or participation in a competitive event. Many athletes consume a Protein beverage both before and immediately after exercise or an event to help repair and rebuild lean muscle tissue.\nIs whey protein compatible with a low-carbohydrate diet?\nA: Yes. Whey protein is not only compatible with low-carbohydrate diets it is an ideal choice. Be sure to select whey protein isolate which provides high quality protein without the carbohydrates and fat often limited in low carbohydrate diets.\nHow does whey protein compare to other types of proteins?\nA: Protein foods are not equal and can vary in a number of ways including the following:\n* Number and quantity of essential amino acids\n* Digestion and absorption rates\n* Fat content\nWhey protein is a very high quality complete protein with rich amounts of all the essential amino acids. Whey protein isolate is the purest form of whey protein, which is why it is absorbed so quickly and efficiently into the body.\nHow does whey protein compare to soy protein?\nHere are some of the differences between whey protein and soy protein.\n* Whey protein is a nutritionally complete protein. It contains bioactive ingredients, like immunoglobulins and lactoferrin, that help support the immune system.\n* Athletes prefer whey protein to soy protein due to its rich abundance of branched chain amino acids and its quick absorption rate. These are important to help repair and rebuild muscles after a workout or competitive event.\n* Whey protein has a fresh, neutral taste compared and will not change the taste of foods you add it to.\n* Whey protein does not contain isoflavones or any other components with potential hormonal effects\nI eat a lot of fish, chicken, eggs & soy. Why do I still need whey protein?\nA: Healthy diets should regularly include high quality, low fat sources of protein, like whey protein. Calories do count and you want to make sure that you are getting the most benefit from the calories you consume. Compared to other proteins, on a gram-to-gram basis whey protein isolate delivers more essential amino acids to the body but without the fat or cholesterol. Nutrition experts recommend a diet with a variety of protein foods but for optimal results make sure that one of them is whey protein.\nCan I get enough whey protein by drinking milk?\nMilk is a highly nutritious beverage however, it only contains about 1% of whey protein. In order to get all the benefits of whey protein, you need to take a concentrated whey protein powder, like whey protein isolate.\nAre all whey proteins the same?\nNo. There may be a major difference in the qualify of whey protein based upon the following factors:\n* Source of Milk\n* Production Method\n* Type of Cheese Produced\n* Individual Manufacturer Specifications\n* Added Ingredients\nIs whey protein easy to digest?\nWhey protein is a soluble, very easy to digest protein. It quickly enters the body to provide the important essential amino acids needed to nourish muscles and other body tissues. This is one of the reasons it is a common ingredient in infant formula and protein supplements for medical use.\nWhat are signs of a good quality protein supplement?\n– High quality whey protein\n– Easily soluble in liquids like milk, juice, etc\n– Easily digestible\nIf I’m lactose intolerant should I avoid whey protein?\nA: Individuals with lactose intolerance should select a pure whey protein isolate, which has less than 0.1 gram of lactose per tablespoon (20 grams). This is less lactose than the amount found in a cup of yogurt and research has shown that most people with lactose intolerance have no trouble taking this very small amount of lactose. Individuals with lactose intolerance should avoid whey protein concentrates as they usually contain lactose and the amount can vary greatly from product to product\nIs whey protein a good choice for vegetarians?\nA: Yes, whey protein is an excellent choice for vegetarians who include dairy products in their diet.\nWhat are the side effects of taking whey protein?\nA: There are no documented side effects provided a person does not have an allergy to dairy proteins or does not need to restrict dairy products for medical reasons. If you are allergic to dairy proteins please consult with a physician prior to consuming any type of whey protein.\nIs whey protein safe for pregnant women and children?\nA: Whey protein is a complete high quality protein and should be an acceptable protein source for healthy pregnant women and children, provided they are not allergic to dairy proteins. The second most abundant component in whey protein is alpha-lactalbumin, which is one of the main whey proteins in human breast milk. Infant formulas often contain whey protein, including special formulas for premature infants. Prior to taking whey protein, both pregnant women and parents of young children should consult a physician to be sure whey protein is right for them. Whey protein is even found in popular children’s nutritional drinks in India."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:abc06fad-4d0f-4899-ac1c-73fff852e199>","<urn:uuid:39fd5665-3618-4529-a4d3-adb05566f6ab>"],"error":null}
{"question":"¿Cuál sería la diferencia técnica fundamental entre stress testing y load testing desde una perspectiva de objetivos y metodología de implementación para un sistema de e-commerce? What are the key technical distinctions between stress testing and load testing in terms of goals and implementation methodology for an e-commerce system?","answer":"The key technical distinction lies in their objectives and execution approach. Stress testing focuses on analyzing system behavior under extreme conditions until breakdown, specifically testing recoverability and error handling when pushed beyond normal capacity. It follows a 5-step process: planning, automation script creation, execution, results analysis, and optimization - often requiring 3-4 cycles to achieve performance goals. In contrast, load testing aims to evaluate system performance under expected real-world traffic volumes, focusing on response times, throughput, and error rates under normal but heavy usage. Load testing follows a methodology of defining test scenarios, configuring load models with gradual user increases, execution and monitoring, results analysis, and iterative tuning. The key metrics and success criteria also differ - stress testing looks for appropriate error messages and recovery behavior, while load testing measures specific performance metrics like response time and throughput under expected loads.","context":["Stress testing is a non-functional form of testing that verified the stability & reliability of the system. This test mainly determines the system on its robustness and error handling under extremely heavy load conditions. It allows the testers to check the endurance power or limit of the system can withstand before breaking down.\nStress testing is also known as endurance testing. It also effectively checks the system’s response and error management under stress and extreme conditions.\nIt helps in finding the following errors in application software:\n- Memory Leak.\n- Data corruption and loss.\n- Loss of resource issues.\n- Timing bugs and synchronization issues.\n- Request priority issues.\nNeed for Stress Testing\n- During festival time, an online shopping site may witness a spike in traffic, or when it announces a sale.\n- When a blog is mentioned in a leading newspaper, it experiences a sudden surge in traffic.\nGoals of Stress Testing\nThe goal of stress testing is to analyze the behavior of the system after a failure. For Stress testing to be successful, the system displays an appropriate error message while it is under extreme condition.\nTo conduct Stress Testing, sometimes, massive data sets may be used which may get lost during Stress Testing. Testers should not lose these security-related data while doing stress testing.\nThe main purpose of stress testing is to make sure that the system recovers after failure which is called as recoverability.\nImplementation of stress testing with the situation\nA software application created for e-commerce application is stress-tested in the following scenario:\nStress testing checks the application response of the user with increased web traffic. The website is tested to work with more than expected users. Testers create virtual users to execute activities such as adding products, viewing products, payment. The number of virtual users is increased until the entire application breaks down. It helps in identifying performance issues, recovery, and mechanism.\nHow to do Stress Testing?\nStress Testing process can be done in 5 major steps:\n- Planning the Stress Test. Here you gather the system data, analyze the system, define the stress test goals\n- Create Automation Scripts: In this phase, you create the Stress testing automation scripts, generate the test data for the stress scenarios.\n- Script Execution: In this stage, you run the Stress testing automation scripts and store the stress results.\n- Results Analysis: In this stage, you analyze the Stress Test results and identify bottlenecks.\n- Tweaking and Optimization: In this stage, you fine-tune the system, change configurations, optimize the code with goal meet the desired benchmark.\nLastly, you again run the entire cycle to determine that the tweaks have produced the desired results. For example, it’s not unusual to have 3 to 4 cycles of the Stress Testing process to achieve the performance goals.\nTypes of Stress Testing\nThe client-server model is a distributed model where the workload is divided between resource or service providers – servers and requesters for service-client.\nThe role of stress server is to distribute a set of stress tests to all stress clients and track on the status of the client. After the client contacts the server, the server adds the name of the client and starts sending data for testing.\nMeanwhile, client machines send signal or heartbeat that it is connected with the server. If the server does not receive any signals from the client machine, it needs to be investigated further for debugging.\nApplication stress testing\nThis testing concentrate on finding defects related to data locking and blocking, network issues and performance bottlenecks in an application.\nTransactional stress testing\nThis method focuses on testing the smoothness of transactions between two or more applications. It improves business transactions.\nExploratory Stress testing\nThis method tests the application across multiple systems running at the same server. In this method, a tester checks the system using different parameters, which is unlikely to happen in a real scenario. This method tests the system response when a large number of users are logged in the system. Exploratory testing also finds the possibilities of system or software failure.\nThis method also checks the activities where the website is not able to access the database. Sometimes, the database is also stressed by adding more than it can store.\nSystematic Stress Testing\nThis method test many systems that are on the server. It enables the testing team to find and report defects where data of one application blocks the flow of application software.\nStress testing’s objective is to check the system under extreme conditions. In system resources such as Memory, processor, network, etc., and checks the ability of the system to recover back to normal status. It checks whether the system displays appropriate error messages while under stress.","Load testing is a vital technique for evaluating an application‘s performance under real-world user traffic. This comprehensive guide will explain what load testing is, why it‘s important, and how to effectively execute load tests during the software development lifecycle.\nWhat Exactly is Load Testing?\nLoad testing simulates multiple users accessing an application simultaneously to determine how the system holds up under different traffic volumes. It helps identify performance bottlenecks before launch so developers can optimize the application.\nThe goal of load testing is to understand application behavior under expected real-world loads. Load testing tools generate scripted user interactions that mimic actual usage patterns. For example:\n- Users browsing product pages\n- Adding items to the shopping cart\n- Checking out and making purchases\nBy scripting common user workflows, thousands of virtual users can be simulated to replicate anticipated traffic volumes and usage spikes.\nAs these virtual users interact with the application, response times, error rates, throughput, and other key metrics are measured to spot performance issues.\nLoad tests simulate multiple concurrent users to model real-world traffic.\nWell-executed load tests follow a systematic methodology to uncover 90% of potential performance problems before launch. Fixing these early in the development lifecycle is much cheaper and faster.\nWhy is Load Testing Absolutely Critical?\nFor modern web and mobile applications, fast performance and stability are essential to user satisfaction. Even minor hiccups frustrate users and damage brand reputation.\n- 49% of users expect a web page to load in 2 seconds or less. (Akamai research)\n- A 1 second delay in page load time can reduce conversions by 7%. (Google research)\n- 80% of users will abandon a slow mobile app after only 2 tries. ([Dimensional Research](https://www. dimensionalresearch.com/resources/solutions-in-action/improve-your-mobile-app-experience.white-paper.pdf))\nWithout load testing, companies risk expensive production outages and frustrated users once the system goes live. Load testing provides confidence the application can withstand peak traffic volumes.\nCost of Performance Problems\nPoor performance under load results in lost revenue, lower productivity, and increased infrastructure costs.\n- Amazon estimated that a 1 second delay cost them $1.6 billion in sales each year. (Akamai)\n- 80% of IT professionals reported performance issues reducing workforce productivity. (AppDynamics Survey)\n- On average, performance problems lead to 100 additional servers being provisioned to compensate. (Netflix)\nThis data shows how critical performance is, and why load testing forms a key line of defense.\nHow Does Load Testing Actually Work?\nLoad tests work by simulating concurrent users carrying out typical activities on the application while measuring performance. Let‘s break down the steps:\n1. Define Test Scenarios\nRealistic user workflows are scripted, for instance:\n- Browsing product category pages\n- Searching for items\n- Adding products to cart\n- Checking out\n- Logging in\n- Updating account\nDifferent scenarios can be combined to model complex user interactions.\n2. Configure Load Model\nThe load model defines the number of concurrent virtual users in each scenario, gradually increasing to peak levels:\n|Step||Virtual Users||Load Level|\nSample load model with an increasing number of virtual users.\n3. Execute Test & Monitor\nThe load test runs each scenario under the target user load while performance is measured from the client perspective. Backend infrastructure is monitored to detect bottlenecks.\n4. Analyze Results\nResponse times, error rates, throughput and other metrics are analyzed to uncover optimization opportunities. Issues are ranked by severity.\n5. Tune & Retest\nDevelopers optimize bottlenecks between test runs. Tests are executed iteratively until performance goals are met at peak load.\nExecuted methodically, load tests surface issues for resolution before launch. Retesting ensures enhancements actually improve performance.\nSummary of the core load testing methodology.\nCritical Load Testing Metrics\nVital load test metrics include:\nResponse time – The time for application pages and API calls to fully load. High response times indicate bottlenecks. The 95th percentile response time filters outliers for a stable view.\nThroughput – Requests per second successfully handled by the application. Low throughput signals potential capacity issues.\nError rate – Percentage of failed requests under load reveals stability problems.\nResource utilization – Memory, CPU, and network consumption highlights overloaded system components.\nAnalyzing these metrics identifies optimization opportunities to resolve before launch. The metrics are measured from actual user locations for accuracy.\nLoad Testing Best Practices\nFollow these expert best practices for successful load testing:\n- Define objective success criteria – Quantifiable goals for response times, throughput, and errors that indicate acceptable performance. Know precisely when a test passes or fails.\n- Model realistic usage – Base test scenarios and data on real user workflows captured in production monitoring and analytics. Achieve authenticity.\n- Leverage real user data – Seed test data from production logs and A/B testing variants to achieve real-world conditions.\n- Ramp load gradually – Steadily increase users over multiple steps to uncover issues as load builds. Avoid unrealistic spike testing.\n- Run long duration tests – Execute tests over hours or days to uncover memory leaks and performance degradation over time.\n- Monitor infrastructure – Watch key application and infrastructure metrics to isolate the true bottlenecks.\n- Iterate and retest – Optimize priority issues between runs for continuous improvement until success criteria are met.\nFollowing these best practices ensures your load tests accurately model real-world conditions and provide actionable results.\nSummary of key load testing best practices.\nCloud Load Testing Services\nCloud load testing services like BlazeMeter, LoadNinja and Neustar provide on-demand load generation without needing to build complex internal test environments.\nThese services offer key capabilities:\n- Instant provisioning of thousands of load test agents globally\n- Script recording and test configuration through intuitive web UIs\n- Detailed analysis of end-user response times, errors, and infrastructure metrics\n- APIs and CI/CD integration to enable automated testing\n- Pay-per-use pricing instead of large fixed costs\nCloud services enable running full scale tests on-demand which minimizes infrastructure costs. Teams can focus on generating insights rather than maintaining test environments.\nOpen Source & Commercial Tools\nFeature-rich open source and commercial load testing tools are available for on-premises usage, including:\nJMeter – Leading open-source tool featuring easy script recording, thread groups to model user loads, and plugins to extend functionality.\nGatling – High performance open-source tool with a domain-specific scripting language optimized for load testing.\nLoadRunner – Mature commercial tool from Micro Focus with comprehensive scripting, correlation, load generation, and analysis.\nNeoLoad – Commercial tool with scriptless recording, CI/CD integration, advanced analytics, and geographic load balancing.\nSelf-managed tools provide complete control over test environments and advanced scripting capabilities for complex scenarios.\nIntegrating Load Testing into CI/CD Pipelines\nTo prevent performance regressions, load tests should be integrated into CI/CD pipelines. Lightweight tests can execute on every build, while full end-to-end tests run nightly or weekly.\nKey practices include:\n- Run API-level load tests per build to validate backend. Mock dependencies.\n- Execute automated UI tests against preview environments on every merge.\n- Schedule full scale load tests weekly to validate complex flows.\n- Set load test success criteria per environment, increasing until production.\n- Leverage canary analysis to validate changes in production with a percentage of users.\nThis testing shift left approach catches regressions early and provides safety nets at each stage. Engineers get rapid feedback to address issues.\nGeographically Distributed Load Generation\nApplications with worldwide users must be tested from different geographic regions to accurately simulate real usage. However, building global test environments is extremely complex and expensive.\nCloud load testing services help solve this by dynamically provisioning load generators near target users. This provides accurate network conditions and latencies for each region.\nResidential proxies also assist with geo-distributed load testing. By routing traffic through diverse residential IPs, tests can simulate users in different countries from a central location. Leading tools integrate proxies to easily model regional user populations.\nRealistic Load Testing with Residential Proxies\nTo generate realistic load, the IP addresses used during testing must mimic actual user traffic:\n- Residential IP addresses – Addresses assigned to home ISPs rather than data centers\n- Geographic diversity – Mix of IP addresses from different cities, states and countries\n- IP rotation – Frequent rotation of source IP addresses per thread\n- Unlimited IPs – Large, constantly changing pool of IP addresses\nResidential proxies satisfy these requirements by routing traffic from real user devices through to the application. This contrasts with data center proxies with easily identified IP ranges:\n|Residential Proxies||Data Center Proxies|\n|Broad geographic distribution||Limited regions|\n|Random IP rotation||Static allocations|\n|Unlimited IP pool||Constrained, fixed IPs|\n|Residential ISP IPs||Data center IP ranges|\nIntegrating residential proxies into load tests is straightforward. Leading tools like BlazeMeter, Loader.io, and JMeter have built-in integrations for proxy rotation.\nThis enables easily simulating geo-distributed users during local load testing. More realistic test traffic improves the validity of test results.\nResidential proxies enable realistic load testing from a central location.\nLoad Testing for Mobile Apps\nThere are several techniques available to load test native and mobile web applications:\n- Real devices – Install and test apps directly on racks of real smartphones and tablets. Provides high fidelity results but requires significant infrastructure.\n- Emulators – Load generators leverage integrated Android and iOS emulators to simulate mobile user traffic. Limited fidelity but low overhead.\n- API testing – Test backend APIs under load to validate server-side performance and infrastructure scaling. Lightweight compared to full UI testing.\nA combined strategy is optimal:\n- Test APIs every build to validate services and integration points\n- Run emulators daily to verify app UI and user flows\n- Schedule quarterly real device tests to benchmark performance\nThis provides comprehensive confidence across backend services, app UI, and end-user experiences.\nCommon Load Testing Mistakes to Avoid\nWhile load testing is valuable, common mistakes diminish its effectiveness:\n- No clear success criteria – Key metrics and thresholds for minimum acceptable performance left undefined\n- Unrealistic usage modeling – Not matching actual workflows and access patterns skews results\n- Ramping up load too quickly – Causes artificial spikes unlike gradual production usage ramps\n- Not monitoring infrastructure – Missing server health metrics prevents isolating true bottlenecks\n- Short test durations – Fails to uncover long-term resource leaks and performance degradation\n- Minimal result analysis – Lack of diagnosis into root causes of issues\n- Late cycle testing – Outages occur because insufficient time remains to fix issues\nAvoiding these pitfalls ensures your load testing practice provides maximum value and actionable insights.\nConclusion and Key Recommendations\nLoad testing confirms your application performs well under real-world user volumes before launch. Follow these recommendations for effective load testing:\n- Integrate into CI/CD pipelines to catch issues early and prevent regressions\n- Model authentic user behaviors based on production analytics for accurate results\n- Validate both front-end and back-end using real browsers, emulators, APIs, and services\n- Utilize load testing best practices like gradual load ramps, long runs, and iterative optimization\n- Leverage cloud services and tools for flexibility and rapid provisioning of large test environments\n- Employ residential proxies to simulate geo-distributed users from a central location\n- Analyze and retest frequently to continuously improve performance at each load level\nAdopting load testing best practices prevents performance problems and ensures your software can deliver great user experiences at any scale. The downstream savings from avoiding launch issues are enormous.\nHopefully this guide provides a comprehensive overview of the importance of load testing, how to execute it effectively, and key tools and techniques for success. Please let me know if you have any other questions!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:ad673d7a-aa0b-43c4-baec-75991f86e380>","<urn:uuid:7e066c80-2454-4b34-b781-8afdb882e0ee>"],"error":null}
{"question":"When comparing the ideal installation height for satellite dishes versus TV antennas, what role does signal quality play in each case?","answer":"The relationship between height and signal quality differs significantly between satellite dishes and TV antennas. For satellite dishes, height is not crucial for signal quality - a dish will receive exactly the same signal strength at head height as it would on top of a skyscraper, as long as there is a clear line of sight to the satellites. The main consideration for satellite dish height is clearing obstacles like trees and buildings. In contrast, for TV antennas, height is usually very important for achieving good signal quality. This is evidenced by the recommendation to 'go high' when trying to improve antenna reception, with placement near or in windows at higher locations generally resulting in better signal strength.","context":["Best Height For Satellite Dish\nIf you’re intending on installing your own satellite dish, it’s important to know what height that you should install this off the ground.There is no hard and fast answer to this as this would depend on whether you wanted the dish accessible or hidden away out of sight, or the height of your neighbouring buildings and trees. I discuss the pro’s and cons of installing your satellite dish in different locations and hopefully help you come to your own informed decision as the best place to install it.\nIdentify The Dish Direction First\nBefore I discuss actual positions to install your satellite dish, it’s important to understand where the the satellite dish alignment and where it needs to point. I will not go into this at great detail as I already have done in an older blog. Essentially there are two alignments that concern us with regards to picking the best height for your satellite dish:\nElevation – This is the angle that the dish points up to the Sky.*\nAzimuth – This the East/ West direction that the dish points.\n*Please note that offset satellite dishes that are the most common do not actually physically point to the Sky like prime focus satellite dishes do. The back reflector angles the signal down to the LNB so it doesn’t actually look like it’s pointing direct to the satellite.\nI recommend using Dishpointer to establish roughly where your dish needs to point. You simply enter in your location and the satellite that you wish to obtain a signal from and it will give you your azimuth and elevation angles. If you do not know which way to point your satellite dish I recommend reading our older blog on the subject.\nThe Satellite Arc\nThe geostationary satellites that we use for satellite TV broadcasts orbit around the equator. From the Northern Hemisphere these will be south, in the Southern Hemisphere these will be north. If you’re on the equator and reading this these will literally be straight up above your head.\nWhen looking either north or south depending where you are the satellites will form an arc, with the satellite closest to due south or north appearing highest and the further that you drift East or West the lower it will appear in the Sky. It’s important to get a basic understanding of the satellite arc as it appears in the Sky as this will help you determine the best height for your satellite dish.\nHeight For Best Signal From Sat Dish\nAs long as you have a clear line of sight from the satellites in space, the satellite signal strength being received will be exactly the same at head height that it will be on the top on a sky scraper. In this respect satellite dishes do not work like conventional TV aerials where height is usually very important to get a good signal.\nLike I said line of sight is very important, so the satellite dish height that you choose must be sufficient to clear any nearby obstructions like trees and buildings. Using the azimuth/ elevation angle that you got when you entered your information into Dishpointer you should be able to visualise what obstructions will be a problem. Use a compass/ inclinometer if you’re not sure.\nInstalling Satellite Dish For Ease of Access/ Maintenance\nIf you’re installing your own satellite dish you may want to install it lower down on a south facing wall for maintenance reasons and to make the installation a bit easy. Providing that you have line of sight remembering that the signals come down at an angle this may be the best option for you. If you’re installing a motorised dish or a satellite broadband dish that is very big and heavy. I would prefer to install this lower down. If anything goes wrong with it of if you’re attempting to align it without professional satellite alignment equipment. This will make it far easy to alter the align and replace any parts if need be.\nThe downside to this is that the installation won’t be very discreet if that’s important to you and it may also not be possible. If you have nearby trees or buildings that are preventing the satellite dish from seeing the satellites you will most likely need to install it higher up the wall.\nInstalling Dishes To Clear Trees & Neighbouring Buildings\nIf you have trees or buildings directly in the direction that you need to point your dish, you will need to get high enough to clear these taking into consideration the angle that the signals come down from the satellites. This will involve installing the satellite dish higher up the wall or failing that onto the chimney stack if you have one as this usually get you a lot more height. It’s best when securing satellite dishes to chimney stacks to not use expansion bolts but a chimney lashing bracket instead. You could use a chimney ratchet strap to achieve the same result, but is easier to install. If you do not have a chimney stack dishes can be clamped onto aerial masts and brackets. I recommend using a 2” diameter mast for this task as you will want to keep the mast wobbling to an absolute minimum.\nHow Low Can A Satellite Dish Be Installed?\nSatellite dishes can actually work at ground level if required. As you as you have a clear line of sight of the satellites it will work the same than as if it was on the wall or chimney with no signal loss. The downside of going low with your satellite dish is that people could walk in front of it and block the satellite signal albeit temporarily. If you have children, pets or just clumsy people in your family (like I do) you also run the risk of the satellite dish being knocked offline which could cost you a visit from a satellite engineer.\nDishes Mounted On Poles In Garden\nFollowing on from the concept of installing satellite dishes at ground level, satellite dishes can be mounted in down the end of your garden or on your patio if you’re struggling to get a signal on your main property, do not want to drill holes into your walls – like with listed buildings where you’re not allowed to do this or just because you want to keep the thing out of site. There are a couple of ways of doing this.\nPatio mount secured to paving slab – Your satellite dish can then be clamped direct onto this. If you’re sitting this on your lawn I recommend digging a square out of your lawn/ earth with a spade the same shape as this slab. This will ensure that it remains in position. You may also want to put some sand/ cement around it to hold it in position.\nConcrete Pole Into Ground – This is suitable for larger dishes. I have done this a couple of times for satellite broadband dishes where the customer has wanted to keep these out of view. The downside of doing it this way is that it requires two separate visits. One to concrete the pole into the ground and another to mount the satellite dish when the concrete has set.\nDish Installed Higher Up To Keep Discreet\nIf you want to keep your satellite dish installation discreet. You may find that installing your satellite dish higher up will help achieve this, on the chimney stack perhaps. If you’re clever with the positioning of the satellite dish you may be able to hide the satellite dish behind the chimney stack itself when looking from ground view.\nOptimum Height For Sat Dish – Out Of Reach\nI hope that by now you can understand that the height isn’t necessarily the most important things when it comes to satellite reception. I would suggest however that an optimum height (if there was such a thing) would be on the wall but out of reach from the ground. This will stop anyone accidentally banging it and knocking it offline and it would less likely to ever be vandalised. This is particularly important if this dish is installed over public highways as I have attending call outs where the customer was unfortunate enough to have their dish vandalised. I have even attended a job where the customer had their satellite dish stolen!\nSatellite Dish Installation Questions – In The Blog Comment Section Only!\nIf you have any questions regarding the best satellite dish height or location please post your questions and comments in the BLOG COMMENT SECTION ONLY PLEASE. Providing that you post your questions here I will be delighted to help when I can, I appreciate your patience when you do post an enquiry as I may not be able to respond as fast as you may like. Please DO NOT CALL OUR TELEPHONE LINES, these are reserved for CUSTOMERS ONLY and we do not offer any free technical advise and I do not give away my personal mobile phone number as is frequently asked. Please also DO NOT E-MAIL and please DO NOT FILL OUT OUR WEBSITE CONTACT FORMS, again these are intended for customers only as you will either not receive a response or you will receive one asking you to post your comment in the blog comment section. By doing this gives me a central location to answer all the questions and everyone reading the blog will get the benefit of the question asked and the answer given.\nShould I Install Network/ Data Cabling In My Home?\nRead this before installing network/ Ethernet cabling in your home, read this for all you need to know. Inc info on data cables/ switches.\nFreesat Wideband LNB System, Benefits & Solutions\nAll you need to know about the new third-generation Freesat 4K wideband system, inc info on LNB needed, common problems & solutions.\nBest Brands/ Manufacturers of TV Wall Brackets/ Mounts\nRead this for my best & recommended brands/ manufacturers of TV wall brackets & mounts. Written by professional AV installer.","Are TV antennas affected by weather?\nIf you do not have cable, your digital television reception can be affected by storms and high winds. Antennas intercept the TV signal, which travels as a low-energy electromagnetic wave. … On calm days, trees are not much of a problem but on windy days, strong winds cause the trees to sway.\nWhat weather conditions affect TV reception?\nTV and radio signals, both analogue and digital, can be affected by atmospheric conditions, including high air pressure (which brings fine weather), heavy rain or snow. On Freeview, this may result in temporary pixelation or viewers receiving signals from outside of their area (or even from other countries).\nWhy do I lose my TV signal when it rains?\nThe negative effect that rain has on satellite signals is called rain-fade. This refers to the absorption of a radio/ microwave signal by atmospheric conditions, such as rain, ice, snow. … Also, the higher the frequency, the greater the effect of rain will be on the signal.\nWhy does my TV antenna signal go in and out?\nA: The reason the signal goes in and out is most likely due to “multipath issues.” When a TV signal travels, it bounces off things it hits (such as mountains and high buildings), and those bounces can reach your antenna, confusing your TV’s tuner.\nHow can I make my antenna signal stronger?\n5 Tricks for Getting the Best Possible Reception with Your Indoor…\n- Find out where the broadcast towers are in your area. Aiming your antenna at TV transmission towers can improve reception. …\n- Place the antenna in or near a window. …\n- Go high. …\n- Test different antenna placements.\nDoes weather affect antenna signal strength?\nTo a lesser and far more variable degree, weather can affect your OTA TV signal – especially severe fog/rain/snow, and large temperature swings – as the signal reflects off moisture in the atmosphere.\nDoes high pressure affect TV reception?\nHigh air pressure can affect TV waves and interfere with transmitter signals, potentially affecting 40 million viewers. … High air pressure can bend or reflect TV and radio waves, which interferes with the signals sent from transmitters to aerials, a spokesperson for Freeview said.\nCan hot weather affect TV reception?\nIn warm weather, the sun heats up the ground and the warm air gets trapped underneath the colder air above. This creates a layer that is in effect a mirror for television signals – meaning signals you would not normally receive can cause pixilation, garbled sound and even a blank screen.\nWhy is my TV suddenly saying no signal?\nCheck the cable connections between the TV and your video device or receiver. Change the channel or try a different input device or movie. The received signal may be weak. If your TV uses a cable or satellite box, you may need to contact your service provider for further assistance in improving the signal strength.\nCan heavy rain affect satellite signal?\nIt’s all down to the weakening of the satellite signal, Avanti say “as it passes through raindrops, fog, heavy cloud cover or high winds”. In a nutshell, rain and other heavy weather conditions can absorb energy from the signal, which in turn lowers the quality of the satellite service."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:3f29ab14-263f-48a6-9966-ead4b6f4d098>","<urn:uuid:52eca993-2402-41d3-b6c1-c76c8fbd1eb0>"],"error":null}
{"question":"Rio Tinto和Haughton陨石坑哪个更适合作为火星模拟环境？Please compare their key features as Mars analogs.","answer":"Both sites have valuable features as Mars analogs, but they offer different advantages. Rio Tinto has jarosite minerals that match those found on Mars and a distinctive red terrain similar to the Martian surface. Meanwhile, Haughton Crater offers more diverse Mars-like features including impact crater geology, ground-ice permafrost, ancient lake sediments, periglacial formations, and valley networks resembling those on Mars. Additionally, Haughton's polar desert environment may be similar to early Mars conditions. While Rio Tinto is useful for testing specific equipment like spacesuits and rovers, Haughton appears to present a wider array of Mars-analog features making it particularly valuable for comprehensive Mars research and mission preparation.","context":["Technology tested for future manned missions to Mars\n\"This is Mars in Europe,\" says Gernot Groemer, from the Austrian Space Forum.\nLooking around, it is easy to see why: the landscape is a vision in red. Only the odd splashes of greenery give the game away.\nRio Tinto in southern Spain is a former mining area, and with its unusual chemical and geological make up, it is surprisingly similar to the Red Planet.\nExpedition leader Professor Groemer, who is based at the University of Innsbruck in Austria, explains: \"We have a mineral here called jarosite - and that is exactly what we have on Mars.\"\nThis makes it an ideal place to test out new technology that could one day make to Mars.\nOne piece of kit being tested in these field trials, which are partially funded by the Europlanet network, is a spacesuit simulator called Aouda.X.\nWith its onboard life-support systems and hi-tech computers, it is specially designed to protect any astronaut inside from the hostile Martian conditions.\nFreezing temperatures, a noxious atmosphere and being bombarded with deadly radiation are just a few of the horrors that any humans there would have to contend with.\nAt Rio Tinto, the team is investigating whether the suit can keep any samples that are collected sterile - a must for any mission to Mars.\nProfessor Groemer says: \"This is a big issue in astrobiology. When we are looking for traces of extinct or extant life on Mars, if we find something, we really need to be sure it is indigenous and not a hitchhiker from Earth.\"\nAlongside the spaceman, some rovers are also getting a test run across the rocky red terrain, including a prototype called Eurobot.\nPhillippe Schoonejans, head of the robotics project office at the European Space Agency (ESA), says: \"It is a human-like robot. It has stereo vision, and two arms that can operate little pieces of payload.\n\"It can do anything that is too difficult, too dangerous or too boring for astronauts to do.\"\nBut it may be a long time before Eurobot gets to lend humans on Mars a helping hand.\nAlthough space agencies around the world have declared great interest in manned missions to our planetary neighbour, no definitive plans have been put forward.\nHowever, the scientists at this field site are optimistic.\nScott Hovland, head of human systems at ESA, says: \"I think that we are capable with a lot of the technology that we have.\n\"But there's still quite a few new technologies that I believe would be needed - we need better ways of getting people there, with more powerful propulsion systems, to shorten the duration - things like that.\n\"But I am pretty positive than in our lifetime, we have a good chance of seeing something like that happening.\"\nIf and when this does come about, the chance to make the first footprints in the red Martian soil would be irresistible to some.\nUlrich Luger, from the Austrian Space Forum, has been testing out the spacesuit. He says wearing it at this Mars-analogue site has given him a glimpse of what a real mission might be like.\n\"If somebody asked me if I wanted to go to Mars, of course I would love to go,\" he says.\n\"But there are two kinds of missions. One with a one-way ticket: you only fly there and don't come back. The other is you fly to Mars and come back safely.\n\"If everything was safe, then I would say yes and I would love to go to Mars. But a one-way ticket? I don't know.\"\nIt may take many more decades before humans arrive on the Red Planet.\nBut the team at Rio Tinto says that if this final frontier is ever to be reached, we need to start laying the groundwork now.\nProfessor Groener says: \"Nobody can say what the technology will be 20 or 30 years from now.\n\"But whatever the microchips look like, whatever the surface coating on a spacesuit will look like, I'm sure that the what we are doing today - the grand ideas, the procedures - that can be defined right now. And that's what we are doing here in this beautiful place.\n\"This is a dress rehearsal for the biggest journey our civilisation has ever taken.\"","Devon Island, Nunavut, Canada\nHaughton Crater, Devon Island\nas a Mars Analog Environment\nFor years, planetary scientists\nlooking for Earthly environments similar to Mars for geological\nand biological research have worked in such places as Antarctica's\nDry Valleys, Chile's Atacama Desert, Iceland's volcanic and ice\nfields, California's Death Valley, Washington State's Channeled\nScabland, or Wyoming's Yellowstone National Park.\nThere is in reality no place\non Earth that is truly like Mars, nor is there any one place\non our planet that would quaitfy as the perfect Mars analog.\nIf anything, this means that nothing will replace actually going\nto Mars. What then, is meant by a \"Mars analog\"?\nMars analogs can be defined\nas locations on Earth where some environmental conditions, geological\nfeatures, biological attributes, or combinations thereof may\napproximate in some specific way those to be encountered on Mars,\neither at present or earlier in that planet's history, such that\nnew insight into the nature and evolution of Mars may be gained\nfrom their study. But in addition to providing scientific insight\ninto our neighbouring world, Mars analogs can serve as valuable\ntest sites for the preparation of the future exploration of that\nIn recent years, a team of scientists\nled by MARS project scientist Dr. Pascal Lee of NASA Ames\nResearch Center has identified a new Mars analog site of high\npromise: the 20 km-diameter Haughton Meteorite Impact Crater\nand its surroundings on Devon Island, in the Canadian High\nArctic. Haughton is a site of much interest because it appears\nto present not just one or a few potential Mars analog features,\nbut an astonishing variety of these.\nDevon Island, Queen\nElizabeth Islands, Canada\n(Click on the image for\na larger picture)\non Devon Island\n(Click on the image for\na larger picture)\nSome of the possible parallels investigated\nby this NASA-supported effort, referred to by its participants\nas the Haughton-Mars Project,\nare listed below:\n- Haughton is an impact crater,\na common and fundamental geological feature ofthe Martian surface\n(and of many other planetary surfaces).\n- Haughton is set in a polar\ndesert,a cold, relatively dry, windy, and sparsley vegetated\nenvironment. While Devon Island is warmer and wetter than Mars\nis today (and so are all other Mars analog sites on Earth), this\npolar desert might be akin to Early Mars, when conditions are\nthought to have been wetter and perhaps warmer.\n- The center of the crater hosts\na very unusual type of terain, impact breccia permeated with\nground-ice. During the impact even, shattered Earth rocks were\nthrown up high in the sky and fell back into the crater in monumental\nheaps of jumbled fragments. These fragments rewelded together\nunder the intense heat of the impact to form impact breccia,\nan analog for the \"regolith\" (rubble) gound on all\nplanets subject to meteorite bombardment. At Haughton, the impact\nbreccia is permeated with \"permafrost\" (ground-ice),\nthus producing what may be the closest natural analog on Earth\nto the Martian regolith.\n- Shortly after its formation\n23 million years ago, the Haughton crater was occupied by a lake\nin which sediments were laid down. The lake has long since drained\naway, but the sediments are still preserved in patches inside\nthe crater, slowly weathering away under the cold arctic climate.\nThese ancient crater lake sediments provide an analog for sediments\nexpected to be found in ancient impact craters on Mars that may\nhave once contained lakes as well. Ancient crater lakebeds on\nMars represent prime candidate sites where a well-preserved climate\nrecord and possibly fossils might still be found.\n- Haughton also provides an\nopportunity to study the amount of warming of early lake waters\nby impact-induced hydrothermal activity. In cold environments\nsuch as that of the Arctic or Mars, the heat released at the\nsite of a freshly-formed impact crater may produce what has been\ncalled a \"phase of thermal biology\", an episode of\nbiological development possible only under the uncharacteristically\nwarm temperatures immediately following an impact.\n- A variety of valleys ranging\nfrom intricate networks of channels to deep canyons dissect the\nlandscape at Haughton. Several types of valleys resemble those\nseen on Mars. The resemblance appears to be more than superficial,\nas the similarities are often specific and unique. Studying how\nthe varieties on Devon Island formed may provide clues to how\nsome valleys on Mars formed. Ultimately, such studies may help\nanswer the long-standing question of what happened to water on\n- The Arctic is host to a variety\nof periglacial formations, geologic features such as ice mounds\nand polygon fields which are indicative of the presence of ice\nconcentrations in the ground. Many features on Mars, especially\nat high latitudes, have been hypothesized to be periglacial formations.\nHaughton and the rest of Devon Island are a paradise of periglacial\nlandforms, providing an opportunity to explore this additional\nparallel. Understanding periglacial formations at Haughton may\nultimately help recognize where ice can be found at shallow depth\n- Haughton also offers examples\nof life adapted to an extreme environment. Biological contrasts\nbetween life inside and outside the crater have also been noted,\nthus shedding light on the role of impact craters as specific\necological niches on planets. Biological research at Haughton\nmay thus have profound ties with exobiological studies on Mars.\nNASA's study of this Mars analog\nwonderland in the cold, remote and barren reaches of Devon Island\nprovides a unique opportunity to study how humans will explore\nMars. The Haughton-Mars Project has already included in its two\nfirst field seasons experiments geared towards developing the\nnew technologies, operational procedures, and experience in human\nfactors that will help realize or optimize the exploration of\nMars by humans. Field communication devices, \"wearable\"\ninformation sharing systems, a coolant-free permafrost drill,\na ground-penetrating radar, robotic vehicles (developed by Carnegie\nMellon University), a field spectrometer, stereo cameras, a field\nemergency medical kit (developed by NASA Kennedy Space Center),\nand EVA requirements and procedures are among the human Mars exploration\nitems that have begun to be studied.\nSuch experiments will be continued\non Devon Island during the 1999 field season of the Haughton-Mars\nProject, which will take NASA scientists back to Devon Island\nfrom 21 June to 31 July, 1999. The Mars Society will participate\nin this expedition to select the site where the MARS will\nbe set up and begin assembly of the first elements. A ground array\nof solar panels may be among the first elements installed."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:b91eda94-d145-49ea-af5c-72a928636f77>","<urn:uuid:f473ab4a-83f9-4b95-b5e3-71d8755a1aa7>"],"error":null}
{"question":"How does a solar inverter convert DC energy from solar panels into AC power?","answer":"A solar inverter (PV inverter) converts the variable direct current (DC) output from photovoltaic solar panels into utility frequency alternating current (AC). This conversion is essential because most electrical devices run on AC energy. Without this conversion process, the harvested solar energy would go to waste since it couldn't power electrical devices. Solar inverters also perform additional tasks like data monitoring and power optimization.","context":["A solar inverter or PV inverter, is a type of electrical converter which converts the variable direct current (DC) output of a photovoltaic (PV) solar panel into a utility frequency alternating current (AC) that can be fed into a commercial electrical grid or used by a local, off-grid electrical network.\nAs well as converting DC to AC, inverters also carry out a number of other tasks, too. For example, they can also act as data monitoring devices or advanced utility controllers and are power optimizers.\nMost electrical devices run off AC energy. So, if you didn’t have an off-grid inverter to transform that DC energy harvested from the sun, you would be unable to power any of your electrical devices and the harvested energy would go to waste. That’s what makes a solar power inverter is an essential piece of any solar power system.\nDifferent types of solar inverters\nThere are different types of inverters available for solar power systems, such as off-grid inverters, grid-tie inverters, pure sine wave inverters, and modified sine wave inverters. Ultimately, they all have the same end goal of converting DC solar energy into AC energy that can power your appliance, but the way in which they function differs slightly. The one that’s best for you will depend on a number of factors including what performance you need and how big your budget is. It goes without saying that the most powerful of solar inverters are the most expensive. But, if you don’t need one quite so powerful, why spend the extra money?\nMost solar power kits will already come with an inverter included. However, if it doesn’t, or you’re looking to upgrade your existing one, here’s a brief overview of each type:\n- String: This type of inverter is often referred to as a centralized inverter and is primarily used in small-scale solar power systems. With a string inverter, each solar panel is connected together in strings. When each of the strings produces energy it all gets sent to the inverter, ready for it to convert into AC power. String inverters have been around a long time and are a tried and tested form of technology. They’re also the cheapest to buy and the easiest to maintain. The downside to having a string inverter in your solar power system setup is that it will only harvest as much energy as its least efficient solar panel. This type of inverter tends to work best on roofs that get a consistent amount of sun during the day.\n- Microinverter: As mentioned above, all inverters do the same thing, they just go about it a slightly different way. With a microinverter, instead of sending the solar panel’s harvested energy to the inverter via strings, these systems convert the DC energy to AC energy right there at the site of the solar panel. The advantage of using microinverters opposed to string inverters is that they’re far more efficient and will continue to produce electricity even if some solar panels are not performing as well as they should be. The downside is that they’re far more expensive to buy initially and tend to cost more to maintain and repair as they’re located on the roof. This type of inverter would work best with solar power systems that face different directions, are quite spread out, or have objects such as chimneys or gables that could cause shade.\n- Power optimizer: This is essentially a compromise between the old faithful string inverter and the more modern microinverters. As with microinverters, power optimizers sit either next to or within the individual solar panels. However, unlike microinverters, they still have to transmit any harvested energy to a centralized inverter. The unique thing about power inverters is that they fix the voltage of the electricity before it gets sent to the inverter. String inverters become far more efficient when paired with a power optimizer and will still cost less than microinverters. Like microinverters, they will still run efficiently even if some of the panels are underperforming and can be used to monitor the performance of individual panels. Typically, power inverters are great for anyone who wants to improve the efficiency of their solar power system but isn’t ready to upgrade to microinverters just yet.\nHopefully, you will now be much more clued up as to what a solar power inverter is and the way it works. Now it’s simply a case of deciding which type will be best for you. Once you’ve figured that out, you’re good to go. Here are a few helpful tips to help you do that:\n- Cost. Obviously, the price of the inverter will vary depending on the type you go for. Just bear in mind that an inverter can typically represent around 20% of the total cost of the system, so is not going to be cheap. And don’t be fooled into buying the cheapest one you can find. You get what you pay for when it comes to inverters and unless you want to replace it every couple of years, don’t skimp in this department.\n- Is it expandable? If you are considering expanding your solar power system in the future you want to make sure the inverter you buy will allow you to do that.\n- Warranty. A decent inverter should last a good 10 to 20 years. While most manufacturers won’t guarantee them for that long, most will cover them for at least 8 years. Be wary of any that are offering less than this."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:58f2437c-b69f-413c-92c6-78ce12156de1>"],"error":null}
{"question":"What specific environmental challenges arise from organic farming's reliance on compost for soil health?","answer":"While composting is important for building soil health in organic farming, it poses a significant environmental challenge because it releases substantial amounts of methane during the composting process. This methane emission makes widespread adoption of this soil-building approach problematic from a climate change perspective.","context":["The guiding principal of organic is to rely exclusively on natural inputs. That was decided early in the 20th century, decades before before the scientific disciplines of toxicology, environmental studies and climate science emerged to inform our understanding of how farming practices impact the environment.\nAs both farming and science have progressed, there are now several cutting edge agricultural practices which are good for the environment, but difficult or impossible for organic farmers to implement within the constraints of their pre-scientific rules.\nThere was one window during which the rules for organic might have been adjusted to reflect a more modern understanding. In 1990 the US Congress charged the USDA with the task of setting a national standard for what products could be legally sold as Organic. That agency was inclined to include more science in a definition of “what is safest for us and for the environment,” but the organic community of that day was adamant that the rule should only reflect the purely natural definition embraced by their existing customer base. Long before the final Organic Standards were published in 2002, it was clear that the industry preference had prevailed and that the rules of organic would still reflect their pre-scientific origins. That is why the following six environmental issues exist for organic farming.\n1. Less Than Optimal Fungicides\nOrganic farmers use pesticides, but only those qualified as sufficiently natural. Thus, copper-based fungicides are among the few options available to an organic grower for the control of fungal plant diseases. These are high-use rate products that require frequent re-application and which are quite toxic to aquatic invertebrates. There are much more effective, and far less toxic, synthetic fungicide options without environmental issues, and which, unlike copper, break down into completely innocuous materials. Organic growers can't use those fungicides. Similarly there are many environmentally benign, synthetic insecticides and herbicides which cannot be used.\n2. A Surprisingly High Carbon Footprint for Compost\nThe greatest original contribution of the early organic movement was its focus on building soil health. One of the main ways that organic farmers do this is by physically incorporating tons of organic matter into the soil in the form of composts. Unfortunately, during the process of composting a substantial amount of methane is emitted which means that broad use of this soil-building approach would be problematic from a climate change point of view.\n3. Practical Barriers to Implementing No-till Farming\nThe best approach to building soil quality is minimizing soil disturbance (e.g. no plowing or tilling) combined with the use of cover crops. Such farming systems have multiple environmental advantages, particularly with respect to limiting erosion and nutrient movement into water. Organic growers frequently do plant cover crops, but without effective herbicides, they tend to rely on tillage for weed control. There are efforts underway to find a way to do organic no-till, but they are not really scalable.\n4. Difficulties Implementing Optimized Fertilization\nFertilizers are associated with many of the biggest environmental issues for agriculture because of the challenges in supplying all a crop needs without leading to movement of those nutrients into surface or ground water or to emissions of the highly potent greenhouse gas, nitrous oxide. The best practice is to “spoon feed” the nutrients through the irrigation system at levels designed to closely track the changing demands of the crop throughout the season.\nDrip Irrigated and Fertilized Grapes\nThis requires water-soluble forms of the nutrients and that is very expensive to do for the natural fertilizer sources allowed in organic. Since the plants absorb those nutrients in exactly the same molecular forms regardless of source, this cost barrier is a non-scientific impediment to doing the best thing from an environmental point of view. Organic fertilizers like composts or manures are also much less practical for variable rate application, an environmentally beneficial option for rain-fed crops in which different amounts of fertilizer are applied to different parts of the field based on geo-referenced soil and yield mapping data. Finally, the organic avoidance of \"synthetic fertilizers\" would mean that these growers would not be able to use what appear to be promising small-scale, carbon-neutral, renewable energy-driven systems for making nitrogen fertilizers.\n5. Lower Land-Use-Efficiency\nThe per-acre yields of organic crops are significantly lower than those for conventional. This has been well documented both by meta-analysis of published research comparisons and by public data generated through USDA commercial production surveys.\nThe shortfall is driven by limited pesticide options, difficulties in meeting peak fertilizer demand, and in some cases by not being able to use biotech traits. If organic production were used for a significant proportion of crop production, these lower yields would increase the pressure for new land-use-conversion - a serious environmental issue because of the biodiversity and greenhouse gas ramifications.\n6. Lack of an Economic Model to Move Beyond Niche Status\nFinally, agriculture needs to change in ways that accomplish both productivity and environmental goals. That optimal farming approach must become the dominant system over time. Even if organic had maintained its growth trend from 1995 to 2008, organic acreage in 2050 would still have represented less than 3% of US cropland.\n|Trend line for US organic cropland as of of 2008|\nThen, between 2008 and 2011, USDA survey data showed no net gain in US organic acreage. Environmentally desirable \"conventional\" practices like no-till, cover cropping and a variety of other precision agriculture innovations are already practiced on a much broader scale and have the potential to be economically attractive for farmers without any price premium mechanisms. Innovations in farmland leases could greatly accelerate the conversion process if growers could be guaranteed long-term control of fields so that they could profit from their investments in building soil quality.\nConsumers Who Want To Do The Right Thing\nThere are many consumers who are willing to spend more for organic food because they believe that they are making a positive difference for the environment. While it is commendable that people are willing to do that, the pre-scientific basis for the organic rules means that the environmental superiority of organic cannot be assumed. While “only natural” is appealing as a marketing message, it is not the best guide for how to farm with minimal environmental impact. Between rigorous, science-based regulation, public and private investments in new technology development, and farmer innovation, modern agriculture has been making excellent environmental progress. That trend, not organic, is what we need to encourage.\nYou are encouraged to comment here and/or to email me at email@example.com\nPennsylvania farm image from USDA Images. Vineyard image Agne27. Copper Sulfate image from Wikimedia commons. Organic yield and acreage information from the USDA-NASS."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:af468a30-ba52-43e0-a071-b93d38314470>"],"error":null}
{"question":"What is proper etiquette for ending conversations in Middle Eastern business meetings? Want to avoid mistakes!","answer":"In Middle Eastern cultures, you should avoid gesturing at your wrist or watch to indicate running out of time, even if you're getting late. In Middle Eastern culture, it's believed that once a conversation starts, it should be allowed to take its natural course to completion. Using time-related gestures to end the conversation prematurely is considered a rude insult. Therefore, you should let conversations progress at their own pace.","context":["From one culture to another, hand gestures adopt a unique meaning and symbolism. Many times, we tend to use our hands to explain our needs and thoughts. The same gesture can mean something quite nasty and disrespectful to a person from a different cultural background. Hand gestures are a very important part of the body language gestures. While visiting a new city or country, it is important to learn what certain gestures mean so that you don’t offend anyone.\nWhat are Hand Gestures?\nThese are a way of communicating with others and conveying your feelings. These gestures are most helpful when one is speaking to someone with no language in common. The meanings of hand gestures in different cultures may translate into different things\nTo explain my point, I take a very common example of former President George W. Bush who had to face a major faux pas during a visit to Australia. He tried to signal a peace sign by waving the two finger or V-sign at the crowd.You may think of this as a simple gesture, but he committed a major error. Instead of his palm facing outwards, it faced inwards. The meaning in Australia meant he was asking the crowd to go screw themselves! Therefore, it is very important to understand the meanings of gestures before you travel to different countries.\nBefore you communicate with people in different cultures, you need to understand the meaning of gestures. Those considered as a good gestures in one country may be termed as an offensive gesture in some countries. So, if you are a frequent flier to different countries, improve your communication skills by learning the meaning of certain gestures.\nInterpretations to Know\nThe following information will cover some of the common gestures with their meanings. You will be surprised to know that some gestures that you perform almost casually has a really different meaning in different cultures.\nThe thumbs up sign in most American and European cultures meaning things are going according to your plans or something you approve of. However, the going good sign translates into a rude and offensive gesture in Islamic and Asian countries. In Australia, it means OK, but if you move it up and down, it is considered as a grave insult.\nThe thumbs down sign obviously means the opposite of a thumbs up sign. It is an indication of something that is bad or something that you do not approve of. It also indicates that something or someone has failed. The thumbs down sign is not used as often as the thumbs up sign. This is a rude and an arrogant way to indicate failure.\nCrossing of fingers is considered as a sign of wishing for good luck or fortune. It also may interpret that someone is hoping for something good to happen. The cross may have originated from the Pagan symbols that means to ward off evil. Many times people cross their fingers before telling a lie, as it is believed to countervail the evil that comes of the lie. It is a positive and negative symbol as it interprets both luck or lies.\nWhen one raises the hand up with the palm facing towards the opposite person, it means ‘to stop’ in America and British countries. It means that an authoritative figure is asking one to ‘Stop’. If the fingers are pushing down, it indicates that the person to sit down or settle. It’s not a defensive gesture and is a gesture to take control over the person it is intended for. In Singapore or Malaysia, it means that one is trying to ‘hail’ someone’s attention like a waiter or asking permission.\nThis gesture is commonly used by mothers and teachers. It is used to warn a single individual. It is a way to silent an individual and ask them to pay attention. This is considered as a rude gesture in a professional environment and termed as a domineering behavior.\nWhile talking to a friend, you may open your palm and stretch out your fingers. But, if you were in Greece, the same gesture would be considered as a traditional manual insult. It means that you are thrusting your hand in the face of the opposite person and using a brash ‘nah’. This suggests that you are asking the person to ‘eat shit’ and leave you alone. The moutza in American terms is similar to the gesture that means ‘talk to the hand’.\nThe Dog Call\nThe dog call is a gesture where you curl your finger and summon someone towards you. It’s mostly seen to be carried out by a tempting woman to her man. However, it should be avoided in Philippines as this is one of the worst forms of hand gesture which is used only for dogs. It could get you arrested or maybe even punishable by breaking your finger, so that you never attempt to do this gesture again. In countries like Japan, it’s considered a rude gesture In Singapore, it’s indication of death.\nThe okay sign is used by curling the index finger over the thumb and the remaining fingers extended above them. This means that everything is good or well. Also, it is usually used by divers to indicate all is well or OK. But, in Latin America and France it’s considered as an insulting sign, it’s thought to mean ‘your anus’, and has negative connotations attached to it. In Australia, it means zero and in Germany it means a job well done. In New Zealand, it’s taken as a cheap way of saying OK.\nSnapping fingers over and over may mean one is trying to remember something someone has forgotten. In Latin America, snapping fingers means asking one to hurry up. In Great Britain and America, one snaps fingers when one remembers something or gets an idea. However, in many cultures, snapping fingers close to someone’s face is considered to be an offensive gesture.\nRunning Out of Time\nMany times when one is getting late or wants to indicate running out of time, they tend to watch their wrist. This is despite the fact that they may or may not be wearing a watch on the wrist. This is a subconscious gesture to indicate an end of conversation or a subtle way that one should take leave. However, in the Middle East, it’s believed that once a conversation starts, it should be allowed to take its time to complete. Gesturing to end the conversation is considered a rude insult.\nIn America and European cultures, it is considered rude to point fingers at others. This hand gesture is an indication of a dominant – to – subordinate behavior in the professional world. It is considered a gesture to single out an individual from a crowd. This aggressive signal is not liked by many, as no one likes to be singled out.\nThe corona is carried out by pointing the index and pinkie finger upwards and the two middle fingers and thumb curled towards the palm. This is considered as the symbol of the devil in many cultures. The two pointing fingers indicate the horns of the Devil. It’s also widely used by rock stars in as a positive gesture. This is also one of the good gestures in cultures like Buddhism and Hinduism. In the Mediterranean, it an old symbol that means ‘cuckold’, that is, your wife is cheating on you.\nThe fig is a gesture that is indicated by a fist. The thumb is seen poking out of the index and middle fingers. It is known as mano fico or fig hand in Roman. It is one of the good hand gestures, as it indicates good luck and fertility and a way to ward off the evil eye. However, the fig is considered a gesture that mimics the female vulva in Italian. Thus, this is a very offensive gesture to the Italians and Turks. If it is carried out by a person of Asian origin, it roughly means ‘screw you’.\nOne of the most offensive and rude gestures around the world is the finger. The middle finger-pointing upwards is considered as an obscene gesture. In some Mediterranean and Arab countries, holding the index finger instead of the middle finger, implies the same obscene gesture.\nThis was some information related to the meanings of gestures in different cultures. You should avoid using certain gestures in countries whose cultures are new to you. You do not want to offend anyone or anything in a new land that may put you in a soup. While you research some information related to travel tips, be sure you also learn something about the body language gestures too. You do not want to inadvertently insult your guests or hosts or trigger an unwarranted violence in a new land."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:f1e201f0-f74b-448a-9170-86ea3aaa8e20>"],"error":null}
{"question":"Could you please compare how the Fiber Network and Washougal Mill contribute to preserving traditional crafts in their communities?","answer":"Both organizations play important roles in preserving traditional crafts, but in different ways. The Fiber Network focuses on teaching and preserving lost arts like tatting, spinning, and weaving through demonstrations at the West Virginia State Fair and visits to elementary schools, where they teach children basic weaving techniques and the origins of wool products. Meanwhile, the Washougal Mill preserves traditional American textile manufacturing as one of the few remaining fabric weaving mills in the United States (out of formerly 800+ mills), while also modernizing with state-of-the-art technology to maintain high-quality wool fabric production.","context":["An interview with Denise Giardina\nADA: Becoming Accessible\nFrom the Director\nA convergence of printmakers and potters\nMonoprints of Barbara Marsh Wilson\nInterview with Mark Wolfe\nFiber artists share old traditions, new ideas\nStory and photographs by Maryann Franklin\nHow many of you know what tatting is? It’s a lace-making technique that uses thread and a plastic or metal shuttle. “The hardest job I ever had,” Eloise Sibold said, “was trying to teach someone who is left-handed to tat. But I finally helped her get the hang of it.”\nSibold is a member of a most unusual and enduring group of artisans, the Fiber Network. The Greenbrier County organization is not about optics or technology, a mistake people sometimes make when they hear the name, but something much more basic. Something, in fact, that serves as a base for other crafts: Fiber for thread and cloth.\nSome guilds or artisan groups join together in search of discipline or inspiration, some to market their wares. The fiber network meets simply to find joy and inspiration in their arts and to share their experience and expertise. They range in age from eight to over 80, although most members fit into the “mature” category. They come from four surrounding counties (Greenbrier, Monroe, Summers and Pocahontas), with an occasional visitor from farther afield (Blacksburg, for example).\nThe Fiber Network is both loosely knit and loosely structured. Though there are about 45 members, most meetings are considerably smaller. Numbers vary from meeting to meeting. When I asked a question, I often got a chorus of individual answers, as they looked up from their labors. “There are no rules.” “We let everybody do what they want.” “Sometimes in the winter, we come just to get out, to escape, to share some camaraderie.”\n“ The best part of being in this group is that we inspire each other,” said one member. “Sometimes, you end up involved in something different just by watching others.”\nAt meetings, they share information both formally and informally. They also take field trips to processing mills to learn about fiber and color blending, etc. They hold a sale of their wares the first Saturday of each December. “We call it the Cabin Sale. It was actually started by a member, a potter, in a log cabin that belongs to her.” Now it’s a holiday tradition.\nIn addition to their individual efforts and regular get-togethers, they schedule an annual retreat weekend at Hawks Nest State Park each spring, committing time to be together and to complete their projects.\nIf someone gets stuck on a difficult knitting or weaving project, she turns to the others for answers. “Someone’s always there to give you instructions and tell you what to do to solve a problem,” one member commented. “You don’t have to reinvent the wheel,” remarked another.\n“I’ve been part of groups where people won’t tell anything,” commented Sibold. “We don’t keep secrets here. We share our knowledge.”\n“It’s silly not to share what you know.”\n“We share information. We share patterns.”\n“ We share magazines.”\n“We share everything except our husbands!”\nA bit of information they shared with me may surprise you, as it did me. The ladies keep their wool and yarn in the freezer. It’s the best way to eliminate moths!\nEvery year the group sets up a spinning and weaving demonstration booth at the West Virginia State Fair, where they teach children the basics of weaving on cardboard looms. This, in fact, is how they earn the money to pay for the large classroom they rent from Carnegie Hall.\n“We contribute to the community as well as to each other, “ explained Laurie Caudill, the group’s acting president. “We’re teaching and preserving lost arts.”\nThe members talk eagerly about their joy in working with children. They often visit local elementary schools to give talks and demonstrations, allowing students to learn in different ways: teaching, seeing, touching, experiencing. Many children don’t even know where wool comes from, or any part of the processes that turn it into cloth. Many have never even seen a spinning wheel.\nEloise Sibold has been visiting elementary schools for many years. She began visiting a Kindergarten class in Frankford when her grandson, now a sophomore in college, was in the Kindergarten class.\n“These things are worth preserving, said member Sarah Allen. “Spinning, to me, is like a Zen meditation. I try to spin for at least 30 minutes every morning.”","PENDLETON’S WASHOUGAL MILL: 100 Years of Weaving in America\nThe looms continue weaving in Washougal, Washington, as the mill celebrates 100 years as a key part of Pendleton Woolen Mills’ operations. Running three shifts a day, the mill’s 190 employees keep the dye house, looms and sewing rooms humming to produce the virgin wool fabric used in Pendleton products.\nWashougal sits on the banks of the Columbia River at the entry to the scenic Columbia River Gorge. Pendleton was already operating a mill in Pendleton, Oregon, when the company acquired the Washougal mill in 1912. The additional mill gave Pendleton the ability to weave a wider variety of fabrics. Sir Pendleton worsted and Umatilla woolen fabric are both woven in Washougal, as well as fabrics for the women’s line. “The Washougal community helped fund the startup of this mill and has supported Pendleton ever since,” said Charlie Bishop, VP of Mill Operations. In turn, the mill has been a major employer in this small Washington town since it opened.\nFabric weaving was once a major industry in the United States, with more than 800 mills in operation. Today only a handful of those mills remain. At 100 years young, the Washougal mill is thriving as a world-class facility with state-of-the-art technology and machinery. In recent decades Pendleton has added dye house computer technology, wider looms to allow for the production of king-sized blankets, additional finishing equipment, more napping machines and a team sewing system to help the Washougal mill meet the tremendous demand for made in the USA textiles. The mill has worked hard to develop environmentally friendly and compliant processes.\n“Few major U.S. manufacturers weave their own fabric in America,” said Bishop, a fifth generation member of the family that founded and operates Pendleton Woolen Mills. “Because we oversee every aspect of the process, including buying the wool, we can trace back every piece we make. It allows us to maintain a standard unmatched in the industry.” Its roots may be historic, but the Washougal mill is a 300,000-square-foot model of modern efficiency. “Mill owners come from around the world to tour it,” said Bishop. “Pendleton continues to lead the world in weaving techniques, dyeing processes, and fabric finishing.”\nWashougal’s Historic Bell\nThe Washougal mill traditionally marks important occasions by ringing a historic brass bell that sits above the boiler room. The bell was cast in 1865 in Boston, Massachusetts, at the famous Revere Foundry, founded by Revolutionary War hero Paul Revere. The bell saw service at Davis & Furber in North Andover, Massachusetts, until 1865, when it was sent by sailing ship around the horn and up to Brownsville, Oregon, the site of another Pendleton mill.\nA young Clarence Morton Bishop worked at the Brownsville mill at the time. According to the Pendleton’s current president, Clarence Morton Bishop III, “There may be some letters in the Pendleton archives where the original CM Bishop laments the bell tolling him out of bed as a young boy.” Brownsville closed in 1918. The bell was given to Clarence Morton Bishop, perhaps as a souvenir of all those early mornings. He moved it to the Washougal mill, officially dedicating it in a ceremony on June 30, 1938. It still hangs there today.\n“To me, sitting atop the boiler room and machine shop, that bell is the centerpiece of our mill,” said Charlie Bishop. Although the bell no longer rings out at 6:45 and 12:15 to remind workers to return to work, it still tolls on special occasions. In May 2012, the bell rang to mark the retirement of Thang Nguyen after 35 years of service. The bell will also ring in August 2012 to mark one hundred years of community and American-made quality at the Washougal Mill. To commemorate this historic milestone, Pendleton Woolen Mills, the City of Washougal, Two Rivers Heritage Museum, and Washougal Town Square are hosting a community celebration Aug. 3rd and 4th at the Mill, the Museum and in the surrounding community.\nCOMMUNITY EVENT SCHEDULE\nFriday, August 3:\nDedication Ceremony, 100 Year Celebration Cake Cutting, Pendleton Woolen Mills, 10 – 10:30 a.m.\n- The Blessing – a blessing of thanks and of continued prosperity by Native American Elder Buzz Nelson\n- The Generation of Family Celebration – Honoring the multi-generations of families who have worked at the Washougal Mill, including community families with 3 generations or more of employees, the Bishop Family (now in its 6th generation of Pendleton Woolen Mills ownership since 1863) and the Washougal family including Mayor Sean Guard.\n- Ringing of the Historic Bell* and Cutting of the 100 Year Celebration Cake (with free servings all day at the Washougal Mill Outlet Store)\nPendleton Mill Tours, mornings at 9:00 and 11:00 a.m., and afternoons at 1:30 and 3:00 pm. Tour the Pendleton Mill to experience both the 103 years of history and the state-of-the-art looms that weave Pendleton’s famous woolen fabrics\nWashougal Days Beer & Wine Garden, 5 – 11 p.m. music from 6 p.m. Continue the celebration with music and food for everyone. Adults 21 and over can enjoy the outdoor beer & wine garden. Hosted by the City of Washougal. Admission charge.\nWashougal Mill Outlet Store, open 8 a.m. – 5 p.m. with specials throughout the store in addition to free cake. Mill tours at 9 am, 10 am, 11 am and 1:30 pm. 2 Pendleton Way, Washougal, WA. For tours: call 360-835-1118 or 800-568-2480.\nPapa’s Ice Cream, open for the celebration from 8 a.m. – 9 p.m.\nHEARTH Restaurant, open for the celebration from 4 p.m. to close with live entertainment in the square.\nSaturday, August 4:\nHeritage Days 5K Run/Walk, 9 a.m. Start the day with this fun run/walk event sponsored by the Camas Lions Club and the Washougal Lions Club. Admission is free; donations accepted.\nKids & Kritters Parade, Pendleton Fields, 20th and A Streets, 10 a.m. Kids (and their parents & grandparents) and “Kritters” of all shapes and sizes are welcome to join the parade. Come a few minutes ahead to get a number and position in line.\nTwo Rivers Heritage Museum, 11:30 a.m. to 5 p.m. The Camas Washougal Historical Society will host its annual Heritage Day celebration and fund raiser at TwoRiversMuseum which includes free admission to the museum, craft booths, blacksmithing demonstration and mountain men setting up a camp with black powder demonstrations. Museum tours include collection of antique sewing baskets and quilts, old tools, blacksmith forge, a horse drawn sleigh and a doctor’s buggy. Enter to win prizes, including a Pendleton Blanket.\nEquestrian Demonstrations, Pendleton Woolen Mills, 1 – 2 p.m. This event will include presentation of the US Flag and National Anthem, equestrian drill teams and pony cart demonstrations.\nWashougal Days Beer & Wine Garden, ReflectionPlaza, 5 – 11 p.m., with music from 6 p.m. Continue the celebration with music and food for everyone. Adults 21 and over can enjoy the outdoor beer & wine garden.\nWashougal Mill Outlet Store and Mill Tours, open 9 a.m. – 5 p.m. with specials throughout the store and tours at the mill next door. The mill is open to the public for tours year-round. Visitors can see (and hear) the entire process that transforms giant bales of scoured wool into Pendleton’s “Warranted to Be” textiles. To learn more about public tours, visit www.pendleton-usa.com. The mill is located at 2 Pendleton Way, Washougal, WA, 98671."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:55d3e14e-096a-4fe8-b663-d2ff08523643>","<urn:uuid:d21f4474-396b-4463-b52c-4091bf54304b>"],"error":null}
{"question":"Hi! Need help to understand patient data privacy... What's good about HIPAA standards for data exchange, pero what are the privacy concerns with government access? 🤔","answer":"HIPAA standards enable structured healthcare data exchange through formats like FHIR and openEHR, facilitating secure information sharing between healthcare providers. However, there are significant privacy concerns as the Health and Human Services (HHS) department demands unlimited access to client records, often without patient consent. This creates a situation where, for the first time historically, the US government can legally retrieve citizens' medical information without their knowledge. Additionally, HIPAA allows free information exchange between insurers, providers, and claims clearinghouses without client awareness.","context":["Developing technology for patient data collection, protection and sharing.\nBy Svitlana Sudorina, R&D HealthTech, FinTech. CEO of Skein Group\nSkein worked with the University of Oxford’s Digitally Enabled Preventative Health research group on defining the requirements and developing technology for patient data collection and analysis. It aimed at addressing the critical challenge of the quality of the data used in the evaluation of clinical outcomes, development of new drugs and vaccines, in precision medicine and personalised health treatments.\nThe research was funded by a European Institute of Innovation and Technology grant.\nWidespread adoption of precision medicine depends on an understanding of the implications of individual variations on drug type, dose, and response in various diseases, and access to high-quality patient data.\nHerpes Simplex Virus (HSV) was used as an example of a disease that affects a huge population. Due to insufficient knowledge of the virus, there are still no vaccines for it despite large numbers of infections globally. Whilst there are many studies, researchers are unable to control the data provenance and quality. The World Health Organisation defined a better collection and management of epidemiologic data as the key first step towards an improved understanding of the virus and thus advancing research.\nData heterogeneity is a key problem in standardising epidemiologic data. Through developing a standardised patient data registry tools, we aimed to facilitate the collection of better quality data for a systematical study of HSV epidemiology. As a part of this project, we also developed an innovative machine learning algorithm for the identification of risk groups among people potentially infected with HSV.\nAI FOR REAL-LIFE MEDICAL EVIDENCE AND PERSONAL DATA INSIGHTS\nComprehensive understanding of patterns requires robust genomic and demographic data, that includes extended data such as family history, ancestry, genomic, biomarker and imaging information.\nAnonymisation is one of the critical instruments for providing a secure environment for data sharing. The pseudo-anonymity techniques provide tools for designing a data collection system that enables users to get access to it without providing his/her identity. It requires the use of a robust cryptographic hash function to anonymise information related to the patient’s identity and solutions for reversible pseudonym generation.\nAn increasing proportion of older people among users means that the technology implementation needs to take that into account and avoid, for example, a purely app-based implementation.\nTo resolve these challenges, we created a patient registry engineering solution using machine learning and data science methods. The source database contained over 600 variables on demographic, socioeconomic, dietary, and health-related information collected by interviews and physical examinations.\nThe process followed the UK Government Agile Service Design framework. At the first stage, Discovery, we learned about the users and their contexts, the technological constraints, defined design requirements and user stories. After completion of Discovery, technologically we are going into Alpha with a set of user-focused requirements and design specifications. At the Alpha phase, a technological blueprint was developed.\nDATA INTEGRATION AND SECURITY\nWe researched the health data exchange standards in operation and development:\n- FHIR is a standard developed by Health Level Seven (HL7) that functions as an API for developers to access needed clinical information from the EMR\n- EN/ISO 13606 – Electronic Health Record Communication\n- Extensible Markup Language (XML)\n- The Resource Description Framework (RDF) and RDF-Schema (RDFS)\n- Simple Knowledge Organization System (SKOS)\n- Common Terminology Services, Release 2 (CTS2)\nFHIR and openEHR are the two most recent, robust and complete healthcare data persistence and exchange specifications.\nBased on the data standards requirements, including openEHR and FHIR, we planned the database architecture that deploys a history of updates, supports the JSON, XML, RDF formats and provides oAuth authorisation. The core database is a relational PostgreSQL with additional NoSQL data storage for unstructured data.\nMACHINE LEARNING MODULE\nWe explored and prototyped solutions based on Natural Language Processing (NLP), XGboost models and Classification and Regression Trees approaches to reducing informational entropy. Ultimately, a CART Random Forest (RF) model was used for generating questions for users in the HSV Diagnostic Tool implementation.\nAn anonymous lifestyle-data based questionnaire with a Random Forest algorithm was devised using Python. The algorithm was optimised to reduce the number of questions and to identify risk groups for HSV. We split the data set in training and validation subsets, which were used for training and performance testing of the model.\nSee more detail in the published academic paper.\nSECURITY: SSL, ENCRYPTION FOR DATA AT REST AND IN TRANSIT REST API REACTJS PYTHON CART RANDOM FOREST MACHINE LEARNING MODELSABOUT\nAbout EIT Health\nEIT Health is a ‘knowledge and innovation community’ (KIC) of the European Institute of Innovation and Technology. It works across borders with approximately partner organisations, bringing together the brightest minds in healthcare to answer some of the biggest health challenges facing Europe. Headquartered in Munich, Germany, with a pan-EU representation via six regional Innovation Hubs.\nAbout Digitally Enabled Preventative Health Research Group\nThe group is focused on high-impact research in digital health, developing and evaluating digital solutions to health-related issues, integrated health data ecosystems and enabling infrastructure.","|Healthcare Training Institute - Quality Education since 1979CE for Psychologist, Social Worker, Counselor, & MFT!!\nConfidentiality Issues in the time of HIPAA\nRead content below or listen to audio.\nLeft click audio track to Listen; Right click to \"Save...\" mp3\nIn the last section, we discussed three changes in ethical boundaries in regards to the disclosure of raw test data. These three test data boundary changes include: shift in standards; effects of HIPAA; and protecting test security.\nOn this section, we will discuss three controversies created by HIPAA and its possible breach of ethical boundaries. These three HIPAA privacy controversies include: governmentally accessed information; contradictory language; and employer access.\n3 HIPAA Privacy Controversies\n♦ Controversy #1 - Governmentally Accessed Information\nThe first HIPAA privacy controversy is governmentally accessed information. The branch of the government, Health and Human Services, otherwise known as HHS, in addition to taking on the responsibility of writing the legislation, also takes on the responsibility of monitoring it. However, to do this, the HHS demands unlimited access to clients’ records with or without consent of the client. Upon the inception of HIPAA, the HHS facilitated the federal government’s admittance to the records of millions of mental health clients.\nFor the first time in history, an act of legislation makes it legal for the United States government to retrieve medical information about its citizens, often without the knowledge of the citizens themselves. In addition, HHS has created what HIPAA expert Michael Freeny terms \"a holy trinity of insurers, providers, and claims clearinghouses\" who can exchange information freely between each other without the client’s knowledge.\nEssentially, the information that the client only wanted and thought would be shared just between him or herself and the clinician becomes a free-for-all among these HIPAA approved entities. However, clients are never aware of these transactions and believe their information to be perfectly confidential because, as we discussed in section 3, the Notice of Privacy Policies are so convoluted and obscure that they cannot cipher through it all. Therefore the federal invasion of privacy remains undetected unless a client extensively researches HIPAA for him or herself.\nI have found that many clients who have undertaken the laborious task of sifting through HIPAA legislation feel cheated and betrayed by the government and most significantly by their therapist. This puts an unnecessary strain on the client-therapist relationship so that is why I try and make the language of my NPP as clear as possible to my clients from the beginning so they are not surprised by newly discovered and interpreted legislation.\nThink of your clients. Can you think of any clients who may be shocked by the extent to which the government has access to his or her records?\n♦ Controversy #2 - Contradictory Language\nThe second HIPAA privacy controversy is contradictory language. Although the legislation presents all appearances of attempts to protect a client’s privacy, upon closer scrutiny, it becomes apparent that certain passages may contradict each other.\nMost specifically, Michael Freeny points out a specific section within the legislation which guarantees consumers the right to see and copy their health records and request corrections of mistakes that may be contained in those records. However, providers are obligated by HIPAA to inform clients that they are not bound to fulfill such requests. Therefore, the client’s rights may not be recognized by the provider if the provider deems it unnecessary.\nIn addition, the guidelines stipulate that clients maintain the right to restrict the distribution and transmission of their medical information. However, Freeny points out, \"It’s not true because the next sentence says that the provider is under no obligation to give you that information.\" Even further, although the client may retain the right to restrict the distribution of their information and the provider consents, there are certain entities to which the provider cannot deny access, specifically the federal government, police, and public health agencies.\nSo although HIPAA may appear to be more restrictive regarding the distribution of records, this restriction only applies to certain individuals not among the HIPAA approved agencies. Many clients and clinicians are not aware of these contradictions due to the convolution of the regulations themselves. Michael Freeny believes that many therapists prefer to do the bare minimum in regards to HIPAA compliance due to the time it would consume to actually understand the guidelines at a deeper level. Do you agree?\n♦ Controversy #3 - Employer Access\nIn addition to governmentally accessed information and contradictory language, the third HIPAA privacy controversy is employer access. As we discussed in section 4, certain authorities may access information illegitimately and use it to the disadvantage of the client. However, certain employers who operate under the Employee Retirement Income Security Act as insurers for their employees have access to their employees' health records.\nThis information includes symptoms, medication, and even diagnoses. According to HHS, employers are not allowed to use this information in any business transaction. However, there are no consequences implemented to force the employer to take responsibility for his or her action.\nJohnny, age 32, suffered from alcoholism. He had just obtained a job with a packing company who acted on the Employee Retirement Income Security Act and had access to his mental health records. After about a month at this company, he was fired, even after a positive evaluation. His employer stated that Johnny had been a part of a series of cutbacks, although no other employees, even those with less experience and worse evaluations, had been let go. Johnny believes that his supervisor had been told about Johnny’s drinking problems through this act, but there was no way to prove he had been the victim of this discrimination.\nThink of your Johnny. Could he or she run the risk of losing his or her employment due to his or her disorder?\nIn this section, we discussed three controversies created by HIPAA and its possible breach of privacy boundaries. These three HIPAA privacy controversies included: governmentally accessed information; contradictory language; and employer access.\nPeer-Reviewed Journal Article References:\nBenefield, H., Ashkanazi, G., & Rozensky, R. H. (2006). Communication and records: Hippa issues when working in health care settings. Professional Psychology: Research and Practice, 37(3), 273–277.\nCampbell, L. F., & Norcross, J. C. (2018). Do you see what we see? Psychology's response to technology in mental health. Clinical Psychology: Science and Practice, 25(2), Article e12237.\nDouglas, S., Jensen-Doss, A., Ordorica, C., & Comer, J. S. (2020). Strategies to enhance communication with telemental health measurement-based care (tMBC). Practice Innovations, 5(2), 143–149.\nGlueckauf, R. L., Maheu, M. M., Drude, K. P., Wells, B. A., Wang, Y., Gustafson, D. J., & Nelson, E.-L. (2018). Survey of psychologists’ telebehavioral health practices: Technology use, ethical issues, and training needs. Professional Psychology: Research and Practice, 49(3), 205–219.\nRichards, M. M. (2009). Electronic medical records: Confidentiality issues in the time of HIPAA. Professional Psychology: Research and Practice, 40(6), 550–556.\nStiles, P. G., & Petrila, J. (2011). Research and confidentiality: Legal issues and risk management strategies. Psychology, Public Policy, and Law, 17(3), 333–356.\nWhat are three controversies created by HIPAA and its possible breach of privacy boundaries? To select and enter your answer go to ."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:930f1ede-d6cf-4ed2-ad32-a11edd1a9463>","<urn:uuid:e84a5fa8-27a3-4eb0-be19-a8ca48d08943>"],"error":null}
{"question":"Could you compare the modern-day transportation advantages of Philadelphia's Old City district and Markham Village for residents who commute to their respective major cities?","answer":"Both areas offer distinct transportation advantages for commuters. In Philadelphia's Old City, residents can easily walk anywhere within 15 minutes, and the area is well-connected to other city neighborhoods. Meanwhile, Markham Village offers more structured commuting options to Toronto, with the Markham Village GO station providing a 30-40 minute train ride to downtown Toronto. The area also benefits from proximity to Highways 7 and 407, making it convenient for both rail and car commuters. Markham Village residents can reach their GO station within 10 minutes by car or 7 minutes by bicycle, and the area is served by York Region Transport buses along four main roads. While Old City emphasizes walkability within Philadelphia proper, Markham Village appears better equipped for longer-distance commuting to Toronto.","context":["WHY GO: From its inception, Philadelphia has been a city of diverse “Makers.” Founded as a center of commerce rather than a religious colony, newcomers didn’t have to pay that irksome Church tithe, so strivers from all backgrounds sought their fortunes here. By 1740, Philadelphia was the largest city in the colonies – an engine of industry. In 1754, one German immigrant observed, “Pennsylvania is heaven for farmers, paradise for artisans, and hell for officials and preachers.” This “paradise for artisans” has gone through a rebirth in recent years, revitalizing Philadelphia’s flagging neighborhoods, bringing a distinctive creative energy to each. You can explore each of 14 identified neighborhoods – none more than a 15 minute walk or drive from Old City – through hot restaurants, attractions and shops. Or, become a “Maker” yourself in one of the many studios cropping up all over the city. Highlighted here are a few neighborhoods to get you started.\nOLD CITY/HISTORIC PHILADELPHIA\nThis is the Philadelphia that most tourists know. Independence Hall, the Liberty Bell, Constitution Museum – all here. But as you can see, Old City encompasses so much more.\nTOUR: Elfreth’s Alley. Built in 1702, Elfreth’s Alley is known as “America’s oldest residential street.” Artisans, including a free African tailor, a Jewish furniture maker, German shoemakers and bakers, female dressmakers (Mary Smith and Sarah Milton) among others lived side by side in 32 tiny homes that also served as shops, sales rooms and studios. Take a tour with loquacious Ed Mauger, President of Elfreth’s Ally Association, and he’ll let you in on a few “clues” about how to tell 300 year old homes from the newest 200 year old homes (hint – it’s in the bricks and number of steps), and point out some of Ben Franklin’s innovations still in use (triple mirror outside 2nd floor window to see who was at front door, and street lamps made with replaceable panes of glass and vent for oil heat). Tours $5, Noon and 3pm, April – December, Tuesdays – Saturdays: 10-5, Sundays: 12-5\nVISIT: National Liberty Museum. It’s a quirky little museum dedicated to the idea of Freedom and its definition. Though you’ll be captivated by beautiful Chihuly glass art, go for this reason: you can hear our “original Liberty Bell,” made in the same foundry in England with the same materials – this one without the crack. $7 adults, $2 kids, open Tues-Sat 10-5, Sun Noon-6.\nDO: The Center for Art In Wood. Create your own “Narrative Vessel,” dreidel or other wooden crafts at this museum cum workshop. Even if you don’t sign up for a four 4-hour hands-on class, stop in to admire the sensual works of art. $35 Make and Take Workshops, One Saturday per month Noon-4pm.\nDO: The Clay Studio. This isn’t your typical “paint already made clay pieces.” The Clay Studio is an actual throw the pot kind of place, and offers Saturday clinics and the occasional Friday Night Date Nights with wine and beer, where you can reenact the sexy pot-throwing scene from Ghost – if only in your head.\nSEE: A Show at Fringe Arts. The new permanent theater and headquarters for what is locally known as “The Fringe” is now based inside a 1903 Fire Pumping Station, the first of its kind in the country and built to create enough pressure for water to reach the tops of new-fangled turn of last century 30-story skyscrapers. Decommissioned in the 1950’s and vacant until 2012, “this building used to save lives and now it will enrich lives,” says David Harrison, VP, who points out the cast iron pipes and pressure gauges kept as architectural elements. With year-round programming, classes, concerts, and events in a 240-seat theater– not to mention a soaring bar/restaurant designed for live entertainment and priced for starving artists to billionaires, Fringe Arts will surely bring offbeat-theater-loving mobs to this special spot under the Ben Franklin Bridge on the Delaware River. Consult website for shows and times\nSHOP: Art in the Age of Mechanical Reproduction. “Art in the Age” began as a T-Shirt line and now stocks all manner of handcrafted items, aka “stuff made with thought.” Shop for jewelry, artwork, clothing and books, including upcycled accessories from wunderkinds Walter and Margeaux Kent’s Peg and Awl. Art in the Age has also partnered with the group that created Henricks Gin to distill 80 proof spirits. Though you can’t purchase bottles of these robust liqueurs named Root (from root beer), Snap (gingersnap) and Rhubarb, in the shop, you can taste them for free during Thursday Night Happy Hour and then buy them from local liquor stores.\nSHOP: United By Blue. Trendy t-shirt merchants, Mike Congi and Brian Linton vow to remove one pound of trash from the planet’s oceans and waterways for every product sold. Utilizing their considerable marketing talents for good, United By Blue is spreading countrywide. This flagship store is the original, and includes a coffee bar where you can purchase a shot of organic esspresso while considering how many t-shirts to buy.\nBest Places to Eat In Old City, Philadelphia\nEAT: Zahav. A selection of salads, fresh-baked breads, humus, Moroccan Chicken ($10), Veal Stuffed Grape Leaves ($12); this is not your father’s Falafel restaurant. James Beard winner, Michael Solomonov elevates Israeli cuisine to remarkable heights in this contemporary golden space (Zahav means “gold” in Hebrew). You’ll want to order from the four course-tasting menu ($39), an insider favorite, so you can all share every single delight.\nEAT: Paesano’s. It’s the kind of place where the ole spring-loaded screen door slaps shut behind you as you cram into a line three-deep waiting to order massive, made-from scratch sandwiches. There’s no place to sit, so take the namesake concoction- brisket, horseradish mayo, provolone, roasted tomato and fried egg mounded on a roll across the street to the Race Street Pier and watch boat traffic glide by as cars pass overhead on the Ben Franklin Bridge.\nEAT: Wedge and Fig. On temperate nights, opt to eat in the adorable outdoor patio garden you’ll discover down a long narrow alleyway in back of this great neighborhood “Cheese Bistro.” Bring your own wine and order cheeses “bundled by region.” Cheese Slates $18 for 3, $26 for 5. Perfect nibble following a day in the workshop.\nEAT: Han Dynasty. This is the original, the first, and the incredible authentic Szechuan food restaurant that just swept Adam Platt, New York Magazine’s food critic, off his feet after his foray to Han Dynasty’s East Village, NYC outpost. Yes, there are currently seven restaurants in the group, but go to the first on Chestnut (soon to move across the street into larger space). Signatures like the Dan Dan Noodles ($6.95), Spicy Cucumbers ($6.95) and Cold Sesame Noodles ($6.95) and entrees identified by “heat” on a 1-10 scale, represent the Gold Standard in Szechuan cuisine and have won over the Travel Channel, Food Network and other aficionados of tongue-tingling sauces.\nICE CREAM: Franklin Fountain. Though not as dense and creamy as say, Ben and Jerry’s, Franklin Fountain’s creations are just as inventive. And the black and white mosaic floor, tin ceiling, Chinese take-out cups, servers dressed in bow ties and pearls, and working 1910 cash register is a heartwarming throwback to simpler times.\nGRADUATE HOSPITAL NEIGHBORHOOD\nTransformed over the last few years, it’s just a 15-minute walk from Center City and growing in popularity with the young professional crowd. Named after the large research hospital that has since closed, Graduate Hospital is known for its bars and overflows with an eclectic mix of restaurants and is just starting to develop retail. You’ll recognize some of the residential areas from the movie The Sixth Sense. The main characters lived here.\nDO/TOUR: NextFab Studio. Proclaimed a “Gym for Innovators,” this collective workspace and idea incubator brings industrious creative’s together and is the nationally recognized premier facility of its kind in the Northeastern US. Founded in 2009, it’s a workshop on steroids, a for-profit, membership based derivative of the FabLab movement. Currently 300 members pay $1300 per year to access two MILLION dollars worth of machinery and a coterie of top-notch instructors. But even non-members can pop in and take a 2-hour class in laser-cutting, 3-D printing, Adobe Illustrator and plenty of other hands-on projects. Free one-hour tours are offered 2-6 daily on weekdays or by appointment on Sat/Sundays. Two hour classes for non-members $79-$99.\nBest Places to Eat in Graduate Hospital Neighborhood, Philadelphia\nCOFFEE: Ultimo Coffee. “Trained and certified baristas, sweet, sweet milk from Lancaster County,” Ultimo has been named “One of the Best Coffee Shops in America.”\nEAT: Honey’s Sit ‘N Eat. In the former Schwemmer’s Hardware store, goods line old shelves, antique billboards and store signs hang from the ceiling: it’s the perfect venue for comfort food. “Nothing healthy but all delicious,” says our waitress. Eggs used to make sizeable omelets come from free-range Lancaster County chickens, and calorie-counting be damned, Honey Cristo ($14) – challa French toast stuffed with double-smoked ham and Swiss cheese, is one amazing signature dish. If you’re watching pennies, come on weekdays between 7 and 9am for the $3.95 two eggs, potato, toast and coffee special.\nEAT/DRINK: Resurrection Ale House. Philly residents from all neighborhoods venture here just for the nationally renowned Fried Chicken and stay for the 12 craft beers on tap.\nDRINK/SCENE: Bob and Barbara’s. Where else can you imbibe with “liquor drinking music” out of a Hammond B-3 Organ combo? Enjoy “Thursday Drag Night” with the “Philly Special” – a shot of Jim Beam followed by a can of Papst Blue Ribbon Beer.\nNorthern Liberties area of Philadelphia was so named because plots of land in the then rural area were given away free with the purchase of acreage in Center City as per William Penn in 1682. Now, steel and glass contemporary buildings coexist with original brick row houses drawing artists and hipsters to this once-blighted former domain of Schmidts Brewery. Developed by Philadelphia’s answer to Donald Trump, Bart Blatstein, the Northern Liberties district has been re-imagined as a place where people can live, work, shop and eat within a cozy few blocks, so it’s no surprise that the neighborhood exudes a trendy European/Founding Fathers vibe.\nVISIT: Yard’s Brewing Company. The owner of Yards Brewery, Tom Kehoe, is as innovative as his brews are awesome. “Ales of the Revolution” named for Washington, Jefferson and “Poor Richard” (Ben Franklin Ale already existed) were brewed following our founding fathers’ own recipes from the 1700’s. Belly up to the tasting bar- a recycled bowling ally lane, to sip dark but mild Brawler, bestseller citrusy Philadelphia Pale Ale and “sweeter than most” IPA among plenty more. Each week a different food truck is stationed right outside the door, and Yards curates a beer-pairing menu for an ultimate lunch or dinner experience. Free 30-minute tours Sat and Sun noon – 4pm – you get three tastes. Tasting room open daily noon to 7pm.\nTOUR: Taste of Northern Liberties. If you learn about history and place through your stomach, this is the perfect tour for you. Humorously entertaining guides lead you through the Piazza and Liberties Walk on a 2-½ hour tour of Northern Liberties, where you’ll sample treats from Bubble Tea to Thai noodles at five restaurants. At $44, this is one of the best deals around. Sat/Sun 1:30-4pm.\nCOFFEE: One Shot Cafe. Philly is known for its coffee-shops-that-are-not-Starbucks, which is a good thing. Indie One Shot fuels this energetic neighborhood.\nEAT: Bar Ferdinand. One of the most stunning interiors, it seems delectable even before you take a bite. But BF is known for excellent Tapas and has the Best Brunch in the City, according to fans.\nBest Places to Stay in Philadelphia, PA\nSTAY: Hotel Monaco or Palomar, Philly. These are Kimpton Hotels where designers are allowed to romp, imagine, and dress up lobbies and rooms to whimsical, colorful, idiosyncratic, yet extraordinarily comfortable and modern effect. A phenomenal complementary afternoon wine and nibble hour just adds to the fun.","Introduction to Old Markham & Markham Village\nThe Markham Village and Old Markham Village community is one of the central areas of Markham from which the rest of the city began to grow. There are some lovely old buildings, a great shopping street, century and modern homes and great amenities. Find out more about this lovely community below.\nThese two communities are adjacent to each other. The north and south boundaries are 16th Avenue and Highway 7 (although Markham Village does extend a little further south toward the rouge river on the east side of Markham road. The east boundary of Markham Village is Ninth Line and the west boundary of Old Markham Village is along the creek that runs parallel with Main Street until it reaches Bullock and then runs straight up west of Peter Street to 16th Avenue. The dividing line between the two communities runs along another creek that runs parallel to Paramount Road and Elm Street down to Highway 7.\nHistory of of Old Markham & Markham Village\nOld Markham is the historic centre of Markham. 64 families of Mennonites from Upstate New York led by William Berczy settled here in 1794 and began farming the land and developing industry around the saw and grist mills. This community was known as Reesorville for a while after a prominent mill owner Joseph Reesor. Over the years other groups moved to the area and industry grew. When the first post office was opened in 1828 the name was changed to Markham. The area has known unrest in the Mackenzie Rebellion of 1837 with tensions between reformers and the family compact. Captain John Button raised armed troops of militia to end the violence. Markham was a divided region during this time.\nBy 1857, improvements in transportation, growing population and urbanisation invited the growth of new industries like tanneries, farm implement and furniture manufacturers. Markham Village Station opened in 1871 but after the turn of the century the industries found themselves competing and losing to larger companies in Toronto and so by 1900 Markham Village was back to being a largely agricultural community.\nAfter World War II, Toronto was expanding and Markham became attractive to immigrants from all over the world and in the 1970’s highway 404 was established to better transport links between Toronto and its northern neighbours.\nThrough all the development, Markham Village has retained its charm and we can still see it’s roots showing through the urbanisation and change that has happened over the years. Markham Main Street especially is proud of its heritage and is careful to preserve of it.\nHomes in these communities vary greatly from historic century homes to brand new-builds. Most homes are built on large lots with no sidewalks on tree-lined streets which give a very country feel to the area. The majority of the homes are mid-century back splits, side splits or ranch bungalows. Many of the homeowners in the area have remodelled their homes over the years as opposed to moving out. The houses that are being sold are being re-built and transformed into much larger homes.\nTransport & Commuting\nThese communities have Markham Village GO station located on Markham Main Street. The proximity of Highway 7 and Highway 407 makes this area extremely convenient for both commuters who use the railway and who drive downtown. You can drive or cycle to the station from any part of the community in around 10-minutes or 7-minutes respectively. The commute from the station to downtown Toronto is usually around 30 – 40-minutes. It is relatively easy to get around by bus to other parts of Markham via York Region Transport as there are four main roads inside the community and four arterial roads surrounding it.\nShopping in Old Markham & Markham Village\nThe main shopping area is in Main Street Markham which is north of Highway 7 to the Bullock Drive area, where most of the stores are individually or family owned businesses. There is a plethora of choice and very interesting wares to see, food to eat and services offered. There is something for everyone. In the summer months there is usually a Farmers Market on a Saturday. Other plazas include Station Plaza – opposite the Station, a plaza at Wooten Way and Highway 7, Mint Leaf Plaza at the corner of Mintleaf Gate and 16th Avenue and a plaza at the corner of Fincham Avenue and 16th. All of these have lots of independently owned stores, restaurants and fast food take-aways.\nParks & Recreation\nOld Markham & Markham Village has a lot of parks to offer. Here is a list:\n- Walker Park – Children’s playground, outside gym equipment, large grassy areas, soccer field\n- Morgan Park – Children’s playground, baseball diamond, outdoor pool, splash pad\n- Carmen Lewis Park – natural wooded area with walking trails\n- Paramount Park – natural wooded area with walking trails\n- Reesor Park – Children’s playground, large grassy area, baseball diamond, 2 soccer pitches\n- Mintleaf Park – Children’s playground, practice soccer pitches, baseball diamond, large grassy area\n- Fincham Park – Children’s playground, soccer field, large grassy area, baseball practice nets.\nThe communities are host to Markham Tennis Club. Additionally, Markham Village Library is large and has meeting spaces for hire. Next door to the library is Markham Village Community Centre which offers a large ice rink with changing rooms and meeting rooms for hire. For fitness facilities and a gymnasium, Markham Centennial Community Centre is nearby.\nSchools in Old Markham & Markham Village\nThere are 3 elementary schools: Edward T Crowle Public School, Reesor Park Public School and Franklin Street Public School. Additionally, there is a high school – Markham District High School. All of which are highly regarded. For more information about schools in these communities click here for a complete guide to schools in Markham.\nOld Markham Village and Markham Village communities retain a huge amount of charm whilst still offering modern amenities, a mix of housing varieties, some lovely local shopping opportunities and great schools. Pride of heritage and community is evident here and it is an incredibly popular part of Markham!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:5039a8a0-8e35-4a3b-9e8f-93be03e2d816>","<urn:uuid:5afa875a-0a75-4e26-bf11-c45dc3acecf5>"],"error":null}
{"question":"What were the most notable achievements of Willie Davis and Jeff Hornacek in their professional sports careers?","answer":"Willie Davis was a five-time Pro Bowl defensive end who played for the Cleveland Browns and Green Bay Packers, winning NFL titles in 1961, 1962, 1965 and Super Bowls I and II. He was selected to the NFL's 1960s All-Decade Team and inducted into the Pro Football Hall of Fame in 1981. Jeff Hornacek, on the other hand, was known for his exceptional free throw shooting in basketball, achieving a career 87% success rate from the free throw line and an impressive 95% in his final NBA season. He was also known for his unique free throw routine of wiping his right cheek three times before each shot as a way to greet his three children.","context":["In the basketball world, free throws are also known as the charity stripe. The free throw line is called the charity stripe because they are free points if you can make them, which means that they are really important to take advantage of. However, there is no more individual aspect of the game of basketball than at the free throw line. Your teammates cannot help you, the coaches cannot help you, and the fans cannot help you. It is just you and the mental games that take place in your head.\nFor many players, the way that they calm themselves down from the rapid pace of the game to the stagnated pace at the free throw line is by going through some type of repetitious motion. Basketball games go 100 miles per hour, while the free throw line comes to a complete halt at 0 miles per hour. That can be a hard adjustment for anyone to make.\nWhat repetition does is help players remain composed, even keeled, and it also triggers muscle memory whenever players step up to the free throw line. There is no discrepancy at the free throw line. The hoop is always fifteen feet away and because players do not elevate to shoot free throws, there really are no variables when shooting free throws. The only exceptions are the way you bend your knees and release the ball.\nThis has led to some of the most interesting and unique approaches at the free throw line. There is no standard way of going through a particular motion prior to taking a free throw. Every player is different and some are even superstitious. The only thing that is important is that the mechanics remain the same without too much variation. And if it is not broke, don’t fix it.\nThis is a list of the top 10 most famous and ugliest free throws of all-time. What sets these free throw shooters apart from everyone else’s free throws are the personalized touch and the uncommon peculiarity that they add to their free throw routines compared to everyone else’s.\n10. Jermaine O’Neal\nJermaine O’Neal has shot anywhere from 50% to 84% from the free throw line. The reason for the discrepancy may be due to his unusual free throw routine. Prior to a series of injuries O’Neal was a 20 and 10 type of player, so he could always score with a high level of consistency. But whenever he got to the free throw line, he abandoned his shooting stroke for something that looked like Charles Barkley’s golf swing. Everything about O’Neal’s free throw form looked good until he was about to release the ball. While most players have a fluid motion, O’Neal has a little stutter in his shot. Instead of releasing the ball in one motion, O’Neal holds it for an extra 2 seconds right before he releases, faking out even his teammates who are trying to box out for the rebound.\n9. Karl Malone\nKarl Malone was not a great free throw shooter when he first entered the league. He actually shot 48.1% his first season and 59.8% his second season. However, by the end of his illustrious career, Malone shot a respectable 74.2% from the free throw line. One of the reasons for his improvement from the charity stripe may have been due to his mysterious routine whenever he got to the free throw line. When the Mailman got to the line, he would spin the ball with both hands and whisper something under his breath as he was about to shoot the ball. No one really knows what Malone was saying, but his free throw routine took so long, opposing crowds from different arenas would start counting “One…Two…Three” to alert the refs how long Malone took to shoot free throws; not to mention throw him off from his fadeaway free throw.\n8. Jerry Stackhouse\nJerry Stackhouse has a smooth stroke, but he also had one of the most interesting free throw routines ever. The fundamentals of basketball say that players are supposed to bend their knees as they shoot the ball, but Jerry Stackhouse takes this to a totally different level. When Stackhouse steps to the free throw line, he not only bends his knees, but he bends his knees so low that he nearly goes into a squatting position. But as exaggerated as his shooting form looked, it must have worked because Stackhouse was a 82.2% free throw shooter for his career.\n7. Nick Van Exel\nNick the Quick had one of the fastest and smoothest releases in the NBA. However, his career free throw shooting percentage was under 80%. One of the reasons why Nick may not have been an elite free throw shooter is because he shot 2 feet behind the basket. The free throw line is fifteen feet from the basket, but Nick at Night shot his free throws seventeen feet from the basket. While most players want to be as close to the basket as possible, for some reason Van Exel shot his free throws the furthest away from the basket.\n6. Gilbert Arenas\nGilbert Arenas had one of the longest free throw shooting routines in the NBA. The former point guard for the Washington Wizards would wrap the ball behind his back three times before he shot the ball. You only get ten seconds to shoot a free throw and Arenas took nearly all of those ten seconds to go through his shooting routine. But it must have worked somewhat for him because Arenas was a career 80% free throw shooter.\n5. Michael Adams\nMichael Adams had one of the ugliest shooting forms the league has ever seen. Adams rarely made use of his guiding hand, which made it look like he was shot putting the basketball. When Adams shot the ball, the ball never even made it up to his forehead. His shot was sort of off of his right shoulder and then heaved forward. The surprising part was that Adams was a career 85% free throw shooter. It just goes to show that even if a shot may look ugly, it does not mean that it is ineffective.\n4. Jason Kidd\nThe Brooklyn Nets head coach had one of the most famous free throw routines when he was playing. Kidd would take a few bounces and then with his left hand blow a kiss to his wife and kids before he shot the ball. Kidd had a rocky relationship with his current ex-wife and was even charged with domestic violence during his playing days, so his free throw kiss was a message to his family every time he shot the ball. By and large, Kidd was not a great free throw shooter when he first entered the league. In fact, he shot below 70% for the first five seasons of his NBA career. However, by the end of his career, Kidd shot a respectable 78% from the free throw line.\n3. Jeff Hornacek\nJeff Hornacek had one of the most personalized free throw motions in the NBA. Before every shot, Hornacek would wipe the right side of his cheek three times. The reason? It was a way of saying hello to his three kids without actually saying it. Hornacek was not only a good father, but also a good free throw shooter. Hornacek shot a career 87% from the free throw line. In fact, in his last season in the NBA, Hornacek shot a ridiculous 95% from the free throw line.\n2. Wilt Chamberlain\nWhile there is no video evidence for this because this was so long ago, Wilt Chamberlain was so poor at shooting free throws that he would attempt to dunk it from the free throw line every time he had to step up to the charity stripe. The rules have changed today, so that the free throw shooter cannot cross the free throw line prior to the ball touching the ball the rim. The reason for this rule today was because of players like Chamberlain who would jump from the free throw line towards the rim on his free throws. Can you imagine that happening today? What a spectacle that would be with today’s athletes.\n1. Rick Barry\nIf you saw Rick Barry shoot free throws today you might laugh. That is until you saw the rate at which Barry was hitting those free throws. Rick Barry had a career average of 89% from the charity stripe. In fact, he shot over 90% in seven different seasons. How did he do it? Barry shot his free throws underhanded like he was playing in a game of horse. The really interesting part about Barry’s underhanded free throws was that he did not put his hands underneath the ball, but on top of the ball. Most people put both hands underneath the ball when they shoot underhanded, but Barry put it on top of the ball for more of a backwards spin.\n- Ad Free Browsing\n- Over 10,000 Videos!\n- All in 1 Access\n- Join For Free!","Sports Birthdays for July 24 — Barry Bonds and More\nHere are some of the people in the sports world celebrating birthdays on July 24:\nProfession: Retired NFL Player\nBest Known For: A five-time Pro Bowl defensive end, WIllie Davis played with the Cleveland Browns and Green Bay Packers from 1958 to 1969. Selected to the NFL’s 1960s All-Decade Team, Davis played on the Packers’ teams that won the NFL title in 1961, 1962, 1965 and Super Bowl I and II. He was inducted into the Pro Football Hall of Fame in 1981.\nProfession: Retired Jockey\nBest Known For: The first-ever female jockey to win a Triple Crown race, Julie Krone retired in 2004 with 3,704 wins. Krone won the 1993 Belmont Stakes aboard Colonial Affair, marking the first time a woman had ever won a Triple Crown race. In 2000, Krone became the first woman inducted into the National Museum of Racing and Hall of Fame.\nProfession: Retired NBA Player\nBest Known For: Nicknamed ‘The Mailman’ for his consistent delivery of points on the basketball court, Karl Malone was a 14-time NBA All-Star power forward while playing for the Utah Jazz and Los Angeles Lakers from 1985 to 2004. A winner of the NBA’s Most Valuable Player Award in both 1997 and 1999, Malone’s 36,298 career points are the second most in NBA history. Malone was a member of the U.S. national team that won the gold medal at both the 1992 and 1996 Summer Olympics. He was inducted into the Basketball Hall of Fame in 2010.\nProfession: Retired MLB Player\nBest Known For: Baseball’s all-time career home run leader, Barry Bonds was a seven-time National League MVP, 14-time All-Star and 12-time Silver Slugger Award winner with the Pittsburgh Pirates and San Francisco Giants from 1986 to 2007. In 2001, Bonds set the major league record for home runs in a season with 73. In 2007, Bonds broke Hank Aaron’s career home runs record by hitting his 756th home run.; he finished his career with 762. Bonds is the only player ever to hit 500 home runs and steal 500 bases. In 2003, Bonds was implicated in the Bay Area Laboratory Co-operative (BALCO) scandal, in which his trainer, Greg Anderson, later admitted to providing athletes with steroids and other performance enhancing substances. Bonds testified about BALCO to a grand jury in 2003, and he was later indicted on four counts of perjury and one count of obstruction of justice in regard to that testimony. Bonds was convicted on the obstruction of justice charge in 2011 and sentenced to 30 days of house arrest. He has yet to serve the sentence pending an appeal.\nProfession: Retired NBA Player\nBest Known For: A member of the Boston Celtics and Los Angeles Lakers from 1991 to 2004, Rick Fox played for Lakers’ teams that won NBA championships in 2000, 2001 and 2002. In addition to basketball, Fox has had a career in entertainment, appearing in such films as ‘Eddie,’ ‘He Got Game’ and ‘Blue Chips,’ as well as being a contestant on the reality TV competition ‘Dancing With The Stars’ in 2010. He was married to actress Vanessa Williams from 1999 to 2004."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:3b2e8643-ec7b-45c0-981a-47c6bc72509b>","<urn:uuid:132dad49-6f81-44bb-b0b5-ee86b023c5b8>"],"error":null}
{"question":"What are the melting points of tungsten and titanium, and how do they compare?","answer":"Tungsten has a melting point of 3,410°C (6,170°F), which is the highest melting point of any metal. In comparison, titanium has a significantly lower melting point of 1,660°C.","context":["Tungsten is a transition metal. The transition metals are a group of elements found in the middle of the periodic table. They occupy the boxes in Rows 4 through 7 between Groups 2 and 13. The periodic table is a chart that shows how chemical elements are related to one another.\nThese metals have very similar physical and chemical properties. One of tungsten's unusual properties is its very high melting point of 3,410°C (6,170°F). This is the highest melting point of any metal. Another of its important properties is its ability to retain its strength at very high temperatures. These properties account for tungsten's primary application, the manufacture of alloys. An alloy is made by melting and mixing two or more metals. The mixture has properties different from those of the individual metals.\nGroup 6 (VIB)\nCredit for the discovery of tungsten is often divided among three men—Spanish scientists Don Fausto D'Elhuyard (1755-1833) and his brother Don Juan Jose D'Elhuyard (1754-96), and Swedish chemist Carl Wilhelm Scheele (1742-86). Tungsten's chemical symbol, W, is taken from an alternative name for the element, wolfram.\nDiscovery and naming\nThe first mention of tungsten and its compounds can be traced to about 1761. German chemist Johann Gottlob Lehmann (1719-67) was studying a mineral known as wolframite. He found two new substances in the mineral but did not recognize that they were new elements.\nAbout twenty years later, Scheele also studied this mineral. He produced from it a white acidic powder. Scheele knew the powder was a new substance. But he could not isolate a pure element from it. Scheele's discovery was actually tungstic acid (H 2 WO 4 ). (See sidebar on Scheele in the chlorine entry in Volume 1.)\nTungsten metal was prepared for the first time in 1783 by the D'Elhuyard brothers. In 1777, they were sent to Sweden to study mineralogy. After their return to Spain, the brothers worked together on a number of projects. One project involved an analysis of wolframite. They produced tungstic acid like Scheele but went one step further. They found a way to obtain pure tungsten metal from the acid. For this work, they are generally given credit as the discoverers of tungsten.\nThe name tungsten is taken from the Swedish phrase that means \"heavy stone.\" In some parts of the world, the element is still called by another name, wolfram. This name comes from the German expression Wolf rahm, or \"wolf froth (foam).\" The element's chemical symbol is taken from the German name rather than the Swedish name.\nTungsten is a hard brittle solid whose color ranges from steel-gray to nearly white. Its melting point is the highest of any metal, 3,410°C (6,170°F) and its boiling point is about 5,900°C (10,600°F). Its density is about 19.3 grams per cubic centimeter. Tungsten conducts electrical current very well.\nTungsten is a relatively inactive metal. It does not combine with oxygen at room temperatures. It does corrode (rust) at temperatures above 400°C (700°F. It does not react very readily with acids, although it does dissolve in nitric acid or aqua regia. Aqua regia is a mixture of hydrochloric and nitric\nOccurrence in nature\nTungsten never occurs as a free element in nature. Its most common ores are the minerals scheelite, or calcium tungstate (CaWO 4 ) and wolframite, or iron manganese tungstate (Fe,MnWO 4 ). The abundance of tungsten in the Earth's crust is thought to be about 1.5 parts per million. It is one of the more rare elements.\nThe largest producers of tungsten in the world are China, Russia, and Portugal. No tungsten was mined in the United States in 1996. Detailed information about the production and use of tungsten in the United States is not available. This information is withheld from the public to protect the companies that produce and use tungsten.\nIn some parts or the world, tungsten is still called by another name, wolfram. This name comes from the German expression Wolf rahm, or \"wolf froth (foam).\"\nFive naturally occurring isotopes of tungsten exist. They are tungsten-180, tungsten-182, tungsten-183, ungsten-184, and tungsten-186. Isotopes are two or more forms of an element. Isotopes differ from each other according to their mass number. The number written to the right of the element's name is the mass number. The mass number represents the number of protons plus neutrons in the nucleus of an atom of the element. The number of protons determines the element, but the number of neutrons in the atom of any one element can vary. Each variation is an isotope.\nAbout a dozen radioactive isotopes of tungsten are known also. A radioactive isotope is one that breaks apart and gives off some form of radiation. Radioactive isotopes are produced when very small particles are fired at atoms. These particles stick in the atoms and make them radioactive.\nNone of the radioactive isotopes of tungsten has any important commercial use.\nTungsten metal can be obtained by heating tungsten oxide (WO\nIt also results from passing\ngas over hot tungstic acid (H\nBy far the most important use of tungsten is in making alloys. Tungsten is used to increase the hardness, strength, elasticity (flexibility), and tensile strength (ability to stretch) of steels. The metal is usually prepared in one of two forms. Ferrotungsten is an alloy of iron and tungsten. It usually contains about 70 to 80 percent tungsten. Ferrotungsten is mixed with other metals and alloys (usually steel) to make specialized alloys. Tungsten is also produced in powdered form. It can then be added to other metals to make alloys.\nAbout 90 percent of all tungsten alloys are used in mining, construction, and electrical and metalworking machinery.\nAbout 90 percent of all tungsten alloys are used in mining, construction, and electrical and metal-working machinery. These alloys are used to make high-speed tools; heating elements in furnaces; parts for aircraft and spacecraft; equipment used in radio, television, and radar; rock drills; metal-cutting tools; and similar equipment.\nA small, but very important, amount of tungsten is used to make incandescent lights. The very thin metal wire that makes up the filament in these lights is made of tungsten. An electric current passes through the wire, causing it to get hot and give off light. It does not melt because of the high melting point of tungsten.\nProbably the most important compound of tungsten is tungsten carbide (WC). Tungsten carbide has a very high melting point of 2,780°C (5,000°F). It is the strongest structural material. It is used to make parts for electrical circuits, cutting tools, cermets, and cemented carbide. A cermet is a material made of a ceramic and a metal. A ceramic is a clay-like material. Cermets are used where very high temperatures occur for long periods of time. For example, the parts of a rocket motor or a jet engine may be made from a cermet.\nA cemented carbide is made by bonding tungsten carbide to another metal. The product is very strong and remains strong at high temperatures. Cemented carbides are used for rock and metal cutting. They can operate at 100 times the speed of similar tools made of steel.\nTungsten has no essential role in the health of plants, humans, or animals. In moderate amounts, it also presents virtually no health danger.","Titanium in Steels\nTitanium in Steels\nTitanium (Ti) (atomic number 22 and atomic weight 47.90) has density of 4.52 gm/cc. Melting point of Ti is 1660 deg C and boiling point is 3287 deg C. Ti is a highly active element. It usually forms a stable oxide coating at room temperature on its surface, which limits further oxidation. The phase diagram of the Fe (iron)-Ti binary system is at Fig 1.\nFig 1 Fe-Ti phase diagram\nTi forms stable compounds with oxygen (O), carbon (C), nitrogen (N) and sulfur (S) at temperatures of steelmaking. It is sometimes used in steelmaking because of its property for fixing of these elements in order to reduce their harmful effects. Ti is also used for the purpose of grain refining in many steels. In many respects, functions of Ti are similar to the addition of both aluminum (Al) and niobium (Nb). Ti is more expensive than Al; hence it is rarely used as a deoxidizer.\nThe reactivity of Ti is similar to that of magnesium (Mg) and it can quite easily be set on fire. It burns with a bright white flame, which can be harmful to look at. Ferrotitanium powder is also flammable, with the powder having finer size and higher Ti content being more hazardous.\nTi ores are mainly ilmenite (FeO.TiO2) and rutile (TiO2).\nTi containing addition agents are Ti metal scrap, ferroalloys and master alloys. Ti metal scrap may be of commercial purity Ti. Ti metal scrap is of two types one with 6 % Al and 4 % vanadium (V) while the second with 6 % Al, 2 % tin (Sn), 4 % zirconium (Zr), and 2 % molybdenum (Mo). Sn is usually an unwanted element in steels. Since the melting point of Ti is higher than that of the liquid steel hence it generally enters the steel by dissolution, rather than melting. Also due to the low density of Ti, it is to be forced into the liquid steel to avoid its oxidation and consequently it has a low recovery.\nFerrotitanium (Fe-Ti) is a ferro alloy of Ti and is available in many grades, but the most common grades contain either 10 % to 20 % Ti or 45 % to 75 % Ti. Fe-Ti may contain a small amount of C. Since the main use of Ti is as a cleansing agent for S, C, O and N, hence Fe-Ti with the lowest possible content of these elements is preferred for steel making. Other impurity elements in Fe-Ti can be chromium (Cr), Ni, Zr and Cu.\nProprietary alloys will contain titanium plus additional elements such as Al, Zr, Cr, or silicon (Si). They are not commonly used as Ti additions per se, but rather for a combination effect, such as sulfide shape control or increased yield strength. In some cases, they are added simply as a protection mechanism for boron (B), which would otherwise be lost from the liquid steel by its reaction with N or O.\nThere has been a trend towards the use of cored wire for the addition of Fe-Ti. Fe-Ti (70 % Ti) of size 2 mm or finer is encapsulated inside a sheath of mild steel, wound onto a coil. By this method, Fe-Ti can be added continuously to a liquid steel bath, launder or tundish. The steel sheath protects the ferroalloy from oxidation. The fine size of the ferroalloy ensures quick and high recovery of Ti.\nFe-Ti with 70 % Ti 30 % Fe falls at the eutectic point of the system, with a melting point of 1085 deg C. It is known as eutectic Fe-Ti. As this is well below steelmaking temperatures, the ferroalloy melts, which is far quicker than the dissolution required for metallic Ti to enter the steel. It is for this reason that eutectic Fe-Ti is the preferred analysis for use in steelmaking.\nSince Ti oxidizes easily, it is necessary that care is taken during additions. In case precautions are not taken during additions then the Ti recoveries are reduced drastically. Hence all efforts are needed to avoid contact between the Ti addition and air or oxidizing slag.\nTi additions are typically made in the ladle as scrap or lump during tapping, as cored wire into the ladle, or as lump during secondary steel making processes. The liquid steel should be thoroughly deoxidized first, usually with a prior addition of Al. Normally it is preferred to add the Ti late in the tapping, when the ladle is half to three fourth full and after all other additions except B. The purpose is to leave as little time as possible for reoxidation of the steel to occur, since the Ti takes up any leftover O, thereby reducing its effectiveness. Also contact with oxidizing slag need to be avoided, since the slag/Ti reaction is detrimental to the Ti recovery and the coating action of slag impedes the rapid melting and dispersion of the Fe-Ti in the liquid steel.\nWhen additions are necessary in the steel making furnace then Ti is to be added only after thorough deoxidation and just before the tapping. A thin layer of reducing slag helps preventing reoxidation and acting as a barrier as well.\nTi recoveries are usually lower than those for most of other additives. Recovery normally varies in the range of 50 % and 90 % depending on the practice of the steelmaking practice and the type of additive used. Fe-Ti usually gives more recovery than scrap because of its higher density (less likely to float on the liquid steel surface and oxidize) and its lower melting temperature. Hence Fe-Ti is normally the preferred addition agent.\nRolling and hot working\nRolling of steels having Ti is fairly straightforward and follows, in general, the pattern established for other controlled rolled products. Titanium nitride (TiN), formed while the steel is still liquid, is carried over into the rolled product thus, changes in soaking temperature has little effect unless niobium (Nb) is also present in the steel. Most of the strengthening imparted by Ti is due to the precipitation of titanium carbonitrides [TiC(N)] while the grain refinement has lower contribution. Hence steels with Ti have poorer impact resistance than steels having other micro alloying elements unless extra precaution is taken to ensure very fine possible grain size in the rolled steels. Lowering the finishing temperature to about 840 deg C has a beneficial effect on impact transition temperature. For higher strength products (YS above 550 MPa), coiling temperatures in hot strip mills are to be kept below 650 deg C.\nSulphides present in steels tend to become elongated during hot rolling. This leads to poor impact properties and reduced ductility. Elongation of sulphides can be harmful in welded structures. Ti controls the sulphide morphology since it allows them to retain a less harmful globular shape throughout the hot rolling process.\nInfluence of Ti on steels\nThe effects of Ti on heat treatment and microstructure are directly related to its reactivity mainly with respect to C. Hence pearlitic Ti bearing steels contains less cementite (less pearlite) because any Ti not already combined with C or N forms the highly stable carbide (TiC). C scavenged in this manner is about 0.25 % of the available Ti. This means that pearlite is completely absent in steels containing Ti which is equal to more than four times the C content. There are no complex carbides of Fe and Ti.\nTi increases the grain coarsening temperature. In this respect it is much more effective than Al when concentrations of either element exceed 0.035 %. However, steel containing 0.03 % Al and 0.005 % Ti has a grain coarsening temperature which is higher by around 30 deg C.\nThere is a complex effect of Ti on hardenability. Hardenability normally decreases with the increase in the Ti concentration unless austenitizing temperatures are raised. This is due to the formation of TiC which lowers the C concentration in austenite. Further, Ti refines the grain size, and this decreases hardenability all the more. If only acid soluble (uncombined) Ti is considered, its hardenability factor is about the same as that for Mo.\nTi steels also exhibit secondary hardening upon tempering due to the precipitation of TiC. The effect is increased when austenitizing temperatures are above 980 deg C.\nThe main use of Ti as an alloying element in stainless steel is for carbide stabilization. It combines with C to form Ti C which is quite stable and hard to dissolve in steel, this tends to minimize the occurrence of inter-granular corrosion when adding approximately 0.25 % -0.60 % Ti. The C combines with the Ti in preference to Cr, preventing a tie up of corrosion resisting Cr as inter-granular carbides and the accompanying loss of corrosion resistance at the grain boundaries.\nTi is used in stainless steels and it has a role of a carbide stabilizer. Austenitic grade of Type 321 contains Ti which is equal to at least five times that of the C content in order to prevent the precipitation of Cr carbides on grain boundaries during extended holding at higher temperatures. Without the presence of Ti, Cr gets depleted at grain boundaries, leading to inter granular corrosion.\nThe carbide fixing properties of Ti are also valuable in ferritic grade stainless steel Type 409 stainless steel. This steel is widely used in the manufacture of automotive catalytic converters and other exhaust components.\nLow strength, highly formable interstitial free (IF) steels are used for auto body pressings of complex geometry and for tinplate beverage cans. Ti is added at typically ten times the C content of the steel, to form TiC and TiN, so freeing the steel of dissolved C and N.\nHigh strength low alloy (HSLA) steels rely on a combination of precipitation of carbides and nitrides, and grain refining for their strengthening mechanism. Though Nb and V are the main micro alloying elements for this, yet Ti is also used, especially for the precipitation of [TiC(N)], following controlled rolling and rapid cooling. [TiC(N)] is the only micro alloyed carbonitride which is stable at the high temperatures attained in the HAZ (heat affected zone) during welding, where it reduces grain growth and increases toughness. Ti also forms its nitride at very high temperatures and is therefore used to reduce grain growth of austenite during hot rolling of plates.\nTi is used to protect B in steels for hardenability. Ti is added before B, to tie up any O and N in order to improve the effectiveness of the B addition.\nIn low alloy steels which have been grain refined with Al, aluminum nitride (AlN) may be the cause of inter granular fracture which is known as ‘panel cracking’. An addition of Ti causes TiN to be precipitated uniformly in the matrix and increases ductility. A Ti content of 0.05 % is typical for this application.\nThe precipitation of Ti inter metallic compounds is one of the principal strengthening mechanisms in maraging steels, which may attain yield strengths in excess of 3450 MPa.\nTi is occasionally used in tool steels, which it can make less susceptible to quench cracking. Because of their reduced air-hardening tendencies, such steels will develop tougher core structures.\nFor enameling steels an addition of typically 0.15 % Ti is needed to control the surface roughness for allowing a good, even coating of enamel."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:3752bd29-fd88-4c4a-a2cb-e9b963d4373e>","<urn:uuid:25ede537-134e-49e5-b8bd-3d7784b1a034>"],"error":null}
{"question":"Are domestic workers in the UK and Trinidad and Tobago both excluded from standard labor protections?","answer":"Yes, both countries have limitations on labor protections for domestic workers. In the UK, domestic workers can be exempt from the National Minimum Wage if they live at their employer's home and are treated as family members. They are also excluded from key limitations on working time and workplace inspections. Similarly, in Trinidad and Tobago, domestic workers cannot seek remedy for arbitrary dismissal under the Industrial Relations Act or the Retrenchment and Severance Benefits Act, and may lack access to basic protections like salary grants if their employer hasn't registered them as workers.","context":["This article is by Natalie Sedacca, a PhD Candidate and Teaching Fellow at UCL Faculty of Laws. Her research focuses on the intersection between labour rights and human rights in relation to domestic workers with two case studies, Chile and the UK.\nThe 16th of June is International Domestic Workers’ Day and marks the eight year anniversary of the International Labour Organisation’s Convention 189 on the rights of domestic workers (‘C-189’). This was a seminal date in the history of a sector marked by devaluation of work and a lack of labour rights. Yet the UK has not ratified C-189, a failing reflected by the fact domestic workers are, in some cases, exempt from some key labour rights applying to workers in other sectors. In addition, migrant workers in this sector are subject to a specific visa regime that increases their vulnerability to exploitation and abuse. The UK must pay close attention to the problems faced by these workers, including learning from countries which have ratified C-189 such as Chile, explored below.\nThe Overseas Domestic Worker visa – short term stays that encourage dependency\nMigrant domestic workers in the UK are often from non-EU countries such as the Philippines, India and Indonesia, and must obtain a specific type of visa for work in this sector – either a Diplomatic Visa for those working in a diplomatic household, or otherwise an Overseas Domestic Worker (ODW) visa. The worker must already have been working for that employer for at least a year before accompanying them to the UK (R159A of the Immigration Rules).\nReforms to the visa system in 2012 introduced a tie between the visa and the employer, making the worker’s right to work and remain in the UK dependent on the continued contract with their specific employer. This tie made it much more difficult for a worker to enforce rights at work and / or leave employment if conditions were poor. Research in 2014 by the migrant domestic worker organisation Kalayaan found that 45% of those working under pre-2012 non-tied visas had no day off, whereas, strikingly, all those surveyed working on the tied visa lacked a day off. Likewise, Mantouvalou’s study of migrant workers who had escaped their original employers found that they had been working 12-20 hours per day, and an independent review of the OWD visa in 2015 found that it lead to a lack of bargaining power, a sense of ‘being “owned” by an employer, or at least being trapped,’ and the risk of creating a large class of undocumented workers who lack legal protection.\nThis review therefore recommended, among other points, a right to change employer within the sector that was ‘not conditional upon claiming or proving any form of abuse.’ The government responded in 2016 by allowing those on an ODW visa ‘the ability to take alternative employment as a domestic worker with a different employer during the six month period for which they are originally admitted’ [emphasis added].\nIn practice, this does not implement the independent review’s recommendations in a meaningful way. As noted by Kalayaan and by Lord Hylton, without a right to renew the visa the same problem will remain: it is likely to be difficult or impossible for a domestic worker to find a new role when they have only months or even weeks remaining on their visa. This can leave them with little choice but to stay in abusive or exploitative employment situations. This highlights that it is not just the tied nature of the visa that poses a problem, but also crucially the fact that it is short-term and generally non-renewable, designed to deter long- or even medium-term residence, and therefore still tied in practice.\nSome Overseas Domestic Workers prevented from working after being found to be potential victims of trafficking\nThere is a limited exception to the non-renewable nature of the ODW visa when a domestic worker is identified as a (potential) victim of modern slavery or human trafficking in R159I-J of the Immigration Rules. The fact that a right which should be accessible across the board is restricted to these specific circumstances is problematic, and there are also serious shortcomings even for those that fit within this category. Empirical research I carried out for Kalayaan involving interviews with 21 workers, which is summarised in this blog post, demonstrates a particular flaw: the fact that some of those identified as potential victims of trafficking are denied permission to work while awaiting a final decision on their status, which can take months or even years. The research finds that the factor determining whether or not an individual in this position has permission to work is arbitrary and relates to factors that are generally outside the worker’s control, and that those who lacked permission to work reported being pushed into destitution and / or informal working, which was often degrading, exploitative and lacking in defined hours.\nBeyond these specific points on slavery and trafficking victims, there is an urgent need to reform the visa system more broadly, allowing all workers the possibility of extending their visa beyond the initial six month period – without which the removal of the tie to the employer is basically illusory. As highlighted in the recent FLEX report, short term and non-renewable visas make workers vulnerable to exploitation: having a high turnover of workers all with only short term stays has a number of negative implications, from making it more difficult to join a union to impeding possibilities for family reunification.\nThe labour law position – a lower standard of protection for domestic workers\nAs well as the visa scheme making it more difficult for domestic workers to access or enforce rights, there are also deficiencies in the level of rights protection formally available to such workers. At present the National Minimum Wage can be disapplied in relation to ‘work relating to the family household’ – the definition of such work includes that the worker lives at their employer’s family home and is treated as a member of the family ‘in particular as regards to the provision of living accommodation and meals and sharing of tasks and leisure activities’ (s57 of the National Minimum Wage Regulations). This shocking exemption, which reflects the devaluation of work carried out by women in the private sphere, is subject to an ongoing legal challenge.\nWorkers ‘employed as a domestic servant in a private household’ are also excluded from a number of the key limitations on working time, including the maximum average working week of 48 hours (s19 Working Time Regulations) and from provisions on workplace inspections that apply in other settings (s51 Health & Safety at Work Act). And while labour regulation in the UK suffers more broadly from a lack of labour inspection and enforcement, these issues are particularly stark in the domestic work sector when taken together with other factors: the isolation of being in a workplace usually consisting of one person, low levels of unionisation and – for migrant workers – the difficulties enforcing rights as a result of visa restrictions.\nWhile the UK lags behind, other countries are improving their standards\nIn December 2012, a joint letter from Anti-Slavery International, Kalayaan, Justice for Domestic Workers (now Voice of Domestic Workers) and the Trade Union Congress called on the UK government to ratify C-189, but more than six years later there is no sign of this being realised. In the meantime, it has now been ratified in total by 28 countries, many of which are in Latin America. My PhD research, in part, is exploring the situation in Chile which ratified C-189 in 2015. While Chile had previously seen a number of relevant reforms from 2008 onwards, substantial legislative changes were made alongside ratification, including on the area of working time, deduction of accommodation and food costs from salary, and uniforms worn in public.\nThese reforms are an improvement to the former situation and were supported by the domestic workers’ union federation who were involved in campaigning in their favour. That said, some difficulties remain – for example, although live-in workers are now entitled to rest on Saturdays (subject to accumulation, splitting or changing for another day with agreement) as well as on Sundays, they are not subject to the same limits and set hours as live-out domestic workers. Chilean sociologist Fernández has described shortcomings in the new law as giving ‘continuity to the culture of servitude’ despite also recognising it as an achievement. Likewise, limitations on inspection remain – entry to a private home is only permitted if the employer agrees, and they otherwise have the option of attending labour offices with required documentation. The union is campaigning for further reform including in the area of inspection.\nRatification of C-189 is needed but it should be seen as a floor not a ceiling\nThe 2012 joint letter to the UK government seeking ratification of C-189 pointed to the convention’s ‘considerable degree of flexibility.’ The Chilean example reflects this flexibility, given that rights have not been entirely equalised. On one hand, this demonstrates that concerns about ratifying the convention because of the burden it would impose are unfounded. On the other hand, it shows how C-189 can be interpreted in a way that is perhaps too flexible, meaning challenges beyond ratification remain. As recently noted by Blackett, an expert on domestic labour who was involved with the agreement of C-189, ‘despite the tremendous amount that has been accomplished… the depth of the challenge may still be underestimated.’\nIn this sense, ratifying C-189 and incorporating its provisions into domestic law should be seen as a starting point – a floor rather than a ceiling – and the process of ensuring that such rights are enforced as an ongoing process, in which domestic workers and their organisations must play a central role. To begin this course, the UK should urgently ensure that all domestic workers are entitled to the minimum wage and subject to defined limits on their working time, as well as fundamentally revising the visa scheme to allow for the possibility of renewal and fully removing the tie to the employer. International domestic workers’ day should bring a renewed impetus to end the current situation, in which an overwhelmingly female and migrant section of the working population can be denied the most basic working rights and further exposed to exploitation through unfair visa conditions.\nWith thanks to Natalie Sedacca for authoring this blog. Find her on Twitter @nataliesedacca.","DR GABRIELLE JAMELA HOSEIN\nTODAY, June 16, marks the tenth anniversary of International Labour Organisation (ILO) Convention 189, on decent work for domestic workers. The convention resulted from decades of advocacy and organising by working-class women and men, including TT’s own Clotil Walcott, who founded the National Union of Domestic Employees (NUDE) in 1974, and her daughter, Ida La Blanc, now general secretary of the union.\nWhy should this convention or even domestic workers matter?\nFirst, domestic workers labour in and around our homes; cooking, cleaning, washing, and caring for children. They help with the sick, disabled and elderly. They are essential workers in these spheres of work, and these are the most necessary and trusted tasks in our lives.\nDomestic workers are more valuable than sports heroes, celebrities and CEOs to the daily functioning of many households. Yet, they often remain insufficiently acknowledged, underpaid and poorly protected.\nTheir marginal status is the result of many things – the low value of care work and housework, and the fact that we don’t count hours worked in the private sphere of the home nor calculate their contribution to GDP.\nMost people who employ domestic workers don’t think of their homes as a part of the waged economy which should be governed by the same rules. As such, informal work takes place without formal agreements, and can include verbal abuse and unpaid extra hours. It is also stereotypically associated with women and femininity, which is key to its low value.\nPoor women also lack political or public policy influence, despite their numbers on the ground in elections, where they are meant to be unquestioningly supportive rather than assert collective demands. There is also the low regard given to labour issues generally, particularly for low-income employment. The disregarded struggle of sanitation workers for improved conditions of work is another example.\nAll this partly explains why domestic workers can now access basic conditions that include maternity benefits, sick leave and vacation leave in the way that salaried workers do, but may still have to fight in court to secure those rights.\nThat said, there has been an assault on salaried labour since PM Manning in 2002, and a deliberate turn to contract-based work, without benefits or job security, particularly for lower-level employees.\nAusterity measures now being adopted are spreading this model across multiple kinds of employers. In this “race to the bottom,” a new model is needed that provides basic protections for broad swathes of insecure and informal labour, including domestic workers.\nToday, we should remind ourselves how much of this we can change. Particularly in our current context where so many have lost income or jobs, we should become more conscious of those who cannot access salary grants if their employer hasn’t registered them as workers, who cannot seek remedy for arbitrary dismissal under the Industrial Relations Act (IRA) or the Retrenchment and Severance Benefits Act, whose employers may not make their National Insurance Scheme (NIS) monthly payments, leaving them without sufficient retirement benefits, and who may be subject to sexual harassment, violence or exploitation in their employer’s home.\nAs migrant women increasingly seek jobs in our economy, their conditions of household labour may be even more precarious and exploitative, particularly in terms of hours of work and wages.\nConvention 189 matters because, once ratified, which is when a state consents to be bound by this international treaty, then local laws have to be amended to include domestic workers in ways they currently do not.\nIn the ten years since this convention was adopted by the ILO, La Blanc and NUDE have tirelessly called for TT to ratify the convention. They have done so across two governments with labour leaders appointed as ministers of labour, and still nothing.\nOur local ILO 144 Tripartite Consultative Committee, comprising labour, business and government, unanimously recommended that “domestic workers” be included under the definition of “worker” under the IRA. It also called for the inclusion of protection for domestic workers in the Occupational Health and Safety Act (OSHA). Still, nothing. Imagine how many politicians employ domestic workers. Still, nothing.\nIn a May 17, 2014, interview in the Guardian, Le Blanc described “an invisible sea of often poorer people who are afraid to speak out about legitimate job issues for fear of job loss.” The pandemic has made that worse.\nSix years later, NUDE continues to call for ratification of ILO Convention 189. Forward ever, Ida. May solidarity with domestic workers help secure recognition and rights long overdue."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:44cf4309-763e-4c1a-936e-a6a4befb333a>","<urn:uuid:20be54df-e5b0-4408-a43f-a86945636663>"],"error":null}
{"question":"When preparing miso soup and Scottish broth, what's the key difference in how vegetables should be cooked at the beginning of the recipe?","answer":"In miso soup, when using vegetables like onions and cabbage, they should be fried over high heat until browned (with some slight burning being acceptable). In contrast, when preparing Scottish broth, the vegetables (onions, leeks, carrots, and celery) should be cooked low and slow without any color, being softened over low heat to sweeten them without caramelizing.","context":["In traditional Japanese cuisine, miso soup is served for breakfast every day, and often served with meals as well.\n- 1/2 tsp. of sesame oil\n- 3 cups of water\n- 2 Tbsp. of miso\n- 2 green onions, finely minced (optional)\n- 1/2 cake tofu, cut into 1/2 inch cubes (optional)\n- 2-3 fresh mushrooms, thinly sliced (optional)\n- 5-6 shrimp, cubed or shredded (optional)\n- Some pieces of seaweed (optional)\n- Heat oil in the bottom of a small pot\n- Add green onion (if using)\n- Cook for a few minutes while stirring frequently\n- Add 2 1/2 cups of water\n- Bring to a boil\n- Dissolve miso in 1/2 cup of water and add to pot\n- Lower heat and add mushrooms, seaweed & tofu if desired\n- 90% to 94% dashi (consider using instant dashi mix)\n- 6% to 10% miso. It can be aka (red) or shiro (white), or a combination.\n- odds and ends for garnish\nThe garnish would typically be two to three items that contrast in color, flavor, buoyancy, shape, texture, etc. Wakame with tofu is a standard and popular combination, especially at restaurants. Some common items for garnish are:\n- finely sliced and deep-fat fried tofu (Agedashi tofu)\n- finely cubed raw (silky) tofu (recommended)\n- wakame seaweed (recommended)\n- konbu (kelp) seaweed, perhaps left over from making dashi\n- chopped scallion (recommended)\n- grated daikon radish\n- boiled and finely cubed potato\n- clams (asari or shijimi)\n- grated eggplant\n- Put dashi in a pot.\n- Add any garnish that needs cooking.\n- Heat the dashi, letting it simmer, cooking any garnish that needs cooking. Do not bring to a rolling boil, as this degrades the flavor.\n- Add any garnish that does not need cooking, and remove from heat.\n- Add the miso to the soup. Avoid boiling the miso; some nuances of the flavor will be lost.\nWhile certainly a traditional food, miso is also suited to modern interpretations. One non-traditional yet delicious way to make miso soup is as follows:\n- Heat frying oil in a small pot\n- Fry onions and cabbage in the oil over high heat until browned. A slight degree of burning is acceptable.\n- Proceed with the traditional method listed above.\nUsing oil leftover from frying bacon and caramelizing the sugars in the onion and cabbage through high heat, this method produces a soup notably different from the traditional variety, and can add new interest to a classic dish. A small quantity of freshly ground black pepper added just before serving rounds out this method very well.\nSimple Restaurant Miso\nThis miso soup is a close approximation of what you get in most cheap western Japanese restaurants. All ingredients are to taste; the primary thing is the ratio of miso to dashi.\n- Dashi (roughly 250ml or 1 cup per person for a typical restaurant portion)\n- Wakame, 5 or so pieces per person (Wakame is a form of edible seaweed, sold in dry curls at most Asian markets)\n- Traditional tofu (medium firmness), a few one centimeter cubes per person\n- Miso paste, either white, red, or a mixture (many restaurants in western Canada use white miso, also called shiro miso)\n- Heat the dashi to a light simmer\n- While this is heating, lay out your bowls and place the wakame and tofu in the bottom\n- When the dashi simmers, take a ladle or so of liquid aside and mix into it approximately 1 tablespoon (about 15ml) of miso paste per cup of dashi stock\n- Remove the pot of dashi from the stove and stir the miso prepared above into it\n- Ladle some of the prepared miso into the bowls and serve\n- Some prefer less miso, some prefer more. The quantity given results in the moderately-salty type popular in restaurants before meals.\n- Do not permit the miso to boil; doing so alters the flavour","How to make Broth, a healthy staple of the Traditional Scottish diet\nGiven the current reputation of Scotland’s diet, you’d be forgiven for thinking “Scottish health food” was an oxymoron. But many traditional Scottish foods are a nutritionist’s dream. And one of the simplest is a Scottish Broth. Packed with nutrients and fibre and endlessly adaptable, it’s a perfect, satisfying meal for the winter months.\nBroth was a staple of the traditional Scottish diet. The way my mother tells it, there was a permanent pan of broth on every stove, in every farmhouse, across the country. That may no longer be true, but as autumn takes hold and winter approaches, it’s time to turn to this Scottish health food to warm your bones.\nWe don’t always celebrate traditional Scottish foods – ok, Robert Burns wrote a whole poem glorifying the haggis – but a Scottish broth is something to celebrate. Don’t confine yourself to Scotch Broth, that is a single interpretation of this style of soup.\nA Scottish broth is comfort food. Rustic food. A thick and chunky soup that’s a meal in itself. The ultimate, healthy-eating one-pot dish. It’s a style of soup fit for the climate in the north of these isles. And isn’t replicated elsewhere.\nBroth is rustic comfort food at its best\nDon’t confuse a Scottish style broth with the current health fashion for “bone broth.” Nothing wrong with bone broths, but nothing new either. Depending on the recipe, they’re stock or consommé rebranded. Healthy – yes. A satisfying meal – no.\nBurns chided the people eating fancy “French ragout” for sneering at the Haggis. And I would chide anyone if they turned their nose up at a Scottish broth in favour of a more “sophisticated” soup. Made well it may be rustic, but its elegance will please even the most refined palate. I know that from experience.\nAs a young commis chef, I worked in the 2 Michelin star Les Hauts de Loire. At the time the hotel was owned and run by the Bonnigal family. One of my responsibilities was preparing the daily potage (a thick French soup) for the terrifying matriarch,Madame Bonnigal. I was well warned. Upset Madame Bonnigal and be ready for the consequences. Young and foolish, I strayed from the standard French fare and made Cock-a-leekie, a classic broth. Horrified, the waiter refused to take it to her. Head chef, Rémy, and the restaurant manager fought over who would take the blame for this disaster. There was no time to change it. The broth went to Madame Bonnigal. 15 minutes later she came to the kitchen to congratulate Rémy. He humbly accepted her praise of his newest recipe. And I kept my job – just.\nTry to make your own broths. Granted, bought soups are more convenient. But they lack flavour and are nutritionally inferior. “Fresh” soups are over-priced and most tinned ones are not much better than junk food. Instant soup powders are junk food.\nHow to make broth\nWhat follows is not a recipe. A Scottish broth is a style of soup. This is a guide to making your own version. I’ve suggested variations that introduce other very un-Scottish flavours – call them an evolution of the traditional broth. As with my guide to making a healthy salad, I encourage you to experiment.\nAt its simplest, a Scottish broth has a lot of vegetables as its base, stock and a starch to thicken it. It’s a health food. I’ve suggested variations, some include extra fat or processed meats. Understand the context – they’re relatively small amounts in the overall dish. It’s still a healthy choice.\nThe starting point for almost any broth is the same. Onions, leeks, carrots and celery softened in butter or oil. The classic combination that professional chefs call a “Mirepoix”.\nDice the vegetables. Chefs get over excited about size and uniformity. But it’s not important. Think about what size you’d like find in your soup spoon later and aim for that.\nWhile you’re chopping, melt butter in a soup pan.\nAdd the vegetables and soften them over a low heat. The idea here is low and slow – no colour on the vegetables. Stir them every couple of minutes. The aim is to sweeten the vegetables without caramelising them. Put a lid on your pan and the vegetables are less likely to catch on the bottom.\nOnions, leeks, carrots and celery should make up the majority of your broth base. Adding other root vegetables to your mirepoix works well, but take care with strong flavours like turnip. And this is a good time to use up the stalks of other vegetables like broccoli. Or add a hard a vegetable like shredded white cabbage. But all tender vegetables should be added later.\nAdding a little meat\nAdding a little meat like streaky bacon to the base works well for some broths. If you want to try it, quickly fry the diced bacon in your pan over a high heat before you add the butter. Unlike the mirepoix, you want the meat to brown and add that tasty flavour to your broth. When the bacon’s a nice golden brown, take the pan off the heat. Let it cool before you add your butter. You don’t want to burn it. Reduce the quantity of butter if the bacon has released a lot of fat.\nLots of soups have a tomato flavoured base. To make a tomato-based broth, add tomato puree to the mirepoix when it’s halfway soft. You’ll need to pay more attention as tomato puree makes the vegetables more likely to catch the bottom of your pan.\nIf you’re going for a more Mediterranean feel for your broth, swap the butter for olive oil and add garlic and a pinch of herbes de Provence to your mirepoix. Instead of streaky bacon, try pancetta or chorizo (and if you add smoked paprika, it’ll boost the chorizo flavour).\nThere’s no harm in making your life easy. If you don’t want to do your own chopping, buy fresh or frozen ready chopped vegetables from the supermarket. Yes, sometimes the combination of vegetables isn’t great and the chopping can be a bit rough. But we don’t always have time to do everything.\nOnce your vegetables are soft add your stock or water and bring to the boil.\nNothing beats a home-made stock – but we don’t always have time. Honestly, it’s no big deal. There are alternatives.\nIf your broth is full of vegetables, a little salt may be all that’s needed to bring out the flavour. For a tomato based broth, replace some water with a tin of chopped tomatoes for greater depth of flavour.\nUsing butter in to soften your mirepoix gives your broth flavour. Take it further by trying a trick I learnt as a boy from a thrifty lady crofter.\nMrs Kennedy used to help my grandfather by doing little cooking for him. I have two lasting memories of Mrs Kennedy. To my young eyes, she was ancient – though in reality she probably wasn’t. And she made the most amazing lentil broth. Her secret? Lots of butter – she never used stock. To this day, I’ll occasionally do the same. And it makes a great lentil broth (or other pulse broths, without pulses the butter will tend to float to the surface). Use roughly 20-30g of butter per portion. In the context of an otherwise fat-free meal, that’s no big deal.\nStock cubes and concentrated stocks\nAll concentrated stocks have one thing in common – the main flavouring is salt. Half salt varieties have half the flavour. Add them to your broth instead of salt. You’ll get the benefit of the background stock flavour and the salt you need to bring out the taste in your broth. They come in different flavours, but vegetable and chicken are the best all-rounders.\nAdding cheap cuts of meat\nIf you want to turn your broth into the centrepiece of your meal, add a cheap cut of meat on the bone. Cook it in the broth and it will fill it with flavour. Good choices are lamb shank, chicken leg or ham hough. A large cut may take longer to cook than your broth. So, add it after the water and before the other ingredients. When it is almost cooked add everything else. When your broth’s ready, gently lift out the meat. It should be falling away from the bone. Let it cool enough to handle and shred it before returning it to the broth. A hearty meal fit for any winter’s day.\nThickening your broth\nAny starchy vegetable, pulse or grain will thicken your broth. Here are a few ideas for you to try.\nLentil broth is an all-time classic. Mirepoix, red lentils and stock. But why not play with it? Add bacon, sausage, tomatoes or curry spices.\nSplit peas are similar to lentils. Classic broths are made with either green or yellow split peas. Ham hough works well with both. Venture away from the classic two and experiment with other types of dahl.\nBorlotti and other beans will give your broth a rustic Italian feel that works well with tomato, garlic and pancetta in the base. Use canned if you don’t have time to pre-soak your beans. Before adding, crush them slightly to let the starch to escape and thicken your broth.\nThe classic Scotch broth is thickened with oatmeal and pearl barley. And it’s worth visiting your local health food shop to find them. Supermarket “broth mixes” are a mix of grains and pulses that don’t taste the same.\nRice is a good choice and used in the classic Cock-a-leekie.\nPasta is another option for an Italian twist to your broth. If your planning on keeping your broth, add your pasta when you reheat it. You’ll avoid it going limp and soggy.\nAny pureed or mashed vegetable will thicken a broth, although potato is the usual choice. It doesn’t over flavour the broth and cooks to a smooth finish. It’s the perfect way to repurpose left-over mashed potatoes. Alternatively, add diced raw potatoes and they’ll break up as they cook.\nIf you want your broth to be the main course of your meal pasta, rice and potato are not the best choices. They’ll fill you up quickly, but you’ll be hungry again in a couple of hours.\nWhatever you choose to thicken your broth add less than you think you need. Over thickening a broth is a classic rookie mistake. You can add more later if your broth isn’t as thick as you’d like and adjust your cooking time.\nStir the broth as you add your thickening ingredients. If you don’t they are more likely to stick to the bottom of your pan. Keep stirring until the broth returns to the boil. Turn it down to a very low simmer. Again, if you use a lid your broth is less likely to catch. Stir the broth occasionally until done.\n3 more tips\nAdding tender vegetables\nTender vegetables like courgettes, peas, green beans or fresh tomatoes are best added towards the end. You want to give them long enough to cook, but not much more. Leafy greens such as spinach and soft herbs can be stirred in just before serving and they’ll cook without going over.\nSalt but not pepper\nYou must add salt to your broth before eating it– unless you’ve used a concentrated stock. Salt brings out the flavours in your broth. Try your broth before serving and carefully add salt to taste.\nIf you’re making a spicy broth its best to be cautious. Hot liquids ramp up the spice kick. And that includes pepper. In a “plain” broth I’d recommend you don’t add pepper to the pan, but add it to taste in your plate.\nCooking your broth\nPreparing your broth is 15 minutes work. But it could take 1-2 hours to cook. If you don’t have that kind of time, set it cooking in a slow cooker before you leave in the morning. You’ll have a delicious supper waiting for you when you get home. Or cook your broth when you do have time, and store it for when you need it.\nStoring and reheating your broth\nIt takes almost the same time to make a large pan of broth as a small one. Save time and make two or three meals in one go. Broth will keep in the fridge for 3-4 days and in the freezer for 2-3 months. Cooling and reheating a broth will actually improve the flavour.\nCooling your broth\nDon’t leave the broth on the stove. It will spoil.\nWhen storing broth, quick cooling is the first step to ensuring your broth is safe to eat later. Cool it as quickly as possible before you put it in the fridge or freezer.\n- Carefully decant the hot broth from the pan into a bowl. Without the residual heat of the pan, it’ll cool more quickly\n- Put the bowl in the coolest room in your home\n- Prop one edge of the bowl up to allow air to circulate underneath the bowl\n- When the broth is at room temperature, cover and store in the fridge\n- If your freezing it, split the broth into portions before freezing\nReheating your broth\nIn the fridge, your broth may set almost solid, so chilled or frozen, the reheating technique is the same. Follow these steps and your broth won’t stick to your pan. Using a pan is easier than a microwave. But if a microwave is what you’ve got – adapt the instructions to suit.\n- Heat a covering of water in the base of your pan\n- When it’s boiling, carefully add your portion(s) of broth\n- Cover with a lid\n- Warm over a low heat\n- Stir every 2-3 minutes\n- Keep heating until the broth boils – all over not just around the edges of the pan\n- Remove the lid and simmer for two minutes stirring regularly\nProperly reheating your broth is the second step to ensuring it’s safe to eat. Microwaves are notorious for cool spots. If you’re microwaving your broth, stir regularly and be especially careful it’s hot right through.\nAutumn’s here. Broth season is open.\nThe clocks change at the end of the month and the leaves on the trees are turning – broth season is starting.\nAnd to borrow from Robert Burns,\nAuld Scotland wants nae skinking ware\nThat jaups in luggies “\n(Old Scotland wants no watery stuff, that splashes in the bowl)\nScottish broth, an easy to make and healthy comfort food for the cold months to come. Follow this guide. Experiment and enjoy.\nI’d love to hear your favourite combinations – they’ll inspire new ones of my own.\nHi, I’m Ralph\nI’m an Associate Registered Nutritionist with over 25 years’ experience as a professional chef.\nMy passion is helping individuals gain control of their diet to achieve food freedom and health in today’s broken nutrition environment.\nI’m based in Edinburgh and provide 1-2-1 online nutrition coaching and support across the U.K."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:c2b181ab-7550-45ab-9a0b-a0264c9969db>","<urn:uuid:ef27882c-58c3-418f-a46b-d588126a74d5>"],"error":null}
{"question":"在实施阶段，方法研究和创新管理流程有什么共同点？(What are the common elements between method study and innovation management processes during the implementation phase?)","answer":"During the implementation phase, both method study and innovation management share several key elements: 1) Both require proper implementation to deliver expected results - method study emphasizes training people who will perform the new procedure, while innovation management focuses on implementing selected ideas to achieve intended outcomes. 2) Both processes include a validation component - method study maintains and monitors the new technique through control measures, while innovation management validates new ideas by checking if they delivered expected results. 3) Both seek cooperation and involvement from different organizational levels - method study requires cooperation from supervisors and workers, while innovation management emphasizes involvement from people across different levels of the organization.","context":["With the rising complexities of the technology world, the desire to simplify work intensifies by the day. Today, we all want our work to be more systematic to be more productive and efficient. To achieve that method study is conducted. So, what exactly is a method study, and what is the procedure of method study?\nWhat is a Method Study?\nMethod study is putting work to systematic, critical examination to improve its effectiveness and efficiency.\nIt is one of the keys to increasing productivity. It intends to study and improve monotonous manual work, and it is also efficient for any type of activity at any level of an organization.\nThe fundamental aim is to simplify jobs or working techniques to perform the essential functionality to utilize as few resources as feasible.\nThe method study is converged on higher productivity. Method study often conducts to simplify work or working techniques to increase productivity.\nIt is always preferable to accomplish the required function with the desired objective of consuming the fewest resources possible.\nMethod denotes how a task is to complete, for example, a description of how we use resources to attain our goal.\nMethods are an essential component of task completion and represent:\n- How successfully our techniques make use of the limited resources available, such as personnel, machinery, materials, and money.\n- How our approaches physically affect the unit’s output.\n- The output quality produces through the use of our approaches.\nMethods can so determine the number of input materials, time power, and money used.\nSo techniques may be regarded as the core where one may seek to decrease resource consumption hence lowering cost per unit output through suitable procedures.\nThe cost and quality of the product generated can be determined by the technique design.\nThe Objective of the Method Study\n(1) Process and procedure enhancements.\n(2) Improvements to production and workplace layout.\n(3) Enhancement of plant and equipment design.\n(4) Lessening of unneeded tiredness and motions\n(5) Make use of better materials, machines, and labor.\n(6) Improved working conditions.\nAnalyses should be performed in a clean and orderly manner while analyzing any problem.\nTypically, the cycle begins with identifying the dilemmas and collecting all essential information about the difficulties.\nThe facts or information next scrutinize critically.\nThen a choice is made and carried out—all of this followed by routine checks.\nScope of Method Study\nThe scope of the method study is not limited to the industrial industry.\nMethod research approaches can also be used effectively in the service industry.\nIt is appropriate for use in offices, hospitals, banks, and other service companies.\nMethod study may successfully implement in the following areas in manufacturing:\n- To enhance work processes and practices.\n- To establish the optimal order in which to do tasks.\n- To intensify the layout and smoothen material flow with little backtracking.\n- To improve working conditions and, as a result, labor efficiency.\n- To alleviate labor boredom.\n- To increase plant and material use.\n- Waste and inefficient processes must eliminate.\n- Reduce production costs by shortening operating cycle times.\nThe Procedure of Method Study\nThe key to Method Study effectiveness is its practitioners’ adherence to a fundamental protocol that guarantees every part of the projects done in the proper sequence.\nThis procedure consists of eight steps, each of which must complete in the exact sequence specified.\nThe process generally observes as linear, with the following key steps:\nSELECT the work to be analyzed and outline its parameters.\nRECORD the critical facts regarding the task and gather any extra data that may require from suitable sources.\nEXAMINE how the works did and question its aim, place sequence, and technique of execution.\nDEVELOP the most practical, cost-effective, and efficient technique, relying on all parties’ contributions.\nEVALUATE many choices to build a new enhanced technique by comparing the cost-effectiveness of the chosen new method to the current performance process.\nDEFINE the new approach as a consequence and convey it to all involved, i.e., management, supervisors, and workers.\nINSTALL the new procedure as a standard practice and teach those who will be using it.\nMAINTAIN the new way and implement control measures to avoid reverting to the older manner of work.\nStep 1. Select\nThe work selected for the method study might be an identified problem area or an identified possibility.\nIt may be defined through a comprehensive review of existing data, standard monitoring or control processes, high levels of discontent and dispute, or as part of a change in business policy, practicing, techniques, or site, and is because it meets specific conditions of uncertainty and priority.\nCertain things must be analyzed while selecting work for method study:\n(1) Economic factors:\nOnly positions with a high economic value are preferred.\nJobs featuring bottlenecks that stymie other manufacturing processes, activities requiring repeated effort, and so on are analyzed and studied.\n(2) Technical factors\nThe availability of the technical knowledge required for the study is a significant element to keep in mind while choosing a career.\nConsider a machine tool that is not operating at a high enough speed to maximize efficiency.\nSo, the research will state that speed should be raised, but can the tool be sped up, or is the machine structure capable of handling this additional load? These are the problems that constantly spring to mind when picking the work to be investigated.\n(3) Human factors:\nHuman reactions are notoriously tricky to forecast.\nAs a result, the expertise of local staff and conditions will help to lessen issues.\nAs a result, if the occupations chosen contain disagreeable aspects such as filthy labor or hard lifting, the workers will happily accept them.\nStep 2. Define\nBefore initiating any method study research, it is vital to create precise terms of reference that specify the research’s goal, scale, scope, and limits.\nIt should also entail ascertaining who “owns” the problem or circumstance and how much ownership is shared.\nIt may spark a discussion about the project’s goals, reporting procedures and frequencies, and success metrics.\nThis procedure seldom introduces as a unique and independent step of method study, referred to as the “Define” step.\nIt leads to an investigative strategy that includes acceptable methodologies, persons, and timeframes.\nStep 3. Record\nFollowing the task selection to be investigated, the following stage in the fundamental approach is documenting all data relevant to the modern strategy.\nIt is crucial to record all relevant facts about the current approach to visualize the activities chosen for investigation in their entirety and improve them through future critical analysis. Records are notably valuable for comparing the efficiency of the suggested better procedure before and after.\nThe recording techniques are determined to simplify and standardize the recording process. Charts and diagrams use to do this.\nTypes of Recording Techniques\nIt is the most common technique of documenting information.\nThe tasks that comprise the occupations document using method study symbols.\nThe charts must carefully prepare so that the information they display is easily understood and recognized.\nThe graphic should include the following data.\nThese charts use to track the movement of an operator or a piece of equipment (i.e., in motion study).\n- Complete summary of the activities.\n- Whether the charting is for the current or planned approach.\n- Precise reference to when the activities will start and complete.\n- Time and distance models applied wherever necessary.\n- Charting’s date and the person’s name who does charting.\nTypes of Charts\nIt categorized into two types:\n(A) Macro motion charts and\n(B) Micromotion charts.\nMacro motion charts are used for macro motion analysis, whereas micro motion charts utilize for micro-motion analysis. A macro motion study can be measured using a stopwatch, but micro-motion research cannot be measured using a stopwatch.\nMACRO MOTION CHARTS\nThe four charts shown below are examples of this type:\n- Operation Process Chart\n- Flow Process Chart\n- Two-Handed Process Chart\n- Multiple Activity Chart\nDiagrams Used in Method Study\nThe flow process chart depicts the sequence and character of movement but not the path of action.\nThere are frequently undesired states in movement pathways, such as congestion, backtracking, and needless lengthy moves.\nA representation of the working area in the form of flow diagrams or string diagrams create to capture these external aspects:\nTo investigate several layout designs and, as a result, pick the most optimal arrangement.\nTo analyze traffic and frequency along the plant’s various routes.\nDetection of backtracking and impediments while moving.\nThere are two kinds of diagrams\n- Flow diagrams\n- and string diagrams\nSymbols Used in Method Study\nGilbert invented the graphic style of recording to portray data simply and without ambiguity, allowing people to comprehend them quickly and clearly.\nSymbols are more beneficial than written descriptions.\nMETHOD STUDY SYMBOLS\nAn operation happens when an object’s qualities purposefully alter in one or more ways (physical or chemical).\nIt denotes the primary steps of a process, technique, or procedure.\nAn inspection happens when an object is examined and compared to quality and quantity standards.\nTransportation refers to the transport of personnel, goods, or equipment from one location to another.\nDelay D (Temporary Storage)\nWhen the immediate execution of the next scheduled thing does not occur, this refers to a delay.\nStorage happens when an object is held in a secure location and safeguard against unwanted removal.\nMaterials are held in stores, for example, to supply to various jobs.\nStep 4. Examine\nExamine the facts carefully. It is a crucial step in the whole study.\nIt is converged on interrogating the various actions of the process in a systematic, logical, and objective manner.\nThe questioning sequence follows a well-established pattern that investigates the objective for which, the area at which, the order in which, the individual by whom, and how each activity performs to eliminate, simplify, combine, and reorganize the various elements of work to improve the work method.\nStep 5. Develop:\nThe enhanced approach will develop as the next step in the method study.\nThe examination of existing work methods serves as a starting point for synthesizing potential job performance enhancements.\nAlternative options are recognized, and the best one select for usage.\nIn general, the evaluation process goes as follows:\n- Choose particular measurable and unquantifiable criteria.\n- Forecast the performance of each alternative concerning these criteria.\n- Convert your estimations to monetary values.\n- Evaluate options using all of the criteria.\n- Choose the chosen option.\nIt may be preferable to design two or more enhanced approaches in many circumstances, some conservative and others radical.\nStep 6. Install:\nInstall the enhanced technique. It entails training individuals who will be performing the new procedure.\nCooperation from both supervisors and workers is wanted for the effective implementation of the new strategy.\nTheir prior involvement in some or all of the preceding processes increases the likelihood that the new technique will effectively implement.\nTrail or pilot runs may conduct, during which small changes may make to simplify functioning.\nWhen the new approach is to install, it is good for the work-study guy to be present on the shop floor.\nStep 7. Maintain\nThe final step in method study is to keep the better technique.\nEven after the operators have tech and adjustments to the equipment and layout have been made, there is no guarantee that the technique will use as intended.\nAs a result, method engineers must monitor the new technique frequently and consult with those affected.","Innovation Management Process for Development. A systematic approach for generating, prioritizing, evaluating and validating new ideas, as well as putting them into practice. Read on to know the process and how each of the processes works.\nWhat is Innovation Management Process?\nInnovation Management Process is a systematic approach for generating, prioritizing, evaluating and validating new ideas, as well as putting them into practice.\nThe first step in the innovation management process is generating ideas. There could be many ways to generate ideas. Some of the ways are\nbrainstorming, brainwriting, reference to experts, etc. But one important thing is to generate ideas from everyone within the organization. It should not be left only to some selected people. Why is it so? It is because you are bound to miss out on some good ideas if you try to complete this process without involving people from different levels in the organization.\n2nd step in the innovation management process is prioritizing ideas. Here you need to put each idea up against certain criteria and then decide what is the most important thing to do next with each of these ideas. Some of the criteria that could be used are- How many people will benefit from this idea? How much money can be saved if this idea is implemented? Also, How much revenue can be generated if this idea is implemented? How much time will it take if we implement this idea? What other things will depend on this idea? And, What else needs to be done before this idea can be implemented? What are the risks associated with implementing this idea? This list can go on and on.\n3rd step in the innovation management process is evaluating ideas. While prioritizing, you already decided what needs to be done first with each of these ideas, but at the evaluation stage, you need to evaluate how well each of these ideas has performed against its objectives and against other criteria that were listed for prioritizing stage.\n4th step in the innovation management process is implementing ideas that have been selected for implementation based upon the evaluation stage. Once an idea has been chosen for implementation, it should be properly implemented so that it delivers its expected results.\n5th step in the innovation management process is validating new ideas after they are implemented. Validation involves seeing whether the new ideas have delivered the expected results or not and whether or not they need any additional inputs or changes in order to improve their performance and make them more effective than they are right now.\n6th step in the innovation management process is repeating all six steps as many times as required until all the new ideas have been fully validated and have become a part of the normal business processes of your organization.\nWhy take a systematic method to innovation management?\nDifferent types of methods are used by different firms to manage innovation, with some being clearly more effective than others.\nAfter all, innovation is only as good as the process that drives it, and it’s tough to avoid confusion and frustration without one.\nA systematic innovation management approach walks you through the whole innovation life cycle, assisting you in making better decisions, eliminating bottlenecks, increasing efficiency, lowering risks, and moving quicker by simplifying an otherwise complicated process."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:eeec1e1d-3820-47e0-a20f-42b59fad3d1f>","<urn:uuid:cc6bc4dd-d1a2-4cc1-9efa-97aad61c724c>"],"error":null}
{"question":"What are the key differences between how Pope Leo II and Canon VII of Ephesus I handle additions to religious texts?","answer":"Pope Leo II and Canon VII of Ephesus I had contrasting approaches to additions in religious texts. Pope Leo II approved additions when they maintained the same faith, as shown when he confirmed the acts of the Sixth Ecumenical Council and emphasized proper interpretation over literal adherence. Regarding Canon VII of Ephesus I, while it prohibited adding to creeds, the interpretation was not about literal word additions but rather about preserving the original faith. This was demonstrated at the Council of Chalcedon, where it was clarified that additions were acceptable if they confirmed the same faith, as seen in how the Constantinopolitan Creed's additions to the Nicene Creed were considered legitimate because they elaborated on the same faith while preventing misinterpretations.","context":["Pope St. Leo II\nLeo was a Sicilian by birth (the son of a man named Paulus). He may have ended up being among the many Sicilian clergy in Rome, at that time, due to the Islamic Caliphate battles against Sicily in the mid-7th century. Though elected pope a few days after the death of Pope St. Agatho (10 January 681), he was not consecrated till after the lapse of a year and seven months (17 August 682). Leo was known as an eloquent preacher who was interested in music, and noted for his charity to the poor.\nElected shortly after the death of Agatho, Leo was not consecrated for over a year and a half. The reason may have been due to negotiations regarding imperial control of papal elections. These negotiations were undertaken by Leo’s predecessor Agatho between the Holy See and Emperor Constantine IV. They concerned the relations of the Byzantine Court to papal elections. Constantine IV had already promised Agatho to abolish or reduce the tax that the popes had been paying to the imperial treasury at the time of their consecration, an imperial policy that had been in force for about a century.\nLeo’s short-lived pontificate did not allow him to accomplish much, but there was one achievement of major importance: he confirmed the acts of the Sixth Ecumenical Council (680–1). This council had been held in Constantinople against the Monothelite controversy, and had been presided over by the legates of Pope Agatho. After Leo had notified the Emperor that the decrees of the council had been confirmed, he made them known to the nations of the West. In letters written to the king, the bishops, and the nobles of Spain, he explained what the council had effected, and he called upon the bishops to subscribe to its decrees.\nDuring this council, Pope Honorius I was anathematised for his views in the Monothelite controversy as tolerant of heresy. Leo took great pains to make it clear that in condemning Honorius, he did so not because Honorius taught heresy, but because he was not active enough in opposing it. In accordance with the papal mandate, a synod was held at Toledo (684) in which the Council of Constantinople was accepted.\nRegarding the decision of the council, Leo wrote once and again in approbation of the decision of the council and in condemnation of Honorius, whom he regarded as one who profana proditione immaculatem fidem subvertare conatus est (roughly, “one who by betrayal has tried to overthrow the immaculate faith”). In their bearing upon the question of papal infallibility these words have caused considerable attention and controversy, and prominence is given to the circumstance that in the Greek text of the letter to the Emperor which the phrase occurs, the milder expression subverti permisit (“allowed to be overthrown…”) is used for subvertare conatus est. At this time Leo put an end to the attempts of the Ravenna archbishops to get away from the control of the Bishop of Rome. The Pope sweetened the deal for the Ravenna bishops by abolishing the tax it had been customary for them to pay when they received the pallium.\nAlso, in apparent response to Lombard raids, Leo transferred the relics of a number of martyrs from the catacombs to churches inside the walls of the city. He also dedicated two churches, St. Paul’s and Sts. Sebastian and George.\nSeventh Sunday after Pentecost\nCommemoration for St. Leo II\nPope and Confessor\nWithin the Octave of\nHoly Apostles St. Peter and St. Paul\nSemi-Double Green Vestments\nMissa ‘Omnes gentes’\nBy their fruits you shall know them…\nIntroitus – Psalm 46:2\nOmnes gentes, pláudite mánibus: jubiláte Deo in voce exsultatiónis. Psalm 46:3 Quóniam Dóminus excélsus terribílis: Rex magnus super omnem terram. V. Glória Patri, et Fílio, et Spirítui Sancto. Sicut erat in princípio, et nunc, et semper, et in sæcula sæculórum. Amen.\nAll ye nations, clap your hands: shout unto God with the voice of joy. Psalm 46:3 For the Lord is most high, He is terrible He is a great King over all the earth. V. Glory be to the Father, and to the Son, and to the Holy Ghost, God, for ever and ever. Amen.\nAll ye nations….\nDeus cujus providéntia in sui disposìtióne non fállitur: te súpplìces exorámus, ut noxia cuncta submóveas, et ómnia nobis profutúra concédas. Per eúmdem nostrum Jesum Christum Fílium tuum, qui tecum vivit et regnat in unitáte Spíritus Sancti, Deus, per ómnia sæcula sæculórum. Amen.\nO God, whose providence faileth not in its designs, we humbly entreat Thee, to put away from us all hurtful things, and to give us all things which be profitable for us, through the same Lord Jesus Christ, Thy Son, Who liveth and reigneth with Thee in the unity of the Holy Ghost, God, for ever and ever. Amen.\nSecond Collect Pope Leo II\nO God of the heavenly powers, creator of all good things, implant in our hearts the love of Your Name, and bestow upon us an increase of godliness, fostering what is good, and, by Your loving care, guarding what You have fostered. Through our Lord.\nThird Collect for Octave of Ss. Peter and Paul\nO God, who hast made holy this day with the martyrdom of Thine apostles Peter and Paul; grant that Thy Church may in all things follow the precepts of those from whom it first received the faith. Through our Lord.\nThe Lesson is taken from the Epistle of Saint Paul the Apostle, to the Romans\nEPISTOLA – Romans 6:3-11\nFratres, Humánum dico propter infirmitátem carnis vestræ: sicut enim exhibuístís membra vestra servíre immundítiæ et iníquitáti ad iniquitátem ita nunc exhibéte membra vestra servíre justítiæ in sanctificatiónem. Cum enim servi essétis peccáti, líberi fuístis justítiæ. Quem ergo fructum habuístis tunc in illis, in quibus nunc erubéscitis? Nam finis illórum mors est. Nunc vero liberáti a peccáto, servi autem facti Deo, habétis fructum vestrum in sanctificatiónem, finem vero vitam ætérnam. Stipéndia enim peccáti, mors. Grátia autem Dei, vita ætérna: in Christo Jesu Dómino nostro.\nEPISTLE – Romans 6:3-11\nBrethren, I speak a human thing, because of the infirmity of your flesh for as you have yielded your members to serve uncleanness and iniquity for iniquity, so now yield your members to serve justice unto sanctification. For when you were the servants of sin, you were free from justice. What fruit therefore had you then in those things, of which you are now ashamed? For the end of them is death. But now being made free from sin, and become servants to God, you have your fruit unto sanctification, and the end life everlasting. For the wages of sin is death. But the grace of God, is life everlasting in Christ Jesus our Lord.\nGRADUALE – Psalm 33:12, 6\nVenite, Fílii, audíte me: timórem Dómini docébo vos. V. Accédite ad eum, et iliuminámini: et fácies vestræ non confundéntur. Allelúja, allelúja. V. ( Psalm 46:2 ) Omnes gentes, pláudite manibus: jubiláte Deo in voce exsultatiónis. Allelúja.\nCome, children, hearken to me I will teach you the fear of the Lord. V. Come ye to Him and be enlightened and your faces shall not be confounded. Alleluia, alleluia V. ( Psalm 46:2 ) O clap your hands, all ye nations: shout unto God with the voice of joy. Alleluia.\nSequentia sancti Evangelii secundum Matthaeum\nIn illo témpore: Dixit Jesus discípulis suis: “Atténdite a falsis prophétis, qui véniunt ad vas in vestiméntis óvium, intrínsecus autem sunt lupi rapáces: a frúctibus eorum cognoscétis eos. Numquid cólligunt de spinas uvas, aut de tribulis ficus? Sic omnis arbor bona fructus bonus fact mala autem arbor malos fructus fácere: neque arbor mala bonos fructus fácere. Omnis arbor, quæ non facit fructum bonum, excidétur, et in ignem mittétur. Igitur ex frúctibus eorum cognoscétis eos. Non omnis, qui dicit mihi: Dómine, Dómine, intrábit regnum Cælórum: sed qui facit voluntátem Patris Mei, Qui in Cælis est, ipse intrábit in regnum Cælorum.\nThe continuation of the Holy Gospel according to St. Matthew\nAt that time, Jesus said to His disciples: “Beware of false prophets, who come to you in the clothing of sheep, but inwardly they are ravening wolves. By their fruits you shall know them. Do men gather grapes of thorns, or figs of thistles? Even so every good tree bringeth forth good fruit, and the evil tree bringeth forth evil fruit. A good tree cannot bring forth evil fruit, neither can an evil tree bring forth good fruit. Every tree that bringeth not forth; good fruit, shall be cut down, and shall be cast into the fire. Wherefore by their fruits you shall know them. Not every one that saith of Me: Lord, Lord, shall enter into the kingdom of Heaven but he that doeth the will of My Father Who is in Heaven, he shall enter into the kingdom of Heaven.\nHomily by St Hilary, Bishop of Poitiers – Commentary on Matthew Chapter 6.\nThe Lord here warneth us that we must rate the worth of soft words and seeming meekness, by the fruits which they that manifest such things bring forth in their works, and that we should look, in order to see what a man is, not at his professions, but at his deeds. For there are many in whom sheep’s clothing is but a mask to hide wolfish ravening. But “Do men gather grapes of thorns, or figs of thistles? Even so, every good tree bringeth forth good fruit, but a corrupt tree bringeth forth evil fruit.” Thus, the Lord teacheth us, is it with men also evil men bring not forth good fruits, and hereby are we to know them. Lip-service alone winneth not the kingdom of heaven, nor is every one that saith unto Christ, “Lord, Lord,” an heir thereof. What use is there in calling the Lord, Lord? Would He not be Lord all the same, whether or not we called Him so What holiness is there in this ascription of a name, when the true way to enter into the kingdom of heaven is to do the will of our Father, Who is in heaven? “Many will say to Me in that day Lord, Lord, have we not prophesied in thy Name?” Already here doth the Lord rebuke the deceit of the false prophets, and the feigning of the hypocrites, who take glory to themselves because of the power of their words, their prophesying in teaching, their casting out of devils, and such-like mighty works. Because of all these things they promised unto themselves that they shall enter into the kingdom of heaven as though in their words and works any good thing were their own, and not all the mighty working of that God upon Whom they call, since reading bringeth knowledge of doctrine, and the Name of Christ driveth out devils. That which is needed on our part to win that blessed eternity, that of our own which we must give, is to will to do right, to turn away from all evil, to obey with our whole heart the commandments laid on us from heaven, and so to become the friends of God. It should be ours rather to do God’s will, than to boast of God’s power. And we must put off from us and thrust away such as are by their wicked works already estranged from His friendship.\nOFFERTORIUM – Daniel 3:40\nSicut in holocáustis arietum, et taurórum, et sicut in míllibus agnórum pínguium: sic fiat sacrifícium nostrum in conspéctu tuo hódie, ut pláceat tibi: qula non est confúslo confidéntibus in te, Dómine.\nAs in holocausts of rams and bullocks, and as in thousands of fat lambs so let our sacrifice be made in Thy sight this day, that it may please Thee: for there is no confusion to them that trust in Thee, O Lord.\nDeus, qui legálium differéntiam hostiárum uníus sacrifícii perfectióne sanxisti: áccipe sacrifícium a devótis tibi fámulis, et pari benedictióne, sicut múnera Abel, sanctífica: ut, quod singuli obtulérunt ad Majestàtis tuæ honórem, cunctis profíciat ad salútem. Per eúmdem Dóminum nostrum Jesum Christum Fílium tuum, qui tecum vivit et regnat in unitáte Spíritus Sancti, Deus, per ómnia sæcula sæculórum. Amen.\nO God, who hast justified the variety of sacrifices of the Law by the perfection of this one Sacrifice: accept the Sacrifice of Thy servants who are dedicated to Thee, and sanctify it with a blessing like to that which Thou didst bestow upon the gifts of Abel, that what each one of us has offered to the honor of Thy Majesty, may profit us all unto salvation, through the same Lord Jesus Christ, Thy Son, Who liveth and reigneth with Thee in the unity of the Holy Ghost, God, for ever and ever. Amen.\nSecret Prayer for Leo II, Pope and Confessor\nMercifully hear our humble prayers, O Lord, and graciously accept these offerings of Your people, and grant that no prayer may be without effect, no petition in vain, so that what we ask in faith, we may really obtain. Through our Lord.\nThird Secret Prayer for Holy Apostles, Ss. Peter and Paul\nMay the prayer of Thine apostles accompany the oblation which we offer to be consecrated to Thy name; and grant that by it we may be cleansed and defended. Through our Lord.\nPREFACE OF THE MOST HOLY TRINITY\nVere dignum et justum est, aequum et salutare, nos tibi semper, et ubique gratias agere: Domine sancta, Pater omnipotens, aeterne Deus. Qui cum unigenito Filio: tuo et Spiritu Sancto, unus es Deus, unus es Dominus: non in uninus singularitate personae, sed in unius Trinitae substantiae. Quo denim de tua Gloria, revelante te, credimus, hoc de Filio tuo, hod de Spiritu Sancto, sine differentia discretionis sentimus. Ut in confessione verare, sempitiernaeque Deitatis, et in personis proprietas, et in essential unitas, et in majestate adoretur aequalitas. Quam laudant Angeli atque Archangeli, Cherubim, quoque ac Seraphim: qui non cessant clamare quotodie, una voce dicentes: Sanctus, Sanctus, Sanctus, Dóminus Deus Sábaoth. Pleni sunt cæli et terra glória tua. Hosánna in excélsis. Benedíctus qui venit in nómine Dómini. Hosánna in excélsis.\nIt is truly meet and just, right and for our salvation, that we should at all times and in all places, give thanks unto Thee, O holy Lord, Father almighty, ever-lasting God: Who, together with Thine only-begotten Son, and the Holy Ghost, are one God, one Lord: not in the oneness of a single Person, but in the Trinity of one substance. For what we believe by Thy revelation of Thy glory, the same do we believe of Thy Son, the same of the Holy Ghost, without difference or separation. So that in confessing the true and everlasting Godhead, distinction in persons, unity in essence, and equality in majesty may be adored. Which the Angels and Archangels, the Cherubim also and Seraphim do praise: who cease not daily to cry out with one voice saying: Holy, Holy, Holy, Lord God of Hosts! Heaven and earth are full of Thy Glory! Hosanna in the highest. Blessed is He Who cometh in the Name of the Lord! Hosanna in the highest.\nCOMMUNIO – Psalm 30:3\nInclína aurem tuam, accélera ut erípias me.\nBow down Thine ear, make haste to deliver me.\nRéspice, Dómine, propitious super haec múnera: quae pro beáti Sacerdótis et Mártyris tui Apollináris commemoratióne deferimus, et pro nostris offensiónibus immolámus. Per Dóminum nostrum Jesum Christum Fílium tuum, qui tecum vivit et regnat in unitáte Spíritus Sancti, Deus, per ómnia sæcula sæculórum. Amen.\nMay Thy healing work, O Lord, both mercifully free us from our perversities, and lead us to those things which are right, through the same Lord Jesus Christ, Thy Son, Who liveth and reigneth with Thee in the unity of the Holy Ghost, God, for ever and ever. Amen.\nLeo II, Pope and Confessor\nWe have been filled with Your gifts, O Lord; grant, we beseech You, that we may be cleansed and strengthened by their effect. Through our Lord.\nSt. Peter Enthroned with Sts. John the Baptist and Paul by Cima Da Conegliano\nThird Postcommunion Prayer Ss. Peter and Paul, Holy Apostles\nThou hast fed us with bread from heaven, O Lord; by the prayers of Thine apostles keep us from all harm. Through our Lord.","In Eastern Orthodox apologetics, Canon VII of the Ecumenical Council of Ephesus I is often invoked as an argument against the Filioque. The argument is that the addition of “and the Son” after the words “proceeds from the Father” in the Constantinopolitan Creed is tantamount to adding to the Nicene Creed–the two Creeds really being one of the same. In the words of one popular apologist, Robert Arakaki:\nFor Roman Catholics the Nicene Creed is under the Pope, not over the Pope. When the Pope inserted the Filioque into the Nicene Creed a major realignment of ecclesial authority took place. The Pope without the assent of the other historic patriarchates: Constantinople, Alexandria, Antioch, and Jerusalem, and without convening an ecumenical council of bishops, unilaterally altered the Nicene Creed. This was done even though the Third Ecumenical Council (Ephesus 431, Canon VII) forbade the creation of a new creed. In essence, the Bishop of Rome was claiming a magisterium (teaching authority) equal to or superior to the Ecumenical Councils.\nIn short, the accusation is that the Filioque is an addition to the Nicene (i.e. Nicene with Constantinopolitan additions) Creed, because it adds words that were not there before.\nBut, does this allegation hold water? After all, Canon VII explicitly condemns adding or subtracting anything to the Creed–wouldn’t the addition of three words be obviously worthy of condemnation?\nIn this article, I argue that the mere adding of words is not condemned Canon VII, due to the teachings on the subject by the fathers in the Council of Chalcedon. We can address this by answering a few questions:\nIs the Creed that cannot be added to or subtracted from, according to Canon VII of Ephesus I, the Constantinopolitan Creed?\nIn a word: NO. The minutes of the Council of Ephesus (specifically the Session of July, 22 431) were read in the Council of Chalcedon. In those minutes, the Creed that they condemned there being additions to and subtractions from was quoted ad verbatim (First Session of Chalcedon’s Proceedings, Paragraph 914). The words were specifically for the Nicene, not the Constantinopolitan, Creed.\nWas there a contemporary understanding that the Constantinopolitan Creed had replaced the Nicene Creed?\nYes and no.\nEvidence for the affirmative can be seen after the letter from archimandrite Eutyches of Constantinople (a monophysite heretic) was read during the First Session. Eutyches quotes only Nicea (not Constantinople I) and then states:\nThis is the creed in which I was born and immediately dedicated to God and accepted by his mercy. With this creed I received the seal of baptism and have lived till today, praying also to die in it. This is the creed that was also confirmed by the aforesaid holy and ecumenical council held here earlier at which our father Bishop Cyril of blessed and sacred memory presided, and at which he issued a decree that whoever added to it in thought or teaching is subject to the penalties then laid down (Ibid., Paragraph 157).\nIn short, Eutyches was making the claim that his Monophysitism was endorsed by Saint Cyril of Alexandria and the Nicene (not Constantinoplitian) Creed. A few Bishops objected to Eutyches’ interpretation of Ephesus and Cyril’s letter.\nDiogenes, bishop of Cyzicus, had a telling response which reveals that he viewed the Nicene Creed to have had been modified by the holy fathers of Constantinople I:\nHe [Eutyches] adduced the council of the holy fathers at Nicaea deceptively, since additions were made to it by the holy fathers on account of the evil opinions of Apollinarius, Valentinus, Macedonius and those like them, and there were added to the creed of the holy fathers the words “He came down and was enfleshed from the Holy Spirit and Mary the Virgin.” This Eutyches omitted [in the letter], as an Apollinarian…The holy fathers who came after clarified the words “was enfleshed” of the holy fathers at Nicaea by adding “from the Holy Spirit and Mary the Virgin” (Ibid., Paragraph 160).\nIn this response, Cyzicus explicitly said that the Nicene Creed has an addition to it, and then quotes part of the Constantinopolitian Creed.\nTellingly, the Monophysites immediately cried afoul of the words of Cyzicus, asserting that Canon 7 of the Council of Ephesus disallowed for the Constantinopolitan additions–\nThe most devout Egyptian bishops and those with them exclaimed: ‘No one admits any addition or subtraction to [the Nicene Creed]. Confirm the work of Nicaea; the orthodox emperor has commanded this’ (Ibid, Paragraph 161).\nCan the preceding be used as justification for saying that the orthodox/catholic fathers of Chalcedon were always speaking of the Nicene and Constantinopolitan Creeds as one of the same? The answer is no. There are at least a couple dozen examples of this, but we will only list a few.\nPapal Legate Paschasinus clearly differentiated between the Creed of Nicea and that of Constantinople:\nIt is clear and cannot be disputed that the faith of the most blessed pope of the apostolic see Archbishop Leo is one and in accord with the creed of the 318 fathers who met at Nicaea, that it upholds both the creed of the 150 who convened at Constantinople and also the decrees of Ephesus under Cyril of holy memory when Nestorius was deposed on account of his errors (Ibid., Paragraphs 2-4).\nTheodosius of Canatha likewise speaks of the Creeds as mutually exclusive:\nThe creed of the 318 is unshakeable. If anyone attempts to shake the unshakeable, he will himself be shaken, while not shaking the unshakeable. This creed we follow and believe, and also the definition defined by the 150 holy fathers who met at Constantinople (Ibid., Paragraph 27).\nEuphratas of Eleutherna concurs:\nWe uphold the creed of the 318 holy fathers as being our salvation and pray to depart from life with it; and that of the 150 is in no way in disharmony with the aforesaid creed (Ibid., Paragraph 98).\nFlorentius of Hadrianopolis said:\nBefore the interpretations of our most God-beloved and blessed father Cyril and the most blessed Archbishop Leo, we adhered to the definition of the holy fathers at Nicaea; so we believed and believe. In addition we assent to the creed of the 150, which clearly states that our Lord Jesus Christ was enfleshed from the Holy Spirit and Mary the Virgin. In this creed of the holy fathers we believe as they have interpreted it, and we doubt nothing.’ (Ibid., Paragraph 133).\nClearly, the majority of Chalcedon’s fathers believed that the Nicene and Constantinopolitan Creeds were mutually exclusive, but that the “definition” of Constantinople was just as correct as Nicea.\nIf Canon VII of Ephesus I did not refer to the Constantinopolitan Creed, does that mean additions can be made to it?\nIn a word: NO. In the Fifth Session of Chalcedon “by a unanimous decree…renewed the unerring faith of the fathers, proclaimed to all the creed of the 318, and endorsed as akin the fathers who received this compendium of piety, that is, the 150 who subsequently assembled at great Constantinople and set their seal on the same faith” (Paragraph 31). Both Creeds were then read ad verbatim (Paragraphs 32-33).\nFinally, the warning not to add or subtract from the Creed was made applicable to both Creeds:\nNow that these matters have been formulated by us with all possible care and precision, the holy and ecumenical council has decreed that no one is allowed to produce or compose or construct another creed or to think or teach otherwise (Paragraph 34).\nWait a second–didn’t Constantinople I in the words of Diogenes, bishop of Cyzicus, “make additions” to the Nicene Creed?\nNO. True, popular Orthodox apologetics equates adding words, whether orthodox in their content or not, to a creed as equivalent to defying the universal decree of the Bishops in Paragraphs 32-34 of the Fifth Session of Chalcedon. However, the fathers of Chalcedon did not view it this way. Such a narrow view was in fact erroneously taken by the monophysite heretics (First Session of Chalcedon, Paragraph 161)!\nInstead, it is more appropriate to interpret the “no additions” rule as forbidding a different faith than that of Nicea, not changes in verbiage per se–otherwise Canon VII would have forbidden the use of the Constantinopolitan Creed (which it in fact never did.)\nFurthermore, we have Chalcedonian fathers affirming the understanding that it was the preservation of the creed’s faith and content that was important. One example we have recorded is from Anatolius of Constantinople, who said, “The creed of our 318 holy fathers at Nicaea and of the 150 who subsequently assembled at Constantinople…confirmed the same faith” (Fourth Session of Chalcedon’s Proceedings, Paragraph 9).\nHence, while the fathers viewed the Creeds as mutually exclusive, the Constantinopolitan Creed did not violate Canon VII of Ephesus I specifically because it was re-iterating what the Nicene Creed said in that it “confirmed the same faith.” In doing so, it confirmed the same faith in such a way to prevent Apollonarian misinterpretations of the plain words of Nicea.\nTherefore, additions to a Creed are viewed as legitimate provided that the faith therein elaborated upon is the same. The words of Papal Legate Paschasinus also allowed for the same logic to be applied to Saint Leo I’s Tome, so we can already see the seeds of dispute over authority that would serve as the basis of the Filioque controversy.\nConclusion. The preceding study leaves the question in the title of this article unanswered: Is the Filioque a legitimate addition to the Creed or not? To this we respond, this is something that cannot be definitively answered unless we have worked out what is the basis of understanding Christian doctrine as a whole.\nThe Filioque was never the historical consensus of the Church nor was it elaborated upon by a consensus of the fathers at these early councils. So, without such consensus reached in the Church, the Orthodox must conclude that it is debatable at best (for there are ways to correctly understand Filioque in Orthodoxy) and heterodox at worst (the Orthodox reject that the Son has some role in the eternal causation of the Spirit.)\nMeanwhile, the Roman Catholics ultimately do not view consensus as the decisive factor. While the Council of Trent affirms that an explicitly expressed consensus in doctrine is dogmatically binding, the agreement of the whole Church on a doctrine is not necessary to prove the small “o” orthodoxy of a doctrine in all cases. Rather, a minority viewpoint (or one never explicitly elucidated in church history) can be the correct one provided that this has been discerned as such by the Roman Pontiff.\nUnless we answer which one of the preceding views (Orthodox or Roman Catholic) is right, we cannot definitively answer whether the Filioque was an “addition” to the Creed in the sense that Chalcedon would have condemned.\nCertainly Catholics must admit that the Filioque is an addition to the verbiage. Orthodox must admit that the mere addition of words is not a violation of Chalcedon.\nRather, the real question is whether the Filioque is an addition to the faith of the Creeds.\nRome knows that the reason Orthodox object to the Filioque is because the latter do not believe the Spirit does not eternally originate from both the Father and the Son. The Filioque potentially allows for this allegedly heterodox idea.\nIn fact, it probably is the position of Rome that the Spirit originates eternally from the Father and the Son. The Roman Catholics and Orthodox disputed this issue at length in the Council of Florence. Saint Mark of Ephesus, an attendee of the Council, reported that when the said council asserted that the Spirit proceeded eternally from the Father and the Son from “a single principle,” that:\n[W]e [Orthodox], together with St. Maximos and the Romans of that time, as well as the Western Fathers, “do not make the Son the Cause of the Spirit”; while they, in their Conciliar Decree [in Florence], proclaim the Son “in Greek, ‘Cause,’ and in Latin, ‘Principle’” of the Spirit.\nThe Council of Florence was published in Greek and Latin. By using the words principle/cause in Latin/Greek respectively, the council ignored the writings of saints such as Maximus the Confessor and John of Damascus (“For the Father alone is cause,” An Exposition of the Orthodox Faith, Chap 12) who specifically rejected this idea.*\n*I am trusting Mark on Ephesus on this, as I cannot read the published Greek or Latin. I presume he would have not made such an easily refutable claim falsely. A Catholic parsing of the term “spiration” appears to confirm this reading.\nFor a moment, let’s consider the before referenced teaching from Saint Maximus the Confessor in his Letter to Marinus which argued that the Filioque was not an addition to the faith. He said this because he alleged that the Catholics of Rome of his time denied that the Filioque taught that the Spirit eternally originated in both the Father and the Son.\nIt would seem that if Rome explicitly and dogmatically took the view as elaborated upon by Saint Maximus, the Filioque dispute would be over. However, instead of endorsing this view, Rome for all appearances has rejected it over the ages.\nFirst, the plain meaning of Rome’s dogmatic constructions (which state that the Spirit was “spirated,” i.e. “breathed out” by the Father and Son as a single “principle”/”cause”) would mitigate against Maximus’ view. Second, Maximus’ Letter of Marinus was alleged by French Catholics of being a forgery in the Council of Florence and was not accepted by Catholics in their elucidation of the Filioque doctrine. It appears that Rome’s view of the Filioque is clearly different than that of Saint Maximus’ otherwise they would have taken the opportunity, which they have had 1,150 years* to do, to clear up the confusion. This means that Saint Maximus the Confessor, confidante of Pope Martin I, was either misunderstanding what Rome taught about the Filoque at his time or that Rome has changed their understanding of the doctrine.\n*If we begin count from the Photian Schism.\nIn the end, even if the interpretation Rome has for the Filioque appears to add to the faith of Nicea, Ephesus, and Chalcedon (the Councils, and their fathers, nowhere endorsed that the Spirit eternally originates from the Son in any way) one would be compelled to presume if the Roman view of authority is correct that the fathers of these councils, in an unsaid way, agreed with the later Roman elucidation.\nThis is because the Roman epistemology does not need to show that the faith of the fathers is consistent with their present day understanding of the Filioque. This is because a consensus of opinion since the time of the Apostles, or even a mere whisper of their view here or there, is even really necessary. Rather, as long as the fathers said nothing to explicitly to contradict the Filioque’s verbiage, what was left unsaid in Chalcedon must implicitly agree with the councils of Second Lyons and Florence simply because they do not explicitly disagree. Hence, the unsaid faith of the past is the same as the explicitly stated faith of later councils.\nIt would appear to the honest and impartial observer that Rome’s epistemology is a negative case where as long as people in the past did not explicitly reject a modern construction of a doctrine, the modern elucidation of the said doctrine may be read back into the past. If there is not a contradiction between past and present, then the doctrine is worthy of approval.\nMeanwhile, Orthodoxy demands a positive case where the faith of the fathers elucidated by the Church in the present must be something that is explicitly taught in the past. Therefore, if the fathers did not explicitly take the view that the Spirit eternally originates in the Father and the Son, the presumption is that this was never the faith of the Church.\nIn short, the epistemological difference may be summed up as follows: Roman Catholicism ultimately reads backwards and nowhere demands an ancient consensus for all doctrines. Orthodoxy reads forwards, requiring ancient consensus to justify modern understandings.\nIt is no surprise that the former view leads to more “changes” and “development,” because there is no governing factor over the Roman church other than how doctrines are defined with Papal approval in the present. Hence, the past is constantly re-evaluated in light of present-day understandings. Orthodoxy, no stranger to change, but with a different epistemological basis, changes much more slowly–as it uses the past to evaluate the present, not vice versa.\nSo, ultimately Rome may define the Filioque in such a way that explicitly states the Holy Spirit does not originate from the Son in any way. How such an explicit definition would not contradict the construction from Lyons II that the “Spirit proceeds eternally from the Father and the Son, not as from two principles, but as from one principle” will be left to the theologians to explain.\nBut, the preceding will not definitively settle the dispute. Three hundred years later, Rome can further elucidate that while the Spirit does not eternally originate from the Son, He eternally “emanates” from the Father and Son equally. It would appear the potential for future elucidations of doctrine ad infinitum epistemologically makes it impossible to ever settle any doctrine. In the Roman system, any doctrine can be perpetually put back into dispute–always waiting for the newest Roman elucidation.\nUntil the issue of epistemology is settled, the Filioque dispute cannot be."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:69806235-bade-4172-abc1-46369818eecf>","<urn:uuid:871683d3-4db4-41f7-b73a-194d99fc7664>"],"error":null}
{"question":"How have mobile communications evolved in healthcare appointments, and what cybersecurity risks do these digital systems face?","answer":"Mobile communications in healthcare have evolved with mainstream adoption of mobile phones, with RMTs now using both SMS and email for appointment-related messaging like bookings and reminders. SMS messages have a 90% read rate within 3 minutes, while emails have about 20% open rate. However, these digital systems face significant cybersecurity risks - the healthcare sector is targeted by over one-quarter of all data breaches, resulting in estimated losses of $5.6 billion. Cybercriminals can use stolen medical records for medical identity theft, creating false identities, or deploying ransomware attacks. This is particularly concerning as healthcare organizations typically have smaller cybersecurity budgets compared to other industries.","context":["Appointment related messages includes booking confirmations, appointment reminders and appointment cancellations (by either the therapist or the patient).\nPerhaps the biggest change business has experienced in the past decade is the mainstream adoption of mobile phone usage. Businesses across Canada have had to change the way they reach their audience by adapting to mobile-friendly alternatives. Since consumers now use their mobile phones at work, at home and on-the-go, RMT’s are also adopting mobile communications in the form of SMS (short message service/text messaging) in addition to emails to reach their patients, primarily in appointment related messaging.\nNon-attendance for appointments (patient no-shows) with RMT’s is a costly matter. Due to the length of RMT’s treatments, RMT’s generally do not have the option of “over-booking” their schedules to account for no-shows. Adoption of automated appointment reminder technologies have proven to significantly improve attendance rates for RMTs.\nMost practice management systems include online booking and automated appointment related messages via both email and/or SMS (text) messaging. The question is which messaging technique should you use in your practice? The answer very likely is that you should use both. It is important to offer the choices to your patients and to manage their appointment communication permissions and preferences. After all, if your patient has not given you permission to use their email address and/or cell phone number to communicate with them, you should not be using them to send appointment related messages.\nSo how should you implement and manage electronic appointment messages for your patients?\nFortunately, modern practice management systems make this an easy task. Your patient database likely will provide you with the ability to collect and store patient contact information including their email address, and mobile phone number. The system will also likely support both email and text appointment confirmation, reminder and cancellation messaging. All you have to do now is ask your patients how they wish to receive their appointment messages and perhaps suggest that you can provide both email and SMS. It is usual that patients will ask you to use both. Then all you have to do is configure their preferences and the system will deliver the messages accordingly until their preferences change. It is also worth noting that some patients may prefer that they receive a telephone call reminder as this is still a viable option.\nLet’s look at the two main communication techniques and compare their relative effectiveness. Here are a few general statistics and details related to the two technologies.\n- While both are reliable services, neither SMS nor email are guaranteed delivery services. It is estimated that 1-5 percent of messages are simply lost within the delivery system.\n- Email message length is basically unlimited while SMS is limited to 160 characters.\n- Email is the most used business communication tool.\n- Texting is the most used (non-social media related) data service on the planet.\n- Approximately 8 trillion SMS messages are sent every year. (Keep in mind the world population is approximately 7.5 billion)\n- Approximately 74 trillion emails are sent each year\n- It has been estimated that 90% of people read their text messages within three minutes of receiving them.\n- It has been estimated that approximately 20 percent of emails are opened. This reality is due to the vast amount of email spam that is sent and ignored. It should also be noted that 50 percent of the opened emails are read on a mobile device.\nAs you can see both technologies are highly used and quite reliable and effective. Combining the two makes great sense. You will recall that neither SMS nor email are guaranteed delivery by any service provider anywhere in the world. When you combine email reminders with text reminders you improve the likelihood of one or both of the messages being received by the patient.\nKeeping your patient contact information up to date is an important related task. Verifying this information with your patient during appointment check-in is strongly recommended, as this will ensure the accurate delivery of their appointment related messages. This will also demonstrate to your patients that you care about their business and offers them a chance to correct any outdated information you may have on their file.\nSince missed appointments and last minute cancellations do hurt your bottom line it is crucial to offer your patients the best appointment reminder options you can. SMS, email or telephone reminders greatly improve the attendance rate and this also provides for increased quality of care to the patient when they attend to regular treatment.\nSuperior service helps drive business growth. Communicating with your patients using the method they prefer can only contribute to having what you truly want… a loyal patient. Loyal patients’ don’t just recommend you to their friends, they insist they use your services!\nUntil next time be well…","Healthcare organizations must modernize their cybersecurity systems or risk falling behind the times compared to other industries that have made significant investments in safeguarding their IT assets. Reports have predicted that the healthcare sector will face more cyberattacks than any other industry, as hackers exploit system weaknesses. Find out need-to-know information regarding healthcare cybersecurity and get the wake-up call you need to protect your healthcare organization.\nThe Rising Cost of Healthcare Industry Data Breaches\nAccording to the Identity Theft Resource Center, over one-quarter of all data breaches target the healthcare industry, resulting in estimated losses totaling $5.6 billion. Accenture, a consulting firm, believes that healthcare industry breaches will cost over $300 billion in cumulative lifetime patient revenue over the next five years.\nIn addition to the financial cost, businesses face significant reputational damage after a cyberattack. In 2015, 113 million health records were exposed in the U.S. An additional 16 million American health records were exposed in 2016, per the U.S. Department of Health and Human Services.\nWhy Cyberattackers Target the Healthcare Industry\nThe numbers show the devastating impact of cyberattacks in the healthcare industry. It’s worth pointing out that the recent rise in healthcare data breaches comes as other industries have increased their investment in cybersecurity systems that detect, block, and mitigate attacks. Hackers searching out new targets turned to industries they previously ignored, such as healthcare. It’s much easier to apply the same strategies to a new industry than invest time in developing new hacks for the latest cyber security, after all.\nOnce cyber thieves get healthcare data, what do they do with it? The black market has many uses for medical records. Cyber criminals can harvest the data to conduct “medical identity theft.” From the harvested data, they can create false or synthetic identities.\nThey can glean enough raw data from an individual’s medical record to perform traditional identity theft. From a medical record, they may be able to open a bank account or credit card, take out a business loan, and more.\nMedical identity theft is more complex than credit card theft. While a consumer can detect and dispute a credit card charge fairly quickly, it is more difficult to resolve medical identity theft.\nCyber criminals have begun using ransomware to extract money from healthcare organizations. Criminals steal the data then promise to return access to the data or systems if a ransom is paid.\nSince healthcare organizations have been slow to adopt cybersecurity best practices, medical staff may be unaware of the risks to patient data. The average cybersecurity budget for a healthcare organization is a fraction of the budget for a financial firm.\nRecommendations for Cybersecurity\nBoth provincial and federal governments set laws regarding privacy of data. These laws indicate who can access medical information, how data must be stored and managed, and what protocol to follow if there is a breach. If you’re not sure what the privacy laws are, or how they apply to healthcare data security, this is the place to start. If it’s been a while since you’ve reviewed the privacy laws, look for recent, relevant findings and rulings that could apply to your business.\nSince many healthcare workers do not know the risks of cyberattack, it’s important to educate staff. Not only should IT workers understand cybersecurity threats and protection, but medical workers should know how their roles pertain to data management. In particular, staff should understand how to properly handle confidential information including patient data. Staff should know how to recognize common types of cyberattacks, such as phishing emails. By training your employees on cybersecurity, you can cut the risk of an attack from 70 percent to 45 percent.\nIndustry Best Practices\nBest practices that combine the particular needs of the healthcare sector with practical cybersecurity solutions do exist. They include strong encryption of data, strong authentication requirements for data, strict access control of data, and regular monitoring of searches and downloads. If a large batch of sensitive data is downloaded or transferred, it should trigger an alarm. Along with implementing best practices, healthcare businesses should research newer technologies that may offer additional levels of protection. These include biometric security, tokenization, and blockchains to record transactions.\nDisaster recovery planning is essential for the healthcare businesses. Comprehensive planning includes defining what needs to be protected, implementing a DR plan, and testing the plan. The growth of DRaaS or disaster recovery as a service allows a healthcare businesses to invest in the disaster recovery they need while keeping costs affordable.\nCyber Insurance A cyber insurance policy can help protect your business from a data breach and many other IT disasters. A comprehensive insurance policy will cover claims made against your business by victims of the breach as well as direct losses you experience.\nIT Service Providers\nIf all of this sounds like a lot to handle on your own, consider turning to the cloud for help. Healthcare organizations are increasingly using the cloud to store data and share data while controlling cost. Cloud service providers offer scalable solutions for healthcare, so your business can access the protections you need now and change your plan as you grow. Many service providers understand the regulations, risks, and needs of the healthcare industry and act as supportive partners.\nGiven the gap between healthcare cybersecurity and the security of other industries, it is critical that healthcare organizations make it a priority to protect their data. As long as the healthcare sectors uses vulnerable hardware and software for transactions and does not move toward stronger cybersecurity practices, organizations will continue to face staggering risk.\nProtecting health data will take a coordinated effort among healthcare companies, care providers, insurance companies, and IT partners who can provide solutions and experience. By adopting the lessons learned by other industries — namely financial and legal — healthcare companies can reduce their risk and protect their patient data before “the worst” happens."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:d38b5f32-2112-4147-b3f0-785453c70466>","<urn:uuid:24b7fcad-d28b-4d04-850a-afb34ef33bb0>"],"error":null}
{"question":"What are the key differences between traditional ground-based Chain Home radar systems used in Britain and modern multi-mission maneuver radars in terms of installation requirements and operational flexibility?","answer":"Chain Home radar systems required expensive, separate installations for transmitters and receivers that broadcast strong beams across entire areas, making them largely stationary and inflexible. In contrast, modern multi-mission maneuver radars, like those developed by RADA, are compact, mobile, and can be organically integrated with combat elements. These modern systems combine transmitter and receiver functions, can operate on the move, and perform multiple missions automatically through software control. They can be mounted on vehicles, provide active protection, support air defense, and offer real-time threat warnings. Their modular design with interchangeable subsystems simplifies support and reduces costs, while their digital beam forming capabilities allow them to adapt their coverage and missions in real-time, making them significantly more versatile than their historical counterparts.","context":["‘Hybrid Warfare’ insurgency rely on the integration of advanced capabilities (precision effects, command, control and coordination) while negating the military forces’ superiority in Intelligence, Surveillance and Reconnaissance. By denying the military the ISR producing target locations for standoff precision attacks, insurgents force a superior military power to strip out of most of its advantages, to fight the insurgents in high attrition, close-in battles, which would take a toll in time, casualties, financial cost and morale.\nFacing such hybrid warfare, modern military forces are relying on small and agile elements of military or special forces to carry our the fight. These teams are often providing the coordination and target acquisition elements controlling and guiding powerful, precision effects delivered from stand-off distance by aerial or fire support. When regular combat units that are required to fight in such hybrid warfare they must be equipped with the tactical means improving their capabilities to operate in terms of uncertainty and quickly respond to surprise attacks.\nNew technologies are key to achieving tactical superiority, by improving force protection, regaining the manoeuvring capability necessary to generate the ‘friction’ that would draw insurgents to respond and fight, thus giving away their main advantage – stealth and surprise. Taking advantage of such ‘friction’ tactics require adequate sensors and intelligence, providing the combat forces at the lowest combat echelons with real-time situational awareness, through the use of advanced sensors, and reliable networking enabling rapid and accurate response to defeat the threats, using smart and precise weapon systems, deployed with the forward most elements and controlled by those units.\nAirborne and forward deployed sensors were considered a must for such tactics, but they require complex coordination and assured air supremacy which cannot be guaranteed everywhere. While EO sensors offer an excellent capability detecting visible targets, they are quite limited in persistently covering large areas to provide early warning on incoming attacks. Radars are providing these services much more effectively, but in the past, analog-only radars could carry out those tasks one by one, as often they were tailored for specific tasks.\nAir surveillance radars, even those designed to detect and track fast jets or helicopters at low altitude are not well equipped to detect small, slow flying targets such as drones, ultralight aircraft (ULA) or gliders. In contrast, ground surveillance radars, designed to detect personnel or vehicles on the ground are not capable of alerting on enemy direct or indirect fire coming at them. These services are provided by yet another type of radar – Counter-Rocket, Artillery and Mortars () radars, that provide effective early warning from such attacks, but are often quite bulky and heavy, require complex networking to distribute early warning on imminent attacks, and are often useless protecting tactical units attacked from close range by mortars or direct fire.\nWith digital, phased arrays becoming smaller and lighter, ‘multi-mission’ radars are reaching the field – Elta’s Multi Mission Radar (MMR), Giraffe AMB fromand ’s G/ATOR are all examples of tactical radars that are powerful and capable of supporting combat forces from a stand-off distance. To be affordable these systems utilise common, commercial off the shelf (COTS) hardware enabling the military to deploy such systems at corps and division level, and in support of contingency operations.\nTheir multi-mission capability is derived by advanced signal-processing and algorithms enabling automated performance. As relatively large systems, these assets are often deployed on trucks, positioned at stationary sites to support the division area of responsibility.\nWhile those assets are providing essential support for the division level, there is still a need for similar support provided integrally to the tactical level, particularly when operating in complex terrain where line of sight and other obstacle impair the coverage by stand-off sensors. These capabilities are now becoming available, with the appearance of radars for the manoeuvre forces.\nSuch tactical radars operated as an organic asset with the combat element are required to deliver threat warning in real time, enabling the troops to take cover, evade the attack or respond with effective counterstrike. To support these functions the system should be able to detect direct fire and ballistic threats, calculate the location of the source of fire and projected impact point, determining the relevance to the protected units. These radars are designed to operate on the move, and perform a multitude of missions automatically, exclusively by software control.\n’s defense electronics manufacturer RADA has developed a full line of such ‘manoeuvre radars’ comprising of two basic families: the Compact Hemispheric Radars (CHR) and Multi-mission Hemispheric Radars ( ) Families. As a manoeuvre tactical radar, the CHR can be used for vehicle active protection, while some of the MHR variants provide ground based air defense, supporting VSHORAD missiles, and providing early warning for mobile forces. Operating in static deployments MHR provides short range C-RAM alerts, conduct perimeter security applications or be used as a sectorial gap filler. Both families are based on identical, interchangeable subsystems, thus simplifying support and reducing cost.\nEmploying modern Active Electronically Scanned Array – AESA antennae technology, these radars provide extremely fast volume coverage performing target search, classification and tracking. Innovative angular measurements techniques are used to overcome the small antenna size, along with Pulse-Doppler processing, and digital, adaptive beam forming, enabling a single radar to monitor a wide range of threat velocities.\nBy electronically stirring multiple beams the radar performs track while scan over a full hemispheric coverage, including very high elevations angles, required for ballistic trajectory calculations of typical RAM targets. It also provides real-time range and angle measurement required to support APS.\nThese software controlled radars are offering beam forming to control the spatial coverage, order of beams and their waveform, to tailor the radar for multiple missions either as a dedicated system or an ‘all in one’ sensor, interleaving several missions over certain periods of time. Switching between missions can be programmable, predefined, upon real-time events or manual.","Radarwas extensively used to detect enemy war planes and bombers throughthe World Wars and ever since. It has also found widespread usage inmodern airports to detect and direct planes wishing to land, andespecially in cases of blind- landing where the pilots have limiteduse of primary sight. Thus, Radar has become a revolutionary aspectof aviation, and a technology likely to find active usage for manymore decades to come1.The application of radar for military purposes, however, hasundergone more rapid evolution since the inception of radar, to thedeclaration of inefficiency of basic radar technologies as anintruder detection system during the rise of stealth technology2.Thousands of enemy aircraft were felled through radar guidance in thelong history of military warfare, but the introduction of radardetection avoidance, such as through the wave dispersion techniquesused by the US B2 Spirit has made radar efficiency very low or zero.This paper will explore the development, usage and the decline ofradar as an essential tool in the modern military\nRadaris basically a technology that uses radio waves to detect objects inspace within the range of a transmitting device. Most types of waveshave the property that they can be reflected off the surfaces ofobjects, in paths that obey the law of reflection such that the anglethat the incoming wave forms with a general tangent of the reflectingsurface is equal to the angle that the reflected wave also forms withthe same tangent in the opposite side of the normal line, and on thesame plane. Radio waves are in the lower half of the electromagneticwave spectrum, but other parts of the spectrum such as visible light,X-Rays, TV waves VHF and UHF spectra are also subject reflection, butall to within different levels. The traditional sense of radarapplies waves in the radio range of frequencies3.The reason for the exclusive use of radio waves is due to theirinherent property of lower attenuation by weather factors, air orother medium density, among other agents of scattering. This is incontrast to other portions of the spectrum such as visible light,infra-red and ultra violet, all of which are relatively more easilyscattered by non intended targets such as mist, fog, atmosphericdust, and rain- which in most military operations are not intended astargets.\nInoperation, a powerful transmitter (in the range of 30KW) casts apowerful beam of radiation into the space in the radar’s range. Most radar systems scan a circular or spherical region in 360degrees. Initial radar designs developed in the early and mid 20thcentury typically broadcast at 30KW and used wavelengths comparablein size to their most frequent targets, such as big fighters andbombers4.Once the radio beam hit the target, a considerable amount wasreflected back to the transmitter, whereby the size and distance ofthe object was determined by the amount of reflected radiation andthe time taken to re-capture the radiation so broadcast5.Poor selection of broadcast wavelength, as well as atmosphericattenuation and poor equipment, led to reception of very poorlydetailed reflected beams. The result was reflected waves that werescattered and vague, thereby giving only the most general guidance asto the presence and estimated location of targets in space.\nInaddition, little capability to detect and differentiate object motionwas available. As a result, extremely limited action was preciselypossible, and numerous misguided counter attack instances werelogged. Towards the latter half of the 20thcentury, more precise radar modifications were possible and shorterwavelengths typically the sizes of large birds were used. This meantthat objects within space were easier to map and identify. Typicalbombers ranged between 10-50 meters, while civilian aircraft weremostly larger, and other objects such as birds were less than a meterin length.\nInaddition to detecting the presence, possible identity, and theposition of intruders, the newer, more advanced systems alsoincorporated intelligent systems to determine the direction and speedof movement of intruders. To accomplish this, systems used DopplerEffect, or wave round-time differential or both. Using Doppler Effectrelies on the property that the frequency of a wave will tend tobecome higher if the emitter is moving towards an observer or tend tobecome lower if the emitter is moving away. Thus, an intelligentcounter in the radar system would pick up the rate at which thefrequency was changing and calculate the rate of motion of the objectaccordingly. The same calculations obtained from comparing thedifference in time of arrival of two successive signals that arereflected by the object are equally accurate6.\nActingon these projections, it was also possible to determine the manner inwhich to counter an attacker, many times to within very accuratedimensions. Radar was not only used in the airfield, but was alsoextensively used in other areas such as the marine security forces.In this use, submarines and vessel carriers were equipped with radarto scan the waters to within hundreds of kilometers for possibleintrusion. These systems were tuned to detect not just large vessels,but also smaller objects such as torpedoes fired towards the vessel.Weather radar and radar astronomy were other applications of radartechnologies. Radar altimeters were also designed to determine thealtitude of aircraft by ground control towers, an importantpossession in events where pilots were to be blind-landed by airtraffic controllers, or where a pilot’s onboard altimeter failedand they had to rely on control directions.\nRadarwas developed mainly in the 1920-1930 period when two researchers,Leo Young and Hoyt Taylor, experimenting for the US Navy, noted thatwaves passing between a transmitter and receiver tended to fade outwhen an obstacle was in the direct line between them. As such, thetwo researchers recommended the possibility of use of this fadepattern to discover objects not directly visible as long as theirabsorption was low. This speculation was especially important fordetection of ships during bad weather. In the decade, numerousresearchers experimented with different materials in the atmosphere,including fog, mist, the ionosphere, aircraft, birds and otherobjects in space in order to come up with a usability report forradar signals.\nBeforethe Second World War, most developed countries today including theUS, UK, France, Germany, Soviet Union, Japan and Italy independentlyworked to model their own versions of radar. The French linerNormandie was designed using radar technology to aid in navigationduring poor visibility. In addition, a soviet engineer designed theRAPID, a system to detect aircraft intruders within 3km. however, itwas not until 1934 that a fully capable radar system was designed bythe US Naval Research Laboratory. In this system, pulse generationwas used to demonstrate radar capability to detect obstacles inspace. The US Army then developed a radar guided strobe light systemin the coastal borders. At the same time, the Great Britain andGermany were experimenting with similar pulse radar systems. Theseexperiments yielded great results, and the first fully operationalradar detection system with aircraft detection efficiency wasdesigned by Watson Watt in Great Britain, in tracking points calledChain Homes. This system was timely as by 1938, the World War II wasimminent and the Royal Air Force was in high alert. This radar systemgreatly aided Britain in the Battle of Britain7.Chain Home CH radar systems relied on strong beams that broadcast inthe entire area of focus, and separate receivers that tracked thedirection of the incoming signals. This meant that very expensiveinstallations had to be made in order for the system to work. In theinitial application, radar was almost entirely ground operated. Asdevelopment commenced, radar was also applied onboard the aircraftand used effectively to scan the airspace surrounding the aircraft.This application is still in place today. This advancement found manyuses, including early warning systems for intruder or approachingenemy aircraft, as well as weapons in pursuit of the aircraft. Italso found widespread usage in ground tower to aircraft or aircraftto tower communications, a well as in emergency land in conditions ofpoor visibility8.This application is still in use today.\nRadartechnology advanced tremendously since the original invention of1935. Some of the major breakthroughs included the use of mono-pulsetechnology to concentrate the beam only in a limited portion of thetotal scan space. This breakthrough enables power saving on militaryand civil radar installations, as well as reduction of installationcosts for the units. As well, the development of this technologyenabled the combination of transmitter and receiver in one terminal,as signal floodlighting was no longer required. The detailing of thetarget was also much more efficient as the radio wavelengths usedwere now variable9.\nInthe 1970s, however, the results of numerous researches made itpossible to significantly avoid detection by radar systems. One suchtechnology was development of stealth. Already, German aircraft inthe First World War attempted detection avoidance by applyingCellulose Acetate. Even though at the time radar was not commerciallyused, the issue of sight detection avoidance was a key research area.Before 1945, when radar was already in use, Germany attempted onceagain to avoid radar detection. The development of H Ho Flying Wing229 towards the end of the war was the most successful attempt todefeat radar, and possibly successful too. The German authorities haddiscovered that avoiding radar would be possible only through eitherabsorbing all or most of the radiation incident on the aircraft, orby avoiding the entire scan space of radar signals. As the later wasnot possible, they focused on absorption capability. The Ho 229 wascovered with carbon lining, and, as later experiments proved, theaircraft would not have been detected at low altitude (50-100 ft) andhigh speed (900Km/h) to CH scanners in Britain.\nIn1970, true radar stealth was developed independently by variousscientists and researchers over the world. Lockheed employee DenysOverholser developed a formula of facet alignment on the body of anaircraft that would aid in radar signature distortion beyondauthenticable levels. Thus, radiation falling on a body is dispersedtoo much that the amount returning to the radar receiver is solittle. Heat signature distortion was also possible. The mostsuccessful project to beat radar was the US military B-2 Spiritdevelopment, even though other successful aircraft such as the F-117and F-22 are stealth capable. The development of the aircraft bomberwas initiated in 1970s by the Northrop Company. The aircraft has amultifaceted body which is flattened and made of heat dispersing bodyto hide the craft’s heat signature. The multiple facets enabledispersion of radar beams so effectively that radar is completelyunable to detect it. These developments were the dusk days of theradar as initially conceptualized. Radar is still a very key elementof today’s military world. The section below highlights some commonapplications of radar today\nRadarin Military World Today\nRadaris still in widespread primary operation today. The first and perhapsfundamental application of radar in military is still for targetdetection. Overall, only a very small percentage of military vesselsare stealth efficient. Thus, for the basic military operations, radarstill provides security against the larger percentage of possibleattacks or boundary intrusions. Radar is in use in airspace, land andwater intrusion detection systems, and still delivers excellentdependability. Only a limited number of countries have stealthweaponry10.\nTheother application of radar in the military is in aircraft navigationsystems. This application is common even for non-military operators.Pilots basically rely on radar to detect objects in their airfieldwith good dependability. It can help avoid collisions with otheraircraft or objects in space, or even crashing on the ground. Theground towers use radar to communicate with pilots, while pilots canalso use them to map the airport’s topology11.\nRadaris also used in the military guided weaponry. Heat seeking missilesapply radar integrated technology to seek targets. On the oppositehand, radar capability is also efficient in detecting missilesincoming towards a vessel or aircraft, and therefore helps incountering this attack. Radar guns are also in use in monitoringapproach speeds of objects such as at border patrol points. Therefore, radar continues to be a major necessity in the militaryapplications today, despite the fact that its applications arelimited in conditions of stealth enabled weaponry.\nKellerJohn. Radartechnology looks to the future.2008. Available athttp://www.militaryaerospace.com/articles/print/volume-19/issue-6/features/special-report/radar-technology-looks-to-the-future.html\nPickering, W. Radar-MilitaryWeapon or Civilian Lifesaver?2012. Available athttp://calteches.library.caltech.edu/119/1/Pickering.pdf\nRaemerHarold R. RadarSystems Principles.CRC Press, 1996\nWatson,Raymond C. RadarOrigins Worldwide: History of Its Evolution in 13 Nations ThroughWorld War II.Trafford Publishing. 2009.\n1 Keller John. Radar technology looks to the future. 2008 p. 1\n3 Pickering , W. Radar-Military Weapon or Civilian Lifesaver? 2012\n4 Keller p. 1\n5 Raemer Harold R. Radar Systems Principles. CRC Press, 1996 p. 4\n8 Watson, Raymond C. Radar Origins Worldwide: History of Its Evolution in 13 Nations Through World War II. Trafford Publishing. 2009. P. 5\n9 Watson, Raymond C. Radar Origins Worldwide: History of Its Evolution in 13 Nations Through World War II. Trafford Publishing. 2009. P. 6\n10 Keller, p. 1\n11 Harold, p. 5"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:83e9b887-60f1-4706-bd7d-60868f0c4e07>","<urn:uuid:9b6d7152-ce71-4cfd-8232-ec06e4cc9616>"],"error":null}
{"question":"Between voles and ground squirrels, which has more sizes of species, and what damage can they cause to gardens?","answer":"Ground squirrels have more size variation among species, with lengths ranging from chipmunk size up to 14 inches for larger species, while voles can only grow up to 7 inches long. Regarding damage, both pests harm gardens but in different ways. Voles feed on leafy vegetation, fruits, roots, and stems, and leave visible runways covered with vegetation. They also cause damage by girdling trees, chewing the darker outer bark to reveal lighter bark underneath. Ground squirrels, on the other hand, primarily forage aboveground, feeding on grasses, grains, and newly-sprouted plants. They dig numerous burrows for shelter and are known to dig up freshly planted seeds and bulbs. Additionally, they can damage garden infrastructure by gnawing on sprinkler heads, irrigation lines, and garden hoses.","context":["Grubs and Yard Voles\nGrubs and yard voles are two annoying pests that can result in varying degrees of damage to your plants, garden and lawn. In addition to plant damage, voles can carry lice, ticks and fleas and infest your lawn with these parasitic arthropods. Thankfully, several control options are available that get rid of nuisances and keep them from returning.\nGrub is a general term referring to the larval stage of various insects such as beetles. Depending on the species, grubs live in the soil or above ground. For example, the grubs living in the soil may be the larvae of masked chafers or may beetles while above-ground grubs could be leaf-eating caterpillars or borer larvae. Soil grubs tend to feed on the roots of plants and turfgrass and above-ground grubs consume leaves, buds and wood pulp.\nOptions available for controlling grubs include pesticides, predatory insects and manual control. Pesticides with diazinon, imidacloprid or sevin as their active ingredient control grubs in yards while the bacterium insecticide Bacillus thuringiensis kills the grubs of various insects above ground. Soil aerating sandals can help control grubs below ground by impaling the larvae with the spikes on the bottom of the sandal. Predatory insects are another option for controlling grubs both above and below ground. The Heterorhabditis and Steinernema species of beneficial nematodes feed on the grubs infesting lawns while woodpeckers consume the grubs of wood borer insects.\nVoles are sometimes confused with house mice. However, voles have smaller ears, a rounded head, rounded snout and a tail shorter than house mice. Voles have a dark brown coat with grayish colored fur on their belly. They feed on various species of plants, munching on leafy vegetation, fruits, roots and stems. A tell-tale sign that voles live in your yard or garden is brown feces that look similar to rice and grass clumps surrounded by grass clippings.\nCommon mousetraps can help control voles both inside and outside your home. Bait the trap by placing peanut butter under the pressure trigger to prevent the vole from stealing the bait without setting off the trap. You must place the baited trap at a right angle at the vole’s tunnel or runway. This control method requires regularly examining the traps once a day and removing any dead voles or re-baiting as needed. To keep voles away, practice proper cultural control by eliminating high grass and weeds near your yard where rodents can hide and create a habitat. Several commercial products designed to repel rodents such as voles are available at department stores and garden centers. These products contain thiram has the active ingredient and each brand has its own instructions you must follow for best results.\n- University of California Integrated Pest Management Program: Masked Chafers (White Grubs) — Cyclocephala spp.\n- Missouri Botanical Garden: Beetles - Borers and Miners\n- University of California Agriculture and Natural Resources: How to Control Soil Insects with Beneficial Nematodes\n- McGill University Urban Nature Information Service: Rodents\n- Virginia State University Cooperative Extension: Controlling Voles\n- National Center for Biotechnology Information: Parasitic Arthropods of Sympatric Meadow Voles and White-Footed Mice at Fort Detrick, Maryland.\n- Minnesota Department of Natural Resources: Microtus Ochrogaster\nAmanda Flanigan began writing professionally in 2007. Flanigan has written for various publications, including WV Living and American Craft Council, and has published several eBooks on craft and garden-related subjects. Flanigan completed two writing courses at Pierpont Community and Technical College.","Self-reliance and sustainability in the 21st century.\nHave you ever gone out to work in your garden and found your prized produce plagued by bites too large to be from any insect? Do you have so many holes in your yard you could use it as a golf course? Have innumerable little burrowing animals chiseled away at the trees in your orchard? If any of these questions hit a little too close to home, then you might have a problem with vertebrate burrowing animals. These pests can leave dirt mounds and holes in your yard and wreak havoc on vegetation. Their destructive habits can leave you wondering how to get rid of moles or squirrels or gophers—and how to keep these animals out of your garden. But before you start blasting holes in your yard, Caddyshack-style, make sure you know what kind of critter you’re dealing with.\nOften mistaken for mice, voles are small rodents with smaller ears and shorter tails than their doppelgangers. They have dark brown fur and can grow to 7 inches long. The three distinct species of vole are the prairie vole, the meadow vole and the pine vole. Each lives in the range that you would expect: Prairie vole habitats cover the Great Plains; meadow voles range in most eastern states and the Midwest; and pine voles live primarily in the forests of western states.\nDamage from prairie and meadow voles is very similar — both leave “runways” that look like little trenches throughout an open area. The burrowing animals usually cover these runways with loose vegetation, such as mulch, grass clippings or leaves, and at the end of the runways they build burrow holes where they breed and nest. All species have a propensity for gnawing at the roots and bases of fruit trees. This “girdling” effect is very pronounced, as they chew away at the darker outer bark to reveal the lighter, more tender bark underneath. “Stalky” and low-fruiting garden plants, such as cauliflower, artichokes and Brussels sprouts, as well as root vegetables, such as beets and carrots, are especially at risk for vole damage.\nPocket Gopher Identification\nPocket gophers live in nearly every state west of the Mississippi River and resemble a cross between a rat and a mole. Their fur color can range from light brown to nearly black, and they sport short tails and large, wide front feet for digging tunnels and dirt mounds. The pocket gopher gets its name from the large cheek pouches it keeps tucked behind its sharp, chisel-like front teeth. These front teeth always protrude from its mouth and grow quickly, so the gopher needs constantly chew to keep its teeth at a manageable size. This habit makes pocket gophers one of the most frustrating burrowing animals to deal with. Not only do they eat and damage the roots of various crops and plants, but also they can easily chew through irrigation lines and power cables. Most of the time a gopher will eat vegetation it comes across while digging, and occasionally it will venture out of its mound to snag something to eat— but by no more than a body’s length.\nPhoto by iStock/Susanne Friedrich\nOther than damage to vegetation and utility lines, the most visible signs of gophers are the distinct dirt mounds they make. These crescent-shaped mounds are often mistaken for molehills, but these dirt mounds have a unique shape. Gopher mounds are shaped like a “C,” with a plug of soil in the center that acts as a door for gophers, whereas molehills are a solid cone of earth with no distinct shape or plug. Gophers also tend to stay deep enough underground that they make no raised tunnels as moles do.\nGround Squirrel Identification\nThe term “ground squirrel” refers to no specific animal; it is an umbrella term for many different kinds of rodents, including chipmunks and marmots. The three most common in the United States are the eastern chipmunk, the thirteen-lined ground squirrel and Franklin’s ground squirrel. Ground squirrels live in nearly every region in the United States and can grow up to 14 inches long, depending on the species. Smaller varieties resemble common tree squirrels but have shorter tails, smaller ears and distinct coloration, again depending on species. The eastern chipmunk sports two “racing stripes” along its back, while the thirteen-lined ground squirrel has many thin stripes separated by rows of small spots. Franklin’s ground squirrel has no distinct markings but has a solid-color fur that can range from brown to silver.\nPhoto by iStock/digi_guru\nGround squirrels cause damage in many ways. While they tend to forage for food aboveground, they dig numerous burrows to use as shelter between meals. Ground squirrels feed on grasses, grains, and newly-sprouted plants and produce, and they also have been known to dig up freshly planted seeds and bulbs, and to gnaw on sprinkler heads, irrigation lines and garden hoses. On rare occasions, ground squirrels also have been carriers of bubonic plague. While there are, on average, only seven cases of bubonic plague per year, you should still take precautions when handling these animals. Also note that in some states, Franklin’s ground squirrel has protected status, so check with your local wildlife bureau.\nAlso known as a woodchuck or whistle pig, a groundhog is a large variety of ground squirrel of the marmot genus. Physically, groundhogs range in coloration, from brownish-gray to black and have stout, stocky bodies that can grow to be more than 16 inches long, and weigh as much as ten pounds. Groundhogs mainly range in the portion of the United States east of the Missouri River but also live in the eastern regions of the “tornado alley” states, such as Kansas and Oklahoma. They prefer open rangeland in which they build their system burrows but can also be found in wooded or “shrubby” landscapes. These burrow systems can have anywhere from two to three openings, each 10 to 12 inches in diameter, with the main entrance marked by a dirt mound. Secondary entrances are hard to locate, as they usually have no dirt mounds.\nThese secondary entrances can be a safety hazard to both humans and large animals because they’re usually hidden to protect the burrow from predators. In addition to the physical damage that comes from these burrowing animals, groundhogs can be the culprits behind destruction to vegetation and utility lines. They feed on alfalfa, clover and other grasses but also eat soybeans, peas, carrot tops and Fabaceae. They primarily feed in the evening and early morning and will not usually venture out of their burrows more than 50 feet. Groundhogs can be very active during the day and can often be seen dozing on fence posts or low tree branches.\nMoles often get a bad rap as yard and garden pests, but they can actually be very beneficial to an ecosystem at large. The tunnels and dirt mounds that these critters create can be an eyesore, but they also help aerate the soil and provide channels for excess water. Moles’ droppings keep things fertile. Moles reside in nearly every region in the United States except the Southwest. They aren’t rodents but are a type of burrowing mammal known as a talpid. All breeds of mole are similar in appearance—they have velvety fur, large front paws for digging, and no noticeable eyes or ears. Depending on the species, moles can grow up to 8 inches long.\nDamage from pocket gophers is often blamed on moles and vice versa, but their burrowing and feeding habits are very different. Whereas gopher tunnels aren’t visible aboveground, moles leave behind raised tunnels between hills. These are the moles’ feeding tunnels, where they hunt for insects and worms that live in the ground. These tunnels are made on an as-needed basis while the moles hunt and can be easily tamped down with a foot or shovel. Moles do very little damage to vegetation and produce, but they have the potential to push newly planted seedlings out of the soil, so farmers and gardeners still have reason to learn how to get rid of moles naturally. As insectivores, moles don’t eat plants; they often will go around or tunnel under obstructions, such as roots and utility lines, as their teeth aren’t adapted for gnawing like rodents’ teeth are.\nHow to Get Rid of Burrowing Animals\nAlthough people use chemical solutions to get rid of burrowing animals, many gardeners and home owners have learned how to get rid of moles and other animals naturally. With a little planning and attention, methods of natural mole control—such as habitat modification and barriers—can effectively keep animals out of your garden and keep their mounds of dirt and holes out of your yard.\nMake sure your space is clear of food sources and ground cover, such as leaves and grass clippings, to greatly reduce the likelihood of a vole infestation. Cover the base of young trees and plants with a partially buried plastic or wire collar to protect them from girdling animals.\nTrapping is another effective method of controlling burrowing animals. You can trap voles with simple mousetraps positioned in their runs near the burrows, whereas you’ll need larger, specialized traps to effectively capture gophers, ground squirrels and moles.\nTo keep burrowing animals out of your garden, construct an L-shaped barrier of 1/4-inch hardware cloth around the area you want to keep pest-free. To do this, bend a few inches of hardware cloth at a 90 degree angle, and then bury it below ground. Make sure you have enough cloth aboveground to protect your garden (specific dimensions for hardware cloth barriers are listed below). This barrier should be built 2 feet from any plant beds to protect developing roots from damage. You can also use hardware cloth to completely line the bottom of raised garden beds to keep the animals out of your garden.\nDiagram courtesy the Washington Department of Fish and Wildlife\nHow to get Rid of Voles:\nWhen dealing with voles, bury your hardware cloth 6 to 10 inches below the ground with a raised section of at least 12 inches. The foot of your “L” should be 1 to 2 inches.\nPhoto by iStock/CreativeNature_nl\nHow to get Rid of Pocket Gophers:\nTo keep gophers out of your garden beds, bury your hardware cloth at least 2 feet deep, with 6 inches of cloth forming the foot of the “L.” Ensure that at least 12 inches of cloth remain aboveground to prevent surface assaults.\nHow to get Rid of Ground Squirrels:\nFor ground squirrels, bury your hardware cloth 6 inches below ground, with 1 to 2 inches of cloth forming the foot of the “L.” Make sure the cloth extends 18 inches aboveground.\nHow to get Rid of Groundhogs:\nBury your hardware cloth at least 12 inches below ground, with 2 extra inches forming the foot of your “L.” There also should be 3 feet of fencing aboveground.\nPhoto by iStock/jimkruger\nHow to get Rid of Moles:\nBury your hardware cloth 2 feet below ground, with 6 inches of bent cloth forming your “L” foot. There also should be 6 inches of cloth sticking up aboveground to deter moles from entering the exclusion area.\nPhoto by iStock/nwbob\nWhile many chemical deterrents for dealing with critter invasions are on the market, waste from domestic animals can work just as well. Plug holes with waste from cats, dogs and even ferrets to dissuade burrowing animals from surfacing.\nCommercial and homemade castor-oil-based repellents have been shown to be somewhat effective for natural mole control. To make a homemade castor oil repellant, combine 1/2 cup of castor oil with 1/2 cup of dishwashing liquid. Mix 2 to 3 tablespoons of this concentrate in a gallon of warm water, and spray the concoction around areas you want to protect from moles.\nYou can also consider a number of electric deterrents that use ultrasonic sound or vibrations to scare pests off your property, but their effectiveness is debatable.\nThere’s always the option to let nature take its course when it comes to burrowing animals. Predators, population fluctuations, and migration due to unfavorable habitat conditions can all be helpful in the fight against pests. However, this ultimate natural method can be inefficient and time-consuming and can end up costing you more damage in the meantime.\nSometimes a good dog or cat can help solve your pest problem, or, if you’re a good enough shot, you can get rid of pests with a firearm.\nThis piece on how to identify and get rid of burrowing animals was adapted from:\nUniversity of California Pest Notes Publication 7439 – Voles (Meadow Mice)\nUniversity of Nebraska-Lincoln Extension, Institute of Agriculture and Natural Resources – Controlling Vole Damage\nAlabama A&M and Auburn Universities – Controlling Voles\nUniversity of California Pest Notes Publication 7433 – Pocket Gophers\nUtah State University Cooperative Extension – Wildlife Damage Management Series: Pocket Gophers\nOklahoma State University Cooperative Extension – Controlling Pocket Gophers\nColorado State University – Natural Resources Series: Managing Pocket Gophers\nUniversity of Illinois Extension – Living With Wildlife in Illinois: Ground Squirrel\nPerdue – Wildlife Conflicts Information Website: Ground Squirrels\nUniversity of California Pest Notes Publication 7438 – Ground Squirrel\nPerdue – Wildlife Conflicts Information Website: Moles\nUniversity of California Pest Notes Publication 74115 – Moles\nWashington Department of Fish & Wildlife – Living With Wildlife: Moles\nMother Earth News – How Do I Stop Moles from Damaging My Garden\nUniversity of Nebraska-Lincoln Extension, Institute of Agriculture and Natural Resources – Woodchucks\nMassachusetts Executive Office of Energy and Environmental Affairs – Preventing Damage by Woodchucks\nMissouri Department of Conservation – Woodchuck (Groundhog)\nUniversity of Missouri Extension – Managing Woodchuck problems in MissouriInternational Union for Conservation of Nature RED list"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:1c8fc0af-eae6-4d19-aabc-603437ba6e23>","<urn:uuid:071b923e-6ef1-471d-a542-5356016cf5ff>"],"error":null}
{"question":"How did UFA's role in German cinema evolve before and during Nazi rule?","answer":"UFA (Universum Film Aktien Gesellschaft) was initially established in 1917 with state money and became one of the world's leading studios, producing notable films like 'The Cabinet of Doctor Caligari' and 'Metropolis'. However, after financial troubles, right-wing financier Alfred Hugenberg gained controlling share. Many of UFA's greatest talents, including Fritz Lang and F.W. Murnau, eventually left for Hollywood. The studio was later nationalized under Hitler's regime and became a tool for mass communication, with control of the nation's screens handed over to propaganda minister Joseph Goebbels.","context":["The Blue Angel\nBlue Angel director Josef von Sternberg (1894-1969) was born under the name of Jonas Sternberg to an impoverished Orthodox Jewish family in Vienna. When his father moved to the United States for several years, the rest of the family later joined him for three years before returning to Vienna. Sternberg moved permanently to the U.S. when he was fourteen, acquired a spotty education (much of it self-taught) and worked at various odd jobs before getting his start in the film industry. By the time of The Blue Angel (1930), he had established himself as one of Paramount's most talented, if not always bankable, directors with films such as the popular gangster movie Underworld (1927), The Docks of New York (1928) and The Last Command (1928). The latter earned Emil Jannings an Academy Award for Best Actor. Erich Pommer of UFA then invited Sternberg to Berlin to make a sound film about Rasputin. \"This failed to interest me,\" Sternberg wrote in his memoirs, Fun in a Chinese Laundry, \"as I had no intention of treading on anyone's historical toes, nor did I wish to film a story which permitted no speculation about its outcome.\" Instead, he proposed an adaptation of the book Professor Unrath by Heinrich Mann. The original 1905 novel was a trenchant critique of German middle-class hypocrisy; in the process of adapting the film for the screen, the entire second half of the book (in which Professer Rath takes his revenge on society) was jettisoned and a new tragic ending was created.\nAlthough Emil Jannings was nominally the lead, Sternberg's real interests lay in the role of the callous seductress Lola-Lola. Sternberg wrote, \"Without the electricity of a new and exciting female, the film would have been no more than an essay reflecting on the stupidity of a school tyrant.\" Various candidates for the role included Heinrich Mann's friend Trude Hesterberg, Brigitte Helm, singer Lucie Mannheim, and Kathe Haack. The latter was already signed up for the role when Sternberg met Marlene Dietrich through Karl Vollmoeller; out of this began one of the most remarkable - and emotionally complex - collaborations between actress and director in the annals of Hollywood.\nBecause of Dietrich's limited vocal range, composer Friedrich Hollander wrote songs specifically to accommodate her voice; Dietrich's unique, much-parodied performance style became the basis of a second career as a concert singer, which lasted long after she retired from the screen. Hollander's songs, especially \"Ich Bin Von Kopf Bis Fuss Auf Liebe Eingestellt\" (\"Falling in Love Again\"), became standards in their own right. Inspired partly by the erotic drawings of Belgian artist Felicien Rops, Sternberg decked out his heroine in silk stockings, high-heeled shoes and a top hat. The combination of actress, costume, music and Sternberg's legendary lighting skills created an icon which resonates even today. Jannings, who still imagined himself star of the film, became jealous of the intimate relationship between Sternberg and Dietrich and the cinematic attention lavished upon her, throwing tantrums on the set and at one point threatening to strangle the leading lady. Although Jannings continued to act in films through the Nazi era (which brought him shame after World War II), The Blue Angel marks his last great moment as an actor; surely his jealousy arose in part because he knew he was being eclipsed.\nOn March 31, 1930, the night of the film's triumphant premiere in Berlin, Sternberg and Dietrich sailed to New York together to pursue a career in Hollywood. Sternberg's wife, Riza Royce von Sternberg, who had sensed his growing affections for his star, had returned to America earlier. She met Sternberg and Dietrich at the dock with her lawyer, serving Dietrich with papers for a lawsuit alleging libel and \"alienation of affection.\" A few months later Sternberg and his wife divorced.\nOne of the first sound films of UFA (Universum Film Aktien Gesellschaft, or Universe Film Company Limited), The Blue Angel was also one of its last great productions. Established partly with state money in 1917, UFA rapidly became one of the leading studios in the world with films such as Robert Wiene's The Cabinet of Doctor Caligari (1919); Ernst Lubitsch's Madame Dubarry (1919); F. W. Murnau's The Last Laugh (1924); and the profligate, visionary Metropolis (1927), which nearly bankrupted the studio and resulted in the right-wing financier Alfred Hugenberg gaining a controlling share. Many of UFA's greatest talents - among them Fritz Lang, Murnau, Lubitsch, the cinematographer Karl Freund - eventually left for Hollywood. After World War II, the defunct UFA's massive soundstages at Neubabelsberg wound up on East German territory; they were used to establish DEFA, the East German studio.\nThe Blue Angel was shot simultaneously in German and English language versions, making it necessary for the actors to read their lines in two different languages. During the early sound era such practices were not uncommon, since existing sound technology made dubbing difficult and studios wanted to create films that could sell to an international market. Another notable example is Tod Browning's Dracula (1931), for which an altogether different cast and director was used to film the Spanish-language version, shot at night on the same sets as the better-known English-language version. The English-language version of The Blue Angel is still exhibited occasionally, though for obvious reasons the German-language version is generally preferred.\nOver the years The Blue Angel has been shown in various public-domain prints, many of which have been cut. The version shown on TCM this month is the full-length German-language version, distributed by Kino International and licensed from the Murnau Foundation in Germany. A new print was struck from the best surviving materials and a new translation of the dialogue was commissioned. During the film's recent theatrical re-release the results were described by Michael Atkinson of the Village Voice as \"the best print anyone has seen in 70 years.\"\nProducer: Erich Pommer\nDirector: Josef von Sternberg\nScreenplay: Carl Zuckmayer, Karl Vollmoeller and Robert Liebmann; based on the novel Professor Unrath by Heinrich Mann\nCinematographer: Gunther Rittau and Hans Schneeberger\nEditor: Sam Winston\nMusic: Friedrich Hollander; lyrics by Robert Liebmann\nSet Design: Otto Hunte, Emil Hasler\nPrincipal Cast: Emil Jannings (Immanuel Rath), Marlene Dietrich (Lola Frolich), Rosa Valetti (Guste), Hans Albers (Mazeppa), Kurt Gerron (Kiepert), Karl Huszar-Puffy (Proprietor), Reinhold Bernt (Clown), Roland Varno (Lohmann).\nIn German with English subtitles\nby James Steffen","During the war years, Walter Ruttmann made propaganda films for the Nazis. The only reason I jump into this essay with that troubling fact is that most summaries of the German experimental filmmaker’s storied life try to slip it in at the end, as if they’re saying it under their breath. His work for the Third Reich raises a lot of questions, both biographical and philosophical, among them why Ruttmann didn’t flee the country like so many of his peers, how much culpability filmmakers and artists who glorified Hitler share in his crimes, and can a work of art be formally, aesthetically appealing even if it’s content is abhorrent. Due the limits of space and to preserve my own peace of mind, we’re not going to go too deep into those hefty topics (and besides there’s plenty of ink spilled over them already in regards to Leni Riefenstahl, whose Triumph of the Will Ruttmann co-edited). Instead, it seems a better use of our time to look further back, before Hitler’s sweeping rise to power, when Ruttmann was making a very important (and very un-fascist) series of films.\nCompleted between 1921 and 1925, Ruttmann’s four-part Opus series bears little resemblance to anything that can be considered propaganda, or even reality. It’s a work of total abstraction, arguably the first of its kind, abandoning representation in an effort to find a truer form of cinema, one more in line with the way music functions rather than trying to shoe-horn the medium into the conventions of theatre and literature. “You can gather together the best mimes in the world,” he wrote about narrative film in the silent age, “You can let them perform in the most exquisite paradise, you can adorn the programs of your film dramas with the names of the most eminent poets – Art will never result that way. A work of art will result only if it’s born of the possibilities and demands of its material.” To Ruttmann, the material of film, the celluloid itself, demanded not storylines but symphonies, made with light instead of sound. Although it would probably be more accurate to say symphonies made with light and sound, since Opus, unlike many of films by his contemporaries, boasts a score composed specifically to accompany it.\nTinting film was a common practice long before color stock was invented, and though it was an expensive process, Ruttmann meticulously explored the technique, laboriously layering multiple colors to create a psychedelic fantasia, one so complex that striking a print suitable for projection meant first carefully assembling hundreds bits of film. Before turning to cinema, Ruttmann was a painter and graphic designer, and his sense of color and space suggests abstract art thrust into dizzying motion. Writing that one must “Work with film as though using a paintbrush and paint”, he often did just that, applying paint directly onto the already colored celluloid. Perhaps to distance his work from the psychological dramas of the German Expressionist films then en vogue in Berlin, Opus was not even quite presented as “films”, in fact, each part bears the label “Lichtspiel”, which roughly translates “light-play” or “light-show”. In all honesty, between the music and the trippy visuals, the screenings held by Ruttmann and his “Color Music” peers in Berlin’s “Absolute film” scene don’t seem to be too far off from later happenings like Ken Kesey’s Acid Tests or Warhol’s Exploding Plastic Inevitable, especially since some of them threw multiple film and slide projectors into the mix. A lot more film theory and a lot less LSD perhaps, but then again, drugs and debauchery weren’t exactly in short supply during Berlin’s “Golden Twenties” either.\nIt was multimedia art before there was a word for it, born out of an attitude among the city’s intellectuals and avant-garde that the new media channels created by the relatively recent proliferation of cinema and radio could be used to mass communicate radical artistic ideas. Indeed, Ruttmann himself went on to create feature length films that met audiences halfway by being at least slightly more direct and representational, as with 1927’s Berlin: Symphony of a Metropolis, which single handedly created the “city symphony” genre, as well as original compositions for radio, like the ahead-of-its-time sound collage “Weekend”, thought lost until a recording surfaced in New York in 1978 (and which later inspired a remix album featuring versions by DJ Spooky and To Rococo Rot). In a cruel irony, the potential artists saw in these new methods of mass communication was about to be harnessed, and not in a good way, as Hitler transforms the entire German media into a hideous brainwashing tool, most notably by nationalizing the country’s largest film concern, UFA, and handing control of the nation’s screens over to propaganda minister and evil-piece-of-human-shit Joseph Goebbels.\nAnd Ruttmann works for them. As the political and social climate in Germany grew more menacing, many in the film industry fled the country, with quite a few of them, including Fritz Lang and Douglas Sirk, ending up influential in Hollywood, but not Ruttmann, who stuck around and had a significant hand in creating the most notorious Nazi propaganda film of all time before dying in 1941, four years before Berlin fell to the Allies. There’s no apologizing for his being a cog in a machine that murdered millions of people, but judging solely from the films he made free of the influence of the Third Reich, he doesn’t seem like much of a hardliner. His traumatic stint in World War I was followed by a nervous breakdown so it would seem doubtful he’d be excited at the prospect of another go around, and though there is an unfortunate jazz age African stereotype in a film from this early era, 1926’s Spiel der Wellen (included here), the overall attitude of the short seems more admiring than xenophobic or racist. As writer and critic Rob Edelman put it, “An artist whose work was initially apolitical, Ruttmann neither protested nor went into exile with the advent of National Socialism. Instead, he conformed.” Rather pathetically, in the end, its plain cowardice and complacency, not conviction or belief that eternally derailed what had once been a courageous career."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:14c6b1d4-8ed2-45f7-908f-b951b78565bf>","<urn:uuid:a3cb4278-bbd2-47c4-8671-3cdb21589fe6>"],"error":null}
{"question":"What determines the taste differences between North American Atlantic oysters, and how much water do they filter daily?","answer":"All native North American oysters from Atlantic-connected waters are the same species. Their taste and flavor differences are determined by the composition of the waters that nourish them, while their size and shape differences relate to the type of underwater bed surfaces where they spawn and grow. Each oyster filters 96 gallons of water per day to obtain its nutrients.","context":["The oysters pictured at top left are Gerrish Island oysters from Wegman’s. Those at bottom right are Pepperell Cove oysters from Wegman’s. Each is being served with a mignonette sauce, easily made by combining ¼ cup of shallots with 1/3 cup of red wine vinegar, then adding a teaspoon of coarsely cracked peppercorns and a large pinch of kosher salt.\nWe commented on these two kinds of oysters when describing the quest for bounty to grace the Christmas dinner table. The man working Wegman’s seafood counter had noted that both were from Maine. He was right. They came from Maine, but neither originated there.\nThe Gerrish Island oysters had a shape more rounded than the Pepperell Cove’s. Distinguishing the Pepperell Coves was a bit of grassy green coloration in their shells. The man at Wegman's informed that the Pepperell Cove’s should taste a bit more salty or briny.\nA quick google confirmed that both Gerrish Island and Pepperell Cove were Maine locations. Both oysters, when opened, proved wonderfully juicy and briny. In fact, they tasted almost identical. Needless to say, by Christmas, all three dozen oysters had been consumed.\nSeveral days later, parked along Pulaski Highway in Baltimore County, was a white truck from which two kinds of oysters were being sold. The choice was between Delaware Bay oysters and Canadian oysters from Prince Edward Island. The $15.00 price for two dozen Prince Edward Island oysters seemed well worth the money. We were intrigued over how much they looked like Pepperell Cove oysters. When opened, they were salty and delicious with a slightly different flavor, also less juicy and a bit meatier. It was a close call, but we preferred the Pepperell Coves.\nFurther research led to the website of Spinney Creek Shellfish, Inc. in Eliot, Maine. It advertised Pepperell Cove oysters from Prince Edward Island, and Gerrish Island Oysters from \"Mid-Atlantic waters.\" Then a google of \"Prince Edward Island oysters\" suggested that the vast majority of oysters from Prince Edward Island were sold as Malpeques, in conjunction with PEI's Malpeque Bay, which has long been famous for its oysters. Could the oysters from the truck on Pulaski Highway have been Malpeques? For that matter, were all oysters from Prince Edward Island actually Malpeques? And what about the Pepperell Cove Oysters? The purveyor at the truck on Pulaski Highway had registered a blank when asked.\nUltimately Lori Howell, one of the principals at Spinney Creek Shellfish, Inc., provided all the answers that were needed by telephone. She explained how her company is in the business of sourcing Atlantic oysters that are brought to its Maine depuration facility.There they are placed in tanks of sterilized Maine seawater for cleaning and later tested at the company’s labs for safety and quality. Then the company brands its oysters. Those from Prince Edward Island are branded with the name Pepperell Cove; The Delaware Bay Oysters are branded as Gerrish Island oysters. When asked if the Pepperell Cove oysters from PEI were in fact Malpeques, Ms. Howell explained that Malpeque was another brand name granted exclusively to producers on Prince Edward Island.\nThe bottom line, Ms. Howell explained, is that all native North American oysters from waters linked to the Atlantic, including the Gulf of Mexico, are one and the same in terms of species: Atlantic oysters. Differences in taste and flavor are determined by the composition of the waters that nourish them. Differences in size and shape relate to the numerous types of underwater bed or bottom surfaces where they spawn and grow. An oyster filters 96 gallons of water a day from which it obtains its nutrients. Needless to say that after spending a number of hours in a tank of sterilized Maine seawater, a Prince Edward Island oyster is going to look the same but taste quite different than when it left Canada. The same can be said for oysters from Delaware Bay or anywhere else.\nHere in Baltimore, everyone knows how Chesapeake Bay oysters, rendered more briny after having been placed into Chincoteague waters, were sold as Chincoteagues. The Chesapeake and Chincoteague oyster harvests have fallen on hard times over subsequent decades. Meanwhile, companies like Spinney Seafood, Inc. are increasingly filling the void."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:0457baf8-f055-4c79-bdcd-7e58d03fe742>"],"error":null}
{"question":"What are the key differences between stainless steel and nanometals in terms of their composition and structural stability?","answer":"Stainless steel and nanometals have distinct compositions and stability characteristics. Stainless steel is defined by its content of 10.5-20% chromium and aluminum, which gives it corrosion resistance and anti-bacterial properties. In contrast, nanometals' properties come from their extremely small grain size (10-1,000 nanometers), but they face stability challenges as their grain boundaries can move even at room temperature, leading to structure coarsening and weakened strength. This instability can be addressed by locking the grain boundaries with small particles. While stainless steel's properties come from its chemical composition, nanometals' strength comes from their physical structure at the microscopic level.","context":["Today, the body of an ordinary family car consists of 193 different types of steel. The steel for each part of the car has been carefully selected and optimised. It is important, for example, that all parts are as light as possible because of the fuel consumption, whereas other parts of the car have to be super strong in order to protect passengers in a collision.\nSmaller metal grains result in stronger metals that could better protect passengers in a car crash.\nSuper strong nanostructured metals are now entering the scene, aimed at making cars even lighter, enabling them to stand collisions in a better way without fatal consequences for the passengers. Research into this field is being conducted worldwide. Recently, a young PhD student from the Materials Research Division at Risų DTU took research a step further by discovering a new phenomenon. The new discovery could speed up the practical application of strong nanometals and has been published in the highly esteemed journal \"Proceedings of the Royal Society\" in London in the form of a paper of approx. 30 pages written by three authors from Risų DTU.\nThe research task of the young student, Tianbo Yu, is to determine the stability in new nanostructured metals, which are indeed very strong, but also tend to become softer, even at low temperatures. This is due to the fact that microscopic metal grains of nanostructured metals are not stable - a problem of which Tianbo Yu's discovery now provides an explanation.\nThe fine structure consists of many small metal grains. The boundaries between these metal grains can move, also at room temperature. At the same time a coarsening of the structure takes place and the strength of the nanometal is consequently weakened. Tianbo Yu's has now shown that the boundaries of the grains can be locked, when small particles are present and that the solution is technologically feasible. This has paved the way for car components to be made of nanometals.\n\"We are cooperating with a Danish company and also a Danish consulting engineering company with the purpose of developing light and strong aluminium materials with a view to their application in light vehicles where especially deformation at high rate as in a collision is in focus. The new findings will be included in this work,\" says Dorte Juul Jensen, head of division and Dr. Techn. She is happy that the excellent findings also have practical applications.\nSmaller metal grains result in stronger metals\nNanometals contain very small metal grains - from 10 to 1,000 nanometers. One nanometer is a millionth of a millimetre. The smaller the metal grains become, the stronger the metal becomes. The metal becomes twice as strong, for example, if the individual metal grains are made four times smaller. That is why the materials scientists work to reduce the size of the individual metal grains. In steel and aluminium, the particles have been reduced to below 1 micrometre, which is one thousandth of a millimetre. There is a great interest in nanometals worldwide. Nanometals are super strong and their super strength can be combined with other desired properties, too.\nA good example of a super strong nanometal is the thin steel wires used in grand pianos and for strengthening lorry tyres and containers, which have to withstand an extremely high pressure. Actually, they have been known for many years, but now they have become the subject of scientists' renewed and strong interest.\nScientists are not only interested in the size of the metal grains. The interfaces between the individual metal grains are also important to a number of properties. A special type of grain boundaries, so-called twin boundaries, provides both strength and good electrical conductivity. This paves the way for producing thinner wires, thereby reducing material consumption.\nSource: Risų National Laboratory for Sustainable Energy","The Most Common Types of Steel for Metal FabricationMac-Tech Inc.\nSteel has become ubiquitous in American life. Most people recognize structural steel when they see skyscrapers under construction. We can find stainless steel in our dining utensils and appliances. And carbon steel is often used to create lightweight yet strong bicycle frames.\nSteel is made of iron and carbon. Different types of steel have different percentages of iron and carbon and other additives that form alloys. Here are three of the most common types of steel for metal fabrication. Learn a little about how they differ from each other.\nThe smooth shine of stainless steel graces everything from dining utensils in kitchen drawers to large architectural structures like bridges and building exteriors. Stainless steel comes in several varieties, but one can distinguish it from other types of steel by the addition of 10.5 to 20 percent chromium and some aluminum.\nStainless steel resists corrosion and has anti-bacterial properties that make it a metal of choice for surgical instruments, food preparation surfaces, and high-touch surfaces like door and cabinet handles. Metal fabricators create stainless steel railings, components for tables and appliances, and decorative architectural components for residences and commercial buildings.\nCarbon steel includes structural steel and has up to two percent carbon content. Fabricators use low-, medium-, and high-carbon steel for different purposes.\nLow-carbon steel, the most common type of steel available for metal fabrication, includes up to 0.3 percent carbon. It’s inexpensive and has the best combination of tensile strength and ductility (the ability to stretch or deform without cracking) to create beams that support building structures. Manufacturers also use fabricated low-carbon steel for machinery, pipes, auto components, and appliances.\nMedium-carbon steel is steel with between .31 and .60 percent carbon, plus .31 to 1.6 percent magnesium. It’s very strong but has lower ductility than low-carbon steel. You may find medium-carbon steel in gears or train tracks.\nHigh-carbon steel has .61 to 1.5 percent carbon and .31 and .90 percent magnesium. It’s very hard and tough, making it durable and difficult to cut or weld. High-carbon steel is used for springs, plates, and railways.\nMetal fabricators use different types of machinery to cut and form steel. One of the most powerful machines is the fiber laser cutting machine, which uses targeted, superheated laser light to cut sheet metal into precise shapes.\nMachines that punch, stamp, or bore through metal are often made of a common type of steel used in metal fabrication called tooling steel. It is hard and heat resistant due to the addition of metals like tungsten, molybdenum, cobalt, and vanadium.\nTooling steel falls into various classifications based on what its particular formulation does best. The classifications relate to how the steel was “quenched” or hardened during manufacturing with hot, cold, or air hardening. There are also grades for high-speed spinning tools, shock resistance, and suitability for tooling uses like cutting, stamping, or punching.\nThese common types of steel are used in metal fabrication for manufacturing, repair, and construction. They can create a wide variety of tools, structures, appliances, parts, and components that we encounter in homes, businesses, and industries."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:a0ee9846-c3b9-48f4-aaf6-556e34324f24>","<urn:uuid:836e55de-07b2-4773-a1c2-eb9f34ad3a71>"],"error":null}
{"question":"How does the Buddhist concept of emptiness relate to physical emptiness, and what ergonomic practices promote an 'empty' or relaxed wrist position?","answer":"In Buddhist context, emptiness (空) refers to the void or unreality of all phenomena, teaching that all things lack self-essence and are compounds that come into existence only to perish. When it comes to physical emptiness in ergonomics, the natural or 'empty' wrist position is achieved by letting the forearm rotate inward naturally on a hard surface, keeping the wrist straight with the palm at a 30-45 degree angle and fingers curled. This position minimizes stress on tendons and nerves, and should be maintained while working to prevent repetitive stress injuries.","context":["Not what you want?\nTry searching again using:\n1. Other similar-meaning words.\n2. Fewer words or just one word.\nBuy an Empty calligraphy wall scroll here!\nStart your custom \"Empty\" project by clicking the button next to your favorite \"Empty\" title below...\n虛空 means empty space, empty sky, or void.\nIn the Buddhist context, it can mean \"emptiness of the material world.\" This can also be used as an adjective to modify other words with a meaning of unreal or insubstantial.\nThe first two characters mean \"karate\" - technically they express \"empty hand.\"\nThe last two express \"fist law\" which is Romanized from Japanese as \"Kenpo\" or \"Kempo.\"\nThat \"empty hand\" translation can be understood better when you grasp the idea that karate is a martial art without weapons (other than the weapons organic to your body, such as your foot, hand, fist, etc). When you practice karate, you do so with empty hands (no weapons).\nNote: There is also an antiquated way to write karate. It has the same pronunciation but a different first character which means \"Tang\" as in the Tang Dynasty. Some dojos use that form - let us know if you need that alternate form, and we'll add it for you.\nCredit is given that karate started in China but migrated and became refined, and vastly popular in Japan. The literal meaning of these characters is \"empty hand method\" or \"empty hand way.\" Karate is a martial art that uses no blades of weapons other than the \"natural weapons\" that God gave to humans (fists and feet). The last character somehow became optional but the meaning of that character is \"method\" or \"the way\" as in Taoism / Daoism.\n団結空手道 is the title for Danketsu Karate-Do, a dojo located in Stroudsburg, PA.\n団結 (danketsu) means union, unity, or combination.\n空手道 (karate-do) means \"empty hand way\".\nIf you need you martial arts school/dojo/academy added to my database, just give me the info (actual Chinese/Japanese text if you have it).\n無 is the simple way to express \"nothing.\"\nHowever, this single character leaves a bit of mystery as to what you might really mean if you hang it as a wall scroll. I'm not saying that's a bad thing; as you can decide what it means to you, and you won't be wrong if you stay within the general context.\nMore info: 無 is usually used as a suffix or prefix for Chinese and Japanese words (also old Korean). It can be compared to \"un-\" or \"-less\" in English. It can also mean \"not to have,\" no, none, not, \"to lack,\" or nothingness.\nA lot of people search our website for \"white.\" I am not sure the purpose, unless your family name is white.\n白 is the universal character for white in Chinese, Japanese Kanji, and old Korean Hanja.\nIn certain context, outside of the white definition, it can mean snowy, empty, blank, bright, clear, plain, pure, innocence or gratuitous. In Korean, this can be a family name romanized as Paek or Baeg.\nAccording to Soothill 眞空妙有 means:\nThe true void is the mysteriously existing; truly void, or immaterial, yet transcendentally existing.\n眞空妙有 is the state of being absolutely nonexistent after removing all errant worldly influences. 眞空妙有 is achieved when all forms of existence is seen for their real nature.\nThis is a complex Buddhist concept. Feel free to add to the conversation about this concept here: Asian Forum: Shinku Myou\nThis single character means empty, void, hollow, vacant, vacuum, blank, nonexistent, vacuity, voidness, emptiness, non-existence, immateriality, unreality, the false or illusory nature of all existence, being unreal.\nIn Buddhist context, this relates to the doctrine that all phenomena and the ego have no reality but are composed of a certain number of skandhas or elements, which disintegrate. The void, the sky, space. The universal, the absolute, complete abstraction without relativity. The doctrine further explains that all things are compounds, or unstable organisms, possessing no self-essence, i.e. are dependent, or caused, come into existence only to perish. The underlying reality, the principle of eternal relativity, or non-infinity, i.e. śūnya, permeates all phenomena making possible their evolution.\nFrom Sanskrit and/or Pali, this is the translation to Chinese and Japanese of the title śūnya or śūnyatā.\nIn Japanese, when pronounced as \"ron\" (sounds like \"roan\") this can be a given name. It should be noted that this Kanji has about 5 different possible pronunciations in Japanese: kuu, kara, sora, ron, and uro. 空 is also an element in the Japanese version of the five elements.\nThis in-stock artwork might be what you are looking for, and ships right away...\nThe following table may be helpful for those studying Chinese or Japanese...\n|Title||Characters||Romaji(Romanized Japanese)||Various forms of Romanized Chinese|\n|kokuu / koku||xū kōng / xu1 kong1 / xu kong / xukong||hsü k`ung / hsükung / hsü kung|\nLaw of the Fist Empty Hand\n|空手拳法||kara te ken pou|\nkara te ken po\n|kōng shǒu quán fǎ|\nkong1 shou3 quan2 fa3\nkong shou quan fa\n|k`ung shou ch`üan fa\nkung shou chüan fa\n|Karate-Do||空手道||kara te dou|\nkara te do\n|kōng shǒu dào|\nkong1 shou3 dao4\nkong shou dao\n|k`ung shou tao\nkung shou tao\n|Danketsu Karate-Do||団結空手道||dan ketsu kara te dou|\ndan ketsu kara te do\n|mu||wú / wu2 / wu|\n|White||白||shiro||bái / bai2 / bai||pai|\n|True Emptiness Yields Transcendent Existence||眞空妙有||shin kuu myou u|\nshin ku myo u\n|zhēn kōng miào yǒu|\nzhen1 kong1 miao4 you3\nzhen kong miao you\n|chen k`ung miao yu\nchen kung miao yu\n|空||kuu / kara / sora / ron|\nku / kara / sora / ron\n|kōng / kong1 / kong||k`ung / kung|\n|In some entries above you will see that characters have different versions above and below a line.|\nIn these cases, the characters above the line are Traditional Chinese, while the ones below are Simplified Chinese.\nSuccessful Chinese Character and Japanese Kanji calligraphy searches within the last few hours...\nAll of our calligraphy wall scrolls are handmade.\nWhen the calligrapher finishes creating your artwork, it is taken to my art mounting workshop in Beijing where a wall scroll is made by hand from a combination of silk, rice paper, and wood.\nAfter we create your wall scroll, it takes at least two weeks for air mail delivery from Beijing to you.\nAllow a few weeks for delivery. Rush service speeds it up by a week or two for $10!\nWhen you select your calligraphy, you'll be taken to another page where you can choose various custom options.\nThe wall scroll that Sandy is holding in this picture is a \"large size\"\nsingle-character wall scroll.\nWe also offer custom wall scrolls in small, medium, and an even-larger jumbo size.\nProfessional calligraphers are getting to be hard to find these days.\nInstead of drawing characters by hand, the new generation in China merely type roman letters into their computer keyboards and pick the character that they want from a list that pops up.\nThere is some fear that true Chinese calligraphy may become a lost art in the coming years. Many art institutes in China are now promoting calligraphy programs in hopes of keeping this unique form of art alive.\nEven with the teachings of a top-ranked calligrapher in China, my calligraphy will never be good enough to sell. I will leave that to the experts.\nThe same calligrapher who gave me those lessons also attracted a crowd of thousands and a TV crew as he created characters over 6-feet high. He happens to be ranked as one of the top 100 calligraphers in all of China. He is also one of very few that would actually attempt such a feat.\nCheck out my lists of Japanese Kanji Calligraphy Wall Scrolls and Old Korean Hanja Calligraphy Wall Scrolls.\nSome people may refer to this entry as Empty Kanji, Empty Characters, Empty in Mandarin Chinese, Empty Characters, Empty in Chinese Writing, Empty in Japanese Writing, Empty in Asian Writing, Empty Ideograms, Chinese Empty symbols, Empty Hieroglyphics, Empty Glyphs, Empty in Chinese Letters, Empty Hanzi, Empty in Japanese Kanji, Empty Pictograms, Empty in the Chinese Written-Language, or Empty in the Japanese Written-Language.\n116 people have searched for Empty in Chinese or Japanese in the past year.\nEmpty was last searched for by someone else on Feb 21st, 2018","Science, Tech, Math › Social Sciences How to Prevent Repetitive Stress Injuries to Your Wrist Share Flipboard Email Print Social Sciences Ergonomics Psychology Sociology Archaeology Economics Environment Maritime By Chris Adams Engineering Expert B.I.D, Industrial and Product Design, Auburn University Chris Adams is a human factors engineer who writes about ergonomics and has 11 years of experience in the field. our editorial process Chris Adams Updated February 26, 2018 Repetitive stress on the wrist can lead to a number of different injuries, like tendonitis, bursitis, and carpal tunnel syndrome. They all have similar symptoms, but most include wrist, hand, and arm pain. Although some conditions can have other primary causes, they are all aggravated by wrist overuse. With that in mind, here are the top 10 tips to prevent repetitive stress injuries of the wrist. 01 of 10 Stay Healthy Eugenio Marongiu / Getty Images Maintain a healthy body weight and a good cardiovascular system. An unhealthy body causes stress everywhere. Add that to any environmental stressors and you may have a problem. 02 of 10 Stay Flexible with Forearm and Wrist Stretches Studio CP / Getty Images Keep your wrist, arm, hand, and fingers strong. It is harder to overuse something if it is normally worked hard. Strengthen the muscles involved and increase flexibility through stretching. 03 of 10 Keep Your Hand in a Natural Position Evgeniy Skripnichenko / Getty Images Lay the outer part of your forearm on a hard surface. Let it rotate inward naturally. Keep your wrist straight. That is the natural wrist position. Notice that the palm is at a 30-45 degree angle and that the fingers are curled. Keep that position whenever possible. Flexing and twisting of the wrist cause all the tendons and nerves to rub over leverage points at the joints which can cause a lot of problems. 04 of 10 Set up an Ergonomic Work Station Mint Images / Getty Images Control the movement of your hand and fingers through muscle use, not tendon/ligament use. One big problem with typing on modern keyboards is the lack of strength needed to press a key. This causes you to simply start a motion of the finger and let momentum carry it through. This can cause minor hyperextensions and wear and tear on the tendons and nerves. Musicians are prone to this as well, due to the speeds they need to achieve. Developing strong, fast twitch muscles is a better alternative. 05 of 10 Take Breaks Gpointstudio / Getty Images Take regular breaks to relieve stress. Take this opportunity to stretch and increase blood flow. You should break for at least 10 minutes for every hour of continuous work with 30-second micro-breaks every 10 minutes. Performing a warm up and cool down stretch will help as well. 06 of 10 Change Positions JGI/Tom Grill / Getty Images Change your position and posture regularly. Change of position will call in different muscles, kind of like a relief pitcher, letting the first group rest. 07 of 10 Get a Good Grip Zave Smith / Getty Images Use a proper sized grip for your hand. Look at your natural wrist position again. Now bring your thumb and fingers together until they are separated by the width of two quarters. That is your grip size for holding things. That is your ideal grip for things like handrails or screw guns. Now continue to close your hand until the thumb overlays the first joint of your index finger. That is your grip size for manipulating things with your wrists, things like hammers, shovels or golf clubs. 08 of 10 Maintain Your Distance Hero Images / Getty Images When working with your hands keep them in the middle ground—not too far, but not too close to your body. This allows muscles in your arms, shoulders, and trunk to help share the load. It also keeps your joints in the middle of their range of motion, which increases blood flow and reduces the flex of tendons/ligaments/nerves over those leverage points at the joints. 09 of 10 Don't Go to Extremes Westend61 / Getty Images Do not flex your joints to the edges of your range of motion while working or driving. Most muscles cannot maintain control of the body at these extremes, which can lead to hyperextension and muscle pulls. It also flexes the tendons and nerves over those leverage points of the joints. 10 of 10 The Low Down CentralITAlliance / Getty Images Do not flex upward. The hand is designed to grip, so most muscle control and the joint range is aimed at a downward flex. There's less leverage on an upward flex, so the body has to work harder to move that way. The tendons and nerves also have harder leverage points to stretch over. Keep palms and fingers somewhere between flat and the grip position. Keep your typing and mouse click upstrokes as short as possible. Do not use the scroll wheel as that motion is almost entirely upward flexing."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:4eb995ed-c321-452e-95ac-c2cd6c1147c1>","<urn:uuid:035352f5-6440-49e0-a698-a3b0c84772ed>"],"error":null}
{"question":"How do television and middle-class speakers differ in their influence on language change - which has a stronger impact on how language evolves?","answer":"Based on the evidence, middle-class speakers have a stronger influence on language change than television. While television can contribute some vocabulary and popular expressions (like 'quotative like'), its influence is greatly exaggerated since people don't model their everyday speech after media personalities they don't interact with personally. Instead, most language change starts subtly among middle-class speakers who have strong local community ties while being connected to other groups. These middle-class speakers are most sensitive to language innovation and help spread changes through interpersonal interactions and social networks. The Glasgow study found some limited TV influence on specific consonant features, but emphasizes that media influence isn't a simple transmission to passive viewers - it requires individual speaker engagement and appropriation. Commonplace conversation and interpersonal interaction remain the primary venues for language change.","context":["Is television a contributory factor in accent change in adolescents?\nEconomic and Social Research Council Award R000239757\nPrincipal investigator: Jane Stuart-Smith\nESRC Research Fellow: Claire Timmins\nStatistical Adviser: Gwilym Pryce\nMedia Adviser: Barrie Gunter\nThis project investigates the possible effects of television on pronunciation in adolescents, using methods adapted from sociolinguistics and mass communications research.As such it marks the beginning of systematic research into the relationship between television and language change. The funded period of the project ran from 2002 to 2005. We are now working to interpret the data, carrying out additional anlayses, and disseminate the project findings.\nYou can find an outline of the project and the research questions here. A short summary of the initial findings is given here. (Adobe Acrobat format); press coverage is here . The project reportpresents a 5,000 word summary of the preliminary results and analyses, and was submitted to ESRC in December 2005. More recent results and interpretation can be found in our recent research outputs.\nIn brief, we find:\n• Glaswegian adolescents show medium exposure to television, but particularly enjoy, and engage with London-based dramas, especially EastEnders\n• Our large-scale, multifactorial statistical analysis reveals robust significant correlations between features of consonant pronunciation ([f] [v] for /th/ /dh/ and l-vocalization) and opportunities for contact with speakers of Southern English English, specific social practices, and engaging with the popular television show, EastEnders\n• other features, such as vowel pronunciations, do not appear to show such links\n• Glaswegian adolescents do not seem to be able to imitate Southern English accent features, nor do they seem to want to sound like such speakers\n• Glaswegian adolescents show some short-term effects of television on speech, in that specific features in some speakers were observed to shift after watching only a short television clip\n• Individual speakers show particular patterns of the innovative speech variants which partly reflect patterns of dialect contact and television engagement, and partly their own personality and capacity for taking up innovations, amongst other things.\nThe results themselves, subsequent analyses, and further research, particularly following discussion with colleagues working on media and language (Jannis Androustopoulos ; Werner Holly), lead us to a positive interpretation, i.e. that in this case, television is influencing variation and change in accent. However, our view of media influence on language does not assume a blanket transmission of features to passive speaker-viewers. Rather we are now developing a model of media influence on language which we call ‘linguistic appropriation from the media’ (cf ‘communicative appropriation’ from Holly and colleagues). This requires understanding how speaker-viewers perceive and learn about media speech in conjunction with the socio-cultural process known as ‘appropriation’, i.e. what each speaker/viewer takes for themself whilst engaging with the media, given their own particular experience of the world. Our more recent work, and the bones of the model, may be found in the most recent research outputs. In fact, it emerges that very little is known about how speakers learn about the accents from speech presented in the media, and has led to the initiation of a new research strand into phonological learning from non-interactive mediated speech: AHRC research proposal (Abode Acrobat format) by Jane Stuart-Smith.","What, Like, Makes Language Change?\nLanguage changes for many reasons. When one language comes into contact with another language, change happens. English has been influenced by every language it has ever come into contact with. New inventions and discoveries bring new words into our language. It sometimes brings new meaning for old words, for example, “mouse”. Are there any other ways that language changes?\nYes, of course. That’s what this is all about, language change. Few people will ever have the magic to be 10 years old forever, like Bart Simpson. He’s still saying, “Eat my shorts”, and he’s been doing it for 25 years now. Language changes, and I wonder if the 10 year old Bart Simpson of 2014 is still using the same language as the 10 year old Bart Simpson of 1990. I believe even he has changed his use of language. How? Read on…\nLanguage sows its own seeds of change, and social context offers fertile ground for its growth and spread. Walt Wolfram explains that language changes differently than we may think. It’s not the media; it’s the middle class.\nWilliam C. Friday Distinguished Professor of English\nwalt_wolfram at ncsu dot edu\nTwenty-five years ago, speakers who used like in, “She’s like, “Don’t leave the house!” were largely confined to Southern California and strongly associated with a stereotypical Valley Girl way of speaking. Today, the specialized use of like to introduce a quote (what linguists call the “quotative like”) has spread throughout the English-speaking world. The rapid, expansive spread of “quotative like” among speakers under the age of 40 is truly exceptional. It also raises important questions about the nature of language change.\nThe common myth in American society is that the English language is now following a single path of change under the irrepressible, homogenizing influence of mass media. However, the truth is that language is far too resourceful, and social structure is far too complicated, to follow any single path.\nLanguage changes subtly…women often lead the way\nChange is one of the inevitable facts in the life of any language. The only language not in a perpetual state of flux is a dead language. Language itself provides the seeds of change, and social circumstances provide fertile ground for their growth and spread. Yet the truth about language change may be different from the popular conception.\nPeople often assume that change begins with the upper class, modeling language for other social groups to follow. In fact, most language change starts subtly and unconsciously among middle-class speakers and spreads to other classes — and women often lead the way.\nPressure to change comes both from within language itself and from its role in society. Because language is a highly patterned code for communication, people collectively pressure it to change in ways that preserve its patterning or enhance its communicative efficiency. At the same time, we use language as a social behavior, to solidify or separate different social groups.\nThe Social Context of Change\nSocial context, including the social evaluation of language differences, is as important in language change as the inner workings of language. The reason that oxen has not yet given way to oxes throughout the English language cannot be found within language itself, but in the social sanctions that have been placed on the use of this form by socially dominant groups.\nBy the same token, one can only speculate as to why mouses (as in, “We asked the IT department to order some new mouses”) would become an acceptable plural for a computer device by the same middle-class speakers who resolutely chastise any speaker who might call rodents mouses. Social evaluation of language may seem inconsistent and arbitrary, but this does not lessen its role in language change.\nMost language change actually starts subtly and unconsciously among the lower classes and spreads. Extremes in social strata, for example, the highest and the lowest classes, tend to be marginal to this process.\nInstead, the middle-class groups, who have the strongest loyalty to the local community while being connected to other groups, are the most sensitive to language innovation.\nThe other side of language innovation is resistance\nOn the other side of language innovation lies resistance. Even when certain changes seem natural and reasonable, they are resisted by socially dominant classes who want to avoid being affiliated with subordinate groups that have already adopted these changes.\nThe regularization of irregular past forms such as knowed and growed or the regularization of such as hisself and theirselves, which have made some inroads among vernacular speakers of English, tend to be resolutely resisted by the middle classes despite their linguistic reasonableness.\nHigher-status groups may often suppress natural changes taking place in lower-status groups to maintain their social distinction through language.\nThe acceptance and rejection of language changes are constrained by the social interpretation of those changes and the relationships that exist among social groups.\nThe Spread of Language Change\nLanguage change can spread via several paths. In American society, one prominent pattern of language change is the cascade or hierarchical model, in which change starts in heavily populated metropolitan areas that serve as cultural focal points.\nFrom these areas, the change spreads first to moderately sized cities that fall under the influence of these urban areas, and then to yet smaller cities and communities, affecting the rural areas lastly.\nThe Northern Cities Vowel Shift is following this pattern of diffusion, which is facilitated by the cultural status of metropolitan areas and by the more extended social networks of cities with larger populations.\nOther paths of diffusion in language change show that it is not simply a matter of population dynamics.\nIn Oklahoma, for example, the structure ‘fixin’ to for ‘intend to’ in a sentence such as, They’re fixin’ to move has spread from its rural roots to larger urban areas rather than the converse.\nThis contrahierarhical model of change is explained in terms of cultural identity and trends in population movement. As more non-Southerner transplants move to large cities in Oklahoma, native residents want to assert their Southern identity to distinguish themselves from outsiders.\nBy adopting a language feature associated with rural Southern speech, natives can counteract the influx of the newer transplants through this symbolic use of language.\nA third diffusion pattern follows a wave-like pattern in geographical space. In this instance, the pattern of diffusion is explained primarily in terms of physical distance — the farther the location is from the site of the innovation, the later the change will take place. The merger of the vowels in field and filled or steal and still in some areas of the South illustrates this pattern of contagious diffusion.\nDifferent diffusion patterns are not necessarily exclusive; they may co-exist in the same region depending on the language differences involved. Population dynamics, social structure and the social meaning ascribed to changing structures help us to understand the paths of language change.\nLanguage Change and the Media\nIs the media homogenizing English?\nIt is sometimes assumed that the language of the media is homogenizing English. After all, everyone watches the same television networks, in which a dialectally neutral English has become the norm.\nDoesn’t this common exposure affect language change and the level of dialect differences? It can be quite difficult to assess the precise role of the media in language change, but a couple of observations are appropriate.\nAlthough TV shows have clearly contributed some words to the vocabulary and facilitated the rapid spread of some popular expressions, including perhaps the use of quotative like, media influence is greatly exaggerated because people do not model their everyday speech after media personalities with whom they have no interpersonal interaction.\nIn ordinary, everyday conversation, most people want to talk like their friends and acquaintances.\nCommonplace conversation, interpersonal interaction and social networks are the venues through which language change takes place, not impersonal media characters.\nFurthermore, the current evidence on language change and variation indicates that language diversity is alive and well.\nSome historically isolated dialects are receding due to outside influences, but other dialects are intensifying and accelerating in their rate of language change. If the media were so influential, that wouldn’t happen.\nPart of this trend toward maintaining diversity is due to the fact that language change is inevitable.\nAnd part of it may be due to a renewed sense of place and region that attaches social meaning to some of the language changes taking place in American society.\nThe Nuts & Bolts of Change and the Language System\nLanguages are highly patterned cognitive systems. Within the system of English, irregular noun plurals such as oxen for the plural of ox and sheep for the plural of sheep go against the dominant grain of forming the plural by adding –s or –es, creating pressure to change these plurals to oxes and sheeps. Vernacular dialects throughout the English-speaking world have succumbed to this linguistic tendency even as standard English has withstood this internal pressure.\nAt the same time, the plural of mouse — mouses — would have seemed unthinkable to any standard English speaker a few decades ago, but this regularized plural is now commonly used to refer to the hand-held computing device, as in “We purchased new mouses for all of our computers.”\nOver time and place, language itself will pressure exceptions into conforming with dominant patterns.\nThe mind organizes language in a way that predisposes it to certain types of change.\nThere is also pressure to expand patterns of application. Lexical items (words), for example, tend to extend their meaning to cover new references; grammatical forms tend to become more general in their application. The term holiday, once limited to a religious event, now refers to any day away from work.\nIn a similar way, the shape associated with the nautical vessel submarine was extended to refer to the fast-food sandwich based on the shape of the roll wrapped around the contents.\nThe use of the word like to introduce a quote as in, “He’s like, What are you doing?” simply extends this grammatically versatile word, already used as a noun, verb, adverb, adjective and conjunction, to set off quoted statements.\nThe human mind organizes language and uses it to communicate thought in a way that predisposes it to certain types of change.\nChange within language is also shaped by our ability to produce and perceive language sounds. The sound of th in think and that, for example, is more difficult to produce and to perceive than the t of tea or the d of dip, one of the reasons that th is not nearly as common in the world’s languages as t or d.\nNot surprisingly, throughout the English-speaking world, the th sounds show phonetic variation and change, with pronunciations that range from t or d (tink for think, dat for that) to f or v (baf for bath, brover for brother).\nSimilarly, sequences of consonant sounds such mpt in attempt or sts in tests are much more phonetically complex than a single consonant at the end of a word (top or this), and therefore subject to change over time and geographical space.\nSounds may also change based on their relation to other sounds in the system. Vowels, for example, are primarily differentiated from one another by where the tongue is positioned in the mouth, somewhat like the different sounds created by whistling into bottles filled with various amounts of water.\nA slight shift in the position in one vowel closer to the position of another vowel may make it more difficult to hear the difference between the neighboring vowels, triggering one of two effects.\nIt may create a domino-like effect, a chain shift, among a series of vowels in which other vowels move to preserve phonetic distance between them, or it may cause adjacent vowels to merge and become one sound. Both chain shifts and mergers have played an important role in past and present-day changes in English language vowels.\nOne important chain shift taking place in the vowels of American English, the Northern Cities Vowel shift (in Chicago, Detroit, Cleveland, Syracuse, Rochester and more), involves the sound of coffee shifting so that it is produced more like (though not identical to) the sound of cot.\nThis in turn triggers a shift in the pronunciation of a word like pop so that it is produced more like pap, which, in turn, triggers a shift in the pronunciation of bat so that it sounds a bit more like bet.\nBut the shift doesn’t end there, as the sound of bet moves back, closer to the sound of butt, and the sound of butt moves closer to that of bought, the place vacated by the original shift of the vowel in bought or caught.\nThis subtle and elaborate shift in vowel production is not conscious, but rather the natural outcome of the rotational force of vowels.\nMeanwhile, Southern vowels rotate in a completely different format, resulting in growing divergence in the vowels of Southern and Northern speech in the United States.\nVowels also shift by merging into one sound.\nIn Eastern New England, including Boston, and in most of the Western United States, a shift in vowels of caught and cot and dawn and Don has resulted in their identical production.\nSimilarly, before a nasal sound, the vowel of pin and pen are merged in the South.\nThough there may be different responses to the movement of vowels, it is natural for them to shift their position over time, affecting other vowels. Though often unnoticed, language change is guided by the pressures of the language system – working in tandem with societal divisions – that assign social meaning to these changes.\nSource: PBS – Do You Speak American?\nWalt Wolfram is the William C. Friday Distinguished Professor at North Carolina State University, where he directs the North Carolina Language and Life Project. He has pioneered research on social and ethnic dialects since the 1960s, publishing 16 books and more than 250 articles on language varieties such as African American English, Latino English, Appalachian English, and Southern Vernacular English.\nWolfram is deeply involved in the application of sociolinguistic information and the dissemination of knowledge about dialects to the public.\nIn this connection, he has been involved in the production of TV documentaries, museum exhibits, and other community-based dialect awareness initiatives; he also served as primary linguistic consultant for the Children’s Television Workshop, the producers of Sesame Street. He has served as President of the Linguistic Society of America, the American Dialect Society, and the Southeastern Conference on Linguistics."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:dba8c82f-c3d5-4b13-9048-a5c0c1afeb5b>","<urn:uuid:25a2c9e4-f908-41a3-9da9-ef27d8fc949f>"],"error":null}
{"question":"Could you compare how confirmation bias manifests differently in farm management decisions versus marketing research decisions, providing specific examples of each?","answer":"In farm management, confirmation bias manifests through farmers selectively interpreting agricultural data that fits their preexisting beliefs. For example, a farmer might dismiss the effectiveness of y-drop systems by focusing only on information that suggests poor ROI, or reject cover crops based on a single negative example. In marketing research, confirmation bias appears differently - particularly in qualitative research interpretation. For instance, there was a case where in-house researchers incorrectly concluded pricing was the issue with slow digital product sales because they focused on data supporting their pre-existing belief about pricing, when interviews revealed the real problem was the bundle size being too large. Both contexts show how confirmation bias can lead to poor business decisions, but they manifest in domain-specific ways - through agricultural technology adoption choices in farming, and through market research interpretation in marketing.","context":["As a farmer, you make 100’s, if not 1,000’s, of decisions during any growing season.\nFrom what you’ll plant, to when you choose to market your crop, and everything in between.\nResearch suggests that we aren’t as rational of thinkers as we think we are.\nThat can be especially true in an industry such as farming where so many factors influencing your financial outcome (weather, markets, etc) are outside of your control.\nThis uncertainty can lead to irrational decision-making. Much of this irrationality can be attributed to the effects of cognitive biases.\nThat’s why I’ve decided to profile how this biases can impact farm management decisions.\nLets get started!\n1 - Confirmation Bias\nConfirmation bias is the tendency to process information by looking for, or interpreting, information that is consistent with a person’s existing beliefs.\nThis is the psychological tendency to favor information that confirms our beliefs and to disfavor information that runs counter to them tends to be magnified in the echo chambers and “filter bubbles\" surrounding the internet and social media.\nNever before has it been so easy to access endless sources of information via the internet, credible or not.\nThis desire to find information that confirms with our existing beliefs tends to cause us to lower our credibility/factual hurdle.\nLet’s look at an example….\nLast fall when commodity prices once again began following their cyclical and seasonal downturn, I hopped online to try to do some research on how or why commodity price could follow their seasonal upturn in 2018.\nI read a few articles and some forum posts and stumbled across someone who’d posted a long-term chart of soybean prices. This chart painted a bullish picture.\nI thought to myself, “that sure makes a lot of sense”. This was the confirmation bias at play.\nI had just got done reading a number of articles by respected industry experts but I was (maybe subconsciously) choosing to give the most weight to an anonymous blog post.\nFurthermore, I solidly believe that no one can predict commodity prices with any repeatable “edge”. But here I was “buying into” a damn anonymous blog post.\nI had to give myself a scolding, “get your crap together Nick”!\nLet’s take a look at how it can affect farm decision making.\nConfirmation bias rears its ugly head more often than most of us would like to admit.\nConfirmation bias can bite us in the butt in all sorts of ways.\n- “I don’t want to spend any money on equipment this year” leads to “I really have a hard time believing that these y-drop systems have a positive ROI”\n- “See, I knew XYZ seed was an underperforming (while reading test plot results from a competitor)”\n- “I knew cover crops don’t work in my area (after seeing one picture of a poor corn stand)”\n- The most obvious example that we will not dig into: politics\nHow can we deal with confirmation bias?\n- Conduct your own testing on your farm or in your farm business. The key is tracking both the results and what leads to the results (attribution analysis).\n- It sounds easy to do, but make a point of really analyzing the credibility and the biases of the sources of your information.\n- Understand that sometimes sample sizes in farming are simply too small to get definitive answers. You may be forced to make educated guesses on marketing/agronomy/capital allocation decisions.\nConfirmation bias is a powerful thing that can really lead us astray in our decision making.\nRemember it the next time you find yourself “buying into” data that fits your preconceived idea/values/beliefs while needlessly discounting information that does not.\nOnto the next bias!\n2 - Survivorship Bias\nSurvivorship bias is the logical error of concentrating on the people or things that made it past a selection process and overlooking those that did not.\nSurvivorship bias does a few things…\n- It focuses on those that made it while ignoring those that did not.\n- It hides the effects of randomness\n- It downplays the effects of luck.\nHere’s a great example of survivorship bias. This story should hammer home the impact of neglecting to realize survivorship bias.\nAbraham Wald was a Hungarian mathematician who emigrated to the United States. During World War II, he was a member of the US military’s Statistical Research Group (“SRG”).\nThe SRG was group of very smart people put together at the request of the White House and made up of people who would go on to compete for and win Nobel Prizes. The SRG was an extension of Columbia University, and they dealt mainly with statistical analysis.\nThey were given a project to learn how to best protect US military planes from enemy gunfire.\nThey would look at planes returning from war, and see which areas had the most bullet holes. Clearly, these areas were in need of protection – so the recommendation was to enforce those areas of the planes with armor for future missions.\nMr. Wald saw an obvious flaw in this analysis which had evaded many of his peers and produced an elegant fix.\nHe reasoned that if a plane is returning from combat, it means that it was not shot down by the enemy. In response, the areas that had a higher concentration of bullet holes were least in need of protection, because the planes could survive a strike on those points.\nInstead, Wald concluded: reinforce the areas where returning planes had suffered no damage. Those areas of the planes were most likely to be mission critical, because no returning plane had bullet holes in those areas.\nThis is how survivorship bias works. It’s an incredible analytical tool, used correctly. If you’re a numbers nerd as I am, you can see a detailed analysis of how Wald conducted this analysis here (PDF).\nSurvivorship Bias and Mutual Funds\nOne of the most obvious impacts of survivorship bias that most of us will face at some point in our lives its involvement with stock market returns.\nIn particular, it tends to really skew the results of mutual funds.\nMutual fund performance reporting typically only includes the returns of funds that exist today. Common sense tells us that mutual fund companies don’t typically shut the doors on successful funds and/or managers.\nIn 1996, a study was done to estimate the size of the bias across the U.S. mutual fund industry (source). It calculated a bias of 0.9% per annum, where the bias is defined and measured as: “Bias is defined as average α for surviving funds minus average α for all funds” (Where α is the risk-adjusted return over the S&P 500. This is the standard measure of mutual fund out-performance).\nWhen you’re presented with investment returns, always keep survivorship bias in mind.\nLets now move on to the farm.\nSurvivorship Bias and The Farm\nWhile most of us will agree with the logic behind the survivorship bias, it becomes a bit less clear how it relates directly to your farm and your farm management decisions.\nI’ve deducted a few things to keep in mind when thinking about ways to improve your farm.\n- Learn from your failures. Unlike Wald and the SRG, you often get a chance to actually learn from the attempts that failed. Many decisions you make on the farm are wrong, embrace them. They give you many opportunities to learn from. Why did a lower seeding rate not “win” or why didn’t split-applying N yield a positive net ROI? Learn, learn, learn!\n- Learn from others’ failures. Instead of just looking at those that have built successful farms, you should seek out and document reasons for failure in other farms and industries. Just because you see a couple successful neighbors with a lot of owned land doesn’t mean that there aren’t many more of them who tried and failed to aggressively buy land and it cost them their farms (hypothetical example!).\n- Possibly the most important (in my opinion)….Keep in mind what Wald found out from his experience at the SRG - sometimes you succeed only because your vulnerability hasn’t been attacked. Be ruthless in evaluating what your vulnerabilities are, and focus on addressing them. They’re probably not where the bullet holes are.\nSurvivorship bias is particularily nasty. Keep your eye open for it.\n3 - Groupthink\nGroupthink is a phenomenon that occurs within a group of people that have an often hidden desire for harmony in the group that often results in irrational decision making.\nGroup members often try to subconsciously avoid raising controversial issues or alternative ways to solve a problem, which creates or leads to a lack of creative and independent thought.\nThis often occurs more in a corporate setting but I’ve seen it pop up in farm groups over the years.\nBefore discussing those farm specific examples, lets review possibly the most famous off-farm example of groupthink.\nBay of Pigs Invasion\nMuch research has been done on the topic of groupthink with most if it initially being done by a researcher named Irving Janis. One of Janis’ primary examples of groupthink was the Bay of Pigs invasion.\nThe Bay of pigs plan was crafted by the Eisenhower administration, but was executed by the Kennedy administration. Janis is found that the administration “uncritically accepted” the plan of the CIA. When a few members of government (Arthur Schlesinger and Senator Fulbright), voiced their objections they were largely ignored by other members of the administration.\nThis caused those with doubts, Schlesinger in particular, to minimize his own doubts causing a form of self-censorship (source: Janis’ groupthink paper).\nHere’s a classroom lecture that discusses the Bay of Pigs invasion and groupthink: https://www.youtube.com/watch?v=rHwKc5lzLY4\nAn important lesson -> Bay of Pigs was a disaster that cost many lives and an embarrassment for the US government. Yesterday I mentioned the importance of learning from failure. Just one year later, the US government successfully used lessons from Bay of Pigs in its handling of the Cuban Missile Crisis.\nGroupthink on the Farm\nI’ll share a story with you a first-hand story I experienced with group think.\nI was called into a tough farm financial situation by a lender. They had a farm client who’d accumulated a multi-year stock pile of corn.\nThey had held onto some of their 2012 corn into the inverted market in the summer of 2013. If anyone remembers, as soon as end users rolled their bids forward it was a cash price bloodbath.\nIn many areas, the cash price for corn fell $.70+ in one day and well over $1.00 in a week. Very very hard to sell after that type of a decline.\nSo, they held…..\nFast forward a to the spring of 2014 they were still holding that 2012 crop along with all of their 2013 crop. Their working capital had gotten crushed. Specifically because this area was also faced with by far the worst corn basis it had ever seen due to rail delays/premiums.\nThere was a silver bullet though….this farm had planted almost the whole farm to corn in 2013 and soybean prices where rallying strongly in 2014.\nMy suggestion to a group of people from the bank (and the farmer) was the following.\n- Plant nearly the whole farm back to soybeans and use a combination of futures, forward contracts, and puts to lock in a $1-2 profit on the whole crop (assuming APH yields)\n- Plan to store the corn for another year but hedge it. I had assumed that there was a 75% chance that the farm could pick up a $1.00 gross return on storage (yes, basis was that bad!).\nThe bankers tore my idea apart. “Why would we reward this guy for his past mistakes!”\nTo them, extending his operating note vs. a full-blown restructuring was a “reward”.\nMy idea of a reward was this strategy that offered a realistic chance of adding back a much-needed $100/acre of working capital to his farm.\nAfter the meeting, one of the junior bankers called me and said something similar to “great thoughts Nick, but it’s not going to fly with this group”.\nI admittedly know very little about banking regulation so I’m going to give them the benefit of the doubt and assume that their hands were tied a bit by my suggestion.\nBut I’ve always had a sneaking suspicion that the prime culprit here was groupthink.\nHindsight is 20/20 so I hate to even mention this, but the plan would’ve worked just as I assumed but they sold him out of his corn.\n(Don’t get me wrong here, the farmer “made the bed” but we all need to focus on how we can make the best decision possibly looking forward not backwards.)\nI will also admit that this is an extreme example of groupthink.\nSee below for a much more common occurrence…..\n——> My key lesson when thinking about groupthink on the farm is this. Be extra cautious of groupthink creeping in when dealing with outside opinions on your farm, especially if they have a financial incentive to keep you happy. That person is highly incentivized to not “rock the ship” and can easily just tell you what you want to hear.\nThat’s all for Part 1 in this series on cogntive biases.\nVisit this link for Part 2 in our series on biases on the farm: https://www.harvestprofit.com/blog/13-biases-that-screw-up-farm-management-decisions-part-2\nIn this blog post, we discuss an uncomfortable topic: farm foreclosure and bankruptcy. We'd be pulling wool over our eyes if we didn't face the fact that there are going to be plenty of tough conversations between farmers and bankers in the coming months and years. This blog post provides actionable advice surrounding farm foreclosure, farm restructuring, and farm bankruptcy.Read More »\nGrain marketing is hard! Volatile commodity markets lead to frustration, greed, and indecision. Today's farmer needs to work hard to find a risk management system that allows them to make less emotional, and more profitable, grain marketing decisions.Read More »","“The real voyage of discovery consists not in seeking new landscapes but in having new eyes.” – Marcel Proust\nWhat is Confirmation Bias?\nConfirmation bias is our unintentional tendency to seek or interpret information in ways that confirm what we already think or believe. Confirmation bias is sometimes called my-side bias or confirmatory bias.\nConfirmation biases are common. We all have them.\nIn some ways, confirmation biases are useful because they help us to quickly filter and categorize the information flood we face every day. But confirmation bias can also reinforce misperceptions that lead to poor decision making based on incomplete, misleading or wrong information.\nIn some cases, confirmation bias can be harmless, such as for the runner who believes that eating a bagel for breakfast will help her run better. She will remember speedy post-bagel workouts and forget those slower runs.\nBut confirmation bias can also be harmful or destructive. The man who considers his unemployed brother’s television viewing habits as evidence that all unemployed people are lazy is damaging to their relationships as well as unfair to the unemployed people he encounters.\nAnd confirmation bias can be devastating to consequential decisions. Considering information that supports the idea of a business merger and discounting information that suggests that the merger is a bad idea could lead to a ruinous decision.\nGive This a Try\nIn 1960, Peter Wason, a cognitive psychologist at University College, London, devised a series of experiments in which he asked participants to identify a rule about three numbers.\nGive it a try yourself. The numbers are 2, 4, 6. Now, what do you think the rule is?\nParticipants in the study were allowed to give examples of their idea of the rule, which the researchers would tell them was correct or not.\nWhat numbers did you use to test your theory about the number rule?\n8, 10, 12? Yes, those numbers conform to the rule.\n34, 36, 38? Yes, those numbers also conform to the rule.\nIn fact, most Wason’s experiment participants thought they had the problem solved after just a few examples that “proved” their idea that the rule was even numbers. Very few gave any examples of other numbers to disprove their hypothesis.\nIf your theory is that the rule is even numbers, you would be wrong. The rule is actually ascending numbers. The series 1, 2, 3 would also follow the rule, as would 723, 900, 4,000. If you had asked if 10, 8, 6 followed the rule, the researchers would have told you that no, it did not.\nThis experiment is a classic example that demonstrates how confirmation bias can lead to wrong conclusions.\nConfirmation Bias in Marketing Research\nConfirmation bias can be a problem in interpreting quantitative data but is a particular bias to watch out for in qualitative research. Focus groups, interviews and other forms of qualitative research is interpreted differently than the numbers from a survey. The researcher must filter unstructured data, sort and classify to identify trends and themes. The qualitative researcher must question and verify conclusions, ensuring that their own beliefs are not influencing the research outcome.\nThe fact that everyone is prone to confirmation bias is one reason that qualitative research, in particular, is best conducted by an outside researcher, rather than by an in-house researcher. Being a part of an association or company, familiar with the thinking, preferences and hopes of that organization’s leadership as well as part of the organization’s culture, creates a filter through which the researcher will search for information and interpret information once it is found.\nExamples of Confirmation Bias\nThe Wrong Pricing Conclusion\nI had a client who had conducted in-house research to determine the problem with slow sales of a bundle of digital product downloads. They believed that pricing for the product was too high and, indeed, their survey research using a standard price sensitivity meter confirmed that their target customers thought that the product was too pricey. They adjusted their offer, creating an even more generous bundle of downloads. But they were baffled when rather than solving their sluggish sales problem, sales decreased.\nI conducted interviews with target customers about the product and pricing. Indeed, the problem was not the price of the product, but the large number of downloads included in the price. The target customers said they would never use so many downloads before they expired and so didn’t want to pay for something they wouldn’t use. As I explained it to the client, it would be like offering a terrific sale on milk as long as you purchased 50 gallons. Who would buy 50 gallons of milk since it would go sour before you could drink it all?\nBarbara is obsessively punctual. In fact, it drives her crazy when people are late and this is putting a strain on her marriage because she believes that her husband Mark is always late. “He is always at least five minutes late. Always.”\nBarbara’s therapist asked Barbara to keep a written record for a week of when Mark was supposed to leave, arrive or do something he promised to do. Barbara thought it would be a fun game and provide written proof the next time she confronted Mark about his tardiness.\nShe was surprised when at the end of a week she had recorded 16 situations when Mark was, in fact, on time or even early leaving for work, picking up their daughter from daycare or meeting her at the car repair place to pick her up. He was only late once during the week. Barbara realized that she had been stewing on the few times that Mark was late and not even noticing when he was on time or early.\nGroupthink can contribute to confirmation bias. When everyone on a work team agrees with everyone else, playing follow-the-thought-leader or avoiding conflict by just keeping quiet when you have a conflicting opinion, you’re contributing to others’ confirmation bias.\nI often saw this at the advertising agencies I worked with, particularly when it came to creative campaigns. Account teams would pile on and agree with a direction\nHow to Avoid Confirmation Bias\nAlthough self-confidence is a valuable quality in the workplace, humility and a healthy dose of self-doubt could save you from the pitfalls that confirmation bias can cause. Don’t always be so sure that you are right and the other side is wrong. Think about Wason’s study and try to prove your ideas or your theory are wrong.\nWhen facing a decision, be honest with yourself about your motives to take a particular action. For example, if you’re anxious to jump on a recommended stock buy and see all the signs pointing to the idea that it’s a good investment, indulge in some self-reflecting before making the buy. Is your desire to get some quick returns clouding your ability to judge fairly the advisability of the purchase? This is the time to have a structured analysis plan in place to avoid making bad decisions.\nHere are some questions to ask when it’s time to make a decision on your beliefs.\n- How do I know I am correct?\n- Have I thoroughly looked at sources and information that are contrary to what I believe?\n- Have I evaluated this information with an open mind?\n- How do my personal views influence how I am viewing this information?\n- How would someone with a different view on this issue interpret this information?\n- How can I broaden my pool of knowledge to include diverse points of view?\nAnd finally, listen to the skeptics of your precious ideas. They provide a counterbalance to your ideas on the issue you are facing. Ask them their views and what shaped those views."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:24fcb09a-582e-4349-a18b-726f7a94eb81>","<urn:uuid:3183bbac-ef70-42e2-9e3d-dd11c6f3ec19>"],"error":null}
{"question":"Between washing raw chicken and patting it dry with paper towels, which method is safer for food preparation? 🧐","answer":"Patting raw chicken dry with paper towels is the safer method. Washing chicken is not only unnecessary but also hazardous as it does not remove bacteria and can actually spread bacteria through water droplets splashing onto counters and utensils. Additionally, store-bought chickens are already cleaned before packaging.","context":["Hello! I’m back for Part 2-Rules for Handling Meat and Poultry in the Food Safety Tips series. Part 1 was about handling food properly, so this post is going to get specific about meat and poultry.\nAs I mentioned in Part 1, CommonGround has put together this nice little booklet called “5-Second Rules for Keeping Food Safe” that informs readers about all aspects of food safety. It has information complied from the U.S. Department of Agriculture (USDA’s) Food Safety and Inspection Services (FSIS), the Food Safety Working Group, the Food and Drug Administration (FDA), the Center for Disease Control (CDC) and food industry initiatives.\nDon’t Be Chicken! 5 Rules for Handling Meat and Poultry\nRed, white, dark or light, U.S. meat and poultry is among the safest in the world, but bacteria present in raw meat and poultry can cause foodborne illness if handled incorrectly. Preventing contamination is easy with these rules.\n- RUNNING TO THE STORE? Make the meat counter your final stop before checkout to keep poultry and meat refrigerated longer. Avoid torn or leaky packages and remember to separate poultry and meat from other items in your cart to prevent cross-contamination.\n- TIME TO UNLOAD? Start with poultry and meat. Refrigerate in leak proof containers to prevent juices from dripping onto other food. If you’re not planning to use the food within a day or two, freeze it. Cook or freeze fresh poultry, fish, ground meats and variety meats within two days; other beef, veal, lamb or pork should be cooked or frozen within three to five days.\n- THREE WAYS TO THAW. Notice none of these methods involves the kitchen counter. Because bacteria can multiply rapidly at room temperature, the safest method is overnight in the refrigerator. Cooking immediately? Thaw in cold water or in the microwave. No time to thaw? Meat and poultry can be cooked from frozen. Keep in mind that cooking will take at least 50 percent longer for most items.\n- AN EXCEPTION TO EVERY RULE. Wash everything but the meat. It’s risk and unnecessary. Rinsing meat and poultry with water can even increase your chance of food poisoning by splashing juices (and any bacteria they might contain) onto your sink and counters.\n- YOU CAN’T SEE DONE. Proper cooking is the best way to prevent food poisoning cause by meat. Use a thermometer to ensure that meat and poultry reach the safe recommended temperatures. These are listed below. Don’t eyeball it-one in four hamburgers turn brown before reaching a safe internal temp.\nKnow Before You Buy\n- There’s no need to pay extra for poultry or pork that’s labeled hormone-free. USDA prohibits farmers from using hormones to raise chickens and pigs.\n- Purchasing organic, grass-fed and free-range meats does not make it safer to consume. These labels refer to how the animals are raised, but all raw meat and poultry can contain bacteria that could cause illness.\n- All poultry and meat sold in the United States is inspected for wholesomeness. Look for these seals stamped on packaging.\nIn closing, I’m going to leave you with these two bits of additional information. I hope you learned something from this post and if you have any questions, please let me know!\nEver wonder why antibiotics are given to livestock? Healthy animals provide healthy food. The judicious use of antibiotics helps prevent and control diseases, which reduces the risk of unhealthy animals entering our food supply.\nThe grass is always greener…Nearly all beef cattle, whether raised organically or conventionally, spend the majority of their lives in pastures eating grass.","So, you just found that there is like a pound of raw chicken in the fridge for seven days. Is it still safe to cook and eat it?\nThe USDA recommends that you only store raw chicken in the fridge for two days. This is regardless if it is whole or cut into pieces. For frozen chicken, the timer starts once you fully thaw the meat.\nRead on to learn more about raw chicken in the fridge for seven days. You will know if it is still safe to eat, and if it is, how should you cook it.\nIs It Safe to Eat Raw Chicken in Fridge for 7 Days?\nIt happens to everyone. You bought a lot of chicken, intending to cook it all for the entire week. However, life got in the way, and you forgot about the chicken in the fridge.\nNow, you don’t want to waste it, but on the other hand, you don’t want food poisoning as well. The question now is it still okay to eat?\nIf you ask the U.S. Department of Agriculture and Food and Drug Administration, you should throw out that chicken. It does not matter if the chicken is whole or cut into serving pieces. You should not store it in the fridge for more than two days.\nWhy You Should Not Store Raw Chicken in the Fridge for Two Days and More?\nHere are the things that may happen if you store raw chicken in your fridge for over two days. You should not let the storage in the fridge go beyond that period because of such effects:\nRaw Chicken in Fridge for Three Days\nHave you forgotten that package of raw chicken in your fridge for three days? Then it will be a bit of a coin toss if it is still safe or not.\nBy the end of the second day, pathogens will start to grow on the surface of the chicken already. It is still a bit too early, though, for any bacteria to change the smell and appearance of the chicken.\nThis means that the chicken might not smell or look different. Despite that, there are still harmful bacteria on the surface of the meat.\nHowever, if your fridge is cold enough, the low temperature might have slowed down the growth of bacteria. The chicken should still be good to eat, provided that you cook it thoroughly.\nRaw Chicken in Fridge for Four Days\nWhat will happen if you have stored the raw chicken in the fridge for four days? Then the chances are that the bacteria have already had enough time to grow.\nThe bacteria might still not be able to alter the smell of the meat. However, some slime might be on the surface now that it has stayed too long without freezing.\nIs it still safe to eat? Probably but only if you cook it thoroughly. However, observe if the first bite seems to taste a bit off. In that case, it might be best to throw the rest of it into the trash. Do not even think of feeding it to your dog or cat. They can suffer from food poisoning too.\nRaw Chicken in the Fridge for Seven Days\nIf the chicken has been in the fridge for seven days, it will show all the telltale signs of spoilage. You can most likely tell that the chicken is not safe by this time.\nEven if you have sealed raw chicken in the fridge for seven days, this is true. It will still most likely have spoiled, and no amount of cooking can save you from food poisoning.\nIt is important to label your meats when you bring them home. By doing that, you can keep track of how long it has been sitting in your fridge.\nIs Pork Good After 5 Days in Fridge?\nHow Long Can Cooked Chicken Stay in the Fridge?\nNow, you might be thinking, what about cooked chicken? Can you keep it in the fridge longer? Well, the FDA and USDA say yes. You can keep cooked chicken in the fridge longer than raw, but not by much.\nAccording to the health experts, you can only keep cooked chicken in the fridge for up to four days. After that, it will start to spoil.\nEven if you carefully seal and store cooked chicken, bacteria will still grow on it after a while. Of course, this is unless you put it in the freezer.\nIf you plan to cook a large batch of food, transfer them into individual plastic containers. Place them in the freezer afterward. This will allow the food to last longer. Before serving, heat the chicken on the stove until it hits the recommended serving temperature.\nHow Long Does Chicken Last in the Fridge – Raw and Cooked\nHow to Tell If Chicken Has Gone Bad\nYou still have to check the chicken in your freezer now and then, whether raw or cooked. The reason is that even if it has been chilling in the fridge, there is still a risk of spoilage.\nBacteria can still survive even when you put them in freezing temperatures, but they will not multiply. The average temperature of a household fridge is 40°F (4°C), so harmful bacteria can still thrive.\nIn other words, although the temperature inside your fridge is low, it is not enough to stop bacteria growth. It will only slow it down a little bit. You can only store raw chicken in the fridge for up to 2 days.\nNow, how can you tell if it has indeed gone bad? Here are some ways to identify spoiled chicken, both raw and cooked:\n1. Check the Expiration Date\nLabel Any Meat You Purchase Before Tossing Them Into the Fridge\nIt is essential to label any meat you purchase before tossing them into the fridge. This will tell you exactly how long it has been since you bought it.\nCheck the Dates Printed on the Packaged Chicken\nThe expiration date is the first thing to check, especially in unopened raw chicken in the fridge for seven days (or less). You will notice that most packaged chicken has two dates printed on it. These are the packaged date and best by date.\nThe packaged date was the date when the meatpacking plant processed it. This date is for the retailers who need to keep track of their inventory.\nCook the Chicken on or Before the “Best By” Date\nThe date that should concern you is the “best by” date. This is the latest date that the manufacturer can guarantee the freshness of the meat. Cook the chicken on or before the best by date to ensure its peak quality.\nPut the Chicken in the Freezer\nIf you plan to cook the chicken immediately or the following day, buy chickens close to their expiration date. You’ll be getting a great deal if you do. On the other hand, if you won’t cook the chicken immediately, put it in the freezer instead of the fridge.\nAlso, during your grocery trips, make it a point to get the meats at the end. This will somehow minimize the time the meats stay in the “danger zone.” Once you get home, stick the meats into the freezer immediately.\nMeat Stored in the Freezer Can Last for More Than Nine Months\nIf kept in the freezer, chicken and other meats can last for more than nine months. Just make sure that they come sealed in an airtight plastic bag or container.\n2. Check the Raw and Cooked Chicken’s Appearance\nYou can tell if chicken, raw or cooked, has gone bad just by looking at it. If you can see the visual cues of spoilage, the chicken has already gone way past its expiration date.\nHere are some guidelines based on whether you have raw or cooked chicken:\nFresh chicken should be slightly pink with a couple of white fatty pieces. It is still good if the fat pieces have gone yellow, but the meat is still pink. There are times when a corn-fed chicken gets yellow fat.\nWhat if the fat pieces are yellow and the meat is gray? Then you can be sure that you already have spoiled chicken. If you can see these signs, you do not need to check for the others. Throw the chicken into the trash immediately.\nOn the other hand, a slight discoloration is not enough to tell if the chicken has gone bad. A bit of darkening or fading of the color is perfectly fine. It is just the result of the oxidization of the red protein in the blood.\nNote that a bit of darkening or lightening of the meat is not a sign of spoilage. It just means that the chicken is bad and not fresh anymore.\nAnother visual sign of spoilage is mold growth. This is even if you see that the other parts of the chicken do not have any discoloration.\nThe fact that mold has grown on one part is proof enough that the remaining chicken is no longer edible. Meats are not like cheese. You cannot just cut off the parts with mold.\nAgain, can raw chicken last seven days in the fridge? The USDA and the U.S. Food and Drug Administration do not recommend storing raw chicken in the fridge for more than one to two days.\nThe longest you should be keeping leftover cooked chicken in the fridge is three days. Also, you should put the leftover chicken into the fridge as soon as you can. The reason is that it can quickly spoil if left at room temperature for a couple of hours.\nThe danger zone for cooked chicken is anywhere between 40°F (4°C) to 140°F (60°C). At this temperature, bacteria can multiply exponentially. This can significantly increase the risks of food poisoning.\nWhat if the cooked chicken has been in the fridge for two days or less but still has significant discoloration? Then it is also an indication it has already spoiled. This goes the same if you notice mold growth on the dish.\nIt can be quite difficult to spot mold on cooked chicken, especially if there is a sauce and other condiments. This makes it necessary to check it carefully.\nMeanwhile, if the chicken is still good, reheat it in the microwave to an internal temperature of 165°F (74°C). This will ensure the obliteration of any bacteria in the chicken.\n3. Presence of Odd Smell\nAnother telltale sign that chicken has gone bad is if it has a foul smell, pun not intended. If you take a whiff of the chicken and smell anything at all, it might already be spoiled.\nFresh chicken has a mild smell, if anything at all. If you can easily smell something off in the chicken, you should trust your nose and throw it out. It is already way past its expiration date if you get a sour or sulfur-like smell from your raw chicken.\nHowever, if the smell is quite faint and you cannot figure it out, look for other signs of spoilage. People’s sense of smell varies.\nThis means not everyone can notice the faint smells of spoilage, especially in cooked chicken. Have someone else take a whiff of the chicken and get their opinion.\n4. Check Its Texture\nIf you cannot find any visual or olfactory signs of spoilage, you can touch the raw chicken. You can do that to figure out if it is still good.\nFresh chicken has a smooth and soft texture. If the chicken feels slimy or sticky, take that as a clear sign that it has already started to go bad. The slimier the chicken, the more spoiled it has gotten.\nOn the other hand, cooked chicken is firmer and drier than raw chicken. However, if you notice that the chicken has gotten softer than usual, it may have already gone bad. This is true if it has developed a slimy and sticky film on the surface.\nHow Long Can You Keep Raw Meat on the Fridge?\nShould You Wash Raw Chicken?\nSome people think that they need to wash chicken before cooking. However, not only is it unnecessary but also more hazardous.\nHere are three reasons why you should not wash chicken, or any meat for that matter, before cooking:\n1. Washing Does Not Get Rid of the Bacteria\nRinsing raw chicken under running water does nothing to remove the bacteria on the surface. Yes, the water will wash off some bacteria, but most of them will still stay on the meat. The only way to get rid of the harmful bacteria is to cook the chicken to the recommended temperature.\n2. May Spread the Bacteria Even More\nWhen you rinse chicken under the tap, you cannot prevent the water droplets from splashing about. These droplets now have harmful bacteria, thanks to being in contact with the raw chicken. They may land on your other utensils and all over your kitchen counter.\n3. No Need\nThe chickens that you can buy from the supermarket were already cleaned and washed before getting there. In addition, you can find the preparation instructions printed on the packaging of the chickens. Nowhere in there are you instructed to wash the chicken before cooking.\nSo, how do you prepare the chicken? According to the experts, it is perfectly fine to pat the chicken dry using paper towels. Removing the excess juices from the poultry will also lessen the number of bacteria.\nFrequently Asked Questions (FAQs)\nWhat Kinds of Bacteria Are Present in Raw Chicken?\nRaw and undercooked poultry usually contains Campylobacter, which is harmful by itself. However, raw chicken typically has more than one bacteria strain.\nSometimes, it can contain Salmonella, Clostridium perfringens, and Yersinia. There are also times when it contains E. coli. All these bacteria strains can cause serious food poisoning.\nWhat Temperature Should You Cook Chicken?\nThe only way to make chicken safe to consume is to cook it completely. You should not cook chicken rare or even medium. Cook all chicken cuts entirely through. The meat should have an internal temperature of at least 165°F (74°C).\nIf you don’t want your chicken to get too dry, use a meat thermometer. Insert it at the thickest part of the bird. This is to make sure that the internal temperature is just right. Do not just rely on outward appearance or if the chicken’s juices run clear or not.\nWhy Can’t You Eat Rare-Cooked Chicken?\nUnlike beef, the bacteria in chicken meat can penetrate through the surface. You can sear the outside of a steak and leave the inside still blue. On the other hand, you should never eat chicken with pink meat inside.\nHowever, some experts claim that free-range chickens don’t have as many bacteria as those raised in cages. Still, to be safe, you should always cook your chicken until well done.\nConclusion – Raw Chicken in Fridge for 7 Days\nAccording to the USDA, you can only store raw chicken in the fridge for two days. If the chicken were frozen prior, the timer would start as soon as you thaw it.\nYou can only store the raw chicken for a couple of days in the fridge. It can last considerably longer when placed in the freezer.\nIf prepared and sealed properly, you can store raw chicken in the freezer for nine months. If you plan to save the chicken for cooking later, place it in the freezer instead of the fridge."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:6d406936-bab6-4e53-a5eb-646f22ba5494>","<urn:uuid:3317506f-ce96-401d-8c99-7eb57b22cac3>"],"error":null}
{"question":"What role did written physicality play in manuscript traditions, and what conservation techniques were developed to preserve these physical artifacts?","answer":"Written physicality was particularly significant in medieval Icelandic manuscript tradition, where scribes were especially aware of their books' physical nature and celebrated it through marginalia and annotations. This physicality was seen as crucial to transitioning from oral to written culture, with manuscripts serving as lasting physical artifacts rather than just prompts for oral performance. For preservation, different manuscript traditions developed specific conservation techniques. For instance, palm-leaf manuscripts required a detailed conservation process including fumigation, careful cleaning with warm water, repairs using special adhesives, and oiling with cedarwood oil to maintain flexibility and protect against insects. These conservation methods were designed to maintain both the physical integrity and readability of the manuscripts.","context":["By Christine M. Schott\nThesis, University of Iceland, 2010\nAbstract: This project investigates what paratextual material—specifically marginalia—can tell us about the way medieval Icelandic readers felt about their books, and how they participated in the creation of the reading experience for future readers through the marks they left on the page. This branch of Material Philology is shedding light on reading and literary practices across medieval Europe, but within the realm of Icelandic literature much still remains to be uncovered.\nThis thesis discusses and provides a transcription of the marginalia in three particular medieval Icelandic manuscripts, focusing first and foremost on a little-noted Jónsbók manuscript: Rask 72a. This book contains a fairly extensive collection of comments by the scribe on his environment and equipment, all written into the margins. I argue that the scribe was led to record such comments in the margins because of the specifically written (and therefore specifically physical) nature of the Jónsbók law code, which is quite different from the sagas or eddas that had at least some roots in the oral history tradition.\nAs a supplement to this discussion of the Jónsbók manuscript, I also examine AM 604 4to (a manuscript of rímur) and AM 433a 12mo (Margrétar saga). This secondary investigation provides a broader basis for the discussion of Icelandic book culture. I argue, for instance, that the extensive recording of proverbs in AM 604 indicates a certain awareness of the manuscript as an archival force—a lasting physical artifact instead of simply a record to prompt oral performance in reading. The presence of such collections of deliberately-formulated marginalia in all three manuscripts indicates a certain consonance of attitude toward these three very different kinds of books: on some level conscious or unconscious, the scribes were aware of the physical, enduring nature of their material as much as they were of the value of the text, and at the same time they participated in the creation of future reading experiences by inscribing themselves on the page.\nIn many ways the story of medieval Icelandic book culture is the story of book culture all over medieval Europe: with the coming of Christianity came the introduction of the codex and the slow but steady process of transitioning from an oral to a written culture. What sets Iceland apart, however, is the surprisingly high volume of manuscript production in comparison to population size, and perhaps more interestingly the unique flowering of vernacular literature that was never subjected to a Latin hegemony. Without falling into romantic notions of a remote island charting its own course in opposition to the literary life of the Continent, we can still say that Icelandic manuscripts were bound up in the unique culture of Iceland with its strong oral history. Yet, possibly because of the continued knowledge of oral culture, Icelandic scribes seem to have been especially sensitive to their books’ very written physicality, a physicality they both celebrated and utilized. They seem, in fact, to have their closest parallels not with the contemporary book culture of fourteenth-century mainland Europe but with that of ninth and tenth-century Anglo-Saxon England, which was similar to Iceland in many ways. However, where England experienced a series of marked breaks in its culture over the centuries, Iceland was fortunate enough to enjoy great continuity over more than a millennium, and the value of its manuscript treasures, though experiencing its own ups and downs, continued to play a role in politics and society even through the twentieth century.","CAMBODIAN PALM-LEAF MANUSCRIPTS:\nPROCEDURES FOR CONSERVATION\nThe manuscripts have been largely stabilized by casing and boxing, with an insect repellant inserted in the box. The manuscripts that have been microfilmed are relatively dust-free. Most of the manuscripts appear to be written by incision, and have a strong image. Some are insect-damaged and broken, particularly at the ends.\nThe following procedures are designed to clean soiled surfaces, strengthen the image, support damaged areas, impart flexibility, and reduce further insect damage.\n1. Fumigation. It is important that, as each group of manuscripts is set aside for conservation treatment, they be removed from their boxes and placed in an airtight container for fumigation. To ensure that each cased manuscript is returned to the correct box, the cases and first leaves of each manuscript should be marked in soft pencil with the box number.\nThe container (a plastic garbage container with lid, for example) should have an appropriate quantity of para-dichlorobenzene covering the base. The manuscripts in their cases may then be stood on end, supported above the level of the para-dichlorobenzene by bricks or stones, but not jammed in too tightly. The lid should be sealed, and the manuscripts left for no less than one week.\nBy employing more than one \"fumigator,\" it should be possible to arrive at a workable schedule, coordinating fumigation exposure to conservation treatment time.\n2. Cleaning. The manuscripts should be removed from the fumigator and the protective cases one at a time. The leaves should be numbered discreetly in the lower right corner with a soft pencil, and the covers and strings carefully removed.\nAll the leaves should be lightly dusted with a soft dry brush to remove dust and insect parts. If manuscript fragments are loose, they should be set aside in a polyester envelope bearing the leaf number for future attachment.\nAfter the bench surface has been cleaned, a sheet of polyester film is laid onto it, and a stable container of warm (preferably distilled) water and a lint-free cloth (preferably cheese cloth) prepared. The manuscript should be checked to ensure that the writing is incised and not surface-written (a magnifying glass and a raking light are helpful).\nThe cloth is dampened, and a small area tested to ascertain the stability of the image. If stable, the leaf is carefully cleaned on both sides, and placed between sheets of blotting paper to dry. The polyester film is wiped, and the next leaf cleaned, until all have been treated.\n3. Repair and Support. If the manuscript is damaged (by insects or splits, etc.) it must be repaired at this point. A stable adhesive that does not readily mold is a mixture of methylcellulose and Elvace 1864 polyvinylacetate. This is made as follows:\nA piece of Tengujo tissue is torn or water-cut to cover the damaged area, and laid onto a small square of polyester film. The tissue is carefully pasted, the tissue laid into place on the palm leaf, and gently rubbed down through the film with a bone folder. The film is removed, and the repair allowed to dry.\nIf a significant portion of the palm leaf is missing, a piece of Kitakata paper is carefully torn or water-cut to match the missing area, and tipped into place. It is then supported on both sides by the tissue in the manner noted above. Loose fragments are similarly secured in place in this fashion.\nAn effective variation of this repair technique is the use of tissue coated with a dry fish gelatin solution. With this technique, the tissue is laid dry over the dampened surface of the damaged palm leaf, and pressed into place with a damp cloth until firmly set. The advantage of this method is that repairs dry quickly, adhesive does not have to be prepared, and adhesion is very strong.\n4. Oiling. The oiling of the palm leaf imparts a smooth surface and slight increase in flexibility. If the oil is insect-repellant, the leaf has added protection.\nThe leaf is laid onto polyester film or glass, and a thin coat of cedarwood oil applied to the entire surface on both sides of the leaf. It will be noted that tissue repairs are not released by the oil, but they blend further into the leaf surface.\nEach leaf should be air-dried in a constant air current to avoid molding. A useful method is to secure the leaves to a line by a peg or spring clip.\nWhen dry, the surface should be lightly polished with a soft dry cloth, the manuscript restrung, recased, and reboxed. It is important that the case and box be clean and the insect repellant in place before reshelving."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:ae78b448-ecad-4d47-b1ff-d33b1b42fad2>","<urn:uuid:b17b58ba-d9dd-41fa-b74c-7dd23da1e98e>"],"error":null}
{"question":"What are the strict operational standards in Bocuse's kitchen versus the complex operational challenges facing utility companies in the EV charging infrastructure?","answer":"Bocuse's kitchen operated with meticulous standards including daily uniform inspections, strict grooming rules (no long hair permitted), and a precise, ordered way of doing everything with no deviations allowed. In comparison, utility companies face multiple operational challenges in EV charging infrastructure, including managing complex billing across multiple stakeholders (charging hosts, operators, service providers), handling various customer types and locations, implementing different pricing structures, and dealing with limited charging networks, underdeveloped fast-charge technology, and unpredictable customer experiences.","context":["Irish chef Kevin Thornton has given an insight into life working under Paul Bocuse, telling The Caterer “He was my Escoffier”.\nThe renowned chef, who gave his name to the Bocuse d’Or culinary competition, died on Saturday at the age of 91.\nThornton had a two-month paid position at Paul Bocuse’s restaurant L’Auberge du Pont de Collonges in Lyon, France in 1987. He arrived following stints in restaurants in Europe and Canada but Bocuse’s “kitchen proved to be the most inspirational and influential training” he would experience.\n“Paul Bocuse’s encouragement of me as a chef hugely strengthened my confidence and self-belief,” he said.\n“His kitchen was run with mesmerizing precision and the efficiency of the operation was military like. There was an ordered way of doing things and no deviation was tolerated. Coppers hung gleaming and proud over the huge stove as they have done in every kitchen I have ran since then. I have no doubt that my appetite for precision and attention to detail was indulged so much at Restaurant Paul Bocuse that it copper-fastened my approach to every aspect of cooking from then on.\n“My first few days there I was ordered to clean the many fridges, which every new chef was tasked with. Within a short while I was promoted to the pastry then to the fish section and finally to the most coveted corner of the kitchen – sauce. I knew I had passed the test of discipline and showed my hunger to achieve. He would say “Superb, petit Irlandais” (which translates to superb, little Irish) when I plated something to his satisfaction and for one who did not praise often these were big words.\n“I have a few stand out memories. Daily inspections of staff uniforms took place and long hair was not permitted under any circumstances. I had a little ponytail that I kept hidden under my chef’s jacket. One day while bending to take a plate from the oven the head chef noticed the ponytail. All hell broke loose! It was like sacrilege; “what would the papers say if they found out a chef at Restaurant Paul Bocuse had long hair? What would Monsieur Paul say?” he exclaimed. I said it was a religious thing, part of my Buddhism, anything to fend of the summons to cut it off, but nothing worked – if I wanted to stay the ponytail had to go.\n“I had a decision to make and, for me, it was a very difficult one. I decided to sleep on it. The next morning I was late leaving my bedsit and I rushed to work bringing my nail scissors with me as I still hadn’t decided what to do. When I got to the door of the kitchen the head chef was waiting for me, arms folded pointing at my head to turn around. I took out the scissors and cut off the ponytail in front of him and handed it to him. That was a big statement of respect for me to make and one that I never regretted.\n“Another day there was an English-speaking guest in the restaurant that none of the staff could understand. I was summoned by Paul Bocuse – “Viens ici, petit irlandais” (come here, little Irish) and duly translated the praise from the guests. He was very happy with their remarks. The next evening I was invited to eat at the chef’s table with Monsieur Paul and the rest of the senior team and that again was a privilege as it didn’t happen often.\n“This photo was taken on my final day at Restaurant Paul Bocuse. He told me I had “mains speciales” (special hands) and he smiled and posed for the camera.\n“I met him again a few years later when I represented Ireland at the Bocuse d’Or competition and he remembered me. A few years after that I was representing Ireland at a Panel of Chefs event in South Africa which he attended also though I was unaware he noticed me at all. Back at my hotel that night I received a note under my door from one of his team inviting me to attend a breakfast with him the following morning.\n“For me, Paul Bocuse was my Escoffier and I feel privileged to have spent time in his kitchen. May he rest in peace.”\nKevin Thornton, former two-Michelin-starred chef-patron of Dublin’s famed Thornton’s Restaurant, is a lecturer, author, radio presenter and TV personality. He currently runs a Kevin Thornton’s Kooks with wife Muriel, which allows guests to watch demonstrations, partake in cookery classes and go on food outings with the duo to forage for ingredients. The team cam also be hired for private events.\nVideos from The Caterer archives","Utilities in the age of electric vehicles\nUtilities in the age of electric vehicles\nUtilities in the age of electric vehicles\nThe impact of vehicles on the environment has driven regulatory mandates to adopt a more sustainable way of commuting. As a result, electric vehicles (EVs), and the necessary infrastructure to operate them, has changed the automobile and utility industries over the past decade.\nElectric vehicles are powered by a charged battery pack and can be separated into two categories:\n- Battery Electric Vehicles (BEVs): These EVs are purely electric with lithium ion batteries suitable for short to medium distances.\n- Plug-In Hybrid Electric Vehicles (PHEVs): Electric vehicles with an internal combustion engine (ICE) with support from a small electric motor.\nWhy Electric Vehicles?\nThe 2015 Paris Agreement has challenged countries to reduce their carbon emissions to “net zero” over the coming years. This international treaty has prompted governments around the world to phase out gas and diesel powered vehicles, shifting instead to EVs:\nSales of electric vehicles have grown steadily over the last decade. The following chart from the International Energy Agency shows China leading market share at 47%. Twenty other countries have reached a market share of above 1%: emissions. As a segment, the automobile industry can tout sustainability and the environmental benefits of emerging technologies to entice consumers to buy EVs.\nAccording to a study by IRENA (International Renewable Energy Agency) on EVs:\n- Electric passenger cars will reach 200 million by 2030\n- Electric two-wheeled and three-wheeled vehicles could outnumber four-wheeled vehicles, with as many as 900 million on roads by 2030\n- Electric buses and light-duty vehicles could surpass 10 million by 2030\nFactors Contributing to EV Adoption\n1. Consumer Interest:\nEco-friendly consumers who want to decrease their carbon footprint prefer to buy EVs. Transportation around the globe is one of the biggest contributors of carbon emissions. As a segment, the automobile industry can tout sustainability and the environmental benefits of emerging technologies to entice consumers to buy EVs. A 2019 international electric vehicle consumer survey (of 7,600 consumers in seven regions) shows consumer interest in electric vehicles is high. 50% of consumers say they’re interested in owning an EV and 28% say they’ll purchase one as their next vehicle.\nConsumer benefits to owning electric vehicles:\n- Reduced operating costs, lower charging prices and simpler maintenance\n- Quieter driving experience\n- Exemption in Clean Air Zones – areas that charge fees to vehicles that pollute the environment\n- Government subsidies that make EVs cheaper than ICE vehicles\n- Preferential parking permits in dense urban areas\nEV technology has vastly improved. Range limitations and charging times have been addressed, alleviating concerns and increasing purchase momentum. Consider these three, top selling EV models in the world in 2020:\n3. NetZero Target:\nIn support of the 2015 Paris Agreement, utility and automobile companies are working to achieve net-zero emissions. To do this, they’re offering customers low-carbon products such as renewable electricity and electric vehicles. By taking advantage of these offerings, individual consumers can reduce their carbon footprint. Businesses can reduce their overall cost of fleet ownership, and organisations can reduce fuel costs, reap tax benefits and take advantage of government incentives.\nChallenges for Utilities:\nEVs help combat climate change. However, barriers to adoption exist:\nCharging Pricing: An increase in the number of electric vehicles can lead to disorganised charging. This makes peak shaving difficult, creates incremental costs for generators, increases transmission and distribution pressures and reduces grid reliability and security. It also degrades power quality and increases the harmonics of the grid. Ultimately, an unreasonable pricing structure can lead to its failure. A dynamic pricing strategy can help utilities overcome the challenges of EVs and can reduce the burden of power on a grid.\nComplex Billing: EVs also present billing challenges for utilities:\n- Number of Stakeholders: Charging hosts, charging point operators, eMobility service providers, roaming network providers, etc. are all involved in the billing process. These stakeholders have to manage multiple plans – pre-paid, postpaid, ad hoc, group plans, etc.\n- Customer Type and Charging Location: Plans offered will vary based on customer type such as individual, fleet, business, public and private. They’ll also vary based on location, including home, office, fleet charging center, parking lot, multi-tenant unit, municipal location and more.\n- Price Per Charge: The customer can be charged based on charge point, price per kWh or by minute/ hour (flat fee). The charging session may include ancillary fees such as a connection fee or a waiting fee for staying connected after reaching a full charge.\nCharging Infrastructure: The mechanics of charging pose challenges to utility companies:\n- Network: Availability remains limited\n- Technology: Fast-charge still in its initial stage and widely unavailable in the network\n- Customer Experience: Unpredictable charging experience negatively affects customer opinion\nService and Maintenance: Electric vehicles require specialised mechanics who are still difficult to find. According to a study done by UK’s Institute of the Motor Industry (IMI), 97% of today’s mechanics aren’t qualified to work on electric vehicles. Of the 3% of mechanics who do qualify, many work directly for EV dealerships, limiting service options for general EV buyers.\nHigher Upfront Investment: Higher manufacturing costs vs. the cost to make a combustion engine vehicle make EVs more expensive to buy. This sticker shock feeds consumer doubt about the long-term economic benefits of an electric vehicle. Government subsidisations help alleviate that doubt, but total consumer buy-in will take time.\nThe Utility Opportunity\nAs more people switch to electric cars, the impact of EV charging loads on generation, transmission and distribution networks translates into more energy and more revenue opportunities for utility companies.\n- Charging Infrastructure: Utilities can play a vital role in modulating charging rates and shifting charging times to provide grid services that support supply and demand. Consider these energy giants already investing in charging infrastructure:\n- Shell recently announced the rollout of 500,000 electric charging stations over the next four years.1\n- Ecotricity a “Big Six” UK energy supplier, partners with Moto, RoadChef and Welcome Break to offer 45-minute fast-charge stations. They call the network “The Electric Highway.”2\n- In the UK, companies like Centrica are building out their EV charging capabilities by acquiring smaller independents. Centrica invested in Driivz, a software company that manages EV fleets and charging networks, to create Centrica Electric Vehicle Services (CEVS) 3\n- New EV Tariffs: Consumer tariff structures (e.g. time-of-use tariffs) reward consumers who slow-charge during off-peak hours. These tariffs, which reduce consumer bills and prevent overloads on the grid, help influence EV drivers to shift their charging behavior. By partnering with EV manufacturers, utility companies can create custom electricity tariffs that can be bundled into the purchase of an electric vehicles. Energy suppliers in the US, UK and other European countries have already begun offering EV energy tariffs.\n- Improved Customer Experience: Careful planning, phased execution and synergy with non-utility businesses can help electric utilities facilitate a smooth transition to EV adoption. With the right customer relationship management (CRM) platform in place, utilities can offer consumers “charging ecosystems” – chargers, charging plans, etc. - for their vehicle. This positive consumer experience, combined with the financial upside of aligning with non-utility companies, translates into increased revenue for the utility company.\n- Vehicle-to-Grid (V2G): While EV tariffs can prevent overloads by shifting charging behavior, they also present challenges. If too many EV drivers charge during off-peak times, it can spike load levels and lead to grid congestion. To counter this, vehicle-to-grid (V2G) enables energy to be pushed back to the power grid from the battery of an electric vehicle. This helps balance the variations in energy production and consumption. Furthermore, V2G can support the integration of renewable energy resources into the grid.\n- AI Driven EV Marketing: With electric vehicle purchases on the rise, utilities must position themselves at the forefront of energy innovation to ensure brand credibility. AI-driven marketing helps identify crucial digital touchpoints for targeted messaging.\n- Data Advantages: Utilities can use data analytics and data science tools to develop services, applications and hyper-personalised product offerings. They can also leverage the data to expand into non-core markets. This allows for:\n- Joint offerings with automotive companies\n- After sales services in conjunction with car dealerships\n- Installation of charging stations at locations where customers park electric vehicles for more than an hour\nGlobally the uptake of electric vehicles is leading to the transportation and electricity sectors becoming increasingly connected.\nEven though barriers to EV adoption exist they are phasing out due to technological advancements. Innovation is the key in identifying opportunities to minimise costs and reducing pain points.\nFor utilities the biggest challenges is to ensure grid reliability and resilience. Providing a successful infrastructure for EV adoption will require coordination among various parties - Vehicle & charging manufactures, Electricity service providers, Distribution network operators, and Regulatory authorities\nIn the long run - Utilities that invest in electric vehicle infrastructure and technology will be well-equipped to offer solutions that benefit future customers.\n2Ecotricity and Nissan install UK electric-car-charging network | Guardian sustainable business | The Guardian\nEXL Utilities Academy"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:de2a13d6-f55f-4557-9b4e-bfd359c0e9e7>","<urn:uuid:dc7f0a0b-46cd-416c-b618-7a830371a8fd>"],"error":null}
{"question":"What is the key difference between evaluating limits using a calculator versus using trigonometric identities in terms of accuracy and reliability?","answer":"Using a calculator for limits can only provide approximations and should be considered a last resort, as explicitly stated that 'your calculator can only be used to approximate limits' and may not be acceptable on free-response questions. In contrast, using trigonometric identities provides exact values and is more reliable, as demonstrated by the comprehensive set of identities including Pythagorean, negative angle, and addition formulas that give precise mathematical relationships. For example, when evaluating limits involving trigonometric functions, using the identities ensures mathematical rigor while calculator approximations may only get close to the true value.","context":["Master AP Calculus AB & BC\nPart II. AP CALCULUS AB & BC REVIEW\nCHAPTER 3. Limits and Continuity\nTECHNOLOGY: EVALUATING LIMITS WITH A GRAPHING CALCULATOR\nYou can sum up the whole concept of limits in one statement: A limit is a height toward which a function is heading at a certain x value. With this in mind, a graphing calculator greatly simplifies the limit process. Many times, the function you are given is bizarre looking, and its graph is beyond the grasp of mere mortal men and women in the time allotted to answer an AP question. The majority of these limit questions appear on the non-calculator portion of the AP test, forcing you to use the substitution, factoring, and conjugate methods to reach an answer. However, evaluating limit problems will sometimes seep into the calculator-active section like a viscous, sticky goo. In these cases, limits are no match for you at all, as the calculator affords you numerous tools in your dual quests for a 5 on the AP test and peace in the universe.\nALERT! Remember that your calculator can only be used to approximate limits. Using the calculator to approximate a limit may not be acceptable on a free-response question. This method is only your last resort!\nExample 12: Evaluate using your calculator.\nSolution: You solved this problem in an earlier exercise using the conjugate method (which works just fine) and got —1/6. Graph it on your calculator—it looks almost like a straight line (but it’s definitely not linear).\nUse the [2nd] → [Trace] → “value” command on your calculator to find the value of the function at x = 9. The corresponding y value should come out blank! This makes sense, because 9 is not in the domain of the function. Therefore, substitution does not work. However, we can use the calculator to substitute a number very close to 9. This value is a good approximation of the limit for which you are looking. Again, use the [2nd] → [Trace] → “value” function of your calculator to evaluate the function at x = 8.9999.\nThe calculator gives a limit of x = —.1666671. Using other x values even closer to 9 (e.g., 8.99999999999), the approximation looks more like —.16666666667, which is approximately —1/6, the exact value we received from using the conjugate method.\nExample 13: Show that using your calculator.\nSolution: Using the rule for rational functions at infinity, the limit is clearly 3/5 as the degrees of the numerator and denominator are the same. The graph certainly appears to approach that height as x approaches both ∞ and —∞.\nTo verify this numerically, press [Trace] and repeatedly press the right arrow key. The function’s height will slowly get closer to 3/5 = .6.\nAnother option open to you is the [2nd] → graph (or [Table]) function of the calculator, which lists function values quickly.\nClearly, the function approaches a limiting height of .6 as x approaches ∞.\nDirections: Solve each of the following problems. Decide which is the best of the choices given and indicate your responses in the book.\n1. If exists, what must be true?\n2. Given find the value of b that ensures p is nonremovably discontinuous only once on (—∞,∞).\n3. What conditions must be met if f(x) is continuous at x = c?\n5. Design two functions, g(x) and h(x), such that g(x) + h(x) = 4x2 + 3x + 1 and\n6. What three function behaviors prevent a limit from existing?\n7. What value of a makes k continuous if\n9. If d(x) is defined by the graph below, answer the following questions:\n(f) List all x values where d is discontinuous.\n(g) Which of your answers to part (f) represents removable discontinuities, and why?\n(h) What value(s) of β make the following statement true?\n11. Give the equations of the horizontal and vertical asymptotes of if a and b are real numbers.\n13. The population, v, of the bacteria Makeyoucoughus hurtyourthroatus is modeled by the equation y = 50e.1013663t, where t is days and y is the number of colonies of bacteria. Use the Intermediate Value Theorem to verify that the bacteria will reach a population of 100 colonies on the time interval [4,7].\n14. If f(x) and g(x) are defined by the graphs below, evaluate the following limits (if possible):\n15. Find the values of c and d that make m (x) continuous if\n16. James’ Diabolical Challenge Problem:\nGiven j(2) = 1 and j(x) is everywhere continuous, find A and B.\nANSWERS AND EXPLANATIONS\n1. The three conditions for a limit to exist are: (1) exists, (2) exists, and (3) they are equal.\n2. The denominator of p factors to (2x — 3)(x + 5), so p will be discontinuous at x = 3/2 and —5. If the numerator contains one of these factors, our goal will be achieved. It makes good sense to force (x + 5) to be a factor, rather than (2x — 3) because the leading coefficient of the numerator is 1. (In other words, the numerator will factor into (x + A)(x + B) and (x + 5) is of this form.) To find b, we factor the numerator using (x + 5) as one of the factors:\nSubtract x2 from each side and factor to get\nThus, 5+^1 (the coefficient of x on the left side of the equation) must equal 4 (the coefficient of x on the right side of the equation).\nTherefore, (x — 1) is the remaining factor. Thus,\nBecause (x + 5) is a factor of the numerator and denominator, exists, and the discontinuity there is removable. The discontinuity at x = 3/2 has no limit because which indicates a vertical asymptote (essential discontinuity).\n3. If f(x) is continuous at x = c, then (1) exists, (2) f(c) exists, and (3) the two are equal.\n4. Multiply the binomials in the numerator to get Because this is a rational function evaluated at infinity, you can use the shortcut method of examining their degrees. Because the degree of the denominator exceeds the degree of the numerator, the function’s limit at infinity is 0. Don’t be confused because x→ —∞. Remember that a rational function approaches the same limits as x→∞ and —∞.\n5. Because is a rational function and we know that the leading coefficients of g(x) and h(x) must be in the ratio 3:1. One possible g(x) is 3x2 + 2x + 1. The matching h(x) would have to be x2 + x. Notice that the sum of the two functions is 4x2 + 3x + 1, as directed, and There are numerous possible answers, but they will work in essentially the same fashion.\n6. If a function oscillates infinitely, increases or decreases without bound, or has right- and left-hand limits that are unequal, the function will not possess a limit there.\n7. Special Limit Rule 4 gives us that Thus, and the first condition of continuity is met. Notice that k(0) = α, and (for k to be continuous) a must equal that limit as x approaches 0. Therefore, α = 0.\n8. Use Special Limit Rule 1 to simplify the fraction as follows:\nThis is a rational function being evaluated at infinity with equal degrees in the numerator and denominator, so the limit is equal to\n9. (a) 0\n(b) -1: Remember the key is that you are approaching —1 from the right, not to the right.\n(d) 2: even though f(2) = —1, the function leads up to a height of 2 when x = 2\n(f) x = —3, —2,2,4\n(g) x = 2: If you redefine d such that d(2) = 2 (instead of —1), then the limit and the function value are equal to 2 and d is continuous there. No other discontinuities can be eliminated by redefining a finite number of points.\n(h) β = —3,∞: d increases without bound as you approach 3 from the left and the right; the graph also increases without bound as x approaches ∞.\n10. You don’t have to use special limit rules here—substitution is possible. The answer is sin 2/2, or .455.\n11. Factor the fraction fully to get\nThe denominator of g will equal 0 when However, the numerator will not equal zero simultaneously. Thus, vertical asymptotes are present, and the equations for the vertical asymptotes are x = —b/3a and x = b/2a. To find the horizontal asymptotes, note that the numerator and denominator have the same degree, and find the limit at infinity.\nFrom this, you know that the horizontal asymptote is y = 1/2.\n12. It’s the revenge of the conjugate method! Multiply the numerator and denominator of the fraction by the conjugate of the expression.\nBecause the degrees of the numerator and denominator are both 1 (since √x2 = x), you find the limit by taking the coefficients of the terms of that degree and ignoring the rest of the problem (just as you’ve done in the past with rational limits at infinity).\n13. The population at t = 4 is approximately 75, and the population at t = 7 is approximately 101.655. The Intermediate Value Theorem guarantees the existence of a c ∈ [4,7] such that f(c) = 100.\n14. (a) No limit: does not exist, so f(x) — 3 g(x) cannot have a limit.\n15. The graph will begin as a semicircle of radius 2 centered at (—3,0), will become a line (since cx + d is linear) between the x values of —1 and 3, and will end as the graph of √x shifted to the right 3 and up 4. To find the first point on the linear section, plug —1 into the first rule: The line will begin at point (—1,0). The line will end at The focus of the problem, then, is to find the equation of the line that passes through (—1,0) and (3,4). Find the slope of the line and use point-slope form to get\nTherefore, the correct c and d values are both 1. The solution is further justified by the graph of m.\n16. To begin, factor the function and use the fact that j(2) = 1.\nSince the function is continuous, must be equal to f(4). So, you get f(4) = 4 — 1 = B (according to the given information); B = 3.\nSUMMING IT UP\n• Remember that the limit of a graph at x = c is the height that the graph reaches at x = c.\n• Limits answer the question “Where is a function heading?”\n• Only point discontinuities are removable, because a limit exists.\n• Rational, polynomial, radical, exponential trigonometric, and logarithmic functions are always continuous at all points in their domain. To help remember this, use the mnemonic device\n• Red Parrots continuously Repeat Everything They Learn.","The Back Bencher’s Tip to learn the Trigonometric Identities\nAll right! So, now you are planning to study the monster called trigonometry! I am sure majority of you would agree with me on this that trigonometry is a hard nut to crack! The concepts involved in it are not very hard but what makes it all the more challenging is that it is over flooded with formulae.\nYou learn first five formulae, reach the sixth and …. you can’t recall the first one! If this is what you are experiencing, then you are one of the numerous students suffering from the trigonometry phobia! Well, you should not be cursing yourself for this because, after all you are human with limited memory and you cannot expect your brain to be a memory card which stores everything permanently!\nDon’t worry my friend, we bring you some of the interesting ways of learning these formulae and befriending the monster called trigonometry! Get ready to explore the interesting world of trigonometry!\nThere are certain basic concepts which you will have to learn (and we can’t really help you in that!) like periodicity of functions and Pythagorean identity. Now, let us begin our tour:\nWe begin with the basic trigonometric functions i.e. sin, cos and tan.\nFor remembering this (although, it’s too simple), write the functions in this order\nsin cos tan\nOH AH OA\ncosec sec cot\nSo, just remember oh ah oa…. or rather you can learn it as SOH CAH TOA which says sine means OH (opposite/hypotenuse), cos means AH (adjacent side/hypotenuse), tan for OA (opposite side/adjacent side).\n- Now, once you know these six basic formulae, next important point is the sign of trigonometric functions. So here comes the technique to remember the signs of trigonometric functions in various quadrants.\nWe know that there are four quadrants. In the first quadrant, all trigonometric functions are positive. In the second quadrant, sine and cosec are positive and all remaining functions are negative. Similarly only tangent and cotangent are positive in the third quadrant while sec and cosec are positive in the fourth quadrant.\nStudents often get confused in memorizing this. This should be better remembered as After School To College or Add Sugar To Tea. The initials stand for the functions which are positive in the respective quadrants. Once you learn it this way, you are sure to remember it throughout your life without getting confused.\nNext is the core of trigonometry i.e. the trigonometric identities. Frankly speaking, there is no substitute for learning the trigonometric identities. One of the options is that you can derive them using the Euler’s formula. But that is possible only if you have sufficient time. Demoivre’s theorem is also useful in this context but that also takes time. Hence, while the sad part is that one has to learn the identities as there is no other alternative, the good part is that we are here to simplify this process for you!\nConsider the figure given below. This figure is called the hexagon of trigonometric identities. Some of the basic identities can be remembered with the help of this hexagon.\ncosec x = 1/sin x\nsec x = 1/cos x\ncot x = 1/tan x\nNext, we can also derive the standard identities reading down any triangle in clockwise direction i.e.\nsin2x + cos2x = 1\n1 + cot2x = cosec2x\ntan2x + 1 = sec2x\nHence, these basic concepts can be easily remembered with the help of this hexagon.\n- We now move on to the sine and cosine laws in triangle:\nIn any triangle ABC having sides a, b and c, we have\n1.The sine law\nsin A / a = sin B / b = sin C / c\nSo, here you can just remember that sine of an angle divided by same side is equal to the rest of the angles divided by the corresponding sides.\n2 The cosine laws\na 2 = b 2 + c 2 – 2 bc cos A\nb 2 = a 2 + c 2 – 2 ac cos B\nc 2 = a 2 + b 2 – 2 a b cos C\nWe take the first one. You can learn this as one side a = (b-c)2 and the 2bc term has cosine of the angle corresponding to the main side i.e. a.\n- Sum and Difference Formulae:\n1. sin (A ± B) = sin A cos B ± cos A sin B\nFirst of all, remember that the formula of sin (A ± B) contains both the functions sin and cos. Now, to memorize the formula, learn “SC student studies CS (computer science)”, where SC→CS imply sin cos and then cos sin. The sign between the terms remains the same i.e. the formula for sin (A + B) will contain a positive sign in between terms and that of sin (A – B) will contain a negative sign.\n2. cos (A ± B) = cos A cos B ∓ sin A sin B\nThis formula of cos contains both the trigonometric functions in pairs. You can learn it as Caste Conflict is a form of Social Sin i.e. CC→SS. Now this statement is in negative sign and hence the sign would be negative i.e. while computing cos (A + B), the sign between the terms is negative and vice-versa.\nOnce you remember this, you can easily derive the remaining formulae.\n- Multiple Angle Formulae:\n1. sin 2A = 2 sin A cos A\nRemember that both the sides contain 2. Further you can learn it as SSC exam i.e. S→SC, which implies that sine of an angle = twice sine and cosine of half of that angle.\n2. cos 2A = cos2A – sin 2A\n= 1 – 2sin2A\n= 2 cos2A -1\nIn this case, there are three formulae for cosine of 2A. But, one should try to learn just one as others can be derived from it using the relationship of sine and cosine of angles.\n3. sin 3A = 3 sin A – 4sin3A\nIn this again, remember that every term contains 3. Secondly, all the terms contain the same trigonometric ratio i.e. sine.\n4. cos 3A = 4cos3A – 3 cos A\nThis term is completely based on the previous formula. By just interchanging both the terms in the previous case and replacing sine by cos we obtain this formula.\n- Some general tips to help you learn the identities:\n1. Remember that sin x = (eix – e-ix)/2i and cos x = (eix + e-ix)/2 and use the exponent rules to derive the identities.\n2. Usually cos carries a positive sign while sine carries a negative sign.\n3. One cannot escape from learning the formulae of reciprocal functions, periodicity of functions and Pythagorean identities.\n4. Instead of attempting to cram the identities, try to establish relations between terms so as to minimize the chances of forgetting the formulae.\n5. Learn one of the sum identities and the remaining sum identities can be easily derived using the facts that cos x is an even function and sin x is an odd function.\n6. It is crucial to have your basics clear as they can help you in reaching the formulae in case you forget or get confused.\n7. Always remember that all the trigonometric functions are periodic i.e. they repeat after a specific interval. This concept also proves useful.\n8. Once you remember the sum and difference identities for sine and cos i.e. the formulae of sin (a ± b) and cos (a ± b), the multiple angle formulae can be derived from these identities by putting b = a, we get the formula for cos 2a.\n9. Practicing numerous questions based on application of these formulae can help you in mugging them up without putting in extra effort.\nWe have listed all the important trigonometric identities here so that students don’t miss out any:\n· Pythagorean Identities\nsin 2X + cos 2X = 1\n1 + tan 2X = sec 2X\n1 + cot 2X = csc 2X\n· Negative Angle Identities\nsin (-X) = – sin X, odd function\ncsc (-X) = – csc X, odd function\ncos (-X) = cos X, even function\nsec (-X) = sec X, even function\ntan (-X) = – tan X, odd function\ncot (-X) = – cot X, odd function\n· Cofunctions Identities\nsin (π /2 – X) = cos X\ncos (π /2 – X) = sin X\ntan (π /2 – X) = cot X\ncot (π/2 – X) = tan X\nsec (π /2 – X) = csc X\ncsc (π /2 – X) = sec X\n· Addition Formulas\ncos (X + Y) = cos X cos Y – sin X sin Y\ncos (X – Y) = cos X cos Y + sin X sin Y\nsin (X + Y) = sin X cos Y + cos X sin Y\nsin (X – Y) = sin X cosY – cos X sin Y\ntan (X + Y) = [ tan X + tan Y ] / [ 1 – tan X tan Y]\ntan (X – Y) = [ tan X – tan Y ] / [ 1 + tan X tan Y]\ncot (X + Y) = [ cot X cot Y – 1 ] / [ cot X + cot Y]\ncot (X – Y) = [ cot X cot Y + 1 ] / [ cot Y – cot X]\n· Sum to Product Formulas\ncos X + cos Y = 2cos[(X + Y)/ 2] cos[(X – Y)/ 2]\nsin X + sin Y = 2sin[(X + Y)/ 2] cos[(X – Y)/ 2]\n· Difference to Product Formulas\ncos X – cos Y = – 2sin[(X + Y) / 2] sin[(X – Y) / 2]\nsin X – sin Y = 2cos[(X + Y) / 2] sin[(X – Y) / 2]\n· Product to Sum/Difference Formulas\ncos X cos Y = (1/2) [cos (X – Y) + cos (X + Y)]\nsin X cos Y = (1/2) [sin (X + Y) + sin (X – Y)]\ncos X sin Y = (1/2) [sin (X + Y) – sin[ (X – Y)]\nsin X sin Y = (1/2) [cos (X – Y) – cos (X + Y)]\n· Difference of Squares Formulas\nsin 2X – sin 2Y = sin (X + Y) sin (X – Y)\ncos 2X – cos 2Y = – sin (X + Y) sin (X – Y)\ncos 2X – sin 2Y = cos (X + Y) cos (X – Y)\n· Double Angle Formulas\nsin (2X) = 2 sin X cos X\ncos (2X) = 1 – 2sin 2X = 2cos 2X – 1\ntan (2X) = 2tan X/[1 – tan 2X]\n· Multiple Angle Formulas\nsin (3X) = 3sin X – 4sin 3X\ncos (3X) = 4cos 3X – 3cos X\nsin (4X) = 4sin X cos X – 8sin 3X cos X\ncos (4X) = 8cos 4X – 8cos 2X + 1\n· Half Angle Formulas\nsin (X/2) = ±√[(1 – cos X)/2]\ncos (X/2) = ±√[(1 + cos X)/2]\ntan (X/2) = ±√[(1 – cos X)/(1 + cos X)]\n= sin X/(1 + cos X)\n= (1 – cos X)/sin X\n· Power Reducing Formulas\nsin 2X = 1/2 – (1/2) cos (2X))\ncos 2X = 1/2 + (1/2) cos (2X))\nsin 3X = (3/4) sin X – (1/4) sin (3X)\ncos 3X = (3/4) cos X + (1/4) cos (3X)\nsin 4X = (3/8) – (1/2)cos (2X) + (1/8)cos (4X)\ncos 4X = (3/8) + (1/2)cos (2X) + (1/8)cos (4X)\nsin 5X = (5/8)sin X – (5/16)sin (3X) + (1/16)sin (5X)\ncos 5X = (5/8)cos X + (5/16)cos (3X) + (1/16)cos (5X)\nsin 6X = 5/16 – (15/32)cos (2X) + (6/32)cos (4X) – (1/32)cos (6X)\ncos 6X = 5/16 + (15/32)cos (2X) + (6/32)cos (4X) + (1/32)cos (6X)\n· Trigonometric Functions Periodicity\nsin (X + 2π) = sin X, period 2π\ncos (X + 2π) = cos X, period 2π\nsec (X + 2π) = sec X, period 2π\ncsc (X + 2π) = csc X, period 2π\ntan (X + π) = tan X, period π\ncot (X + π) = cot X, period π"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:5f54c15b-02af-4fc1-95ed-421ab2e636d4>","<urn:uuid:7e49d248-af3a-41be-8606-b1de5f23f660>"],"error":null}
{"question":"Do rock cavies live longer in the wild than raccoons?","answer":"Rock cavies have a 91% probability of living to 3 years in the wild, and can live up to 11 years in captivity. Raccoons can live up to 12 years in the wild, which is longer than rock cavies' typical wild lifespan.","context":["Cavia species. The tail is absent or vestigial, adult weight is around 1000 g, and the ears are short. Adult length is between 200 to 400 mm and is occasionally longer. Rock cavies have whitish throats, while other upper parts are generally grayish, and the stomach is yellowish brown. The rostrum is longer than other caviids and the incisor to premolar diastema is proportionally greater. Unlike other caviids, has blunted nails on all digits excepting a small grooming claw on the outermost digit of the foot, and the hands and feet are additionally padded with a leather-like surface. Other than size differences, day-old highly precocial young appear very similar to adults. (Arends and McNab, 2001; Eisenberg and Redford, 1999; Lacher, 1981; Nowak, 1999; Roberts, et al., 1984)is a hystricomorph rodent very similar in appearance to\nMale rock cavies defend hollows and dens made by the surrounding granitic boulders. Females seek refuge in these dens and a system of resource-defense polygyny emerges as a result. Mates are easy to find, therefore, a well established dominance hierarchy exists among males. Dominant males defend prized rock piles with large harems and are able to mate more frequently than subordinates. During mating, multiple males encircle one female in order to prevent her escape. Often these males include the dominant male and his progeny, as all others are treated aggressively. After encircling, the dominant male mounts the female to copulate. (Kleiman, et al., 1979; Lacher, 1981; Roberts, et al., 1984; Tasse, 1986)\nRock cavies breed year round and give birth to 1 to 3 highly precocial young per pregnancy, although singleton births are most common. They have a gestation period of 76 days which is notably longer than other caviids. Infants are weaned at 35 days, and juveniles reach sexual maturity around 133 days. Small litter size, comparatively long gestation period, and other reproductive parameters (i.e. low birth mass: maternal mass ratio) are considered specialist adaptations to rigid maternal energy constraints and a highly unpredictable environment. (Lacher, 1981; Oliviera, et al., 2006; Roberts, et al., 1984)\nRock cavies exhibit well developed parental care. Both sexes engage in grooming, huddling, and indirect social behaviors which rear the young until independence. Weaning occurs at 35 days, yet young have been observed foraging and eating vegetation as early as 2 days after birth. Early consumption of whole food suggests that suckling behavior may involve socialization as well as nourishment. After weaning, juveniles live in their fathers' den. Males born to the dominant male are not treated aggressively by their father. Living among his harem allows for increased socialization and a potential chance to become dominant in the future. Females born to dominant males live in the harem as potential mates, yet aggression from other females, including the mother, is common. Young females may leave the den as a result. (Lacher, 1981; Tasse, 1986; Lacher, 1981; Tasse, 1986; Lacher, 1981; Tasse, 1986)\nRock cavies are relatively long lived for inhabiting such an unpredictable environment. In captivity, individuals live as many as 11 years. In the wild, the probability of an individual living to 3 years of age is 91%. (Kleiman, et al., 1979; Kleiman, et al., 1979)\nRock cavies are highly social and exhibit many behaviors attributed to habitat specialization. Both males and females adhere to social hierarchies dictated by agonistic interactions. Dominant individuals are better able to “win” these interactions. Subordinate individuals remain in their role until new members arrive at the colony. Agonistic interactions between females are most frequent and female hierarchies are highly linear. Pregnant females are most aggressive, especially toward juveniles, many of which are fatally wounded by female aggression. Male hierarchies are not linear but there is a social hierarchy among males. Social grooming is common among males, and the dominant male is most often groomed. Intersexual grooming and female grooming are infrequent. A number of other behaviors, including face nuzzling and crawling over one another, are used to enhance and maintain social relationships. Most activity outside rock dens involves foraging in trees and is crepuscular. (Eisenberg and Redford, 1999; Lacher, 1981; Tasse, 1986)\nNo information was found.\nRock cavies produce a variety of vocalizations, many of which are thought to represent anxiety or fear. Five distinct vocalizations are described, yet few are thought to directly communicate information to other individuals. Although scent marking is common among other caviids, rock cavies are not known to regularly scent mark. Rather, posturing and tactile interactions allow rock cavies to communicate, as individuals are often in close proximity around rock pile colonies. Social rank is continually communicated through allogrooming and agonistic interactions which include head thrusts and chases. (Lacher, 1981)\nAlthough known as a habitat specialist, rock cavies are generalist folivores where diet is concerned. In their unpredictable Caatinga environment, drought often leaves trees leafless. During dry periods, vegetation continues to grow in the rock piles where rock cavies live, allowing colonies to live through periods of low productivity. During periods of high productivity, rock cavies feed on leaves, buds, flowers, and bark, most often foraging from the ground or on tree branches. In captivity, rock cavies eat a variety of fruits and vegetables. (Lacher, 1981; Tasse, 1986; Willig and Lacher, 1991)\nUsing sound or smell, rock cavies are able to detect predators approaching from a distance. The confines of surrounding rock hollows provide predator protection, and \"alarm whistle\" vocalizations are used to alert the colony to take refuge when needed. Once the repeated high pitch whistle is heard, surrounding individuals echo the call. When predators are seen approaching, individuals flee into surrounding rocks. This decreases the chance of any one individual indiscriminately running toward the approaching predator. The predator avoidance strategy employed by rock cavies is presumably a specialist adaptation to their rocky habitat which limits field of view and visual predator detection. (Lacher, 1981)\nRock cavies are the only mammal endemic to the Caatinga region of Brazil. Because of their specializations towards living in such an unpredictable environment, few other mammals continually interact with rock cavies. Dry periods drive out many other mammals, and no specific predators are mentioned in the literature. Presumably, however, local avian and mammalian carnivores prey on rock cavies. As generalist folivores, rock cavies impact a variety of local flora through consumption of leaves, bark, and flowers. Parasitic nematodes are also found in the large intestines of rock cavies. (Lacher, 1981; Rodrigues, et al., 1985; Willig and Lacher, 1991)\nHumans native to northeastern Brazil regularly hunt rock cavies for meat. Efforts have even been made to domesticate them as a reliable food source in areas afflicted by drought and poverty. Although not common, rock cavies can be kept as pets as well. Medicinal uses of (Alves, et al., 2008; Oliviera, et al., 2006)include rubbing rock cavy fat under the eyes to ease \"tired eyes\" and mixing rock cavy manure with coffee to treat effusion.\nThere are no known adverse affects ofon humans.\nAlthough habitat destruction and hunting have threatened rock cavies in the past, (Catzeflis, et al., 2008)is currently listed as \"least concern\". Numerous protected areas currently provide refuge from hunting and habitat loss.\nAlex White (author), University of Michigan-Ann Arbor, Phil Myers (editor, instructor), Museum of Zoology, University of Michigan-Ann Arbor, Tanya Dewey (editor), Animal Diversity Web.\nliving in the southern part of the New World. In other words, Central and South America.\nuses sound to communicate\nhaving coloration that serves a protective function for the animal, usually used to refer to animals with colors that warn predators of their toxicity. For example: animals with bright red or yellow coloration are often toxic or distasteful.\nhaving body symmetry such that the animal can be divided in one plane into two mirror-image halves. Animals with bilateral symmetry have dorsal and ventral sides, as well as anterior and posterior ends. Synapomorphy of the Bilateria.\nuses smells or other chemicals to communicate\nused loosely to describe any group of organisms living together or in close proximity to each other - for example nesting shorebirds that live in large colonies. More specifically refers to a group of organisms in which members act as specialized subunits (a continuous, modular society) - as in clonal organisms.\nan animal that mainly eats the dung of other animals\nactive at dawn and dusk\nranking system or pecking order among members of a long-term social group, where dominance status affects access to resources or mates\na substance used for the diagnosis, cure, mitigation, treatment, or prevention of disease\nanimals that use metabolically generated heat to regulate body temperature independently of ambient temperature. Endothermy is a synapomorphy of the Mammalia, although it may have arisen in a (now extinct) synapsid ancestor; the fossil record does not distinguish these possibilities. Convergent in birds.\nparental care is carried out by females\nunion of egg and spermatozoan\nan animal that mainly eats leaves.\nA substance that provides both nutrients and energy to a living thing.\nAn animal that eats mainly plants or parts of plants.\noffspring are produced in more than one group (litters, clutches, etc.) and across multiple seasons (or other periods hospitable to reproduction). Iteroparous animals must, by definition, survive over multiple seasons (or periodic condition changes).\nparental care is carried out by males\nhaving the capacity to move from one place to another.\nthe area in which the animal is naturally found, the region in which it is endemic.\nthe business of buying and selling animals for people to keep in their homes as pets.\nhaving more than one female as a mate at one time\nscrub forests develop in areas that experience dry seasons.\nremains in the same area\nreproduction that includes combining the genetic contribution of two individuals, a male and a female\nassociates with others of its species; forms social groups.\nuses touch to communicate\nLiving on the ground.\ndefends an area within the home range, occupied by a single animals or group of animals of the same species and held through overt defense, display, or advertisement\nthe region of the earth that surrounds the equator, from 23.5 degrees north to 23.5 degrees south.\nA terrestrial biome. Savannas are grasslands with scattered individual trees that do not form a closed canopy. Extensive savannas are found in parts of subtropical and tropical Africa and South America, and in Australia.\nA grassland with scattered trees or scattered clumps of trees, a type of community intermediate between grassland and forest. See also Tropical savanna and grassland biome.\nA terrestrial biome found in temperate latitudes (>23.5° N or S latitude). Vegetation is made up mostly of grasses, the height and species diversity of which depend largely on the amount of moisture available. Fire and grazing are important in the long-term maintenance of grasslands.\nuses sight to communicate\nreproduction in which fertilization and development take place within the female body and the developing embryo derives nourishment from the female.\nbreeding takes place throughout the year\nyoung are relatively well-developed when born\nAlves, R., H. Lima, M. Tavares, W. Souto, R. Barboza, A. Vasconcellos. 2008. Animal-based remedies as complementary medicines in Santa Cruz do Capibaribe, Brazil. BMC Complementary and Alternative Medicine, 8: 1-9.\nArends, A., B. McNab. 2001. The comparative energetics of ‘caviomorph’ rodents. Comparative Biochemistry and Physiology - Part A: Molecular and Integrative Physiology, 130: 105-122.\nCatzeflis, F., J. Patton, A. Percequillo, C. Bonvicino, M. Weksler. 2008. \"Kerodon rupestris\" (On-line). 2008 IUCN Red List of Threatened Species. Accessed April 05, 2009 at http://www.iucnredlist.org/details/10988.\nEisenberg, J., K. Redford. 1999. Mammals of the Neotropics. Chicago, IL: The University of Chicago.\nKleiman, D., J. Eisenberg, E. Maliniak. 1979. Reproductive Parameters and Productivity of Caviomorph Rodents. Pp. 173-183 in J Eisenberg, ed. Vertebrate Ecology in the Northern Neotropics. Washington D.C.: Smithsonian Institution Press.\nLacher, T. 1979. Rates of growth in Kerodon rupestris and an assessment of its potential as a dometicated food source. Papeis Avulsos de Zoologia, Mus Zoo Univ Sao Paulo, 33: 67-76.\nLacher, T. 1981. The comparitive social behavior of Kerodon rupestris and Galea spixii and the evolution of behavior in the Caviidae. Bulletin of Carnegie Museum of Natural History, 17: 1-71.\nNowak, R. 1999. Walker's Mammals of the World. Baltimore: The Johns Hopkins University Press.\nOliviera, M., A. Carter, M. Bonatelli, C. Ambrosio, M. Miglino. 2006. Placentation in the Rock Cavy, Kerodon rupestris (Wied). Placenta, 27: 87-97.\nRoberts, M., E. Maliniak, M. Deal. 1984. The reproductive biology of the rock cavy, Kerodon rupestris, in captivity: A study of reproductive adaptation in a trophic specialist. Mammalia, 48/2: 253-265.\nRodrigues, H., J. Vicente, D. Gomes. 1985. Stronglyoides ferrierai New Species Nematoda Rhabdiasoidea from the rodent Kerodon rupestris in Brazil. Memorias do Instituto Oswaldo Cruz, 80: 407-410.\nTasse, J. 1986. Maternal and Parental Care in the Rock Cavy, Kerdon rupestris, a South American Hystricomorph Rodent. Zoo Biology, 5: 27-43.\nWillig, M., T. Lacher. 1991. Food Selection of a Tropical Mammalian Folivore in Relation to Leaf-Nutrient Content. Journal of Mammology, 72: 314-321.","Orlando Wildlife Trappers Information\nRACCOON: Procyon lotor\nSIZE - Average 12-20 lbs. in Florida\nLIFE EXPECTANCY - Up to 12 years in the wild\nBREEDING - Mating in December, 3-5 young born in April\nDIET - Omnivorous - meat, berries, grains, vegetation\nHABITAT - Mixed forests and especially urban areas\nFACTS - 40 teeth, nimble hands, black mask, very strong\nEASTERN GRAY SQUIRREL: Sciurus carolinensis\nSIZE - About 1 lb for adults\nLIFE EXPECTANCY - Up to 10 years, but usually shorter\nBREEDING - Two litters per year - Mating in December and June, 44 day gestation, 3-4 young born, weaned in 10 weeks\nDIET - mostly nuts and seeds\nHABITAT - Forests and suburban areas\nFACTS - 22 teeth, bushy tail, very agile\nOPOSSUM: Didelphis virginiana\nSIZE - Adults 10-14 lbs.\nLIFE EXPECTANCY - no more than 2-3 years\nBREEDING - Marsupial - mates in January, young are born very tiny after only 12 days, then they stay in the pouch and grow\nDIET - Omnivorous. Will eat almost anything but prefers meat.\nHABITAT - Often nomadic, but will use human buildings\nFACTS - 50 teeth, prehensile tail, oposable thumbs, plays dead\nNINE BANDED ARMADILLO: Dasypus novemcinctus\nSIZE - Adults 12-16 lbs.\nLIFE EXPECTANCY - Up to 15 years\nBREEDING - Mates in summer, delayed fertilization in November, 120 day gestation, gives birth to four identical quadruplets\nDIET - Digs up worms and grubs from the ground\nHABITAT - Southern fields and forests\nFACTS - Has bony, leathery shell. Can carry leprosy\nROOF RAT: Rattus rattus\nSIZE - Adults 6-10 oz. Body 8 in. tail 8 in.\nLIFE EXPECTANCY - Rarely more than one year.\nBREEDING - Females can mate year-round, produce up to 5 litters per year, composed of 6-10 young. Gestation is 21-28 days.\nDIET - Almost anything, prefers fruits and grains\nHABITAT - Lives above ground, prefers buildings\nFACTS - Nocturnal, great climbers, small home range\nEASTERN MOLE: Scalopus aquaticus\nSIZE - Adults average 3 oz. and 6 inches.\nLIFE EXPECTANCY - 2-3 years\nBREEDING - Breeds in January, gives birth to 2-4 young after a 45 day gestation.\nDIET - Almost entirely earthworms, also grubs\nHABITAT - Underground- fields, loose soil\nFACTS - Blind. Digs tunnels underground\nPIGEON: Columba livia\nSIZE - Average 12 inches long and 1 lb. weight\nLIFE EXPECTANCY - Average 3-5 years\nBREEDING - Nests any time of year. Mates for life. Young hatch in 19 days.\nDIET - Mostly seeds, but will eat a varied diet\nHABITAT - Mostly urban areas and architecture\nFACTS - Flying rats. Gutter Birds. Feathered friends.\nFREE-TAIL BAT: Tadarida brasiliensis\nSIZE - Wingspan about 6 inches, weight 0.5 oz.\nLIFE EXPECTANCY - Up to 16 years\nBREEDING - Breeds in fall, delayed fertilization, female gives birth to one pup in early June.\nDIET - Entirely insectivorous\nHABITAT - Caves and often human buildings near water\nFACTS - Not blind. Uses echolocation to help navigation.\nSIZE - Depends on species - 6 inches to 6 feet.\nLIFE EXPECTANCY - Up to 20 or more years, depending on species\nBREEDING - Some give birth to live young, some lay eggs\nDIET - carnivorous\nHABITAT - many habitats, though undeveloped areas are better\nFACTS - No eyelids. Smells with tongue. 4 venomous species in FL\nThe above facts are just the basics for many of the wildlife species that I deal with in the Orlando area. It is important to understand the behavior and breeding cycles of the wildlife that I deal with. For example, many\nanimals enter attics only when they bear young. Then when I trap and remove the wildlife, it's important that I know if the animal does have a nest of young, and if so, I should find and remove the baby animals. Knowledge\nof wildlife diet not only helps me use the best baits to catch them, but also may tell me what nearby food sources are attracting the animals to the area. The life expectancy of an animal gives me an idea of its conservation\nstatus. I'd rather kill a rat - which can have up to 50 young per year and which has a 95% first-year mortality rate, than kill a precious bat, which has only one young per year and can live up to 16 years. Information is\noften the best tool when dealing with wildlife - more valuable than a steel trap or snare pole! If you want to hire the best in Orlando trappers and animal trapping, give us a call."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:0ee81ccb-83e4-4e4b-9ba5-8db0aa8a55c7>","<urn:uuid:081a5865-8d77-4208-823c-1bf949d0d781>"],"error":null}
{"question":"Did both Stravinsky's Rite of Spring and Falla's El retablo premieres cause audience disruption?","answer":"While The Rite of Spring's 1913 premiere caused significant audience disruption with booing, fistfights, and police intervention, there is no mention of any audience disruption during the 1923 premiere of El retablo de maese Pedro at the Teatro San Fernando in Seville or its subsequent performances in Paris and other cities.","context":["Spanish composer. The music of Manuel de Falla incorporates elements of popular Andalusian culture. One of his most famous works, El retablo de maese Pedro (Master Peter’s Puppet Show, 1923), originates from puppetry traditions.\nIn 1918, Winnaretta Singer, the Princesse de Polignac, commissioned Falla (along with Igor Stravinsky and Erik Satie) to write an orchestral work reduced to just a few characters that could be staged in the elaborate puppet theatre in her palace salon. Falla wrote a libretto for puppets, El retablo de maese Pedro, based on Chapters 25 and 26 of the second part of Cervantes’ Don Quixote, which depicts a puppet play. He was perhaps also inspired by the traditional puppet shows performed by the theatre La Tía Norica in Cádiz (his brother Germán Falla reported this connection to the folklorist, Arcadio de Larrea, who recorded the story in the Revista de Dialectología y Tradiciones Populares Journal of Dialectology and Folklore in 1950).\nA number of other artists then became engaged in the production of El retablo de maese Pedro. The development of the design of the play was a collaboration between Paris and Granada with Manuel Ángeles Ortiz, Jose Viñes Roda and Hernando Viñes for set, figurines, and design of the “retablo”, Hermenegildo Lanz González for the design and manufacture of puppets, and Mme Lazarski for costumes.\nThe full score was performed at the Teatro San Fernando in Seville on March 23, 1923. For this recital, Falla and his collaborators created the Orquesta Bética de Cámara (Chamber Orchestra of Andalusia). The complete show was first presented on June 25 at the Parisian home of the Princesse de Polignac. Subsequent performances took place in Seville on January 31, 1925, and later that year in Barcelona and Zurich. In the spring of 1926, it was performed at the Opéra-Comique in Paris with new designs by the Spanish painter Ignacio Zuloaga. On April 26 it was performed in Amsterdam under Luis Buñuel’s stage direction, and on December 17 in Cádiz, in the theatre that now bears the composer’s name.\nSince its creation, the play has been produced around the world – from New York to Venice, and at the Universal Exposition in Seville in 1992 – with various directors and “maestros”, from Pablo Sebastián to Barceló, from Maese Villarejo to Francisco Peralta. Each version has assumed its own unique style and aesthetic. However, possibly the most moving staging was that presented by the company, La Tía Norica, on November 24, 2001 with the Manuel de Falla Orchestra on the composer’s 125th birth anniversary.\nIn addition to El retablo de maese Pedro, Manuel de Falla provided piano accompaniment to an evening of puppets hosted by Federico García Lorca on January 6, 1923.\n- Henken, John. Program Notes: Master Peter’s Puppet Show. Los Angeles Philharmonic, September 2006.\n- Horowitz, Joseph. Celebrating Don Quixote Program Notes. Brooklyn Philharmonic 2003/2004 Season.\n- “Manuel de Falla et le Retablo de Maese Pedro” [Manuel de Falla and Master Peter’s Puppet Show]. Musiques en mouvement. Charleville-Mézières: Institut International de la Marionnette, 1993.\n- A filmed version in colour of the opera is included on the DVD release Nights in the Gardens of Spain. It features Justino Díaz as Don Quixote, Xavier Cabero as the Boy, and Joan Cabero as Maese Pedro, with Charles Dutoit conducting the Montreal Symphony Orchestra. In this production, the human characters are portrayed by real actors, while the puppets remain puppets. The production has been released without English subtitles, unlike the original telecast and the VHS edition. In the DVD edition, an English translation of the opera is included in the accompanying booklet.","Something different in the works today, which debuted in May of the blissful spring of 1913 We gonna get our “art” music on.\nIgor Stranvinsky – The Rite of Spring\nListen to it here: http://grooveshark.com/#!/playlist/Igor+Stravinsky+Rite+Of+Spring/67058600\nRead about it here: http://en.wikipedia.org/wiki/The_Rite_of_Spring#Premiere\nI do highly suggest reading at least a little about this piece, for quick facts I can tell you this was music to accompany a ballet and\nThe première involved one of the most famous classical music riots in history. The intensely rhythmic score and primitive scenario and choreography shocked the audience that was accustomed to the elegant conventions of classical ballet.\nThe evening’s program began with another Stravinsky piece entitled “Les Sylphides.” This was followed by, “The Rite of Spring”. The complex music and violent dance steps depicting fertility rites first drew catcalls and whistles from the crowd. At the start, some members of the audience began to boo loudly. There were loud arguments in the audience between supporters and opponents of the work. These were soon followed by shouts and fistfights in the aisles. The unrest in the audience eventually degenerated into a riot. The Paris police arrived by intermission, but they restored only limited order. Chaos reigned for the remainder of the performance. Stravinsky had called for a bassoon to play higher in its range than anyone else had ever done. Fellow composer Camille Saint-Saëns famously stormed out of the première allegedly infuriated over the misuse of the bassoon in the ballet’s opening bars (though Stravinsky later said “I do not know who invented the story that he was present at, but soon walked out of, the première.” ). Stravinsky ran backstage, where Diaghilev was turning the lights on and off in an attempt to try to calm the audience.\nWith art music the conductor and the orchestra can make a lot of difference…unfortunately with grooveshark’s labeling system I have had an nearly impossible time giving credit to these performers….I did do an extensive search to find performances from the same group and the “best” one I could find.\n|Igor Stravinsky – Rite of Spring\nLeast favorite track/tracks:\nOverall (1-5 stars): 4\n|Igor Stravinsky – The Rite of Spring: 3 stars…if I have to give a rating\nPrecon: A lot of stuff that’s gonna go over my head, but if I was a music major I might jimp. I was hoping it would at least sound nice, but the more I read about it, the more it seems like a boundary pushing type thing so that’s probably out the window. Oh well, let’s see how it goes.\nFavorite Track: Augurs of Spring\nThis is pretty cinematic, which makes sense. For Augurs of Spring I’m seeing a scary factory scene in an old cartoon. Or maybe a Hitchcock movie.\nMagic the Gathering Card: Rites of Spring\n|Igor Stravinsky: Rite Of Spring\nPreconceived Notions: Classical. Heard of him. Haven’t heard this. Not super excited about it, but since I don’t want to appear as if I’ve been raised by wolves, I should focus my attention. I’d probably rather be watching the ballet.\nWell, after reading the Wikipedia, it turns out I MUST have heard at least part of it because it’s featured in Fantasia…one of my least favorite movies ever.\nI hear a lot of what would eventually become “bastardized” parts of Spielberg soundtracks, so clearly an influence on John Williams. Oh and also a bunch of sound a-like moments to West Side Story, so clearly Bernstein was a fan too. I think Bernstein would have been a huge fan of the oboe’s role in this work. I’m guessing Bernard Hermann was also a fan.\nAfter Listening: I appreciate that each sequence tells a distinct story. I can’t stand when classical pieces just flow together with no clear break. I suppose this is also mostly due to the fact that this was a ballet, and well, you need to tell a story, more often than not, for a ballet to work. It’s an emotionally evocative piece for sure, and, slightly on the dark side, if I’m just going by how I felt listening to it. There’s something ominous about the bass drum and frightening about the combination of trumpets and high-pitched strings. It’s interesting that we’re listening to this work now, days after I’ve seen John Carter because I feel like they’re hampered by the same issue. Both this and Burroughs work are technically “originals”. Or as original as any art influenced by other art/artists can be. And yet, because we’re so much farther along the pop culture scale, I’ve already seen West Side Story, heard Jaws and Star Wars a million times and so, also, cannot help but making the comparison. It’s impossible for me to say whether one of these things is better or more of an artistic accomplishment because they’re kind of intertwined. I’m at least glad to be aware of the originals now. And pleased that I didn’t get as lost in the story as I thought I might. However, if I dream of tutued hippopotami tonight, I’m definitely blaming Igor. On a side note, reading his bio on Wikipedia, I got the same feeling I had when I saw Midnight In Paris. That guy lived a charmed life.\nNo least or favorite tracks, since, if you take one away I’m pretty sure the whole thing suffers. Also, like I said, they all made me feel something, imagine something, so that has to count in the positive column for this.\n|Preconceived Notions: This one is mine, I was trying to introduce the group to “classical” or more accurately in this case “art” music. I know, I know, quit rolling your eyes…I’m just trying to change it up. I had assumed everyone here has at least heard pieces by Beethoven, Mozart, Bach and the like – so there was no need to rehash that. Likewise Wagner, Schubert, Chopin, and even Debussy are highly stylized in their own rights – but similar enough to what I think RC’s general exposure to art music has been. I picked Stravinksy for this little experiment for two reasons 1.) He’s a good transitional composer between highly tonal works (like Beethoven on through the impressionists) and the more atonal guys I like to listen to in the 20th century (Schonberg, Ives, Bartok, etc…), and 2.) because there’s a chance this particular group might identify the beginnings of the modern film score in Stravinsky’s work.\nAfter Listening: We can get into the awesome polyrhythmic devices here, but not in any detail that is really worth going on and on about- if you don’t have that frame of reference. And honestly, I’m worried already that people hear terms like that and think I’m trying to lecture or take on an air of pompousness. I’m not, to describe what’s at the core of this work however, you will have to take a measure of analysis beyond chords and melody lines though. And to avoid writing a novel no one will care about the basic construction of this piece, it is built around the idea that little segments of musical notes are cut up and blasted at you (and then at each other in interplay) in various places in the musical arrangement (for example the infamous oboe opening), while the rest of the supporting orchestra remains very harmonically stagnant. The stagnation creates an unease in the listener who wants the piece to revert to some type of tonal resolution (aka something that sounds “final” or “completed”). For example if you listen to sections like Sacre 1.2 Augurs of Spring you should hear the origins of the theme from Jaws. In other sections of increased tempo Stranvinsky presents some consonant harmonic backing- but it serves to present the segment as frantic – Sacre 1.5 for example has sections that are very reminiscent of Star Wars films. What I’m getting at here is that Stravinsky used a multitude of musical devices to evoke powerful emotions, so much so that many of our contemporary film composers owe him a huge debt for breaking ground with pieces like this. Again a full scale (pun intended) musical analysis of this has been done and done again by people far more qualified in the field than me and my fading concepts of theory…so I’ll stop about here.\nOverall 5.0 It is sort of pointless to break out these sections to rate one over the other, as a whole they were part of an accompanying ballet and presented in audio form only may also be challenging to the listener enough, without parting them out. This is to say, that this music is so vivid and evocative that the ballet or other visual medium (aka the parts of this used in Disney’s Fantasia) helps frame the work for many in a context that’s relatable. We’ve done a lot of concept albums in record club – some with a literal story, some with just the grain of a unifying theme…and to that end “art” music really is the 1st and ultimate concept music. When Stravinksy presented these musical ideas a 100 years ago there were riots in the aisles, nowadays they are employed as soundtracks to popular films. And while one is certainly free to dislike The Rite of Spring, it’s a very good lesson in transitional artists and the evolution of musical ideas. I mean, I dream of the day we as culture toss on a Schonberg composition…and riot over a Bruno Mars show. That’s probably never gonna happen…but ya gotta keep fighting the fight- and if you need marching music for that fight….The Rite of Spring isn’t a bad place to start.\nExtra Credit: The composer himself conducts a piece of Firebird – (which is generally considered a more mature work than Rite of Spring)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:4a20189a-783a-40f6-b917-3c94eac63a4a>","<urn:uuid:2850f6c2-880f-4a64-b019-33d6423589cb>"],"error":null}
{"question":"What similarities exist between the Mayan understanding of zero and the biblical concept of divine temporality?","answer":"The Mayan understanding of zero and the biblical concept of divine temporality both dealt with representing abstract concepts that were difficult to physically experience. The Mayans developed a placeholder symbol for zero (resembling a shell) to represent nothingness, but could only use it between other numbers and couldn't perform calculations with it. Similarly, biblical writers acknowledged that eternity and divine temporality couldn't be physically experienced or touched, only grasped in the mind's eye, as evidenced when Jesus spoke of heavenly things that crowds couldn't understand (John 3:12). Both systems struggled with representing concepts that existed beyond normal human experience, though they approached this challenge through different means - mathematical symbolism for the Mayans and theological language for biblical writers.","context":["Seeing the word eternity may conjure up the image of an outstretched horizon receding as far as we can see, or a type of heavenly realm—an Elysian\nplain with warm sun and cloudless sky. However, eternity is actually a complex idea that relates to how people understand God and their place in the universe.\nWhile eternity is not something one can actually experience, biblical writers use the concept to describe two related issues: First, biblical writers use eternity as a way to describe divine temporality\n(as in a quality of God, e.g., Gen 21:33; Deut 33:27; Isa 40:28). Second, they also use eternity to describe the timeframe of future existence for people (as in an afterlife, Tob 3:6; John 3:16).\nEternity as Divine Temporality\nConcepts of eternity are rooted in how people perceive time, especially as it relates to God’s interaction with time. There are essentially two options: God is temporal, meaning that he exists in time; and God is atemporal, meaning that he exists apart from time. Because of the difficulty of describing eternity in human language, the Bible seems to hint at both of these options. For example, the psalmist depicts God as living an innumerable number of days (Pss 88:29, 90:2; 1 Chr 16:36) and contrasts God’s life with the brevity of a person’s life (Pss 39:5, 90:3–10, 103:15–17, 144:4). Similarly, God seems to exist in time when he does human-like things such as change his intentions (e.g., Exod 32:14) and dwell among people in the incarnation\n(John 1:14). In contrast, the Bible also depicts God as apart from creation and, therefore, outside of time (Gen 1:1–5; Deut 33:27; Eccl 3:11; Isa 43:13; Rom 1:20). New Testament\nbooks such as John afford Jesus a divine temporality as a function of his divinity (John 1:1, 8:58; and looking back to the Old Testament\n, see Prov 8:22–31; Isa 9:6; Mic 5:2).\nEternity as Future Existence\nThese concepts of eternity also affect the way in which people describe their future existence. Because God is eternal, his covenant with his people will also be eternal (Gen 9:16, 17:7; Lev 16:34; 2 Sam 23:5; Sir 44:18, 45:7; Bar 2:35; Heb 13:20). Since the final resting place for those possessing this covenant with God is with God (Dan 12:2; 2 Macc 7:9; 2 Cor 5:1; Rev 21), then it is also, by extension, eternal as God is eternal (Ps 49:9; Eccl 12:5; Isa 45:17). For the New Testament, the culmination of this covenant coming through right belief is eternal life (John 3:16). Thus, when the New Testament speaks of eternity, it also views eternity not just as innumerable days but also as the mode of life with God in the age to come (Mark 10:20; John 4:14). Just as heaven is eternal, so does hell appear to be also (Matt 25:41; 4 Macc 9:9; 2 Thess 1:9; Jude 6–7; Rev 20:10).\nThe biblical writers could not see or touch eternity; it was only something they could grasp in their mind’s eye. Even when Jesus spoke of heavenly things, the crowds did not grasp his meaning (John 3:12). Yet concepts of eternity allow the reader to contrast the limitations and weakness of their present existence with an outstretched horizon of God’s eternal goodness and love (John 3:16).","How many elephants are in the same room as you right now? Most people would answer zero to that question (if you answered something else, we should be friends). The concept of zero is familiar to us. Earlier today, my two-year-old cousin told me that his baby sister is zero years old. I filed sales taxes for my business and typed up countless zeros. Today, zero is part of daily life. Even a two year old understands the concept of zero.\nZero is nothingness — a void. If you think deeper, it’s fairly amazing that we throw around such a profound term. I can see, touch and count the number of teabags left in a box, but I can’t see, touch or count the number of elephants in my bedroom. There are also zero storm troopers, zero cookies and zero dinosaurs in my bedroom. In my bedroom, there are an infinite number of zeros. Our number zero, symbolized by “0,” enables us to do calculus, and it’s even half of the reason my computer works right now. In the early days of math, zero didn’t exist — there wasn’t even a word for it, which made even simple arithmetic a bit complicated. Thankfully, ancient Babylonian, Mayan and Indian mathematicians developed the concept of zero and paved the road for truckloads of discovery and innovation.\nJust like ours, the Babylonian number system (2000 BC) was positional. In our base 10 system, having a positional number system simply means you have a position for ones, tens, hundreds, etc. Babylonians used the same concept except their ones position included the numbers 1-59 instead of 1-9. Regardless of base, the problem with having no zero is the numbers ‘11’ and ‘101’ suddenly both look like ‘11’. Most people can’t read minds, so that makes understanding other people’s writings a bit difficult. The Babylonians developed a place holding symbol to solve this dilemma. For example, if we used a period as a placeholder, those numbers would look like ‘11’ and ‘1.1’. It dispersed some confusion, but the placeholder could only be used between numbers, so ‘1’ and ‘100’ both looked like ‘1’. Without a zero, modern mathematics had no chance of developing.\nSimilarly to the Babylonians, the Mayans developed a placeholder symbol that stood for zero. They developed the notion completely independently of the Babylonians — after all, they were half way around the world and didn’t have texting. Their symbol for zero supposedly looks like a shell. To me, it looks more like a spaceship, but I digress. They had the concept of a placeholder, but like the Babylonians, they didn’t use the symbol on its own. Again, its a start, but you can’t add, subtract or multiply using a placeholder.\nThe hero of this story is a Hindu astronomer by the name of Brahmagupta. Around 628 AD, Brahmagupta wrote down rules for getting to zero using addition and subtraction and the results of using zero in equations. There are earlier traces of zeros in Cambodia and various parts of India, but Brahmagupta’s account is primary because it gave the rules behind using zeros. Brahmagupta called zero ‘sunya’ or ‘kha’ which mean ‘empty’ and ‘place’ respectively. His rules included things like ‘the sum of two zeros is zero’, ‘the product of a zero and any other number is zero’, and ‘zero divided by a zero is zero’. These rules were revolutionary. As simple as they seem, this one list of rules effectively changed the entire human world. You may have noticed something wrong with one of those rules — our modern mathematics don’t allow you to divide by zero. Brahmagupta’s rules about dividing by zero may have been flawed, but that just means he left something for G.W. Leibniz and Isaac Newton to work on later!\nAfter zero became a fully formed number, it spread like wildfire. Along with spices and other tradable goods, Arabian voyagers brought zero back from India. A hundred years after Brahmagupta discovered zero, it reached Baghdad. In the 9th century, a man named Mohammed ibn-Musa al-Khwarizmi started to develop algebra by working on equations that equaled zero. He called zero ‘sifr’ which turned directly into our word ‘cipher’ and eventually developed into our word ‘zero’. Come 879 AD, people wrote zero almost exactly like we do today; the only difference between our zero and theirs was size. They used an oval that was smaller than the other numbers — it became ‘1’, ‘1o’ and ‘1oo’. Finally, when the Moors invaded Spain they brought zero to Europe, and by the mid-1900s, Al-Khowarizmi’s work reached England at last.\nZero is universal; it transcends culture, space and time. It is part of our global language and is one of the most fundamental ideas in calculus, physics, engineering, computers, and a lot of financial and economic theory. Our lives are full of zeros. Plus, after traveling around the entire world and changing the course of human history, zero inspired this brilliant little video. Enjoy!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:ee763ef6-4132-4da5-915b-d70fcbda3e13>","<urn:uuid:bfc02041-9236-4d9c-8180-67d585fdd8bd>"],"error":null}
{"question":"How do the daily maintenance routines compare between vegetable gardens and indoor cacti? They're both popular choices these days!","answer":"Vegetable gardens require more intensive daily maintenance compared to indoor cacti. Vegetable gardens need daily checking for pests, regular trimming of dead leaves and vines, consistent weeding, and careful attention to watering schedules. In contrast, indoor cacti require minimal maintenance - they need very limited watering (especially during dormant winter months), occasional fertilizing during growing season, and mainly require attention to proper light exposure in a south-facing window. While vegetable gardens need regular soil amendments with compost and manure, cacti only need repotting when they outgrow their containers, using a special soil blend for succulents.","context":["Vegetable gardening is basically used for growing vegetables and plants that are perfect for human consumption.\nThere are different kinds of vegetable gardens. There’s container gardening, a kind of garden where you can plant vegetables, herbs, and flowers at once. This one becomes more popular than the old or usual rows or blocks style gardening.\nKitchen gardening, that is present in most American households, is becoming popular again. People are wanting fresh, locals foods, and foods that are available every season. It’s amazing how many folks are returning to eat what’s in season.\nThere’s really no need for more acreage, or more tools! Most plants can grow even in a container. You can plant some of your vegetables, flowers and herbs in smaller sized containers or planters for sprouting on your balconies, decks, and windowsills.\nBefore you achieve the vegetable garden you want, follow these easy steps that I recommend for a successful garden.\n- Place or Location: You should choose a place to your house that has enough protection from strong winds, and that are free from rocks and weeds. And of course, abundant sun light all day is very important. Don’t make it too far from your house. You will spend more time in it, if it’s not so far away.\n- Soil: Another essential for your plants to grow is a good choice of healthly soil. Compost and aged manure is very helpful for growing plants, add those every year. And don’t forget to test the soil every 2 to 3 years.\n- Composition or Layout: For a good rotation, plats should be rotated in position from year to year. Just don’t forget that higher plants should not interfere the shorter plants. Rotating plants helps prevent pests and diseases in your vegetable garden.\n- Size: Gardening involves lots of effort to meet your goal. Begin with the small ones, and don’t get overwhelmed. A tended garden is what will get you results.\n- Choice of seeds: Choose only those seeds that you and your family like to eat. Because if you like it, it will motivate you to continue.\n- Do a little research: This will help you to have you enough knowledge to grow your vegetables in accordance to your area and climate.\n- Water: I recommend you water your plants early in the day. Water it deeply near the base of the plants. Watering the leaves is not a help, and the sun is likely to just burn them. Remember to water it enough and not too much!\n- Checking: Be sure to check on your plants every single day to protect them from pests. Not a big deal, just a simple walk through. The sooner you notice pests, the easier it is to do something about it.\n- Problem solving: Before you act whenever you find problem with pests, be sure to make some research and properly treat the problem and not aggravate the problem. It will, also, save you time and money.\n- Cutting or Trimming: Maintain your garden shapes by cutting, trimming or removing unwanted dead leaves, vines and weeds. This will help your plants to grow healthy.\nRemember, as long as you are committed, and consistent in maintaining your garden, you will achieve the successful garden you want. A successful garden means better tasting foods on your dinner table!","Looking for an easy-to-grow indoor plant that requires little watering? Do you like the desert southwest look of succulents? You’re in luck- a small cactus plant is particularly suited to most indoor, low humidity conditions. See below for tips and tricks for growing a cactus indoors.\nCacti is a type of succulent, native to North America. Several varieties can be grown as houseplants and each are different in size, shape and color. A few of the most popular are:\nAlso known as sea urchin cactus, this globe-shaped succulent is spineless. Its chubby body looks similar to a sand dollar.\nOld Man Cactus\nThis white-haired, fuzzy cactus certainly has its own unique character. Reminiscent of grandfather’s beard, the deftly-named Old Man cactus grows long, white hairs over its surface.\nThis winter flowering houseplant is a popular holiday gift. Christmas cactus is also easy to propagate. Just cut a Y-shaped section from the tip of a healthy stem.\nA long-leafed succulent, agave grows into an attractive rosette shape, and produces cup-shaped blooms. Some agave varieties die after blooming, but produce offshoots to live on.\nBunny Ears Cactus\nPerfect for beginning growers, the cute bunny ears cactus is an easy care variety. Thick, green pads grow in ear-like pairs, and are covered in fuzzy, short bristles that resemble rabbit fur.\nWhere to Find Cactus for Growing Indoors\nGardening centers, florists, and the outdoor department of home improvement stores are the best places to purchase cactus and succulents. Thinking about digging up your own cactus? Some agave and cacti species are protected under state native plant laws, so check before you dig.\nTo grow a cactus indoors you’ll need plenty of sunlight. The best exposure is in a south-facing window, but light from and east or west window can work as well. Furthermore, make sure the plant receives light for a good portion of the day. However, if you don’t have enough light, a fluorescent grow light will work. Hang the grow light a minimum of 6 inches above the plant to prevent scorching the leaves.\nYou can set cacti outdoors, on a covered deck or patio, in summer. Although, you’ll need to monitor the amount of water they receive.\nMany indoor cacti have met an untimely death from over-watering. If the weather is cloudy or may become cloudy, don’t water. Your watering schedule will vary depending on the time of year. Cacti actively grow in spring and summer, but are dormant; resting in winter. Here’s how to water your cactus indoors according to the season:\nSpring, Summer Watering:\nWater the plant well, allowing excess water to drain. Cacti do not like to sit in water, so make sure your plant pot has several drainage holes. Adding a layer of pebbles at the bottom will prevent soil from clogging up the drainage holes. Water again when the top 1/2 inch of soil is dry. If your pot is on a saucer, make sure to drain water from the saucer to prevent plant rot.\nFall, Winter Watering:\nWater very little, at a maximum rate of once every 2 weeks. Apply only enough water to dampen the soil near the roots.\nFertilizing Your Cactus\nUse a liquid fertilizer to feed your plant several times during the growing season. A complete, balanced fertilizer will work well, or purchase a plant food especially formulated for succulents.\nIdeal Growing Temperature\nDuring the spring and summer, the optimum room temperature is 70 to 75 degrees (F) during the day, and 60 degrees (F) at night. During the fall and winter dormant period, reduce temperatures to 45 to 55 degrees (F). If your room is warmer, place your cactus close to the window but not touching. The room temperature will be 5 to 10 degrees cooler next to the window.\nRe-potting Your Cactus\nIf your cacti grows too large for its container, re-pot it. Use a soil blend especially formulated for succulents or make your own by mixing:\n- One part coarse sand\n- One part loam, and\n- One part peat moss\nLastly, delay watering for a couple of weeks after re-potting to prevent root rot.\nMoving Your Cacti to A Different Spot\nWear protective gloves when moving your cactus indoors to a different location. In addition, if you happen to brush your hand against the plant and get stuck by a spine, use a piece of adhesive tape to gently pull it out.\n(Photo Courtesy of: opacity)\nLearn more about plants that thrive indoors, here.\nWant to read more gardening tips? Click here."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:0ac71bc4-51d2-4bac-ae1a-386b72748556>","<urn:uuid:829e4a6e-cde6-4bc6-aecc-02c3b5c5dee9>"],"error":null}
{"question":"Which came first chronologically: the Carter Family's country music career or Johnny Cash's birth?","answer":"The Carter Family's country music career began before Johnny Cash's birth. The Carter Family first recorded in 1927-28, as evidenced by their Victor Recordings from that period, and they became the first stars of country music with hits like 'Can The Circle Be Unbroken (By and By)' and 'Wildwood Flower.' Johnny Cash was born several years later on February 26, 1932, in Kingsland, Arkansas. Interestingly, there would later be a connection between Cash and the Carter family, as June Carter Cash (a member of the Carter Family) would become Johnny Cash's second wife.","context":["History of Country Music\nCountry music was one of the first genres of modern American popular music, and old-time music was its earliest style. It developed in the southeastern states of the USA as a mix of folk music from the British Isles, church music and African American blues. It was played on instruments like acoustic guitar, mandolin, autoharp, fiddle and the banjo. Old-time music was first recorded in the 1920s, with recordings of the Carter Family becoming the most popular. A. P. Carter collected folk songs and also wrote new songs, and he sang them in harmony with his guitar-playing sister-in-law Maybelle and his wife Sarah, who also played autoharp. Songs like Can The Circle Be Unbroken (By and By) and Wildwood Flower became hit records, and the Carter Family became the first stars of country music.\nJimmie Rodgers, another of country music's earliest stars, was recorded at the same recording sessions as the Carters. Jimmie was taught how to play guitar and sing blues and work chants by African Americans in railroad gangs in which he worked. He also heard old-time music and folk songs and combined all these styles in his own songs. He often used a vocal technique called yodelling, and his first hit record, Blue Yodel, sold nearly half a million copies in 1927.\nCountry Music 1930 - 1960\nBefore television, American families often sat together and listened to the radio. One of the most popular programs was a live country-music variety show called the \"Grand Ole Opry\". It was broadcast from Nashville, Tennessee, which had become the centre of the country-music business. Listeners heard old-time music as well as another style called Western music. This style often had horse-like clip-clop rhythms and songs about lovesick cowboys and gun-fighting outlaws. Western music became popular in the 1930s and 40s when singing cowboys began appearing in Hollywood cowboy movies called \"Westerns\". Singing cowboys like Gene Autry and Roy Rogers became huge country-music stars, and Nashville executives decided the cowboy image was better for country music than the hillbilly image of old-time music. They renamed the genre \"Country and Western music\" and began dressing their musicians in cowboy clothes.\nMeanwhile, a style of Western dance music called Western swing became popular in Texas, Oklahoma and California. Western swing bands used amplified instruments like pedal steel guitar to create music loud enough to be heard in large dance halls. Their music was a lively mix of Western country music and swing jazz, and one of the most popular bands was Bob Wills and The Texas Playboys. Another style called rockabilly developed when Western swing bands began playing R&B songs as well as country songs. When singers like Elvis Presley heard this new mix of country music and R&B, they formed rockabilly bands with acoustic guitar, electric guitar, stand-up bass and drums. Elvis had several rockabilly hits early in his career, as did Carl Perkins, Roy Orbison and Johnny Cash. Cash became one of country music's biggest artists in the 60s when he combined the sounds of rockabilly with those of honky tonk. He soon became known as the \"man in black\" because he wore black clothes instead of cowboy clothes, as did Roy Orbison who wore dark sunglasses as well to complete his look.\nHonky tonk music first developed in the 1940s in working-class honky tonk bars near the oil fields of Texas. Honky tonk bands usually included acoustic guitar, pedal steel guitar, fiddle, stand-up bass and drums, and honky tonk songs were often about loneliness, love, heartbreak and pain. Working-class people could relate to these songs, especially those of country music's greatest singer-songwriter, Hank Williams. Hank drank too much, had a difficult relationship with his wife Audrey, and died at 29. But in his short, troubled life he wrote hundreds of beautiful, powerful songs, many of which have become country-music standards like Lovesick Blues, Cold, Cold Heart and I Saw The Light. Other important artists include Ernest Tubb, Lefty Frizzell and Jean Shepard. The honky tonk sound has often been revived when country music has become too commercial and fans want a more authentic sound.\nAnother style called bluegrass developed in the early 1950s. It was a revival of old-time country music led by Bill Monroe and the Blue Grass Boys. In the 70s, the Nitty Gritty Dirt Band became popular, and since the mid-80s the most successful bluegrass artist has been Alison Krauss.\nLater Country Music\nIn the mid-1950s, record companies in Nashville were losing sales to rock and roll and soul artists who topped the charts. To compete, Nashville producers created a new style that would appeal to white adults who didn't like rock and roll or soul, but didn't usually buy country records either. They found singers with smooth voices and had them sing sweet ballads over orchestral strings and choirs. Authentic country instruments like fiddle, guitar and banjo weren't often used, and the plan worked. Sales of records from Nashville companies soon began to increase, especially for artists like Jim Reeves and Patsy Cline.\nBut many artists weren't happy about what Nashville was doing to country music, and in the early 60s these artists developed a new style that combined the authentic country-music sounds of honky tonk with the rebellious attitude of rockabilly. Merle Haggard, Johnny Cash, Waylon Jennings and Kris Kristofferson were some of the most important artists in what's now called outlaw country, and many people loved their music. Songs like Johnny Cash's Ring of Fire, Merle Haggard's Mama Tried and Kris Kristofferson's Sunday Mornin' Comin' Down became huge hits, and country music was saved once again.\nAnother style called country rock began to develop in the mid-60s. Gram Parsons created some of the earliest country rock when he added rock and roll piano, rock guitar and elements of folk rock to his band's country-music sound. The style developed further when Gram worked with the Byrds and then the Flying Burrito Brothers. Bob Dylan also began mixing elements of country music into his folk rock sound in the mid-60s. Dylan had been writing poetic folk songs since the early 60's, especially protest songs like Blowin' in the Wind and A Hard Rain's a-Gonna Fall. When he switched from acoustic to electric guitar in 1965, his sound moved closer to rock. But he didn't make a real country-rock album until 1969 when he recorded the album Nashville Skyline with country musicians like Johnny Cash. In the 70s, artists like New Riders of the Purple Sage, Emmylou Harris, Linda Ronstadt and Neil Young developed the style further.\nAnother style that appeared in the 60s was country pop. In the early 60s, former rockabilly singer Roy Orbison began producing some of the best pop records ever made by a country-music artist. Orbison's voice was one of the most emotionally powerful in all of popular music, and he had a major influence on many later artists. In the late 60s and early 70s country pop artists like Glen Campbell, Dolly Parton and Kenny Rogers had many hit records, and female artists like Loretta Lynn, Tammy Wynette, Carrie Underwood and Taylor Swift have been successful more recently.\nCountry music styles, artists and albums\n- Old-time Music: The Carter Family - Anchored in Love: Complete Victor Recordings (1927-28)\n- Honky Tonk: Hank Williams - 40 Greatest Hits, Lefty Frizzell - Listen to Lefty\n- Western Swing: Bob Wills & His Texas Playboys - For The Last Time, Billy Jack Wills - Crazy Man Crazy\n- Rockabilly: Various Artists - The Sun Records Collection, Carl Perkins - Dance Album\n- Outlaw Country: Johnny Cash - The Man In Black, Merle Haggard - Hag: The Best of Merle Haggard\n- Bluegrass: The Nitty Gritty Dirt Band - Will the Circle Be Unbroken, Stars & Stripes Forever\n- 60s Country Rock: The Byrds - Sweetheart of the Rodeo (1997 reissue), Bob Dylan - Nashville Skyline\n- 70s Country Rock: New Riders of the Purple Sage - self-titled first album, Neil Young - Harvest, On The Beach\n- Early Country Pop: Roy Orbison - The Essential Roy Orbison, Dolly Parton - The Essential Dolly Parton\n- Later Country Pop: Carrie Underwood - Some Hearts, Taylor Swift - Speak Now\nacoustic (adjective): without inbuilt electrical equipment to amplify the sound - I can play acoustic guitar, but I can't play electric guitar.\namplify (verb): to make sounds louder, esp. by using electrical equipment - If we don't amplify the drums, they'll be hard to hear.\nauthentic (adjective): real or genuine - You can still see an authentic Chinese opera in Beijing.\nautoharp (noun): a small harp with buttons to press for playing chords - Is the autoharp used much in bluegrass music?\nbanjo (noun): an African American stringed instrument based on the African kora - Do you play four-string or five-string banjo?\nbluegrass (noun): a style of country music based on old-time Appalachian music - We're going to a bluegrass concert tonight.\ncommercial (adjective): made in order to be popular and make money - Garth's country music is much too commercial for me.\ncountry pop (noun): a style that mixes pop and country music - Country pop is really popular in America these days.\ncountry rock (noun): a style that mixes rock and country music - My brother doesn't like country pop, but he loves country rock.\nfiddle (noun): another word for \"violin\", esp. in country and folk music - Who's playing fiddle on that record?\nfolk rock (noun): a style that mixes folk and rock music - We heard lots of folk rock bands in San Francisco in the early 60's.\nhillbilly (noun): an impolite word meaning a poor mountain farmer in the USA - In Nashville, old-time music was called hillbilly music.\nhonky tonk (noun): a country music style known for its powerful, emotional songs - Who's your favourite honk tonk singer?\nmandolin (noun): a stringed instrument like a guitar with a curved back - You can hear mandolin on those early old-time recordings.\nold-time music (also \"hillbilly music\") (noun): country music originating in the Appalachian mountains of the USA - Do people still play old-time music much?\noutlaw country (noun): a style of country music popular in the 1960s - Most outlaw country singers wore black clothes instead of cowboy clothes.\npedal steel guitar (noun): an electric steel guitar on a stand with foot pedals for changing the sound - My mum loves the sound of pedal steel guitars.\nprotest song (noun): a song with lyrics that protest against war, injustice, etc. - Why don't people write protest songs anymore?\nrecording session (noun): time spent recording in a music recording studio - We've got a recording session on Monday morning.\nrevival (noun): the return to popularity of an old style or form - There was a rockabilly revival during the punk music years.\nrockabilly (noun): a style that mixes Western swing and R&B - Sam recorded lots of rockabilly songs at Sun Studio in Memphis.\nstandard (noun): a song that is often recorded and performed - Lots of Roy Orbison's songs have become pop standards.\nstand-up bass (or \"string bass\") (noun): another word for \"double bass\", esp. in country music - Rockabilly bands had stand-up bass instead of bass guitar.\nWestern (country) music (noun): a style of country music that developed in the western states of the USA - My dad likes Western music more than old-time country music.\nWestern swing (noun): a style that mixes Western music and big-band swing jazz - Those Western swing records are great to dance to.\nyodel (verb): to sing in a way that quickly changes from a very high voice to a normal voice - When I tried to yodel, everyone laughed.","Johnny Cash was born on February 26, 1932 in Kingsland, AR. His older brother, Jack, died when he was 12 years old.\nJohn R. \"Johnny\" Cash (February 26, 1932 – September 12, 2003) was an American singer-songwriter, actor, and author who was considered one of the most influential musicians of the 20th century. Although he is primarily remembered as a country icon, his songs and sound spanned other genres including rock and roll and rockabilly —especially early in his career—and blues, folk, and gospel. This crossover appeal won Cash the rare honor of induction in the Country Music Hall of Fame, the Rock and Roll Hall of Fame, and the Gospel Music Hall of Fame.\nCash was known for his deep, distinctive bass-baritone voice, for the \"boom-chicka-boom\" sound of his Tennessee Three backing band; for a rebelliousness, coupled with an increasingly somber and humble demeanor; for providing free concerts inside prison walls;]page needed[ and for his dark performance clothing, which earned him the nickname \"The Man in Black\". He traditionally began his concerts with the phrase \"Hello, I'm Johnny Cash.\", followed by his standard \"Folsom Prison Blues\". Music\nCountry music is a genre of American popular music that originated in the rural regions of the Southern United States in the 1920s. It takes its roots from the southeastern genre of American folk music and Western music. Blues modes have been used extensively throughout its recorded history. Country music often consists of ballads and dance tunes with generally simple forms and harmonies accompanied by mostly string instruments such as banjos, electric and acoustic guitars, fiddles, and harmonicas.\nThe term country music gained popularity in the 1940s in preference to the earlier term hillbilly music; it came to encompass Western music, which evolved parallel to hillbilly music from similar roots, in the mid-20th century. The term country music is used today to describe many styles and subgenres. In 2009 country music was the most listened to rush hour radio genre during the evening commute, and second most popular in the morning commute in the United States.\nThe Grammy Legend Award, or the Grammy Living Legend Award, is a special award of merit given to recording artists by the Grammy Awards, a ceremony that was established in 1958 and originally called the Gramophone Awards. Honors in several categories are presented at the ceremony annually by the National Academy of Recording Arts and Sciences of the United States for outstanding achievements in the music industry.\nThe first Grammy Legend Awards were issued in 1990 to Andrew Lloyd Webber, Liza Minnelli, Smokey Robinson and Willie Nelson. The honor was inaugurated to recognize \"ongoing contributions and influence in the recording field\". The next year four more musicians (Aretha Franklin, Billy Joel, Johnny Cash and Quincy Jones) were acknowledged with Grammy Legend Awards. The award was given to Barbra Streisand in 1992 and Michael Jackson in 1993.\nThe Pine Bluff Metropolitan Statistical Area, as defined by the United States Census Bureau, is a three-county region in southeast Arkansas, anchored by the city of Pine Bluff. As of the 2010 census, the MSA had a population of 100,258. It is also a component of the larger Little Rock-North Little Rock, AR Combined Statistical Area.\nThe United States of America (USA), commonly referred to as the United States (US), America, or simply the States, is a federal republic consisting of 50 states, 16 territories, a federal district, and various overseas extraterritorial jurisdictions. The 48 contiguous states and the federal district of Washington, D.C., are in central North America between Canada and Mexico. The state of Alaska is the northwestern part of North America and the state of Hawaii is an archipelago in the mid-Pacific. The country also has five populated and nine unpopulated territories in the Pacific and the Caribbean. At 3.79 million square miles (9.83 million km2) in total and with around 316 million people, the United States is the fourth-largest country by total area and third largest by population. It is one of the world's most ethnically diverse and multicultural nations, the product of large-scale immigration from many countries. The geography and climate of the United States is also extremely diverse, and it is home to a wide variety of wildlife.\nPaleo-indians migrated from Asia to what is now the US mainland around 15,000 years ago, with European colonization beginning in the 16th century. The United States emerged from 13 British colonies located along the Atlantic seaboard. Disputes between Great Britain and these colonies led to the American Revolution. On July 4, 1776, delegates from the 13 colonies unanimously issued the Declaration of Independence. The ensuing war ended in 1783 with the recognition of independence of the United States from the Kingdom of Great Britain, and was the first successful war of independence against a European colonial empire. The current Constitution was adopted on September 17, 1787. The first 10 amendments, collectively named the Bill of Rights, were ratified in 1791 and guarantee many fundamental civil rights and freedoms.\nKingsland is a city in Cleveland County, Arkansas, United States. Its population was 447 at the 2010 U.S. census. It is included in the Pine Bluff, Arkansas Metropolitan Statistical Area. It is famous as the birthplace of Johnny Cash.\nValerie June Carter Cash (June 23, 1929 – May 15, 2003) was an American singer, dancer, songwriter, actress, comedian and author who was a member of the Carter Family and the second wife of singer Johnny Cash. She played the guitar, banjo, harmonica and autoharp, and acted in several films and television shows. Carter Cash was inducted into the Christian Music Hall of Fame in 2009. She was ranked No. 31 in CMT's 40 Greatest Women in Country Music in 2002.\nJohn Gale \"Johnny\" Horton (April 30, 1925 – November 5, 1960) was an American country music and rockabilly singer most famous for his semi-folk, so-called \"saga songs\" which began the \"historical ballad\" craze of the late 1950s and early 1960s. With them, he had several major successes, most notably in 1959 with the song \"The Battle of New Orleans\" (written by Jimmy Driftwood), which was awarded the 1960 Grammy Award for Best Country & Western Recording. The song was awarded the Grammy Hall of Fame Award, and in 2001 ranked No. 333 of the Recording Industry Association of America's \"Songs of the Century\". His first hit, a number #1 was in 1959, with When It's Springtime in Alaska (It's Forty Below)\nDuring 1960, Horton had two other successes with \"North to Alaska\" for John Wayne's movie, North to Alaska, and \"Sink the Bismarck\". Horton is a member of the Rockabilly Hall of Fame and the Louisiana Music Hall of Fame\nIn journalism, a human interest story is a feature story that discusses a person or people in an emotional way. It presents people and their problems, concerns, or achievements in a way that brings about interest, sympathy or motivation in the reader or viewer.\nHuman interest stories may be \"the story behind the story\" about an event, organization, or otherwise faceless historical happening, such as about the life of an individual soldier during wartime, an interview with a survivor of a natural disaster, a random act of kindness or profile of someone known for a career achievement. Law Crime"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:e31dbe21-b8bb-4631-a8a4-e72dc4b2dc1e>","<urn:uuid:27dc7628-5ea0-4ed8-9aa7-2a2e68e71175>"],"error":null}
{"question":"What's the connection between latest research on quantum materials & porous media simulations? How these 2 fields use computational methods differently?","answer":"The research approaches in quantum materials and porous media use computational methods quite differently. In quantum materials research, computational methods focus on investigating strongly correlated materials using laser pulses and spectroscopy, with theoretical techniques ranging from first-principles electronic structure methods to many-body perturbation theory. In contrast, porous media simulations concentrate on developing algorithms for generating realistic granular models and solving fluid dynamics equations, particularly using methods like Lattice-Boltzmann simulations to calculate permeability and fluid flow. While quantum materials research emphasizes understanding electronic and magnetic properties at the quantum level, porous media simulations focus on macroscopic transport phenomena and physical properties at larger scales.","context":["|Phone:||+49 711 685-67652|\n|Fax:||+49 711 685-63658|\n|Email:||Thomas.Zauner _at_ icp.uni-stuttgart.de|\nInstitute for Computational Physics\nThe general field of my research is the computational investigations of natural porous media. A typical example of such a natural porous medium is sandstone. It consists of very small consolidated grains that form a solid backbone but, due to the space between individual grains, is permeable for many fluids. Insight into physical transport phenomena of natural porous media, such as flow rates, trapping behavior and prediction of effective transport coefficients, has always been of great practical interest for the oil and gas industry and has currently drawn public attention in the context of CO2 sequestration. Porous materials are also used for many industrial applications, for example in filtration processes, as catalyst in chemical reactions or building materials with specially physical properties.\nThe scientific problems that I address in my work are:\n- Developing algorithms that can generate realistic and macroscopic computer models for granular porous media. These models will be made of many hundred millions of individual grains whose positions, orientations and sizes must fulfill certain correlations and restrictions. The associated large computational demand requires highly efficient parallelized computer algorithms.\n- Identifying and evaluating numerical methods that allow accurate and predictive calculations of physical properties of porous media. These calculations typically require numerically solving partial differential equations within a chosen discretization scheme and at a specific resolution or order of accuracy.\nRealistic computer models for granular porous media\nIn the reconstruction procedure we use a dense packing of spheres is created. The packing must fulfill given restrictions such as density, overlap, size distribution and others. The spheres are later substituted by more complex geometrical objects determined by the shapes of the grains used in the specific model. One option is to use polyhedrons as grain shapes. The resulting model then is a continuum model in the sense that the geometry is defined by a points in the continuum and polyhedrons with analytical defined shapes, sizes and orientations. An example of such a model for Fountainebleau sandstone is shown in image 1. Such a continuous computer model can then be discretized at any desired resolution, see image 2, for further analysis and computer simulations.\nA porous mediums permeability is an example of a physical transport parameter. It describes the mediums ability to transmit fluid flow through it. Using Darcy's law the permeability of a porous medium can be calculated from the velocity and corresponding pressure field on the porescale. These fields are solutions of hydrodynamic partial differential equations describing fluid flow on the porescale. Many different numerical methods that solve different hydrodynamic equations can be used to obtain the velocity and pressure field. Possibilities are Navier-Stokes simulations and Lattice-Boltzmann simulations. Our Lattice-Boltzmann implementation, for example, numerically solves the Boltzmann equation using a transient explicit finite-difference scheme. The simulation is ran until a stationary solution has been reached. From this stationary solution the hydrodynamic fields can be calculated. Simulations in stochastic geometries often require very large system sizes to obtain representative solutions. In our investigations, typical sizes of the discretized system are in the range 1024³-4096³ voxel.\n A. Narvaez, T. Zauner, F. Raischel, R. Hilfer, J. Harting, “Quantitative analysis of numerical estimates for the permeability of porous media from lattice-Boltzmann simulations”, Journal of Statistical Mechanics, P11026, 2010, Preprint\n S. Grottel, G. Reina, T. Zauner, R. Hilfer, T. Ertl, \"Particle-based Rendering for Porous Media\", SIGRAD, 2010, Preprint\n J. Harting, T. Zauner, R. Weeber, and R. Hilfer Numerical modeling of fluid flow in porous media and in driven colloidal suspensions in High Performance Computing in Science and Engineering '08, edited by W. Nagel, D. Kröner, M. Resch, Springer ,2008. Preprint","Condensed Matter and Materials Physics\n- Quantum Materials\n- Non-equilibrium Magnetic Materials\n- Novel sp2-Bonded Materials and Related Nanostructures\n- Van der Waals Heterostructures: Novel Materials and Emerging Phenomena\n- Theory of Materials\n- Scalable Computational Tools for Discovery and Design\n- The Materials Project\n- SciDAC: Topological and Correlated Matter\n- Characterization of Functional Nanomachines\n- Electronic Materials\n- Sub-wavelength Metamaterial Physics and Applications\n- Mechanical Behavior of Advanced Materials\nThis program seeks to investigate the properties of strongly correlated materials by shining light onto them. Intense laser pulses have been used to restructure the free-energy landscape of these materials, and to generate rapid switching between the various forms of order deriving from the strong correlation. State of the art high harmonic generation photoemission spectroscopy and time- and spin- resolved photoemission spectroscopy systems have been developed. The future goal is to stimulate materials at selective excitations with higher intensity laser pulses to establish a mean for manipulation and control of materials properties.\nM. Yi, M. Wang, A.F. Kemper, S.-K. Mo, Z. Hussain, E. Bourret-Courchesne, A. Lanzara, M. Hashimoto, D.H. Lu, Z.-X. Shen, and R.J. Birgeneau. Bandwidth and Electron Correlation-Tuned Superconductivity in Rb0.8Fe2(Se1-zSz)2. Phys. Rev. Lett. 115, 6403 (2015).\nM.G. Kim, M. Wang, G.S. Tucker, P.N. Valdavia, D.L. Abernathy, S.Chi, A.D. Christianson, A.A. Axcel, T. Hong, T.W. Heitmannn, S. Ran ,P.C. Canfield, E.D. Bourret-Courchesne, A. Kreyssig, D.H. Lee, A.I. Goldman, R.J. McQueeney, and R.J. Birgeneau. Spin Dynamics near a putative quantum critical point in Cu-substituted BaFe2As2 and its relation to high temperature superconductivity. Phys. Rev. B 92, 214404 (2015).\nA.K. Yadav, C.T. Nelson, S.L. Hsu, Z. Hong, J. D. Clarkson, C. M. Schlepüetz, A. R. Damodaran, P. Shafer, E. Arenholz, L.R. Dedon, D. Chen, A. Vishwanath, A. M. Minor, L.Q. Chen, J.F. Scott, L. W. Martin, and R. Ramesh. Observation of polar vortices in oxide superlattices. Nature 530, 198 (2016).\nThe program focuses on fundamental science of non-equilibrium magnetic materials and phenomena with emphasis on those enabled by Interfaces and Spin-Orbit-Coupling (SOC). It encompasses design, fabrication, measurement, and modeling of static and dynamic magnetic properties of thin film materials exhibiting strong spin-orbit interactions and inversion symmetry breaking due to interfaces, particularly dynamics and thermodynamics of spin accumulation, skyrmions and spin textures. The research addresses three interrelated sub-projects: i) static and quasi-static novel thin film spin structures such as skyrmions and chiral magnetic textures; ii) strong spin accumulation and consequent control of magnetization created by interfaces between ferromagnet/non-magnet with strong spin-orbit coupling; and iii) highly non-equilibrium magnetic states produced in these heterostructures and studied through electron, optical and x-ray pump/probe techniques at time scales ranging from nsec to fsec. Through strategic choice of materials and utilizing a collective expertise in growth, magnetic, electrical, optical, and thermodynamic characterization, spectromicroscopy, and theoretical modeling, the NEMM team aims to control the strength of the critical underlying, sometimes competing, interactions (spin-orbit, exchange, single ion anisotropy, Dzyaloshinskii-Moriya, Coulomb, disorder) within and between dissimilar materials in proximity to each other, enabling development of models for resulting magnetic states and their dynamics. Theoretical modeling will provide a basis for understanding the structure (atomic, electronic, magnetic) and dynamics (electron and spin transport, magnetization response) of these states, and will guide the exploration of the multidimensional space of new material systems.\nN. Roschewsky, T. Matsumura, S. Cheema, F. Hellman, T. Kato, S. Iwata, S. Salahuddin. Spin-orbit torques in ferrimagnetic GdFeCo alloys. Appl. Phys. Lett. 109, 112403 (2016).\nM. Charilaou, C. Bordel, P.-E. Berche, B. B. Maranville, P. Fischer, F. Hellman. Magnetic properties of ultrathin discontinuous Co/Pt multilayers: comparison with short-range ordered and isotropic CoPt3 films. Phys Rev B 93 224408 (2016).\nX. Shi, P. Fischer, V. Neu, D. Elefant, J. C.T. Lee, D. A. Shapiro, M. Farmand, T. Tyliszczak, H.-W. Shiu, S. Marchesini, S. Roy, S. D. Kevan. Soft x-ray ptychography studies of nanoscale magnetic and structural correlations in thin SmCo5 films. Appl Phys Lett 108, 094103 (2016).\nThe sp2 program investigates, both theoretically and experimentally, sp2-bonded structures which include carbon nanotubes, graphene, nanowires, onions, fullerenes, nanocrystals, hybrid structures, non-carbon nanomaterials (including BN), and nanococoons. Interest is in the design, synthesis, characterization, and application of sp2-bonded materials whose dimensions range from 1-100 nm. This program has three major thrusts: The first, fundamentals, focuses on theoretical predictions of new stable structures, theoretical and experimental examinations of intrinsic electronic, magnetic, and mechanical responses, transport measurements (electrical resistivity, thermal conductivity, isotope effects, Raman, photoemission spectroscopy, TEM, STM), and mechanical properties and tensile strength. The second focus is on functionalized nanosystems, where two or more distinct nanostructures are brought together and allowed to interact. Here, focus is on methodologies to integrate nanosystems comprised of graphene, hBN, nanotubes, and other nanoparticles interfaced to each other or to complementary nanostructures and materials. The third and final thrust is on growth of nanostructures. Novel synthesis methods are explored for non-equilibrium growth of sp2-based and other nanoscale materials. This program also seeks to develop specialized instrumentation for synthesis, characterization, and applications.\nS. Onishi, M. Moreno Ugeda, Y. Zhang, Y. Chen, C, Ojeda-Aristizabal, H. Ryu, S.-K. Mo, Z. Hussain, Z.-X. Shen, M. Crommie, and A. Zettl. Selenium capped monolayer NbSe2 for two-dimensional superconductivity studies. Physica Status Solidi B (2016).\nJ. Lee, D. Wong, J. Velasco Jr, J. F. Rodriguez-Nieva, S. Kahn, H.-Z. Tsai, T. Taniguchi, K. Watanabe, A. Zettl, F. Wang, L. S. Levitov, and M. F. Crommie. Imaging electrostatically confined Dirac fermions in graphene quantum dots. Nature Phys. (2016).\nY. Zhang, M. M. Ugeda, C. Jin, S.-F. Shi, A. J. Bradley, A. Martín-Recio, H. Ryu, J. Kim, S. Tang, Y. Kim, B. Zhou, C. Hwang, Y. Chen, F. Wang, M. F. Crommie, Z. Hussain, Z-X. Shen, and S.-K. Mo. Electronic Structure, Surface Doping, and Optical Response in Epitaxial WSe2 Thin Films. Nano Lett. 16, 2485 (2016).\nC. Jin, J. Kim, J. Suh, Z. Shi, B. Chen, X. Fan, M. Kam, K. Watanabe, T. Taniguchi, S. Tongay, A. Zettl, J. Wu, F. Wang. Interlayer electron-phonon coupling in WSe2/hBN heterostructures. Nature Physics (2016).\nThe goal of this program is to understand and compute material properties and behaviors, covering a range of systems that include complex materials, nanostructures, superconductors, reduced-dimensional materials, and strongly correlated electron systems. The major objectives include studies on: superconductivity and mechanisms; excited states in novel materials and nanostructures; methodology developments; symmetry and topological phases of matter; and transport phenomena. A variety of theoretical techniques are employed, ranging from first-principles electronic structure methods and many-body perturbation theory approaches to new conceptual and computational frameworks suitable for complex materials/nanostructures and strongly interacting systems. Close collaboration with experimentalists is maintained. Equally important is the development of computational methods suitable for increasingly complex materials, reduced dimensional systems, and strongly correlated materials.\nJ Ma, ZF. Liu, JB Neaton, LW Wang. The energy level alignment at metal–molecule interfaces using Wannier–Koopmans method. Appl. Phys. Lett. 108, 262104 (2016).\nJ. I. Mustafa, S. Coh, M. L. Cohen, and S. G. Louie. Automated construction of maximally localized Wannier functions for bands with nontrivial topology. Phys. Rev. B 94, 125151 (2016).\nS. Coh, D.-H. Lee, S. G. Louie, and M. L. Cohen. Proposal for a bulk material based on a monolayer FeSe on SrTiO3 high-temperature superconductor. Phys. Rev. B 93, 245138 (2016).\nThis is a multidisciplinary program of physical scientists, applied mathematicians, and computational scientists whose goal is to develop and implement new first-principles methods/theories to predict excited-state phenomena in materials. Advanced algorithms and many-body theory techniques are employed to compute electron excitations, optical spectra, transport properties, and other excited-state properties/processes. Methods and algorithm advances are incorporated into the BerkeleyGW package, an open-source code available freely to the community.\nF Bruneval, T Rangel, S M Hamed, M Shao, C Yang, and JB Neaton. Many-body perturbation theory software for atoms, molecules, and clusters. Comput. Phys. Commun. 208, 149 (2016).\nM. Y. Shao, L. Lin, C. Yang, F. Liu, F. H. Da Jornada, J. Deslippe, and S. G. Louie. Low rank approximation in G0W0 calculations. Sci China Math 59, 1593 (2016).\nM. Y. Shao, F. H. da Jornada, C. Yang, J. Deslippe, and S. G. Louie. Structure Preserving Parallel Algorithms for Solving the Bethe-Salpeter Eigenvalue Problem. Linear Algebra Appl. 488, 148 (2016).\nThe Materials Project aims to accelerate materials discovery and education through advanced scientific computing and innovative design methods, scale those computations to cover all known inorganic compounds, and disseminate that information and design tools to the larger materials community. Specifically the Project is achieving this by:\n- Providing unprecedented data and materials design tools as well as comprehensive capabilities for scientists to share their processes and results.\n- Leveraging high-throughput calculations, state-of-the-art electronic structure methods as well as novel data mining algorithms for surface, defect, electronic and finite temperature property predictions for tens of thousands of materials to yield an unparalleled materials design environment.\n- Demonstrating the derived data and infrastructure in the design of novel functional electronic materials: photovoltaics, thermoelectrics, transparent conductors and photocatalytic materials.\nMaarten de Jong, Wei Chen, Randy Notestine, Kristin Persson, Gerbrand Ceder, Anubhav Jain, Mark Asta, and Anthony Gamst. A Statistical Learning Framework for Materials Science: Application to Elastic Moduli of k-nary Inorganic Polycrystalline Compounds. Sci Rep. 6:34256 (2016).\nTran, R.; Xu, Z.; Radhakrishnan, B.; Winston, D.; Sun, W.; Persson, K. A.; Ong, S. P. Surface energies of elemental crystals. Sci. Data, 3:160080 (2016).\nJain, A., Shin, Y. & Persson, K. A. Computational predictions of energy materials using density functional theory. Nat. Rev. Mater. 1, 15004 (2016).\nThis is a BES-funded SciDAC program that seeks to improve the effectiveness of two methods, QMC and DMRG, widely used for studies of superconducting or topological quantum materials. These are both full-Hilbert-space methods called out in the recent Basic Research Needs for Quantum Materials. For DMRG, a goal is to develop efficient parallelizations for large-scale DOE machines. QMC work will understand competing phases in high-temperature superconductors by sign-problem-free simulations.\nTo be added later...\nThe goal of this program is to understand the fundamental principles of nanomachine systems and to apply those principles toward the creation of new molecule-based nanomachines capable of converting energy into directed mechanical action at the nanoscale. The program seeks to understand the microscopic mechanisms underlying nanomechanical energy conversion in both synthetic and naturally occurring nanomachines that operate in different dissipative environments. Establishing new techniques to achieve controlled \"bottom-up\" fabrication of molecular nanostructures at surfaces is an important component of this program.\nP. Rodriguez-Aliaga, L. Ramirez, F. Kim, C. Bustamante, and A. Martin. Substrate-translocating loops regulate mechanochemical coupling and power production in AAA+ protease ClpXP. Substrate-translocating loops regulate mechanochemical coupling and power production in AAA+ protease ClpXP. Nat. Struct. Mol. Biol. 23 (2016).\nA. Riss, A. Perez Paz, S. Wickenburg, H.-Z. Tsai, D.G. de Oteyza, A.J. Bradley, M.M. Ugeda, P. Gorman, H.S. Jung, M.F. Crommie, A. Rubio, and F.R. Fischer. Imaging Single-Molecule Reaction Intermediates Stabilized by Surface Dissipation and Entropy. Nat. Chem. 8, 678-683 (2016).\nH.-Z. Tsai, A.A. Omrani, S. Coh, H. Oh, S. Wickenburg, Y.-W. Son, D. Wong, A. Riss, H.S. Jung, G.D. Nguyen, G.F. Rodgers, A.S. Aikawa, T. Taniguchi, K. Watanabe, A. Zettl, S.G. Louie, J. Lu, M.L. Cohen, and M.F. Crommie. Molecular self-assembly in a poorly screened environment: F4TCNQ on graphene/BN. ACS Nano 9, 12, 12168 (2015).\nElectronic Material Program (EMAT) discovers and creates semiconductors of novel composition and morphology for energy applications by removing chemical and physical constraints that limit materials performance and growth. The program has three thrust areas:\n- Thrust 1: Defect control in 2D semiconductors\n- Thrust 2: Interface enabled band engineering\n- Thurst 3: New synthesis modes for qunatum membranes\nCommon to these research themes is a synthetic strategy that allows to control structure and phase transitions at the nanoscale. In the case of quantum membranes, the ability to make and control free-standing 2-D semiconductors enables study of the interplay of quantum confinement and surface/interface properties with electronic structure and carrier transport at a fundamental level. Focus has been on exploring their electrical and optoelectronic properties at the ideal limit by repairing/passivating their defects. The control of materials composition via highly non-equilibrium synthesis techniques allows to tune band structures and interface properties on command. By exploiting new insights in the nanoscale control of phase transitions, one can synthesize new functional structures. As an example, single-crystalline growth of InP with user-defined shapes and geometry, from a few nm to 10’s of μm in lateral dimensions, on amorphous substrates is demonstrated for the first time.\nChen, K.; Kapadia, R.; Harker, A.; Desai, S.; Kang, J. S.; Chuang, S.; Tosun, M.; Sutter-Fella, C. M.; Tsang, M.; Zheng, Y.; Kiriya, D.; Hazra, J.; Madhvapathy, S. R.; Hettick, M.; Chen, Y.-Z.; Mastandrea, J.; Amani, M.; Cabrini, S.; Chueh, Y.-L.; Ager, J. W.; Chrzan, D. C.; Javey, A. Direct growth of single-crystalline III-V semiconductors on amorphous substrates. Nat. Comm. 7, 10502 (2016).\nJaquez, M.; Yu, K. M.; Ting, M.; Hettick, M.; Sanchez-Royo, J. F.l; Welna, M.; Javey, A.; Dubon, O. D.; Walukiewicz, W. Growth and characterization of ZnO1−xSx highly mismatched alloys over the entire composition. J. Appl. Phys. 118, 215702 (2015).\nAmani, M.; Lien, D.-H.; Kiriya, D.; Xiao, J.; Azcatal, A.; Noh, J.; Madhvapathy, S. R.; Addou, R.; Santosh, KC; Dubey, M.; Cho, K.; Wallace, R. M.; Lee, S.-C.; He, J.-H.; Ager, J. W.; Zhang, X.; Yablonovitch, E.; Javey, A. Near-unity photoluminescence quantum yield in MoS2. Science 350, 1065-1068 (2015).\nThis program explores a new class of photonic materials, metamaterials, by designing artificial atoms and molecules and their interactions. It investigates the novel physics of metamaterials to uncover unprecedented materials properties beyond that of natural materials.\nRamezani, Y. Wang, E. Yablonovitch & X. Zhang. Unidirectional Perfect Absorber. IEEE Journal of Selected Topics in Quantum Electronics, 22 (2016).\nK. L. Tsakmakidis, R. W. Boyd, E. Yablonovitch & X. Zhang. Large spontaneous-emission enhancements in metallic nanostructures: towards LEDs faster than lasers. Optics Express, 24, 16 (2016).\nH. Ramezani, M. Dubois, Y. Wang, X. Zhang. Directional Excitation Without Breaking Reciprocity. New Journal of Physics (2016).\nThe attainment of strength and toughness is a vital requirement for structural materials; unfortunately, these properties are generally mutually exclusive. It is the lower strength, and hence tougher, materials that find use for most safety-critical applications where premature fracture is unacceptable. Accordingly, the development of strong, tough materials has traditionally been a compromise between hardness vs. ductility. The aim of this program is thus to seek the fundamental strategies to solve this “conflict” by defining basic scientific principles underlying the development of damage-tolerance in structural materials suitable for the strategic missions of transportation, energy conservation and creation. Our approach involves understanding the scientific origins of damage-tolerance, principally in advanced multiple-element metallic alloys, principally bulk-metallic glasses and high-entropy alloys, where the salient mechanisms involve a balance of intrinsic vs. extrinsic toughening. A second approach involves using concepts of natural hierarchical design to examine the development of biomimetic hybrid ceramics. We include 2D materials, such as graphene, in this work. Our overall hypothesis is that by identifying the salient strengthening and toughening mechanisms at multiple length-scales, and by mimicking natural structural architectures, all guided by theoretical modeling at atomistic to continuum levels, we can develop the scientific foundations for unique structural materials with unprecedented levels of damage-tolerance. In all cases, our focus is on the scientific interplay between the individual nano/micro-mechanisms that contribute to strength and toughness, that of plasticity and crack-tip shielding, noting that these phenomena can originate at very different structural length-scales.\nB. Gludovatz, A. Hohenwarter, K. V. S. Thurston, H. Bei, Z. Wu, E. P. George, R. O. Ritchie. Exceptional Damage-Tolerance of a Medium-Entropy Alloy CrCoNi at Cryogenic Temperatures. Nature Communications, vol. 7, pp. 10602 (2016).\nH. Bai, F. Walsh, B. Gludovatz, B. Delattre, C. Huang, Y. Chen, A. P. Tomsia, R. O. Ritchie. Bioinspired Hydroxyapatite/Poly(Methyl Methacrylate) Composite with Nacre-Mimetic Architecture by a Bidirectional Freezing Method. Advanced Materials, vol. 28 (1), pp. 50-56 (2016).\nH. Bai, Y. Chen, B. Delattre, A. P. Tomsia, R. O. Ritchie. Bioinspired Large-Scale Aligned Porous Materials Assembled with Dual Temperature Gradients. Science Advances, vol. 1 (11), pp. e1500849 (2015)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:1c025da0-6fb2-415d-99d1-47cf23c37bca>","<urn:uuid:ac0cbd19-f6eb-44c6-a882-0015da831bba>"],"error":null}