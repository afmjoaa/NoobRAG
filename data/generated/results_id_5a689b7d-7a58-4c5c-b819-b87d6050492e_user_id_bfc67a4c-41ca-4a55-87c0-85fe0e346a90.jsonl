{"question":"Can someone explain how battery hens adapt to free-range life, and what are the key warning signs of stress or aggression to watch for during this transition? 📋","answer":"Battery hens initially find the transition to free-range life challenging as they're creatures of habit. At first, they may not understand basic behaviors like scratching the ground, but within a few days they begin exploring their new environment, and within a few months, they look like normal hens with regrown feathers. During this transition, watch for signs of stress or aggression such as the bird flashing its eyes and contracting pupils quickly, leaning forward with its head to bite, leaning away from hands, or making unhappy vocalizations. These behaviors indicate the bird may be fearful, overexcited, or experiencing true or displaced aggression. Birds may also bite if they feel uncomfortable with new people or situations.","context":["Biting and Screaming in Birds\nWhy does my bird people?\nA bird may bite out of fear, excitement, true aggression, or displaced aggression, and there are warning signs that every bird owner should be aware of. Beware of a bird flashing his eyes and contracting the pupils quickly, as he is likely overexcited and about to bite! Birds can become overly excited if a new bird is close by and bite the nearest person. Birds may bite their owner if a human they do not like is approaching (displaced aggression). Some birds do not like the person approaching or trying to pick them up.\nHow can I manage my bird’s biting?\nWhen your bird bites, try to remain calm without reacting. If you react, your bird may learn that biting gives him control or dominance, and he may learn to bite more. Even negative reactions, such as yelling at or striking your bird, can inadvertently positively reinforce the biting behavior. If your bird bites while on your hand, slowly put him down and walk away, like giving a time-out to a child.\nNEVER hit a bird! Birds do not respond to physical discipline, which often results in losing trust in their owners and becoming fearful of hands. Wait until he is calm again and will step up on your hand without biting. If your bird continues to bite, stop attempting to pick him up. If he continues to refuse to step up without biting, you will need to regain his trust. Additionally, it is generally recommended not to use gloves to pick birds up. Most birds are afraid of gloves. Instead, use a thick towel over your hand to avoid injury.\n\"If he continues to refuse to step up without biting, you will need to regain his trust.\"\nYou can start by slowly and gradually hand-feeding a favorite treat that is only given in special circumstances. First, offer your bird the treat without asking him to step up. Once he takes the treat several times over several days without attempting to bite, offer the treat so that he must step with one foot onto a perch and eventually with both feet onto the perch to access it. This may take several days or weeks of practice before your bird trusts you enough to step fully on the perch.\nOnce your bird has mastered stepping onto a perch for treats, you can teach him to step up onto your hand to accept a treat similarly. This process can take a long time, so be patient.\nPay attention to your bird's body language to understand when he may prefer some personal space. He may lean forward with his head to bite, lean away from your hand, or make an unhappy vocalization to indicate that he does not want to step up at that time and may attempt to bite. Consult your avian veterinarian for advice on dealing with behavioral problems if you encounter these issues with your bird.\nWhy does my bird scream?\nThis is a common complaint from bird owners and a challenging dilemma. The most talented talker can also be a skilled screamer. Screaming or loud vocalization is a natural way for wild parrots and other birds to communicate with each other in their flock environments. They will also scream if they are alarmed. Birds will vocalize if they are frightened, bored, lonely, stressed, or unwell. Pet birds often vocalize when people are talking loudly, vacuuming, chatting on the phone, or playing music. They may see these times as appropriate for vocalizing back as part of normal flock behavior.\n\"Pet birds often vocalize when people are talking loudly, vacuuming, chatting on the phone, or playing music.\"\nBirds usually scream in the early morning and dusk when they naturally gather in the trees to socialize and eat. Because you are part of the flock, your bird likely wants to communicate. Unfortunately, what may be natural behavior for birds can be annoying to their owners in the confines of their homes.\nHow can I manage my bird’s screaming?\nFirst, know that your bird does NOT understand that it is NOT acceptable to scream and squawk in your house; this is a human problem. Your bird is screaming to communicate or ‘talk’ and to get attention.\nThere are many different approaches to managing this human problem. Often, people make it worse by yelling back at their birds and inadvertently positively reinforcing the screaming. Rushing to the ‘aid’ of a distressed screaming bird or yelling at him gives him the attention he demands. If you run to your bird when he screams, even to reprimand him for screaming, he will quickly learn that it is the way to get the attention he is seeking and will likely continue screaming for it.\nNote: Most birds vocalize when having a seizure, so one must realize that a vocalization that has never been heard before warrants immediate attention.\nSome people cover the cage or turn the lights off their bird screams. This is another form of positive reinforcement for the bird, as it gives him attention, even if just to come to him to cover his cage. Any acknowledgement of screaming, even if negative, should be avoided.\n\"A more effective approach is to reward calm, quiet behavior by offering a favorite treat.\"\nA more effective approach is to reward calm, quiet behavior by offering a favorite treat. Common ‘healthy’ treats are pieces of walnuts, almond sticks, baby carrots, or pieces of coconut. This response positively reinforces quiet behavior. When your bird learns that screaming does not get your attention, he may ultimately decide to stop screaming in favor of displaying quiet behavior that does.\nOf course, birds are not machines, and there may be times when even the best-behaved bird screams inappropriately. Overall, however, positive reinforcement training of acceptable behaviors is the best way to deal with problem behaviors in birds.\nMany good training books and videos about positive reinforcement training for birds are available for reference. You can also consult your avian veterinarian for help and possibly a referral to an avian behaviorist.\n© Copyright 2022 LifeLearn Inc. Used and/or modified with permission under license.","If you are new to chickens or have simply never rehomed battery hens before then these Frequently Asked Questions about rehoming ex-batts are for you! From the practicalities of the size coop and run you need, what to buy before you get them and what to feed them to general care for them when you get them. This is a good starting place if you’re thinking of getting a few ex-batts…\nHow difficult is it to look after ex-battery hens?\nEx-Battery hens do require a little bit of extra care and attention during the initial couple of months that you get them until they have adapted to their new life and routine. At first, they may not put themselves to bed (but this is often the case with new chickens anyway) and may not lay their eggs in the nest boxes but with a little extra attention during the first couple of months, they will soon learn the routine. Once they have settled in, they aren’t much trouble at all and are easier to look after than many pure breeds of chickens.\nYou will need to make sure you have the time to give them fresh water and food every day and make sure you can lock them up at night safe from foxes. Automatic door keepers are a very worthwhile investment.\nAutomatic Door Closer\nYou can view automatic door keepers for sale here.\nKeeping chickens, like any animal does give you an extra tie if you want to go away. You will need to make suitable arrangements to make sure your hens are looked after properly when you are on holiday but neighbours and friends are usually willing, especially if they get to take home some eggs!\nWhat condition will the hens be in when I get them?\nSome battery hens can be poorly feathered when they come out from the farms, a few can be very sparsely feathered and can look almost ‘oven ready’ but the average hen will have just a few bare patches.\nHens will usually feather up within a few months and will then look far more like ‘normal’ hens. Hens will be pretty exhausted when you get them (as will the rescue co-ordinators after a pretty gruelling day getting the hens out and rehoming them).\nHow easily do battery hens adapt to a free range life?\nIt is a difficult time for a battery hen when she comes out of a cage, even though it is for a much better life. Hens are creatures of habit and anything different can cause them to be stressed and a little nervous. There are many things they will not understand at first, like being able to scratch the ground beneath their feet. This is one of the most natural behaviours for a hen though and will soon return. It is one of the most amazing sights seeing hens taking their first few steps out of their house. Within a few days they will be moving around, exploring their new home and enjoying a free range life. After a few weeks, their new feathers will start to grow, and within a couple of months, they will soon look like ‘normal’ hens again.\nHow many hens should I rehome?\nMost people will keep a minimum of 3 chickens. The main reason for this is that hens are social creatures that live in flocks. Battery hens are frail and if you lose one then you will still have 2, the minimum number of birds to keep so that they have company of their own kind. With ex-battery hens, they have had a hard life and there is an increased risk of losing one before they have had a chance to recover and settle in. It’s for this reason that we would really recommend you try to rescue a minimum of 4 hens although 3 is of course the bare minimum.\nWhat size coop and run do I need for ex-battery hens?\nWhen you buy a chicken house, the manufacturer will usually state how many chickens they can house. Some manufacturers will always go for a higher number than is comfortable so it’s always a good idea to stock your house with less hens than is recommended. The main requirement is sufficient perch space for the hens because normally they will be outside during the day. You need to allow 30cm of perch space per hen to make it comfortable for them. It is good to be able to have enough space in the chicken house to hang a feeder if you don’t have a covered run since you will want to keep their food dry and away from the wild birds who will soon empty an open feeder left outdoors. There is some guidance on cheap chicken houses here but do remember, you get what you pay for when it comes to wooden chicken houses.\nChicken runs come in many sizes but the more space you give the hens the better. If you have a very small run, they will get bored quickly and will soon pick up bad vices like feather pecking. There is more guidance on chicken run size and keeping chickens in small spaces in this article.\nWhat do I need to buy before I get my ex-batts?\nBefore you pick up your hens, you will need to get the following:\nA secure chicken house and run. Foxes are the number one predator to protect against but Badgers will also take chickens from time to time. Battery hens are used to sleeping on the floor so remove perches in their house for the first few weeks. If they do climb up on a perch, they can damage their bones when jumping off so exercise caution with perches until they are settled in.\nFood and water containers. Hang these inside their house at first so that they are close to the hens. They are not used to having to go far for their food or water so make sure they are within easy reach of your new girls. Some water containers come with a handle that allows them to be hung from above (hanging basket chains with a nice big hook on the end are useful for this) but remember you may not have somewhere to hang them once they eventually move outside. Some containers have plastic feet that keep them off the ground which are useful for this purpose.\nClick here to see a range of plastic water containers.\nDry layers mash (not layers pellets) – this is what battery hens are fed. In time, you can slowly change their diet to layers pellets if you wish but wait until they have settled in for a couple of months first. Alternatively, Smallholder Feeds make a special feed for Ex-Battery Hens that helps promote weight gain and feather growth and has been specially designed for the nutritional needs of the ex-battery hen. We have heard nothing but praise from this feed from both rehomers and rehoming charities.\nPoultry Grit – chickens don’t have teeth of course but do have a gizzard that grinds their food down using insoluble grit. Most free range hens will pick up enough grit from grazing but it is always wise to provide grit just in case. Soluble grit (Oystershell) can be provided mixed with insoluble (flint) grit. This provides calcium for the hens to produce strong egg shells. Commercial battery hens often have brittle bones since the calcium from their bones is taken to produce egg shells. Most commercial feeds these days provide sufficient calcium but again, it is a good idea to provide Oystershell grit ad-lib. A hen can then take it if she needs it. There is more information here on the types of poultry grit.\nDried cat food – yes, that’s right – cat food. Feathers are 80% protein. Chickens that are growing new feathers benefit from the animal protein in cat food so crushing up some dried cat food is an old poultry keepers trick that is used to help chickens through their moult. Do not use dog food because the protein in dog food is derived from cereals.\nA vitamin drink / supplement. Ex- battery hens will benefit from a vitamin supplement added to their drinking water or food to get back into condition. There are many different supplements available on the market, some more expensive than others. Whatever you chose to buy, it will not be wasted since birds can benefit from these vitamin supplements during the annual moult or during the colder winter months when greens are in short supply. Most of these come as a liquid that is diluted down and added to drinking water but there are a few that come as a powder and can be added to drinking water or to a wet mash mixture.\nA.C.V. – Apple Cider Vinegar. This is remarkably good for chickens in many ways but the best thing is scientific tests have shown it actually helps them to cope with stress and changes to their environment.\nApple Cider Vinegar\nYou can buy Apple Cider Vinegar here.\nWhat should I feed my ex-batts?\nBattery hens are used to being fed dry layers mash. Changes are stressful for chickens and ex-battery hens have enough to deal with coming out of their cages into a completely new environment so it is important for you to be able to continue feeding them layers mash until they have settled in for a few weeks, after which, you can gradually change their feed over to pellets if you prefer.\nTo digest their food, chickens must always have a clean / fresh supply of water at all times. During hot weather, this is particularly important when they will drink more. Chickens do not sweat and can only lose heat from their bodies by panting and drinking water that is cooler than their body’s temperature, it is for this reason their water container should be kept in the shade during very hot weather. Don’t forget chickens also need grit in order to digest their food correctly so they should always have (insoluble) flint grit available to them at all times for digestion. Oystershell grit or soluble grit is also important as this contains calcium which helps them source the calcium required to produce eggs. Chickens also love most fresh green vegetables and fruit. Corn on the cob, cabbage, broccoli, apple, strawberries, grapes, tomatoes are all appreciated and you can hang some of these in their run for them to enjoy.\nThere is more in-depth information on feeding chickens here if you are concerned.\nWill ex-battery hens lay lots of eggs?\nBattery hens are the result of many years of breeding and selection. They have been bred to lay the largest possible number of eggs in their first couple of years of their life. If you want ex-battery hens solely for eggs, then they aren’t really for you. Ex-battery hens should still have a large number of eggs to lay for you but keep in mind that commercially they are ‘spent’ hens and like any chicken will lay less and less every year, sometimes with a thinner egg-shell as they get to 3 or 4 years old. A very small number of hens that are rescued do not lay although most will usually start laying very quickly. Like all hens, expect them to lay most of their eggs during the spring and summer months and expect them to stop laying when they go into moult in the late autumn and when the daylight hours are reduced over the winter.\nWhere can I buy ex-battery hens?\nThere are a number of charities / rescue organisations in the U.K. that rehome ex-battery hens. Most of these require you to register with them so that they can allocate some hens to you from their next rescue in that area. They usually do not charge a set fee for hens, but they will ask you for a donation to help support their work and running costs. We would encourage you not to go straight to the farmer. The charities will check birds over for you and will often trim overgrown nails or beaks before giving your birds to you. A list of the rehoming charities can be found at the bottom of our article: Ex-Battery Hens For Sale\nDo I need to register with DEFRA?\nIn the UK, you do not need to register with DEFRA unless you are keeping 50 or more poultry. When you count your birds, you must remember ‘poultry’ doesn’t just mean chickens, you must include ducks, geese, guinea fowl, quail and so on. There is more information on the laws that concern chicken keepers on this page here.\nFinally… Once Ex Bats have settled in, they are just like other chickens and can be treated as such. There are lots of articles in our keeping chickens section that can help you to look after their needs. Most of all though, have some fun keeping them!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:d2858c06-931f-482b-8721-02995536bb6f>","<urn:uuid:3b092688-d933-4545-aa04-f5a93d73d4ed>"],"error":null}
{"question":"What is the basic working principle of a jaw crusher in material processing?","answer":"The working principle of jaw crushers involves breaking materials in a crushing chamber through compression. Powered by a diesel or gas motor, materials are pushed inside the chamber from the top opening, where they are compressed between a stationary and a movable surface. Once crushed, the materials are released through the bottom opening.","context":["Index Terms - Bottle Crusher, Can Crusher, Crusher, Crusher Design, Jaw Crusher I. INTRODUCTION Plastic recycling is the process of recovering scrap or waste plastic and reprocessing the material into useful products. Since plastic is non-biodegradable, Concept Development We have come up with a concept of designing a crusher in such a way.Concepts of jaw crusher in iron ore industries Products. As a leading global manufacturer of crushing, grinding and mining equipments, we offer advanced, reasonable solutions for any size-reduction requirements including, Concepts of jaw crusher in iron ore industries, quarry, aggregate, and different kinds of minerals.\nJaw crusher design jaw crusher type. jaw crusher designed specifically for crushing iron ore minerals. liming can supply you PE series iron ore jaw crusher and JCE type A New Design Concept For A Continuous Double-toggle Jaw.The right jaw design can make a big difference for your jaw crusher. Our customers have proven that the right jaw plate design can make a dramatic difference, depending on the application. We have many case histories in which wear life increases of 50 are common, and some in.\nDec 09, 2014 • Types Of Crushers • Jaw crushers • Gyratory crushers • Difference • Difference between these jaw crushers is the feed rate and product rate and design, we are, however, going to talk about jaw crushers in this particular presentation. 5. • Jaw Crusher • Jaw crusher is a type of crusher which produces coarse particle.Jan 01, 1990 New ideas in primary Jaw Crusher design — Design and manufacture of 66-inch 84-inch (167.64-cm 213.36-cm) Jaw Crusher utilizing open feed throat concept, power savings and automation features.\nJaw crusher is one of the main t ypes of primary crushers in a mine or ore processing plant. The size of a jaw crusher is designated by the rectangular or square opening at the top of the jaws.A theoretical design concept of a continuous Blake type jaw crusher has been elucidated in this article, where forward and return strokes of the moving jaw can both be utilised for crushing. It may be presumed that the design concept as presented here if put into developing a continuous type of crusher, may deliver nearly twice the output of.\nWorking principles of jaw crusher pdf. The working principle of jaw crusher would help you to understand the . The News Pumice Cone Crushing Machine . pumice crusher machine design pdf. Jaw Crusher Working Principle 911 Metallurgist. Jul 2, 2018 A sectional view of the single-toggle type of jaw crusher is shown below.Jun 07, 2020 The jaw crusher is designed by using the jaw crusher plates, which is used to break the different type’s stones into small pieces because these plates have done most of the work in the system. Jaw crusher plates are playing an important role in the jaw crusher. This curved jaw design has a high production capacity.\nJaw crushers These are the oldest type of and the most commonly used crushers in use and have changed little from the original design. In Jaw Crusher the feed is compressed between a stationary and a movable surface. A few of common types of Jaw crushers, in use, are described below • Double toggle jaw crusher • Single toggle jaw crusher.Jun 26, 2020 Design Of Crusher Foundation Fact Jeugd Noord. Jaw crusher concrete foundation design jaw crusher concrete foundation designoundation jaw crusher youtubepr 10 2017 details contactp jaw crushers lay foundation for mining recently the professional r d rusher wikipedia crusher is a machine designed to reduce large rocks into smaller rocks gravel or rock dust in a mobile.\nJaw Crushers With Types And Differences. jaw crushers with types and differences. july 20, 2021 by ames. jaw crushers are used as primary crushers, or the first step in the process of reducing rock. they crush primarily by using compression. the distinctive feature of this class of crusher is the two plates which open and shut like animal jaws. the jaws are set at an acute angle to.Design Single-toggle jaw crushers are characterized by the swing jaw being suspended directly on the eccentric drive shaft and the lower part of the swing jaw being braced against the crusher frame by means of a toggle plate. The kinematics of this type of toggle.\nDec 25, 2014 The working principle of the jaw crushers is very simple. Powered by a diesel or gas motor, the jaw crusher brakes materials in a crushing chamber. The materials are pushed inside the chamber from the top opening and when crushed they are released through the bottom opening. The crushing power of the jaw crushers depends on the size of the chamber.Jaw Crushers are sized by the top opening of the crushing chamber. For example, a 32 x 54 Jaw Crusher measures 32 from jaw die to jaw die at the top opening or gape opening and 54” across the width of the two jaw dies. The narrower bottom opening of the crushing chamber is used to size the discharge material.\nCone Crushers. A Cone Crusher is a compression type of machine that reduces material by squeezing or compressing the feed material between a moving piece of steel and a stationary piece of steel. Final sizing and reduction is determined by the closed side setting or the gap between the two crushing members at the lowest point.Static Jaw Crusher. Rugged, field proven designs and almost 200 years of combined jaw crusher manufacturing experience provide the foundation for our Terex MPS jaw crusher lines. Industry leading brand names like Cedarapids and Jaques provide the platform from which other machines are measured. With rated capacities from 50 tph to well over.\nJaw crusher movement is guid-ed by pivoting one end of the swinging jaw. and an eccentric motion located at the opposite end . 1.3 Different Types of Jaw Crusher . According to the amplitude of motion of the moving face Jaw crusher are classified as follows - 1.3.1 Blake Type Jaw Crusher . Blake type jaw crusher, primary crushers in the mineral.Jaw crusher movement is guided by pivoting one end of the swinging jaw. and an eccentric motion located at the opposite end.  The size of a jaw crusher is designated by the rectangular or square opening at the top of the jaws .For instance, a 22 x 30 jaw crusher has an opening of 22 by 30 , a 46 x 46 jaw crusher has a opening of 46 square.\nGroup, has installed over 10 000 jaw crushers since the 1920s. Today the Nordberg C Series is indisputably the world’s favourite jaw crusher. Where cost-efficient primary reduc-tion of hard, abrasive materials is concerned, C Series crushers represent the highest tech-nical and manufacturing knowledge. All C Series jaw crushers are based on a.Sep 14, 2018 Continuous Jaw Crushers A new Design Concept for a Double-toggle System (A.K. Muk If this is your first visit, be sure to check out the FAQ by clicking the link above. You may have to register before you can post click the register link above to proceed.\nHammer crusher jaw crusher impact crusher manufacture in China. The main products are Jaw Crusher, Impact Crusher, Heavy Hammer Crusher, Vibrating . Our unique design concepts ensure our products the perfect balance among .Bench scale and pilot scale design for comminution circuits Factors influencing the selection of comminution circuits Types and characterisation of crusher equipment and circuit flowsheet Selection and sizing of primary crusher Computer aided design of Jaw Crusher Selection and sizing of secondary and tertiary crushers Optimising the Eccentric speed of cone crusher Selection and sizing of High."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:34044b15-2d6a-4917-99e1-70f883733e27>"],"error":null}
{"question":"How do the water-related challenges compare between the Black Mesa community and the Montezuma's Well settlement?","answer":"The Black Mesa community had to undertake a strenuous daily task of carrying water from the Agua Fria River far below, with young maidens making the rigorous journey each morning carrying large pots on their heads up the steep slope. In contrast, the Sinaqua people at Montezuma's Well had direct access to a remarkable water source - a sinkhole that, despite receiving less than 13 inches of annual rainfall, was continuously replenished with 1.5 million gallons daily from an underground spring.","context":["By Gary Every\nSpecial to the Crier\nThousands of people drive past Black Mesa every day, zipping along Interstate 17 without any idea that atop the steep black cliffs is one of the bigger sets of Native American ruins in the entire state. Some of the stone walls stand fifteen feet high and there are more than forty rooms comprising Black Mesa. One reason the ruins remain so unknown and undisturbed is that you cannot spot the stone walls from the highway. One must negotiate the steep, slippery slope, navigating between agave and prickly pear, before you gain your first glimpse of the ruins. Then you still have more climbing to attain the mesa. From the top you can see everything – the views are tremendous with vistas stretching in every direction.\nBlack Mesa ruins are located just off Interstate 17 on the east side of the road, barely north of Black Canyon City between mile markers 245 and 246. The little known ruins are only ¾ of a mile from the road but be forewarned it is a very steep and slightly treacherous ¾ of a mile – nearly straight up. Because of the steepness and because the mesa is comprised of black stone, this is not a good hike for the hot summer months. Because of the steepness, slippery slope, and plants with thorns, this is not a hike to be taken lightly.\nBut the rewards for those willing to climb Black Mesa are tremendous. The ruins are in amazing shape for being over 800 years old.\nOn one particular winter day I hike with my Lakota friend Clint. We explore the former village of Black Mesa, pondering what life must have been like for the inhabitants eight hundred years ago. One thing we can say for certain is that the view from their home was tremendous in all directions. Inspired, Clint and I circumnavigate the mesa. The top of the mesa is maybe twice the size of a football field and littered with pottery shards. I believe they were farming the top of the mesa, planting corn, beans, and squash. Traditionally, it was the job of the young maidens to journey in the early hours of the morning and return with water, large pots carried atop their heads. What an undertaking that must have been every daybreak, the long and rigorous journey to the Agua Fria River far below and then back up, carrying heavy water vessels.\nAfter we finish circumnavigating the mesa, enjoying the stupendous views, we pause to look at the petroglyphs on the east side of the ruins. The rock art is scattered with only a dozen or so boulders adorned with images, many of them geometric, some stick figure men in various poses, and a few footprints etched into the rock. We stare with curiosity at this ancient penmanship, unable to decipher the meaning while the stone walls of the village tower above us.\nI realize I have a stone in my pocket, a treasure from a recent hike, a fossilized snail shell about the size of a fingernail, embedded in white Kaibab limestone, dating back to the days of the dinosaurs when all of this was the bottom of an ancient ocean. I show Clint my fossil and he begins to tell me a tale of his youth. Usually his stories involve the Detroit Tigers, favorite baseball team for us both, but today his story involves visiting his grandfather’s ranch in the heart of the Lakota Reservation and attending the Sun Dance Ceremony. Usually we talk about books, the language of nerds, but when Clint speaks about his native heritage his speech becomes clipped and vowels rounded with long pauses between sentences as his reservation accent grows.\nAs a boy it was Clint’s job to go to the river. He would gather rocks and small boulders for use in the sweat lodge ceremonies. Once when he was gathering limestone boulders for the Sun Dance Ceremony he pulled a large rock from the stream, discovering himself staring face to face with a bison.\nStartled, Clint dropped the rock with a loud splash but thank god it didn’t break. Clint picked the stone up again and stared eye to eye with the bison face embedded in the rock. The bison never acknowledged his existence but Clint stared with fascination at the mostly complete bison visage with both horns clipped short and all but a trace of jaw visible in the stone. Later, paleontologists would speculate that the bison had died 30 to 40,000 years ago, probably by trying to cross the river while it was frozen and falling through the ice. The bison body on the river bottom was quickly covered by sediment and the process of fossilization began.\nAfter he took the stone back to his grandfather’s ranch, the elders placed it upon the altar for the Sun Dance Ceremonies. The drums beat, the dancers danced, and the singers sang, while the fossilized bison never changed expression. As the ceremonies continued the drums beat and beat until they began to mimic the sound of a million buffalo hooves stampeding across the prairie, the earth thundering with their percussion, echoes of long ago days when vast herds spread across the grasslands.\nI slip my snail fossil back into my pocket.\nThis style of hilltop fortress marks a specific period in archeological history when the Hohokam and Salado began to build large stone hilltop fortresses; such as Black Mesa, Forteleza, and Tuzigoot. Archeologists are still split whether this trend in hilltop architecture was a sign of warfare or was done for religious and aesthetic reasons. What we do know is that those willing to undertake the rigorous hike to the top of Black Mesa are rewarded with the remains of a village pueblo some of whose fifty or so stone walls still stand fifteen feet tall and some incredible views in all four directions.","Throughout the southwest, ancient people left their mark but mysteriously abandoned their villages between 750 and 1350 AD/Current Year (CE). The ruins are beautiful reminder of their heritage.\nRecently, the term Anasazi has been replaced by Ancestral Puebloans, referring to people who lived in the Colorado Plateau roughly 2000 to 700 years ago. Why? Anasazi is a Navajo word that translates to “enemy ancestors”, and the peoples were not necessarily enemies.\nThe Dwellings of Puebloan People\n- Pit-houses were the earliest Ancestral Pueblo residences. Subterranean wood and earth structures with roof entryways were cool in the summer and warm in the winter.\n- Pueblos, above ground stone-masonry structures, later replaced pit-houses.\n- Cliff Dwellings were compact, masonry-walled pueblos located in cliffs. Ladders used to access the dwellings were pulled up for protection.\n- Kivas are round, semi-subterranean structures used for ceremonial purposes or other large gatherings.\nVisiting the Ruins\nPalatki Heritage Site, Sedona.\nCliff dwellings and rock art from the Sinqua people from 1150 to 1350 AD/CE. Sinqua means “without water”.\nPalatki is a Hopi word for Red House; however, the Hopi had no specific name for this site. It was named by an archaeologist.\nSince its discovery in the 1900’s, about 70 to 90% of the structure has disappeared due to looting and misuse. It has since been designated a World Heritage Site.\nMontezuma’s Well, Sedona\nAn oasis/sinkhole with cliff dwellings for the Sinaqua in the 1100’s. Despite less than 13″ of annual rainfall, the well is replenished daily with 1.5 million gallons from an underground spring percolating through miles of rock.\nDespite the name, the site has nothing to do with Monetzuma or the Aztecs, the name is based on an inaccurate assumption by those who discovered the ruins.\nMontezuma Castle National Monument, Sedona.\nBuilt 90 feet up a cliff face, possibly to protect from flooding, this is a five-story structure with 20 rooms for 30 to 50 Sinaqua. 1100-1450 AD/CE. (Montezuma is a misnomer).\nTuzigoot National Monument, Sedona.\nTuzigoot is a word that means “crooked water”. The pueblo ruins are two to three stories with 110 rooms that housed 250 Sinaqua.\nWupatki National Monument, north of Flagstaff.\nWupatki is Hopi for “Tall House”: By 1182, 85 to 100 Puebloans lived in Wupatki Pueblo with 100 rooms, a community room and a ball court. The community room was likely used for trading and interaction with the larger community. Within a day’s walk, a population of several thousand lived in separate villages. Wupatki, the largest building within 50 miles, and the nearby hamlets were mysteriously abandoned circa 1250 AD/CE.\nPlayball!: There are over 200 ball courts in southern Arizona, common from 750 to 1200 AD. The Wupatki people intermingled with their neighbors between villages, (a days walk).\nThe Blowhole – A crevice in the earth’s crust that connects to an underground passage in the village. Archaeologists have no idea why. There was a nice warm updraft that I appreciated on a freezing day in May.\nWukoko a neighboring hamlet\nWukoko Pueblo, an elegant hamlet 3 miles from Wupatki Pueblo, is built on a giant rock and stands three stories tall. Pieces of wood beam remain.\nThe Sad Truth\nThe Visitor Center displays testimonials from Navajo families that called this place home, but were evicted due to competition and conflict with ranchers, the railroad, and later the National Park Service.\nCoombs Excavation Site, Boulder, UT,.”Anasazi” State Park\nA village of almost 100 rooms occupied between 1050 and 1175 AD/CE. The excavation site shows evidence of early pit houses and masonry pueblos. The people departed in 1175 AD and the village was burned, possibly by the inhabitants. One artifact on display, the Atlatl (Spear Thrower) is 2,000 years old.\nPetroglyphs are carvings or etchings in the rock. Pictographs are painted figures.\nPalatki Rockart (1150 to 1350 AD), Sedona. The rock art predates the ruins and are thousands of years old.\nNewspaper Rock, Near Needles/Canyonlands UT. Petroglyph panel recording 2000 years of early human activity. Etchings on the rock date from BC time to 1300 AD/CE by Ute, Navajo, Europeans, Americans (and some recent high school grads).\nPetroglyphs Fremont Cliffs Capitol Reef. 600 to 1300 AD/CE\nResources and Information\nSubscribe to receive emails for new posts\nAt A Glance: Click Here for Archived USA Posts – Click Here for Arizona Posts\n6 thoughts on “AZ-The Ancients, Native Americans”\nSo awesome! I have seen the sign for Montezuma’s Castle several times but never stopped. Next time I am in the area I will definitely visit! Thanks for sharing!!!\nI will definitely go!!!\nInteresting images those Petroglyphs Fremont Cliffs Capitol Reef. 600 to 1300 AD/CE.\nLove this post! Seeing these dwellings together is awe-inspiring mixed with some sad truths. Thanks for the gorgeous pics and history lesson. xo\nComments are closed."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:f2bdf174-9334-4207-b90d-b70a0b23d2bb>","<urn:uuid:700bc875-7701-4267-8885-ac096f530d13>"],"error":null}
{"question":"Can you compare the storage capabilities and purposes of the Svalbard seed vault and the Millennium Seed Bank? What are their roles in future food security?","answer":"The Svalbard seed vault and the Millennium Seed Bank serve similar but distinct roles in preserving plant biodiversity. The Svalbard vault contains around 860,000 samples and was designed to protect up to 2.25 billion seeds, serving as an ultimate safety net for food security in case of catastrophic events like nuclear war or asteroid impacts. The Millennium Seed Bank, located at Kew's Wakehurst Place, has banked over 12.5% of wild plant species with over 2.3 billion seeds, focusing specifically on endangered, endemic, and economically important species. Both facilities contribute to future food security - Svalbard's genetic diversity helps breed crops adapted to future climates, while the Millennium Seed Bank's wild relatives of crop species can be bred into crops to help food systems adapt to climate change. Both serve as active solutions rather than just passive reserves.","context":["Rising concern over the ability of countries to grow food has led to the first ever request for a deposit from the “doomsday” seed vault in a frigid corner of Norway. Thanks to Syria’s civil war, the region’s primary seed vault in Aleppo has been forced to operate in a limited fashion, amid fighting that has left several hundred thousand dead and forced an estimated 11 million to become refugees. As ISIS controls part of Syria and refugees stream across Europe, destruction of antiquities and infrastructure continues.\nSyrian scientists have re-established the International Center for Agricultural Research in the Dry Areas in a decentralized model, with staff in eight surrounding countries and a temporary headquarters in Beirut. This week the organization asked for some of their seeds backfrom the doomsday vault as a precautionary measure to duplicate the material. Those seeds are needed by plant researchers who are working on the next crops to be planted in the Middle East and beyond, in order to try to stay a step ahead of pests and drought and increase yields to feed a growing population.\nScientists have specifically requested drought-friendly seeds of wheat, barley, and grasses from the vault, which is tucked into an Arctic mountainside on the archipelago of Svalbard, about 800 miles (1,300 kilometers) from the North Pole.\nThe Svalbard seed bank opened on the island of Longyearbyen in 2008 and contains around 860,000 samples, from countries all over the globe. It was designed to serve as the ultimate safety net for food security in case of nuclear war, asteroid impact, or other apocalyptic scenarios. The mostly subterranean facility was designed to protect up to 2.25 billion seeds. (Learn more about the seed vault.)\n“I’d say doomsday is happening every day for crop varieties,” Cary Fowler, executive director of the Global Crop Diversity Trust, which helps manage the facility, said in a previous interview.\n“Lots of people think that this vault is waiting for doomsday before we use it. But it’s really a backup plan for seeds and crops. We are losing seed diversity every day and this is the insurance policy for that,” said Fowler, referring to the fact that many crop varieties are disappearing thanks to shifts in weather, societal preferences, and market pressures.\nIn fact, the United Nations’ Food and Agriculture Organization estimates that three-quarters of the world’s crop biodiversity is no longer being planted in farmers’ fields. When crops are consolidated they become more vulnerable to disease, pests, droughts, or other threats. The genetic diversity found in the vault provides a critical safety net, the agency says.\nEven a seemingly simple crop, such as wheat, may have 200,000 different varieties. And each variety has a suite of individual traits that determine how it fares in high or low temperatures, during droughts, or against certain diseases or pests.\n“Even conservative projections of changing climate now indicate that by mid-century huge areas of some countries, in Africa for example, will be experiencing climates that are unlike any that have existed since the beginning of agriculture in those countries,” Fowler explained.\n“How will they become adapted to future climates? One way they can is by tapping into this rich storehouse of diversity and breeding new crops with traits that allow them to succeed in those climates. It’s essential to future food security,” Fowler said.\nAnd thanks to war in Syria, scientists have tapped that resource for the first time. Seed vaults have previously been destroyed in Iraq and Afghanistan, and one was badly damaged by a flood in the Philippines.\nThis story was updated at 11:40 a.m. ET on September 28 to clarify that the International Center for Agricultural Research in the Dry Areas has distributed their staff across eight countries, that the seed bank in Aleppo is in fact not damaged (but is only able to operate in a limited fashion), and that seeds from the center are distributed to other parts of the world in addition to the Middle East.","23 September 2019\nPlants and policy: Tackling global challenges\nBiodiversity loss, climate change and sustainable development are urgent global issues. 2020 is a critical year to address all three - and plants are at the core of many solutions.\nIn October 2020, governments from 196 countries will gather at the UN Convention on Biological Diversity (CBD) in China, to set ambitious new global targets that aim to reverse biodiversity loss. This Post-2020 Global Biodiversity Framework will provide a roadmap for conservation efforts everywhere, going forward.\nThis is occurring at a time when the scale of biodiversity loss is staggering. Plants are intricately linked to human life, yet one in five plants are threatened with extinction. Drawing on over 250 years of plant and fungal expertise, Kew’s scientific research is at the forefront of informing policy - and contributing to biodiversity targets. From accelerating assessments of plants most at risk of extinction, to increasing food security in a changing climate, to protecting vulnerable ecosystems, Kew’s work is contributing plant and fungal based solutions to some of the world’s most pressing challenges.\nWhere to start in a biodiversity crisis?\nWith at least 390,000 plant species across the planet, and new species being discovered every year, there is a serious shortage of conservation resources to understand and monitor every single species. There is a risk that many species may go extinct before scientists can assess them, or before they are even discovered – potentially losing irreplaceable medicines, food sources, habitats, and climate regulators.\nIUCN Red List Assessments\nOver the past three years, Kew has been accelerating the number of plants being assessed for the IUCN Red List – the global source of information on species extinction risk. From 126 species of coffee (Coffea), to a plant genus that forms the basis of the Amazon rainforest (Myrcia), the status of many plant species has been captured for the first time. As well as continuing to assess new species, Kew’s Plant Assessment Unit will also be conducting further assessments – creating more ‘snapshots’ for species, so that not only the threat status of species can be determined, but also trends over time.\nThis is a particularly important indicator for measuring progress towards conservation targets - it allows us to answer key questions such as how many species are threatened with extinction, and are conservation efforts reversing these trends? These form an important component of our understanding of the current biodiversity crisis and help us measure progress towards global conservation targets.\nKew is working to champion plants and fungi, and increase their representation in these metrics, which have historically been under-represented in these assessments.\nSeeds as back-ups, and solutions\nWith land use change and exploitation of species accelerating, and climate change amplifying the challenges for species, the list of species in the biodiversity crisis is growing. Plants are fundamental to all life on Earth and seed banking is playing an essential role in creating a safety-net against extinction.\nThe Millennium Seed Bank\nWith a focus on endangered, endemic and economically important species, Kew is working with a network of partners to bank over a tenth of all wild plants in the Millennium Seed Bank Partnership. More than 12.5 per cent of wild plant species have been banked so far, with over 2.3 billion seeds currently housed at Kew’s purpose-built facility in Wakehurst Place.\nYet these seeds are not just passive ‘reserves’, locked away in a vault - they are also active solutions. The genetics of these seeds contain tools that can be applied to some of the world’s most urgent global challenges. They can be actively used in projects to restore degraded habitats, increase the resilience of ecosystems and recover species in the wild. Banking the wild relatives of crop species can reveal a suite of adaptations that can be bred into crops – enabling food systems around the world to adapt to a changing climate.\nWith a growing network of partners across the globe, this project meets all three objectives of the CBD: to conserve, sustainably use, and equitably share the benefits arising from the use of biodiversity.\nThe multiple value(s) of plants\nConservation work always requires a combination of approaches – and conserving species in their native habitats is crucial. An important limitation of seed banks is that not all seeds can be banked. This is particularly true of many tropical plant species – the very same species that form the basis of ecosystems that are globally relevant for climate regulation, and critical for millions upon millions of people locally, who depend on forests for food, medicine and livelihoods.\nThe tropics are therefore a major priority for biodiversity conservation at a global level - nowhere are the intricate links between biodiversity, sustainable development and climate change clearer.\nTropical Important Plant Areas (TIPAs)\nBuilding on an approach that has secured habitats across Europe, the Mediterranean and North Africa, Kew has carefully adapted Important Plant Area (IPA) criteria to suit the paradox of the tropics: a relative lack of plant data, but urgent need for action.\nWorking closely with partners in-country, Tropical Important Plant Areas (TIPAs) are identified within each country according to a range of criteria – from areas of unrivalled botanical richness, to concentrations of threatened species, to areas that are important locally because of plants that are used.\nFormally recognising these areas can help make a compelling case for protecting habitats, in regions that are trying to balance development with preserving irreplaceable biodiversity. TIPAs can help with the planning of protected areas, contributing to global goals to protect 17 per cent of land, or lead to legal recognition for community areas, contributing to the UN Sustainable Development Goals (SDGs). They can also put plants on the radar in the planning stages of development projects, allowing short-term and long-term trade-offs to be more accurately considered.\nTIPAs therefore act as a tool to empower local decision making, whilst simultaneously highlighting how national plant conservation fits within an international picture – bridging gaps between global policy and national context.\nA critical opportunity\nBending the curve of biodiversity loss is a momentous task. It requires a combination of approaches that include all aspects of biodiversity: from species, to genetics, to ecosystems.\nThe upcoming UN Convention on Biological Diversity represents a critical opportunity to galvanise the support and resources needed at a global level. The decisions made in these rooms will filter down to plants, people and habitats in every corner of the planet.\nKew will be striving to ensure plants and fungi are at the forefront of this. And our science continues to reveal the same message loud and clear - biodiversity loss is a major global problem, but conserving biodiversity offers major global solutions too.\nCorlett, R.T. (2016). Plant diversity in a changing world: status, trends, and conservation needs. Plant Diversity, 38(1), 10-16.\nDarbyshire, I., Anderson, S., Asatryan, A., Byfield, A., Cheek, M., Clubbe, C., Ghrabi, Z., Harris, T., Heatubun, C.D., Kalema, J. & Magassouba, S. (2017). Important Plant Areas: revised selection criteria for a global approach to plant conservation. Biodiversity and Conservation, 26(8), 1767-1800.\nMace, G.M., Barrett, M., Burgess, N.D., Cornell, S.E., Freeman, R., Grooten, M. & Purvis, A. (2018). Aiming higher to bend the curve of biodiversity loss. Nature Sustainability, 1(9), 448-451.\nWillis, K. J. (ed.) 2017. State of the World’s Plants 2017. Report. Royal Botanic Gardens, Kew.\nRead & watch\n16 January 2019\nA high extinction threat for wild coffee could rattle the sector\n6 November 2018\nPlant cryobiotechnology at Kew: past, present and future\n6 July 2018\nThe critically endangered Madagascar Banana\n22 May 2019\nOur food, our health\n17 May 2019\nSaving endangered species\n23 July 2019\nWe can see beyond the trees"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3bcb955c-9b7f-4e11-b0a1-22a7877533ec>","<urn:uuid:2e4dd975-72b3-4fb8-b7c8-d0ba8f0a9056>"],"error":null}
{"question":"I'm fascinated by the evolution of positioning theory 📚 - could you explain what Kotler considers the fundamental relationship between positioning and consumer welfare? Trying to understand the deeper philosophical aspects!","answer":"Kotler establishes that positioning is fundamentally linked to consumer and societal well-being. He argues that marketing should be the center of company strategy, with the focus primarily on customer satisfaction and product benefits rather than distribution and financial value. He emphasizes that companies must prioritize consumer needs and ensure customers feel part of the business, as without this consideration, any marketing attempt will fail.","context":["What is positioning in marketing?\nPositioning is defined as the place that a brand, product, or service occupies in the mind of the consumer; its attributes, user perceptions, and its perceived value. It consists of developing a unique proposal that stands out from the competition and thus obtains greater attention from a specific segment of consumers.\nDepending on the positioning of our product/brand or company, we can design market entry strategies or probably modify some programmed action. Product positioning is a decision that the company exercises, trying to achieve a defined brand image, in relation to competitors within a market segment. In this sense, the term product positioning applies to decisions regarding brands, but it is also used for commerce, companies, and product categories.\nPositioning according to Kotler\nBorn May 27, 1931, in Chicago, United States; Philip Kotler is considered the master of Marketing.\nAfter his career as a university professor, in 1967 he published the book Marketing Management: Analysis, Planning, and Control, called “the bible of Marketing”. Among his achievements of him, it is worth mentioning that he has provided advice to large companies such as IBM, Motorola, and Bank of America.\nOne of the key points in his speech is that positioning is necessarily linked to the well-being of the consumer and society, so marketing should not only be part of the strategy of any company, but it should be the center of the same. He insists on the idea that the focus of companies should be more focused on customer satisfaction and the benefits of their product, than on distribution and financial value.\nKotler was the manager of the marketing concept for exclusive wholesale sales to existing. According to the American, you have to create new ways to stand out, without stagnating; and as he explains it, it is also necessary to worry about society, that sensitivity is not lost in daily tasks; and that a segment is achieved for customers; that is, that the product is as close as possible to the needs of consumers. He affirms that traditional Marketing is dead and that aligning marketing actions with strategic planning helps companies make decisions with the possibility of making as few mistakes as possible. What Is Positioning In Marketing?\nIt is of great importance that consumers feel part of the business because if this point is not taken into account, any marketing attempt simply dies.\nIn this virtual age, Phillip Kotler gives some tips that will surely help to capture ideas on how to position yourself:\n- Online presence brand: The power exerted by digital marketing and social networks is indisputable, if a company does not join these, it is making the most serious mistake.\n- Operates through the pipeline : (the funnel) of sales.\nEvery company must have commercial channels that Kotler describes as follows:\n- Prospecting clients.\n- Understand the needs of the target audience.\n- Develop solutions.\n- Create the proposal.\n- Negotiate contracts.\nUnderstanding the above, the responsibility of leading the first three stages corresponds to the Marketing department, and to the sales department the rest.\nBy developing this scheme, you avoid making a large number of errors when employing suitable personnel.\n- Physical stores must be reformulated: they must not only be sold, it is necessary to allow the customer to live an experience that generates a differential positioning.\n- Do not leave design aside: Understand that the environment plays a determining role when it comes to building customer loyalty.\n- Allow 24-hour attention: In the wide world of Internet browsing, it is necessary to monitor the number of visits and comments on what is published, since this must become a priority in any company or person so that at any time it can make a change.\n- Telling stories: A positioning that lasts is based on the feeling of the relationship between the brand and the consumer.\nPositioning according to Armstrong\nGary Armstrong is a Distinguished Professor at the University of North Carolina at Chapel Hill. He has contributed numerous articles to major business magazines, as well as being a consultant and researcher. His work stands out in many companies carrying out market research, sales management, and marketing strategies.\nAccording to Armstrong, positioning consists of establishing the best way to serve the target audience, discovering through research, the positioning of the competition in terms of the product, and then creating a detailed marketing plan.\nThe position of a product corresponds to the way in which consumers define it, based on its main attributes; that is, the place it occupies in their minds, in relation to the products of the competition.\nFor Gary Armstrong, positioning means inserting the unique benefits of the brand and its distinction, in the minds of customers. Today, consumers are getting too much information about products and services, with the balance being their relief. To streamline the buying process, customers organize products, services, and companies into categories, and then position them in their minds.\nA company recognized for its quality in certain segments will seek this position in a new segment, as long as there are enough buyers looking for quality. But in many cases, two or more companies will pursue the same position, each finding different ways to achieve distinction.\nEach company must differentiate its offering by creating a unique set of benefits that appeals to a substantial group within the segment.\nArmstrong simplifies the positioning task into three steps:\n- Identify a set of possible competitive advantages and build a position based on them.\n- Choosing the right competitive advantages.\n- Select a general positioning strategy. The company must formalize the chosen position to the market in an effective way.\nHe concludes that it is not possible to build solid positions on empty promises. If a company positions its product as offering improved quality and service, it must deliver the quality and service that has been promised.\nPositioning according to Jack Trout\nJohn Francis » Jack » Trout, was born on January 31, 1935, and died on June 4, 2017, from intestinal cancer. In the world of Marketing, he was a pioneer of positioning theory.\nI have worked for major companies such as General Electric, AT&T, Apple, Hewlett-Packard, IBM, Pfizer, Procter & Gamble, Airlines, Xerox, and the Papa John’s pizza chain.\nIn 2002, Trout began working directly with the United States government training new diplomats in the art of projecting a positive image of the country abroad. His love for him for radio led him to promote the importance of sound in advertising media. outdoor time nature"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:cbcd2669-8103-4622-9b78-09e83d7bf40c>"],"error":null}
{"question":"Could you explain how the naturalness bias affects workplace decisions, and what legal protections exist against discrimination in hiring?","answer":"The naturalness bias affects workplace decisions by causing people to implicitly prefer 'natural talent' over hard work, even when they claim to value effort more. This was demonstrated in entrepreneurship studies where business proposals were rated higher when attributed to natural talent rather than hard work, and people were willing to sacrifice objective qualifications to select supposedly naturally talented individuals. As for legal protections, Title VII of the Civil Rights Act of 1964 prohibits workplace discrimination in employment actions including recruitment, hiring, firing, advancement, training, and salaries. Discrimination claims can be based on either disparate treatment (intentional discrimination) or disparate impact (neutral policies that disproportionately affect protected groups). To prove disparate treatment in hiring, a claimant must show they belong to a protected class, were qualified, applied, were rejected, and the position remained open.","context":["The naturalness bias - operating implicitly and across domains and costly\nResearchers Tsay & Banaji (2011) found evidence for a naturalness bias which is the tendency to prefer supposed 'naturals' over 'strivers'. Even though expert musicians stated that they found hard work more important than talent they implicitly preferred talent over hard work. Tsay and Banaji demonstrated this by presenting more than 100 professionally trained musicians with two\nprofiles of two professional musicians, and a sample musical clip to\nlisten to from each musician.\nThe naturalness bias in music appreciation\nThe participants were then asked questions about how talented and successful they perceived the performer to be, and how willing they might be to hire this person. In fact, both clips were the same musical excerpt, and the profiles differed only in their mention of whether the musician had natural or learned talent. The results ultimately showed two effects: “We found even in experts and ostensibly professionally trained musicians, most of them could not tell that the recordings were the same. And on average, people seemed to prefer the ‘naturally’ talented individual, even when they said they believed hard work was more important than natural talent.”\nThe naturalness bias in a work settingTsay (2015) extended this work by investigating whether the natural bias also applies in other domains. In a preliminary survey (N=67) she found that while music was seen as a field in which natural talent is a stronger contributor than striving, entrepreneurship was seen as field in which striving and hard work is more important than talent. She then did three studies to test the existence and impact of the naturalness bias in the field of entrepreneurship.\nIn study 1a (N=212) she had participants rate a fictional business proposal made by an entrepreneur. When participants were made to believe that this proposal was based on natural talent both the entrepreneur and the proposal were rated higher than when they were made to believe the same proposal was based on hard work. This demonstrated that the existence of the naturalness bias in this field too. This bias was also found in participants with more expertise in entrepreneurship.\nIn study 1b (N=383) a similar finding was found. In addition to this participants were asked to describe how they came to their judgments. While they implicitly had preferred talent over hard work, they mentioned hard work more often as a factor influencing their judgment.\nIn study 2 (N=294) a selection decision process was simulated. Participants had to choose entrepreneurs from 18 pairs of target individuals who differed on five attributes, four of which were presented as more objective indicators of performance. In addition to this, a fifth attribute was mentioned which was whether the person was a natural or a striver (hard worker). While participants had said that they did not necessarily see talent as better than hard work, their choices suggest that they did. They were even willing to select the supposed talented individual at the cost of sacrificing some objective quality.\nThe naturalness bias thus appears to exist across domains, operates at an implicit level, and leads to sacrificing objective qualifications.\nDiscussionResearch has shown hat people make different attributions of failure and success depending on their mindsets (read more). People with fixed mindsets tend to attribute failure and success to stable personal characteristics (this is called an ability attribution). People with growth mindsets tend to make effort attributions. This research by Tsay into the naturalness bias shows that ability attributions in assessing other people and their performance are common and that they can be costly. Also, it implies that opportunists may exploit this bias by subtly suggesting their performance is not based on hard work but on talent.\nKnowing about the existence and costs of the naturalness bias may be a beginning of protecting ourselves against it.","Employment Discrimination on the Basis of Ethnicity\nWhen catastrophic events such as 9/11 or the Boston Marathon bombing happen, individuals who are perceived to be responsible for these events may be the targets of angry actions or even harassment. Employers are tasked with guarding against discrimination even after horrifying tragedies that are perceived to be associated with people of a particular national origin or ethnic background.\nWorkplace discrimination based on religion, national origin, race or color is prohibited under Title VII of the Civil Rights Act of 1964. These characteristics are generally considered to include ethnicity, even though the word “ethnicity” is not expressly stated in federal anti-discrimination laws.\nTitle VII Protection Against Adverse Employment Actions\nEmployment actions covered by Title VII include recruitment, hiring, firing, advancement, training, and salaries. There are two basic theories for proving race or national origin discrimination: disparate treatment and disparate impact. Whether you allege one or the other of these grounds, your complaint regarding ethnic discrimination under Title VII will have to be couched within the protected categories of race, national origin, religion, or color.\nDisparate treatment occurs when an employer is motivated to treat employees differently based on their membership in a particular group. At the start of a disparate treatment case based on a hiring decision, the claimant must show that:\nHe or she is a member of a protected class;\nHe or she was qualified for the job;\nHe or she applied;\nHe or she was rejected; and\nThe position remained open after the rejection.\nFor example, a clothing store may not refuse to hire a qualified salesperson of Guatemalan origins because the management believes that all Latinos are lazy. Similarly, a software company may not refuse to promote a Chinese person because the CEO thinks that any Asian is likely to steal trade secrets.\nSeemingly neutral employment actions that are not motivated by dislike for a particular ethnicity but still have a disparate impact may be found discriminatory under Title VII. For example, a workplace policy that forbids employees from wearing head coverings may have a disparate impact on Punjabi Sikh men.\nTitle VII of the Civil Rights Act of 1964\nProhibits workplace discrimination based on your ethnicity. Examples of this type of discrimination include:\nHarassment based on your affiliation with an ethnic group\nDiscrimination based on the way you speak, such as your accent, or the way you dress according to a religious affiliation\nHarassment based on perception - e.g., blanket statements made about an entire group of people\nProhibited employment actions include harassment and retaliation. Harassment is any conduct unwelcome to the victim that is based on a protected characteristic, including ethnicity. Harassment is unlawful if enduring the behavior is a condition of continued employment, or the conduct is so severe and pervasive that it creates a work environment that a reasonable person would find hostile, intimidating, or abusive. Minor or subtle irritations are not considered harassment. Usually, an isolated incident is not considered harassment unless it is severe.\nFor example, if a coworker makes a stray comment involving a stereotype about Hispanic people within earshot of a Mexican employee, that would be unlikely to rise to the level of harassment. However, if two coworkers attack the Mexican employee in a hallway at the office because they dislike his accent, this incident alone could be considered workplace harassment. Similarly, harassment would likely be found if coworkers routinely intimidate or mock the Mexican employee to the point that he is too overwrought to perform the job. If another person at the workplace finds the conduct offensive, he or she also may have a sound complaint, even though he or she is not the actual person being harassed. Unlawful harassment can occur without economic injury.\nHarassment may be based on any perception that somebody is a member of a particular ethnicity, whether or not it is accurate. For example, it is illegal for a company to refuse to hire a person from North Africa because the hiring manager believes the person is from the Middle East."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3a186700-18bf-4a8f-9269-fc4eede760f9>","<urn:uuid:f7746652-133f-4199-af0a-f3e3ea771279>"],"error":null}
{"question":"Which is more resource-intensive to simulate: 2D grid-world environments or finite element analysis of composite materials?","answer":"Based on the documents, 2D grid-world environments like DeepMind Lab2D are significantly less resource-intensive to run and typically do not require specialized hardware like GPUs to attain reasonable performance. In contrast, finite element analysis of composite materials requires computationally intensive simulations that may need cloud-solving options for longer, more complex analyses. While 2D simulations can run at 250,000 frames per second on a single CPU core, composite material analysis requires advanced mechanical simulation tools and powerful solvers like Autodesk Nastran for accurate results.","context":["We present DeepMind Lab2D, a scalable environment simulator for artificial intelligence research that facilitates researcher-led experimentation with environment design. DeepMind Lab2D was built with the specific needs of multi-agent deep reinforcement learning researchers in mind, but it may also be useful beyond that particular subfield.\nAre you a product of your genes, brain, or environment? What will an artificial intelligence (AI) be? The development of AI systems is inextricably intertwined with questions about the fundamental causal factors shaping any intelligence, including natural intelligence. Even in a completely abstract learning scenario, prior knowledge is needed for effective learning, but prior learning is the only realistic way to generate such knowledge.\nThe centrality of this dynamic is illustrated by experiments in biology. In laboratory animals, manipulations of the rearing environment produce profound effects on both brain structure and behavior. For instance, laboratory rodent environments may be enriched by using larger cages which contain larger groups of other individuals—creating more opportunities for social interaction, variable toys and feeding locations, and a wheel to allow for the possibility of voluntary exercise. Rearing animals in such enriched environments improves their learning and memory, increases synaptic arborization, and increases total brain weight (van2000neural). It is the interaction of environmental factors that is thought to produce the enrichment effects, not any single factor in isolation.\nWhile it is conceivable that AI research could stumble upon a perfectly general learning algorithm without needing to consider its environment, it is overwhelmingly clear that environments affect learning in ways that are not just arbitrary quirks, but real phenomena which we must strive to understand theoretically. There is structure there. For instance, the question of why some environments only generate trivial complexity, like tic-tac-toe, while others generate rich complexity, like Go, is not just a matter of the state space size. It is a real scientific question that merits serious study. We have elsewhere referred to this as the problem problem (leibo2019autocurricula).\nOne reason it is currently difficult to undertake work aimed at the problem problem is that it is rare for any individual person to possess expertise in all the relevant areas. For instance, machine learning researchers know how to design well-controlled experiments but struggle with the necessary skills that are more akin to computer game design and engineering. Another reason why it is difficult to pursue this hypothesis is the prevailing culture in machine learning that views any tinkering with the environment as “hand-crafting”, “special-casing”, or just “cheating”. These attitudes are misguided. In our quest for generality, we must not forget the great diversity and particularity of the problem space.\nA diverse set of customizable simulation environments for large-scale 3D environments with varying degrees of physical realism exist (todorov2012mujoco; kempka2016vizdoom; beattie2016deepmind; leibo2018psychlab; juliani2018unity). However, the ecosystem is thinner for 2D simulations. While some excellent tools exist (stepleton2017pycolab; zheng2019magent; gym_minigrid; LanctotEtAl2019OpenSpiel; shuo2019maenvs; suarez2019neural; platanios2020jelly), they fall short in at least one of our requirements of composability, flexibility, multi-agent capabilities, or performance.\n1.1 DeepMind Lab2D\nDeepMind Lab2D (or ‘‘DMLab2D’’ for short)111https://github.com/deepmind/lab2d is a platform for the creation two-dimensional, layered, discrete “grid-world” environments, in which pieces (akin to chess pieces on a chess board) move around. This system is particularly tailored for multi-agent reinforcement learning. The computationally intensive engine is written in C++ for efficiency, while most of the level-specific logic is scripted in Lua.\nThe environments of DMLab2D consist of one or more layers of two-dimensional grids. A position in the environment is uniquely identified by a coordinate tuple . Layers are labelled by strings, and the - and -coordinates are non-negative integers. An environment can have an arbitrary number of layers, and the order in which the layers are rendered is controlled by the user.\nThe environments of DMLab2D are populated with pieces. Each piece occupies a position , and each position is occupied by at most one piece. Pieces also have an orientation, which is one of the traditional cardinal directions (north, east, south, west). Pieces can move around the -space and reorient themselves as part of the evolution of the environment, both relatively to their current position/orientation and absolutely. It is also possible for a piece to have no position, in which case it is “off the board”. Pieces cannot freely move among layers; instead, a piece’s layer is controlled through its state (described next).\nEach piece has an associated state. The state consists of a number of key-value attributes. Values are strings or lists of strings. The possible values are fixed by the designer as part of the environment. The state of each piece can change as part of the evolution of the environment, but the state change can only select from among the fixed available values.\nThe state of a piece controls the piece’s appearance, layer, group membership, and behavior. Concretely, the state of a piece comprises the following attributes:\nlayer (string): The label of the layer which the piece occupies.\nsprite (string): The name of the sprite used to draw this piece.\ngroups (list of strings): The groups of which this piece is a member. Groups are mostly used for managing updater functions.\ncontact (string): A tag name for a contact event. Whenever the piece enters (or leaves) the same -coordinate as another piece (which is necessarily on a different layer), all involved pieces experience a contact event. The event is tagged with the value of this attribute.\nAn attempt to change a piece’s state fails if the piece’s resulting -position is already occupied.\nMost of the logic in an environment is implemented via callbacks for specific types (states) of pieces. Callbacks are functions which the engine calls when the appropriate event or interaction occurs.\nRaycasts and queries.\nThe engine provides two ways to enumerate the pieces in particular positions (and layers) on the grid: raycasts and queries. A raycast, as the name implies, finds the first piece, if any, in a ray from a given position. A query finds all pieces within a particular area in the grid, shaped like a disc, a diamond, or a rectangle.\n1.2 Why 2D, not 3D?\nTwo-dimensional environments are inherently easier to understand than three-dimensional ones, at very little, if any, loss of expressiveness. Even a game as simple as Pong, which essentially consists of three moving rectangles on a black background, can capture something fundamental about the real game of table tennis. This abstraction makes it easier to capture the essence of the problems and concepts that we aim to solve. 2D games have a long history of making challenging and interpretable benchmarks for artificially intelligent agents (shannon1950chess; samuel1959some; mnih2015human).\nRich complexity along numerous dimensions can be studied in 2D just as readily as in 3D, if not more so. When studying a particular research question, it is not clear a priori whether specific aspects of 3D environments are crucial for obtaining the desired behavior in the training agents. Even when explicitly studying phenomena like navigation and exploration, where organisms depend on complex visual processing and continuous-time physical environments, researchers in reinforcement learning often need to discretize the interactions and observations so that they become tractable. Moreover, 2D worlds can often capture the relevant complexity of the problem at hand without the need for continuous-time physical environments. This pattern where studying phenomena on 2D worlds has been a critical first step towards further advances in more complex and realistic environments is ubiquitous in the field of artificial intelligence. 2D worlds have been successfully used to study problems as diverse as social complexity, navigation, imperfect information, abstract reasoning, exploration, and many more (leibo2017multi; lerer2017maintaining; zheng2020ai; rafols2005using; ullman2009help).\nAnother advantage of 2D worlds is that they are easier to design and program than their 3D counterparts. This is particularly true when the 3D world actually exploits the space or physical dynamics beyond the capabilities of 2D ones. 2D worlds do not require complex 3D assets to be evocative, nor do they require reasoning about shaders, lighting, and projections. In most 2D worlds, the agent’s egocentric view of the world is inherently compatible with the allocentric view (i.e. the third-person or world view). That is, typically the agent’s view is simply a moveable window on the whole environment’s view.\nIn addition, 2D worlds are significantly less resource-intensive to run, and typically do not require any specialized hardware (like GPUs) to attain reasonable performance. This keeps specialized hardware, if any, available exclusively for the intensive work of training the agents. Using 2D environments also enables better scalability to a larger number of agents interacting with the same environment, as it costs only very little to render another agent’s view. Running 2D simulations is within the capabilities of smaller labs, whereas most 3D physics-based reinforcement learning is still prohibitively expensive in many settings.\n1.3 Multi-player support and benchmarking with human players\nA large fraction of human skills are social skills. To probe these, simulation environments must provide robust support for multi-agent systems. Most existing environments, however, only provide poor support for multiple players.\nDeepMind Lab2D supports multiple simultaneous players interacting in the same environment. These players may be either human or computer-controlled, and it is possible to mix human and computer-controlled players in the same game.\nEach player can have a custom view of the world that reveals or obscures particular information, controlled by the designer. A global view, potentially hidden from the players, can be set up and can include privileged information. This can be used for imperfect information games, as well as for human behavioral experiments where the experimenter can see the global state of the environment as the episode is progressing.\n1.4 Exposing metrics, supporting analysis\nDeepMind Lab2D provides several flexible mechanisms for exposing internal environment information: The simplest form is through observations, which allow the researcher to add specific information from the environment to the observations that are produced at each time step. The second way is through events, which, similar to observations, can be raised from within the Lua script. Unlike observations, events are not tied to time steps but instead are triggered on specific conditions. Finally, the properties API provides a way to read and write parameters of the environment, typically parameters that change rarely.\n2 Example results in\ndeep reinforcement learning\nFor an example, we consider a game called “Running With Scissors” (Fig. 1). A variant of this game with simpler graphics was first described in vezhnevets2020options. It can be seen as a spatially and temporally embedded extension to Rock-Paper-Scissors. As such, it inherits the rich game-theoretic structure of its parent (described in e.g. weibull1997evolutionary). The main difference is that, unlike in the matrix game, agents in Running With Scissors do not select their strategies as atomic decisions. Instead, they must learn policies to implement their strategic choices. They must decide how to “play rock” (or paper, or scissors), in addition to deciding that they should do so. Furthermore, it is possible—though not trivial—to observe the policy one’s partner is starting to implement and take countermeasures. This induces a wealth of possible feinting strategies, none of which could easily be captured in the classical matrix game formulation.\nAgents can move around the map and collect resources: rock, paper, and scissors. The environment is units in size but agents view it through a window.222This partial viewing window is a square. The agent sees 3 rows in front of itself, 1 row behind, and 2 columns to either side. Elementary actions are to move forward, backward, strafe left, strafe right, turn left or turn right, and fire the interaction beam. A gameplay video may be viewed at https://youtu.be/IukN22qusl8. The episode ends either when a timer runs out or when there is an interaction event, triggered by one agent zapping the other with a beam. The resolution of the interactions is driven by a traditional matrix game, where there is a payoff matrix describing the reward produced by the pure strategies available to the two players. In Running With Scissors, the zapping agent becomes the row player and the zapped agent becomes the column player. The actual strategy of the player depends on the resources it has picked up before the interaction. These resources are represented by the resource vector\nis the standard 2-simplex. The initial value of the vector is the centroid . The more resources of a given type an agent picks up, the more committed the agent becomes to the pure strategy corresponding to that resource. The rewards and for the (zapping) row and the (zapped) column player, respectively, are assigned via\nTo obtain high rewards, an agent should correctly identify what resource its opponent is collecting (e.g. rock) and collect the resource corresponding to its counter-strategy (e.g. paper). In addition, the rules of the game, i.e. the dynamics of the environment are not assumed known to the agents. They must explore to discover them. Thus Running With Scissors is simultaneously a game of imperfect information—each player possesses some private information not known to their adversary (as in e.g. poker (sandholm2015solving))—and incomplete information, lacking common knowledge of the rules (harsanyi1967games).\nIn Rock-Paper-Scissors, when faced with an opponent playing rock, a best responding agent will come to play paper. When faced with an opponent playing paper, a best responding agent will learn to play scissors. And, when one’s opponent plays scissors, one will learn to counter with rock. Fig. 2 shows that the same incentives prevail in Running With Scissors. However, in this case, the policies to implement strategic decisions are more complex since agents must learn to run around the map and collect the resources. Much more complex policies involving scouting and feinting can also be learned in this environment. See vezhnevets2020options for details.\nTo give an indication of the simulation performance, two random agents playing against one another, receiving full RGB observations of size px (px per tile, with tiles), run with an average of 250,000 frames per second (measured over 1000 episodes of 1000 steps each), on a single core of an Intel Xeon W-2135 (“Skylake”) CPU at 3.70GHz. The training example shown in Fig. 2 took several days to complete, and the cost of running the simulation is thus entirely negligible.\nArtificial intelligence research based on reinforcement learning is beginning to mature as a field. The need for rigorous standards by which the correctness, scale, reproducibility, ethicality, and impact of a contribution may be assessed is now accepted (osband2019behaviour; mitchell2019model; henderson2020towards; khetarpal2018re).\nBut in all these well-received calls for rigor in AI, the humble simulation environment gets shortchanged. It would appear that many researchers consider the environment to be none of their concern. A more holistic (and realistic!) view of their work suggests otherwise. Research workflows involve significant time spent authoring game environments and intelligence tests, adding analytic methods, and so forth. But these activities are usually not as simple and easy to extend as they ought to be, though they are clearly critical to the success of the enterprise.\nWe think that progress toward artificial general intelligence requires robust simulation platforms to enable in silico exploration of agent learning, skill acquisition, and careful measurement. We hope that the system we introduce here, DeepMind Lab2D, can fill this role. It generalizes and extends a popular internal system at DeepMind which supported a large range of research projects. It was especially popular for multi-agent research involving workflows with significant environment-side iteration. In our own experience, we have found that DeepMind Lab2D facilitates researcher creativity in the design of learning environments and intelligence tests. We are excited to see what the research community uses it to build in the future.\nWe thank the following people for their contributions to this project: Antonio García Castañeda, Edward Hughes, Ramana Kumar, Jay Lemmon, Kevin McKee, Haroon Qureshi, Denis Teplyashin, Víctor Valdés, and Tom Ward.","Finite element analysis and modeling software\nSimulation Mechanical software helps engineers easily set up advanced simulations so they can create better products. The software provides a wide range of advanced mechanical simulation tools. Powerful features include Mechanical Event Simulation (MES), Autodesk® Nastran® FEA solver software, multiphysics simulations, multiCAD format support, cloud-solving options, and composites.\nFinite element creation & analysis\nGenerate meshes based on CAD solid or surface geometry from many sources, as well as on 2D outline sketches.\nUse structured meshes to gain control over the size, shape, and quality of every element in the model. You can build structured meshes directly within Simulation Mechanical software—no solid or surface CAD model is needed.\nMixed model support\nCombine line, planar, and solid element types in a single model. You can combine other element types as well, such as trusses, beams, membranes, composites, rigid elements, and actuators. Assemblies containing thin components such as sheet metal solve efficiently using shell elements. With Simulation Mechanical software, you can use solid and shell elements in the same analysis.\nCreate pressure vessels and their components with the included PV-Designer. You can also use this tool to produce mesh that conforms to your analysis requirements. Use Simulation Mechanical software to simulate the performance of the vessel to help ensure that it satisfies requirements.\nComposite parts definition\nUse failure criteria to address the layer-by-layer properties and fiber orientation of composite parts. Simulation Mechanical software works with Simulation Composite Design software to help you decide which composite layout to use, and to analyze within the context of the overall assembly, including its various materials (steel, rubber, and other fibers and composites).\nAutodesk Nastran solver\nRun more accurate and reliable simulations. The solver in Autodesk Nastran FEA solver software, now included in Simulation Mechanical software, offers advanced analysis capabilities.\nWide range of analysis types\nInvestigate your assembly's response to vibration, thermal, and electrostatic loads, such as current and voltage.\nNonlinear structural analysis\nUse Simulation Mechanical software to understand when and why you would switch from a linear to a nonlinear analysis.\nIdentify and eliminate linear and nonlinear vibration issues in your part or assembly. Meet seismic requirements by performing analysis types such as random vibration, harmonic, response spectrum, Dynamic Design Analysis Method (DDAM), and time history analysis. See stresses, deflection, and the factor of safety of your model under time- and frequency-dependent loads.\nMechanical Event Simulation (MES)\nUse this powerful nonlinear dynamic analysis tool to simulate and solve problems with drop test, impact analysis, and the metal forming of parts.\nAnalyze thermal heat transfer using convection, conduction, and radiation effects. View temperatures and heat flow rates. Loads and results can be steady or transient. If they are transient, you can determine the change in temperature over a period of time.\nBuilt-in fatigue simulation helps determine when and where products might fail. Perform stress- and strain-based fatigue analysis to determine product life and factor of safety. Simulation Mechanical software offers both high- and low-cycle fatigue options.\nParametric design study\nAnalyze multiple variations of an Inventor 3D CAD model by manipulating its parameters within Simulation Mechanical software. Speed and simplify design iterations by analyzing ranges of dimensions and feature suppression.\nDrop test analysis\nThe Drop Test wizard makes it easy to set up a test scenario. Simply specify the height, gravity, direction, and other details.\nDetermine optimal plate thicknesses and beam cross-sections with the Design Optimization utility, which automatically chooses the appropriate design dimensions. The utility works with all structures that contain beam or plate-type elements.\nDynamic Design Analysis Method (DDAM)\nDDAM is a type of shock spectrum analysis that estimates the dynamic response of a component to shock-loading caused by the sudden movement of a vessel. Use DDAM to analyze the shock response at the mountings of various shipboard equipment such as masts, propulsion shafts, rudders, exhaust uptakes, and other critical structures.\nCombine analysis types\nComplete a number of different multiphysics workflows using Simulation Mechanical software. Setting up advanced simulations is easy thanks to standard engineering terminology, visual process guidance, and user-friendly tools and wizards that transfer simulation results between multiple analyses. This helps designers, engineers, and analysts focus on product performance rather than advanced numerical or simulation methods.\nSimulation CFD interoperability\nApply pressure and temperature results from Simulation CFD software to simulate thermal and flow stresses (fluid structure interaction) on your mechanical components. After completing your flow simulation, launch the CAD model in Simulation Mechanical software, assign the settings to define the thermal stress analysis, and select the Simulation CFD design study file as the temperature source.\nSimulation Moldflow interoperability\nValidate and optimize plastic part designs. Exchange data, including fiber orientation and residual stress, between Simulation Moldflow and Simulation Mechanical software.\nSimulation results data management\nFactor of safety\nAnalyze and output the factor of safety of every linear design, based on the strength of the material for each component. The allowable stress is predefined for most library materials, but you can also choose your own allowable value.\nCreate reports with a click of a button. Reports have a cover page, table of contents, material information, boundary conditions, mesh properties, and the results of the analysis, including images and animations. You can choose from a variety of output formats.\nVault data management software is embedded in the Simulation Mechanical interface. Use it for search and revision control, as well as to help secure your intellectual property.\nIntegrate with other Autodesk products to help create high-end renderings. Use FBX files to import results to visualization tools such as 3ds Max 3D modeling software and VRED 3D visualization software, or transfer data directly to Showcase 3D presentation software.\nImport models, set up and run simulations, and visualize and report results through a single, common user interface. Reporting tools automatically convert all user input and result views into a formatted HTML report that you can save as a PDF or Microsoft Word document.\nDirectly import and evaluate designs from software including AutoCAD, Inventor, Dassault Systèmes SolidWorks, Dassault Systèmes Catia, PTC ProE, and PTC CREO, and from file formats including NX, Rhino, IGES, and STEP.\nA built-in library with over 9,000 materials, from structural steels to reinforced polymers, helps you better understand the real-world behavior of products. It’s easy to add and manage custom data: Simulation Mechanical software integrates with MatWeb and Matereality, and manual entry and import options are also available.\nFlexible cloud solving options\nExtended solving options let you simulate locally or in the cloud, depending on your needs. If you are testing the setup of an analysis, use your local machine to iterate and optimize. When you are ready to initiate a longer, more computationally intensive simulation, use the power of the cloud, saving local resources for other tasks. Compare Simulation Flex vs. Desktop offerings."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:d568bbc9-9282-4a5a-903a-1a29051610c0>","<urn:uuid:decafac0-f2bb-4d05-87d3-519bf837c36c>"],"error":null}
{"question":"What are the differences in supervised clinical experience requirements between West Virginia and Minnesota for obtaining a professional counselor license?","answer":"In West Virginia, LPC candidates need 2 years/3,000 hours of postmaster's supervised professional counseling experience (or 1 year/1,500 hours if they have a doctoral degree). At least 50% must be direct client services, with 1 hour of direct individual supervision required for every 20 hours of practice. In Minnesota, LPCC candidates must complete 4,000 hours of supervised clinical experience, including 1,800 hours of direct client contact, with two hours of supervision required for every 40 hours accrued. The supervision must focus on assessments, diagnoses, and interventions of mental illnesses with children and adults.","context":["Counselor Licensing Requirements in West VirginiaCounselor Licensing Requirements in West Virginia are covered on this page. We want to ensure that you have accurate information about Counselor Licensing Requirements in West Virginia so that you can make informed decisions regarding your counselor career.\nFind schools and get information on the program that’s right for you. (It’s fast and free!)\nWest Virginia Counselor Credential TitlesLicensed Professional Counselor (LPC) Provisional Licensed Professional Counselor An applicant who has met the education and exam requirements, but not the 2 years supervised experience requirement\nWest Virginia Counselor Education RequirementsMaster’s degree or higher from a program accredited by CACREP or CORE, or a comparable accrediting body, that includes 60 graduate semester hours (or 90 quarter hours) and a practicum and internship. Acceptable graduate degrees include a specialization in community agency counseling, mental health counseling, pastoral counseling, rehabilitation counseling, school counseling, and substance abuse or addictions counseling. Similar degrees that include the word “counseling” and include specific coursework, and are determined by the board to be a closely related field, are also acceptable.\nWest Virginia Counselor Experiential Requirements\nMust already possess provisional license 2 years/3,000 hours of postmaster’s supervised professional counseling experience. If obtained a doctoral degree: 1 year/1,500 hours of postdegree supervised professional counseling experience. At least 50% of the supervised counseling experience must be in the direct provision of counseling services to clients. A minimum of 1 hour of direct individual supervision is required for every 20 hours of practice. Supervision must be under a board approved professional.\nWest Virginia Counselor Exam RequiredNCE, CRCE, or NCMHCE\nWest Virginia Requirements for Clinical SupervisorsWest Virginia LPC Licensed as a professional counselor, or other qualified supervisor as determined by the Board. At a minimum the professional should be licensed for 2 years with 5 years of counseling experience, completion of training in clinical counseling supervision, is currently licensed, and board approval. Supervisor must be pre-approved before hours can begin to accrue. The professional supervisor provide a statement detailing counseling philosophy, supervision experience, and counseling experience as well as being able to demonstrate skills necessary to address core areas of practice.\nWest Virginia Requirements for Out of State ApplicantsAn individual may apply for reciprocity review if the individual is currently licensed in another state whose licensing requirements are greater than or equal to the requirements in this rule as determined by the board. It is the applicant’s responsibility to provide the documentation necessary for the board to make an appropriate decision on reciprocity. The following documentation is required for the board to review for reciprocity: a completed WV application form and the required fee; copies of graduate transcripts from the issuing institutions; a certified copy, including exam results, of the applicant’s file from the jurisdiction where currently licensed; and two completed professional recommendation forms from individuals permanently licensed as professional counselors, psychologists, social workers, or psychiatrists on forms provided by the board. Persons with a temporary license cannot complete the recommendation form.\nWest Virginia Scope of Practice“PROFESSIONAL COUNSELING” means the assessment, diagnosis, treatment and prevention of mental, emotional or addiction disorders through the application of clinical counseling procedures. Professional counseling includes the use of psychotherapy, assessment instruments, counseling, consultation, treatment planning, and supervision in the delivery of services to individuals, couples, families and groups. “CLINICAL COUNSELING PROCEDURES” means an approach to counseling that emphasizes the counselor’s role in systematically assisting clients through all of the following including, but are not limited to, observing, assessing and analyzing background and current information; utilizing assessment techniques useful in appraising aptitudes, abilities, achievements, interests or attitudes; diagnosing; and developing a treatment plan. The goal of these procedures is the prevention and elimination of symptomatic, maladaptive, or undesired behavior, cognitions, or emotions in order to integrate a wellness, preventative, pathology and multicultural model of human behavior to assist an individual, couple, family, group of individuals, organization, institution or community to achieve mental, emotional, physical, social, moral, educational, spiritual, vocational or career development and adjustment through the life span of the individual, couple, family, group of individuals, organization, institution or community. The licensee’s areas of competence in counseling and the services provided, based on training and experience, from the following list, as is appropriate: career counseling, consultation, diagnosis and treatment of mental and emotional disorders, employee assistance counseling, family counseling, human resources counseling, marriage counseling, mental health counseling, rehabilitation counseling, school counseling, substance abuse and addictions counseling, supervision, and vocational counseling.\nSalary Information for West Virginia CounselorsIn the state of West Virginia, being a counselor can be both mentally and fiscally satisfying. According to The Bureau of Labor Statistics, a counselor in the state of West Virginia will earn between $27,780 and $43,040 per year, in addition to changing the lives of individuals in their communities. Now that you are familiar with the counselor licensing requirements in West Virginia, visit our How to Become a Counselor in West Virginia page for a more detailed look into the steps you will need to take to make your dream of becoming a counselor a reality.\nState InformationWV Board of Examiners in Counseling 812 Quarrier Street, Suite 212 Charleston, WV 25301 304/558-5494 304/558-5496 (fax) Executive Director Jean Ann Johnson CONTACT: Roxanne Clay [email protected] WEBSITE: www.wvbec.org Application fee: $200 Initial License fee: depends ondate applicant is licensed","Minnesota Counseling License Requirements\nEarning counseling licensure in Minnesota can be a long journey as the state sets specific education and work requirements for each type of counseling license. There are over 17,000 counselors working in the state with most employed in substance abuse, behavioral disorder, and mental health counseling.1-5 A Minnesota counseling license can open up many opportunities to work with diverse populations in challenging situations. For example, Licensed Professional Clinical Counselors (LPCCs) provide mental health and psychotherapeutic counseling services to all age groups. If you’d like to know more about what it takes to develop a Minnesota counseling career, this page summarizes the major types of counseling licensure in the state.\nTable of Contents\n- How to Become a Counselor in Minnesota\n- Licensed Professional Clinical Counselor (LPCC) Licensing Process\n- Additional Counseling Careers and Licenses in Minnesota\n- Licensed Marriage and Family Therapist (LMFT)\n- School Counselor\n- Licensed Alcohol and Drug Counselor (LADC)\n- Other Professional Counseling Careers\n- Minnesota Counseling Career and Salary Information\n- Counseling Associations in Minnesota\n- Frequently Asked Questions\nHow to Become a Counselor in Minnesota\nMinnesota requires that prospective counselors complete a degree that meets the coursework requirements for the practice area pursued. Many Minnesota counseling programs are available that meet these requirements, though degrees from out-of-state schools may also be sufficient. Depending on the license pursued, applicants may also need to meet other work experience and testing requirements.\n1. Decide which area of counseling to pursue.\nThe first step towards a career in counseling is to decide which area of counseling you’d like to pursue. The pathways to Minnesota counseling licensure vary and knowing which type of counseling interests you is important because it will help you focus on the correct degree program required for licensure.\n2. Earn the degree(s) required for your counseling practice area.\nIn Minnesota, you will need at least some post-secondary education to begin your career as a counselor. Some types of licensure also require graduate-level education in a related field. For example, mental health counselors must have a master’s degree with an internship component and specific clinical coursework and marriage and family therapists must have a master’s degree in marriage and family therapy or a closely related field. School counselors can begin the licensure process with a bachelor’s degree and 24 credits of related graduate study, but a graduate degree is required for full licensure. Substance abuse counselors are not required to have a graduate degree, but a bachelor’s degree with a supervised practicum is required.\n3. Get licensed to practice counseling in Minnesota.\nThe final step is to become licensed by in Minnesota. In Minnesota, licenses for clinical mental health counselors and substance abuse counselors are issued by the Minnesota Board of Behavioral Health and Therapy and licenses for marriage and family therapists are issued by the Minnesota Board of Marriage and Family Therapy. School counselors are licensed by the Minnesota Professional Educator Licensing and Standards Board. Continue reading to learn more about these licensure processes.\nLicensed Professional Clinical Counselor (LPCC) Licensing Process\nThe Minnesota Board of Behavioral Health and Therapy is responsible for issuing licenses to mental health counselors, known as Licensed Professional Clinical Counselors (LPCCs). LPCCs use professional counseling knowledge and skills to diagnose, evaluate, and treat psychosocial and behavioral issues and mental illnesses. The Board also offers LPC licensure, which allows trained counselors to provide psychotherapeutic services except for the diagnosis and treatment of mental illness. For more about a career in professional counseling, read our mental health counselor career guide. LPCC applicants in Minnesota must have an accredited graduate degree comprised of at least 48 credits, including a 700-hour internship and clinical coursework in six key areas, including diagnostic assessments and clinical ethics.\n1. Pass the National Clinical Mental Health Counselor Examination (NCMHCE).\nProspective LPCCs must pass the National Clinical Mental Health Counselor Examination (NCMHCE) administered by the National Board of Certified Counselors (NBCC). The NCMHCE is a simulation-based exam designed to test your knowledge of clinical counseling theories and interventions. The NBCC provides a handbook and links to exam prep resources to help you study.\n2. Accrue supervised experience.\nThe Board has strict work experience requirements for LPCC candidates. You must complete 4,000 hours of supervised clinical experience, including assessments, diagnoses, and interventions of mental illnesses with children and adults. Additionally, 1,800 of the 4,000 hours must be direct client contact and you must receive two hours of supervision for every 40 hours accrued. Supervisors must have the LPCC Supervisor designation from the Board or submit an application and credential verification form to earn approval. Acceptable LPCC supervisors must have four years of experience, including two years of clinical diagnosis and treatment experience, and must complete 45 hours of supervision training.\n3. Apply for and receive your LPCC license.\nYou can begin the application process for LPCC licensure while you complete your supervised experience or after it has been completed. The application form is available online and should be mailed to the Board. At the time of application, you must pay the $150 application fee, the $250 initial licensure fee, and the $33.25 fingerprinting fee (fees current as of September 2019). The Board will send you more information about how to complete the fingerprint background check, which must be completed before your license will be approved.\nProfessional Counselor Licensure by Reciprocity in Minnesota\nMinnesota accepts and evaluates applications for licensure by reciprocity on a case-by-case basis. The burden is on the applicant to demonstrate that the licensure process in the state where they are licensed is equivalent to Minnesota requirements for LPCC licensure. The license held should also have a similar scope of practice to LPCCs in Minnesota, including clinical mental health skills. Reciprocity applicants must submit transcripts, exam scores, and documentation of 4,000 hours of supervised clinical experience to be considered.\nCounselor Licensing Renewal and Continuing Education Information\nThe Board issues renewal notices and forms 45 days prior to expiration. Minnesota has a complex continuing education (CE) system for LPCCs. The first renewal period for an LPCC license is four years. During that time, the licensee must complete 12 credits of related graduate study, up to a maximum of 60 credits, including the graduate degree, and 40 hours of CE. One credit of graduate coursework may be counted as 15 hours of CE during this period up to the 40-hour requirement. Thereafter, the license must be renewed every two years and licensees must complete 40 hours of CE during each renewal cycle. CE activities must be approved by the Board by submitting a request form at least 60 days before the proposed activity.\nAdditional Counseling Careers and Licenses in Minnesota\nChoosing an area of focus in the counseling profession will determine the steps of your career path. In addition to mental health counseling, some of the other major types of counseling licensure summarized below include licensed marriage and family therapists, school counselors, and substance abuse counselors.\nLicensed Marriage and Family Therapist (LMFT)\nLicensed marriage and family therapists are licensed by the Minnesota Board of Marriage and Family Therapy. LMFT applicants must have a graduate degree in marriage and family therapy or a closely related field that is preferably accredited by COAMFTE. If the program is not accredited by COAMFTE, it must meet specific coursework requirements set by the Board. In Minnesota, marriage and family therapy is a specialized form of counseling and LMFTs are licensed to provide psychotherapy to individuals, couples, and families for emotional and mental problems affecting interpersonal dynamics, including premarital and marital counseling, divorce counseling, and family therapy. To become a licensed marriage and family therapist, follow these steps:\n- Apply to the Board for permission to take the AMFTRB national exam in marriage and family therapy.\n- Accrue supervised experience.\n- Apply for your LMFT license.\n- Pass the Minnesota marriage and family therapy oral exam.\n- Receive your LMFT license.\nTo learn more about LMFT careers, visit our LMFT career guide.\nThe Minnesota Professional Educator Licensing and Standards Board is responsible for issuing school counselor licenses. Minnesota school counselors work with students from kindergarten through twelfth grade to help promote age-appropriate academic, emotional, and behavioral development, and resiliency skills among students. There are three school counselor licensing tiers (tier two, tier three, and tier four) and tier four is considered a full license. The minimum requirement to begin the licensure process at tier two is a bachelor’s degree and 24 credits of graduate-level study in school counseling, but an accredited graduate degree in school counseling will enable you to apply directly to tier three. If you have a master’s degree that is not in school counseling, you must complete an approved school counselor preparation program before applying. After completing your graduate education, earn tier four school counseling licensure by completing the following steps:\n- Register as a tier three school counselor with the Board.\n- Earn supervised experience.\n- Request and receive your tier four school counseling license.\nRead more about opportunities in this field on our school counseling career guide.\nLicensed Alcohol and Drug Counselor (LADC)\nThe Minnesota Board of Behavioral Health and Therapy is responsible for issuing licenses for substance abuse counselors, known in Minnesota as the Licensed Alcohol and Drug Counselor (LADC) credential. There are two pathways to licensure, known as the standard method and the supervision method. Both pathways have the same minimum required education, which is a bachelor’s degree with at least 18 credits in drug and alcohol counseling and an 880-hour supervised practicum. The application form can be found online. In Minnesota, LADCs work in a variety of settings, including treatment facilities, correctional facilities, and hospitals, to assess, evaluate, and modify addictive behaviors related to alcohol and drug abuse. To earn a LADC license, follow these steps:\n- Pass the required exam and earn supervised experience if required.\n- Apply for your LADC license.\n- Complete a fingerprint background check.\n- Receive your LADC license.\nOther Substance Abuse Counseling Credentials Offered in Minnesota\n- Advanced Alcohol and Drug Counselor Reciprocal – Minnesota (AADCR-MN)\n- Alcohol and Drug Counselor – Minnesota (ADC-MN)\n- Alcohol & Drug Counselor Reciprocal – Minnesota (ADCR-MN)\n- Certified Clinical Supervisor Reciprocal (CCSR)\n- Certified Criminal Justice Addictions Professional Reciprocal (CCJPR)\n- Certified Peer Recovery Specialist (CPRS)\n- Certified Peer Recovery Specialist Approved Supervisor (CPRSAP)\n- Certified Peer Recovery Specialist Reciprocal (CPRSR)\n- Certified Prevention Professional (CPP)\n- Certified Prevention Professional Advanced (CPPA)\n- Certified Prevention Professional Reciprocal (CPPR)\nMore information about what substance abuse counselors do can be found on our substance abuse counseling career guide.\nOther Professional Counseling Careers\nIn addition to the major fields listed above, there are many other ways to develop a professional counseling career. Some examples of other types of career pathways include:\n- Rehabilitation Counselor\n- Gambling Counselor\n- Genetic Counselor\n- Youth Counselor\n- Guidance Counselor\n- Pastoral Counselor\n- Recreational Therapist\nMinnesota Counseling Career and Salary Information\nAccording to the Bureau of Labor Statistics, there are 17,610 counselors in Minnesota with the majority working as substance abuse, behavioral disorder, and mental health counselors (7,500) and educational, guidance, school, and vocational counselors (4,790).1-5 Average annual salaries range from $41,400 for rehabilitation counselors, which is above the $39,930 national average, to $58,360 for educational, guidance, school, and vocational counselors.4,3 Substance abuse, behavioral disorder, and mental health counselors also earn above the national average ($49,280 compared to $47,920 nationally).1\nThe job market outlook for counseling positions in Minnesota varies depending on the area of counseling. Marriage and family therapists are projected to see the fastest growth through 2026 at 19.6%, followed by counselors, all other (11.4%) and educational, guidance, school, and vocational counselors (6.7%), although not all projections were reported.6 Educational, guidance, school, and vocational counselors were projected to increase by 280 new jobs total through 2026, with 470 annual openings per year, including replacements.6 Although rehabilitation counselor positions were projected to decrease overall through 2026 (-5.3%), there may still be 370 openings each year in this area.6\n|Occupation||Number Employed||Average Annual Salary|\n|Counselors, All Other||560||$47,440|\n|Educational, Guidance, School, and Vocational Counselors||4,790||$58,360|\n|Marriage and Family Therapists||1,040||$57,770|\n|Substance Abuse, Behavioral Disorder, and Mental Health Counselors||7,500||$49,280|\nData from the Bureau of Labor Statistics.1-5\nCounseling Associations in Minnesota\n- Minnesota Association for Marriage and Family Therapy (MAMFT): A non-profit organization that promotes the marriage and family therapy profession through public education and advocacy and connects practitioners through networking events, professional development, and issue-specific committees.\n- Minnesota Counseling Association (MnCA): MnCA connects all types of counselors in the state to advocate for the profession and share best practices through conferences and training.\n- Minnesota School Counselors Association (MSCA): A professional organization for school counselors that work with students of all ages, MSCA holds an annual conference and organizes regional divisions to support school counselors from all Minnesota school boards.\nFrequently Asked Questions\nWhat type of clinical coursework do I need to take for LPCC licensure?\nLPCC candidates must complete specific clinical coursework in order to be eligible for licensure, including six credits in each of diagnostic assessments and evidence-based clinical interventions and three credits in each of clinical treatment planning, evaluation of interventions, clinical ethics, and cultural diversity. Details of these courses must be included with your application and you must submit all course syllabi so the Board can verify that your education meets the clinical coursework requirements.\nWhat do substance abuse counselors do in Minnesota?\nIn Minnesota, licensed alcohol and drug counselors (LADCs) are expected to be proficient in 12 core areas to be licensed. Some of these core skills include conducting initial screenings to determine if a client is eligible for services, assessing client strengths, weaknesses, and needs with regards to substance abuse, creating treatment plans that identify short- and long-term goals, providing appropriate counseling services to the client, responding to crisis situations when the client is in distress, and making appropriate referrals when the client’s needs are outside the scope of their expertise.\nAre there counseling careers without a degree in Minnesota?\nAll the major types of counseling licensure summarized on this page require a degree, and some require a graduate degree with specific coursework. Mental health counselors and marriage and family therapists must have a graduate degree in a related field, preferably from an accredited program. Substance abuse counselors must have a bachelor’s degree. For school counseling licensure, a bachelor’s degree is necessary and although you can begin the licensure process with a minimum of 24 credits of school counseling graduate education, ultimately you must complete a graduate degree to achieve full licensure.\nDo I need to apply for an LPC license before I can become an LPCC?\nIt is not necessary to apply for the LPC license if you meet the requirements for LPCC, nor is it necessary to progress to the LPCC license if you are an LPC in good standing. Individuals who are already licensed as LPCs are able to apply for LPCC licensure through the conversion application once they meet the requirements, including a related master’s or doctoral degree, 24 credits of clinical assessment and counseling, a passing score on the NCMHCE exam, and 4,000 hours of supervised clinical work experience.\nWhere can I find a counseling job in Minnesota?\nAccording to the Bureau of Labor Statistics, counselors work in many areas of the state. The Minneapolis-St. Paul-Bloomington metropolitan area, which is partially located in Minnesota, ranks sixth in the country for substance abuse, behavioral disorder, and mental health counselors (5,050 employed) and rehabilitation counselors (2,300 employed).1,4 Southeast Minnesota ranks highly in second place among nonmetropolitan areas for marriage and family therapists (60 employed).2 Minnesota also ranks fifth in the country for the concentration of rehabilitation counseling jobs (3,720) with the Mankato-North Mankato metropolitan area ranking ninth overall (150 employed).4\n1. Bureau of Labor Statistics Occupational Employment and Wages, Substance Abuse, Behavioral Disorder, and Mental Health Counselors: https://www.bls.gov/oes/current/oes211018.htm\n2. Bureau of Labor Statistics Occupational Employment and Wages, Marriage and Family Therapists: https://www.bls.gov/oes/current/oes211013.htm\n3. Bureau of Labor Statistics Occupational Employment and Wages, Educational, Guidance, School, and Vocational Counselors: https://www.bls.gov/oes/current/oes211012.htm\n4. Bureau of Labor Statistics Occupational Employment and Wages, Rehabilitation Counselors: https://www.bls.gov/oes/current/oes211015.htm\n5. Bureau of Labor Statistics Occupational Employment and Wages, Counselors, All Other: https://www.bls.gov/oes/current/oes211019.htm\n6. Projections Central, Long Term Occupational Projections 2016-2026: https://projectionscentral.org/Projections/LongTerm"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:770d9f63-6666-414b-82a4-3ff025c21edb>","<urn:uuid:89a7e291-b991-4555-b67e-dfddaccf978a>"],"error":null}
{"question":"Could you explain both the environmental impacts and efficiency considerations of concrete usage in construction? I'm particularly interested in understanding current challenges and emerging solutions in this field, as well as how it connects to global emissions reduction goals.","answer":"Concrete usage presents significant environmental challenges due to its large embodied energy and substantial carbon dioxide emissions during manufacture. The transition from clay-brick to concrete construction poses environmental hazards due to overuse and overdesign of concrete structures. However, the environmental impact can be minimized through careful engineering to avoid material waste and by utilizing local coal-burning power plant fly ash as a portland cement replacement, which reduces carbon dioxide emissions. From a global perspective, cement production has tripled since 1995 and now generates about 7% of the world's total emissions. The industry is pursuing various decarbonization strategies, including using waste as fuel and substituting clinker with industrial waste like fly ash. Advanced solutions are being developed, such as the LEILAC 2 project at HeidelbergCement's plant in Hanover, Germany, which is testing carbon capture technology. However, implementing these solutions at scale requires substantial capital investments and additional research and development, along with a supportive ecosystem.","context":["for Beijing Materials in Beijing Multi-family Dwellings\nProblems to address:\nContext and Approach:\nThe selection and use of building materials,\nin particular for large scale housing projects will have significant\nenergy, cost, and comfort implications. The impact of the current\nand imminent growth in China's construction sector represents a\npotential new prominence in world resource consumption.\nCareful and creative selection of materials\nand design and detailing of installation can significantly mitigate\nthis impact. The use of renewable materials should be given preference\nto the use of non-renewables. Reusable and recyclable materials\nshould be give preference to non-recyclable, non-reusable materials.\nConsideration should be given to the project's role in carbon dioxide\nchains; outputs and inputs should be balanced. Designs must be reasonably\nconstructable; the need to import either labor or materials should\nbe minimized and limited to those resources that have significant\nThe building envelope, its walls and roof, should\nbe instrumental in minimizing the consumption of non-renewable resources\nduring the life of the building. This will minimize negative impacts\nof fuel usage including operational costs.\nThe team recognizes potential risks and\nbenefits of the current transition to occupant ownership. The environmental\nrisks include: The effect of larger units, and the resultant per\ncapita material and energy consumption; Comfort zone evolution to\na more limited region that may will require more responsive buildings\nand HVAC systems; The decentralization of knowledge base to the\nmarket eliminates the potential for a unified approach The associated\nbenefits include the owner's increased stake in their dwellings\nmay lead to improved maintenance due to onsite care taking and responsibility.\nAlso, the potential of decentralized knowledge base may provide\nmeans to implement new solutions.\nReturn to top\n- AC units: Individual units are not efficient,\nand typically are not carefully installed. Energy inefficiency\nis compounded by air infiltration through unsealed condensate\nand electric lines. Water leakage may present a long-term durability\nand maintenance issue near these interruptions in the façade.\n- Lack of insulation. There is conflicting\ninformation regarding the use of insulation in existing and\nrecent construction. Insulating value is typically attributable\nto clay or masonry materials that have very high thermal conductivity.\nTheir advantage in thermal lag are advantageous only when mean\ndiurnal temperatures are in the comfort zone. Therefore insulation\nwill be beneficial in most regions. The industry is currently\nunaccustomed to designing insulated walls.\n- Window construction: Typical systems are\nsingle glazed and leaky (loose-fitting). According to the Ministry\nof Construction, windows and doors account for 50% of energy\nloss in Chinese buildings.\n- Effective use of concrete: As the transition\ncontinues from clay-brick to concrete construction, the overuse\n/over design of concrete structures poses a significant environmental\nhazard due to its large embodied energy and the significant\nvolumes of carbon dioxide emitted during its manufacture.\n- Loss of agricultural land due to clay mining.\nApparently, 10,000 hectares of farmland is destroyed annually\nto make clay brick. The apparent alternative is concrete blocks.\nThis technology is, like clay brick, also high in embodied energy.\n- Integrated design approach: Holistic thinking\nshould be the first consideration in improving the quality of\nhousing. Integrating the consideration of materials with the\nplanning of mechanical systems can eliminate aesthetic problems\nwhile avoiding damaging moisture penetration and energy consuming\nair infiltration. A similar approach may be taken in the planning\nand detailing of window openings. Vast improvements may be made\non each of these fronts by complete planning and rigorous follow-through\n- Windows: A lot of money can be spent improving\nthe quality of windows. High-tech solutions are available globally\nthat would provide a radical improvement on the energy losses\nand associated discomfort related to leaky windows. However,\nimporting technology shouldn't be considered as the only solution,\nor even necessarily the best solution. Importation suggests\nexporting jobs and money, and requires the expenditure of time,\nmoney and energy for the transport of products. If windows are\nto be imported, consideration should be given to purchasing\nonly frame stock. This will reduce the transportation associated\nwith heavy glass units, while concentrating on accessing the\nhigher-tech and highly engineered frames. The higher quality\nframes will mitigate a majority of the energy and comfort issues\nif installed correctly.\n- Insulation: The most expensive way to insulate\na building is to leave the insulation out of the building. The\nresultant energy consumption of buildings without insulation\nis more expensive than simply putting insulation in the wall\nto begin with. The most effective location for building insulation\nis on the outside of the structure where it can be run past\nthe floor slabs, creating a continuous protective layer for\nthe building thereby minimizing thermal bridges.\n- Centralized HVAC systems or planned locations\nand proper sleeves for penetrations.\n- Apply vernacular / regional technologies:\nThe environmental impact of concrete construction can be minimized\nby careful engineering of the structures to avoid wasting material.\nLocal coal-burning power plants should be considered as mines\nfor fly ash, a material that can be used to replace portland\ncement and thereby reduce carbon dioxide emissions of the concrete\n- Air barriers and flashing systems: These\nsystems are proven in their effectiveness to minimize energy\nconsumption and maximize building longevity.\nReturn to top","Decarbonising heavy industries and the manufacutring sector - the hard to abate industries - requires support and collaboration of governments and industrial players, introduction of innovative technologies and above all, the willingness of market forces to make the necessary changes.\nThe efforts required will be similar to that led to the cost reduction and scale up of renewable energy generation, say experts.\nManufacturing is essential for economic growth, and its supply chains are pivotal for ensuring the wellbeing and advancement of people and economies. Among the largest and most critical manufacturing industries are steel, cement, glass, and chemicals.\nThese industries are the biggest industrial consumers of energy, helping to make heavy industry responsible for 20 percent of global direct carbon dioxide emissions today, says an International Finance Corporation (IFC) report, Strengthening Sustainability Decarbonizing Manufacturing Industries.\nGiven the integral role that these industries play in the global economy and in people’s lives, it will be critical for them to decarbonise their production processes if the world is to meet emissions-reduction goals and prevent intensifying climate disasters, it points out.\nClearly, decarbonising heavy industries will take decades of work, trillions in investments for capital and operational expenses, and global coordination. Besides adequate supplies of renewable energy, countries and companies, particularly in developing markets, will need support for capital expenditures to retool production, it says.\nThe steel, cement, glass, and chemicals industries have all been growing exponentially to keep up with the world’s increasing population and the associated demand for building and infrastructure materials. Global demand for steel -- the world’s most traded commodity after oil -- has increased by a factor of three since 1971. Today, steel manufacturers use 8 percent of the world’s energy, mostly coal, and emit 6 to 7 percent of the world’s greenhouse gases.\nCement production, meanwhile, has tripled from just 1995, and now generates about 7 percent of the world’s total emissions, while global sales for chemicals have increased 2.4 times over the last two decades, with the industry accounting for about 5 to 6 percent of the world’s emissions.\nGlobal consumption of glass is forecast to grow by a compounded annual rate of 3.5 percent between 2019 and 2027, driven by both traditional uses and the material’s incorporation into new technologies, including renewable energy systems. While the glass industry only generates 0.3 percent of worldwide CO2 emissions, the energy intensity of glass production surpasses that of cement. The production process also emits combustion byproducts that contribute to climate change.\nMcKinsey & Company has identified steel, cement, and two subsectors of the chemicals industry (specifically, ammonia, used to make fertilizer, and ethylene, used in many industrial production chains), as the manufacturing sectors emitting the most CO2. McKinsey estimated in a report in 2018 that it would cost over $21 trillion through 2050 just to decarbonise these sectors.\nDespite the costs and technical complexities involved, growing global concerns over climate change are putting an increasing focus on sustainability.\nThe steel, cement, glass, and chemicals industries have all been growing exponentially. In decarbonising these industries, the private sector together with governments and a range of other stakeholders are pursuing carbon-abatement strategies to make manufacturing more energy efficient and less harmful to the planet, notes the IFC report.\nThese strategies involve 1) shifting manufacturers to cleaner energy sources; 2) encouraging more efficient production processes; 3) offsetting the financial costs and risks of decarbonising; and 4) investing in technologies that can eliminate waste by capturing, recycling, remanufacturing, and reusing spent material.\nOil, gas, and coal remain the principal energy sources for manufacturers operating in a wide range of sectors, including steel, cement, glass, and chemicals. This has helped to make these industries among the most difficult to decarbonise. The production of steel, cement, and glass all require extremely high temperatures and thus large amounts of fossil fuels.\nGlobal regulation will also be key. For example, because steel is traded globally, regulating emissions unevenly could shift production to areas where costs are lower and emission standards -- if they even exist - are lax or unregulated.\nManufacturers over the last few decades have substantially improved their energy efficiency and reduced emissions per output. In the steel industry, a growing number of producers are switching to Direct Reduced Iron (DRI) and electric arc furnaces (EAF). These methods rely on natural gas and electricity and use DRI, scrap steel, or their combination, to make steel, resulting in lower emissions and energy-intensity production. In the United States, two-thirds of steel production involves recycled steel from electric arc furnaces.\nThe cement industry uses waste as a fuel, and substitutes clinker with industrial waste, such as fly ash from power-generation plants; the glass industry has increased its use of cullet, which is made from crushed glass and requires approximately 40 percent less energy than when using raw materials. The chemical industry has secured reductions through process and energy improvements, but it faces a unique challenge, since half of its products use carbon, such as from crude oil, for feedstock.\nHowever, process improvements and equipment retrofits alone won’t achieve the deep cuts that are required for heavy industries to meet the goals set under the Paris Agreement. To guide further reforms, the steel, cement, glass, and chemical industries have drawn up their own roadmaps for achieving net zero by 2050.\nMany of the innovative technologies needed to achieve these goals already exist, and some are being piloted. In the steel sector, the Al Reyadah Carbon Capture, Use, and Storage Project in the United Arab Emirates is capturing CO2 from the flue gas of an Emirates Steel production facility and injecting it to conduct enhanced oil recovery in the Abu Dhabi National Oil Company’s oil fields nearby. The project includes capture, transport, and injection of up to 800,000 tons of CO2 annually and is part of a bigger potential plan to create a CO2 hub to manage carbon dioxide supply and injection requirements in the UAE, notes the report.\nThe cement sector is also exploring the use of carbon capture, use, and storage, with the LEILAC 2 project testing the technology at HeidelbergCement’s plant in Hanover, Germany, with a slated 2023 operational date. To roll out these cutting-edge solutions at scale, however, will require huge capital investments at attractive commercial terms as well as additional research and development for heavy industry to decarbonise coupled with a supportive ecosystem.\nChallenges & Opportunities\nCoal makes up 75 percent of the fuel used to produce iron and steel, and 60 percent of the fuel used to make cement. Natural gas and oil are the dominant fuels used in the manufacturing of petrochemicals. Finding substitutes for old fossil fuel-powered blast furnaces and factories is a significant challenge. Old equipment is expensive to replace, and it’s difficult to find enough renewable capacity capable of generating industrial-grade heat on a 24/7 basis.\nVarious strategies can drive behavioral change among companies and their consumers and investors. Governments and other stakeholders including the International Finance Corporation (IFC) are working with the private sector on a range of such strategies to help manufacturers in heavy industries achieve greener production -- plus financial savings and a competitive edge.\nShifting to Cleaner Energy\nCapacity for wind and solar power is growing, and policies announced by the US and EU in 2022 should accelerate this trend. The International Energy Agency estimates that renewable’s share must double from 30 percent in 2021 to more than 60 percent by 2030 to meet the green energy needs required for the world to achieve net zero by 2050.\nHeavy industries will require four to nine times as much clean energy to decarbonise, compared to the status quo. Using biocarbons as replacement fuel or feedstock in some heavy manufacturing provides another way to reduce emissions. This change typically could be implemented without expensive retrofits to blast furnaces; the challenge is finding enough sustainably produced biofuels. And biofuels still emit CO2, though their net emissions are lower.\nGreen hydrogen offers one of the most promising long-term solutions. Hydrogen exists in abundance in nature and is used in some production processes including for feedstock. It also has the potential to power manufacturing processes and to store and transport energy. But production of carbon-neutral hydrogen requires renewable power -- and enough of it -- to break apart hydrogen’s bonds and release energy through electrolysis.\nHydrogen is also used in other ways in the production process, including as a lower emission reduction agent in steel production. Used to make ammonia, green hydrogen could help to decarbonise the agricultural food supply chain.\nTrillions of dollars of investment will be required to phase out the thousands of existing coalcombusting steel, cement, and chemical plants around the world that are responsible for most industrial emissions. Investing in more efficient production offers one of the fastest ways to reduce manufacturers’ carbon footprint, while also boosting their bottom line through reduced energy and raw material use.\nA wide range of options for improving efficiency is available to hard-to-abate industries, and IFC can advise companies on what works best for them.\nIFC supports a number of strategies aimed at decarbonising heavy industries, with a focus on promoting energy efficiency, renewable and alternative energy, emerging technologies, and resource efficiency.\nConstruction Materials (Cement, Steel, Glass): Potential energy-related strategies include adopting modern kiln technology and vertical roller mills (for cement); optimising blast furnace operations (steel); upgrading furnaces such as by installing waste-heat boilers (glass); and employing electric arc furnaces (steel and glass).\nThe three industries could employ waste-heat recovery and power cogeneration systems, adopt solar and wind power, and incorporate biomass or biogas as alternative reductants or fuels. Emerging technologies that could help all three industries include green hydrogen and CCUS, while the cement industry could also increase its use of blended and green cement.\nTo improve resource efficiency, industries could alter designs to lightweight products or change the production process (such as using the narrow neck press and blow approach to produce glass bottles), saving on raw materials and energy. They could increase the use of recycled content, including recycled concrete, cullet or waste glass, and scrap steel.\nOffsetting Costs and Risks\nGovernments can help offset the costs and risks of decarbonisation by providing tax breaks and subsidies for businesses that shrink their carbon footprint. Regulations can also help to make the enabling environment more conducive to decarbonisation, such as through cap-and-trade programmes. Policy makers can assist in creating a supportive ecosystem through such measures as clean energy standards and investment in infrastructure supporting recycling and reuse.\nMeanwhile, development organisations such as IFC can help mitigate the financial risk by extending risk-sharing lending arrangements or by anchoring investments.\nIn the longer term, decarbonising operations can help manufacturers maintain their competitive edge. Companies are not only facing growing pressure to become more sustainable from consumers, investors, and corporate boards, but must also consider how best to protect their operations from a changing climate and potential future shortages in essential inputs, from water to rare earth minerals.\nInvesting in Technology\nTechnologies are capable of eliminating waste by capturing, recycling, remanufacturing, and reusing spent material. One proven method for limiting CO2 emissions is carbon capture, utilization, and storage (CCUS), with the gas then stored or used for other purposes.\nWidespread adoption will depend in part on the economic viability of sequestering carbon.\nCurrently, technologies are in the early stage, with years of development needed before they become commercially viable. Broad use of CCUS will also require the development of transportation and storage infrastructure. Another decarbonising strategy is to simply reduce demand for certain manufactured products. Significant research and development is going into finding more sustainably manufactured products such as building materials that can replace steel, cement, and glass."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:6ff9c928-99be-4d4c-84ae-ffb58a50ba16>","<urn:uuid:7554ea03-d621-4d08-92d9-9ebdb9407c28>"],"error":null}
{"question":"What are the key differences between the Roman method and the Network Imperative approach for ensuring design integrity in the digital age?","answer":"The Roman method and Network Imperative approach represent contrasting approaches to design integrity. The Roman method, developed in the Macedonia-Roman period, relied on replicating proven, durable designs with minor improvements, using encyclopedic books for guidance. In contrast, the Network Imperative advocates for rapid adoption of emerging and disruptive technologies, often immediately without phased implementation, to remain competitive in the digital age. While the Roman method emphasized careful replication of successful past designs, the Network Imperative pushes for quick adaptation to digital transformation through network-based business models and continuous innovation.","context":["An Incomplete Contracts Theory of Information, Technology and Organization\nAlthough there is good reason to expect that the growth of information work and information technology will significantly affect the trade-offs inherent in different structures for organizing work, the theoretical basis for these changes remains poorly understood. This paper seeks to address this gap by analyzing the incentive effects of different ownership arrangement in the spirit of the Grossman-Hart-Moore (GHM) incomplete contracts theory of the firm. A key departure from earlier approaches is the inclusion of a role for an \"information asset\", analogous to the GHM treatment of property. This approach highlights the organizational significance of information ownership and information technology. For instance, using this framework, one can determine when 1) informed workers are more likely to be owners than employees of firms, 2) increased flexibility of assets will facilitate decentralization, and 3) the need for centralized coordination will lead to centralized ownership. The framework developed sheds light on some of the empirical findings regarding the relationship between information technology and firm size and clarifies the relationship between coordination mechanisms and the optimal distribution of asset ownership. While many implications are still unexplored and untested, building on the incomplete contracts approach appears to be a promising avenue for the careful, methodical analysis of human organizations and the impact of new technologies.\nPlease report citation or reference errors to , or , if you are the registered author of the cited work, log in to your RePEc Author Service profile, click on \"citations\" and make appropriate adjustments.:\n- Sherwin Rosen, 1987.\n\"Transactions Costs and Internal Labor Markets,\"\nNBER Working Papers\n2407, National Bureau of Economic Research, Inc.\n- Erik Brynjolfsson & Thomas W. Malone & Vijay Gurbaxani & Ajit Kambil, 1994.\n\"Does Information Technology Lead to Smaller Firms?,\"\nINFORMS, vol. 40(12), pages 1628-1644, December.\n- Erik J. Brynjolfsson & Thomas Malone & Vijay Gurbaxani & Ajit Kambil, 1991. \"Does Information Technology Lead to Smaller Firms?,\" Working Paper Series 123, MIT Center for Coordination Science.\n- Alchian, Armen A & Demsetz, Harold, 1972.\n\"Production , Information Costs, and Economic Organization,\"\nAmerican Economic Review,\nAmerican Economic Association, vol. 62(5), pages 777-95, December.\n- Armen A. Alchian & Harold Demsetz, 1971. \"Production, Information Costs and Economic Organizations,\" UCLA Economics Working Papers 10A, UCLA Department of Economics.\n- Holmstrom, Bengt, 1989. \"Agency costs and innovation,\" Journal of Economic Behavior & Organization, Elsevier, vol. 12(3), pages 305-327, December.\n- George J. Mailath & Andrew Postlewaite, 1990. \"Workers Versus Firms: Bargaining Over a Firm's Value,\" Review of Economic Studies, Oxford University Press, vol. 57(3), pages 369-380.\n- Winter, Sidney G, 1988. \"On Coase, Competence, and the Corporation,\" Journal of Law, Economics and Organization, Oxford University Press, vol. 4(1), pages 163-80, Spring.\n- Brynjolfsson, Erik., 1993. \"Information technology and the 'new managerial work',\" Working papers 3563-93., Massachusetts Institute of Technology (MIT), Sloan School of Management.\n- Holmström, Bengt, 1989. \"Agency Costs and Innovation,\" Working Paper Series 214, Research Institute of Industrial Economics.\nWhen requesting a correction, please mention this item's handle: RePEc:wop:mitccs:126. See general information about how to correct material in RePEc.\nFor technical questions regarding this item, or to correct its authors, title, abstract, bibliographic or download information, contact: (Thomas Krichel)\nIf references are entirely missing, you can add them using this form.","Standard Handbook of Machine Design\nAuthor: The McGraw-Hill Companies\nCategory: Technology & Engineering\nTo the late Joseph Edward Shigley Joseph Edward Shigley was awarded bachelor degrees in electrical (1931) and mechanical (1932) engineering by Purdue University, and a master of science in engineering mechanics (1946) by The University of Michigan. His career in engineering education began at Clemson College (1936-1956) and continued at The University of Michigan (1956-1978). Upon retirement, he was named Professor Emeritus of Mechanical Engineering by the Regents in recognition of his outstanding achievement and dedicated service. At the time when Professor Shigley began thinking about his first book on machine design, many designers were unschooled, and textbooks tended to give results with only a brief explanation—they did not offer the reader many tools with which to proceed in other or new directions. Professor Shigley's first book, Machine Design (1956), showed his attention to learning and understanding. That milestone book is currently in its fifth edition. Other books followed, among which are Theory of Machines and Mechanisms (with John J. Uicker, Jr.), Mechanical Engineering Design (with Charles R. Mischke), and Applied Mechanics of Materials. Early in the 1980s, Professor Shigley called Professor Mischke and said, \"I've never done a Handbook before; there is no precedent in machine design, and it is time there was one. I propose we do it together. Take a couple of months to consider what ought to be in it, the organization and presentation style. Then we can get together and compare notes.\" The result was the first edition of the Standard Handbook of Machine Design (1986), which won the Association of American Publishers Award for the best book in engineering and technology published in 1986. Eight Mechanical Designers Workbooks followed. Professor Shigley received recognitions such as the grade of Fellow in the American Society of Mechanical Engineers, from which he also received the Mechanisms Committee Award in 1974, the Worcester Reed Warner Medal in 1977, and the Machine Design Award in 1985. I believe he would have given up all the above rather than give up the effect he had as mentor and tutor to students, and in guiding boys toward manhood as a scoutmaster. He indeed made a difference. Charles R. Mischke PREFACE TO THE SECOND EDITION The introduction of new materials, new processes, and new (or more refined) analytical tools and approaches changes the way in which machines are designed. Complementary to the urge to update and improve, it is useful to look back in order to retain a perspective and appreciate how all this fits into the fabric of machine design methodology. Many of the machine elements we know today were known to the ancients. We have the advantage of improved materials, better manufacturing methods, and finer geometric control, as well as insightful theory and the opportunity to stand on the shoulders of the giants among our predecessors. Assuring the integrity of a contemplated design, its components, and the aggregate machine or mechanism has always been a problem for the engineer. The methods of record include the following: • The Roman method This method, developed in the Macedonia-Roman period, was to replicate a proven, durable design (with some peripheral improvements). Encyclopedic \"books\" were compiled for the guidance of designers. In strengthlimited designs, the essential thought was, \"Don't lean on your element any harder than was done in the durable, extant designs of the past.\" There are times when contemporary engineers still employ this method. • The factor of safety method (of Philon of Byzantium) In today's terms, one might express this idea as loss-of-function load strength n = : —— = impressed load stress for linear load-stress relations. Alternatively, loss-of-function load Allowable load = n or AA lIlI owabuil e s^t ress = strength n for linear load-stress relations. The factor of safety or design factor was experiential and came to consider uncertainty in load as well as in strength. • The permissible stress method Since the concept of stress was introduced by Cauchy in 1822, some engineers have used the idea of permissible stress with load uncertainty considered, and later with the relevant material strength included, as for example in 0.405, < (aall)bending < 0.605, It is not clear whether the material strength uncertainty is included or not. When the word \"allowable\" or \"permissible\" is used among engineers, it is important to clearly define what is, and what is not, included. • Allowable stress by design factor The definition of allowable stress oan is expressed as strength °all = ~^n n\nUsing Design to Achieve Key Business Objectives\nAuthor: Thomas Lockwood,Thomas Walton\nPublisher: Skyhorse Publishing, Inc.\nHow can design be used to solve business problems? That's the question answered, in many innovative ways, by Building Design Strategy. Mark Dziersk, EunSool Kwon, Arnold Levin, Laura Weiss, and many more top-name contributors share their experience and insights. Topics explore the full range of issues today, including thinking ahead; adapting to challenges; developing tangible strategies; using design to convey ideas; choosing worthwhile projects to help growth; using design to create fiercely loyal customers.\nThe Essential David Orr\nAuthor: David W. Orr\nPublisher: Island Press\nFor more than three decades, David Orr has been one of the leading voices of the environmental movement, championing the cause of ecological literacy in higher education, helping to establish and shape the field of ecological design, and working tirelessly to raise awareness of the threats to future generations posed by humanity’s current unsustainable trajectory. Hope Is an Imperative brings together in a single volume Professor Orr’s most important works. These include classics such as “What Is Education For?,” one of the most widely reprinted essays in the environmental literature, “The Campus and the Biosphere,” which helped launch the green campus movement,and “Loving Children: A Design Problem,” which renowned theologian and philosopher Thomas Berry called “the most remarkable essay I’ve read in my whole life.” The book features thirty-three essays, along with an introductory section that considers the evolution of environmentalism, section introductions that place the essays into a larger context, and a foreword by physicist and author Fritjof Capra. Hope Is an Imperative is a comprehensive collection of works by one of the most important thinkers and writers of our time. It offers a complete introduction to the writings of David Orr for readers new to the field, and represents a welcome compendium of key essays for longtime fans. The book is a must-have volume for every environmentalist’s bookshelf.\nAn Agenda for Governance Reform\nAuthor: Richard Tomlinson,Marcus Spiller\nPublisher: CSIRO PUBLISHING\nSince the early 1990s there has been a global trend towards governmental devolution. However, in Australia, alongside deregulation, public–private partnerships and privatisation, there has been increasing centralisation rather than decentralisation of urban governance. Australian state governments are responsible for the planning, management and much of the funding of the cities, but the Commonwealth government has on occasion asserted much the same role. Disjointed policy and funding priorities between levels of government have compromised metropolitan economies, fairness and the environment. Australia’s Metropolitan Imperative: An Agenda for Governance Reform makes the case that metropolitan governments would promote the economic competitiveness of Australia’s cities and enable more effective and democratic planning and management. The contributors explore the global metropolitan ‘renaissance’, document the history of metropolitan debate in Australia and demonstrate metropolitan governance failures. They then discuss the merits of establishing metropolitan governments, including economic, fiscal, transport, land use, housing and environmental benefits. The book will be a useful resource for those engaged in strategic, transport and land use planning, and a core reference for students and academics of urban governance and government.\nHow to Survive and Grow in the Age of Digital Business Models\nAuthor: Barry Libert,Megan Beck,Jerry Wind\nPublisher: Harvard Business Review Press\nCategory: Business & Economics\nDigital networks are changing all the rules of business. New, scalable, digitally networked business models, like those of Amazon, Google, Uber, and Airbnb, are affecting growth, scale, and profit potential for companies in every industry. But this seismic shift isn’t unique to digital start-ups and tech superstars. Digital transformation is affecting every business sector, and as investor capital, top talent, and customers shift toward network-centric organizations, the performance gap between early and late adopters is widening. So the question isn’t whether your organization needs to change, but when and how much. The Network Imperative is a call to action for managers and executives to embrace network-based business models. The benefits are indisputable: companies that leverage digital platforms to co-create and share value with networks of employees, customers, and suppliers are fast outpacing the market. These companies, or network orchestrators, grow faster, scale with lower marginal cost, and generate the highest revenue multipliers. Supported by research that covers fifteen hundred companies, authors Barry Libert, Megan Beck, and Jerry Wind guide leaders and investors through the ten principles that all organizations can use to grow and profit regardless of their industry. They also share a five-step process for pivoting an organization toward a more scalable and profitable business model. The Network Imperative, brimming with compelling case studies and actionable advice, provides managers with what they really need: new tools and frameworks to generate unprecedented value in a rapidly changing age.\nenergy and technology in architecture\nAuthor: Ralph W. Crump,Martin J. Harms\nPublisher: Van Nostrand Reinhold Company\nCategory: Technology & Engineering\nRapid Technology Adoption for Digital Transformation\nAuthor: Stephen J Andriole,Thomas Cox,Kaung M. Khin\nPublisher: CRC Press\nCategory: Business & Economics\nThe pace of technological change is accelerating, hyper competition is growing, opportunities for business model disruption are exploding, and comprehensive cloud delivery is readily available. These factors challenge every aspect of business technology strategy. The Innovator’s Imperative: Rapid Technology Adoption for Digital Transformation prepares twenty-first century businesses leaders for competing and leading in this disruptive digital environment. Five years of research conducted by the authors suggests that leading companies have all but abandoned the requirements analysis and modeling best practices of the twentieth century. Accordingly, the authors put forth the innovator’s imperative that contends: All companies wanting to be competitive should adopt emerging and disruptive technologies as quickly as possible, and in many cases, immediately. Technology is driving business strategy, and companies are rethinking their technology strategy, especially the governance that determines how and why technology investments are made. Based on their research the authors have developed a five-step framework for digital transformation: Model and simulate Identify high-leverage opportunities Prioritize transformational targets Identify digital opportunities Find courageous leaders The book explains each of these steps to guide business leaders in architecting digital transformation projects according to their organization’s market positions, budgets, objectives, and corporate culture. Hyper-competitive, disruptive companies are jumping across technology adoption phases without regard to any phasing whatsoever. Companies focused on digital transformation often adopt emerging technologies immediately. They have become early adopters of technologies that can impact existing—and create whole new—business models and processes. This book examines this jump into new technologies, processes, and business models to prepare twenty-first century business leaders to make that leap.\nTools & Techniques for Managing the Design Process\nAuthor: David Holston\nPublisher: Simon and Schuster\nThe design profession has been asking itself some important questions lately. How do designers deal with the increasing complexity of design problems? What skills do designers need to be competitive in the future? How do designers become co-creators with clients and audiences? How do designers prove their value to business? Designers are looking for ways to stay competitive in the conceptual economy and address the increasing complexity of design problems. By adopting a process that considers collaboration, context and accountability, designers move from 'makers of things' to 'design strategists.' The Strategic Designer shows designers how to build strong client relationships, elevate their standing with clients, increase project success rates, boost efficiency and enhance their creativity.\nAuthor: Kingston Heath\nSustainable design requires that design practitioners respond to a particular set of social, cultural and environmental conditions. 'Vernacular Architecture and Regional Design' defines a set of strategies for understanding the complexities of a regional setting. Through a series of international case studies, it examines how architects and designers have applied a variety of tactics to achieve culturally and environmentally appropriate design solutions. • Shows that architecture and design are inextricably linked to social and environmental processes, and are not just technical or aesthetic exercises. • Articulates a variety of methods to realise goals of socially responsible and environmentally responsive design. • Calls for a principled approach to design in an effort to preserve fragile environments and forge sustainable best practice. 'Vernacular Architecture and Regional Design' will appeal to educators and professional practitioners in the fields of architecture, heritage conservation and urban design. Dr. Kingston Wm. Heath is Professor and Director of the Historic Preservation Program at the University of Oregon. Previously he was Professor of Architecture at the University of North Carolina, Charlotte where he taught seminars on vernacular architecture and regional design theory. He holds graduate degrees from the University of Chicago and Brown University. In addition to numerous articles in scholarly journals, he is the author of Patina of Place, and winner of the Abbott Lowell Cummings Award from The Vernacular Architecture Forum for excellence in a scholarly work. He has earned an international reputation in the field of vernacular architecture and has directed field schools in Italy and Croatia.\nAuthor: Lihui Wang,Fengfeng Xi\nPublisher: Springer Science & Business Media\nCategory: Technology & Engineering\nThis book presents a collection of quality chapters on the state-of-the-art of research efforts in the area of smart devices and novel machine design, as well as their practical applications to enable advanced manufacturing. The first section presents a broad-based review of several key areas of research in smart devices and machines. The second section is focused on presenting an in-depth treatment of a particular device or machine. The book will be of interest to a broad readership.\nDriving Superior Returns on Marketing Investments\nAuthor: Michael Dunn\nPublisher: John Wiley and Sons\nCategory: Business & Economics\nMaking accountable marketing decisions to improve the efficiency of spending In this practical guide, Prophet CEO Michael Dunn teams up with marketing effectiveness expert Chris Halsall to help marketing managers and CMOs make better marketing spending decisions and better evaluate the success or failure of these decisions. They show how to sort through the clutter of metrics, measurement, and analytic options, and provide the practical information needed to help establish the marketing accountability imperative--highlighting the critical need for more effective stewardship of marketing spending.\nAuthor: David Gerrold\nLeft alone on the moon, Chigger and his two brothers are unable to return to an Earth that has descended into chaos and decide to leave Geosynchronous Station to find a planet where they can build new lives for themselves, but their plans are threatened by an interplanetary manhunt after the powerful synthetic intelligence that the three boys are rumored to be carrying. 20,000 first printing.\nNatural Design for the Real World\nAuthor: Victor J. Papanek\nPublisher: Thames & Hudson\nIn this book Papanek looks at the exciting possibilities for the future if architecture and design were to become environmentally and socially responsible. He shows how people can contribute to the well-being of the planet through awareness of design.\nAn Annotated Bibliography, 1960-1986\nAuthor: Suresh Kumar Gupta,I. C. Gupta\nPublisher: Concept Publishing Company\nAuthor: Anastassia Lauterbach,Andrea Bonime-Blanc\nThis practical guide to artificial intelligence and its impact on industry dispels common myths and calls for cross-sector, collaborative leadership for the responsible design and embedding of AI in the daily work of businesses and oversight by boards. • Provides a strategic framework for corporate boards and executive leadership teams to remain competitive in the age of AI • Offers practical and clear advice on AI and machine learning, introducing technical concepts and translating research trends into practical applications while simultaneously incorporating critical governance, ethics, sustainability, and risk considerations • Provides traditional businesses and their boards with practical questions to ask their teams, suppliers, and technology partners and offers guidance on market trends and players to which to pay attention"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:6c5cad58-24ae-4cb0-8e21-bddd03f0e698>","<urn:uuid:917113fc-8686-4642-9d7c-4b540dfed54c>"],"error":null}
{"question":"Which conservation organization manages more diverse wildlife habitats: Travis Audubon or Mass Audubon?","answer":"Mass Audubon manages a more diverse range of wildlife habitats. While Travis Audubon primarily focuses on bird habitats at their sanctuaries (Baker Sanctuary, Chaetura Canyon, and Blair Woods), Mass Audubon manages multiple habitat types including forests, grasslands, meadows, heathlands, shrublands, beaches, and living shorelines from the Berkshires to the Cape and Islands.","context":["About Travis Audubon\nTravis Audubon was founded in 1952 by visionary Central Texans who recognized the vital connection between conserving wildlife habitat and the ecological balance necessary for healthy, sustainable, and habitable communities.\nInspiring conservation through birding.\nTravis Audubon promotes the enjoyment, understanding, and conservation of native birds and their habitats through:\n- Land Conservation\n- Habitat Restoration and Management\n- Environmental Education\n- Conservation Advocacy\nTravis Audubon is an independent chapter of the National Audubon Society. We are a non-profit corporation with tax-exempt status, governed by a 12-person Board of Directors and managed by a full-time Executive Director. Many of our programs and activities are organized and led by a dedicated, experienced, and highly skilled corps of volunteers.\nLeast Grebe, Alan Schmierer, Creative Commons\nWhat we do\nTravis Audubon promotes the enjoyment, understanding, and conservation of native birds and their habitats.\nWe protect critical habitat for the endangered Golden-cheeked Warbler at our 715-acre Baker Sanctuary. Our Chaetura Canyon Sanctuary is home to more than 30 nesting avian species and is world renowned for research and conservation of Chimney Swifts. Blair Woods in east Austin is a ‘living lab’ for habitat restoration and education. Travis Audubon continues to spearhead conservation programs locally and support them abroad.\nWe lead diverse and interesting field trips and bird walks nearly every weekend and on weekdays, both at local birding hotspots and exotic getaways. We offer an exciting array of monthly speakers and workshops, as well as a variety of popular programs including Youth Birding Camp, our annual Birdathon, and several different seasonal bird counts and surveys.\nTravis Audubon offers year-round classes from birding basics to advanced classes for the identification of sparrows, raptors, gulls, butterflies and dragonflies, and even grasses. Our outreach programs strive to educate the community about the vital connection between conservation and sustainable, healthy human habitats.\nWhy it matters\nBirds are part of the natural system. They are pollinators for many plants as well as essential for seed dispersal. Rodent control would be an immense problem without birds to keep their numbers in check. Birds save millions of dollars a year by eating pests in gardens and farms, thereby reducing the amount of pesticides needed.\nBird studies help with scientific advancement. They teach us about climate and the environment. Birds are also key to indicating environmental changes. Large, rapid declines in bird populations can alert us to an environmental problem that needs immediate attention.\nBird watching is the fastest growing outdoor recreation in the country. According to the U.S. Fish and Wildlife Service, about 50 million Americans go birding and spend money on gear, gas, food, and lodgings.\nBirding is a growing hobby that appeals to all ages and provides a great family activity. Time spent in nature provides positive physical and mental benefits. Birding is a gateway activity to connect people with nature, building a deeper understanding of our relationship with the natural world and the role we all serve as stewards of the environment.\nThe beauty, song, and aerial abilities of birds have inspired artists, poets, and inventors for centuries. From the eagle to the dove, birds are iconic representations of our ideals and values.\nLearn more here: http://travisaudubon.org/","Mass Audubon protects thousands of acres of wildlife habitat across the state, maintaining and enhancing uncommon, exemplary, and vulnerable natural communities on our land.\nOur habitat management tools include invasive species control, prescribed burning, mowing, forestry, and carefully planned restoration, all done within an adaptive approach in which monitoring informs ongoing management decisions. We also advise other conservation organizations, municipalities, and private landowners on habitat management options and implementation.\nWhen so much of the Massachusetts landscape reflects the impacts of human activities, it is critical that we apply our knowledge of natural systems to optimize habitat for rare or declining species on our wildlife sanctuaries and beyond. This frequently involves a passive approach: monitoring our land to ensure our objectives are being met. However, in specific cases this requires active intervention to improve conditions for plants and wildlife.\nResilience is the capacity of a natural system to respond to disturbances by changing while maintaining basic functions such as pollination, plant succession and nutrient cycling. Climate change and other ecosystem stressors are altering the habitats of Massachusetts. Reducing as many stressors as possible increases resilience. We strive to reduce ecosystem stressors on all lands we manage. Examples include:\nInvasive Species Control\nInvasive species are one of the greatest threats to the nature of Massachusetts. These species out-compete, displace, or kill native species, altering the structure and composition of forests and other habitat types.\nWe actively control invasive species on our own sanctuaries, work with partners on regional efforts, and educate landowners about what they can do. Learn more >\nReducing White-tailed Deer Density\nWhite-tailed deer are a native species in Massachusetts, but in the absence of their natural predators, wolves and mountain lions, these charismatic animals have reached an unsustainably high population density in many parts of the state. At very high densities, white-tailed deer devour the forest understory, decreasing plant diversity, removing nesting and perching habitat for birds and other wildlife, and removing tree seedlings and saplings that should be the next generation of our forests. We have initiated limited hunting on some of our properties to bring deer density down to sustainable level. Learn more >\nWhere appropriate, we actively intervene to restore landscape function by undoing previous human development. We remove dams and other impediments to stream connectivity. We take down unused houses, barns, and other structures that fragment upland habitats. We convert former agricultural fields to native habitats such as floodplain forest.\nMaintaining Habitats for Uncommon Species\nMany species rely on habitats that require management to maintain suitable conditions. For example, grasslands and shrublands provide critical habitat for a particular suite of plants and animals. These species and habitats are among the most rapidly declining types in Massachusetts.\nWe employ mowing, grazing, prescribed fire, and other methods to maintain grasslands, meadows, heathlands, and shrublands from the Berkshires to the Cape and Islands. We also work with partners to create living shorelines and to restore eroding beaches to maintain nesting sites for coastal birds.\nForests of various ages provide many ecological services including wildlife habitat, flood and erosion control, public health benefits, recreational opportunities, and carbon sequestration. Protecting our forests from development is a crucial first step to secure these natural benefits.\nIn addition to forest protection, there are various ways to steward forests to retain or at times enhance their ecological services. This includes passive and active approaches. Learn more >"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:a6bfaffd-1ae6-4436-a358-79e6b74b985f>","<urn:uuid:52dd11de-1553-4508-8351-cd1f9c59a375>"],"error":null}
{"question":"What are the community engagement initiatives and partnerships that support the student farm?","answer":"The student farm has developed multiple community partnerships including collaboration with Boys and Girls Club, Freeport Park District, and Highland Area Garden Club for workshops and lessons. They receive support from various volunteers and organizations: Patrick Kennedy serves as a community volunteer, Rex Prince helps with disking, Grant McCarty from U of I Extension provides guidance on pest and disease management, and Pearl Valley Organix donates organic fertilizer. They also engage the community through Facebook updates, educational sessions at the garden, and partnerships with organizations like Archer Daniel's Midland, Seeds of Change Grant, Pioneer Seed Co., and others who have contributed to purchasing a greenhouse.","context":["- Agronomic: potatoes\n- Fruits: melons, apples, berries (other), berries (blueberries), cherries, pears, berries (strawberries)\n- Vegetables: sweet potatoes, asparagus, beans, beets, broccoli, cabbages, carrots, cauliflower, cucurbits, eggplant, garlic, greens (leafy), onions, peas (culinary), peppers, radishes (culinary), sweet corn, tomatoes, turnips, brussel sprouts\n- Education and Training: youth education\n- Farm Business Management: community-supported agriculture, marketing management, value added\n- Production Systems: organic agriculture\n- Sustainable Communities: ethnic differences/cultural and demographic change, leadership development, local and regional food systems, urban agriculture, urban/rural integration\nPROJECT DESCRIPTION AND RESULTS\nPrior to this grant, I was working as a classroom agriculture teacher. We would talk about sustainable agriculture in the classroom setting, but students were not able to engage in hands on sustainable agricultural production. We defined sustainability as being socially, financially, and environmentally sustainable. We also talked about food miles and investigated how far our food travels. We talked about integrated pest management too. Prior to the garden, we were only able to talk about these things from theoretical standpoints. With the garden, the students have experience to build knowledge on; they have practical applications/references to the theoretical concepts.\n- Establish a student farm that would serve as a space where students will be educated about sustainable vegetable production as well as sustainable marketing options for small farm businesses.\n- Provide urban students the opportunity to have an SAE (Supervised Agricultural Experience) which is critical to the success of an Illinois Agricultural Education program as part of the FFA program\n- Students will learn to grow vegetable produce and they will learn to market their produce through CSA (Community Supported Agriculture) market. Potential CSA members will include employees of Freeport School District and community members. The profit of the farm will be put back into the high school agriculture department to help fund student leadership, educational, and agricultural experiences.\n- Obtain administrator support (principals, superintendents).\n- Recruit students to work and manage the garden by re-marketing and urbanizing the high school agriculture program as relevant, healthy, and cool rather than associating it with a type of farming that urban students may have a stereotype against. We did this by changing our course names to Urban Vegetable Production, Food and the Environment, and Food Sales and Marketing.\n- Engage community members and school staff through sales of vegetables\n- Engage community volunteers to assist with tilling, mulching\n- Create a Facebook page to get the word out about what we are doing.\n- Allow students to take garden produce home to families\n- Host baking and preservation workshops with the students utilizing garden produce.\n- Create a recipe book with lots of recipes utilizing garden produce.\n- Engage community members through educational sessions at the garden\n- We worked with Boys and Girls Club, the Freeport Park District, and the Highland Area Garden Club to host workshops and lessons at our garden.\n- Patrick Kennedy- Community Volunteer, co-program manager\n- Rex Prince- He disked the garden for us three years in a row.\n- Grant McCarty, U of I Extension’s Local Foods Educator, mentored us on pest and disease problems in the garden.\n- Jack Carey, Freeport Park District, donated mulch to our garden project and tilled our garden for the 2015 growing season.\n- Cliff Luke and Cliff Lang, volunteers of Freeport Area Church Cooperative, take care of our seedlings at a greenhouse across town.\n- Pearl Valley Organix donated organic fertilizer to our garden in exchange for us running a test of their products vs. other organic fertilizers.\n- Archer Daniel’s Midland, Seeds of Change Grant, Pioneer Seed Co., Pearl City Coop, Stephenson Service Company, Country Financial, Windy Groves Inc., Karl McPeek Memorial, and other community donations have been pooled to purchase a greenhouse for our program.\n- Joel Riesman of Growing Power advised us on crop nutrient deficiencies, water levels, and general crop health.\n- The results we achieved were…\n- Engaged an increasing number of students in our agriculture department at Freeport High School from the 2012-2013 school year to the 2014-2015 school year. Each year, the number of students enrolled in courses and the number of students engaged in the garden increased. The first year, 21 students were enrolled in agriculture courses, the second year, that number increased to 50, and the third year, that number increased to 65. The first year, 4 students were engaged in the garden during the summer, the second year 8 students were engaged in the garden, and for the 2015 growing season, we have 12 students engaged.\n- Fundraised and increasing number of dollars through our vegetable sales from year 1 to year 2. The first year, we raised approximately $1,100. The 2nd year, we raised approximately $4,500.\n- The program is socially sustainable as students enjoy the work that they are doing and they are building friendships across racial lines.\n- The program is economically sustainable as students because produce sales cover the operating costs of the farm.\n- The program is environmentally sustainable as we manage the farm with sustainable practices including crop rotation, organic fertilization, and composting.\n- Students learn about all three pillars of sustainability (socially, financial, and environmental) in the courses that are taught.\n- The youth that we are trying to reach are 9-12 high school students, and those are the students we are currently reaching.\n- We share our project with the public as they visit our project throughout the summer weeks and we also post updates on our Facebook page. We also organized the following formal programs that educated people in the community about our project.\n- Boys and Girls Garden Lesson: May 2014\n- Highland Area Garden Club Tour: July 2014\n- Community Appreciation Banquet: December 4, 2014\n- Freeport Park District Special Education Garden Lesson: May 21, 2015\n- U of I Extension Fruit Tree Pruning Workshop hosted at our orchard: March 19, 2015\n- Future Outreach Plans: We hope to partner with the Sleezer Youth Home in Freeport as well as the department of corrections. The Sleezer Youth Home is a child welfare agency and a childcare institution providing residential housing to female youth in need of psychological treatment. We would like to provide programing for these girls at the student farm. We would also like to become recognized as a volunteer site with the department of corrections for teens who must participate in community service as a result of their correction plan.\n- Links to newspaper clippings:\n- Links to publications:\n- You may see many photos on our Facebook page at facebook.com/FreeportStudentFarm.\n- We are incredibly grateful to be a recipient of the SARE grant. Without this grant, I am not sure if we ever would have gotten started. Our program has grown, is quite successful, and we hope to continue to grow with the addition of a greenhouse to our program. Thank you so much for the opportunity that you have given us. Children and adults in our community are about to think about sustainable food production and use our student farm as an example."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:02aaa314-6b69-420f-9b09-c5aaf50f6203>"],"error":null}
{"question":"How do beliefs about spiritual judgment after death differ between the Hopi and ancient Egyptian traditions?","answer":"In the Hopi tradition, there isn't a formal judgment process after death - spirits simply journey to the underworld where they become friendly to the living and return with kachinas to bring rain. In contrast, ancient Egyptian beliefs included a complex judgment process in the Hall of Two Truths, where the deceased faced 42 judges and gods. Their heart was weighed against the feather of truth, symbolizing Maat the goddess of justice. If they passed this test, they entered the perfect afterlife; if not, they were sent to the Devourer of the Dead.","context":["Religious Beliefs. The Hopi universe consists of earth, metaphorically spoken of as \"our mother,\" the upper world, and the under world from which the Hopi came and to which their spirits go after death. Although the concept of original creation is unclear, there are various accounts of the Emergence into this present world from three preceding ones, the place of emergence, or the sipapu, being located in the Grand Canyon. Each of the preceding worlds came to an end because of some evil done by witches, and the present world will someday come to an end also. In order to forestall this and to keep the world in harmony, ceremonies are performed by ceremonial societies and by kiva members. The universe is balanced between a feminine principle, the earth, and a masculine one, manifested in the fructifying but dangerous powers of sun, rain, and lightning. Evil is caused by the deliberate actions of witches, called \"two-hearts\" because they have bargained away their hearts for personal gain and must steal another's heart to prolong their own lives. When a ceremonial leader is believed to \"steal\" the heart of a relative to ensure that the ceremony will be successful, there is an element of magical human sacrifice in this belief.\nThere are three major classes of supernatural. The most individualized are the gods and goddesses, each having his or her special area of concern. Figures or impersonations of these deities are used in ceremonial activity. The next category is the kachinas. A few of the kachinas are individuals, but most of them are classes of beings each with its different character and appearance. In kachina dances the dancers wear the costume appropriate to the kachina type they portray. Some types are more popular than others; new ones are invented and old ones drop out of use. Finally, there are the generalized spirits of natural objects and life-forms, who will be offended if one of their earthly representatives is treated improperly. Thus, when a game animal is killed, its spirit, and the generalized spirits of that animal type, must be placated.\nReligious Practitioners. The leaders of the clans that control ceremonies are the chief priests or priestesses of these ceremonies and clan members take leading roles in them. Every Hopi is initiated into one of the two kachina societies, which are responsible for putting on the kachina dances. In former times, every man joined one of the four fraternities that put on the Emergence ceremony, and most women joined one of the three sororities. There are also special-purpose societies, controlled by clans but open to membership to anyone in the village, which conduct ceremonies. Villages vary in the number of societies still in existence, but all put on kachina dances, which are organized through kiva membership.\nCeremonies. The Hopi follow a ceremonial calendar determined by solar and stellar positions. The ceremonial year begins with Wuwtsim, the Emergence ceremony, in November. Soyal, occurring at the time of winter solstice, is conducted by the village chief, and its officers are the men holding the leading ceremonial positions in the village. It is at this time that ceremonial arrangements for the coming year are planned. Powamuya, in February, is a planting festival in which beans are sprouted in the kivas in anticipation of the agricultural season. This is a great kachina festival, with many types being represented. Kachina dances begin after Soyal and continue until July, when Niman or Home Dance is held. This celebrates the return of the kachinas to their unearthly homes in the mountain peaks and the under world. Snake-Antelope and Flute Dances alternate biennially in August, the first emphasizing war and the destructive element and the second emphasizing the continuity of life after death. In September, Mamrawt, or the principal women's ceremony, is held. This contains many elements found in Wuwtsim. The other women's societies hold their ceremonies in October. Along with these ceremonies, there are some that are held only from time to time and others that have been defunct for many years. In addition, there are many small rituals. Accounts of the late nineteenth century indicate that hardly a day passed without some ritual activity taking place somewhere in each village. While ceremonies have specific purposes, all are in some way thought to bring rain, which is valued both for itself and as a symbol of abundance and prosperity. The kachinas, especially, are rain-givers. Kachina dances are joyous public events, consisting of carefully choreographed dance sets interspersed with comical performances of clowns. The clowns, like ignorant children, mock everything and understand nothing. Social deviants are shamed by the clowns' mockery.\nArts. Traditional objects are produced as art objects as well as for use. Kachina dolls, nonsacred representations of kachinas given to girls and women as symbols of fertility and for toys, became tourist items in the late nineteenth century and have undergone several stylistic revisions since then. Modern techniques of silverwork were introduced by American artists associated with the Museum of Northern Arizona in Flagstaff in the 1920s. Using Hopi designs, this is a flourishing craft. There are several contemporary Hopi painters in oil and other media, as well as poets and art photographers. Aesthetic standards for dance, song, and costume are high and clearly articulated.\nMedicine. Sickness can be brought on by witchcraft, by contact with dangerous forces like lightning, or, more commonly, by sad or negative thoughts, such as anger or jealousy, which disturb the harmony of the body. Curing is done by shamans who diagnose and heal the ailment or by members of ceremonial societies that control the cures for certain diseases. Today, most Hopis make use of government hospitals along with native home remedies and shamanistic treatment.\nDeath and Afterlife. A peaceful death in old age is a natural death. Other deaths may be attributed to witchcraft or the other factors causing disease. Burial by a son or other close relative is completed as soon as possible outside of the village. During its journey to the under world, the spirit of the dead may try to induce others to come with it, and various rites protect against this. Once safely in the under world, the dead are friendly to the living and will return to earth along with the kachinas to bring rain.","Definition of the Egyptian Afterlife\nWhat was the Egyptian afterlife and how was it described?. Definition of the Egyptian Afterlife: The Egyptian Afterlife was seen as a perfect existence in an ideal version of Egypt. The Afterlife was a place of complete bliss, delight and peace. The Afterlife was referred to as the Field of Rushes or Field of Offerings. Ancient Egyptians provided for their afterlives according to their earthly means. The Ancient Egyptians were preoccupied by death and believed that after death they would go to the dark and terrifying place called the Underworld. The Underworld was a land of great dangers and various tests through which every Egyptian would need to pass before passing into the Afterlife.\nEgyptian Religious beliefs led to the Egyptian Afterlife\nTo understand the Egyptian Afterlife it helps to be aware of the major elements of their beliefs and religion. The religion of the Ancient Egyptians was extremely important to them and their belief in various gods and goddesses were fundamental to their religion. Some of the gods looked after matters of daily importance and others governed the realms of the dead. The Egyptian priests created legends and myths about the Egyptian Afterlife and every Egyptian aspired to this perfect existence.\nHow the Soul entered the Egyptian Afterlife\nThe Egyptians believed that the soul were perishable and therefore at great risk. The tomb, the process of mummification, rituals and magic spells promoted the well-being, and ensured the preservation, of the dead and the elements of the soul called the Ka, Ba and Ahk. After undertaking the perilous journey through the underworld the deceased would face his day of judgement at the Hall of the Two Truths. The god of the dead, Anubis, would lead the dead to the Hall of Two Truths, where the deceased would stand in front of forty two judges and gods. The deceased was led to a set of scales where his or her heart, containing the deeds of their lifetime, was weighed against the feather of truth, which symbolised Maat the goddess of justice. The fate of the deceased would then be decided - either entrance into the perfect afterlife or to be sent to the Devourer of the Dead - the Great Swallower. If the deceased passed the test the judges in the Hall of the Two Truths pronounced the following divine order:\n\"He is justified. The Swallowing Monster shall have no power over him.\"\nDescription of the Ancient Egyptian Afterlife\nFollowing the ordeal in the Hall of Truths the deceased was welcomed by Osiris into the Egyptian Afterlife. The Pharaoh would join the realms of the gods and all others would enter Everlasting Paradise. In this beautiful world the life of the deceased was mirrored but there were no problems there was only happiness. The afterlife was seen as a perfect existence in an ideal version of Egypt. There were fields, crops and the celestial Nile. In this ideal land the deceased met his ancestors and the loved ones who had gone before him. He continued working in the role he had undertaken before death. But there was no hardship only joy and happiness. There were no disasters and the crops grew bigger and higher than those found on the mortal plane. His leisure activities were replicated as were all the pursuits of his mortal existence.\nProvisions for the Ancient Egyptian Afterlife\nAncient Egyptians provided for their afterlives according to their earthly means. When they died, the Egyptians put all the things in their tombs that they would need in the afterlife to reflect their lifestyle in the mortal existence including jewelry, furniture, clothes, knives, spoons, plates, cosmetics, ornaments, statues and tools. They made drawings of any items which were too big to fit in the tombs which they considered would be just as good as the actual items. These large drawings were often carved on to the walls of the tomb.\nAncient Egyptian Afterlife\nEach section of this Egyptian website addresses all topics and provides interesting facts and information about the Golden Age of Egypt. The Sitemap provides full details of all of the information and facts provided about the fascinating subject of Egypt, the Egyptians and of the Pharaoh Tutankhamun, King Tut."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:4c593e02-8557-4898-8101-23b999167bee>","<urn:uuid:a57a0581-3961-48af-8116-389cf862570e>"],"error":null}
{"question":"How do current healthcare market inefficiencies impact costs, and what solutions does value-based healthcare propose?","answer":"Healthcare market inefficiencies significantly impact costs due to unwarranted price variations and limited competition, with over half of markets having fewer than three hospital systems. Traditional market approaches have failed because patients rarely use price transparency tools and high-deductible plans reduce both high-value and low-value care indiscriminately. Value-based healthcare offers solutions by fundamentally changing payment structures to reward patient outcomes rather than service volume. This model implements coordinated care delivery through medical homes and ACOs, where providers share data and work as teams. The approach has shown promise in controlling costs by reducing unnecessary services, preventing chronic diseases, and decreasing hospitalizations, while simultaneously improving care quality and patient satisfaction.","context":["This is the last part of the Fundamentals of U.S. Health Policy series! And it’s a super interesting one. Michael Chernew, Ph.D., wrote about the role of market forces in U.S. health care. Since this is squarely in my area of focus, I have a lot of thoughts. Thus, this week I’ll stick to summarizing Dr. Chernew’s article, and then next week I’ll provide some commentary.\nForewarning, I’m following the paper’s logic flow, which, to my brain, is a little meandering, so it’s easy to lose one’s place, but I’ll clarify as much as I can now and then attempt to provide additional insight next week.\nRemember how Total Healthcare Spending = Price x Quantity? (Well, actually, it’s the sum of the price x quantity of all the different services being provided in our healthcare system.) Dr. Chernew is basically using that equation when he starts out by saying that our challenge is to reduce the quantity of low-value services provided and to lower prices.\nAnd then the big question . . .\nWhat role should markets play in doing that?\nHe finally gets to the answer at the end, which is that markets and government should both be used to complement each other. Markets can be leveraged inasmuch as they will help, and this should be paired with the government regulations needed to help them work as well as they can.\nI won’t list his specific recommendations quite yet about how we could do that because first I need to review what he says in the rest of the article about markets and how they work.\nFirst, he says that markets are the “foundation of our economy,” and they promote efficient production and cost-reducing innovation. He doesn’t exactly give the step-by-step explanation of how they do that, but you can gather it from his next several paragraphs. Markets create competition, which is when consumers (in this case, patients) have “the ability and incentives . . . to seek low-price, high-quality providers. . . .” And because of that competitive pressure to win consumers, the players in a market are forced to innovate in ways that make production more efficient.\nGreat, so a good healthcare market will help patients choose low-price, high-quality providers. Unfortunately, healthcare markets are more imperfect than other markets. Want a big piece of evidence for this? Look at the extent of unwarranted price variations that exist in healthcare. It’s way more than in other markets.\nBut why is the healthcare market so bad?\n“Competition in health care fails for several fundamental reasons. First, patients often lack the information needed to assess both their care needs and the quality of their care. Second, illness and health care needs are inherently difficult to predict, exposing people to financial risks that they must insure against. This risk gives rise to an insurance system that shields patients from the price of care, dampening their incentive to use care judiciously and to seek care from providers offering high-quality care at affordable prices. The information problem, amplified by insurance, reduces the ability and incentives for patients to seek low-price, high-quality providers and impedes well-functioning markets. This problem has been magnified lately by consolidation of health care providers.”\nSo, basically, it’s difficult for patients to really know what care they need, they have a hard time assessing quality of care, they’re shielded from prices because of insurance, and consolidation has limited their options. The result of all that is they have neither the ability nor the incentives to choose low-price, high-quality providers.\nThis, by the way, sounds almost exactly like what I’ve written (or linked to) a thousand times before, which is that patients need to start making value-sensitive decisions, and to do that they need (1) multiple options, (2) the ability to identify the value of each option, and (3) the incentive to choose the highest-value option.\nRegarding consolidation, he gives some interesting data, which show that only 51% of markets have 3 or more hospital systems.\nBased on all of that, many would conclude that we should abandon markets altogether in healthcare. But he says, “The weaknesses associated with market-based health care systems are severe, but that does not mean the market should be abandoned.”\nAnd then he proceeds to give a few examples of beneficial things that have come from markets already, such as new payment models, telemedicine, a shift from inpatient to outpatient care, and narrow networks (which allows for lower prices).\nThose, however, end up being overshadowed by the list of ways we’ve tried and failed to bolster market function by providing patients with better information about quality and prices and by changing insurance benefit designs.\nThe summary of this section of the paper is that giving patients better information about quality and prices have had very little success because . . .\n- Patients rarely use price- and quality-transparency tools\n- These sorts of decisions are complex\n- Patients fear disrupting their relationships with their physicians\nChanging benefit designs to get patients to directly pay for more of their care (e.g., implementing high deductibles) has had a larger effect on utilization, but it hasn’t significantly impacted the market because . . .\n- What tends to happen is higher-value and lower-value care both decrease\n- Not enough patients end up getting steered toward higher-value providers to actually impact market prices.\nHe provides his explanation for all these failures: “The core problem is that for markets to work, patients must face the economic consequences of their choices, but labor-market concerns dampen employers’ enthusiasm for adopting plans that impose such consequences.”\nIn the realm of getting patients to choose higher-value insurance plans, there’s been a little bit of headway with insurance exchanges, although there are many drawbacks to those, too . . .\n- Beneficiaries make poor plan choices\n- Insurance exchanges induce more price sensitivity, which leads people to choose lower-premium plans that impose greater financial risk on them, which they often cannot bear\nAnd, to make things worse, many of the downsides of insurance exchanges can worsen inequity.\nDr. Chernew is not exactly giving a glowing review of market-based reform attempts, is he? His comments are all accurate though.\nNext, though, he says that “in evaluating their merits, we need to compare them with other systems, such as government-run models.” And government-run models have their own set of limitations.\nLuckily, we are not facing an either-or decision. The important question is how government and markets can complement one another. “We do not need to abandon markets–we can make them better.”\nFinally, getting to his recommendations about how to use markets and government to complement each other, he says we could work to increase the effectiveness of transparency initiatives, limit provider consolidation, and impose gentle regulations to prevent the most severe market failures (like limits on surprise billing and instituting price caps on the most excessive prices).\nDr. Chernew’s conclusion is that, “If we fail to improve market functioning, stronger government involvement will most likely be needed.” Agreed.\nNext week, I’ll give my thoughts on all this!","Value-based healthcare is a healthcare delivery model in which providers, including hospitals and physicians, are paid based on patient health outcomes. Under value-based care agreements, providers are rewarded for helping patients improve their health, reduce the effects and incidence of chronic disease, and live healthier lives in an evidence-based way.\nValue-based care differs from a fee-for-service or capitated approach, in which providers are paid based on the amount of healthcare services they deliver. The “value” in value-based healthcare is derived from measuring health outcomes against the cost of delivering the outcomes.\nWhat Are the Benefits of Value-Based Healthcare Delivery?\nThe benefits of a value-based healthcare system extend to patients, providers, payers, suppliers, and society as a whole.\nManaging a chronic disease or condition like cancer, diabetes, high blood pressure, COPD, or obesity can be costly and time-consuming for patients. Value-based care models focus on helping patients recover from illnesses and injuries more quickly and avoid chronic disease in the first place. As a result, patients face fewer doctor’s visits, medical tests, and procedures, and they spend less money on prescription medication as both near-term and long-term health improve.\nProviders achieve efficiencies and greater patient satisfaction.\nWhile providers may need to spend more time on new, prevention-based patient services, they will spend less time on chronic disease management. Quality and patient engagement measures increase when the focus is on value instead of volume. In addition, providers are not placed at the financial risk that comes with capitated payment systems. Even for-profit providers, who can generate higher value per episode of care, stand to be rewarded under a value-based care model.\nPayers control costs and reduce risk.\nRisk is reduced by spreading it across a larger patient population. A healthier population with fewer claims translates into less drain on payers’ premium pools and investments. Value-based payment also allows payers to increase efficiency by bundling payments that cover the patient’s full care cycle, or for chronic conditions, covering periods of a year or more.\nSuppliers align prices with patient outcomes.\nSuppliers benefit from being able to align their products and services with positive patient outcomes and reduced cost, an important selling proposition as national health expenditures on prescription drugs continue to rise. Many healthcare industry stakeholders are calling for manufacturers to tie the prices of drugs to their actual value to patients, a process that is likely to become easier with the growth of individualized therapies.\nSociety becomes healthier while reducing overall healthcare spending.\nLess money is spent helping people manage chronic diseases and costly hospitalizations and medical emergencies. In a country where healthcare expenditures account for nearly 18% of Gross Domestic Product (GDP), value-based care has the promise to significantly reduce overall costs spent on healthcare.\nHow Does Value-Based Healthcare Translate to New Delivery Models?\nThe proliferation of value-based healthcare is changing the way physicians and hospitals provide care. New healthcare delivery models stress a team-oriented approach to patient care and sharing of patient data so that care is coordinated and outcomes can be measured easily. Two examples are reviewed here.\nValue-Based Care Models: Medical Homes\nIn value-based healthcare models, medical care does not exist in silos. Instead, primary, specialty, and acute care are integrated, often in a delivery model called a patient-centered medical home (PCMH). A medical home isn’t a physical location. Instead, it’s a coordinated approach to patient care, led by a patient’s primary physician who directs a patient’s total clinical care team.\nPCMHs rely on the sharing of electronic medical records (EMRs) among all providers on the coordinated care team. The goal of EMRs is to put crucial patient information at each provider’s fingertips, allowing individual providers to see results of tests and procedures performed by other clinicians on the team. This data sharing has the potential to reduce redundant care and associated costs.\nValue-Based Care Models: Accountable Care Organizations\nAccountable care organizations (ACOs) were originally designed by the Centers for Medicare & Medicaid Services (CMS) to provide high-quality medical care to Medicare patients. In an ACO, doctors, hospitals, and other healthcare providers work as a networked team to deliver the best possible coordinated care at the lowest possible cost. Each member of the team shares both risk and reward, with incentives to improve access to care, quality of care, and patient health outcomes while reducing costs. This approach differs from fee-for-service healthcare, in which individual providers are incentivized to order more tests and procedures and manage more patients in order to get paid more, regardless of patient outcomes.\nLike PCMHs, ACOs are patient-centered organizations in which the patient and providers are true partners in care decisions. Also like PCMHs, ACOs stress coordination and data sharing among team members to help achieve these goals among their entire patient population. Clinical and claims data are also shared with payers to demonstrate improvements in outcomes such as hospital readmissions, adverse events, patient engagement, and population health.\nHospital Value-Based Purchasing\nUnder CMS’s Hospital Value-Based Purchasing Program (VBP), acute care hospitals receive adjusted payments based on the quality of care they deliver. According to the CMS website, the program encourages hospitals to improve the quality and safety of acute inpatient care for all patients by:\n- Eliminating or reducing adverse events (healthcare errors resulting in patient harm)\n- Adopting evidence-based care standards and protocols that make the best outcomes for the most patients\n- Changing hospital processes to create better patient care experiences\n- Increasing care transparency for consumers\n- Recognizing hospitals that give high-quality care at a lower cost to Medicare\nCMS is expected to continue to refine its VBP measurements, making it important for hospitals to continuously improve their clinical outcomes so they can simultaneously improve reimbursement and their reputation among healthcare consumers.\nWhat Is the Future of Value-Based Healthcare?\nMoving from a fee-for-service to a fee-for-value system will take time, and the transition has proved more difficult than expected. As the healthcare landscape continues to evolve and providers increase their adoption of value-based care models, they may see short-term financial hits before longer-term costs decline. However, the transition from fee-for-service to fee-for-value has been embraced as the best method for lowering healthcare costs while increasing quality care and helping people lead healthier lives."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c49e0bc8-39fc-4713-be9c-670e4394c2ad>","<urn:uuid:9d09430b-00ce-4b27-9d69-a6706b4f1818>"],"error":null}
{"question":"Which had more devastating consequences: the potential future mega-earthquake in Manila Trench or the 1960 Valdivia earthquake in Chile?","answer":"Based on the information provided, the 1960 Valdivia earthquake in Chile was confirmed to have devastating consequences, causing around 1,655 deaths, 3,000 injured, and two million homeless people, with damage estimated between US$400-800 million. While the Manila Trench has the potential for a serious mega-earthquake, as it has stored considerable energy and has characteristics similar to other devastating earthquakes, its impact remains theoretical. The Manila Trench could potentially generate an 8.5-magnitude earthquake every 205 years or a 9.0-magnitude earthquake every 667 years, but since there are no records of any mega earthquake since 1560, we can only speculate about its consequences.","context":["Mar 17, 2011, 6 days after the mega earthquake hitting North-east Japan, Dr.Tso-Ren Wu, Associate Professor at the Graduate Institute of Hydrological and Oceanic Sciences of the National Central University of Taiwan, has published an article titled “Research and Reflection on Japan’s Mega Tsunami 2011”.\nIt explained that for such a massive tsunami to take place in Fukushima, it is because this tsunami has all the necessary conditions that enable a mega tsunami:\n1. It requires an undersea megathrust earthquake. This time the scale was a 9-magnitude megathrust earthquake.\n2. It must be a Thrust Fault (or reverse fault). This earthquake was caused by plate subduction, which is a standard Thrust Fault. Out-of-sequence thrust fault causes direct vertical displacement of the seafloor, carrying a column of water and thus forming tsunamis.\n3. The epicenter must be shallow. The focal depth of the epicenter was 25km in the South-Asia tsunami while that of Fukushima earthquake was only 10km. Therefore the energy release could be converted to tsunami energy more completely.\n4. The underwater depth must be deep to allow sufficient volume of water to store potential energy. The underwater depth of Fukushima Tsunami was about 3km. It is of sufficient water depth to store the seismic energy and be converted into Tsunami waves.\n5. There is a uniform slope. Slopes can magnify tsunamis and create Edge Wave Effect. There are some beautiful slopes in the outer ocean of Fukushima.\nAlthough Hong Kong is not located within the subduction zone, therefore seemingly less a chance for earthquakes to happen, does it really mean that mega earthquakes and tsunamis that took place in North-east Japan will not happen here in Hong Kong?\nWe got to understand that opposite to the shore of Hong Kong, at the eastern side of the South China Sea is the Luzon Island of Philippines. Its west coast sits the notorious Manila Trench.\nThe article points out that this trench has been identified as the most active trench in the world by the USGS. Its extremely long thrust length of approx. 1,500km is similar to that causing the South-Asia Tsunami. Also, it is a subduction plate in which the Eurasian Plate is subducting under the Philippine Sea Plate.\nThe GPS geodesy measurements show that the convergence rate across the Megathrust is at 8.7cm per year, which is considered as an active thrust fault.\nAlso, there is no record of any mega earthquake since 1560, a time when the Philippines started written records; even the monitoring in recent years has not shown any earthquakes that are of magnitude 8 or above.\nThis is very similar to the South-Asia and the Fukushima Tsunamis. Energy is being accumulated under the crust which is potentially of serious danger.\nThe article also points out that according to the analysis of earthquake recurrence interval, the recurrence interval of a 8.5-magnitude earthquake in the Manila Trench is 205 years and that of a 9.0 magnitude is 667 years.\nFrom the data above, it shows that the Manila Trench has already stored a considerable amount of energy that can potentially lead to a mega earthquake in the future.\nAnd if a mega earthquake takes place in the Manila Trench, the tsunami created can potentially be impacting Hong Kong.","List of Most Dangerous Earthquakes Ever Recorded in History\nThis list provides information about earthquakes in all parts of the world, which are handed down through historical reports or because of their impact through media reports. The preconceived map shows the concentration of earthquake epicentres along the tectonic plate boundaries.\nEarthquakes that occur under the seabed are often referred to as “seaquakes” or “seaquakes”. If, in this case, raised areas and / or depressions occur over large areas, large volumes of seawater are displaced jerkily. The energy is spread out over water waves and carries the additional danger of tsunamis on coasts.\nHere is the list of the 10 Most Powerful Earthquakes Ever Recorded:-\n10. 1833 Sumatra earthquake\n|Date: November 25, 1833|\n|Location: Sumatra, Indonesia|\n|Duration: 5 minutes|\nThe earthquake in Sumatra occurred on November 25, 1833, at approximately 22:00 local time, with an estimated magnitude of M w = 8.8 to 9.2. This caused a great tsunami that flooded the southwestern coast of the island. There are no reliable records of loss of life, but it is described as numerous victims. The magnitude of this event has been estimated using the elevation records taken from the micro-atoll coral.\n9. 1762 Arakan earthquake\n|Date: April 2, 1762|\n|Location: Chittagong, Bangladesh|\n|Duration: 4 minutes|\nThe 1762 Arakan earthquake occurred at about 17:00 local time on 2 April, with an epicentre somewhere on the coast from Chittagong (modern Bangladesh) to Arakan in modern Burma. It had an estimated magnitude of as high as 8.8 on the moment magnitude scale and a maximum estimated intensity of XI (Extreme) on the Mercalli intensity scale. It triggered a local tsunami in the Bay of Bengal and caused at least 200 deaths. The earthquake was associated with major areas of both uplift and subsidence. It is also associated with a change in course of the Brahmaputra River to from east of Dhaka (Old Brahmaputra River) to 150 km to the west via the Jamuna River.\n8. 869 Sanriku earthquake\n|Date: July 9, 869|\n|Location: Pacific Ocean, Tōhoku region, Japan|\nBasierend auf den beschriebenen Schäden für diesen Ort, die auf eine seismische Intensität von mindestens der Stufe 5 schließen lassen, wird vermutet, dass das Erdbeben eine Magnitude von 8,3 hatte. Eine Simulation durch Minoura et al. von 2001 lokalisierte das Erdbeben zwischen 37° und 39° N, 143° und 144,5° O, wobei die Verwerfung etwa 200 km lang, 85 km breit war und in 1 km Tiefe stattfand. Der Tsunami besaß demnach dann eine Höhe von etwa 8 m. Satake et al. bestimmten 2008 die Verwerfung mit einer Länge von 100 bis 200 km und einer Breite von 100 km bei einer Momenten-Magnitude MW 8,1 bis 8,4. Die Erdbeben-Datenbank des National Geophysical Data Center der US-amerikanischen NOAA gibt eine Oberflächenwellen-Magnitude von MS 8,6 an.\nGeologische Untersuchungen fanden marine Sedimentablagerungen, die auf diesen Tsunami zurückzuführen sind, in der Ebene zwischen dem heutigen Sendai und Sōma mehr als 4–4,5 km landeinwärts. Allerdings lag die Ebene damals etwa einen halben Meter niedriger als heute. Dies bestätigt die beschriebenen großflächigen Überflutungen und die hohe Zahl der Todesopfer (verglichen mit der Tatsache, dass die Gegend damals weit weniger und nur sehr verstreut besiedelt war). So wird für das 8. Jahrhundert für diese zweitbevölkerungsreichste Provinz eine Bevölkerung von 186.000 angenommen.\nZudem wurden Hinweise auf zwei ähnlich verheerende, vorangegangene Tsunamis mit ähnlichen Auswirkungen gefunden: einen zwischen 910 und 670 v. Chr. und einen zwischen 140 v. Chr. und 150 n.Chr. Basierend darauf wird angenommen, dass derartige Tsunamis diese Küstengegend etwa alle 800 bis 1100 Jahre, bzw. unter Hinzunahme des Keichō-Sanriku-Erdbebens 1611 alle 450–800 Jahre treffen. Minoura et al. meinten 2001, dass ein ähnlich starker Tsunami, der etwa 2,5–3 km ins Land eindringe, zu erwarten sei. Diese Vorhersage wurde häufig mit dem Tōhoku-Erdbeben und -Tsunami vom 11. März 2011 identifiziert und dieses wiederum dem Jōgan-Erdbeben 869 gleichgestellt.\n7. 1700 Cascadia earthquake\n|Date: January 26, 1700|\n|Location: Pacific Ocean, USA and Canada|\nThe Cascadia earthquake of 1700 was a mega-shooter between 8.7 and 9.2 on the momentum scale, which occurred in the Cascadia subduction zone on January 26, 1700. In the earthquake were involved the Plate of Juan de Fuca and the Pacific Plate from the island of Vancouver in Canada to the northern coast of California in the United States. The breakage size of the fault was estimated at about 1000 kilometers with a slip of at least 20 meters.\nThe subsequent tsunami struck the east coast of Japan.\nThe earthquake originated at 21:00 on January 26, 1700, although there are no exact records of the time and even of its occurrence. Indigenous texts reveal that a major earthquake occurred, as did Japanese records claiming that large quantities of red cedar were killed by the pre-tsunami sea retreat.\n6. 1868 Arica earthquake\n|Date: August 13, 1868|\n|Location: Arica, Chile (then Peru)|\n|Duration: 5-10 minutes|\n|Max. intensity: XI (Extreme)|\nThe earthquake of Arica of 1868 was an earthquake registered the 13 of August of 1868 near the 16:00 local time. Its epicenter was located at -18,500, -70,350 off the coast of Arica, the present capital of Arica and Parinacota Region, Chile (then capital of the Province of Arica, Department of Moquegua, Peru) and estimated Which released an energy equivalent to an earthquake of 9.0 Mw.\nThe earthquake event devastated much of southern Peru, especially the cities of Arequipa, Moquegua, Tacna, Islay, Arica and Iquique (the latter two currently in Chile). The earthquake was also perceived differently between Lambayeque in the north and Valdivia in the south, and even Cochabamba in Bolivia. Following the main movement, a tsunami swept the Peruvian coasts between Pisco and Iquique and crossed the Pacific Ocean, reaching even California, the Hawaii Islands, the Philippines, Australia, New Zealand and Japan.\nThe estimated death toll would reach 30 people in Chala, 10 in Arequipa, 150 in Moquegua, 3 in Tacna, 300 in Arica and 200 in Iquique.\n5. 1952 Severo-Kurilsk earthquake\n|Date: November 4, 1952|\n|Location: Kamchatka, Russian SFSR, Soviet Union|\n|Casualties: 2,336 dead|\n|Tsunami: 18 m (59 ft)|\nThe main earthquake occurred at 16:58 GMT (4:58 local time) on November 4, 1952. Initially assigned a magnitude of 8.2, the earthquake was revised to 9.0 Mw in recent years. A large tsunami resulted, causing the destruction and loss of life around the Kamchatka peninsula and the Kuril islands. Hawaii was also hit, with estimated damages of up to $ 1 million and livestock losses, but no human casualties were reported. Japan also reported no casualties or damage. The tsunami hit places as far as Alaska, Chile and New Zealand.\nThe hypocenter was located at 52.75 ° N 159.5 ° E, at a depth of 30 km. The length of the fracture subduction zone was 600 km. Replicas were recorded in an area of approximately 247,000 km2 with an epicenter at a depth of km between 40 and 60. A recent analysis of the distribution of the tsunami on the basis of historical and geological records gives some indications about the slip breaking off.\n4. 2011 Tōhoku earthquake and tsunami\n|Date: March 11, 2011|\n|Location: Pacific Ocean, Tōhoku region, Japan|\n|Casualties: 15,894 deaths, 6,152 injured, 2,562 people missing|\n|Duration: 6 minutes|\n|Depth: 29 km (18 mi)|\n|Total damage: US$14.5 to $34.6 billion|\nThe 2011 earthquake of the Pacific coast of Tōhoku in Japan is a magnitude 9.0 earthquake that occurred off the northeast coast of Honshū Island on March 11, 2011. Its epicenter is located at 130 Km east of Sendai, capital of Miyagi Prefecture, in the Tōhoku region, located about 300 km northeast of Tokyo. The maximum seismic intensity is recorded at Kurihara and is 7 on the Shindo scale (highest grade). It spawned a tsunami whose waves reached a height estimated at more than 30 m in places. They have traveled up to 10 km inland, ravaging nearly 600 km of coastline and partially or totally destroying many towns and harbor areas.\nHowever, this earthquake of magnitude 9 is responsible for only a few victims and damage due to the quality of Japanese earthquake resistant construction. The magnitude of the disaster is largely due to the tsunami that followed, which is responsible for more than 90% of the 18,079 deaths and missing persons, destruction and injuries. This tsunami also resulted in the Fukushima nuclear accident placed at level 7, the highest on the international scale of nuclear events (INES) of nuclear and radiological accidents.\nThe reconstruction will take several years and its estimated cost is already the most expensive earthquake in history after that of Kobe in 1995. Estimated economic losses are in the order of $ 210 billion.\n3. 2004 Indian Ocean earthquake and tsunami\n|Date: December 26, 2004|\n|Location: Indian Ocean, Sumatra, Indonesia|\n|Casualties: 230,000–280,000 dead and more missing|\n|Duration: 8.3 and 10 minutes.|\n|Depth: 30 km (19 mi)|\nThe earthquake in the Indian Ocean, also known as the Sumatra Andaman quake, on 26 December 2004 at 00:58 UTC (07:58 UTC time in West Indonesia and Thailand) was a submarine megathrust earthquake with a magnitude of 9, 1 and the epicenter 85 km off the northwest coast of the Indonesian island of Sumatra. It was the third largest ever recorded quake and triggered a series of devastating tsunamis on the coasts of the Indian Ocean. Tourists spent their Christmas holidays on many coastal sections; On the beaches were many people for sunbathing and bathing. In total, the earthquake and its consequences killed some 230,000 people, of whom 165,000 alone were living in Indonesia. Over 110,000 people were injured, over 1.7 million coastal inhabitants around the Indian Ocean were homeless. The event was exceptionally well documented: many tourists had a videocamera to hand or a digital camera, which could also film.\n2. 1964 Alaska earthquake\n|Date: March 27, 1964|\n|Location: Prince William Sound, Alaska, United States|\n|Casualties: 139 killed|\n|Duration: 4-5 minutes.|\n|Depth: 25 kilometres (16 mi)|\n|Total damage: $311 million|\nThe Good Friday quake, also known as the Great Alaska Quake, was the strongest single earthquake in the history of the United States. After the earthquake of Valdivia in 1960 it was the earthquake with the second highest magnitude since the beginning of the regular recording of earthquakes carried out from around 1950 onwards.\nIt occurred on March 27, 1964 at 5:36 pm local time (March 28, 03:36 UTC) and had a moment magnitude of MW 9.2. The epicenter was located in Prince William Sound in southern Central Alaska. Most of the damage occurred in Anchorage, 120 kilometers northwest of the epicenter.\nBy the quake, 125 people were killed, almost all by tsunamis, which reached the fjords of the Prince William Sound and the Kenai Peninsula and reached a maximum height of about 67 meters. Victims were also reported from California and Oregon. The quake lasted nearly three minutes in Anchorage. The biggest destruction in the city was caused by landslides and massive land shifts. Nearly every house near the Turnagain Heights was destroyed by the quake.\nIn the novel The smell of houses of other people of the American-American Bonnie-Sue Hitchcock from 2016, the quake plays an essential role, in that it plays a vital role for some of the youthful protagonists, because of the dying death of their father.\n1. 1960 Valdivia earthquake\n|Date: May 22, 1960|\n|Location: Valdivia, Chile|\n|Duration: 11–13 minutes|\n|Depth: 33 km (21 mi)|\n|Total damage: US$400 million to 800 million|\nThe earthquake of Valdivia on May 22, 1960, also known as the Great Chile Earthquake, was the earthquake with the world’s largest ever recorded magnitude and the heaviest earthquake of the 20th century. At 3:11 pm local time (19:11 UT), the quake on the moment magnitude scale reached a value of Mw 9.5. The topographic shape of large areas of the Little South of Chile was changed, especially the area around the provincial capital Valdivia.\nThe earthquake triggered a tsunami, which caused serious destruction throughout the Pacific. An estimate of the United States Geological Survey (USGS) is about 1,655 deaths, 3,000 injured and two million homeless."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:fcd935cb-75b2-4ac9-ad4a-a2fbf5a0e1bd>","<urn:uuid:31e78549-e9c7-4f34-b72d-2288144ba48f>"],"error":null}
{"question":"How do the requirements for altitude measurement equipment compare between standard aircraft operation and RVSM operation between FL290 and FL410?","answer":"For RVSM operation between FL290 and FL410, aircraft must have two independent altitude measurement systems, an altitude-reporting transponder, an altitude alert system, and an automatic altitude control system. Additionally, an operational autopilot or automated attitude control system is required that can work with either of the two altitude measurement methods. In contrast, standard aircraft operations have less stringent requirements, mainly focusing on having functional pitot-static systems with proper moisture drainage and ice prevention for IFR-certified aircraft.","context":["Basic for flight\nBy Jim Sparks\nAir is one of the basics needed to support life. In fact for most of us, it is also the basis for our profession. After all, without air, there would be no aircraft. Going back to the first days of A&P school, we learn that when there is adequate airflow over an airfoil, the result is lift. How does a pilot know when the aircraft is capable of sustaining flight? It is not by coupling a speedometer to a wheel on the landing gear. It is of course by comparing the ram air pressure generated by aircraft forward movement to relative atmospheric pressure. As lift increases, flight is achieved and height is measured by sensing the air pressure of the atmosphere. Obviously, at higher altitudes, there is less air pressing down, resulting in a lower air pressure. The pilot will utilize the airspeed indicator and altimeter to determine appropriate aircraft speed and relative position in the atmosphere. A dedicated system is needed to supply the various pressures to the components used to calculate and display this information to the flight crew. Plumbing, consisting of either rigid or flexible tubes, is routed strategically to bring the specific air pressures to the necessary equipment.\nWhat could possibly be complicated with a system such as this? Many Airworthiness Authorities around the world have adopted a policy of verifying the integrity and accuracy of altitude indicating systems every 24 months. In the United States, this requirement is listed in FAR 91 section 411. Many aircraft today are complying with Reduced Vertical Separation Minimums (RVSM), which will allow aircraft to maintain 1,000 feet vertical separation rather than 2,000 feet in certain areas. In this situation, errors resulting in the static system have to be closely monitored.\nProper tube installation is critical\nPart 23 (Airworthiness Standards), of the Federal Aviation Regulations (FAR), lists numerous situations that should be avoided to insure integrity of the system. The design and installation of a static system should provide for positive moisture drainage. Proper tube installation is critical. First, the material used should be suitable for the application — many aircraft utilize rigid aluminum tubing. This of course should always be inspected for the obvious chafing or dents. Anytime a dent is observed, cracks should be expected. In all cases, this should be checked with a magnifying glass. As tube bending results in a reduction in material thickness, all bend radius should be considered as potential for leakage. Another factor to consider when replacing a rigid line is the type of material. Some alloys have different degrees of porosity, which means small holes may open anytime the metal is stretched (once again bending is an issue).\nCorrosion is another concern with metal lines and in accordance with FAR 23.1325, corrosion protection should always be applied. Also, certain adhesives may react with certain alloys that can create a situation where corrosion occurs when a replacement tube needs to be manufactured or an identification decal needs to be applied to an existing line. Flexible lines are also widely used in air data systems and just like in their rigid system counterparts, numerous types of plumbing are used. Older aircraft often used synthetic rubber lines. Components of this type were often resilient to damage and provided ease of removal and installation of attaching equipment.\nOne drawback to the use of synthetic rubber is that it tends to break down with age. Most aircraft using these hoses have a mandatory time change or life limit.\nLater technology introduced Teflon® and plastic lines and fittings. Plastic tubing, although durable, may not possess the same heat resistance as metal lines. In the event the plastic should come in contact with some type of electronic component that produces heat, significant damage or even failure could result. Just like rigid plumbing, flexible tubes are prone to damage if proper bend radius is not properly observed.\nAppropriate documentation should always be referenced prior to conducting maintenance. Correct techniques must be utilized during disassembly and re-assembly to prevent damage. In a non-pressurized aircraft, the integrity of static lines may not be a significant issue as cabin inside pressure is essentially the same as outside. A proof test of the plumbing is required per FAR Part 23.1325 and for non-pressurized aircraft, the static system should have a suction applied that will lift a column of mercury approximately one inch. This is equivalent to a reading on the altimeter of about 1,000 feet above present field elevation. Then, after one minute without additional suction, the column of Mercury should not drop more than ten percent (that is about 100 feet on the altimeter).\nIn an aircraft with a pressurized cabin, the testing is a bit more stringent. The static system has to be tested at a pressure differential equivalent to the cabin maximum differential pressure. An aircraft that has a differential of around eight pounds per square inch can typically maintain a sea-level cabin up to an altitude of 23,000 feet.\nIn this case, the static system would have its pressure adjusted to equal an altitude of 23,000. With the suction removed, leakage should be less than two percent of the testing altitude or 100 feet, whichever is greater. The aforementioned tests are part of the certification process and should not be substituted for testing procedures called out in specific aircraft maintenance manuals. They are however, a good way to validate the integrity of the plumbing.\nEach airspeed indicating system must also be calibrated. United States FAR 23.1323 provides a guideline for determining system error. In this case, three percent of calibrated airspeed or up to five knots is considered acceptable. Defects in plumbing, such as those discussed earlier, can create unacceptable system errors.\nNeither snow, nor rain, nor heat\nPitot systems installed on aircraft certified for Instrument Flight Rules (IFR)are required to have some provision to prevent ice formation in the sensor as well as provide moisture drainage. In some cases, aircraft flying in intense rainstorms may encounter situations where the pitot tubes ingest significant amounts of water. Often, the wattage of the pitot tube heating element may be determined not only on the ability to prevent ice build-up, but also to be able to help relieve water saturation. In all cases, caution should be exercised when operating the heating circuits as personal injury or physical probe damage can result.\nA moisture-laden pitot-static system can also result if the aircraft is washed without using specifically designed protective covers for all air data inlets. These covers should be frequently inspected to verify proper fit and function as damaged covers could promote rather than prevent the flow of water into the air data system.\nOne result of excess water ingress in a pitot system may be that freezing temperatures might cause obstructed airflow. This would result in the airspeed indicator having a constant pressure available even though the aircraft speed may change significantly. Although most aircraft manufacturers install moisture drains in air data systems, they might not be in the exact location where moisture may accumulate under all circumstances.\nAn effective method of moisture removal is to disconnect all components attached to the system and cap the lines. Introduce very low pressure nitrogen, then slightly loosen the caps to allow a slight flow, with the majority of the nitrogen exiting through the pitot or static ports. By allowing a constant flow for up to 15 minutes, the majority of moisture can be evacuated.\nAnother common fault is air data system leakage. Should the leak occur within the cabin of a pressurized aircraft, the altitude will often appear lower than actual and the error will vary with both cabin pressure differential and aircraft altitude.\nLocation, location, location\nThe location and attachments of pitot and static system sensing devices is every bit as critical as the condition of the plumbing. Pitot tubes are mounted on the aircraft in such a way that they are always sensing uninterrupted airflow relative to fuselage at a specific angle. In fact, on Transport Category Aircraft where independent air data systems are required for pilot and co-pilot, the location of air data probes on the fuselage have to be such that a single bird strike will not take out both systems.\nRVSM and static pressure sensing\nReduced Vertical Separation Minimums (RVSM) has played an important role in the maintenance and inspection of static pressure sensing. Aircraft that wish to take advantage of the most frequently used altitudes in the high-density tracks over the North Atlantic have to have a very accurate altitude indicating system. Part of the process is to check the condition of the aircraft fuselage forward of the static ports. A ripple in the skin, a rivet protruding into the airstream, or even a build-up of paint at the leading edge of the static atmospheric pressure sensor can cause a discrepancy in the airflow over the static port and introduce a source error. This error will often vary with changes in airspeed or aircraft deck angle. In the event of a reported discrepancy from the crew regarding indicated altitude, one of the first questions asked should be if aircraft speed or attitude has any affect on the reported problem. Part of the qualification for the aircraft to be RVSM compliant is a specific Maintenance and Inspection Program, which may require personnel to attend special training programs. Depending on aircraft registration and mode of operation (FAR 91,121,135), not just any A&P can sign off maintenance on the static system. Also, any damage to the fuselage forward of the static ports may negate RVSM compliance. Airframe manufacturers frequently will locate static ports on the forward fuselage just after the removable nose compartment. An inappropriately secured nose compartment could conceivably lead to significant static source error. In the event testing with a pitot static test set cannot reveal any sign of problem, static source error should be considered and should involve detailed inspection of the area forward of the static ports.\nIn many aircraft, static sensing ports are installed on both sides of the fuselage. This provides a redundancy if one port becomes plugged but also provides a truer static sense in the event of a prolonged yaw condition.\nAnother regulation that comes to mind during the discussion of Altitude Indicating Systems is FAR 43 Appendix E. This is an Altimeter System Test and is divided into two parts. The second part deals strictly with the altimeter and is often accomplished in an approved shop, with the altimeter installed on a test bench.\nThe first part of the test involves the aircraft system and should include a comprehensive visual inspection to make sure the system is free of moisture and that the plumbing is not restricted or damaged. Next, the system is pressurized and leakage is monitored. Another requirement is a visual inspection of the airframe surface in the area around the static source that could introduce an error. Finally if the static port has a heater installed, a functional test is performed to verify proper operation. FAR 91.411 will even refer back to redoing the system leak test anytime a component or line is disconnected — except for the opening of water drains or activation of an alternate static source. Strangely enough, on many aircraft, the leading cause of Air Data leaks are malfunctioning moisture drains.\nIt is a good maintenance practice to leak check the system anytime anything is opened.\nWith troubleshooting pressure sensing systems, a good flight crew debriefing can really be a time saver. When does the problem occur? Are cabin pressure or aircraft attitude a factor? Also ask about flying conditions such as rain and if the aircraft was recently washed.\nPitot Static Systems —Simple? Maybe, but there sure is a lot to consider.","Reduced Minimum Vertical Separation (RVSM) is specified to reduce the vertical separation between aeroplanes at flying levels from 29,000 feet to 1,000 feet from 29,000 feet to 41,000 feet. RVSM has been developed to improve the capacity of airspace and offer access to fuel efficiency standards. RSVP is a global norm requiring RVSM approval for aircraft flying between 290 and 410 flight levels (including). The height-keeping monitoring of an aircraft is a significant component of the Approval process, as it supports the safety assessment and safety monitoring function necessary to carry out RVSM. To preserve RVSM certification status, the operator must comply with their minimal aircraft monitoring standards defined by their respective State authorities. RVSM training is necessary for pilots permitted to operate between FL 290 to FL410. The RVSM training requirement comes from your Ops Specifications in Part 135 or LOA B046 in Part 91.\nTo get the Ops Spec or LOA, you must indicate how you intend to comply in your training manual (part 135) or your handbook for operations (part 91). Details on compliance with B046 are provided in AC 91-85A/B. It also defines the necessity for initial and “recurring” training and the standards for knowledge to be satisfied. Themes such as pre/in-flight, MEL, TCAS, Mountain Wave/Turbulence, etc. — Themes that impact you’ll be appropriately segregated. One way to comply with this requirement is by receiving a certificate upon completion of this course. Additionally, the recurrent RVSM training is needed to be completed every 12 calendar months for pilots flying under the terms part 91K, part 121 and part 135\nThe requirements to operate RVSM include:\n- 2 Independent measurement systems for altitude\n- Altitude-reporting transponder (SSR) secondary radar surveillance\n- System of Altitude Alerts\n- An Altitude Control System Automatic\n- Additionally, an autopilot of an automated attitude control system in the RVSM airspace is needed. It must also be operational from any of the two different methods for measuring altitudes.\n- However, The RVSM airspace must not be entered by a traffic collision prevention system (TCAS). The only need is for TCAS II to be upgraded to version 7.0 or later if TCAS II is already installed.\n- Unless the Authority agrees, an operator shall not operate an aeroplane in designated areas of airspace, where a vertical separation of at least 300 m (1000 ft) shall apply under the regional air navigation agreement (RVSM Approval). SPA.RVSM.100 and SPA.RVSM.110, EU-OPS 1.241 EASA IR- OPS SPA.RVSM.110 See EU-OPS 1.872 as well.\n- The State shall be satisfied before approving the RVSM…that:\na) The aircraft’s vertical navigational capability satisfies the (as required);\nb) The operator has established appropriate procedures for the continuing operation of air navigability practices and programs (sustainable maintenance and repair); and; and\nc) The operating procedure for flight crews in the RVSM airspace has been established by the operator.\nBenefits of RVSM\n- In terms of economic and on-road airspace capacity, the RVSM airspace delivers substantial benefits.\n- Reduced vertical separation from the standard 2,000 feet to 1,000 feet to 300 metres. Between the FL290 and FL410 aircraft. add six more stages of flight:\nFL 300, FL 320 FL, 340,FL 360, 380 FL, 400 FL, 340, 380."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:504d32be-b386-4d97-8e43-824f4d379483>","<urn:uuid:02a9038d-a15c-47ad-b36b-15cc00ca4f1a>"],"error":null}
{"question":"How did the USS Housatonic contribute to mine warfare during WWI, and what was the historical significance of German mine development both before and after that conflict?","answer":"The USS Housatonic made significant contributions to WWI mine warfare as part of Mine Squadron 1, laying a total of 9,339 mines across 13 minelaying excursions from June to October 1918 near Inverness, Scotland. The ship could carry up to 900 mines and was equipped with six Otis elevators capable of transferring two mines every 20 seconds from storage to launching deck. As for German mine development, they had established a strong mine warfare capability before WWI, with large stocks of reliable Hertz horn contact mines. Following WWI, Germany continued advancing mine technology, establishing a mine warfare research and development command in 1920. They successfully tested airborne mines in 1931 and developed acoustic and magnetic mines before WWII. The Germans' expertise in mine warfare was so respected that the British even copied a captured German Hertz horned mine in 1917 to produce their first reliable contact mine.","context":["USS Housatonic (SP-1697)\n|Name:||USS Housatonic (SP-1697)|\n|Namesake:||The Housatonic River|\n|Builder:||built in 1899 by Newport News Shipbuilding in Norfolk, Virginia.|\n|Commissioned:||25 January 1918|\n|Decommissioned:||5 August 1919|\n|Type:||Minelayer (in 1918)|\n|Length:||405 ft (123 m)|\n|Beam:||48 ft (15 m)|\n|Draft:||20 ft (6.1 m)|\n|Capacity:||830 mines (900 max)|\n|Crew:||21 officers and 400 men|\nThe second USS Housatonic was the Southern Pacific freighter El Rio temporarily converted for planting the World War I North Sea Mine Barrage. Newport News Shipbuilding and Dry Dock Company launched El Rio at Newport News, Virginia in 1899 for service between New York City and Gulf of Mexico seaports of New Orleans and Galveston, Texas. The United States Shipping Board took control of the ship from Southern Pacific Steamship Company in 1917.\nShe was fitted out for United States Navy service at Tietjen & Lang's shipyard at Hoboken, New Jersey. Work began on 25 November 1917. Gun platforms were added for two anti-aircraft guns forward and a 5\"/51 caliber gun aft. The minelaying conversion enabled her to carry mines on three decks, and included six Otis elevators individually capable of transferring two mines every 20 seconds from the storage decks to the launching deck. Stern ports were cut for launching the mines and the rudder quadrant was raised to give adequate clearance. Watertight subdivision was improved by strengthening existing bulkheads and building two new bulkheads to divide the largest compartments so the ship might stay afloat if only one compartment were flooded. Quarters were enlarged to accommodate messing and berthing arrangements for a crew of about 400. The main machinery was overhauled and auxiliary machinery was added for the elevators, for heating the berthing spaces, for refrigerated food storage, for additional fresh water distilling capacity, for magazine sprinklers and galley and washroom plumbing, and enlarged electric generators for lighting and radio communications. Existing coal bunkers on the third deck were replaced with a bunker in the hold forward of the boiler room with chutes to load coal over the mines. Larger boats and heavier anchors required larger davits and anchor windlass, and the mines required specialized handling machinery.\nUSS Housatonic was commissioned on 25 January 1918 with Captain John Greenslade, USN, in command. While operating as part of Mine Squadron 1 out of Inverness, Scotland, from 7 June until the close of the war on 11 November 1918, Housatonic laid a total of 9,339 mines:\n- planted 769 mines during the 1st minelaying excursion on 7 June,\n- planted 800 mines during the 2nd minelaying excursion on 30 June,\n- planted 840 mines during the 3rd minelaying excursion on 14 July,\n- planted 830 mines during the 4th minelaying excursion on 29 July,\n- planted 320 mines during the 5th minelaying excursion on 8 August,\n- planted 810 mines during the 7th minelaying excursion on 26 August,\n- planted 820 mines during the 8th minelaying excursion on 7 September,\n- planted 830 mines during the 9th minelaying excursion on 20 September,\n- planted 860 mines during the 10th minelaying excursion on 27 September,\n- planted 840 mines during the 11th minelaying excursion on 4 October,\n- planted 820 mines during the 12th minelaying excursion on 13 October, and\n- planted 800 mines during the final 13th minelaying excursion on 24 October.\nHousatonic then made three trips returning soldiers of the American Expeditionary Forces to the United States before decommissioning on 5 August 1919 for return to Southern Pacific Steamship Company.\n- John Greenslade was awarded the Navy Distinguished Service Medal while aboard Housatonic.\nIn the words of British Rear Admiral Lewis Clinton-Baker, the North Sea mine barrage was the \"biggest mine planting stunt in the world's history.\" The United States converted eight civilian steamships as minelayers for the 100,000 mines manufactured for the barrage. The largest of these were four freighters owned by Southern Pacific Steamship Company. Southern Pacific Transportation Company had evolved from the First Transcontinental Railroad to become the dominant transportation provider in California. Owners of the original Central Pacific Railroad were known as the Big Four. Sailors similarly referred to these former Southern Pacific ships as the Big Four.\n- El Siglo became No. 1694 USS Canandaigua\n- El Dia became No. 1695 USS Roanoke\n- El Cid became No. 1696 USS Canonicus\n- El Rio became No. 1697 USS Housatonic\n- Belknap, Reginald Rowan The Yankee mining squadron; or, Laying the North Sea mining barrage (1920) United States Naval Institute pp.46-47,74&110\n- Daniels, Josephus The Northern Barrage and Other Mining Activities (1920) Government Printing Office pp.70-71\n- \"Navy Distinguished Service Medal to Captain Greenslade\". MilitaryTimes.","In the war of Schleswig-Holstein (1848-50), a mine field was planted to defend Kiel Harbor. Wine barrels were used as mine cases and held a 300 lbs. (136 kg) charge of gunpowder. These were controlled by a shore installation, which had a wet battery for each mine to heat a platinum wire placed in the middle of the charge. The Danes learned of this field and made no attack on Kiel, but it is not known how much if any effect the existence of the mine field had on their decision.\nIn the Austria-German War of 1866, the coasts of Istria and Dalmatia were defended by fairly dense mine fields. In 1868, the Hertz horn was invented (see British Mines and USN Mines) and by 1914 these had become the most reliable way to detonate contact mines. During the Franco-German War of 1870, the Jode, Elbe and Weser rivers were defended by mine fields. Soon after the war and union of the German states, Germany began a large mine development program and by the time of World War I was well able to wage mine warfare. Large stocks of reliable Hertz horn contact mines were available, all equipped with automatic anchors that used hydrostats to set mine depth and lock the mooring cables. Most of her capital ships and cruisers as well as many destroyers and auxiliary vessels were able to lay mines. On-going research included a program for laying mines via U-boats.\nGerman mines had a high reputation in both World Wars for reliability and innovation. The British paid perhaps the ultimate compliment in 1917 by copying a captured German Hertz horned mine to produce their first reliable contact mine. Following World War I, Germany established a mine warfare research and development command in 1920. Airborne mines were successfully tested in 1931 and acoustic and magnetic mines were developed before the war started. Some 1,500 magnetic mines were available in the spring of 1940, far short of the 50,000 originally ordered. Pressure mines were developed in 1943 but not used until the night of 6-7 June 1944 in the Normandy invasion area. This late deployment was to avoid their capture and duplication by the Allies. However, the Allies had already developed their own but had not used them for similar reasons. German pressure mines that could not be swept were nearly ready for deployment at the end of the war. Postwar, Germany used British and USA mines until the 1960s, at which time Denmark and Germany began cooperating in developing a new generation of magnetic ground mines.\n|BM||Airborne mines laid without parachutes|\n|EM||Moored Contact mines. Mostly Hertz Horns.|\n|FM||Shallow water contact mines, mostly moored types.|\n|KM||Anti-invasion coastal mines|\n|LM||Airborne parachute mines|\n|MT||Ground mine torpedoes laid from torpedo tubes|\n|RM||Shore-controlled or independent ground mines|\n|SM||Moored magnetic mines laid from mine tubes on U-Boats|\n|TM||Magnetic (influence) mines laid from torpedo tubes|\n|UM||ASW contact mines|\nGerman mines of World War II were all designated by letters, the first two indicating the function and the third the series designation within that category, usually indicating a modification.\nPostwar, new mine designations originally included the year that development started, but new mines now are designated by function.\nDuring World War I, Britain classified German mines into four general types. Britain used a two letter code sequence during World War II to identify German mines, with the first letter being G (for \"German\") and the second letter given in a sequence as to when they were captured. Where possible, these codes are included in the tables below.\nGermany started laying operations almost as soon as war was declared with the first mine field being planted by Koenigin Louise off Lowestoff on the night of 4/5 August 1914. Koenigin Louise was sunk the next day by the British cruiser HMS Amphion, but had her revenge on the following day when one of her mines sunk Amphion.\nDuring World War I, Germany laid more than 43,000 mines which claimed 497 merchant vessels of 1,044,456 gross tons (GRT), with one source claiming that the total was 586 Allied merchant ships. The British alone lost 44 warships and 225 auxiliaries to mines. Allied losses included two British and one French pre-dreadnoughts sunk by Turkish mines during the Dardanelles campaign.\nFor all of the Allies, losses to mines included the new battleship HMS Audacious, seven pre-dreadnoughts and six cruisers. The loss of the cruiser HMS Hampshire was especially damaging, as the casualties included the British Secretary of War, Lord Kitchener.\nThe British and Russians lost at least eight submarines to mines with the possibility that some of the six Russian and ten British submarines that disappeared without a trace during the war were also sunk by mines.\nSeveral U-boats were fitted as mine layers and one of these planted a field off the east coast of the USA which claimed the armored cruiser USS San Diego (ACR-6).\nIn World War II, some 534 merchant vessels of 1,406,037 GRT were sunk by German mines. At least ten UK and US destroyers were sunk by German mines, but the only major Allied warship sunk was the cruiser HMS Neptune and the mines that sunk her were actually deployed by Italian ships.\nThe peak success came in 1939-1940, when German aircraft, destroyers and minelayers were actively laying minefields off British harbors and no counter-measure against the magnetic mine had yet been developed.\nHowever, the greatest single success ever achieved by a minefield was by the one laid off Cape Juminda in the Gulf of Finland. This field inflicted great damage to the Soviet forces withdrawing from Tallinn (Reval) in August 1941. Out of the 195 warships, transport vessels, auxiliaries and 23,000 people that evacuated Tallinn, 53 ships and 4,000 lives were lost en route. Among the ships sunk were 25 out 29 of the larger transports, five destroyers, two corvettes, two submarines and two patrol boats. Some 2,828 German and Finnish moored mines were laid, mostly contact types along with some antenna types. In addition, about 1,500 explosive anti-sweeping devices were deployed in this field.\nAs mentioned above, the Germans had reliable Hertz horned contact mines for much of World War I. Magnetic, acoustic and pressure mines were developed during World War II.\nThe magnetic mines were fired by a change in the vertical field strength and were continually improved during the war. Various mechanisms were incorporated to prevent the mine from detonating until after a predetermined number of actuations occurred or a fixed time had passed.\nPost-war mines have a variety of acoustic, magnetic and hydrodynamic/pressure sensors.\nThe standard explosive fillings for World War II mines were SW18 and SW36; S16, S18 and SW36 were used in the airborne mines. S16 was 31.4% ammonium nitrate, 5.9% sodium nitrate, 2.3% potassium nitrate, 9.7% cyclonite, 10.1% ethylene diamine dinitrate, 0.6% TNT and 40% aluminum.\nInformation on the kind of explosives used for World War I mines and post-World War II mines is not available at this time. The former were possibly TNT-hexanitrodiphenylamine mixtures, similar to torpedo explosives of that time.\nCharge of 551 or 628 lbs. (250 or 285 kg). \"KA\" gear used a 98 foot (30 m) tube over the upper part of the mooring wire. When swept, this tube closed a switch and exploded the mine. This gear replaced the \"KE\" switch which fired the mine when the mooring cable was cut or the strain on it relieved. Both of these anti-sweeping mechanisms were later discarded as they were determined to be ineffective against sweeping operations.\nMagnetic mine captured by the British in July 1940. Later fitted with acoustic or acoustic/magnetic triggers. Weight 1,213 lbs. (550 kg) with a charge of 661 lbs. (300 kg). Had a 22 second delay impact fuze which detonated the mine if it fell in shallow water or dry land. An earlier version was designated by the British as GA.\nImproved moored contact mine with five Hertz and three switch horns. Charge was 88 lbs. (40 kg) and was 33 inches (84 cm) in diameter. Could be moored at 210, 330, 490, 640 or 980 feet (65, 100, 150, 200 or 300 m). A hydrostatic flooding device that acted if the mines rose above a set depth was introduced in 1940 so that German ships could pass over minefields.\nA ground influence mine in service in 1939. Later, acoustic and acoustic/magnetic fuzed types were introduced. The cylindrical shell was aluminum alloy and was said to lay badly on the seabed. Charge varied between 926-1,235 lbs. (420-560 kg) and the mine was normally laid in waters of 12-15 fathoms (22-27 m). A design for laying from the torpedo tubes of S-boats, the TMB/S, was introduced in 1940.\nJoint Dutch-German project developed for launching from fast attack craft. Mines were originally designated as SGM 80 (Seegrundmine 1980). The fuze is microprocessor based and accepts acoustic, magnetic and hydrodynamic signal processing channels. Weight of 1,543 lbs. (700 kg) with a 1,102 lbs. (500 kg) charge. Size is 71 in X 24 in D (180 cm L X 60 cm D). IGM 10 is a derivative intended to be laid by submarines. Manufactured by STN Atlas Elektronik GmbH and introduced in 1986.\nAnother Dutch-German project, this one manufactured by AEG/Telefunken (Germany) and Wedel (Denmark). Initials stand for Seemine Anti-Invasion. Intended to be laid by landing craft and helicopters in close-coastal waters. Weight of 254 lbs. (115 kg) with a 132 lbs. (60 kg) charge. Size of 31.5 in L X 14.2 in D (80 cm L X 36 cm D) when deployed with mine-laying rails on surface ships. Introduced in 1987.\n- \"Naval Weapons of World War Two\" by John Campbell\n- \"S-Boote: German E-boats in action 1939 - 1945\" by Jean-Philippe Dallies-Labourdette\n- \"America's Use of Sea Mines\" by Robert C. Duncan, Ph.D.\n- \"The Naval Institute Guide to World Naval Weapon Systems 1991/92\" by Norman Friedman\n- \"The King's Ships Were at Sea: The War in the North Sea August 1914 - February 1915\" by James Goldrick\n- \"Depth Charge: Royan Naval Mines, Depth Charges & Underwater Weapons 1914 - 1945\" by Chris Henry\n- \"German Destroyer Minelaying Operations off the English Coast 1940-41\" article by Pierre Hervieux in \"Warship Volume IV\"\n- \"Find and Destroy: Antisubmarine Warfare in World War I\" by Dwight R. Messimer\n- \"U.S. Navy Bureau of Ordnance in World War II\" by Lt. Cmdr. Buford Rowland, USNR, and Lt. William B. Boyd, USNR\n\"Admiral Kuznetsov's Memoirs\" by the Admiral Kuznetsov Foundation\nSpecial help by Renard Kanter"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:90db1796-152b-405c-ae4d-34dd945691bf>","<urn:uuid:aabceaba-98d2-4b2e-a89f-cbcee1edf749>"],"error":null}
{"question":"What discoveries did Ernest Rutherford make about radioactive decay types, and how are these decay rates mathematically characterized?","answer":"Rutherford differentiated radioactivity into three types in 1903: alpha radiation, beta radiation, and gamma radiation, classifying them according to their increasing penetration capacity. He also introduced the concept of half-life. Regarding the mathematical characterization, decay rates are proportional to the amount of radioactive nuclide present, with the decay rate (A) or -dN/dt being proportional to the number of nuclei (N) through a proportionality constant called the decay constant (λ). The relationship can be expressed as ln N = ln No – λt, where No is the number of nuclei at t=0.","context":["Ernest Rutherford, 1st Baron Rutherford of Nelson (born August 30, 1871 in Spring Grove near Nelson , New Zealand , † October 19, 1937 in Cambridge , United Kingdom ) was a British physicist who received the 1908 Nobel Prize in Chemistry . Rutherford is considered one of the most important experimental physicists .\nIn 1897, Rutherford recognized that the ionizing radiation from uranium consists of several types of particles. In 1902 he hypothesized that chemical elements change into elements with a lower atomic number through radioactive decay. In 1903 he differentiated radioactivity into alpha radiation , beta radiation and gamma radiation according to their increasing penetration capacity and introduced the term half-life . This work was awarded the Nobel Prize in Chemistry in 1908 .\nHis best-known contribution to atomic physics is Rutherford's atomic model , which he derived from his experiments in scattering alpha particles on gold foil in 1911 . Rutherford expanded the atomic model of Thomson , who had assumed a uniform mass distribution.\nRutherford demonstrated experimentally for the first time in 1917 that irradiation with alpha particles can convert one atomic nucleus (in his case nitrogen ) into another (in his case into the next heavier element oxygen ). During these experiments he discovered the proton . Under his guidance, John Cockcroft and Ernest Walton \"smashed\" an atomic nucleus with artificially accelerated particles; Lithium bombarded with protons was converted into two alpha particles, i.e. helium nuclei. Another scientist in Cambridge, James Chadwick , succeeded in 1932 in experimentally demonstrating the neutron , which Rutherford had theoretically postulated years earlier.\nLive and act\nOrigin and education\nErnest Rutherford was the fourth of twelve children of James Rutherford (1838-1928) and his wife Martha Thompson (around 1843-1935). His parents immigrated to New Zealand as a child . From Spring Grove , the family moved to Foxhill in 1876 . From March 1877, Rutherford attended the Primary School led by Henry Ladley there . In 1883 the family moved on to Havelock , where their father ran a flax mill he built on the Ruakaka River . For economic reasons, the family had to move again five years later, this time to Pungarehu on New Zealand's North Island . Supported by a grant from the Marlborough Education Board, Rutherford attended Nelson College from 1887 to 1889 . There he played in the rugby team and was head boy in 1889. His interest in mathematics and science was encouraged by his teacher William Still Littlejohn (1859-1933).\nFrom February 1890, Rutherford studied at Canterbury College in Christchurch . There, the professor of mathematics and natural philosophy Charles Henry Herbert Cook (1843-1910) promoted Rutherford's mathematical talent, while the professor of chemistry Alexander William Bickerton (1842-1929), who also taught physics, aroused Rutherford's interest in physics. In 1892 Rutherford passed the exams for the Bachelor of Arts , in 1893 he earned a Master of Arts degree and a year later a Bachelor of Science degree . Rutherford's first research dealt with the influence of high frequency Hertzian waves on the magnetic properties of iron and was published in the Transactions of the New Zealand Institute .\nDuring this time, Rutherford lived in the house of the widowed Mary Kate De Renzy Newton, a secretary for the Woman's Christian Temperance Union . There he met her daughter, his future wife Mary \"May\" Georgina Newton (1876–1945).\nRutherford applied in 1894 for the New Zealand place for an \" 1851 Exhibition Scholarship \", a scholarship funded from the surpluses of the Great Exhibition of 1851 in London. He was defeated with his application to the chemist James Scott Maclaurin (1864-1939) from Auckland University College . When Maclaurin did not accept the £ 150 scholarship intended for a study visit to Great Britain, Rutherford was awarded it as the second applicant.\nOn August 1, 1895, Rutherford left New Zealand on a steamship from Wellington . During a stopover, he demonstrated his Hertzian wave detector to William Henry Bragg at the University of Adelaide and received a letter of recommendation from Bragg. In October 1895 Rutherford began his work at the Cavendish Laboratory of the University of Cambridge , directed by Joseph John Thomson . Initially, he was concerned with improving the sensitivity of his detector, which he could soon use to detect radio waves at a distance of about half a mile. Thomson, who quickly recognized Rutherford's experimental talent, invited Rutherford at the beginning of the Easter semester in 1896 to help him investigate the electrical conductivity of gases . They used the X-rays discovered a few months earlier to trigger conductivity in the gases. Rutherford developed the experimental techniques to measure the rate of recombination and the velocities of the ions formed under the action of the X-rays . In the following years Rutherford continued these experiments using ultraviolet radiation .\nAfter two years in Cambridge, Rutherford received his \"BA Research Degree\" in 1897. Through Thomson's advocacy, he was awarded the 250 pound per year Coutts Trotter Fellowship of Trinity College in 1898 , which enabled Rutherford to spend another year at Cambridge.\nProfessor in Montreal, Manchester and Cambridge\nIn 1898 he received a call to the McGill University in Montreal ( Canada ), where he worked until 1907. For the research he carried out during this time, he received the Nobel Prize in Chemistry in 1908 .\nIn 1919 he went to Cambridge as a professor , where he was director of the Cavendish Laboratory . 1921 his work appeared Nuclear Constitution of Atoms (dt .: About the core structure of atoms) . From 1925 to 1930 he was President of the Royal Society .\nAwards and recognition\nRutherford is one of the world's most honored scientists. The British crown ennobled him in 1914 as a Knight Bachelor , accepted him in 1925 in the Order of Merit and raised him to hereditary peer in 1931 as Baron Rutherford of Nelson , of Cambridge in the County of Cambridge .\nA 1,906 newly discovered and Willy Marckwald described uranyl -mineral received his honor the name rutherfordine . In addition to the Nobel Prize in Chemistry, which was awarded to him in 1908, Rutherford received numerous scientific and academic prizes and honors. The Royal Society awarded him the Rumford Medal in 1904 and honored Rutherford in 1922 with their highest honor, the Copley Medal in gold. The Accademia delle Scienze di Torino honored him in 1908 with the award of the Bressa Prize (Premio Bressa). The Columbia University gave Rutherford every five years awarded Barnard Medal for the year 1910th\nIn 1906 he was elected to the Academy of Sciences in Göttingen , in 1911 to the National Academy of Sciences and in 1915 to the American Academy of Arts and Sciences . He received the Franklin Medal of the Franklin Institute in Philadelphia in 1924 , the Albert Medal of the Royal Society of Arts in 1928 and the Faraday Medal of the Institution of Electrical Engineers in 1930 . In 1921 he was elected an Honorary Fellow of the Royal Society of Edinburgh . In 1922 he became a corresponding and in 1925 honorary member of the Academy of Sciences of the USSR . In 1932 he was made an honorary member of the German Academy of Natural Scientists Leopoldina .\nIn mid-1946, Edward Condon and Leon Francis Curtiss (1895-1983) proposed by the US National Bureau of Standards to introduce a new physical unit Rutherford with the unit symbol rd for measuring activity , but this did not prevail.\nIn the 2005 New Zealand's Top 100 History Makers series , Rutherford was voted the most influential New Zealander in history. A memorial was built in his place of birth and his former primary school in Foxhill is keeping his memory alive with the Rutherford Memorial Hall.\nA Mars crater was named after him in 1973 and a moon crater in 1976. Rutherford Ridge in Antarctica has been named after him since 2008, and an asteroid has been named after him since 2021: (5311) Rutherford .\nOriginal English editions\n- Radio activity . 1st edition, At the University Press, Cambridge 1904 ( online ); 2nd edition, 1905 ( online ).\n- Radioactive Transformations . Archibald Constable & Co., London 1906 ( online ).\n- Radioactive Substances and Their Radiations . At the University Press, Cambridge 1913 ( online ).\n- Radiations From Radioactive Substances . University Press, Cambridge 1930 (with James Chadwick and Charles Drummond Ellis).\n- Artificial Transmutation of the Elements. Being the Thirty-fifth Robert Boyle Lecture . (= Robert Boyle Lecture, Volume 35), H. Milford, Oxford University Press 1933\n- The Newer Alchemy . University Press, Cambridge 1937.\n- The radioactivity . Authorized edition supplemented with the assistance of the author by Emil Aschkinass, Julius Springer, Berlin 1907 ( online ).\n- Radioactive conversions . Translated by Max Levin, Friedrich Vieweg and Son, Braunschweig 1907 ( online ).\n- Radioactive substances and their radiation . Translated by Erich Marx, Akademische Verlagsgesellschaft, Leipzig 1913.\n- About the core structure of the atoms. Baker lecture . Authorized translation by Else Norst, Hirzel, Leipzig 1921.\n- Uranium Radiation and the Electrical Conduction Produced by It . In: Philosophical Magazine . 5th episode, volume 47, number 284, 1899, pp. 109-163 ( doi: 10.1080 / 14786449908621245 ).\n- A Radio-active Substance emitted from Thorium Compounds . In: Philosophical Magazine . 5th episode, volume 49, number 296, 1900, pp. 1-14 ( doi: 10.1080 / 14786440009463821 ).\n- Radioactivity produced in Substances by the Action of Thorium Compounds . In: Philosophical Magazine . 5th episode, volume 49, number 297, 1900, pp. 161-192 ( doi: 10.1080 / 14786440009463832 ).\n- Comparison of the Radiations from Radioactive Substances . In: Philosophical Magazine . 6th episode, volume 4, number 19, 1902, pp. 1-23 (with Harriet T. Brooks ; doi: 10.1080 / 14786440209462814 ).\n- The Cause and Nature of Radioactivity. - Part I . In: Philosophical Magazine . 6th episode, volume 4, number 21, 1902, pp. 370-396 (with Frederick Soddy ; doi: 10.1080 / 14786440209462856 ).\n- The Cause and Nature of Radioactivity. - Part II . In: Philosophical Magazine . 6th episode, volume 4, number 21, 1902, pp. 569-585 (with Frederick Soddy; doi: 10.1080 / 14786440209462881 ).\n- The Magnetic and Electric Deviation of the Easily Absorbed Rays from Radium . In: Philosophical Magazine . 6th episode, volume 5, number 25, 1903, pp. 177-187 ( doi: 10.1080 / 14786440309462912 ).\n- A Comparative Study of the Radioactivity of Radium and Thorium . In: Philosophical Magazine . 6th episode, volume 5, number 28, 1903, pp. 445-457 (with Frederick Soddy; doi: 10.1080 / 14786440309462943 ).\n- Condensation of the Radioactive Emanations . In: Philosophical Magazine . 6th episode, volume 5, number 29, 1903, pp. 561-576 (with Frederick; doi: 10.1080 / 14786440309462959 ).\n- Bakerian Lecture. Nuclear Constitution of Atoms. In : Proceedings of the Royal Society of London / A. 97, number 686, 1920, pp. 374-400 ( doi: 10.1098 / rspa.1920.0040 ).\n- Edward Andrade: Rutherford and the Nature of the Atom . (= Science Study Series . Number 29). Heinemann, 1964.\n- Edward Andrade: Rutherford and the Atom. The beginning of the new physics . Translated from the American into German by Klaus Prost, Desch, Munich 1965.\n- Lawrence Badash (Ed.): Rutherford and Boltwood. Letters on radioactivity . Yale University Press, New Haven 1969.\n- Lawrence Badash: Rutherford Correspondence Catalog . American Institute of Physics, New York 1974.\n- John Campbell: Rutherford. Scientist Supreme . AAS Publications, Christchurch 1999, ISBN 0-473-05700-X .\n- John Campbell: Rutherford's Ancestors . AAS Publications, Christchurch 1996, ISBN 0-473-03858-7 .\n- James Chadwick (Ed.): The Collected Papers of Lord Rutherford of Nelson . 3 volumes, George Allen and Unwin, London 1962–1965.\n- Arthur Eve: Rutherford . Cambridge University Press, Cambridge 1939.\n- Mark Oliphant: Rutherford. Recollections of the Cambridge Days . Elsevier, Amsterdam 1972, ISBN 0-444-40968-8 .\n- Richard Reeves: Force of Nature: The Frontier Genius of Ernest Rutherford . WW Norton & Company, 2008, ISBN 978-0-393-33369-5 .\n- David Wilson: Rutherford. Simple genius . MIT Press, Cambridge 1983, ISBN 0-262-23115-8 .\n- Literature by and about Ernest Rutherford in the catalog of the German National Library\n- Newspaper article about Ernest Rutherford in the 20th century press kit of the ZBW - Leibniz Information Center for Economics .\n- Information from the Nobel Foundation on the 1908 award ceremony for Ernest Rutherford\n- Entry about Lord Ernest Rutherford of Nelson in the database of the Wilhelm Exner Medal Foundation .\n- Short biography at the Public Broadcasting Service\n- Brian Sweeney, Jacqueline Owens: Ernest Rutherford: Atom Man\n- Nobel Prize for Chemistry 1908, original lecture\n- Thaddeus J. Trenn: Rutherford on the Alpha-Beta-Gamma Classification of Radioactive Rays. Isis Vol. 67 (1976) p. 61ff doi.org/10.1086/351545\n- John Campbell: Rutherford's Ancestors . 1996, p. 12.\n- John Campbell: Rutherford's Ancestors . 1996, p. 20.\n- John Campbell: Rutherford's Ancestors . 1996, p. 39.\n- WJ Gardner: Cook, Charles Henry Herbert . In: Dictionary of New Zealand Biography , accessed March 4, 2013.\n- HN Parton: Bickerton, Alexander William . In: Dictionary of New Zealand Biography , accessed March 4, 2013.\n- Brian R. Davis: Maclaurin, James Scott . In: Dictionary of New Zealand Biography , accessed March 4, 2013.\n- John Campbell: Rutherford. Scientist Supreme . 1999, p. 192.\n- Arthur Eve: Rutherford . 1939, p. 13.\n- Sungook Hong: Wireless: From Marconi's Black Box to the Audion . MIT Press, 2001, ISBN 0-262-08298-5 , pp. 13-16.\n- John Campbell: Rutherford. Scientist Supreme . 1999, p. 246.\n- Johannes-Geert Hagmann: How physics made itself heard - American physicists engaged in \"practical\" research during the First World War . Physik Journal 14 (2015) No. 11, pp. 43–46.\n- History. cara1933.org, archived from the original on May 7, 2015 ; accessed on September 4, 2013 .\n- Rutherford, Oliphant, Paul Harteck: Transmutation effects observed with heavy hydrogen, Proc. Roy. Soc. A, Vol. 144, 1934, pp. 692-703, and under the same title, Nature, Vol. 133, 1934, p. 413\n- The discovery of DD fusion , EuroFusion, 2010\n- PP O'Shea: Ernest Rutherford. His Honors and Distinctions . In: Notes and Records of the Royal Society of London . Volume 27, number 1, 1972, p. 67 ( doi: 10.1098 / rsnr.1972.0009 ).\n- London Gazette . No. 28806, HMSO, London, February 24, 1914, p. 1546 ( PDF , accessed October 1, 2013, English).\n- London Gazette (Supplement). No. 33007, HMSO, London, January 1, 1925, p. 3 ( PDF , accessed October 1, 2013, English).\n- London Gazette (Supplement). No. 33675, HMSO, London, January 1, 1931, p. 1 ( PDF , accessed October 1, 2013, English).\n- Relazione sul XV Premio Bressa . In: Atti della Reale Accademia delle scienze di Torino . Volume 43, 1907-1908, pp. 579-586 ( online ).\n- Report of the Committee on the Bernard Medal . In: Report of the National Academy of Sciences for the Year 1910 . United States Government Printing Office , Washington 1911, pp. 14-15 ( online ).\n- Fellows Directory. Biographical Index: Former RSE Fellows 1783–2002. (PDF file) Royal Society of Edinburgh, accessed April 5, 2020 .\n- Christel Dell, Danny Weber, Thomas Wilde: The Academy Awards . Honorary membership. In: Jörg Hacker (Ed.): German Academy of Natural Scientists Leopoldina . Structure and members. German Academy of Natural Scientists Leopoldina eV, Halle (Saale) 2015, p. 353 ( leopoldina.org [PDF; accessed September 25, 2016]).\n- Edward Uhler Condon, Leon Francis Curtiss: New Units for the Measurement of Radioactivity . In: Physical Review . Volume 69, number 11-12, 1946, pp. 672-673 ( doi: 10.1103 / PhysRev.69.672 )\n- John Barry: Currency trends and developments . In: Reserve Bank of New Zealand Bulletin . Volume 57, number 4, 1994, p. 352 ( PDF ).\n- Helga Neubauer: Foxhill / Brightwater . In: The New Zealand Book . 1st edition. NZ Visitor Publications , Nelson 2003, ISBN 1-877339-00-8 , pp. 1011 f .\n- Ernest Rutherford in the Gazetteer of Planetary Nomenclature of the IAU (WGPSN) / USGS\n- Ernest Rutherford in the Gazetteer of Planetary Nomenclature of the IAU (WGPSN) / USGS\n|ALTERNATIVE NAMES||Rutherford, Ernest, 1st Baron Rutherford of Nelson; Rutherford, Sir Ernest|\n|BRIEF DESCRIPTION||New Zealand nuclear physicist, Nobel laureate in chemistry|\n|BIRTH DATE||August 30, 1871|\n|PLACE OF BIRTH||Spring Grove at Nelson|\n|DATE OF DEATH||October 19, 1937|\n|PLACE OF DEATH||Cambridge|","Radioactive Decays – transmutations of nuclides\nRadioactivity means the emission of alpha (a) particles, beta (b) particles, or gamma photons (g) from atomic nuclei. Radioactive decay is a process by which the nuclei of a nuclide emit a, b or g rays. In the radioactive process, the nuclide undergoes a transmutation, converting to another nuclide.\nThe variation of radioactivity over time is called decay kinetics. The characteristics of kinetics are expressed in decay constant and half life. Variations of radioactivity in mixtures of radioactive nuclides and consecutive decays are often considered, and decay kinetics serves science and technology in many applications.\nIn radioactive decay processes, some of the things are conserved, meaning they do not change. The number of nucleons before and after the decay is the same (conserved). So are electric charges and energy (including mass). The relationship between nuclides is best seen in a chart based on the number of neutrons and the number of protons. Such a chart clearly shows relationships among isobars, isotopes, isotones, and isomers.\nThe mass number of a nuclide does not change in b and g decays, but it decreases by 4 in a decay due to the emission of a helium nucleus. Thus, there are four families of radioactive series based on mass numbers starting with 232Th, 237Np, 238U, and 235U respectively. Masses of their family members are in the 4n, 4n +1, 4n +2, 4n +3 categories, where n is an integer. Sources of natural radioactive materials such as radium, radon, and polonium, came from the natural occurring radioactive nuclides 235U, 238U, and 232Th, whereas 237Np is a man made nuclide, because this nuclide no longer exists in the planet Earth.\nStudies of radioactive decays led to theories of nuclear stability and nuclear structure. Some of these theories will be examined as we take a closer look at atomic nuclei. Concepts such as energy states of nucleons, angular momentum, parity of nuclear energy state etc. will be introduced. These concepts and theories provide the tools for the discussion of energy in radioactive decays.\nWe will look at radioactivity and decay kinetics, look at transmutation of nuclides in radioactive decay, the nuclide chart, which is used to discuss the four families of radioactive decay, look at atomic nuclei closely, and look at the energy aspect in radioactive decays.\nRadioactivity and Decay Kinetics\nThe emission of alpha (a), beta (b) or gamma (g) rays by a sample of substance is called Radioactivity. A sample may emit one or more types of radioactive ray. The number of a, b or g rays emitted per unit time is called the decay rate. The study of radioactive material requires the identification of types of rays emitted, decay rates, changes in decay rates, and the nuclides in the sample that emit the rays. The variation of decay rates over time from a fixed amount of nuclide is called decay kinetics, which is an important topic in the study of radioactivity.\nRadioactivity Units, Decay Constants and Half Lives\nThe decay rate is measured in decays per unit time, and the SI unit for radioactivity is becquerel (Bq) which is 1 decay or disintegration per second (dps)*. The widely used unit curie (Ci) defined as the decay rate of 1.0 g of radium earlier is 3.700 x 1010 Bq.\nA Summary of Decay Kinetics\nNo is number of nuclei at t = 0,\nln N = ln No – l t\n- How are decay rates related to the amounts of radioactive nuclide?\nHow do decay rates vary over time?\n- Do chemical states of a radioactive nuclide (element) affect its decay rate?\nSamples containing the same amount of uranium and thorium have very different decay rates. A radioactive source contains one or more radioactive nuclides. The decay rate of a sample is proportional to the amount of radioactive nuclide present. Decay rate of a nuclide is unaffected by its chemical or physical state, studies have shown.\nThe amount of radioactive nuclide can be expressed in unit g, mole, or number of nuclei (N). The disintegration rate of N nuclei (called activity, A, or in mathematical notation ‑ dN/dt) at any given time is proportional to N. The proportional constant is called the decay constant, l. A summary of decay kinetics is given in the text box.\n* Long before SI units were established, radioactivity was compared to a quantity called curie (Ci), which was originally the radioactivity of 1.0 g of radium. One Ci is now defined as the quantity of any radioactive material that gives 3.700 x 1010 dps or Bq. Thus, Ci is considered a cgs (cm, gram & second) unit, and 1 Ci = 3.700 x 1010 Bq."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:0cb6ab44-4e7e-4fc8-9399-3c27eed06dcb>","<urn:uuid:ca72e686-3faa-4f2f-80a4-6e3102252cfc>"],"error":null}
{"question":"Which was completed earlier: Sibelius' Symphony No. 7 or Tchaikovsky's Symphony No. 5?","answer":"Tchaikovsky's Symphony No. 5 was composed in 1888, while Sibelius' Symphony No. 7 was completed in 1924, making Tchaikovsky's work the earlier of the two by 36 years.","context":["Rattle conducts Sibelius – Symphony no.7, in the last of a three-concert residency from the Berlin Philharmonic Orchestra, performing all the composer’s symphonies\nBerlin Philharmonic Orchestra / Sir Simon Rattle – Barbican Hall, live on BBC Radio 3, 12 February 2015.\nListening link (opens in a new window):\non the iPlayer until 13 March\nFor those unable to hear the broadcast, here is a Spotify link. Sir Simon has not recorded this piece with the Berlin Philharmonic, but this is a recording he made with the City of Birmingham Symphony Orchestra for EMI (track 6):\nWhat’s the music?\nSibelius – Symphony no.7 in C major, Op.105 (1924) (22 minutes)\nWhat about the music?\nThis symphony is a remarkable piece of work that reveals more and more with each subsequent listen. Initially it can seem too simple in its melodic material or too dense in the sheer amount of ideas, but in fact it is an amazingly self-contained unit, like a single long melody lasting for just over twenty minutes.\nSibelius worked on it at the same time as his Sixth Symphony, hence the reason for Rattle performing the two together without a break – but the recommendation (from here at least!) is to make the most of each piece on separate terms.\nIn my mind’s eye I often feel as though this piece is a seascape, with the spray almost tangible to the touch. The music is brooding at times, and its complex harmonies can twist the human response, but it is an overwhelmingly positive way in which to finish a symphonic cycle. And how better to finish than with a C major chord, regarded as the purest in all music?\nRattle’s interpretation of the Seventh would appear to be spot-on tempo-wise, and as is the conductor’s wont it picks apart the structure to highlight all the different themes the composer uses – yet is always moving forward to the next musical ‘signpost’.\nIn each of the three occurrences of the trombone theme he stresses its heroic quality, and the overall impression of the symphony is a positive, resilient one.\nWhat should I listen out for?\nThe symphony is in a single section, and though it is possible to break it in to constituent parts, it is so compressed and tightly bound together that is it best to listen to it as a single whole.\n1:31:24 – a single timpani roll ushers in an ascending scale on the lower strings. Already the music is noticeably broader than the Sixth Symphony.\n1:32:13 – the wind play a relatively distant figure that assumes great importance as the symphony progresses.\n1:36:43 – the strings swell to a rousing theme on the trombones, just about rising above the whole orchestra.\n1:41:00 – now the music is speeding up, with the strings adopting a similar figure to that found in faster moments of the Sixth Symphony.\n1:41:52 – the swirl of the violins gets gradually slower, until 1:42:11, where the trombones return with their tune, now more isolated.\n1:43:43 – the quicker theme returns on the woodwind.\n1:48:21 – the ascending scale from the opening of the work simmers, but there is a tension between two different speeds before the trombone theme returns at 1:48:40.\n1:51:43 – the final section, which ends with what seems the simplest resolution at 1:53:02.\nWant to hear more?\nAfter the Symphony no.7 – if you’re on Spotify – keep listening and you will hear another of Sibelius’s orchestral ‘tone poems’ – that is, an orchestral piece that describes a particular story or event. This one, Nightride and Sunrise, is not so well known, but is a descriptive work that draws on an unknown sequence of events for the composer.\nFor more concerts click here","Notes and Editorial Reviews\nThese are splendid Tchaikovsky performances.\n“I think Tchaikovsky was always ready for immortality ... and with his final three symphonies he secured his place in the pantheon of Great Composers.” Valery Gergiev.\nThe EMI Classics label has raided their extensive back catalogue for this three disc compilation.\nSymphony No.4 in F minor, Op. 36 The Fourth Symphony was written at a particularly crucial point in Tchaikovsky’s life. 1877 was not only the year of his disastrous marriage but also the year in which he began his fifteen-year correspondence with his patroness Nadezhda von Meck. The F minor Symphony has always been a popular work with its muscular and melodic writing. Infused throughout\nthe score is the sense of ‘fate’ which Tchaikovsky believed controlled his destiny as he described in a letter to Madame von Meck, “the fateful force which prevents the impulse to happiness from achieving its goal … which hangs above your head like the sword of Damocles.”\nIn the opening movement Andante sostenuto - Moderato con anima the performance from the Philadelphia Orchestra under Muti is as exciting as one is likely to hear. Muti blends passion and power to perfection and the conclusion was awe-inspiring. The moving and robust Andantino in modo di canzona was so convincing that I was left with a compelling sense of Tchaikovsky’s despair and fatigue. In the pizzicato section of the Scherzo: Pizzicato ostinato (Allegro) the Philadelphia strings are on their finest form and the woodwind deserve praise for their assured contribution at 1:44-3:04. There’s tremendous energy and drama in the Finale. Allegro con fuoco. It is hard to imagine better playing and I found myself on the edge of my seat. The sound quality throughout is to demonstration standard.\nThis performance of the Fourth Symphony is in the premier league of the alternative recordings and I believe it is probably the best of all the versions. Other favourite accounts from my collection are those from Jansons with the Oslo Philharmonic Orchestra on Chandos CHAN 8361 (c/w Romeo and Juliet Overture); Mengelberg with the Concertgebouw on Music & Arts mono CD809 (c/w Symphonies 5 and 6); Rozhdestvensky and the LSO on Regis RRC 1212 (c/w Marche Slave and 1812 Overture); Mravinsky with the Leningrad PO on DG 419 745-2GH2 (c/w Symphonies 5 and 6); Karajan with the VPO on Decca Penguin 460 655-2 (c/w Romeo and Juliet Overture) and Gergiev and the VPO from Vienna in 2002 on Philips 475 6315 0 PX3 (c/w Symphonies 5 and 6).\nFantasy Overture, Romeo and Juliet Composed in 1869, revised in 1870 and again in 1880 the Fantasy Overture, Romeo and Juliet is Tchaikovsky’s musical interpretation of Shakespeare’s greatest tragedy and one of the most enduring works in popularity. The tone poem contains musical themes that principally represent: Friar Laurence, the Montague and the Capulets feud, and the love music of Romeo and Juliet.\nMysterious, powerful, sensuous and passionate, Muti and his Philadelphia players take the listeners through a broad spectrum of colour and emotions. The recording is decent enough but a touch close for my taste.\nI remain a great admirer of the account of the Romeo and Juliet Overture from Pletnev and the Russian NO on DG 471 742-2 (c/w Pathétique) and also the performance from Karajan with the VPO on Decca ‘Penguin series’ 460 655-2 (c/w Fourth Symphony).\nSymphony No.5 in E minor, Op. 64 Composed in 1888 the Fifth Symphony is generally considered to be the most attractive of Tchaikovsky’s major works. When he first began writing the symphony he was suffering from a deep depression. However, he moved to the countryside and his state of mind became much more relaxed, enjoying the peace and quiet, gaining a new-found pleasure from his garden. This E minor Symphony reflects all the violent and conflicting emotions that he was experiencing at the time of its composition.\nMuti and the Philadelphians provide a grey and sombre opening Andante - Allegro con anima that gives way to increased weight and power. One, however, wonders if Muti is keeping something in reserve. The principal elements of melancholy and beauty are blended to considerable effect in the Andante cantabile, con alcuna licenza, although the speeds feel rather too measured. Here Muti builds up great tension in a moving interpretation. In the Valse - Allegro moderato the infectious playing is light with a convincing lilt. With highly authoritative playing the Philadelphia convey a triumphant mood in the Finale: Andante maestoso - Allegro vivace movement. I found the recording clear, bright and fairly close.\nFrom my collection I highly rate the accounts of the Fifth Symphony from Jansons and the Oslo PO on Chandos CHAN 8351; Rozhdestvensky and the LSO on Regis RRC 1213 (c/w Capriccio Italien); Mravinsky with the Leningrad PO on DG 419 745-2GH2 (c/w Symphonies 4 and 6); Ormandy and the Philadelphia Orchestra on Sony SBK 46538 (c/w Serenade for Strings) and Gergiev and the VPO from Salzburg in 1998 on Philips 475 6315 0 PX3 (c/w Symphonies 4 and 6).\nFrancesca da Rimini: Symphonic Fantasy after Dante, Op. 32 Tchaikovsky composed Francesca da Rimini in less than three weeks during his visit to Bayreuth in 1876. The premiere performance was given in Moscow in 1877 and proved so popular that the work was repeated twice a couple of months later. This is programme music of Dante’s Francesca da Rimini with the first section depicting the gateway to the Inferno and the agonies of the condemned. The middle section represents the tragic love of Paolo and Francesca, and the third part returns to the Inferno followed by a concluding section.\nIn Francesca da Rimini Muti and the Philadelphians revel in the fierce and stormy passages but provide contrast in music of contemplation with an air of mystery. The clarinet playing from Anthony Gigliotti especially at 8:28-9:25 is impeccable.\nMy preferred version of Francesca da Rimini is from the New Philharmonia Orchestra under Igor Markevitch on Philips Classics Duo 446 148-2 (c/w Symphonies 1-3).\nSymphony No.6 in B minor, Op. 74 ‘Pathétique’ Tchaikovsky’s Sixth Symphony, universally known as the Pathétique, is among the most deeply moving and profound of all works. An enduring masterwork which Tchaikovsky considered to be his greatest composition. Once again the struggle against ‘fate’ is central to this symphony which was to be the last Tchaikovsky wrote. The première took place in October 1893 at St. Petersburg and just eight days later the composer was dead. Few farewells in music are more poignant.\nMuti and the Philadelphia in the opening movement Adagio - Allegro non troppo impart a sinister air of shadowy foreboding with vigour and passion. The main theme is performed with just the right level of strength and poignancy. The Allegro con grazia is smooth and good humoured and I was impressed with the assured control and potency that Muti conveys in the Allegro molto vivace. Tchaikovsky’s mood of intense desperation and torment is impressively communicated in the Finale: Adagio lamentoso - Andante as they bring the score to a harrowing conclusion. A special mention goes to the woodwind section for their splendid playing throughout. Decent sound quality, reasonably clear and well balanced.\nI have several favourite versions of the Pathétique Symphony in my collection that I find deeply satisfying. The account from Pletnev and the Russian NO on DG 471 742-2 (c/w Romeo and Juliet Overture); Jansons with the Oslo PO on Chandos CHAN 8446; Rozhdestvensky and the LSO on Regis RRC 1214 (c/w The Storm Overture); Mravinsky with the Leningrad PO on DG 419 745-2GH2 (c/w Symphonies 4 and 5) and Gergiev and the VPO from Vienna in 2004 on Philips 475 6315 0 PX3 (c/w Symphonies 5 and 6). I still admire and regularly play my first recording of the work, which is on a vinyl LP, conducted by Fritz Reiner and the Chicago Symphony Orchestra on Camden Classics CCV 5024.\n1812 Overture, Op. 49 The Festival Overture ‘The Year 1812’ was composed in 1880 as a commission for the Moscow Exhibition of 1882; principally for the consecration of the Temple of Christ the Redeemer constructed to commemorate the Russian victory over Napoleon. The highly popular 1812 Overture is noted for its use of Russian themes, cannon shots and church bells in the coda.\nMuti and the Philadelphians provide a vigorous and characterful performance and is my premier recommendation of the score. I found the recording most acceptable although two friends thought the forte passages a touch fierce. This interpretation is also on the outstanding Muti/Tchaikovsky set from Brilliant Classics 99792 (see below).\nThose looking for a complete surveys of Tchaikovsky’s six symphonies and the Manfred Symphony may wish to turn to Muti’s superb earlier set with the Philharmonia from London in 1975-81. The set is now available at super-budget price on Brilliant Classics 99792. Also included are Muti’s recordings of the Philadelphia Orchestra during 1981-91 of the Francesca da Rimini, 1812 Overture, Swan Lake Suite and Serenade for Strings and a version of the Romeo and Juliet Overture from the Philharmonia in 1977 in London.\nAnother splendid alternative is from Mariss Jansons and the Oslo Philharmonic Orchestra. The set was recorded in the Philharmonic Hall, Oslo in 1984-86 on Chandos CHAN 86728 and reissued in 2006 on Chandos CHAN 10392 (c/w Capriccio Italien).\nReturning to the present set: these are splendid performances from Riccardo Muti and the Philadelphia Orchestra on EMI Classics. However, the competition in these scores is extremely fierce and Muti’s earlier set on Brilliant Classics is generously filled and makes a tempting first choice.\n-- Michael Cookson, MusicWeb International\nWorks on This Recording\nRomeo and Juliet Overture by Peter Ilyich Tchaikovsky\nWritten: 1869/1880; Russia\nLength: 20 Minutes 10 Secs.\nFrancesca da Rimini, Op. 32 by Peter Ilyich Tchaikovsky\nAnthony Gigliotti (Clarinet)\nWritten: 1876; Russia\nLength: 23 Minutes 12 Secs.\n1812 Overture, Op. 49 by Peter Ilyich Tchaikovsky\nWritten: 1880; Russia\nLength: 15 Minutes 26 Secs.\nSymphony no 4 in F minor, Op. 36 by Peter Ilyich Tchaikovsky\nWritten: 1877-1878; Russia\nSymphony no 5 in E minor, Op. 64 by Peter Ilyich Tchaikovsky\nWritten: 1888; Russia\nSymphony No. 4 in F minor Op. 36: I. Andante sostenuto_Moderato con anima\nSymphony No. 4 in F Minor, Op.36: II. Andantino in modo di canzone\nSymphony No. 4 in F Minor, Op.36: III. Scherzo: Pizzicato ostinato, allegro\nSymphony No. 4 in F minor Op. 36: IV. Allegro con fuoco\nFantasy Overture - Romeo and Juliet\nSymphony No. 5 in E Minor, Op.64: I. Andante - Allegro con anima\nSymphony No. 5 in E Minor, Op.64: II. Andante cantabile, con alcuna licenza\nSymphony No. 5 in E Minor, Op.64: III. Valse: Allegro moderato\nSymphony No. 5 in E Minor, Op.64: IV. Andante maestoso - Allegro vivace\nFrancesca da Rimini Op. 32\nBe the first to review this title"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:21c9479e-b229-47b5-ada6-4aba396b19cd>","<urn:uuid:1634176c-6877-448b-a210-f5185234546c>"],"error":null}
{"question":"What is the difference in accuracy between a conceptual estimate and an order of magnitude estimate?","answer":"A conceptual estimate typically has a margin of error between ±20% (for feasibility studies) to ±6% (for complex conceptual design estimates), while an order of magnitude estimate has a much wider accuracy range of -50% to +100% of the actual cost. Conceptual estimates require more skill and experience, with estimators basing their projections on real-world examples and past projects, whereas order of magnitude estimates are rough estimations used in very early project stages and can sometimes be made without any historical data.","context":["Assigning costs is necessary in every stage of planning, and, ultimately, constructing a building. Sometimes, unfavorable budgets, zoning requirements, or a tricky economy might preclude a building from being constructed. However, understanding the planning process and the key differences between the various types of estimating strategies (including conceptual estimates, budgets, and design-build) will help you determine where the proposed building is in its planning cycle — and help you make a more informed decision about your participation in the project.\nConceptual estimates, budgets, and design-build estimating all involve assigning a monetary value to a construction project. Conceptual estimates and budgets may or may not result in a building being constructed, while design-build estimates are much more likely to do so. Because they're all based on varying degrees of information, the following three distinct estimate types each have their own characteristics and should be bid in different ways.\nThe description of a conceptual estimate can range from a feasibility study, which carries ±20% margin of error with regard to the actual project cost, to a complex conceptual design estimate, which only carries a ±6% margin of error. Typically, conceptual estimates happen before a specific approach is decided for the project.\nConceptual estimating is part art and part science. The estimator has to have a “big picture” view, looking for long-range solutions rather than shortsighted projections. Conceptual estimators hone their craft with practice and experience, basing their estimates on real-world examples and past projects. According to the Design-Build Institute of America, Washington, D.C., a conceptual estimate is “the skill of forecasting accurate costs without significant graphic design information about a project.” However, the lack of information does not negate the importance of a conceptual estimate, because it can make an appreciable difference in the cost effectiveness, feasibility, efficiency, and outcome of a project.\nIn developing a conceptual estimate, working with the developer can bring about the best possible outcome. But keep in mind that this approach may not necessarily be the least expensive. An estimator must look to the lifetime cost, which may include higher up-front costs but a lower lifetime cost of the building. You must also be aware that the best interest of the building owner may be at odds with the desires of the developer, who may want to get in and out of the building quickly with the least amount of cost.\nFor example, let's consider “green” building techniques. The use of photovoltaic panels, green roofs, geothermal wells, and other energy-efficient building materials still cost more than a conventionally built structure. However, the payback of reduced energy costs and reduced carbon footprint over the life of the building can outweigh the initial increased costs.\nResearch is always part and parcel of the conceptual estimating process, as there is no “outline” for this process. Each estimator must forge his own way and stretch the imagination to put his proposal into an interesting, understandable and usable format to move the project to the next level.\nOne reason to complete a budget is to obtain full funding for the project and to provide a cost-control mechanism for the fabrication and construction process. Developers can minimize exposure to cost overruns by accurately developing “real” costs. There are many new and exciting tools that can be used to anticipate how a building will look, function, and be built most effectively. By developing an accurate scope of work for the project, contractors can avoid disputes that almost always result in cost overruns. In other words, the budgeting phase is essential.\nConstruction budgets can be tedious, but the fiscal element of any project can never be underestimated or overlooked, especially in today's economy. The constructability of a design results in a cost-effective budget and smooth construction processes. The Empire State Building in New York City is often referred to as the epitome of this concept, as it was completed on time and under budget, taking just one year and 45 days to complete.\nThere are at least four types of budgets used in the construction field: conceptual, schematic, design development, and working drawings. Because we've already discussed conceptual budgets, we'll move right into schematics.\nSchematic budgets signal the start of work by the architect, allowing him to illustrate the design concept, including the site plan, floor plans, exterior elevations, and key building sections. Design development budgets depict a coordinated description of all aspects of the design, including fully developed floor plans, sections, exterior elevations, interior elevations, reflected ceiling plans, wall sections, and other key details. The basic mechanical, electrical, plumbing, and fire protection systems are also defined in this stage, becoming the basis for the construction documents that follow. Working drawing budgets are the most detailed in the budgeting process. Any information missing from previous budgets will be added at this phase, and the modified drawings will become the construction or bid documents.\nA budget can develop in two different ways. A budget can turn into a design-build project, whereby the contractor continues to develop the design and adjusts based on additional information as the project progresses. In most cases, an electrical contractor engages in a design-build project with the anticipation of actually completing the project. Although this is not always the case, it's usually a risk worth taking.\nThe other avenue is for the budget to develop into a conventional design-bid-build project. In this case, an electrical contractor might have been involved in the budgeting of a project, which was being designed in a conventional manner with an architect, engineer, and team. Once the working drawings are completed, the job moves on to the bidding phase. The electrical contractor who originally budgets the job might have an advantage, but this is not always the case. He will now be bidding against anyone who meets the requirements of bidding the job — be it bonding capacity, staff level, or experience. The contractor who successfully bids the job — and is ultimately awarded the job — will be the contractor who completes the job, regardless of his prior work on the project.\nThe least like the other two methods, a design-build project approach requires the construction team to be responsible for taking a concept that is developed with the owner, complete the detailed design, and then proceed with the construction. This is different than the conventional design-bid-build process, whereby contractors bid on a design that has been completed before their involvement. If they are the successful bidder, they will build the project. Conceptual estimating often opens the door to the design-build process.\nA design-build project is one that is not necessarily drawn or engineered but one that has some sort of written scope and building footprint upon which to build. Design-build estimates are often confused with budgets, but they are not the same. Because most owners have fiscal restraints that affect their capital investment, they depend heavily on the estimator to develop accurate cost forecasts at every stage of the project. Therefore, an estimator must have a comprehensive understanding of the costs of labor, materials and equipment, not to mention the means and methods of both design and construction in order to accomplish the design-build task.\nOften, a design-build project will result from a municipal or state request for proposal based on a written scope. Relationships between general contractors, building owners, and an electrical contractor may also result in the parties developing a design-build project. Relationships are not only vital to developing a solid business, but can lend a comfort level to owners or general contractors, to the point where that comfort level influences project development. An electrical contractor might be in the right place at the right time or submit a budget price for a project based on a conceptual estimate, which we already discussed. Conceptual estimates can definitely turn into design-build projects. In addition, the design-build process may be faster and more efficient than a conventional design-bid-build project.\nThe goal of a design-build project estimate is to identify the true costs of items and to determine along the way if the cost of that item is feasible to the owner or developer. Identifying the proper scope will always be a crucial element of this process. A minimum/spec job will cost less than a building designed to purpose/function. Code issues and utility company requirements must be considered. Some key questions to consider include: What kind of building is it? What is the building construction? What are the needs of the potential tenant(s)? Did the customer give you a list of equipment that needs to be wired?\nCost efficiencies and energy rebates of certain types of fixtures may be important as well. If the design-build project were to be based strictly on “cost to install,” the developer may be satisfied but the owner or tenant may not be. In the long run, the total life cost of the fixture will be more important, especially as energy costs rise. The largest risk an electrical contractor takes in a design-build project is doing all of the work, only to have the developer take the design and proposal and put it out to bid, back to the traditional design-bid-build.\nCandels is president of Candels Consulting, an electrical estimating consulting firm in Niantic, Conn. She can be reached at email@example.com.","Projects and other undertakings require cost estimation to determine resource requirements. Of course, estimates are not definite and their accuracy depends on the project stage in which they are undertaken and the sources of data. Nonetheless, there are three types of cost estimation classified according to their scope and accuracy. These are (1) order of magnitude estimate; (2) budget estimate; and (3) definitive estimate.\nThe difference among the three types of cost estimation\n1. Order of magnitude estimate: An order of magnitude estimate is a rough estimation of costs used at the very early stage of a project, particularly during the evaluation and planning stages. The purpose of this type of cost estimation is to have an idea about general and total expenditures instead of itemizing expenses based on project activities and deliverables.\nManagers and planners can us historical data of a related project or an aggregate of data from different projects to perform an order of magnitude estimation. There are situations when no data is used at all. The accuracy is around -50 percent to +100 percent of the actual cost. Figures generated from this method of estimation are sometimes called ballpark figures.\n2. Budget estimate: Also referred to as preliminary cost estimates, budget estimate is more accurate than an order of magnitude estimate. Accuracy is around -10 percent to +25 percent. The purpose of this type of estimation is to generate a preliminary itemized list of expenditures based on the major components of the project. Therefore, this is done within the actual planning stage of the project.\nThere are specific types of cost estimation under budget estimate. One example is top-down estimate in which costs of the major components of the project are estimated using historical data of a related project or obtainable data using the project plan. Another example is an estimation of expected labor cost or a summative costing of equipment and materials requirements.\n3. Definitive estimate: A definitive estimate has an accuracy of around -10 percent to +15 percent and in some standards, around -5 percent to +15 percent. Sometimes called as detail cost estimates, this type of cost estimation are not only more accurate than order of magnitude of estimate and budget estimate but are also more detailed.\nReports generate from a definitive estimation include an extensive itemized list of project activities and deliverables with brief descriptions either for justifications and clarity. However, note that conducting a definitive estimation is only possible with a fully developed project plan and a work breakdown structure\nA note on cost estimation methodologies\nIt is important to reiterate the fact that the accuracy of cost estimation depends on the availability of data, as well as the project stage in which it is particularly conducted. Take note that as the project plan becomes more developed, more data can be generated and thus, budget estimates or detailed estimates become more possible.\nNonetheless, regardless of the methodology used, cost estimation is a standard process observed in project management all the way down to portfolio management. It helps identifying resource requirements and capabilities, setting priorities, and allocating resources for project planning and controlling. Costing and valuation are often the immediate byproducts of cost estimation and a review of generated reports can help guide decision-makers."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:fe4a938a-7f5a-40f0-abb6-ad1651a16d25>","<urn:uuid:eab8bdb9-d302-4807-b7a7-73e27b0ea45a>"],"error":null}
{"question":"How can parents reduce the risk of having a child with double outlet right ventricle (DORV)?","answer":"To reduce the risk of having a child with DORV, parents can limit exposure to teratogens during pregnancy, especially in the first trimester. Specifically, they should avoid: radiation from dental or medical procedures, tobacco and alcohol, exposure to infections, and exposure to teratogenic substances like retinoids, theophylline, and valproate. However, it's important to note that most cases of DORV are not the result of exposure to these substances.","context":["Double Outlet Right Ventricle in Children\nWhat is double outlet right ventricle in children?\nDouble outlet right ventricle (also known as DORV) is a type of congenital heart malformation. This means it is present from birth. In this condition, the heart and the main blood vessels leaving the heart do not develop the right way. This can cause symptoms in your child.\nA normal heart has 4 chambers: 2 atria (upper chambers) and 2 ventricles (lower chambers). Blood flows from the right atrium into the right ventricle and from the left atrium into the left ventricle. There is a wall (called the septum) between the ventricles and between the atria. It stops blood from flowing between the left and the right sides of the heart.\nIn a child with DORV, both the pulmonary artery and the aorta are connected to the same heart chamber, the right ventricle. In a normal heart, the aorta connects to the left ventricle, the ventricle that pumps blood to the body. The pulmonary artery normally connects to the right ventricle, the ventricle that pumps blood to the lungs. These great arteries may be connected either partly or fully. There is also almost always an opening in the wall between the left and right ventricles. This opening is called a ventricular septal defect.\nIn many cases, DORV occurs with other heart malformations. For example, a heart valve may not work right. Or a ventricle may not be fully formed. DORV actually refers to a range of heart problems that share a common feature. With all of them, both great arteries are connected to the right ventricle. Depending on the exact nature of the DORV, the lungs might not get enough blood flow. Or they might get too much.\nOnly a small number of newborns have heart malformations. Of these, DORV is relatively rare.\nWhat causes double outlet right ventricle in a child?\nResearchers do not fully understand what causes DORV. Some substances (teratogens) can lead to a heart malformation if a mother is exposed to them during a vital time in her pregnancy. These might cause some cases of DORV.\nSometimes, DORV results from abnormalities in a child’s genes. Scientists have found a number of different genes that might lead to DORV. If a child has an abnormal number of copies of certain genes, that can lead to DORV. DORV is also linked to a number of genetic syndromes. But in most cases, the cause of DORV is unknown.\nWhich children are at risk for double outlet right ventricle?\nMost cases of DORV happen without any known cause. But they may be more likely to happen with certain syndromes. Some of these include:\n- Trisomy 13 (Patau syndrome). This is a developmental disorder that occurs when a child has 3 copies of chromosome 13 instead of 2.\n- Trisomy 18 (Edwards syndrome). This is a condition that slows growth and causes abnormalities in organ development. This happens because a child has 3 copies of chromosome 18 instead of 2.\n- Robinow syndrome. This is a rare disorder that affects bone growth.\nEach of these syndromes has its own symptoms, which sometimes include DORV.\nWhat are the symptoms of double outlet right ventricle in a child?\nThe symptoms of DORV may vary depending on the exact heart abnormalities involved. Symptoms are often present at birth. But they may not appear until later. They can include:\n- Bluish color (cyanosis) or pale skin\n- Fast breathing or problems breathing\n- Tiring easily, especially when feeding\n- Failure to gain weight normally\n- Swelling in the legs or abdomen\n- Sleepiness or unresponsiveness\nSurgery may greatly ease or stop these symptoms. That is especially the case if the surgery happens very early in life.\nHow is double outlet right ventricle diagnosed in a child?\nSometimes, healthcare providers can diagnose DORV in a fetus, before the birth of the baby. A diagnosis after birth starts with a health history and physical exam. A cardiologist will need to do tests to make the diagnosis as well. The most important of these is an echocardiogram. This test shows the structure of the heart and the blood flow through the heart.\nThe healthcare provider may want other tests, too. These might include:\n- Electrocardiogram (ECG), to check the heart rhythm\n- Chest X-ray, to look at heart size and the lungs\n- Cardiac MRI, to look at the structure of the heart\n- Cardiac catheterization, to get more information about the pressures in the different chambers of the heart and in the lungs\n- Blood tests, to look for oxygen levels in the blood or other factors\nHow is double outlet right ventricle treated in a child?\nDORV can be treated with surgery. Surgery can correct blood flow so that it moves the right way from the left ventricle to the aorta and from the right ventricle to the pulmonary artery. Surgery is also needed to fix the defect in the ventricular wall, as well as any other heart defects. The type of surgery will depend on the subtype and exact anatomy of the DORV and other heart problems. The timing of surgery varies. Healthcare providers may recommend it soon after birth, within the first few months of life, or later. Some infants with DORV will need more than one surgery. The outcome depends on the type of DORV, other heart problems, and overall health at age of diagnosis.\nSome children with DORV also need medicine. That may especially be the case if they haven’t yet had surgery and they have symptoms of heart failure. Specific treatment will vary based on the type of DORV and the severity of symptoms. Possible treatments include:\n- Diuretics (“water pills”) to reduce swelling\n- ACE inhibitors or digoxin to improve heart output\n- Beta-blockers to reduce how hard the heart pumps\nSome children with DORV will also need blood thinners (anticoagulant medicine) to help prevent blood clots. These are only needed after certain kinds of surgery for DORV.\nMost children with DORV go on to lead normal and active lives. But they will always need special follow-up care with cardiologists. Some may need follow-up surgery as adults.\nWhat are the complications of double outlet right ventricle in a child?\nDORV can lead to a number of complications. The risk for complications varies according to the type of DORV, other heart conditions present, and the time of diagnosis. Early treatment can cut the chance of later problems. Possible complications include:\n- Heart failure which can lead to feeding and growth problems\n- Rapid or trouble breathing\n- Harmful high blood pressure in the lungs\nChildren with heart abnormalities like DORV are also at higher risk for infection of the heart valves. To prevent it, your child’s healthcare provider might prescribe antibiotics before and after certain medical and dental procedures.\nWhat can I do to prevent double outlet right ventricle in my child?\nYou may be able to lower your risk of having a child with DORV by limiting your exposure to teratogens during pregnancy. This is especially true during the first trimester. Avoid:\n- Radiation from dental or medical procedures\n- Tobacco and alcohol\n- Exposure to infections\n- Exposure to teratogenic substances, such as retinoids, theophylline, and valproate\nBut most cases of DORV are likely not the result of exposure to these substances.\nHow can I help my child live with double outlet right ventricle?\nYour child’s healthcare provider may give you more instructions about helping your child manage DORV. But be sure to do the following:\n- Ask your child’s healthcare provider what kind of exercise is right for your child.\n- Teach your child to eat a heart-healthy diet.\n- Tell your child’s healthcare providers and dentists about your child’s DORV.\n- Be sure your child sees a specialist in congenital heart disease regularly.\nWhen should I call my child’s healthcare provider?\nCall your child’s healthcare provider right away if your child has a hard time breathing or has other severe symptoms.\nKey points about double outlet right ventricle in children\n- Double outlet right ventricle is a type of heart malformation. It is present from birth.\n- DORV is a serious condition often treated early in life with surgery.\n- DORV can cause serious complications. These include heart failure, high blood pressure in the lungs, and death.\n- Many children with DORV can lead full and active lives. But they need lifelong follow-up care.\nTips to help you get the most from a visit to your child’s healthcare provider:\n- Know the reason for the visit and what you want to happen.\n- Before your visit, write down questions you want answered.\n- At the visit, write down the name of a new diagnosis, and any new medicines, treatments, or tests. Also write down any new instructions your provider gives you for your child.\n- Know why a new medicine or treatment is prescribed and how it will help your child. Also know what the side effects are.\n- Ask if your child’s condition can be treated in other ways.\n- Know why a test or procedure is recommended and what the results could mean.\n- Know what to expect if your child does not take the medicine or have the test or procedure.\n- If your child has a follow-up appointment, write down the date, time, and purpose for that visit.\n- Know how you can contact your child’s provider after office hours. This is important if your child becomes ill and you have questions or need advice.\nOnline Medical Reviewer:\nAyden, Scott, MD\nOnline Medical Reviewer:\nBass, Pat F., III, MD, MPH\nLast Review Date:\n© 2000-2018 The StayWell Company, LLC. 800 Township Line Road, Yardley, PA 19067. All rights reserved. This information is not intended as a substitute for professional medical care. Always follow your healthcare professional's instructions."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:0046976d-aa69-4135-a22b-88b5b4b782b2>"],"error":null}
{"question":"What are the different types of cost estimation methodologies used in project planning, and what practical steps can be taken to reduce project costs while maintaining quality?","answer":"There are three main types of cost estimation: 1) Order of magnitude estimate, with accuracy of -50% to +100%, used in early stages for general expenditure estimation; 2) Budget estimate, with accuracy of -10% to +25%, used for preliminary itemized expenditures; and 3) Definitive estimate, with accuracy of -10% to +15%, providing detailed itemized lists of activities. To reduce project costs while maintaining quality, several steps can be taken: looking for cheaper resources through vendor comparison, reducing project duration by performing tasks simultaneously, reducing project scope to essential processes, reviewing workload estimates, using agile management techniques, implementing project management software, and seeking process improvements like remote work or outsourcing. It's recommended to reserve 10-20% of the budget for unforeseen circumstances.","context":["Projects and other undertakings require cost estimation to determine resource requirements. Of course, estimates are not definite and their accuracy depends on the project stage in which they are undertaken and the sources of data. Nonetheless, there are three types of cost estimation classified according to their scope and accuracy. These are (1) order of magnitude estimate; (2) budget estimate; and (3) definitive estimate.\nThe difference among the three types of cost estimation\n1. Order of magnitude estimate: An order of magnitude estimate is a rough estimation of costs used at the very early stage of a project, particularly during the evaluation and planning stages. The purpose of this type of cost estimation is to have an idea about general and total expenditures instead of itemizing expenses based on project activities and deliverables.\nManagers and planners can us historical data of a related project or an aggregate of data from different projects to perform an order of magnitude estimation. There are situations when no data is used at all. The accuracy is around -50 percent to +100 percent of the actual cost. Figures generated from this method of estimation are sometimes called ballpark figures.\n2. Budget estimate: Also referred to as preliminary cost estimates, budget estimate is more accurate than an order of magnitude estimate. Accuracy is around -10 percent to +25 percent. The purpose of this type of estimation is to generate a preliminary itemized list of expenditures based on the major components of the project. Therefore, this is done within the actual planning stage of the project.\nThere are specific types of cost estimation under budget estimate. One example is top-down estimate in which costs of the major components of the project are estimated using historical data of a related project or obtainable data using the project plan. Another example is an estimation of expected labor cost or a summative costing of equipment and materials requirements.\n3. Definitive estimate: A definitive estimate has an accuracy of around -10 percent to +15 percent and in some standards, around -5 percent to +15 percent. Sometimes called as detail cost estimates, this type of cost estimation are not only more accurate than order of magnitude of estimate and budget estimate but are also more detailed.\nReports generate from a definitive estimation include an extensive itemized list of project activities and deliverables with brief descriptions either for justifications and clarity. However, note that conducting a definitive estimation is only possible with a fully developed project plan and a work breakdown structure\nA note on cost estimation methodologies\nIt is important to reiterate the fact that the accuracy of cost estimation depends on the availability of data, as well as the project stage in which it is particularly conducted. Take note that as the project plan becomes more developed, more data can be generated and thus, budget estimates or detailed estimates become more possible.\nNonetheless, regardless of the methodology used, cost estimation is a standard process observed in project management all the way down to portfolio management. It helps identifying resource requirements and capabilities, setting priorities, and allocating resources for project planning and controlling. Costing and valuation are often the immediate byproducts of cost estimation and a review of generated reports can help guide decision-makers.","An improper allocation of resources can adversely affect the project’s profitability, cause the project to go over budget compared to the project baseline, and, in some cases, lead to project failure.\nSometimes, the project’s scope may suddenly change, which can disrupt the project plan, and you may need to add more resources that were not initially considered.\nThe project manager needs to ensure that the budget is reviewed regularly and, more importantly, in such a situation.\nIn addition, the project’s costs can be monitored by reviewing its progress during each stage and making sure there are no discrepancies.\nAllocate competent resources during the project start-up phase\nIdentifying and assigning the right resources during the start-up phase helps deliver the project on time and budget.\nIt also helps avoid allocating insufficient or overqualified – and therefore expensive – resources to certain project activities.\nOn the other hand, assigning underqualified resources will cause delays in delivery and compromise the quality of results.\nLook for cheaper resources\nThe resources used by a project can often be expensive and can reduce profit margins.\nThere are many instances of companies increasing the cost of the resources they offer when competing and many projects wasting resources, which means you are paying higher costs than you should be.\nHowever, there are many ways to reduce the amount and cost of resources used, such as comparing pricing from multiple vendors and implementing streamlined processes.\nReduce project duration\nAnother way you can reduce the expense of a project is to shorten its duration.\nThe longer a project is, the more resources will need to be used, including materials, administrative costs, energy, and employees, all of which will need to be paid.\nProject length is one of the main reasons why companies fail to meet their original budgets.\nTo shorten the critical path, you might consider performing multiple tasks simultaneously, turning full dependencies into partial dependencies that can run parallel.\nReduce project scope\nIf the project’s cost is a concern, one solution might also be to consider reducing the scope of the project.\nAlthough the scope is determined during the initial planning stages, this can prove costly and time-consuming if it is too broad.\nYou can reduce the scope by reducing the number of tasks that will be met simultaneously, limiting the project to only the most essential processes and requirements.\nReview workload estimates\nDo resources need 8 hours a day to perform a task, or can some of them work fewer hours a day on that given task?\nNot only should duration estimates be evaluated to shorten the program, but also workload estimates.\nManage your budget correctly\nIn addition to the aforementioned tips, it is also essential to implement a sustainable budget into the project plan and for the project manager to be able to monitor progress and adjust it to needs when necessary.\nMost budgets are relatively rigid because of budget approval early in the planning phase, which often includes funding from project sponsors.\nTherefore, the project manager must be able to work within these boundaries throughout the process.\nTo do this, you need to map your expenses before you start and use budget tracking tools to stick to them throughout the project lifecycle.\nUse agile management techniques\nAgile management can be the perfect way to avoid wasting resources, leading to lower project costs.\nAgile management uses short development cycles to advance and improve a project continuously.\nUse project management software\nAlthough project management software often has a high upfront cost, using it can help reduce the cost of a project.\nThat’s because dedicated software can help reduce the cost of manual processes and the extra time to be invested and allow employees to be more productive through automated processes, resource optimization, the ability to meet deadlines, and the reduction of administrative errors.\nIn this regard, the advice is to try a free project management software such as Twproject.\nSeek process improvements\nA final solution might be to see if there are ways in which you can achieve your goals differently.\nFor example, is it possible to do work remotely instead of requiring office visits?\nIs it possible to gather requirements in one day, through a joint design session, rather than in three weeks using traditional methods?\nIs it possible to outsource some of the work at a lower cost than doing it in-house?\nThese and other questions can lead to money-saving process changes.\nIn conclusion, the main goal of cost management for a project is to eliminate unnecessary costs without compromising the quality of the final results.\nHowever, this is an ongoing process within project management and requires constant monitoring to try to maintain the project budget.\nAlso, determining discrepancies helps you take timely action and avoid going over budget.\nOften, project managers reserve 10 to 20 percent of the budget for unforeseen circumstances and create the project plan with the remaining budget so that there are no last-minute hiccups."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:eab8bdb9-d302-4807-b7a7-73e27b0ea45a>","<urn:uuid:52bbaf1e-7a87-4504-9e85-5c913526549e>"],"error":null}
{"question":"What are the key differences in British and Union naval strategies when attacking New Orleans in 1815 and 1862 respectively?","answer":"The British attack in 1815 was part of a broader Gulf Coast Campaign, with Admiral Cochrane implementing a littoral campaign strategy from September 1814 through February 1815. This included operations at various locations like St. Mary's, Cumberland Island, and Pensacola. In contrast, the Union's 1862 naval strategy focused specifically on the Mississippi River approach, featuring David Farragut's fleet and David Porter's innovative mortar boats. The Union plan involved a nighttime passage past the Confederate forts, with ships camouflaged with river mud and saplings, and reinforced with anchor chains and sandbags. Their strategy proved successful, as fourteen of Farragut's ships passed the enemy defenses in just 25-30 minutes, leading to New Orleans' surrender.","context":["The Louisiana Institute of Higher Education will present a historical symposium on the Battle of New Orleans on Jan. 10 and 12 at the Nunez Community College in Chalmette. The event is free. Registration is not required. The program is financed by the Greater New Orleans Foundation Exxon-Mobil Fund.\nOn Jan. 10, a social history of the battle will explore the individuals and groups instrumental in the battle. The welcome will begin at 10:30 a.m. :The Battle of New Orleans: A Holistic Approach to Cochrane's Littoral Campaign in the Gulf of Mexico (September 1814 through February 1815)” will be featured from 10:40 to 11:15 a.m. Martin K.A. Morgan will discuss the British military strategy leading up to the Battle while also detailing the role of Native-Americans and slaves. From 11:20 a.m. to 12:25 p.m., “Old Hickory and Old Creoles: Andrew Jackson and the Town He Saved” will be featured. Christina Vella will discuss New Orleans and the world in 1815 as well as Andrew Jackson. Lunch will be from 12:30 to 1:25 p.m. Participants may attend a complimentary lunch while informally interacting with lecturers, participating in book-signings and enjoying impromptu talks about the finer points of the battle.\nFrom 1:30 to 2:05 p.m., “A Trifling, Tedious Distraction: British Attitudes to the War of 1812 and their Impact on New Orleans and the Southern Campaign” will be held. Samantha Cavell will present an overview of the British perspective of the Battle, America and New Orleans. From 2:10 to 2:55 p.m., “The Battle that Never Ended: The Long-lasting Impact of the War of 1812 on New Orleans’ Free People of Color” will be presented. Ina Fandrich will discuss the role of free people of dolor, detailing two regiments, African American drummer Jordan Noble and the bi-racial Creole family of Marie Laveau. From 3 to 3:35 p.m., “Islenos of St. Bernard Parish: the Third Regiment of the Louisiana Militia” will be presented. William Hyland will discuss the role of Islenos in the battle.\nA reception and buffet dinner will be held from 3:45 to 6:30 p.m.\nOn Jan. 12, a military history of the battle will explore the idea of the battle as one part of a full-scale Gulf Coast Campaign.\nThe welcome will begin at 10:30 a.m. From 10:40 to 11:15 a.m., “The Battle of New Orleans: Part of a full-scale Gulf Coast Campaign and Vitally Important to the Future of America as an Independent Nation” will be discussed. Timothy Pickles will discuss the context of the battle within the War of 1812 and broader international affairs.\nFrom 11:20 to 11:55 p.m., “Leaks and Losses: Cochrane’s Grand Southern Strategy and the Intelligence Debacle that helped Britain lose the Battle of New Orleans” will be featured. Sam Cavell will discuss important events leading up to the battle, including military operations at St. Mary’s and Cumberland Island, Georgia and Pensacola, Fla.\nFrom noon to 12:35 p.m., “Cochrane and the Tonnant in the War of 1812” will be discussed. Bill Griffin will discuss the story of Vice-Admiral Sir Alexander Cochrane and his flagship, the HMS Tonnant, and their effect on the War of 1812, including the lead-up to the Battle. From 12:35 to 1:25 p.m., a lunch and talk on the contemporary remnants of the battle will be held. Tim Strain will discuss the remnants of the battle in today’s world.\nFrom 1:30 to 2:05 p.m., “The Battle of New Orleans: But For A Piece of Wood,” will be presented. Ron Chapman will discuss the battle, especially the operations on the West Bank. From 2:10 to 2:55 p.m., “The Battle of New Orleans: The Perspective of the 7th U.S. Infantry” will be featured. Steve Abolt will discuss the importance of the 7th U.S. Infantry in the battle.\nFrom 3 to 3:35 p.m., “They Ran So Fast that the Hounds Couldn't Catch 'em: An Analysis of the Exfiltration Operation that followed the Battle at Chalmette (Jan. 8 through Feb. 12, 1815)” will be presented. Martin K. A. Morgan will discuss the aftermath of the battle, which included a British military victory at Fort Bowyer.\nFor information on the symposium, call Curtis Manning at 504.512.5120 or email email@example.com","A Queen Falls\nThe Surrender of New Orleans was a Critical Moment in the Civil War.\nOne-hundred-fifty years ago, April 1862, New Orleans fell to Union forces one year into the Civil War. The impact of this loss proved critical. At the time, England and France, who relied on southern cotton and northern wheat, were debating mediating between the belligerents. President Lincoln sought to have Europe remain neutral; President Jefferson Davis urged Europe to recognize the Confederacy as an independent nation.\nMajor Pierre G.T. Beauregard, a second lieutenant in the Army Engineers, had drained New Orleans and constructed the Custom House. A Creole to the core, he immediately resigned his commission and joined the Confederacy when war became a reality. He considered the Mississippi River the city’s most vulnerable point and formulated a plan of defense based upon an attack from below.\nBeauregard realized that the two forts south of New Orleans, Fort St. Philip and Fort Jackson, could protect the city, but only if the trees were cut from the batture to permit a clear line of fire, and only if the forts were properly armed with long guns. In addition, he ordered that a strong, lighted chain barrier to be stretched across the river to inhibit any attempts to sail upstream and that “fire rafts” be strategically placed along the river’s bank to be set alight and floated downriver in the event of an attack. Gunboats would add additional firepower.\nBeauregard noted that the city must be able to repel an attack for about 30 minutes and do massive damage in that short time because they could only hold the enemy at bay for that long.\nOn April 12, 1861, the same P.G.T. Beauregard had ordered his gunners to fire on Fort Sumter, thus opening the Civil War. In response, President Lincoln blockaded southern ports. New Orleans commerce crashed from $550 million per year to a mere $51 million per year within months.\nNearly all of the troops, arms and ammunition available to New Orleans had been ordered to Virginia. Cannons lacked shells and carriages. The ironclad Louisiana stood unfinished, lacking a driveshaft, guns and some armor. The ironclad Manasses likewise remained uncompleted in dry dock.\nWorse still, Confederate leadership mistakenly believed that New Orleans could only be taken by an attack from upriver, which required passing the heavily defended Vicksburg, Miss., – an impossible task.\nIn December of 1861 Union forces occupied Ship Island, the landing point for the British during the Battle of New Orleans. New Orleans residents shouted: “Chalmette’s glories will be repeated!”\nRichmond persisted in the assertion that an attack would only come from upriver.\nSoon shortages of food and basic supplies plagued New Orleans. The city established free markets for the hungry, which serviced 723 families on the first day. By March 7, 1862, the families needing assistance climbed to 1,862. By March 25, 1,921 families depended upon charity for sustenance.\nGovernor Thomas O. Moore appointed General Mansfield Lovell, formerly of Massachusetts, to defend the city, but he lacked arms and men and his command was limited to land troops. He had no authority over the unfinished gunboats Louisiana or Manasses, which belonged to private enterprises; he could not command the river fleet, nor could he give orders to the fire rafts.\nNews reports from Washington indicated that a major invasion force was headed for the Gulf of Mexico. An alarmed Lovell responded by sending a message to his superiors in Richmond: “The forts can be passed, we are disorganized, and have no general officer to command and direct.”\nTheir response: “The fleet is not headed for New Orleans and fears of our people are without cause.” Confederate leaders steadfastly refused to alter their belief that the Union attack must come from upriver.\nOn September 19, 1862, the Union warship USS Water Witch crossed the bar and took possession of Head of the Passes. On Oct. 10, four additional ships joined the Water Witch, thus closing the river to all commerce.\nFor a brief moment Confederate forces has cause for celebration. On what is called “The Night of the Turtle,” Confederate seamen took the ironclad steam ram Manassas downriver. In total darkness, the Manassas surprised the Union ships and rammed one. Captain Pope, who commanded the group of ships, ordered a full retreat, abandoning the passes and embarrassing the Union.\nHowever, the celebration proved short-lived; the collision had dismounted one of the Manassas’ two steam engines and the ship had to limp back to port for repairs.\nWhile this action unfolded, Union generals were making plans.\nNaval Officer David Porter planned to construct a new type of naval vessel, a mortar boat, for bombarding the forts so as to open the gate for a naval invasion from the mouth of the river.\nRunning a Gauntlet\nGeneral John Barnard, Chief Engineer of the Army of the Potomac, who – along with Beauregard – had been in charge of rebuilding the forts in the 1840s, calculated that ships steaming upriver would run a gauntlet of fire for 3.5 miles. For 2 miles they would face the fire of 100 to 125 guns, and 50 to 100 guns for 1.5 miles. That, of course, assumed that these forts were properly equipped and manned. It would take only 25 to 30 minutes to pass the forts. It must be done at night.\nLincoln approved Porter’s mortar boats, and also Porter’s recommendation that David Farragut, Porter’s foster brother, be made fleet commander.\nThe Washington Star, a northern newspaper, disclosed details of the Union plan of attack. General Lovell received a copy. Meanwhile, Confederate commanders in Richmond still refused to accept reality.\nAs for the forts, Lovell later reported: “I found matters generally so deficient and incomplete that I was unwilling to commit their condition to writing for fear of their falling into the wrong hands.”\nFurther complicating matters, the River Defense Fleet’s gunboats left New Orleans to support the defense of Island No. 10, which occupied a double bend of the Mississippi River near New Madrid, Mo., blocking the Union’s advance. Maintaining this position was a critical concern for the South.\nLovell’s problems mounted. The Mississippi River ran at an exceptionally high flood tide during spring 1862. High water and debris pressed against the blocking chain. It gave way. Although repaired, it still lacked the strength needed to repel a concerted attack by a determined enemy.\nShip Island became the staging area for the fleet and General Benjamin “The Beast” Butler’s 15,000 troops. With all details complete, Farragut hoisted anchor and crossed the sandbars at the mouth of the Mississippi River.\nOn April 18 of that year, Porter took his position below the forts, around a bend in the river to avoid coming under fire from the Confederate forts, and began shelling the forts. The massive volleys continued for several days, but Farragut was growing impatient. He ordered a midshipman to climb a mast and signal which mortar shells fell into the forts and which landed short or long. There were more “outs” than “ins.” The forts weren’t being reduced.\nFarragut camouflaged his ships with river mud, saplings and paint. He reinforced bulkheads with anchor chains and engines with sandbags. He sanded the decks to provide traction against water and blood.\nOn April 23, 1862, Farragut ordered the attack. At 2 a.m. on the 24th, he ordered two lanterns hoisted from the mizzenmast of his flagship Hartford, signaling his ships to advance at 3:30 a.m. The Union ships approached in three divisions: the first led by Captain Theodurus Bailey, the center under Admiral Farragut and the last under Captain H.H. Bell.\nAt 3:30 a.m., Confederate Sergeant Herman in the water battery below Fort Jackson reported to Captain William Robertson that he detected “… several black, shapeless masses barely distinguishable from the surrounding darkness moving silently but steadily up the river.” At 3:40 a.m. the guns of both forts Jackson and St. Philip opened fire.\nA fierce battle ensued involving competing cannon fire from the forts and Farragut’s ships. Confederate gunboats added to the melee with occasional efforts to ram Union ships by the hastily repaired Confederate ironclad Manassas. The few fire rafts that were lighted had little effect.\nThe Confederate gunboat Louisiana, upon which so much depended, remained tied to the shoreline above the fort.\nDavid Porter of the mortar boat flotilla stated: “I shake a little now when I think how near we came to being defeated. One day’s more delay and the game would have been blocked on us. They would have put the Louisiana in the only narrow channel where ships had to pass, and she would have sunk everything that came by.”\nRather than have these dangerous ironclads fall into Union hands, both would be set on fire and sunk.\nDaylight on April 24 found the Union fleet above the forts and on their way to New Orleans. Fourteen of Farragut’s ships had passed the enemy. One, the Varuna, lay disabled alongside the bank; three others sustained damage and turned downstream to the protection of Porter’s gunboats. Commander Alden wrote in his log: “Victory! The American flag floats over everything on the Mississippi River this morning.”\nThe batteries at Chalmette and the West Bank were hopelessly outgunned by the massive broadsides the Union fleet delivered. The West Bank battery possessed nine guns; Chalmette had only five. The Confederates abandoned their position after running out of ammunition.\nNew Orleans had no defenses. No batteries had been erected and only Lovell’s army remained. Lovell sent a telegram to Richmond: “The enemy has passed the forts. It is too late to send any guns here; they had better go to Vicksburg.”\nTotal casualties for the entire invasion amounted to fewer than 50 Confederates fewer than 200 for the Union.\nFarragut’s fleet found the river littered with cotton bales, burning ships, sugar casks and other debris. Wharves and warehouses were on fire. Curiously, sailors witnessed some civilians running along the levees waving either white flags of surrender or American flags. Others played “Dixie” and cursed the offending Union navy.\nWatching in Disbelief\nAt 2 p.m. Farragut’s fleet arrived in New Orleans under a heavy rain. Thousands of soaked New Orleanians stood on the levees and watched in disbelief as the USS Mississippi struck up the “Star-Spangled Banner.”\nForts Jackson and St. Philip now lacked communication with New Orleans. On April 27, 1862, Captain David Porter sent Confederate commander Duncan a request to surrender. Duncan refused, taking notice that he had no orders from the city to give up his position.\nSoon, events took on a momentum of their own. Duncan and Higgins noted a change in attitude among the men at Fort Jackson, ending in a small mutiny. “They seized the guards, turned the guns around and started spiking the guns.” Some immediately left the fort while others refused to obey orders. Duncan realized that holding out was impossible because “… there was no longer any fight in the men remaining behind … they were completely demoralized.”\nOn April 28, Duncan contacted Porter and arranged a surrender of the forts. Duncan and Higgins boarded the Union ship USS Harriet Lane and conceded the forts to the Union.\nBack in New Orleans, capitulation was under way as well. Under the pelting rain, Farragut ordered Captain Theodorus Bailey and Lieutenant George Perkins ashore protected by a white flag to demand the city’s surrender. They rowed to the wharf at Laurel Street. According to Bailey: “No one received us, although the whole city was watching our movements, and the levee was crowded in spite of the heavy rain-storm. Among the crowd were many women and children, and the women were shaking rebel flags and being rude and noisy.”\nAuthor George Washington Cable recalled years later that the actions of these two Federal officers was: “… one of the bravest deeds I ever saw. [they] walked abreast, unguarded and alone, looking not to the right or left, never frowning, never flinching, while the mob screamed in their ears, shook cocked pistols in their faces, cursed and crowded, and gnashed upon them. So through the gates of death those two men walked to City Hall to demand the town’s surrender.”\nDespite vitriolic abuse, Bailey presented David Farragut’s Unconditional Surrender document that called for hoisting the American flag over the Custom House, the Mint and the Post Office, and removal of the State Flag over the City Hall.\nMayor Monroe met with the City Council. They hated to surrender, but now lacked any means of defending the city. Bailey and Perkins were then secretly escorted back to their ships in a covered coach with a Confederate escort as locals pounded and kicked the mayor’s office door.\nFarragut ordered a troop of Marines to hoist the “Stars and Stripes” on the flagstaff at the U.S. Mint on Esplanade Avenue as well as the at Custom House. A citizen named William Mumford tore down the flag at the Mint.\nIn the meantime, discussions with the city council continued as anti-Union mobs ran through the streets. Finally, on April 28, Farragut informed Mayor Monroe that he had 48 hours to remove women and children before the fleet opened fire.\nOn April 29, New Orleans surrendered under occupation of only 250 Marines and two howitzers. On May 1 General Butler had Mumford hung for tearing down the American flag on the U.S. Mint.\nSouthern morale suffered a serious blow. Mary Boykin Chestnut confided in her diary: “New Orleans is gone, and with it the Confederacy! Are we not cut in two? The Mississippi ruins us if it is lost.”\nThe fall of New Orleans irreparably damaged the southern cause in the Civil War militarily, politically and diplomatically. This next month marks the 150th anniversary of this critical historic event. The tide had turned April 29, 1862, when the queen fell.\nRon Chapman is a professor of history at Nunez Community College."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:60383414-0acc-47f3-a952-c7779eca5417>","<urn:uuid:7a85c207-e5fd-4612-b6ed-83b3c1b3bf82>"],"error":null}
{"question":"What's the deal with the ACEF Score? I keep hearing about it in cardiology discussions!","answer":"The ACEF Score is a risk model specifically designed for predicting operative risk in patients undergoing elective coronary artery bypass grafting (CABG). It uses just three variables: the patient's age, left ventricular ejection fraction, and creatinine. The formula is: age (years)/ejection fraction (%) +1 (if creatinine>2mg/dl). The score has been validated for elective patients and shows similar sensitivity/specificity and predictive values compared to other risk models including the Euro SCORE.","context":["Our mission is to become a worldwide reference for education in the field for all professionals involved in the process to disseminate knowledge & skills of Acute Cardiovascular Care.\nOur mission is to promote excellence in clinical diagnosis, research, technical development, and education in cardiovascular imaging in Europe.\nOur mission is to promote excellence in research, practice, education and policy in cardiovascular health, primary and secondary prevention.\nOur mission is to reduce the burden of cardiovascular disease in Europe through percutaneous cardiovascular interventions.\nOur mission is to improve the quality of life of the population by reducing the impact of cardiac rhythm disturbances and reduce sudden cardiac death.\nOur mission is to improve quality of life and longevity, through better prevention, diagnosis and treatment of heart failure, including the establishment of networks for its management, education and research.\nThe ESC Working Groups' goal is to stimulate and disseminate scientific knowledge in different fields of cardiology.\nThe ESC Councils' goal is to share knowledge among medical professionals practising in specific cardiology domains.\nOUR MISSION: TO REDUCE THE BURDEN OF CARDIOVASCULAR DISEASE\nProf. Nawwar Al-Attar\nDr. Thierry Folliguet\nIt used to be that the cardiologist would act as gatekeeper for percutaneous coronary intervention and coronary artery bypass grafting. In contrast, the concept of the Heart Team is being advocated whereby a surgeon, interventionalist, cardiologist and other specialties as required work together to decide on needed treatment. Read here about the genesis of the Heart Team, its advantages and limitations, and find a review of risk scores and how a model can help guide decision-making.\nSince the introduction of coronary artery bypass surgery (CABG) in 1968 and percutaneous transluminal coronary angioplasty (PTCA) as a nonsurgical alternative in 1977 - with bare-metal (1987) and then drug-eluting stents (1991) broadening the range of acts performed on the lumen and PTCA becoming part of the broader term percutaneous coronary intervention (PCI) - both CABG and PCI remain an option in a large part of patients with coronary artery disease today (1).Nevertheless, cardiovascular variability of PCI-to-CABG from one country/center to another is high and raises concerns over inappropriate revascularisation and elected procedure.Leaving aside varying economic considerations and expanded PCI indications, many decisions are case-by-case and surgeons and cardiologists can differ in the information they give regarding the choice between PCI and CABG and PCI to patients resulting in PCI to be overly used on the whole (2). There is also growing awareness that a multidisciplinary approach to medicine, as seen in tumor boards or organ-approach centers, improves level of care where physicians work together and thus avoid one individual physician-related factors driving treatment. Indeed, patient centric evidence-based care multidisciplinary Heart Teams would probably offer the best care to patients (3). For example, in trans-aortic valve implantation, a heart-team approach decided which maneuvres to attempt in complications resulted in better survival than when physicians decided on strategies on their own (4).Risk scores also play an important part in deciding on the optimal strategy. While they are important in the decision-making process - the 2009 perioperative guidelines as well as the 2010 guidelines on myocardial revascularisation state that risk scoring as the first of its take home message, they should only be used as a guide. After describing the history of the Heart Team concept, its advantages and limitations, we describe how to use risk scores to the Heart Team's advantage.\nThe heart team approach can be found in the SYNTAX trial (2008) as a means to select and randomise the patients according to their coronary lesions (1). It was an all-comers trial including 1,800 patients with three-vessel disease, left main disease or LM equivalent with or without 1, 2 or 3 diseased vessels, de novo lesions with at least 50% stenosis and myocardial ischemia. The Heart Team was designed as a 1) heart surgeon and 2) an interventional cardiologist and both reviewed each clinical case. Consensus agreement was obtained as to which procedure(s) the patient was eligible for – CABG or PCI.In 2010, the ESC took this approach into its guidelines on myocardial revascularisation (2010) (5) and wrote about the concept of the Heart Team jointly with the European Association for Cardio-Thoracic surgery. The guidelines state that revascularisation strategy should be based on best evidence and discussed on a multidisciplinary level, within a Heart Team, that will require interaction between 1) cardiologists and 2) cardiac surgeons, 3) referring physicians or other specialists as desirable - as anaesthesiologists, geriatricians, or intensivists.The ACCF/AHA guideline for CABG (6) and one for PCI (7) (2011) followed up as well to recommend a Heart Team approach, defined as a multidisciplinary approach for revascularisation, made up of an 1) interventional cardiologist, 2) a cardiac surgeon, and often 3) the patient’s general cardiologist in patients with unprotected left main or complex CAD. The latest ESC guidelines in valvular heart disease recommend that decision-making in patients with valvular disease should be driven by a ‘‘heart team’’ with particular expertise in valvular heart disease (VHD), including cardiologists, cardiac surgeons, imaging specialists, anaesthetists and, if needed, other specialists. (8)\nPrior to the above mentioned guidelines, trials and a prior guideline had prefigured the Heart Team.\nThe concept of choosing one treatment over another is organised around the concept of the heart team which involves various physicians such as cardiac surgeons, interventional cardiologist, general cardiologists as well as intensivists. However, much criticism has risen regarding the role of the heart team or on the impact of recent guidelines recommending the heart team approach on a daily routine. Reasons we have observed for not implementing a heart team in the cardiologists decisions include:\nWhile finding solutions to the above mentioned hindrances will require time and effort on all parts, risk models are readily available to physicians, interventional cardiologists and cardiac surgeons and can help to ensure that their patients are able to weigh the individual benefits and risks of medical therapy, PCI, and CABG, and come to their own informed decision.\nHigh risk in PCI is characterised more by lesion complexity than co-morbidities, and is therefore best assessed by risks models of angiographic characteristics such as the SYNTAX score. The SYNTAX score is derived only from the coronary anatomy and lesion characteristics, and is calculated using dedicated software, enabling complex coronary artery anatomy to be quantified (12). It is able to predict 3-year outcomes in patients who have complex coronary artery disease (triple vessel and/or left-main stem) (13). The higher the score, the more the team should turn to CABG. Amongst patients having CABG both retrospective and prospective studies have concluded, as expected, that the SYNTAX score is a poor predictor of events in patients with complex coronary disease undergoing CABG (14, 15). Limitations of the SYNTAX score are the absence of clinical variables and the subjective nature of assessment which can lead to inter-observer variability and the absence of SYNTAX score algorithms for the increasing numbers of patients with prior revascularisation (PCI/CABG) who return for secondary revascularization with PCI. (4).However, the recently published on SYNTAX II score added eight clinical variables (predictors) to its scoring process: anatomical SYNTAX score, age, creatinine clearance, left ventricular ejection fraction, presence of unprotected left main coronary artery disease, peripheral vascular disease, female sex, and chronic obstructive pulmonary disease.Anatomical and clinical characteristics are thus included into scoring to guide decision making between coronary artery bypass surgery and percutaneous coronary intervention for individual patients, and is able to predict 4-year outcomes (16).\nThe FAME study established that utilising flow reserve (FFR) measurements - FFR = Pd/Pa (Pd = pressure distal to the lesion, Pa = pressure proximal to the lesion) - to determine the functional significance of individual coronary lesions and guide subsequent coronary intervention, leads to a potential prognostic impact when compared with angioplasty guidance alone (17, 18).\nThe ACEF Score is risk model for predicting operative risk in patients undergoing elective CABG uses just three variables: 1) patient’s age, 2) left ventricular ejection fraction and 3) creatinine in the formula: age (years)/ejection fraction (%) +1 (if creatinine>2mg/dl). So far, validation has only been performed in elective patients, where the score was shown to have similar sensitivity/specificity, and similar positive/negative predictive values compared to other risk models including the additive/logistic Euro SCORE (19).\nThe potential of the Euro SCORE to risk stratify patients undergoing PCI has been assessed in two prospective, single centre non-randomised studies (25, 26). Its role in the assessment of patients getting CABG is undisputed; whilst, its role in the risk stratification of patients undergoing PCI still requires further evaluation. Several parameters in the score have no relevance for PCI, and therefore modifications to the score are required before it can be fully adopted as a risk model in PCI.\nRisk models should not be relied upon to determine an individual patients' actual risk, but instead should be used as a guide to help make more informed clinical decisions. Physician experience is often one of the most important factors in the final decision, which should always be made at a multidisciplinary level.The Heart Team approach to assessing a global risk classification would be to combine the Syntax and the Euro SCORE. This approach of global risk classification should allow physicians to better select the appropriate revascularisation strategy. Indeed, a simple graph can be constructed based on the guidelines' recommendations (Fig. 1) combining:\nThe more complex the coronary anatomy the more CABG has been shown to improve survival and therefore it is the preferred theoretical treatment choice (5, 6). However, the risk score of the patient should be nevertheless be assessed. For a high surgical risk, other options such as PCI or medical treatment or incomplete revascularisation - off pump left internal mammary artery to left anterior descending artery (LIMA LAD) bypass - or hybrid strategy should be discussed.This course allows providing a risk/benefit analysis for an individual patient. Additionally, the need for a given patient may differ according to its lifestyle. Incorporating functional SYNTAX II score may also serve to implement this strategy, as it should be tested in future studies.\nFigure 1. Current coronary revascularisation according to risk score and anatomical risk.SIHD: stable ischemic heart diseaseCABG: Coronary arteries bypass graftingPCI: percutaneous coronary interventionAnatomy High risk: Syntax ≥ 33, Intermediate risk: Syntax 23-32, Low risk: Syntax 0-22Risk score; low risk Euro SCORE < 3; intermediate ES 3-5; high risk ES > 6\nFigure 2. Heart team recommendation for coronary revascularisation according to risk score and anatomical risk.SIHD: stable ischemic heart disease CABG: Coronary arteries bypass grafting PCI: percutaneous coronary interventionAnatomy High risk: Syntax ≥ 33 Anatomy Intermediate risk: Syntax 23-32 Anatomy Low risk: Syntax 0-22 Risk score; low risk Euro SCORE < 3;intermediate ES 3-5; high risk ES > 6ConclusionsThe guidelines on myocardial revascularisation were the fruit of surgeons' and interventionalists'/cardiologists' societies, mirroring the \"heart-team\" approach to coronary revascularisation decisions recommended in the guidelines.Indeed, it was recognised that coronary artery disease decisions on a general recommendation level are best addressed by both associations.Individual CAD decisions can be complicated and selection of optimal treatment primarily depends on both the complexity of coronary anatomy and comorbidity of the patient therefore use of risk scores can also help with decision-making - the model combining the Syntax and Euroscore that we present being very useful. Heart teams may also decide to develop local-based protocols. The Heart Team will then be best able to adequately inform patients regarding mortality, stroke, myocardial infarction and angina as well as recovery time, medication and expected lifestyle after each procedure. Although the Heart Team approach receives the highest level of recommendation (I) it only receives a C level of evidence because heart team decision making needs to be the object of randomised controlled trials: data on heart teams for patients with CAD should build in the next few years. Distancing us from turf battles and professional silos between specialties we might ease into a Heart Team approach, and perhaps move on to implement Hybrid teams or rooms in the future, where interventionalists and surgeons work in common.\n1. The SYNTAX Trial: A Perspective.Rajiv Gulati, et Circ Cardiovasc Interv. 2009 Oct;2(5):463-7.2. The rationale for Heart Team decision-making for patients with stable complex coronary artery disease. Stuart J. et al. The heart team of cardiovascular care. Am Coll Cardiol. 2013 Mar 5;61(9):903-7. Holmes DR Jr et al.3. Venn diagrams in cardiovascular disease: the Heart Team concept. Holmes DR Jr et al. Eur Heart J. 2013 Jan 11. [Epub ahead of print]4. Severe intraprocedural complications after transcatheter aortic valve implantation: calling for a heart team approachSeiffert M et al. Eur J Cardiothorac Surg. 2013 Feb 6. [Epub ahead of print]5. Guidelines on myocardial revascularizationWilliam Wijns, Philippe Kolh et al. The Task Force on Myocardial Revascularization of the European. Society of Cardiology (ESC) and the European Association for Cardio-Thoracic Surgery (EACTS).6. Guideline for Percutanous Coronary Intervention A report of the American College of Cardiology Foundation/American Heart Association Task force on Practice Guidelines and the Society of Cardiovascular Angiography and Interventions. Circulation. 2011;124:e574-e651.7. 2011 ACCF/AHA Guideline for Coronary Artery Bypass Graft Surgery: A Report of the American College of Cardiology Foundation/American Heart Association Task Force on Practice Guidelines. Hillis LD et al. Circulation; 124(23):e652-735.8. Guidelines on the management of valvular heart disease (version 2012). Vahanian A, Alfieri O, Andreotti F, et al. The joint task force of valvular heart disease of the European Society of Cardiology (ESC) and the European Association of Cardio-Thoracic Surgery (EACTS). European Heart Journal (2012) 33, 2451–2496.9. Angioplasty or surgery for multivessel coronary artery disease: comparison of eligible registry and randomized patients in the EAST trial and influence of treatment selection on outcomes. Emory Angioplasty versus Surgery Trial Investigators. King SB 3rd, et al. European Heart Journal (2009) 30, 2769–2812.10. Long-Term Clinical Outcome in the Bypass Angioplasty Revascularization Investigation Registry.Frederick Feit, et al. Comparison With the Randomized Trial. Circulation. 2000 Jun 20;101(24):2795-802.11. Guidelines for pre-operative cardiac risk assessment and perioperative cardiac management in non-cardiac surgery Don Poldermans et al. The Task Force for Preoperative Cardiac Risk Assessment and Perioperative Cardiac Management in Non-cardiac Surgery of the European Society of Cardiology (ESC) and endorsed by the European Society of Anaesthesiology (ESA) European Heart Journal (2009) 30, 2769–2812.12. The SYNTAX Score: an angiographic tool grading the complexity of coronary artery disease. Sianos G. et al. EuroIntervention 2005; 1(2):219-227.13. Comparison of coronary bypass surgery with drug-eluting stenting for the treatment of left main and/or three-vessel disease: 3-year follow-up of the SYNTAX trial.Kappetein AP, et al. Eur Heart J; 32(17):2125-213414. Prognostic value of the Syntax score in patients undergoing coronary artery bypass grafting for three-vessel coronary artery disease.Lemesle G et al. Catheter Cardiovasc Interv 2009; 73(5):612-617.15. Assessment of the SYNTAX score in the Syntax study. Serruys PW et al. EuroIntervention 2009; 5(1):50-56.16. Anatomical and clinical characteristics to guide decision making between coronary artery bypass surgery and percutaneous coronary intervention for individual patients: development and validation of SYNTAX score II. Farooq et al. Lancet. 2013 Feb 23;381(9867):639-5017. Angiographic versus functional severity of coronary artery stenoses in the FAME study fractional flow reserve versus angiography in multivessel evaluation. Tonino PA et al. J Am Coll Cardiol; 55(25):2816-282118. Fractional flow reserve versus angiography for guiding percutaneous coronary intervention in patients with multivessel coronary artery disease: 2-year follow-up of the FAME Pijls NH et al. J Am Coll Cardiol; 56(3):177-184. 19. Risk of assessing mortality risk in elective cardiac operations: age, creatinine, ejection fraction, and the law of parsimony. Ranucci M, et al. Circulation 2009; 119(24):3053-306120. The Society of Thoracic Surgeons: 30-day operative mortality and morbidity risk models. Shroyer AL, Coombs LP, Peterson ED et al. Ann Thorac Surg 2003; 75(6):1856-1864; discussion 1864-1855.21. The Society of Thoracic Surgeons 2008 cardiac surgery risk models: part 1--coronary artery bypass grafting surgery. Shahian DM et al. Ann Thorac Surg 2009; 88(1 Suppl):S2-2222. European system for cardiac operative risk evaluation (EuroSCORE). Nashef SA et al. Eur J Cardiothorac Surg 1999; 16(1):9-13.23. The logistic EuroSCORE.Roques F, Michel P, Goldstone AR, Nashef SA. Eur Heart J 2003; 24(9):881-88224. The logistic EuroSCORE in cardiac surgery: how well does it predict operative risk?Bhatti F. et al. Heart 2006; 92(12):1817-1820\n26. EuroSCORE as predictor of in-hospital mortality after percutaneous coronary intervention. Romagnoli E et al. Heart 2009; 95(1):43-48.\nThierry Folliguet, CHU Nancy, Nancy, FranceNawwar Al-Attar, Golden Jubilee Hospital, Glasgow, UKWorking group on Cardiovascular SurgeryAuthors disclosures: None declared.\n© 2017 European Society of Cardiology. All rights reserved"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:f5db891b-6cd6-4d20-bba3-ed82001cb684>"],"error":null}
{"question":"黑洞IC 310和Sombrero Galaxy的距离哪个离地球更远？Can you tell me the distance difference between them? (Which one is farther from Earth - black hole IC 310 or Sombrero Galaxy?)","answer":"IC 310 is farther from Earth at approximately 260 million light years away, while the Sombrero Galaxy is about 28 million light years away. The difference in distance between them is 232 million light years.","context":["The radio galaxy IC 310 in the Perseus constellation is approximately 260 million light years away from Earth. Astronomers assume there is a supermassive black hole at its centre. This black hole was the setting for an extremely powerful outburst of gamma rays which was observed by the MAGIC telescopes on La Palma in the Canary Islands. The source exhibited one of the fastest radiation variations which researchers have ever been able to detect in an extragalactic object at these energies.\nIn the course of the outburst, the strength of the measured radiation from the core of the IC 310 galaxy varied within only five minutes, much to the surprise of the researchers. Yet the event horizon of the black hole – the boundary surface in space-time beyond which events are no longer visible – is three times the distance between Sun and Earth, i.e. 450 million kilometres. Light would need at least 25 minutes to travel this distance.\n“No object can suddenly light up its entire surface faster than light takes to travel across it,” says Julian Sitarek from the Institut de Fisica d’Altes Energies (IFAE) in Barcelona. And MAGIC spokesperson Razmik Mirzoyan from the Max Planck Institute for Physics in Munich adds: “The region from which the gamma radiation originates must therefore be significantly smaller than the event horizon of the black hole.”\nThis means that the astronomers have apparently observed the black hole with the MAGIC telescopes in great detail. But what is so special about the gravitational trap at the heart of IC 310?\nThe masses of galactic black holes range from one million to several billion solar masses. If matter falls onto these objects, this occasionally leads to unusually powerful radiation outbursts at wavelengths across the whole spectrum. Such active galactic nuclei (AGN) produce so-called jets, in which the gas is blown into space with approximately the speed of light as if being ejected from a nozzle.\nIC 310 in the Perseus galaxy cluster belongs to this class of AGN. The Fermi satellite and the MAGIC telescopes had already discovered the high energy emission of this galactic system back in 2009. But what causes these very fast temporal variations in the radiation?\nThe researchers assume that the black hole in the nucleus of IC 310 rotates rapidly and is surrounded by a magnetic field. “In the polar regions, there seem to be strong electric fields which accelerate the particles to relativistic speeds, thus generating high energies and ultimately – in interaction with particles of low energies – gamma rays with very fast time variations,” says Masahiro Teshima, a Director at the Max Planck Institute for Physics.\n“You can imagine this as lightning in a thunderstorm,” says Karl Mannheim from the University of Würzburg. Every few minutes the lightning discharges the energy it has accumulated in a region the size of our solar system. In the process, particles with relativistic speeds – i.e. close to the speed of light – are ejected into the outer regions of the galaxies.\n“When we observe black holes at high energies, we are looking into the galactic nucleus to very great depths. We are trying to look directly into the machinery at the centre, as it were,” says Razmik Mirzoyan.\nThe jet at the heart of the galaxy was observed with extremely high-detail resolution by the European VLBI Network with the help of the 100-metre radio telescope in Effelsberg. “The complementary effect between MAGIC and the VLBI Network provides us with a unique view into this huge object,” says Eduardo Ros from the Max Planck Institute for Radio Astronomy in Bonn.\nAccording to Mirzoyan, the observations of IC 310 provided initial indications as to how jets form in the vicinity of black holes. And MAGIC’s high sensitivity has enabled it to achieve this result. It was the most suitable instrument of all the imaging Cherenkov telescopes for this kind of research.","One of the most photogenic images in our universe, the Sombrero Galaxy is roughly 28 million light years away from Earth. It’s not hard to see where it gets its name from and scientists suspect that it probably has a super massive black hole at the center. The image you see is actually a mash up of several other images taken by both the Spitzer and Hubble Space Telescopes.\nResembling the body of an ant, what you are looking at is actually a star in process of dying. Those jets of gas being shot out into space are actually moving at about 620 miles per second. Don’t worry though, the nebula is located approximately 8,000 light years from Earth (from left to right the image above spans a length of about 1 light year)\nHelix Nebula (Eye of God)\nTaken in 2003 by the Hubble Space Telescope, this image of a dying star 700 light years from the Earth saw heavy circulation on the internet and became one of the most famous deep space pictures yet.\nWhat has become a classic picture of deep space, the Whirlpool Galaxy is actually visible from Earth if you can get your hands on some quality binoculars. It is believed to spiral the way it does due to the gravitational disturbance caused by the dwarf galaxy in the upper right.\nAbout a thousand years ago a star exploded in the night sky and was recorded by astronomers all around the world. Records show that Chinese, Arab, and Native American stargazers all observed the event to be visible in broad daylight for almost 1 month and at night for over 2 years. It was until recently, however, that we could get a closeup.\nLists Going Viral Right Now\nThe Enigmatic Cloud\nIt’s technical name being nebula IRAS 05437+2502, little is known about this obscure nebula near the central plane of our galaxy. First discovered by the Infrared Astronomical Satellite (IRAS) in 1983, it was recently spotted again by the Hubble.\nThis image of Centaurus A using the most advanced instrument on the Hubble Space Telescope, Wide Field Camera 3, reveals previously concealed portions of this spectacular galaxy.\nThe Edge-On Galaxy\nTechnically named ESO 510-G13, this image gives you an idea of what a spiral galaxy like ours looks like when captured edge-on.\nThe Flocculent Spiral\nUnlike our galaxy, NGC 2841 has much shorter spiral arms rather than long extensive limbs and is therefore known as a flocculent galaxy.\nThe Red Spider Nebula\nThis extremely warm nebula is home to one of the hottest known stars in the universe and it generates stellar winds with waves over 100 billion km high.\nConsisting mostly of ionized hydrogen gas, this nebula located in the Large Magellanic Cloud is a place of extremes and due to numerous supernova remnants it is one the brightest nebulae in our intergalactic neighborhood. Another fun fact – it’s home to the heaviest star on record.\nKnown as Stephan’s Quintet, this groups of galaxies appears to be constantly running into each other making for some intense intergalactic stargazing.\nAs the closest star forming region to Earth, the Orion Nebula is 24 light years in diameter and 1,500 light years from Earth. It is actually visible to the naked eye if you look in the direction of the Orion constellation.\nNo list of space pictures would be complete without on of our nearest galactic neighbors, the Andromeda Galaxy. One of the only things in the night sky outside of the Milky Way that is visible with the naked eye, it spans approximately 200,000 light years.\nAlso known as the Bug Nebula, this interstellar cloud of dust and has can be found about 4,000 light years from the Earth. The dying star in the middle of this fiery explosion is actually 200,000 degrees Kelvin.\nJust like anything else in the universe, galaxies can collide into one another. The two captured above will eventually come to form one elliptical galaxy but the process will probably take over a billion years.\nWhile some galaxies just go ahead and crash into each other head on, others will try to steal one another’s stars first. What you see above is actually known as tidal stripping where the larger galaxy sucks the stars out of the smaller galaxy before the two merge into one after billions of years.\nThe Double Cluster\nFound in the Large Magellanic cloud, one of the closest galaxies to the Milky Way, this crowded cluster of young stars gives us a look into the intensity of the star formation process.\nCat’s Eye Nebula\nAs the first planetary nebula to be discovered in the night sky, the Cat’s Eye is is also one of the most complex. These type of nebulae occur when dying stars eject their gaseous outer layers into space.\nIn 2002 the star V838 Monocerotis suddenly became 600,000 times brighter than the Sun. In fact, for a few weeks it was the brightest object in our galaxy. Moreover, due to something known as a light echo illuminating its surrounding rings of gas, the star appeared to be expanding rapidly as well. It has since died down, however, and astronomers are still no sure what caused the outburst.\nStellar Cluster R136\nThis colorful image shows an extremely volatile region of the Large Magellanic Cloud near our galaxy, the Milky Way. The red gasses you see are hydrogen, the green are oxygen, and the blue “diamonds” are actually some of the largest known stars in the universe with several being hundreds of times bigger than the Sun.\nHeart and Soul Nebula\nThis infrared picture captured by Wide-Field Infrared Survey Explorer shows the Heart Nebula on the right and the Soul Nebula on the left. Located about 6,000 light years from Earth the image above spans a distance of about 580 light years.\nHome to Eta Carinae, a star four million time brighter than the Sun, the Carina Nebula is a cloud of gas 300 light years in length that can found around 7.500 light years from the Earth.\nThe Pillars of Creation\nThere are few things in this universe that can make you feel smaller than this now famous image of what has come to be known as the “Pillars of Creation”. An aptly named cloud of dust and gas, it is responsible for the birth of millions of new stars and can be found 6,500 light years away from Earth. Each one of those gaseous arms you are looking at are in fact trillions of miles long."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:0f91e61d-ba48-41d3-a4ba-da1834412b63>","<urn:uuid:638d373a-2542-42a4-bfe1-bcff4609dbac>"],"error":null}
{"question":"How does Anoka County's historical water management connect to modern watershed planning, particularly in terms of its historical logging industry and current watershed protection strategies?","answer":"Anoka's history as a logging town, situated on two rivers that powered sawmills, has evolved into comprehensive water management approaches. While historically the rivers were used primarily for logging transportation and mill operations, today the area employs integrated water resources management at the watershed scale. Modern strategies include protecting riparian areas to reduce soil erosion, increase stream channel stability, and decrease phosphorus and nitrogen loading. The transition from industrial use to environmental protection is reflected in programs like One Watershed, One Plan, which supports partnerships of local governments in developing targeted implementation plans for watershed management.","context":["Our History Programs bring Anoka County history to you!\nHave a historian present a one-hour program to your group at your location. Choose one of our current programs, or we can develop a special program for your group.\nMileage is charged for programs presented outside of Anoka County.\nAdditional fees apply for development of special programs.\nLiving History Programs\nCivil War Christmas\nA fashionable women of the 1860s would have worn her very best to celebrate the season and that is just what this living history presentation brings! A spirited talk from a woman presented in full 1860s fashion shares stories of how a tree might have been decorated in this area in the 1860s, what Christmas treats could have been prepared, the kinds of gifts exchanged and more. Guests hear the music of the season and get a taste of what a Christmas in Anoka County celebrated during the hard years of the American Civil War may have been. Suitable for all ages.\nThis program is presented by a fashionably dressed woman of 1860, her dress reflecting the era of early Minnesota statehood and one of the most recognizable fashion periods of American history. The audience will look at the history of clothing worn in the mid to late 19th century, following women’s fashions from the sewing machine to ready-made clothing that came from the mail order catalog. The influences on fashion from political, social, and etiquette issues provide a glimpse at how our foremothers dressed and why. The program also features period clothing and allows guests to see what a slat bonnet really was or how stiff corset boning felt. Though focused on women’s clothing—it changed the most—the influence of men on style is included and makes this program fun for everyone.\nSoldiers’ Aid Society\nThe patriotic fervor that swept across the North when the first shots were aimed at Fort Sumter in 1861 did not just affect the men who enlisted in droves—women were caught up in the excitement of cause as well. Restricted by convention and society, the ladies of the North soon created a way they could support the war effort in a new and highly organized system, the U.S. Sanitary Commission. The root of this vast network of women was the Soldier’s Aid Societies and nearly every community had one. This living history presentation shares stories of their work and the results of what these determined women accomplished.\nTastes and Sounds of the Civil War\nThis program provides a hand-on approach to a discussion of the Civil War. Artifacts, music, and a special taste of a soldier’s typical fare are shared with guests by an authentically costumed presenter. Stories of Minnesota troops and their misconceptions, adventures, and disappointments during the turbulent years of war help people of all ages gain a basic understanding of the Civil War from a Minnesota point of view.\nThese programs are talks with accompanying PowerPoint slides to show images. Programs are 60 minutes long unless otherwise specified.\nElections in Anoka County\nWe have just come through the most electric and polarizing Presidential election in modern memory…or have we? Come investigate the controversial elections of Anoka County’s past including the tumultuous years during WWII, the Progressive Era, and even as far back as the 1800s. Who got caught in the middle and who came out the winner? You’ll find out during this presentation.\nFarms to Flamingos: Suburbanization in Anoka County\nAnoka County was largely a rural county with a population of just over 35,000 people in 1950. By 1960, there were more than 85,000 people calling it home—a population increase of 181% in a single decade! Almost overnight, the face of Anoka County changed from one of farms to sprawling Suburbia. How did this happen? Two men, Vern Donnay and Orrin Thompson, were largely responsible for the boom of the 1950s and 60s, but they were not the first ones to see living in the clean air and healthy environment outside of the “big city” as potential for development. Learn about the first local ideas of suburbanization in Anoka County and how those ideas were replayed decades later to build the communities we know and live in today. This program looks at the challenges (roads, schools, safety) and pleasures (country clubs, big yards) of building and living in Anoka County’s suburbs.\nSo, What Are You Gonna Do About It?\nThe Era of the Great Depression (1930-1940) brought many hardships to the people of the United States and across the world. The decade saw a stock market crash bring people to complete ruin. Millions of people were unemployed with almost no social safety nets to provide even the basic necessities of life. The decade saw the coldest winters on record as well as some of the hottest and driest summers ever, leading to crop failures and the Dust Bowl. This program looks at how the Great Depression affected people in everyday middle America. Stories and reports of what was done to make do and get by and how the Federal government responded to the crisis puts very human faces on the struggle to survive. With a focus on Anoka County, Minnesota, these stories bring home the people and the programs that got Americans through the tough years of the Great Depression and points out the echoes of those programs that still remain today.\nThe Anoka State Hospital\nThe Anoka State Hospital opened its doors to patients over 100 years ago and became an integral part of the community. Learn why this local institution was built in Anoka, get details about the buildings and grounds, and hear behind the scenes stories of what life was like there for patients and staff. This program provides an overview of the sometimes-fraught history of this institution, as well as our shifting views of mental illness and what we can do to help those who suffer from it.\nThe Decentralization of Mental Health Care\nThe State Hospital of Anoka has a long and storied past, but what happened after it closed? Where did the patients go and how did this change affect their families? Using Oral History interviews conducted by the historical society, this program will discuss the decline of the institutional system following the 1950s and the rise of decentralized mental health care that followed. (30 minutes)\nLove and Valentine’s Day in Anoka\nValentine’s Day. It is the day of love, couples and all things romance. The skeptics may say that it is only a plot from the greeting card and candy companies….but Anoka County residents were celebrating and engaging with Valentine’s Day long before Hallmark Company even began. Find out how Valentine’s Day has been celebrated here from the county’s founding up until the present. Valentines, newspaper advertisements, parties, and more tell the story.\nGreetings From the Halloween Capital of the World\nIn September 2016, the Halloween Capital of the World served as the official First Day of Issue site to unveil four Jack-O’-Lantern Forever stamps, bringing together the worlds of Anoka Halloween and stamp collecting. In this program, you can learn more about the philatelic world, investigate the heritage of the Postal System in Anoka County, discover why the Old Post Office is listed on the National Register, and of course, learn about the history of Halloween events in this river town.\nA Logger’s Life\nAnoka was a logging town. Situated on two rivers, it was a perfect place to power the saw mills needed to processes the logs floated down the waterways from logging camps. With this program, you will learn about the different aspects of the logging industry in Minnesota and its connection to Anoka. We will explore how a logging camp worked from first-hand accounts, learn what River Pigs were, and look at the sawmills and manufacturers on the banks of the Rum River.\nDeath touches everyone’s life at some point. This program looks at how death has been treated over the last 200 years. From burial practices and mourning rituals, the way we remember those who have passed, how we show respect for the dead, and ways we commemorate their lives are discussed.\nQuilt Facts, Folklore and FABRICation\nWhat do we really know about the history and traditions of quilt making? So many stories have been handed down, but they don’t always hold up under a close look at history. Find out the real stories behind what we all thought we knew about the history of quilting. Actual quilt squares, small quilts, books and period patterns are available for visitors to see and compliment the many color photos of antique quilts in the program. The “warmth” and variety of this program is sure to delight even non-quilters.\nQuilts and Their Stories\nQuilts can do much more than warm your toes on a cold winter night! The can tell stories, paint pictures, and bring back memories. This program presents slides of some very special quilts, along with their stories of life in Anoka County.\nRum Runners and Temperance Tantrums\nThe 18th Amendment made it illegal to manufacture, sell, or transport intoxicating liquors and it took effect on January 17, 1920. That, however, did not eliminate the problem! The clash between those opposed to intoxicating beverages and those who wanted to imbibe was sometimes violent, sometimes comical or even just amazing. Why was such a law thought necessary, how was it enforced (or not) and the people it affected make for great stories. Bootleggers and rum runners thrived in Anoka County and Minnesota even before prohibition went nationwide in 1920. The Minnesota map was a patchwork of dry and wet counties, dating back to before statehood. This program looks at the battle over booze, illegal and otherwise, that divided communities.\nWomen of the Civil War\nThe War Between the States may have been fought by great armies of men, but women filled many vital roles in society and even in the military. Women such as Clara Barton and Dorothea Dix immediately come to mind, but what about Minnesota’s women? This program highlights the incredible stories of Minnesota women who protected their homes, supported their soldiers, served the army in many different ways, and kept the home fires burning throughout the four years of the Civil War. Some were dedicated enough to continue working for veterans even after the war. Their stories, long overlooked and forgotten, bring a new perspective to the history of America’s most bloody war.\nHistory of [Your City]!\nWant to know more about the cities in Anoka County? We do programs about the communities that make up our fascinating county! See the list below for programs that are currently available. If you don’t see your city, let us know and we can work on creating a new program!\n-History of Lexington & Circle Pines\n-History of Hilltop & Columbia Heights\n-History of Blaine\nDon’t see a program you like? We might be able to create one for you! If you have interest in a special history topic, contact us to develop a special program for your group.\nAdditional fees apply to special program development and are dependent on staff or volunteer availability. Contact us for details.","Water planning within Minnesota and nationwide has been evolving to emphasize integrated water resources management at a watershed scale to solve soil and water resource issues. Issues related to water planning are wide-ranging and vary between watersheds, including urban stormwater management, agricultural water quality, subsurface sewage treatment, land use practices, plant community management, construction practices and climate change impacts.\nMany types of water planning have been conducted in Minnesota at different levels of government, including watershed district management plans and watershed management organization plans, counties, and other local and regional partners.\nPlanning Methods and Programs\nOne Watershed, One Plan\nOne Watershed, One Plan (1W1P) is a BWSR program that supports partnerships of local governments in developing prioritized, targeted, and measurable implementation plans. Key principles are planning at the major watershed scale and aligning local plans with state strategies. Plans created through the 1W1P program are called comprehensive watershed management plans and are described in §103B.801.\nCounty Comprehensive Local Water Management Planning:\nCounties with their planning and land-use authorities, are uniquely positioned to link many land-use decisions with local goals for surface and groundwater protection and management. Through the Comprehensive Local Water Management Act, counties are encouraged to make this link through the development and implementation of comprehensive local water management plans (county water plans). BWSR's role in local planning is to ensure that county water plans are prepared and coordinated with existing local, and state efforts; and that plans are implemented effectively. BWSR fulfills this role through Board review and approval of the plans while BWSR staff provide overall program guidance, process related grants, and provide plan review and comments. All parts of Minnesota have state-approved and locally-adopted county water plans in place. However, water management in Minnesota has been evolving towards watershed-based, rather than jurisdictionally-based water plans (i.e., One Watershed, One Plan (pdf)).\nThe Nonpoint Priority Funding Plan (NPFP):\nIn 2013, the Minnesota Legislature passed a law requiring the Board of Water and Soil Resources (BWSR) to prepare and post on its website a Nonpoint Priority Funding Plan (pdf) to prioritize potential nonpoint restoration and protection actions based on available Watershed Restoration and Protection Strategies (WRAPS), Total Maximum Daily Load (TMDL) plans and local water plans. The Nonpoint Priority Funding Plan is a criteria-based, systematic process to prioritize Clean Water Fund (CWF) nonpoint implementation investments.\nMinimal Impact Design Standards (MIDS):\nState agencies are focusing on using low impact design, and volume control best management practices (BMPs) to address issues related to large storm events. Impervious surfaces are increasing faster than population growth. This increase in impervious surface coupled with larger storm events will have a significant impact on receiving waters. Minimal Impact Design Standards (MIDS) are being used to increase infiltration and reduce runoff (including green infrastructure like rain gardens, urban forestry/trees, pervious pavement, swales, etc.). These practices are now focusing on volume control in addition to pollutant and rate control. Volume control, and working to mimic natural hydrology helps to result in less dramatic runoff events, which reduces stream erosion and scouring.\nMinnesota Hydrogeology Atlas:\nThe Minnesota Department of Natural Resources (MN DNR), Ecological and Water Resources Division’s Minnesota Hydrogeology Atlas (MHA) is a series of statewide, county, and regional maps and reports that can be used by government agencies in planning efforts to protect and preserve groundwater, to provide information for water appropriations permitting, for source water protection and well sealing programs, and for emergency response to contaminant releases. The MHA expands, compiles, and updates data developed by the County Geologic Atlas Program (CGA) providing more accessibility to information such as regional water pollution sensitivity to bedrock surface and near-surface materials, depth to water table, and water table elevation. The MHA includes a user’s guide, and lists training contacts and funding sources. Numerous links present users with a variety of state and national resources to assist in planning efforts including the DNR’s Watershed Health Assessment Framework, the USDA Web Soil Survey, and the Minnesota Geospatial Commons.\nMinnesota State Hazard Mitigation Plan:\nThe State Hazard Mitigation Plan, updated in 2019 by the Minnesota Division of Public Safety Department of Homeland Security and Emergency Management (HSEM), provides unified guidance to reduce and/or prevent injury and ensure coordination of recovery-related hazard mitigation efforts following major hazard events. The State’s 20 major hazards are identified, including flooding, drought, and ground and surface water supply contamination as they relate to geographic and demographic characteristics, development trends, and climate change. Local governments can use the county by county assessment of mitigation goals, strategies, actions, and initiatives to develop and update their plans. The plan provides information and resources on ranking and criteria for hazard identification, and determining steps to declare an emergency. Resources include a listing of agencies and organizations that may assist with mitigation efforts, and an inventory and brief descriptions of funding programs including the Hazard Mitigation Grant Program. Hazard mitigation success stories are also listed.\n1) Sustainable Regional and Urban Planning. Urban and regional planning that considers protection of natural areas, development of green infrastructure, and minimization of impervious areas during development and re-development can help to treat both water quality and quantity though a variety of practices.\n2) Protecting and Restoring Natural Areas. Protecting and restoring diverse natural habitats provides multiple benefits including water quality protection for groundwater and surface water, stable plant composition to resist invasive species, protect pollinator populations, preserved and improved wildlife habitat, and resiliency to weather extremes. For water planning it is recommended to identify high priority natural habitats including wildlife and water quality complexes and corridors, and promote a combination of agricultural BMPs, buffer programs, conservation plantings, wetland projects and riparian activities that will protect, restore and link water quality and habitat corridors. Minnesota’s Wildlife Action Plan and Prairie Conservation Plan are resources that can be used to aid planning efforts.\n3) Riparian Management. Protecting and restoring riparian areas, including adjacent floodplains brings multiple benefits by reducing soil erosion, increasing stream channel stability, decreasing phosphorus and nitrogen loading, flood attenuation, improving wildlife habitat and wetland functions. Water plans should identify high priority areas for riparian buffer easements, riparian erosion and sediment reduction, wetland restoration and other water storage and nutrient treatment opportunities, and target implementation efforts in those areas.\n4) Treating Water Close to Where it Falls. Higher amounts of rainfall from storm events in recent years increases the need to capture precipitation as close to where it falls as possible; prior to flowing into streams, lakes, and rivers and contributing to erosion and flooding. Stormwater runoff also transports high concentrations of pollutants into water bodies, causing impairment. To reduce these impacts, best management practice methods like cover crops, detention basins, vegetated swales, catch basins, infiltration basins, infiltration trenches, bioretention cells, grass swales, buffer strips, green roofs, etc. can improve water quality.\n5) Using Water Treatment Trains in Agricultural Landscapes. In agricultural landscapes it is beneficial to implement combinations of best management practices that promote soil health and the ability of soils to capture and store rainfall, and store carbon. Examples of key practices for agricultural areas include perennial crops, conservation tillage, conservation drainage, cover crops, buffer strips, and wetland restoration to manage water resources. These practices reduce runoff, recharge groundwater, maintain agricultural productivity, improve water quality, and 9educe flooding.\n6) Using Stormwater Treatment Trains in Urban Landscapes. In urban areas it is beneficial to implement combinations of practices to slow water volume and improve water quality. Practices in urban areas that are commonly combined include raingardens, infiltration trenches, treatment swales and detention basins.\n7) Strategic Site Selection. Water quality practices should be located where they will have the greatest landscape benefits to maximize the value of conservation funding. The Nonpoint Priority Funding Plan (pdf) (NPFP) helps to prioritize potential nonpoint restoration and protection actions based on available Watershed Restoration and Protection Strategies (WRAPS), Total Maximum Daily Load (TMDL) plans and local water plans.\n8) Nutrient Management. Detailed Nutrient Management Plans play a key role as an operational practice along with conservation practices in protecting water bodies from nutrients in agricultural areas.\n9) Wetland Protection and Restoration. Wetland protection and restoration provides benefits for water quality, peak flow reduction, habitat and wildlife. Water plans should support the continued implementation of the Wetland Conservation Act and look for opportunities to improve coordination across jurisdictional boundaries. The plan should also identify high priority areas for wetland restoration and strategically target restoration projects to those areas. The Restorable Wetlands Prioritization Tool is one resource that can be used to help identify areas for wetland restoration.\n10) Protecting Groundwater. It is important to identify priority areas for drinking water/groundwater planning, and management such as source water protection areas, groundwater management areas, and areas of groundwater/surface water interaction; or concerns about groundwater overuse or contamination that need to be addressed in plans. Ineffective septic systems and subsurface treatment practices are also important considerations.\n11) Sustainable Forest Management. Protecting the health of forests by sustainable forestry practices and control of invasive species can maintain the health of forest soils and plant communities that promote the infiltration and filtering of water before it reaches streams, rivers, lakes and wetlands. Long-range forest management plans are an important tool for maintaining healthy forests.\n12) Minimizing Landscape Stressors. Investigate opportunities to improve environmental conditions throughout watersheds to decrease environmental stressors such as flooding, water level fluctuations, sedimentation, environmental pollutants, decreasing water tables, or invasive species that can significantly detract from key ecological functions into the future.\n13) Increasing Perennial Vegetation. Areas of plant diversity supports wildlife species and increases resiliency by helping plant communities function as intact ecological systems during climate variation. Planting native species also prevents the establishment of invasive species. Diverse state seed mixes are available for a variety of project types and the Minnesota Wetland Restoration Guide summarizes restoration strategies for uplands and wetlands. The MPCA publication Plants for Stormwater Design summarizes environmental tolerances of native vegetation.\n14) Preserving and Restoring Soil Health. The use of cover crops and perennial vegetation is recommended to promote good soil structure, organic content and microorganism populations that promote healthy soils and sustain productive ecological and agricultural landscapes. Maintaining more vegetation more of the time increases evapotranspiration during the spring and fall seasons, reduces runoff and erosion and helps recycle nutrients. Root systems increase organic matter in the soil profile, which increases infiltration and water holding capacity for plant available water, and also reduces runoff, erosion and nutrient transport.\n15) Adapting to Climate Change. A major climate trend in Minnesota has been an increase in intense rainfall events that stress aquatic systems, cause erosion, and transport sediment and nutrients. Partners that are working on water plans should consider the potential for more extreme weather events and the implication for water and land resources. BWSR’s Climate Change Trends and Action Plan (pdf) provides details about climate change adaptation for conservation and protection of natural resources. All of the strategies summarized in this Toolbox play a role in climate adaptation efforts. In addition to the strategies already summarized related to water planning it is also important that NOAA Atlas 14 rainfall frequency data and good BMP/landscape planning and design practices are used to address larger storm events. It is also important to identify landscape and populations at risk from climate change trends.\n16) Public Engagement. Finding ways to engage landowners in projects within urban or rural communities can be an important way to promote conservation efforts. This can be accomplished though volunteer events, tours or promoting community gatherings where projects are featured. Having landowners speak about the benefits of projects on their property can be an effective method of convincing other landowners to sponsor projects.\n17) Practicing Adaptive Management. Adjust management practices based on monitoring efforts and experience with successes and failures to improve the long-term effectiveness of management practices and resiliency of plant communities. Practices such as prescribed burning, water level management and prescribed grazing and haying may replicate natural disturbances and promote diversity and resiliency. BWSR’s What’s Working webpages summarize strategies that have been successful for conservation professionals.\n18) Disaster Response. Flooding has caused significant damage to private lands and conservation practice infrastructure in Minnesota. Since 2000, BWSR has distributed $53 million in southeast, northeast and northwest Minnesota under the Disaster Recovery Assistance program to install, repair, or rehabilitate erosion and sediment control and water quality and watershed protection projects damaged by flooding. The Division of Homeland Security and Emergency Management (HSEM) also helps Minnesotans prevent, prepare for, respond to and recover from disasters among its other responsibilities. This includes efforts to reduce the risk to people and property from natural and human-caused hazards by developing and implementing long-term mitigation measures that will reduce or eliminate the severe effects of future disasters."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:468a0754-d409-43d1-94ae-3b42aa7aa3c4>","<urn:uuid:c9f9f51c-177b-4378-b726-79f2fb338b80>"],"error":null}
{"question":"¿How does the humidity affect the coffee processing in Indonesia compared to the processing at Las Lajas micromill en Costa Rica?","answer":"In Indonesia, the high humidity (70-90% year-round) and heavy rainfall (up to 2,000 mm annually) make traditional drying methods problematic, forcing farmers to use wet hulling to speed up the process and prevent bacteria-caused defects. In contrast, at Las Lajas micromill in Costa Rica, the conditions allow for slower, more controlled processing methods, where they can even measure the Brix content in coffee cherries to determine optimal picking time and perform honey processing with minimal water use.","context":["Wet Hulled Coffee\nWet Hulled Coffee\nALL of our Indonesian Coffees are Wet Hulled\nEver heard of wet hulling? If you have, did you wonder what on earth it actually means and what it has to do with coffee?\nWould you be interested to hear how it enables coffee production in a country with 70-90% humidity all year round; typhoons; and, in some places, typically 2,000 mm of rainfall a year? Or what about how it creates a high risk of defects—but a rich and strong body?\nIf you answered yes to any of those questions, this article is for you. We’re going to cover everything you need to know about the Indonesian method of wet hulling, its pros and cons, and how it affects both roasters and baristas.\nSo What Actually Is Wet Hulling?\nBefore any coffee can be roasted and consumed, it goes through a processing method that converts it from a fruit to a dry green bean. There are many different ways to do this, but the main three are natural; washed, or wet processed; and honey. Don’t confuse wet hulling with wet processing, though!\nWet hulling, or giling basah, is the traditional method used in Indonesia. And although both the name and the method are very similar to wet processing, the cup characteristics are incredibly different.\nHow Does Wet Hulling Work?\nInside every coffee cherry is a bean, a layer of parchment, and a layer of mucilage (along with a few other layers). Hulling itself refers to the removal of the parchment from the bean—but to fully understand wet hulling, we should look at the whole process.\nThe first step is to remove, or pulp, the beans, which are still coated with that parchment and mucilage, You then ferment them in concrete water tanks or plastic rice bags overnight. This helps break down the pectin in the mucilage, which makes for easier removal.\nWe remove the mucilage by washing the beans. This usually happens the following day, but it depends on the water supply at the farm. If there’s an insufficient amount of water, then the coffee may be sold to a larger processing mill in the area where they will continue the process.\nOnce the mucilage is removed, you then have what’s known as a wet parchment coffee, i.e. the coffee bean is covered in a wet parchment layer. It’s then sun dried in its parchment for 2-3 days—no more! The moisture content will have reached about 20-24%, leaving the bean at the required hardness for hulling.process.\nSEE ALSO: Everything You Need to Know About Honey Processing\nYet at 20-24%, the bean itself is soft to the touch and inflated with water, while the parchment is semi-dry. So to finish this wet hulled coffee we have to put the coffee through a huller that’s specially designed to handle semi-dried parchment. What’s the difference? Well, this kind of huller requires more power, since removing the parchment requires more friction.\nHulling this semi-dry parchment isn’t as clean a process as dry milling. The high moisture content means the parchment clings to the bean, making it harder to remove than with other processing methods. Normally pieces of parchment will still be left on the beans, whose pale-green colour shows that they’re still wet.\nAs of such, you need to be really careful at this point. The coffee is so wet and soft that if you were to poke it firmly with your finger it would be crushed. There’s a high risk that the machinery that hulls the coffee will also split it at the ends, due to the bean’s low density. This is called kuku kambing or goat’s nail, reflecting its shape.\nThe hulled beans are then dried to 12-13%. They are left in the sun during the day, but stored in bags inside over night to continue fermentation. The sun drying will take a few days, but afterwards, the beans are ready for export. You can recognise ready-for-export wet hulled coffee by the bean’s dark green and patchy colour. Some even call it blue.\nSo Why Do Indonesian Farmers Wet Hull Their Coffee?\nThere are two reasons why wet hulling dominates in Indonesia: history and geography.\nCoffee was first introduced to Indonesia by Dutch colonists in 1699, whose purpose was financial gain. From their profit-orientated perspective, every day the coffee remained on the farm was money lost. Since wet hulled coffee take several less days to dry, they were able to see returns quicker—and for less labour.\nYet even had the Dutch not taken this position, they would probably have resorted to wet hulling. Drying coffee beans in the intense humidity of the country can be a problem. In a warm climate, it takes at least 2-3 weeks to dry an average coffee. In most parts of Indonesia, where rainfall and humidity are high, it would take far longer. Bacteria would cause coffee defects long before the beans would be at a low enough moisture content to dry. Remember, coffee is usually dried in at least its parchment—which protects it from damage and promotes consistency while the bean is drying. Yet wet hulling removes the parchment so the sun and heat can directly penetrate the bean, therefore allowing it to dry 2-3 times","Black Diamond – Las Lajas – Costa Rica – Gardelli – Espresso and Filter\nThis honey process Costa Rican coffee from 2018 World Roasting Champions, Gardelli, has great complexity, with notes of black cherry, cinnamon, red grape and cocoa. Suitable for use as espresso and filter.\nDona Francisca and Don Oscar Chacon of Las Lajas micromill are third-generation coffee producers located in the slopes of the Poas volcano in Alajuela, Central Valley. They inherited their farms from their grandparents, and are known for being among the first to process high-quality Honeys and Naturals in Central America, and for participating in the Cup of Excellence auction in 2009.\nLas Lajas is an organic micromill located in Sabanilla de Alajuela in the Central Valley region of Costa Rica. Organic coffee in Costa Rica is almost nonexistent, and with this caliber of cup makes it one of a kind; they believe in the preservation of the environment hence their organic practices. Las Lajas processes coffee from their family farms; their estate is equipped with a state of the art micro mill where they process coffee on different, innovative ways. As an alternative to the fully washed process, the Las Lajas micro mill uses the Penagos aquapulper which forms the basis for their various honey coffees. These lots are fully traceable and separated by day. Water use is minimal, since the coffee is not washed. During the harvest, Francisca will measure the Brix content in the coffee cherry to determine the optimal time to pick the coffee. 21–22% Brix content has been the maximum they’ve seen.\nCaturra coffee varietal was developed by the Alcides Carvalho Coffee Center of the IAC, Instituto Agronomico of the Sao Paulo State in Brazil.\nIn 1937, IAC received seed samples of genetic materials originated on the border of the states of Minas Gerais and Espírito Santo. It was from Red Caturra and yellow Caturra cultivars. These two cultivars originated by natural mutation of Bourbon Red, originally a tall coffee shrub, found in the Serra do Caparaó , which is now a mountainous National Park north east of the city of Rio de Janeiro.\nThese are the main agronomic characteristics of the Red and Yellow Caturra varietals:\n1. It is the of small size, of reduced length of internodes, leaves and side branches, providing compact appearance to the coffee shrub.\n2. This is the first naturally occurred coffee mutation found, with small size and high yield capacity\n3. They have excellent quality in the cup because they have virtually 100% of the Bourbon coffee in their genetic makeup.\n4. the conditions in which they were planted in Brazil to cultivate Caturra showed low hardiness and consequent lack of vigor after a few harvests, which led to the premature depletion in yield.\nIn the honey process the coffee cherry peel is removed right after picking from the coffee tree, but some amount of the fleshy inside, the “mucilage”, remains while the beans are dried over raised beds. The white and yellow honeys have less mucilage left after being mechanically washed. Gold, red, and black honey coffees, on the other hand, have much more mucilage remaining and usually are not washed at all.\nBlack honey coffees usually take longer to dry because they are dried under shade.\nThis coffee is a ‘black’ honey, as opposed to the Chacon’s white, yellow and red honey coffees.\nIt is dried with an intentional slowness in mind. In fact, the first day on the raised beds it is not moved at all. It rests with all its mucilage intact simply concentrating in flavour as it sits. From then on after it turned over or raked once a day, but that is it. All in all, this coffee could take up to three weeks to dry, like that of a natural."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:85c8ebfb-1ac0-40ec-89d1-816268f25ecc>","<urn:uuid:3e42571e-0955-47dc-8948-fdb71f796716>"],"error":null}
{"question":"¿Qué similitudes y diferencias hay entre Best-Worst Scaling (BWS) y Conjoint Analysis para evaluar productos nuevos? Es crucial entenderlo para mi investigación sobre desarrollo de productos. 📊","answer":"Best-Worst Scaling (BWS) and Conjoint Analysis have different approaches to product evaluation. BWS, which includes MaxDiff as a subset, involves choosing the best and worst items from a set, either through evaluating all possible pairs or through sequential selection. Conjoint Analysis, on the other hand, is specifically designed for product development and presents different combinations of product features (attributes) that consumers must consider together. While BWS focuses on independent comparisons, Conjoint Analysis creates realistic purchase scenarios where consumers must make trade-offs between different feature combinations, making it particularly useful for predicting consumer behavior before product launch and determining which features customers value most.","context":["The MaxDiff is a long-established academic mathematical theory with very specific assumptions about how people make choices: it assumes that respondents evaluate all possible pairs of items within the displayed set and choose the pair that reflects the maximum difference in preference or importance. it may be thought of as a variation of the method of Paired Comparisons. Consider a set in which a respondent evaluates four items: A, B, C and D. If the respondent says that A is best and D is worst, these two responses inform us on five of six possible implied paired comparisons:\n- A > B, A > C, A > D, B > D, C > D\nThe only paired comparison that cannot be inferred is B vs. C. In a choice among five items, MaxDiff questioning informs on seven of ten implied paired comparisons.\nRelationship to Best-Worst Scaling\nMaxdiff and Best-Worst Scaling (BWS) have erroneously been considered synonyms. Respondents can produce best-worst data in any of a number of ways, with a maxdiff process being but one. Instead of evaluating all possible pairs (the maxdiff model), they might choose the best from n items, the worst from the remaining n-1, or vice versa (sequential models). Or indeed they may use another method entirely. Thus it should be clear that maxdiff is a subset of BWS; maxdiff is BWS, but BWS is not necessarily maxdiff. Indeed, maxdiff might not be considered an attractive model on psychological and intuitive grounds: as the number of items increases, the number of possible pairs increases in a multiplicative fashion: n items produces n(n-1) pairs (where best-worst order matters). Assuming respondents do evaluate all possible pairs is a strong assumption. Early work did use the term maxdiff to refer to BWS, but with Marley's return to the field, correct academic terminology has been disseminated throughout Europe and Asia-Pacific.\nCommercial software named maxdiff\nIn North America the term maxdiff continues to be used for studies that are in fact BWS studies. This probably reflects the fact that Sawtooth Software in the United States implements a procedure it has named maxdiff. Indeed, it is unclear whether this procedure implements maxdiff procedures in estimating parameters of their models, or whether the simpler, sequential BWS model is used.\nConduct and analysis of a maxdiff survey\nA maxdiff survey may be designed and analysed in two ways:\n- Through manual design and researcher-led analysis using standard statistical software procedures (see BWS) or\n- By utilising a commercial survey company which supports this type of discrete choice model.\n- Marley, Anthony AJ; Louviere, Jordan J. (2005-01-01). \"Some probabilistic models of best, worst, and best–worst choices\". Journal of Mathematical Psychology. 49 (6): 464–480. doi:10.1016/j.jmp.2005.05.003.\n- Louviere, Jordan J; Flynn, Terry N; Marley, A A J (September 2015). Best-Worst Scaling: Theory, Methods and Applications. Cambridge University Press. ISBN 9781107043152. Retrieved 2 October 2015.\n- Marley, A. A. J.; Louviere, J. J. (2005-12-01). \"Some probabilistic models of best, worst, and best–worst choices\". Journal of Mathematical Psychology. Special Issue Honoring Jean-Claude Falmagne: Part 1. 49 (6): 464–480. doi:10.1016/j.jmp.2005.05.003.\n- Flynn, Terry N.; Louviere, Jordan J.; Peters, Tim J.; Coast, Joanna (2008-11-18). \"Estimating preferences for a dermatology consultation using Best-Worst Scaling: Comparison of various methods of analysis\". BMC Medical Research Methodology. 8 (1): 76. doi:10.1186/1471-2288-8-76. ISSN 1471-2288. PMC . PMID 19017376.","Conjoint Analysis: a comprehensive practical guide\nDeveloping a new product and establishing it on the market provides challenges for many companies. During the developing process the central questions are: Which features do my customers expect and which of those features are considered the most important?\nRoughly 95% of all new product launches fail. Mostly because the products do not meet the customers’ requirements and expectations nor fulfil their needs. Hence, it is important to do some research to answer the questions above.\nHowever, these questions can already be answered during the development phase by means of a conjoint analysis. Thanks to this, it is possible to predict the behaviour of potential consumers in advance by presenting realistic purchase scenarios to identify gaps in the existing product competition.\nThe word conjoint is a portmanteau of the terms \"considered\" and \"jointly\", from which the conjoint analysis also gets its definition: various product features (attributes) are considered together and then weighed against other variants.\n2. What is the conjoint analysis?\nConjoint analysis has its origins in psychology and was developed by Robert Luce and John Tukey in 1964. Since then, it has been used primarily in market research and product development to determine at the development stage which attributes consumers want and which they perceive as particularly important.\nAttributes include, for example, certain functions, designs or functional features such as weight, size and price.\nHowever, since consumers tend to want as many attributes as possible for as little cost as necessary, conjoint analysis takes a different approach than, for example, the MaxDiff Method.\nThis is because the distinctive feature of the conjoint method is the combination of different attributes instead of independent comparison.\nBoth for high-priced products, such as automobiles; hardware, such as laptops or smartphones; and luxury goods, as well as for everyday products or during the conception phase, conducting a conjoint analysis is a good idea and ensures clear conditions.\nThe concept of the analysis is basically quite simple. In the context of conjoint analysis, consumers are shown different products, each of which differs in terms of the combination of features. In this way, a realistic experience can be created that comes very close to an everyday purchase decision.\nIn this sample conjoint analysis, the aim is to determine which types of chocolate consumers prefer and what price they are willing to pay for each type.\nThe respective attributes are leveled, i.e. they are displayed in a certain form. For the chocolate example the filling attribute is divided into the levels vanilla cream, strawberries & cream and, kiwi ganache.\nIn this way, a ranking can be created that shows which attributes are most important and which characteristics are most attractive.\nThis evaluation can then be used to decide which combination is both most appealing to consumers and most profitable for you and your company.\n3. What is the difference between the conjoint method and Discrete Choice?\nWhile there are some similarities between the conjoint analysis and the Discrete Choice Model (short: DCM) there also are some distinctive differences. Both models are preference structured models that are designed to see what's underlying a consumption choice. The big difference: While respondents see each of the product profiles with their respective attributes in smaller groups, in a DCM they will see all of the products simultaneously.\nTherefore DCM is a bit more realistic when it comes to predict buyer behaviour than the conjoint analysis. On the other hand the DCM can be overwhelming for the respondents as they see plenty of options. With a conjoint analysis it is also possible to gain more information about the attributes' relativity and importance to each other and their contribution to he final buying decision.\nAnother advantage: the conjoint analysis is a great way to predict behaviour before the product is launched. This is rather unlikely when using a DCM.\n4. The Choice-Based conjoint method\nChoice-based conjoint analysis (CBC for short) is the most frequently used form of conjoint analysis. And not without reason. This is because this type of conjoint analysis deliberately aims to get consumers to decide between variants and thus to accept trade-offs.\nThe choice-based method therefore offers a detailed analysis that is nevertheless based on realistic scenarios that are close to the market. After all, everyone makes a large number of decisions every day and weighs up different attributes against each other.\nIn the Choice-Based conjoint analysis, all previously defined attributes are combined evenly so that a statistically valid ranking can be created at the end of the analysis.\nIn addition to CBC analysis, which is performed at Appinio, there are other types of conjoint analysis. These include the Adaptive Choice conjoint and the Menu-Based conjoint analysis. However, they cannot be used as flexibly as the Choice-Based conjoint method.\n5. Three Examples of use for conjoint analysis\nSince individual attributes can be defined for each new execution and combined with each other as desired in a conjoint analysis, this market research method is suitable for a wide range of use cases.\nRight at the beginning of the product development stage, it is a good idea to carry out a conjoint analysis. In this way, concepts that are not accepted by consumers can be directly disregarded. Instead, potentials can be identified and then further developed based on the evaluation. In the best case, this procedure can save valuable resources and capacities and also minimises the risk of a failed product.\nDiversifications and product range expansion\nEven if a mature product range already exists, conjoint analysis is a good choice. Especially in the case of diversifications such as new product sizes, flavours or colours, it can be essential to test these beforehand through market research.\nBy the way: The conjoint method can also be used for packaging and design tests.\nPrice determination with the conjoint method\nTo perform a price analysis using this methodology, a Van Westendorp price analysis can be performed as a basis. However, the conjoint analysis also works wonderfully as a stand-alone method for price determination. In addition, this variant has the great advantage that several concepts can be tested directly with regard to their willingness to pay a price and not just one product.\n6. Conjoint method: These are the best practices\n- Use short and concise descriptions of product features. This helps to avoid misunderstandings that could possibly distort the analysis.\n- Use pictures. This makes it easier to distinguish between the different variants and makes it even easier for respondents to imagine what the survey is about.\n- Use descriptive comparisons for attributes. Everyone perceives abstract attributes such as weight or size differently. Therefore, levels such as \"light\", \"heavy\" or \"large\" and \"small\" should be avoided. In such cases, concrete comparisons are more appropriate, such as: As heavy as a similar product.\nBy the way, the best way to implement these tips is with the Appinio Conjoint Analysis Tool, because there you will find the corresponding setting options. Please feel free to send us a non-binding inquiry.\n7. Setting up a conjoint analysis (with Appinio)\nRegister on the Appinio platform.\nDefine the 3-4 most important product features (e.g. price, design) to be tested.\nContact one of our market research experts. They will guide you with formulating the definition of the product features right up until your survey goes live.\n- Go live with your survey\n- Our professional market researchers do a final check of your survey before it goes live.\n- Afterwards, your survey will be answered by our panel immediately.\n- Analyse the data:\nAnalyse your base data in our interactive dashboard in real time.\nThe results of the conjoint survey are calculated and visualised in bar graphs and tables by our research consultants to show the utilities and importance of each factor.\nAccordingly, the results can be used immediately for decision-making.\nExport PowerPoint, Excel or CSV files at any time.\n8. What are the advantages and disadvantages of a conjoint analysis?\n- Thanks to the conjoint analysis, it is easy to determine which features a product must have and which the consumers would forgo.\n- Due to many different combinations of attributes and levels even subconscious decisions can be measured and analysed with a conjoint analysis.\n- The research design is highly flexible and can be adapted to almost every product or concept.\n- The conjoint method is incredibly versatile. A multitude of studies can be covered in a single study: price willingness, design tests or product attributes.\nDisadvantages:In addition to the advantages, as with any method, there are also a few disadvantages in the analysis. For example, it is possible that:\n- Respondents opt for luxury variants because they don’t spend any money and consequently have no sense of making a real purchasing decision.\n- This can lead to a discrepancy between survey results and actual market behavior.\nCarrying out a conjoint analysis offers a multitude of advantages and can be used for a wide range of use cases. The conjoint method is particularly suitable in the areas of product development and marketing. Another particular advantage is that several combinations and variants can be tested without consumers having to choose their favourites from a list of attributes. The conjoint analysis realistically reflects an everyday purchase decision."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:581d8c18-f10b-4bdd-8182-5d1eca6e376b>","<urn:uuid:a0357f8e-9df2-40e5-a9c3-772793a21356>"],"error":null}
{"question":"想在草坪上使用阔叶杂草除草剂,在什么天气条件下施用效果最好?","answer":"The best conditions are on a clear and calm day with temperatures between 50 to 80 degrees Fahrenheit. Avoid applying when soil is too dry as it's less effective and can damage turf. If it rains after application, wait 10 days before reapplying.","context":["What’s the Best Broadleaf Weed Killer for Your Lawn?\nHow To Minimize The Impact Of Broad Leaf Weeds\nTips for Controlling Broadleaf Weeds On The Lawn\nWhen talking about the most problematic broad-leaf weeds, dandelions and chickweed are the ones that spring to mind. Of course, there are hundreds of different varieties besides the usual suspects and they are easy to spot on a lawn as they don’t resemble grass.\nControlling broadleaf weeds is typically easy, but to completely eradicate them can be almost impossible and impractical and there are not many weeds that people want growing in their lawn.\nThe best way to manage broadleafs or any type of weed for that matter is by practicing good horticultural practices. Without good practices, you only encourage the weeds to thrive which is not what you want.\nGood practices include cutting grass regularly but not so that it is too short, not making use of fertilizers and too much or too little water. Weeds are as strong as they are invasive and can easily tolerate dry or wet conditions and compete with the grass and other plants for nutrients by effectively starving them.\nWhere Do Broadleaf Weeds Come From?\nBroadleaf weeds are naturally occurring plants that appear from seeds that are spread by the wind and wildlife or by root encroachment. The problem with broadleaf weeds is that they can survive in the soil for 30 plus years and the high quantity of seeds they produce can mean a never-ending cycle of removal and invasion.\nAnother place that broadleaf weeds can come from is from low-quality grass seeds, think along the lines of budget brands. They can also appear from the use of topsoil introduced to the garden that contains root fragments or dormant seeds.\nChemical Weed Killer to Control Invasion.\nOne of the most common chemical weedkiller methods is to use a mixture that is composed of two to three of the individual herbicides or active ingredients. Using a combination of ingredients helps target different reproductive cycles of the weeds and disrupts seed production, death at the root and sterilization.\nTiming weedkiller application can help enhance its effectiveness. For perennial broadleaf weeds, it is best to start applying in September to November. When winter is fast approaching, weeds store their energy reserves in the roots and stems which allow the chemicals to enter the plant’s food reserves thus leading to the poisoning and death of the plant.\nAfter fall and winter have passed, the next best time is in spring and early summer after the weeds have already flowered but before the produce seeds. The plant will have expended most of its energy when flowering making it weaker and more susceptible to herbicides.\nTake care to avoid using it on or near areas containing your flowers, tree’s, shrubs and vegetables as these can be affected by herbicides resulting in plant damage or death.\nIf you are lucky enough to have just a few random weeds on your lawn, you can use the spot method rather than apply the chemical mix to your whole lawn. The spot treatment approach uses a quantity that is enough to dampen the leaf but not leave it saturated.\nIt is best to apply herbicides to actively growing young weeds as they are vulnerable at that point and try to avoid using it when the soil is too dry as it is less effective and can increase the risk of turf damage. The best type of day to apply it is on a clear and calm day with temps between 50 to 80 degrees Fahrenheit.\nIf it happens to rain after an application, wait for around 10 days to see if it has worked before reapplying again. It is important to avoid using herbicides on new or freshly laid grass and areas that have been excessively wet. You should also avoid mowing any areas you have treated for a minimum of 3 days post treatment.\nSummer Annual Broadleaf Weed Control\nWeeds such as knotweed, spurge, and purslane thrive in the summer months and they are also some of the most difficult plants to completely eradicate. As they germinate at various different points and have different growth cycles, a single application of herbicide may only control one species or one wave of growth. To control summer broadleafs requires a multifaceted approach.\nBeginning in April you want to apply a herbicide that contains idoxiban, a herbicide which destroys seeds before they have a chance to sprout. Then in May, you want to apply one application of a general broadleaf herbicide.\nSo What’s the Top Broadleaf Weed Killer for Lawns?\nSome resistant and stubborn weeds such as ground ivy and wild violets are difficult to manage as they proliferate via underground root systems. The best way to try to deal with weeds like these is with multiple and regular herbicide treatments. Broadleaf herbicides containing 2,4-D, MCPP, and dicamba should be used. A herbicide containing triclopyr or fluroxypyr can also be helpful in controlling these weeds.\nWe recommend Agri Star’s Triclopyr 4E Herbicide as the best best broadleaf weed killer for lawns, because it provides a cost effective solution for treating almost any property with trilopyr. Available in a 1 gallon container means you’ll have ample supply to eradicate broadleaf for years (average size yard).\nYou need to remember that to have a 100% weed free garden is going to be impossible, nature has a way of ensuring the spread of them and unless you grow in a glass covered garden with sterilized soil and microscopic selection of seeds, it’s never going to happen. If you use a good broadleaf fertilizer in the fall you can, however, minimize the following year’s growth.\nGood Garden Practices.\n- When cutting the lawn, it is imperative to keep the length at around 3 inches and avoid exposing the soil by cutting too short. Try to mow often so that you are only removing the top tips of the blades of grass. This could mean mowing the lawn twice a week in the summer months.\n- Water the grass by giving it a deep soak around once every two weeks. It is best to wait until the grass becomes stressed, which leads to a slight graying of the lawn, before watering again. Always deep and never frequently.\n- Feed you lawn and create a dense layer of grass that prevents weed seeds from taking root by preventing them from reaching the soil and blocking out any sunlight. 2 to 4 pounds of nitrogen per 1000 ft2 per year is sufficient. If you have a lot of clover emerging then you know that your lawn is deficient in nitrogen and needs to be fed.\n- Lastly, ensure your lawn is not covered by shade as this can lead to balding patches where many broadleafs thrive. In fact, broadleafs love a shady and damp soil to grow in, so if you have an area of shade in your garden, resolve to fix the issue as quickly as possible.\nHow to Easily Adjust Your Lawn Mower’s Height"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:b2e19a03-c390-4539-8cc3-56b9808719a2>"],"error":null}
{"question":"¿Cómo afectaron los ferrocarriles victorianos al paisaje rural de Inglaterra, y cuál fue su impacto en el crecimiento poblacional de Londres? Me interesa entender los dos lados de esta transformación histórica! 🚂","answer":"Victorian railways had two major impacts. In rural England, they were criticized for dividing and disfiguring the landscape, with writers like George Meredith describing them as causing 'disfigurement of our dear England' through noise, smoke, and the cutting up of land. However, regarding London's growth, railways were transformative - they enabled the city to expand dramatically from 1 million people across 5 miles in 1801 to over 6.5 million people across 17 miles by 1901. Research shows that without the railway network, Greater London's population would have been 30% lower in 1921, as railways allowed people to live further from their workplaces and enabled the first large-scale separation of residential and commercial areas.","context":["any Victorian writers of fiction and non-fiction described the havoc wreaked upon urban centers by railways, such as driving up real estate prices, devastating working class housing, and adding to urban congestion. Novelists also protested the effects of the new transportation technology upon the countryside, but here the protests seem aimed more at some preserving some sort of primal innocence in places in which as Gerard Manley Hopkins put it,\nGenerations have trod, have trod, have trod;\nAnd all is seared with trade; Bleared, smeared with toil;\nAnd wears man's smudge and shares man's smell: the soil\nIs bare now, nor can foot feel, being shod. [\"God's Grandeur\"]\nThe objections all center on the charge, now hard to take seriously, that laying the railway tracks through a landscape in some way irremediably chopped and divided it — as if the English countryside were not already criss-crossed by centuries-old fences and stone walls. In George Meredith's Diana of the Crossways, for example, the heroine objects to the way the railways cut through the landscape, expressing\nher personal sorrow at the disfigurement of our dear England. . . Those railways! When would there be peace in the land? Where one single nook of shelter and escape from them! And the English, blunt as their senses are to noise and hubbub, would be revelling in hisses, shrieks, puffings and screeches, so that travelling would become an intolerable affliction. 'I speak rather as an invalid,' she admitted; 'I conjure up all sorts of horrors, the whistle in the night beneath one's windows, and the smoke of trains defacing the landscape; hideous accidents too. They will be wholesale and past help. Imagine a collision! I have borne many changes with equanimity, I pretend to a certain degree of philosophy, but this mania for cutting up the land does really cause me to pity those who are to follow us. They will not see the England we have seen. It will be patched and scored, disfigured . . . a sort of barbarous Maori visage — England in a New Zealand mask.\nAnd, she continues, \"I love my country. I do love quiet, rural England. Well, and I love beauty, I love simplicity. All that will be destroyed by the refuse of the towns flooding the land.\" Ah, it's all those members of the lower orders freed from their urban settings.\nIn the same novel a countryman complains:\n\"Once it were a capital county, I say. Hah! you asks me what have happened to it. You take and go and look at it now. And down heer'll be no better soon, I tells 'em. When ah was a boy, old Hampshire was a proud country, wi' the old coaches and the old squires, and Harvest Homes, and Christmas merryings. — Cutting up the land! There's no pride in livin' theer, nor anywhere, as I sees, now.\"\n\"You mean the railways.\"\n\"It's the Devil come up and abroad ower all England!\" exclaimed the melancholy ancient patriot.\nTrollope made a somewhat different point in The Three Clerks when he claimed that \"it is very difficult nowadays to say where the suburbs of London come to an end, and where the country begins. The railways, instead of enabling Londoners to live in the country, have turned the country into a city. London will soon assume the shape of a Hammersmith, will be the nucleus, and the various railway lines will be the projecting rays. [Ch 3 \"The Woodwards\"] The coming of the railway marks a watershed, a turning point and a border not only between places but between times as well. Therefore to locate his story of Castle Richmond in a time before modernity arrives, Trollope's narrator tells us that \"Castle Richmond stands close upon its banks, within the Mallow and Killarney railway now passes, but which some thirteen years since knew nothing of the navvy's spade, or even of the engineer's theodolite.\"\nKellett, John R. Railways and Victorian Cities. London: Routledge & Kegan Paul, 1979; Toronto: U. of Toronto Press, 1979.\nThe essential book for anyone who wants to learn about the relations of Victorian railways to contemporary government, industry, finance, urban life, and so on, Kellett's volume is packed with quotations from primary sources, including parliamentary reports and contemporary periodicals; it also has valuable maps and illustrations [GPL].\n- Railways and Victorian Literature — An Introduction\n- Railways in Victorian Fiction — The Effect upon the City\nLast modified 26 December 2005","The thought that major cities were built by transport networks is obvious – even trivial – the moment one starts to think about it all. And yet there’s still interest in the details of which technologies and how. Our own insistence, around here, would be that while commuting, work and home, are interesting elements of the story the real influence is going to be in supply. How in heck do you feed 4 million people with horse drawn carts for example? The necessary catchment area is going to be larger than horse drawn carts can cover, no? Yet commuting is still interesting:\nThe making of the modern metropolis: Evidence from London\nStephan Heblich, Stephen Redding, Daniel Sturm 12 October 2018\nOver the last two centuries, transportation innovations have drastically changed urban landscapes. This column explores how the mid-19th century transport revolution shaped the urban agglomeration of London. The results show Greater London’s population would have been 30% lower in 1921 without the railway network. The findings and the quantitative urban models employed highlight the role of modern transport technologies in sustaining dense concentrations of economic activity.\nModern metropolitan areas include vast concentrations of economic activity, with Greater London and New York City today accounting for around 8.4 and 8.5 million people, respectively. These intense population concentrations involve the transport of millions of people each day between their residence and workplace, as surveyed in Redding and Turner (2015). Today, the London Underground alone handles around 3.5 million passenger journeys per day, and its trains travel around 76 million kilometres each year (about 200 times the distance between the earth and the moon). Yet relatively little is known about the role of these commuting flows in sustaining dense concentrations of economic activity. On the one hand, these commuting flows impose substantial real resource costs, both in terms of time spent commuting and the construction of large networks of complex transportation infrastructure. On the other hand, they are also central to the creation of predominantly commercial and residential areas, with their distinctive characteristics for production and consumption.\nHow the separation of workplace and residence led to agglomeration\nIn new research, we use the mid-19th century transport revolution from the invention of steam railways, a newly created, spatially disaggregated dataset for Greater London from 1801-1921, and a quantitative urban model to provide new evidence on the contribution of the separation of workplace and residence to agglomeration (Heblich et al. 2018). The key idea behind our approach is that the slow travel times achievable by human or horse power implied that most people lived close to where they worked when these were the main modes of transportation. In contrast, the invention of steam railways dramatically reduced the time taken to travel a given distance, increasing average travel speeds from around 6 mph for horse-drawn vehicles and 3 mph for walking to around 21 mph, which permitted the first large-scale separation of workplace and residence. This separation enabled locations to specialise according to their comparative advantage in production and residence. Using both reduced-form and structural approaches, we find substantial effects of steam passenger railways on city size and structure. We show that our model is able to account both qualitatively and quantitatively for the observed changes in the organisation of economic activity within Greater London.\nLondon during the 19th century is arguably the poster child for the large metropolitan areas observed around the world today. In 1801, London’s built-up area housed around 1 million people and spanned only five miles East to West. In contrast, by 1901, Greater London contained over 6.5 million people, measured more than 17 miles across, and was on a dramatically larger scale than any previous urban area. By the beginning of the 20thcentury, London was the largest city in the world by some margin (with New York City and Greater Paris having populations of 3.4 million and 4 million, respectively, at this time), and London’s population exceeded that of several European countries. Furthermore, London developed through a largely haphazard and organic process during this period, which suggests that both the size and structure of the city responded to decentralised market forces. Therefore, 19th century London provides a natural testing ground for assessing the empirical relevance of models of city size and structure.\nWe begin by providing several pieces of reduced-form evidence on the impact of steam railways on the organisation of economic activity within Greater London. We first establish changes in parish population growth rates that coincide closely with the arrival of the railway. We next show that these changes in population growth are heterogeneous across parishes. In Figure 1, we display average population growth in the 30 years after the arrival of a railway station minus average population growth in the 30 years before its arrival. As apparent from the figure, we find that more central parishes (as measured by distance to the Guildhall in the City of London) experience a decline in population growth relative to more suburban parishes, consistent with the railway redistributing residential population within Greater London.\nFigure 1 Changes in parish population growth rates between the 30-year period after the arrival of a railway station and the 30-year period before its arrival\nTo interpret this reduced-form evidence, we develop a new structural estimation procedure for an entire class of urban models characterised by a gravity equation for commuting. Although we only observe these bilateral commuting flows between the 99 boroughs within the boundaries of Greater London at the end of our sample period in 1921, we show how this framework can be used to estimate the impact of the construction of the railway network. In a first step, we use our bilateral commuting data for 1921 to estimate the parameters that determine commuting costs. In a second step, we combine these parameter estimates with historical data on population, land values and the evolution of the over- and underground railway network going back to the early-19th century. Using a combined commuter and land market clearing condition in the model, we solve for the implied unobserved historical values for employment by workplace and commuting patterns. Although we estimate the model’s commuting parameters using 1921 information alone, we find that its predictions provide a good approximation to the available historical data. As shown in Figure 2, we find that the model captures the sharp divergence between the nighttime and daytime population in the City of London from the mid-19th century onwards. We also find that the model replicates the property of early commuting data that most people lived close to where they worked at the dawn of the railway age.\nFigure 2 Daytime and nighttime population in the City of London in the data and our theoretical model\nOur methodology holds in an entire class of urban models, because it uses only the assumptions of gravity in commuting and land market clearing, together with the requirements that payments for residential floor space are proportional to residential income and payments for commercial floor space are proportional to workplace income. An implication of this property is that our results hold under a range of assumptions about other model components, such as the costs of trading goods, the determinants of productivity and amenities including the strength of agglomeration forces, the supply elasticity for floor space, and the reservation level of utility in the wider economy. Given the data for the initial equilibrium in 1921, we show that the observed changes in population and land values are sufficient statistics in the model for determining historical workplace employment and commuting patterns, and control for other potential determinants of the spatial distribution of economic activity.\nWhile our baseline quantitative analysis controls for these other forces, another key question of interest is the counterfactual of what would have been the effect of the new commuting technology in the absence of any other changes. To address this question, we make additional assumptions about these other model components, and pick one quantitative model within our class of urban models in order to solve for a counterfactual equilibrium. In particular, we choose an extension of the canonical urban model of goods trade and commuting following Ahlfeldt et al. (2015), which is particularly tractable, and enables us to explore alternative assumptions about structural parameters in a transparent and flexible way. Holding the supply of floor space and productivity and amenities constant, we find that removing the entire railway network reduces the total population and rateable value of Greater London by 30% and 22% respectively, and decreases commuting into the City of London from more than 370,000 in 1921 to less than 60,000. By comparison, removing only the underground railway network diminishes total population and rateable values for Greater London by 8% and 6% respectively, and brings down commuting into the City of London to just under 300,000 workers. In both cases, the increase in the net present value of land and buildings substantially exceeds historical estimates of the construction cost of the railway network. Allowing for a positive floor space supply elasticity or introducing agglomeration economies magnifies these effects. Using a calibrated floor space supply elasticity of 2.86 and elasticities of productivity and amenities to employment density of 0.05, in line with empirical estimates, we find that much of the aggregate growth of Greater London can be explained by the new transport technology of the railway.\nTaken together, we find that a class of quantitative urban models is remarkably successful in explaining the large-scale changes in the organisation of economic activity observed in 19th century London, and our findings highlight the role of modern transport technologies in sustaining dense concentrations of economic activity."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:23070180-217a-4ba8-bcce-873828e66548>","<urn:uuid:07107fc9-a36a-459f-848f-bb65ba140a0c>"],"error":null}
{"question":"Which creates more consistent coffee grounds - blade grinders or tampers used in espresso making? 🤔","answer":"Blade grinders create inconsistent grounds with varying particle sizes, while tampers are not actually grinding tools but rather tools used to compress already-ground coffee into an even pellet for espresso extraction. For grinding, burr grinders (not blade grinders) are recommended as they produce uniformly sized particles. Blade grinders operate like blenders, chopping beans unevenly, while proper tamping with a metal tamper ensures even compression of pre-ground coffee for consistent water flow during espresso extraction.","context":["Espresso Tamping is an art that is often neglected in espresso preparation.\nThe goal is to create a pellet of coffee through which the\nhot water from the espresso machine will penetrate evenly.\nSince the water from the espresso machine is under pressure,\nthe espresso pellet must be hard and evenly tamped.\nThe water only knows how to go from a region of high pressure\nto a region of low pressure. Therefore it is important\nto prevent paths of least resistance in the coffee pellet\nand force the water to evenly permeate and extract the coffee.\nchapter assumes that one has read and understood how to\ndose the coffee properly. After the ground coffee\nhas been dosed into the porta-filter it is unevenly distributed. Hold the porta-filter in one hand\nwhile using the other hand to quickly, but gently level\nthe coffee. This is usually accomplished by pulling\nthe coffee to one side of the basket with a slightly curled\npinky, then pushing the coffee back to the opposite side\nof the basket (Picture 2). The key is to evenly distribute the\ncoffee without pressing into the grounds or leaving any\nempty space on the sides of the basket.\nCorrect Espresso Tamping\nyou are done distributing the grounds, it is time for the\nfirst tamp. Without moving the porta-filter, hold\nthe espresso tamper so that the base of the handle fits into the\npalm of your hand. Your wrist should be straight,\nand the espresso tamper should be a straight extension of your arm.\nPress gently on the coffee with five pounds of pressure\nYou will notice that some of the grounds will stick to the\nside of the basket. Therefore, one must gently\ntap the basket with the handle on the porta-filter to knock\nthe grounds onto the flat pellet you just formed (Picture\nThe next step is to apply the\nfinishing espresso tamp (Picture 5). The shape of the pellet has already\nbeen formed, and the finishing tamp confirms this impression.\nWith the tamping tool held as before, press on the pellet with\nthirty pounds of pressure. It is useful to tamp espresso on\na bathroom scale until you become comfortable with the amount\nof force necessary to achieve the appropriate pressure.\nAfter tamping, turn the espresso tamper 720° while continuing to\napply pressure to polish the surface. Make sure you\ntamp evenly. An uneven espresso tamp will result in an uneven\nabove steps should be carried out in about thirty seconds.\nAlthough speed is important, it is necessary to be careful\nnot to bump the basket during this process. Sharply\nhitting the basket will unevenly distribute the grounds\nallowing shortcuts for the water to pass through.\nAs any scientist can appreciate, the path of least\nis preferred. If there are any weak spots or holes\nin the espresso pellet the water will push through this\narea, over extracting this portion of coffee while under\nextracting the rest of the pellet. Improper espresso tamping\nwill result in a twirling pour or white crema.\nthe spacing above the espresso pellet and below the dispersion\nscreen. You can do this by dosing, distributing, and\ntamping as described above. Then insert the porta-filter\nand remove it. Does the screen or screw press down\nonto the coffee? If so, the coffee will not have enough room to expand as it is brewing. On the other\nhand you want to have between 16-18 grams of coffee in the\nbasket. Experiment with these two factors until you\nachieve a good height for the espresso pellet.\nChoosing the Correct Espresso Tamper\nJust as correct espresso tamping is essential, so is the use of a proper espresso tamper. The first action you should take is\nto throw away the plastic round bottom tamper that you currently\nhave. The espresso tamper should be made of aluminum or similar\nlight metal and should have a diameter so it fits firmly into the basket. Marzocco baskets are 58mm,\nso order the appropriate size. Without a flat packing\nsurface, you create indents which cause uneven extraction.\nYou may order your espresso tamper from Vivace's which sells several\ndifferent size pistons to perfectly fit your basket.\nCurrently, Vivace's has recommended the use of a round bottom\ncoffee tamper rather than the traditional flat bottom tamping machines.\nThis suggestion is welcome to further discussion.\nCurrently, my observations do not support the conclusion\nthat the rounded bottom tamper is better. Initial\ncrema is thicker, but overall crema is less. I will\ncontinue tests with other espresso machines and with other\nespresso blends. You should also order the book \"Espresso\nCoffee: Professional Techniques,\" a great\nbook on espresso preparation.\nVideo of tamping process: Large\n(6.72 Mb) or Medium (929","If I could offer one piece of advice that will improve your coffee more than anything else, it would be this:\nLearn how to grind coffee beans.\nYou can improve your coffee a lot with different brew methods, using the right temperature of hot water, or using the right amount of grounds. But all of those pale in comparison to grinding your own coffee, using the right grind size, and choosing a good coffee grinder.\nWhy Learn How to Grind Coffee Beans?\nCoffee is best when it’s made from freshly ground beans. The flavor and aroma decay very quickly after the beans are ground, so pre-ground coffee is generally well on its way to being stale by the time it lands in your pantry.\nThere are steps that you can take to store coffee grounds better, and you can buy from brands that vacuum-seal their grounds quickly after production, but none of those steps will be as reliable as just getting your own coffee grinder and grinding fresh beans before every brew.\nAs I’ll discuss more throughout this article, grind size is very important for improving your cup of coffee. Pre-ground coffee is typically sized for either drip coffee makers or espresso machines.\nIf you want to try any other brewing method, you could have a hard time finding grounds that will suffice. Even if you make drip coffee or espresso, you may find that you prefer a grind size that is slightly different than what is available in the store-bought grounds.\nWhat Makes a Good Coffee Grind?\nGrind size accuracy\nThe grind size is a crucial aspect. Larger particles will extract slower, filter easier, and allow water to flow more freely through them. Small particles will extract rapidly, may be hard to filter, and will pack together tightly enough that water may have a hard time getting through. We’ll talk more about the specific grind size needed for each type of coffee maker in the next section.\nGround size consistency\nThe ground particles need to be uniformly sized. If you have a mix of small and large particles, you’ll either under extract the larger ones or over-extract the smaller ones — possibly both all at once. In some methods, small particles may end up either leading to clogs or sooty coffee. In other methods, large particles can ruin the packing of the coffee, letting water flow through too easily.\nDon’t burn the beans!\nThis is an often-overlooked factor, but some coffee grinders heat up a lot while you are using them. Not only is this bad for the motor, but it can also burn the grounds. This can leave a bitter, ashy flavor that will ruin your brew.\nTypes of Coffee Grinds\nTexture: Small pebble, ground peppercorn\nUsed for: Cold brew coffee makers, cowboy coffee\nThese are the coarsest of all the coffee grinds that you can make with a typical grinder. They resemble ground peppercorn and are perfect for cold brew, but you won’t find many other uses for them.\nTexture: Coarse salt\nUsed for: French press coffee makers, cold brew, percolators, cupping/tasting\nA coarse grind is the size of coarse or kosher salt and is most frequently used as the grind for French press coffee.\nTexture: Coarse sand\nUsed for: Chemex, pour over, siphon brewers\nMedium-coarse is like coarse or rough sand, and it is most frequently used in Chemex and a couple of other specialty brewing methods.\nTexture: Brown or white sand\nUsed for: Drip coffee makers, flat filter drip, AeroPress, siphon brewers, pour over\nThis is probably the most common coffee grind, and it resembles normal brown or white sand. If you buy pre-ground coffee for a drip coffee maker, it’s probably a medium grind.\nTexture: Slightly more powdery than sand\nUsed for: Pour over, AeroPress, siphon brewers, Moka pots, V60\nMost pour-over coffee uses a medium-fine grind, which is somewhere between sand and sugar in texture. This is also a great starting grind for AeroPress coffee makers.\nTexture: Table salt / granulated sugar\nUsed for: Espresso machines, AeroPress, Moka pots\nThe fine grind, or espresso grind, resembles table salt or granulated white sugar. It’s mostly used for espresso makers, including stovetop espresso makers (Moka pots). It can also be used for some AeroPress recipes.\nTexture: Baby powder / powdered sugar\nUsed for: Turkish coffee makers (Ibriks)\nAn extra fine grind — also known as a Turkish grind or pulverized — resembles powdered sugar and is used almost exclusively for making Turkish coffee. Many coffee grinders won’t even make a grind this fine, so you may need different equipment if you want to grind beans for Turkish coffee.\nCoffee Grind Size Chart\nHere’s a quick reference for common types of coffee makers.\n|Brewing method||Grind size|\n|Cowboy coffee||Extra coarse|\n|Cold brew||Extra coarse – coarse|\n|Siphon||Medium coarse – medium fine|\n|Pour over||Medium coarse – medium fine|\n|Flat filter drip||Medium|\n|AeroPress||Medium – fine|\n|Moka Pot||Medium fine – fine|\n|Ibrik (Turkish)||Extra fine|\nWhat If You Don’t Use the Right Grind?\nToo coarse a grind will leave your coffee under-extracted. In this case, the brew will taste sour because the sweet and complex notes will remain in the grounds.\nWhen you use too fine a grind, you instead over-extract the coffee grounds. Over-extracting the grounds causes bitterness, potentially enough to entirely overwhelm the flavor of the coffee.\nGrind size is not the only factor in under- or over-extraction, though. Keep in mind that the brew time, the temperature of the water, and the coffee-to-water ratio can all change the extraction level. A bitter flavor in your coffee could also mean the coffee beans have gone stale.\nChoosing a Coffee Grinder\nLet’s briefly look at the common types of coffee grinders and how you can choose the best coffee grinder for your needs.\nBurr vs blade\nBlade grinders operate much like blenders, chopping beans apart instead of truly grinding them. This results in a very uneven grind, with a range of differently-sized grounds all mixed together in a single batch.\nBurr grinders operate by crushing the beans between two closely-spaced steel or ceramic plates covered in tooth-like blades. They offer a much more consistent grind size than blade coffee grinders, owing to the fact that the spacing between the plates strictly controls the coffee grind sizes that can get through.\nBlade grinders don’t have size settings. Instead, the average size of the grounds is controlled by the contact time with the blades. If you want smaller coffee particles, you run the grinder for longer.\nBurr coffee grinders, on the other hand, let you set a finer or coarser grind by adjusting the spacing between the plates. Running the grinder for longer won’t create finer grinds because the grounds don’t stay in contact with the burrs after they achieve the proper size.\nThe inconsistent grinds produced by a blade grinder will not allow you to maximize your coffee brewing technique. The only advantage of blade grinders is their price, but an inexpensive burr grinder will outperform even a high-end blade grinder.\nIn short: you need to buy a burr grinder if you want great coffee.\nConical burr vs flat burr\nThere are two major types of burr grinders: conical and flat. They differ, as the names suggest, in the shape of their burrs.\nConical burr grinders have two cone-shaped burrs — one hollow and one solid — with the solid one placed inside the hollow one. The beans are dropped vertically between the burrs, falling through the bottom when they have reached the proper size.\nFlat burr grinders also have two burrs, but they are flat and placed one on top of the other. The beans are dropped so as to fall between the plates and eventually be pushed out vertically.\nConical burrs tend to be cheaper and quieter than flat burrs, which have to operate at a faster speed. Flat burrs create grinds with somewhat more consistent sizes, but the difference is minor compared to the advantage of burrs over blades. The difference is so small that even many coffee shops use conical burr grinders.\nCeramic vs steel burrs\nYou’ll also have to choose between ceramic vs steel burrs for your grinder. Ceramic burrs tend to be more expensive and they don’t start off as sharp as steel burrs. They do retain their sharpness better over time, though. Steel burrs won’t last as long as ceramic, but they are cheaper and start off much sharper.\nManual vs electric\nAt low price ranges, manual coffee grinders are higher-quality, but they can be difficult to use for large quantities. Electric grinders vary a lot more in quality, but high-end electric burr grinders can outperform any manual grinder.\nMost handheld grinders are manual, especially handheld burr grinders. Countertop grinders come in both manual and electric varieties, although electric is far more common.\nIf you grind a lot of beans, you probably want an electric grinder. If you need a small travel grinder or you just want to save money, a manual grinder is probably the better option.\n3 Mistakes to Avoid When Grinding Coffee Beans\n1. Using a blade grinder\nI mentioned this before, but it’s worth reiterating: blade grinders are terrible. People buy them because they are cheap, but a low-cost burr grinder will far outperform even the most expensive blade grinder. Ditch the glorified blender.\n2. Grinding your beans too early\nCoffee grounds go bad a lot faster than coffee beans because of the high surface area. The finer the grind, the faster it will go bad, but any grounds are only at peak freshness for the first half-hour or so after grinding.\nIdeally, you should be grinding your coffee beans no more than an hour before use. Don’t try to grind your beans for multiple days at once. If for whatever reason, you do have leftover grounds, you should store them in an airtight container in a cool, dark place.\n3. Grinding the wrong amount of coffee\nAs I mentioned earlier, there’s more to the perfect cup of coffee than the grind size. The amount of grounds you’ll need will depend on what type of coffee maker you are using and what strength of coffee you want. To make that whole process easier, I suggest using the coffee-to-water ratio calculator.\nThe main takeaways here are:\n- Buy whole bean coffee and grind it yourself — pre-ground coffee is not fresh.\n- Use a burr grinder — blade grinders are terrible.\n- Choose your type of grind (grind size) based on your brewing method.\nI know this can seem complicated at first, but it will go a long way toward improving your coffee brewing experience. People think of coffee as a bitter or simple flavor, but that’s just because they haven’t had the good stuff.\nCoffee can exhibit a complex and beautiful variety of flavors and aromas — and freshly ground coffee is the pathway to exploring everything our favorite bean has to offer."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:93975826-6e15-4629-9aa6-0b1045757b36>","<urn:uuid:b50a37d5-62f2-41ca-8724-7afa9f536046>"],"error":null}
{"question":"What role do both environmental factors and personal attachments play in shaping human behavior in modern spaces? Please address both the physical and psychological dimensions.","answer":"In modern spaces, environmental factors like indoor air quality are significantly influenced by human presence, with occupants being the primary source of volatile organic compounds that affect behavior and productivity. The ventilation systems and occupancy patterns create a dynamic chemistry that impacts the overall environment. On the psychological side, parental attachment serves as a protective factor that influences human behavior, particularly in terms of technology use and interpersonal adaptation. Research shows that the relationship between parental attachment and problematic behaviors (like excessive smartphone use) is mediated by interpersonal adaptation and moderated by self-control, with individuals having lower self-control being more vulnerable to maladaptive behaviors.","context":["Oct 08, 2019 09:10 AM EDT\nHow Much Air Pollution Do You Bring In The Office?\nHow much influence does a person have in their office space? More than the work that they do, people influence their office space more than they think they do. Mere breathing and the deodorant they're wearing contribute to the pollutants present in the air inside the office. A growing body of evidence shows that their mere existence in the office may be polluting the air.\nA team of engineers from Purdue University started the largest study of its kind. They filled an office building rigged with sensors. Thousands of sensors will be used to identify the contaminants present indoors. From there, they hope to recommend ways to control them through the building design as well as the office operations processes.\nAir quality significantly affects the productivity of employees. Thus, it is a must that people understand the contaminants present in the air to make the proper recommendations as to what can be done to improve it. The factors that influence the production and removal of contaminants should be identified as well. Brandon Boor, a professor of civil engineering, believes that the process of identification can help control air pollution within an office space.\nThe data collected shows that the ventilation system and the people in the office greatly make up the chemistry of indoor air. It possibly affects everything else in the office as well. The team of researchers is expected to present the results of their study during the 2019 American Association for Aerosol Research Conference in Oregon on October 14-18.\n\"Indoor air comes with dynamic chemistry,\" Boor said. He described it as ever-changing throughout the day, affected by conditions outdoors. The occupancy pattern in the office, as well as the ventilation system installation, affects the quality of air.\nThe building used in the study is called the Living Labs at Ray W. Herrick Laboratories in Purdue. It is filled with an array of sensors to track the flow of indoor air in four open-plan office spaces. The team worked on a new technique to track the occupancy by installing temperature sensors in every desk chair.\nThrough the Living Lab, the team led by Boor was able to identify the behavior of several volatile organic compounds. They identified their transformation through the ventilation system as well as how they are eliminated in the air through the filters. The goal was to show how the ventilation system affects the air people breathe indoors.\nThe team was able to discover that the greater number of people inside the office meant more emissions of these compounds. The preliminary results of their study show that people in the office are the dominant source of these volatile organic compounds. They also found that the amount of these compounds is 10 to 20 times higher indoors than outdoors.\nAn employee's contribution to the workspace is not limited to the work that he does. His mere existence affects everything else including the quality of air everyone breaths in the office. But the same is true for office space. It is not just a space for work. Rather, it is a space where systems and people combine to create meaningful products.\nSimply put, if the office space does not have proper ventilation, these volatile compounds may adversely affect worker productivity and overall health.\nJoin the Conversation","Research has revealed that parental attachment may be a protective factor against problematic smartphone use. However, few studies have examined the underlying mechanisms that may mediate or moderate this association. To fill this gap, this study examined the mediating role of interpersonal adaptation and moderating role of self-control in the association between parental attachment and problematic smartphone use. A sample of 764 Chinese young adults completed measures of parental attachment, problematic smartphone use, interpersonal adaptation, and self-control. Results showed that interpersonal adaptation mediated the relationship between parental attachment and problematic smartphone use. Moreover, this mediating effect of interpersonal adaptation between parental attachment and problematic smartphone use was moderated by self-control, with the effect being stronger for individuals with lower self-control. To our knowledge, this was the first study examining how and when parental attachment leads to problematic smartphone use. Limitations and implications of this study are discussed.\nThis is a preview of subscription content, log in to check access.\nBuy single article\nInstant access to the full article PDF.\nPrice includes VAT for USA\nSubscribe to journal\nImmediate online access to all issues from 2019. Subscription will auto renew annually.\nThis is the net price. Taxes to be calculated in checkout.\nArmsden, G. C., & Greenberg, M. T. (1987). The inventory of parent and peer attachment: Individual differences and their relationship to psychological well-being in adolescence. Journal of Youth and Adolescence,16(5), 427–454. https://doi.org/10.1007/BF02202939.\nBaumeister, R. F., Vohs, K. D., & Tice, D. M. (2007). The strength model of self-control. Current Directions in Psychological Science,16(6), 351–355. https://doi.org/10.1111/j.1467-8721.2007.00534.x.\nBeranuy, M., Oberst, U., Carbonell, X., & Chamarro, A. (2009). Problematic Internet and mobile phone use and clinical symptoms in college students: The role of emotional intelligence. Computers in Human Behavior,25(5), 1182–1187. https://doi.org/10.1016/j.chb.2009.03.001.\nBowlby, J. (1973). Attachment and loss: Separation: Anxiety and anger. New York: Basic Books.\nBowlby, J. (1982). Attachment and loss: Retrospect and prospect. American Journal of Orthopsychiatry,52(4), 664.\nBronfenbrenner, U. (1994). Ecological models of human development. International encyclopedia of education,3(2), 37–43.\nBronfenbrenner, U. (2005). Making human beings human: Bioecological perspectives on human development. Thousand Oaks, CA: Sage.\nCarbonell, X., Chamarro, A., Oberst, U., Rodrigo, B., & Prades, M. (2018). Problematic use of the internet and smartphones in university students: 2006–2017. International Journal of Environmental Research and Public Health,15(3), 475. https://doi.org/10.3390/ijerph15030475.\nChen, L., Yan, Z., Tang, W., Yang, F., Xie, X., & He, J. (2016). Mobile phone addiction levels and negative emotions among Chinese young adults: The mediating role of interpersonal problems. Computers in Human Behavior,55, 856–866. https://doi.org/10.1016/j.chb.2015.10.030.\nChina Internet Network Information Center. (2017). Fortieth statistical report on the development of China Internet Network. Retrieved from http://www.cnnic.cn/hlwfzyj/hlwxzbg/hlwtjbg/201708/t20170803_69444.htm\nCho, M. (2014). The relationships among smart phone use motivations, addiction, and self-control in nursing students. Journal of Digital Convergence,12(5), 311–323. https://doi.org/10.14400/jdc.2014.12.5.311.\nChoi, Y. Y., & Seo, Y. S. (2015). The relationship between insecure attachment and smartphone addiction: The mediation effect of impulsiveness moderated by social support. Korean J Psychol Couns Psychother,27(3), 749–772.\nCollins, N. L. (1996). Working models of attachment: Implications for explanation, emotion, and behavior. Journal of Personality and Social Psychology,71(4), 810. https://doi.org/10.1037/0022-35126.96.36.1990.\nDeng, L. Y., Fang, X. Y., Wan, J. J., Zhang, J. T., & Xia, C. C. (2012). The relationship of psychology needs and need gratification with internet addiction among college students. Psychological Science,35(1), 123–128.\nDeniz, M. (2011). An investigation of decision making styles and the five-factor personality traits with respect to attachment styles. Educational Sciences: Theory and Practice,11(1), 105–113.\nEdwards, J. R., & Lambert, L. S. (2007). Methods for integrating moderation and mediation: a general analytical framework using moderated path analysis. Psychological Methods,12(1), 1–22. https://doi.org/10.1037/1082-989x.12.1.1.\nElhai, J. D., Tiamiyu, M., & Weeks, J. (2018). Depression and social anxiety in relation to problematic smartphone use: The prominent role of rumination. Internet Research,28(2), 315–332. https://doi.org/10.1108/IntR-01-2017-0019.\nFairchild, A. J., & MacKinnon, D. P. (2009). A general model for testing mediation and moderation effects. Prevention Science, 10(2), 87–99.\nFischer, D. G., & Fick, C. (1993). Measuring social desirability: Short forms of the Marlowe-Crowne social desirability scale. Educational and Psychological Measurement,53(2), 417–424. https://doi.org/10.1177/0013164493053002011.\nFonagy, P., & Target, M. (2002). Early intervention and the development of self-regulation. Psychoanalytic Inquiry, 22(3), 307–335.\nGoldstein, A. L., Haller, S., Mackinnon, S. P., & Stewart, S. H. (2018). Attachment anxiety and avoidance, emotion dysregulation, interpersonal difficulties and alcohol problems in emerging adulthood. Addiction Research & Theory. https://doi.org/10.1080/16066359.2018.1464151.\nHaggerty, G., Hilsenroth, M. J., & Vala-Stewart, R. (2009). Attachment and interpersonal distress: Examining the relationship between attachment styles and interpersonal problems in a clinical population. Clinical Psychology & Psychotherapy,16(1), 1–9. https://doi.org/10.1002/cpp.596.\nHan, L., Geng, J., Jou, M., Gao, F., & Yang, H. (2017). Relationship between shyness and mobile phone addiction in Chinese young adults: Mediating roles of self-control and attachment anxiety. Computers in Human Behavior,76, 363–371. https://doi.org/10.1016/j.chb.2017.07.036.\nHayes, A. F. (2013). Introduction to mediation, moderation, and conditional process analysis: A regression-based approach. New York: Guilford Press.\nHoffmann, J. P. (2011). Delinquency theories: Appraisals and applications. London: Routledge.\nHofmann, W., Friese, M., & Strack, F. (2009). Impulse and self-control from a dual-systems perspective. Perspectives on Psychological Science,4(2), 162–176. https://doi.org/10.1111/j.1745-6924.2009.01116.x.\nJia, R., & Jia, H. H. (2016). Maybe you should blame your parents: Parental attachment, gender, and problematic internet use. Journal of Behavioral Addictions,5(3), 524–528. https://doi.org/10.1556/2006.5.2016.059.\nJin, C. C., Zou, H., & Yu, Y. B. (2011). The trait of filial piety belief and the relationship of filial piety belief, attachment and interpersonal adaptation of middle school students. Psychological Development and Education,27(6), 619–624.\nKang, J., Park, H., Park, T., & Park, J. (2012). Path analysis for attachment, internet addiction, and interpersonal competence of college students. In Computer applications for web, human computer interaction, signal and image processing, and pattern recognition (pp. 217–224). Springer, Berlin, Heidelberg.\nKhang, H., Kim, J. K., & Kim, Y. (2013). Self-traits and motivations as antecedents of digital media flow and addiction: The Internet, mobile phones, and video games. Computers in Human Behavior,29(6), 2416–2424. https://doi.org/10.1016/j.chb.2013.05.027.\nKim, E., Kim, E. J., & Cho, C. I. (2017). Structural equation model of smartphone addiction based on adult attachment theory: Mediating effects of loneliness and depression. Asian Nursing Research,11(2), 92. https://doi.org/10.1016/j.anr.2017.05.002.\nLemola, S., Perkinson-Gloor, N., Brand, S., Dewald-Kaufmann, J. F., & Grob, A. (2015). Adolescents’ electronic media use at night, sleep disturbance, and depressive symptoms in the smartphone age. Journal of Youth and Adolescence,44(2), 405–418. https://doi.org/10.1007/s10964-014-0176-x.\nLepp, A., Li, J., Barkley, J. E., & Salehi-Esfahani, S. (2015). Exploring the relationships between college students’ cell phone use, personality and leisure. Computers in Human Behavior,43, 210–219. https://doi.org/10.1016/j.chb.2014.11.006.\nLerner, R. M. (2001). Concepts and theories of human development. London: Psychology Press.\nLeung, L. (2008). Linking psychological attributes to addiction and improper use of the mobile phone among adolescents in Hong Kong. Journal of Children and Media,2(2), 93–113. https://doi.org/10.1080/17482790802078565.\nLi, Y. J., Chen, F. M., Lu, F. R., & Wang, Y. (2015). Effect of cyber victimization on deviant behavior in adolescents: The moderating effect of self-control. Chinese Journal of Clinical Psychology,23(5), 896–900.\nLiu, H., & Wang, H. L. (2012). The relationship between mobile phone addiction and loneliness among College students. Chinese Mental Health Journal,26(1), 66–69.\nLu, X. F. (2003). The compilation and standardization of the College Student Adaptability Inventory, Ph. D. Thesis. Wuhan: Huazhong Normal University.\nLuo, J. J., Dong, H. N., Ding, Q. W., & Li, D. P. (2017). Cumulative ecological risk and adolescent internet addiction: A moderating role of effect control. Chinese Journal of Clinical Psychology,25(5), 893–901.\nMikulincer, M. (1998). Adult attachment style and affect regulation: Strategic variations in self-appraisals. Journal of Personality and Social Psychology,75(2), 420. https://doi.org/10.1037/0022-35188.8.131.520.\nMou, S. D. (2017). Study on the relationship among parent child attachment, loneliness and mobile phone addiction of the junior college students. Journal of Gansu Higher Normal College,22(5), 55–58.\nMuller, D., Judd, C. M., & Yzerbyt, V. Y. (2005). When moderation is mediated and mediation is moderated. Journal of Personality and Social Psychology,89(6), 852–863.\nOdaci, H., & Çikrikçi, Özkan. (2014). Problematic internet use in terms of gender, attachment styles and subjective well-being in university students. Computers in Human Behavior,32(1), 61–66. https://doi.org/10.1016/j.chb.2013.11.019.\nPanova, T., & Carbonell, X. (2018). Is smartphone addiction really an addiction? Journal of behavioral addictions,7(2), 252–259. https://doi.org/10.1556/2006.7.2018.49.\nStepp, S. D., Morse, J. Q., Yaggi, K. E., Reynolds, S. K., Reed, L. I., & Pilkonis, P. A. (2008). The role of attachment styles and interpersonal problems in suicide-related behaviors. Suicide and Life-Threatening Behavior,38(5), 592–607. https://doi.org/10.1521/suli.2008.38.5.592.\nTan, S. H., & Guo, Y. Y. (2008). Revision of self-control scale for Chinese college students. Chinese Journal of Clinical Psychology,16(5), 468–470.\nTangney, J. P., Baumeister, R. F., & Boone, A. L. (2004). High self-control predicts good adjustment, less pathology, better grades, and interpersonal success. Journal of Personality,72(2), 271–324. https://doi.org/10.1111/j.0022-3506.2004.00263.x.\nThorberg, F. A., & Lyvers, M. (2010). Attachment in relation to affect regulation and interpersonal functioning among substance use disorder in patients. Addiction Research & Theory,18(4), 464–478. https://doi.org/10.3109/16066350903254783.\nXie, X., Dong, Y., & Wang, J. (2018). Sleep quality as a mediator of problematic smartphone use and clinical health symptoms. Journal of Behavioral Addictions,7(2), 466–472. https://doi.org/10.1556/2006.7.2018.40.\nYi, J., Ye, B. J., & Liu, M. F. (2016). Affiliation with deviant peers and tobacco and alcohol use: The moderating of effortful control. Chinese Journal of Clinical Psychology,24(3), 544–546.\nZhang, Y. L., Lu, G. Z., Liu, Y. L., & Zhou, Y. (2017). The mediating role of College Students’ self identity in the relationship between interpersonal adaptability and mobile addiction tendency. Chinese Mental Health Journal,31(7), 749–774.\nZhang, Y., Zhou, Y. G., & Pei, T. (2015). The mediating effect of loneliness in the relationship between interpersonal adaptability and mobile phone addiction of college students. Chinese Mental Health Journal,29(10), 749–774.\nZhao, B. B., Jin, C. C., & Zou, H. (2018). Relations among parent-adolescent relationship, negative social adjustment and Internet addiction of adolescents: A mediated moderation effort. Psychological Development and Education,34(3), 353–360.\nZhou, Y., Liu, Y., & Chen, J. S. (2015). Mediation rule self-control between mobile phone addiction and self-esteem. Chinese Journal of School Health,36(7), 1032–1034.\nThis study was funded by the Humanity and Social Science Youth foundation of Ministry of Education, China [Grant Number: 18YJC880133]; Special project of “13th Five-Year” planning of Education Science in Jiangsu Province, China [Grant No. C-b/2016/01/23].\nConflict of interest\nYan Zhang has received research grants from Ministry of Education of China. Tingting Lei has received research grants from Jiangsu Education Science Planning Office. Ding‑liang Tan declare that they have no conflict of interest.\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nAbout this article\nCite this article\nZhang, Y., Tan, D. & Lei, T. Parental Attachment and Problematic Smartphone Use Among Chinese Young Adults: A Moderated Mediation Model of Interpersonal Adaptation and Self-control. J Adult Dev 27, 49–57 (2020). https://doi.org/10.1007/s10804-019-09331-2\n- Interpersonal adaptation\n- Parental attachment\n- Problematic smartphone use\n- Young adults"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:bf04cbc1-ecfd-4247-914f-dc7a1d3b3bf9>","<urn:uuid:71e7fbd6-4371-4e82-a13a-73be87edf338>"],"error":null}
{"question":"Can you compare the conservation implications of the spiny dogfish's feeding adaptations versus their trade management - specifically, how do their biological capabilities affect sustainable fishery practices?","answer":"The spiny dogfish's biological adaptations as a generalist predator, capable of consuming various prey types through multiple feeding methods (ram, suction, and biting), make them successful hunters but also vulnerable to fishing pressure. Their feeding versatility has contributed to their widespread presence, yet this same success has led to intensive commercial exploitation. While the U.S. Atlantic stock has achieved sustainability under a Fishery Management Plan, other regions face conservation challenges. The social network analysis reveals that declining U.S. exports have led to increased exploitation in African, Asian, and South American coastal areas, potentially threatening regional dogfish stocks. This situation demonstrates how the species' biological success has paradoxically contributed to management challenges, requiring careful balance between their natural predatory capabilities and sustainable exploitation levels.","context":["- Andrea Dell’Apa\n- Jeffrey C. Johnson\n- David G. Kimmel\n- Roger A. Rulifson\n- Jeffrey C. Johnson, Ph.D\n- UF Department of Anthropology\nThe management of the spiny dogﬁsh (Squalus acanthias) is a matter of international concern, as this species was a candidate for inclusion in lists for trade regulation. The major demand for its meat is from the European Union (EU) market, with the U.S. and Canada as two major contributors. The U.S. has yet to support a spiny dogﬁsh listing, although the U.S. Atlantic stock (including Florida) is under a Fshery Management Plan (FMP) that proved to be successful in providing a certiﬁed sustainable ﬁshery.\nThis study employed a cumulative sum technique to compare trade data for frozen spiny dogﬁsh export from the U.S. and Canada to the EU in relation to the FMP adoption. The study also constructed a social network to visualize changes in the European trade scenario for spiny dogﬁsh after adoption of the FMP and to predict future trade ﬂow potentially affecting the conservation status of regional dogﬁsh stocks in relation to recent management measures introduced in Europe.\nThe social network analysis revealed that the exclusion of spiny dogﬁsh from trade regulation lists eventually will affect the conservation status of dogﬁsh stocks in Africa, Asia, South America, and the Mediterranean and Black Seas. Study results suggest that the species listing would provide an economic beneﬁt for the U.S. Northwest Atlantic ﬁshery, and will eventually foster the conservation status of other regional stocks worldwide and the search for a more sustainable global exploitation of spiny dogﬁsh.\nOne-mode network for the pre-FMP period (1990 – 1999) in Fig a, and for the post-FMP period (2000 – 2010) in Fig. b. Nodes represent countries and edges represent link-relationships between countries based on quantities of dogﬁsh exported, with arrows pointing from the exporter to the importer country. Countries are displayed based on their geographical location: North America (red ellipse), Central America (red rectangle), South America (red triangle), Africa (brown ellipse), Asia (yellow rectangle in Fig.a and yellow polygon in Fig. b), Europe (blue polygon), and Oceania (green triangle).\nThe adoption of the U.S.-FMP for the Northwest Atlantic spiny dogﬁsh stock corresponds to signiﬁcant changes in the species international trade. As a direct result, Canada increased its dogﬁsh exports to the EU market appreciably, while U.S. exports declined because of planned management quota reductions. In light of the effectiveness of the US-FMP in achieving sustainability for the Northwest Atlantic spiny dogﬁsh stock, and given the current state of the international exploitation and trade, global and local conservation status, the U.S. government would reap economic beneﬁt from the species inclusion in Convention on International Trade in Endangered Species (CITES’s Appendix II).\nThe network analysis also indicated that new areas increased exploitations to supply the EU market demand as U.S. exports declined, potentially affecting the conservation status of regional and local spiny dogﬁsh stocks in African, Asian and South American coastal areas. Although there is no directed ﬁshery for spiny dogﬁsh off South Africa, our results, and available information on the species biology and management regulations introduced in the Northeast Atlantic, suggest that the South African-Namibian coastal area may be a potential ﬁshing ground for dogﬁsh in the future. This ﬁshery should be considered for the employment of a management strategy prior to exploitation to ensure the ﬁshery is sustainable and will help preventing the species overexploitation.\nConsidering both the reported and forecasted increased exploitation of spiny dogﬁsh stocks in the Mediterranean and Black Seas (e.g. Spain, Romania, and Bulgaria), awareness in the conservation status of these spiny dogﬁsh stocks is also needed in order to encourage the introduction of conservation measures in this area, which is under the authority of the EU ﬁshery management but lacking behind in terms of spiny dogﬁsh conservation measures.\nEffective and successful management systems are based on ﬁnding the best trade-off between contrasting biological, socio- economic, and political objectives. The case for managing the international trade of spiny dogﬁsh shows that a major goal for managers should be to aim at integrating all these different aspects to effectively contributing to the analysis of risks related to global exploitation.\nThis study indicated that the employment of new analytical techniques, such as social network analysis of available trade data, can be useful in the discussion for implementing the ﬁshery management and international biodiversity protection.\nDell’Apa, A, J.C. Johnson, D. G. Kimmel, R.A. Rulifson. The international trade and fishery management of spiny dogfish: A social network approach. Ocean and Coastal Management 80 (2013): 65-72.\nACKNOWLEDGEMENT: Photo from Andrea Dell’Apa, artwork by Elaine A. Porter, editing by Joeffery Ibisagba","There’s a bit of a pardox of public opinion regarding the effectiveness of spiny dogfish as predators. Depend on who you ask, they’re either forming a swimming wall of teeth annihilating everything in their path or they’re weak scavengers, poor excuses for sharks. This much-maligned species gets the double-whammy of both being a pest and being unworthy of respect as a predator. However, science (a lot of it coming out of the Wilga lab at my undergrad alma mater) seems to back up the “formidable predator” side of the argument, and these little sharks earn their place in the marine food web through a number of neat adaptations. For this installment of Perfect Little Killing Machines, we’re going to look at the most obvious adaptation for a predatory lifestyle: the jaws.\nFirst off, let’s get a “last thing a herring ever sees” view of the business end of a spiny dogfish.\nThe image above (from my Master’s degree field work aboard the NOAA R/V Henry B. Bigelow) shows a lot of the tools a dogfish uses to snag, chop up, and swallow its prey. There are several nifty features of the spiny dogfish jaw that reflect the almost absurdly generalist feeding strategy of this species, and what it doesn’t accomplish through physical adaptations it can make up for through behavior.\nIn fishes it is typically thought that there are three main forms of feeding: ram feeding (in which prey is run down and swallowed whole), suction (in which prey is inhaled), and biting (in which larger prey is chopped up into tiny bits). Most fish use a combination of these, and spiny dogfish may make use of all three in the capture and consumption of any given prey item. Basically, a dogfish is physically capable of taking down and consume any available prey in its path.\nFirst off, the jaw itself has evolved a combination of features found in other, more specialized sharks. When comparing the skeletal and muscular structure of the jaw to other elasmobranchs, the spiny dogfish almost seems as if it was cobbled together from pieces of other species. Work by Ramsay (2012) shows that the jaw of the spiny dogfish incorporates structures found in both the whitespotted bamboo shark (a primarily suction-feeding species) and the sandbar shark (which primarily relies on biting prey). The combination of these structures allows spiny dogfish to open their jaws rapidly enough to generate low pressure within the mouth and suck prey in, while still giving their jaws enough stability to reliably hold onto prey as they bite into it. In addition, the labial cartilages (I’ll let you go ahead an snicker at the name now), which can be seen on the sides of the dogfish mouth in the above image, snap forward as the dogfish opens its mouth, creating a tubular shape to the open mouth that isn’t seen in purely bite-oriented species (Wilga and Motta 1998). This allows for a narrower opening to force water through when slurping up prey. Not to be outdone in the ram-feeding department, in spiny dogfish the upper jaw is attached to the skull in a way that the front portion “hangs” from the skull, attached primarily by the muscle responsible for projecting the jaw forward. This allows dogfish to project their jaws an impressive 30% of their head length as they bite, which is advantageous when running down elusive prey like schooling fishes and squid (Wilga and Motta 1998).\nHowever, being generalists means that spiny dogfish aren’t necessarily optimized for either suction or biting, and there are some trade-offs. Balaban (2013) found that the structures relating to gape size in spiny dogfish were smaller than in more ‘bitey” species like sandbar sharks and smooth dogfish, meaning dogfish can’t open their mouths quite wide enough to get their jaws around larger prey. To compensate for this, the oropharyngeal cavity (basically the “throat” of the dogfish around where the gills are) expands more than in other suction feeding sharks to allow larger prey to be swallowed, but this comes at a cost of increasing the time needed to generate pressure (Wilga et al. 2012). However, Wilga et al. (2012) also noted that unlike other suction-feeding sharks, spiny dogfish attack on the run and usually don’t stop moving during the initial strike, which means ram-feeding behavior may compensate for their suction-feeding shortcomings.\nSpiny dogfish incorporate their jack-of-all-trades feeding style by tailoring their attack and prey processing behavior to specific prey items. As Wilga and Motta (1998) found, ram, suction, and bite-feeding all play a role in the process of a dogfish attack.\nThe above image shows the four main stages of feeding identified by Wilga and Motta (1998). Column A is the initial prey capture, which is accomplished with a combination of ram and suction feeding. Column B shows manipulation by biting as the dogfish essentially tries to orient the prey for easy swallowing. In column C the dogfish has successfully captured and manipulated its prey and is now swallowing it via suction. Finally column D shows the head shaking behavior typical of many sharks. This behavior drags the prey back and forth across the shark’s teeth. Spiny dogfish teeth, by the way, are arranged in interlocking rows that form a uniform cutting surface along the jaws. This unique arrangement allows dogfish to literally saw apart prey that would otherwise be too large to swallow. For really large prey (in some cases larger than the shark itself) a dogfish may be capable of latching on through a combination of biting and suction and then can use head-shaking to saw chunks off for easy consumption. If that isn’t enough crazy feeding mechanics for you, spiny dogfish are also capable of moving the muscles on either side of their jaws independently, meaning they can choose which side of their jaws are biting harder and may actually adjust their bites to maximize the sawing action of their head-shaking behavior (Gerry et al. 2008).\nWhenever the feeding behavior of sharks is discussed, sooner or later the issue of bite force comes up. Pure bite force may be the only aspect of feeding behavior where spiny dogfish fall behind: Huber and Motta (2004) found that spiny dogfish bite with a fairly weak force of 20 Newtons (N). This is greater than that of a bluehead wrasse (5 N), but below that of a lab rat (50 N), actual dog (550 N), human (680 N), silky shark (889 N), and American alligator (a bone-crushing 13300 N). This is not, however, and indication that the spiny dogfish is a weak predator. As Wilga and Motta (1998) showed, biting in the dogfish is more for the purpose of capture and manipulation while the actual cutting is accomplished by way of the head shaking behavior. This makes the spiny dogfish well-adapted for a generalist fish-eating diet, and is apparently sufficient to allow these small sharks to attack and consume relatively large fish.\nSo to summarize, the dogfish makes use of all three of the main forms of feeding, and because of this is the ultimate generalist feeder in the shark world. If they slack off in any department it’s in sheer bite force, but this is mitigated by both sharp teeth and behavior. From a purely biomechanical perspective alone these are pretty impressive predators, so when handling dogfish you should probably keep your hands clear of the mouth.\nThis post is an updated version of this other post from way back in the first year of the blog’s existence. It has been updated with new information and some additional editing has been done because, frankly, I thought it would read better.\nBalaban J (2013). The morphology and biomechanics of jaw structures in chondrichthyes. Master’s Thesis, University of Rhode Island: Link\nGerry SP, Ramsay JB, Dean MN, & Wilga CD (2008). Evolution of asynchronous motor activity in paired muscles: effects of ecology, morphology, and phylogeny. Integrative and Comarative Biology, 48 (2), 272-282 DOI: 10.1093/icb/icn055\nHuber DR, & Motta PJ (2004). Comparative analysis of methods for determining bite force in the spiny dogfish Squalus acanthias. Journal of Experimental Zoology. Part A, Comparative Experimental Biology, 301 (1), 26-37 PMID: 14695686\nRamsay JB (2012). A comparative investigation of cranial morphology, mechanics, and muscle function in suction and bite feeding sharks. PhD Dissertation, University of Rhode Island: 1081470345\nWilga C, & Motta P (1998). Conservation and variation in the feeding mechanism of the spiny dogfish Squalus acanthias. The Journal of Experimental Biology, 201 (Pt 9), 1345-58 PMID: 9547315\nWilga C, Stoehr AA, Duquette DC, & Allen RM (2012). Functional ecology of feeding in elasmobranchs. Environmental Biology of Fishes, 95, 155-167 DOI: 10.1007/s10641-011-9781-7"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:b7963bad-d637-424c-a9e2-d46d9331635a>","<urn:uuid:e7526f4f-53f6-4410-a8b5-2927d8fbbed0>"],"error":null}
{"question":"What's the relationship between fast break execution and zone defense positioning, and how do mental factors affect both?","answer":"In fast breaks, the goal is to produce not just quick shots but good shots, with players required to take wide-open shots when available. However, mental factors like fear can cause players to pass up these opportunities, turning good plays into bad ones by dribbling into traffic. On the defensive side, zone formations like the 2-3 (two players up top, three on bottom) and 1-3-1 (one top, three middle, one bottom) are designed to contain offensive movement and protect specific areas. The effectiveness of both offensive and defensive plays depends on mental approach - successful players must overcome fear and hesitation while making split-second decisions.","context":["The idea of the fast break is not only to produce a QUICK shot but also a GOOD shot. That concept means players, if they have a wide-open shot, must TAKE THE SHOT. That sounds almost too basic. One might say, “Well, of course, they are going to take the wide-open shot.” Not always. All too often, you see a player … WIDE OPEN at 22′ from the basket … pass up that shot to use one or two dribblers toward to the basket to supposedly get a ‘better shot.’ The logic says a closer shot will be an easier shot.\nHere’s the deal on that: THERE IS NO BETTER SHOT THAN A WIDE-OPEN SHOT. Here’s what happens when that player dribbles in to get a closer shot: (a) the defense comes out and covers him and, instead of having a wide-open shot, he now has a closely guarded shot and he misses; (b) he gets himself in a traffic jam and winds up losing the ball on an infraction, a turnover or a charging foul. Every time I see this when doing games on TV, I say, “He turned a good play into a bad play.”\nThere is something else the matter with passing up a good shot … FEAR. The player that does this is thinking (my interpretation … and I am not alone in this): “Oh, if I miss this wide-open shot, people will say I choked on it. So, I’ll dribble in and get a more difficult shot. That way, if I miss, I can say it was a more difficult shot or that I was fouled.” Well, here is MY take on that player: he’s afraid. He’s afraid of the responsibility. He’s afraid of missing. He’s afraid of … facing up to his own fears.\nHere is something I say all the time on TV, as I did as a coach: “Shooting is not just a technical gesture, it’s also a COMPETITIVE gesture. How times have we seen the guy with the perfect shooting technique, but without a perfect mental approach, miss a clutch shot? Lots of times. And, how many times have we seen a guy with a defective shooting technique, but with a heart bigger than a house, hit a clutch shot? Lots of times. Give me the guy with heart over the guy with flawless technique every time.\nI had one of those guys with Olympia Milan: 6’6” Roberto Premier. Early in the 1986-87 season, we were almost knocked out of the European Cup. We’d lost by -31 vs. ARIS in Thessaloniki, Greece. The next week, we had to win at home by +32. Late in the game, we’d gotten back 29 and Mike D’Antoni passed over to Roberto Premier on a trailing situation on the break. Premier, with so-so-technique, went up for the 3-point shot and drilled it to put us at +32 … and we won by +34. Premier had a heart bigger than any house.\nI would take a guy out of a game if he passed up a wide-open shot. I was usually very diplomatic with my players but, on those occasions, I would say, “Why did you pass up that open shot? Tell me, straight out: Are you afraid to take that shot?” Of course, that angered them no end but they would never pass up another good shot. The Rule: If you are open, shoot; if you are covered, pass. Another rule: Shooting is a competitive gesture.","Defense In Basketball\nTracy Murray (12-Year NBA Veteran) gives expert video advice on: What is a 'zone defense' in basketball?; What is a 'man-to-man' defense in basketball?; What is a 'box and one' defense? and more...\nWhat is a 'zone defense' in basketball?\nA zone defense is the opposite of a man-to-man system. You play an area. You guard an area. If the ball is in your area, you have that man in your area. And a zone is usually geared to keep the ball out of the paint. And sometimes it works and sometimes it doesn't, depending on the movement and how hard you're working in the zone.\nWhat is a 'man-to-man' defense in basketball?\nA man-to-man defense is basically guarding your man. You have your best defender on the best offensive player, and everybody else match up and guard somebody that they can guard.\nWhat is a 'box and one' defense?\nA box and one defense is a combination of a zone and a man-to-man. You have your best defender on the best offensive player, and you will zone up in a box formation. The rest of the guys will zone up. And that's a box and one.\nWhat is a 'triangle and two' defense?\nA triangle and two will be your two best defender's either chasing one guy or chasing the best two offensive player's outside man to man with them, and the other 3 players will be in a triangle, one man up top and two on the bottom.\nWhen should I switch off my player in a 'man-to-man' defense?\nYou only switch off your player in a 'man-to-man' defense, only if you communicate with your teammate. Off of a screen, or if you get beat, and your teammate that you communicated with is switching off with you, \"Help! Help! Help! Switch!\" That's when you switch, otherwise you stay with your man.\nWhat does it mean to 'deny the ball' in basketball?\nTo deny the ball or to deny the wings is when you overplay the wings that are coming out to catch the ball even by that's coming to catch the ball, that's the next pass. That's usually high pressure defenses, they don't want the ball to advance and normally, in order to beat that is the back door pass that definitely can get you beat. So, if you deny the wings, you don't want to be at the back door.\nWhat is a '2-3 zone defense' in basketball?\nA two three zone is when you stick two people up top and three on the bottom. That means you have two people maybe free-throw line, maybe three-point line extended depending on if the other team is a great three point shooting team or not. The three guys on the bottom are, two guys on the ends are going to cover the corners and the guy in the middle is going to cover the paint.\nWhat is a '1-3-1 zone defense' in basketball?\nA 1-3-1, you have one guy at the top, three in the middle, one on the bottom. Usually the one on the bottom is running from corner to corner, the guy on top is containing everything at the top, and the guys in the middle are doing different rotations in formation of a 2-3 or a 3-2, depending on the formation or where the guys are with the ball.\nWhat is a 'matchup zone defense' in basketball?\nA matchup zone defense is like a man to man but you're in a zone. When the ball is in your area then you pressure man to man, like that's your man. That's who you're guarding. The rest of the guys are zoning up behind you."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:bc485d25-55fd-4714-b8f8-66ce6c4726e3>","<urn:uuid:b67ceca3-fdf6-4e1d-9ff6-5f61908a66d7>"],"error":null}
{"question":"What's the most effective power distribution strategy: rack circuit failover or modular UPS systems?","answer":"While rack circuit failover can face limitations due to load capacity constraints and potential failure when balancing between circuits, modular UPS systems offer superior power distribution capabilities. Traditional rack circuit failover can fail when the combined load capacity exceeds a single circuit's limit, potentially causing loss of entire rack sets. In contrast, modular UPS systems provide greater flexibility with 'plug and play' power modules, allowing both vertical and horizontal expansion. Modern UPS systems also achieve higher efficiency (up to 96.5% peak efficiency) and include self-diagnosis mechanisms for preventive maintenance, making them more reliable for power distribution in data centers.","context":["Capping of Power Consumption in Data Centers through Load Shedding\nLoad shedding is a means of distributing electrical power demand across multiple power sources. When the demand for power is greater than what the primary power source can supply, load shedding helps to relieve stress on the primary power source. Every wish of an organization is to reduce power consumption while ensuring uninterrupted normal operations.\nMost buildings including data centers Toronto, purchase power from a utility provider. Needless to say, data centers are power gluttons. This is because of the various functions that they perform from the explosion of digital content, big data, e-commerce and internet traffic which all contribute to making them the fastest growing power consumers in the world. At this stage, there may need to embark on load shedding as a data center solution of power consumption reduction. Here, the data center operator may negotiate an agreement with the power provider to voluntarily load shed on a pre-schedule or on-demand basis. Since all buildings have a secondary power source, when there is a load shedding the building sources power from the secondary such as on-site diesel generators, wind-based renewable power or even contracted solar photovoltaic.\nWith such a load shedding agreement, energy-intensive data centers may opt to load shed during peak usage periods. However, this may cause disruptions to the data center systems. To avoid this, data centers can install uninterruptible power supply systems and power distribution units that will always moderate the flow of power to data center sensitive equipment. Also, make sure that you have a high-quality power distribution control from secondary sources.\nAs now perceived, load shedding can be the solution to many data center problems that are power related. Rack circuit-failover failure is one of the most common problems that load shedding could provide a solution to. There is no need to put a hard power capacity limit on a rack or set of racks to ensure failover success.\nPDUs are made in such a way that they have limits to the load capacity of a single device. Others may prefer to call them ‘floor PDU circuit breakers’. They work just like the normal circuit breakers. But when it comes to the capacity limit of a rack, it may be way much and exceed the limit of a single rack circuit. What does this mean?\nEach circuit is always under a specific load capacity of its own. In some cases, you may be forced to load balance between two or more circuits by adding the loads together. This may happen because of loss of one failover or circuit. When load balancing happens, the capacity of the two circuits may exceed the capacity of one of those circuits. When this happens, the failover will not succeed and in most cases, you could lose the whole rack or set of racks that were being connected to load balance.\nOne way that is very effective in ensuring that you do not exceed the capacity of a single rack or any set of racks is load shedding. When such cases arise, the outlets on a PDU will just be turned off thus disabling their usage.","Data center power and cooling strategies for increasing rack power density\nIn recent years, data centers have faced many challenges due to rapid changes in IT technologies and trends. High density data centers have become an issue that enterprises must face.\nAs people have stricter demands for internet services, enterprises must think of ways to satisfy user expectations for 24x7 availability. The number of servers keeps increasing while more computing capabilities and resources are incorporated in smaller and smaller cases. Small Form Factor (SFF), Blade Servers to Hyperscale servers to Composable Infrastructure used to achieve more flexible frames, are all being developed in response to this demand.\nHigher density servers mean higher power consumption is needed for each server. As the world focuses more and more on energy consumption, enterprises have started to treat energy efficiency issues and environmental responsibilities more seriously. Virtualization technology offers solutions for enterprises that allow each server to handle a greater workload so they are utilized to their fullest. At the current stage of the global cloud trend, many enterprises already see “Cloud First” as one of their top strategies. More IT budgets are investing in “cloud” related IT infrastructure and software and some enterprise organizations or IT suppliers even use “Cloud Only” as their main strategy. When servers are densely placed in cloud data centers, it will definitely bring greater maintenance challenges.\nSince the development of mobilization and social media platforms, the data required for massive computing power to analyze and extract has increased rapidly. With the continued development of the Internet of Things (IoT), data collected through sensors will be guided to backend data centers to perform big data analysis. These changes are leading data centers to develop towards high density. With the rapid growth in the density of data center equipment, data centers built along traditional concepts are no longer enough.\nPotential issues from high power density\nAccording to a research report by Colocation America (2014), the power density of a single rack cabinet in data centers was approximately 6kW in 2008, which reached 12kW in 2016. It is estimated that by 2020, the power density of a single rack cabinet in data centers will achieve 16.5kW. For example, when Intel retrofitted two foundries into a green data center with high power density, the power density per rack reached as high as 43kW.\nSource: Colocation America, 2014\nWith the rapid growth of power density per rack in data centers as a leading trend, enterprises must find more effective ways to face the challenges that come with it. For example, high density achieves better space utilization and the response time of system failure is reduced significantly. However, once there is a power failure, the large amounts of heat generated by the equipment cannot be extracted and will result in a server shutdown.\nThe ever-increasing power density has also far exceeded the processing capabilities of most old facilities. In previous years, each rack in a data center was designed for 6kW power density. However, when faced with high density racks of 15kW or above, facilities clearly do not meet requirements. When enterprises use technology that requires massive resources such as cloud computing or big data analysis, they also face expansion problems for the difference between available and needed capacities. In the past data center cooling design assumed that the IT work load was even and well distributed, but the actual operating environment was not so, especially in certain high density rack cabinets. Enterprises are realizing that their cooling capacities are seriously insufficient.\nThe backup power mechanism originally designed for data centers may also disappear due to this deficiency. The original UPS and cooling system designed using N+1 configuration will be forced to become fully operational due to insufficient capacities, and lose their backup functions. In addition, after deploying virtualization solutions, IT staffs can also move virtual machines dynamically. Data center loads will also change due to this and hot spots will become elusive. Power requirements will also change and result in unnecessary shutdowns.\nThe emerging modularization provides higher flexibility\nHigh density data centers still have many potential problems as administrators of data centers face greater pressure. In addition to maintaining an increasingly dense computing environment and improving its availability, they must also reduce cost and increase efficiency. Fortunately, industry professionals are integrating the modularization concept into equipment and products designed for data centers to bring greater flexibility and prepare for future workloads.\nAt the current stage, the main modularization concepts are applied to space and facility designs. Space modularization refers to the use of modules from IT infrastructure, rack cabinets and facilities provided for IT equipment to operate. Each depends on and relates to the other. In practice, the data center spaces at enterprises are used to assess the capacity needs of existing services and future expansion considerations, and are further divided into smaller spaces and viewed as modules. The modularization of facilities refers to the use of modular designs for infrastructure, including power systems such as UPSs, power distribution cabinets, in-row cooling, server racks and cold/hot-aisle containment.\n“The development of IT technology is changing every day, and infrastructure is developing towards ‘microservice’ architectures. Simply put, ‘microservice’ architecture refers to the use of modularization to form complicated large-scale applications. Modular solutions were developed for data center infrastructure years ago to provide flexibility for enterprises responding to expansion needs and to overcome power and cooling insufficiency and space challenges,” said Dr. Charles Tsai, the general manager of Delta’s mission critical infrastructure solutions business unit.\nThe modularization of the UPS system\nAccording to a research conducted by a UPS vendor, approximately 50% of respondents believe that the main cause of power outage at data centers is UPS equipment failure. This shows how important UPS systems are for maintaining data centers. As data centers acquire higher densities, they must replace old UPS systems with efficiency as a major consideration. UPS systems designed ten years ago usually have an efficiency of 85% when operating at 40% load to serve dual power input servers. The energy efficiency of current UPS systems is even greater. Take Delta’s UPS solution for example, under a light load of 20%, the AC-AC efficiency for the DPH 500kVA series UPS can be around 95% and the peak efficiency can be up to 96.5% for obvious energy cost savings.\nAnother consideration is effective space utilization in the data centers. Generally speaking, power rooms plan to install power facilities even though the space of the power room may be very limited. When data centers develop towards higher density in each rack cabinet, backup power must also increase accordingly. Enterprises can replace their legacy UPSs by new generation units with a higher power capacity. For example, Delta recently released the Modulon DPH 500 kVA modular UPS for large data centers that only takes up a space of a 19” rack cabinet and provides the world’s highest power density. The parallel expansion can also be configured up to 8 units, providing a maximum power capacity of 4MVA.\nSince it has self-diagnosis and aging detection mechanisms, it can detect the health of batteries, fan, IGBT module, DC capacitors and AC capacitors for preventive maintenance to reduce the risk of malfunctions and power loss, and protect the customer’s equipment investment.\nFor enterprises, another advantage of modular UPSs is the “plug and play” design of power modules. Either vertical (within a single system cabinet) or horizontal (in parallel) expansion can be achieved per enterprise needs. Enterprises can flexibly purchase UPSs according to their initial capacity needs and count on future operational expansion to further lower CAPEX.\nRowCool systems near hot spots reduce power losses\nAs server and IT equipment densities become higher, the requirements for facilities are also becoming more rigorous. In addition to higher power supply density, cooling has become an issue in data centers. The cooling design of data centers assumes that the IT work load is even and well distributed, but in real enterprise environments, uneven heat distribution may be generated due to dynamic moving of virtual machines or improper deployment of IT equipment.\nInsufficient cooling will become a common challenge that high-density data centers will face. The advantage of RowCool is that it is close to hot spots, and is different from RoomCool systems where losses are generated in the air delivery path under the raised floor. RowCool systems can provide sufficient cooling capacity nearby to eliminate hot spots. They are equipped with high power-saving DC or EC fans with variable fan speed control for more energy savings. A 10% fan speed reduction can save a maximum 27% of energy consumption. In addition, the N+1 backup design is used for the overall architecture and group control functions that are provided to perform linked control for the RowCool units in the area to solve the hot spot problems caused by sudden load increases.\nRowCool systems also have modular designs. For example, RowCool 29/43kW not only has built-in dual power supplies and can significantly increase the reliability and protection for system power; but it also supports hot-swappable power supplies and fans, and can reduce the maintenance time needed. Its variable fan speed control design can adjust fan speed according to the actual work load. The modularization concept is also applied to RowCool units, which can be added to required spots as needed.\nCase Study: Medium and High Density Zones co-exist in a data center\nAccording to the data center density categories of AFCOM, the data center managers' association, a low density environment is when each cabinet is under 4kW, mid density is 5kW-8kW, high density is 9kW-15kW and ultra-high density is 16kW and above. Different data centers may face different rack power density challenges. In some data centers, there might even be a mix of rack cabinets with different densities, but use the same design concepts for power and cooling systems. A leading IC design company in Taiwan plans to build a new data center at their headquarters because they need to integrate their existing IT equipment and related network and operating environments. According to onsite inspection, there are up to 80 mid-density and ultra-high density rack cabinets in this enterprise’s data center, with the ultra-high density cabinets up to 25kW.\nDelta recommends that when enterprises need both low-density and high-density/ ultra-high density rack designs for different application needs, it should divide the data center space into a high-density cabinets area and an ultra-high density cabinets area to optimize its design. Facility planning should not only include overall power and cooling needs, but also space usage for the deployment of IT systems. There are different options for the use of cooling solutions. For example, rack cabinets under 4kW may only need RoomCool for effective cooling, while it is better to equip RowCool units for higher density racks from 9kW to 15kW for optimal heat removal.\nIn this case, Delta used the modular design concept to plan the ultra-high density area for its client, and placed all of the ultra-high density rack cabinets in one zone. For its high density applications, the client chose Delta’s RowCool 95kW with the industry largest cooling capability within a 600mm wide cabinet. Delta also recommended the use of hot-aisle containment technology to prevent the mixing of cold and hot air and ensure optimal cooling efficiency.\nTechnology is developing every day. Emerging IoT, artificial intelligence, AR/VR has integrated cloud, mobile, social media and big data technologies. For enterprises, the challenges for data centers have become more and more difficult. Development towards high-density will definitely continue and infrastructure will become the most important backup when enterprises promote their innovative application services.\nFor data center administrators the development of high-density data centers will definitely bring more maintenance and management issues. The availability of data centers will also be a challenge, while costs must decrease and efficiency must increase. The use of modular designs and related facilities, including space modularization, cold or hot aisle containment technology, RowCool systems and more efficient UPSs, can ensure higher reliability architectures and more deployment flexibility for data centers.\nHow Do I Deploy the Best Back-up Power and Environment Monitoring & Management Systems in a Datacenter?"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:30ca391a-7170-4c06-ba1b-a50dbca27c52>","<urn:uuid:d0fd43e9-4a48-4be7-aa16-1fa00b2dfc77>"],"error":null}
{"question":"What role do central heroic figures play in advancing the narratives of the Epic of Gilgamesh's flood story and the Sundara Kandam?","answer":"In the Epic of Gilgamesh, Utnapishtim serves as a wise figure who saves humanity by following divine instructions to build a boat and preserve all living things, ultimately being rewarded with eternal life. In the Sundara Kandam, Hanuman serves as the central heroic figure, described as extremely handsome and powerful ('like a Golden Mountain'), who undertakes difficult missions including flying across the ocean to Lanka, searching for Sita, battling Ravana's forces, and setting Lanka ablaze. While both characters serve divine purposes, Hanuman's role is more active and martial, focusing on rescue and combat, while Utnapishtim's role centers on preservation and survival.","context":["Article by Sri Sadagopan Iyengar Swami, Coimbatore\nCountless books have been authored since creation, by countless worthies, dealing with practically every subject under the Sun. And to each of these books, its author, with parental pride, has given a name, which he or she thought suited it best. To each author, his creation, however insignificant it may appear to others, is indeed a magnum opus deserving of appreciation and acclaim. However, none has considered his or her book fit enough to be called “The Beautiful Book”. You must have read any number of books—have you come across one titled simply “The Beautiful Book” There is indeed one, for which the venerated author couldn’t think of any other name than the beautiful one mentioned above. Looked at from any angle, it appeared so fine and striking to him, that try as he might, he couldn’t come up with any other title, which suited the work to a “T”. Irrespective of what impressive names we might give our children, it is their character, conduct and comportment that ultimately give their names popular acceptance—if a dunce carries the name “VivEkAnandan”, in course of time, people would coin for him nickname which indicates his true colours, rather than call him by the given name. So too is the case of any name. That the author-given name of “The Beautiful Book” (for the work we talked about) has gained popular acceptance too, is indicative of the book’s enchanting and enticing contents.\nThe “Sundara KANdam” of Srimad Ramayanam carries an extremely apt name. Sri Valmiki himself was so impressed by its contents that he could think of no other name for it than the aforesaid. While other KANdAs of the epic have been named after the places where the events are enacted (AyOdhyA KANdam, AraNya KAndam, KishkindhA KANdam) or after the nature of happenings (BAla KANdam, Yuddha KANdam), this particular portion of the saga is simply named “Sundara KANdam”.\nDoes this mean that the other parts of the epic are not beautiful? Why should this particular KANdam be celebrated as the Sundara KANdam?\nWhy does this specific KANdam command much greater popularity among Rama bhaktAs than the other parts of the great saga? And if the recital of this particular KANdam is traditionally held to be capable of fulfilling all of one’s wishes, what could be the special content that it has, which puts it head and shoulders above the other portions of the epic?\nWhile there are several reasons for this part of Valmiki’s magnum opus getting this “beautiful” sobriquet, the principal ones are the following:-\n1. This part of the Epic really contains enthralling descriptions of several beautiful objects, places and people. Be it the portrayal of Lanka, of the exalted Pushpaka vimAnam, the AshOka vanikA, the Madhu vanam or of natural phenomena like the Moonrise, Sri Valmiki excels himself in painting gripping pen-portraits, with elaborate and enthralling accounts of flora, fauna, the majesty of nature at its best, the lure and lustre of Lanka etc. Being a past master at apt similes, the Maharshi’s portrayals often appear to be more glorious than the objects of their adoration, if such a thing were possible. Here is a simple sample from the 5th Sarga of this KANda, devoted mainly to the description of the rising Moon over the city of Lanka. Observe the lilting metre, the thrilling portrayal, the extremely apt similes and the cumulative effect of unparalleled prettiness and picturesqueness the entire scenario presents—\n“Tata: sa madhyam gatam amsumantam\nJyOtsnA vitAnam mahat udvamantam\nDadarsa dheemAn divi BhAnumantam\nGoshttE vrisham mattham iva bhramantam”\nThe huge Silver Orb in the sky, surrounded by a halo of pleasing luminance, occupying the centre spot among an admiring audience of countless stars exhibiting their adulation through constant twinkling, looked like a virile Bull pacing majestically among admiring cows, says Sri Valmiki.\nHere is a slew of more similes from the facile pen of the Adikavi, comparing the resplendent Moon glowing in the bewitching night sky, to a graceful Swan in a silver cage, to a majestic lion inhabiting a broad cave in the Mandara mountain and to a victorious centurion ensconced on an imposing elephant—\n“HamsO yathA rAjata panjarasttha: simhO yathA Mandara kandarasttha:\nVeerO yathA garvita kunjarasttha: ChandrOpi babhrAja tathA ambarasttha:”\nWhat beautiful metre, what incomparable similes, what inimitable portrayals! Is it any wonder that this forms part of the “Sundara KANdam”? Though it is extremely tempting to quote one lilting verse after another, which competes for our attention and adulation, I leave readers to feast themselves on these gems at their leisure.\nHowever, is it merely due to the gloriously graphic descriptions that this KANdam is known as the Sundara Kandam, for, equally facile portrayals are to be found in other parts of the epic too, especially the accounts of Spring, Autumn and the Monsoon seasons and innumerable other pen-portraits, of which only Sri Valmiki is capable.\n2. This Kandam is almost exclusively about the exploits of an extremely beautiful person (“Sundara:”)—none other than Sri Hanuman. It is beyond dispute that Sri AnjanEya is extremely handsome, virile and strikingly attractive. “KAnchanAdri kamanIya vigraham” says the PArAyaNa slOka, telling us that the VAnara veera possessed glorious good looks and shone like a veritable Golden Mountain, with his gigantic physique and commanding personality. When we go through Sundara KANdam, we find it to be one long and continuous saga of Sri Hanuman’s adventures and exploits, in the service of His Lord and Master, Sri Rama. The KANda begins with a vivid account of Hanuman’s launch into the long, difficult and hurdle-filled flight across the ocean to Lanka, a feat none else would even contemplate, leave alone attempt. What follow are enthralling accounts of the MahAkapi’s rigorous search for Sri Sita, his timely arrival on the scene to prevent Sri Mythily from taking Her life and to extend assurances of imminent rescue by Sri Raghava, his vanquishing the minions, ministers and sons of Ravana in a patently unequal battle, his challenge to the demoniac king and departure to Kishkindha to convey the good news to the Prince of Ayodhya, leaving Lanka engulfed in flames.\nThus, from commencement to conclusion, this KANda is verily monopolised by the “Sundara”, Sri Hanuman, who is on centre stage, throughout.\n3. What could be more beautiful than tales of the Lord and accounts of His handsomeness, valour, majesty, generosity and boundless compassion? Anything incorporating such a narrative definitely deserves the name “Beautiful Book”. The Sundara Kanda contains several such descriptions of Sri Raghava’s glorious physical and psychological traits. The 31st Sarga contains a detailed account of the IkshvAku Vamsam and Sri Rama’s glorious guNAs, the 35th a mouth-watering description of His matchless physical beauty, the 51st again a narrative of the Prince’s valour, prowess and compassion. With the recurring theme of Bhagavat divya mangaLa vigaraha varNanam and Bhagavat guNa anubhavam, there is little wonder that this portion of the epic is known as the Sundara Kanda.\n4. Whether it be in physique or in character, could anyone hope to equal Sri Mythily, the epitome of womanly virtue and seemliness?\nShe is thus a real “Sundari”, beautiful beyond imagination and looking as if She was put together by the Divine Architect, using up all the stock of beauty on hand—“DEva mAyEva nirmitA”. It was a divine beauty, such as only the Divine Consort could possess. If we come to think of it, this Kandam devotes a major portion of its content to this “Sundari”, Her travails amidst the rAkshasIs of AshOka vanikA, Her encounters with the dastardly Ravana, Her compassionate advice even to Her abductor to perform Sharanagati at the Lord’s lotus feet, Her oscillation between hope and despair, Her meeting with Maruti and the consequent reassurance gained about imminent rescue by Her beloved and Her prayers for the safe return of Sri Hanuman to KishkindhA. Though the entire Srimad Ramayana is but a saga of Sita (“SItAyA’s charitam mahat”), yet the Sundara Kanda brings out the depth of Piratti’s character in all its glory, the glory of gold, which glitters all the more, when Passed through fire.\nTo conclude, we can’t do better than to enjoy a verse of one of the erudite commentators on Srimad Ramayana, viz., Tilaka, who, instead of enumerating the beautiful things in this Kandam, queries us as to what is there in the Sundara Kanda, which is not Sundaram (beautiful). Everything about this Kanda is indeed beautiful—\n1)the city of Lanka, which forms the picturesque backdrop for the momentous events of this Kanda,\n2)the magnificent story narrated in the Kanda, with its numerous twists and turns, affording full scope for display of the nava rasAs, 3) the incredible beauty of Sri Janaki Devi, who forms the focal point of the narrative—\nindeed everything about the Sundara Kandam is beautiful beyond depiction. Here is the beautiful verse from the “Tilakam”, a commentary on the great epic—\n“SundarE sundarI LankA, sundarE SundarI kathA\nSundarE sundarI SitA sundarE kim na sundaram\nSundarE SundarIm SitAm akshatAm MarutE: mukhAt\nShrutvA hrishta: tathaivAstu sa Rama: satatam hridi”.\nArticle by Sri Sadagopan Iyengar Swami, Coimbatore","Over the years there have been several adaptations regarding the biblical story of Noah’s Ark, whether the comedy film starring Steve Carell, or a film narrating the Bible story. However, the adaptation that the majority of people do not recognize is the Bible story itself. The story of Noah and his ark was actually adapted from the eleventh and twelfth tablets of the Epic of Gilgamesh. The Epic of Gilgamesh is an epic poem that narrates the adventures of a fictional hero named Gilgamesh.1 The reason that Noah’s Ark is an adaptation is due to the many parallels between the two stories. Some similarities include the wrath of a deity upon humankind, as well as the creation of a type of vessel in order to survive the flood.\nBefore the 19th century, the Bible was believed to be the most credible source of historical information about the Ancient Near East. However,“The Epic of Gilgamesh is the oldest surviving epic poem in history, dating from about 2500 B.C.E.”2 The discovery of the Epic of Gilgamesh, specifically the tablets containing the excerpts detailing the Great Flood myth, caused turmoil among the ancient historical community, due to the fact that the Great Flood myth was written about a thousand years before the Bible story of Noah.3\nThe eleventh tablet narrates one of Gilgamesh’s adventures in search for immortality.4 Yet his search leads him to a wise man named Utnapishtim. Utnapishtim then reveals to Gilgamesh how he achieved immortality. He begins to explain how the god Ea informed him of the devastating flood created by the gods in order to extinguish humanity. He was instructed to construct a boat of immense size and to tell the people of Shuruppak to assist him in the building of the boat. Once the boat was complete, he was to load it with every living thing and his family in order to survive. Seven days later the great flood began its reign of destruction upon humankind. During this time, Utnapishtim and his ark ran aground on a mountain peak. He then released a dove in order to find land but the dove returned, not having found land. The same thing happened when he sends a swallow. However, the third time he releases a raven that never returned. Upon reaching land, the gods in heaven realized the great service Utnapishtim had done by saving humankind; thus, they granted him and his wife eternal life.\nMuch like the Epic of Gilgamesh, the story of Noah’s Ark conveys a similar destruction tale.5 The book of Genesis narrates how God began to despair over the creation of humankind due to humanity becoming sinful and evil. Therefore, God decided to create an immense flood in order to destroy and cleanse the world. However, God chose a man named Noah due to his immaculate behavior, and He instructed him to build a boat or an ark. Once he completed the ark, he was to load it with a pair of every animal on earth along with his family. For the next forty days, God plunged the earth with devastating rains, causing the earth to be flooded for a whole year. Noah then released a dove and it never returned, meaning that it had found dry land. Once the water receded, the earth was restored and became once again fertile. God made a covenant with Noah promising that his lineage will be fertile and that he will never destroy humanity again by flood.\nThe parallels between both stories are clear to see, due to the similarity in content and story structure. For example, both narratives include an extremely powerful deity or deities, that form a plan to wipe out humankind by creating a great flood in order to restore the earth, as well as how a single man was chosen by a higher power to save humanity.6 Another parallel is how both individuals were instructed to construct a boat in order to survive the coming flood. The content of both vessels is also similar due to them being loaded with all the living things on earth, even though in the Epic of Gilgamesh it was all living things while in Noah it was the pair of every animal on earth, along with their families. Once the earth was flooded, both Utnapishtim and Noah release birds in order to find out if the land was yet dry. Finally, both men upon reaching land are rewarded by higher powers, due to their involvement in saving humanity, and the creation of a new world.\n- Benjamin R. Foster, Douglas Frayne, and Gary M. Beckman, The epic of Gilgamesh: a new translation, analogues, criticism (New York: Norton, 2001), 60-65. ↵\n- Jerry Bentley, Herbert Ziegler, Heather Streets Salter, Traditions & Encounters: A Brief Global History Volume 1 (New York, NY: McGraw-Hill Publishers, 2016), 17. ↵\n- James B. Pritchard, Ancient Near Eastern texts relating to the Old Testament (Princeton, N.J.: Princeton University Press, 1969), 3. ↵\n- Pritchard, Ancient Near Eastern texts, 273. ↵\n- Jerry Pinkney, Noah’s ark (New York : SeaStar Books, 2002), 20-30. ↵\n- Pritchard, Ancient Near Eastern texts, 10. ↵"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:80fc3ea3-4fc2-462b-a466-23d188411afb>","<urn:uuid:31b9bb5e-02c2-4c92-81e0-fe8dfa70de55>"],"error":null}
{"question":"How can chronic stress affect the body and when is stress actually beneficial?","answer":"Chronic, long-term stress can be harmful to the body, leading to illness, mood issues, and insomnia. However, stress in short bursts can be beneficial and necessary for optimal body function. For instance, some stress is helpful when preparing for an exam or work project as it helps with focus and engagement by activating certain hormones that keep us alert and active. The key distinction is between acute and chronic stress - while short-term stress can enhance performance, chronic stress makes people miserable and should be addressed professionally.","context":["Expert Q&A: How to manage stress\nWomen, especially, feel a lot of chronic stress. A psychotherapist gives us some perspective about how to manage stress.\nStress is a part of everyday life. Statistically, women are 28 percent more likely to be stressed than men. So how do we deal with everything on our plate and reduce stress? The Osher Center for Integrative Medicine at Vanderbilt‘s Michelle Foote-Pearce, DMin, LPC-MHSP, shares some insights about how to manage stress and self-care.\nQuestion: What do you do here at Osher?\nAnswer: I am a psychotherapist. I counsel people about stress reduction, and direct our mindfulness and outreach programs.\nQuestion: When it comes to women and stress, what are the most common stressors you hear about in your practice?\nAnswer: Externally imposed stressors we have are all the demands placed on women, our time and our sense of responsibility. We try to meet all those demands and have enough time to do everything that needs to be done. Then, we also have this internally imposed stressor of wanting to do everything really well, wanting to have our house clean and look nicely decorated, taking care of our children, meeting our spouse’s needs and maybe even caring for our parents. Those are a lot of demands, and we put a lot on ourselves when we think we have to do it all perfectly.\nQuestion: How do you cope? How do you not always feel overwhelmed and stretched too thin?\nAnswer: Making a list of all the things that you need to do is a great example of why it feels so stressful. Each day we have all the things we need to do, and we hold that in our mind. Our body intuitively knows, “I can’t do all those things at once.” Our body goes into this physiologic stress response. We have this mental energy that we’re consuming by always thinking about all the things that we have to do and feeling that pressure, and then the physiologic stress that happens when we’re thinking about all of those things. One of the keys is to be realistic about what we can get done now. Do one thing at a time and engage with it. Maybe do our to-do list in the beginning of the day, then just check back periodically, but don’t think about everything that we have on our plates constantly. Be present and intentional.\nQuestion: Is there a such thing as multitasking? Should we not have more than three things?\nAnswer: The reality is that we talk about multitasking. Moms, in particular, talk about multitasking because we’re cooking dinner, watching the children and maybe helping Sally with homework. In reality, our brain does not multitask. We flit back and forth. We have less brain efficiency when we’re flitting back and forth. We have to do that sometimes when we’re cooking dinner and taking care of the kids. If we’re focused at work and we’re trying to multitask, that puts a lot of stress on the brain.\nQuestion: What other stress relievers would you say are useful?\nAnswer: Mindfulness is a way to be in the present moment, aware of everything that’s going on with us, internally and externally — if we can do this with acceptance and kindness. Whether we’re feeling stressed, sick or whatever we’re dealing with, if we can just recognize, “Oh, I’ve got a lot on my plate today.” Then, give ourselves a little kindness. Ask, “Are we treating ourselves the way we would want to treat our loved ones?” That helps us to take a breath and calm down. Our body starts to unwind, and that intensity we feel when we think about everything we have to do softens. Then, the good hormones can start going through our bloodstream, and we can have that sense of well-being and a little warm peace. It takes 30 seconds. Let me just hit the pause button, be kind to myself and then think about what’s the next things that I need to do right now.\nQuestion: As a parent, why is important for children to see how you manage stress? What does it show them?\nAnswer: The idea that we are supposed to be this perfect mom is not good role modeling for our children. We’re human beings. We have emotions. We make mistakes. Role modeling in a really helpful way for our kids is about showing what it’s like to be human. Mom makes mistakes. She doesn’t do things perfectly, but she’s resilient. She can problem solve. It’s going to be okay. We’re going to move ahead, and we’re going to do it with kindness to ourselves and others.\nQuestion: Is there ever a time when stress is good or necessary or helpful in any way?\nAnswer: Yes. Technically, stress is a physiologic response of our body. Stressors are the things that make us feel stressed. Our body needs stress to optimally operate, but it likes it in only short bursts. The chronic stress is the (harmful) thing. (Stress) engages a lot of the hormones that keep us active and alert … For example, for an exam or a work project, you want to be optimally stressed because that helps you focus and be engaged. It’s the chronic, long-term stress that really puts strain on our body, and makes us sick and have mood issues and insomnia.\nQuestion: If you feel chronically stressed, is that a sign that you should seek professional help?\nAnswer: Yes, because it’s not necessary. No matter what your responsibilities are, there are ways for you to find a lifestyle that works for you – one that ensures more happiness and well-being. Chronically, stress makes people pretty miserable.\nQuestion: What does self-care mean to you? What are the different ways that that plays out in women’s lives?\nAnswer: What self-care is not is trying to meet the ideal and that perfection. It’s really about: What do I need to take care of myself at this stage of my life with this level of stress? Maybe it’s “I really need to eat more healthfully and simply.” Maybe it’s “I’m not getting enough sleep.” There are so many ways we can take care of ourselves, but it’s about listening to yourself, identifying what you need and being intentional about it. What’s keeping you engaged in life? What makes you feel connected? What makes you feel alive and energized? If we’re not filling ourselves up, then we’re getting depleted more and more and we’re not going to have that energy and that presence to give to our children or our job. We’re not going to be optimally functioning.\nQuestion: What if someone doesn’t know where to start with self-care? How do you start?\nAnswer: I usually ask people, “What’s the thing that you feel the most stressed about? What’s depleting you the most?” Let’s start there. What’s the hardest thing for you? People usually know what that hardest thing is, and we start there with baby steps. Then we begin to shift that and can tackle the other things. We’re not going to do everything at once because that’s going to be more stressful.\nQuestion: If a woman is looking for professional guidance, how can counseling services or things offered at Osher help her?\nAnswer: We have a team-based approach here. First, a nurse practitioner or a physician would talk with that person and do a whole-person assessment. You’re really getting a sense of not only what’s hard for you, but also what’s going on with your body and your mind? Then, they will actually help you come up with a plan. It could be a sequenced, let’s-do-this-first plan, but it’s always based on what’s most important to you. What’s consistent with your values? What drives you in your life? Then, from there, it might be mindfulness classes, a way to be more present and less critical of and kinder to yourself. It might be yoga or massage. Some way to reconnect with and help your body calm, soothe and feel more relaxed. Or acupuncture. Any of those can be helpful as long-term healing self-care practices.\nThe Osher Center for Integrative Medicine at Vanderbilt provides health care designed around your whole health: mind, body and spirit. The center cares for people with chronic pain and other ongoing health challenges. Call 615-343-1554 for an appointment."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:1ae75e36-e341-4401-a28c-6ad8b2f8bdf7>"],"error":null}
{"question":"Compare differences in sauce placement in traditional Detroit pizza vs Philly square pizzas - interessante to know for cooking class!","answer":"In Detroit-style pizza, the marinara sauce is traditionally served on top of the pizza. In contrast, Philadelphia's square pizzas show variation in sauce placement - Santucci's features 'upside-down' toppings with sauce on top, while Cacia's tomato pie has the tomato sauce as a primary topping element on the dough. Some major Detroit-based chains like Jet's Pizza and Little Caesars notably place the sauce under the cheese, though this differs from traditional Detroit style.","context":["Everyone knows what you’re supposed to eat in Philadelphia. A cheesesteak, right? Of course. But there’s way more to Philly’s always-evolving food scene. The city’s most iconic dishes are rooted in Italian-American traditions, original Americana recipes, and the contributions of its many immigrant communities. Yes, get a cheesesteak or two — but this map highlights many more classic Philly dishes everyone needs to try.Read More\n24 Restaurants Offering Philly’s Most Iconic Dishes\nThe best places for cheesesteaks, injera, pho, water ice, and more of the most iconic foods in Philly\nWings at Cafe Soho\nAt the moody, K-Pop video lit dining room of Cafe Soho, opt for double fried wings or boneless, then choose from a menu of sauces ranging from classic soy garlic to sweet chili and spicy, and add an extra order of pickled daikon.\nAlso featured in:\nJerk Chicken at Jamaican D's Caribbean & American Restaurant\nCaribbean platters of jerk chicken, fried whiting, curry chicken, and oxtail are all to-die-for at this Chelten Avenue destination. You basically can’t go wrong with anything you order (by phone or on Grubhub), but don’t be intimidated by the lines. It’s just one more sign that Jamaican D’s is a beloved hotspot for some of the tastiest mac and cheese, collard greens, and plantains in the city.\nPeking Duck at Sang Kee Peking Duck House\nThe Chinatown spot that claims to have first introduced Peking duck to Philly back in the 1980s is still the place to go for the Chinese specialty. These ducks have a crisp mahogany skin and come with pancakes, steamed buns, scallions, and hoisin. Dine in, or order online for pickup and delivery through all the typical outlets.\nEmpanadas at Jezabel’s Argentine Café & Catering\nNo trip to Philadelphia is complete without a visit to Jezabel’s in West Philly, where chef Jezabel Careaga offers some of the best empanadas outside of northwest Argentina, where Careaga is from. Order an empanada pack with eight assorted empanadas of your choosing — meat, vegetarian, or vegan — and enjoy with the spicy sauce of the day. Alfajores for dessert are not to be missed, either.\nCombo Platter at Abyssinia\nIn West Philly, homey Ethiopian restaurant Abyssinia has convenient meat or vegetable platters on its menu that are perfect for sampling several dishes. Use the injera to scoop up lentils, collard greens, spicy berbere chicken, tender cubes of beef, and more. Also in West Philly, Dahlak has been a reliable option for Eritrean-Ethiopian fare for decades.\nScrapple at Down Home Diner\nScrapple: It’s a weird and beloved Pennsylvania thing. The Pennsylvania Dutch breakfast staple combines pork scraps, cornmeal, and heavy seasoning into a loaf which gets sliced and pan-fried. Try it at Down Home Diner in the iconic Reading Terminal Market, either for breakfast or on top of a burger if you’re feeling adventurous. Order online in advance or dine in.\nIce Cream at Bassetts Ice Cream\nBassets has been making ice cream in Philadelphia since 1861, and packing pints out of their Reading Terminal location since the market’s debut in 1893. Unlike other ice cream companies who have chosen to modernize with lavish mix-ins and out there flavors, this family run creamery keeps it simple with a straightforward line up of flavors ranging from simple chocolate chip and French vanilla to throwback scoops like rum raisin and butter pecan.\nAlso featured in:\nOysters at Oyster House\nFor the last three generations, the Mink family has been in the oyster business. The current location of Oyster House, which has been around since the 1940s, is polished, modern, and known for its happy hour oysters. All of the seafood is served within 24 hours of being taken out of the water.\nLamb Shoulder at Zahav\nAs a nationally renowned restaurant that’s been part of the charge for exceptional Israeli food in America, Zahav is a major point of Philly pride. The lamb, braised in pomegranate molasses and strewn with chickpeas, is one of the restaurant’s signature dishes.\nCheesesteak at Angelo's Pizzeria\nThere’s plenty of debate about where to eat a cheesesteak, but the point is simple: Eat one. For better or for worse, there is no more iconic food in this city than the cheesesteak. Angelo’s makes a showstopper version, as do a couple dozen other places in town. The key factors are a fresh, locally baked roll, flavorful (if moderately greasy) meat, Cheez Whiz (or American), and (optional) sautéed onions. Call to order for pickup.\nSquare Pizza at Santucci’s (multiple locations)\nThe square pie from Santucci’s has been a Philly favorite for many years, starting in the 1950s in Northeast Philly. The pizza is well-charred in cast-iron pans and piled with “upside-down” toppings, meaning the sauce is on top. Order online or dine in.\nWater Ice at John's Water Ice\nWhile Philly has many places to get good water ice, John’s in Bella Vista is a longstanding favorite. The cold treat is made with only water, fruit, and sugar, just like when the South Philly takeout shop opened in 1945. Open for walk-ups or call ahead.\nAlso featured in:\nMeatballs and Gravy at Villa Di Roma\nThis retro Italian Market stalwart is beloved for many reasons, not the least of which is its famous meatballs and gravy (otherwise known as red sauce). Get them over spaghetti, atop ravioli, on a sandwich at lunch, even to grab and go and pass off as homemade — just get them.\nAlso featured in:\nTacos at Taqueria La Prima\nThe tender chicken tinga, spicy chorizo, and savory lengua are all stellar at Taqueria La Prima in the Italian Market, and the nopales, when available, are almost reason enough to visit on their own. As an added bonus, the tiny, no-frills eatery is open all day and doesn’t stop serving until late night. (It’s the same spot that used to be called Prima Pizza.)\nSoft Pretzel at Center City Pretzel\nAsk any true Philadelphian about soft pretzels and they’ll tell you about how they considered them a basic food group when they were growing up, about the guys at every intersection selling them in brown paper bags, and about how they used to be much cheaper. Well, the guys at the intersections are mostly gone and the prices have gone up, but pretzels are still part of the standard diet here in Philly. The good ones are almost-crunchy on the outside, doughy-soft in the middle, and baked to a rich, golden brown. Center City Pretzel opens its bakery doors at 6 a.m. to serve the hungry masses, and get its wholesale orders out the door. For more than three decades, this bakery has been going strong and still makes its pretzels with just the basic ingredients: flour, yeast, water — plus that crunchy salt on top.\nBanh Mi at Ba Le Bakery\nIn the rest of the country, these pâté, butter and veggie filled baguettes are known as banh mi, but in Philly they’re commonly referred to as Vietnamese hoagies, because, well, it’s Philly. Ba Le has been baking since 1998 and their order by number menu full of charcuterie, chicken and prawn packed banh mi garnished with spears of cucumber, carrot, daikon, a sprig of cilantro, and racy rounds of jalapeño. You can preorder in advance.\nTacos at South Philly Barbacoa\nCristina Martinez’s lamb barbacoa tacos regularly draw national attention to this casual Italian Market eatery. The lamb is marinated and cooked overnight before tender chunks, chopped to order, are served on house-made tortillas. South Philly Barbacoa is open weekends, from 5 a.m., for takeout.\nPho at Pho Saigon\nThere are two Pho Saigons in Philly — one in the Northeast on Bustleton Avenue and the other in a shopping center right by the entrance to I-95. Either destination will hit the spot with one of Philly’s most revered dishes: a rich, hot bowl of pho, topped with all the necessary herbs and stewed in a lovely bowl of broth. Are you a morning pho kind of person? You can get Pho Saigon’s famous noodle soup starting at 9 a.m.\nCannoli at Termini Brothers Bakery (multiple locations)\nSouth Philly is known for its picture-perfect family-run Italian bakeries, spilling over with sweets like torrone, lobster tails, zeppoli, and cannoli. One must-try spot is the century-old Termini Brothers Bakery, where the cannoli is integral to the Philly experience.\nGudeg Jackfruit Stew at Hardena\nThis unassuming South Philly corner has been serving incredible Indonesian food for the past 20 years. Stop by for takeout on any given day for savory stews like fish in yellow curry and tender braised beef rendang, but its the vegan gudeg jackfruit stew that has the most devoted fans. Real Philadelphians know that the stew is unctuous and rich and unlike anything else you can get in Philly. Don’t skimp on the housemade sambal.\nCheesesteak Torta at Café y Chocolate\nResidents of Philadelphia can argue ad nauseam about where to find city’s best steak, but there’s no doubt that this cozy Mexican breakfast and lunch spot does does one of the more intriguing takes on the famed sandwich. Cafe y Chocolate brightens the steak and cheese combo with serrano peppers, chipotle mustard, and fresh pico de gallo.\nRoast Beef Sandwich at Old Original Nick’s Roast Beef (multiple locations)\nGrab extra napkins because this densely stacked sandwich with thick-sliced roast beef drowning in its own intense beef gravy is messy. But the kaiser roll soaking up some of the juice is one of the best parts of the well-loved South Philly sandwich.\nAlso featured in:\nRoast Pork Sandwich at John's Roast Pork\nPhiladelphia may be best known for cheesesteaks, but locals know a little secret: Get the roast pork instead. The no-frills John’s Roast Pork in South Philly has been perfecting the sandwich since 1930, even earning a James Beard Award as an American Classic. And if you must have a cheesesteak too, this is an excellent place to get one.\nTomato Pie at Cacia's Bakery\nDepending on geographic location, the words tomato pie mean different things to different people. In Philadelphia, it’s a stripped down take on pizza — Italian bakery-made dough, tomato sauce, and depending on who’s asking, a pit of cheese. With a vibrant, lightly sweet tomato sauce and soft crust, Cacia’s textbook tomato pie is offered by the rectangular slice, alongside banana pepper and American topped pieces of pizzaz and white broccoli pie.\nAlso featured in:","|Part of a series on|\nDetroit-Style Pizza is a style of pizza developed in Detroit, Michigan. It is a square pizza similar to Sicilian-style pizza that has a thick deep-dish crisp crust and toppings such as pepperoni and olives and may be served with the marinara sauce on top. The square shaped pizza is the result of being baked in a square pan, which is often not a pizza pan. Rather, industrial parts trays are often used, which were originally made to hold small parts in factories.\nThe crust of a Detroit-style pizza is noteworthy because in addition to occasionally being twice-baked, it is usually baked in a well-oiled pan to a chewy medium-well-done state that gives the bottom and edges of the crust a fried/crunchy texture. Some parlors will apply melted butter with a soft brush prior to baking. The resulting pizza has a warm, chewy texture that, when combined with the sauce and toppings, is intensely flavorful.\nThe origins of \"Detroit-style\" pizza can traced back historically to Buddy's Rendezvous, which In 1936, existed as a \"blind pig,\" which as an establishment, skirted the State and Federal laws that governed the on-site sale and usage of alcohol. The owner at the time was August Gus Guerra. In 1944, Gus turned the blind pig into a legitimate tavern, but with World War II still raging, business was suffering. In 1946, Gus decided to add Sicilian style pizza to the menu. Soon the neighbors, and out-of-towners, were becoming hooked on Gus's unique recipe. The legend of Detroit's original square pizza was born. In 1953, Jimmy Bonacorse and Jimmy Valenti purchased Buddy's and its celebrated pizza recipe. Then, sixteen years later, William \"Billy\" and Shirlee Jacobs visited Buddy's and fell in love with it. In 1970, they bought it. Buddy's Pizza, is informally credited with being the original chain that serves Detroit-style pizza. Over the next several decades the chain grew and developed, cooks moved on, and some cases they opened their own pizzerias. Cloverleaf, which was later founded by Gus Guerra as an Italian restaurant in Eastpointe serves Detroit Style Pan Pizza as does Luigi's \"the Original\", Tower Inn in Ypsilanti, Shield's Pizza and Louie's. All of them are still thriving and each has their own loyal fan base. In 2009, both Buddy's Detroit-style square pizza and Luigi's \"the Original\" of Harrison Township, Mich were singled out as two of the 25 best pizzas in America by GQ magazine food critic, Alan Richman.\nPizza Papalis in Greektown, Dearborn, and Troy are among Detroit's unique specialty pizzerias offering deep dish pizza. Niki's in Greektown, Little Caesars in Detroit, Jet's in Sterling Heights, Tower Inn in Ypsilanti, Papa Bella's in Ortonville, Green Lantern in Madison Heights, The Gathering Place & Marinelli's in Troy, and Detroit Style Pizza Company are among Detroit's unique specialty pizzeria's offering deep dish pizza and or Detroit-Style Pizza in Southeast Michigan. The pizza at these restaurants is square or rectangular, with a thick crispy crust. Other restaurants that serve square specialty pizzas, similar to Detroit Style, but more closely related to Sicilian style include Benito's Pizza, Cottage Inn, and Alibi in Troy.\nIn April 2013, Little Caesars launched the first Detroit-style Deep Dish pizza that is available nationwide.\nIn recent years this style of pizza has seen a growth in popularity around the United States as native Detroiters have relocated to other cities. Outside of Detroit, Detroit-Style pizza can be found in Austin, Texas at Via 313 Pizza; Telluride, Colorado at Brown Dog Pizza which was founded by former Birmingham, Michigan native and University of Michigan football player Jeff Smokevitch.; Boca Raton, Florida at Grande & Augy's Pizza; Raleigh, North Carolina at Klausie's Pizza; Las Vegas, Nevada at Northside Nathan's; Columbia, Missouri at Pizza Tree; Buffalo, Minnesota at Norm's Wayside, and Louisville, Kentucky at Loui Loui's Authentic Detroit Style Pizza. Shawn Randazzo, of Detroit Style Pizza Company, has created a website designed to serve as a one-stop source of information on the history and characteristics of the style and to provide listings of certified pizzerias. Click on http://detroitstylepizza.com/ for more information.\nPizza companies based in Detroit \nSoutheast Michigan is also known as the headquarters of some of the largest pizza chains in the United States including Domino's Pizza (Ann Arbor), Hungry Howies (Madison Heights), Dolly's Michigan, Jet's Pizza (Sterling Heights) with the sauce under the cheese, and Little Caesars (Detroit) also sauced under the cheese. Although none of those chains specializes in Detroit-style pizza, Little Caesars does sell a square deep dish pizza locally and offers sauce with it that can be applied on top of the pizza by the customer to make the traditional Detroit-style pizza. It was reported on March 30, 2013 that Little Caesars Inc. would be commencing a national rollout on April 1st of a new product - Little Caesars Deep!Deep! which is a deep dish, Detroit Style pizza. Little Caesars is playing into a growing awareness and curiosity about the Detroit deep-dish pan pizza approach, noting that recent studies and an article in GQ magazine have mentioned Detroit-style pan pizzas favorably, in comparison with other approaches such as Chicago-style deep-dish pizzas that are mostly round and often have stuffed crusts. In a manner of extreme coincidence, Madison Heights-based pizza chain Hungry Howie's introduced square-cornered, deep-dish pizzas to its menu on Monday April 1, 2013. Hungry Howie’s is a national chain with more than 550 stores in 20 states. Hungry Howie’s, though, is the first to offer a flavored deep-dish crust which has been their signature since the 1980s.\nSee also \n- Giesler, Jennie and Gerry Weiss. \"Poke around in Michael Moore's past.\" Erie Times-News. October 2, 2009. Retrieved on February 13, 2010.\n- Haurwitz, Ralph K.M. \"Chaps' fight for football title pays off.\" Austin American-Statesman. December 22, 1996. A1. Retrieved on February 12, 2010. \"deep-dish, Detroit-style pizza with the sauce on top,\"\n- Rector, Sylvia (January 23, 2011). \"Shortage of steel pans has Detroit-style pizza makers scrambling\". Detroit Free Press. Retrieved November 23, 2012.\n- 25 best pizzas around the country, May 22, 2009\n- Metro Times - Food: Slices of heaven\nFurther reading \n- Rector, Sylvia (March 31, 2011). \"Detroit-style pizza gaining fame, winning fans nationwide\". Detroit Free Press. Retrieved March 31, 2013.\n- Woods, Ashley C. (January 18, 2012). \"Blogger invents the all-crust Detroit pan pizza\". Michigan Live. Retrieved November 23, 2012.\n- \"Buddy's Pizza revives Detroit tradition: Friday night bocce is back\". The Detroit News. August 12, 2007. Retrieved November 23, 2012. (subscription required)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:45fdf308-97bd-4b06-be38-644174347a08>","<urn:uuid:81b2c4ea-4236-4216-8603-b16cd211cd36>"],"error":null}
{"question":"What are the Boston-based writing workshops for vulnerable populations, and how do they align with evidence-based intervention practices for at-risk individuals?","answer":"Boston offers several specialized writing workshops for vulnerable populations through Writers Without Margins, including programs at Bridge Over Troubled Waters, the Wyman Re-Entry Program for post-incarceration individuals, and Step by Step for adults with psychiatric disabilities. These programs align with evidence-based intervention practices by providing targeted services based on individual needs and risk levels. The workshops embody the principle that intensive interventions should be matched to higher-risk individuals, while ensuring some level of support for all participants. This approach follows the triage concept of bringing order to complex situations and setting paths for individualized treatment, while efficiently allocating limited resources.","context":["THE UNDERGROUND WRITERS’ COLLECTIVE\nBridge Over Troubled Waters, 47 West Street, Boston, MA\nWe’ve added an arts component to the education programs at Bridge Over Troubled Waters where the mission, since 1970, has been “transformi\nTHE WYMAN WRITERS’ WORKSHOP\nWyman Re-Entry Program, 201 River Street, Mattapan, MA\nThis writing, reading, discussion and speaker series aims to engage men in a post-incarceration and addiction recovery program with additional tools for a stronger sense of authority and engagement, through literature. Utilizing a broad range of styles, aesthetics, and forms, we encourage experimenting with new ideas, acknowledging and practicing the power of language through the four poetic temperaments of: story, structure, music and imagination.\nTHE STEP BY STEP WRITERS’ WORKSHOP\nStep by Step Supportive Services, Brookline, MA\nThe Step by Step Writers’ Workshop aims to engage adults with psychiatric, cognitive, and social disabilities through the healing power of creative writing. Through guided exercises, art, guest speakers and discussion, we explore the ways in which language, poetry, literature, storytelling, and other expressive modalities can promote healing, recovery, and authority.\nTHE THERAPEUTIC WRITING & ARTS WORKSHOP\nWednesdays 10:15 – 11:15am\nGrove Hall Branch of the Boston Public Library, 41 Geneva Ave, Dorchester, MA\nThis five-week workshop combines various methods, genres, and styles of expression. By exploring the trailblazers that came before us, and connecting with contemporary artists, we will find new ways to unlock our own language. In a supportive environment, this workshop offers a unique space combining safety and challenge, allowing freedom and structure for the creative process. Beginners Welcome.\nTHE BOSTON PUBLIC LIBRARY SERIES\nEvenings 6:00-7:30. Check the Boston Public Library calendar or our Writers Without Margins, Inc on Facebook for listings.\nPublic Library, Central Branch, 700 Boylston Street, Copley Square, Boston, MA\nWe’ve teamed up with the Boston Public Public Library to provide free creative writing workshops, catering to the interests and energies of library patrons and specializations of our facilitators, who come from the surrounding universities, neighborhoods and communities we share. In Summer 2018, we offered workshops on Writing & Recovery and Creative Nonfiction. This Fall, we’ll provide a six week, multi-genre, workshop entitled, “The Personal is Historical” and our Winter session will be, “On Silences: An Advanced Poetry Workshop.”\nTHE WRITERS’ WORKSHOP AT THE ENGAGEMENT CENTER\nAdjacent to the Southampton Shelter, 112 Southampton Street, Boston, MA\nIn partnership with the City of Boston and the Boston Public Health Commission, this workshop is offered amidst the shelters and treatment centers concentrated in South Boston. Located at the Engagement Center, a safe space designed to reach individuals seeking services and support, this group will provide a creative outlet for men and women who are interested in sharing and discussing the universal messages of literature and/or creating their own, through guided exercises, ranging from performance to meditation as a means of clarity, connection, and communication.\nTHE TAKING AUTHORSHIP PROGRAM\nContact us through the “Contact Us” page to arrange a workshop throughout Greater Boston\nOur lives are full of stories, and we can change the next chapter. So, how can we use the lessons within literature and the craft of creative writing to increase empowerment and empathy while building the skills within us? This flexible program is offered to organizations, allies, centers, groups, and clubs seeking to expand and hone the crafts of language for personal insight and public advocacy through themed workshops or intensive seminars. Our first collaboration took place with Brockton’s Family Center and Providence’s College Unbound. Since then, we have offered youth workshops on Poetry as Protest as well as Culture & Identity and an adult seminar on Thoreau & Civil Disobedience.","In this section, you will learn the importance of prioritizing resources and targeting intervention strategies based on system and individual factors. Clearly, given the diversity of the jail population, unpredictable lengths of stay, limited resources, and principles of evidence-based practice, it is not possible or desirable to provide the same level of intervention to everyone who enters the jail setting.\nIn fact, to obtain an optimum level of efficiency and effectiveness, quick screening tools should be used to separate low-risk offenders from their medium- and high- risk counterparts. The key is to match the right person to the right resources so that higher risk individuals receive more intensive interventions in the jail and the community.\nThis research-driven practice of targeting the needs of higher risk offenders is often controversial. Many jails and communities tend to “over-program” lower risk offenders and use valuable resources to change their behavior despite the fact that intensive programming of lower risk offenders is likely to make them worse rather than better. As a part of such a practice, not only does it make lower risk offenders worse, but valuable treatment opportunities are missed for higher risk offenders who are far more likely to reoffend at a much higher rate and frequency than their lower risk counterparts. Therefore, the TJC project recommends a triage system to help a system determine “who gets what.”\nThe word triage comes from the French term “trier,” to sort. We often think of triage scenarios when natural and human disasters occur and decisions have to be made quickly to identify and treat the most seriously injured.\nTriage protocols are effective because they:1\n- Bring order to a chaotic situation.\n- Quickly sort a large number of people on the basis of a serious condition.\n- Set the path for individualized treatment.\n- Facilitate a coordinated effort between jail- and community-based supervision agencies and providers.\n- Are fluid enough to accommodate changes in the number of people involved in the process, the availability of resources, and the extent of need.\nA jail setting is a busy and sometimes chaotic environment, but decisions still have to be made at reception to determine each individual's risk and needs. This is a particularly acute problem within a jail facility because of the rapid rate of turnover and short length of stay of most inmates. A triage matrix, tailored to the needs, resources, and timelines of your jurisdiction, will help determine the appropriate allocation of services by categorizing individuals and identifying the appropriate mix of targeted interventions.\nThe following case studies will help you to begin thinking about the unique risk and needs of your population.\nCase study 1. Mrs. Thomas is a 42-year old, married mom of two children awaiting trail and charged with driving under the influence. She is a recovering addict, has one prior felony for drug possession for which she served 180 days in jail, and has been drug free for two years before her most recent relapse. Mrs. Thomas has also worked part-time at a convenience store for the last two years. She does not have a history of failure to appear.\nCase study 2. Mr. Banks is a 33-year-old, single male serving a nine-month sentence in the county jail for possession of a half of gram of methamphetamine. Mr. Banks started using drugs when he was 12, dropped out of school in the 11th grade, and served his first prison term at 19 for robbery. He has spent 8 years in prison, the last time before his recent jail stay was for stealing a car while under the influence. At the time of his current arrest, Mr. Banks was unemployed and living in a shelter after losing his construction job for not showing up to work on time.\nCase study 2. Mr. Jones is a 19-year-old, single man serving a 15-day sentence for possession of marijuana and medication (i.e., Concerta, a stimulant used to treat ADHD) for which he didn't have a prescription. Prior to his arrest, Mr. Jones had no prior criminal record, attended community college, was employed part-time as a waiter at a local eatery and lived with his mother.\nUsing these three case studies, ask yourself the following questions about these individuals:\n- Which screenings and assessment instruments are needed to identify their risk and needs as they enter your facility?\n- What are their unique risks?\n- What is the likelihood of Mrs. Thomas in case study 1 appearing in court when required?\n- How pressing is the need for intervention?\n- How extensive is the need for intervention?\n- What is the likelihood of reoffending and how severe might the crime be?\n- What are their unique needs?\n- Do you know their length of stay?\n- What factors would you use to sort them by risk and needs?\n- What type of jail and community intervention is required?\n- What type of transition planning and which specific targeted interventions, if any, are needed?\nDon't worry if you don't have all the answers. In this module and the next three modules, you will learn how to perform the following 11 tasks (outlined in the Targeted Intervention Strategies section of the TJC Implementation Roadmap and designed to address these and related topics):\n- Complete the Triage Matrix Implementation Tool.\n- Apply screening instruments to all jail entrants.\n- Apply risk/needs assessment instrument(s) to selected jail entrants.\n- Produce transition case plans for selected jail entrants.\n- Develop pretrial practices to support jail transition\n- Define scope and content of jail transition interventions currently in place.\n- Provide resource packets to all jail inmates upon release.\n- Deliver in-jail interventions to selected inmates.\n- Deliver community interventions to selected released inmates.\n- Provide case management to selected jail entrants.\n- Provide mentors to selected jail entrants.\nTo begin, review The Triage Matrix Implementation Tool referenced in Task 1 and developed by the TJC project team to help your jurisdiction prioritize goals, identify target populations, and allocate limited resources to your jurisdiction's intervention strategies. The underlying concept is that everyone in the jail population should get some intervention, which may be as minimal as receiving basic information on community resources, but the most intensive interventions are reserved for inmates with higher risk and needs. The triage matrix includes the following four sections:\n- Screening and Assessment\n- Transition Case Plan\n- Pre-Release Interventions\n- Post-Release Interventions\nThe triage matrix includes a worksheet for each section and a sample matrix with all sections completed. All content in the sample triage matrix is approximate and should be adapted to fit your community. We recommend that you fill in the triage matrix as soon as possible to better understand the strengths and gaps in your present transition system.\n1 United States Army, Office of the Division Surgeon, 10th Mountain Division. Presentation delivered as part of a Trauma-Focused Training. Fort Drum, NY.\n2 Fretz, Ralph. 2006. What Makes a Correctional Treatment Program Effective: Do the Risk, Need, and Responsivity Principles (RNR) Make a Difference in Reducing Recidivism? Kearney, N.J.: Community Education Centers, Inc. http://www.academia.edu/31758960/What_Makes_A_Correctional_Treatment_Pro....\nSection 1: Resources\n- Bogue, Brad, Nancy Campbell, Mark Carey, Elyse Clawson, Dot Faust, Kate Florio, Lore Joplin, George Keiser, Billy Wasson, and William Woodward. 2004. Implementing Evidence-Based Practice in Community Corrections: The Principles of Effective Intervention. Washington, DC: National Institute of Corrections.\n- Christensen, Gary. January 2008. Our System of Corrections: Do Jails Play a Role in Improving Offender Outcomes? Washington, DC: Crime and Justice Institute and the National Institute of Corrections.\n- Dunworth, Terry, Jane Hannaway, John Holahan, and Margery Austin Turner. 2008. Beyond Ideology, Politics, and Guesswork: The Case for Evidence-Based Policy. Washington, D.C.: The Urban Institute.\n- Urban Institute. Case planning worksheet.\n- U.S. Department of Health and Human Services, Substance Abuse & Mental Health Services Administration. October 2007. A Guide to Evidence-Based Practices on the Web.\nLet's revisit what we have learned so far in the Targeted Intervention Strategies module. Please answer the following question.\n|The TJC Triage Matrix helps you to:|\nNow that you have completed this section, you should understand that incarcerated people have varying needs. Some require intensive interventions, while others require little or no intervention. The Triage Matrix Implementation Tool and the TJC Implementation Roadmap can help you prioritize goals, identify task and target populations, and allocate resources efficiently and effectively."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:044035bb-b4ee-40c9-b41b-8909ba8415cd>","<urn:uuid:34c56b3b-074b-4532-9a52-447327b9a2c7>"],"error":null}
{"question":"Is high blood pressure equally dangerous for liver and kidneys? Quick facts needed!","answer":"High blood pressure affects these organs differently. For kidneys, hypertension is one of the main causes of chronic kidney disease (CKD) and is specifically listed as leading to hypertensive kidney disease. For the liver, high blood pressure is not directly listed as a main cause of liver disease. However, repeated episodes of heart failure with liver congestion (which can be related to high blood pressure) can contribute to cirrhosis. The primary causes of liver damage are instead related to hepatitis infections, alcohol consumption, and toxic substances.","context":["The liver is the largest internal organ in the body. Its main functions are to:\n- metabolize most of the nutrients that are absorbed by the intestine\n- store nutrients\n- produce proteins\n- detoxify blood by removing medications, alcohol, and potentially harmful chemicals from the bloodstream and treating them chemically so they can be excreted by digestive or urinary systems\nBecause the liver comes in close contact with many harmful substances, it is protected against disease in two main ways. First, it can regenerate itself by repairing or replacing injured tissue. Second, the liver has many cell units responsible for the same task. Therefore, if one area is injured, other cells will perform the functions of the injured section indefinitely or until the damage has been repaired.\nDifferent types of liver disorders include hepatitis, cirrhosis, liver tumours, and liver abscess (collection of pus), just to name a few. The focus here will be the two most common forms: hepatitis and cirrhosis.\nThere is more than one type of hepatitis, and although they have similar symptoms, they're contracted in very different ways.\nHepatitis A is the most common and the most infectious, spreading easily from person to person like most other viruses. It affects millions around the world and is responsible for more than 2 million deaths a year.\nHepatitis B is acquired through exposure to infected blood, vaginal fluids, or semen. It's estimated that about 300,000 Canadians have hepatitis B.\nHepatitis C affects about 3.5 million North Americans. About 15% of those with hepatitis C may have been exposed to infected blood products before widespread blood testing began.\nHepatitis D is unique because it can only affect those that already have hepatitis B.\nThe second type of liver disorder is called cirrhosis. It's the final stage of many different forms of liver disease. Cirrhosis involves permanent scarring of the liver that can severely impact the proper functioning of the organ.\nHepatitis is an inflammation of the liver that can be caused by a virus, by inherited disorders, and sometimes by certain medications or toxins such as alcohol and drugs. Scientists have identified four main types of viral hepatitis: hepatitis A, hepatitis B, hepatitis C, and hepatitis D. A fifth type, hepatitis E, is generally not found in North America.\nHepatitis A is waterborne and spread mainly via sewage and contaminated food and water.\nHepatitis B is transmitted by contact with infected semen, blood, or vaginal secretions, and from mother to newborn. Hepatitis B is most commonly spread by unprotected sex and by sharing of infected needles (including those used for tattooing, acupuncture, and ear piercing).\nHepatitis C spreads via direct blood-to-blood contact.\nHepatitis D is spread by infected needles and blood transfusions.\nImproved screening of donated blood has greatly reduced the risk of catching hepatitis B or C from blood transfusions. Both hepatitis B and C can be spread through sharing of razors, toothbrushes, and nail clippers.\nThe main cause of cirrhosis is chronic infection with the hepatitis C virus. Other causes include:\n- long-term, excessive alcohol consumption\n- chronic infection with hepatitis B virus\n- inherited disorders of iron and copper metabolism\n- severe reactions to certain medications\n- fatty liver caused by obesity\n- infections from bacteria and parasites usually found in the tropics\n- repeated episodes of heart failure with liver congestion and bile-duct obstruction\nWith cirrhosis, the liver tissue is irreversibly and progressively destroyed as a result of infection, poison, or some other disease. Normal liver tissue is replaced by scars and areas of regenerating liver cells.\nSymptoms and Complications\nBoth hepatitis and cirrhosis show few warning signs. In the acute phase of most forms of hepatitis, there are flu-like symptoms such as tiredness, fever, nausea, loss of appetite, and pain (usually under the ribs on the right side of the abdomen). There may also be some jaundice (yellowing of the skin and whites of the eyes.)\nFollowing the acute stage, hepatitis A will be cleared from the body and lifelong immunity develops. In hepatitis B and C, viral particles may linger in the body producing a chronic infection that lasts for years. This can eventually lead to liver cirrhosis and, in some cases, liver cancer.\nSigns and symptoms of cirrhosis include:\n- abdominal pain\n- general fatigue\n- intestinal bleeding\n- jaundice (yellowing of the skin and eyes)\n- loss of interest in sex\n- nausea and vomiting\n- small red, spider-like blood vessels under the skin or easy bruising\n- swelling in the abdomen and legs caused by fluid accumulation\n- weight loss\nIf you have cirrhosis, you should seek emergency help if you experience any of the following:\n- mental confusion\n- rectal bleeding\n- vomiting blood\nMaking the Diagnosis\nDoctors diagnose hepatitis with blood tests and a complete personal history. They will ask if you have:\n- used intravenous drugs\n- recently eaten shellfish from polluted waters\n- travelled to countries where hepatitis infections are common\n- had a blood transfusion or been in contact with fresh blood\n- had potentially risky sexual practices\n- taken certain medications in the past few months\nDiagnosing cirrhosis is based on your clinical or medical history and appearance, and blood test results. A liver biopsy may also be performed to confirm the diagnosis.\nTreatment and Prevention\nThere is no specific treatment for acute hepatitis. Bed rest isn't always essential, although you may feel better if you limit your amount of physical activity. It is important to maintain an adequate intake of calories. Your doctor may recommend small, frequent high-calorie meals, with plenty of fluids. Alcohol should be avoided or limited in order to help the liver recover. If you are unable to eat or drink, you may be hospitalized.\nSome people with chronic hepatitis B or C may benefit from medications that can slow the replication (reproduction) of the virus to decrease the amount of virus in the body. The risks and benefits of these medications should be discussed with your doctor.\nWith hepatitis B or C, your doctor may check blood periodically for a few months to watch for any continuing signs of inflammation in the liver. It isn't usually necessary to isolate people with hepatitis, but those who are close to someone with hepatitis should be aware of how the virus spreads. Hand-washing after going to the bathroom is very important.\nThere are a number of ways that governments and health professionals are fighting the spread of hepatitis. For example, there's an effective vaccination for hepatitis A. Global immunization programs exist against hepatitis B, and screening of blood donations is now common practice to check for hepatitis C. In Canada, hepatitis B vaccination is recommended for the entire population and is included as one of the primary series of vaccinations for infants. If you are travelling to countries where hepatitis is common, check with your doctor or travel medicine clinic to see if you are a candidate for hepatitis A or B immunization. There is no immunization against hepatitis C.\nTo prevent the spread of viral hepatitis, thorough hand-washing by medical personnel who come into contact with contaminated utensils, bedding, or clothing is critical. Health care workers should be vaccinated, as they are at higher risk for infection due to exposure to people who are infected.\nWhile there are no effective treatments for liver cirrhosis, its progression can be greatly reduced by complete abstinence from alcohol. Caution should also be taken when considering the use of medications that can worsen liver disease. For example, people with cirrhosis should discuss with their doctor how much acetaminophen* they can take safely because acetaminophen is metabolized by the liver. Sometimes anti-inflammatory medications need to be avoided.\nTreatment is mainly focused on complications and may include salt restriction to combat fluid retention, diuretic medications (\"water pills\" that help get rid of excess water in the body), at times a low-protein diet, and vitamin supplements such as vitamins K, A, and D. Itching may be controlled with special medications. Laxatives may be prescribed to speed up removal of toxins from the system. In some cases, a liver transplant may be necessary.\n*All medications have both common (generic) and brand names. The brand name is what a specific manufacturer calls the product (e.g., Tylenol®). The common name is the medical name for the medication (e.g., acetaminophen). A medication may have many brand names, but only one common name. This article lists medications by their common names. For information on a given medication, check our Drug Information database. For more information on brand names, speak with your doctor or pharmacist.","Kidney disease has become a major illness, given the aging population, the growth in the frequency of diabetes mellitus and hypertension. Kidney disease doesn’t present with any symptoms, and thus may only be diagnosed once it has progressed significantly, to a stage where it is irreversible. The impacts of any disease are not only physical but also emotional and psychological. It takes a toll on not only the patient but also on the people caring for that person, as well as other family members, or the health care team.\nToday, on World Kidney Day, we wanted to share some helpful facts and research done in this area.\nFacts about kidney disease\n- More than 850 million people worldwide have some form of kidney disease.\n- Prevalence of CKD is higher among females than males\n- High blood pressure and diabetes are the main causes of CKD\n- Kidney disease can go undetected until it is very advanced.\n- The kidney is one of the few human organs that can be donated by an altruistic donor\n- More than 82% of the patients awaiting an organ transplant are in need of a kidney\n- Kidney disease has an indirect impact on global morbidity and mortality by increasing the risks associated with at least five other major diseases: cardiovascular diseases, diabetes, hypertension, infection with human immunodeficiency virus (HIV) and malaria.\n- 13 people die each day while waiting for a life-saving kidney transplant\nSigns of kidney failure\n- Feeling tired, weak, and low in energy\n- Nausea and vomiting\n- Itchy skin\n- Low appetite\n- Weight loss\n- Shortness of breath\n- High Blood pressure\n- Swollen legs\nMajor types of kidney disease\n- Diabetic kidney disease\n- Polycystic kidney disease, commonly known as PKD\n- Hypertensive kidney disease\n- Inflammatory kidney disease\nTreatment options on a kidney failure\n- Kidney Transplant || Read more\n- Hemodialysis || Read more\n- Peritoneal dialysis || Read more\n- Conservative Care || Read more\nSocial Aspects of kidney disease\n- Employment || Read more: https://psychonephrology.com/social-impact/employment/\n- Social Life || Read more https://psychonephrology.com/social-impact/social-life/\n- Marital Relationships || Read more https://psychonephrology.com/social-impact/marital-relationships/\n- Family || Read more https://psychonephrology.com/social-impact/family/\n- Sex and intimacy || Read more https://psychonephrology.com/social-impact/sex-and-intimacy/\n- Travel || Read more https://psychonephrology.com/social-impact/travel/\nPsychological Aspect of kidney disease\n“The trauma of dialysis initiation” discusses at length the psychosocial impact of serious medical illness, such as kidney disease, which affects not only patients but also caregivers and clinicians. Although turbulent behavior may arise, as a consequence of inner conflicts and anxieties, at any time throughout the illness trajectory of the patient, it is appreciated more at the time of dialysis start.\nGeneral Tips for keeping your kidneys healthy\n- Drink lots of water\n- See your family doctor annually\n- Exercise 3-4 times a week\n- Eat healthy meals\n- If you already have a medical condition, always consult your healthcare team before making any changes in your lifestyle/ medications\nFailing kidneys and different treatment options\nStages of kidney disease\nhttps://www.youtube.com/watch?v=4ivERHvLVcc&t=4sSymptoms of Kidney Disease\nThese twin bean-shaped organs of our body are vital to our well-being and quality of life."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:e7da6a1a-b651-4bfa-aeb0-09da02057a74>","<urn:uuid:2f89ab6b-16bb-4391-b186-dbb9ae383998>"],"error":null}
{"question":"Hey! I'm planning to visit the Harbin Ice Festival - what's the best time to see the attractions during the day vs night, and what are the opening hours? ✨","answer":"For Ice and Snow World, the best time to enter is around 3 pm to see both daytime and nighttime views - you can appreciate transparent ice sculptures in daylight and then experience the dazzling colored light effects after dark. It operates from 11am to 10pm daily. For Sun Island, visiting during daylight hours is recommended as the snow sculptures aren't lit at night - it's open from 8am to 5pm and requires 2-3 hours for a full visit. Zhaolin Park opens from 9am to 9pm and can be visited at any convenient time since it's centrally located. For Ice and Snow World specifically, if you want to avoid crowds, you can consider entering before 2pm and re-entering after 8pm, though this requires two separate entrance tickets.","context":["The annual Harbin Ice and Snow Festival opens in late December and lasts for around 2 months, till late February of the next year.\nThe official opening ceremony is predicted to be on January 5th, 2023, but the ice and snow sculptures are open to public viewing around the Christmas period of 2022. The exhibitions and entertainment are mainly in the Ice and Snow World, the biggest venue in Harbin. There is no closing ceremony for this festival but the ice melts in March.\nEver think of what Elsa's ice palace might look like in real life? Ice stairs, ice walls, ice railings, and ice turrets? When winter comes, Harbin turns into a kingdom of ice and snow. It's the right time and the right place to experience a real ice palace. Attend the Harbin International Ice Festival with your family and make your snow king/queen dream come true in China's Ice World.\nWhat are the most recommended days for seeing the Harbin Ice Festival?\n- To see how the sculptures are being carved and made: around Christmas. The sculptures are not yet fully completed, but it's a great time to see .\n- To join in the grand celebrations: January 5th for opening ceremony and December 31st for fireworks.\n- To avoid large crowds: From January 6th to 20th and in February, when the sculptures and facilities are well established, but visitors are fewer compared with the China's public holiday periods (December 31st to January 2nd for New Year's Day, January 21st to 27th for Chinese Spring Festival).\nBut remember, as long as the ice and snow sculptures are open to visitors, it is impossible to take photos of the charming ice buildings without any visitors in!!! This might be different from what you see in the media (that have special access).\nSo, how should I avoid the crowds? A good question! If you have plenty of time in Harbin, to avoid the rush hours, you may consider entering before 2pm and re-entering after 8pm for night views. Two entrance tickets are needed if you choose to visit in this way.\nIf you are planning a Harbin tour and need more accurate information, please contact us for updates.\nWhat’s New at Harbin Ice Festival 2023?\nThe huge Snowflake Ferris Wheel has already been built inside the Ice and Snow World. You can ride on the Ferris wheel to overlook the ice castles and exhibits and take awesome photos. The giant snowflake shining its spectrum of colors against the background of the night sky is very photogenic in itself.\nHarbin Ice Festival 2022/2023 Location and Venues\nIce and snow sculptures are dotted throughout Harbin city, but there are three main venues with different exhibitions to celebrate the festival.\n- Harbin Ice and Snow World — a Colorful \"Ice Disneyland\"\n- Sun Island — See the Art of Snow Sculpting\n- Zhaolin Park — Small but Lovely Ice Lanterns\n1) Harbin Ice and Snow World — a Colorful \"Ice Disneyland\"\nWhen talking about China Ice Festival, people are typically referring to the Harbin Ice and Snow World, which displays ice sculptures of huge proportions and opens usually from 11am to 10pm each day.\nBest time to enter: Harbin gets dark at around 4 pm in winter. The best time to enter the ice world is around 3 pm to appreciate the transparent and shining ice sculptures in the daytime, soon followed by the dazzling colors and light effects at night. It is recommended to spend no more than 4 hours inside the ice world, since it is well below freezing, especially after sunset.\nActivities to take:\n- Put on your Elsa dress, walk into dreamlike but real ice buildings, and inspect ice castles and people sculptures.\n- Experience the speed and excitement of icy super slides. You have the choice of eight ice slides of different sizes. The longest is 300 meters (980 feet). It's better to go before 3 pm or you'll get the experience of waiting in line for an hour but still sliding for no more than a minute.\n- Entertain yourself with various amusements like ice climbing, skating, skiing, ice mazes, snow fights, and ice biking.\n- Attend the Harbin Show — a splendid ice-and-snow–themed show combing ice dances, ice magic, clown comedy, catwalk models, ice acrobatics, etc. The Harbin Show is performed by artists from all over the world, three times a day after 5 pm, each show lasting around 40 minutes. Due to COVID-19 measures, the show may be canceled, but as alternatives, group dancing on the Dream Stage, winter sports performances, like figure skating, and other mini-shows are continuously presented.\n- Last, but also the most important thing to do, keep trying the various (local) food shops. Warm you up with a cup of hot coffee. It is extremely important for you to stay safe in the cold (i.e. warm enough to stop hypothermia or frostbite from the beginning).\n2) Sun Island — See the Art of Snow Sculpting\nThe Sun Island venue, or the International Snow Sculpture Art Expo, is another important event of the Harbin Ice Festival. Square miles of giant snow sculptures made by artists from all over the world are displayed there.\nBest time to enter: It is open from 8 am to 5 pm, and it usually takes 2–3 hours for a full visit. Unlike the better-known ice sculptures, the snow sculptures are not lit up at night and are best seen in sunlight.\nActivities to take:\n- Admire the massive snow-white sculptures.\n- Have fun with the various snow sports. Compared with the entertainments at the Ice and Snow World, you are likely to find fewer people there, so you can feel freer to enjoy the displays and have fun.\n3) Zhaolin Park — Small but Lovely Ice Lanterns\nThe Zhaolin Park is mostly an ice world focused on small kids. The ice lantern sculptures are mainly made by college students or young ice sculpture enthusiasts. Over 300 fine ice sculptures are displayed, including internally-illuminated ice lanterns, magical figures, and animals that are loved by children.\nThere are fun activities that kids may enjoy, such as small ice slides. You may also find some un-carved ice blocks inside the park which are used for family competitions. If you happen to be there with an ice sculpture competition, join in for fun.\nUnlike the other two venues which charge a comparatively high entrance fee of 280–330 yuan/person (42–49 USD), Zhaolin Park is free to enter.\nThe park is usually open from 9am to 9pm. Choose a convenient time to visit around your other activities, as it is perfectly located in the city center.\nHow to Plan a Wise Ice Festival Tour in Harbin?\n1) Get Harbin Ice and Snow Festival Ticket in advance\nDuring the Covid-19 prevention and control period, the park will limit the flow of visitors, it is better to book the tickets online in advance.\nOfficial online booking is only open to domestic travelers with a Resident Identity Card. For foreign passport holders, you can buy on arrival or submit an inquiry below. We'll keep you in the priority list and contact you to confirm the booking once the tickets are available officially.\n2) Book the Tickets to Get to Harbin as Early as You Can\nHarbin can be reached by bullet trains and flights.\nFor coming from northern China, like Beijing, the bullet train takes no more than 5 hours. If you are coming from the eastern or southern part, a flight is recommended. Book the flight tickets as earlier as you can, you would find limited discounts on airfare when time is getting close. It is the high season of traveling in Harbin in Winter.\n3) Book Your Accommodation in Harbin\nHotels with affordable prices are mainly located near the Central Street of Harbin, within walking distance of the main sightseeing sites like the St. Sofia Orthodox Church and Zhaolin Park, and are convenient for shopping, eating out, and night activities.\nFor a luxury experience or family-friendly hotels, you may choose to stay in the Songbei District area, where many high-standard luxury hotels and family-friendly hotels are located.\nYou are welcome to contact us for more hotel recommendations.\n4) Pack for Harbin Ice Festival\nHarbin also called \"the Ice City\" is always well below 0°C in mid-winter. It could feel comparatively \"warm\" in the daytime in the sunlight, but pay special attention to keeping warm during the evening.\nFor packing, follow the link for the travel tips below: \"Don't underestimate the cold\".\n5) Eat in Harbin\nHarbin cuisine is a type of Northeast China cuisine, with strong flavors and a richness of calories and salt to combat the harsh weather. Harbin dumplings and their stew dishes are highly recommended. Bordering Russia, you can also find authentic Russian restaurants in Harbin.\nWhile you may find it is most convenient to have your dinner inside Harbin Ice and Snow World rather than the city center, don't worry: there are different types of restaurants for you to choose from — from local Harbin cuisine to Western fast food like KFC and Pizza Hut, etc.\nMore on Harbin cuisine >>>\n6) Enliven your Harbin Ice Festival Tour: There's More to Explore…\nHarbin is not just about the Ice and Snow Festival: there are many more interesting and fun things for you to explore.\nA mind-blowing show of winter swimming: winter swimming enthusiasts of different ages including seniors, drill a hole in the solid Songhua River, build a diving platform with ice blocks, and dive into this icy \"swimming pool\" in just their swimsuits! This is incredible to see. If you are wrapped up warm, you may still feel freezing just watching the show.\nUnbeatable fairy-tale games for families: Songhua River can be frozen up to 4–5 months a year, when the Songhua River Ice Carnival offers fun to all comers, especially kids and anyone who rarely plays in the snow and ice. Ice games and activities like ice go-carting, ice sledging, ice slides, ice mobiles, ice horse rides, and ice bikes are offered. You can buy a combined ticket at around 250 yuan/person (38 USD) or a ticket for each activity from 50–80 yuan/person (8–12 USD).\n- 1-Day Harbin Ice and Snow Fairyland Tour\n- 4-Day Harbin Ice Wonderland and Short Ski Tour\n- 6-Day Harbin, Yabuli, and China Snow Town Tour\n3 Harbin Ice Festival Travel Tips: 2 Don'ts and 1 Must-Do\n1) Don't underestimate the cold.\nEven if you are used to living in cold areas or skiing on snowy slopes every winter, attending the Ice festival could be a totally different experience. It is just like staying in a freezer. So, wrapping up warm is very important. Here are some tips to prevent harm from the cold:\n- Sub-zero clothing: In Harbin in winter, the temperature is typically from -25°C (-13°F) to -10°C (14°F), and that's without wind chill! Hence thermal underwear, gloves, (scarves, earmuffs), hats, (face masks), and thick arctic clothing and boots are necessary. If you have ski boots, they would be ideal.\n- Wear sunglasses, or better still snow glasses, to prevent snow blindness.\n- Drink plenty of fluids: Despite the cold, it is very dry in Harbin (average humidity 2%).\n- Frostbite prevention: If your hands or feet start feeling chilled, do some exercise to warm them up or go indoors. If your hands, feet, ears, nose, or other extremities lose feeling due to the cold, don't toast them or wash them with hot water. Ideally, retreat to a warmer indoor environment and rub them vigorously to restore circulation slowly.\n- Protect your camera batteries and mobile phones: The average January high in Harbin is -13°C (8°F). The cold saps battery life and could make your mobile phone shut down. Usually, a fully charged camera battery will last for 50 photos (without flash), but in these cold conditions around 25 is good. If you take the battery out and warm it for a while, you may be able to take another ten. You can try repeating this action. It is also recommended that you keep your camera and mobile phones inside your down jacket when not using them. If you want to take lots of photos prepare more batteries for your camera and keep them warm.\n2) Don't take public transportation to/from the festival venues.\nWhy? With so many people rushing to the festival venues, you may not be able to get on a bus or find a taxi. It will be freezing cold outdoors, and even if you are clothed heavily, while waiting on the streets, with scarcely a taxi even stopping, you could find yourself getting cold quickly. If you are lucky enough to find one, the buses/taxis still stop about 2–3 km (1–2 miles) from the entrance. You may find it a big challenge to walk a long way in the snow and on the icy pavements.\nWhen you finish your adventure, cold and tired, it could be a nightmare to find a taxi back. This is not an unrealistic threat, but a genuine experience of many, as attested by feedback from visitors attending the Ice Festival in previous years.\nYes, Subway Line 2 does stop close to the ticketing hall of the Ice and Snow World in 2022 but think of the crowds. Everyone will be rushing and squeezing inside the limited carriage space.\nIf this doesn't sound like it's the way for you, see our \"must-do\" solution below.\n3) Do Take a Harbin Ice Festival Tour\nIf you are looking for an unbeatable Ice Festival experience that will satisfy all your group members, do not worry. You have a choice that is second-to-none: Take the tour with China Highlights!\nWhy China Highlights?\n- 1. Your belongings will be kept safe from harm due to the cold.\n- 2. You'll be transferred safely and directly to each venue.\n- 3. Time-smart plans will help save you time and avoid long queues.\n- 4. Your time in the festival venues will be maximized with expert guidance.\nNo matter whether you are a solo traveler looking for a travel partner, a couple looking for something romantic and fairytale, or traveling with young children or with friends for fun and adventure, we can create a trip that is right for you. Here's our customizable One-Day Ice and Snow Fairyland Tour for inspiration or scan the QR code and tell us your needs.\nSurprising Facts on Harbin Ice Festival\n1) Why is Harbin International Ice and Snow Festival celebrated?\nIn the 1980s, after Harbin's traditional ice lantern show re-continued following the Cultural Revolution, ice lanterns in Harbin became famous all over the world.\nAt that time, when receiving Hong Kong, Macao, and Taiwan compatriots, who came to Harbin to enjoy the ice lanterns, the Propaganda Department of Harbin found that visitors not only loved ice lanterns but were also interested in snow displays and activities.\nSo, the idea of bringing Harbin's ice and snow to the world and the concept of the Harbin International Ice and Snow Festival came about. The first Harbin International Ice and Snow Festival was held in 1985.\n2) How is the Harbin ice world made?\nIn early December, a traditional Ice Harvest Ceremony is held on the frozen-solid Songhua River in Harbin. From then on, thousands of workers cut and collect ice blocks from the river while artists carve and connect the blocks to create various sculptures or buildings. In around 3–4 weeks, Harbin is transformed into a winter wonderland.\n3). How do artists in Harbin make ice sculptures?\nFirst, workers obtain the materials — ice blocks from the Songhua River.\nThen, the ice blocks are transported to the venue and stuck together to form large shapes of the desired dimensions.\nFinally, the artists use tools, such as shovels and saws to cut, polish, and shape the ice sculptures that are finally presented to visitors.\n4) Why the Harbin Ice Festival is so special?\nThis winter wonderland is dazzlingly awesome, yet ephemeral. It is a large project, but short-lived: it lasts no more than 2 months. How big is it? Consider for example the amount of ice used: over 200,000 cubic meters (80 Olympic pools). As a normal truck carries 4–7 cubic meters of ice blocks per time, at least 30,000 to 50,000 truck journeys are used for transportation.\n5) What happens to the ice sculptures after the festival?\nTemperatures in Harbin stay well below 0°C (32°F) for the three months of the Ice and Snow Festival. The ice sculptures do not melt until March or April when the weather gradually warms up. Some small ice sculptures are safe to let melt naturally. Some large ice sculptures that are dangerous when they collapse during the melting process are dismantled by construction demolition methods, e.g. implosion blasting or excavator and breaker.\nOur 4-Day Harbin Ice Wonderland and Short Ski Tour is customizable to include the Ice Harvest experience.","Harbin Ice Festival\nHarbin Ice Festival Facts\nOpening Time: early January – March\nVenues: Sun Island, Ice and Snow World and Zhaolin Park\nHarbin Ice Festival, also known as Harbin Ice and Snow Festival, is China's original and greatest ice artwork festival, attracting hundreds of thousands of local people and visitors from all over the world.\nThe 37th session of the festival will be held from November 2020 to April 2021. The best collections of ice artworks will be mainly exhibited in the following venues:\nSun Island Scenic Area\nIce and Snow World\nIce Lantern Garden Party\nYabuli International Ski Resort\nHistory of Harbin Ice Festival\nThe city's location in northeast China accounts for its arctic climate which provides abundant natural ice and snow. Subsequently, the 'Ice City' of Harbin is recognized as the cradle of ice and snow art in China and is famous for its exquisite and artistic ice and snow sculptures. The fabulous Ice Lantern Festival is the forerunner of the Harbin Ice Festival and is still the best loved part of the overall event in the opinion of all who go to that city each year.\nThe first Ice lanterns were a winter-time tradition in northeast China. During the Qing Dynasty（1644 - 1911), the local peasants and fishermen often made and used ice lanterns as jack-lights during the winter months. At that time these were made simply by pouring water into a bucket that was then put out in the open to freeze. It was then gently warmed before the water froze completely so that the bucket-shaped ice could be pulled out. A hole was chiseled in the top and the water remaining inside poured out creating a hollow vessel. A candle was then placed inside resulting in a windproof lantern that gained great popularity in the region around the city.\nFrom then on, people made ice lanterns and put them outside their houses or gave them to children to play with during some of the traditional festivals. Thus the ice lantern began its long history of development. With novel changes and immense advancement in techniques, today we can marvel at the various delicate and artistic ice lanterns on display.\nToday's Ice Lantern\nNowadays, ice lantern in broad sense refers to a series of plastic arts using ice and snow as raw material combining ice artworks with colored lights and splendid music. The specific patterns of ice lantern include sculptures, ice flowers, ice architectures and so on.\nToday, Harbin Ice and Snow Festival is not only an exposition of ice and snow art, but also an annual cultural event for international exchange. Every year, there are many ice sculpture experts, artists and fans from America, Canada, Japan, Singapore, Russia, China, etc. gathering in this city to participate ice sculpting competitions and to communicate with each other in the ice and snow world. Also, Harbin ice lanterns have been exhibited in most of China's main cities as well as in many countries in Asia, Europe, North America, Africa and Oceania. For more than 40 years, the city's local natural resource of ice and snow has been fully explored to provide joy and fun for visitors to the city. Now during the festival, many sporting competitions are also popular including ice-skating, sledding and so on. Weddings, parties and other entertainments are now very much a feature of this ice world, adding their own contribution to the celebrations of this great festival of art, culture, sports and tourism.\nThree Major Venues\n1. Sun Island\nIt is the site of the Snow Sculpture Exposition displaying a wonderful snow world. It has the world's largest indoor ice and snow art museum and it opens to the public from November every year.\n|Admission Fee||Snow Sculpture Exposition: |\nCNY 330 for adults\nCNY 165 for children between 1.2-1.4m (3.9 and 4.6 feet)\nFree for children under 1.2m\nOther time: CNY 30 (just admission fee)\n|Expo Opening Time||Late Dec. 2020 to the end of Feb., 2021|\n|Operating Hours during the Expo||08:00-17:00|\n|Transportation||1. Take bus 29, 47, 80, 119, 125, 126, 127, 211, 212, 213, or 215, and get off at Taiyangdao Daokou (Sun Island Crossing) Station. |\nHarbin Bus Search\n2. Take the cable car over the Songhua River directly to the scenic area.\n2. Ice and Snow World\nIt came into being in 1999 and is one of the world's largest ice architecture parks. The inspiration for the ice and snow sculptures there usually is derived from traditional Chinese fairy tales or world famous architectures such as the Great Wall, the Egyptian Pyramids, etc.\n|Admission Fee||CNY 330 for adults |\nCNY 200 for children between 1.2-1.4m\nFree for children under 1.2m\n|Opening Time||Late December 2020 to March 2021|\n|Operating Hours||11:00-21:30 (The lights are on from 16:30)|\n|Transportation||Take bus 29, 47, 80, 119, 125, 126, or 127 to Bingxue Dashijie (Ice and Snow World) directly. |\nHarbin Bus Search\n3. Zhaolin Park\nIt is a 'must see' during the Harbin Ice Festival because it has a traditional program that shows the most excellent ice lanterns. With water, lights and the natural ice from the Songhua River running through this city as the material, the ice lanterns are made by freezing water, piling up ice or snow, then carving, enchasing, decorating, etc. The Ice Lantern Garden Party has been held here annually since 1963. There are numerous pieces of ice artworks in the park arranged in groups according to different themes depicting Chinese classic masterworks, European folktales and customs and so on. A great variety of objects such as buildings, gardens, flowers, waterfalls, European-styled churches, lions, tigers, dragons are carved from ice. In the daytime, the ice sculptures are magnificent and verisimilitude. Moreover, with the interspersion of the sparkling colored lights embedded in the sculptures at night, the park becomes a glorious and amazing ice world.\n|Admission Fee||During the festival: CNY 150 per adult; free for children under 1.2m |\nOther time: Free\n|Ice Lantern Garden Opening Time||Dec. 30, 2020 to Feb., 2021|\n|Operating Hours during the Festival||10:00-21:00|\n|Transportation||Take bus 8, 23, 29, 52, 53, 83, 114, 132, 136, 201 or 206, and get off at Zhaolin Gongyuan (Zhaolin Park) Station. |\nHarbin Bus Search\nThe 37th Harbin Ice Festival Highlights (2021)\nIt is expected to start from November 2020 to April 2021, covering three important Chinese festivals – New Year's Day, Chinese New Year and Lantern Festival. A variety of activities about ice and snow, concerning tourism, culture, fashion, the economy and trade and sports, will welcom all tourists. People can enjoy beautiful ice sculptures and snow arts in the Sun Island Scenic Area, Ice and Snow World, Zhaolin Park and Central Street. In addition, St. Sophia Church, Harbin Polarland and Songhua River are also worth a visit during the festival.\n☆ 2020 - 2021 activities schedule for your reference:\n|The 27th National Snow Sculpture Competition||Dec. 23~28, 2020||Sun Island Snow Expo|\n|The 34th International Snow Sculpture Art Expo||Dec. 23, 2020~ Feb., 2021||Sun Island|\n|The 13th International College Students Snow Sculpture Competition||Jan. 4~7, 2021||Harbin Engineering University|\n|The 35th International Ice Sculpture Competition||Jan. 6~8, 2021||Zhaolin Park|\n|The 37th Ice Festival City Circuit Procession||Jan. 5, 2021||Harbin Expo Center to Ice and Snow World|\n|The 37th Harbin Ice Festival Opening Ceremony||Jan. 5, 2021||Harbin Expo Center|\n|The 37th Harbin Ice Snow Group Wedding Ceremony||Jan. 5, 2021||Sun Island|\n|The 32nd Hundred-Flower Dance Contest of senior people||Jan. 13, 2021||People's Stadium|\n|2021 New Year Concert||Dec. 31, 2020||City Concert Hall|\n|2021 International Children Art Variety Tournament||Jan. 5~10, 2021||Harbin Youth Theater|\n|2021 International Winter Swimming Game||Jan.1~5, 2021||Harbin Ice Festival Swimming Venue|\n|Songhua River Ice Snow Happy Valley||Jan. 1~Feb. 5, 2021||Songhua River|\n|The 27th International Snow Sculpture Competition||Jan. 10~13, 2021||Sun Island|\n|36th Harbin Ice Festival International Trade Fair||Jan. 5~8, 2021||Harbin Expo Center|\n|10th Central Street Ice Sculpture Art and Culture Festival||Dec. 25, 2020~Feb., 2021||Central Street|\nFurther Reading: Top 10 Things to Do in Harbin"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b5b7ab57-0274-4a12-8756-1a8dae2e4671>","<urn:uuid:6a264e72-e719-41bd-a729-a06006106a19>"],"error":null}
{"question":"Can games be used effectively for serious real-world purposes, and what evidence exists about their educational impact in college settings?","answer":"Games can indeed be used effectively for serious purposes, as demonstrated by examples like charitable fundraising through FarmVille raising $1 million for Haiti relief and educational testing being inherently game-like with clear objectives and constraints. Regarding educational impact in college settings, research shows mixed results - while active learning through games can increase academic performance by 0.47 standard deviations, some studies found that elements like leaderboards can actually demotivate students and lower academic results. Students are up to 150% more likely to fail in traditional lecture courses compared to those using active learning strategies.","context":["Amid all the enthusiasm that has emerged around using games for learning, health, persuasion, and social change, there’s a nagging problem: it’s the persistent cultural bias that games must necessarily be frivolous activities, disconnected from the important business of the real world.\nWe’re all aware of this prejudice because it’s deeply rooted in the way that we understand games and play. It’s apparent in the way we speak about games. When we say that something “is just a game,” we mean that we cannot consider it to be terribly important. When we warn that something “is not a game,” we’re saying that it must not be so debased. In our collective consciousness, the seemingly artificial nature of games clashes with these new substantive purposes to which they’re being applied.\nWhile user experience designers may be drawn to the power of games to command human attention, the cultural bias can make it difficult to imagine how we can best use games in practical situations. In order to create games that can have meaningful impact in the world, we need a different way of thinking about them.\nCharacteristics of Games\nTop-down approaches to defining games tend to be either so specific that it’s easy to find exceptions, or so vague that they don’t help to inform design. For those reasons, I’ll stay away from a standard dictionary-style definition, and instead build from the bottom up by focusing on the shared characteristics of games.\nAll games have objectives, some explicit, measurable, and reliable conditions that all players try to meet or maintain. Many games, such as Trivial Pursuit, contain a hierarchy of objectives that culminate in a final challenge that brings the play to an end. Other games have objectives without any winning condition; when people play tag, they just keep meeting the same objective over and over again until no one feels like playing any more.\nAll games contain elements that place physical limits on what the players can and cannot do, including the boundaries and structure of the play space. Chess is played on a grid of 8×8 squares of alternating colors; it wouldn’t work the same way if it were played on a 7×7 grid or a 9×9 grid. Environmental constraints also include the artifacts like dice, playing cards, pawns, balls, and bats. The physical characteristics of these things also shape the gameplay. A deck of cards only has four aces, no matter how much you might need a fifth one.\nGames also constrain people’s actions through rules. These formal constraints perform a job similar to the environmental constraints, but they’re fundamentally different because players are bound to the rules only by mutual agreement. In the absence of hard physical constraints, people must volunteer to place limits on their own freedom. The players who join a game must then see some value in the experience.\nThese three characteristics represent everything that you need to uniquely describe a particular game. A fourth represents the additional attributes of video games.\nThis characteristic is specific to a sub-type that includes video games, slot machines, pinball, and pachinko. These games arbitrate achievement of objectives and compliance with the rules, either through some physical mechanism or by computer. This relieves players of the burden of policing one another, sorting out winners from losers, or supporting various mundane bookkeeping functions. It also creates a huge design advantage in video games, because it allows the rules to be much more complex and to be executed much faster than would otherwise be possible.\nThere are, of course, many other characteristics of games, but they are the qualities that describe the heart of a game, and they cannot be altered without fundamentally changing the game. Chess can be played with a variety of different figures, against a computer, or fully life-sized without changing the fact that you’re still playing chess. But if you were to make any change to the objectives, the environmental constraints, or the formal constraints, it just wouldn’t be chess.\nTaking the common characteristics a step further, any human task that can be defined using the characteristics can be understood as a game, even if it’s a part of what we would normally call real life. Three examples are educational testing, dieting, and charitable fundraising. These activities are not just game-like; nor do they contain some elements of games. They are indistinguishable from games, even though we are unaccustomed to thinking of them that way. Moreover, taking advantage of the principles of game design, we can make these activities more compelling and increase our effectiveness when engaging in them.\nGames have long found a place in education, from competitions like spelling bees or debates, to video games like The Oregon Trail and Math Blaster! Their natural fit within the classroom makes abundant sense if we define education itself as a game. Consider how a science test conforms to the characteristics used to define games:\n- Objective: Demonstrate your understanding of the structure of the solar system.\n- Environmental constraints: The set of questions selected for the test, the form of the questions, and the information they provide. For example, multiple choice questions providing insight into possible responses.\n- Formal constraints: You can’t copy other students’ work; you have a specific amount of time in which to complete the test.\nConsidering that students are usually drawn to game experiences, it’s a shame that testing can be a source of anxiety. Game design can promote a better way of doing things. The wildly successful Professor Layton series for the Nintendo DS presents puzzles that can be similar to SAT questions, but wraps them inside a game narrative that involves solving a mystery. Lee Sheldon, an instructor at Indiana University, structures his game design class as a game itself, with students working in teams called “guilds” to earn “experience points” that they can apply to defeat an ultimate “boss”—the final exam. Approaches that take advantage of the inherent gamefulness of education have the potential to transform our systems of learning.\nYou have an objective to lose a certain amount of weight by some point in time. You have to do it despite hard physical constraints on how your body metabolizes food, your current level of physical fitness, the foods that are available to you, and your personal tastes in food. There are also formal rules that you follow, eating only foods that are on the diet, maintaining daily exercise, and avoiding shortcuts like weight-loss drugs.\nA video game that completely capitalizes on the gamefulness of dieting has yet to be made, but there are some existing designs that address parts of the experience. Zamzee gives players a movement sensor and awards a virtual currency for higher levels of activity, which can be used to purchase virtual or physical rewards. HopeLab, the game’s developer, reports that teens who play Zamzee are thirty percent more active than peers who don’t. My game Fitter Critters tasks players with buying food for a virtual pet to improve its health over time. In the process, players need to weigh the nutritional attributes of different foods, and build a mental database of better and worse food choices.\nWe can speculate about the form that a video game focused specifically on dieting might take. It could, for example, be conceived as a Dungeons & Dragons role-playing game, featuring an avatar that’s directly based on the height, age, and body mass index of the player. The avatar’s attributes could be affected by the player’s actual physical activity and food choices, leveling up and achieving new goals as the player reaches milestones in the diet. Such a game could encourage adherence to the diet by tying it to success in the game world.\nFundraising comprises all the characteristics of games. The objective is a specific level of funding; the environmental constraints are the means of communication with potential donors and the funds they have to contribute, and the formal constraints are laws protecting nonprofit charitable organizations, as well as the interpersonal norms of asking for donations and expressing gratitude.\nFollowing the earthquake that devastated Haiti in 2010, the Facebook game FarmVille offered virtual white corn seeds for sale for real money and donated the proceeds to Haitian relief. A special crop granted players a large amount of experience points. It was lucrative. You could harvest it after a short time, and it would never wither and die like other crops if you left it untended. It cost Zynga almost nothing to create white corn, but they were able to use it to raise $1 million in donations in five days.\nPlayers of the related Facebook game CityVille also have the option of getting game-specific city cash by making actual monetary donations to local chapters of United Way. These innovations generate real value from many small donations, tying game-based rewards to desired behaviors. They have the potential to bring in many more donors and to vastly increase the effectiveness of fundraising campaigns.\nGames in Vital Contexts\nGames are a part of life, often guiding our actions in unrecognized ways. By capitalizing on the inherently gameful attributes of everyday activities, user experience designers have the opportunity to influence human behavior and to create new kinds of interactions. Of course, not everything in life can fit our definition of games, and the approach to design isn’t well-suited to activities that fall outside our defined structure. But this caution should not be an obstacle to applying game design to the right projects.\nLet’s get rid of the cultural predisposition that games are inherently frivolous, which only holds us back from exploiting a deep mode of interactivity. Using these techniques, we can create satisfying games that can bring about real change in the real world.\nRetrieved from http://uxpamagazine.org/games-in-the-real-world/\nComments are closed.","This is a guest post by Dr. Szymon Machajewski, Blackboard MVP, faculty with the School of Computing & Information Systems at Grand Valley State University.\nGamification: the use of game design elements in nongame contexts (Deterding, 2012)\nGamification is a buzzword that sometimes receives a skeptical welcome in higher education. Some may wonder whether playing games has any place in a college level class, and the research results are mixed. Some researchers found that applying leaderboards to a college course can actually demotivate students and lower academic results (Hanus & Fox, 2015). Gartner warned that as much as 80% of gamification projects are likely to fail (Gartner, 2012). So why bother with a pedagogy that may be risky to adopt and at the same time represents controversial characteristics of childish play and perceived lack of seriousness?\nIn my view, the risk is worthwhile. Lack of engagement in traditional courses, especially introductory college courses, discourages students and faculty. This lack of engagement can perhaps be due to what has been called ‘dysfunctional illusions of rigor.’ Craig E. Nelson from Indiana University coined the term to reflect on research-debunked traditional views of college instruction. Some examples include: “1. Hard courses weed out weak students. When students fail it is primarily due to inability, weak preparation, or lack of effort. 2. Traditional methods of instruction offer effective ways of teaching content to undergraduates. Modes that pamper students teach less.”\nRather than ‘pampering’ students, active learning has proven to make a significant difference in academic results and student engagement. In addition, research published by the American Psychological Association (APA) found that play is good for learning and it holds a large potential for teaching new forms of thought and behavior. In that same study, playing digital games was found to boost learning, health, and social skills.\nAt Grand Valley State University, I have conducted research on gamification which has led to the development of a “short and long game theory” (Machajewski, 2017). The application of gamification in academic courses requires the gameful design of individual lecture periods, as well as the gameful design of the semester-long student journey.\nSome examples of short game elements include:\n- Daily Journal\n- One Minute Paper\n- Problem Based Learning\n- Quizlet Live\nExamples of long game elements include:\n- Exam Peace of Mind Points PofM\n- Formative Assessments\n- My Progress instead of My Grades\n- XP Ledger – MyGame\nWhile active learning fits well in the “short game” design, something else is needed to make the course “long game” worth playing for students. We have implemented the “long game” design in our Introduction to Computing course at Grand Valley State University, which earned exemplary course status in the 2017 Blackboard ECP program. The key to the “long game” is the adoption of an experience points (XP) ledger. This instrument allows students to earn XP during lectures, homework assignments, hands-on practice sessions and track the total across the length of the course.\nSchool grades used to serve as the “long game” strategy. However, today they are more likely to demotivate students than encourage them to conduct deeper exploration or to appreciate the subject matter. Grades have become a high stakes extrinsic reward. Just as money—in the research of Daniel Pink—is a poor motivator of knowledge workers, grades are a poor motivator of intellectual performance for students.\nStudents are knowledge workers and require creative thinking. To motivate knowledge workers, we need to appeal to autonomy, mastery, and purpose. These three characteristics are based on the self-determination theory (Ryan & Deci, 2000). Not only do we care about the intellectual performance of students during class, but we also want students to grow their affinity for the subject matter. In other words, we want to avoid the sentiment: “I got an A in the class, but I hate math, and I hope I will never have to use it again in my life.”\nFor gamification to be successful in academic courses, you need to have exemplary instructional design, which is why the value of the Blackboard ECP program is directly related to gamification. Clarifying instructions for students, setting attainable course and learning objectives, and chunking content to encourage progress are all efforts to turn a course into well-designed work, which is one of the definitions for a game.\nA focus on student experience creates the common ground between instructional design and gamification. Best practices of instructional design recommend active learning as the most effective way to engage students in the classroom. While tutoring may increase grades by two standard deviations (Bloom, 1984), active learning could increase academic performance by 0.47 standard deviations (Ruiz-Primo, Briggs, Iverson, Talbot, & Shepard, 2011). Some students are 150% more likely to fail in courses dominated by traditional lectures over courses with active learning strategies (Freeman et al., 2014).\nMy gamification research (Machajewski, 2017) recommends Quizlet Live and Kahoot! as active learning platforms with a competitive edge. The “short game” in courses depends on classroom, hybrid, or online deliveries. However, the “long game” approach is just as important to consider. Opportunities for improvement can be found in mitigating student exam anxiety, sufficient encouragement of failure in practicing of hands-on material, and adopting intermediate due dates to ensure an ongoing growth of mastery.\nThe specific technologies used in my Introduction to Computing course were presented in New Orleans at the BbWorld 2017 conference and can be reviewed in the conference publication. My theory of the “short and long game” in academic courses is being developed on the background of case studies and emerging gamification tools. Some examples of the case studies include a Germanic Studies course and STEM introductory college course.\nGamification in an academic course can be considered a complex system of many tools within two main categories: short-term engagement and long-term participation. Adoption of just a few of them separately, such as a leaderboard, badges, a points system, or classroom response system may not lead to an intrinsically satisfying experience and expected high levels of motivation. This follows the Anna Karenina principle (Bornmann & Marx, 2012) present in complex systems.\nConversely, gamification studies showing negative results should not reflect a general lack of applicability of gameful design in academic courses. A consideration of both the short game and the long game will help faculty experience more benefits of gameful design. In turn, the reciprocal quality of engagement is likely to affect the faculty and the students.\nBloom, B. S. (1984). The 2 Sigma Problem: The Search for Methods of Group Instruction as Effective as One-to-One Tutoring. Educational Researcher, 13(6), 4. doi:10.3102/0013189X013006004\nBornmann, L., & Marx, W. (2012). The Anna Karenina principle: A way of thinking about success in science. Journal Of The American Society For Information Science & Technology, 63(10), 2037-2051. doi:10.1002/asi.22661\nBurke, B. (2014). Gartner Redefines Gamification. Retrieved from http://blogs.gartner.com/brian_burke/2014/04/04/gartner-redefines-gamification/\nDeterding, S., (2012). Gamification: designing for motivation. Interactions 19, 14–17.\nFreeman, S., Eddy, S. L., McDonough, M., Smith, M. K., Okorafor, N., Jordt, H., and Wenderoth, M. P., (2014). Active learning increases student performance in science, engineering, and mathematics. Proceedings of the National Academy of Sciences (PNAS), 111(23),8410-8415.\nHanus, M., & Fox, J. (2015). Assessing the effects of gamification in the classroom: A longitudinal study on intrinsic motivation, social comparison, satisfaction, effort, and academic performance. Computers & Education, 80152–161.\nMachajewski, S. (2017). Application of Gamification in a College STEM Introductory Course: A Case Study. Retrieved from https://eric.ed.gov/?id=ED574876\nRyan, R., & Deci, E. (2000). Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being. American Psychologist, 55, 68–78.\nRuiz-Primo, M.A., Briggs, D., Iverson, H., Talbot, R., Shepard, L.A. (2011). Impact of undergraduate science course innovations on learning. Science 331, 1269–1270."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:6a741d31-3c7d-4ead-901c-854c93744639>","<urn:uuid:db208e75-c6e0-4f02-aea4-44c708cac664>"],"error":null}
{"question":"What's the difference between how diamonds are polished and how their hardness is measured?","answer":"Diamonds are polished through grinding against cast-iron wheels embedded with diamond fragments, a process that creates soft amorphous layers that can be removed to leave clean surfaces. In contrast, diamond hardness is measured using the Mohs scale, where a pyramid-shaped diamond needle is used to scratch the surface of minerals - diamond ranks at the top of this scale at 15, serving as the hardness standard against which other minerals are compared.","context":["Secret of diamond polishing revealed\nDec 2, 2010 2 comments\nIt is the hardest everyday material on Earth, so why does diamond glisten when rubbed against another diamond? Now, the ancient but mysterious process of diamond grinding may have been explained by physicists in Germany, who have created a model for explaining the frictional interactions at the molecular level.\nFor centuries precious-stone merchants have polished diamonds by grinding them with cast-iron wheels embedded with coarse diamond fragments. It is not clear why this procedure is so effective at cleaning diamonds, but experience suggests that it works far better when the diamond is fixed at certain angles to the wheel than others.\nThis directional dependence of diamond grinding has now been investigated by Lars Pastewka at the Fraunhofer Institute for Mechanics of Materials who set out to investigate the phenomenon. Working with colleagues at several other institutes across Germany, he has developed a quantum mechanical model to study the atomic interactions in \"diamond-like\" carbon films, which are often used in industry to reduce friction in machinery.\nDiamond in the rough\nBut when the researchers applied their model to diamond itself, they were surprised to find that it accurately predicted the experimental wear rates for this material – even though the exact wear mechanism has so far remained poorly understood. \"At this point we became very excited about this work and analysed our simulations in much more detail to uncover the details of the process,\" Pastewka told physicsworld.com.\nPastewka's team set about simulating diamond grinding using 70 computer processors running for a year, and discovered that during the grinding the diamond surfaces were being transformed into soft, amorphous layers. These thin films can then be easily removed by either chipping them away, or through carbon molecules bonding with oxygen in the atmosphere, leaving behind clean diamond surfaces.\nThis creation of the amorphous film occurs because of existing imperfections at the diamond surface, including the build-up of dirt over time. As a diamond atom slides over the surface it repeatedly pulls at the diamond crystal's atoms, and sometimes removes an atom from the crystal surface, which becomes part of the amorphous layer.\nLike a stack of paper clips\n\"Imagine you have a stack of paper clips neatly arranged on your desk,\" explains Pastewka. \"Now you take a magnet and move that over these clips at a certain height. You cannot keep the height ideally constant, so if the height is right you will pull some paper clips to your magnet and others will remain on the desk.\"\nChangfeng Chen, a materials scientist at the University of Nevada in the US is impressed by the research and its potential to boost industrial processes. \"This research is of particular significance in nanotechnology where the orientations of nanoscale crystallites can be well defined and controlled,\" he says. \"The predicted orientation-dependent anisotropic amorphization wear mechanism may open doors to a new level of material processing, ranging from better designer jewellery to superior high-tech device components.\"\nTo develop the work, however, Pastewka's team intends to further investigate diamond's surface chemistry, and is currently writing a paper on the oxidation of the amorphous layer.\nThis research is described in a research paper in Nature Materials.\nAbout the author\nJames Dacey is a reporter for physicsworld.com","Moh’s Scale of Hardness\nThere are two methods to measure the hardness of materials: scratch hardness and static load indentation hardness. Scratch hardness, also known as Mohs hardness, is a relative hardness and is rather rough.\nIt uses ten natural minerals as standards. The hardness order does not represent the absolute size of a particular mineral’s hardness, but indicates that a mineral of higher hardness order can scratch a mineral of lower order. The hardness of other minerals is determined by comparison with these standard minerals.\nThe unit of Mohs hardness is kilogram-force per square centimeter (kgf/cm²), denoted as [Pa]. It’s a standard for expressing a mineral’s hardness, first proposed in 1824 by German mineralogist Frederich Mohs. The hardness is represented by the depth of the scratch made on the surface of the tested mineral using the scratch method with a pyramid-shaped diamond needle.\nThe hardness scale is as follows: talc 1 (softest), gypsum 2, calcite 3, fluorite 4, apatite 5, orthoclase (also known as feldspar or periclase) 6, quartz 7, topaz 8, corundum 9, diamond 10 (hardest). Mohs hardness is also used to express the hardness of other solid materials.\nFor a more specific method: one would scratch the mineral to be tested against the standard hardness on the Mohs hardness scale to determine the hardness of the tested mineral.\nFor example, if a mineral can scratch calcite and be scratched by fluorite, then the hardness of that mineral is between 3 and 4. Alternatively, one can use a fingernail (hardness 2-2.5), a coin (hardness 3.5), or a small knife (hardness 5.5) to scratch the mineral in order to broadly determine its hardness.\n|Representative Mineral Names||Common Uses||Hardness Scale|\n|Talc, Graphite||Talc is the softest known mineral, commonly used in the form of talc powder.||1|\n|Skin, Natural Arsenic||1.5|\n|Nails, Amber, Ivory||2.5|\n|Gold, Silver, Aluminum||2.5~3|\n|Calcite, Copper, Pearls||Calcite can be used as carving material and industrial raw material.||3|\n|Fluorite (also known as Fluorspar)||Carving, Metallurgy, Building Materials||4|\n|Phosphorite||Phosphorus is an important component of biological cells; it is used as raw material in feed, fertilizer, and chemical production.||5|\n|Glass, Stainless Steel||5.5|\n|Orthoclase, Tanzanite, Pure Titanium||6|\n|Teeth (outer layer of crown)||The main component is hydroxyapatite.||6~7|\n|Soft Jade – Xinjiang Hetian Jade||6~6.5|\n|Pyrite||It is used as raw material for the production of sulfuric acid; gold refining; and can also be used in medicinal purposes.||6.5|\n|Hard Jade – Burmese Jadeite and Jade||6.5~7|\n|Quartz Glass, Amethyst||7|\n|Electric Stone, Zircon||7.5|\n|Quartz||According to the old hardness scale, quartz is rated as 7.||8|\n|Topaz, Chromium, Tungsten Steel||On the old hardness scale, topaz is rated as 8.||9|\n|Moissanite||Synthetic gems are 2.5 times brighter than diamonds and cost 1/10th of the price.||9.5|\n|Corundum||Corundum is rated as 9 on the old hardness scale. Natural gems such as rubies and sapphires are now considered types of corundum, as is the hardness of synthetic sapphire crystals.||12|\n|Diamond||Diamonds are rated as 10 on the old hardness scale, making them the hardest natural gem on earth.||15|\nWhat is Mohs Hardness?\nMohs Hardness is a standard that indicates the hardness of minerals, first proposed in 1824 by German mineralogist Friedrich Mohs. This standard is established by using a pyramid-shaped diamond drill to scratch the surface of a mineral, with the depth of the scratch indicating the hardness.\nThe hardness of a mineral refers to its ability to resist certain external mechanical forces such as scratching, indentation, or grinding. In mineralogy, the hardness often referred to is Mohs hardness, which is the scratch hardness compared to the Mohs hardness scale.\nThe Mohs hardness scale is based on ten minerals of different hardness, divided into ten levels from low to high: 1. Talc; 2. Gypsum; 3. Calcite; 4. Fluorite; 5. Apatite; 6. Orthoclase; 7. Quartz; 8. Topaz; 9. Corundum; 10. Diamond.\nIn use, standard minerals are scratched against minerals of unknown hardness. If the mineral can be scratched by apatite but not by fluorite, its hardness is determined to be between 4 and 5.\nThis method was established and named by German mineralogy professor Friedrich Mohs (1773-1839). However, accurate measurement of mineral hardness still requires a microhardness tester or hardness tester. Mineral hardness is also one of the physical properties of minerals. Minerals with high hardness have been widely used in industrial technology.\nDiamonds, corundum, and other minerals are not only used in industry, but also become precious gemstones. As gemstones, they usually have a high hardness.\nFor example, the hardness of opal is 5.5-6.5, quartz is 6.5-7, sphalerite is 7.5-8. Tsavorite is 8.5, and the hardness of sapphires and rubies is 9, second only to diamonds. People choose high-hardness minerals as gemstones, probably because they are wear-resistant, symbolizing their timeless value!\nAccording to needs, people have also developed a gem hardness scale to identify the mineral hardness of gemstones, from the softest to the hardest minerals: talc, gypsum, calcite, fluorite, apatite, zircon, corundum, silicon carbide, boron carbide, diamond, etc.\nWhen there is no standard hardness mineral, the simplest way to measure hardness is with a fingernail or a small knife. The hardness of a fingernail is 2.5, a copper coin is 3, and glass and a small knife are both 5. Those above 6 are almost all gemstone-like minerals."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:51dac3f7-2c98-4af3-9c87-ea06b7d39370>","<urn:uuid:6f5dff6d-ddef-43d0-937c-cbff7f05f328>"],"error":null}
{"question":"Tell me recent trends in web design for busy devs - logo setup + accessibility hacks?","answer":"For logos, research shows left-aligned placement is currently the most effective approach, leading to better brand recall and easier navigation. The logo should be exported in high-quality format and positioned on white space to avoid distractions. For accessibility, key implementations include: adding alt text/descriptions to images and banners, implementing skip links to help keyboard navigation, using proper heading hierarchy, ensuring good color contrast, and creating descriptive link texts. These elements make websites more accessible to users with disabilities while maintaining effective branding.","context":["Where to Put Your Logo? What the Research Says\nYou’ve got a beautifully designed logo. Well done. Now, what do you do with it?\nOnce your high quality logo is designed and ready to go, it should appear on all your branded material, including your WordPress website. Typically, there are three schools of thought as to where logos can go: in the top-left, top-middle, or top-right corners of a page. So this begs the question: which position is right for your logo?\nIf we’re going strictly based on UX logic, then your logo belongs in the top-left corner of your website. No questions asked. There are two reasons for this:\n- For those of us with a native language that reads from left to right, our eyes naturally look to the left first.\n- In the earlier years of web design, logos were always on the left, and that’s where most people assume they are located now.\nDespite what logic says, there are some websites that have eschewed the norm for logo placements in the center or right corner of a website. Would a unique placement of your logo fit better with your brand identity?\nLet’s take a closer look at what the studies show, and see if left really is right for your site.\nExperts Weigh In: Where to Place Your Logo?\nAccording to the Nielsen Norman Group, there are three purposes that logos serve on modern websites:\n- They remind visitors where they are. In other words, whose website am I on?\n- They allow for easy navigation to the home page since most websites no longer include a “Home” button in the navigation.\n- They aid in brand recognition as the logo always remains there at the top of the site, and sometimes even follows visitors as they scroll down a page.\nKnowing this, the NNGroup sought to find out whether there really was an ideal location for a logo. Here is what they found:\nLeft vs. Right\nIn the left-versus-right study, the NNGroup tested and observed the response of 128 users. Similar to an A/B test, each user was shown only one version of a website: either the original with the logo on the left or the one the NNGroup manipulated in order to place the logo and navigation on the right.\nThey gave the users a minute’s time to review their version of the website. They were then asked a series of questions and shown photos of 10 hotel websites. This test aimed to establish what sort of effect logo placement had on brand recall.\nThese were the results:\n- Left-aligned logos lead to greater brand recall. Specifically, the average brand recall for left-aligned logos was 39% as opposed to 21% for the right-aligned version.\n- Left-aligned logos are more likely to be labeled “unique.” Despite the traditional placement of a left-aligned logo, respondents were still more likely to label a left-aligned logo as “unique” and “stylish” than one that appeared on the right.\nLeft vs. Center\nThe Nielsen Norman Group conducted an additional study to discover what happens when users were exposed to a center-aligned logo. They conducted two different tests to determine the viability of a centered logo.\nUnlike the A/B test conducted between left- and right-aligned logos, this first survey provided 50 users each with one retail website to interact with. Eight of those websites had a centered logo whereas six had a left-aligned logo. They were then asked to complete different tasks that would test their ease in using the navigation and return to the home page.\nThis was the result:\n- Left-aligned logos are better for navigation. In the study, only 4% of users failed to navigate home in a single click when the logo appeared on the left. When the logo was centered, however, 24% of users failed to get there in one click.\nIn the second of these tests, the NNGroup presented 128 users with five different hotel websites. Four of the logos showed variations of a logo on either the left or in the center, while the fifth site included a right-aligned logo. Users were then asked a series of questions to determine brand recall.\nThis was the result:\n- Brand recall is unaffected by the difference between left or centered logos. Despite presenting users with variations of the same logo in different spots, brand recall was inconsistent in this comparison between left-aligned and centered logos.\nSummary of the Results: Left vs. Center vs. Right\nAfter reviewing the results from the logo placement studies, it appears that logic does prevail:\nLeft is best.\n- Many people don’t think about looking in the right-hand corner of a website for a logo, and so brand recall can be severely compromised as a result.\n- Many people are conditioned to look for a navigation in one of the top corners of a site, so placing a logo and/or navigation in the center confuses the process of getting around a website.\nIn summary, left-aligned logos are ideal as they are located exactly where the eye is naturally drawn.\nJust remember that web design’s primary concern should always be with the user experience. While a centered logo may look sleek, it probably isn’t ideal if you have more than one page on your website (and you expect people to visit those other pages). The same goes for a right-aligned logo. It may seem like you’re giving your site a unique edge, but you may hurt your brand’s recall in the process.\nIt’s also important to keep in mind that it’s not just logo placement that matters. Logos should be exported in a high-quality file format, created using a legible font face, and positioned on top of white space so that background imagery, shadows, and colors can’t distract from it.\nIf you’re trying to get creative with your WordPress site’s design, there are other ways to go about doing it. You could create a killer landing page, add push notifications, or revamp your CTA buttons.\nIf this study has shown us anything, it’s that you shouldn’t mess around with the placement of your logo. Left is always best.","As the number of internet users keeps rising, web accessibility is becoming a more pressing concern. Due to the way some websites are designed, they can be difficult for people with special requirements to access, such as those who use screen readers. Not considering accessibility can lose you a lot of traffic.\nFor this reason, it’s vitally important that you design your affiliate program with accessibility in mind. This ensures that as many people as possible can use it without problems. Not only will this improve your affiliates’ user experience, it can even help to make your program more successful.\nIn this article, we’ll discuss web accessibility and why it’s important. We’ll also show you how to take your first steps towards making your program more accessible for everyone. Let’s get started!\nA Brief Introduction to Web Accessibility\nWeb accessibility is the practice of designing websites that can be used by anyone, regardless of whether they have a disability or other special needs. This is an important topic, especially as the number of internet users increases.\nIf you design any type of site and don’t make it accessible, you’re effectively shutting out a lot of your potential audience. This can be especially harmful for an affiliate program, as you’re limiting the number of potential affiliates who can sign up. As such, it’s important to consider how you can improve the accessibility of your affiliate website.\n3 Ways You Can Make Your Affiliate Program More Accessible\nNow, let’s discuss some of the ways you can improve accessibility for your affiliate program. Bear in mind that this is by no means a comprehensive list, so we recommend that you refer to sources like WebAIM and W3C for more information on designing with web accessibility in mind. However, the following three techniques are a perfect way to get started right now.\n1. Add Image Descriptions to Your Banners\nIncluding alt text for your site’s images is a crucial aspect of web accessibility. Alt text displays when the image itself cannot be loaded for whatever reason. However, it’s also used by screen readers to ‘describe’ the image to vision-impaired users.\nAs such, you’ll want to take care to add alt text to all the images on your site. While you aren’t able to do this for the banner ads you upload to your affiliate program, you can still achieve the same result by adding an image description. You can do this when creating a banner in Affiliate Royale > Links & Banners:\nBy adding a detailed description to the Info field, you can describe the image so users with assistive technology will understand what it contains. This can even be helpful for other affiliates, such as those who aren’t fluent in English and are using automatic translation software.\n2. Implement ‘Skip Links’\nOne of the most important aspects of creating an accessible site is making sure it’s keyboard-friendly. This means that a user should be able to easily navigate your site using only a keyboard (since some people have difficulty using a mouse). In practice, most sites achive this by enabling users to navigate using the tab key to jump between objects on the page.\nOne issue that can result from this setup is that users are required to tab through all your navigational elements just to get to a page’s content. To avoid this hassle, you can implement ‘skip links’, which let people jump ahead to the actual content. This is especially important for your affiliate dashboard, as affiliates may otherwise be stuck in your menus when they just want to access their personal information and resources.\nThere are a number of ways you can add skip links, but one of the easiest is to use a plugin. WP Accessibility is an excellent option, and makes it easy to add skip links to your site’s theme (among other features).\n3. Design Your Program’s Site With Accessibility in Mind\nFinally, in order to create a truly accessible affiliate program, you’ll need to keep the main elements of web accessibility in mind throughout the entire website design process. Let’s quickly run through some of the most important things you’ll want to consider:\n- Use headings correctly. Make sure that you implement headings and use them correctly, as they help assistive software to better understand the layout and content of your site.\n- Consider color contrast. If you use colors that are too similar or clash too much, your site could become difficult to read. This can cause problems for users with colorblindness or photosensitive epilepsy.\n- Use descriptive links. Your links’ anchor text should always clearly describe their purpose. You can do this both when designing your site and your affiliate dashboard, but also when adding descriptions to your affiliate links.\nWhile this can seem like a lot of work, it will help you avoid creating a site that’s difficult for a considerable number of potential affiliates to use. Building an accessible site will benefit both your affiliates and your program in the long run. Plus, most of these techniques can be used to improve an existing site as well as a brand-new one.\nWeb accessibility is becoming a more important consideration by the day, and it’s important to keep it in mind when designing the interface for your affiliate program. Fortunately, WordPress and Affiliate Royale can help you make your site more accessible, which makes the experience of using your program more enjoyable and increases its long-term potential for success.\nIn this article, we’ve shown you a few ways you can improve your affiliate program’s accessibility. These include:\n- Add image descriptions to your banners.\n- Implement skip links.\n- Design your program’s site with accessibility in mind.\nDo you have any questions about how to make your affiliate program as accessible as possible? Let us know in the comments section below!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c8b9ccd4-abef-4435-99a3-98cfa960ac6f>","<urn:uuid:4c095307-e893-4d71-8413-66317d8ea493>"],"error":null}
{"question":"Can the GeForce GTX 1650 and Dark Souls run together, considering their technical requirements?","answer":"Yes, the GeForce GTX 1650 can run Dark Souls as it exceeds the game's recommended system requirements. The GTX 1650 features GDDR6 memory and a boost clock of 1785MHz, which surpasses Dark Souls' recommended graphics card requirement of a GeForce GTX 660. The game needs only 2GB of graphics memory, while the GTX 1650 provides 4GB of GDDR6 memory.","context":["Turing Architechture, Cuda Cores: 896, Boost Clock: 1785MHz, GDDR6, 128-bit Memory, 1 x DisplayPort, 1 x HDMI, 1 x DVI-D\nThis product is only available to buy when in stock\nSorry, this product is no longer available to purchase.\nPlease view our whole range of NVIDIA GTX 1650 here\nOrder Code: ASU-165TOP\nThe GeForce® GTX 1650 is built with the breakthrough graphics performance of the award-winning NVIDIA Turing™ architecture. It’s a supercharger for today’s most popular games, and even faster with modern titles. Step up to better gaming with GeForce GTX.\nFeaturing concurrent execution of floating point and integer operations, adaptive shading technology, and a new unified memory architecture with twice the cache of its predecessor, Turing shaders enable awesome performance increases on today’s games. Get 1.4X power efficiency over previous generation for a faster, cooler and quieter gaming experience that take advantage of Turing’s advanced graphics features.\nEasily upgrade your PC and get game ready with performance that’s up to 2X the GeForce GTX 950 and up to 70% faster than the GTX 1050 on the latest games.\nCapture and share videos, screenshots, and livestreams with friends. Keep your GeForce drivers up to date and optimise your game settings. GeForce Experience™ lets you do it all. It’s the essential companion to your GeForce graphics card.\nThis powerful photo mode lets you take professional-grade photographs of your games like never before. Now, you can capture and share your most brilliant gaming experiences with super-resolution, 360-degree, HDR, and stereo photographs.\nCreate without compromise. Studio combines NVIDIA GeForce GPUs with exclusive NVIDIA Studio Drivers designed to supercharge creative applications. Unlock dramatic performance and reliability—so you can create at the speed of imagination.\nThe TUF Gaming GeForce GTX 1650 stacks an arsenal of weapons that bring reliable 3D horsepower to the TUF Gaming ecosystem. Each card is built using Auto-Extreme manufacturing, protected by a rigid backplate that prevents PCB flex, and features fans with space-grade lubricant that are sealed to IP5X standards. And it's all backed by a rigorous battery of validation tests to ensure compatibility with the latest TUF components. If you're looking for the tank of graphics cards, lock and load your rig with the TUF.\nDust is enemy number one for fans. It's easy to clean the blades, but taking the fan apart to remove dust that has made its way inside is not a simple task. Passing the IP5X certification means the housing is safe from nasty particulates, so you'll never need that deep clean.\nInfused with a durable lubricant often found in aerospace applications, these sleeve bearings deliver a quieter acoustic profile than dual ball bearing designs and match their durability.\nAuto-Extreme Technology is an automated manufacturing process that sets new standards in the industry by allowing all soldering to be completed in a single pass. This reduces thermal strain on components and avoids the use of harsh cleaning chemicals, resulting in less environmental impact, lower manufacturing power consumption, and a more reliable product overall.\nThe PCB is reinforced by a backplate that adds structural rigidity, helping to prevent flex and protect components and trace pathways from damage.\nPerfect proportions maximise compatibility with the latest chassis\nThe ASUS GPU Tweak II utility takes graphics card tuning to the next level. It allows you to tweak critical parameters including GPU core clocks, memory frequency, and voltage settings, with the option to monitor everything in real-time through a customisable on-screen display. Advanced fan control is also included along with many more features to help you get the most out of your graphics card.\nASUS TUF Gaming and TUF Gaming Alliance products from our partners are subjected to a rigorous battery of validation tests to ensure a seamless plug-and-play experience. All products also share a common design language, making it easier to build an entire battlestation that has a unified aesthetic.\nEach card must meet rigorous performance and reliability standards before it ships. Performance and stress tests are run with the latest chart-topping titles like Fortnite, League of Legends, Overwatch, and PlayerUnknown's Battlegrounds. We also carry out reliability trials that include a 144-hour stability test and a series of 3DMark benchmarking runs to ensure the card performs well when pushed to the limits.\n|Graphics Engine||GeForce GTX 1650 (Turing)|\n|Bus Standard||PCI Express 3.0|\n|Boost Clock||1785 MHz (OC Mode)\n1755 MHz (Gaming Mode)\n|Video Memory||GDDR6 4GB|\n|Memory Speed||12.0 Gbps|\n|Video Resolution||Digital Max Resolution: 7680x4320|\n|Video Interface||1 x HDMI 2.0\n1 x DisplayPort 1.4\n1 x DVI-D\n|Dimensions||178mm x 111mm x 38mm|\nGet your order delivered the next working day for £5.99, available when ordering Monday - Thursday and before 17:00 on that day if the product is in stock and your payment has cleared.\nGet your order delivered the next working day before 12:00pm for £10.98, available when ordering Monday - Thursday and before 17:00 on that day if the product is in stock and your payment has cleared. Some orders if via PayPal and the first customer order may not clear in time.\nIf you need your order at the weekend, for £12.58, you can specify the day you wish to have it delivered to you, Saturday or Sunday providing your order has been placed by 17:00 on the Friday before.\nEconomy delivery is available on all orders of any value being shipped within the UK.","You are here\nDark Souls Review\nDark Souls is an action fantasy RPG developed by FromSoftware Studios and published by Namco Bandai Games in 2011.\n- Dark fantasy setting\n- High skill orientation\n- Large open world\n- Unique multiplayer\nDark Souls Gameplay\nDark Souls is the second episode of the series that started with a Demon’s Souls game developed by FromSoftware as well. The game received overall positive reviews for the depth and uniqueness of the combat system, although the difficulty of the fighting is quite complicated to master. Still, the gameplay complexity stays relevant to this day; it became the primary feature of the Souls series and migrated into the other projects later in the years. All of the similar games earned a name of “souls-likes”, as a praise for the genre origins.\nThe plot of the game tells a story about the kingdom of Lordran, where the player takes up a role of the cursed undead creature who begins an adventure to uncover the mysteries that gave birth to their kind. The protagonist collects the souls of the most powerful monsters which allows to fulfil the prophecy and open up the truth. The main character travels throughout the worlds using the central hub called Firelink Shrine. Each unique world has specific monsters and traps that require the player to learn, explore and think out how the adventure would continue and how to evade death.\nThe primary concept and feature of the Dark Souls RPG are based on death itself; the characters will irresistibly die a lot. The protagonist will get stabbed with swords, crushed with axes, hammers and boulders that fall from above; they will be burned, poisoned and electrocuted, as well as pushed off the cliff ledges and accidentally jumping off themselves. Dying will cause you to lose the souls of the previously defeated enemies; however, they can be obtained back by killing the foe that eliminated you, although this chance is given only once and in case of failing before reaching the particular enemy, the souls get permanently lost.\nAfter death, the main character gets resurrected at the bonfire - a checkpoint that is unique for the whole series of souls-likes, but all of the enemies that roamed along the way will be brought back to life as well, except bosses and friendly NPCs. Each enemy possesses unique strategy and skills which require the watch the unfriendly mobs to learn how to evade their attacks and defeat them. Souls that are gained for assassinations can be spent on bettering the physical statistics of the player, and at the vendor’s shops to buy weapons, armour or additional items.\nThe combat of the game includes a wide variety of melee-oriented attacks. In the world of this RPG, there is an extensive range of highly-detailed medieval weapons, such as swords, greatswords, axes, maces and clubs. Aside from conventional weaponry, the player can obtain specific magical wields that can put additional harmful effects on the enemies. The armour can be found, bought and upgraded throughout the game progress. As the project doesn’t require to choose a specific skill, players can decide themselves who they want to be, what kind of gear they want to wear and whether or not they want to use magic. Archetypes can be created to immerse in the RPG aspect of Dark Souls fully. However, some classes might find it more challenging to complete the game, as they do not have a needed level of protection and armour.\nAnother unique feature of Dark Souls is the multiplayer system, connected with a humanity scheme. The protagonist can travel the worlds in two forms: human and hollow. The human form allows to summon other players and NPC’s to help them in battle but brings additional dangers, such as aggressive players’ invasions who seek to restore their humanity. Hollow form grants the game progress but takes away the summoning and multiplayer ability. It can be restored by seeking out the last dying point or by consuming a particular item.\nThe communication between players is restricted, which allows them only to use limited messaging in certain places and a small number of common phrases and gestures. This feature creates an immersive and dark atmosphere in the game, granting a feeling of danger, insecurity and fear. The game has several different versions, expanded Prepare to Die Edition released in 2012 and Dark Souls Remastered with improved graphics and controls. Pay attention that different editions have different system requirements.\nDark Souls System Requirements\n- Memory: 6 GB RAM\n- Graphics Card: GeForce GTX 460, 1 GB\n- CPU: Intel Core i5-2300 2.8 GHz\n- File Size: 8 GB\n- OS: Windows 7 (64-bit)\n- Memory: 8 GB RAM\n- Graphics Card: GeForce GTX 660, 2 GB\n- CPU: Intel Core i5-4570 3.2 GHz\n- File Size: 8 GB\n- OS: Windows 10 (64-bit)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:88a6c560-cc9b-46aa-9ffd-ca90e3196cbf>","<urn:uuid:734c2ea0-dc14-4a96-b054-24ce09f23fa4>"],"error":null}
{"question":"How do the success factors for human rights integration compare to the principles of effective copyright consultation in terms of stakeholder involvement?","answer":"Both domains emphasize stakeholder involvement but approach it differently. For human rights integration, success factors include internal commitment across levels (especially middle management), creating an enabling environment with cross-functional ownership, and developing a common organizational language. For copyright consultations, the principles focus on being inclusive of all affected stakeholders, ensuring equality in weighing different perspectives, and maintaining accountability through transparency. Both frameworks stress the importance of meaningful stakeholder engagement, but copyright consultation specifically emphasizes democratic purposes and equal weighting of different types of evidence, while human rights integration focuses more on internal organizational buy-in and practical implementation.","context":["\"The intent of integration is to make respecting human rights part of the parameters within which business is conducted – like ethical behaviour, compliance should ensure leadership from the top to embed respect for human rights, include human rights in relevant training, and develop the capacity to respond to unforeseen situations in a manner that respects human rights.”*\nHuman rights are the basic rights of each human being, independent of race, sex, religion, political opinion, social status or any other characteristic.\nAt this time in history, there are compelling reasons why businesses should involve human rights in their policies and practices. Businesses increasingly need a stable international environment in which to operate, with sustainable markets and a “level playing field” of opportunities. Human rights offer a common framework for businesses to understand societies’ expectations and deliver value to stakeholders in a more sustainable way.\nFor business, human rights provide a universal benchmark for minimum standards of behaviour. Many national laws and regulations have evolved as a result of a state’s obligation to implement human rights standards. Business must, of course, observe such laws in all countries and jurisdictions in which they operate.\nCompanies are using sustainability and integrated reports to publicly report on their human rights practices. In particular, reporting frameworks (such as the GRI G4 Guidelines) provide specific indicators to report human rights practices against. However, in order to report, companies first require human rights policies, strategies, management plans and programmes as well as indicators to measure human right practices against.\nAbove all, effective human rights strategies are to be integrated and embedded across operations in order to report.\nBased on research conducted on more than 100 sustainability reports, across industries and across geographies, Next Generation has come to understand the following challenges.\nKey challenges in embedding human rights\n- Human rights commitments are only partly understood, implemented and managed\n- No coherent, systematic human rights approach exists: In general, human rights are included in the context of labour or social compliance and “by accident” happen to also fulfil the criteria for human rights compliance.\nBased on our work specifically in the mining and resources sectors in Africa, we identified the following factors that could aid more effective human rights management practices.\nKey success factors to conquer the challenges\n- Internal commitment on all levels, particularly that of middle management\n- Creating an enabling environment, i.e. resources and ownership across functions and divisions.\n- Developing a common language for human rights management throughout the organisation.\n- Making human rights as specific and contextual as possible, i.e. geographic.\n- Other success factors depend on the industry: For some, industry collaboration has been a key to success, for others that may not be a possibility.\nOur advice for more effective integration of effective human rights practices include:\n- Define your company’s ambition level. Link this process clearly to your business strategy. Clearly define who you are as a business and why it is relevant for your organisation to manage human rights. Be ambitious but pragmatic. Set a concrete, business-minded goal for your company’s human rights approach. This way you speak the language of business from the very start while making the direction clear for everyone.\n- Map out what you are already doing. Collect all human rights-related commitments, resources, systems and key contacts into a description of the status quo. This way you will ensure you are not rebuilding something that already exists.\n- Conduct a gap analysis. For this, the UNGP and UN Global Compact’s instruments offer excellent starting points, but it is also important to integrate your ambition level and specific business-minded goals in the framework against which the gap analysis is conducted.\n- Develop an action plan and a roadmap. Here, several findings from the gap analysis can be used to guide actions:\n- Integrate, integrate, integrate. Look at existing processes and think which are relevant for the chosen human rights approach. And then – integrate.\n- Make human rights as specific as possible. The more concrete the issues and actions are, the easier it is to understand the business relevance of human rights and thus to get the necessary business buy-in.\n- Invest in middle management’s commitment. Our research shows that, when moving from commitment to implementation, human rights management lives and dies with the commitment of middle management to the issue. Actions speak louder than words – and this is where you need middle management close to the operational level of the business and its human rights risks.\n- Build a training-focused audit/due diligence programme. Combine human rights audits/due diligence with training wherever possible and relevant. This is the best way to get the most out of audits and taps into the current trend by pioneering companies. This helps build long-term commitment with suppliers, which may bring down operational costs as well.\n- Communicate transparently but carefully, keeping different stakeholders/rightsholders and their interests in mind. Communicating about human rights issues is not easy, due to their vastness and political nature. So make it concrete and contextual but also understand what fuels the different stakeholder groups. Transparency is the talk of the day and the best policy here, but choose your battles well and keep your backyard in order. White-washing is out of fashion for good.\nExamples of management practices to achieve integration:\nThere are several methods and instruments that companies employ to implement the human rights policy and accelerate integration:\n- Include criteria related to human rights commitments in human resources processeslike recruitment, hiring and performance appraisals.\n- Train employees on business principles and codes of conduct, including sharing dilemmas on human rights and understanding impact of certain business decisions.\n- Develop long-term incentives, including non-financial appraisals and bonuses (e.g. making bonuses dependent on lowering the number of complaints over time or improving employee engagement scores).\n- Install strong disincentives for conduct that is out of line with the human rights policy– i.e. impose disciplinary measures when someone is responsible for a human rights abuse.\n- Organise capacity to cope with human rights dilemmas and improve performance, such as training for specific employees, corporate risk and responsibility committee, ethics committee.\n- Make human right part of governance systems, including supporting policies, procedures, monitoring and other oversight mechanisms.\n- Include human rights principles in contracts with business partners(e.g. suppliers, joint venture partners, contractors) and train and monitor the company’s own buyers in these relationships.\nKey points to consider when moving towards implementation of a human rights policy:\n- Undertaking a human rights baseline study for the business, including country-risk analyses.\n- Undertaking a human rights impact assessment at the pre-feasibility stage of any major project with the potential to negatively impact human rights, for new operations, and on an ongoing basis, as human rights risks may change over time. Some operating contexts, such as conflict-affected areas, will require additional attention. As part of their human rights approach, companies need to consult potentially affected groups and other stakeholders/rightsholders in a meaningful way.\nIntegrating human rights throughout the company\n- Integrating and acting on the findings of the impact assessment.\n- Engaging in ongoing stakeholder consultation to build relationships across the company and with external groups.\n- Reviewing company training to include human rights criteria as appropriate and identifying target groups that may need additional learning support.\n- Embedding human rights into management systems, including responsibilities in job descriptions and performance appraisals.\n- Integrating human rights into the company’s internal and external communications (such as intranet) to ensure relevant stakeholders understand the policy and business implications of not adhering to it or failing to act on impact assessment findings.\n- Identifying indicators for measuring human rights performance, developing and implementing systems for acquiring qualitative and quantitative data, drawing on both internal and external feedback, including from affected stakeholders, and reviewing the findings to inform future strategy.\nCommunicating on how impacts are addressed\n- Communicating on commitments, targets and performance (through the Global Compact Communication on Progress for United Nations Global Compact participants). The communication should be of a form and frequency that reflects the enterprise’s human rights impacts and that is accessible to its intended audiences.\n- The communication should provide information that is sufficient to evaluate the adequacy of an enterprise’s response to the particular human rights impact.\n- Reporting on progress with regard to implementing and assessing human rights strategies, policies and programmes in company reports i.e. sustainability and integrated reports.\n- If a company finds that it has caused or contributed to negative human rights impacts, it is expected that it engages actively in remediation.\n- The company should provide for or cooperate in the remediation of such impacts through legitimate mechanisms. Establishing or participating in operational-level grievance mechanisms can be an effective means of providing access to remedy for individuals and communities that may be adversely impacted by the company’s activities if these mechanisms meet certain criteria.\n*Source: Special Representative online forum: www.srsgconsultation.org (“Integration”)","Developing a stakeholder-centric consultation model\nResearchers develop new guidelines for working in complex and contested areas.\nResearchers working with CREATe (School of Law, The University of Glasgow) have published the report ‘Improving Deliberation, Improving Copyright’. It sets out a new approach that addresses some of the limitations of copyright consultations in practice. Authored by Dr Lee Edwards (London School of Economics and Political Science) and Dr Giles Moss (University of Leeds) the research develops important guidelines and a set of associated tools to improve consultations in contested policy fields and will be of use to consultation exercises of all kinds.\nTraditionally, copyright consultations have taken place in a landscape characterised by uneven resources, knowledge and expertise among stakeholders. Through the research, which adopted a stakeholder-centric approach that privileged the experiences of a wide range of copyright stakeholders and the public, a variety of shortcomings and potential improvements for copyright consultations were identified. These specific and detailed suggestions were underpinned by generic purposes and principles for conducting consultations, which can be applied beyond copyright contexts.\nThe findings show that consultations should have a clear epistemic purpose (to develop knowledge that will improve policy) and/or a democratic purpose (to enable stakeholders to contribute to policy and improve the accountability of policy decisions). They should also be grounded in four key principles that should guide the structure and practice of consultations:\nGiven that consultations are systems of different, connected activities, the aim in designing consultations should be to maximise the achievement of each of the principles as far as possible, and across all the different activities. For example, if the consultation topic was the parody exception, stakeholders might be invited to explore the fundamental definition and role of parody that should underpin policy (democratic purpose); and/or to provide evidence to underpin the final exception policy (epistemic purpose). To meet the four principles, the design might include: involving all those who have relevant knowledge or are affected by the parody exception (inclusive); ensuring openness to a wide range of evidence, not only economic metrics but also social and cultural benefits of parody (well-informed); ensuring equal weighting for different types of evidence (equality); providing opportunities for stakeholders to engage with each other and develop a mutual, reflective understanding of their own and others’ positions (well-informed); and ensuring a high level of transparency and justification about the processes followed and decisions made (accountable).\nProf. Martin Kretschmer, Director of CREATe, says: “This is innovative research of the highest quality. It has the potential to bring sustained change to practice. CREATe in its role as lead of the Intellectual Property workstream of the AHRC Creative Industries Policy & Evidence Centre (PEC) are delighted to support the publication of this important work”.\nFor the current project, stakeholder dialogues and stakeholder workshops were conducted, with final recommendations presented to the Intellectual Property Office. The dialogues were individual discussions with 34 stakeholders and ten members of the public, to understand their experiences of consultations. These discussions then formed the basis of the stakeholder workshops, where 28 participants worked in small groups to answer three broad questions: who should participate in consultations; how they should participate; and why they should participate. The deliberative principles of inclusivity, equity and mutual respect guided the project, informing how researchers worked with participants and how participants interacted with each other.\nDr Lee Edwards said “This project has shown the value of encouraging and enabling conversations between stakeholders, engaging in a form of ‘meta-deliberation’ or ‘consultation about consultations’ in a contested policy space. The new, co-produced deliberative model for consultations that emerged from this project is grounded in stakeholders’ experiences, and provides a new way of thinking about how policy might be explored and developed in a variety of complex contexts outwith copyright, such as internet safety or platform regulation.\n“Adopting a stakeholder-centric view of consultations is an effective way of identifying whether the assumptions and beliefs of consultation leaders are actually the experience of stakeholders. In all areas of media policy, effective consultation is essential; the innovative methodologies used in this and our previous studies, offer ways for policymakers and researchers to develop more democratic processes and practices, and ultimately improve the legitimacy of the policy outcomes they achieve.”\nFurther Project Information\nThe project grew from an initial investigation that examined the ways in which copyright was understood and evaluated by industry, activist groups and users (Grant reference ESRC RES 062-23-3027). CREATe subsequently funded a project, enabling a deliberation exercise with a representative group of the general public at a weekend event designed to allow the discussion of the nature of copyright law, its implementation, and ways it might change. CREATe Working Paper Living With(in) Copyright Law was published in 2017.\nCREATe Working Paper Series: https://www.create.ac.uk/publications/\nMedia contact: Liz Buie, Communications and Public Affairs Office, University of Glasgow. firstname.lastname@example.org; tel. 07527 335373\n‘Improving Deliberation, Improving Copyright’ is an AHRC funded project (grant AH/S007075/1), following on from CREATe grant AH/K000179/1. The project was led by researchers at the LSE and Leeds University, in collaboration with the Intellectual Property Office, Ofcom and CREATe (now in its role as lead of the Intellectual Property workstream of the AHRC Creative Industries Policy & Evidence Centre, grant AH/S001298/1)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:ef888805-bd73-4c96-92ea-e046496939a8>","<urn:uuid:f9c50f01-10b5-49b2-90cb-1f2c2ba8cafc>"],"error":null}
{"question":"Could you explain the key differences between a mediator's and an arbitrator's role in resolving disputes?","answer":"A mediator facilitates communication and helps parties reach their own voluntary agreement without making decisions. They promote understanding, assist in identifying needs and interests, and use creative problem-solving techniques. In contrast, arbitrators take on a role similar to a judge, typically working in panels of three (with each side selecting one arbitrator and those two selecting a third), making decisions about evidence and giving written opinions. While mediators help facilitate discussion and resolution, they don't judge the case, whereas arbitrators make decisions by majority vote that can be binding or non-binding.","context":["Ethical Guidelines for Mediators\nTable of Contents\nETHICAL GUIDELINES FOR MEDIATORS\nMediation is a process in which an impartial person - a mediator - facilitates the resolution of a dispute by promoting uncoerced agreement by the parties to the dispute. A mediator facilitates communication, promotes understanding, assists the parties to identify their needs and interests, and uses creative problem solving techniques to enable the parties to reach their own agreement.\nA mediator should explore with the parties prior to the mediation commencing that each party will have the necessary authority to conclude any settlement.\n- A mediator should provide information about the process, and help the parties identify their real concerns and all their options. The primary role of the mediator is to facilitate voluntary resolution of disputes by the parties themselves.\n- A mediator cannot personally ensure that each party has made a fully informed decision when reaching an agreement to resolve a dispute, but it is good practice for the mediator to make the parties aware of the importance of consulting other professionals, where appropriate, to help them make informed decisions.\n- The mediator must address with the parties any instances of deceit, fraud and misleading statements before any settlement is reached.\nA mediator may mediate only those matters in which the mediator can remain impartial and even handed. If at any time the mediator is unable to conduct the process in an impartial manner the mediator should withdraw.\nAccordingly, a mediator must avoid:\n- partiality or prejudice; and\n- conduct that gives any appearance of partiality or prejudice.\n- Whatever their own views and standards mediators should not only not be partial or prejudiced but should avoid the appearance of partiality or prejudice by reason of such matters as the parties' personal characteristics, background, values and beliefs or conduct at the mediation.\n- Mediators should be conscious of behaviour which, however innocent, may be interpreted as indicating partiality or prejudice, such as spending more time with one party than another without good reason, socialising with a party and adopting different modes of address.\n- Even if all the disputants agree that they would like the mediator to express an opinion on the merits, there is a substantial risk in giving such an opinion that the mediator may no longer appear to be impartial. As a result the mediator may be obliged to withdraw.\n- Should the disputants agree to terminate the mediation and enter an alternative process, using the mediatior, the mediator must consider the suitability of continuing as the appointed resolver and may need to withdraw altogether notwithstanding the parties’ wishes.\n3. CONFLICTS OF INTEREST\nBefore the mediation begins, the mediator must disclose all actual and potential conflicts of interest known to the mediator. The mediator should:\n- discuss any circumstances that may, or may be seen to, affect the mediator’s independence or impartiality; and\n- at all times be transparent about the mediator’s relations with the parties in the mediation process.\nDisclosure must also be made if conflicts arise during the mediation.\nAfter making disclosure the mediator may proceed with the mediation if all parties agree and the mediator is satisfied that the conflict or perception of conflict will not preclude the proper discharge of the mediator's duties. The mediator must be certain of:\n- the parties’ agreement; and\n- the mediator’s ability to undertake the mediation with independence and neutrality so as to ensure impartiality.\n- Conflicts of interest may arise in recommending the services of others. It may be preferable to recommend referral services or associations which maintain rosters of qualified persons.\n- External pressures should never influence the mediator. The mediator's commitment should be to the parties and the process.\n- Interests which should be disclosed include any association with a party or adviser or representative of a party, which could reasonably be seen to affect the impartiality of the mediator.\n- The mediator should disclose to the participants any circumstances which may cause, or have tendency to cause, a conflict of interest. In particular a mediator who is a partner or an associate of any representative retained by either of the parties should not act as mediator without the fully informed consent of all the parties.\n- The mediator should not establish a professional relationship with one of the parties in relation to the same dispute.\nA mediator must not mediate unless the mediator has the necessary competence to do so and to satisfy the reasonable expectations of the parties.\nA person who agrees to act as a mediator holds out to the parties and the public that she or he has the competence to mediate effectively.\n- Competence comprises appropriate knowledge and skills which would normally be acquired through training, education, and experience.\n- Mediators should have available for the parties information regarding their training, education and experience.\n- When a person is appointed or nominated to a panel or list of mediators, the appointing court, tribunal, institution, or agency should ensure that the mediator has through training, education and experience acquired the necessary knowledge and skill for inclusion on the particular panel or list.\n- The qualifications for inclusion on a list of mediators should be made public and available to interested persons.\nSubject to the requirements of the law a mediator must maintain the confidentiality required by the parties.\n- As the parties' expectations regarding confidentiality are important, the mediator should discuss those expectations with the parties and endeavour to meet them. The mediator should clarify when the mediation begins and when it ends, and whether conversations on the telephone, in meetings and communications by email and other means are also confidential.\n- The parties' expectations of confidentiality depend on the circumstances of the mediation and any agreements they, and any other persons present at the mediation, and the mediator may make.\n- A mediator should not disclose any matter that a party requires to be kept confidential (including information about how the parties acted in the mediation process, the merits of the case, any settlement offers or agreed outcomes) unless:\n- the mediator is given permission to do so by all persons attending at the mediation with an interest in the preservation of the confidence; or\n- the mediator is required by law to do so.\n- The parties and the mediator may make their own rules with respect to confidentiality, or the accepted practice of the mediator or an institution may mandate a particular set of expectations.\n- If the mediator intends to hold private sessions with a party, the mediator should before such sessions discuss with the parties the confidentiality attaching to them.\n- Any reporting which requires a subjective judgment by the mediator of the conduct of the parties is likely to destroy the integrity of the mediation process.\n- Under appropriate circumstances, researchers may be permitted to obtain access to statistical data.\n- With the permission of all of the parties, researchers may be permitted access to individual case files, to observe mediations, and to interview participants.\n- A mediator should render anonymous all identifying information. When materials emanating from a mediation are used for research, supervision, or training purposes, the mediator should remove all identifying information from them.\n6. TERMINATION OF MEDIATION\nA mediator may terminate the mediation if the mediator considers that:\n- any party is abusing the process; or\n- there is no reasonable prospect of settlement.\nThe mediator if appropriate should inform the parties, and may terminate the mediation if:\n- a settlement is being reached that to the mediator appears illegal having regard to the circumstances of the dispute; or\n- the mediator considers that continuing the mediation is unlikely to result in a settlement.\n7. RECORDING SETTLEMENT\nIf the mediation results in a settlement between the parties, the mediator should encourage the parties to continue the mediation until the parties have:\n- addressed any enforceability issues; and\n- recorded terms of settlement in writing.\n- Normally agreement to record the terms of any settlement in writing should be made prior to the commencement of the mediation.\n- The mediator ought to be cautious about direct involvement in drafting the terms of agreement, as their involvement in drafting may be construed as providing legal advice.\n- The mediator should however assist the parties to take whatever steps may be necessary to formalise any settlement agreement, and satisfy themselves regarding its enforceability. The mediator may consider seeking to reconvene the mediation at a later time to allow the parties to finalise a settlement deed or any necessary court orders.\n8. PUBLICITY AND ADVERTISING\nA mediator must not engage in misleading or deceptive publicity or advertising.\nA mediator must not make any false or misleading statement including statements or claims as to the mediation process, its costs and benefits, or the mediator's role, skills, or competence.\nA mediator must fully disclose his or her fees to the parties.\n- As early as practicable, and before the mediation session begins, a mediator should obtain the agreement of the parties regarding all fees and other expenses to be charged for the mediation, and by whom and when the fees and expenses are to be paid.\n- The better practice is to record in writing the arrangements in respect of fees and costs.\n- A mediator should not agree to a fee which is contingent upon the result of the mediation or amount of settlement.","Article courtesy of FindLaw\nLitigation is generally something people seek to avoid. It’s expensive, time consuming, emotionally draining and unpredictable –- until a judge or jury decides the case, you can never be certain of the outcome.\nBecause litigation is so inefficient for most of us, alternative dispute resolution, such as arbitration and mediation, has become increasingly popular. But before moving forward with possible alternative dispute resolutions, you should first know the difference between arbitration and mediation.\nMediation and Arbitration: What’s the Difference?\nArbitration and mediation are similar in that they are alternatives to traditional litigation, and sometimes they are used in conjunction with litigation (opposing parties may first try to negotiate, and if that fails, move forward to trial).\nBoth arbitration and mediation employ a neutral third party to oversee the process, and they both can be binding. However, it is common to employ mediation as a non-binding process and arbitration as a binding process. In simpler terms, binding arbitration replaces the trial process with the arbitration process.\nArbitration is generally conducted with a panel of multiple arbitrators who take on a role like that of a judge, make decisions about evidence and give written opinions (which can be binding or non-binding).\nAlthough arbitration is sometimes conducted with one arbitrator, the most common procedure is for each side to select an arbitrator. Then, those two arbitrators select a third arbitrator, at which point the dispute is presented to the three chosen arbitrators. Decisions are made by majority vote.\nMediation, on the other hand, is generally conducted with a single mediator who does not judge the case but simply helps to facilitate discussion and eventual resolution of the dispute.\nThe Success of Mediation in Modern Litigation\nMediation has enjoyed increasing popularity as an important part of the litigation process. For example, in Florida, almost all lawsuits are required to be mediated before a court will allow them to be put on the trial calendar. The reasoning behind this requirement, according to the Florida senate, is because mediation has proven effective in reducing court dockets and trials, and offers a more efficient, cost-effective option to litigation.\nMediation enjoys such high success rates because the parties are brought together in an environment where they can freely and confidentially present their position in front of a neutral third party.\nMediation attempts to limit the issues and put them into proper perspective. Participants often feel much better after having an opportunity to get things “off their chest,” and also benefit from hearing the other party’s point of view. The neutrality and more relaxed atmosphere of mediation may eliminate the desire to continue hostile litigation once both parties have seen all the issues in a fair light.\nMediation can be used for any kind of dispute; there is no need to wait until a dispute results in a lawsuit and is sent to mediation by a judge.\nPre-lawsuit mediation is becoming more widely accepted as a sensible way of resolving disputes before they turn into litigation. Besides being confidential and non-binding, mediation is relatively quick and inexpensive compared to litigating a dispute.\nPlease note that while most certified mediators are attorneys, mediators will not give legal advice during the mediation and are not supposed to make legal conclusions about the merits of either party’s position. When the parties come to an eventual agreement, the parties themselves will put the agreement in writing and sign it so that it then becomes a binding contract.\nIf you want to mediate a dispute, you and the opposing party should enter into a pre-mediation contract. This simple contract should include the following:\n- The mediation should be confidential and non-binding.\n- The parties should agree on who will conduct the mediation and how the mediator will be paid. The cost of the mediator is typically split between the two parties.\n- The parties should agree on the length of the mediation. Most mediation is scheduled for either a half-day or a full day.\n- The parties should agree to mediate in good faith until either party reasonably determines that it is fruitless to continue. If the parties cannot reach an agreement, the mediation will result in what is known as an impasse.\nWhere Should You Begin?\nIf two or more parties have a dispute that they think may be appropriate for mediation, they may contact an attorney to advise them on the benefits of mediation versus litigation, and to help them locate a mediator.\nIf the parties like to mediate on their own without the help of attorneys, then they should contact their state bar association who will have a list of mediators to contact for an appointment. Alternatively, they can get in touch with a mediation and/or arbitration organization.\nMediation is a less hostile, less confrontational format for resolving disputes, where parties attempt to resolve their differences between themselves rather than relying on an inefficient, expensive, and time-consuming judicial system.\nBefore your litigation advances, work with your attorney to determine if mediation is right for your case.\nFor more information on how you can find legal representation for your business disputes, read more under Sigmon Law’s Business Litigation."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:eda71d1f-741b-4f71-bdad-e586991a8436>","<urn:uuid:f453be68-92a5-49e5-9e24-95a217e41548>"],"error":null}
{"question":"How do you execute conventional swing bowling technique, and what maintenance should be done to cricket grounds in summer?","answer":"For conventional swing bowling, you need to focus on four key elements: ball condition (maintaining one shiny side), bowling action, grip (seam tilted 20 degrees left for outswing or right for inswing), and wrist position (fully upright behind ball during delivery). For release, use index finger last for outswing and middle finger for inswing. Regarding summer ground maintenance, regular verti-cutting, scarifying and mowing of the square must be continued while preparing pitches. The outfield should be maintained at 15-18mm height, avoiding scalping. With increased evapotranspiration rates, proper irrigation is essential, ensuring water penetrates to 150mm depth. Fertilizer should be applied according to soil analysis, and repairs to used pitches, especially foot holes, require particular attention.","context":["Have you ever watched cricket and wondered how professional bowlers can use conventional swing to move the ball in both directions? If you have, then you’re in the right place. Most people who have watched cricket will know the name Jimmy Anderson. This English fast bowler is arguably the most skilful swing bowler of his generation. With no noticeable change in his action, he can swing the ball both into and away from the batsman’s pads at will. This makes it incredibly hard for batsmen to know which balls to play at, and which balls they can leave. Anderson has taken an incredibly large amount of the wickets in his career by exploiting this uncertainty in his opponents’ mind.\nFor people at a lower level of the game conventional swing can be a confusing thing, and even once it is understood, it’s still incredibly hard to control! With this post I hope I can dispel some myths about swing bowling, and teach you the basics that are required to be able to move the ball in the air effectively.\nTo get a cricket ball to swing, you’re going to have to take care of the following things:\n- The Condition Of The Ball\n- Your Bowling Action\n- The Way The Ball Is Gripped\n- Your Wrist Position & Release\nIn this post I’m going to take you through each of those thins in more detail, and I’ll also teach you how you can get the ball to swing later rather than early in its flight! Let’s get started…\nThe Condition Of The Ball\nWe’ve all seen cricketers rubbing the ball vigorously on their trousers during games. A casual observer watching this might wonder what on earth is going on! Well, it turns out that a cricket ball is ideal for swing bowling when it has one side that is shiny, and one side that is rough. With conventional swing, the ball will always move towards the direction of the rough side in the air, as the shinier side moves through the air quicker.\nThe ball condition should be an important focus for the fielding side. There are a few main things to remember when looking after the ball:\n- Keep the ball as dry as possible. A wet ball is much harder to swing in the air.\n- Always hold the ball with your fingertips on the seam. Do not place your palms on the side of the ball as this can add unwanted moisture.\n- To shine the ball, apply a small amount of saliva to your fingertips and rub this on to the side of the ball you wish to keep shiny.\n- Then, rub the shiny side of the ball on the back of your trouser leg, polishing the ball and providing a nice smooth finish.\n- Ensure that the entire side of the ball is shiny. This includes all of the leather right up to the seam.\nThis process can help you swing the ball significantly more! Make sure the person in charge of looking after the ball has a good attention to detail!\nYour Bowling Action\nMany coaches believe that the bowlers’ action is one of the most important aspects of being able to bowl conventional swing. Many of them also believe that having either a ‘side-on’ or a ‘front-on’ action will mean that you are more likely to swing the ball one way rather than the other. This may be true, however it does not mean that you can’t learn to swing the ball the other way too.\nIn my own personal experience, my bowling action is quite front-on, and this seems to mean that I can bowl inswing without any great effort. However, I have also developed a way of bowling outswing too!\nThere are many professional cricketers who can swing the ball both ways accurately, and there are numerous more important factors that allow them to do this.\nThe Way The Ball Is Gripped\nOne of the more important factors behind your ability to swing the ball is the grip that you use. The way you grip the ball should be different depending on whether you are trying to bowl outswing or inswing. Let’s examine the two different grips in detail…\nOutswing Grip (To a Right Handed Batsman)\n- When picking up the ball, ensure that the rough side is on the left hand side, with the shiny side to the right.\n- Hold the ball in the basic grip, and then tilt the seam 20 degrees to the left so that it now points in the direction of first slip.\nInswing Grip (To a Right Handed Batsman)\n- When picking up the ball, ensure that the rough side is on the right hand side, with the shiny side to the left.\n- Hold the ball in the basic grip, and then tilt the seam twenty degrees to the right so that it now points towards fine leg.\nNote: For outswing/inswing to a left handed batsman, simply reverse the direction of the ball in the hand.\nYour Wrist Position & Release\nThe position of the wrist is always very important to both conventional swing and seam bowlers. When delivering the ball, you should always be aiming for the seam to be upright when it hits the pitch, as this gives you the best chance of getting lateral movement and hopefully getting the batsman back in the pavilion. The best way to do this is to ensure that the wrist is fully upright and behind the ball during the delivery. If the wrist is not behind the ball, this can cause the seam to wobble, which is not always ideal if you want to get movement in the air and off the pitch.\nOk, so now we know why correct wrist position is important, how do we achieve it? Andrew Flintoff (a seam bowler) adopted the method of keeping the wrist locked in the desired position throughout his entire run up and delivery stride. In my personal experience, this is definitely something that has worked for me. Holding the wrist in this position while running in to bowl helped to ingrain the wrist position in my muscle memory, and the more and more I practised this in the nets, the more it started to feel natural.\nWhen we look at swing bowling, the principle is exactly the same as with seam bowling. The wrist must be fully behind the ball as it leaves the hand. For best results, you must cause the ball to spin along its axis, with no wobble in the seam as it travels towards the batsman.\nJimmy Anderson believes there’s a very easy way to learn how to do this, and it doesn’t involve changing your wrist position at all, it just requires you to change how you release the ball…\nFor Jimmy’s outswinger, he will hold the ball in the outswing grip, run up and go through his delivery stride as normal. However, when the ball is about to leave the hand, he will ensure the last finger that touches the ball is his index finger. With the last bit of momentum being applied to the ball by this finger, this causes the seam to rotate in the direction of first slip, making it more likely to swing away from the batsman.\nWhen bowling the inswinger, Jimmy’s process is largely the same. He holds the ball with the inswing grip, goes through his run up and delivery stride and then prepares to release the ball. This time, when the ball leaves his hand, he will make sure that it leaves his middle finger last. The momentum imparted by the middle finger causes the seam to rotate in towards the batsman, making the ball more likely to swing towards his pads.\nHow Can You Get The Ball To Swing Later?\nOk, so now we’ve highlighted the basics that you need to be able to bowl conventional swing, how do we refine it and make it even more effective?\nOne thing that a lot of elite bowlers try to work on is how late they can get the ball to move in the air. Skilled batsmen will read the ball out of the bowlers hand expertly, and keep their eye on the ball until it makes contact with their bat. Therefore, if the ball starts to swing as soon as it begins its flight, this is a lot easier for the batsman to see and react to as he can follow the trajectory the whole way. An example of a bowler who uses conventional swing to move the ball very early is David Willey of England. He doesn’t possess great pace, and the movement he gets in the air is largely easy to read. This is one of the reasons why he may never make it as a test match bowler.\nContrast this with bowlers like Dale Steyn of South Africa or Mohammad Amir of Pakistan. At the top of their game, these two can be nearly unplayable due to their pace, accuracy, and their ability to swing the ball very late. Although sometimes it may not look as if they move the ball as much as someone like David Willey, the late swing gives the batsman less time to react to the movement, meaning that a good ball is more likely to take the edge. When you add this to the fact that these two are capable of swinging the ball both ways, you can begin to understand what sort of problems they cause.\nThere are various thoughts from coaches and professionals on how to achieve late swing more consistently. Here are a couple to cast your eye over:\nHaving a High Arm\nThere is a school of thought that the key to making conventional swing happen later is being very upright in your action, and staying very tall as your bowling arm comes over in a relatively straight line. This is what is meant by having a ‘high arm’. This means that there is more momentum directly behind the ball as it begins its journey through the air, and the swing will often occur later in the flight. Again, Jimmy Anderson and Dale Steyn are excellent examples of this.\nThe opposite of this would be having a ‘round arm’ action, and a good example would be Matthew Hoggard. He wasn’t a bowler who stood very tall during his delivery stride, and therefore the rotation of his bowling arm never came through in a very straight line. This means that the ball is pushed down the pitch at a slightly more off-centre trajectory than it is with a high arm bowler, and is more likely to experience conventional swing straight from the hand.\nSome people believe that late swing will occur more often if you hold the ball in a slightly different way to the outswing and inswing grips that I showed you above. Instead some bowlers prefer not to turn the ball as far first slip (for the outswinger) and fine leg (for the inswinger) in the hand, and would rather use the basic grip with only a slight tilt of the seam.\nFinal Things To Remember\nKeep in mind that sometimes the amount of swing achievable can be limited by the conditions and the length that you bowl. When watching a game of cricket you will often hear a commentator say that ‘the conditions are perfect for swing bowling’, which often refers to the weather being slightly overcast. Simply put, some days the ball will swing more than others for reasons that are out of the players’ control.\nAs for the length you should be bowling, trying to get the batsman to come forwards and play on the front foot is a vital component of effective swing bowling. This means that the ball will be in the air longer and therefore it will have more chance to move around!\nLastly, with this post I have tried to highlight the main principles to help you impart conventional swing onto a cricket ball. However, this should not become more important than what you feel is comfortable for yourself. The beauty of cricket is that there are so many different techniques and ways of getting the job done. There is no single correct way! Take my post as a guide, find something that works well for you, and you shouldn’t go far wrong!\nIf you want to read more tips on fast bowling, then click here to read one of my most detailed posts on the subject! It will help you improve your speed, you accuracy, and your variation deliveries as well as other things!","Mother Nature is up to her old tricks again, heat wave in April, gales and heavy rain in May. Let’s hope for some sunshine in June.\nAs we approach the midway point of the Cricketing Calendar though, groundstaff will be working hard to produce good, quality playing surfaces for their clubs. Moisture levels will still be moderate, encouraging good growing conditions as temperature begin to rise. With weather conditions continuing to dictate the work required, tailor your fertilising programmes to your conditions so to give your square a head start when the weather turns.\nAs you move through the month, regular Verti- cutting, scarifying and mowing of the square will need to be continued whilst preparing pitches. Repairs and renovation to used pitches should also be undertaken, paying particular attention to your foot holes, as they may require more intense work. With the drying winds and the rise in temperature, irrigation, so often ignored, is a key management tool, so it will be a case of watering little and often when you can.\nDiary Compiled by Robert Stretton\nMassey Ferguson Sports & Social Club\nKey Tasks for June\nMost of your work this month will be focused on preparing wickets, as well as your outfield. Pitch preparations such as mowing and marking should be in full swing. Using the 10-12 day guideline, try to produce a consistent wicket with fast medium pace. Be sure to get your lines accurate and straight, and start the month off with a good irrigation of the square if you haven't already done so. Remember to follow any feedback from your soil analysis if applying liquid or granular fertiliser.\nDo not neglect your grass practice nets as they will also be in need of some remedial work. Try and rotate your netting bays so some recovery can take place to run ups and batting creases. Use the same process as with your foot holes, albeit may be on a larger scale.\nYou’re out field should not be neglected either, as this is the largest area of maintenance. It still needs to be carefully managed!\nContinuing pitch preparations and mowing of the square @ 10- 12mm. The outfield should be boxed off or gang mowed @ 15-18mm, avoiding scalping. Fertilising of the square can be undertaken if not already done so, remembering to allow granular feeds to be well watered in.\nWith the drying winds and the rise in temperature, Irrigation is a key management tool so it will be a case of watering little and often when you can. Crickets clubs who do not have any water at all are often left in the lap of the gods. The use of covers or groundsheets to help protect pitches will increase the moisture content providing they are not left on to long. Facilities that do not have or use pitch covers will also be more vulnerable to the changing weather.\nEvapotranspiration rates should begin to rise in the coming month, initiating the need to begin watering your facilities. The combined water loss from both the plant and soil surfaces will now be rising due to the warmer climatic conditions. Watering will be essential for wicket preparation and repairs. Irrigate uniformly and ensure the right amount is applied.\nIt is important to ensure that the water gets down deep into the rootzone, to a minimum of 150mm to encourage deep rooting. Check with a probe. Allow to dry and repeat the irrigation process. Allowing surfaces to remain dry for a period of time can lead to problems of dry patch, a condition that prevents water infiltration into the soil and thus forming areas of non-uniform turf quality.\nThe use of covers (flat or raised) will be invaluable during the preparation of match wickets; take care to ensure any surface water is prevented from running on to the protected pitch.\nKeeping some additional grass cover will help retain some soil moisture, thus slowing down the soils capacity for drying out. You may want to consider raising the height of cut on the square by 1mm to maintain some additional grass cover.\nAny period of rain will have stimulated the Poa grass species in the square, thus increasing thatch and procumbent growth; regular Verti- cutting will alleviate any thatch build up and stand up the sward prior to mowing.\nWith the drier weather now expected, the bounce and pace of the wickets should start improving. More and more Groundsmen are now taking the opportunity to measure and monitor the performance of their pitches. Having a better understanding of the condition of your square is paramount in deciding on what level of maintenance inputs are required.\nThe ECB have an excellent guideline booklet, TS4, which provides a wealth of information on construction, preparation and maintenance of cricket pitches.\nThe 10 -12 day prep shown below is only a guide; most grounds men will have their own interpretation.\nPitch preparation should start 10-12 days prior to the match. Following the guild lines below will help you achieve a good standard of pitch. Marking out the crease should be done with care, using frames or string to help achieve clear, straight lines.\nDAY 1 String out pitch lines to ensure correct width, 10 ft; Mow out @ 8mm. Always double mow (up and down the same line), using an 8 bladed pedestrian cylinder mower for maintaining the square. Test the pitch with a key or knife for moisture. Water the pitch thoroughly in the event that the pitch has dried out through pre season rolling.\nDAY 2 Brush / light rake, mow @ 8 mm, light roll to consolidate surface levels.\nDAY 3 Scarify or Verti cut to remove lateral growth and surface thatch avoiding deep surface disturbance. Mows @ 7 mm. continue medium light rolling 1000 kg 10-15 minutes.\nDAY 4 Roll pitches increasing roller weight to consolidate the surface.\nDAY 5 Scarify with hand rake to raise sword after rolling. Reduce HOC to 6mm\nDAY 6 20-30 minute’s with heavy roller.\nDAY 7 Light scarify by hand to raise sward, mow @ 6 mm, increase weight of roller to 1500- 1700 kg continue rolling 30 minutes reducing speed to consolidate surface.\nDAY 8 Continue rolling for 30 minutes at slow speed to achieve consolidation. Cover pitch over night to encourage moisture to rise to surface.\nDAY 9 Brush / rake lifting any lateral grasses, reduce mower (with a shaver blade) to 4mm, try to avoid scalping. Roll using heavy roller slow speed (crawling) 30 minutes morning & again late afternoon where possible. Cover pitch over night.\nDAY 10 Brush & mow pitch, roll morning and afternoon slow as possible (crawling).\nDAY 11 Brush, mow & roll to polish surface, test bounce with an old ball along edge of pitch. Continue rolling to consolidate surface. Cover pitch over night.\nDAY 12 Brush, mow & roll polish up pitch. Your pitch should effectively have take on a straw like coloration, a sign that the preparation has been achieved. String and mark out as in accordance to E.C.B guidelines. (TS4 booklet)\nMowing heights for the cricket square during the playing season should be:-\n8-12mm April-September (playing season)\n5-6mm Wicket preparation\n3-4mm Final cut for match\nA simple method for testing the ground for rolling is to insert a knife or slit tine into the soil profile and see if it comes out clean. If it does, it's the right time to roll.\nIf you find you may not have completed your pre season rolling, don’t despair, rolling of the entire square can still be carried out on separate occasions during May, spaced out between one another with a roller weight between 1000-2500 kg. The first pass should be across the line of play, returning along the same path until the whole square is rolled. Choosing and using the correct weight of roller is also critical for preparing cricket surfaces.\nContinue to verticut, training the grass to grow vertically to produce a cleaner cut. Do not disturb the surface profile!! If you don't have a verticut options then use a drag brush or rake to help stand the grass up prior to mowing. If using verti-cutting unit,s be very careful not to mark/scar the soil surface as these scars will be hard to remove as the square dries out.\nA spring/summer fertiliser should now be applied to encourage top growth, using manufacturers recommended rates. Rye grasses are more wear tolerant when fed correctly. Fertiliser treatment and turf tonic can be continued in accordance with your annual maintenance programme. If you haven't got a fertiliser programme, have your soil tested; try an independent soil analysis company for an impartial set of results.\nIt is essential to have water available for irrigation purposes. Irrigation is required for pitch preparation, repairs and the health of the plant. Irrigate uniformly and ensure the right amount is applied. It's important to ensure that the water penetrates into the rootzone to a minimum of 100- 150mm to encourage deeper rooting. Check with a probe. Allow to dry and repeat irrigation process. Allowing surfaces to remain dry can lead to problems such as dry patch, scorching and death of the plant.\nRolling should start and finish in line with the direction of play. After match pitch repairs begin with the brushing and sweeping up of any surface debris. Soak the wicket, scarify and spike, top-dress foot holes and overseed. Additional work may be required to repair foot-hole damage.\nSeeding of the ends where the grass is weak, sparse or bare can be continued, as the rise in temperature will help germination. Use germination sheets to aid this process but remove the sheets regularly to check for diseases. Remember that without good seed to soil contact the operation is useless. Ensure you use new seed as old material may not give you the required germination rates.\nRemember not to neglect the outfield; it too has a major effect on a game if unattended. The outfield should be treated the same as any other natural grass pitch, carrying out regular mowing, raking or Verti cutting, aerating and feeding programmes to maintain a healthy sward.\nA light harrowing/raking helps restore levels and keep surfaces open. Apply balanced fertilisers such as a 9-7-7 as part of your annual maintenance programme to help stimulate growth and recovery. Aerating the outfield will help to increase aerobic activity and get some much needed oxygen around the grass plants root system?\nRegular spiking and, if possible, an application of sand dressings to the profile will definitely improve soil water movement in the top 100mm.You may wish to Hollow core your outfields and then brush the cores back into the surface (recycling the existing material) this also helps to restore levels, reduce thatch and helps speed up the surface.\nIdeally, whilst maintaining a cutting height of between 10-14 mm, many outfields tend to be undulating and uneven preventing close mowing at these heights and in reality most are probably mown at a height between 12-25 mm. Also the type of mower used will dictate what Height of Cut can be achieved.\nRotary mowers tend to scalp undulating ground, where, as boxing off with a cylinder or a hydraulic gang mower with floating heads can give a better finish. Outfields which have been predominantly over seeded with rye grasses are subject to stress if mown to short. Fescues and Smooth stalked meadow grasses are quite tolerant to close mowing and are less likely to be stressed out.\nTaking a number of soil samples on a regular basis helps monitor the condition of your soil profile, enabling you to see for yourself any problems that may be occurring, such as root breaks, poor root growth, soil layering and depth of thatch. All of which can be rectified by appropriate actions. With the advent of digital cameras we now have an excellent tool for recording what we see.\nKeeping records are essential; the ECB booklet also promotes the use of Performance Quality Standards (PQS) as part of your management strategy; there are three categories of measurement that relate to the overall quality of a facility:\n* The Physical Structure (the profile make up)\n* The Presentational Quality (the visual impact)\n* The Playing Quality (the performance ratings)\nAlways keep an eye open for turf disease. Prevention is always better than a cure. The combination of moist soils and surface moisture on the leaf blade can make the plant susceptible to disease attack. Many turf grass diseases such as Fusarium and Red Thread can be active at this time of the year.\nSymptoms of Fusarium (Microdochium nival), the most common and damaging disease, are orange/brown patches 2.5-5cm across increasing in size under suitable conditions as the disease progresses. Active patches have a distinctive 'ginger' appearance when viewed early in the morning. Creamy white mycelium resembling cotton wool can be seen in the centre and towards the outer edge of the patch.\nGrass in the active patches is often slimy; once the disease is controlled the scars will remain until there is sufficient grass growth to fill in. Regular brushing, switching or drag matting in the mornings to remove the dew from the playing surfaces will reduce the likelihood of disease outbreak.\nRed Thread is ill-defined bleached grass with Pink mycelium visible in early morning dew. Close inspection will reveal red needle like structures which are attached to the leaf blades. The needles become brittle upon death and are easily detached allowing fragments to spread the disease.\nSystemic curatives and protective fungicides such as Chlorothalonil and Iprodione, applied in liquid form with water as a carrier, can be used to control any outbreaks. By mixing two or more products in the same tank can help reduce the potential for disease resistance developing. Fungicides are selected with different modes of action so that resulting mixture will attack the target disease on two or more fronts. This makes it more difficult for the pathogens to develop resistance to treatments.\nPests: - Worm can be very active at this time of the year so treatments can be carried out, if needed; the use of Carbendazim is the only active ingredient for controlling worms. All personnel should be suitably qualified in the application of chemicals. Moles can be active where worms are prevalent and need to be treated as they can cause a lot of damage to the surface.\nAll machinery should now have been returned from any servicing in time for use, with ongoing inspection and cleaning after use being vital. Breakdowns cost money as well as inconveniencing pitch preparations. The workshop should be kept in a good order; good housekeeping is important, a tidy workshop reflects a tidy worker.\nTime to repair and recommision raised covers systems and cricket net areas ready for the new season.\nKeep a good supply of materials such as loam and seed at hand for repairs and maintenance.\nPitchcare is the only provider of LANTRA accredited training courses in the maintenance of Cricket Pitches. It is a one day course designed to provide a basic knowledge of Cricket Pitch (square and outfield) maintenance. The course enables the Groundsman to grasp the basic needs of a cricket square and outfield.\nThere are two courses - Spring & Summer Maintenance and Autumn & Winter Renovations.\nDelegates attending the courses and using the accompanying manuals will be able to develop their own skills, working knowledge and expertise, by understanding the method of instruction and the maintenance principles they set out.\nIncluded in the Course Manuals are working diaries showing the range of tasks needed to be accomplished each month. The Course Manual is available for purchase separately.\nPitchcare also provide a range of courses suitable for tennis clubs. In most cases, the courses can be held on site using the club's own equipment and machinery.\nSome of the courses available are:\nChainsaws - CS30 and CS31\nH&S Refresher Training on Combined Turf Care Equipment; Tractors and Trailers; All Mowers (Ride-on and Pedestrian)\nMachinery Courses on ATVs; Tractors: Brushcutters/Strimmers; Mowers (ride-on and Pedestrian)\nPesticide Application (PA courses)\nStem Injection of Invasive Species (Japanese Knotweed etc.)\nBasic Trees Survey and Inspection\nMore details about all the courses can be found here, or you can email Chris Johnson for information.\n• Clean down and carry out service of machinery after use.\n• Keep you garage and storage areas clean and tidy.\n• Inspect flat sheets, covers and other cricket equipment, checking for wear and tear and that they are fit for purpose.\n• If you use a white line for your boundary, make sure it is clearly visible for match days."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b25f8c89-80bd-495a-8067-526cebdfce97>","<urn:uuid:6421c706-271e-4290-8a22-85bf1e7ce3bd>"],"error":null}
{"question":"What's the difference between wool vs synthetic carpets for durability, and what are the specific cleaning requirements for maintaining both types long-term?","answer":"Wool carpets are luxurious, durable and excellent insulators that enhance room elegance. For synthetic (man-made) carpets, they are hardwearing and ideal for high-traffic areas, capable of handling heavy footfall, dirt, and spillages. Regarding maintenance, wool carpets require immediate cleaning of stains and spills to reduce absorption and bacteria. For both types, key cleaning requirements include: daily vacuuming to remove dirt before it embeds deeply, proper blotting (not rubbing) of stains from outside inward, and professional cleaning once a year. For general stains, shaving cream can be effectively used by applying it directly on stains for 30 minutes. Additionally, placing doormats helps reduce tracked dirt and extends carpet life regardless of type.","context":["- He says determining whether the dyes are good or bad lies within whether or not the dyes are colorfast.\n- Ssentongo adds that fugitive dies are not good for carpets. He observes that spilling anything water or oil based, will result in a bleeding dye mess on your carpet.\nCarpets bring a nice ambience to any interior. Jackeline Biira of Spaces Interior says carpets are made from different kinds of material. These include wool, wool mix, and man-made fibres. She explains that each type has particular qualities and benefits.\n“Wool carpets are truly luxurious and will enhance the elegance of any living room. They are wonderful to walk on and durable. Wool is an excellent insulator and so will keep your living room warm and reduce noise,” she says.\nAccording to Asaad Kabugo Tamale of Fox Wood interiors and Architects before selecting the type of carpet for a living room, it is important to consider different aspects.\nShe explains that wool mix carpets are perfect for busy families. She says they are great insulators which are resistant to wear-and-tear and can easily be cleaned.\nFor the man-made carpets, Biira says they are attractive and hardwearing and are perfect for a big family. “If you have young children or pets and require a high-performance living room carpet, a man-made blend is ideal. It can cope with heavy footfall, dirt, and spillages,” she says.\n“Don’t choose a carpet that you will not be able to maintain, if your living room always has too much traffic then it is better you don’t go for heavy wool,” he advises.\nTamale says a home owner must provide good carpet care to maintain the look and condition of their carpets.\n“Stain resistant carpets are great for high traffic areas because they require less cleaning. Textured carpets are suitable for high traffic areas because of their resistance to dirt.\nMake sure you are aware of the maintenance requirements for a carpet type before you buy so that you can avoid high maintenance carpetes that you cannot care for yourself,” he says.\nColours and patterns\nBiira says choosing the right colour or pattern for your carpeting can be the most intensive part of the process because of the seemingly endless options available. However, she says some things you should decide before choosing a carpet include the kind of feeling or mood you want to create and whether you want the carpet to be a focal point or background furnishing. “Take some carpet samples home with you to see what they look like with the rest of the room in the natural light before making a decision.”\nWhat you need to know when choosing colours\nWarm colors such as red really help to brighten up the room and create a cosy ambience. Biira says that red carpets often go well with neutral and warm colored walls and are especially effective in living rooms.\nShe explains that a modern carpet trend is the use of earthy tones.\n“Greens, light blues and stone colours help to create a calm and serene atmosphere. However, they will show up stains and dirt easily so avoid placing them in high traffic areas that see large levels of child or pet activity,” she observes.\nThe most popular carpet colour is beige.\n“Although it has an undeserved reputation for looking dull and being a little too safe, the great thing about beige is that it compliments most other colours. No matter what color the walls in a room are, beige will almost certainly blend in well,” says Biira.\nDifferentiating genuine carpets from fake ones\nGeorge Ssentongo a trader dealing in carpets says some carpets loose color and that is a characteristic of fake ones.\n“Most dyes will appear brilliant and true to exactly what you are looking for, but the real test is how well they last under a stain and frequent traffic,” he says.\nHe says determining whether the dyes are good or bad lies within whether or not the dyes are colorfast.\nSsentongo adds that fugitive dies are not good for carpets. He observes that spilling anything water or oil based, will result in a bleeding dye mess on your carpet.\nKeeping carpets clean\nBiira says the doormat does wonders to reduce the amount of sand and dirt tracked into the home. “Buy a doormat to expand the life of your carpet and keep it clean and longer,” she says.\n“Daily vacuuming helps pick up dirt and debris before it sinks deep into the carpet. Less dirt expands the life of your carpet,” says Biira.\nClean spots and spills immediately.\nBiira says it is important that any person with a woolen carpet clean the stains and spills immediately.\n“Not only will a quick clean reduce the rate of absorption but it also helps reduce the bacteria and other problems associated with a spill,” she notes.\nShe adds that regardless of what type of carpeting you choose, having them cleaned with professional carpet cleaning services once a year can greatly extend their life.","A lot of people prefer carpets for flooring since they are soft, warm and elegant. Nevertheless, there still others who fear having carpets because they think that carpets involve a lot of hard work and maintenance. Your carpet will ultimately become a victim of drops, accidents, spills and whatever that is at the bottom of the shoes regardless of your efforts. In that case, you should consider hiring professional carpet cleaners. However, if you decide to clean it yourself, then there are probably a number of things that you need to keep in your mind.\nBelow is a complete list of 10 carpet cleaning tips from pros:\n1. Blot Stains, Don’t Just Rub Them\nDab the stains using a good cleaning solution along with a sponge, paper towels or clean cloth. The most important thing here is blotting. Blotting normally puts a smaller pressure on the stains to soak them up. Rubbing normally causes the dust particles to be grounded into carpet fibers which can lead to premature breakdown of the fibers. Always ensure that you’re blotting from outside of the stains inward because blotting outwards can spread the stains.\n2. Club Soda Process\nBlot up the area using the club soda placed on a cloth. In case it does not work out, put a solution of water and vinegar in a handheld sprayer. You should spray the whole stained area with the solution and leave it for up to 10 or 15 minutes so as to soak in. Press a sponge onto the affected area in order to soak up this solution as well as the stain. You might have to repeat the process to remove the stain. Rinse the dirty spot using warm water when the stain is removed. Using your hands, brush carpet strands following their natural direction. Lastly, put a white paper towel on the area and add more weight on top of them. The towel helps to absorb the wetness from the carpet. Do not remove the towel until when your carpet is dry for around one day.\n3. Try Using Shaving Cream\nThe best carpet cleaner used on general stains is the ordinary shaving cream as it removes almost every type of stains. You should apply a shaving cream directly on these stains and leave it for close to 30 minutes. After the cream has set, remove it using a dry cloth before spraying this area with a solution of vinegar and water before wiping it away using a cloth.\n4. Freeze-Dried Gum\nIn order to remove dried chewing gum on your carpet, you need to get some ice cubes and then freeze the dried gum for close to 30 seconds. When the gum becomes frozen solid, lift the glob using a spoon and then cut the carpet strands that are as close as possible to the gum. Cutting a small piece of carpet cannot be easily noticeable.\n5. The Dishwasher Detergents vs Grease\nUsing one drop or two drops of grease-cutting dishwashing detergents inside a cup of water is also a method of removing difficult-to-clean grease stains. It will clean the grease inside the carpet just as it normally does on the dishes. Put this solution in the spray bottle and then spray on the stain. You may be required to do this several times for the large stains.\n6. Heat Wax\nBurning candles might result in the wax dripping on your carpet therefore drying up and becomes embedded. In that case, you need to put a white cloth on your iron box and then place it on top of wax so as to warm it. You can thereafter scrape off this wax using a butter knife. When you’re done, you need to lay a towel on the affected area and iron the towel. This makes the wax melt and bind on paper, and the candle wax would be gone after doing it for a few applications. However, you should not use an iron box in this way for over 30 seconds or you’ll risk burning your carpet.\n7. Bring Hydrogen Peroxide to Your Rescue\nA cut on the finger and a few drops of blood on your carpet does not mean that your shag is all but ruined.\nFirstly, you need to loosen the dried blood using water and a little detergent. Scrape off the blood from the carpet fibers. In order to remove the remainder, you have to directly apply hydrogen peroxide on the stain. Hydrogen peroxide foams immediately it comes into contact with blood so you don’t have to be surprised. You should then dab hydrogen peroxide with a towels so as to dry your carpet.\n8. Clean The Pet Accidents Organically\nWell-trained pets can also have accidents on the carpet. Organic cleaners are preferred for cleaning the mess rather than chemicals. You should spray the organic cleaner at the stain and scrub. The organic cleaner should then be wiped using a cloth or a towel. Besides, Eco-Spot and similar cleaners can be used to clean other types of stains such as coffee and sauces.\n9. Candy Crushed\nYou should try to remove the candy using the spotter brush or even a butter knife. Thereafter, use a sponge to apply water that is mixed with a little soap on the carpet. It is very crucial to remove all the sugar on the carpet area that is affected. By not removing the sugar, the area will attract dirt and debris very easily. You should dry this spot just by blotting it using a cloth or a towel once you have removed the candy.\n10. Deep-Clean Regularly\nCleaning your carpet is vital in maintaining it. Steam cleaning involves cleaning the carpet by injecting a cleaning solution which is under pressure deep into your carpet through water-jet nozzles. The machine removes the solution together with the debris and dirt found in the carpets. The water enters the fiber down towards the backing and thus loosens any embedded soil, removes grease and oil deposits. As such, you get your carpet as clean as it was before. It is recommended that you do deep cleaning after every 6 months regardless of your family size.\nPrevent your carpets from harming your health. Carpets might also become harmful to the health of your guests and other family members. They can spread bacteria, attract allergens and expose you and your whole family to pollutants. In a residential home with pets, young children or even the elderly, regular carpet cleaning is very essential and the above tips will certainly help you in keeping your carpet clean at all times."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:9092930f-a4b8-4aab-a3ae-2964ea1d6594>","<urn:uuid:d4a9f012-10e5-4613-92b4-386b27594fc0>"],"error":null}
{"question":"Compare the storage container prep requirements between boilable freezer bag method vs home canning tomatoes - which needs more sterilization?","answer":"Home canning tomatoes requires more thorough sterilization of containers. For canning, both jars and lids must be completely sterilized either through a dishwasher or hot water bath, with lids specifically needing to be sterilized in boiling water, as proper sterilization is crucial for keeping the food bacteria-free over months. In contrast, for the boilable freezer bag method, the requirements are less stringent - the instructions only specify that the work area and equipment need to be 'spotlessly clean', with no specific sterilization requirements for the freezer bags themselves.","context":["Freezing vegetables doesn’t always have to be a drawn-out process. In The Beginner’s Guide to Preserving Food at Home (Storey Publishing, 2009), author Janet Chadwick provides new techniques for fast and easy ways of freezing fruits and vegetables that leave your produce tasting more like fresh, even in the winter months. Taken from “Chapter 3: Basic Techniques for Preserving Food,” this excerpt explains how to revamp the standard method of freezing vegetables and adds new methods, such as unblanched freezing and the boilable freezer bag method, to your preserving repertoire.\nYou can purchase this book from the MOTHER EARTH NEWS store: The Beginner’s Guide to Preserving Food at Home.\nFreezing maintains the natural color, fresh flavor, and high nutritive value of fresh foods. The objective is to bring foods to the frozen state quickly. When properly done, fruits and vegetables are more like fresh than when preserved by any other method. Best of all, freezing vegetables and fruit is fast and easy.\nI had been freezing garden vegetables for years when I began experimenting with the process. I discovered that the old standard method of washing and preparing the vegetables, then blanching, cooling, drying, packing, and freezing them was not always the fastest, easiest way to produce the best finished product. Many vegetables can be frozen without blanching (although their shelf lives in the freezer will be shorter), and greens can be stir-fried instead of blanched for a better product.\nTip: To prevent injury when slicing vegetables with a manually operated rotary slicer, blade slicer, or slaw slicer, wear a clean cotton garden glove on the hand that is apt to come in contact with the slicing blade.\nUnblanched Freezing: 5 Quick Steps\nThis is the fastest, easiest method of freezing vegetables. It was originally thought that this method was acceptable only for chopped onions, peppers, fresh herbs, or other vegetables that were to be stored for less than 1 month. But I have found that many unblanched, frozen vegetables can be stored for up to twice as long and still maintain good color, flavor, and texture. Try this method with onions, peppers, herbs, celery, corn in husks, cabbage, sugar snap peas, summer squash, young tender broccoli, and green beans. It is the preferred method to use with berries. It can also be used with super-quality fruits, especially ones you plan to use semi-thawed, or baked in a dessert such as a crisp or a crumble.\nCooking tip: Frozen, unblanched vegetables are best cooked by stir-frying. To do so, melt 1 teaspoon of butter per serving in a heavy, preheated skillet. When the butter has melted, add the frozen vegetables and stir and toss the vegetables over high heat to the desired degree of tenderness. Cook until all moisture is evaporated. If more moisture is needed to cook to desired stage of doneness, add water, 1 tablespoon at a time.\n1. Make sure your work area and all equipment are spotlessly clean. Assemble your equipment and set your tools where they will be most useful. You will need a scrubbing brush, towels, freezer bags, a small pillow (optional), and a labeling pen and tape.\n2. Select vegetables that are slightly immature. Wash the vegetables and drain on towels.\n3. Prepare the vegetables: slice, dice, chop, julienne, or leave whole. Leave berries whole; peel and slice or chop other fruit.\n4. Pack in freezer bags, expelling as much air as possible. Label with name of product and date.\n5. Freeze in a single layer in the coldest part of the freezer.\nWhen freezing fresh vegetables without blanching, they should be used within 6 to 8 weeks. The best methods of cooking vegetables frozen in this manner are stir-frying and steaming.\nBoilable Freezer Bag Method: 12 Steps\nThis method of freezing vegetables often produces the best-tasting product. Since the vegetables never come in contact with water, all color, flavor, texture, and most nutrients are preserved. Adding butter to the bag, when desired, coats the vegetables with a protective film that further enhances the quality and flavor of the finished product. Experiment with combinations of vegetables, such as peas and tiny onions, or peas and carrots. Sliced, diced, or julienne vegetables work best. Whole carrots and beets do not freeze well by this method. Strong-flavored vegetables, such as broccoli, cauliflower, cabbage, and turnips, should not be frozen by this method.\nTime is saved with this method because bags of food can be blanched in multiples; cooling requires no special timing or handling (allowing you to continue packing); and since all vegetables are processed within the package, pans need only to be rinsed and dried, making cleanup a snap.\nI tested this method against the standard freezing method with green beans. After the initial washing and trimming (time for both was the same), I timed the balance of the freezing procedure. I was able to pack, blanch, and cool a half-bushel of green beans by the boilable freezer bag method in 29 minutes, versus 1 hour and 25 minutes for the standard method. Try it yourself!\nThe real challenge with this method for freezing vegetables is locating a bag sealer and suitable freezer bags. For step-by-step illustrations of this process, check out the images at the top of the first page of this article.\n1. Make sure your work area and all equipment are spotlessly clean. Assemble your equipment and set your tools where they will be most useful. You will need a scrub brush, towels, a chopping board, knives, a food processor, a wide mouthed funnel or large spoon, freezer bags, a small pillow (optional), a bag sealer or electric flat iron, an indelible marking pen and freezer tape, a large roaster half-filled with water for blanching, a large kettle for cooling, ice packs, cubes, or chunks of ice, tongs, potholders, and a timer.\n2. Select young, fresh vegetables that are just table-ready or slightly immature. Wash well; drain on towels.\n3. Begin heating water in the roaster for blanching.\n4. Prepare vegetables as desired: slice, dice, chop, julienne, or leave whole (except for large, dense vegetables, such as carrots and beets).\n5. Plug in the bag sealer or iron. Fill four boilable bags with vegetables in meal-size portions, making sure that when the vegetables are distributed, the package is no thicker than 1 inch. Add butter and seasonings if desired. Use as much as you would use if cooking for a meal.\n6. Expel as much air as possible and seal with the automatic bag sealer. Or place the bag on a towel, cover with a damp cloth, and seal with an electric iron. Label and date bag.\n7. When four bags are packed, drop them into the boiling water and blanch with the pan covered. (The bags will float on top of the water. This is all right, as long as each bag has one side in contact with the boiling water.) Check the timing (see chart in the image gallery on the first page of this article) and set the timer. A rule of thumb is to blanch for double the length of time suggested for the standard blanching method. Use a slightly shorter time for tender young vegetables, and a slightly longer time for more mature ones. Start counting the time as soon as you replace the cover.\n8. Fill the cooling kettle with cold water and ice packs or cubes or chunks of ice. Continue packing while the first batch is blanching.\n9. When the blanching time is up, remove the packages from the blanching kettle and place them in the ice water. Make sure the ice holds the bags down in the water, since air left in the bag tends to make the bags float. During the chilling time, occasionally knead the bags to move the cold into the center of the packages.\n10. Add four more bags to the blancher and continue as before, until all the vegetables are blanched. Leave the processed bags in the ice until you are completely finished, unless you need the space for more vegetables.\nWhen all the vegetables have been blanched, leave the bags in the ice water for an additional 10 minutes. While the bags are cooling, clean up your work area.\n11. Remove the bags of vegetables from the water and dry with towels.\n12. Freezing vegetables in a single layer in the coldest part of the freezer is best. Remove the ice bags from the cooling kettle, pat dry, and return to the freezer.\nFreezing fruits and vegetables this way allows produce to be prepared in the bag , removed and steamed, or stir-fried for faster cooking.\nThis excerpt has been reprinted with permission from The Beginner’s Guide to Preserving Food at Home by Janet Chadwick, published by Storey Publishing, 2009. Buy this book from our store: The Beginner’s Guide to Preserving Food at Home.","I am a beginner when it comes to canning veggies and fruits. I have only canned tomatoes on my own so far. Last fall was the first time I had ever even canned tomatoes on my own. Growing up I had helped my mom and grandma plenty of times, but never with out someone holding my hand. Tomatoes are a great thing to start with though. Since this is the second year I have canned them I have learned a few things, and of course tomatoes are super easy. Last year I didn’t use a ice cold sink full of water to cool the tomatoes off after boiling them. Be sure to do this. I learned that the hard way. I remember last year my hands were literally burned from peeling tomatoes because I didn’t chill them afterwards. This year I didn’t miss that step.\nOne other thing I learned is I need a dish washer. The little house we are living in right now doesn’t have one. Come next year though we will be moved into a new house, with a dish washer. You want your jars nice and hot before packing the tomatoes into them. Since I don’t have a dishwasher I just have them in hot water after washing them very well. Anyways, my point is that it would be a quicker process if you can use a dishwasher, but it can be done in a small kitchen with no dishwasher at all. Anyone can can tomatoes, I promise you!\nCanned tomatoes can be used for so many things as well. Some of the things I use them for are stews, soups, spaghetti, lasagna, and sort of pasta. The sky’s the limit with canned tomatoes!\nThis recipe is very easy, the only thing you really must be sure of is to make sure your jars are sterilized properly in order for the food within to keep bacteria free over the coming months.\nWhat you will need:\nWide mouth Quart canning jars\nI like to use the wide mouth quart jars because they are easier to get your hand in the jar to push the tomatoes in.\nYou need to sterilize the jars: You can either run them through the dish washer, leave them in the hot dishwasher and take one out at a time as you fill them with tomatoes. After you fill them with tomatoes put them in your water bath canner that has hot water but not boiling in it. Canning jars are sturdy, but to be safe you don’t want to put a cold jar into boiling water.\nI have found that you need one quart jar for every three pounds of tomatoes.\nYou’ll also need rings and lids. I always by new lids, but reuse rings. The lids need to be sterilized as well. I put them in boiling water while I am getting the jars ready.\nYou could use a jar lifter and a funnel, but they are not necessary.\nice and water\nwater bath canner\none small pot to sterilize lids and one large pot to boil tomatoes\nProcessing the Tomatoes:\nThis recipe is for tomatoes canned in their own juices:\nFirst sterilize your jars in your dishwasher or wash in hot water, then place them in your water bath full of hot water but not boiling. If you use the dishwasher way just leave the jars in the dishwasher and remove them one at a time as you fill them with tomatoes.\nPlace 5-6 tomatoes in the pot of boiling water for at least 60 seconds or until you see the skins peeling. Immediately take them out of the boiling water and place them in the sink full of cold icy water to stop the cooking. The skins should come off very easily once cooled. Sometimes you might need a knife to get the stubborn ones.\nPlace the skins in the other side of your sink. I like to cut my tomatoes into fourth or half before placing them in the jars. Add a few tomatoes then add your lemon juice and salt. I add 1 Tablespoon of salt and 2 Tablespoons of lemon juice to each quart jar. If you are doing pints to half of that. Fill your jars until you have a few inches left to the top. Pack them in with your hands to get as many as you can in there.\nTake the lid out of your boiling water with a fork and carefully place it on top the jar. Be sure to line it up right. Then put the ring on tightly. Then place them in the rack of your water bath. The one I used was a 7 quart water bath, so the rack could old 7 quarts at a time.\nTurn the heat up to high on your burning. Once the water is boiling set your timer for and hour and twenty minutes to process the tomatoes.\nWhen your timer goes off remove the jars from the water bath canner and place on a towel on the counter. Make sure to not have the jars touching each other. Once the jars begin to cool you will hear the lids pop. This means they have sealed. Now you can enjoy your yummy garden tomatoes all year long!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:a2c611ca-644b-47d9-a103-94b530500c20>","<urn:uuid:ed1ddd18-7eed-4690-9ffc-f85c5f9f268a>"],"error":null}