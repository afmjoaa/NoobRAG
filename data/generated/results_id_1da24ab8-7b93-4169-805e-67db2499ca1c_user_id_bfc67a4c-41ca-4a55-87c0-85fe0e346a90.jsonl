{"question":"What's the distinction between how rebuttal evidence works in a criminal trial versus a domestic violence case?","answer":"In a criminal trial, rebuttal primarily occurs through cross-examination of witnesses and presenting contrary evidence to challenge the state's burden of proving guilt beyond reasonable doubt. In domestic violence cases, rebuttal takes on additional importance as victims often need to present corroborating evidence like photographs, physical items, or documentary evidence to counter potential claims by abusers who may deny the violence occurred or claim self-defense.","context":["Court Process Overview\nEntering a Plea\nYou must decide upon and enter a plea to the charge against you on or before the response date on your citation. If you signed a citation in front of an officer, you did not plead guilty, but only signed a promise to appear in court within 20 business days. There are three possible pleas to a complaint: Guilty, Nolo Contendere (No Contest), or Not Guilty.\nYour decision on what plea to enter is the most important decision you will have to make. Whether you feel that you are guilty or not, we suggest that you read the following explanations of all three types of pleas before making your decision.\nBy a plea of guilty, you admit that you committed the act charged, that the act is prohibited by law, and that you have no defense for your act. Before entering your plea of guilty, you should understand the following:\n- The state has the burden of proving its case against you. The law does not require you to prove anything. You have the right to hear the state's evidence and to require it to prove its case against you.\n- If you were involved in a traffic accident at the time of the alleged offense, your plea of guilty could be used later in a civil suit for damages as an admission that you were at fault or were the party responsible for the accident.\nA plea of \"nolo contendere\" (no contest) means you do not contest the state's charge against you. You will be found guilty upon a plea of \"nolo contendere,\" but it is not an admission by you that you are guilty. Also, a plea of \"nolo contendere\" or \"no contest\" cannot be used against you in a civil suit for damages as can a plea of guilty.\nA plea of guilty or \"nolo contendere\" and a waiver of jury trial may be entered in writing by mail before the trial date. You should be prepared to pay your fine upon entering a plea of guilty or \"nolo contendere.\"\nA plea of not guilty means that you are informing the court that you deny guilt in this case, and that the state must prove what it has charged against you.\nIf you plead not guilty, you have the right to a trial by judge or jury. You will need to decide whether to employ a lawyer to represent you at trial. You may defend yourself (Pro Se), but no one except a lawyer may represent you.\nAll proceedings will be conducted according to the rules of criminal procedure and the rules of evidence. If you choose to represent yourself Pro Se, you must be prepared.\nInnocent until Proven Guilty\nUnder our American system of justice, all persons are presumed to be innocent until proven guilty beyond a reasonable doubt. On a plea of not guilty, a trial is held and the state is required to prove what it charges in the complaint, \"beyond a reasonable doubt,\" before a guilty verdict can be reached.\nUnder Texas law, you can be brought to trial only after a formal complaint is filed. The complaint is the charging document that alleges what you have done, and the fact that such action is unlawful. You can be tried only for what is alleged in the complaint. Trials are conducted under the Code of Criminal Procedure as adopted by the Texas legislature. These laws may be found in Chapter 45 of the Texas Code of Criminal Procedure.\n- You have the right to inspect the complaint before trial and have it read to you at the trial itself.\n- You are entitled to hear all testimony introduced against you.\n- You have a right to cross-examine any witness who testifies against you.\n- You have the right to testify in your own behalf. You also have the right not to testify. If you choose not to testify, your refusal cannot be considered in determining your innocence or guilt of the charge.\n- You may call witnesses to testify in your behalf at the trial, and have the right to have the court issue subpoenas to these witnesses to ensure their appearance at the trial.\nPresenting the Case\nAs in all criminal trials, the State will present its case first by calling witnesses to testify against you. You will have the right to cross-examine each prosecution witness. In other words, you can ask the witness questions about their testimony. However, you cannot argue with the witness. Your cross-examination of the witness must be in the form of questions only. Do not attempt to tell your version of the incident at this time - you will have an opportunity to do so later if you testify.\nAfter the state has presented its case, you may present your case. You have the right to call any witness who knows anything about the incident, but the witnesses can testify only about matters of which they have personal knowledge.\nIf you choose, you may testify in your own behalf. Since you are the defendant, you cannot be compelled to testify. It is your choice, you may do as you wish, and your silence cannot be used against you.\nThe state also has the right to cross-examine all witnesses called by you. If you testify in your own behalf, the state may cross-examine.\nAfter testimony is concluded by both sides, you can make a closing argument by telling the court why you feel that you are not guilty of the offense charged. But such statement can only be based on the testimony heard during the trial. Additional testimony is not admissible in the closing argument.\nThe verdict will be based on the testimony and the facts presented during the trial. In making the determination, the judge or jury can only consider the testimony of the witnesses who testify under oath. If found not guilty, you will be acquitted of the charges. If you are found guilty, the judge will announce the penalty at that time.\nThe amount of fine assessed by the court is affected only by the facts and circumstances of the case. Mitigating circumstances may lower the fine, even if you are found guilty. On the other hand, aggravating circumstances may increase the fine. In no case may the fine exceed $200 for most traffic violations; $500 for certain penal code violations, and $200 to $2,000 for certain city ordinance violations. (These amounts do not include state court costs.)\nCourt costs will be charged if you are found guilty and assessed a fine, regardless of the amount. Court costs in the municipal court are set by the state, not by the court. Court costs must also be charged even if the fine is suspended and final disposition of your case is dismissed under the Deferred Disposition procedure. If an arrest warrant is issued, a warrant fee of $50 will be added to the fine and state court costs.","What is the order of events in the courtroom?\nEach state and court may have variations on this, but this is generally the order of events:\n- The judge, clerk, or bailiff will call your case. In some courts, a mediator might first talk with you, and then with the other party, to see if you might be able to reach an agreement without a trial.\n- In a criminal case or in other types of formal or more complicated cases, the judge might ask the parties to give opening statements. This is your chance to talk about what evidence you will show at trial and how you want the judge or jury to rule, based on that evidence.\n- In less formal proceedings, the bailiff or the clerk will swear you and the other party in by asking you both to state your names and to swear or affirm to tell the truth.\n- The party that filed the first court papers, usually called the complaint or the petition, is considered the plaintiff or the petitioner. The plaintiff/petitioner will tell his/her side of the story first. This includes the party’s testimony, calling any witnesses s/he may have, and entering any evidence that s/he has. We have more detailed information about this process in the rest of this section.\n- The other party or his/her lawyer may ask you and your witnesses questions as part of cross-examination, which you must answer truthfully. You may have the right to object to certain questions that the other party asks.\n- If your witnesses testimony was damaged during cross examination, you may be able to ask clarifying questions during re-direct.\n- The judge might also ask questions to you or to any other witnesses.\n- Next, the other party, who will usually be known as the defendant or respondent, will be allowed to present his/her case. S/he can testify and tell his/her side of what happened, call witnesses, and enter his/her evidence. It may be very different from yours. The same principles apply as mentioned earlier, about the right to ask cross examination questions and make objections during the abuser’s testimony, if legally appropriate.\n- The judge might then ask for closing statements, or not, depending on the court and the type of case. Closing statements are your chance to sum up the evidence and ask the judge again for a specific decision.\n- The judge will make a decision after hearing both sides and considering the evidence. The judge may make the decision right away or may take a recess to give the decision. The recess may be only for a few hours or it may take days or weeks to give the final decision.\n- If your case is for a restraining order. The judge may grant you and sign the final restraining order that day at your hearing. If so, then make sure you get a copy, review it, and ask the judge if you have any questions about it. If the judge is not giving his/her decision that day, make sure to ask the judge to extend your temporary restraining order if you have one.\nWhat do I keep in mind when going to court?\n- Be on time.\n- Have your witnesses there and ready.\n- Have your evidence ready.\n- If you have subpoenaed witnesses or documents and they are not in court, you should inform the judge.\n- Dress appropriately (as if you had a job interview).\n- Speak directly to the judge; s/he should understand if you feel nervous. Do not speak or argue with the abuser during the hearing. Although it may be upsetting to hear the abuser say things that are untrue, you should have the opportunity to tell your story directly to the judge.\n- Always address the judge as “Your Honor.”\n- Be prepared to spend all day in court. (There may be hearings before yours.) If you have children, try to find someone to take care of them while you are in court.\n- If the abuser comes to court with a lawyer and you do not have a lawyer, ask the judge for a “continuance” so you can look for a lawyer.\n- While you are waiting to be called, it is your right to move seats if the abuser sits next to you, and to receive help from court staff in keeping the abuser away from you. Tell the bailiff or any sheriff, police or security guard if you are afraid for your safety.\n- Stand when the judge enters and sit when the judge or bailiff asks you to.\n- Try to remain calm but it is OK if you show emotion.\n- Take deep breaths if you feel yourself getting tense. Never lose your temper in the courtroom.\n- Always tell the truth.\n- If you don’t understand a question, just say so. Don’t answer a question that you don’t understand.\n- If you don’t know the answer to a question, just say so. Never make up an answer.\n- Remember that you know your story better than anyone - you are the expert. Don’t let the abuser or the judge or a lawyer throw you off.\nWhy would I enter evidence in court?\nThe only way a judge can decide a court case is based on the evidence the parties present during the case. In many cases, the evidence that a judge has is mostly testimony from the parties and testimony from their witnesses. However, there are also other kinds of evidence that you may be able to show to the judge to help prove your case. Additional evidence can be especially important if the other party is going to lie or tell a different version of what happened than you are. When there is conflicting testimony, the judge has to decide who s/he believes is telling the truth. The judge wil often look to other evidence and witnesses to decide which party is telling the truth.\nIf you have a case that involves domestic violence, having evidence to present that corroborates your version of the events can be especially important. Many survivors of domestic violence face disbelieving judges who are quick to accept the abuser’s efforts to explain away the violent incidents or behavior. Some abusers even claim that the violence did not happen at all and that the survivor is making up allegations to try to get an advantage in court. Others will claim that the victim is actually the abusive partner and that any injuries to the victim were from self-defense. It is important to anticipate these tactics and have evidence ready that you can show the judge to prove your version of the events and to get the judge to rule in your favor.\nWhat evidence can I show the judge?\nYou can give almost anything to the court as evidence as long as it is relevant to the case and it does not violate one of the rules of evidence. For example, you generally cannot give in a letter that someone wrote for your trial or tell the judge something that someone else said due to the rule against hearsay. The most common types of evidence are:\n- Testimonial – The testimony presented by the parties and any witnesses that they call to help with their case.\n- Expert – Testimony from a witness who has expertise in a technical area and will give an opinion that the judge could rely upon when deciding the outcome of the case.\n- Physical/real evidence –Any physical, tangible, or touchable thing that relates to the case. For example, a torn shirt from a domestic violence incident, an item that may have been broken by the abuser, a weapon, etc.\n- Documentary – Letters, emails, photographs, and other documents relevant to the case.\n- Demonstrative – A chart or illustration that you create for the trial to explain something to the judge that might not be easily understood without some visual assistance. For example, when describing why you had to use physical force in self-defense, you might draw out the layout of the house and indicate where the parties were standing to show why you could not escape.\nDoes testimony count as evidence?\nTestimony is a kind of evidence, and it is often the only evidence that a judge has when deciding a case. When you are under oath in court and you are testifying to the judge, what you say is considered to be truthful unless it is somehow challenged (“rebutted”) by the other party. If the other party can show the judge that you aren’t telling the truth, through testimony, evidence, or effective cross-examination, s/he can “rebut” your testimony. Then a judge has to make a decision on who s/he thinks is being more truthful (“credible”)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:3cf4249b-f4db-4e80-8055-2d8d43462001>","<urn:uuid:87963e03-d33f-46aa-84fd-4b998651af8b>"],"error":null}
{"question":"¿Cuál es más destructivo para las estructuras de madera, the eastern subterranean termite (Reticulitermes flavipes) or the Formosan termite (Coptotermes formosanus)?","answer":"The Formosan termite is more destructive. While the eastern subterranean termite (Reticulitermes flavipes) is described as the most economically important termite species in Texas and the U.S., the Formosan termite is specifically characterized as the most aggressive and destructive timber pest in the United States. Formosan termites can cause major structural timber damage to homes within a few months, and a single colony does more damage than colonies of other U.S. subterranean species. They can develop huge nests containing millions of termites that aggressively seek and devour structural timbers.","context":["So in lieu of any official list, here is my list of the important WDIs in Texas. If you're from another state, the list is likely to be similar, although the relative importance of each of these pests varies from region to region.\n|Termite wings provide good diagnostic clues to the type of |\ntermites infesting a home. Reticulitermes wings are smooth,\nwithout hairs, and have two heavier veins along the leading\nedge of the forewing.\n- Subterranean termites (Reticulitermes species. Note: when the word \"species\" is written after the genus name, it refers to multiple different species within the genus). Knowing the exact species is not important for a professional doing an inspection, but keep in mind that there may be some differences (e.g., swarming times or potential for destruction) among the different termite species. The most economically important termite species in Texas, and the U.S., is Reticulitermes flavipes, the eastern subterranean termite. These termites are called \"subterranean\" because they need contact with the soil, and almost always maintain their nest underground while foraging on both underground and above-ground wood. Inspections should focus on looking for alates (swarmers) and the presence of mud tubes extending from the soil into the structure. Alates can be confirmed as Reticulitermes by their forewings, which bear two strong, dark veins along the leading edge. Wood damage from this and other Reticulitermes species consists of galleries chewed into the spring wood, following the growth rings. Also, because they are subterranean, gallery walls will be covered with specks of \"mud\", a combination of feces, saliva and soil.\n- Formosan subterranean termites Coptotermes formosanus. This is an exotic termite species that has established in parts of Texas, but is also found in other parts of the South and in parts of California. One of the principal means of its spread appears to be via recycled railroad ties. The Formosan termite belongs to the same family as Reticulitermes termites, and shares some of the same characteristics, including subterranean nests and mud spattered galleries; hence, it is also technically a \"subterranean\" termite. Identify Coptotermes by its nighttime swarming habits and its large, yellowish-colored alate. The wing veination is similar to Reticulitermes, but the wing membranes are covered with fine hairs. Workers are not easily distinguished from our other subterranean species, but soldiers have a teardrop-shaped head, contrasted with the rectangular head of Reticulitermes workers. Galleries are similar to other subterranean termites, but Formosan termites produce \"carton\", a dense, honeycombed structure made from mud and wood pulp cemented together with saliva and feces.\n- Drywood termites (Cryptotermes and Incistitermes species) The so-called drywood termites are distinguished from subterranean termites by their above-ground nests and lack of contact with the soil. Because they live in wood, which usually has lower moisture content than soil, drywood termites are very efficient conservers of water. The most obvious sign of this skill are the fecal pellets they produce. Unlike the wet, smeared feces of subterranean termites, drywood feces are hard, dry pellets. In the drywood termite rectum all water is squeezed from the feces by six rectal pads, leaving small (1/32 inch-long), six-sided pellets. Finding these pellets is hard proof of a drywood termite infestation. In addition, drywood termite galleries do not follow the grain of the wood, but may extend across multiple annual rings. Alates fly at night and have three strong, dark veins at the leading edge of the forewing. Drywood termites are most common in warm, high-humidity regions, such as the Gulf coastal areas, and parts of southern California. They may be found in other parts of the country, however, when brought in on furniture or infested lumber.\nLyctid powderpost beetle adults (left) are distinguished from\nthe common pantry pest, red flour beetle (right), by their\nround eyes and two-segmented antennae.\n- Lyctid powderpost beetles (family Bostrichidae, subfamily Lyctinae). These small, cylindrical beetles are usually brought into homes in infested hardwood trim or flooring, or in infested furniture. They are one of the most important WDIs in Texas. If you're not in the habit of looking carefully for signs of these beetles during a WDI inspection, you're opening your business up to a potential legal mess. The presence of these beetles in new homes has become highly litigious, as the owners look for someone to blame for their new, beetle-infested home. It's critical to look carefully at all hardwood trim, including wainscoting, baseboards, windowsills, cabinetry and wood flooring. On horizontal floors or trim, sawdust usually accumulates in volcano-like piles surrounding the adult beetle emergence hole. On vertical wood, small piles of very fine, almost silky, frass will accumulate on cabinets, edges or floors underneath the emergence hole. Adults can be distinguished from the similar-appearing red flour beetle by their darker color; globular, protruding eyes; 2-segmented antennal club and enlarged hind coxae.\n- Carpenter ants (Campanotus species). This insect is usually listed as a WDI; however in Texas carpenter ants are not very likely to do structural damage. They are more likely to occur in small colonies in wall voids and in insulation, but rarely as destroyers of sound wood. Some species of carpenter ant, like C. modoc and C. pennsylvanicus, are well-known wood destroyers, especially in the Pacific Northwest and in the northeastern states. They have given all carpenter ants a reputation as wood destroyers. For this reason, and because they are not difficult to report, I would recommend including carpenter ant evidence on a WDI inspection report. The similarly-colored (but physically very different) acrobat ant, is not reported to eat or damage wood, therefore I would not report it as a WDI. Carpenter ants are relatively easily identified by their large size, polymorphism (different sized workers in the same colony), single node and smooth curved thoracic profile. Inspectors should also be on the lookout for carpenter ant frass, which is the colony's trash dump. Frass piles may include wood, insulation, dead insects, and sometimes pupal cases. Any unusual debris piles that contain insect fragments are likely carpenter ants.\n- Old House borer (Hylotrupes bajulus). The round-headed borer family, to which the old house borer (OHB) belongs, consists mostly of larger beetles with long antennae. Most infest only dying or recently killed trees, so pose little long-term threat to a structure (although they may emerge from infested wood in the first year or so of a new home). The OHB is an exception to the low-threat rule because of its ability to re-infest homes after its first emergence. Most infestations occur in homes up to 10 years old, but they can also infest and re-infest older homes. According to Dr. Harry Moore, retired WDI expert from North Carolina State University, the OHB prefers wood with moisture content between 15 and 25%. This moisture level is higher than normal in all but the more humid parts of the country. Adult beetles are distinctive, emergence holes are oval in shape and 1/4 to 1/3 inch maximum diameter. Frass consists of very fine powder and tiny, elongate, blunt-ended pellets. Infestations of OHB are most common in the humid Piedmont areas of the mid-Atlantic seaboard; however they can be found in homes throughout the eastern U.S. Infestations are relatively rare in Texas.\n- Anobiid and other Bostrichid beetles. Any emergence holes in structural wood or trim, of course, should be reported on a WDI report. However, when the other pests listed above are ruled out, most of the remaining culprits will belong to miscellaneous species of anobiid (ANN oh BEE id) and bostrichid (boss STRICK id) beetles. Nationwide, anobiid beetles are the more important group. These beetles feed on sapwood of both hard- and soft-woods. They leave circular exit holes 1/16 to 1/8 inch in diameter, and produce a fine powdery frass with conspicuous pellets. Adults of these beetles are relatives of the cigarette beetle and have a similar with a oval to cylindrical shape and downward pointing head, hidden from above. Anobiid beetles thrive in wood with higher moisture content (15-30%) and probably for this reason, infestations in Texas are uncommon except in damp crawl spaces. Some of the smaller holes bored by anobiid beetles may be confused with Lyctid powderpost beetles, but the frass is distinguished by the presence of rough pellets.\nBostrichid beetles (with the exception of the Lyctid powderpost beetles) are generally incapable of re-infesting wood. Holes made by emerging adults are round and 3/32 to 9/32 inch in diameter. Frass is tightly packed inside tunnels and tends to stick together. Most bostrichid beetles have several rasp-like teeth on the front of the pronotum, presumably to aid in collecting and packing frass in the galleries. One of the most common ways bostrichid beetles are introduced into homes is via wicker baskets and furniture.\n|Formosan alate wings are covered with fine hairs, visible|\n|Drywood termite wings are often dusky and have at least|\nthree strong veins along the leading edge of the forewing.\n|Carpenter ants are easily identified by their large size, single|\nnode between abdomen and thorax, and smoothly rounded\nthoracic profile. Many Texas species are bi-colored, like this\nspecimen; but color alone can be misleading.\n|The old house borer is a medium-sized beetle (0.6 to 1 inch)|\nwith two raised bumps and a mustache-shaped ridge on\nthe pronotum (shield behind head).\n|Bostrichid beetles are mostly a minor WDI pest. The rasp-like|\npronotum and cylindrical body with abruptly angled wing\ntips are characteristic of this family.","Economic Significance: Formosan termites are the most aggressive\nand destructive timber pests in the United States. It is an imported species,\nnative to China.\nIt can develop huge nests containing millions of termites aggressively\nand relentlessly seeking and devouring structural timbers, utility poles\nand other timber structures, including ships and barges. Infestation can\noccur to living trees, such as oak, cypress, pine and maple.\nThey often cause power failures by chewing through electrical cabling.\nA termite to be feared - it is known to cause major structural timber\ndamage to homes and buildings within a few months.\nArea of Distribution: The Formosan subterranean termite (Coptotermes\n) has now become established in South Carolina and other\nsouthern states. At least one colony has been found In California (1995).\nFormosan termites are a serious timber pest in Hawaii and coastal regions\nof Texas, South Carolina, Louisiana, South Carolina, South Carolina, Tennessee,\nGeorgia, South Carolina and southern California, as well as, inland towns\nThe Formosan termite is rarely found North of 35° N latitude. They\nhave been reported from the following states: Alabama, California, Florida,\nGeorgia, Hawaii, Louisiana, Mississippi, North Carolina, South Carolina,\nTennessee, and Texas. Their distribution will probably continue to be\nrestricted to southern areas because their eggs will not hatch below about\n20° C (68° F).\nIdentification of Swarmers and Soldiers: Formosan termites swarm in huge\nnumbers in late spring or summer; usually following a warm rainy day.\nThey prefer to swarm in times of high humidity in the evening hours from\ndusk to midnight. The swarmers are attracted to lights and are about 1/25'',\nincluding wings. Their body color is pale yellowish brown. A fontanelle\n(frontal gland pore) is present.\nThe swarmers have four wings of equal size with dark hard veins in the\nfront portion of the front wing. The wings are a translucent, slightly\nmilky color and covered with tiny hairs. The armoured head of the soldier\nis rounded tapering toward the front. A fontanelle (frontal gland pore)\nis present on the soldier's forehead. They have large mandibles relative\nto their body, which is flat and narrower than the head.\nWhen disturbed the Formosan soldier termite may emit a white sticky latex\nsubstance from it's fontanelle - a defensive measure to ensnare their\nenemies, primarily ants.\nIdentification of Timber Damage: Formosan termites eat mainly the springwood\nof susceptible timbers, most often leaving the summer wood sections. Timbers\ninfested by Formosan termites usually have layered sections packed with\nmoist soil in high activity areas.\nBiology and Habits: Formosan termites are subterranean termites that typically\nlive in the ground and a large mature nest will periodically emit swarmers\nin large numbers over a wide area to find a mate from another colony nest\nto start up a new colony. A suitable location for nesting should provide\na constant moisture source and a readily available timber food source\nSeveral years are required before the termite colony reaches the typically\nmature size, which may contain millions of termites foraging for timber\nfood sources within a 400 feet radius, actively feeding on trees and free-standing\npoles as well as buildings and other timber structures.\nThe colony nests of Formosan termites are usually located in the ground\nbelow the frost line, but above the water table. They typically construct\nmud galleries or \"shelter tubes\" across hard objects in order\nto gain access to timber food sources.\nFormosan termites constantly search for new food sources. They are known\nto enter buildings through cracks in concrete flooring or to travel under\nparquetry or tile flooring through gaps of less than 1/16\" wide.\nThe space between the foundation and the first mortar joint is often enough\nspace for termites to enter a home.\nFormosan termites can establish secondary colonies in very moist wood\nof upper stories of buildings (even several stories above ground) and\ndo not need soil contact if there is a nearly constant moisture source.\nWhere moisture regularly collects inside the wall or other cavities of\na building, say from faulty plumbing or broken roof tiles, the Formosan\ntermite can develop a subsidiary colony nest, which may not require contact\nwith the ground to ensure its survival. This is particularly prevalent\nin areas of high humidity where wood moisture is above average.\nDue to its size and aggressive foraging behaviour a colony of Formosan\ntermites does more damage than single colonies of other U.S. subterranean\nspecies, and can cause significant structural damage to a home within"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:a9b9a205-1f57-4bca-93c6-c94467113db6>","<urn:uuid:d02f5856-8658-4de8-9c11-4edc40b9fc25>"],"error":null}
{"question":"How did Pierre Jeanneret and Le Corbusier's furniture from Chandigarh, India make its journey to Western collectors? Please provide the sequence of events.","answer":"The journey of the furniture from Chandigarh to Western collectors followed several steps: First, the furniture was removed from its original location in India, where it had been in everyday use or stored in poor condition. Then it was transported on container ships moving from East to West. After arrival, the pieces went through restoration in specialist studios. Subsequently, the furniture was sold at auctions for five-figure sums. Finally, the pieces ended up in luxurious settings across the West - in high-end homes in Paris and the Hamptons, and even on a yacht in the Mediterranean.","context":["Taken at face value, Amie Siegel’s Provenance is about the furniture designed by Pierre Jeanneret and Le Corbusier for the Punjabi capital of Chandigarh, its dispersion from that city, and later acquisition by Western collectors. In the context of its current installation at the Metropolitan Museum of Art, however, one could argue that it also happens to comment on every single one of the Met’s multitudinous holdings.\nThe first element of Provenance is a forty-minute video that traces the journey of Jeanneret and Le Corbusier’s highly prized furnishings—mostly desks, lounge chairs, sofas, and tables—from their Indian origins, through the art market, and into collectors homes in Europe and America. A story told in reverse, the film shows the furniture in rarefied domestic spaces from Paris and the Hamptons to a yacht in the Mediterranean. Siegel’s video then cuts to various auctions where the pieces are sold for five-figure sums. Subsequent scenes in a restorer’s studio are followed by images of the chairs on a container ship, making the journey from East to West. The film ends in India, where the objects are finally shown in their original setting, either in quotidian use or stockpiled in disrepair.\n“I didn’t see the chairs [or other furniture pieces] in person until we started filming,” Siegel told Vogue during a studio visit this fall. “Which I guess is like meeting an actor who you’ve been dreaming about appearing in your project. Once they’ve agreed to do it and you finally meet them for coffee, it’s like an apparition.” Provenance, an excerpt of which can be viewed below, is not only shot in vivid HD color befitting a contemporary Hollywood film, but also employs cinematic tropes that feel as highly produced as the luxury homes in the video. (The suspense, for instance, is nearly Hitchcockian by the end.) The result is as elegiac as it is detailed, layered with political and socioeconomic meaning. Comparing her shoot locations to those of shelter magazines like Architectural Digest and The World of Interiors, Siegel says, “There’s this uncanny quality that almost feels like the moment after the apocalypse. All the lights are on and nobody’s home. There’s something eerie and almost poetic in it—like when the curtains move because of the wind, but nobody’s there to see it.”\nThe video Provenance, first exhibited in 2013, was only the first component of a larger project that, according to Siegel, “presents a sort of condensed version of the marketplace.” Also on view at the Met are Lot 248, a video of Provenance being sold at auction at a Post-War & Contemporary Art sale at Christie’s London (which Siegel calls “the ne plus ultra, the Ur-auction room of all time”), and Proof (Christie’s 19 October, 2013), a Lucite sculpture that contains the page of the corresponding auction catalogue detailing Provenance and its condition of sale. (In other words, Lot 248 is the video of an auction of a video of, among other things, auctions. Talk about meta.) Siegel has recently also published an artist’s book entitled Catalogue, a bound collection of pages from various auction-house catalogues selling Chandigarh furniture. The book ends with a copy of the same page from the Christie’s catalogue that figures in Proof (Christie’s 19 October, 2013).\nOf course, the fact that Provenance is now at the Metropolitan Museum adds a further layer to Siegel’s contemplation of the art market and the cultural exchange it facilitates. “It’s fascinating [that Provenance is] exhibited in a museum with such a tremendous history for the acquisition of antiquities and other objects in curious, debatable, and wonderful circumstances across the board,” she says."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:0672f12d-f43a-4320-ac8b-00c73b4fa52c>"],"error":null}
{"question":"What similarities exist between Livestock Guardian Dogs and Newfoundland dogs in terms of their protective working roles?","answer":"Both types of dogs were historically bred as working dogs with protective duties, though in different environments. Livestock Guardian Dogs were bred to protect farm animals from predators like wolves and bears, while Newfoundland dogs specialized in water rescues and protection, even being strong enough to tow boats to shore. Both types are typically large dogs - LGDs often weigh more than 100 pounds, while Newfoundlands can reach 100 to 150+ pounds. Both breeds also developed specialized coats for their work - LGDs have protective coats to guard against predator bites, while Newfoundlands have a water-resistant double coat essential for their water rescue work.","context":["Livestock is an investment that farmers need to ensure is protected. Most farms are found in rural places where predators roam, and most will lose a few or more goats, sheep, cattle, and other livestock.\nWhen a farmer or rancher loses their livestock, they aren’t just losing the animal; they are losing an investment, they will not be able to recover. One way to curb this from happening is to have a livestock guardian dog on the farm.\nSo what is a livestock guardian dog? Livestock Guardian Dogs (LGDs) are a type of dog that is bred specifically for the duties of protecting farm animals from predators. These dogs are medium to large-sized, often weighing more than 100 pounds, and have a loud, incessant bark to alert the flock and farmer of dangers.\nTable of Contents\nHistory and Origin of LGDs\nLivestock guardian dogs have been used for thousands of years in Asia and Central Europe to protect livestock from wolves, bears, and wild cats. The earliest records of shepherd dogs date back to nomadic Sumerians around 3585 BC.\nTraditionally, livestock guardian dogs were most often used for sheep and goats, but they also are known to be great protectors of cattle, horses, poultry, deer, ostriches, alpacas, and llamas. These dogs were also used to guard royal fruit tree groves and property.\nLivestock guardian dogs were not introduced to America until the 1970s when the government outlawed using lethal chemicals on predators. The success of using livestock guardian dogs was quickly proven with farmers seeing as much as a 100% reduction in the loss of their livestock to predators.\nThe practice of using these dogs to guard livestock quickly spread through all of North America, South America, Africa, and Australia.\nToday there are plenty of breeders throughout the United States that offer plenty of puppies to choose from. These puppies are bred from the best bloodlines of highly trained dogs that have a reputation for being amazing livestock guardian dogs.\nAlthough the best breeds for livestock guardian dogs can be found from many different breeders, not all breeders have training or knowledge to raise up proper livestock guardian dogs. A reputable, good breeder will have paperwork, and the breeding dogs will be registered.\nHow Do The Dogs Know What To Do?\nRegistered livestock guardian dogs have centuries of breeding throughout their bloodlines so that their instincts are attuned to protecting livestock and being territorial against predators. Even when a farmer chooses a livestock guardian dog that is not from a breeder, the best breeds for livestock guardian dogs are highly intelligent and self-thinkers so they can be trained to guard livestock.\nFarmers need to be able to dedicate several weeks to months of training their livestock guardian dog on commands, territory, and their job. This training, which should start when the dog is a puppy, will help ingrain the instinct to join and protect the flock.\nThese dogs have a high work drive, are loyal, and their centuries of breeding have made them naturally aggressive towards predators such as coyotes, foxes, wolves, and wild cats. They are most effective against single, lone predators and can also alert to human intruders on a property as well.\nWhen larger predators are a risk such as mountain lions, bears or packs of wolves, the farm or ranch should consider having at least two livestock guardian dogs.\nPopular Guardian Dog Breeds\nCurrently, there are more than 40 breeds that are considered eligible to be livestock guardian dogs. Most countries in Europe and Asia have their own developed breed of guardian dogs with varying degrees of aggressiveness and size.\nA popular breed for both livestock guardian dogs and pets in the United States because they are less aggressive than other LGDs and are nurturing to young animals and children. Great Pyrenees are best in cooler climates because they have long, thick, fur, and need a lot of grooming.\nIn France, these dogs are very popular for livestock guarding but, in the US, most are bred for companionship so working with a professional trainer and breeder for LGDs may be necessary for ranchers that want a Great Pyrenees.\nImage Credit: Steve Slater\nAnatolian Shepherds originated in Turkey and can weigh between 90 and 150 pounds and stand 29 inches high on average. They are fast, agile, and have a very muscular build with broadheads and thick necks.\nTheir fur is short and rough and helps the shepherds do well in warmer or tropical environments. Anatolian Shepherds have excellent sight and hearing, more so than other LGDs, but are also known for roaming and needing a lot of socialization and training.\nEstrela Mountain Dog\nImage Credit: gailhampshire\nEstrela Mountain Dogs usually weighs around 100 pounds and gets along very well with kids but is wary of strangers and very protective of their herd. They have a particularly loud, threatening bark and need a lot of socialization to get along well with their human companions. They are great problem solvers and self-thinkers.\nMastiffs are one of the largest breeds of dogs and perfect for livestock guarding because of their intimidating size.\nThe Pyrenean Mastiff tends to be suspicious but not overly aggressive.\nSpanish Mastiffs are the largest, reaching over 200 pounds; however, they tend to be more aloof and passive unless a threat is present and active.\nTibetan mastiffs are one of the most well-known of the mastiff breeds and have been used in the mountains for eastern Asia for thousands of years. Tibetan mastiffs are also the breeds that originated the mastiff breed. Mastiffs have medium to long coats that require grooming and make them more suitable for cooler climates.\nKomondors are best known for their dreadlocked fur that protects the dogs from cold weather and wolf bites. This coat requires a lot of maintenance and upkeep unless owners want to clip it. Komondors average around 90 pounds full grown and are very territorial.\nThey require a lot of socialization and are better outdoor working dogs than indoor companions. Komondors are perfect for guarding livestock from larger predators, coyotes, and wolves in forested environments with cooler climates.\nImage Credit: Simone\nMaremma Sheepdogs are working dogs that need to have a constant dog to do. They do not make good pets but are perfect for large farming or ranch operations. The Maremmas are smaller livestock guardians, reaching around 70 to 80 pounds at maturity and have long coats.\nThey are aloof and don’t interact with humans much but will race around the farm, making sure all the livestock are safe and sound.\nPolish Tatras are great companion dogs who can also work on the farm guarding livestock out to pasture. Their gentle temperaments make them a great addition to properties that have a lot of human visitors. Tatras have heavy coats that require frequent grooming and can reach over 100 pounds well full grown.\nThese dogs are not attack dogs, however, and will only become aggressive if predators get close; otherwise, they alert bark and move between the herd and any predators to create a shield.\nBulgarian Shepherd Dogs\nImage Credit: Nicholay Atanassov\nBulgarian Shepherds were bred to protect livestock from bears and wolves in Bulgaria. They are smaller in stature and can have long or short fur, so ranches or farms in any climate can be the perfect home for a Bulgarian Shepherd Dog.\nThese dogs are known for having an even temperament and being docile to their human companions but do need a job to do and prefer to spend their time outside with the livestock they are meant to protect.\nWell, That’s A Wrap!\nLivestock guardian dogs have great effectiveness at keeping predators of livestock at bay and even alerting to human intruders. These dogs are also known to protect family members and other people on the farm, such as workers or buyers. There is no question of whether not a livestock farmer needs one of these dogs.","The Newfoundland dog is nothing short of massive in every way.\nThese dogs may now be considered somewhat iconic due to the runaway success of the “Peter Pan” story, but once upon a time they were simply humble working dogs that assisted people with hunting, guarding, and – most famously – water rescues.\nIt is hard to believe it just by looking at them, but the gigantic and hairy Newfoundland dog is actually a great swimmer!\nBut it is all that hair that we are going to talk about in this article. Specifically, we will look at whether Newfoundland dogs shed and, if so, how much do they shed and how can you prepare for that.\n- 1 Do Newfoundland Dogs Shed?\n- 2 Learn About Newfoundland Coat Blows and Shedding\n- 3 Meet the Newfoundland Dog Breed\n- 4 Understanding the Layers of the Newfoundland’s Working Dog Coat\n- 5 Why You Should Never Ever Shave a Newfoundland Dog’s Coat\n- 6 Why Do Newfoundland Dogs Shed More Seasonally?\n- 7 What About Shedding the Outer Coat Layer?\n- 8 Do Newfoundland Puppies Shed and Blow Coat?\n- 9 How Often Do You Have to Brush and Groom Your Newfoundland Dog?\nDo Newfoundland Dogs Shed?\nDo Newfoundland dogs shed? Boy, do they! Ask any Newfoundland dog owner about their enormous shedding canine and how they manage it and you are likely to be listening to tips for quite some time!\nNewfoundland dogs are not just big in terms of how much they weigh and how tall they stand. These dogs have the thick, double-layer, seasonally shedding coats that nearly all working group dogs have. This can add up to a lot of shedding year-round and seasonally for your Newfoundland dog and you.\nLearn About Newfoundland Coat Blows and Shedding\nAs this short owner-made YouTube video explains, approximately twice per year, the Newfoundland dog goes through a type of seasonal shedding called “blowing coat.”\nThis is exactly what it sounds like. The coat sheds out profusely and for a period of time, it can feel like you are literally surrounded in a snowstorm of sorts made of fur.\nLuckily, there is a lot you can do to keep even these extreme seasonal coat sheds manageable for you and your family, not to mention your clothing and home furnishings.\nMeet the Newfoundland Dog Breed\nSo let’s spend a moment getting acquainted with the Newfoundland dog breed and their thick and luxurious working-dog coat.\nAccording to the American Kennel Club (AKC) breed profile, the Newfoundland dog can easily weigh 100 to 150+ pounds and stand 28 inches tall (paw pads to shoulders).\nAnd every inch of the Newfie, as breeders and owners like to call this dog breed, is covered in dog hair!\nIn other words, the Newfoundland has the double-layer coat that is characteristic of nearly every purebred dog breed that has ever worked alongside people in any job.\nAccording to the Antarctica Newfoundland Club, the Newfoundland dog excels at several different jobs, including hauling, carting, hunting, guarding, and protection and conducting water rescues, often at long distances from land.\nSpecifically, Newfoundlands have made a name for themselves over the centuries for their ability to rescue people set adrift at sea when their vessels sank. Clearly, the coat of this dog must be specialized to do such icy cold and dangerous work!\nUnderstanding the Layers of the Newfoundland’s Working Dog Coat\nMost working dog breeds – that is, dogs bred specifically to do jobs alongside people – tend to be bred with an extra-protective coat that non-working dogs often lack.\nAccording to the Newfoundland Club of America (NCA) breed standard, the Newfoundland is a dog that should be “heavily coated.”\nThe breed standard also notes that this working dog coat type has two layers. Each layer is specialized to keep that dog protected, warm, and safe while out working.\nOuter coat layer\nThe outer layer of a working dog’s coat may be long, short or medium in length. But generally speaking, it will always be somewhat water-repellant, coarse, and thick.\nThis is because the outer coat layer is designed to help rain, ice, or snow (or seawater in the Newfoundland dog’s case) sheet right off the dog’s body instead of soaking through to the skin.\nThe coarseness of the outer layer can also serve as a buffer against pests, abrasion, and injury while the dog is working.\nAnd the coat can help protect the dog’s skin from getting sunburned or windburned.\nInner coat layer\nIn direct contrast to what you just read, the inner layer of a working dog’s coat generally has a completely different quality. It is thick and soft and downy.\nIf this description makes you think of a fluffy down comforter, you’ve already got the right idea. This inner layer is the dog’s insulation against cold from any number of sources.\nWhy You Should Never Ever Shave a Newfoundland Dog’s Coat\nIf this is your first time coping with the extreme shedding that Newfoundland dogs can go through seasonally, you may have had more than a passing fantasy about simply shaving your dog and being done with it.\nBut this is something you should NEVER do for several reasons.\nYour dog will lose the coat’s protection\nAll the protective properties you just read about – protection from sun, wind, rain, ice, snow, abrasion, pests, and injury – go away the moment you shave your Newfoundland dog’s thick, double-layer coat.\nIt can be a dangerous thing to be a dog without a coat. The impact would be similar to if you decided to go out and about in the world wearing just your birthday suit.\nSuddenly danger would lurk around every corner. Every place you sat down or walked around, everyone you met and everything you did could cause you harm.\nJust like your body needs clothing to protect your skin from dangerous, your dog needs their coat to stay safe and healthy.\nThe coat will never grow back properly\nAt this point, you might be thinking that even if you shaved your Newfoundland during the hottest months or during the seasonal sheds, the coat would just grow back, right?\nUnfortunately, the fact that the coat will grow back is not enough. Once the coat is shaved, it will never again grow back in two separate and distinct layers like it is now.\nYour Newfoundland’s body will not understand that it needs to grow the downy underlayer and keep that separate from the coarse and water-repellant outer layer. The two layers won’t grow in stages.\nRather, the two layers of the coat will grow in together, with the two different hair types mixing and intermingling. This means the coat will have neither it’s insulating nor its protective properties intact.\nFor you, this will mean that for the rest of your dog’s life you will have to take extra precautions to protect your Newfoundland from getting sunburn, windburn, heat exhaustion, injury and abrasion, and cold.\nIt is far better to learn how to deal with the seasonal shedding than to have to deal with all of these dangers every day for the rest of your dog’s time with you.\nWhy Do Newfoundland Dogs Shed More Seasonally?\nTo understand why Newfoundland dogs go through a seasonal shedding cycle, it helps to remember that one function of all that fur is to keep a dog that works in cold climates warm while doing outside work.\nAs this dog’s breed name implies, Newfoundland dogs may have gotten their modern breed start in Newfoundland, Canada – an area that gets very long, icy, and snowy winters.\nAs PetMD points out, not only have Newfoundland working dogs historically rescued people from deep-sea boating accidents, but they have actually towed boats in the ocean back to shore.\nThese dogs are so strong, they have often been tasked with the job of jumping into the ocean, gripping ship leads (long ropes), and actually pulling boats in to shore when the water got too choppy for accurate navigation.\nKnowing this makes it very easy to understand why a dog might need a very thick winter coat to do this type of work during seasonal storms!\nBut what about when summer arrives? Canada’s summer season is not long but it can get quite hot. All that thick insulating fur might cause a Newfoundland dog to overheat while working during the summer months.\nAnd this is why Newfoundlands do a seasonal shed.\nCold to warm-season shed\nThe biggest coat blow, or seasonal shed, the period is usually during the transition from the cold season to the warm season.\nThis is because your dog has to drop quite a bit of the thick, insulating undercoat rather quickly to keep from overheating as the temperature rises.\nYour dog will shed out a great deal of the undercoat over a short time period, often in big chunks of hair at one time, which is where the nickname “coat blow” comes from.\nAs My Brown Newfies owner blog points out, the shedding isn’t actually triggered by the temperature increasing outside. It is triggered by an increase in daylight hours.\nSo if you live in an area where you have a long winter season, your dog might start the coat blow well before you feel the warmer temperatures starting to creep in.\nThis is normal because the seasonal sheds are connected with your dog’s internal circadian rhythms.\nWarm to cold season shed\nA slightly less impactful coat blow, or seasonal shed, often happens during the transition from the warm season back to the cold season again.\nAt this time, the coat will be replenishing its most important insulating properties by shedding out the well-used summer undercoat and growing in a fresh and thick undercoat.\nWhat About Shedding the Outer Coat Layer?\nThe Newfoundland dog sheds every single day and all day long. This dog can shed while sleeping and often sheds lightly when patted.\nThe outer guard hairs, with their water-resistant and protective properties, are continually being replenished and renewed.\nFor this reason, there needs to be some amount of consistent shedding going on. If a hair gets damaged, it needs to be replaced without delay.\nDo Newfoundland Puppies Shed and Blow Coat?\nAs the South Central Newfoundland Dog Club explains, Newfoundland puppies will typically only go through one single coat blow period.\nThis happens when the puppy coat starts shedding out and the adult coat grows in.\nBecause Newfoundland dogs are giant breed dogs, they can take a lot longer to grow up fully than do smaller breed dogs. This means the coat transition may not happen until your dog is two or even three years old.\nThe Newfie puppy coat is a single layer coat that is softer than the adult coat. The coat will blow out in patches and can get kind of messy for a time until all the puppy hair has shed out and all the adult hair has grown in.\nThis is also the right time to get your Newfoundland puppy used to be brushed and groomed and trimmed and checked. It will be a lot harder to brush and groom your massive adult dog, so you want to have a dog that is happy to allow you to do this.\nHow Often Do You Have to Brush and Groom Your Newfoundland Dog?\nOne big question many aspiring Newfoundland dog owners have is about the frequency of brushing and grooming their dog.\nThe frequency is up to you. However, the more you brush your dog now, the less shed hair you will have to sweep up or vacuum up later.\nEither way, the hair is coming out, so it is really just a question of whether you want the shed, dead hair to come out in the dog brush, or onto the couch cushions or your clothing.\nMany experienced Newfoundland owners make time for at least a daily brushing, often at a particular time of day so the dog gets used to the routine and comes to look forward to this time with you.\nBy understanding in advance what you are choosing by picking a giant Newfoundland dog with a thick, double-layer, shedding coat type, you can prepare yourself with the right brushing and grooming supplies to care for your dog."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:1c7b99a7-a1f5-42c0-828c-9d418276ed64>","<urn:uuid:eb18a671-4576-453e-9d64-32cc6abebb41>"],"error":null}
{"question":"As astronomy student - can you tell core composition and size differences between small vs large stars?","answer":"Low to intermediate mass stars (0.075 to 5 solar masses) have different core compositions throughout their evolution. They start with hydrogen fusion in their cores, then develop helium cores, and eventually form degenerate carbon/oxygen cores. Stars below 0.25 solar masses become white dwarfs without progressing to advanced burning stages. In contrast, more massive stars (particularly supergiants with dozens of solar masses) consume hydrogen much more rapidly and can undergo additional fusion cycles. Low-mass stars experience a helium flash when igniting helium due to their degenerate cores, while more massive stars transition more gently into helium burning. Size-wise, red dwarfs (the smallest stars) range from 0.075 to 0.5 solar masses, while supergiants can reach dozens of solar masses and are the largest stars in the universe.","context":["The life-cycle of stars\nStars are born in massive clouds of gas and dust. Star forming regions are triggered by disturbances to these clouds (say by a nearby supernova) which initiate gravitational collapse. The chemical composition, mass and angular momentum of the gas and dust in the collapsing region are key ingredients that determine the evolution of the star that eventually forms.\nAs more and more material is drawn into the centre of the collapsing region a proto-star develops. Proto-stars above a critical mass (0.075 Mʘ, where Mʘ is the mass of the Sun) eventually become hot enough to initiate hydrogen fusion in their cores: the key requirement for it to attain the status of a true star.\nFigure 1 is an example of a Hertzsprung-Russell (HR) diagram for low to intermediate (0.075 Mʘ - 5ʘ) mass stars; one of the most important tools in stellar astronomy. The two scientists, whom the diagram is named after, discovered that by plotting the brightness of stars against their surface temperature an evolutionary sequence could be determined. The moment the star is powered by core hydrogen burning the star is said to have joined the main sequence. It is the first of several key phases of the stellar life-cycle. The main sequence is a quiescent stage of evolution, marked by a gradual increase in surface temperature and luminosity. It is the longest phase of evolution and as the name suggests where a majority of stars are found in the HR-diagram.\nOnce the energy supply in the core is exhausted and all the central hydrogen converted into inert 4He, stars enter a new phase of evolution. Stars with mass above approximately 0.25 Mʘ 'turn off' the main sequence and develop into subgiants. Stars below this mass fail to progress to more advanced burning stages and eventually become white dwarfs. During the subgiant phase, the star's structure begins to change significantly - hydrogen burning shifts to a shell around the core, the envelope swells up and the surface temperature begins to drop. It becomes a true giant once the shell burning impacts upon the surface luminosity. As the luminosity steadily increases, the star ascends the red giant branch (RGB). During this phase the hydrogen-burning shell adds processed (helium-rich) material to the core.\nEventually the central temperatures reach upwards of 100 MK and the conditions allow for the burning of helium in the core via the triple-alpha process. How helium ignition occurs will depend on the mass of the star. Low-mass stars (M > 2.25 Mʘ) are characterised by a degenerate helium core. This state of matter is governed by the principles of quantum mechanics and ignition under these conditions occurs via a violent flash. In more massive stars (with non-degenerate cores), a more gentle transition into central helium burning occurs.\nThe core-helium burning phase of evolution (labelled as the horizontal branch in Fig 1) is another quiescent epoch in the stellar life-cycle. This phase is also known for more metal-rich stars as the red clump (RC) as they tend to clump together in the same part of the HR-diagram. This phase of evolution lasts about one tenth that of the main-sequence lifetime, during which time low-mass stars display only a small increase in their surface temperature before the central supply of helium is exhausted. As was the case with the main sequence analogue, the exhaustion of fuel in the core facilitates the development of shell burning as the primary source of energy generation.\nThe new structure sees a secondary shell, burning 4He and fuelled by the ashes of the hydrogen burning shell, surrounding a degenerate C/O core. This double shell configuration is known as the asymptotic giant branch (AGB) phase of evolution. The AGB is called so because the early colour-magnitude diagrams revealed that these stars evolved asymptotically to the location of first ascent giant branch stars (i.e., RGB) and occupy the same colour-magnitude space. The AGB accounts for only a fraction of the stellar lifetime but is responsible for the complex nucleosynthesis that are a key source of enrichment for the Galaxy and interstellar medium. These stars are characterised by thermal instability and mixing events that dredge material from the deep interior to the surface. Through efficient mass loss, the material is expelled into the interstellar medium stripping the star's envelope bare. A brief planetary nebula phase (10,000 years) may precede the star's journey to the white dwarf cooling track and eventual thermodynamic death.","1 CORITHIANS 15:41\n41[There is] one glory of the sun, and another glory of the moon, and another glory of the stars: for [one] star differeth from [another] star in glory.\"\nTypes of Stars\nA star is a star, right? Well, not exactly. There are many different types of stars, from the tiny brown dwarfs to the red and blue supergiants. There are even more bizarre kinds of stars, like neutron stars and Wolf-Rayet stars. Let’s take a look at all the different types of stars there are.Protostar\nA protostar is what you have before a star forms. A protostar is a collection of gas that has collapsed down from a giant molecular cloud. The protostar phase of stellar evolution lasts about 100,000 years. Over time, gravity and pressure increase, forcing the protostar to collapse down. All of the energy release by the protostar comes only from the heating caused by the gravitational energy – nuclear fusion reactions haven’t started yet.T Tauri Star\nA T Tauri star is stage in a star’s formation and evolution right before it becomes a main sequence star. This phase occurs at the end of the protostar phase, when the gravitational pressure holding the star together is the source of all its energy. T Tauri stars don’t have enough pressure and temperature at their cores to generate nuclear fusion, but they do resemble main sequence stars; they’re about the same temperature but brighter because they’re a larger. T Tauri stars can have large areas of sunspot coverage, and have intense X-ray flares and extremely powerful stellar winds. Stars will remain in the T Tauri stage for about 100 million years.Main Sequence Star\nThe majority of all stars in our galaxy, and even the Universe, are main sequence stars. Our Sun is a main sequence star, and so are our nearest neighbors, Sirius and Alpha Centauri A. Main sequence stars can vary in size, mass and brightness, but they’re all doing the same thing: converting hydrogen into helium in their cores, releasing a tremendous amount of energy.\nA star in the main sequence is in a state of hydrostatic equilibrium. Gravity is pulling the star inward, and the light pressure from all the fusion reactions in the star are pushing outward. The inward and outward forces balance one another out, and the star maintains a spherical shape. Stars in the main sequence will have a size that depends on their mass, which defines the amount of gravity pulling them inward.\nWhen a star has consumed its stock of hydrogen in its core, fusion stops and the star no longer generates an outward pressure to counteract the inward pressure pulling it together. A shell of hydrogen around the core ignites continuing the life of the star, but causes it to increase in size dramatically. The aging star has become a red giant star, and can be 100 times larger than it was in its main sequence phase. When this hydrogen fuel is used up, further shells of helium and even heavier elements can be consumed in fusion reactions. The red giant phase of a star’s life will only last a few hundred million years before it runs out of fuel completely and becomes a white dwarf.\nWhite Dwarf Star\nWhen a star has completely run out of hydrogen fuel in its core and it lacks the mass to force higher elements into fusion reaction, it becomes a white dwarf star. The outward light pressure from the fusion reaction stops and the star collapses inward under its own gravity. A white dwarf shines because it was a hot star once, but there’s no fusion reactions happening any more. A white dwarf will just cool down until it because the background temperature of the Universe. This process will take hundreds of billions of years, so no white dwarfs have actually cooled down that far yet.\nRed Dwarf Star\nRed dwarf stars are the most common kind of stars in the Universe. These are main sequence stars but they have such low mass that they’re much cooler than stars like our Sun. They have another advantage. Red dwarf stars are able to keep the hydrogen fuel mixing into their core, and so they can conserve their fuel for much longer than other stars. Astronomers estimate that some red dwarf stars will burn for up to 10 trillion years. The smallest red dwarfs are 0.075 times the mass of the Sun, and they can have a mass of up to half of the Sun.\nIf a star has between 1.35 and 2.1 times the mass of the Sun, it doesn’t form a white dwarf when it dies. Instead, the star dies in a catastrophic supernova explosion, and the remaining core becomes a neutron star. As its name implies, a neutron star is an exotic type of star that is composed entirely of neutrons. This is because the intense gravity of the neutron star crushes protons and electrons together to form neutrons. If stars are even more massive, they will become black holes instead of neutron stars after the supernova goes off.\nThe largest stars in the Universe are supergiant stars. These are monsters with dozens of times the mass of the Sun. Unlike a relatively stable star like the Sun, supergiants are consuming hydrogen fuel at an enormous rate and will consume all the fuel in their cores within just a few million years. Supergiant stars live fast and die young, detonating as supernovae; completely disintegrating themselves in the process.We have written many articles about stars on Universe Today. Here’s an article that talks about how massive stars form.\nWant more information on stars? Here’s Hubblesite’s News Releases about Stars, and more information from NASA’s imagine the Universe.\nWe have recorded several episodes of Astronomy Cast about stars. Here are two that you might find helpful:"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:468c487a-34b6-439d-8af6-b656cff1629f>","<urn:uuid:7b8758e4-ad13-427a-85a4-0dc3fa51daf1>"],"error":null}
{"question":"¿Qué iniciativas están being implemented para ayudar a los coffee farmers en Vietnam, both in terms of sustainable production y adaptación al climate change? 🌱","answer":"Multiple initiatives are supporting Vietnamese coffee farmers. The Chappi Mountains Arabica Cooperative works with around 50 smallholder farmer families across 50 hectares, paying premium prices for coffee cherries and collaborating with Dutch NGOs (SNV and IDH) to provide coffee seedlings, organic fertilizer, and education on sustainable farming. Additionally, the Coffee Climate Care (C3) project helps producers identify climate change risks and implement adaptation measures. Through C3, farmers learn sustainable agricultural practices like planting cover crops, implementing shade trees, and improving fertilization management. These initiatives have led to a mindset shift among farmers, who now recognize the necessity of sustainable practices for long-term success.","context":["Marc Zihlmann is building up a sustainable coffee production “from farm to cup” in line with international quality standards, creating jobs and improving the livelihood of local farmers in one of the poorest areas of Vietnam.\nby Thảo Nguyên, Minh Tuấn\nFour years ago, Marc decided to quit his corporate job at Credit Suisse in Zurich to explore his passion elsewhere. In early 2018, he unexpectedly found both the inspiring product and the love of his life while travelling around Vietnam.\nHis future wife NGUYỄN Lê Thạch Thảo had already started her own coffee company back in 2015. After visiting her factory in the Central Highlands and meeting the poor local ethic farmers growing Arabica coffee as their main income source, Marc took a life-changing decision and moved to Vietnam.\nVietnam is the second biggest producer and exporter of coffee after Brazil, with a global market share of 17% in 2020. However, until a few years ago, the focus in Vietnam was more on output quantity than on quality and around 95% of the coffee produced in Vietnam is Robusta coffee, which is of lower quality and mostly used in instant coffee products.\nChappi Mountains Coffee is one of the pioneers in Vietnam in the niche of high-quality and specialty coffee products based on international standards. The main focus of the company is on the sustainable production of Arabica coffee “from farm to cup”.\nThe couple has initiated the Chappi Mountains Arabica Cooperative to work closely with local coffee farmers, control the quality “from farm to cup” and help the farmers improve their livelihoods through the payment of a substantial premium for their coffee cherries compared to the standard market price.\nAt the moment, the company is collaborating with around 50 small holder farmer families who are growing coffee on 50 hectares.\nTogether with two Dutch NGOs (SNV and IDH), whose activities are financed by the German Federal Ministry of the Environment, Nature Conservation and Nuclear Safety, the company is supporting the farmers with coffee and intercropping seedlings, organic fertilizer and educating them on sustainable farming and eco-tourism related activities.\nBesides the production of Arabica and Robusta coffee, the company has developed a range of flavored specialty coffee products with chocolate and herbal ingredients like ginseng and ganoderma, which are also sourced from local farmers.\nThe couple has decided to focus on roasted coffee products rather than on green coffee in order to generate as much added value as possible in Vietnam by creating many new and stable jobs in one of the poorest areas in the Central Highlands.\nAsked about his daily life as a farmer in a remote mountainous village, Marc told SVBG writers via video call with a chuck: “I don’t have anything to complain about. Vietnam treats me well. Life here is like that of my father’s family in the old times when Switzerland was less urbanized. My work now starts early in the morning at the farm or in the coffee factory.\nOver the last few years, I have learned a lot about coffee farming and processing. It is a very complex topic. The experience of my wife’s family has helped me a lot to broaden my know-how.\nThe more I’ve learned, the stronger passion for coffee has built up.”\nSince December 2020, Chappi Mountains Arabica coffee is Vietnam’s first fully UTZ certified coffee “from farm to cup”. All the other roasted coffee products are likewise certified according to UTZ/Rainforest Alliance standards.\nIn 2019, the Vietnamese government has started to recognize Marc and Thảo’s special efforts to develop a sustainable and inclusive business model. They are now promoting the Chappi project as a role model for the future development of the agriculture in the Central Highlands.\nThe Chappi Mountains Arabica Coffee has been selected by the Vietnamese Government to become a member of OCOP (One Commune One Product), a program which is supporting sustainable and socially responsible business practices.\nMarc and Thảo are also organizing farm trips for coffee enthusiasts who want to learn more about coffee cultivation, harvesting and processing techniques. They are operating a coffee shop/restaurant and a homestay retreat with 10 wooden bungalows in the middle of the coffee plantation area.\nCurrently the company is supplying their high-quality products in Vietnam to many coffee shops and high-end hotel customers like Sheraton, IBIS, Novotel, The Anam, Banyan Tree etc., as well as to a growing community of coffee lovers and enthusiasts.\nMarc is now very determined to bring his Chappi Mountains Coffee brand to his homeland Switzerland and to Europe. The products’ unique taste, international production standards as well as the sustainable and community-based approach surely cater to the most demanding clientele in the region. SVBG\nCompany Contact Info:\nMarc Zihlmann, Owner and International Business Representative","To combat the rising temperatures and extreme weather patterns producers in Vietnam are facing due to climate change, UTZ has been running a 3 year project to help producers recognize and identify the risks they face and introduce measures that will enable them to adapt.\nThe project, Coffee Climate Care – C3, helps the producers recognize their vulnerabilities to climate change and implement measures to cope with them. Henriette Walz, UTZ’ Climate Change & Environmental expert is at the moment in Vietnam visiting the farmers participating in the C3 project and assessing its status and effects.\nHenriette explains: “Most striking for me so far is a shift in mindset of the farmers concerning good agricultural practices. Through learning in-depth about climate change and at the same time feeling the impact it has on their farms, a lot of them now want to implement agricultural practices that might have been recommended before, but were not implemented because of a focus on short term profits. In a way, climate change makes sustainable practices a necessity.”\nMost striking for me so far is a shift in mindset of the farmers concerning good agricultural practices. In a way, climate change makes sustainable practices a necessity.\nIn the upcoming weeks Henriette will be sharing her experience in Vietnam. Today, she interviews Pham Van Hoan, a 62 year old farmer, father of two, owner of a 0.9 ha Arabica coffee farm in the region of Lam Dong.\nHW: How do you see climate change happening on your farm?\nI know about climate change from the training in the C3 program. The weather used to be foreseeable, but it is not anymore. For example, in the rainy season, there used to be sun in the morning and it rained in the afternoon. Now it is not so regular. This is a big problem for the growth of coffee plants and a big problem for coffee production in the whole area. Through C3 we now know where it comes from and what we can do to prepare the coffee plantations and try to minimize where we cause climate change ourselves.\nHW: What is the danger for your coffee crops?\nThat the flowers don’t develop into fruits but dry out. Also, before we did not need irrigation, now we do. Last, but not least, there are new pests. I have been growing Arabica coffee since 1999 and we never had any mosquito bugs affecting the plants. Now we do have them.\nHW: What is the most important thing you have learned in the C3 project that will help you to deal with the effects of climate change?\nI have learned which measures I can take to deal better with the challenges we face. Those are for example cover crops; I plant cover crops at the side of the plot and leave the grass on the field to protect against erosion. In the past I used to clear the plot, now I only cut the weeds before fertilization. This brings nutrition to the plants and lowers the temperature of the soil.\nI am also planting shade trees. They are really important for Arabica trees. They get stronger and develop less secondary branches. My neighbors have seen this and now want to do the same, so have opened a nursery for pepper seedlings to supply them.\nAlso, I have changed the fertilization management. I had to hire additional labor for this, as I now apply the fertilizer much more targeted in trenches and I balance the NKP content (note: NKP stands for nitrogen, potassium and phosphate, the three most common components of fertilizers) so that it is adequate for my coffee trees.\nHW: What has changed for you since your farm has been UTZ certified?\nWhen I started working with UTZ, I got training on Good Agricultural Practices. Since then I haven’t sprayed any pesticides, for 3 years no herbicides, only fungicides.\nHW: How do you see the future of coffee farming in this region?\nMaybe the area of coffee production will be smaller in the future, or the productivity lower. This year has already been very tough, very dry. When Catimor (note: Catimor is the type of Arabica coffee he plants) flowers, they need a lot of water. In addition, prices are very low at the moment (6000VND-30Euro Cents per kg fresh cherry), so we will see how this develops.\nThe biggest challenge of climate change in the Vietnamese Lam Dong region might be the lack of water in the dry season, while at the same time a higher need for irrigation due to higher temperatures. Only a combination of many actions will prepare farmers against this including some work on community and maybe governmental level. Nonetheless farmers are incredibly motivated to implement measure to make the farms more suitable against climate change impacts and decrease their own footprint after noticing the impacts and learning about it through the C3 trainings.\nIn the following days, Henriette will be moving north to visit the farms in the region of Dak Lak, don’t miss her stories.\n“How farmers adapt best to climate change depends on the region and the situation of the plantations: I am now traveling from our pilot group in the Lam Dong Area, where it is crucial for farmers to plant more shade trees, to the lower Dak Lak area where plantations are already shaded, as temperatures have already been higher in the past. Curious to see which measures are prioritized there!”\nThe Coffee Climate Care project is sponsored by the Deutsche Investitions- und Entwicklungsgesellschaft (DEG) with public funds of the German Ministry for Economic Cooperation and Development (BMZ).\nMotivation: Climate change is already affecting agriculture in many regions of the world. At the same time food production accounts for a large part of greenhouse gas emissions. By assisting consumers in supporting sustainable production methods, UTZ can reduce the impact of agriculture on the climate and increase the resilience of farmers to the effects of climate change."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:684d02fc-8810-406b-9372-479491812b8c>","<urn:uuid:c3cbb02a-2779-4833-9b68-73077fa55017>"],"error":null}
{"question":"How did Toulmin's educational background in mathematics and natural sciences compare to Bialik's traditional religious education in terms of their later intellectual development?","answer":"Toulmin received formal academic training with a Bachelor's degree in mathematics and natural sciences from King's College, followed by Masters and PhD degrees from Cambridge. In contrast, Bialik received a traditional Jewish religious education at the Zhitomir yeshiva and Volozhin Talmudic academy, but later broadened his intellectual pursuits by studying Russian and German languages and engaging with modern Jewish culture in Odessa. Both men's educational backgrounds influenced their later work, with Toulmin developing structured analytical models for arguments, while Bialik merged traditional Jewish learning with modern literary pursuits.","context":["Stephen Toulmin is one of the modern day leaders of rhetorical theory. He did not start out as a rhetorician though. He was born in London, England in 1922. He received a Bachelor of Arts degree in mathematics and natural sciences from King's College in 1942. Later he received his Masters of Arts and Doctor of Philosophy degrees from Cambridge. He has spent much of his life teaching at various universities around the United States. Toulmin has written several books. The most important of these works to the field of rhetoric is his book entitled The Uses of Argument. In this book he explains a structural model by which rhetorical arguments can be analyzed. We are going to study this model as a way of evaluating, analyzing, and critically reading written texts.\nHere are some of the web site's highlights . . .\nTo respond analytically to an argument is to do much more than state a basic agreement or disagreement with it; it is to determine the basis of our agreement or disagreement. In other words, analysis is a process of discovering how the argumentative strategies an author employs (the how and why levels of an argument) lead us to respond to the content (the what level) of that argument in the way that we do. Sometimes, such analysis can cause us to change our minds about our judgment about how effective or ineffective an argument is.\nThe Toulmin method, in short, is an effective way of getting to the how and why levels of the arguments we read. It is a type of textual \"dissection\" that allows us to break an argument into its different parts (such as claim, reasons, and evidence) so that we can make judgments about how well the different parts work together.\nThe Toulmin Method is a way of doing a very detailed analysis, in which we break an argument into its various parts and decide how effectively those parts participate in the overall whole. When we use this method, we identify the argument's claim, reasons, and evidence, and evaluate the effectiveness of each.\nThe Parts of an Argument according to the Toulmin Model:\ni) Claims - are the main point of the argument. Another name for the claim would be the thesis. The claim may be explicitly stated at the beginning of an argument, at the end, or somewhere in the middle, or it may not be stated anywhere. It may be implied, in which case you will be expected to infer it.Adapted from Wood, Nancy V. Perspectives on Argument. Third Edition. Upper Saddle River, New Jersey: Prentice Hall, 2001.\nii) Data (support) - provides the evidence, opinions, reasoning, examples, and factual information about a claim.\niii) Warrants - are assumptions, general principles, conventions of specific disciplines, widely held values, commonly accepted beliefs, and appeals to human motives. Most warrants are not stated in an argument.\niv) Backing - is audience specific and it bridges the gap between the author's warrant and the audience's opinion.\nv) Rebuttals - establish what is wrong, invalid, or unacceptable about an argument and they may present counter arguments or new arguments that represent different points of view.\nvi) Qualifiers - are words throughout the argument that quantify the argument. Some examples include: always, never, is, are, all, none, and absolutely, always and never change to sometimes, is and are change to may be or might, all changes to many or some, none changes to a few, and absolutely changes to probably or possibly.\nThe Toulmin model has six points: the claim, support, backing, warrant, qualifier, and rebuttal.Adapted from Wood, Nancy V. Perspectives on Argument. Third Edition. Upper Saddle River, New Jersey: Prentice Hall, 2001.\n- The claim is what the author is trying to say (the author's thesis/main idea).\n- The support is what the author uses to back up his/her claim (the author's evidence).\n- The backing is support for the warrant, which makes it more acceptable to the audience. These can be scientific and philosophical truths believed by the targeted audience.\n- The warrant is the implicit, unspoken understanding between the author and the audience. It is traditionally a shared belief that allows the author to relate directly to the audience's expectations.\n- The qualifier is a single word used to modify the argument allowing it to fit a broader range of situations. All and everything are much more difficult qualifiers to support than some and a few. If you say all people love ice cream, you are guaranteeing that the one audience member who is lactose intolerant is going to be the first to the microphone during the question and answer period. We are constantly trying to find the one exception to such expansive qualifiers.\n- The rebuttal is the where the author addresses the audience's opposing viewpoints or possible objections in order to strengthen his/her argument. The rebuttal can also be a place where the author concedes any weaknesses in his/her own argument or strengths in opposing arguments, but then carefully qualifies the effect these weakness have on the general strength of his/her original argument. You will use this less traditional rebuttal for Paper One's counterpoint.\nThis handout is arranged as a worksheet (with thanks to Tim Ray) so that you can download it from the following web site and use it whenever you want to conduct a Toulmin analysis of an argument . . .","July 31, 2014\nHayim [Chaim] Nahman Bialik\nHayim [Chaim] Nahman Bialik (Russia [Ukraine])\nBorn in Radi, Volhynia (then Russia, now Ukraine) to a traditional Jewish family, Yosef and Dinah, Hayim Bialik studied at a yeshiva in Zhitomir. His father died when he was seven years old, and throughout his life Bialik romanticized the difficulties of his childhood, noting the “seven orphans left behind”; contemporary biographers doubt the quantity of his siblings.\nIn Zhitomir, the young poet was raised by Orthodox grandfather, Yaakov Moshe Bialok. At 17 he was sent to the renowned Talmudic academy in Volozhin, Lithuania, where he focused on the Jewish Enlightenment Movement (Haskala). Joining the Hovevei Zion group, the young man gradually shifted away from yeshiva life, reflecting his ambivalent feelings about his “narrow” way of life in his early poem from 1898, “HaMatmid” (“The Talmud Student”).\nAt 18 he left for Odessa, the center of modern Jewish culture in Ukraine, becoming active in literary circles there. In Odessa he also met Abad Ha’am, who influenced Bialok’s Zionish outlook for the rest of his life, as well as Mendele Mocher Sforim. The young poem studied Russian and German languages, dreaming of enrolling in the Modern Orthodox Rabbinical Seminary in Berlin. During this period he also published his first poem, “El HaTzipor” (“To the Bird”), reflecting his growing Zionist feelings.\nIn 1892, hearing that the Volozhin yeshiva had closed, he quickly returned to Zhitomir in order to prevent his grandfather from discovering that he had discontinued his religious studies there. Upon his arrival he discovered both his grandfather and his older brother dying.\nAfter their deaths, Bialik married Mania Averbuch in 1893. For some time he worked as a bookkeeper in his father-in-law’s lumber business near Kiev. In 1897 he moved to Sosnowiec in southern Poland, working as a Hebrew teacher while earning extra income as a coal merchant. But the provincial life deeply depressed him, and he returned to Odessa, having secured a teaching job there.\nFor the next two decades Bialik taught and continued his activities in Zionist and literary groups. In 1901 he published his first collection of poems in Warsaw, which received some acclaim, including being hailed as “the poet of national renaissance.”\nHe moved to Warsaw for a brief period of time in 1904, where he became the literary editor of the weekly journal, HaShiloah, founded by his friend Abad Ha’am.\nIn 1903 he was sent by the Jewish Historical Commission back to Odessa to interview survivors of the Kishinev pogroms. As a result of his findings, Bialik wrote an epic poem In the City of Slaughter, an expression of the anguish felt by the Jews. So powerful was his attack against anti-Semitic violence that it is thought to have influenced Jewish self-defense groups in Russia and the Haganah in Palestine. In 1909 Bialik visited Palestine.\nDuring this early period he founded, with others, a Hebrew publishing house, Moriah, which focused on Hebrew classics and texts for school students. He also translated numerous European works, including Shakespeare, Schiller, Cervantes, Heine, and Ansky. Bialik also published 20 of his own Yiddish poems and collaboratively published Sefer HaAggadah (The Book of Legends), a three-volume publication the folk tales and proverbs embedded in the Talmud.\nIn 1921 Bialik moved to Berlin, founding the Dvir publishing house, which he moved to Tel Aviv in 1924, devoting himself to cultural activities and public affairs. While still in Germany he joined a community of Jewish authors that included Samuel Joseph Agnon, Simon Dubnow, Israel Isidor Elyashev, Uri Zvi Greenberg, Jakob Klatzkin, Moshe Kulbak, Jakob-Wolf Latzki-Bertoldi, Shaul Tchernichovsky, Martin Buber and numerous others. They met at the Hebrew Club (Beith haWa’ad ha’Ivri) or in Café Monopol, which had a Hebrew speaking corner.\nIn 1927 he became head of the Hebrew Writers Union which had been established six years previously. He retained this position until his death in Vienna in 1934 of prostate cancer.\nPoems, Warsaw: Tushia, 1901); Shirim (Cracow: Hovevei Hashira Haivrit: 1907); The Writings of H. N. Bialik (Berlin: Hovevei Hashira Haivrit, 1924); Poems and Songs (children’s book) (Tel Aviv: Dvir, 1933); The Writings of H.N. Bialik (four volumes) (Tel Aviv: Dvir, 1938); Collected Poems Critical Edition (Tel Aviv: Dvir, 1983 / 1990)\nENGLISH LANGUAGE TRANSLATIONS\nPoems from Hebrew (London: Hasefer, 1924) / as Selected Poems (New York: New Palestine, 1926) / Philadelphia: Jewish Publication Society, 1939) / as Complete Poetic Works (New York: Histadrut Ivrit of America, 1948) / (New York: Block, 1965) / as Selected Poems (New York: Union of American Hebrew Congregations, 1972) / as Selected Poems (Tel Aviv: Dvir and the Jerusalem Post, 1981) / (Columbus, Ohio: Alpha, 1987); The Short Friday (Tel Aviv: Hashaot, 1944); Knight of Onions and Knight of Garlic, Herbert Danby, trans. (New York: Jordan, 1939); Songs from Bialik: Selected Poems of Hayim Nahman Bialik (Syracuse, New York: Syracuse University Press, 2000); Selected Poems (New York: Overlook/Duckworth, 2004)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:62cc2ee4-eede-41cc-bc20-367e451452ef>","<urn:uuid:e6a312af-2ac6-4751-91fa-cc629d9a9a86>"],"error":null}
{"question":"What strategies help improve speech communication, and how can similar protective measures be applied to social media usage?","answer":"Speech communication can be improved by focusing on elements like facial expressions, gestures, eye contact, presentation speed, and conscious word selection, particularly choosing upbeat over negative words. Similarly, social media security can be enhanced through several protective measures, including using ad blockers on corporate devices, avoiding password sharing, being wary of urgent messages, rejecting unknown friend requests, avoiding public Wi-Fi usage, and regularly changing passwords. Organizations can also implement browser isolation technology and AI-powered email filtering to prevent social media-based attacks.","context":["On Upbeat and Downbeat Words\nArticle By: Brian Shute, Ph.D., CCC\nOriginally published in Kids: For Moms and Dads, May 2008.\nWhen kids and adults think of speech-language therapy, they often think of it as working on the sounds that form the words we say. Certainly this is true, but speech is a lot more than just the articulation of sounds that stream from our mouths. Speech communication is a two-way street, an intricate system that includes deciphering elements of meaning, understanding, word finding and vocabulary, sentence formulation, and speech processing. The reasons why we speak to each other are important too. Along these same lines, speech includes the way we present ourselves during the process. Things like facial expressions, gestures, eye contact, presentation speed, and word selection are crucial facets of our speech.\nIn speech-language classes, we sometimes talk about word choice and the importance of being upbeat in our words and conversations. It’s easy to select words that are negative over ones that are positive and most people don’t give it much consideration. Have you ever spoken to someone who left you feeling depressed because of their word choice? Words that are upbeat give listeners a feeling of contentment and ease. They have a calming effect and brighten conversations. Importantly, they offer some of the building blocks to leadership, making friends, and effective communication. On the other hand, words that are downbeat create negativity and dampen the conversational mood. They can be offensive and often restrict the message and distract from the communication process. They leave listeners with a bad feeling. Interestingly, both upbeat and downbeat words are conversationally contagious.\nA third category of words termed, “neutral,” are neither upbeat nor downbeat. They serve as nondescript nouns, verbs, and articles. Typically, neutral words carry no implications or connotations, and lack linguistic color and zest. Words such as Home, Walk, Plant, Of, And, the are examples of neutral words. Importantly, neutral words can be converted into upbeat ones with a little skill. Consider the following changes: Castle/home, Mosey/walk, Greenery/plant. There is not a lot that can be done with connector words like, The, And, Of, and Is. But that’s alright…articles or connector words have no content, and they make up only a small part of our language.\nMoms and dads can try this fun activity that increases awareness of upbeat, downbeat, and neutral words, sentences, and ideas in our conversations. After explaining the difference between the three word categories have your child identify them in examples that you give. Start with individual words like, Happy, Smelly, Boring, Street, Exciting, Ugly, Colorful, and Cruddy. Then try sentences like, “I really like that artwork” or “What stinks in here?” or “This tastes really good.” Ask your child if the word or sentence is upbeat positive, downbeat negative, or free wheeling neutral. Talk about how each of you determined which words or sentences were which. You can also make up fictitious words and determine the feelings they exude, like, Superlicious or Ginormous or Crudola. Make up your own sentences and have fun. Try experimenting with ways you can re-word an idea using all upbeat words and sentences. With a little practice, you’ll be surprised at how fast children tune into the difference. You will also be amazed at how easy it is to replace downbeat words with upbeat ones.\n© InlandSpeechPathology.com, 2003-2010.","Social Media Threat Definition\nSocial media offers an outlet for people to connect, share life experiences, pictures and video. But too much sharing—or a lack of attention to impostors—can lead to a compromise of business and personal accounts.\nAttackers often use social media accounts during the reconnaissance phase of a social engineering or phishing attack. Social media can give attackers a platform to impersonate trusted people and brands or the information they need carry out additional attacks, including social engineering and phishing.\nHow Social Media Threats Happen\nBusinesses can’t control what people do in their private lives. But unfortunately, attackers can take advantage of employees who post too much information on social media.\nThe methods used by an attacker depend on the social media platform targeted. Facebook allows users to keep their images and comments private, so an attacker will often friend a targeted user’s friends or directly send a friend request to a targeted user to access their posts. If an attacker can connect to several of the targeted user’s friends, then it’s more likely that the targeted user will accept the friend request based on the number of connected friends.\nLinkedIn is another common social media target. LinkedIn is known for business networking, and users’ networks are typically filled with colleagues and other employees within the same organisation. If an attacker targets a business, LinkedIn is an excellent social media site to collect business emails for a phishing attack. A large enterprise could have several networked employees who list their employer and their titles. An attacker can use this public information to find several employees who have access to financial information, private customer data or high-privilege network access.\nCollecting information to steal data isn’t the only reason to use social media for reconnaissance. The information posted on social media could be used to obtain passwords or impersonate business users. Many online accounts allow users to reset passwords if they enter a security question. With enough information from social media posts, an attacker could guess the answer to these security questions based on the private information posted by a targeted user.\nBrand impersonation is another social media threat. With enough gathered information, an attacker can impersonate a business brand to trick users into sending money, divulging private information or provide an attacker with account credentials. Attackers also use this threat to perform cross-site scripting (XSS) or cross-site request forgery (CSRF) attacks. These attacks can lead to more massive data breaches and business infrastructure compromise.\nExamples of Social Media Threats and What They Look Like\nBecause many social media platforms publicly display user posts, attackers can silently collect data without a user’s knowledge. Some attackers will take further steps into gaining access to user information by contacting targeted users or their friends.\nThe way a social media threat is carried out by an attacker depends on their goals.\nIf an attacker is looking for a high-stakes reward, the best way to quickly earn monetary rewards for their efforts is to target businesses. An attacker might first review LinkedIn for a list of possible targets. Targets can be a mix of high-level corporate employees and low-privilege users who could be tricked into sending additional corporate data or fall for a phishing attack that gives the attacker access to account credentials.\nWith a list of targets, an attacker could then review social media accounts for personal information. Personal information can help the attacker gain the target’s trust in a social engineering attack. It can also be used to guess answers to security questions for an account takeover or used to get closer to a user with higher privileges. The names of pets, favourite sports teams and education history are all potential password clues or answers to questions used to verify the user’s identity to reset a password.\nAfter the attacker collects all the data needed, the next step is to launch the attack. An attacker can use any of the following methods:\n- Social engineering. An attacker might call employees to trick them into sending private data, proving credentials or wiring the attacker money. In a complex attack, the attacker can pretend to be a high-level executive to trick the targeted user into transferring money to the attacker’s account.\n- Phishing. An attacker may use collected social media information to spoof the sender of an email message and trick users into clicking links or sending the attacker private data. A high-level employee’s spoofed email address could send a message instructing the recipient to send money, click a malicious link or reply with sensitive data.\n- Brand impersonation. Using brand employee names, the attacker can trick customers into thinking requests are from the legitimate brand. This type of brand fraud could be used to trick users into divulging personal information or account credentials.\n- Site compromise and data theft. With enough information from social media, an attacker could write malware explicitly targeting the business or perform an attack that would provide internal network access where the attacker can then exfiltrate data.\n- Spread malware. Like brand impersonation, an attacker could create domains and websites that claim to be the legitimate business and trick users into downloading malware or providing credentials.\n- Data breach. If an attacker gains access to account credentials, it could lead to a significant data breach targeting an organisation.\nBecause there are several social media platforms on the internet, an attacker can perform social engineering and phishing using a variety of threat methods. There is no “one size fits all” social media threat for an attacker. But basic reconnaissance and research using social media are the same. Any public information on private and business social media accounts could be used in further attacks.\nWays to Prevent Social Media Threats\nMost social media security threats stem from employees disclosing too much private and business information publicly. These accounts are personal, so businesses can’t stop users from having a social media presence. But they can educate users on the best ways to protect data and their credentials.\nEducation is key to stopping social media security threats. Individuals can educate themselves. But businesses must conduct training programs for every employee so that they can detect and prevent social engineering and phishing. The first step is educating users on the dangers of disclosing too much information online to the public. Even social media accounts set to private could be used in an attack should the attacker gain access to private feeds. Users should never post private corporate information on their social media accounts or information that could be used in an account takeover.\nSome organisations hand out mobile devices and allow users to install social media apps. These companies should provide an acceptable usage policy that determines what users can post using company devices. It’s also critical to protect these devices from malware to avoid company social media accounts from being hacked. Remote wiping software should be installed should an employee physically lose their device or it gets stolen.\nSome other educational points for employees include:\n- Use ad blockers on corporate devices. If ad blockers are not feasible, instruct employees to avoid clicking ads, especially on popups that instruct users to download software to view content.\n- Employees should not share passwords—even if it’s within the same department.\n- Attackers use fear and urgency in their engagements, and employees should recognise this tactic as suspicious. Any messages or social media posts that urge employees to act quickly should be ignored.\n- Don’t accept friend requests from unknown people even if the user has several friends in common.\n- Avoid using social media sites on public Wi-Fi hotspots. Public Wi-Fi is a common location for attackers to snoop on data using man-in-the-middle (MitM) attacks.\n- User account passwords should change regularly. But users should also be encouraged to change their own private social media account passwords.\nIT staff should have social media cybersecurity defences in place to help users avoid being victims of an attack. Email servers can use artificial intelligence applications to catch suspicious emails with malicious attachments and links.\nSuspicious messages can be quarantined and reviewed by administrators to determine if the organisation is the target of an attack. Browser isolation is also an option for organisations that let users browse the internet. This technology allows users to freely browse the internet, but confines personal web activity to a protected container that prevents downloads, uploads and form fills to keep threats out of the environment."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:472da2aa-84f5-44f0-bcfc-dcbb4c8eadd9>","<urn:uuid:a9a72fed-011a-491c-8fdd-818b9d95a65f>"],"error":null}
{"question":"What is the relationship between solar flares and coronal mass ejections, and how frequently do they occur?","answer":"While coronal mass ejections are often associated with solar flares, a direct causal relationship hasn't been established. Near solar maximum, the Sun produces about three CMEs every day, while near solar minimum, there is approximately one CME every five days. The most energetic solar flare ever detected was the Carrington Event of 1859, with an energy release of 1-5 x 10^32 ergs, which is tiny compared to flares observed in other stars.","context":["Some people, no matter what the occasion, are always the life of the party. They not only occupy the center of the action, but they instigate it as well. However, it is hard to sustain a lively party without others quietly serving in a consistently steady and dependable manner on the sidelines.\nRecently published research shows that our Sun “stands out” by being dull compared to our galaxy’s other stars. It displays hardly any brightness or flaring activity, making it the “Steady Eddie” among stars. Yet, thanks to the Sun’s low activity level, life on Earth can thrive and party on.\nMany readers will appreciate the scientific details that explain the Sun’s brightness stability, lack of superflares, and low rate of ordinary flares. Other readers can skim the next few sections and begin again at Why So Stable?\nKepler Space Telescope Observations\nMore than a decade ago, astronomers began a quest to determine whether the Sun was unique among all solar-type stars in manifesting a very low flaring activity level and a very low level of luminosity variations. The Kepler space telescope was launched in 2009 with the mission to detect planets by the transit method. Planets orbiting these stars were detected by the periodic dimming that resulted from planets crossing in front of the stars. Astronomers quickly realized, though, that Kepler data could be used to achieve two other missions: (1) study the brightening of stars caused by major stellar flares, and (2) analyze stellar brightening variability caused by starspots and the rotation of the stars. The brightness changes due to flares and starspots can be distinguished easily since flares vary over minutes while starspots vary over days to months.\nIn 2012, nine Japanese astronomers led by Hiroyuki Maehara used Kepler data to analyze the brightness changes of 83,000 G-dwarf type stars (the Sun is a G-dwarf star) over a period of 120 days.1 They detected 365 superflares with observed energies ranging from 3 x 1033 ergs to 3 x 1036 ergs.2 For comparison, the most energetic solar flare ever detected was the Carrington Event of 1859, a flare that ignited fires along telegraph systems in both Europe and the United States.3 Its total energy release according to one estimate was 1 x 1032 ergs4 and, according to a second estimate, 5 x 1032 ergs (tiny compared to flares observed in other stars).5\nMaehara’s team developed a subsample from their 83,000 stars. This subsample consisted of 14,000 stars that closely matched the Sun’s rotation rate and had effective temperatures in the range of 5,600–6,000 Kelvin. (Sun’s effective temperature = 5,772 Kelvin.) In this subsample they detected 14 flares with energies exceeding 4 x 1034 ergs. This number corresponds to flares at least 80 times more energetic than the Carrington Event occurring once every 350 years per star.\nIn follow-up studies published in 2013 and 2015, Maehara’s team—over a 500-day observing period—detected 1,547 superflares (energies greater than 1034 ergs) on 279 G-dwarf stars6 and 187 superflares on 23 stars that most closely matched the Sun’s characteristics.7 For the most Sun-like stars they calculated a superflare occurrence rate of once every 800–5,000 years per star.8\nThey noted that a flare’s energy release correlates tightly with the proportion of a star’s surface that is covered by the starspot responsible for the flare.9 They concluded that the exceptionally small sizes of sunspots during the past 9,000 years explains why the Sun has had no superflares during that time period.\nKepler Plus Gaia Spacecraft Observations\nIn 2019, a team of ten astronomers led by Yuta Notsu complemented their Kepler data with data from the Gaia spacecraft that was launched in 2013 and with observations performed on the Apache Point 3.5-meter telescope. They limited their analysis to Sun-like stars with rotation periods of about 25 days.10 (The Sun’s rotation period is 24.47 days.11) The superflares (energies greater than 5 x 1034 ergs) they observed on these stars implied that superflares occur on them once every 2,000–3,000 years per star.\nOn May 1, 2020, a team of seven astronomers published the most detailed study to date comparing the Sun with stars tightly matching the Sun’s characteristics. Led by Timo Reinhold of the Max Planck Institute, the seven astronomers combined the latest data from Gaia with Kepler data to produce a sample of 369 Kepler-observed stars with effective temperatures between 5,500 and 6,000 Kelvin\n(the Sun’s effective temperature = 5,772 Kelvin), ages between 4 and 5 billion years (the Sun’s age = 4.567 billion years12), surface gravity and metallicity values (abundance of elements heavier than helium) close to the Sun’s, and rotation periods between 20 and 30 days.13\nBy limiting their sample to stars with ages and surface gravity values similar to the Sun’s, Reinhold’s team eliminated evolved stars, which are inactive. This cut avoided the dilution of detected variability in solar-like stars that was the case for previous analyses.14 The 369 stars that remained comprised the largest sample, assembled to date, of stars tightly matching the Sun’s characteristics.\nReinhold’s team determined that the average brightness variability for the 369 stars was 0.36 percent. This variability compares with 0.07 percent for the Sun’s median variability. That is, the brightness variability of stars that most closely match the Sun’s effective temperature, rotation period, metallicity, surface gravity, and age is 5 times higher than the Sun’s over the 4-year period during which Reinhold’s team conducted their analysis.\nThe figure below shows (to scale) the Sun’s brightness variations compared to the solar-like star KIC7849521. The latter’s brightness variations well represent the average variability of the sample of 369 stars. Thus, compared to similar stars’ brightness variability, the Sun appears “dull.”\nFigure: Brightness Variations for the Sun (top) and KIC7849521 from March 2009 to April 2013\nAdapted from figure 2, Timo Reinhold et al., “The Sun Is Less Active Than Other Solar-Like Stars,” Science 368 (May 1, 2020): 519, doi:10.1126/science.aay3821.\nKepler Plus LAMOST Observations\nThe same day that Reinhold’s team published their findings from combined Kepler and Gaia data, a team of eleven astronomers led by Jinghua Zhang published their comparison of the Sun’s variability with that of 254 stars assembled from combined Kepler and LAMOST data.15 The Large Sky Area Multi-Object Spectroscopic Telescope (LAMOST) was used to measure the chromospheric activity of solar-like stars while Kepler data was used to determine the photospheric variability of solar-like stars. Zhang’s team concluded that the Sun is much less active (more steady and dull) than solar-like stars with near-solar rotation periods both in terms of photospheric variability and chromospheric activity.\nBoth Reinhold’s team and Zhang’s team also analyzed a large number of stars that approximately matched the Sun’s characteristics with the exception that their rotation periods could not be measured. They pointed out that if these stars had sunspots as few and as tiny as the Sun’s, then their rotation periods would be undetectable. Hence, there could be a population of solar-like stars that are as photosphericly stable as the Sun. The Sun, however, is both photosphericly and chromosphericly stable.\nWhy So Stable?\nAstronomers have produced two different explanations to explain why the Sun is so exceptionally stable. One explanation is that the solar dynamo is currently in transition to a lower activity level because of a change in the differential rotation deep down in the Sun’s convective layer.\nMany years ago, astronomers identified two distinct relationships between a star’s rotation rate and the length of its starspot cycle.16 Over the past 410 years the Sun has exhibited a sunspot cycle with an average periodicity of 11.06 years.17 The Sun is an outlier in that it is rotating too quickly for its 11-year sunspot cycle.18 This observation implies that the Sun’s rotation rate and magnetic field currently may be in a transition phase that is characteristic of the Sun now being exactly halfway through its nuclear-burning time period.19\nThe second explanation for the Sun’s exceptional stability is that the Sun could be alternating between epochs of high and low activity levels on time scales of 9,000 years or more. That is, besides the 11-year sunspot cycle there may be a much longer solar cycle in operation.\nIn either case, the past 9,000 years has been a period of truly remarkable solar luminosity stability. It has also been a period amazingly devoid of superflares. This combination and simultaneity of the remarkable and the amazing has made possible global high-technology civilization on Earth. Without extreme solar brightness stability, a lack of superflares, and a low rate of ordinary flares there is no way that billions of humans could live and thrive on Earth at one time. It seems more than a coincidence that we humans arrived on Earth at the optimal moment in the history of our star, the Sun, where that star possesses unique design features that make modern life and all its celebrations possible.\nFeatured image: Sunspots During the August 2017 Solar Eclipse\nImage credit: Hugh Ross","Coronal mass ejection\nCoronal mass ejections are often associated with other forms of solar activity, most notably solar flares, but a causal relationship has not been established. Most ejections originate from active regions on the Sun's surface, such as groupings of sunspots associated with frequent flares. Near solar maxima, the Sun produces about three CMEs every day, whereas near solar minima, there is about one CME every five days.\n- 1 Description\n- 2 Cause\n- 3 Impact on Earth\n- 4 Physical properties\n- 5 Association with other solar phenomena\n- 6 Theoretical models\n- 7 Interplanetary CMEs\n- 8 Related solar observation missions\n- 9 History\n- 10 See also\n- 11 References\n- 12 Further reading\n- 13 External links\nCoronal mass ejections release huge quantities of matter and electromagnetic radiation into space above the sun's surface, either near the corona (sometimes called a solar prominence), or farther into the planet system, or beyond (interplanetary CME). The ejected material is a plasma consisting primarily of electrons and protons, but may contain small quantities of heavier elements such as helium, oxygen, and even iron. The theory of heavier element emissions during a CME is speculative information and requires further verification. It is highly unlikely that a CME contains any substantial amount of heavier elements, especially considering that the sun has not yet arrived at the point of helium flash and thus cannot begin to fuse elements heavier than helium.\nRecent scientific research has shown that the phenomenon of magnetic reconnection is responsible for CME and solar flares. Magnetic reconnection is the name given to the rearrangement of magnetic field lines when two oppositely directed magnetic fields are brought together. This rearrangement is accompanied with a sudden release of energy stored in the original oppositely directed fields.\nOn the sun, magnetic reconnection may happen on solar arcades—a series of closely occurring loops of magnetic lines of force. These lines of force quickly reconnect into a low arcade of loops, leaving a helix of magnetic field unconnected to the rest of the arcade. The sudden release of energy in this reconnection causes the solar flare. The unconnected magnetic helical field and the material that it contains may violently expand outwards forming a CME. This also explains why CMEs and solar flares typically erupt from what are known as the active regions on the sun where magnetic fields are much stronger on average.\nImpact on Earth\nWhen the ejection is directed towards Earth and reaches it as an interplanetary CME (ICME), the shock wave of the traveling mass of solar energetic particles causes a geomagnetic storm that may disrupt Earth's magnetosphere, compressing it on the day side and extending the night-side magnetic tail. When the magnetosphere reconnects on the nightside, it releases power on the order of terawatt scale, which is directed back toward Earth's upper atmosphere.\nSolar energetic particles can cause particularly strong aurorae in large regions around Earth's magnetic poles. These are also known as the Northern Lights (aurora borealis) in the northern hemisphere, and the Southern Lights (aurora australis) in the southern hemisphere. Coronal mass ejections, along with solar flares of other origin, can disrupt radio transmissions and cause damage to satellites and electrical transmission line facilities, resulting in potentially massive and long-lasting power outages.\nHumans at high altitudes, as in airplanes or space stations, risk exposure to relatively intense cosmic rays. Cosmic rays are potentially lethal in high quantities. The energy absorbed by astronauts is not reduced by a typical spacecraft shield design and, if any protection is provided, it would result from changes in the microscopic inhomogeneity of the energy absorption events.\nA typical coronal mass ejection may have any or all of three distinctive features: a cavity of low electron density, a dense core (the prominence, which appears as a bright region on coronagraph images embedded in this cavity), and a bright leading edge.\nMost ejections originate from active regions on the Sun's surface, such as groupings of sunspots associated with frequent flares. These regions have closed magnetic field lines, in which the magnetic field strength is large enough to contain the plasma. These field lines must be broken or weakened for the ejection to escape from the sun. However, CMEs may also be initiated in quiet surface regions, although in many cases the quiet region was recently active. During solar minimum, CMEs form primarily in the coronal streamer belt near the solar magnetic equator. During solar maximum, they originate from active regions whose latitudinal distribution is more homogeneous.\nCoronal mass ejections reach velocities between 20km/s to 3200km/s with an average speed of 489km/s, based on SOHO/LASCO measurements between 1996 and 2003. The average mass is 1.6×1012kg. The values are only lower limits, because coronagraph measurements provide only two-dimensional data analysis. The frequency of ejections depends on the phase of the solar cycle: from about one every fifth day near the solar minimum to 3.5 per day near the solar maximum. These values are also lower limits because ejections propagating away from Earth (backside CMEs) can usually not be detected by coronagraphs.\nCurrent knowledge of coronal mass ejection kinematics indicates that the ejection starts with an initial pre-acceleration phase characterized by a slow rising motion, followed by a period of rapid acceleration away from the Sun until a near-constant velocity is reached. Some balloon CMEs, usually the slowest ones, lack this three-stage evolution, instead accelerating slowly and continuously throughout their flight. Even for CMEs with a well-defined acceleration stage, the pre-acceleration stage is often absent, or perhaps unobservable.\nAssociation with other solar phenomena\nCoronal mass ejections are often associated with other forms of solar activity, most notably:\n- Solar flares\n- Eruptive prominence and X-ray sigmoids\n- Coronal dimming (long-term brightness decrease on the solar surface)\n- EIT and Moreton waves\n- Coronal waves (bright fronts propagating from the location of the eruption)\n- Post-eruptive arcades\nThe association of a CME with some of those phenomena is common but not fully understood. For example, CMEs and flares are normally closely related, but there was confusion about this point caused by the events originating beyond the limb. For such events no flare could be detected. Most weak flares do not have associated CMEs; most powerful ones do. Some CMEs occur without any flare-like manifestation, but these are the weaker and slower ones. It is now thought that CMEs and associated flares are caused by a common event (the CME peak acceleration and the flare impulsive phase generally coincide). In general, all of these events (including the CME) are thought to be the result of a large-scale restructuring of the magnetic field; the presence or absence of a CME during one of these restructures would reflect the coronal environment of the process (i.e., can the eruption be confined by overlying magnetic structure, or will it simply break through and enter the solar wind).\nIt was first postulated that CMEs might be driven by the heat of an explosive flare. However, it soon became apparent that many CMEs were not associated with flares, and that even those that were often started before the flare. Because CMEs are initiated in the solar corona (which is dominated by magnetic energy), their energy source must be magnetic.\nBecause the energy of CMEs is so high, it is unlikely that their energy could be directly driven by emerging magnetic fields in the photosphere (although this is still a possibility). Therefore, most models of CMEs assume that the energy is stored up in the coronal magnetic field over a long period of time and then suddenly released by some instability or a loss of equilibrium in the field. There is still no consensus on which of these release mechanisms is correct, and observations are not currently able to constrain these models very well.\nCMEs typically reach Earth one to five days after leaving the Sun. During their propagation, CMEs interact with the solar wind and the interplanetary magnetic field (IMF). As a consequence, slow CMEs are accelerated toward the speed of the solar wind and fast CMEs are decelerated toward the speed of the solar wind. Fast CMEs (faster than about 500 km s−1) eventually drive a shock wave. This happens when the speed of the CME in the frame of reference moving with the solar wind is faster than the local fast magnetosonic speed. Such shocks have been observed directly by coronagraphs in the corona, and are related to type II radio bursts. They are thought to form sometimes as low as 2 Rs (solar radii). They are also closely linked with the acceleration of solar energetic particles.\nRelated solar observation missions\nNASA mission Wind\nOn 1 November 1994, NASA launched the WIND spacecraft as a solar wind monitor to orbit Earth's L1 Lagrange point as the interplanetary component of the Global Geospace Science (GGS) Program within the International Solar Terrestrial Physics (ISTP) program. The spacecraft is a spin axis-stabilized satellite that carries eight instruments measuring solar wind particles from thermal to >MeV energies, electromagnetic radiation from DC to 13 MHz radio waves, and gamma-rays. Though the WIND spacecraft is nearly two decades old, it still provides the highest time, angular, and energy resolution of any of the solar wind monitors. It continues to produce relevant research as its data has contributed to over 150 publications since 2008 alone.\nNASA mission STEREO\nOn 25 October 2006, NASA launched STEREO, two near-identical spacecraft which from widely separated points in their orbits are able to produce the first stereoscopic images of CMEs and other solar activity measurements. The spacecraft orbit the Sun at distances similar to that of Earth, with one slightly ahead of Earth and the other trailing. Their separation gradually increased so that after four years they were almost diametrically opposite each other in orbit.\nThe largest recorded geomagnetic perturbation, resulting presumably from a CME, coincided with the first-observed solar flare on 1 September 1859, and is now referred to as the Carrington Event, or the solar storm of 1859. The flare and the associated sunspots were visible to the naked eye (both as the flare itself appearing on a projection of the sun on a screen and as an aggregate brightening of the solar disc), and the flare was independently observed by English astronomers R. C. Carrington and R. Hodgson. The geomagnetic storm was observed with the recording magnetograph at Kew Gardens. The same instrument recorded a crochet, an instantaneous perturbation of Earth's ionosphere by ionizing soft X-rays. This could not easily be understood at the time because it predated the discovery of X-rays by Röntgen and the recognition of the ionosphere by Kennelly and Heaviside. The storm took down parts of the recently created US telegraph network, starting fires and shocking some telegraph operators.\nFirst clear detections\nThe first detection of a CME as such was made on 14 December 1971, by R. Tousey (1973) of the Naval Research Laboratory using the seventh Orbiting Solar Observatory (OSO-7). The discovery image (256 × 256 pixels) was collected on a Secondary Electron Conduction (SEC) vidicon tube, transferred to the instrument computer after being digitized to 7 bits. Then it was compressed using a simple run-length encoding scheme and sent down to the ground at 200 bit/s. A full, uncompressed image would take 44 minutes to send down to the ground. The telemetry was sent to ground support equipment (GSE) which built up the image onto Polaroid print. David Roberts, an electronics technician working for NRL who had been responsible for the testing of the SEC-vidicon camera, was in charge of day-to-day operations. He thought that his camera had failed because certain areas of the image were much brighter than normal. But on the next image the bright area had moved away from the Sun and he immediately recognized this as being unusual and took it to his supervisor, Dr. Guenter Brueckner, and then to the solar physics branch head, Dr. Tousey. Earlier observations of coronal transients or even phenomena observed visually during solar eclipses are now understood as essentially the same thing.\nOn 1 August 2010, during solar cycle 24, scientists at the Harvard-Smithsonian Center for Astrophysics (CfA) observed a series of four large CMEs emanating from the Earth-facing hemisphere of the Sun. The initial CME was generated by an eruption on 1 August that was associated with sunspot 1092, which was large enough to be seen without the aid of a solar telescope. The event produced significant aurorae on Earth three days later.\nOn 31 August 2012 a CME connected with Earth's magnetic environment, or magnetosphere, with a glancing blow causing aurora to appear on the night of 3 September. Geomagnetic storming reached the G2 (Kp=6) level.\n- List of Coronal Mass Ejections\n- Forbush decrease\n- Health threat from cosmic rays\n- Magnetic cloud\n- Orbiting Solar Observatory\n- Solar and Heliospheric Observatory\n- Space weather\n- Christian, Eric R. (5 March 2012). \"Coronal Mass Ejections\". NASA.gov. Retrieved 9 July 2013.\n- Nicky Fox. \"Coronal Mass Ejections\". Goddard Space Flight Center @ NASA. Retrieved 2011-04-06.\n- \"Coronal Mass Ejections: Scientists Unlock the Secrets of Exploding Plasma Clouds On the Sun\". Science Daily.\n-  NASA Science\n- \"The Mysterious Origins of Solar Flares\", Scientific American, April 2006\n- Baker, Daniel N., et al. (2008). Severe Space Weather Events – Understanding Societal and Economic Impacts: A Workshop Report. National Academies Press. p. 77. ISBN 978-0-309-12769-1. \"These assessments indicate that severe geomagnetic storms pose a risk for long-term outages to major portions of the North American grid. John Kappenman remarked that the analysis shows \"not only the potential for large-scale blackouts but, more troubling, ... the potential for permanent damage that could lead to extraordinarily long restoration times.\"\"\n- Wired world is increasingly vulnerable to coronal ejections from the Sun, Aviation Week & Space Technology, 14 January 2013 issue, pp. 49–50: \"But the most serious potential for damage rests with the transformers that maintain the proper voltage for efficient transmission of electricity through the grid.\"\n- Wilson, J. W.; Wood, J. S.; Shinn, J. L.; Cucinotta, F. A.; Nealy, J. E. A proposed performance index for galactic cosmic ray shielding, materials. Washington, DC: NASA; Report No. TM-4444; 1993a\n- Carroll, Bradley W.; Dale A. Ostlie (2007). An Introduction to Modern Astrophysics. San Francisco: Addison-Wesley. p. 390. ISBN 0-8053-0402-9.\n- Andrews, M. D., A search for CMEs associated with big flares, in Solar Physics, 218, pp. 261–279, 2003\n- Vourlidas, A., Wu, S.T., Wang, A. H., Subramanian, P., Howard, R. A. \"Direct Detection of a Coronal Mass Ejection-Associated Shock in Large Angle and Spectrometric Coronagraph Experiment White-Light Images\" in the \"Astrophysical Journal\", 598, 2, 1392–1402, 2003\n- Manchester, W. B., IV, T. I. Gombosi, D. L. De Zeeuw, I. V. Sokolov, ;, Roussev I., I., K. G. Powell, J. Kóta, G. Tóth, and T. H. Zurbuchen (2005). Coronal Mass Ejection Shock and Sheath Structures Relevant to Particle Acceleration. The Astrophysical Journal, Volume 622, Issue 2, pp. 1225–1239. 622 2: 1225–1239\n- Spacecraft go to film Sun in 3D BBC news, 2006-10-26\n-  NASA Mission Page, STEREO\n- AW & ST, Wired world . .\n- R.A. Howard, A Historical Perspective on Coronal Mass Ejections\n- Obit with brief bio for Dr. Brueckner\n- \"NASA's SDO Sees Massive Filament Erupt on Sun\". NASA. 4 September 2012. Retrieved 11 September 2012.\n- \"August 31, 2012 Magnificent CME\". NASA/Goddard Space Flight Center. 31 August 2012. Retrieved 11 September 2012.\n- \"Space Weather Alerts and Warnings Timeline: September 1–16, 2012 (archive)\". NOAA. Retrieved 24 September 2012.\n- Chillymanjaro (6 September 2012). \"Geomagnetic storming levels back to normal\". The Watchers. Retrieved 11 September 2012.\n- Gopalswamy, Natchimuthukonar; Mewaldt, Richard A; Torsti, Jarmo, eds. (2006). Solar Eruptions and Energetic Particles. Geophys. Monograph Series 165. Am. Geophys. Union. doi:10.1029/GM165. ISBN 0-87590-430-0.\n- Internet articles\n- Bell, Trudy E; Phillips, Tony (6 May 2008). \"A Super Solar Flare\". Science@NASA. NASA.gov.\n- Phillips, Tony (27 May 2008). \"Cartwheel Coronal Mass Ejection\". Science@NASA. NASA.gov.\n- Odenwald, Sten F; Green, James L (28 July 2008). \"Bracing the Satellite Infrastructure for a Solar Superstorm\". Scientific American.\n- Lavraud, Benoit; Masson, Arnaud (21 November 2007). \"Cluster captures the impact of CMEs\". ESA Science & Technology. ESA.int.\n- Morring Jr., Frank (14 January 2013). \"Major Solar Event Could Devastate Power Grid\". Aviation Week & Space Technology.\n|Wikimedia Commons has media related to Coronal mass ejection.|\n- NOAA/NWS Space Weather Prediction Center\n- Coronal Mass Ejection FAQ\n- STEREO and SOHO observed CME rate versus the Sunspot number (PNG plot) / (text version)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:d29c20f5-d9ee-4bb0-ae23-7083a72c80bb>","<urn:uuid:d5f7cd70-9e3a-439b-a3a7-e16bb9abdede>"],"error":null}
{"question":"What's the key structural difference between creating artificial fly wings vs dragonfly nymph development into adults?","answer":"The structural development differs significantly. Artificial fly wings are carefully constructed by selecting symmetrical sections from bird feathers, tearing them in half for a double-wing effect, and attaching them at the shoulder with precise silk binding techniques. In contrast, dragonfly nymphs transform into adults through a direct process where they crack open their backs and emerge as full adults with no intermediate pupal stage - this is a key characteristic that distinguishes nymphs from larvae, which require a pupal stage.","context":["A box of dry flies, wet flies and nymphs in a variety of colours and sizes is the pride and joy of a game angler. But there is even more satisfaction for the man who has tied them all himself\nTo create even a rough impression of a fly some knowledge of its anatomy is needed. The head (not simulated on all tied flies) is set on a body which is divided into two sec-tions – the upper thorax and larger abdomen. The wings vary in size but can be represented by different hackles made from pieces of feather. Because dry flies float on the surface, lightness and buoyancy in their construction are most important. The tying must be tight and even to prevent waterlogging. Where heavy or porous material would add to the attractiveness, these are given a last minute coating of a suitable oil to help keep the fly afloat.\nFor tying your own flies a small fly-tying vice is essential. Select a model that has a firm base, jaws capable of holding hooks from size 16 up to salmon sizes, and a vice which can be rotated for ample access to the fly. Still smaller hooks can be tied if you buy tiny ‘midge’ jaws which are gripped between the standard jaws. Hackle pliers are needed to hold the hackle firm and to prevent it unwinding when released. Two pairs of scissors with short, pointed blades are required: one for cutting quills, tinsel etc., and one for „ feather fibres and delicate materials. A bobbin holder is essential – one with a long spigot is advisable as it & enables turns of silk to be laid on accurately.\nA cake of cobbler’s wax will also be needed unless you are going to use prewaxed tying silk. Pulling silk quickly through a block of wax will coat it in such a way that it adheres as you apply it to the fly. A good selection of materials, in-cluding pieces of fur, tinsel, hair, feathers on the skin, wool and silk threads should be to hand as, after basic methods have been mastered, experimental and unusual varieties can be created. There are other useful (though non-essential) accessories, including tweezers and a whip-finish tool which completes the tying with a neat knot.\nThe angler is now ready to tie a basic fly. A trout hook (size 14-8) is held in the vice with the shank protruding horizontally and with the point clamped out of sight. An exposed point will catch and fray thread as you work.\nOur basic body shape is constructed from about 10 in of single-ply floss silk. (Floss is often sold with three or four strands plaited together.) Floss is a convenient body substance as it covers thoroughly and quickly.\nThe tying thread you will be using to tie in the different parts and materials of the fly should be attached just behind the eye. Make a few turns towards the bend, then tie in one end of the floss silk with two or three tight turns. Continue winding the tying thread down to the bend.\nThe tail comes next. Three or four fibres torn from a large cockerel or hen hackle are bunched together and tied in tightly at the bend.\nAt the same time as introducing the tail, tie in a length of tinsel, wire or lurex in silver or gold. This will be the rib whose glitter adds greatly to the attractiveness of an artificial. Trim off the waste ends of hackle fibres and wind the tying thread back to where you tied in the floss. The weight of the bobbin keeps the thread under tension, leaving both hands free to apply a coat of varnish to the shank. This fixes the first layer of floss silk.\nWind the floss in touching turns down to the bend and back up to the tying thread. Tie off the floss with three or four tight turns of thread and trim the surplus floss.\nNow you can wind the rib forward using a spiral opposite to that with which the floss was wound on. This way it won’t disappear among the turns of silk. Now tie off the rib as you did the floss and trim any waste.\nWhen you have completed the body, you have the choice of turning your fly into a dry fly with hackle and head or one with wings, hackle and head, a wet fly with wings, hackle and head or one with just hackle and head, or a nymph with thorax, hackle and head. Whichever you choose – it may be best to leave wings until you have become fairly dexterous – the head is added last of all.\nSelecting a wing I Strip the down fibre from the bottom of a suitable feather and separate out a wing section using a dubbing needle. 2 Grasp fibres at tips – pull down and outwards, if they separate, grip the extreme tips, pull outwards, and rub your fingers up and down until the fibres lock together again. For paired wings, select sections from two feathers which are as alike as possible.\nIn dry fly tying, stiff, glossy, water repellant cock hackles are best for buoyancy, though some people use the softer hen hackles in the belief that these present less of an obstruction on the strike.\nTo prepare a feather for hackling, hold it by the tip and run moistened fingers down the fibres so that they stand out at right angles and separate. Then, having stripped the base of the quill, lay the feather at right angles to the hook shank, just forward of the body, and lash the bared quill to the shank with tying silk. The fronded remainder is then held at the tip with hackle pliers and three or four turns made towards the eye.\nBinding a hackle needs a steady hand since the binding (gilt wire in a Palmer) is pulled between separate fibres of the feather: the wire needs to be kept taut and wound on in a spiral opposite to that used in winding the hackle. Any trapped fibres should be teased out with a sharp needle. Wire binding in the Palmer makes for durability and is attractive.\nThe weight of the pliers hanging down from the tip will prevent your work springing undone while freeing your hands to fasten it with turns of the silk. Cut off the tip of the feather. The Palmer is hackled along the whole length of its body. If you were tying a nymph, your artificial would have an enlarged thorax in place of a hackle. A thorax can be furbished out of materials with interesting textures, such as chenille, ostrich herl, seal’s fur or angora wool, but is rarely tied on wet or dry flies. The bodies of the less streamlined flies are dressed or ‘dubbed’ with animal fur – mole for example – which is applied to the waxed silk by rolling a pinch of fur on to the silk between thumb and index finger. The furry silk is then wound on to the shank.\nPreparation and tying in 3 To make hackle stand erect on a dry fly or Palmer, hold the tip, and draw the feather up between the fingers of the other hand. 4 Tie in stripped butt behind the eye, the outside of the feather facing forward. Secure with figure-of-eight turns of silk. Tie in on edge or the fibres will not stand erect.\nI Hold wings firmly along top of shank, so that wingtips just reach the hook-bend. Pass silk up between thumb and near wing and down between forefinger and far wing. Draw silk down firmly. Make two turns towards hook eye. 2 Before releasing, raise wings to the vertical. Take two turns round their base and behind. 3 Separate the wings, passing silk forward between them and around the shank, then back between them and around the shank again.\nThe simplest of dry flies – the Snipe series and Spider patterns – are simply hook, hackle and silk, but with natural colours or by dyeing, this combination affords many possibilities. The addition of wings, however can improve the balance of a dry fly, and makes for more spec-tacular creations. They can be tied in at the same stage as the tail and the fastenings hidden when the body is built up. Alternatively, a space can be left between thorax and hook-eye, and the wings added when the body is complete. There needs to be some silk round the shank before wings can be attached successfully.\nDry flies often sport double wings which also makes them more buoyant. Two feathers are needed from symmetrically identical positions on either side of a bird’s wing or tail fan. Symmetrically matched sections are cut from the centre of these feathers. These diamond-shaped sections have to be torn in half (for the double-wing effect) and eased into rectangles between finger and thumb (without the herls separating). Commercial wing-cutters are available for stamping wing shapes out of cock hackles. The four butt ends are pinched firmly between finger and thumb and introduced at the shoulder, two on either side of the body.\nNow a loop of silk has to be squeezed between finger and feather, passing up between thumb and wing, over the wing and down between index finger and wing of the far side. Do this twice more then pull the silk tight. Three turns of silk must be firmly drawn down without the wings splitting. To finish off, wind the thread in front of and behind the wings, passing it between them to keep them separated. The figures-of-eight will be concealed by any hackle which may be added afterwards.\nUnless a large, varnish head is to be moulded, the fly can be finished off with a neat whip finish near the eye. Vycoat, a modern varnish not prone to cracking, can be dabbed on to represent a glossy black head. This cleverly conceals any waste ends.\nThe varieties of dry fly patterns have occupied many volumes, and the possibilities are not yet exhausted. Nor is their use restricted to fishing for trout and salmon, for such coarse fish as rudd, roach, chub, dace, grayling, perch and pike are all taken on artificial flies.","South of our house, there is a little dead-end road called Springbrook Lane, and near the end of the road is a little pool, about ten feet across and maybe thirty feet long, and only a couple of feet deep. S_ thought it would be interesting to go down there with an insect net, and see what was there. She was right, it was interesting. A single scoop turned up a bunch of tadpoles and insect nymphs, including this one:\nThis is a dragonfly nymph. I’m not sure what species, but it was less than an inch long so it is probably one of the smaller ones. They all look a lot alike at this stage, and I guess the best way to find out the species is to let them grow up and see what they turn into.\nDragonfly nymphs are fully aquatic, and have gills to get their oxygen. One odd feature is that their gills are inside the abdomen, so they can pull water in and out of an opening in their rear. This helps them to move around, they can expell water fast enough for jet propulsion when they need to move quickly. This next picture is looking at the tip of the abdomen, and I think that the opening to the gill chamber is visible right there at the tip.\nThese pictures were all taken with the nymph in water, because when it came out of the water the legs were plastered to the side and it didn’t look like much. I wanted to get pictures of the underside, and was having trouble coming up with a good method to photograph it from underneath while it was in water. Then I remembered that, in the lab, we have these disposable cuvettes for the spectrophotometer. These are water-tight plastic chambers with two optically-flat sides, so if I put an aquatic insect into them, I can then photograph from the side (which I expect to be very helpful when I finally get around to scoring some mosquito larvae). In this case, the nymph was long enough that it had to orient up and down in the cuvette, so it was easy to get shots of the underside:\nIn that last picture you can see another interesting feature of dragonfly nymphs – the extendable lip. Adult dragonflies are voracious carnivores, and the nymphs are, too. But, it is hard work swimming through water to catch prey, so the nymphs have come up with an easier way. That structure under the head that looks like an extended chin is actually a greatly enlarged, articulated lip that the nymph can shoot out, as shown in this sketch (which comes from here):\nAnd here is a closer view of the actual lip:\nThe lip has claspers on the end, that can grab onto the prey item and drag it back into the dragonfly nymph’s waiting maw. I think you can see the claspers on this picture:\nThe claspers fold out kind of like jacknife blades, and have a serrated edge for holding onto the prey item. As for what the dragonfly nymphs eat, that depends on how big they are. Newly-hached nymphs will go after things like mosquito larvae, and as they grow they work their way larger and larger. The bigger species of dragonflies will even tackle small fish.\nWhen they mature, the nymphs will crawl out of the water (sometimes just up a reed stem, other times completely up on shore ambling across the beach) until they find a good spot. Then they crack open their backs, and the dragonfly emerges. This is one of the distinctions between nymphs and larvae: a nymph molts straight to the adult with no intermediate pupal stage, while a larva becomes a pupa first.\nThe smaller dragonfly nymphs are very effective at clearing out mosquito larvae, by the way: when S_ scooped with the net, one thing we did not find was mosquito wigglers. Dragonfly nymphs are also the subject of one of my favorite insects-as-food recipes. The recipe, in its entirety, is:\n(a) Boil dragonfly nymphs.\n(b) Eat them.\n This was actually a different nymph which was found, dead, in Coles Creek last year. It was badly decomposed and moldy, so most of it wasn’t worth looking at, but the lip structure was still in OK shape.\n From Unmentionable Cuisine, by Calvin Schwabe. Great book, great book."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:6122f357-d064-4a1c-abbd-594f6c474615>","<urn:uuid:7acf3aa2-5823-4c37-abfc-487c9dabc85c>"],"error":null}
{"question":"In what ways do the collaborative research principles of LCSL intersect with the historical example of scientific collaboration demonstrated at Bletchley Park during Turing's time?","answer":"Both cases demonstrate the power of interdisciplinary collaboration, but in different contexts. LCSL operates through a collaborative agreement between IIT and MIT, bringing together expertise from computer science, statistics, and signal processing to tackle complex learning problems. Their partnerships extend to multiple institutions including robotics, neuroscience, and cognitive science departments. Similarly, at Bletchley Park, Turing's successful breaking of the Enigma code was achieved through collaboration with various partners, including Dilly Knox and other code breakers, as well as cooperation with Polish cipher bureau and US Navy cryptanalysts. In both cases, complex problems were addressed by combining different areas of expertise and institutional resources, though LCSL focuses on advancing learning theory while Bletchley Park's collaboration was focused on wartime cryptanalysis.","context":["The Laboratory for Computational and Statistical Learning (LCSL) is the result of a collaborative agreement between the Istituto Italiano di Tecnologia and the Massachusetts Institute of Technology. Our aim is to advance the frontiers of learning theory and machine learning, while building algorithmic tools for the analysis of complex systems and high dimensional data. Our approach blends well established concepts and methods from computer science, statistics and signal processing with modern results in (high dimensional) probability, into new computational tools for statistical inference. Our goal is to channel theory and algorithms into new applications including smarter technologies and sophisticated engines for inference from high-dimensional data/signals. The ultimate objective of the lab is a future generation of intelligent technologies.\nThe research efforts of LCSL are organized in the following main projects:\nIt is well know that learning how to solve complex tasks becomes much simpler once the right features are found. Yet, feature (representation) learning is largely an open problem and one of the main challenges in machine learning. We tackle this problem considering two complementary data representation principles, namely invariance and selectivity and efficient coding.\nInvariance and Selectivity\nThe key idea is that useful representations should be invariant to semantically irrelevant transformation while sufficiently discriminative (selective). Following this principle, we develop deep learning algorithms inspired by current neuroscience models of the information processing in the visual cortex.\nWhile a good data representation should ultimately reduce the need for supervision, in practice unsupervised approaches to data representation rely on reconstruction as a general starting requirement. An efficiency requirement needs to be further specified to make the approach sound. Here efficiency means looking for forms of parsimonious reconstruction.\nThe availability of large scale datasets requires the development of ever more efficient machine learning procedures. A key feature towards scalability is tailor computational requirements to the generalization properties in the data, rather than their raw amount. This project aims at blending statistical and optimization principles to design new, sound and scalable learning machines.\nClassical machine learning, based on estimating multivariate function with scalar outputs, might not be adequate to cope with a variety of structured data with no natural vectorial representation. Leveraging on analytic and optimization techniques, we develop novel modeling principles and corresponding implementations to deal with structured learning problems and in particular transfer learning problems.\nRobotics is a natural testbed for machine learning solutions. The variety of sensory modalities robots are endowed with requires for the robot learning to adapt and interact with the environment and humans. We develop and apply cutting edge machine learning techniques to solve perception, cognition and control problems in humanoid robotics.\nIn recent years, statistical machine learning approaches had a huge impact in the solution of computer vision problems such as object classification. We develop novel algorithms for image representation that could reduce the need of labeled data and scalable machine learning solutions to classify large image and video datasets.\nThe dynamic and highly variable nature of speech signals provide a challenging setting for machine learning methods. Automatic speech recognition is still largely unsolved, despite recent advances through deep learning representations. The goal of this project is to derive invariant speech representations for recognition and classification.\n- iCub Facility (IIT)\n- Robotics Brain and Cognitive Sciences (IIT)\n- Neuroscience and Brain Technologies (IIT)\n- Massachusetts Institute of Technology, Center for Brains, Minds and Machine\n- Department of Informatics, Bioengineering, Robotics and Systems Engineering, Università degli studi di Genova\n- Google DeepMind\n- Duke University","Last Updated on November 10, 2020 by Sean B\nAlan Turing is one of the biggest names in the field of computer sciences, crypto analysis, artificial intelligence, and even the philosophy of artificial intelligence and mechanic consciousness.\nAlan Mathison Turing left an enormous legacy behind him and is an inspiration for many in the field of artificial intelligence and computer science. Turing not only lent his contributions towards the development of computers and artificial intelligence, but he also saved millions of lives during World War II.\nIn this blog, we shall be looking at a brief biography of Alan Turing and his contributions towards the development of computer science, Artificial Intelligence, and modern-day Chatbots.\nAlan Mathison Turing was born in Maida Vale on 23rd of June 1912, in London. Alan’s father, Julius Mathison Turing, was part of the Indian Civil Service (ICS) and was stationed in India’s Odisha state. Alan Turin’s mother was Ethel Sara Turing.\nDue to Turing’s father’s position in ICS, the family of Turing spent a lot there in India. However, Turing’s parents wanted to raise their son in Britain, so the family moved to Maida Vale in London.\nTuring was the second son of Julius and Ethel. Since his adolescence, Alan Turing showed signs of genius. The job of Julius requires him to visit and travel to different places. Parents of Turing would leave their two sons with a retired army couple.\nAlan Turing was enrolled in St Michael’s, a day school at 20 Charles Road, St Leonards-on-sea when he was six. Since his adolescence, he was known by his teachers for his remarkable genius. Alan Turing was regarded as a child prodigy.\nBetween 1922 and 1926, Turing was admitted to Hazelhurst Preparatory School. Then in 1926, Turing was admitted to Sherborne School at the age of 13. Sherborne school was a boarding school in the town of Sherborne in Dorset.\nTuring was enormously inclined towards the subjects he was interested in. Alan Turing once rode to his school on a bicycle for 97 kilometers to attend the first day of his term, which coincided with the 1926 general strike.\nTuring was able to accomplish many impressive feats in school life. Turing was able to solve advanced problems in 1927 without taking courses on even the elementary versions of Calculus. At age 16, Alan Turing was able to grasp Einstein’s work, specifically Einstein’s criticism of Newtonian physics.\nUniversity Life and Research\nAfter completing his studies in Sherborne, Turing started studying at King’s College, Cambridge. Turing received many first-color honors in mathematics. At age 22, Turing proved the Central Limit Theorem.\nIn 1936, Turing spent his time studying under the church at Princeton University. Turing also studied cytology along with his mathematical works. Turing also built three stages of an electro-mechanical binary multiplier. Turing received his Ph.D. from the Department of Mathematics in Princeton in June 1938.\nThe Entscheidungsproblem was a problem in mathematics and computer sciences first posed by David Hilbert and Wilhelm Ackermann in 1928. The German word for the problem translates to a decision problem.\nThe problem asks for an algorithm that can respond to our logical questions in yes or no answers. The questions will consist of universally accepted notions such as axioms. Turing published his work “On Computable Numbers, with an Application to the Entscheidungsproblem.”\nTuring’s work put forth the idea of his “Universal Turing Machine” and promised that his machine would be able to perform the feats the problem demands it to. If an algorithm is provided to the machine, then the machine will be able to perform any mathematical operation.\nTuring’s paper has been known as the most influential work in mathematics. Ad mist his work, Turing gives the abstract idea of the Universal Turing Machine, which acted as an antecedent towards the development of modern computers.\nAlan Turing accomplished many feats throughout his career, which includes providing an abstract idea of a Universal Turing Machine, breaking the Enigma Code, and proposal of the Turing test.\nUniversal Turing Machine\nUniversal Turing machine was an abstract idea of a machine capable of stimulating the description of the machine as well as the input on that machine, which is stored in a tape. Alan Turing introduced his idea in 1936.\nThe Universal Turing machine acted as an inspiration for a stored-program computer created by John Von Neuman in 1946. These Turing ideas helped the computer sciences take their intuitions about automata one step further and opened limitless probabilities of advancement in the field of computer science.\nThe Code Breaker and Cryptoanalysis of the Enigma Machine\nAlan started working part-time with the British Government before the events of World War II at the Code and Cypher School (GC&CS;), the British code-breaking organization. Alan became a full-time participant in the cryptanalysis of the German ciphers at Bletchley Park in 1939.\nAlan’s work was focused on deciphering the encrypted messages of the Enigma Machine used by Nazi Germany. His work was in collaboration with his partners Dilly Knox and a senior code breaker at GC&CS;.\nThe polish cipher bureau was successful in deciphering the Nazi codes before it. However, Germans had strengthened their safety protocols, which made breaking the enigma code a harder task than before.\nWith the sharing of information given by the polish cipher bureau, Alan Turing and his team at the Bletchley Park was able to create a bombe machine used for decryption of the enigma codes. Alan Turing also lent his help deducing the indicator procedures used by Nazi Germany.\nThe Turing-Welchman Bombe was an electromechanical machine specified by Alan Turing. It was based on the Polish Bomba Kryptologizna. The Bombe was highly effective at breaking and deciphering of Nazi Germany’s messages encrypted by the Enigma Machine.\nThe Bombe worked using a crib-based decryption system. It searched for possible settings used by the Enigma Machine, such as the rotor order, rotor settings, and plugboard settings using a crib. The crib performed logical deductions and was effective at reducing the work of the cryptanalysts at Bletchley Park.\nAlan Turing broke the code of the Enigma Machine and the code of the German navy. The British departments were facing increasing difficulties in breaking the German navy code, so Alan decided.\nAlan solved the German navy indicator and further advised the use of Banburismus, a cryptanalytical process that was essential in breaking the code of the German navy. Alan Turing, along with Us navy cryptanalysts, worked on the naval Enigma and his efforts were effective in reducing the work of the cryptanalysts in deciphering the codes.\nThe Turing Test and Antecedents Towards the Development of Early Computers and Artificial Intelligence\nAlan Turing initially called his test the Imitation Game when he created it in 1950. The fact that it has come to be known as the Turing Test shows his importance in the field of Artificial Intelligence. The Turing Test revolves around testing a machine’s capability to exhibit intelligent behavior akin to a human being. Turing wrote a paper in 1950 titled “Computer Machinery and Intelligence,” where he first proposed his test.\nThe Turing Test can be summed up in one question, “Can machines think?” Alan Turing pondered upon this profound question and advised a test to determine the machines’ ability to think.\nA Turing Test is carried out between two participants and an interrogator. One of the participants is human, while the other is a machine. The interrogator’s job is to simultaneously hold a conversation between the two participants and determine who is human and who is a machine.\nTuring Test was profoundly inspirational in the sense that it opened up new possibilities for automata and was a precedent to early developments in the field of Artificial Intelligence. The Turing test’s real-world implication can be seen in the form of the Loebner Prize, which announces a prize for whoever wins the Turing test.\nA reversed version of the Turing Test is used in the form of CAPTCHA as a safety protocol on websites to find out whether the person interacting with the website is human or a bot.\nDeath and Legacy\nAlan Turing died early because he was homosexual; he and his 19-year-old sexual partner Murray were charged with indecency after Turing admitted to being in a sexual relationship with Murray. At the time, being homosexual was illegal in Britain, and sadly in much of the world. Under section 11 of the Criminal Law Amendment Act 1885, both Alan and Murray were charged with gross indecency. Turing was convicted and was given a choice between imprisonment and probation with chemical castration.\nAlan chose the second option; Alan would be given Diethylstilbestrol injections, a synthetic estrogen designed to eliminate his libido. The doses also resulted in the development of breast tissues in Alan Turing.\nAlan Turing was found dead by his housekeeper on 8 June 1954. The cause of death was regarded as Cyanide poising, and it is believed that he committed suicide by taking lethal doses. Turing’s body was cremated, and the ashes were scattered in Working Crematorium.\nAfter his death, the prime minister of Britain, Gordon Brown, issued an apology to Turing following a petition by John Graham-Cumming. Britain made a policing and crime act law in 2017, which included an Alan Turing Law, which pardons men who were historically convicted under homosexual acts.\nAlan Turing has left an enormous legacy behind him, his contributions in the field of computer sciences and Artificial Intelligence are limitless, and there’s no doubt that he is a pioneer in mathematics, logic, and cryptoanalysis.\nFirst of all, Alan Turing’s contributions to the world were astounding, and in retrospect, I believe that all decent human beings can see that his treatment simply because of who he loved was horrible and even criminal. Now that I have made that statement clear, let’s talk about his work. Turing, along with the others at Bletchley Park, solved the encryption of the Enigma Machine using an earliy computing device he helped design and build. It is estimated that this saved the lives of more than 14 million people. That act alone makes him one of the most important figures of the 20th Century.\nBut his contributions to the developments of Computing and Artificial Intelligence may have had an even greater impact. How many lives have been saved by the use is AI in the fields of Medicine? How many lives have been saved by the use of Computers to predict the paths of Hurricanes and Tornadoes? And those are just two fields that Computers have had an impact in.\nThe creation of chatbots is a small part of what Alan Turing had a hand in creating, but that’s how it is with great minds, they touch so many different small things with their minds that we tend to forget how big they were as a whole.\nAlan Turing was a giant.\nWe hope that the blog was informative.\nWe look forward to reading your thoughts in the comments."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:f60c5046-aa01-41ce-a3b9-3ba57ee06f52>","<urn:uuid:94ce1616-26de-4ebf-b7c8-dfcab87c00ce>"],"error":null}
{"question":"How do Stop & Shop's and the food waste machine's organic waste processing methods compare in terms of their environmental impact?","answer":"Both systems process food waste but operate differently. Stop & Shop's anaerobic digestion facility converts 95 tons of inedible food daily into biogas, which generates electricity for their distribution center, providing 40% of its energy needs. This helps them divert 88% of waste from landfills. The food waste machine, on the other hand, uses microorganisms to decompose food waste into water and carbon dioxide through respiration, with very low power consumption of around $30 per ton of waste. While Stop & Shop's system produces usable energy, both methods effectively prevent food waste from ending up in landfills.","context":["Six Massachusetts companies ranging from a ski resort in the Berkshires to the largest grocery chain in New England have been named winners of the inaugural Associated Industries of Massachusetts (AIM) Sustainability Award. The award recognizes excellence in environmental stewardship, promotion of social well-being and contributions to economic prosperity.\nAIM announced today that Jiminy Peak Mountain Resort of Hancock; PeoplesBank of Holyoke; W.D. Cowls, Inc. of North Amherst; Cavicchio Greenhouses, Inc. of Sudbury; Gorton’s Seafood of Gloucester; and the Stop & Shop New England Division of Ahold USA were selected from among 33 nominations. The six companies will be honored at a series of regional celebrations throughout Massachusetts in September and October.\n“These companies set the standard for sustainably managing their financial, social and environmental resources in a manner that ensures responsible, long-term success,” said AIM President and Chief Executive Officer Richard C. Lord.\n“Sustainability guarantees that the success of employers benefits our communities, our commonwealth and our fellow citizens. We congratulate our honorees and all the worthy companies that were nominated.”\nSustainability has gained widespread acceptance in recent years as global corporations such as Wal-Mart, General Electric and IBM make it part of their business and financial models.\nThe six honorees were selected by a committee that included the co-chairs of AIM’s Sustainability Roundtable - Johanna Jobin, Director of Global EHS and Sustainability at Biogen; and James McCabe, Sustainability Manager, Global Operations Group, Waters Corporation.\nAIM initiated the Sustainability Roundtable in 2011 to provide employers the opportunity to exchange sustainability best practices and hear from experts in the field. That opportunity has attracted dozens of participants from companies such as Bose, Siemens, Coca-Cola, Boston Beer, MilliporeSigma, Ocean Spray, Analogic and Cisco.\nHere are summaries of each recipient, along with the date and location of the celebration when each will receive the award.\nStop & Shop New England, Division of Ahold USA – October 24, Gillette Stadium, Foxboro\nStop & Shop in April opened an innovative, state-of-the-art, Green Energy Facility in Freetown that uses a natural process called anaerobic digestion to convert inedible food that cannot be donated into clean energy. The process produces up to 40 percent of the energy for Stop & Shop's 1.1 million square-foot adjacent distribution center, enough power to operate the center for four months of the year.\nEach day, 95 tons of inedible food from 208 Stop & Shop stores is brought to the 24,000 square-foot facility to be processed and converted into biogas. The bio-gas fuels a generator that in turn, generates electricity providing power for heating, lighting and air conditioning systems in the sprawling distribution center.\n“As a responsible retailer, one of our top priorities is reducing our environmental footprint, specifically through the conversion of food that would otherwise go into a landfill,” said Mark McGowan, President, Stop & Shop New England. “The Green Energy Facility is a perfect example of our ongoing efforts to be greener in our operations.”\nThe Green Energy Facility is part of Stop & Shop’s strategic and long-term efforts to reduce its environmental footprint. Today, Stop & Shop diverts 88 percent of its total waste from landfills with the goal to be “zero waste” by 2020.\nJiminy Peak Mountain Resort, Hancock – September 20, Interprint Inc., Pittsfield\nJiminy Peak Mountain Resort will add to its extensive renewable energy portfolio this season by installing high-efficiency snowmaking guns on its downhill ski facility in Hancock. The resort expects to be completely powered by renewable energy by 2017.\nThe 450 new snow gun heads will replace older, less efficient technology and will reduce the amount of electricity used for making snow. The new LPXY snowmaking guns generate twice as much snow using half the amount of compressed air as the older model. Jiminy was able to take advantage of a National Grid energy efficiency incentive program to help offset a portion of the cost for the upgrade.\nThe project, along with the installation of LED lights on nine night-skiing trails, is the latest in a long history of environmental stewardship for the resort. Recent additions to the portfolio include a 2.3 megawatt community shared solar facility with Nexamp and a co-generation facility located in the Country Inn. These renewable projects are in addition to the 1.5 megawatt GE wind turbine for which the resort was awarded the Golden Eagle Award from the National Ski Areas Association.\nJiminy Peak is the largest ski and snowboard resort in southern New England and a premier four-season resort in the Berkshires. The 167-acre facility includes extensive conference and wedding facilities.\n“Controlling and reducing the snowmaking operating costs, maximizing snow production and optimizing the benefits of all capital reinvestments have long been the mantra of this organization,” the company says.\n“Jiminy's policy of environmental awareness is seen in all of our activities. We seek to raise the environmental awareness of guests and employees, and to broaden their knowledge and appreciation through educational programs.”\nJiminy is also a participant in the NSAA Climate Challenge, joining other resorts striving to reduce dependence on fossil fuels and preserving the earth for future generations.\nCavicchio Greenhouses, Inc., Sudbury – October 17, Hanover Theatre, Worcester\nCavicchio Greenhouses, Inc. established in 1910, is a company that works and lives off the land, so it’s no surprise that the company spends a lot of time understanding and mitigating its impact on the environment.\nCavicchio is New England’s most comprehensive wholesale horticultural grower and distributor, cultivating and sustaining more than 250 acres of annuals, perennials and nursery stock, complimented by a premium selection of loam, mulch, stone, and landscape supplies.\nThe company operates 10 acres of state-of-the-art greenhouse space powered by computerized environmental control systems. Water usage has been addressed by installing flood floor systems to recycle water and by grading the fields (160 acres) so that irrigation water circulates back to irrigation ponds.\nThe company utilizes wood-fueled heating and cooling systems in its greenhouses year-round. The wood that fuels these boilers is chipped on-site from logs and cut trees brought in by landscape contractors. The process provides a convenient way for customers to recycle debris and has eliminated the need to use fuel oil.\nCavicchio Greenhouses, Inc. communicates, both internally and externally, the importance of not misusing the land they occupy, and has engaged customers by implementing free plastic-pot and tray recycling programs, recycling over 300 tons of plastic annually. And, Cavicchio has dedicated 10 acres to composting and recycling of grass, leaves, soil, brush, asphalt, concrete and cement. Other environmental efficiency improvements include incorporating electric carts and replacing diesel-powered tractors.\nPeoplesBank, Holyoke – October 20, Wood Museum of Springfield History, Springfield\nPeoplesBank is not only building environmental responsibility into its own future, but also helping others do the same.\nThe 131-year-old community bank based in Holyoke recently constructed LEED® certified branches in Springfield, West Springfield, and Northampton – the first of their kind in the area. LEED is the nation’s preeminent program for the design, construction and operation of high-performance green buildings.\nAfter building the branches, PeoplesBank initiated community education events to spread the word about environmentally friendly construction and building operation. It also installed electric-vehicle charging stations at three locations and held e-recycling events that have collected more than 100,000 pounds of material to date.\nBut the bank didn’t stop there. Using one of its core business capabilities, PeoplesBank has financed more than $100 million in wind, solar, and hydroelectric power in the region. The organization regarded tackling the sheer complexity of financing these projects as another opportunity to serve the community.\nThe bank even sponsors of a farmers’ market for associates, a program that led to the formation of an Environmental Committee to promote green values at home and at work.\n“Through our commitment and actions to support environmental sustainability, we believe that we can make the region a healthier place to live, work and raise a family,” said Tom Senecal, PeoplesBank President and Chief Executive Officer.\nW.D. Cowls, Inc., North Amherst – October 20, Wood Museum of Springfield History, Springfield\nHere’s proof of sustainability - W.D. Cowls Inc. has been continuously managing generations of the same forest land in western Massachusetts since 30 years before the Revolutionary War.\nThe North Amherst forestry, real estate and building-materials company has been working woodlands in the Pioneer Valley since 1741, through nine generations of family ownership. Cowls grows and harvests diverse mixed species - including pine, oak and hemlock - along with other forest products. It also retails lumber, paint, hardware and building materials at its Cowls Building Supply store.\nThe long-term sustainable view that has led Cowls to care for and maintain the same Massachusetts forest for more than 275 years led the company to create the largest private conservation project in Massachusetts history, placing a Conservation Restriction on 3,486 acres of timberland now known as the Paul C. Jones Working Forest. An additional 2,000 to 3,000 acres are due to be conserved during the next 12 months.\nIn a textbook definition of Smart Growth, Cowls uses revenue earned conserving outlying open space to sustainably develop downtown North Amherst. Cowls’ generational Home Farm redevelopment is as sustainable as the company’s forest practices.\nEvery generation of the Cowls family since 1741 has built what was needed on the 20-acre Home Farm in North Amherst. The site in the past has produced tobacco and onions, and housed such diverse operations as the Amherst-Sunderland branch of the Holyoke Street Railway system, a major dairy operation and the first electric sawmill in the country.\nFor this generation, the Cowls’ Home Farm is becoming a new town center called The Mill District. The project already includes Cowls Building Supply; Atkins Farms Market in the old cow barn; The Lift Salon and Bread and Butter Café in the new Trolley Barn; and more than a dozen apartments.\nThe next phase of growth is about to begin. North Square in The Mill District will feature 130 apartments and 22,000 square feet of restaurants, shops and services underneath.\nToday, as the state’s largest private landowner, Cowls sets the bar for environmental and economic sustainability.\nGorton’s Seafood, Gloucester – October 26, Riverwalk Complex, Lawrence\nOne of the most recognizable names in seafood discovered that reducing its environmental impact is a matter of degree – nine degrees to be exact.\nGloucester-based Gorton’s Seafood, for many years a leader in preserving the oceans that yield its products, utilized data and scientific analysis to determine that raising the temperature at which its frozen seafood was distributed would significantly reduce diesel emissions tied to climate change. So the company changed its recommendation for delivery temperatures from minus 10 degrees to minus one degree\nThe result was that the company’s carriers saved 15,000 gallons of diesel fuel a year, the equivalent of removing 85 cars from the road or planting 696 trees. Gorton’s was also able to smooth out significant swings in temperature to which its seafood is often subjected during the long journey from plant to grocery store to home freezer.\nThe program is part of a broader sustainability program at Gorton’s called Trusted Catch. As part of that commitment to sustainability, Gorton’s currently sources 97 percent of its wild-caught seafood from fisheries that are certified as sustainable by a third party.\n“As a seafood industry leader located in America’s oldest seaport, Gloucester, Massachusetts, we recognize that preserving our oceans and natural resources is not merely an option, it is a mandate. It is a mandate that we have followed for generations by partnering with a select group of suppliers and adhering to strict quality controls and standards that are among the toughest in the industry,” the company said.\nAll of the regional award celebrations are free and open to AIM members, but registration is required.\nRegister | Pittsfield\nRegister | Worcester\nRegister | Springfield\nRegister | Foxboro\nRegister | Lawrence","Here are some frequent asked questions about our product for your reference. If you have any other questions, welcome to contact us!\nQ 1: Does food waste need sorting or pre-treatment before putting into the machines?\nA : There is no need for sorting and pre-treatment since our machine can decompose most type of food waste, except for the things that cannot be digested such as metals, plastics and bones, which will stay inside the machine occupying the space and results in lower machine efficiency.\nQ 3: Does the machine need labor handling?\nAny other hand work is needed during operation?\nA : No, our machine is fully automatic from the beginning to the end. Users are only required to dump the food waste into the machine and it will turn food waste into watery liquid automatically.\nQ 5: What’s the working principle of your food waste machine?\nA : Our machine contains high density of effective microorganisms that convert organic waste (including food) into water and carbon dioxide by respiration.\nQ 7: Can soup be decomposed by your machine?\nA : Yes, please note that hot soup is not allowed to put into the machine since it will kill the microorganisms.\nQ 9: Will it cause high power consumption if the machine operates everyday?\nA : Our machine power consumption is very low, the treatment cost could be as low as $30 per ton of food waste.\nQ 11: If anything accidentally drops into the machine, is it safe to fish it out by hand?\nA : Yes, there is a paddle mechanism inside the machine that slowly rotates to stir the food waste, but it will automatically stop once the door is open. Rubber gloves should be worn when retrieving something from inside the machine.\nQ 13: Do we need to collect and put the food waste into the machine at the appointed time?\nA : Of course not. Users can collect and put the food waste into the machine at any moment.\nQ 2: Does the machine need to be cleaned after the decomposition of food waste?\nA : There is no need for cleaning after the decomposition of food waste since all the food waste will break down to carbon dioxide and water which is directly discharged thru the pipes.\nQ 4: What can I put in the machine?\nA : The general rule is that if a human can eat the food, the machine will process the food. The machine and the microorganisms are designed to process a wide range of food waste types in mixed quantities.The machine cannot process anything that is not organic food waste and cannot be easily broken down such as bones, seeds and stones, packaging, etc.\nQ 6: How long will it take for food to be decomposed completely?\nA : It depends on the type of food, usually within one day.\nQ 8: Can I put refrigerated or frozen food waste into your machine?\nA : Please wait for the food waste to come to room temperature before putting into the machine. If food waste at very low temperature is added, it will change the conditions within the machine and may kill the microorganisms.\nQ 10: Do we need to replenish the microorganisms of the machine regularly?\nA : There is no need to frequently replenish it. However, we suggest it is better to replenish the microorganisms at least once a year to maintain the quality of work.\nQ 12: Does detergent affect the normal operation of the machines?\nA : Users do not need to worry about this issue, because the impacts of detergent to the microorganisms are not very significant.\nQ 14: Does detergent affect the normal operation of the machines?\nA : It is important to have a constant supply of food waste to the machine. If the machine is not used for 1 month, the microorganisms inside will die, so the food waste introduced later could not be digested properly. In this situation, microorganisms should be replenished to the chamber before the appliance is used."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:23cd49a3-3d10-43c7-afe6-18ab406c6022>","<urn:uuid:13e917cc-ff55-4a29-8408-061b922f4c9c>"],"error":null}
{"question":"How do air source heat pumps work and what makes them environmentally sustainable?","answer":"Air source heat pumps work by capturing heat from outside air to warm a liquid refrigerant. The heat pump converts the refrigerant into a gas, compresses it, and then condenses it back into a liquid, generating heat energy in a continuous cycle. They are sustainable because they use renewable heat from the air, which is naturally warmed by the sun and weather conditions. While they run on electricity, they use less power than traditional electric heating systems to produce the same amount of heat. They can operate in temperatures down to -15 degrees and can save approximately 4 tonnes of CO2 per annum. Their sustainability can be further enhanced by combining them with solar energy.","context":["An Air Source Heat Pump (ASHP) captures heat from the air circulating outside your property and boosts the temperature to provide heat and hot water. It is a very effective, low carbon way of heating your home and can be combined with a number of renewable and energy efficient technologies to deliver a truly sustainable home or commercial premises.\nAn ASHP can be installed in new and existing properties and the Government’s Renewable Heat Incentive provides cash payments for all eligible installations, which makes the investment even more attractive.\nIn warmer months ASHPs can operate as air conditioning units, meaning that you benefit from your heating system all year round, whatever the weather!\nHow air source works\nAn Air Source Heat Pump uses the same technology as your kitchen fridge to move heat from one place to another.\nAn ASHP uses the temperature of the air circulating outside to warm a liquid refrigerant.\nThe heat pump turns the refrigerant into a gas, compresses it and then condenses back into a liquid again which generates heat energy and the cycle starts again.\nWhat makes heat pumps sustainable?\nThe temperature of the air outside is determined by the sun and local weather conditions and is therefore a renewable source of heat.\nUsing this renewable energy to help heat your home is more sustainable than fossil fuel alternatives.\nHeat pumps are electric, but by using heat from the air they use less electricity to produce heat than traditional electric heating systems.\nCombining your ASHP with your own solar energy increases their sustainability and further reduces carbon emissions.\nAre air source heat pumps efficient?\nThe efficiency and performance of an ASHP is measured using Coefficient of Performance (COP) and this varies by model and brand.\nA number of factors including the insulation levels that your property has, your existing heating system and whether you are using under floor heating versus radiators will determine the overall performance of your ASHP.\nGovernment incentive schemes to encourage investment in low carbon technology can currently cover the cost of installation and ongoing maintenance costs are lower than with traditional heating systems.\nGeo Green Power have extensive experience and will ensure you understand the options available.\nLearn more about how ASHPs work\nIf you choose Air to Air your ASHP will provide hot air to heat your property and you will need an alternative system to provide hot water.\nIf you choose Air to Water then your ASHP will provide hot water to heat your property and provide hot water for use within your home.\nIt’s important to consider your requirements and any unique aspects of your property and location to create the best solution.\nWhen designed and configured correctly your ASHP does not require a back-up heating option.\nHeat pumps are designed to maintain properties at a consistent temperature. This is when they run at their optimum efficiency and are most effective.\nASHPs can be combined with over-sized radiators or underfloor heating. Underfloor heating is often more efficient, but great results can be achieved with other forms of heating.\nWe will ensure that we recommend the right option for you and your property\nASHPs work all year round and in temperatures down to -15 degrees.\nThey are more efficient when the outside temperatures are higher, however in the UK the difference between seasons is very minimal.\nAll air that is above the temperature of absolute zero has heat energy!\nIn recent years ASHP technology has improved considerably. The cost of ownership has come down and the units have become quieter and more efficient.\nRequest a call to discuss your project\nWhat is the installation process?\nASHPs are fitted to the outside of your property and are about 1m wide, 1m high and around 30cm deep.\nNo ground works are required.\nDepending on your existing system, your ASHP may easily link to your water tank and heating with limited additional works. You may need to replace your radiators for larger units.\nA domestic installation in a single property is usually completed and commissioned within 5 working days.\nLearn more about installation\nASHPs have very few moving parts and require less ongoing maintenance than traditional heating systems. We recommend an annual service to ensure that your system is running at its optimum efficiently.\nYou do not require planning permission for an ASHP.\nYour ASHP is positioned outside your property and is best in a sheltered location which has good air flow. Most properties can easily accommodate an ASHP in a location that is both effective and convenient.\nASHPs work best when they are combined with other energy saving technology such as insulation and double glazing. Running your heat pump to maintain your property at a consistent temperature is also advisable.\nChanging from Oil Fired Heating to Air Source\nWhen Paul took on a 1970s property he wanted to move away from oil fired heating. He carefully compared the cost of replacing the old oil boiler with an up to date condensing oil boiler or an Air Source Heat Pump system and found that the heat pump option was the right choice for him.\nCosts & Expected Returns\nWhat is the installation cost for air source?\nOur ASHP installations start from around £12,000\nHow much do ASHPs cost to run?\nIt’s very difficult to estimate the running costs of your installation as it will depend on the size of the space you want to heat, the temperature you are running the system at and a number of other factors such as insultation and whether you are using underfloor heating or radiators.\nWe will be happy to provide an estimate of your running costs if you would like to discuss your project.\nWhat will my return on investment be?\nThe return on investment that you receive will be determined by the type of fuel you are looking to replace and the cost of running your system. Higher returns can be achieved when switching from oil or LPG heating systems.\nThe current Renewable Heat Incentive provides payments that, in a number of cases, can cover the cost of the installation.\nHeat pumps require less maintenance and repair than most traditional heating systems, which also makes them an attractive investment.\nGet in touch to discuss your ASHP project\nJames oversees all of our air source heat pump installations. Get in touch today to organise a no-obligation consultation about your next project.\nWhat is the RHI and are Grants available for ASHPs?\nThe current Government scheme is the Renewable Heat Incentive (RHI).\nThe domestic scheme provides payments for 7 years and applies to eligible heating systems that are used for one dwelling\nYour system will have an Energy Performance Certificate (EPC) and a Seasonal Performance Factor and these two values will be used to calculate your payments based on your usage and the current tariff.\nEstimated output: 12,000kWh per annum\nEstimated RHI income: £900.00 per annum (7 year contract)\nEstimated CO2 savings: 4 tonnes per annum\nASHPs in Commercial & Domestic Buildings\nASHPs in commercial buildings\nASHPs can easily be installed in commercial premises to provide heating and air conditioning and are particularly efficient when running to provide a consistent temperature for offices and workspaces.\nASPHs in domestic houses\nASHPs provide a low carbon, energy efficient way of heating homes. The systems are low maintenance and combining them with other energy efficient technology helps to ensure that they perform at their best.\nASHPs in New Builds vs Existing Properties\nASHPs installation in new builds\nThe construction of a new property is the perfect opportunity to consider and implement energy efficient, sustainable technology to ensure that the finished result is as cost effective and environmentally friendly as possible.\nThe UK Government is due to bring in the Future Homes Standard in 2025 which will prohibit any new build property from having a fossil fuel heating system.\nIncorporating low carbon, renewable heating into new build projects will ensure that you remain ahead of legislation.\nASHPs installation in existing and historic buildings\nASHPs can be retrofitted into existing properties to replace oil, LPG and mains gas heating systems. The return on investment will vary depending on the existing heating system and other factors such as the property’s energy ratings.\nAir Source vs Alternative Heating Solutions\nComparing ASHPs to existing gas and oil systems\nAn ASHP provides an energy efficient, low carbon option when compared to gas, oil or LPG heating systems. It delivers a significant reduction in CO2 emissions and has lower ongoing maintenance costs.\nThe costs of installation may be higher than traditional fossil fuel systems, but this is offset considerably by the RHI payments you will receive.\nRunning costs are usually comparable to mains gas systems, and significantly cheaper than oil or LPG.\nAir source heat pump or ground source\nAn ASHP installation is often cheaper and less intrusive than a GSHP installation and is ideal for smaller properties and those with limited outside space.\nIf you have a large property to heat, and enough outside space a GSHP will outperform an ASHP. A GSHP system can collect more renewable heat from the earth and therefore requires less electricity to produce the same heat output.\nAir source heat pump or solar panels\nWhen considering an ASHP or solar panels, the best choice for you will depend on your heating requirements, budget and energy usage and we will be happy to discuss your options with you.\nIt may be most cost effective for you to consider a scheme that incorporates both. Using solar power to run your heat pump could deliver the greatest cost efficiency and deliver the most sustainable scheme.\nFind out more about renewable energy…"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:416a8aee-e3a5-4e75-a89b-505befe84c0e>"],"error":null}
{"question":"How do the storage requirements differ between unopened latanoprost eye drops and cyclosporine eye drops?","answer":"Cyclosporine should be stored in a closed container at room temperature, away from heat, moisture, and direct light. In contrast, unopened latanoprost eye drops must be stored in a refrigerator, though once opened, they can be stored at room temperature away from moisture, heat, and light.","context":["Cyclosporine (Into the eye)\nTreats chronic dry eye.\nRestasis, Restasis MultidoseThere may be other brand names for this medicine.\nWhen This Medicine Should Not Be Used:This medicine is not right for everyone. Do not use it if you had an allergic reaction to cyclosporine.\nHow to Use This Medicine:\nLiquid Mixture, Drop\n- Your doctor will tell you how much medicine to use. Do not use more than directed.\n- Read and follow the patient instructions that come with this medicine. Talk to your doctor or pharmacist if you have any questions.\n- Turn the package upside down a few times to mix the medicine before you use it. Use the medicine right away after you open the package.\n- Wash your hands with soap and water before and after using this medicine.\n- To use the eye drops:\n- Lie down or tilt your head back. With your index finger, pull down the lower lid of your eye to form a pocket.\n- Hold the dropper close to your eye with the other hand. Drop the correct number of drops into the pocket made between your lower lid and eyeball. Gently close your eyes. Place your index finger over the inner corner of your eye for 1 minute. Do not rinse or wipe the dropper or allow it to touch anything, including your eye.\n- Wait at least 15 minutes before you use other lubricant eye drops (such as artificial tears).\n- If you wear contact lenses, take them out before you use this medicine. Wait at least 15 minutes before you put your contact lenses back in.\n- Missed dose: Take a dose as soon as you remember. If it is almost time for your next dose, wait until then and take a regular dose. Do not take extra medicine to make up for a missed dose.\n- Store the medicine in a closed container at room temperature, away from heat, moisture, and direct light.\nDrugs and Foods to Avoid:\nAsk your doctor or pharmacist before using any other medicine, including over-the-counter medicines, vitamins, and herbal products.\nWarnings While Using This Medicine:\n- Tell your doctor if you are pregnant or breastfeeding, or if you have an eye infection.\n- Call your doctor if your symptoms do not improve or if they get worse.\n- Keep all medicine out of the reach of children. Never share your medicine with anyone.\nPossible Side Effects While Using This Medicine:\nCall your doctor right away if you notice any of these side effects:\n- Allergic reaction: Itching or hives, swelling in your face or hands, swelling or tingling in your mouth or throat, chest tightness, trouble breathing\nIf you notice these less serious side effects, talk with your doctor:\n- Blurred vision or other changes in vision\n- Feeling like something is in your eye\n- Mild, brief burning in your eyes\n- Red, watery, or itchy eyes\nIf you notice other side effects that you think are caused by this medicine, tell your doctor\nCall your doctor for medical advice about side effects. You may report side effects to FDA at 1-800-FDA-1088\nLast Updated: 1/4/2018\nThe information provided herein should not be used during any medical emergency or for the diagnosis or treatment of any medical condition. A licensed medical professional should be consulted for diagnosis and treatment of any and all medical conditions. Call 911 for all medical emergencies. Links to other sites are provided for information only -- they do not constitute endorsements of those other sites.\nTruven Health Analytics. All rights reserved.","Manufacturer: Sun Pharma\n|0.005% 2.5ml × 1 solution||£ 52.74||£ 52.74||Buy Now!|\n|0.005% 2.5ml × 2 solution||£ 40.75||£ 81.49||Buy Now!|\n|0.005% 2.5ml × 3 solution||£ 32.21||£ 96.62||Buy Now!|\nLATANOPROST reduces pressure in the eye by increasing the amount of fluid that drains from the eye.\nLatanoprost (for the eyes) is used to treat certain types of glaucoma and other causes of high pressure inside the eye.\nTo make sure you can safely use Latanoprost, tell your doctor if you have swelling or infection of your eye.\nLatanoprost may cause a gradual change in the color of your eyes or eyelids and lashes, as well as increased growth or thickness of your eyelashes. These color changes, usually an increase in brown pigment, occur slowly and you may not notice them for months or years. Color changes may be permanent even after your treatment ends, and may occur only in the eye being treated. This could result in a cosmetic difference in eye or eyelash color from one eye to the other.\nFDA pregnancy category C. It is not known whether Latanoprost will harm an unborn baby. Tell your doctor if you are pregnant or plan to become pregnant while using this medication. It is not known whether latanoprost passes into breast milk or if it could harm a nursing baby. Do not use Latanoprost without telling your doctor if you are breast-feeding a baby.\nDo not use Latanoprost while wearing contact lenses. This medication may contain a preservative that can discolor soft contact lenses. Wait at least 15 minutes after using Latanoprost before putting your contact lenses in.\nUse Latanoprost exactly as prescribed by your doctor. Do not use in larger or smaller amounts or for longer than recommended. Follow the directions on your prescription label.\nWash your hands before using the Latanoprost eye drops.\nTo apply the eye drops:\nTilt your head back slightly and pull down your lower eyelid to create a small pocket. Hold the dropper above the eye with the tip down. Look up and away from the dropper as you squeeze out a drop, then close your eye.\nUse only the number of drops your doctor has prescribed.\nGently press your finger to the inside corner of the eye (near your nose) for about 1 minute to keep the liquid from draining into your tear duct.\nIf you use more than one drop in the same eye, wait about 5 minutes before putting in the next drop.\nWait at least 10 minutes before using any other eye drops that your doctor has prescribed.\nDo not allow the tip of the Latanoprost dropper to touch any surface, including your eyes or hands. If the dropper becomes contaminated it could cause an infection in your eye, which can lead to vision loss or serious damage to the eye. Tell your doctor right away if you have any eye injury or infection, or if you need to have any type of surgery, especially eye surgery. Do not use the eye drops if the liquid changes colors or has particles in it.\nStore an unopened bottle of Latanoprost eye drops in a refrigerator. Once you have opened and begun using the drops, you may store them at room temperature, away from moisture, heat, and light. Keep the bottle tightly closed when not in use.\nDo not use the Latanoprost eye drops if it has been longer than 6 weeks since you first opened the bottle. If you still need to use the medication, call your doctor for a new prescription.\nTake the missed dose as soon as you remember. Skip the missed dose if it is almost time for your next scheduled dose. Do not take extra medicine to make up the missed dose.\nAvoid using too much of Latanoprost, which can actually make it less effective in lowering the pressure inside the eye.\nDo not use other eye medications unless your doctor tells you to.\nGet emergency medical help if you have any of these signs of an allergic reaction to Latanoprost: hives; difficulty breathing; swelling of your face, lips, tongue, or throat. Stop using Latanoprost and call your doctor at once if you have any of these serious side effects:\nLess serious Latanoprost side effects may include:\nThere may be other drugs that can interact with Latanoprost. Tell your doctor about all medications you use. This includes prescription, over-the-counter, vitamin, and herbal products. Do not start a new medication without telling your doctor.\nStore at room temperature away from moisture and heat."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:1b4e15de-e739-4cde-8889-75720745f3cd>","<urn:uuid:157293c1-0ae9-4ed7-be57-1fa4eaf77e58>"],"error":null}
{"question":"What specific reforms has the Biden administration implemented regarding healthcare competition since 2021, and how do these changes affect information sharing practices in the healthcare industry?","answer":"The Biden administration directed several healthcare competition reforms through the 2021 Executive Order, including requiring HHS to develop a plan to prevent prescription drug price gouging, implementing over-the-counter hearing aid rules, standardizing insurance plans, and encouraging the FTC to ban 'pay for delay' agreements. Regarding information sharing, the DOJ's 2023 withdrawal of healthcare policy statements significantly impacted practices by removing established safety zones for information exchange. Healthcare industry participants must now navigate information sharing without clear guidance, as the DOJ deemed previous policies 'overly permissive' and shifted to a case-by-case enforcement approach rather than providing broad industry guidance.","context":["Another One Bites the Dust: DOJ Pulls 3 Policy Statements, Leaving Trade Associations Guessing\n- The Antitrust Division of the U.S. Department of Justice (DOJ) on Feb. 2, 2023, announced its withdrawal of antitrust policy statements applicable to healthcare markets, expressing a preference for a \"case-by-case\" enforcement approach over advance guidance for industry participants.\n- Withdrawal of the policy statements creates uncertainty for entities outside of the healthcare industry that had relied on an established antitrust \"safety zone\" when conducting industry benchmarking surveys.\n- Going forward, trade associations and others conducting surveys or supervising information exchanges among industry participants cannot depend on rigid adherence to the safety zone factors to avoid antitrust scrutiny. Before proceeding, trade associations will instead need to consider how, if at all, the proposed survey could reduce competition.\nDuring the course of the Biden Administration, the U.S. Department of Justice (DOJ) and Federal Trade Commission (FTC) have demonstrated an invigorated and sustained policy interest in increasing competition across various industries. Many high-profile actions have had an explicit focus on cracking down on mergers: the announcement of an Executive Order Promoting Competition in America (July 2021), the repeal of the Vertical Merger Guidelines (September 2021) and review of the Horizontal Merger Guidelines (January 2022). Other actions, such as the FTC announcement expanding the scope of liability under Section 5 of the FTC Act, have sought to increase the agencies' antitrust enforcement powers. The healthcare industry, in particular, has been a focus of the administration's enforcement activity, public statements, guidance revisions as well as a listening forum to gather feedback from consumers and industry participants. Combined with these changes, the FTC and DOJ have pursued aggressive litigation strategies in the healthcare industry among others seeking to push the boundaries of decades of antitrust law: criminal liability under Section 2 of the Sherman Act, challenging vertical mergers, and seeking to enjoin mergers based on speculative potential future harm to competition.\nWithdrawal of Policy Statements\nConsistent with these recent developments, on Feb. 2, 2023, the DOJ announced its withdrawal of three antitrust policy statements concerning healthcare markets: Department of Justice and FTC Antitrust Enforcement Policy Statements in the Health Care Area (Sept. 15, 1993); Statements of Antitrust Enforcement Policy in Health Care (Aug. 1, 1996); and Statement of Antitrust Enforcement Policy Regarding Accountable Care Organizations Participating in the Medicare Shared Savings Program (Oct. 20, 2011). Broadly speaking, these policy statements addressed a number of topics, including hospital mergers, hospital joint ventures involving high-technology or other expensive medical equipment, physicians' provision of information to purchasers of healthcare services, hospital participation in exchanges of price and cost information, healthcare providers' joint purchasing arrangements and physician network joint ventures.\nIn practice, these policy statements provided guidance to companies across the healthcare industry by outlining circumstances in which the government would or would not challenge transactions involving hospitals, physician group practices and other companies in the industry. Beyond the merger context, the policy statements also provided safe harbors and specific guidance as to conduct that did not run afoul of antitrust laws and that the DOJ would not seek to challenge in an enforcement action. Echoing earlier commentary on the need to modernize the federal antitrust enforcement regime, the DOJ's Assistant Attorney General Jonathan Kanter reiterated in comments concerning the withdrawal of the policy statements that \"[t]he healthcare industry has changed a lot since 1993, and the withdrawal of that era's out of date guidance is long overdue.\" These policy statements, according to the DOJ, are \"overly permissive on certain subjects, such as information sharing,\" and \"a case-by-case enforcement approach\" rather than broad guidance would allow it to better evaluate healthcare mergers and competition. The FTC is expected to make a similar announcement in coming weeks.\nInformation Exchanges: Prior Guidance and Potential Impact\nThe impact of the withdrawal of the DOJ's policy statements will be felt outside of the healthcare space because the safe harbors that they had laid out for the sharing of information among competitors had been adopted across industries. Since industry benchmarking surveys and other information exchanges \"can in certain circumstances increase economic efficiency and render markets more, rather than less competitive,\" United States. v. United States Gypsum Co., 438 U.S. 422, 441 n.16 (1978), the DOJ's policy statements provided welcome guidance to trade associations and others that sought to engage in procompetitive information sharing without crossing the line and facing a potential antitrust investigation or enforcement action. The DOJ's policy statements established a safety zone under which an information exchange would be considered reasonable if: 1) the exchange was managed by a third party, such as a trade association; 2) the information was more than three months old; and 3) at least five participants provided the data underlying each statistic shared, no single participant's data contributed more than a quarter of the amount of any statistic shared, and the shared statistics were sufficiently aggregated in a manner where no participant could discern the data of any other participant. The FTC also endorsed the same safety zone, providing clear guidance as to the antitrust agencies' enforcement intentions surrounding these routine and often procompetitive interactions.\nIn withdrawing the policy statements, the DOJ has replaced clear guidance with uncertainty. Trade associations regularly conduct information exchanges as part of their benchmarking and lobbying efforts. They had relied upon the existence of clear rules concerning how and when information exchanges would be permissible and structured their efforts to be compliant with the stated guidelines. Navigating the \"case-by-case\" enforcement approach now advocated by the DOJ in the absence of clear guidance will likely be a challenge. To best insulate themselves from antitrust scrutiny, trade associations should consider revisiting and adjusting core principles detailed below.\nInformation Sharing Takeaways\n1. Review and Rethink Existing Data Aggregation and Information Exchange Mechanisms\nTrade associations and other entities involved in benchmarking surveys and other information exchanges should, based on the DOJ's action, reevaluate their current processes for aggregating data and exchanging information with their members. The DOJ's withdrawal of its policy statements does not alter the fundamental reality that many information exchanges, if conducted to meet the safety zone requirements, would not harm competition or subject those involved in the information exchange to a likely antitrust investigation. However, trade associations and others can no longer rely rigidly on whether their proposed survey or information-sharing activity would meet the specific requirements of the now-withdrawn safety zone. Each decision to proceed with the collection and exchange of competitor information should consider whether the proposed survey or information sharing could be used to (or be perceived to be used to) reduce competition. Based on the DOJ's explanation for the withdrawal, companies planning a survey or information exchange should consider at least the following:\n- Is the data sufficiently historical? Based on the antitrust agencies' guidance, many trade associations have used a three-month look-back as the rule of thumb. Associations should consider whether, under present competitive conditions, data only three months old has lost its competitive significance or potential for misuse by industry participants. In the current enforcement environment, incorporating only older data would be prudent.\n- Is the type of anonymized data more likely to raise price-fixing concerns? Broadly speaking, information exchanges on pricing, costs, volumes of sales, terms of sales, capacity and production levels, customer/supplier-specific information and information relating to strategic plans are more likely to create concerns. Even in light of the policy changes outlined here, information exchanges related to nonproprietary technology, quality control issues and environmental or safety matters are likely to be less of a concern. Regardless of the type of data, it remains a best practice to ensure that only aggregated information is shared with competitors, and not the underlying data collected from them.\n- Is there an important reason for the information exchange? Using routine or informal meetings occurring multiple times per year as a venue for sharing aggregate data may no longer be considered appropriate. Trade associations should be thoughtful about how frequently information exchanges occur and consider whether such exchanges provide meaningful benefit.\n2. Update Antitrust Policies\nThe results of these internal assessments related to the aggregation of data and information should be memorialized into an updated policy. A successful antitrust policy is one that is both accurate and achievable. Detailing an aspirational practice that is cost-prohibitive is not an effective policy. For example, if a trade association is unable to involve an independent third party in their data aggregation practices, then the policy should not reflect that such practice is followed. However, if a trade association decides to extend their look-back period, their membership criteria or the scenarios in which information exchanges will take place, such information should be stated clearly and then followed consistently. Understand that an antitrust policy, like all policies, is a living document and should continue to be revisited as business practices and laws change.\n3. Training and Monitoring Member Compliance\nA bullet-proof antitrust compliance policy is of questionable merit if trade group members are not educated on its provisions and their corresponding compliance. While it has always been a good practice for a trade group to ensure that members are familiar with the applicable antitrust policy, investing in educating members is an even more worthy exercise in this enforcement environment. In addition to educating members on existing antitrust policies, industry trade associations should also consider the following:\n- starting meetings with an antitrust compliance reminder\n- keeping accurate and concise minutes of meetings\n- ensuring discussions do not deviate from approved agenda items into unplanned, informal discussion, and into any discussions in which participants disclose information about their prices, costs, margins, terms of sale, business strategies or wages\n- carefully overseeing the sharing of any business information concerning members that might be seen as competitively sensitive (or avoid such exchanges altogether) including limiting off-the-record conversations among competitors\n- creating a mechanism for reporting improper information sharing and ensuring any reports are addressed\nWhat the future holds for trade associations and others engaged in what might have been routine and benign information exchanges is not entirely clear. Only time will tell whether this move is a precursor to stricter enforcement from the DOJ related to trade associations, but these organizations should tread lightly and approach information exchanges with eyes wide open regarding the potential risks. The past few years have been tumultuous for healthcare industry participants seeking to navigate antitrust risk exposure.\nHolland & Knight's Antitrust Team, which includes former leaders in federal and state antitrust enforcement agencies, stands ready to assist clients as they navigate this new enforcement environment and consider steps that they can take to minimize potential risks this environment presents to their businesses. If you have any questions, please contact the authors of this alert.\nInformation contained in this alert is for the general education and knowledge of our readers. It is not designed to be, and should not be used as, the sole source of information when analyzing and resolving a legal problem, and it should not be substituted for legal advice, which relies on a specific factual analysis. Moreover, the laws of each jurisdiction are different and are constantly changing. This information is not intended to create, and receipt of it does not constitute, an attorney-client relationship. If you have specific questions regarding a particular fact situation, we urge you to consult the authors of this publication, your Holland & Knight representative or other competent legal counsel.","On 9 July 2021, President Joe Biden issued an Executive Order (\"Order\") and a supporting Fact Sheet announcing 72 initiatives to increase vigorous antitrust enforcement. The Order sets competition-law priorities for the Federal Trade Commission (FTC), the US Department of Justice (DOJ), and more than a dozen other federal agencies coordinated through a new White House Competition Council.\nThe Order also prioritizes certain conduct for antitrust enforcement, rulemaking and/or guidelines including employee non-compete clauses, occupational licensing requirements, sharing of wage data among employers, customer data collection, restraints on use of third-party product repair options, “pay-for-delay” pharma patent settlements, and standard-essential patent licensing.\nIt also calls for “greater scrutiny of mergers” through revisions to the DOJ/FTC Merger Guidelines and more aggressive enforcement for mergers involving internet platforms, hospitals, drug companies, banks, and others.\nThe Biden Administration claims this broad-reaching Order will benefit American consumers through increased competition across various sectors and reduced prices of essential goods. It mandates more aggressive merger enforcement by the antitrust agencies to address market power of large companies and recommends other regulatory and legislative changes to promote competition. The Order prioritizes the following key industries for scrutiny: Technology, Healthcare & Life Sciences, Banking and Finance, Transportation, Agriculture, Intellectual Property, Defense, and Real Estate. In addition, the Order focuses on labor markets, which touch upon all industries and sectors. The highlights of the Order are summarized below.\nThe Order encourages the FTC to increase enforcement in labor markets, with the following objectives:\n- Ban or limit non-compete agreements and other clauses that may unfairly limit worker mobility (for more specifics, see our recent blog post on this topic);\n- Ban unnecessary occupational licensing restrictions that impede economic mobility;\n- Strengthen guidance regarding collaboration by employers to prevent suppression of wages and reduction of benefits;\n- The Treasury Department must submit a report on the impact of the current lack of competition in labor markets within 180 days.\n- The Order encourages greater scrutiny of mergers by “dominant” internet platforms with particular attention to the “killer acquisitions” of nascent competitors or serial mergers;\n- The FTC is encouraged to issue rules regarding\n- Surveillance and accumulation of data;\n- Barring “unfair” competition on internet marketplaces; and\n- Anticompetitive restrictions on using independent, third party repair shops;\n- The Federal Communications Commission is encouraged to implement changes in pricing, fees, and transparency in internet services.\n- The Secretary of the Treasury must submit a report within 270 days “assessing the effects on competition of large technology firms’ and other non‑bank companies’ entry into consumer finance markets.”\nHealthcare & Life Sciences\nThe Order directs the agencies to tackle four areas of healthcare (prescription drugs, hospital consolidation, hearing aids, and insurance).\n- The DOJ and FTC are encouraged to review and revise their merger guidelines for hospital consolidations;\n- The FTC is encouraged to ban “pay for delay” settlement agreements between brand and generic drug manufacturers, also known as “reverse payments”;\n- The Food and Drug Administration is directed to work with state agencies to import prescription drugs from Canada;\n- The Health and Human Services Administration (HHS) will issue a comprehensive plan within 45 days to prevent price gouging and rising prescription drug prices;\n- HHS will increase support for generic and biosimilar drugs, improve transparency rules in hospital prices, and conclude the implementation of federal legislation to address hospital billing;\n- HHS to issue proposed rules within 120 days for allowing hearing aids to be sold over the counter;\n- HHS is directed to standardize insurance plan options in the National Health Insurance Marketplace.\n- DOJ is instructed to work with the Department of Transportation (DOT) to ensure competition in air transportation, particularly “the ability of new entrants to gain access;”\n- The DOT is directed to issue rules around fee structure, with respect to items such as refunds, baggage, change and cancellation fees;\n- The Federal Maritime Commission is encouraged to “vigorously enforce” against shippers who impose excessive charges against American exporters;\n- Railroad track owners shall be required to provide rights of way to passenger rail and increase obligations of fair treatment of other freight companies.\n- The FTC is encouraged to issue rules limiting equipment manufacturers from restricting people’s ability to use independent repair shops;\n- The FTC is directed, along with the US Department of Agriculture (USDA), to ensure that the intellectual property system does not reduce competition in seed and other input markets;\n- The FTC is ordered, along with the USDA, to report on the effect of retail concentration and retailers’ practices on the competition in the food industry (USDA must issue a plan to increase opportunities for farmers to access markets within 180 days and a report on the effect of retail concentration on competition in the food industries within 300 days);\n- The DOJ, FTC, and the Department of Treasury are directed to issue a report of threats to competition and barriers to new entry in the beer, wine, and spirits markets;\n- The USDA is directed to issue new rules for filing claims by farmers and to adopt anti-retaliation protections for farmer whistleblowers;\n- USDA is directed to issue new labeling rules for permitting the use of “Product of USA” on meat products.\nBanking & Financial Services\n- The DOJ and the banking agencies1 are encouraged to update their guidelines and provide more robust scrutiny of banking mergers (within 180 days);\n- The Consumer Financial Protection Bureau is encouraged to issue rules allowing customers to download their banking data.\nThe DOJ and Department of Commerce are encouraged to revise their position on the intersection of antitrust and intellectual property policy (including the previous administration’s Policy Statement on Remedies for Standards-Essential Patents Subject to Voluntary F/RAND Commitments issued in December 2019). This is based on concerns about the anticompetitive extension of market power “beyond the scope of granted patents,” and the protection of standard-setting processes.\nThe Department of Defense is directed to conduct a review of the state of competition within the defense industrial base and make recommendations for improving the solicitation process to promote greater competition (within 180 days).\nThe FTC is encouraged to issue a rule addressing unfair tying and other exclusionary practices in the brokerage or listing of real estate.\nImmediately after the Order’s signing, a joint Statement was published by Acting Assistant Attorney General of the DOJ Antitrust Division, Richard Powers, and the new FTC Chair, Lina Khan. The Statement declared that the agencies would be jointly launching a review of the merger guidelines to determine whether they are “overly permissive.” Indeed, the FTC recently approved several measures, including rulemaking changes and broad enforcement authorizations, to spur more enforcement.\nThe Order establishes a White House Competition Council, which will monitor the progress on and execution of these initiatives. It strongly encourages coordination among the agencies to tackle these objectives in both enforcement and policy development. Shortly after the issuance of the Order, the Attorney General of the United States, Merrick Garland, declared that the DOJ would immediately comply with the “whole of government approach” of the Order—pointing to healthcare, technology, and labor as three areas of regulatory focus and improved collaboration.\nWhile the Order and the related pronouncements are important and signal the direction of the Biden Administration’s antitrust policy, there remain hurdles to full implementation. In particular, some recommended actions may require a change in existing law. Several relevant legislative proposals are currently being discussed in Congress (e.g., Competition and Antitrust Law Enforcement Reform Act (CALERA), S. 225 or Platform Competition and Opportunity Act of 2021 (H.R. 3826)), but the likelihood and timing of legislative action is unclear.\nWe will continue to monitor the statements, actions, and initiatives undertaken in accordance with the Order and will provide pertinent updates. In the interim, we are available to respond to any specific questions you have about how the Order may affect your business.\n1 The Federal Reserve, the Federal Deposit Insurance Corporation, and the Office of the Comptroller of the Currency"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:12d3c33b-950e-4bd1-b219-03d49757e991>","<urn:uuid:c90670ac-2b67-4172-b17a-d130b1d34cb1>"],"error":null}
{"question":"What are the dimensions of the La Laiterie hall and its speaker setup?","answer":"The La Laiterie hall is 23 meters long and 9 meters high (75x30 ft), excluding mezzanines. The speaker setup consists of 32 Amadeus PMX 5 coaxial speakers integrated at inferior purlins at mezzanine height, arranged with ten speakers spaced lengthwise and six across the width. Eight Amadeus ML 12 SLIM subwoofers are built into the low section of the walls, with four speakers on each side lengthwise. Additionally, UFD 215 slim bass speakers are installed under each mezzanine.","context":["Paris, France – July 15, 2019 – Domaine des Étangs is the biggest 5-star ‘hotel resort’ in France. Located in Massignac, it spans a thousand acres of unspoiled natural land including forests, pastures, and ponds. The Domaine des Étangs (‘Pond Domain’) offers outstanding hospitality experiences. This unique site hosts an eleventh-century castle, fully renovated and decorated with contemporary works of art, detailed landscaping, six tenant farms, a former dairy (La Laiterie), a vegetable garden, a wellness center, Gallo-Roman baths, two swimming pools, a floating tennis court, and a top-rated gastronomic Michelin-starred restaurant. Built of stone and wood, La Laiterie is a new gallery space within the Domaine des Étangs complex that is dedicated to exhibitions about the ‘Meeting of Art and Nature’.\n“As we collaborated in advance of the sound system installation with Amadeus experts, we were able to get rid of the speakers’ common aesthetic nuisances and at the same time ensure a sound quality at a level rivaling the most famous concert halls,” said Manuel Gomez, architect and music lover, who was in charge of the renovation work for La Laiterie at the Domaine des Étangs.\nDesigned in a circular layout to get the best envelopment for visitors and to adapt to the geometric, technical, and aesthetic constraints in the Laiterie galley, the sound system comprises 44 speakers made by Amadeus and the HOLOPHONIX spatial sound processor.\n“The specification was very complex. Mrs. Garance Primat, the owner of the Domaine des Étangs, is a great music lover and a contemporary art collector; she wished to turn this former 18th-century barn into a place where past, present, and future art would meet to serve culture and presentation,” said Gaëtan Byk, Marketing Manager for the Amadeus brand.\n(photo by Arthur Pequin)\n“This space had to be able to host art exhibitions, events, acoustic or amplified music, electroacoustic music creations, etc., without having to modify, reconfigure or move speakers according to the application. The sound system had to be as versatile, scalable and integrated as possible. It appeared obvious to develop our project around the HOLOPHONIX processor,” revealed Byk.\nMezzanines excluded, the hall is 23-meter long and 9-meter high (75x30 ft). Thirty-two (32) Amadeus PMX 5 coaxial speakers are integrated at inferior purlins at mezzanine height: ten evenly-spaced speakers lengthwise and six speakers across the width.\nEight low-frequency Amadeus ML 12 SLIM bespoke subwoofer speakers are integrated into the low section of the walls, spread out with four speakers on each side lengthwise along the walls. In addition, a new purpose-designed ‘slim bass’ speaker, named UFD 215, is integrated under each mezzanine.\nThe audio system is built around the spatial sound processor HOLOPHONIX. The integrated hardware and software product was designed by Amadeus in collaboration with the STMS (Sciences et Technologies de la Musique et du Son), a laboratory founded in 1995 and hosted in the IRCAM premises, in association with CNRS, the Sorbonne Université, the Ministère de la Culture, and the Institut de Recherche et Coordination Acoustique/Musique.\nThe HOLOPHONIX processor ensures a specified coherency between all speakers, each one receiving a dedicated audio signal. Several standard presets for HOLOPHONIX were designed on location according to the rooms’ specifications to enhance the system versatility and adaptability according to the needs of the installation or event.\nThe basic audio configuration uses three virtual sources, and synthesizes the signal on all loudspeakers, according to the WFS (Wave Field Synthesis) principles. The first source is stereophonic, it processes the output signal of the Lumin U1 player/streamer, used to play high-resolution music tracks.\n“Most audio signals come from sources like TIDAL or Qobuz. They are played in native PCM format, with sample frequencies of 44.1 to 192kHz, with 16-bit or 24-bit resolution. The AES/EBU digital audio signal is then ‘transcoded’ in Dante, to preserve the digital chain and processed at 192kHz – the HOLOPHONIX processor is set up to work with this sampling frequency, within a 64 I/O limit,” explains Michel Deluc, Amadeus’ R&D Manager.\nThe two other sources for HOLOPHONIX are monophonic, they synthesize the audio signal coming from Shure ULXD4D digital wireless microphone receivers. They can easily be moved in real time, to follow the actual position of the persons using the microphones.\nThe HOLOPHONIX processor creates an extremely advanced platform which is able to mix, reverberate, and spatialize sound contents played from various devices using several different spatialization techniques in two or three dimensions.\nThe hardware offers a quasi-unlimited number of spatialization buses, each one able to run one of the different sound algorithms available, including: Higher-Order Ambisonics (2D, 3D) Vector-Base Intensity Panning (2D, 3D), Vector-Base Amplitude Panning (2D, 3D), Wave Field Synthesis, Angular 2D, k-Nearest Neighbor, Stereo Panning, Stereo AB, Stereo XY, and Binaural.\nList of Amadeus products installed in the “Laiterie” gallery space:\n2 x Amadeus PMX 15 (1 x 15’’ LF; 1 x 3’’ HF)\n2 x Amadeus UDF 215 (2 x 15’’ LF)\n32 x Amadeus PMX 5 MK4 (1 x 5.25’’ LF; 1 x 1.75’’ HF)\n8 x Amadeus ML 12 SLIM (1 x 12’’ LF)\nList of Powersoft items installed in the “Laiterie” gallery space:\n4 x Powersoft Ottocanali 4K4 DSP+DANTE\n4 x Powersoft Ottocanali 8K4 DSP+DANTE\nAmadeus develops, manufactures and sells a wide range of high-end sound reinforcement loudspeakers, studio monitors and various signal processing interfaces that combine its own custom innovative technology with stylish design, for the professional audio industry. Created in 1992 from the collaboration between the French designer Bernard Byk and the scientist and musician Michel Deluc, the Amadeus brand has grown to become a leading supplier of high-end audio solutions and services for theaters, opera houses, museums, recording studios, research centers and touring concert sound rental operations. All development and product manufacturing is centralized in Amadeus headquarters in France, with field offices and authorized distributors located in Europe, Asia, and UAE.\nAll rights reserved. All other registered trademarks are the property of their respective owners."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:dd82b4ea-d314-436d-b02a-2abac4414508>"],"error":null}
{"question":"How do audio monitoring setups compare between Logic Pro X and Final Cut Pro?","answer":"Final Cut Pro requires connecting self-powered speakers by connecting the main left and right audio outputs to corresponding speakers, and while it can't edit speaker assignments, it supports 5.1-channel surround sound monitoring. For Logic Pro X, monitoring focuses on bit resolution and sampling frequency settings, with options like 16/24-bit resolution and sampling rates up to 192 kHz depending on the sound card capabilities.","context":["Final Cut Pro: Understanding External Audio Monitors\nThe first step to understanding and manipulating audio in Final Cut Pro is to know how to properly connect the external audio monitors to your editing system. When you connect your external audio monitors or your speakers, always make sure that they are properly connected to the built-in computer audio output or audio interface. It is also important that the audio interface is properly configured in Final Cut Pro for you to maximize the use of this program.\nStep 1: Setting up the Audio Monitoring Environment\nAside from having a good quality or high performance speaker, it is also important that you have a good audio monitoring environment. The shape as well as the material of the room is important.\nStep 2: Connecting Self-Powered Speakers\nTo connect self-powered speakers to your computer, you first need to connect the main left audio output of your audio interface to the left speaker. After connecting the left audio output, you then need to connect the main right output of the audio interface to the right speaker.\nStep 3: Configuring Sequence Audio Outputs\nAlthough Final Cut Pro is unable to support editing of speaker assignments or multi-channel surround sound mixing, you can configure your system so that you can monitor certain types of 5.1-channel surround sound files.\nThere are 2 ways you can configure your sequence audio outputs. The first is to use the 5.1 monitoring audio preset. This way is suitable if you already know that you will be dealing with a 5.1-channel surround sound audio file in your sequence. All you need to do is just assign to your sequence the 5.1-Monitoring audio preset.\nThe second way you can configure your audio outputs to 5.1-channel surround sound audio files is to use the Match Audio Outputs command in Final Cut Pro. This will help you automatically configure the sequence audio outputs.\nStep 4: Configuring 5.1-Channel Surround Sound Monitoring\nTo configure 5.1-channel surround sound monitoring using the Media Audio Outputs command, you first need to import a multichannel surround sound audio file to Final Cut Pro. After importing the multichannel surround sound QuickTime audio file, edit the multichannel clip in the Timeline. You then need to choose Sequence and then Match Audio Outputs. You will see a notification stating that changes will be made to the audio outputs. Just click OK.\nOnce you see an alert stating that the changes have been made, verify the audio output assignment of each track. To do this, just control-click the Auto Select control or Lock Track control of the track. After this is done, you can now connect your 5.1-channel surround sound speaker system.\nStep 5: Setting the Muting the Sound System and Monitoring Levels\nMixing audio often requires you to monitor a consistent volume setting. It is important to remember that any changes to the volume should be done through Final Cut Pro and not through the volume setting of the speaker or the external audio outputs itself. However, if your entire audio track needs some adjustment, you may need to make adjustments on the overall volume setting of your audio.Popular P&S Cameras for High Quality Photos:","I received an email asking me how to choose the resolution in Logic Pro X.\n16 or 24 bit resolution?\nBefore recording your guitar or voice in Logic, you can choose in the Preferences menu, Recording what is called the recording resolution. I already talked in a previous article about Logic’s audio recording formats.\nMost sound cards offer 16 or 24-bit resolution and sampling rates up to 192 kHz. Below is the yellow resolution of my Universal Audio Apollo 16 card.\nIf your sound card only offers 16 bits / 44.1 kHz. For example, you will not be able to choose another resolution in your sequencer. Because Logic Pro X will determine the resolution of your sound card!\nWhy record with 24-bit resolution?\nWe know that a CD is in 16-bit / 44.1 kHz format. So why choose to record with a 24-bit resolution?\nSimply because it offers you a wider dynamic range. Thanks to this wider dynamic range, this allows you to record less close to 0 dB. The 0 dB being a value not to be exceeded in digital under the risk of non-musical saturation.\nTo better understand….\nThe signal-to-noise ratio is an indicator of the quality of information transmission. It is the ratio of powers between:\n- the signal of maximum amplitude for which the distortion at the output remains below a limit value.\n- background noise, which is not significant information that generally corresponds to the signal present at the output of the device. In the absence of a signal at the input.\nIt is generally measured in decibels (dB).\nLet’s consider that 1 bit corresponds to about 6 dB, I mean about ! So when calculating, 16 bits allow a dynamic range of 16 x 6 = 96 dB approximately. Because in reality we are closer to 98 dB.\nSo 24 bits allow 24 x 6 = 144 dB of dynamics. With 24-bit, you earn 44dB of margin. It is therefore a significant comfort! Since you no longer have to try to get absolutely close to 0 dB to get better quality.\nYou can thus gain in dynamics. This does not prevent the use of a console slice. For the color of its preamp, its equalization to refine frequencies during recording. Or its compressor to reduce peaks by 2/3 dB.\nEQ and compression should be applied slightly, but they are not necessary! This is one of the practical aspects of Apollo sound cards. Apply in the mixer an EQ + Compression treatment (perso I use a lot of Cambridge EQ and LA2A or 1176LN compressors). With the UAD Rec option to save the processing in the Logic audio file. And / or UAD My only for the monitoring and comfort of the artist…. Processing chain obviously reusable in Logic on playback if it has not been applied to the recording…. Sorry, I’m getting off topic….\nWhy do some software have 32-bit resolutions?\nThe resolution of your sound card will not exceed 24 bits. There is no sound card that has a 32-bit resolution. So your 24-bit recordings will be encoded in a 32-bit floating point format by some software. This will not change the quality of the initial records themselves. But will create audio files in this format before they are processed by plug-ins.\nMy Mac works in 64 bits!\nThe computing resolution of your computer’s processor is probably 64-bit. This has no influence on the resolution of the 24-bit audio file recorded. This brings you more processing with your plug-ins and increased speed of your computer…\nWhat about the sampling frequency?\nThe sampling frequency is the number of samples recorded per second:\n- 44.1 kHz = at 44100 samples per second\n- 48 kHz = at 48000 samples per second\nthe debate is more open. A frequency of 44.1 kHz is theoretically sufficient to recover the entire spectrum audible by the human ear. Now, some people perceive other things when recording in 96 or 192 kHz. Only your ear is the judge personally I work in 24 bits 44.1kHz. Simply the higher the files are in terms of sampling frequency, the more space they take up on the hard disk. This requires more processor resources for their processing through your plug-in chains. I prefer to save space on my hard disk by staying at 44.1 kHz. Knowing that very often I deliver a wav and mp3 file….\nFor your information, I made a 96 kHz recording once in my career. For SYLVANIA’s Marcus OPUS 1 album. A classical guitar album. My client’s request was to get closer to the quality of Alexandre LAGOYA’s recordings. I must admit that I had a very beautiful experience….\nThis is in Logic where you set the sampling rate.\nPerform a blind test with a file recorded at 44.1, 48 and 96 KHz. Ask a friend to do this test! Without knowing the frequencies to avoid being influenced. Maybe the difference will not exist in your ears.\nSound card and sound card!\nChoosing the resolution in Logic Pro X requires quality! The converters play on the quality of the retranscription of the analog signal into digital. 2 sound cards of different brands working in 24 bits/96 kHz will not necessarily give the same rendering. This depends on the quality of the converters.\nIf we’re in 16 bits and we want to go to 24 bits. Logic will add 8 bits in the form of zero, this does not change the sound quality of the file. If recorded in 24 bits 44.1kHz to make an audio CD. We are obliged to lower the 24-bit resolution to 16-bit resolution. You have to go through a dithering algorithm. The algorithm is independent of the quality of the sound card, it is different according to the sequencers. In Logic Pro X, we have access as shown in the image below to different dithering algorithms.\n- POWr for Psychoacoustically Optimized Wordlength Reduction\n- POW-r #1 Dithering: uses a special dithering curve to minimize the noise induced by quantification.\n- POW-r #1 Noise Shaping: uses an additional process to shape sound over a wide frequency range. This extends the dynamic range of the file from 5 to 10 dB.\n- POW-r #1 Noise Shaping: uses an additional and optimized sound shaping process to extend the dynamic range by 20 dB within a range of 2 to 4 kHz (Range to which the human ear is most sensitive).\n- UV22HR: adds an inaudible energy concentration, generated algorithmically, around 22 kHz. Technically, it is known as “Sub-Nyquist-band dither”. It is a proprietary dithering to the Apogee brand.\nIn the image above, UV22HR appears grayed out because I don’t have an Apogee card.\nNow you know how to choose the resolution in Logic Pro X!\nI hope these explanations have helped you understand the importance of choosing the right resolution before making your recordings. And finally, the resolution setting in Logic is permanent. Unlike sampling frequency, which can vary from one project to another. In what resolution now will I receive your audio tracks to make your mix or mastering?"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:93230a16-7d38-4bd6-88cf-1b5ac14c377d>","<urn:uuid:b54640a7-1581-4cb1-a159-316cec7b689f>"],"error":null}
{"question":"I'm writing a research paper on German language variations. What are the key differences between Austrian and Swiss Standard German in terms of writing conventions and their roles in everyday communication?","answer":"Austrian and Swiss Standard German show distinct differences in both writing conventions and everyday usage. In writing, Swiss Standard German notably replaces the German ligature ß with ss (e.g., Strasse instead of Straße), while Austrian German maintains the ß usage. As for daily communication, Austrian German serves as the variation used in media and formal situations, with strong everyday presence. In contrast, Swiss Standard German is mainly restricted to written forms and very specific formal spoken situations like news broadcasts and parliament sessions. In Switzerland, the respective local Swiss German dialects are the default spoken language in almost all informal situations, creating a clear separation between written and spoken forms. This differs from Austria, where Austrian German maintains higher sociolinguistic prestige and is used more broadly in formal spoken contexts.","context":["||This article includes a list of references, but its sources remain unclear because it has insufficient inline citations. (February 2012)|\n|Standard Austrian German\nAustrian High German\n|Österreichisches Standarddeutsch, Österreichisches Hochdeutsch|\n|Pronunciation||[ˈøːstɐʁaɪ̯çɪʃəs ˈʃtandaʁtdɔʏtʃ] (or [-ˈstan-])\nAustrian German (German: Österreichisches Deutsch), Austrian Standard German, Standard Austrian German (German: Österreichisches Standarddeutsch) or Austrian High German (German: Österreichisches Hochdeutsch), is the variety of Standard German written and spoken in Austria and North Italy. It has the highest sociolinguistic prestige locally, as it is the variation used in the media and for other formal situations.\nAustrian German has its beginning in the mid-18th century, when empress Maria Theresa and her son Joseph II introduced compulsory schooling (in 1774) and several reforms of administration in their multilingual Habsburg empire. At the time, the written standard was Oberdeutsche Schreibsprache, which was highly influenced by the Bavarian and Alemannic dialects of Austria. Another option was to create a new standard based on the Southern German dialects, as proposed by the linguist Janez Žiga Popovič (Johann Siegmund Popowitsch). Instead they decided for pragmatic reasons to adopt the already standardized Chancellery language of Saxony (Sächsische Kanzleisprache or Meißner Kanzleideutsch), which was based on the administrative language of the non-Austrian area of Meißen and Dresden. Thus Standard Austrian German has the same geographic origin as the Standard German of Germany (Bundesdeutsches Hochdeutsch, also Deutschländisches Deutsch) and Swiss High German (Schweizer Hochdeutsch, not to be confused with the Alemannic Swiss German dialects).\nThe process of introducing the new written standard was led by Joseph von Sonnenfels. Since 1951 the standardized form of Austrian German for official texts and schools is defined by the Austrian Dictionary (Österreichisches Wörterbuch), published under the authority of the Austrian Federal Ministry of Education, Arts and Culture.\nGeneral situation of German\nAs German is a pluricentric language, Austrian German is merely one among several varieties of Standard German. Much like the relationship between British English and American English, the German varieties differ in minor respects (e.g., spelling, word usage and grammar) but are recognizably equivalent and largely mutually intelligible.\nStandard German in Austria\nThe official Austrian dictionary, das Österreichische Wörterbuch, prescribes grammatical and spelling rules defining the official language. The 1996 spelling reform is somewhat practiced in Austria. The \"sharp s\" (ß) is used in Austria, as in Germany.\nWith German's being a pluricentric language, German dialects in Austria should not be confused with the variety of Standard German spoken by most Austrians, which is distinct from that of Germany or Switzerland. Distinctions in vocabulary persist, for example, in culinary terms, where communication with Germans is frequently difficult, and administrative and legal language, which is due to Austria's exclusion from the development of a German nation-state in the late 19th century and its manifold particular traditions. A comprehensive collection of Austrian-German legal, administrative and economic terms is offered in Markhardt, Heidemarie: Wörterbuch der österreichischen Rechts-, Wirtschafts- und Verwaltungsterminologie (Peter Lang, 2006).\nFormer spoken standard\nThe \"former standard\", used for about 300 years or more in speech in refined language, was the Schönbrunner Deutsch, a sociolect spoken by the imperial Habsburg family and the nobility of Austria-Hungary. It differed from other dialects in vocabulary and pronunciation; it appears to have been spoken with a slight degree of nasality. This was not a standard in a modern technical sense, as it was just the social standard of upper-class speech.\nSpecial written forms\nFor many years, Austria had a special form of the language for official government documents. This form is known as Österreichische Kanzleisprache, or \"Austrian chancellery language\". It is a very traditional form of the language, probably derived from medieval deeds and documents, and has a very complicated structure and vocabulary generally reserved for such documents. For most speakers (even native speakers), this form of the language is generally difficult to understand, as it contains many highly specialised terms for diplomatic, internal, official, and military matters. There are no regional variations, because this special written form has mainly been used by a government that has now for centuries been based in Vienna.\nÖsterreichische Kanzleisprache is now used less and less, thanks to various administrative reforms that reduced the number of traditional civil servants (Beamter). As a result, Standard German is replacing it in government and administrative texts.\nWhen Austria became a member of the European Union, the Austrian variety of the German language — limited to 23 agricultural terms — was \"protected\" in Protocol no. 10, regarding the use of Austrian-specific terms in the framework of the European Union, which forms part of the Austrian EU accession treaty. Austrian German is the only variety of a pluricentric language recognized under international law or EU primary law. All facts concerning “Protocol no. 10” are documented in Markhardt's Das österreichische Deutsch im Rahmen der EU, Peter Lang, 2005.\nIn Austria, as in the German-speaking parts of Switzerland and in southern Germany, verbs that express a state tend to use sein as the auxiliary verb in the perfect, as well as verbs of movement. Verbs which fall into this category include sitzen (to sit), liegen (to lie) and, in parts of Carinthia, schlafen (to sleep). Therefore, the perfect of these verbs would be ich bin gesessen, ich bin gelegen and ich bin geschlafen respectively (note: ich bin geschlafen is a rarely used form, more commonly ich habe geschlafen is used).\nIn Germany, the words stehen (to stand) and gestehen (to confess) are identical in the present perfect: habe gestanden. The Austrian variant avoids this potential ambiguity (bin gestanden from stehen, \"to stand\"; and habe gestanden from gestehen, \"to confess\").\nThere are many official terms that differ in Austrian German from their usage in most parts of Germany. Words primarily used in Austria are Jänner (January) rather than Januar, heuer (this year) rather than dieses Jahr, Stiege (stairs) instead of Treppe, Rauchfang (chimney) instead of Schornstein, many administrative, legal and political terms – and a whole series of foods such as: Erdäpfel (potatoes) German Kartoffeln (but Dutch Aardappel), Schlagobers (whipped cream) German Schlagsahne, Faschiertes (ground beef) German Hackfleisch (but Hungarian fasírt, Slovenian informal faširano), Fisolen (green beans) German Gartenbohnen (but Czech fazole, Italian fagioli, Slovenian fižol, Hungarian folkish paszuly), Karfiol (cauliflower) German Blumenkohl (but Hungarian and Slovak karfiol, Italian cavolfiore), Kohlsprossen (Brussels sprouts) German Rosenkohl, Marillen (apricots) German Aprikosen (but Slovak marhuľa, Polish morela, Slovenian marelice, Croatian marelica), Paradeiser (tomatoes) German Tomaten (but Hungarian paradicsom, Slovak paradajka, Slovenian paradižnik, Serbian paradajz), Palatschinken (pancakes) German Pfannkuchen (but Czech palačinky, Hungarian palacsinta, Slovenian palačinke), Topfen (a semi-sweet cottage cheese) German Quark and Kren (horseradish) German Meerrettich (but Czech křen, Slovak chren, Slovenian hren, etc.).\nThere are, however, some false friends between the two regional varieties:\n- Kasten (wardrobe) instead of Schrank, as opposed to Kiste (box) instead of Kasten. Kiste in Germany means both \"box\" and \"chest\".\n- Sessel (chair) instead of Stuhl. Sessel means \"easy chair\" in Germany and Stuhl means \"stool (faeces)\" in both varieties.\n- Vorzimmer (hall[way]) instead of Diele. Vorzimmer means \"antechamber\" in Germany\n- Ofen (oven) instead of Kamin. Kamin is Schornstein in Germany\n- Dialects of the Austro-Bavarian group, which also comprises the dialects of German Bavaria\n- Central Austro-Bavarian (along the main rivers Isar and Danube, spoken in the northern parts of the State of Salzburg, Upper Austria, Lower Austria, and northern Burgenland)\n- Southern Austro-Bavarian (in Tyrol, South Tyrol, Carinthia, Styria, and the southern parts of Salzburg and Burgenland)\n- Vorarlbergerisch, spoken in Vorarlberg, is a High Alemannic dialect.\nIn addition to the standard variety, in everyday life most Austrians speak one of a number of Upper German dialects.\nWhile strong forms of the various dialects are not fully mutually intelligible to northern Germans, communication is much easier in Bavaria, especially rural areas, where the Bavarian dialect still predominates as the mother tongue. The Central Austro-Bavarian dialects are more intelligible to speakers of Standard German than the Southern Austro-Bavarian dialects of Tyrol.\nViennese, the Austro-Bavarian dialect of Vienna, is most frequently used in Germany to do an impression of the typical inhabitant of Austria. The people of Graz, the capital of Styria, speak yet another dialect which is not very Styrian and more easily understood by people from other parts of Austria than other Styrian dialects, for example from western Styria.\nSimple words in the various dialects are very similar, but pronunciation is distinct for each and, after listening to a few spoken words, it may be possible for an Austrian to realise which dialect is being spoken. However, in regard to the dialects of the deeper valleys of the Tirol, other Tyroleans are often unable to understand them. Speakers from the different states of Austria can easily be distinguished from each other by their particular accents (probably more so than Bavarians), those of Carinthia, Styria, Vienna, Upper Austria, and the Tyrol being very characteristic. Speakers from those regions, even those speaking Standard German, can usually be easily identified by their accent, even by an untrained listener.\nSeveral of the dialects have been influenced by contact with non-Germanic linguistic groups, such as the dialect of Carinthia, where in the past many speakers were bilingual with Slovene, and the dialect of Vienna, which has been influenced by immigration during the Austro-Hungarian period, particularly from what is today the Czech Republic. The German dialects of South Tyrol have been influenced by local Romance languages, particularly noticeable with the many loanwords from Italian and Ladin.\nInterestingly, the geographic borderlines between the different accents (isoglosses) coincide strongly with the borders of the states and also with the border with Bavaria, with Bavarians having a markedly different rhythm of speech in spite of the linguistic similarities.\n- de-AT is an IETF language tag that conforms with the current specification BCP 47 Language Tags (where de-AT happens to be mentioned explicitly). It is often used, for instance in major operating systems (e.g. , )\n- \"The problems of Austrian German in Europe\". euro|topics. 16 March 2006. Retrieved 2015-05-13.\n- Russ (1994:7, 61–65, 69, 70)\n- Sanders, Ruth H. (2010), German: Biography of a Language: Biography of a Language, New York: Oxford University Press, Inc., pp. 197–198, ISBN 978-0-19-538845-9\n- Moosmüller, Sylvia (2007), Vowels in Standard Austrian German: An Acoustic-Phonetic and Phonological Analysis (PDF), retrieved May 13, 2015\n- Perfetti, Charles A.; Rieben, Laurence; Fayol, Michel, eds. (1997), Learning to Spell: Research, Theory, and Practice Across Languages, Lawrence Erlbaum Associates, p. 88, ISBN 1-4106-0458-6\n- Tedesco austriaco\n- Here are some examples of Schönbrunner Deutsch:\n- \"Documents concerning the accession of the Republic of Austria, the Kingdom of Sweden, the Republic of Finland and the Kingdom of Norway to the European Union\". European Commission. 29 August 1994. p. 370. Retrieved 24 October 2015.\nThe specific Austrian terms of the German language contained in the Austrian legal order and listed in the Annex to this Protocol shall have the same status and may be used with the same legal effect as the corresponding terms used in Germany listed in that Annex.\n- Otto Back, Erich Benedikt, Karl Blüml, et al.: Österreichisches Wörterbuch (neue Rechtschreibung). Herausgegeben im Auftrag des Bundesministeriums für Unterricht, Kunst und Kultur. Auf der Grundlage des amtlichen Regelwerks. 41. circulation, Österreichischer Bundesverlag, Wien 2009, ISBN 978-3-209-06875-0\nReferences and further reading\n- Ammon, Ulrich: Die deutsche Sprache in Deutschland, Österreich und der Schweiz: Das Problem der nationalen Varietäten. de Gruyter, Berlin/New York 1995.\n- Ammon, Ulrich / Hans Bickel, Jakob Ebner u. a.: Variantenwörterbuch des Deutschen. Die Standardsprache in Österreich, der Schweiz und Deutschland sowie in Liechtenstein, Luxemburg, Ostbelgien und Südtirol. Berlin/New York 2004, ISBN 3-11-016574-0.\n- Grzega, Joachim: „Deutschländisch und Österreichisches Deutsch: Mehr Unterschiede als nur in Wortschatz und Aussprache.“ In: Joachim Grzega: Sprachwissenschaft ohne Fachchinesisch. Shaker, Aachen 2001, S. 7-26. ISBN 3-8265-8826-6.\n- Grzega, Joachim: “On the Description of National Varieties: Examples from (German and Austrian) German and (English and American) English.” In: Linguistik Online 7 (2000).\n- Grzega, Joachim: “Nonchalance als Merkmal des Österreichischen Deutsch.” In: Muttersprache 113 (2003): 242-254.\n- Krech, Eva Maria; Stock, Eberhard; Hirschfeld, Ursula; Anders, Lutz-Christian (2009), \"Die Standardaussprache in Österreich\", Deutsches Aussprachewörterbuch, Berlin, New York: Walter de Gruyter, ISBN 978-3-11-018202-6\n- Muhr, Rudolf / Schrodt, Richard: Österreichisches Deutsch und andere nationale Varietäten plurizentrischer Sprachen in Europa. Wien, 1997\n- Muhr, Rudolf/Schrodt, Richard/Wiesinger, Peter (eds.): Österreichisches Deutsch: Linguistische, sozialpsychologische und sprachpolitische Aspekte einer nationalen Variante des Deutschen. Wien, 1995.\n- Pohl, Heinz Dieter: „Österreichische Identität und österreichisches Deutsch“ aus dem „Kärntner Jahrbuch für Politik 1999“\n- Russ, Charles (1994), The German Language Today: A Linguistic Introduction, London: Routledge, ISBN 0-203-42577-4\n- Wiesinger, Peter: Die deutsche Sprache in Österreich. Eine Einführung, In: Wiesinger (Hg.): Das österreichische Deutsch. Schriften zur deutschen Sprache. Band 12. (Wien, Köln, Graz, 1988, Verlag, Böhlau)","Swiss Standard German (German: Schweizer Standarddeutsch), or Swiss High German (German: Schweizer Hochdeutsch or Schweizerhochdeutsch), referred to by the Swiss as Schriftdeutsch, or Hochdeutsch, is the written form of one of four official languages in Switzerland, besides French, Italian and Romansh. It is a variety of Standard German, used in the German-speaking part of Switzerland. It is mainly written, and rather less often spoken.\nSwiss Standard German is not a German dialect, but a variety of standard German. It is not to be confused with Swiss German, an umbrella term for the various Alemannic German dialects (in the sense of \"traditional regional varieties\") that are the default everyday languages in German-speaking Switzerland.\nGerman is a pluricentric language. In contrast with other local varieties of German, Swiss Standard German has distinctive features in all linguistic domains: not only in phonology, but also in vocabulary, syntax, morphology and orthography. These characteristics of Swiss Standard German are called Helvetisms. Besides influences from Alemannic German, those characteristics include extensive use of loan words from Romance languages, especially French.\nWritten Swiss Standard German\nSwiss Standard German (SSG) is the official written language in German-speaking Switzerland. It is used in books, all official publications (including all laws and regulations), in newspapers, printed notices, most advertising and in other printed matter. Authors write literature mainly using Swiss Standard German; some dialect literature exists. SSG is similar in most respects to the Standard German in Germany and Austria; there are a few differences in spelling, most notably the replacing of the German ligature ß with ss. For example:\n- Strasse = Straße (Germany) = street\nThere are some differences in vocabulary, including, for instance, using a loanword from another language. For example:\n- Billett (from French) = Fahrkarte (Germany) = ticket (for bus/tram/train etc.)\n- Führerausweis or Billet (colloquial) = Führerschein (Germany) = driving licence\n- Velo (from French) = Fahrrad (Germany) = bicycle\n- Natel or Handy = Handy/Mobiltelefon (Germany) = mobile phone\n- parkieren = parken (Germany) = to park\n- Poulet (from French) = Hähnchen (Germany) = chicken\nIn addition, SSG uses different orthography in letter writing, and the salutations used for the same also differ from Standard German.\nThe Swiss use the Swiss Standard German word Lernfahrausweis for a learner's driving permit (note how it differs from the SSG word for a \"regular\" driving license: Führerausweis).\nThe Swiss use the Standard German word Spital (hospital). Spital is also found in volumes of Standard German language dictionaries; however, Germans from northern Germany prefer to use Krankenhaus, whereas Spital is also used in areas of southern Germany, Austria, and Liechtenstein.\nThere are differences in gender for some nouns:\n- de-ch: das Tram (neuter); de: die Tram (feminine) (Straßenbahn is used more frequently in Germany); en: tram\n- de-ch: das E-Mail (neuter); de: die E-Mail (feminine); en: e-mail\nSome expressions are borrowed from French and thus differ from usage in Germany, such as\n- de-ch: ich habe kalt (literally \"I have cold\"), de: mir ist [es] kalt (literally \"[it] is cold to me\")\n- de-ch: das geht dir gut, de: das passt dir gut (it suits you)\nThe Swiss keyboard layout has no ß key, nor does it have the capital umlaut keys Ä, Ö and Ü. This dates back to mechanical typewriters that had the French diacritical marks letters on these keys to allow the Swiss to write French on a Swiss German QWERTZ keyboard (and vice versa). Thus a Swiss German VSM keyboard has an ä key that prints an à (a-grave) when shifted. However, it is possible to write uppercase umlauts by use of caps lock or by using the ¨ dead key.\nThe names of municipalities, towns, stations, and streets are often not written with a starting capital umlaut, but instead with Ae, Oe and Ue, such as the Zürich suburb Oerlikon, or the hamlet Aetzikofen, or the Bernese municipality Uebeschi. However, field names, such as Äbenegg, Ötikon (near Stäfa), or Überthal, and any other word, such as Ärzte (English: physicians), usually start with capital umlauts.\nAs for the various dialects of Swiss German, they are occasionally written, but their written usage is mostly restricted to informal situations such as private text messages, e-mails, letters, notes, or within social media such as Facebook. The ability of German Swiss to transliterate their language into writing is an integral and important part of the identity and culture of German-speaking Switzerland.\nSpoken Swiss Standard German\nThe default spoken language in German-speaking Switzerland is the respective local dialect. Due to a rather large inter-cantonal migration rate (about 5% p.a.) within modern Switzerland for decades, many different Swiss German dialects are spoken in any one place, especially in urban areas; for example, in the city of Zürich (end of 2013): of the 272,700 Swiss (total: 400,000) living in Zürich, only 40% (28%) are from Zürich itself with 51% (36%) from the entire canton of Zürich.\nOutside of any educational setting, Swiss Standard German is only spoken in very few specific formal situations, such as in news broadcasts and reputable programmes of the public media channels; in the parliaments of German-speaking cantons; in the federal parliament in Berne (unless another official language of Switzerland is used), although dialect is certainly encroaching on this domain; in loudspeaker announcements in public places such as railway stations, etc. Church services, including the sermon and prayers, are usually in Swiss Standard German. Generally in any educational setting Swiss Standard German is used (during lessons, lectures or tutorials). However, outside of lessons Swiss-German dialects are used, even when, for example, talking to a teacher about the class. The situations in which Swiss Standard German is spoken are characteristically formal and public, and there are situations where written communication is also important.\nIn informal situations, Swiss Standard German is only used whenever a German Swiss is communicating with a non-Swiss and it is assumed that this person does not understand the respective dialect. Among each other, the German-speaking Swiss use their respective Swiss German dialect, irrespective of social class, education or topic.\nUnlike in other regions where German varieties are spoken, there is no continuum between Swiss Standard German and the Swiss German dialects. The speakers speak either Swiss Standard German, or a Swiss German dialect, and they are conscious about this choice.\nNevertheless, about 10%, or 828,200, of Swiss residents speak High German (also called Standard German) at home, but mainly due to the presence of German immigrants.\nThe concurrent usage of Swiss Standard German and Swiss German dialects has been called a typical case of diglossia. This claim has been debated because the typical diglossia situation assumes that the standard variety has high prestige, whereas the informal variety has low prestige. In the German-speaking part of Switzerland, however, the Swiss German dialects do not have a low prestige and permeate every socio-economic class of society.\nSince Swiss Standard German is the usual written language and the Swiss German dialects are the usual spoken language, their interrelation has been called a medial diglossia.\nAttitude to spoken Swiss Standard German\nMost German Swiss can speak fluent Swiss Standard German, but may or may not like doing so, as it feels stilted and unnatural to many. When they compare their Swiss Standard German to the way people from Germany speak, they think their own proficiency is inferior because it is studied and slower. Most German Swiss think that the majority speak rather poor Swiss Standard German; however, when asked about their personal proficiency, a majority will answer that they speak quite well."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:1654878e-316a-4892-a12b-68520adab475>","<urn:uuid:b2b97b5d-b805-417f-aa2e-01e58172f929>"],"error":null}
{"question":"How does the Night Sky Observer's Guide's coverage of deep-sky objects in Gemini compare to the constellation's actual deep-sky content?","answer":"The Night Sky Observer's Guide specifically focuses on deep-sky objects like galaxies, nebulae, and clusters, which interestingly aligns with what Gemini has to offer. The constellation contains several notable deep-sky objects including M35 (an open cluster of magnitude 5), the Eskimo Nebula (a planetary nebula with magnitude 9.2), and the Medusa Nebula (a planetary nebula with magnitude 15.9). However, as noted, Gemini doesn't contain many deep-sky objects due to its location away from the Milky Way.","context":["The Night Sky Observer's Guide\nThis item is out of stock but you can place an order now and we’ll ship when it becomes available.\nby George Robert Kepple and Glen Sanner (vol. 1 and 2)\nby George Robert Kepple, Ian Cooper and Jenni Kay (vol. 3)\nAmateur astronomers today are exceptionally fortunate to be living in an era when high quality, and very large, optics are so affordable. In the first half of the 20th century the telescope deluxe for the amateur was the 6-inch refractor. However, such telescopes were so expensive that very few amateurs could afford them: the majority of stargazers had to content themselves with instruments in the 60mm range. Consequently, most observing guides published during that time emphasized double and multiple stars, with honorable mention for variable stars and planetary nebulae, objects which do well in long focal length refractors. Webb’s 1858 Celestial Objects for Common Telescopes and Olcott’s 1936 Field Book of the Skies were not superseded for so many decades simply because the average amateur instrument did not dramatically improve during the century after Webb. By the 1950s the mass-produced or homemade 6-inch parabolic mirror brought medium-sized optics into the price range of the average amateur, and with it the emission nebulae, open clusters, and galaxies that had been seen only as amorphous blobs-if seen at all-in small refractors. The 1948 Skalnate Pleso Atlas of the Heavens had already displaced the classic Norton’s Star Atlas as the frontline sky-chart for amateurs, but the observing guides badly needed rewriting. However, not until the 1970s and Burnham’s Celestial Handbook was there an observing guide worthy of the 6-inch Newtonian reflector or of the more expensive, but increasingly popular, 8-inch Schmidt-Cassegrain telescope.\nBy the early 80s another revolution in amateur optics was underway thanks to the inexpensive and easily-constructed mounting for large aperture Newtonian reflectors invented by John Dobson. With these big “light buckets” one can see scores of emission nebulae, hundreds of star clusters, and thousands of galaxies, with details visible in virtually all of them.\nBut once again observing literature failed to keep pace with the optics. The purpose of The Night Sky Observer’s Guide was to close this re-widened gap by providing the owner of a medium or large aperture telescope with some idea of what to look for in such instruments—both what objects can be seen, and what details may be seen within these objects. It all began in 1987 when George Kepple and Glen Sanner, founded the Observers Guide, a bi-monthly magazine that set out to describe, with their readers as active participants, what could be seen with telescopes 8-inches and larger from mid-northern latitudes. Unlike an ordinary magazine it would have a finite life because each issue was devoted to one-or occasionally several smaller constellations. When completed in the early 1990s 64 constellations had been covered. Based upon that experience the authors began working on what would become The Night Sky Observer’s Guide\n, a two volume work that was published in 1998 to critical acclaim. However, The Night Sky Observer’s Guide was not yet an all-sky guide—the southern skies, much of which is visible from southern Florida, Texas, New Mexico and Arizona and down to the South Celestial Pole remained to be covered. The problem had always been the general lack of a large number of reliable observing reports from these locations but several things were happening to correct this problem—the migration of observer/retirees to these locations and the rise of the World Wide Web.\nIn 2003, in search of warmer weather and the pursuit of his lifelong love of astronomy, George Kepple moved from Pennsylvania to the clear skies of southwest. George’s new observing site provided a southern horizon with a view down to declination minus 58 degrees, just 32 degrees short of the South Celestial Pole. With retirement George also had the time to “surf” the WEB looking for methodical observer’s “down under”. He was soon rewarded when he struck up an acquaintance with Ian Cooper of New Zealand. Encouraged by his new observing site and what he found via the web he again approached Willmann-Bell to inquire if there was interest in a “southern” volume. The answer was a resounding yes.\nWith George assuming the responsibilities of Editor and Ian Cooper providing observational data as co-author, work began. As things progressed other observers were recruited, one of which was Jenni Kay of South Australia who was especially productive and so was asked and accepted a co-authorship. The result of their labors is The Night Sky Observer’s Guide, Volume 3 — The Southern Skies.\nWhile all three volumes of The Night Sky Observer’s Guide are aimed at amateurs especially interested in observing galaxies, nebulae and clusters — double and variable stars have not been neglected. Data tables for these are provided and these stars are labeled on maps and finder charts. Moreover, the most famous or visually impressive of these are given written descriptions similar to those for other deep-sky objects. Splitting doubles is nearly as popular with amateurs today as it was thirty or forty years ago. They offer the observer many fine, and even spectacular, sights in the eyepiece and often can provide an evening of enjoyable viewing when conditions are poor for “faint fuzzies”. Read a Sample Chapter in PDF The number of celestial objects of each type covered in the three volumes of The Night Sky Observer’s Guide are:\n|Misc. (QSOs, Asterisms)||14|\nIn addition there are:\nARRANGEMENT OF THE GUIDE\nEach chapter is devoted to a constellation. The first page is devoted to general comments about the constellation. The second page is a map of the constellation which faces a stellar data table which usually fills the entire page. The remaining pages of each chapter contain photographs, sketches and finding charts - and all of these pages include written descriptions of objects as seen through different sized instruments.\nVOLUME 1: Autumn and Winter\nAndromeda, Aquarius, Aries, Auriga,Camelopardalis, Cancer, Canis Major, Canis Minor, Cassiopeia, Cepheus, Cetus, Columba, Eridanus, Fornax, Gemini, Lacerta, Lepus, Lynx, Monoceros, Orion, Pegasus, Perseus, Pisces, Piscis Austrinus, Puppis, Pyxis, Sculptor, Taurus, and Triangulum.\nVOLUME 2: Spring and Summer\nAntlia, Aquila, Bootes, Canes Venatici, Capricornus, Centaurus, Coma Berenices, Corona Australis, Corona Borealis, Corvus, Crater, Cygnus, Delphinus, Draco, Equuleus, Hercules, Hydra, Leo, Leo Minor, Libra, Lupus, Lyra, Microscopium, Ophiuchus, Sagitta, Sagittarius, Scorpius, Scutum, Serpens Caput, Serpens Cauda, Sextans, Ursa Major, Ursa Minor, Virgo, and Vulpecula.\nVOLUME 3: The Southern Skies\nApus, Ara, Caelum, Carina, Centaurus (expanded coverage beyond that found in Volume 2), Chamaeleon, Circinus, Crux, Dorado, Grus, Horologium, Hydrus, Indus, Mensa, Musca, Norma, Octans, Pavo, Phoenix, Pictor, Reticulum, Telescopium, Triangulum Australe, Tucana, Vela,Volans plus extensive coverage of The Large Magellanic and Small Magellanic Clouds.","How to Identify the Constellation Gemini in the Sky\nThis constellation is one of the twelve constellations of the Zodiac. It lies along the ecliptic and is best seen in the night sky during the winter and spring seasons, in the northern hemisphere and during summer in the southern hemisphere. It´s recognizable by two bright stars which form the heads of the twins-castor and Pollux.\nGemini is seen from January till May coming out from the horizon in the early evening of this months and it sets two hours earlier with each proceeding month. During January and February, Gemini is seen coming up the horizon after dark and it will reach its highest point in the sky during February around 10:00 p.m.\nConstellation Gemini in the Sky\nLocation of Gemini in the Sky\nGemini is located along the ecliptic-the imaginary line in the sky, where the sun, planets and moon transit. Gemini is one of the twelve constellations of the Zodiac and occupies the third place in the celestial sky, with the first being Aries. This constellation is easy to locate knowing which other constellations lie close to it.\nBeing the second constellation in the Zodiac, Taurus lies to the west of Gemini and Cancer in the east. Taurus is easy to spot, as it´s very prominent in the winter night sky. Auriga and Lynx are in the north and Monoceros and Canis Minor are located south of Gemini.\nGemini is recognized for its two brightest stars; Castor and Pollux The easiest way to locate these stars is to take as reference the constellation Taurus and look to the east of it or Locate Orion´s belt-three stars and then Rigel-Orion´s left foot and Betelgeuse-Orion´s left shoulder and extend an imaginary line through them that will point to the twin´s heads.\nLocating Gemini in the Night Sky\nUsing Orion to find Gemini in the Sky\nOrion is one of the most noticeable constellations in the winter night sky, with lots of recognizable stars. This constellation is often used by star gazers when they want to locate another celestial body. For those who are experienced at locating stars and other features in the sky, it may result easy to know their way to Gemini; however, for those who are starting to recognize the night sky features, using Orion as a reference is the most straightforward way.\nDuring winter, Orion the hunter appears prominently high in the sky, along with other outstanding constellations and stars. This constellation is most commonly known for three stars in a row, known as Orion´s belt. Below these stars is Rigel and Saiph-Orion´s left and right feet and above then is Betelgeuse and Bellatrix-Orion´s right and left shoulders. Drawing an imaginary line-yellow straight line in the picture, starting at Rigel and extending it through Betelgeuse takes you to Castor and Pollux-the two brightest stars in Gemini.\nLocating Gemini using the Big Dipper\nThe Big Dipper or the bowl as it is also known can be seen every night in the northern hemisphere. This is a circumpolar asterism, meaning it never sets below the horizon and is always visible by those living in the northern hemisphere. This asterism is composed of seven bright stars-four of them-Megrez, Dubhe, Phecda and Merak, comprising the bowl and three stars-Alkaid, Mizar and Alioth, giving shape to the handle.\nAn imaginary line, starting at the star Megrez and extending through Merak leads to the heads of the twins in Gemini; Castor and Pollux. This is shown in the picture by a blue straight line. Not far from the Big Dipper lies the little Dipper whose tip of its handle is the north polar star. An imaginary line starting at Merak and going straight into Dubhe leads directly to the pole star, which is shown in the picture by straight while line.\nStars in Gemini\nThere are 85 visible stars in Gemini-the brightest of which are Pollux-an orange giant star of magnitude 1.2 at 34 light years from earth and Castor (a six star system) visible at magnitude 1.9 and located 52 light years from earth. Alhena is a blue-white star of magnitude 1.9 and situated 105 light years from earth. Mebsuta is a double star system consisting of a yellow supergiant visible at magnitude 3.1 and its companion a star of magnitude 9.2. Mekbuda is a double star system consisting of a Cepheid variable star of period 10.2 days and a companion star of 7.6 magnitude.\nPropus is a binary variable star system with a period of 500 years. The primary star is a blue giant of magnitude 4.1 and the secondary is a star of magnitude 8.0. 38 Gem is a binary star system 91 light years away from earth whose primary star is a white star with a magnitude of 4.8 and a secondary yellow star of magnitude 7.8. Mu Gem is a star with an apparent magnitude of 2.9. It´s an irregular variable star which varies from 2.75 to 3.02 over a period of one day.\nDeep Sky Objects in Gemini\nSome of the most prominent objects in Gemini are M35, the Eskimo Nebula and the Medusa Nebula. M35 is an open cluster of magnitude 5; it covers 0.2 degrees in the sky and is visible during clear night skies. The Eskimo Nebula is a planetary Nebula with a magnitude of 9.2 and visible with a mall amateur telescope. The Medusa Nebula is a planetary nebula with an apparent magnitude of 15.9 and at 1.500 light years from earth. Geminga is a neutron star at 800 light years.\nNGC 2129 is an open cluster with an angular distance of 2.5 arcminutes and lies at approximately 7,200 light years from earth. NGC 2158 is an open cluster with an apparent magnitude of 8.6 and NGC 2355 is another open cluster of stars at approximately 5,400 light years away from earth and is visible at an apparent magnitude of 9.7. Gemini does not contain too many deep sky objects due that it´s in a region that is away from the Milky Way.\nGeminids Annual Meteor Shower\nThe Geminids are a luminous meteor shower that occurs in December and peaks on December 13-14. On the early hours of the second of these days, an estimated quantity of 120 meteors every hour can be observed and is considered as one of the most intense meteor showers of the year. The point at which the meteors seem to radiate is in the direction of the constellation Gemini; therefore, it´s said that the radiant is in Gemini.\nGeminids Meteor Shower with Radiant in Gemini\nDescription of the constellation Gemini\n© 2017 Jose Juan Gutierrez"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:700b91b9-556e-4b6e-999c-585df783de42>","<urn:uuid:7ac5d1a2-fdfd-4002-9fbe-bdb88975987a>"],"error":null}
{"question":"Looking to grow my own food - what types of gardening facilities are available at Washtenaw community gardens, and how do they contribute to food access in the area?","answer":"Washtenaw community gardens offer extensive facilities including tool sheds with hand and power tools, water access systems, compost bins, and shared crop areas - for example, Chelsea Community Garden provides water towers and access to shared crops like blueberries and rhubarb. Regarding food access, these gardens contribute significantly to community food networks, as gardeners often choose to donate portions of their produce to senior citizens, homeless individuals, and families in need, helping to improve food access for the entire community. Some gardens, like the Growing Hope Urban Farm, also provide educational workshops to teach people how to grow their own food.","context":["Looking for a way to get those green thumbs digging in the dirt? Washtenaw County offers plenty of community gardens for local residents to try their hand at gardening. Garden plot rentals are the perfect solution for families that may not have the green space readily available. Plot reservations are filling up quickly, but many offer waiting lists for spots that may become available in the future. Reserve your spot today!\nAnn Arbor Center for Independent Living Discovery Garden\nFee: $50 for one plot, $80 for two.\nPlots: This garden was created for children, youth, and adults with disabilities to give them a completely accessible gardening experience. There are sixteen raised beds in total, twelve of which are fully handicap accessible. Plots are 24-34 square feet. Concrete paths, completely wheelchair accessible to the tool shed.\n3941 Research Park Dr., Ann Arbor. 734-971-0277. annarborcil.org.\nContact: Beth Wichterman at firstname.lastname@example.org\nChelsea Community Garden\nFee: Each plot is $20, with a $20 annual rental fee.\nPlots: This all-organic garden offers forty 20 by 20 plots. Members are required to work eight hours of community service at the gardens each year. The garden has a water tower and a large suite of compost bins. Two tool sheds house a variety of hand and power tools. Members also share in a bounty of crops, such as blueberry, rhubarb, and asparagus.\nSibley Rd., Chelsea. 734-719-0035. facebook.com/chelsea.community.garden.\nContact: 734-719-0035, email: email@example.com\nChildren’s Services Community Garden\nFee: Covered by local sponsors\nPlots: This garden pairs with Washtenaw County Master Gardeners and offers two courtyards with 7-8 beds each. Students do the planting, tending, and harvesting with help from the garden experts.\n4125 Washtenaw Ave., Ann Arbor.\nCity of Dexter Community Garden\nFee: Full plots are $80 with a $20 refundable deposit, and half plots are $50 with a $20 refundable deposit.\nPlots: Full plots measure 20 by 20, and half plots of 10 by 20 are also available. Herbicides, insecticides, and inorganic fertilizers are prohibited. Clean-up is required at the end of the season.\n7651 Dan Hoey Rd., Dexter. 734-426-8303. dextermi.gov/community/community_garden.php.\nContact: 734-426-8303, email Olga Austin at Austin.firstname.lastname@example.org\nEMU Giving Garden\nFee: 4 by 4 plots $5, 6 by 6 plots $10\nPlots: Available to EMU students, faculty, and Ypsilanti residents.\n24 Westview St., Ypsilanti. facebook.com/thegivinggardenemu.\nThe Farm at St. Joe’s\nFee: 10 by 9 and 10 by 10 plots for $15, 10 by 12 plots for $20, 10 by 15 plots for $25. $20 refundable deposit required.\nPlots: Available on a first-come, first-serve basis. The fees cover the use of plot, compost, water, pathway, and fence maintenance.\n5301 McAuley Dr., Ypsilanti. 734-712-4667. stjoesfarm.org.\nContact: Rozelle Copeland at Rozelle.Copeland@stjoeshealth.org\nGrowing Hope Urban Farm\nFee: Funded through outside sources\nPlots: This 1.4-acre community space teaches youth and adults how to grow fresh food with workshops, events, field trips, and volunteer training.\n922 West Michigan Ave., Ypsilanti. 734-786-8401. growinghope.net.\nContact: 734-786-8401 or growinghope.net\nPittsfield Township Community Garden\nFee: 10 by 4 raised beds for $30, 6 by 4 double raised beds on ADA accessible surface for $25, 4 by 4 table height beds on ADA accessible surface for $20.\nPlots: The organic garden features compost bins, equipment shed, water access, rain barrels, and a picnic area. There is also a native pollinator garden.\n6201 W. Michigan Ave., Ann Arbor. 734-822-3135. pittsfield-mi.gov/1635/Community-Garden.\nContact: 734-822-3135 or email email@example.com\nFee: Varies by Location\nThis local organization promotes organic gardening and helps connect gardeners with open plots in the community. It also offers classes and events, along with volunteer opportunities. Project Grow has a few community gardens around Washtenaw County, so be sure to check out their website to pick which garden is right for you.\nVarious locations. 734-996-3169. projectgrowgardens.org.\nContact: 734-996-3169 or apply through their website.\nSaline Community Garden\nPlots: Come enjoy either a 4 by 6.5 raised bed plot, 4 by 10 plot, and 10 by 10 plot to grow with! This also comes along with a water spigot and compost bin. Unfortunately, the Saline Community Garden is sold out of plots for the 2021 season. But they’ll have more plots available next season.\nWoodland Drive, Saline. 734-401-4020. salineschools.org/saline-community-garden.\nYpsilanti District Library\nPlots: Whittaker Garden encourages youth and families to plant seeds, harvest plants, and learn about the science of gardening. Michigan Branch features a pollinator garden and two raised beds for community members to plant and harvest produce. Superior Branch welcomes all levels of volunteer gardeners, and fresh produce is given away to library patrons throughout the growing season.\nWhittaker Branch: 5577 Whittaker Rd., Ypsilanti. Michigan Branch: 229 W. Michigan Ave., Ypsilanti. Superior Branch: 8795 MacArthur Blvd., Ypsilanti. 734-482-4110. Ypsilibrary.org.\nDon’t see your favorite community garden? Let us know! Email us at firstname.lastname@example.org or leave a comment to have us include your favorite Community Garden!","“The varied purposes and benefits of community gardening make it an ideal interventional strategy for community-based practitioners because myriad goals may be addressed through this approach.”\nCommunity gardening emerged in the United States in the 1890s as, “a means to address urban congestion caused by immigration, economic instability and environmental degradation”. Immigrants, children and the poor were the original targets for the efforts, but economic strife caused by war and the Great Depression resulted in nearly universal participation until later in the century when the industrialization of agriculture resulted in the emergence of convenience foods. It was not until the recession in 2009 that community gardens enjoyed a resurgence (19% increase). This suggests that participation in community gardening activities is correlated with the socio-economic conditions of the time.\nBeyond necessity, the five main purpose/concerns that relate to community gardens are:\n- Engaging Youth;\n- Health Benefits (dietary, mental, and physical);\n- Gardener vs. Land Holder Conflicts;\n- Social Capital;\n- Participant Motivations and Perspectives.\nPositive outcomes are related to socializing opportunities and experiences. This includes social connections that are formed from accessing resources and developing collaborative efforts with outside organizations like universities, youth programs and health centers. Furthermore, multiple social processes (ex. reciprocity) formed during activities related to community gardening transfer to situations outside of the garden. This strengthens a widespread sense of community. This finding emphasizes that social interactions are imperative to the success and perpetuation of community gardens.\nCommunity gardens have also been identified as a mechanism for individuals and communities to preserve, express and affirm culture. This is accomplished by growing specific foods, designing the gardens to reflect cultural heritage and providing a venue for cultural expression with dances, musical performances and festivals.\nParticipant motivations include access to fresh and better tasting food, enjoyment of nature, health benefits, opportunities to socialize, to beautify and give back to the community and to support the conservation of green space. Some participate in community gardens as a leisure and/or recreation activity. Others see community gardens as a means for neighborhood beautification and believe that they can serve as a way to revitalize distressed areas. Still, others anecdotally report that community gardens are correlated with a reduction in crime once the garden becomes established. This occurs when garden spaces become identified as safe places for individuals and families – especially those who otherwise lack access to open spaces – come together.\nThe fight for the right to public land use has given participants an opportunity to participate in local politics for the first time. This often occurred when there was an issue of land-use rights. In many of these cases, this conflict served as a means for instigating organization and mobilization of community members. Oftentimes, these individuals would not otherwise interact.\nFinally, it was found that in many areas gardeners choose to donate a portion of their produce, typically to senior citizens, the homeless or poor individuals and families which works to improve food access networks for the community as a whole. This indicates that when people create something, they want to share it with the world.\nDraper, C., & Freedman, D. (2010). Review and Analysis of the Benefits, Purposes, and Motivations Associated with Community Gardening in the United States. Journal Of Community Practice, 18(4), 458-492."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:577375a5-6547-4372-ace3-cf41e3c7a24e>","<urn:uuid:2982295d-0e78-4d8c-b84e-a08869d8b8e8>"],"error":null}
{"question":"What were the technological upgrades at Fine Arts Theatre Place compared to the National Theatre's technical evolution?","answer":"The Fine Arts Theatre Place's technological upgrades included state-of-the-art computer systems, light and sound equipment, while maintaining its classic 35mm film reels and vintage projector with crucial updates. The National Theatre's technical evolution was marked by the introduction of NT:LIVE in the 2000s under Nicholas Hytner's leadership, which involved sophisticated filming and broadcasting capabilities to beam productions to cinema venues worldwide, representing a significant step in theatrical technology adaptation.","context":["Historical pictures, before-and-afters, and our Grand Re-Opening!\nYes, our beloved Theatre was originally the town's stables, owned by James. A. Coughlan!\nCars arrived in Maynard in 1901, and Coughlan converts his stables into a garage and car dealership. Here, the first car in Maynard.\nDespite the automobile that moved Coughlan to convert his stables to a garage, trains were still king in Metro West. Here, a derailment in 1911 at High Street.\nMoving pictures arrive in the 1920s. Here, the People's Theatre, one of three cinemas that once entertained Maynardites, is under construction on Nason Street.\nBurt Coughlan, son of James, manages the People's Theatre and develops a passion for all things Hollywood.\nBurt Coughlan opens Fine Arts Theatre Place where his father's garage and stables once stood. The People's Theatre would close just a few years later.\nBy the 1960s, Fine Arts had become one of the highest grossing Disney cinemas in the country. Here, moviegoers file into the lobby. Check out the car in the reflection!\nBefore and After Photos\nIt was a sad day when Fine Arts Theatre Place closed in 2012. Here, the Theatre's unsightly front entrance awaits some TLC.\nThe restoration preserved the Theatre's classic signage and exterior, with some nice new technology at the box office window.\nHere, the hallway carpets are worn out and roof tiles show visible signs of water damage.\nDropcloths protect the floor and hallway murals as walls are painted and the wiring, lights, and ceiling are replaced.\nOur restored hallway: new carpets, ceiling, paint and all the rest! New director's chairs are perched near Steve's Corner.\nThe lobby's condition made an unattractive entrance to our proud Theatre.\nOur restoration team found old wiring and cobwebs at every turn. Here, the lobby ceiling is opened up.\nAn amazing restoration retained the classic concessions stand and made our lobby beautiful again!\nOur newly restored lobby made for a festive holiday season.\nThey don't build 'em like this anymore. And it makes the most delicious movie popcorn you'll ever have!\nWater was coming through the roof in multiple places. No wonder our Theatre was so damp!\nThat took care of the leaks. Now to repair all of the damage inside...\nThis was a monumental job, and those Theatre ceilings are high!\nAside from crumbling bricks, our Theatre screens were waterlogged!\nWhat a difference! Refurbished seats, brand new screen, state-of-the-art light and sound, and so much more.\nThe Asian murals remain, two of the unique features that make our Theatre a classic Hollywood gem.\nLike her partner across the way, the Empress gives a royal welcome to Metro West moviegoers.\nLike most of the Theatre, this room needed more than aesthetic fixes. State-of-the-art computer, light, and sound systems were needed. Cue Mark Grundstrom, technology guru!\nThe belly of the beast! Our classic 35mm. film reels remain, as does our vintage projector (with some crucial updates).\nHere, the red carpets are out for our Grand Re-Opening. Wanna see the pics? Check out our next slideshow!\nGrand Re-Opening – December 5, 2014\nSteve and Melissa are ready for the noontime ribbon cutting ceremony.\nLocal leaders and politicians celebrate Fine Arts Theatre Place.\nState Representative Kate Hogan, center, stands on with Selectman Jason Kriel, left, and State Senator Jamie Eldridge.\nGetting ready to cut the ribbon...\nIt's official! Welcome to the newly restored Fine Arts Theatre Place.\nMore than 100 people turned out to see the new Theatre!\nThe crowd stretched down the street.\nFamilies take their first look at our new lobby.\nRooftop lights and classic cars were on hand for our evening celebration!\nPeople as far away as Waltham could see the lights.","In 1879 the Royal Shakespeare Company was founded in Stratford and known as the New Shakespeare Company. There was still no building in the capital city to act as a ‘National’ theatre.\nIn 1948 the London County Council passed a ‘National Theatre Act’ in Parliament, and a site on London’s Southbank next to the Royal Festival Hall was allocated for the project. Funding issues continued to disrupt the venture, with the Government seeming reluctant to sponsor another public funded arts body, in addition to the RSC, Sadler’s Wells and the Old Vic.\nIt took until 1962 for construction plans to be agreed, when the National Theatre Company was formed. In the interim the Old Vic was to be leased, with the new company remaining there until 1976 when the South Bank venue was finally completed. The first production was Hamlet opening on October 22nd 1963 and staring Peter O’Toole.\nLord Laurence Olivier was the first Artistic Director of the National Theatre Company and had been a figurehead of the project from the very beginning. At the time he was one of the greatest stage and film actors, appearing in famous film adaptations of Shakespeare plays, such as the 1944 War-time film of Henry V which acted as a vital piece of propaganda, creating support for the war and boosting national spirit. His production of Hamlet was the inaugural production by the National Theatre Company During his time with the company he produced both contemporary plays and revivals of classic and lesser known Shakespeare plays, using the finest new actors in the country. Highlights included:\n- Hamlet, directed by Olivier, starring Peter O’Toole as Hamlet and Michael Redgrave as Claudius (1963).\n- Othello, directed by John Dexter, with Laurence Olivier ‘blacking up’ in the title role alongside Maggie Smith as Desdemona (1964).\n- The Merchant of Venice, directed by Jonathan Miller, with Laurence Olivier as Shylock and Joan Plowright (Olivier’s later wife) as Portia (1970)\n- Hedda Gabler by Henrik Ibsen, directed by Ingmar Bergman, with Maggie Smith as Hedda (1970)\n- Long Day’s Journey into Night by Eugene O’Neill, starring Laurence Olivier as James Tyrone (1971)\nAfter Olivier retired from the position, Peter Hall took on the post as Artistic Director building on the venue’s many successes but continuing to develop in terms of repertoire. Hall’s experience stemmed from running the Shakespeare Memorial Theatre in Stratford where he helped to develop the Royal Shakespeare Company. His work at the National lasted the longest term of any of the Artistic Directors, and saw more new writing especially from America, including the first musical to be performed by the company under the helm of Richard Eyre. Notable productions during Peter Hall’s time included:\n- The Tempest starring John Gielgud as Prospero (1974).\n- Amadeus by Peter Shaffer, directed by Peter Hall, with Paul Scofield and Simon Callow (1979–80)\n- Guys and Dolls, directed by Richard Eyre, starring Bob Hoskins, Julia McKenzie, and Julie Covington (1982)\n- Antony and Cleopatra directed by Peter Hall, with Anthony Hopkins and Judi Dench (1987)\nAfter working as an associate director for the National Theatre, Richard Eyre took over as Artistic Director in 1988. Having worked extensively in large regional theatres, Eyre was well equipped to meet the demands of the new position. He accounts his time at the National in his diaries during the time which are called ‘National Service’ and describe the changing face of the theatre alongside the changing political face of Britain up until Labour’s landslide election in 1997. Eyre’s most significant production was Hamlet staring Ian Charleson, which won multiple awards and raised the profile of the popular actor who died of AIDS in 1990. After Eyre’s success with Guys and Dolls the National opened it’s doors to more musical theatre pieces, including many works by American composer Stephen Sondheim, including Sunday in the Park with George and A Little Night Music. Highlights throughout this period included:\n- Hamlet, starring Ian Charleson, directed by Richard Eyre (1989)\n- Richard III starring Ian McKellen, directed by Richard Eyre (1990)\n- Angels in America by Tony Kushner, directed by Declan Donnellan (1991–92)\n- Sweeney Todd by Stephen Sondheim and Hugh Wheeler, directed by Declan Donnellan (1993)\nTrevor Nunn took the Theatre into the dawn of a new Britain, and as a financial supporter of the Labour Party he seemed to be the right person for the job. As the venue had to justify the public money spent on it more than ever before, the program was forced to become more populist and more diverse. Trevor Nunn’s experience with large scale musical theatre pieces during the 1980s such as Les Miserables and Cats allowed him to use these connections to present West End style musicals, with revivals of classics such as Oklahoma, Carousel, South Pacific and My Fair Lady all throughout his time. Highlights throughout the period included\n- Oklahoma! by Richard Rodgers and Oscar Hammerstein, directed by Trevor Nunn, with Maureen Lipman and Hugh Jackman (1998)\n- The Merchant of Venice directed by Trevor Nunn, with Henry Goodman (1999)\n- Honk!, Winner of the 2000 Olivier Award for Best Musical (1999)\n- The Winter’s Tale by William Shakespeare directed by Nicholas Hytner, with Alex Jennings (2001)\n- The Coast of Utopia, a trilogy by Tom Stoppard, comprising: Voyage, Shipwreck and Salvage, directed by Trevor Nunn (2002)\n- Anything Goes by Cole Porter, directed by Trevor Nunn, starring John Barrowman and Sally Ann Triplett (2002)\nNicholas Hytner took over the role in 2003 taking the company in a new direction. Adapting to modern changes Hytner introduced a partnership with Travelex to offer £10 tickets to selected sponsored performances. Over the past two years Hytner has developed the NT:LIVE concept, filming selected productions at the theatre, which are then beamed around the world to cinema venues. As well as broadening accessibility this move continued to globalise the branding, increasing financial support in the most difficult of times. Many productions under Hytner have transferred to the West End and Broadway, including The History Boys and War Horse with many also being turned into blockbuster films. Recent highlights have included:\n- Jerry Springer: The Opera, by Stewart Lee and Richard Thomas (2003)\n- His Dark Materials, a two-part adaptation of Philip Pullman’s novel directed by Hytner starring Anna Maxwell Martin (2003)\n- The History Boys by Alan Bennett directed by Nicholas Hytner starring Richard Griffiths (2004)\n- War Horse based on a novel by Michael Morpurgo, adapted by Nick Stafford, directed by Marianne Elliott and Tom Morris, presented in association with Handspring (2007–2009)\n- The Habit of Art, by Alan Bennet, Richard Griffiths (2010)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:7c2cbd95-3140-4354-bc92-bdc3fd49e91c>","<urn:uuid:6c361b04-8ced-4b03-a484-4efc8e5398d8>"],"error":null}
{"question":"what r animal therapy places in Ireland and how they help depression?","answer":"In Ireland, there are several animal therapy centers including Festina Lente in Bray, The Carefarm in Castlemartyr, and organizations like Irish Therapy Dogs that provide pet therapy services. These centers offer animal-assisted therapy through various programs. Research has shown that animal-assisted therapy, particularly dog visiting programs, can be effective in managing depressive symptoms, especially in older adults. Animal therapy services help provide comfort and companionship to people in long-term care, and can be particularly beneficial for mental health needs.","context":["ACCESSIBLE TRANSPORT SERVICES\nWeb and mobile based search facility to source wheelchair accessible transport throughout Ireland\nANIMAL ASSISTED THERAPY\nFestina Lente centre in Bray\nNot-for-profit organisation which provides a range of occupational, learning and employment opportunities for people whose needs have not been met in mainstream services\nIrish Therapy Dogs\nPet therapy for the provision of comfort and companionship to people in long-term or daily residential care.\nThe Carefarm -Caslemartyr Co Cork\nA small farm where children who have extra needs come to benefit from Animal Assisted Therapy, Horticultural Activities and one on one tuition.\nAutism Assistance Dogs Ireland\nAADI is a national charity providing highly trained assistance dogs for children with autism and their families.\nDogs for the Disabled\nAssistance dogs are specially trained to assist physically disabled children and adults to carry out a range of practical tasks in order to achieve greater independence.\nIrish Guide Dogs for the Blind\nEnables both blind and vision impaired persons and families of children with Autism to achieve increased independence and an improved quality of life.\nMy Canine Companion Autism Assistance Dogs\nprovide highly trained and skilled assistance dogs to people with disabilities, particularly autism.\nEDUCATIONAL RESOURCES WEBSITES\nSensational Kids Run a child development and learning store which stocks a range of skill building toys and therapy product and books\nABC School Supply Centre (educational supplies)\nIncludes a Special Educational Needs portfolio of books, hands on resources and software\nFree downloadable resources with social skills and behavioral regulation activities and guidance, learning songs and games, communication cards, academic material, and transition guides\nDyslexia Dublin CETC Educational Store\nEarly years and specific learning needs resource store\nLists & descriptions of educational Apps\nOutside the Box Learning Resources\nFree teaching & learning resources for students with special needs and learning disabilities.\nPhonics based educational games\nAHEAD (Association for Higher Education Access and Disability)\nAn independent non-profit organisation working to promote full access to and participation in further and higher education for students with disabilities\nDARE (Disability Access Route to Education)College and university admissions scheme which offers places on a reduced points basis to school leavers under 23 years old with disabilities who have completed an Irish Leaving Certificate.\nOnline training and software development company offering online support and skills development courses created and facilitated by an Educational Psychologist.\nOnline guide for parents and teachers of gifted children in Ireland\nHome Education Network\nAdvice and support for parents who choose to homeschool. Includes link to HEN Eire social network for home schooling in Ireland\nIrish Learning Support Association\nNationwide body that represents the interests of Learning Support, Resource and Language teachers\nNational Learning Network\nNational Learning Network is the training and employment division of the Rehab Group.\nNational Council for Special Education\nManages education services and supports to pupils with special educational needs arising from disabilities.\nPortal for Irish education.\nUCD ACCESS and Lifelong Learning\nAims to increase diversity and widen access to third level education\nIrish Autism Alliance was formed in 2001 by a group of parents who took a public stance in relation to the needs and rights of their children. Irish Autism Action (IAA) is an umbrella body currently representing parent groups across, support groups and schools for those on the autistic spectrum across Ireland.\nThe Society was formed with the goal of creating awareness of autism and convincing the authorities that early diagnosis and specialised education would greatly enhance the quality of life for people with Autism.\nIt is a coordinated initiative which aims to enhance and support the whole person and family affected by Autism by providing access to information.\nRehab is a leading non-governmental organisation which works towards a world where every person has the opportunity to achieve their potential. They work in local communities to provide high-quality services and opportunities to people who need them.\nTheir mission is to support people with Asperger Syndrome (AS) to lead full and independent lives.\nTheir role is to enhance the quality of learning and teaching in relation to special educational provision.","Recognizing the role of animal-assisted therapies in addressing mental health needs during the COVID-19 pandemic\nContributor(s):: Nagendrappa, S., Shoib, S., Rehman, S., Grigo, O., Ransing, R.\n2020Asian J Psychiatr531023901876-2018 (Print)1876-201810.1016/j.ajp.2020.102390engDepartment of Psychiatry, National Institute of Mental Health and Neurosciences(NIMHANS), Bengaluru-29, India. Electronic address: firstname.lastname@example.org.Department of Psychiatry, Jawahar Lal Nehru...\nBetydningsfulle elementer ved gårdsdyr-assisterte intervensjoner for personer med depresjon. Erfaringer fra personell med helse- og sosialfaglig bakgrunn.\n| Contributor(s):: Maria Hemmingsen-Larsen\nBakgrunn: Depresjon er en av de vanligste psykiske lidelsene i vesten. I Norge er det en økt satsing på velferdstjenester i brukernes nærmiljø, og mange har behov for arbeidsrettede tiltak. Gårdsdyr-assistert intervensjon (GDAI) kan være et arbeidsrettet...\nLos beneficios del gato en la depresión. Mito o realidad\n| Contributor(s):: Mónica Sastre Martínez\nEn la siguiente revisión bibliográfica, se ha estudiado la literatura reciente en cuanto a los beneficios del gato en la salud de las personas. A pesar de la poca información que existe, se ha intentado recaudar el máximo de información posible sobre los...\nEffects of Dog Assisted Therapy for Adults with Autism Spectrum Disorder: An Exploratory Randomized Controlled Trial\n| Contributor(s):: Wijker, C., Leontjevas, R., Spek, A., Enders-Slegers, M. J.\nDog Visiting Programs for Managing Depressive Symptoms in Older Adults: A Meta-Analysis\n| Contributor(s):: Borgi, M., Collacchi, B., Giuliani, A., Cirulli, F.\nThe Psychological and Physiological Effects of Using a Therapy Dog in Mindfulness Training\n| Contributor(s):: Henry, Courtney L., Crowley, Susan L.\nThe present study was a randomized controlled trial examining the psychological and physiological effects of adding animal-assisted therapy (AAT) to a modified Mindfulness-Based Stress Reduction program (MBSR) for clients experiencing psychological distress. It was hypothesized that AAT would...\nAnimal-assisted therapy for dementia\n| Contributor(s):: Lai, N. M., Chang, S. M. W., Ng, S. S., Tan, S. L., Chaiyakunapruk, N., Stanaway, F.\nBACKGROUND: Dementia is a chronic condition which progressively affects memory and other cognitive functions, social behaviour, and ability to carry out daily activities. To date, no treatment is clearly effective in preventing progression of the disease, and most treatments are symptomatic,...\nEffects of an animal visitation intervention on the depression, loneliness, and quality of life of older people: A randomised controlled study\n| Contributor(s):: Chanelle J Buckle\nOlder people—especially those living in residential facilities—comprise a vulnerable and oftentimes frail, but large and growing subset of the global population. Various age-related and socio-political, -economic, and -environmental factors place the quality of life of older people...\nEffect of group integrated intervention program combined animal-assisted therapy and integrated elderly play therapy on live alone elderly\n| Contributor(s):: Kil, T., Yoon, K. A., Ryu, H., Kim, M.\nThe effectiveness of group combined intervention using animal-assisted therapy and integrated elderly play therapy\n| Contributor(s):: Kil, T., Kim, H. M., Kim, M.\nCanine-Assisted Therapy in Hospitalized Patients Awaiting Heart Transplantation\n| Contributor(s):: Snipelisky, D., Smidt, J., Gallup, S., Myrick, J., Bauer, B., Burton, M. C.\n| Contributor(s):: Borgi, Marta PhD, Collacchi, Barbara MSc, Giuliani, Alessandro PhD, Cirulli, Francesca PhD\nPreliminary efficacy of service dogs as a complementary treatment for posttraumatic stress disorder in military members and veterans\n| Contributor(s):: O'Haire, M. E., Rodriguez, K. E.\nBlue Rein Ranch\nAt Blue Rein Ranch, we offer a variety of services to help people heal from trauma and abuse, as well as cope with mental health concerns like anxiety, depression, ADHD, or grief.\nGaitWay Therapeutic Horsemanship\nGaitWay Therapeutic Horsemanship provides equine therapy to children and adults with a variety of disabilities. By riding, they grow stronger, both mentally and physically. We serve children and adults who have cerebral palsy, autism, developmental delays, scoliosis, ADHD, depression, anxiety,...\nPets enhance antidepressant pharmacotherapy effects in patients with treatment resistant major depressive disorder\n| Contributor(s):: Mota Pereira, J., Fonte, D.\nControlled clinical trial of canine therapy versus usual care to reduce patient anxiety in the emergency department\n| Contributor(s):: Kline, J. A., Fisher, M. A., Pettit, K. L., Linville, C. T., Beck, A. M.\nFounded in 2008, Macon TRACS, Inc. is a 501-C-3 non-profit and a PATH (Professional Association of Therapeutic Horsemanship) Center. Since 2009, Macon TRACS has served individuals with the following Special Needs: Cerebral Palsy, Autism, ADD, ADHD, Down’s Syndrome,...\nPet Therapy: Helping Patients Cope\n| Contributor(s):: Gertz, A., Rabinowitz, P. M.\n2017Am Fam Physician9674640002-838xengtext\nHooves for the Heart\nout of 5 stars\nHooves for the Heart, LLC offers hippotherapy lessons and programs in Western Colorado. In our program the use of equine therapy is designed to help heal children who suffer from emotional problems including Reactive Attachment Disorder and mental traumas by interacting with equines on..."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:88205822-f121-4a12-a325-e1ade3847052>","<urn:uuid:6c24f6d8-a164-4248-bb3d-2bd8befeb431>"],"error":null}
{"question":"Estimado experto, podría explicarme what are the four main factors of production that influence supply in an economy?","answer":"The four main factors of production are: 1) Labor, which provides the desired input in terms of firm needs, 2) Land, which represents natural resources or raw materials needed in an economy, 3) Capital, which stands for capital goods like machinery, equipment, and chemicals used in the production process, and 4) Entrepreneurship, which is the drive to profit from innovations.","context":["Introduction to Labor Markets\nThe economic theory of consumption highlights scarcity as one of the main determinants of decision making among consumers. In the same manner, firms make decisions to increase on reduces prices, among other decisions based on the current market incentives. In both situations, it all comes down to decisions that must be made in the decision-maker's best interest. Things are not very clear. They go back to data from similar situations and compare the most probable outcomes.\nLabor markets are one of the main concepts of modern economies, especially when it comes to making decisions that influence a firm's growth and the general economy. Labor is defined as the amount of physical, mental, and social effort used to produce goods and services in an economy. It is crucial in the supply of expertise, manpower, and services necessary in creating a finished product out of raw materials. Traditionally, the labor market deals with human workers and human capital. However, with the rise of technology, machines are coming up to take some of the people's main roles. Nevertheless, people still remain crucial to the markets since they are in charge of running these machines. In other words, there would be not machines if people are not involved. It is people who make an informed decision based on what affects society at large.\nAt the end of the day, it is people who consume the goods and services provided. They receive wages and use the same to buy the goods and services, even those they don't produce themselves. If possible, people would be making everything they need from the comfort of their homes to not spend anything but what they need. But as we mentioned, scarcity at the beginning of this subject is the reason people work hard every day. Most people are motivated by the need to live a comfortable life, which includes having everything the market can afford. Human needs are too many that they overwhelm the little resources available. In other words, human needs are unlimited, whereas resources to meet them are limited. Hence, there is a need for a good relationship between households and markets. People provide labor and the human-powered needed to run firms, whereas markets offer the products and services necessary to meet the modern demands of life. Those who don't have the desired skills or abilities often don't get paid living wages, whereas those with the highest setting to skills (mostly education) get the highest paying jobs. And in many countries, minimum wages have been set to make sure their workers get enough wages to cover living costs.\nThere are four main factors of production that influence supply. The main one is labor, which provides the desired input in terms of firm needs. The second factor is land, which represents natural resources or raw materials needed in an economy. Then there is the capital, which stands for capital goods like machinery, equipment, and chemicals used in the production process. The fourth fact is entrepreneurship, which is the drive to profit from innovations. It is in labor that issues like investing in the human capital rise. Modern economies have discovered the importance of investing in people through training and proper education systems. Societies that are seen to advance more economically are defined by how much they invest in their people. Qualified labor leads to the production of desired goods and encourages faster growth. Market economies use these components of supply to meet consumer needs.\nIn simple terms, the economy runs smoothly and efficiently if their members on a particular job that put their skills to maximum use. Efficiency in economies calls for the properly-outlined production process, which is mostly determined by labor. When people are paid according to their worth, they are incentivized to work more and produce more. Today, one of the main economic drives is to ensure the best match between skills, jobs, and pay, which keeps the supply of labor dynamic. This leads to some form of natural unemployment at every stage of economic growth. For instance, the aspect of frictional unemployment gives workers the ability to quit a job as they seek a better one.\nWe can all agree that labor is one of the main and most important aspects of economic growth and development, and it can be measured. This is done through the labor force of the labor pool. One must be available, willing to work, and have searched for work to be considered part of the labors. Labor forces come in different sizes based not only on the number of adults but also on the possibility of them getting a job. They appear in the number of people employed and unemployed in a country.\nPeople are not automatically counted just because they don't have a job. Many are jobless because they are not looking for employment. For instance, stay-at-home mums and dads, retired students, are unemployed due to the nature of their current life. There are also discouraged workers who have given up on looking for work.\nThe real unemployment rate takes into account everyone who would like to have full-time employment. And this rate includes discouraged workers, those working part-time since they are not fully employed, and everyone else who fall in this bracket. Economy analysts use the labor force to help discover this rate. Then they use the unemployment formula, which is the number of unemployed people divided by the labor force as provided in the economy. The result is the number of people who don't have a job by they are actively looking for one.\nAnother approach is using the labor force participation index, which is the labor force divided by the civilian non-institutionalized population. The result is the number of people available and looking for work.\nIt is hard to look at the labor force without considering productivity, which is the number of goods and services that the labor force creates. In case a specific amount of labor and fixed amounts of goods created are a lot, that is called high productivity. High productivity means greater profits; it gives the workers, company, industry, and country a competitive advantage.\nTypes of labor and the theory Labor market\nThere are many ways of categorizing labor. The main factor considered is the levels of skills. In this case, we have unskilled labor, which does not require training. It mostly includes manual labor such as farmworkers, but it can also include service work like janitorial. The next category is semi-skilled, which calls for some education or training, like manufacturing jobs.\nAnother categorization concerns the nature of the relationship with the employer. Most workers are based on wages, which means there is a boss to supervise them. They receive a set of weekly benefits, which are there wages and incentives. Other work under contract labor, which includes a contract that specifies how much work should be produced. The worker determines how much they can don, and they are paid either on commission or standard set fee. There are not benefits in this case.\nThirdly, there is slave labor, which is illegal. The worker is forced to work under bad conditions. This category may also include child labor, where children are forced to work – they may or may not be paid.\nThe labor market is a term used by economists to describe all the different markets of labor. There are many labor markets in any economy, without any that can stand alone. Instead, there are defined by the different markets in which they operate. As discussed above, labor is the backbone of economic growth and development. It differs by the type of work, e.g., retail sales are different from a scientist. It is also differentiated by skill level where we have entry and more experienced workers, and the location. It is general knowledge that markets for administrative assistance are more local or regional compared to university presidents' markets in terms of location. Even though there are many differences in the labor markets, they all tend to operate in the same manner. For instance, if there is a positive growth in the economy and wages rise in one market, it tends to affect others. It is these similarities that economists refer to when they are talking about the labor market.\nThe market, just like any other, is characterized by demand and supply. Firm demand labor and employers are willing to pay for this labor. It is not the employer who likes you or has a social conscience, but because labor is something that brings forth revenue. It is one of the most important things an employer invests in. How much a firm is willing to pay depends on the employee's skills and experience on board. The first rule of labor markets is that firms never pay more for a worker than the value of their marginal productivity. This is the only way firms can make a real profit. If, for instance, a worker produces two widgets per hour and the firm sells each for $3, the worker is bringing in $6 per hour. A profit-maximizing firm will pay the worker up to $6, not anything higher because that is how much the worker is worth the company.\nThese figures also follow the calculation of marginal product, which is the additional output a firm can produce by adding one worker to the process of production. The employer will often pay for labor by the hour, which means the marginal product is the additional output the firm gets when they add one more worker to their production line. But for this definition to work, there are some assumptions you will need to follow. For instance, you may have to assume that workers are homogeneous, meaning they come from the same backgrounds, have the same experience and skills, and put in the same effort. Hence, marginal product depends on the capital and technology that the workers have to use. For instance, a typist can type more pager per hour with an electric typewriter when using a manual typewriter. And they can make even more pages using a personal computer and a word processing software. In other words, technology has a significant impact on the labor market.\nThe labor market allocation theory\nA labor market is seen as a market where people offer their skills to employers, who, in turn, pay them in wages, salaries, and other forms of compensation. Those who participate in the labor market include anyone who is looking to offer their services for compensation, and any individual or organization looking for people to perform some labor for them. It is, therefore, a market that includes firms looking to employ and people seeking employment.\nThe standard labor theory, as seen above, is like any other resource. In general, the market determines how labor is allocated and how much it should cost. This is to say the market determines where a person should work and the minimum/maximum compensation they should receive. This is where the theory of market allocation comes in.\nLabor market theory is simply a model whose main task is to simplify reality so that we can understand a complex idea. In this case, there are some assumptions that this model will make. The labor market theory assumes that the most important motivation in people's labor market is a wage, which includes other monetary compensations. It also assumes that workers are fungible; they are mobile; and that wages are flexible.\nLike any other, in a labor market, the relationship between sellers (people looking for employment) and buyers (firms employing) is significant. Note that reality can be oversimplified when using the assumptions above in relation to these factors to get the general idea of how things are. The model will not predict the correct outcome if the assumptions are true in any specific use. Most important, demand and supply for labor follow the general economic laws of demand and supply. When there is too much demand for labor by firms, the prices go high, and if there is too little demand, the prices go high. This means, demand for labor increases as wages decrease and vice versa. It is important to understand these concepts and everything that relates to labor, including human capital and unemployment, for success in this course.\nAuthor: James Hamilton"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:28e9db45-925b-4b3e-b774-2cbe1cc039b3>"],"error":null}
{"question":"I recently traveled abroad and noticed some weight changes - which has more severe health consequences: getting infected with tapeworms or trying the tapeworm diet for weight loss?","answer":"Trying the tapeworm diet is extremely dangerous and can have more severe consequences than accidentally getting infected, since deliberately swallowing tapeworm eggs puts you at high risk for serious complications. While both situations can cause malnutrition and weight loss, intentional tapeworm ingestion can lead to cysticercosis (tissue infection), neurocysticercosis (affecting brain and spinal cord), epilepsy, meningitis, and dementia. Tapeworms can grow up to 30-80 feet inside a host, block intestines, and interfere with organ function. Natural infections, while still dangerous, are typically treated effectively with antiparasitic medication and have a cure rate over 95%.","context":["Parasites are organisms that live in and feed off a living host. There are a variety of parasitic worms that can take up residence in humans. Among them are flatworms, thorny-headed worms, and roundworms.\nThe risk of parasitic infection is higher in rural or developing regions. The risk is great in places where food and drinking water may be contaminated and sanitation is poor.\nRead on to learn more about parasitic worms, plus how to avoid becoming an unwitting host.\nWhen it comes to parasitic infection, flatworms and roundworms are likely culprits. These two types of parasitic worms can be found in a variety of habitats. They aren’t always visible to the naked eye.\nYou can get a tapeworm, which is a type of flatworm, by drinking water contaminated with tapeworm eggs or larvae. Raw or undercooked meat is another way tapeworms can find their way into people.\nTapeworms embed their heads into the intestinal wall and remain there. From there, certain types of tapeworms can produce eggs that mature into larvae that migrate to other parts of the body.\nFlukes are a type of flatworm. People are less likely than animals to contract flukes. Raw watercress and other freshwater plants are the main sources of flukes in humans. You can also get them when you drink contaminated water.\nThey make their home in your intestines, blood, or tissues. There are many varieties of flukes. None reach more than a few inches in length.\nHookworms are transmitted through feces and contaminated soil. The most common way to make contact with this type of roundworm is to walk barefoot on soil infested with hookworm larvae. They can pierce through the skin.\nHookworms live in the small intestine, where they attach themselves to the intestinal wall with a “hook.” They’re usually less than half an inch long.\nPinworms are tiny, fairly harmless worms. They’re more common in children. These roundworms, when fully matured, live in the colon and rectum. The female lays eggs around the anus, usually during the night.\nThe eggs can survive on bedding, clothing, and other materials. People contract them when they touch the eggs and end up putting them in their mouths. The eggs are so small you can even breathe them in if they become airborne. They’re easily passed among children and caregivers or in institutions.\nAlthough pinworm infections are usually harmless and easily treatable, there have been less common cases of pinworms in the appendix that, when present, are usually in children and rarely in adults. One journal article found pinworms to be a rare cause of acute appendicitis.\nAnother journal article noted that pinworms in the tissues of a surgically removed appendix are an infrequent finding, and researchers also state that parasite infections only rarely cause acute appendicitis.\nHowever, these articles note that the symptoms of an intestinal parasite infection may mimic symptoms one would see in acute appendicitis, although appendicitis may or may not actually be occurring.\nTrichinosis roundworms are passed among animals. The most common way humans get trichinosis is by eating undercooked meat that contains the larvae. The larvae mature in your intestines. As they reproduce, those larvae can travel outside the intestines into muscle and other tissue.\nIt may be hard to believe, but you don’t always know when you have an uninvited guest inside you. You may not have any symptoms, or they may be quite mild.\nThe symptoms you could have include:\n- lack of appetite\n- abdominal pain\n- weight loss\n- general weakness\nIn addition, tapeworms can cause:\n- lumps or bumps\n- allergic reaction\n- neurological problems such as seizures\nIt may take weeks or months to notice additional symptoms of fluke infection. These may include:\nAdditional symptoms of hookworms include:\n- itchy rash\nAs trichinosis worms travel through the bloodstream and enter other tissue or muscles, they can cause:\n- swelling of the face\n- muscle pain and tenderness\n- light sensitivity\nIf you’re experiencing any unusual symptoms, especially if you’re returning from a trip to another country, consult your doctor. They’ll work with you to determine the cause of your symptoms.\nDiagnostic tests will be necessary to identify the culprit:\n- A fecal test involves checking a stool sample for parasites, larvae, or eggs.\n- A colonoscopy can be useful when stool samples turn up no evidence of parasites as a cause of diarrhea. They may also help eliminate other causes for your symptoms.\n- A blood test can be used to detect some types of parasites in the blood.\n- Imaging tests like MRI, CT scan, or X-rays can be used to detect organ injury caused by parasites.\n- A tape test involves placing clear tape around the anus. The tape can be examined under a microscope for the presence of pinworms or their eggs. But even with the naked eye, sometimes you may be able to see evidence of pinworms around a child’s anus within the first few hours of falling asleep.\nThe main treatment is prescription antiparasitic medication. This family of drugs can kill parasites and help pass them through your system.\nThe antiparasitic medication you’ll receive, doses schedule, and duration of treatment will depend on the type of parasite infection you have. Don’t stop taking the medication in the middle of the course, even if you feel better.\nIn very severe cases in which parasites have invaded other parts of the body, additional treatments like surgery and other medications to address additional problems caused by the parasites may be necessary.\nAsk your doctor if you should follow a special diet or take nutritional supplements during this time. Follow up with your doctor as advised.\nMost people respond well to treatment and feel better within a few weeks. A full recovery can be expected in most cases.\nIt may take longer to recover if you have:\n- a severe case\n- compromised immune system\n- a coexisting health condition\nThe following tips can often help prevent parasitic worm infection:\n- Never eat raw or undercooked meat, fish, or poultry.\n- Avoid cross-contamination during food prep by keeping meat separate from other foods.\n- Disinfect all cutting boards, utensils, and countertops that touched raw meat.\n- Don’t eat watercress or other freshwater plants raw.\n- Don’t walk barefoot in places where soil may be contaminated by feces.\n- Clean up animal waste.\nAlso be sure to give your hands a good scrubbing with soap and water at these times:\n- before eating\n- before food prep\n- after touching raw meat\n- after using the toilet\n- after changing a diaper or caring for someone who’s sick\n- after touching an animal or animal waste\nIt’s more difficult to prevent parasitic worm infection when you’re traveling to foreign countries, especially in regions where sanitation is a problem. That’s when you should be extra vigilant.\nWhen traveling, be sure to:\n- Be aware of how your food is prepared.\n- Drink only bottled water.\n- Carry hand sanitizer. Soap and water is best, but if you don’t have access to soap and running water, it can help prevent parasitic worm infection.\nHealthline and our partners may receive a portion of revenues if you make a purchase using a link above.","Yes, There is a Tapeworm Diet. No, You Should Not Try It!3 years ago | Nutrition\nBy Joy Stephenson-Laws, JD, Founder\nHave you ever heard about the Victorian Tapeworm Diet? This diet involves swallowing a parasite (a potentially disease-causing organism). It reportedly gained popularity in the early 1900s among Victorian women who were put under immense pressure to be thin, wear tight corsets and “look perfect.”\n“The idea is simple, and gross. You take a pill containing a tapeworm egg. Once hatched, the parasite grows inside of the host, ingesting part of whatever the host eats. In theory, this enables the dieter to simultaneously lose weight and eat without worrying about calorie intake,” according to one source.\nWhile the possibility of being able to eat whatever you want and still lose weight may sound appealing, tapeworms may cause major harm to the host.\nThe first thing to know is that the term “tapeworm” actually refers to several different types of parasites that can make their home in the human intestine for a very long time – sometimes for 20 years! These parasites are considered to be in the taxonomic class of flatworms called Cestoda, and more than 1,000 species have been identified.\nThey are categorized by where they live before their eggs infect their human host via consumption of raw or undercooked meat, animal feces or contaminated water. Tapeworms may be found in pork, beef, fish and dog.\nHere are some different types of tapeworms:\n- Taenia saginata (beef tapeworm)\n- Taenia solium (pork tapeworm)\n- Taenia asiatica (Asian tapeworm, also called Asian fish tapeworm)\nThese species of tapeworms cause the parasitic infection taeniasis.\nTapeworms are equal-opportunity parasites that show absolutely no preference for gender, ethnicity or age. With that said, children are more prone to getting them since they usually are not all that careful with hand-washing and personal hygiene, both of which can limit the spread and infection with the parasite.\nTapeworms can grow very long in a host and interfere with proper organ function. In this case, doctors removed a six-foot long tapeworm from a man's intestine. Several sources even say that tapeworms can grow to 30 feet or even 80 feet inside a host! Tapeworms may even cause epilepsy, meningitis and dementia.\nUnless you deliberately swallow some tapeworm eggs (as someone following the tapeworm diet would do), if you have a tapeworm you may not even know it since most people have no symptoms. Symptoms of having tapeworms usually include:\n- Being tired all the time or general weakness\n- Gastrointestinal distress such as stomach pain, nausea, vomiting and diarrhea\n- Weight loss without going on a diet or changing your exercise routine\n- Sleeping difficulties\n- Seeing tapeworm eggs, larvae or segments from the tapeworm’s body in your stool\nTaeniasis can develop into cysticercosis, which is an infection of the tissue. Cysticercosis is caused by larval cysts of the pork tapeworm. “These larval cysts infect brain, muscle, or other tissue, and are a major cause of adult onset seizures in most low-income countries. A person gets cysticercosis by swallowing eggs found in the feces of a person who has an intestinal tapeworm. People living in the same household with someone who has a tapeworm have a much higher risk of getting cysticercosis than people who don’t,” reports the Centers for Disease Control and Prevention (CDC).\nRecently, an 18-year-old boy in India died of neurocysticercosis, when cysticercosis affects the brain and spinal cord. You can read more about this tragic incident here.\n“Neurocysticercosis is a leading cause of adult onset epilepsy worldwide. It is costly to diagnose and treat but entirely preventable,” reports the CDC.\n“There are an estimated 1,000 new hospitalizations for neurocysticercosis in the United States each year. Cases are most frequently reported in New York, California, Texas, Oregon, and Illinois. Additionally, neurocysticercosis creates a tremendous economic burden. In a recent study, the average charge of hospitalization due to neurocysticercosis was $37,600, with the most common form of payment being Medicaid (43.9%). Currently, there is little being done to monitor prevent, or identify and treat neurocysticercosis.”\nI think what you can gather from all of this information about tapeworms is that it would be absolutely crazy and a major risk to your health to intentionally swallow a pill containing a tapeworm egg, in an attempt to lose weight.\nBut the risks have not stopped some people.\nBack in 2013, a woman in Iowa swallowed a tapeworm she reportedly bought of the internet. After swallowing the pill that contained the tapeworm, she told her doctor what she had done.\n“To get the parasite out of a person's body, doctors will usually prescribe an anti-worm medication like praziquantel or niclosamide, which force all the muscles in the worm's body to contract, killing it. The tapeworm will then harmlessly pass through the intestines and out of the body,” according to this news report discussing the woman in Iowa.\nThis report from 2010 discusses how dieters in Hong Kong were swallowing tapeworms to lose weight. According to the report, the worms can lay up to 200,000 more eggs a day inside the body!\nDr. Michael Mosley, who is known for “experimenting on himself,” ingested a tapeworm to see if it was worth the “weight loss benefits,” according to this 2014 Medical Daily report.\n“Mosley traveled to Kenya in order to pick up the parasites, which came from cysts on a cow’s tongue that contained tapeworm eggs. After ingesting the tapeworm, Mosley tracked his progress for about six weeks. He kept a food diary in which he noted any changes in his behavior or physical appearance. Contrary to what some may believe about the tapeworm diet, Mosley said he found himself craving carbohydrates and sugars.”\nIt makes sense that you may crave carbs and sugar, considering a parasite is invading your body and depleting your energy stores.\nHe actually gained two pounds from this experiment and took medication to rid his body of the tapeworm.\nYou are more likely to become malnourished, not slender and “pretty to look at.”\n“One tapeworm can't absorb enough food and nutrients to make a big difference in weight, according to scientific research. But the parasite can cause anemia and malnutrition,” according to one source.\nMalnutrition, in my opinion, is one of the greatest risks of ingesting a tapeworm -- especially a deficiency in vitamin B12. As mentioned, tapeworms can block your intestines and take up space that other organs need. This can make it harder for these organs to do their jobs in keeping you healthy.\nYour bodies need to absorb and efficiently use nutrients, such as water, vitamins, minerals, fats and proteins, in order to lose weight and maintain a healthy weight.\nSo if you suspect you have a tapeworm, it’s important to see a competent healthcare professional as soon as possible. Diagnostic tests for tapeworms include blood tests to look for antibodies caused by the tapeworm infection, imaging scans such as an MRI or X-Ray, stool tests to look for eggs and larvae and organ tests to make sure your vital organs are functioning properly. The good news is that with the proper treatment, usually oral medication, tapeworms are more than 95 percent curable.\nHow to be Proactive\nThe first thing you need to know is that if you’re trying to get to a healthy weight, swallowing a tapeworm is not the way to do it. There are many ways, including exercising and eating healthily, to lose weight in a healthy way and then keep it off. This is a great goal since being overweight puts you at risk for a wide variety of health problems and diseases, from joint stress to diabetes and from cardiovascular disease to kidney disease. You can find a wealth of information about dieting and weight management on the pH website.\nThere are several things you can do to protect yourself and your family from a tapeworm infection. These include:\n- Making sure to cook all meats and fish to internal temperatures recommended by the CDC of 145 degrees F for pork and other meats, and 160 degrees F for ground meat (you can also freeze meat and fish for seven days, which is effective for killing eggs and larvae)\n- Boil, disinfect or drink bottled water if you are not sure about the water’s potability\n- Be sure to thoroughly wash your hands (and teach your children to do so) after using the bathroom, gardening or being around farm animals (or any other animals) and before eating\n- Regularly clean and disinfect food preparation surfaces in your kitchen and be sure to wash your hands after handling raw meats and fish\n- Make sure your dog is treated for tapeworms as indicated by your veterinarian, and never feed your pet raw meat or fish\nSince tapeworms literally “share” the nutrients you are getting from your food, if you or anyone in your family has been treated for tapeworms you should get a nutrient test to see what your body may be lacking. Getting this test is also a good idea in general, especially if you are dieting.\nTo learn about specific nutritional deficiencies that may be preventing you from achieving your weight loss goals, read here.\nEnjoy your healthy life!\nThe pH professional health care team includes recognized experts from a variety of health care and related disciplines, including physicians, attorneys, nutritionists, nurses and certified fitness instructors. This team also includes the members of the pH Medical Advisory Board, which constantly monitors all pH programs, products and services. To learn more about the pH Medical Advisory Board, click here."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:3c711f53-eabc-4758-ad03-2014a96ba5e1>","<urn:uuid:a06e9bf9-38b6-424b-b030-3c484bc59471>"],"error":null}
{"question":"I'm interested in fusion research breakthroughs. What are the target timelines for the BETHE program's fusion development compared to the disruption control system being developed at PPPL?","answer":"The BETHE program aims to develop grid-ready fusion demonstration within approximately 20 years, with the goal of making fusion contribute to low-carbon energy demand in the latter half of the 21st century. In contrast, the disruption control system being developed at PPPL (the electromagnetic particle injector) focuses on more immediate technical challenges, targeting response times of less than 10 milliseconds to mitigate plasma disruptions, particularly for use in ITER. This is significantly faster than current gas-propelled systems which take around 30 milliseconds.","context":["The US Department of Energy announced the winners of $32 million in funding for 15 projects as part of the ARPA-E Breakthroughs Enabling THermonuclear-fusion Energy (BETHE) program. (Earlier post.) These projects will work to develop timely, commercially viable fusion energy, with the goal to increase the number and performance levels of lower-cost fusion concepts.\nFusion energy technology holds great potential to be a safe, clean, reliable energy source, but research and development of fusion technology is often constrained by prohibitive costs. BETHE teams will build on recent progress in fusion research and the growing fusion community to lower costs and further foster viable commercial opportunities for the next generation of fusion technology.—Under Secretary of Energy Mark W. Menezes\nBETHE projects will work to deliver higher-maturity, lower-cost fusion options via three research categories:\nConcept Development to advance the performance of inherently lower-cost but less-mature fusion concepts;\nComponent Technology Development that could significantly reduce the capital cost of higher-cost, more-mature fusion concepts; and\nCapability Teams to improve/adapt and apply existing capabilities (including theory/modeling, machine learning, and diagnostics) to accelerate the development of multiple concepts.\nCommercial fusion technology has long been viewed as an ideal energy source. However, there remains a need to lower the costs of fusion development and accelerate its development timeline to have appreciable impact. If a grid-ready fusion demonstration can be realized within approximately 20 years, while achieving cost competitiveness, then fusion can contribute to meeting low-carbon energy demand and achieving cost-effective deep decarbonization in the latter half of the 21st century.\nBETHE projects address these needs by building on Advanced Research Projects Agency–Energy’s (ARPA-E) first focused fusion program, ALPHA, to grow the number of privately funded fusion companies. BETHE teams will pursue additional approaches that reduce cost, unit size, and complexity of fusion systems, while also smoothing the path to fusion commercialization to include public, private, and philanthropic partnerships with the BETHE teams.\nWinning BETHE projects are:\nCategory A: Development of Lower-Cost Concepts\nUniversity of Wisconsin-Madison. An HTS Axisymmetric Magnetic Mirror on a Faster Path to Lower Cost Fusion Energy - $5,000,000\nThe Wisconsin High-field Axisymmetric Mirror (WHAM) project at the University of Wisconsin-Madison will leverage advances in the stability and confinement of the mirror fusion concept, innovative plasma heating, and high-field superconducting magnets to demonstrate a potentially transformative development path toward a low-cost linear fusion device. The project aims to demonstrate a novel “end cell” that confines stable, heated plasmas with electron temperatures exceeding 1 keV and a fusion triple product exceeding 1018 keV s/m3. Success in this project would justify follow-on pursuit of the low-cost Break-Even Axisymmetric Tandem (BEAT) device, which would use two of the end cells at either end of a longer central mirror cell to reach breakeven conditions.\nZap Energy. Sheared Flow Stabilized Z-Pinch Performance Improvement - $1,000,000\nThe SFS Z pinch is an approach that gets smaller as the fuel is heated closer to fusion conditions. An electrical current is driven through the fusion fuel, creating self-generated magnetic fields that compress and heat the fuel toward fusion conditions. This may be the simplest and most compact of all known controlled fusion approaches, as it does not require magnetic coils nor any external heating systems other than the source to drive the electrical current. Under the ALPHA and OPEN 2018 programs, the SFS Z-pinch demonstrated a fusion triple product exceeding 1017 keV s/m3.\nThis project will enable Zap Energy to implement an upgrade to their new SFS Z-pinch device to allow independent control of the plasma formation and acceleration stages. They will use the new, upgraded device to advance their triple product toward breakeven conditions.\nUniversity of Maryland, Baltimore County.\nCentrifugal Mirror Fusion Experiment - $4,000,000\nThe University of Maryland, Baltimore County will advance the performance of the centrifugal mirror (CM) fusion concept, which has previously demonstrated stable plasmas with temperatures above 100 eV. The CM has a simple, axisymmetric geometry and provides a potential low-cost pathway to a breakeven experiment.\nThe team will azimuthally rotate a mirror-shaped magnetized plasma to supersonic speeds using high-voltage biasing between a central rod and outer electrode rings. The rotation will stabilize, heat, and centrifugally confine the plasma, potentially eliminating the need for costly auxiliary heating systems requiring high recirculating power, which would degrade the economics of a fusion power plant. The project aims to overcome engineering challenges of the high-voltage biasing, and scientific challenges of achieving good stability and confinement while pushing into higher-temperature regimes. The project aims to achieve a triple product exceeding 1017 keV s/m3.\nNK Labs, LLC.\nConditions for High-Yield Muon Catalyzed Fusion - $830,000\nNK Labs seeks to advance muon-catalyzed fusion, which uses muons (a subatomic particle much more massive than an electron) to induce fusion between deuterium and tritium at temperatures 100,000 times lower than that of conventional fusion approaches. If each muon can catalyze around 500 fusion reactions (compared to the present state-of-the-art of 110–150 reactions), then a net-energy-gain system may become feasible. NK Labs will measure muon-catalyzed fusion rates and muon-recycling rates in a laser-heated, diamond anvil cell at 100-times higher pressures than what has been done. They will update publicly available computer models and databases based on their results. If the results are favorable, it could enable a new low- cost path toward net-gain fusion.\nUniversity of Washington.\nDemonstration of Low-Density, High-Performance Operation of Sustained Spheromaks and Favorable Scalability Toward Compact, Low-Cost Fusion Power Plants - $1,500,000\nThis project will advance the technical viability of a novel method for efficiently sustaining stable, high- performance spheromak plasmas to serve as the basis of compact, low-cost fusion power plants. The University of Washington will advance the method of Imposed-Dynamo Current Drive (IDCD) for sustaining spheromak plasmas. IDCD can efficiently couple large amounts of power to the plasma at much lower costs relative to other methods of higher-frequency plasma heating. The proposed R&D aims to increase plasma performance to enable ion and electron temperatures > 100 eV during sustainment on an existing experimental prototype, and increase confidence in projections and designs of future, higher-performance devices.\nLos Alamos National Laboratory. Target Formation and Integrated Experiments for Plasma-Jet Driven Magneto-Inertial Fusion - $4,618,001\nLos Alamos National Laboratory (LANL) will develop an innovative approach to fusion energy: plasma-jet driven magneto-inertial fusion. This concept uses a spherical array of plasma guns to form a spherically imploding plasma shell, or “liner,” which compresses and heats a pre-injected magnetized plasma “target” in a bid to attain fusion conditions.\nThis project seeks to develop a sub-scale magnetized target plasma, and perform first integrated liner-on-target compression experiments at LANL’s Plasma Liner Experiment facility. Compression and heating will be studied and compared with computer simulations. The results will illuminate the viability and scaling behavior of this class of fusion devices, informing the prospects for future development and energy scaleup of this concept.\nCategory B: Technology Development to Lower the Cost of More- Mature Concepts\nCommonwealth Fusion Systems. Pulsed High Temperature Superconducting Central Solenoid for Revolutionizing Tokamaks - $2,390,000\nCommonwealth Fusion Systems (CFS) aims to resolve several higher-risk engineering challenges in order to design a new fast-ramping, high-temperature-superconducting (HTS) central solenoid (CS). The CS sits at the center of a tokamak (a donut-shaped fusion machine) and induces an electric field and electrical current along the toroidal axis of the donut. An HTS CS could transform tokamak development, enabling repetitive, long-pulse (hours) tokamak operation that could potentially eliminate or drastically reduce the need for costly auxiliary current drive. If successful, this could open a new pathway for pulsed HTS tokamaks to become the quickest and lowest-cost pathway to a tokamak-based fusion power plant.\nPrinceton Plasma Physics Laboratory.\nStellarator Simplification using Permanent Magnets - $3,000,000\nPrinceton Plasma Physics Laboratory (PPPL), in partnership with SABR Enterprises, will design and build a prototype structure that houses an array of rare-earth permanent magnets to generate the precise shaping fields of an optimized, quasi-axisymmetric stellarator design. The stellarator concept might arguably claim the best combination of achieved performance and remaining scientific risk of any fusion approach, and has low recirculating power, minimal auxiliary systems, and no time-dependent magnet systems, all of which are major advantages for a commercial fusion system. However, present state-of-the-art stellarators require complex 3D magnetic coils with very high fabrication and assembly costs. Use of permanent magnets along with planar toroidal field coils could dramatically simplify stellarator construction, assembly, and maintenance, and place the stellarator on a compelling path toward lower-cost fusion energy.\nUniversity of Rochester. Advanced Inertial Fusion Energy Target Designs and Driver Development - $1,750,000\nThe University of Rochester Laboratory for Laser Energetics and the Naval Research Laboratory will advance inertial fusion energy (IFE) by developing (1) innovative direct-drive, high-bandwidth, high-gain target designs using high-bandwidth laser technologies with < 1 MJ of laser input energy, and (2) high-efficiency, high- bandwidth IFE drivers to eventually enable experimental demonstration of the advanced target designs.\nThe new laser-driver technologies, including both diode-pumped solid-state and excimer lasers, are expected to mitigate laser-plasma instabilities, potentially allowing for greater and more-symmetric energy coupling to the target. This work leverages the multiple decades of investment into inertial confinement fusion (ICF), which has achieved high values of fusion triple product, and will help place ICF on a path toward lower-cost IFE.\nCategory C: Capability Teams to Support Multiple Concepts\nVirginia Polytechnic Institute and State University. Capability in Theory, Modeling, and Validation for a Range of Innovative Fusion Concepts Using High-Fidelity Moment-Kinetic Models - $2,400,000\nVirginia Tech and PPPL will apply a versatile set of computational plasma modeling capabilities to better understand and advance the performance of lower-cost fusion concepts. This Capability Team will use fluid and reduced kinetic models to achieve this goal, including building on its existing open-source simulation technology, Gkeyll, and a multi-phase, incompressible magnetohydrodynamic model to study liquid-metal wall dynamics in the presence of fusion plasma. The team will perform high-fidelity kinetic plasma simulations that can also account for complex plasma-wall interactions, to support the development of multiple lower-cost concepts.\nSapientai, LLC. Data-Enabled Fusion Technology - $1,650,000\nSapientai’s DeFT (Data-enabled Fusion Technology) Capability Team will use machine learning and artificial intelligence to accelerate the path toward commercial fusion energy. Fusion experiments generate substantial amounts of data. This project will carefully curate the data from multiple projects, and use the curated database to aid in discovering and enhancing predictive understanding, and in helping guide and prioritize experiments that take a lot of time and funding. The project seeks to apply state-of-the-art techniques in machine learning to accelerate the development of multiple lower-cost fusion concepts.\nUniversity of Rochester. A Simulation Resource Team for Innovative Fusion Concepts - $2,000,000\nEstablishing an adequate simulation capability for a new fusion concept can easily be more expensive and time-consuming than building the first experiment. This Capability Team will provide simulation support for Concept Teams and independent analysis of fusion concepts. The codes central to this project will be FLASH, TriForce, and OSIRIS, chosen because they are flexible, high-performance computing codes, capable of 1D, 2D and 3D simulations, and can eventually all be used by concept teams to carry out their own simulations.\nMassachusetts Institute of Technology. Radio Frequency Scenario Modeling for Breakthrough Fusion Concepts - $1,250,000\nFusion requires confining plasmas at extraordinarily high temperatures. One of the most promising ways to heat plasmas to these temperatures is with electromagnetic waves. By increasing power by thousands of times more than the kilowatts used in household appliances, a plasma is heated to the temperatures required for fusion. Complex analytic theory and computer simulations are required to design effective and efficient plasma- heating scenarios.\nMIT’s project seeks to apply established state-of-the-art theoretical and simulation tools, developed and tested by the fusion community on more traditional concepts, to accelerate the development of potentially transformative, lower-cost fusion concepts.\nOak Ridge National Laboratory.\nMagnetic Field Vector Measurements Using Doppler-Free Saturation Spectroscopy - $600,000\nOak Ridge National Laboratory (ORNL) will build a non-perturbative, portable diagnostic to measure the topology of the equilibrium magnetic field vector in potentially transformative, magnetically confined fusion devices. The measurement is based on an innovative spectroscopic technique that has already been demonstrated in the laboratory. The new diagnostic will be built and tested during this project, and will be ready to deploy to multiple fusion experiments around the country.\nLos Alamos National Laboratory.\nElectromagnetic and Particle Diagnostics for Transformative Fusion-Energy Concepts - $375,000\nLos Alamos National Laboratory will provide a suite of proven, absolutely calibrated, vacuum ultraviolet (VUV) and soft x-ray diagnostics to characterize the performance of a number of lower-cost, potentially transformative fusion-energy concepts. Spectral measurements will enable the identification of impurities and their spatial and temporal variation in the plasmas. LANL will also seek to advance a state-of-the-art, solid-state x-ray imager to make soft x-ray movies of the hot plasma core, enabling visualization of the evolution of instabilities.","Fast action: A novel device may provide rapid control of plasma disruptions in a fusion facility\nScientists seeking to capture and control on Earth fusion energy, the process that powers the sun and stars, face the risk of disruptions — sudden events that can halt fusion reactions and damage facilities called tokamaks that house them. Researchers at the U.S. Department of Energy’s (DOE) Princeton Plasma Physics Laboratory (PPPL), and the University of Washington have developed a novel prototype for rapidly controlling disruptions before they can take full effect.\nThe device, called an “electromagnetic particle injector” (EPI), is a type of railgun that fires a high-velocity projectile from a pair of electrified rails into a plasma on the verge of disruption. The projectile, called a “sabot,” releases a payload of material into the center of the plasma that radiates, or spreads out, the energy stored in the plasma, reducing its impact on the interior of the tokamak.\nDeeply penetrating payloads\nThis process may prove faster and may allow payloads to penetrate more deeply into the plasma than today’s most developed techniques. Current systems release pressurized gas or gas-propelled shattered pellets using a gas valve into the plasma, but with velocity limited by the mass of the gas particles. “The primary advantage of the EPI concept over gas-propelled systems is its potential to meet short-warning time scales,” said Roger Raman, a University of Washington physicist on long-term assignment to PPPL and lead author of a Nuclear Fusion paper that describes the new system.\nThe risk of disruptions is particularly great for ITER, the large international tokamak under construction in France to demonstrate the feasibility of fusion power. ITER’s dense, high-power discharges of plasma, the state of matter that fuels fusion reactions, will make it difficult for current gas-propelled methods of mitigation to penetrate deeply enough into the highly energetic ITER plasma to take good effect.\nOn ITER, mitigation is desired in less than 20 milliseconds, or thousands of a second, from the warning of a disruption, with 10 milliseconds as ideal. Tests of the EPI prototype show that it can deliver a payload of correctly sized particles in fewer than 10 milliseconds, compared with 30 milliseconds for gas-propelled systems.\nThe prototype, built at the University of Washington, harkens back to a fusion reactor fueling system that Raman worked on years ago. That system injected plasmoids, football shaped plasmas with their own magnetic fields, that were injected into a fusion plasma at high velocity. Raman adapted some features of the system to allow much more mass to be injected in a simpler configuration, as would be required for a long standby mode of operation, to develop the EPI.\nElectrically conducting rails\nThe prototype houses the sabot between two electrically conducting rails located some 2-to-3 centimeters apart and connected to a capacitor bank that holds an electrical charge. Discharging the bank produces electromagnetic forces that accelerate the sabot, enabling release of the payload in just 2 milliseconds. The material, consisting of light-metal granules or pellets, would radiate the energy of a disruption from the center of the plasma to the edge, spreading out the energy and weakening its impact on the tokamak walls.\nFurther development of the EPI system has been proposed to be conducted at PPPL. Plans call for construction of second-and third-generation prototypes with increasingly strong magnetic fields over a three-year period, followed by deployment on a tokamak in the fourth year. Results so far, as reported in Nuclear Fusion, provide a degree of confidence that an effective EPI system can be developed to mitigate powerful disruptions on ITER.\nJoining Raman in this research publication are Jonathan Menard and Masa Ono of PPPL, and Wei-Siang Lay and Thomas Jarboe of the University of Washington. Support for this work comes from the DOE Office of Science.\nPPPL, on Princeton University's Forrestal Campus in Plainsboro, N.J., is devoted to creating new knowledge about the physics of plasmas — ultra-hot, charged gases — and to developing practical solutions for the creation of fusion energy. The Laboratory is managed by the University for the U.S. Department of Energy’s Office of Science, which is the largest single supporter of basic research in the physical sciences in the United States, and is working to address some of the most pressing challenges of our time. For more information, please visit science.energy.gov.\nSlow-motion video of a 5 millisecond EPI shot. (Video courtesy of Wei-Siang Lay.)\nPrinceton Plasma Physics Laboratory is a U.S. Department of Energy national laboratory managed by Princeton University.\n© 2019 Princeton Plasma Physics Laboratory. All rights reserved."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:ce5a9dc1-fad5-487c-b66f-84cc3ebcda06>","<urn:uuid:710e97be-b9c8-4399-9abe-b851bd858e5d>"],"error":null}
{"question":"What factors influence enzyme synthesis in the human body and what is driving the growth of the global enzyme market?","answer":"In human biology, enzyme synthesis can be disrupted by multiple factors including chronic stress, genetics, illness, malnutrition, and overuse of pharmaceutical drugs. When enzyme function is compromised, it can lead to disease and various health issues. In the global market context, the enzyme industry is experiencing growth driven by several factors: increasing need for sustainable solutions, advancements in biotechnology and protein engineering, and the introduction of new probiotic products. The COVID-19 pandemic has positively impacted the overall industry as consumers have become more health-conscious, particularly boosting demand in the cleaning product, food, animal feed, and specialty enzyme sectors, though it negatively affected the biofuel and textile enzyme segments.","context":["Everybody knows that cells are the fundamental building blocks of life. Every living organism is composed of cells, which have certain specialized functions. While some living organisms (e.g. bacteria) are unicellular, humans have millions of cells in their body. An average adult human being has around 100 trillion cells!\nCan you imagine the countless chemical reactions that are carried out inside a human body? Enzymes facilitate most of these reactions. These reactions are extremely important to our overall health and vitality. Now, embrace your inner nerd and check out the facts below about what enzymes are and why we should care about consuming foods that are “enzyme rich”.\nWhat are Enzymes?\nEnzymes are particular proteins that are found in the cells of living organisms. Like any other protein, enzymes consist of long chains of amino acids held together by peptide bonds.\nEnzymes control any chemical reaction that takes place in the body. They speed up the chemical reactions, making them millions of times faster.\nWhat do Enzymes do?\nGenerally, every enzyme acts upon a specific target called “substrate” which is transformed into usable products through the action of the enzyme. Once the reaction is complete, the enzyme remains intact and the substrate transforms into a product.\nWhile most enzymes are associated with metabolic processes like digestion and breathing, there are some, which are related to muscle and nerve function, blood clotting, healing of wounds, controlling the production of hormones, and even destroying pathogens and environmental toxins. Some enzymes help to break down molecules, while others merge small molecules and form large ones. Certain enzymes can transform one molecule into another.\nLet’s consider the following examples:\n- The digestive system - enzymes help the organism cut down complex molecules into smaller ones, which are then used by the body as a “fuel”.\n- DNA replication - Enzymes unwind the DNA coils and replicate the information.\n- Liver enzymes - the liver cuts down toxins in the body. It uses a range of enzymes to be able to do this.\nExamples of specific Enzymes\nThere are numerous types of enzymes in the human body. But a specific enzyme is responsible for a particular chemical reaction only!\nEnzymes can be broadly classified into:\n- Metabolic enzymes – they are mainly responsible for reactions related to detoxification and energy production.\n- Digestive enzymes - they are involved in chemical reactions that decompose the food we eat and convert it into energy.\n- Food enzymes - The body gets food enzymes from the food we eat or from the consuming of enzyme supplements. The enzyme called cellulase, which is responsible for digesting fibers, is not produced by the body. It has to be derived from the raw food we eat.\nWhy are enzymes important for digestion?\nEnzymes are essential for good digestion and a healthy body. They work with other chemicals in the body to decompose food into molecules. For example, carbohydrates fuel your body with energy. Proteins build and repair your muscles and are very helpful when you put out a lot of physical effort; during an intense exercise session or while working on a physical task like shoveling your driveway, etc.\nFactors that affect Enzyme action\n- Temperature - Usually, high temperatures increase the rate of reactions involving enzymes. The best temperatures for such reactions are said to be around 37 ºC to 40 ºC. Once the temperature goes above this level, the enzymes get altered, and they are no longer fit for reaction with substrates. At lower temperatures, they will still function but much more slowly.\n-pH - Variations in pH may also influence the functioning mechanism of enzymes. The optimum pH level may vary from one enzyme to another, as per the site of their action. Variations from that pH level may weaken the activity of enzymes. A very high or low pH results in converted enzymes that cannot hold the substrate perfectly. Similarly, enzymes can only function in a certain pH range (acidic/alkaline). Their predilection depends on where they are found in the body. For instance, enzymes in the intestines work best at 7.5 pH, whereas enzymes in the stomach work best at 2.0 pH because the stomach is a lot more acidic.\n-Concentration - The rate of enzymatic activities may increase with the concentration of enzymes and substrates. Scientists have studied the effect of high enzyme concentration upon the reaction rate. They concluded that the substrate must be present in an excess amount.\nGreat Food=Healthy Enzymes=Healthy Body\nEnzymes are extremely important for any living body as they are organic catalysts which facilitate chemical reactions in the body. Enzymes are needed for metabolic processes such as respiration, digestion and immunity. When enzymes function properly, homeostasis within the body is protected. If an enzyme is lacking or has an incorrect shape due to a mutation, this can lead to disease within the organism. Chronic stress, genetics, illness, malnutrition and the overuse of pharmaceutical drugs can disturb the internal synthesis of enzymes.\nMany food products that people eat today have not only been harshly processed but also transported long distances before being sold in stores. Unfortunately, our diets generally lack fresh, living, enzyme-rich foods. Even superficial enzyme deficiencies may cause digestive strain and can contribute to a mixture of uncomfortable GI symptoms including constipation, bloating or cramping. This disruption of gut balance caused by processed foods can cause vitamin and mineral deficiencies as well as a compromised immune system. All this can amplify existing issues and threaten overall body health.\nWhat can you do to feel more healthy? Eat more veggies, fruits, lean meats!","Enzymes are biological catalysts; they accelerate a chemical reaction. Globally, enzymes are used in industries like Biofuel, cleaning products/detergents, food, animal feed, textile and speciality. Further, the market for industrial enzymes is continuously rising, owing to the growing need for sustainable solutions. Advancement in biotechnology, especially in protein engineering, has also brought in new probiotic products, driving the industrial enzymes market to the next level. According to Renub Research, the Global Enzymes Market is projected to reach US$ 17.4 Billion by 2027.\nindustrial enzymes market\nCarbohydrases, Amylases, Cellulases, Proteases and Lipases are major variants existing in the global Enzymes Market. By Types, these enzymes are used in numerous industries. For instance, the food industry primarily uses carbohydrases in baking for manufacturing fruit juices, winemaking and cheese manufacturing. Similarly, other applications of Carbohydrases are detergents, textiles & leather, and bioethanol. Carbohydrases also have added advantages to various industrial applications due to their low cost, less time & space consumption, and ease in modifying and optimizing the process. The Industrial Enzymes Market Size was estimated at US$ 10.6 Billion in 2020.\nBesides, protease enzymes are widely used in the Speciality Industries. The Speciality Industry uses protease enzymes in a large quantity in the manufacturing of medicines. Protease enzymes treat multiple diseases such as lungs, heart, eye abnormalities, digestive tract, skin ulcers, and soreness. Thus, the Speciality Industry is expected to fuel the demand for protease enzymes in the foreseeable future. Also, with the help of developing science and technology, protease enzymes are used in the several bio-meditation process and leather treatments.\nFurthermore, the Industrial Enzymes Market in North America is influenced by rising demand for enzymes from the food industry and significantly has a large consumer base by countries in the region. Nonetheless, the market in Asia-Pacific is owing to sound growth opportunities induced by soaring demand for carbohydrase and proteases in the applications of the pharmaceutical and food industry, remarkably in emerging countries such as China, India, and Japan. These countries have started investing in the biotech sector to introduce more effective global enzyme products.\nAdditionally, expanding investment in the food and beverage and pharmaceutical industry is anticipated to stimulate the overall industrial enzymes market. With improving disposable income, customer can now afford expensive food product, which is further expected to increase the demand for industrial enzymes products. As per our analysis, The Industrial Enzymes Industry will likely grow at a CAGR of 7.34% from 2020-2027.\nNevertheless, stringent supervision and controlled temperature & PH levels of enzymes await to restrict the market’s growth to some extent. On the other hand, concerns associated with quality, safety, and consumer perception towards enzymes is the key challenge of this market.\nBy Sources, the Industrial Enzymes Market has been fragmented through microorganisms, plants and animals. The microorganism is the most common source of industrial enzymes holding one of the highest shares in the market.\nBy Application, the Global Enzymes Market has been segregated into Bio-fuel Enzymes, Cleaning Product Enzymes, Food Enzymes, Animal Feed Enzymes, Textile Enzymes and Specialty Enzymes. Amongst the segments, Specialty Enzymes Market holds a significant market existence.\nBy Region, we have covered North America, Asia-Pacific, Europe and the Rest of the World.. Furthermore, the Asia-Pacific region is expected to witness the highest growth in the forecasted period and is anticipated to nurture its dominance in the future years.\nSome major companies operating in the industrial enzymes market are BASF SE, Advanced EnzymeTechnologies, Novozymes, DuPont Danisco, DSM and Kerry Group PLC.\nCOVID-19 Impact on Global Enzymes Market\nThe spread of COVID-19 across the globe has positively impacted the overall Industries. The rising adoption of enzymes in multiple industries such as Cleaning Product, Food, Animal Feeds, and Specialty has to propel the market growth during the COVID-19 pandemic as consumers across the world have become more alert and conscious towards their health and lifestyle.\nHowever, Biofuel and Textile Enzymes Market’s growth has been hampered by the COVID-19 pandemic. The lockdowns, social distances, and the shutdown of production plants globally have impacted the automotive industry, which in turn, negatively influenced the biofuel industry.\nSimilarly, the Textile Enzymes Market is no different; the operations of numerous textile industries had either been temporarily halted or were functioning with a minimal workforce due to enforced lockdowns and imposed restrictions by respective governing bodies. This factor has a significantly negative impact on the revenue growth of the Textile Enzymes Market.\nRenub Research latest report “Industrial Enzymes Market Global Forecast by Types (Carbohydrases, Amylases, Cellulases, Proteases, Lipases), Source (Microorganism, Plant, Animal), Application(Bio fuel, Cleaning Product, Food, Animal Feed, Textile, Specialty), Region (North America, Europe, Asia Pacific, Rest of the World), Company (BASF SE, Advanced Enzyme Technologies, Novozymes, DuPont Danisco, DSM, Kerry Group PLC)” provides a detailed analysis of Enzymes Market.\nRequest a Free Sample Copy of the Report: https://www.renub.com/request-sample-page.php?gturl=industrial-enzymes-market-p.php\nBy Types market has been covered from 5 viewpoints:\nBy Source market has been covered from 3 viewpoints:\nBy Application market has been covered from 6 viewpoints:\n- Bio fuel Enzymes\n- Cleaning Product Enzymes\n- Food Enzymes\n- Animal Feed Enzymes\n- Textile Enzymes\n- Specialty Enzymes\nglobal enzymes market\nBy Regions market has been covered from 4 viewpoints:\n- North America\n• Company Initiatives\n• Sales Analysis\n- BASF SE\n- Advanced Enzyme Technologies\n- DuPont Danisco\n- Kerry Group PLC\nAbout the Company:\nRenub Research is a Market Research and Consulting Company. We have more than 10 years of experience especially in international Business-to-Business Researches, Surveys and Consulting. We provide a wide range of business research solutions that helps companies in making better business decisions. We partner with clients in all sectors and regions to identify their highest-value opportunities, address their most critical challenges, and transform their businesses. Our wide clientele comprises major players in Healthcare, Travel and Tourism, Food & Beverages, Power & Energy, Information Technology, Telecom & Internet, Chemical, Logistics & Automotive, Consumer Goods & Retail, Building and Construction, & Agriculture. Our clients rely on our market analysis and data to make informed knowledgeable decisions. We are regarded as one of the best providers of knowledge. Our pertinent analysis helps consultants, bankers and executives to make informed and correct decisions.\nOur core team is comprised of experienced people holding graduate, postgraduate and PhD degrees in Finance, Marketing, Human Resource, Bio-Technology, Medicine, Information Technology, Environmental Science and many more. Our research helps to make business decisions: on strategy, organization, operations, technology, mergers & acquisitions etc. We support many blue chip companies by providing them with findings and perspectives across a wide range of markets. Our research reports offer a blend of information insight, analysis and forecasting that is essential in today’s ultra-competitive markets."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:27e0dad3-256b-4efc-89c9-527f760a4e04>","<urn:uuid:8c719086-e8a5-48a2-928d-a1410b39d9bd>"],"error":null}
{"question":"Could you compare the risks and benefits of refinishing engineered wood floors versus using floor sanding screens? Please present them in a clear format.","answer":"Benefits of floor sanding screens:\\n- Quick and cost-effective maintenance\\n- Can be used on multiple surfaces (wood, concrete, marble, etc.)\\n- Waterproof and can be rinsed between uses\\n- Two-sided design extends product life\\n- Less invasive to floor surface\\n\\nBenefits of refinishing engineered wood:\\n- Restores original beauty\\n- Removes scratches and discoloration\\n- Increases floor durability\\n- More cost-effective than full replacement\\n- Adds value to home\\n\\nRisks of floor sanding screens:\\n- Limited to surface-level improvements\\n- May not address deeper damage\\n\\nRisks of refinishing engineered wood:\\n- Can damage floors if wear layer is too thin\\n- Risk of uneven surfaces if not done professionally\\n- Removes factory-applied finish\\n- Not suitable for addressing deep mold issues","context":["Floor Sanding Screens for Buffers\nWhat is a Floor Sanding Screen?\nFloor sanding screens are either round or rectangular buffer attachments used for the first step of the floor screening and recoating process. They are constructed with open-weave nylon webbing that is embedded with silicon carbide abrasive grains. This open-weave mesh design allows you to remove old finishes and scuffs from floors without damaging the surface below the finish.\nWhile the screens are exceptionally resistant to loading, some does occur during use. These are two sided and can be flipped over to speed up your work extend the life of the product. Another benefit of the webbing design is that it makes the pads waterproof. This allows you to screen floors in a wet environment and also gives you the ability to rinse the screens between uses, which makes them useful for a much longer time.\nAre they Just Wood Floor Screens?\nNo. The screens can be used on a variety of surfaces. Wood is the most common surface for sanding screens to be used on, but the mesh screen with silicon carbide grains can also be used on other surfaces such as concrete, drywall, tile, sheetrock, marble, and other natural and artificial stones.\nFloor Screening vs Sanding\nScreening floors is generally done when your coating is in need of a new finish, but the underlying wood is still in good condition. It is a quick and cost effective way of keeping a healthy finish on hardwood floors.\nFloor sanding is a much more labor-intensive job that needs to be done when there is damage to the floor like dents, deep scratches, wear and tear from heavy foot traffic, and stains. Screens give you a cost-friendly option if you cheese to keep your floors in immaculate condition rather than waiting until they need to be completely resurfaced.\nTypes of Sanding Screens for Floors\nRound Floor Buffer Sanding Screens\nThese round screens range in size from 17” across to 20” across and can be used similarly to a floor sanding discs or round floor buffing pads. Floor buffer machines will continuously spin the screen like a giant orbital sander.\nCompatible Machines - Orbital floor buffing machines\nFloor edgers work similarly to the full size floor buffers, but smaller and handheld. Their design allows you to screen and buff flooring all the way up to the edges of a room.\nCompatible Machines - Floor edger sanders such as, but not limited to, Clarke floor edger, a Hold edger sander, a Porter Cable floor edger sander, and Skil edger\n12x18 Sand Screen Sheets\nWhen most people think of a floor buffing machine, they’ll usually picture the giant round floor buffer. There are also floor buffers and sanders that have a rectangular shape, sometimes called square orbital sanders...even though they aren’t an actual square. To sand and finish flooring, the machine vibrates and makes small but fast rotations to cause friction against the floor surface.\nThese types of floor sanding screen sheets can be used on the entire floor, against the edges of the room like edge screens, and even deeper into corners of room that the round screens cannot reach.\nCompatible Machines - 12\" x 18\" Buffers\nHow to Attach Sanding Screen to Floor Buffer\nThe weight of your sanding machine/buffer will keep your screens in place. Some buffer models require attaching your sanding screen in another way, but most round and square buffers can be installed like this:\nMake sure the machine is off and unplugged - safety first!\nSet the sandscreen on the floor near the buffer\nIf your machine requires a buffer pad, lay that on top of the screen\nTilt the machine back on its wheels\nLower the machine directly over the screen/pad so the entire base is covered\nIf your machine doesn’t have wheels, lift it from the base and place it over the screen. Never lift a buffer by its handles, which can damage the buffer.","The short answer is Yes, you can refinish engineered wood floors because the top thin layer is made of real hardwood. However, before you begin refinishing, you need to understand the engineered wood floor more deeply so that it can be confirmed whether your engineered wood floor can be refinished or not.\nThe base of an Engineered Wood Floor is made of high-quality plywood, and the top surface (also called the wear layer) is a layer of real hardwood. That’s why engineered wood flooring looks almost identical to hardwood flooring.\nSo whether you can refinish your Engineered Wood Floors or not depends on the thickness of the wear layer of the Engineered Wood boards. If the wear layer is thick enough, then you can finish; otherwise, refinishing is not recommended.\nGenerally, the thickness of the board is 10-20mm, and the thickness of the wear layer is about 1-3mm. The thickness depends on the company, quality of the boards and construction type of boards.\nIf the thickness of the wear layer of your engineered wood blocks is more than two mm, then you can refinish. Because when sanding, It takes one millimeter off the surface of the boards. Thus if your wear layer is around 3 mm, you can refinish it twice.\nWhy You Shouldn’t Refinish Engineered Wood Floors?\nThere are some situations and considerations where you should not refinish engineered wood floors. Here are some reasons:\nLimited Thickness of Wear Layer\nIf the wear layer on your engineered wood floor is too thin, it is not enough thickness to refinish. Or if refinishing of your same floor has already been done earlier, you should not do refinishing. By this, you can damage your floor.\nRisk of Damage\nIf the sanding process is not done by a professional or in the right manner, there is a high chance of uneven surfaces, scratches or dents on the floor, as the top layer can handle wear and tear less than solid hardwood.\nEngineered wood floors come with factory-applied finishes designed to be durable and last. Refinishing removes this and affects the durability and stability of the floor.\nMold or Rot\nIf you are refinishing your floors to remove mildew or rot, check to see how far the mildew has spread. Refinishing a floor only removes mildew from the top surface. If the core of engineered wood blocks has mold, you should replace the entire block instead of refinishing.\nWhy Should You Refinish Engineered Wood Floors?\nRefinishing Engineered wood floors has many benefits, similar to refinishing solid hardwood floors. Here are the benefits of refinishing engineered wood floors:\nOver time, engineered wood floors can become scratched, dull from wear and tear, or discolored from direct sunlight. Refinishing can restore their original beauty, making them look new again.\nThe refinishing process involves sanding down the top layer of engineered wood, removing imperfections and exposing a new layer. Applying a new finish over it increases the durability of the floor.\nRefinishing engineered wood boards is generally more cost-effective than replacing an entire floor board. This allows you to extend the lifespan of your flooring without the high cost of purchasing and installing new flooring.\nReclaimed floors add value to your home. If you are planning to sell your property in future, you are likely to get a good price for your home."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:08d38055-fb6c-439f-89c9-0ba4a030d20d>","<urn:uuid:b9c9c5e2-e67f-4f8c-ad60-66247ab6daaa>"],"error":null}
{"question":"What is the relationship between p21 and cellular polarity, and how does this impact stem cell behavior?","answer":"P21 acts as a cyclin-dependent kinase inhibitor that controls cell cycle progression through interaction with cyclins and CDKs, leading to cell cycle arrest. When examining cellular polarity in stem cells, research shows that polarity is crucial for stem cell function and maintenance, particularly in hematopoietic stem cells (HSCs). The distribution of cellular components during division, including regulatory proteins, correlates with cell fate determination. Both asymmetric/polar and symmetric/nonpolar divisions are utilized by HSCs, with both intrinsic and extrinsic signals determining polarity during division. This polarity is essential for proper stem cell-niche interactions and maintaining stem cell quiescence.","context":["Upstream eventp21 (CDKN1A) expression, increase\ncell cycle, disrupted\nKey Event Relationship Overview\nAOPs Referencing Relationship\n|AOP Name||Adjacency||Weight of Evidence||Quantitative Understanding|\n|Histone deacetylase inhibition leading to testicular toxicity||adjacent||High||Moderate|\n|Homo sapiens||Homo sapiens||High||NCBI|\n|Rattus norvegicus||Rattus norvegicus||High||NCBI|\n|Mus musculus||Mus musculus||High||NCBI|\nLife Stage Applicability\n|Not Otherwise Specified||High|\nKey Event Relationship Description\nCell cycle regulation through p21 (cyclin dependent kinase inhibitor 1A; CDKN1A) activation is demonstrated by the interactions of p21 with cyclins [Dotto, 2000]. p21 interacts directly with cyclins through a conserved region in close to its N-terminus (amino acids 17-24; Cy1) [Dotto, 2000]. The cyclin dependent kinase inhibitor, p21 has the secondary weak cyclin binding domain near its C-terminus region (amino acids 153-159), which overlaps with its proliferating cell nuclear antigen (PCNA) binding domain [Dotto, 2000]. Kinase activity of cyclin-dependent kinase (Cdk) was inhibited by Cy1 site of p21 that is important for the interaction of p21 with cyclin-Cdk complexes [Chen, 1996]. The p21 inhibits Cdk complexes such as cyclin A/E-Cdk2 or cyclin D-Cdk4 complexes, leading to the cell cycle disruption as G1/S arrest [Chen, 1996].\nEvidence Supporting this KER\np21 has a separate cyclin-dependent kinase 2 (CDK2) binding site in its N-terminus region (amino acids 53-58) and optimal cyclin/CDK inhibition requires binding by this site as well as one of the cyclin binding sites [Dotto, 2000]. The peptide containing Cy1 site inhibited the kinase activity of cyclin E-Cdk2 and cyclin A-Cdk2 [Chen, 1996]. The p21WAF1/CIP1/sdi1 gene product inhibits the cyclin D/cdk4/6 and the cyclin E/cdk2 complexes in response to DNA-damage, resulting in G1/S arrest [Moussa, 2015, Ogryzko, 1997]. p21 inhibits cyclin-dependent kinases and regulates cell cycle to promote cell cycle arrest. Deletion of either cyclin binding site in N-terminus or C-terminus of p21, or CDK binding domain was sufficient for the kinase activity inhibition [Chen, 1996].\n- TSA induces p21 expression leading to cell cycle arrest [Gartel, 2002].\n- The up-regulation of p21 signaling and in testicular germ cells was observed in diabetes [Kilarkaje, 2015].\n- A study investing the effects of miR-6734 that has a sequence homology with a specific region of p21WAF1/CIP1 promoter on HCT-116 colon cancer cell growth indicated that miR-6734 up-regulated p21 gene expression and induced cell cycle arrest [Kang, 2016]. This result suggests that the direct enhancement of p21 gene expression is related to the alteration of the cell cycle distribution [Kang, 2016].\n- The study of postnatal telomere indicated that dysfunction of premature telomere induces cell-cycle arrest through p21 activation in mammalian cardiomyocytes [Aix, 2016].\n- The p21WAF1/CIP1/sdi1 gene product inhibits the cyclin D/cdk4/6 and the cyclin E/cdk2 complexes in response to DNA-damage, resulting in G1/S arrest [Moussa, 2015, Ogryzko, 1997].\nUncertainties and Inconsistencies\nTSA promotes apoptosis via HDAC inhibition and p53 signaling pathway activation [Deng, 2016a]. It is suggested that furazolidone induces reactive oxygen species leading to suppression of p-AKT and p21, and induction of apoptosis [Deng, 2016b]. The dual roles of p21 in cell cycle arrest and anti-apoptotic effect in the testicular germ cells of diabetic rats are suggested [Kilarkaje, 2015]. The anti-apoptotic effect of p21 is mediated by caspase-3 inhibition, which demonstrates the possibility of cell-cycle independent effect on apoptosis [Deng, 2016b]. It has been demonstrated that p21 induces apoptosis in human cervical cancer cell lines [Tsao, 1999], whereas p21 is implicated in apoptosis inhibition by blocking activation of caspase-3 or interacting with ASK1 [Gartel, 2002, Zhan, 2007]. Up-regulation of p21 is implicated in the activation of DNA damage pathways, and deletion of p21 improved stem cell function and lifespan without accelerating chromosomal instability, which indicates that p21-dependent checkpoint induction affects the longevity limit [Choudhury, 2007].\nQuantitative Understanding of the Linkage\nThe peptide containing cyclin-binding domain of p21 in N-terminus inhibited the kinase activity of cyclin E-Cdk2 with 296 nM of the concentration in which kinase activity is inhibited in 50% (Ki) [Chen, 1996].\nThe peptide containing cyclin-binding domain of p21 in C-terminus showed 32,000, 800, or >300,000 nM of Ki for inhibition of the kinase activity of cyclin E-Cdk2, cyclin A-Cdk2 or cyclin D1-Cdk4, respectively [Chen, 1996].\nKnown modulating factors\nKnown Feedforward/Feedback loops influencing this KER\nDomain of Applicability\nDNA replication in Xenopus was suppressed by the GST fusion protein of p21 without amino acids 17-24 or the peptide containing cyclin binding site in N-terminus of p21 protein [Chen, 1996]. P21 regulates the E2F transcriptional activity to control cell cycle in human U2OS osteosarcoma cells (Homo sapiens) [Delavaine, 1999]. Cell cycle is regulated by p21 through cyclins and CDKs in mice (Mus musculus) [Sherr CJ, 2004].\nDotto GP (2000) p21WAF1/Cip1: more than a break to the cell cycle? Biochim Biophys Acta 1471: M43-M56\nChen J et al (1996) Cyclin-binding motifs are essential for the function of p21CIP1. Mol Cell Biol 16: 4673-4682\nMoussa RS et al. (2015) Differential targeting of the cyclin-dependent kinase inhibitor, p21CIP/WAF1, by chelators with anti-proliferative activity in a range of tumor cell-types. Oncotarget 6:29694-29711\nOgryzko VV et al. (1997) WAF1 retards S-phase progression primarily by inhibition of cyclin-dependent kinases. Mol Cell Biol 17:4877-4882\nGartel AL and Tyner AL (2002) The role of the cyclin-dependent kinase inhibitor p21 in apoptosis. Mol Cancer Ther 1: 639-649\nKilarkaje N and Al-Bader MM. (2015) Diabetes-Induced Oxidative DNA Damage Alters p53-p21CIP1/Waf1 Signaling in the Rat Testis. Reproductive Sciences 22: 102–112\nKang MR et al (2016) miR-6734 up-regulates p21 gene expression and induces cell cycle arrest and apoptosis in colon cancer cells. PLoS One 11: e0160961\nAix E et al (2016) Postnatal telomere dysfunction induces cardiomyocyte cell-cycle arrest through p21 activation. J Cell Biol 213: 571-583\nDeng Z et al. (2016a) Histone deacetylase inhibitor trichostatin A promotes the apoptosis of osteosarcoma cells through p53 signaling pathway activation. Int J Biol Sci 12:1298-1308\nDeng S et al (2016b) P21Waf1/Cip1 plays a critical role in furazolidone-induced apoptosis in HepG2 cells through influencing the caspase-3 activation and ROS generation. Food Chem Toxicol 88: 1-12\nTsao YP et al (1999) Adenovirus-mediated p21WAF1/SDII/CIP1 gene transfer induces apoptosis of human cervical cancer cell lines. J Virology 73: 4983-4990\nZhan J et al (2007) Negative regulation of ASK1 by p21Cip1 involves a small domain that includes serine 98 that is phosphorylated by ASK1 in vivo. Mol Cell Biol 27: 3530-3541\nChoudhury AR et al (2007) Cdkn1a deletion improves stem cell function and lifespan of mice with dysfunctional telomeres without accelerating cancer formation. Nat Genet 39: 99-105\nDelavaine L and La Thangue NB (1999) Control of E2F activity by p21Waf1/Cip1. Oncogene 18: 5381-5392\nSherr CJ and Roberts JM (2004) Living with or without cyclins and cyclin-dependent kinases. Gene Dev 18: 2699-2711","Adult somatic stem cells are central to homeostasis in tissue that present with a higher cellular turnover just like the epidermis intestine as well as the hematopoietic program. may be a book method of ameliorate or revert aberrant somatic stem cell function also. Stem Cells 2010; 28:1623-1629. and deletion outcomes in an immediate increase of basal mitosis a progressive loss of apical membrane protein location and increasing failure of apically directed interkinetic nuclear migration. Consequently these Cdc42-deficient progenitors acquire the fate of the progenitors located in the subventricular zone that cannot self-renew for long time and gradually deplete . It was also recently shown that a planar cell polarity pathway triggered Rabbit Polyclonal to TSEN54. by Wnt7settings the number of muscle mass stem cells and the regenerative potential of muscle tissue  again most likely by regulating the mode of stem cell divisions. On division murine HSCs distribute Numb asymmetrically to child cells a mechanism already explained for asymmetric division of neuroblasts . Mammalian Numb displays a complex pattern of functions such as controlling cell fate decision endocytosis cell adhesion cell migration and ubiquitination of specific substrates and may interact with several signaling pathways (i.e. Notch Hedgehog p53). Alterations of Numb-dependent events and/or of Numb distribution during asymmetric cell division suggest an important part SGI-1776 for Numb in disease and malignancy progression  (Fig. ?(Fig.22). Number 2 Polarized (A) and not polarized (B) HSCs. The picture is definitely representative of tubulin (blue) and Numb (green) localization in freshly isolated mouse Lineage? c-kit+ Sca-1+ CD34? Flk2? HSCs (long-term repopulating HSCs) from (A) young … An additional example implying a role for polarity in stem cell function comes from analysis of human being hematopoietic stem/progenitor cells that can differentially localize the tetraspanins CD53 SGI-1776 CD63 the transferrin receptor or CD71 and CD62 or l-selectin while dividing in vitro [32 35 An asymmetric distribution of cellular components on division has also been proven with a fluorescent Notch-activity signal program . Cytokine distribution provides been proven to correlate with cell destiny determination on department ; nonetheless it is unclear whether cytokines are instructive in this technique in fact. Collectively these and various other released data support that both settings of cell department (asymmetric/polar and symmetric/nonpolar) are utilized by HSCs with both intrinsic aswell as extrinsic indicators identifying polarity on department. Lately a stem cell stroma synapse-like framework continues to be postulated in analogy towards the well-characterized immune system cell synapse [50-52] that represents the contact airplane between T-cells and antigen-presenting cells [53-55]. Latest data demonstrating polarity in non-dividing HSCs getting together with specific niche market cells support a job for polarity in the stem cell synapse just like the reported T-cell-polarity on connections with antigen-presenting cells [56 57 Adult stem cells surviving in their specific niche market are mostly within a quiescent cell routine condition. SGI-1776 Although cell routine quiescence has up to now not been often associated with mobile polarity recent outcomes examining mice deficient for Cdc42 in HSCs claim that polarity founded by Cdc42 SGI-1776 may be essential for both adhesion of HSCs towards the niche aswell as their quiescence as these mice display a rise in the quantity and the rate of recurrence of phenotypic short-term HSCs and a lack of long-term HSCs [58 59 Consequently albeit backed by just few experimental results so far polarity might be necessary in maintaining HSC quiescence by functioning in the formation of the stem cell-niche synapse and polarity alterations might importantly impair stem cell quiescence or function. Such a polarity-based synapse model though leaves the question open whether adhesion to the niche induces polarity in stem cells (extrinsic regulation of polarity) or whether stem cells present an intrinsic polarity axis in which a polar interaction with the niche might only be secondary to this intrinsically established polarity [16 50 Although polarity in migration has been extensively studied in differentiated progeny of stem cells-like neutrophils [5 60 61 the role of polarity in stem cell migration has not been investigated in great detail. Obviously more research in this area is necessary although there is evidence that stem cell migration and migration-associated polarity are SGI-1776 also regulated by small RhoGTPases . STEM CELL POLARITY IN CANCER AND AGING Whether there is a causal relationship between altered stem cell polarity and cancer initiation and."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:c06752b4-ea30-462f-9fd9-fcfc5a68316c>","<urn:uuid:a838df80-db02-4df8-a0b9-96a43c510a0a>"],"error":null}
{"question":"What is the historical significance of the Gion Matsuri festival in Kyoto, and how has its elaborate float tradition evolved over time?","answer":"The Gion Matsuri originated in 869 A.D. in response to a devastating plague, when people held ceremonies at Yasaka Shrine to pray for deliverance. Emperor Seiwa declared that the gods needed attention, and the festival initially featured 66 decorated halberds along with mikoshi (portable shrines). By 970, it became an annual event. Over time, the halberds were replaced with massive floats called hoko, and smaller floats called yama joined the procession. During the Medieval Period and especially the Edo Period, wealthy merchants transformed these floats into impressive structures known as yamahoko, standing 6-8 meters high with 2-meter wheels. The yamahoko floats feature diverse themes and incorporate international elements, including Middle Eastern carpets, Chinese silks, and European textiles. Their craftsmanship is so exceptional that they earned UNESCO Intangible Cultural Heritage status. Today, these floats participate in processions on July 17 and 24, recently returning to the traditional two-procession format after being limited to one during post-World War II economic development.","context":["Natsu Matsuri (II): Japanese Summer Festival\nIn Japanese, the summer festivals are called Natsu Matsuri (夏祭り), which is a collective term for all the festivals that are held from early July to late August. Among all the festivals, Gion Matsuri, Tenjin Matsuri, and Sanno Matsuri are the three greatest Natsu Matsuri with their long duration, a huge number of participants, and grand scale. Moreover, they are located in Kyoto, Osaka, and Tokyo respectively, which are the three most popular tourist resorts in Japan.\nGion Matsuri is a festival held every July in Kyoto and lasts for a month. Featuring Natsu Matsuri, the Gion Matsuri is considered the largest and most famous festival in Japan. It was awarded the title of World Intangible Cultural Heritage in 2009.\nGion Matsuri originated in 869 A.D. when a plague struck Japan and claimed numerous victims. Therefore, people held a ceremony in Yasaka Shrine to pray for the deliverance from the disaster. Since 970, the Gion Gion Matsuri has been an annual event.\nGion Matsuri follows many traditions. For example, a local boy will be chosen as a “divine boy” to visit the shrine and pray for good luck. From July 13 to July 17, he is carried on a palanquin and his feet don’t touch the ground.\nVarious events are held throughout the month during the Gion Matsuri. However, the parade floats from the 17th to the 24th are the absolute highlight. Each district in Kyoto will design a gorgeous float to participate in the parade. During the parade, more than thirty giant floats will appear. Among them, the most remarkable is the traditional float called “Yamahoko” and Geisha in the parade.\nTenjin Matsuri means “the festival of the gods” and was first held in honor of Sugawara Michizane, a scholar who is regarded as the Shinto god of learning. Since then, the festival has been held on his death anniversary, from July 24 to 25, in Osaka for 1,000 years. It is the largest water festival in the world.\nThe Tenjin Festival is mainly composed of the Yomiya festival on July 24 and the Honmiya festival on July 25. During the festival, traditional Japanese arts such as kagura (Shinto ritual music) and bunraku (traditional Japanese puppet plays) are performed in various places around the city.\nThe highlight of the festival is the afternoon of the 25th. More than 3,000 people dressed in traditional attire of the Nara period and Heian period (8th to 12th century) carry a mikoshi (a portable shrine) through the city. The procession includes ceremonial floats, musicians, dancers, and costumed characters, who are led through the streets by Taiko drummers. Afterward, the procession boards boats from the shore of the Tenjinbashi Bridge, and proceed with their parade. There will be more than 100 boats sailing on the Okawa River. In the evening, there will be bonfires and fireworks, which mark the end of the festival.\nSanno Matsuri/Kanda Matsuri\nKanda Matsuri, Sanno Matsuri, and Fukagawa Matsuri are Tokyo’s three most famous festivals. Among them, Sanno Matsuri is held only in even-numbered years, alternating with the Kanda Matsuri held in odd-numbered years.\nThe Sanno Matsuri originated in the Edo period (1603-1867). It is held in June and will last for more than a week, featuring a variety of small-scale events, such as flower arranging, dancing, and performances and rituals, especially for children. The biggest event of the festival is a procession of three mikoshi (portable shrines) through central Tokyo in nine hours. The parade begins and ends at Hie Shrine, which enshrines the guardian deity of Tokyo.\nKanda Matsuri began in the Edo period (1603-1867) to celebrate the prosperity under the Tokugawa Shogunate’s governance. Kanda Matsuri is presided over by Kanda Myojin Shrine, which enshrines three gods: Daikokuten, Ebisu, and Taira Masakado. Since Daikokuten and Ebisu are in charge of the harvest and commerce respectively, Kanda Matsuri has gradually become a celebration of prosperity and good fortune. Kanda Matsuri is held on Saturday around May 15. On the evening before the festival, a Shinto ceremony is held to invite the three gods to enter three mikoshi. On the day of the festival, these three mikoshi and the accompanying thousand people, including musicians and monks on horseback, perform the first procession in the morning. In the afternoon, floats, musicians, and dancers join the procession for a second procession. The next day, nearly 100 mikoshi of various towns and villages will gather to continue the procession. These mikoshi carry the local god of each town and village, which is believed to bless his/her people during the procession.","One of Japan’s most important festivals revives a tradition that has been dormant for decades.\nBy Alec Jordan\nSince being brought into the spotlight last year by Tokyo 2020 Olympic bid ambassador Christel Takigawa, omotenashi continues to be one of the most popular words in Japan. Of course, this supremely Japanese concept of unobtrusive hospitality is nothing new: it is a dynamic that can be felt in the relationship between staff and customer almost anywhere you go.\nIn fact, the importance of omotenashi is something that even the gods need. This was on the mind of the ancient emperor Seiwa who, in the 9th century, declared that the gods were in need of some attention, following a severe plague that had struck the city. The emperor required the people of Kyoto to pay impressive respects to the resident gods of the Yasaka shrine, located in the well-known entertainment district of Gion. In addition to the mikoshi or portable shrines—hallmarks of Japan’s religious festivals that serve as temporary containers for the gods themselves—66 decorated halberds were created for the occasion.\nWhether the gods truly appreciated the ritual or not, the intensity of the outbreak abated, leading the people of Kyoto to hold the festival any time that the plague struck the city again. But within a century, the leaders of the city had decided that the Gion Matsuri should become an annual affair—the festival had become popular among both the human and the divine. Over the years, Gion Matsuri became more and more extravagant: the halberds were replaced with huge floats, called hoko, and smaller floats called yama joined the procession of the mikoshi, as they travel first to their temporary home for the duration of the festival, then to a purifying bath in the Kamo River, and finally back to the Yasaka Shrine.\nCollectively known as the yamahoko, these floats display a level of craftsmanship that is unparalleled for festival arts in Japan, earning them a place among UNESCO’s Representative List of the Intangible Cultural Heritage of Humanity. The yamahoko started to develop into impressive structures during the Medieval Period, but it was the wealth of the Edo Period merchants who lived in the area that transformed these floats into the giant vessels that navigate their course through the streets of Gion today. Accompanied by dozens of attendants, the yamahoko are impressive to behold, standing a total height of 6 to 8 meters above the crowds that gather for the procession, with wheels that roll nearly 2 meters high.\nThe yamaboko stand out not only for their sheer size, but also the splendor of their decorations and design. Each float features a different theme, from Buddhist and Shinto deities and prominent figures in Kyoto’s history to animals and celestial bodies. Colors abound as the massive floats make their way through Gion, and each one has a unique style that has been developed over generations. Most of the decorative handicraft that goes into festooning the yamahoko are of Japanese origin, but many items found on the floats come from other countries, and this is a little-known international element of this eminently Japanese event. During periods when trade with other nations was booming, carpets from the Middle East, silks from China, and textiles from India and Europe all made their way onto the floats. As these pieces wore down over time, the artisans who worked on the yamahoko learned how to restore these objects themselves, and in so doing, found themselves maintaining craft making traditions that had died out in the lands where the objects originally came from.\nIn the early days of the Gion Matsuri, the main processions of the festival were held during the sixth lunar month on Japan’s ancient calendar, but along with the increase of the size and scale of the floats, the festival’s events expanded over time, and now as it is celebrated, Kyoto celebrates Gion Matsuri during the entire month of July. It begins with the opening ceremonies held in Gion’s various neighborhoods and the lottery that will decide the order of the floats in the procession, peaks during the first procession of the yamahoko and the parading of the mikoshi to their temporary home on Shijo Street, and closes with the final procession of the mikoshi back to Yasaka Shrine and the last ceremony on July 31. Throughout the festival, the streets of Gion take on a new life as they fill with food and craft stands, making each night more festive than the ones before it.\nAside from the days of the yamahoko procession—July 17 and 24—some of the most impressive sights can be seen on the three days immediately leading up to the processions. On these days, known as yoiyama, the already lively atmosphere rises to another level: cars are not allowed on the streets of Gion for the runup, and you can expect to see even more people walking about, many dressed in colorful yukata. This is also the time when many of the shops and homes in the Shinmachi and Muromachi neighborhoods open their doors and display some their most prized objects. At one door, it might be a centuries-old piece of hanging calligraphy, a ceramic piece made by a celebrated master, or a painted folding screen, from which this particular tradition takes its name: Byobu Matsuri, or the Festival of Folding Screens. At many of the houses, visitors are welcome to come inside and take the rare opportunity to see the interior of a traditional Kyoto house first hand.\nTraditionally, there have been two yamahoko processions: one to accompany the mikoshi leaving the shrine (the sakimatsuri, or first festival), and one to accompany their return (the atomatsuri, or second festival). However, in the years following World War II, economic development took precedence. Stopping traffic for another three or four days would have put the brakes on Japan’s booming businesses, and the yamahoko was held to just one day. This year, the people of Kyoto have chosen to make a return to two separate processions, in keeping with the idea of making sure that the gods are happy. 2014, then, marks the opportunity to enjoy twice as much of the ebullient atmosphere that makes the Gion Matsuri one of Japan’s most memorable cultural experiences.\nHow to get there: Kyoto is most easily reached from Tokyo by Shinkansen—a Nozomi or Hikari train will get you there in less than three hours. Trains leave every 5–10 minutes. The starting and ending point of Gion Matsuri is Yasaka Shrine (see map below), and the processions and main events of the festival can be found to the west of the shrine."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:d8719ee1-6094-45f7-98da-d1fc218f06d5>","<urn:uuid:0c07500e-84ae-4d0a-9fb9-cae4e2b8a22d>"],"error":null}
{"question":"What makes Jinek Bay a unique snorkeling destination, and what safety precautions should visitors take to prevent drowning?","answer":"Jinek Bay, nicknamed 'The Natural Aquarium', offers exceptional snorkeling with beautifully translucent water and magnificent corals in superb condition. The site features diverse marine life including clown surgeonfish, butterflyfish, fire clownfish, and occasional Maori wrasse sightings. However, like any water activity, safety precautions are essential. Snorkelers should wear proper equipment including a snorkel mask, tube, and vest, stay within their physical limitations to avoid exhaustion, and be aware of water currents. It's crucial to avoid snorkeling during high winds, check tide forecasts before planning trips, and preferably snorkel with a buddy. First-time snorkelers should practice in controlled environments before venturing into the ocean to prevent panic-induced accidents.","context":["Looking over Jinek Bay from the cliffs that surround it, you’ll quickly understand why this must-see site in Lifou is nicknamed “The Natural Aquarium”. Through the surface of its beautifully translucent water, you can already see the magnificent corals that make up the reputation of this spot. Here, it’s an invitation to discover some of the most beautiful reefs of Lifou, where hundreds of species of tropical fish flourish and swim about.\nJinek Bay is located in Easo, on the northwest coast of Lifou island. To get there, it is better to have a rental car. Otherwise, hitchhiking works pretty well on the island. From Wé, the main village of Lifou, it takes about 35 minutes by car. It is recommended to use a geo-tracking application to find the site, though it is poorly indicated.\nThere are stairs installed on the small cliff that allow yourself to be equipped and ready to launch into the water safely.\nThe area to be explored covers the coral reef that has developed in the calm waters of the bay. You can easily spot the coral bommies from the stairs and the different lookouts. In this sheltered area and after the absence of a beach, the corals developed only a few meters from the shore, just below the surface of the water.\nAt this spot, the underwater landscape is rather constant. It alternates between more or less extensive coral plateaus just below the surface of the water (↕0.5m) and then deeper intermediate zones (↕3-8m), with sandy bottoms.\nJinek Bay greatly deserves its nickname of “Natural Aquarium”. Before completely immersed in the water, everything is already an incredible wonder: multicolored corals swept by sun rays, giant clams unfolding their fluorescent mantle in crystal clear water, sea lilies swaying by the movements of the water. Here, the decor is divine. Although some areas are slightly damaged, especially near the stairs, the corals are in overall superb condition on this spot.\nClown surgeonfish, butterflyfish, parrots and sixbar wrasse come and go on the reef. In the deepest areas, groups of hundreds of black and white damsels stand above the drop-offs. During your exploration, you may be surprised by a Maori wrasse (quite regularly seen in the bay), a yellowed-lipped sea krait sneaking between the corals, or a bench of convict surgeonfish storming a rock covered with small algae. The reef is also home to very localized concentrations of sea anemones (↕1.5-2m), around where dozens of fire clownfish gravitate. Sharks and turtles are not as common in Jinek Bay, but are sometimes sighted by the luckiest in the blue.\nThis site is natural and you will not find any restaurants in the immediate vicinity. A water source is available outside the sanitary block on the site, which is often closed.\nThese snorkeling spots are accessible to beginners and kids. You will enter the water gradually from a beach, or in a less than 3ft. deep area. The sea is generally calm, shallow, with almost no waves or currents. These spots are usually located in marked and/or monitored swimming areas. It is not necessary to swim long distances to discover the sea life.\nThis level only apply when the spot experiences optimal sea and/or weather conditions. It is not applicable if the sea and/or weather conditions deteriorate, in particular in the presence of rough sea, rain, strong wind, unusual current, large tides, waves and/or swell. You can find more details about the definition of our snorkeling levels on our snorkeling safety page.\nYou must be logged in to post a comment.\nSnorkeling spots are part of a wild environment and their aspect can be significantly altered by weather, seasons, sea conditions, human impact and climate events (storms, hurricanes, seawater-warming episodes…). The consequences can be an alteration of the seabed (coral bleaching, coral destruction, and invasive seagrass), a poor underwater visibility, or a decrease of the sea life present in the area. Snorkeling Report makes every effort to ensure that all the information displayed on this website is accurate and up-to-date, but no guarantee is given that the underwater visibility and seabed aspect will be exactly as described on this page the day you will snorkel the spot. If you recently snorkeled this area and noticed some changes compared to the information contained on this page, please contact us.\nThe data contained in this website is for general information purposes only, and is not legal advice. It is intended to provide snorkelers with the information that will enable them to engage in safe and enjoyable snorkeling, and it is not meant as a substitute for swim level, physical condition, experience, or local knowledge. Remember that all marine activities, including snorkeling, are potentially dangerous, and that you enter the water at your own risk. You must take an individual weather, sea conditions and hazards assessment before entering the water. If snorkeling conditions are degraded, postpone your snorkeling or select an alternate site. Know and obey local laws and regulations, including regulated areas, protected species, wildlife interaction and dive flag laws.","Can You Drown When Snorkeling?\nSnorkeling is a great activity for kids and adults alike. But as with any water activity, there are always some safety measures you have to observe. Everyone knows that you need to have a snorkel mask, tube, vest and flippers on when you enter the water.\nCan you drown when snorkeling? Yes, you can. It’s not common that it would happen but being in the water always entails the risk of drowning. Most accidents do happen when the snorkeler gets exhausted from either being in the water too long or fighting currents. Staying within your physical limitations reduces such a risk significantly!\nOver time, the varieties of masks and snorkel tubes have grown to cater to different types of snorkelers. This has also led to the use of full-face snorkel masks, which eliminate the use of a separate tube entirely. But these seemingly useful masks have been associated with several deaths of snorkelers.\nThis leads to the all-important question: are snorkelers at a risk of drowning, and if so, how?\nCan You Drown While Snorkeling?\nOver the years, there have been many reported cases (and many undocumented ones) of people who drowned while they were snorkeling. Since the activity does not really involve going deep into the water, it is often hard to understand how this is possible. There are a number of ways a snorkeler can be at risk of drowning:\nEssentially, a rip tide is a strong water current that pulls away from the shore, and if you’re stuck in one, it’ll drag you along with it.\nIf you’re a recreational snorkeler, you are probably not well acquainted with the tides and how quickly they can change. It doesn’t matter if you’re snorkeling close to the shore. You might have only been four feet deep in the water, but a sudden rip tide can drag you out into the depths of the ocean and take away your last breaths within a matter of minutes.\nRip tides have very strong water currents, which even the best swimmers might find hard to fight against. One way to potentially save yourself is to look for coral or a big rock and hold on to it very tight till the current passes. Even is this case, chances are you might not be able to hold on long or strong enough.\nThe best way to avoid the danger of getting sucked into rip tides is to:\n- Inform an instructor or snorkel guide about your whereabouts so they may keep an eye out and warn you of any sudden changes in the water current.\n- Check the forecasts before planning a snorkeling trip. Low tide is ideal for snorkelers. You can check the water tide online or through various weather apps. One such option is Tide-Forecast.com.\nAvoid snorkeling when the wind speed is high since this can also reduce your visibility and create a greater risk of drowning.\nFull-Face Snorkel Masks\nTropical locations, such as Hawaii are popular vacation spots for snorkelers. While the place might be beautiful, there have been a few reported cases of drowning during snorkeling.\nOf these, many of the tragedies have been attributed to full-face snorkel masks. While many new designs of snorkel tubes and masks are meant to keep water out, they can also unintentionally choke a snorkeler.\nFull-face snorkels cause the buildup of exhaled carbon dioxide inside due to a lack of ventilation. With no fresh air and oxygen to breathe in, snorkelers may start to suffocate.\nIt initially starts out as a slight amount of discomfort while breathing. If the mask is removed and the diver breathes in fresh air, they’ll be fine. If the feeling of discomfort continues, the snorkeler will start to suffocate and may be unable to breathe.\nSome researchers also suggest that carbon dioxide buildup is a possibility in regular snorkel masks as well and that the real cause of drowning is panic. Amateur snorkelers and first-timers may start to hyperventilate and run out of breath.\nThe research on whether full-face snorkel masks are dangerous for divers or not is still ongoing. The leading such masks are all considered safe for use when snorkeling. We would caution the use of cheap no-name versions that are not sufficiently tested for the CO2 build-up within the mask.\nHowever, in the meantime, some ways to avoid drowning while snorkeling are to:\n- Research the type of full snorkel mask you’re buying. Tried, tested and renowned manufacturers are ideal. Some people also say that full-face masks are harder to remove in cases of emergency. However, these masks are great if you have claustrophobia or are anxious as they provide a wider field of view and give the impression of more space around you.\n- If you’re a first-time snorkeler or someone who has a tendency to panic when they get in the water, it is better to get some practice sessions before going into the ocean. There are also segmented areas set up by professional snorkelers and instructors for beginners. Once you feel more comfortable, you’re ready to go out at sea.\n- Don’t panic. Anxiety is a major cause of drowning as it leads to hyperventilation, shortage of breath and eventually, silence.\nDry Drowning When Snorkeling\nDry drowning isn’t caused by choking in water. It is caused when you either swallow or inhale water in a way that it causes your vocal cords to spasm and prevent air from getting into your lungs.\nFor snorkelers, this may be possible either because of some water that may get into their snorkel tube or by accidentally taking in some sea water.\nSudden muscular paralysis can also lead to dry drowning in snorkelers. Dry drowning is characterized by breathing difficulties and a feeling of exhaustion due to the reduced oxygen intake.\nIf any of these symptoms are experienced, it is essential to get the snorkeler to a hospital or paramedic immediately. It should be treated as an emergency case because if it persists, it can lead to swelling in the brain. The longer the symptoms remain, the greater the damage and the higher the chance of a person dying.\nIs Snorkeling Dangerous?\nThe simple answer is that no, snorkeling is not dangerous. You’re close to the surface, you have a snorkel vest on, and you don’t even need to know how to swim to snorkel. However, there are some key safety precautions that need to be taken to stay safe.\nSafety Precautions When Snorkeling\n- Snorkelers need to be very comfortable in the water. Panic and stress can lead to ultimately fatal accidents.\n- Snorkelers need to be comfortable with the equipment they are using. This means knowing how to use their snorkel tube properly (to prevent water from getting in) or having a well-adjusted mask if they’re using a full one.\n- Snorkelers should be accompanied by a snorkeling buddy to help them out in case of emergencies.\n- Snorkelers should only go in waters that are safe and not known for frequent and dangerous rip tides.\n- Snorkelers should be aware of any potentially harmful sea creatures they need to look out for when they’re in the water.\nYou’re All Set to Snorkel\nWith the right equipment and safety measures, snorkeling is a safe activity for anyone. It is important to remember that at no point should a diver be overconfident and swim too far out or go too deep in the water. Water currents are unpredictable."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:e354b278-b721-4adf-9ca3-a10780541125>","<urn:uuid:7b48bfe0-798e-49fc-bf14-a0e6ff214d8c>"],"error":null}
{"question":"How do moisture concerns differ between rugs and hardwood floors?","answer":"For rugs, moisture can lead to mold and mildew development, requiring the use of dehumidifiers in humid conditions and proper storage in breathable fabrics like cotton rather than moisture-trapping plastic. For hardwood floors, moisture concerns are even more critical - wet-mopping should never be done as water can dull the finish, damage the wood, and leave discoloring residue. Additionally, hardwood floors need humidity control throughout the seasons to prevent gaps and cracks, requiring whole house humidifiers in winter and air conditioning or dehumidifiers in summer.","context":["Rugs significantly contribute to the beauty and coziness of our living areas. They bring in vibrant colors, diverse textures, and a sense of warmth, transforming a mere house into a welcoming home. However, the journey doesn’t end with just picking any rug; it involves careful selection and ongoing maintenance to guarantee that these rugs not only elevate the appearance of your space but also endure over time. Understanding the critical factors in choosing the ideal rug and following effective maintenance strategies are vital steps in this process.\nUnderstanding Rug Selection for Your Space\nChoosing the right rug for your space can greatly enhance the look and feel of a room. It’s important to consider various factors to make the best choice.\nThe size of the rug is crucial. It can define a room’s look. A small rug in a large room might look out of place. On the other hand, a rug that’s too large can overwhelm the space. Generally, in a living room, a rug should be large enough for the front legs of the furniture to rest on it. This helps to unify the look. For a dining room, ensure that the rug is big enough to accommodate the chairs, even when you pull it out. In a bedroom, a rug that extends around the bed creates a cozy atmosphere.\nColor and Pattern\nThe color and pattern of the rug play a significant role in setting the mood of your space. If your room already has a lot of colors and patterns, you might want to choose a rug with neutral tones and simple designs. This helps to balance the room’s appearance. Conversely, if your room has more neutral shades, a rug with bold colors or intricate patterns can add a touch of vibrancy. Remember, light-color rugs make a room appear more spacious, while dark-color rugs create a cozier feel.\nThe material of the rug is another important aspect. Different materials offer different textures and durability. Wool rugs are popular for their softness and durability. They are ideal for high-traffic areas like living rooms. Cotton rugs are lighter and easier to clean, making them suitable for kitchens or children’s rooms. Synthetic materials like nylon or polyester are stain-resistant and affordable, perfect for busy households. Natural fibers like jute or sisal offer a rustic look and are eco-friendly.\nComfort and Maintenance\nComfort is key, especially in areas where you spend a lot of time. Plush, soft rugs are great for bedrooms or living rooms where you might walk barefoot. For areas that require frequent cleaning, like dining rooms or entryways, choose rugs that are easy to clean and maintain. It’s also important to consider if anyone in your home has allergies. Some materials, like synthetic rugs, are hypoallergenic and a good choice for allergy sufferers.\nBalancing with Furniture and Decor\nThe rug should complement your furniture and remodeling. It acts as a connecting element in the room. Consider the style of your furniture and decor when choosing a rug. A traditional Persian rug might suit a room with classic furniture, while a contemporary geometric rug is better suited for modern decor. The rug should tie the room together, not clash with it.\nEssentials of Rug Care and Maintenance\nMaintaining and caring for rugs is key to ensuring they last long and stay beautiful. This article provides simple and effective tips for rug care and maintenance. Understanding these tips can help you keep your rugs in great condition.\nRegular Cleaning is Crucial\nRegular cleaning is essential for rug upkeep. Dirt and dust can accumulate over time, damaging the rug fibers. Vacuum your rug at least once a week. Be gentle when you vacuum. Avoid vacuuming the fringe, as it can cause damage. For high traffic areas, you may need to vacuum more frequently.\nSpot Cleaning Spills and Stains\nAccidents happen, and quick action is important. Blot spills immediately with a clean, dry cloth. Do not rub, as this can push the spill deeper into the rug. Use a mild detergent and water for spot cleaning. Test any cleaner on a small, hidden area first to ensure it does not harm the rug.\nProfessional cleaning is high priority for traditional rugs Virginia every one to two years. Experts possess the necessary tools and expertise to clean these rugs without causing any damage. This is particularly crucial for high-value or antique oriental rugs, where improper cleaning can lead to irreversible harm. Professional cleaning effectively removes deep dirt and helps in restoring the original beauty of wholesale oriental rugs, preserving their quality and appearance for the long term.\nAvoiding Sun Damage\nSunlight can fade rugs over time. To prevent this, rotate your rug periodically. This ensures even exposure to sunlight. If possible, use curtains or blinds to reduce direct sunlight on the rug.\nDealing with Moisture and Humidity\nMoisture can be harmful to rugs. It can lead to mold and mildew. Ensure not to expose the rug to constant moisture. In humid conditions, use a dehumidifier to maintain a healthy level of humidity in your home.\nWhen storing a rug, clean it first. Roll it instead of folding to avoid creases. Wrap the rug in a breathable fabric, like cotton, to protect it from dust. Avoid plastic, as it can trap moisture. Store the rug in a cool, dry place.\nRotating Your Rug\nRotating your area rug in the living room every few months can prevent uneven wear. This is especially important in high-traffic areas. Rotating helps in distributing wear and tear evenly, prolonging the rug’s life.\nPadding for Protection\nUsing a pad under your rug can extend its life. Pads reduce wear by absorbing the impact of footsteps. They also provide additional comfort and prevent the rug from slipping.\nBeing Mindful with Furniture\nHeavy furniture can damage rugs. Use coasters under furniture legs to distribute the weight. This prevents the fibers. Be careful when moving furniture on the rug to avoid tearing or stretching the rug.\nImmediate Action on Pet Damage\nIf you have pets, be vigilant. Address pet accidents immediately. Use cleaners specifically for pet stains. Trim your pet’s nails to avoid pulls or tears on the rug.\nRugs are more than just decorative items; they are investments in the comfort and aesthetic of our homes. Choosing the right rug involves understanding the interplay of size, color, pattern, and material with your existing decor. Maintaining Persian rugs VA through regular cleaning, proper handling, and protective measures ensures their longevity and beauty. By giving thought to both selection and care, you can enhance your living space and enjoy the warmth and style that a well-chosen rug brings to your home.","Wood floors are one of the easiest floor types to maintain. Here are a few do’s and dont’s that will help keep your wood floors looking their best. The vast majority of wood floors have a polyurethane finish. It is never a good idea to wax or oil a polyurethane finish. You also DO NOT want to use any harsh cleaning agents such as Comet©, SOS® Pads, or Murphy’s® Oil Soap on your hardwood floors. Also, do not use steam cleaners on your hardwood floors. For general day to day cleaning, we recommend Bona’s Hardwood Floor Care System. Please call us if you would like to purchase a cleaning kit. There are also some easy things you can do to maintain your hardwood floors:\n1. Sweep often\nDirt and grime are the enemies of your wood floors. Use a microfiber dusting pad to pick up the maximum amount of dirt. Dirt and grit will scratch the surface. If you do not own a microfiber dusting pad, we recommend sweeping with a soft bristle broom or vacuuming with a soft brush attachment. Remember to clean any throw rugs or door mats regularly because the dirt from them can easily spread to your hardwoods.\n2. Wipe Up Spills Immediately\nClean spills right away, especially if the spill has color to it. This can permanently stain your hardwood floor if left to soak in. Use a slightly dampened towel.\n3. Never Wet-Mop a Wood Floor\nFor general cleaning, never wet-mop a wood floor. Water can dull the finish, damage the wood, and leave a discoloring residue. Use the Bona Swedish Hardwood Floor Cleaner and spray directly on the floor or on the microfiber cleaning pad.\nIn addition to regular day to day maintenance, we also recommend the following preventative measures. These tips will extend the life of your floors by lengthening the intervals between resurfacing your floor. Remember, it’s not uncommon for hardwood floors to last 50+ years with proper maintenance.\n1. Schedule a Kimminau Clean and Coat.\nThis is the single most effective way to extend the life of your floors. This one-day process deep cleans your wood floors and applies a protective coating to the floors to help restore the shine. This can be done every year or two and helps prolong the life of your floor’s finish.\n2. Use Throw Rugs\nDirt and debris are enemies to your wood floor. Throw rugs capture grit and keep it from being tracked onto your wood floor where it can cause damage through abrasion. Some recommended places include both inside and outside of doorways, in front of kitchen appliances, and in high traffic areas. But don’t add throw rugs until 30 days after your floor has been refinished. This gives the finish time to cure properly.\n3. Use Fabric Pads or Glides Under Table Legs\nPut soft fabric pads or glides under the legs of tables and chairs to prevent scratching the floor when they are moved. Clean these pads regularly to keep grit from getting embedded in them. A pad with grit in it is no better than a hard chair leg. Occasionally, you will have to replace these glides if they get too dirty or lose their stickiness and fall off. They are available at most furniture and hardware stores and are inexpensive.\n4. Check Your Shoes\nSports shoes with cleats can damage a wood floor, so never walk on the floor while wearing these. Even women’s high heels can cause dents, so it’s best to take them off. Definitely check to make sure the heel pad is not worn down to the nail. This can do serious damage to a wood floor. Both men’s and women’s shoes should be examined regularly and properly maintained to avoid damaging the wood surface.\n5. Get a Dolly When Moving Heavy Furniture\nMoving heavy items like furniture can easily damage your floors. Sliding, rolling or dragging furniture, appliances or other heavy items across your hardwood floors can scratch, scrape and even gouge your floor. If you cannot pick up the item, we suggest using a dolly or cart with air-inflated rubber tires.\n6. Control Humidity\nHumidity in a home changes with the seasons. Although we take measures to acclimate your wood floors to the environment before installing them, gaps and cracks can appear in the winter months. Installing a humidity control system will help greatly. Ask your HVAC dealer to recommend a whole house humidifier to help keep moisture in the air in the winter months. In the summer, keep your air conditioner on or run a dehumidifier.\n7. Trim Your Pets’ Nails\nPets can cause major damage to wood floors. Be sure to keep dogs nails trimmed to minimize surface scratches. Even the most durable of finishes can be scratched by dogs so use rugs in traffic areas and other areas where your pets spend the most time."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:adda58bf-f57c-459e-b35a-27aa2c2a88e1>","<urn:uuid:4afcebe0-12fa-4e02-815f-f95fa1ba3d7e>"],"error":null}
{"question":"How does migration income affect rural business development in China?","answer":"Migration income positively affects rural business development in China in several ways. First, it increases the probability of rural households starting non-farm businesses. Second, it raises the likelihood that these households will remain in non-farm business after starting up. Third, migration income directly increases non-farm business income. This is particularly important because rural areas are typically credit deprived, and the income brought home by migrant workers provides vital capital resources that facilitate diverse business operations.","context":["Does Migration Income Help Hometown Business? Evidences from Rural Households Survey in China\nThis empirical study examines effects of household migration income on non-farm business in rural China. The restrictions on labor mobility in China were loosened after the economic reform in 1978. As a result, more and more rural households have family members engaging in temporary migration, working and living between rural home and urban areas, which forms a large \"floating\" population of migrant workers. The income migrant workers bringing home provides a vital capital resource for the credit deprived rural areas, and hence strongly promotes hometown non-farm business. This paper raises three questions: first, how does migration income affect the probability that rural households will start non-farm business? Second, how does migration income impact the probability that rural households will remain in non-farm business after starting up? Third, whether and how much does migration income increase non-farm business income? The findings indicate that migration income not only raises the probability of starting and remaining in non-farm business, but also increases non-farm business income. The empirical results in this paper confirm that, for financially constrained rural households in China, migration income offers a valuable capital resource and facilitates the development of diverse business operation in rural China.\nVolume (Year): 30 (2010)\nIssue (Month): 4 ()\n|Contact details of provider:|| |\nReferences listed on IDEAS\nPlease report citation or reference errors to , or , if you are the registered author of the cited work, log in to your RePEc Author Service profile, click on \"citations\" and make appropriate adjustments.:\n- repec:ebl:ecbull:v:15:y:2002:i:7:p:1-5 is not listed on IDEAS\n- Axel Heitmueller, 2005.\n\"Unemployment benefits, risk aversion, and migration incentives,\"\nJournal of Population Economics,\nSpringer;European Society for Population Economics, vol. 18(1), pages 93-112, 01.\n- Heitmueller, Axel, 2002. \"Unemployment Benefits, Risk Aversion, and Migration Incentives,\" IZA Discussion Papers 610, Institute for the Study of Labor (IZA).\n- Axel Heitmueller, 2002. \"Unemployment Benefits, Risk Aversion, and Migration Incentives,\" CERT Discussion Papers 0207, Centre for Economic Reform and Transformation, Heriot Watt University.\n- Hillel Rapoport, 2002. \"Migration, credit constraints and self-employment: A simple model of occupational choice, inequality and growth,\" Economics Bulletin, AccessEcon, vol. 15(7), pages 1-5.\n- Zai Liang & Yiu Por Chen & Yanmin Gu, 2002. \"Rural Industrialisation and Internal Migration in China,\" Urban Studies, Urban Studies Journal Limited, vol. 39(12), pages 2175-2187, November. Full references (including those not matched with items on IDEAS)\nWhen requesting a correction, please mention this item's handle: RePEc:ebl:ecbull:eb-10-00461. See general information about how to correct material in RePEc.\nFor technical questions regarding this item, or to correct its authors, title, abstract, bibliographic or download information, contact: (John P. Conley)\nIf you have authored this item and are not yet registered with RePEc, we encourage you to do it here. This allows to link your profile to this item. It also allows you to accept potential citations to this item that we are uncertain about.\nIf references are entirely missing, you can add them using this form.\nIf the full references list an item that is present in RePEc, but the system did not link to it, you can help with this form.\nIf you know of missing items citing this one, you can help us creating those links by adding the relevant references in the same way as above, for each refering item. If you are a registered author of this item, you may also want to check the \"citations\" tab in your profile, as there may be some citations waiting for confirmation.\nPlease note that corrections may take a couple of weeks to filter through the various RePEc services."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:b0f2e5ed-6b5d-4ea3-b4d6-549593cfb352>"],"error":null}
{"question":"I need a detailed breakdown comparing early human artistic achievements and dog domestication! Please list the major milestones chronologically from oldest to most recent.","answer":"Chronological timeline of major milestones: 1) 100,000+ years ago: Earliest possible decorated Nassarius shells in Israel 2) 82,000 years ago: Decorated and pierced Nassarius shells in Morocco 3) 75,000 years ago: Etched ochre and pierced shells in Blombos cave, South Africa 4) 45,500 years ago: Oldest known cave painting in Sulawesi, Indonesia 5) 35,000 BCE: Female figure carved from mammoth ivory in Germany 6) 30,000-32,000 years ago: Human-dog bond formation in Ice Age Europe and cave paintings at Chauvet-Pont-d'Arc 7) 15,000 BCE: Cave paintings at Lascaux and Altamira","context":["Humans and dogs have been the very best of pals for about 30,000 years at this point.\nResearch suggests the eternal bond was formed at some point in Ice Age Europe.\nThere are a lot of theories about how the human-canine relationship ultimately developed.\nMany seem to believe dogs are the descendants of a breed of wolves friendlier than most of their more blood-thirsty and aggressive compatriots.\nThese early wolf-dogs were permitted to assume the role of moochers, feeding off human scraps in exchange for their friendship and, at times, protection and security, among other functions.\nInterestingly, research also suggests dogs did not evolve from the gray wolves we see roaming the wild today, but present-day dogs and wolves are both likely descendants of a common wolf-like animal that went extinct thousands of years ago. Either way, they share common traits.\nToday, there are a multiplicity of dog breeds, and canines come in all shapes and sizes. But, regardless of whether we're talking about adorable little teacup Yorkies or muscly German Shepherds, it sometimes feels as if dogs act more like wolves than domesticated animals.\nThis is particularly evident when they pass other dogs on the street and end up going insane.\nAnyone who's ever owned a dog has likely experienced this. More often than not, your dog is the epitome of sweet and friendly when it's just you two, but instantaneously becomes vicious when another dog walks by. It's always somewhat shocking.\nDog behavior is often difficult to explain. There are any number of reasons your dog acts like another dog is Satan incarnate when walking by it on the street.\nPrimarily, it's likely because of the love your dog has for you and a desire to protect you from potential or ostensible threats.\nDogs are inherently social. After all, they've evolved from a wolf-like species. Wolves live in packs and depend on community for survival.\nSimilarly, your dog depends on you for survival. You're its primary source of socialization, companionship, affection and sustenance.\nThus, when a dog passes by an unfamiliar dog, it might perceive the other as a threat and display what is known as \"protective aggression.\"\nDogs show many forms of aggression, but protective aggression is exhibited when canines seek to protect a member of their social unit.\nIn this case, it's their owner. This is also why your dog might act somewhat aggressively toward other humans.\nDogs can actually instinctually detect a friendly face and are excellent at perceiving human emotions.\nAlternatively, your dog might be hostile toward other canines or animals out of fear or defensiveness. Dogs are programmed to protect themselves and fight back.\nThey also, at times, aim to preemptively intimidate perceived threats (\"The best defense is good offense\" approach).\nThis is perhaps why we frequently see smaller dogs tenaciously barking at larger ones, to the bigger dog's confusion.\nBut the main point here is when your dog is acting a little weird, it's often because canines have big hearts and truly love humans.\nThousands of years ago, humans and dogs formed an immortal friendship out of a need for companionship in an unforgiving world.\nSome argue the bond between humans and dogs is the reason civilization persisted because they helped one another survive.\nThe instincts that allowed this relationship to flourish are evident to this day, and it's part of what makes dogs such amazing animals and great friends.\nCitations: We Didnt Domesticate Dogs They Domesticated Us (National Geographic), Humans and dogs first became best friends around 30000 years ago claim scientists (Mirror), Aggression in Dogs (ASPCA), Dogs Not as Close Kin to Wolves as Thought (Discovery), Dogs can instinctively sense a friendly face (The Telegraph)","The oldest art: ornamentation\nHumans (Homo sapiens) make art. We do this for many reasons and with whatever technologies are available to us. Recent research suggests that Neanderthals also made art.\nExtremely old, non-representational ornamentation has been found across Africa. The oldest firmly-dated example is a collection of 82,000 year old Nassarius snail shells found in Morocco that are pierced and covered with red ochre. Wear patterns suggest that they may have been strung beads. Nassarius shell beads found in Israel may be more than 100,000 years old and in the Blombos cave in South Africa, pierced shells and small pieces of ochre (red Hematite) etched with simple geometric patterns have been found in a 75,000-year-old layer of sediment.\nThe oldest representational art\nSome of the oldest known representational imagery comes from the Aurignacian culture of the Upper Paleolithic period (Paleolithic means old stone age). Archaeological discoveries across a broad swath of Europe (especially Southern France, Northern Spain, and Swabia, in Germany) include over two hundred caves with spectacular Aurignacian paintings, drawings, and sculpture that are among the earliest undisputed examples of representational image-making. Among the oldest of these is a 2.4-inch tall female figure carved out of mammoth ivory that was found in six fragments in the Hohle Fels cave near Schelklingen in southern Germany. It dates to 35,000 B.C.E.\nThe caves at Chauvet-Pont-d’Arc, Lascaux, Pech Merle, and Altamira contain the best known examples of pre-historic painting and drawing. Here are remarkably evocative renderings of animals and some humans that employ a complex mix of naturalism and abstraction. Archaeologists that study Paleolithic era humans, believe that the paintings discovered in 1994, in the cave at Chauvet-Pont-d’Arc in the Ardéche valley in France, are more than 30,000 years old. The images found at Lascaux and Altamira are more recent, dating to approximately 15,000 B.C.E. The paintings at Pech Merle date to both 25,000 and 15,000 B.C.E. The world’s oldest known cave painting was found in Sulawesi, Indonesia in 2017 and was made at least 45,500 years ago.\nWhat can we really know about the creators of these paintings and what the images originally meant? These are questions that are difficult enough when we study art made only 500 years ago. It is much more perilous to assert meaning for the art of people who shared our anatomy but had not yet developed the cultures or linguistic structures that shaped who we have become. Do the tools of art history even apply? Here is evidence of a visual language that collapses the more than 1,000 generations that separate us, but we must be cautious. This is especially so if we want to understand the people that made this art as a way to understand ourselves. The desire to speculate based on what we see and the physical evidence of the caves is wildly seductive.\nThe cave at Chauvet-Pont-d’Arc is over 1,000 feet in length with two large chambers. Carbon samples date the charcoal used to depict the two head-to-head Rhinoceroses (see the image above, bottom right) to between 30,340 and 32,410 years before 1995 when the samples were taken. The cave’s drawings depict other large animals including horses, mammoths, musk ox, ibex, reindeer, aurochs, megaceros deer, panther, and owl (scholars note that these animals were not then a normal part of people’s diet). Photographs show that the drawing at the top of this essay is very carefully rendered but may be misleading. We see a group of horses, rhinos, and bison and we see them as a group, overlapping and skewed in scale. But the photograph distorts the way these animal figures would have been originally seen. The bright electric lights used by the photographer create a broad flat scope of vision; how different to see each animal emerge from the dark under the flickering light cast by a flame.\nA word of caution\nIn a 2009 presentation at University of California San Diego, Dr. Randell White, Professor of Anthropology at New York University, suggested that the overlapping horses pictured above might represent the same horse over time, running, eating, sleeping, etc. Perhaps these are far more sophisticated representations than we have imagined. There is another drawing at Chauvet-Pont-d’Arc that cautions us against ready assumptions. It has been interpreted as depicting the thighs and genitals of a woman but there is also a drawing of a bison and a lion, and the images are nearly intertwined. In addition to the drawings, the cave is littered with the skulls and bones of cave bear and the track of a wolf. There is also a footprint thought to have been made by an eight-year-old boy."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:3ec809a7-d246-4076-800c-2a192742e1f7>","<urn:uuid:f2ea795e-b75d-450f-b30f-df6484d0216d>"],"error":null}
{"question":"How does telematics gather información sobre vehicle emissions and fuel usage?","answer":"Telematics gathers information through three main sources in each vehicle: 1) GPS for location data, 2) A sensor on the engine's diagnostic port that provides data on fuel economy, service needs, and carbon dioxide output, and 3) An accelerometer that provides information about the vehicle's movement through axis-based motion sensing.","context":["Going Green: Using Telematics to Reduce Fleet Emissions\nDate posted: 5 May 2017\nTelematics has been a big name in the fleet management and insurance industries for the past couple of years, and their influence shows no sign of slowing down. Seemingly offering innumerable benefits in terms of fleet safety, logistics and overall performance, since they entered the industry, telematics has provided a platform for enormous advancement and improvement in the procedures and techniques of fleet management and revolutionised the way fleets can be monitored and utilised.\nThe massive amount of aggregate telematics data collected on fleets across the globe has given fleet managers the vital opportunity to accurately assess and analyse both the micro and macro facets of fleet operations. By comparing individualised data to company-wide trends, fleet managers have been able to direct more efficient routes, timetables and maximise profits.\nWhy Telematics Effect Fleet Emissions\nJust as the general increase in the adoption of telematics across the industry has led to an upwards trajectory in the efficiency and responsibility of fleet use, a similar correlative reduction in emissions has been noted.\nWith the gradual introduction of new, and increasingly strict, legislation by the EU on reining in carbon emissions, understandably more companies are putting a higher impetus on carefully monitoring their potential environmental impact. Similarly, a careful consideration of emissions, or more directly, your fleet’s fuel usage can have an extraordinary impact on your fuel economy. But where do telematics come into it?\nAs stated above, telematics provides an in-depth overview of the operation of each vehicle in a fleet, specifically three individual sources in each vehicle: the GPS, a sensor on the diagnostic port of the engine, and the accelerometer. Respectively, each target area provides information about specific locations, dates and times, as well as the health of the vehicle – providing data on fuel economy, service need and, importantly, carbon dioxide output. The accelerometer similarly provides distinct information about the movement of the vehicle, through axis-based motion sensing, the importance of which will soon become clear.\nThe information that telematics provides can be analysed, leading to actionable and pragmatic strategies that enable all fleets to lower emissions.\nHow Telematics Reduce Fleet Emissions\nUsing telematics, or specifically, the information they provide can lead to a significant reduction in your fleet emissions, through the following ways.\nIncreasing Route Optimisation\nAllowing fleet managers to monitor the miles and specific routes individual routes drivers use, and have used in the past, whilst comparing them to larger company operational information provides vital opportunities to analyse and eradicate unnecessary miles through optimal route planning or consolidating certain routes – simultaneously reducing the number of vehicles necessary, as well as the miles necessary to perform the jobs. Thus reducing the environmental and financial impact of each job.\nReducing Idle Time\nUnnecessary idling is an often hidden financial burden on every fleet, the American Environmental Agency stating that idling can waste as much as half a gallon (depending on vehicle classification) of fuel per hour. Whilst reviewed singularly, does not seem like much, when each individual vehicle is correlated, the costs can add up quickly.\nTelematics can help prevent this by capturing idling incident data, which can be programmed to begin at a specific number of minutes, enabling fleet managers to hold drivers more accountable and to enact better training and incentive programmes.\nIdentifying Unauthorised Vehicle Use\nUnsurprisingly, the unauthorised, or out of work hours use of vehicles wastes fuel and pointlessly increases carbon emissions. Using telematics, fleet managers can implement a real-time alert system to spot when drivers are using their vehicles without warrant or express permission, allowing them to address the issue quickly and mitigating the financial impact.\nBy identifying specific drivers and repeat offenders, fleet managers can implement better accountability and consequential procedures.\nEnsuring Timely Maintenance and Repairs\nAn important step to improving fuel efficiency, and consequently, reducing harmful emissions, is to properly maintain vehicles, and ensure that any repairs are done within a reasonable amount of time. Telematics, alongside offering a better operational understanding of a fleet, and thus, allowing fleet managers to more effectively coordinate specific maintenance and repair, saving money and mitigating operational disruption, are key to better fuel economy.\nKeeping on top of vital repairs is an important to improving mileage and fuel efficiency, as a faulty engine component can often cause a lag that affects the entire usage of the vehicle. By configuring your telematics systems to send real-time warnings when diagnostic codes are triggered, a vehicle can be immediately pulled in for repair, without causing further damage to itself or the environment.\nTelematics systems which include engine diagnostic tools give management the tools to proactively maintain vehicles, increasing vehicle performance and reducing fuel and greenhouse gas emissions.\nPromoting Greener Driving Behaviour\nTelematics, through an extended focus on the accelerometer, can initiate a programme of alerts to be sent to drivers when they are accelerating too quickly, idling too long, or displaying bad behaviour through speeding or repetitive stopping and starting.\nDrivers have a massive impact on the environmental efficiency of a vehicle, behaviour like speeding can have costly effects, producing enough heat to nearly double the wear on tires at increased road speeds, and simultaneously upping the maintenance cost.\nWith the oversight provided by telematics to pinpoint the areas where a driver’s road behaviour may need improvement, managers can provide exact and useful feedback for driver’s to improve upon."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:dc590008-47f5-433e-98fd-c3f70e1f0da1>"],"error":null}
{"question":"牙齿清洁和根管治疗哪个更能预防感染？🦷","answer":"Both procedures help prevent infection but in different ways. Regular dental cleaning by a hygienist removes plaque and tartar buildup that can lead to gum disease, helping prevent bacteria from entering the bloodstream through gum pockets. Root canal treatment addresses existing infection by removing infected pulp from inside the tooth and cleaning the canal, preventing the spread of infection into the jawbone. While cleaning is preventive maintenance, root canal treatment is a corrective procedure for already infected teeth.","context":["A family dentist plays a key role in the oral health of patients both young and old. According to the Centers for Disease Control, approximately one in five children between the ages of 5 and 11 have untreated cavities. There is more access to dental care than ever before, but many parents struggle with establishing…\nWhat Is a Family Dentist Exam Like\nRegular trips to the family dentist are essential for staying healthy. Ensuring that the mouth is in tip-top shape can aid the body in defending against harmful bacteria. Even with a good at-home oral health regimen, keeping up with regular exams as recommended is crucial to catching new developments sooner.\nWhy is dental health so important?\nThe body's immune system repels foreign invaders, such as viruses and bacteria. However, the immune system is not always able to fend off all attacks, and a person can become ill. Sometimes this is due to the way the harmful cells enter the body. The mouth can become an unwitting ally to bacteria if it is not well maintained.\nWhen oral health is compromised by a condition such as gum disease, the pockets between the gum tissue and teeth allow an easy path for harmful bacteria to enter the body's bloodstream. While at-home dental health is crucial to keeping bacteria away, only a dental exam can find and fix certain oral health issues that put the body at risk for infection.\nWhat to expect at a family dentist exam\nSometimes a person may stay away from regular dental care out of fear. This trepidation often can be resolved with better understanding of the common elements of a family dentist exam. Knowing what to expect can ease the mind of a nervous patient.\nOne way a dentist can diagnose dental issues is through X-rays. These allow the provider a peek under the gums and inside teeth to find any underlying issues. Annual X-rays also help dentists keep track of the progression of things such as gum disease from year to year. Improvements in dental health can also show in follow-up visits.\nA dental hygenist performs a basic cleaning of the teeth during a regular dental exam. The hygenist can spot areas of the mouth that need extra attention. Even performing twice-daily home cleanings does not completely keep teeth free of plaque and tartar buildup. Plaque is sticky and helps attract new bacteria to the surface of the tooth. Tartar is a hard shell that plaque eventually becomes when left on the surface. According to the Mayo Clinic, tartar can lead to gum disease, so having the teeth cleaned by a hygenist effectively removes trouble spots.\nThe dentist reviews a patient's X-rays and the dental hygienist's observations before examining the mouth. During this time, the dentist notes imperfections in the teeth and any indications that the gums are infected. At this time, the dentist also goes over any findings such as cavities or problems with roots. If required, a treatment plan is recommended for rectifying any critical issues.\nCheck out what others are saying about our dental services on Yelp:\nFamily Dentist in Aberdeen Township, NJ\nIncluding a trip to the family dentist among the schedule for regular medical exams can help keep your body healthy. Allowing a dental practitioner a glimpse into the mouth can not only diagnose any developing gum disease or other issues but also keep your smile bright for many years to come.\nWhen a tooth develops decay, it can be restored to normal function through a dental filling by a family dentist. Decay that does not cause pain is often discovered through routine dental visits, but sometimes cavities are symptomatic. While the type of filling recommended by the dentist depends on the location and size of the…\nFor many people, a family dentist is the right choice for routine dental care. While this type of practice is usually equipped to handle the specific needs of pediatric patients, it is still able to provide thorough care for adults as well. Family dentists typically offer a wide variety of services in one location and…","Your dentist may suggest root canal treatment if there is inflammation or infection inside one of your teeth. They will clean and fill inside the tooth to prevent further damage or infection. Root canal treatment can save your tooth from having to be taken out.\nWhat is a root canal?\nYour teeth have ‘roots’ that anchor them into your jawbone. Inside each of your teeth there is a mixture of blood vessels and nerves called pulp. Pulp sits inside a space called the pulp chamber and this extends down into the roots of the tooth. The pulp chamber within the root is called the root canal.\nWhy might I need root canal treatment?\nIf your tooth is damaged, bacteria can get inside it. This causes irritation, pain and swelling. If the pulp inside the tooth becomes infected, this can spread into the pulp inside the root canal. The aim of root canal treatment is to prevent your tooth from being taken out by removing the damaged pulp and treating the infection.\nYou may need root canal treatment for a number of reasons, including:\n⦁ Severe tooth decay\n⦁ Repeated dental treatment on the tooth\n⦁ A cracked tooth\n⦁ A broken crown\n⦁ Cracked or loose fillings and Gum disease\n⦁ An injury, such as a blow to your mouth\nIf an infection isn’t treated, it can spread further and cause an abscess, which is a collection of pus. This can be painful, tender and can cause swelling around your tooth and jaw.\nSometimes your tooth may look darker in colour than your other teeth, which means that the nerve inside your tooth is dead or dying. Without treatment, the infection may spread further into your jawbone and you may need to have your tooth removed.\nIf you’re experiencing toothache or would like to find out more information about root canal treatment, contact your dentist. We have over 350 dental practices around the UK, find your local Bupa Dental Care dentist here; we’d be happy to hear from you.\nPreparing for root canal treatment\nRoot canal treatment is done by a dentist. But if your tooth is difficult to treat, your dentist may refer you to an endodontist. This is a dentist who specialises in root canal treatment.\nYour dentist will examine you and ask about your symptoms, including any pain you’re having. They may also ask you about your medical history and any previous treatment you’ve had on your teeth. Your dentist will take an X-ray of your tooth. This can help to show which tooth is causing your pain and needs treatment, and how far any infection has spread. These examinations are necessary for your dentist to make sure that the tooth is not too badly damaged for root canal treatment.\nAlthough some root canal treatments can be completed within one appointment, most are done over two or more sessions with your dentist. Root canal treatment is usually done under local anaesthetic. This completely blocks pain from your tooth and jaw area, and you will stay awake during the procedure.\nWhat are the alternatives to root canal treatment?\nSometimes, damaged teeth can’t be repaired with root canal treatment. This is usually if your tooth is badly damaged or if you have severe gum disease which prevents your tooth from healing or being well supported after treatment. If this happens, your dentist may suggest you have the tooth removed (extracted).\nFind your local Bupa Dental Care practice\nWhat happens during root canal treatment?\nOnce the area is numb, your dentist will place a thin rubber sheet inside your mouth. This is called a dental dam and covers everything except the tooth being worked on. It helps to keep the area around the tooth clean and stop the spread of any infection.\nUsing a drill, your dentist will make a hole in the top of your tooth and remove the pulp. They will clean out the empty hole using small instruments and a liquid to irrigate and disinfect the inside of the tooth. The instruments help to make the canals a more regular shape to enable the tooth to be filled and cleaned more precisely. The irrigating liquid makes sure that all infected material is flushed out.\nOnce the tooth is clean, your dentist will fill and seal it. Unless the dentist is sure that all infection has been removed, they will put in a temporary filling. You then have a second appointment to have the tooth permanently filled.\nIf your tooth is badly worn or is at risk of further damage, your dentist may suggest having a crown fitted. This is an artificial cap that fits over your tooth. You’re more likely to need a crown if you’re having one of your back teeth treated, because these are used for chewing.\nHaving root canal treatment can be uncomfortable because you have to sit still with your mouth open for longer than you’re used to. Your dentist will make you as comfortable as possible before the procedure starts.\nWhat to expect afterwards\nAfter a local anaesthetic, it may take several hours before the feeling comes back into your jaw and face. Take special care not to bump or knock the area. You may need pain relief to help with any discomfort after the anaesthetic wears off and for the next couple of days. You can take over-the-counter painkillers such as paracetamol or ibuprofen. Always read the patient information leaflet that comes with your medicine and if you have any questions, ask your pharmacist for advice.\nFind your local Bupa Dental Care practice\nRecovering from root canal treatment\nWhen you next need to see your dentist will depend on the reasons for your treatment and how the procedure went. Your dentist may ask you to have further X-rays and check-ups to make sure that the tooth is secure and you’re healing well.\nSome people need to have further root canal treatment if the tooth cracks or more decay develops.\nAfter your treatment, it’s important to take care of your repaired tooth. These tips will help to keep your mouth, teeth and gums healthy:\n⦁ Brush your teeth twice a day using a fluoride toothpaste\n⦁ Rinse your mouth with a mouthwash, if your dentist recommends it\n⦁ Clean in between your teeth every day, either using floss or inter-dental brushes\n⦁ Don’t smoke\n⦁ Try to limit sugary foods and drinks and only have them at mealtimes\n⦁ See your dentist regularly for check-ups\nSide-effects of root canal treatment\nYou may have some slight tenderness around the tooth that has been treated, but usually this lasts for only a short time. If you have severe pain or any pain or discomfort that gets worse, see your dentist. The area around your tooth may also be swollen and bruised. This should get better within two weeks of your treatment.\nComplications of root canal treatment\nComplications are when problems occur during or after treatment. All medical and dental procedures come with some risk. But how these risks apply to you will be different from how they apply to others. Be sure to ask for more information if you have any concerns.\nMost root canal treatment is successful, but sometimes further problems can occur. Possible complications include:\n⦁ Damage to the root canal or the tissue surrounding the tooth\n⦁ Damage to the nerves around the tooth\nIf your root canal treatment doesn’t work, you may be advised to have more treatment. This is called re-treatment. You might also need re-treatment if your symptoms return years after you’ve had root canal."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:c02d15fb-7d40-4972-857f-c97af69c7acc>","<urn:uuid:c3ddf997-337e-4344-b05f-f94c78524ad5>"],"error":null}
{"question":"What are the different approaches to growing plants in space between the Chang'e 4 mission and NASA's Mars mission plans?","answer":"The Chang'e 4 mission conducted a short-term lunar plant experiment in a sealed chamber with cotton, rapeseed, potato, and arabidopsis seeds, but the plants died after about a week due to extreme lunar night conditions. In contrast, NASA's Mars mission plans, targeted for 2030, involve a more comprehensive bioregenerative system with a specific top-ten list of plants including lettuce, spinach, carrots, green onions, cabbages, tomatoes, radishes, bell peppers, strawberries, and fresh herbs. These plants are selected not just for food but also to provide oxygen, remove CO2, and purify water for the astronauts during their 5-year mission.","context":["Will a little piece of the Red planet go green in 2030?\nThe latest research into bioregenerative systems will complement the nutritional, physical and psychological needs of feeding a team of astronauts, for their 5-year sojourn to Mars. The details of preparations being made for manned missions to our nearest planetary neighbor are being presented this week, by Maya R. Cooper, at the 242nd National Meeting & Exposition of the American Chemical Society (ACS).\nVeg in space\nSuch a mission is unlikely to kick off until 2030, but preparations are well under way. The challenging nature of feeding planetary explorers is thrown into perspective when you take into account that, on the International Space Station, astronauts chomp through 4 pounds of food per day. Scale that up by the expected 1800 day time-frame for a mission, and that's a lot of food - nearly 7,000 pounds of food per person. Hauling all of that along in an interplanetary pantry is going to be non-starter, says Cooper of the NASA Advanced Food Technology project.\"That's a clear impediment to a lot of mission scenarios. We need new approaches,\" she said. \"Right now, we are looking at the possibility of implementing a bioregenerative system that would involve growing crops in space.\" Foods grown by such a system will have to meet some pretty hefty demands. Most of the plant should be edible, it should be easy-to-grow, and it should provide other vital services for the Mars-bound spaceship's ecosystem.\nFeeding, cleaning, and breathing for astronauts\nSo they would need to help with things like providing oxygen, removing CO2 and purifying the water that the astronauts would drink. And of course, they must provide both nutrition and culinary excitement to the space travelers. Putting that shopping list of requirements together, NASA's team came up with a top-ten list of plants that make the grade, for being the first veg in interplanetary space.\nAnd it's a list that many a beginner vegetable gardener would be familiar with - lettuce, spinach, carrots, green onions and cabbages are considered good space-staples, whilst tomatoes, radishes, bell peppers, strawberries, fresh herbs add a little zing and interest to the diet. Because the astronauts will be dealing with fresh produce, culinary skills for preparing meals will be an essential skill for the next generation of space traveler - no more sucking food from toothpaste-like tubes for the Mars-bound.\nFrom red to green\nBut while the spaceship's garden will provide a flourishing source of vitamins and fiber - plus an interesting hobby to fend off those space-travel blues - bulky staples will still be needed. These are likely to be prepositioned along the route, and on the Red Planet itself, by unmanned rockets sent up to two years before the manned phase of the mission kicks off.\nOf course, arguments may rage over the future of that other planet, Earth, and the advisability of spending billions of dollars just to leave mankind's footprint on Mars. But the idea of a little square of the Red Planet being turned over to a sustainable green veg-patch can only stir hope in man's eternal optimism.\nTop Image Credit: Mars surface illustration © Eraxion","Learning from Plant Astronauts and Ancient Farmers\nThat’s one small sprout for plant, one giant leaf for plantkind.\nJanuary 3, 2019\nOn January 3, 2019, Chang’e 4 became the first lunar exploration lander to touch down on the far side of the Moon. Chang’e, named for the Chinese Moon goddess (it was launched by China), had a special payload onboard. Along with a bunch of other scientific instruments, there was a tiny greenhouse. Four days after landing the first sprouts were seen. Along with cotton seeds, rapeseed, potato seeds, and arabidopsis seeds were chosen for their ability to germinate in challenging conditions. The biosphere experiment also carried fruit fly eggs and some yeast.\nThe organisms were kept in a sealed chamber, protected from the extreme temperatures and intense radiation on the Moon’s surface. However, the biosphere wasn’t designed to withstand the extreme cold of the 14 day lunar night. After about a week, the sprouts died.\nAlthough the experiment was short-lived, these brave plant astronauts are helping pave the way to growing food on future long-term space exploration missions. They’re also helping us understand plant biology here on Earth too. Onboard the International Space Station plants are helping us understand the scientific connection to the ancient wisdom of lunar almanacs. It turns out that farmers millennia ago were onto something.\nWe’ve known for decades that plants follow a 24 hour circadian rhythm, their leaves rising and falling throughout the day. Humans have a similar “biological clock” and scientists had assumed plants had a similar internal mechanism. Even plants grown in darkness show this same motion of rising and falling leaves.\nThe breakthrough came in the last few years. Peter Barlow (1942-2017), of the University of Bristol, UK, looked at data recorded from the 1920s onward on the leaf movements of beans and other plants. He matched these with computer models of the gravitational influence of the Moon at the time and location of the experiments. The data showed that when the lunar tide turns, so do leaf movements!\nHere’s where things go from cool to awesome. Barlow looked at data from plants on the International Space Station, and found they follow a 90 minute cycle instead of a 24 hour one. This is because the ISS orbits the Earth every 90 minutes, meaning its position relative to the Moon is changing faster!\nIt’s not clear exactly how the Moon influences this change, but Barlow believed it has to do with the movement of water inside the plant. Ocean tides are produced by the pull of the Moon’s gravity and Earth’s rotation creating bulges of water on opposite sides of the planet. A similar process might be happening at the microscopic level inside each plant.\nThis brings me back to the wisdom of our farming ancestors. They believed that when the Moon was waxing (becoming full), the Earth “exhaled” and sap (water) would “flow up”. This was the optimal time to plant seeds and harvest crops. When the Moon was waning (becoming new), the Earth “inhaled” and sap (water) would “flow down”. This was the barren time. A time to kill weeds and fertilize crops.\nIt turns out that some farmer 5,000 years ago discovered something that took us sending plants to the most complex facility humans have built in low Earth orbit to confirm. Sometimes I think we’re really smart with our fancy smartphones and spaceships, but then we discover (rediscover?) something like this. Check out Uncle Jim’s Almanac, it turns out it might be more science than pseudoscience after all."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:3a824305-ec85-448c-aa6d-0babc418948f>","<urn:uuid:96aae70e-1a29-45e4-94ea-cfa9ada1fbc1>"],"error":null}
{"question":"How do the R/V Sally Ride and electric ferries compare in terms of their power systems and environmental impact?","answer":"The R/V Sally Ride uses an integrated diesel-electric plant with four Cummins QSK38-DM main engines providing 3,900 kW of power, managed by Siemens' Blue Drive system for enhanced efficiency and reduced fuel consumption. In contrast, modern electric ferries can be fully battery-powered, capable of transporting over 100 cars and 300 passengers while completely eliminating diesel fuel usage. For instance, Washington State Ferries' switch to battery-electric operation will save 20 million gallons of diesel across 22 vessels. While both vessels represent advances in maritime technology, electric ferries achieve greater emissions reduction through full electrification, whereas the Sally Ride focuses on optimizing diesel-electric efficiency.","context":["Shipping, Pollution and Technology – Electrification and Energy Storage in Maritime Shipping\nIn addition to traditional sails and newer rotor sails, electric technologies and energy storage are emerging as alternatives to reduce fossil fuel consumption in the maritime shipping industry.\nElectric propulsion and battery storage systems will play a key role in reaching the IMO’s global target of cutting annual emissions in maritime shipping 50% by 2050 from 2008 levels. Big players, from suppliers to shippers to fleet operators, are moving quickly to deploy and commercialize the technology. Although internal combustion engines will remain the standard for the time being, electric and battery technologies have matured enough for use in maritime applications, thanks to advancements in the automotive sector. In addition, continued technology advancements will bring costs down and improve viability of these technologies for maritime applications.\nIncumbents Delivering Innovation\nInnovation in this sector is primarily taking place with incumbents – traditional suppliers are leveraging core competencies and building out new capabilities in electric propulsion and battery storage to:\n- Expand their offering\n- Meet demand for lower-emission technologies\n- Remain competitive in the transition away from fossil fuel-based propulsion.\nIn addition, suppliers are developing partnerships to leverage expertise in areas outside of their core competency.\nMaersk is piloting a 600 kW marine battery system on board the Maersk Cape Town, a 250-meter container ship with a 4500 TEU carrying capacity, to improve vessel performance and reliability. This vessel also includes a waste heat recovery system, allowing battery charging from waste heat. Siemens has developed a suite of electric solutions, led by the Drive LV, a diesel-electric propulsion system for low-voltage drives up to 5MW.\nABB, a leading supplier of electric transportation systems, is developing new solutions for marine vessels. In 2018, ABB signed a memorandum of understanding with Ballard Power Systems to develop a megawatt-scale fuel cell power solution for larger ships. The system leverages and optimizes existing kilowatt-scale fuel cell technologies and provides an electrical generating capacity of 3MW. ABB also recently signed an agreement with SINTEF Ocean to expand the work in fuel cells and test the viability of the technology for main ship propulsion. ABB has been working with the SINTEF laboratory for research purposes, including development of the ABB Onboard DC Grid, which allows for integration of energy storage and fuel cells into onboard power systems.\nFuel can represent 50%-60% of a ship’s total operating costs. As emissions regulations force the use of lower-emitting fuels and place upward pressure on this cost, low-emission technology will become more attractive. Across all sectors, the main drivers for electric propulsion and battery storage are:\n- Emissions reduction\n- Improving performance and reliability\n- Providing redundant power\nIn the near term, the primary use of energy storage systems is to improve performance, reliability and the efficiency of on-board electrical systems. Battery storage on vessels can provide redundant power and prevent unnecessary use of on-board auxiliary generators by evening out rapid changes in . The main initial targets for hybrid- and fully-electric propulsion are short-distance, smaller vessels with particularly inefficient operations. Two examples of this are ferries and tug boats.\nFerries are initial targets for electric operation, as they cover shorter, fixed routes. This simplifies charging infrastructure needs and reduces the range needed, compared to larger cargo ships. Battery packs can fully power ferries for their duration of their journey, and predictable routes allow for efficient deployment of shoreline charging infrastructure. Growing demand for passenger and car transport by ferry, particularly in the UK and Europe, is allowing ferry operators to invest in new ships with hybrid electric and battery electric drive systems and new routes. Fully electric ferries can transport over 100 cars and 300 passengers. A ferry fleet can use almost as much fuel as a mid-sized airline. As such, electric ferries can save millions of dollars in annual fuel costs, allowing fleet operators to recover the higher up-front cost quickly. This segment is growing quickly – 101 battery-equipped ferries are in operation today, with another 76 under construction.\nIn this segment, electric systems are competing with lower-cost diesel ferries. In some cases, hybrid-electric systems can offer a 35% fuel savings. The switch to battery-electric by Washington State Ferries, the world’s second largest fleet operator, will save 20 million gallons of diesel across 22 vessels. Although still more expensive than diesel vessels, declining battery costs have made electric vessels more cost-competitive, as the vessels will pay for themselves quickly from fuel savings.\nTug boats are also initial targets for energy storage systems. These small boats typically have minimal energy requirements, but they must have enough capacity to aid larger vessels when required. Additional expensive and heavy engines have typically been installed to provide surges in power, but these are rarely used and create additional weight on the vessel. The addition of battery storage would allow tugs to require only one engine, with peak power provided by a battery bank. Tugs could run on battery alone when moving to and from jobs, with a single engine assisting the batteries during surges in power demand.\nOnboard installation of energy storage provides peak shaving and dynamic support for diesel generators. The generators typically deployed on drill ships must provide dynamic power demand to ensure the power system remains online, even in challenging conditions. This creates peaks and troughs in electricity demand. However, batteries can store electricity during times of low demand for deployment during periods of high demand, allowing the generators to be turned off and reduce fuel consumption.\nWhen vessels are operating over long distances, key challenges to use battery electric propulsion are time and capacity to recharge. As a result, the amount of energy that can be stored by onboard batteries is limited. This creates an opportunity for the use of fuel cells in combination with battery storage. Pilot projects are testing feasibility for fuel cells in ships. The Zero-V project, which involved a fully hydrogen-powered, 10-knot research vessel with 2400 nautical mile range, found the use of renewable hydrogen dramatically reduced emissions along the hydrogen pathway.. In addition to zero-emission operations of the vessel itself. The research vessel was able to refuel at 4 different ports along the west coast of the U.S., and the project was ultimately considered a success. The most promising applications for fuel cells include low-speed, large capacity vessels, in contrast with the short-distance, high-power applications for battery electric.\nAlthough battery storage and electric propulsion offer an alternative to combustion engines, the latter will remain the norm for some time. Leading suppliers in the industry, such as ABB and Siemens, are developing new products to serve market demand for connected, low-emission technologies and build out core competencies in these areas. Due to the high cost of developing new drive systems for ships, barriers to entry in this segment are high. Startup activity in the maritime sector has remained focused on connectivity and digitization to optimize vessels, routes and port calls.\nKeep an eye out for…\nMy next piece on maritime transportation will focus on connected and digitized revolution in the maritime industry. Many fleet operators have invested in gathering operational data, fueling a wave of startups developing data analytics solutions to optimize vessel operations, routing and port calls. Some startups are using geospatial data to track and analyze ship operations. Increasing digitization and connectedness are providing increased fuel efficiency, compliance and performance audits and operating cost reductions.\nAccelerate the Progress of Your Innovation\nWhether your focus is renewable energy, agriculture, next-generation mobility or the circular economy, The Solar Impulse Foundation’s 1000 Efficient Solutions label could give you the market access and investor exposure that can take your company to the next level. If you have a sustainable innovation with commercial potential, 1000 Efficient Solutions is an opportunity you won’t want to miss.","Sally Ride's Legacy Lives On in AGOR 28\nAuxiliary General Oceanographic Research vessel (AGOR 28) began its life as the R/V Sally Ride on August 9th, 2014, with the traditional crack of the champagne bottle. But that's where its comparison to \"traditional\" ends. The R/V Sally Ride is the latest high-tech vessel constructed for the Office of Naval Research at Dakota Creek Industries (DCI) Shipyard in Anacortes, Washington. The Sally Ride is the first research vessel named after a woman, a fitting tribute to the first American woman in space.\nOutfitted with the latest in sonar, over-the-side winches, triple mapping capability, and a host of other innovations designed to improve deep sea exploration; the vessel will join the research fleet at Scripps Institution of Oceanography at the University of California, San Diego (UCSD) in 2016. Dr. Sally Ride served on the faculty of UCSD from 1989 to 2007.\nThere remains a lot of work to be done before the first scientists step onboard, fire up the computer lab, and begin the first of thousands of research projects that will be hosted aboard the vessel over its expected 20- to 30-year lifespan. Dakota Creek will work with Siemens Marine to install the automation equipment and complete the outfitting required in Phase 3 of the project.\nThe christening marks a milestone in the Navy's Fleet Renewal Plan that began 14 years ago. The Navy's commitment to maintaining six research vessels means two ships will be retired to make room for the R/V Sally Ride and its twin-sister ship the R/V Neil Armstrong launched in March. For Scripps, that means decommissioning the R/V Melville over the next two years. It also means building a $25M pier to accommodate the newest member of the fleet.\nDick Nelson, owner of DCI, and Guido Perla, Chairmen of Guido Perla Associates (GPA) and the designer of the vessel, reminisced about their partnership in the design competition, Phase 1 of the project, held by the Navy to select the design and construction Shipyard for the vessels.\nKathryn Sullivan, Undersecretary of Commerce for Oceans and Atmosphere, and NOAA administrator delivered the principal address. Sullivan served with Ride onboard the space shuttle Challenger and was the first woman to walk in space. In her address to the crowd, she pointed out \"...we currently have better maps of the moon and Mars and Venus than we have of our own oceans.\" And that an estimated 60 to 80 percent of marine species remain undiscovered. She felt the R/V Sally Ride with its state of the art computer and research labs will undoubtedly advance our knowledge of our oceans. She closed by wishing the vessel and her crew, \"...great adventures, grand discoveries, and safe passage home.\"\nRepresentatives from NOAA, NASA and the Navy joined together with Ride's family and her lifetime partner, Tam O'Shaughnessy, to officially welcome the ship to the Scripps fleet. Dr. O'Shaughnessy, head of the Sally Ride Science organization, is the ship's sponsor and hopes to maintain a professional connection to the vessel as it serves the scientific community.\nDr. O'Shaughnessy expressed her appreciation for the honor. She hoped the ship would instill Sally's \"...adventurous spirit and quest for knowledge in all who are privileged to sail on her.\"\nStewardship of the vessel was determined through a bid process with several research organizations competing for the right to operate the new AGOR ships. Scripps was chosen for AGOR 28 with AGOR 27 going to Woods Hole Oceanographic Institution. Captain Tom DesJardins and Sr. Chief Engineer, Paul Bueren, of Scripps, will likely take command of the vessel in mid-2015. Bueren has been onsite at DCI for the construction and will stay on through commissioning and sea trials.\nNAVSEA funded the $145M, two-ship project and will retain ownership. Scripps will operate, crew, and maintain the vessel, as well as manage the scheduling of research projects. Shipboard time will be available to students and researchers from UCSD as well as other universities.\nIn addition to its scientific mission, the vessel is also intended for global operations in support of national security interests in the marine field. As such, the design specifications were demanding. The ship had to be an efficient, fully-integrated and highly-resilient blue-water laboratory capable of exploring the undersea environment anywhere in the world. Most of all, it had to be quiet. Since a great deal of ocean research involves listening, excessive ship noise was unacceptable.\nGPA's unique hull design met the \"bubble sweepdown\" performance requirement by diverting bubbles away from the sensitive sonar area. To complete the noise-dampening goal, engineers chose systems, defined equipment locations, and designed special installation methods with acoustics as a priority.\nWorking deck space is a premium commodity in oceanographic work. The AGOR vessels have 2,557 square feet of clear deck space, with 1,873 square feet of that space on the open aft deck.\nHousing the most modern scientific laboratory afloat gives scientists the ability to analyze specimens and data in real time onboard instead of simply collecting materials and bringing it back to a landside lab for later analysis. With analysis occurring onboard, electrical power quality, sample purity, and vessel stability were strong considerations. Thanks to Kongsberg and Siemens propulsion controls and capabilities, the ship can remain fully operational in Sea State 4, and can handle dynamic positioning relative to a fixed position in Sea State 5 with a 2-knot current and 35-knot winds.\nDCI called upon a multitude of local, national and international vendors to outfit the ship with the best equipment available. Siemens will begin installing their new Blue Drive™ system by late summer. This advanced, multi-drive, low-voltage system manages the speed of the AC propulsion motors controlling the propellers, stern thruster, and bow thruster. The system provides enhanced reliability and efficiency, multiple failsafe features, reduced fuel consumption, lower maintenance costs, and increased ease of operation for the crew. Siemens is also supplying the majority of the electrical switchgear, the ACCU automation, and their custom, condition-based equipment monitoring system.\nFour vibration-isolated Cummins QSK38-DM main engines provide more than 3,900 kW integrated electric power for propulsion and all other ship functions. The integrated diesel-electric plant allows for multiple generator configurations, ensuring the diesel engines operate at peak efficiency in all modes.\nSiemens uses a \"combinator\" style control function to integrate motor speed and propeller pitch, which allows the operator to set the propeller at its most efficient setting throughout the entire range of operations, from cruising to heavy towing. While the combinator is fairly common in ships with controllable pitch propellers with direct drive diesels, it is unusual in a variable-speed electric drive system.\nSpecifications for the deck equipment were stringent. Cranes and winches load equipment and deploy ROVs and buoys weighing in excess of 20,000 pounds. Allied Marine supplied the stern frame with its 12-foot inboard and outboard reach, along with the TK4-30 portable crane and the TK 70-70 aft-deck main crane. They also supplied the motion-compensated CTD handling system and the starboard side handling device, both of which extend to the waterline for improved safety and load control.\nMarkey Machinery of Seattle supplied two electric motor-driven CAST-6-125 hydrographic winches, and the DETW-9-11 traction winch, both with AC variable frequency drives for precise control.\nAs production moves into Phase 3 of the project, Kongsberg will supply a SONAR synchronization system along with their advanced multi-beam SONAR units, a HiPAP gantry with a Sonardyne single beam survey system, and a sub-bottom profiler SONAR. Additional equipment plans include a transducer array, a mid-water echo sounder, and three current profilers operating at different frequencies. This system provides scientists a greatly expanded mapping capability over existing vessels.\nThe ship's design is compliant with 46CFR Subchapter U (Oceanographic Vessels) and built to ABS Under 90 Meter rules. It will be certified as A1, Circle E, AMS, ACCU, NIBS, Ice Class D0, and UWILD.\nAccording to Hollie Anthonysz, Program Manager of Vessel Construction, the DCI team is looking forward to completing Phase 3 of both ships over the next year. Scripps and Woods Hole are both eager to see the vessels at sea."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:172e6e12-aa28-43ac-9719-904f6d0d11e6>","<urn:uuid:430ed832-40f5-4bcc-81f3-e02d6ebe8e91>"],"error":null}
{"question":"How do ESSA's literacy program goals compare to the learning objectives outlined in standards-based report cards?","answer":"ESSA's literacy program focuses on improving reading and writing achievement through comprehensive literacy instruction plans, professional development, and support for school library programs in low-income areas. Standards-based report cards, however, take a broader approach by evaluating student progress across multiple subject areas using four proficiency levels (1-4) that measure specific grade-level standards and expectations. While ESSA emphasizes systemic support and resource allocation, standards-based report cards provide detailed individual student assessment across the curriculum.","context":["Jeff Frost, CSLA Legislative Advocate\nOn December 10, 2015, President Obama signed into law a new education reauthorization bill that had been worked on for several weeks by Congressional representatives. The new law, called the Every Student Succeeds Act (ESSA), makes a number of important changes on how education policy will be handled and allows states, not the federal Department of Education, set standards for educational success. This act had not been updated since the NCLB was agreed to in the first term of George Bush’s presidency. Additionally, this bill makes strides toward including school libraries as an educational priority.\nOne of the biggest changes to the new Act is the inclusion of specific library references. School librarians and school libraries are now considered true educational partners. AASL President Leslie Preddy said, “For school-age students, ESSA represents an historic new chapter in federal support of education, one that will ensure effective school library programs are there to help them learn how to use new technology tools, develop critical thinking, and the reading and research skills essential to achievement in science, math and all other ‘STEM’ fields.”\nThe new ESSA Act authorizes the Innovative Approaches to Literacy program that allows the education secretary to “award grants, contracts, or cooperative agreements, on a competitive basis” to promote literacy programs in low-income areas, including “developing and enhancing effective school library programs.” Those funds can go toward library resources and providing professional development for school librarians. States and districts can also use Title II funds for “supporting the instructional services provided by effective school library programs.” And the bill encourages local education agencies to assist schools in developing effective school library programs, in part to help students gain digital skills.\nThe new ESSA statute includes several references to school libraries and teacher librarians. A summary of those changes is outlined below:\nTitle I – Improving Basic Programs Operated by State and Local Educational Agencies – Under Title I of ESEA, local educational agencies (LEAs) must develop plans to implement federal education activities.\n* The ESSA includes new provisions that authorize local plans to include a description of how the LEA will assist schools in developing effective school library programs to provide students an opportunity to develop digital literacy skills and improve academic achievement.\nTitle II, Part A – Supporting Effective Instruction – Under current law, Title II, Part A provides funds for States and school districts to increase academic achievement by improving teacher and principal quality (primarily through professional development).\n* The Conference Agreement includes new provisions that authorize States to use funds to support the instructional services provided by effective school library programs.\n* The Conference Agreement includes new provisions that authorize LEAs to use funds to support the instructional services provided by effective school library programs.\nTitle II, Part B, Subpart 2 – Literacy Education for All, Results for the Nation (LEARN) – Includes a new literacy program to help improve student academic achievement in reading and writing by providing federal support to States to develop, revise, or update comprehensive literacy instruction plans.\n* The ESSA requires local grants that focus on children in kindergarten through grade 5 to provide high-quality professional development opportunities for teachers, literacy coaches, literacy specialists, English as a second language specialists (as appropriate), principals, other school leaders, specialized instructional support personnel, school librarians, paraprofessionals, and other program staff.\n* The ESSA requires local grants that focus on children in grades 6 through 12 to provide training for principals, specialized instruction support personnel, school librarians, and other school district personnel to support, develop, administer, and evaluate high-quality comprehensive literacy instruction initiatives.\n* The ESSA authorizes all local grants (that serve children in kindergarten through grade 5 or children in grades 6 through 12) to provide time for teachers (and other literacy staff, as appropriate, such as school librarians or specialized instructional support personnel) to meet to plan comprehensive literacy instruction.\nTitle II, Part B, Subpart 2, Section 2226 – Innovative Approaches to Literacy – Includes a new authorization of the Innovative Approaches to Literacy program (previously funded through appropriations legislation) that provides dedicated funding to promote literacy programs in low income communities.\n* The ESSA authorizes funds to be used for developing and enhancing effective school library programs, which may include providing professional development for school librarians, books, and up-to-date materials to high need schools.\nTitle II, Part B, Subpart 2, Section 2232 – Presidential and Congressional Academies for American History and Civics – Includes a new American History and Civics Education program to improve the quality of American history education and teaching.\n* The ESSA defines an eligible entity as an institution of higher education or nonprofit educational organization, museum, library, or research center with demonstrated expertise in historical methodology or the teaching of American history and civics.\nTitle IV, Part A – Student Support and Academic Enrichment Grants – Authorizes a new Student Support and Academic Enrichment grant program to help States and school districts target federal resources on local priorities.\n* The ESSA authorizes States to use funds to assist LEAs with identifying and addressing technology readiness needs, including Internet connectivity and access to school libraries.\n* The ESSA authorizes States to use funds to assist LEAs in providing teachers, paraprofessionals, school librarians and media personnel, specialized instructional support personnel, and administrators with the knowledge and skills to use technology effectively, including effective integration of technology, to improve instruction and student achievement.\nTitle IV, Part B – 21st Century Community Learning Centers – This program supports community learning centers that provide academic enrichment opportunities during non-school hours for children.\n* The ESSA authorizes grants to be used to support expanded library service hours (consistent with current law).\nTitle VIII – General Provisions – Contains definitions that apply to all activities under ESEA.\n* The ESSA updates the definition of “specialized instructional support personnel” to include “school librarians.”","Standards Based Report Cards\nWhat are educational standards? Standards help teachers ensure their students have the skills and knowledge they need to be successful by providing clear goals for student learning. The standards clearly communicate what is expected of students at each grade level. Standards are specific learning outcomes that students must meet at certain grade levels. They provide the foundation for 21st century learning opportunities. Grade level standards are aligned with the New Jersey Student Learning Standards. See links below for more information on educational standards.\nWhat is a standards-based report card? A standards-based report card highlights the most important skills students should learn in each subject at a particular grade level. In our elementary school learning environment, students receive marks that report student progress toward meeting content and performance standards three times a year. Benchmarks for each of the trimester marking periods are used to determine if the student is progressing toward the end-of-year standards.\nStandards based report card provides parents with a more detailed outline of expectations. It informs parents about their child’s progress towards achieving specific learning standards. A standard based report card expands the subject area into a list of skills and knowledge our students are responsible for learning. The purpose is to provide feedback that is more detailed, rather than one letter grade for a content area. A standard based report card measures how well a student is doing in relation to grade level standards—NOT the work of other students. This provides a better understanding of a child’s strength and weaknesses.\nWhat is a trimester marking period? Our standards-based report card is based on three marking periods (December, March, and June). Teachers will use marking period grading benchmarks to evaluate student progress. Benchmarks often change each trimester to reflect new standards and expectations for student growth.\nWhat are the proficiency levels on the report card? The proficiency level represents a student’s progress on the report card. There are four categories, 1-4.\n4-Exceeds Standards—Earning a “4” means the student has advanced understanding and exceeds grade-level expectations. A student receiving a “4” Consistently grasps, applies, and extends key concepts, process and skills. This student shows initiative, challenges himself or herself, and demonstrates this advanced knowledge at school. A “4” is difficult to obtain and indicates unusually high achievement beyond grade level benchmarks.\n3- Meets Standards—Earning a “3” means the student has proficient understanding and meets grade-level expectations. We want all of our students to reach a level “3.” A student receiving a “3” is right on track with our high academic expectations. A “3” is something to be celebrated!\n2-Approaching Standards—Earning a “2” means the student has basic understanding and partially meets grade-level expectations. A student receiving a “2” is beginning to grasp and apply key concepts, processes, and skills, but has not yet reached the proficient level. A “2” should indicate to parents that their child may need some extra help or extra time to practice/understand that concept or skill.\n1-Needs Support—Earning a “1” means the student has minimal understanding and does not meet grade-level expectations. A student receiving a “1” has academic delays according to our district standards, and interventions may be needed to learn and stay on track with district expectations.\nPlease keep in mind that these numbers are not indicators of a linear performance from trimester to trimester; rather, they are reflective of the goals of the current trimester. For example, your child may receive a “2” in a content skill in one trimester, but he/she may not automatically receive a “3” in that same content skill in the next trimester. This can be reflective of the natural progression of skills and curriculum benchmarks that developmentally change throughout the year.\nWe want parents to recognize that our teaching and assessments are directed at ensuring that all students have an opportunity to meet our curriculum standards. Different students progress at different rates, so standards may be met in varying amounts of time, with varying amounts of teacher support.\nHow do teachers determine proficiency levels? Teachers determine which proficiency level your child has earned by using a report card benchmark. A report card benchmark outlines exactly what your child needs to know or be able to do to earn a 4, 3, 2, or 1 on the report card.\nThroughout the trimester, teachers assess students. With the standards for that grade level in mind, they consider: knowledge of the child (How does this child learn best? In what way does this child communicate his/her learning?); evidence of student performance (work samples that have been collected, student performance on activities and assessments, teacher anecdotal notes); and, finally, knowledge of what the child is expected to know or be able to do (district curriculum and standards, report card benchmarks). Effort, participation, adherence to rules, attendance, late work or missing assignments are not measures of achievement. These behaviors will be noted in a separate section.\nAfter going through this process, the teacher assigns a proficiency level for each line of the report card.\nWill there be conference opportunities available to discuss my child’s progress? Yes. Parent-teacher conferences are scheduled twice a year (October and February).\nWill parents be able to view the grading benchmarks? The elementary report card benchmarks are available to parents and students through the link on this website. We want you to understand exactly what your child knows and is able to do. Paper copies are available upon request at your child’s school office.\nWhat have I learned about standard based report cards?\nStandard based report cards make high standards the focus of every classroom. It will be clear to everyone what learning should take place during the school year; and teachers will be collaborating more than ever to create quality work that aligns to the standards. In a traditional system, students get higher grades when they turn in work on time and complete every task, even if they do not show mastery of the standards. Sometimes students receive lower grades because they do not have good work habits, even if they have mastered the standards. Fair grading will show mastery of content, as well as initiative and work habits. Students are assessed on their ability to apply the standards to real life situations, not just state facts that have been memorized. Teaching and learning must be focused on standards, rather than just coverage of materials."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:9fa213ed-3208-48c3-a957-f4bfb9927de0>","<urn:uuid:e18d7b18-9c3a-41ba-9f03-82f57b064d1c>"],"error":null}
{"question":"How do the DASH diet and expert recommendations compare in their approaches to sodium reduction?","answer":"The DASH diet emphasizes consuming 7-8 servings of grains, 4-5 servings each of fruits and vegetables, 2-3 servings of low-fat dairy, limited meat and sweets, while recent expert recommendations focus on specific strategies like choosing fresh foods over processed ones, reading nutrition labels (looking for 5% DV or less), using sodium-replacement salts with potassium, and diluting salt in liquid-based dishes. Both approaches have been shown to effectively lower blood pressure.","context":["Presentation on theme: \"Hypertension Blood pressure levels are a function of cardiac output multiplied by peripheral resistance (the resistance in the blood vessels to the flow.\"— Presentation transcript:\nHypertension The major factors which help maintain blood pressure (BP) include the sympathetic nervous system and the kidneys. Optimal healthy blood pressure is a systolic blood pressure of <120 mmHg and a diastolic blood pressure of <80 <120/80.\nHypertension Approximately one in four American adults has hypertension. As many as 2.8 million children also have high blood pressure. The prevalence of hypertension increases with age.\nPrevalence of Hypertension by Age Age 18-29 30-39 40-49 50-59 60-69 70-79 80+ % Hypertensive 4 11 21 44 54 64 65\nHypertension When the normal regulatory mechanisms fail, hypertension develops. Hypertension is so dangerous because it gives off no warning signs or symptoms.\nUntreated hypertension can result in: Arteriosclerosis--Kidney damage Heart Attack--Stroke Enlarged heart--Blindness\nFactors Influencing the Development of Hypertension High-normal blood pressure Family history of hypertension African-American ancestry Overweight\nFactors Influencing the Development of Hypertension Excess Consumption of Sodium Chloride Certain segments of the population are ‘salt sensitive’ because their blood pressure is affected by salt consumption\nFactors Influencing the Development of Hypertension Alcohol consumption\nFactors Influencing the Development of Hypertension Exercise Less active individuals are 30- 50% more likely to develop hypertension.\nFactors Influencing the Development of Hypertension Other Dietary Factors Potassium: Calcium: Magnesium:\nTreatment for Hypertension Maintain a healthy weight, lose weight if overweight. Be more physically active. Drink alcoholic beverages in moderation. Reduce the intake of salt and sodium in the diet to approximately 2400 mg/day.\nThe DASH Diet DAS H The D ietary A pproaches to S top H ypertension clinical trial (DASH) Diet rich in fruits, vegetables, and low fat dairy foods, can substantially lower blood pressure in individuals with hypertension and high normal blood pressure.\nDash Study Control: Ca, Mg, & K ~ 25% of US diet Macronutrients and fiber ~ US average Fruits and Vegetables Fruits and vegetables increased to 8.5 servings K and Mg to 75% Combination: Add 2-3 servings low-fat dairy to fruit & vegetable diet. Ca, K and Mg increased to 75%\nDash Study Outcomes Fruit and Vegetable Diet: Decrease in systolic and diastolic blood pressure in entire study group and in the hypertensive subgroup. Combination Diet: Significant decrease in both systolic and diastolic blood pressure in both groups. Greatest drop was in systolic BP in hypertensive group (11.4 mmHg)\nDash Diet Implications Combination diet affects comparable to pharmacological trails in mild hypertension. Population wide reductions in blood pressure similar to DASH results would reduce CHD by ~ 15% and stroke by ~27% Great potential in susceptible groups: African Americans and elderly.\nThe DASH Diet DASH Diet The DASH Diet includes: 7-8 servings of grains and grain products 4-5 servings of vegetables 4-5 servings of fruits 2-3 servings of low fat dairy products 2 or less servings of meat, poultry and fish 2-3 servings of fats and oils Nuts, seeds and dry beans 4-5 times /week Limited ‘sweets’ low in fat.\nEffects of increasing Calcium-Rich Dairy Food in Black Hypertensives Increases urinary sodium excretion Decreases volume Decreases peripheral vascular resistance Decrease blood pressure Reduces left ventricular mass and risk of left ventricular hypertrophy Effects sustained for one-year period of study\nSodium in Foods Conversion of milligrams to milliequivalents (mEq): mg/atomic weight x valence = mEq. Atomic weight sodium = 23, valence = 1 2400 mg/23 x 1 = 104.3 mEq sodium\nReducing Sodium in the Diet Use fresh poultry, fish and lean meat, rather than canned or processed. Buy fresh, plain frozen or canned with “no salt added” vegetables. Use herbs, spices and salt-free seasoning blends in cooking and at the table; decrease or eliminate use of table salt. Choose ‘convenience’ foods that are lower in sodium.\nReducing Sodium in the Diet When available, buy low- or reduced- sodium or ‘no-salt-added’ versions of foods like: Canned soup, canned vegetables, vegetable juices cheeses, lower in fat condiments like soy sauce crackers and snack foods like nuts processed lean meats\nFood Labels ClaimAmount Low Sodium>140 mg/serving Very Low Sodium>35 mg/serving Sodium Free>5 mg/serving Reduced Sodium25% less than original","UMMC expert offers easy suggestions for lowering daily sodium intakePublished on Friday, August 3, 2018By: Ruth CumminsNote: This story appears in the August 2018 edition of CONSULT, the monthly e-newsletter published by the UMMC Division of Public Affairs that focuses on cutting-edge clinical advances, innovative educational programs and groundbreaking research occurring at UMMC. To receive CONSULT in your email, visit www.umc.edu/CONSULT to sign up.They are among our favorites. And they are among the saltiest.Pizza, pickles and deli meats are among the foods Americans crave, yet their health “minuses” include lots of excess sodium. Ditto for canned soups, salted snacks, fresh and processed chicken, cheese, burgers and sandwiches.On average, Americans are consuming about 3,400 milligrams of sodium daily, according to the Centers for Disease Control and Prevention. That’s 50 percent more than the maximum 2,300 milligrams of table salt per day – about a teaspoon – that experts recommend. Ideally, it should be no more than 1,500 milligrams per day for most adults.Too much salt in their diet leaves many people more vulnerable to high blood pressure, stroke and heart attack. The higher their blood pressure, the greater the strain on their heart, arteries, kidneys and brain. Salt often is a culprit in heart and kidney disease and can lead to those organs’ failure.Bidwell“Salt attracts water, so when we consume too much salt, we hang on to more water than we need,” said Dr. Josie Bidwell, associate professor of nursing and preventive medicine at the University of Mississippi Medical Center. “That increases our blood volume, which makes our blood pressure go up. The more volume you have, the harder your heart has to work to pump it around.”As much as 75 to 80 percent of sodium in a person’s diet comes from packaged and processed foods, said Bidwell, who runs UMMC’s wellness clinic.“We used to think that it was table salt, and of course we want to limit that, but if we can change the amount of processed or packaged foods we start with, then adding a little salt during the cooking process is not the big problem.”Face it: People won’t eat food if it’s not appealing.“If you don’t add seasonings to food, it doesn’t taste good,” Bidwell said. “Our taste buds have become accustomed to wanting things that are super-salty, so we over-salt. But if you need to reduce your salt, first work on cutting down on processed and packaged things.”Cutting down on salt doesn’t mean cutting it out of your diet entirely, Bidwell said. Instead, make better choices.“Snack time at the office is a good opportunity,” she said. “Instead of reaching for something from the vending machine or eating a package of peanut butter crackers, change to a piece of fruit and a little handful of nuts.”Set some reasonable goals when it comes to salt consumption. If you’re eating fast food seven days a week, she said, cut it down to five – and choose more wisely what you order.“If you normally get a burger and fries, that’s a ton of salt and calories,” she said. “Get maybe one of them, and instead of the other, pick a side salad or a fruit cup.”She recommends a few more tips: • When possible, choose fresh foods, not processed foods that almost always are highly salty. • Use the nutrition facts label to check the amount of sodium in a food or drink. Look for foods with 5 percent Daily Value or less of sodium. A DV of 20 percent or more is high. When cooking grains, don’t add salt. • Look for foods labeled “low sodium” or “no salt added.” • Load up on foods with no salt, such as fresh fruits like apples, oranges or bananas; fresh vegetables like spinach, carrots or broccoli; frozen vegetables without added butter or sauce; and canned vegetables with no salt added. • When cooking grains, don’t add salt. • Try one of the sodium-replacement salts available in grocery or discount stores that taste like salt but include potassium instead of sodium as a key ingredient. Experts say these replacements might actually help to lower blood pressure because of the helpful effects of potassium. If food comes with a seasoning packet, use only part of the packet. • Dilute salt by adding liquid to dishes like soups, stews, casseroles and other liquid-based dishes. Add fat to “absorb” the taste of the salt for most dishes. Or add bland foods such as potatoes, grains or bread, or replace salt with something tangy or sour, like lemon juice. For more tips, visit the U.S. Department of Health and Human Services’ healthfinder.gov website."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:8140e59d-ede2-47eb-a8d9-500df172c6fa>","<urn:uuid:8b8c9058-7dd3-4cdc-a6f6-1adedc7121c9>"],"error":null}
{"question":"大家好！我是一个农场主 😊 想请问一下，在农场举办agritainment活动时，保险和责任方面最重要的两个考虑因素是什么呢？Which insurance coverage is more critical - regular farm policy or event-specific coverage?","answer":"There are two critical insurance and liability considerations for agritainment activities: 1) Regular farm insurance policies typically do not cover agritourism activities, so farms need specific event insurance coverage or endorsements, especially for activities involving alcohol. 2) Farms need to assess and prevent risks, as injuries can lead to lawsuits - there have been cases where farms got sued over injuries and some even had to close down. According to experts, having the proper insurance coverage specifically for agritourism is crucial since a regular farm policy won't provide adequate protection for these activities.","context":["SOUTH HERO, Vt. (AP) — As more farms open themselves up to visitors for apple picking, hay rides and some extra income, experts are advising owners to take steps to prevent accidents — be they small or fatal.\nFarming is one of the more dangerous occupations in the U.S. mostly due to the machinery and equipment, according the Centers for Disease Control and Prevention. But a growing interest in local food has led to agritourism becoming a big business, with the number of U.S. farms reporting income from such activities rising 42 percent between 2007 and 2012, according to the latest Census of Agriculture.\nAdding urbanites, who may not be familiar with hazards such as irrigation ponds or farm equipment that could pique a child’s interest, can be a recipe for accidents. So, experts say farmers have to purchase the proper insurance, know where the hazards are and keep tourists away from those areas. Doing so can prevent injuries, lawsuits and notoriety and keep farms in business.\nIt isn’t known how many agritourism-related injuries have occurred in the U.S., said Marsha Salzwedel, an agritourism safety specialist with the National Children’s Center for Rural and Agricultural Health and Safety in Marshfield, Wisconsin. Two people were killed last year: A teenager who, along with other riders, was thrown off a hayride trailer into trees in Maine and a 2-year-old girl who was pinned when a van shuttling passengers from a New Jersey farm’s harvest festival rolled into another van.\nBut, Salzwedel said, “the majority of these incidents if not all of them are pretty much preventable.”\nThe first key is assessing the risks, said Brian Schilling of Rutgers’ Cooperative Extension in New Brunswick, New Jersey. “If you’ve grown up on a farm you’re sort of blind to a lot of these things,” he said, advising owners to have an extension agent, emergency official or insurance agent walk the farm to identify hazards.\nThe extension also has a safety checklist that reminds farmers to, among other things, designate areas that are closed to the public, train employees to property operate farm machinery, secure and restrict areas that contain chemicals, provide hand-washing or hand-sanitizing stations and have employees assist with parking.\nRon Hackett takes several safety precautions around his business, Hackett’s Orchard in South Hero, Vermont, including playground inspections and a Plexiglas guard around its apple-sorting machine. He also transports visitors on a tractor-pulled trailer and has an attendant making sure riders are sitting down and keeping their hands from the side of the trailer.\n“You’ve got to be ever vigilant,” said Hackett, who is only one of three people that is allowed to drive the tractor, which is the newest in his fleet.\nBeing correctly insured is also important, since a regular farm policy does not typically cover an agritourism activity, said Salzwedel, who added she’s seen a number of farms end up getting sued over injuries.\n“In some incidences, they’re able to work through it and they work with the insurance companies if necessary and things go on fine,” she said. “In other incidences, it actually will close down the farm.”\nIn last year’s hayride accident, a corporation that owns the Harvest Hill Farm property in Mechanic Falls, Maine, has filed for bankruptcy, citing personal injury claims as a liability. The district attorney charged the farm with manslaughter, and brought misdemeanor charges against the driver and a mechanic; all pleaded not guilty last month. Farm owner Peter Bolduc denied knowing about any mechanical problems with the 1979 Jeep that was hauling the wagon.\nVisitors can help to prevent injuries, too, by watching their children, making sure that hands are washed after activities involving livestock and following any farm’s posted rules.\nOpening a farm to the public is not for everyone, so extension agents advise considering the implications first.\n“In working with the farmers,” said Salzwedel, “a lot of them have plain out told me that you know I just sleep better at night when I know that I’ve done what I can to make my farm safe.”","Cornucopia Of Compliance, Part 2: Liability, Terms, And Beers-Oh My!\nIt’s the time of year where most farmers are out of the field and at the table planning next year’s crops and business plans. To help guide any planning related to on-farm events we started a series on Agritainment. Last month we covered topics related to a farmers' property. Specifically, we mentioned how zoning, the building code, and the Americans with Disabilities Act can impact a farmer’s ability to host agritainment activities.\nBut, there are still many more considerations that a farm should consider before hosting events such as farm-to-table dinners, festivals, and farm events generally. So, this month, we’re revisiting the topic as the second part in our series. We’ll cover some important labor, food and alcohol considerations.\nAre you paying your workers properly?\nSometimes, farmworkers can be exempt from the Fair Labor Standards Act (FLSA) minimum wage and overtime requirements. The exception is complicated, but we summarized the exception in this blog. Essentially, there are a number of ways a farmer can be exempt from paying minimum wage and overtime. However, agritainment/agritourism activities do not count as “agricultural” work.\nFLSA’s definition of agriculture is very narrow. FLSA defines agricultural work as incidental to or in conjunction with the farming operations. Therefore, farmworker exemptions do not apply to work that is essentially entertaining guests. For example, a farmworker helping drive the tractor for hayrides would not fall under the exemption and be entitled to minimum wage and overtime for that work.\nCan you serve food and alcohol?\nLast month we talked about the difference between a public and private event and how that may affect what regulations affect your event, and the same is true for food and alcohol. If you host an event with mostly friends, it is unlikely that any of the regulations we discuss below will apply. However, it is not always clear when an event becomes public. We lawyers often refer to this as “the sliding scale of risk.”\nIf you are hosting a family potluck, you’re likely safe from meeting state and local food safety requirements and alcohol licensing. But, if you’re having a potluck for CSA members a strict inspector may say that falls into the more public category. However, if you’re having a farm festival that anyone can attend, the regulations DEFINITELY apply. It is up to the individual farm as to how much risk they are ok with, but since we’re lawyers we’re gonna say it’s best to err on the side of following the regulations.\nThere are many regulations around serving food. We covered Pennsylvania food production laws in-depth in this blog. To summarize, food production is generally regulated by the Food Code. The Food Code is typically enforced by the PA Department of Agriculture. However, depending on what county your farm is in, the regulations may be different.\nThere are six counties, Allegheny, Bucks, Chester, Erie, Montgomery, and Philadelphia, where the local health department has the authority to create more strict laws or additional requirements.\nThe Food Code generally requires that a place that prepares or serves food to the public to register with the state. Essentially, to serve food to the public, you will have to register as a retail food establishment. You can read about the processes for registering here. However, there are some exceptions. For instance, if your establishment operates for three days or less per year, there is no need to register. More exceptions and information about retail food establishments can be read about here, in the PA code.\nAside from registering as a retail food establishment, you might also need to register as a commercial food establishment if you serve time and temperature-sensitive food. This type of food requires time/temperature control for safety to limit pathogenic microorganism growth or toxin formation.\nFor example, serving meat, cheese, juices, milk, and other perishable items may require you to register. However, if you are not selling time and temperature-sensitive food, you can register as a limited food establishment. Also, if you plan to operate for less than 14 days in conjunction with an event, you might be able to obtain a temporary event license.\nThere are more requirements based on what you will be selling, i.e. meat, cheese, milk, eggs. This blog, about on-farm sales, goes into detail about these distinctions.\nGenerally, you cannot sell alcohol in PA without a liquor license. There are exceptions to this requirement, but it is unlikely any would apply to an agritainment event. You can read about the exception here.\nHowever, you are permitted to give away alcohol for free to people who are 21 or older if they are not visibility intoxicated. You are not permitted to charge anything but can ask for a donation so long as it is generally unrelated to the direct distribution of alcohol. You can read more about this here. We strongly advise talking to an attorney before involving alcohol to your event.\nHaving vendors sell food and alcohol\nIt might not be a bad idea to let others come onto your property to serve food and alcohol. Vendors are typically already licensed and permitted to sell their products. Thus, you won’t have to wait for the approval and can just hire someone to properly serve food.\nBut, before you allow just anyone onto your property to sell food to your guests, you may want to have the vendor agree to some terms.\nFor example, you should probably get it in writing that the vendor is legally allowed to serve food. You also should make sure the vendor is solely responsible if someone gets sick from the food. Other terms should include considerations like property damage, insurance coverage, and equipment. The more explicit your vendor terms, the less you will have to deal with ambiguities in liability and responsibilities. You can always talk to us or another attorney about how to draft helpful, but friendly vendor terms.\nHow else can you limit liability?\nPerhaps, one of the best ways to limit your liability is by ensuring your insurance covers agritainment activities. Typically, traditional farm liability insurance does not cover on-farm events. So, you may need to look into purchasing additional insurance such as an “event endorsement.”\nAlso, if you will be serving alcohol, you should definitely look into getting an additional policy endorsement or requiring anyone serving alcohol at your event to have one. Furthermore, if you are allowing someone else to host events on your farm, you should require them to purchase event insurance.\nAnother way to limit your liability is by setting up a separate entity to host farm events. By creating another business that will organize, pay for, and run these events, you will shield your farm from any potential claims. For instance, it will be harder for the farm to be connected to the event if an event was hosted by a separate entity that contractually agreed to be solely responsible for the event.\nAlso, if you’re allowing others to host events on the farm you should have them agree to be solely responsible as well. This is called “indemnification.” If, and when, it makes sense for you to have a separate entity is always a question that is very specific to each farm so we recommend talking it over with a lawyer and accountant to see if it makes sense for your activities.\nFinally, you should consider writing waivers or having proper signage with instructions or warnings for people who come onto the farm depending on the type of agritainment.\nYou should also check out this blog which talks about how in Pennsylvania, there is a special agricultural immunity statute for u-pick operations that helps limit a farmer’s liability.\nFarming is hard enough. So, planning these events can be stressful, but it’s important to take the time to figure out all of the nuances with the law, or at the very least understand your risk and get good insurance! Otherwise, you can incur more financial trouble then these events are worth.\nAlso, there may be additional considerations that apply to your farm that are not detailed in these blogs or you may need help navigating how all these regulations apply to you. So, we always advise talking to us or another attorney about your specific situation and the best course of action for your farm.\nDISCLAIMER: This blog post is meant for informational purposes only and does not constitute specific legal advice or create an attorney-client relationship. Readers should discuss their specific situation with an attorney."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:183ed21a-fe72-402e-a3dc-3bde3a15d41e>","<urn:uuid:0e3daeb7-1fec-4a07-98b1-be66b43fa502>"],"error":null}
{"question":"What are the storage recommendations for bottled water, and what alternatives exist to reduce plastic microbead pollution?","answer":"Bottled water should be stored in a clean, cool, dry environment away from direct sunlight. Once opened, the contents should be consumed and any remainder refrigerated. For emergency storage, while manufacturers give bottled water a two-year shelf-life, Health Canada recommends replacing stored water after one year. Regarding alternatives to plastic pollution, many countries have banned plastic microbeads in rinse-off cosmetics to prevent marine pollution. However, care must be taken when selecting alternatives, as their long-term environmental and human health effects are unknown. Life cycle assessments are being used to evaluate potential substitutes to ensure their environmental costs don't outweigh the benefits of the microbead bans.","context":["Bottled water is a topic of interest to Canadians. In this section you will find answers to frequently asked questions about the quality of bottled water sold in Canada.\nBottled water is water which has been packaged in sealed containers for human consumption. The water can come from a variety of sources including springs, aquifers, or municipal supplies and may be treated to make it fit for human consumption.\nIn Canada, bottled water is regulated as a food and therefore it must comply with the Food and Drugs Act. Section 4 of the Act prohibits the sale of foods which contain poisonous or harmful substances and section 5(1) of the Act prohibits the labelling, packaging, treating, processing, selling or advertising of any food in a manner that misleads or deceives consumers as to the character, value, quantity, composition, merit or safety of the product.\nThere are specific regulations for bottled water set out in Division 12 of Part B of the Food and Drug Regulations. The regulations provide definitions for different types of bottled water and specify microbiological standards, acceptable treatments and labelling requirements for these products.\nIn addition to the Food and Drugs Act and Regulations, the Health Products and Food Branch also issue guidelines for bottled water and other products. These guidelines are published in the \"Health Products and Food Branch Standards and Guidelines for the Microbiological Safety of Foods - An Interpretive Summary\" published in the Compendium of Analytical Methods. As a result of microbiological surveys conducted across Canada, the Health Products and Food Branch has set guidelines for two additional bacteria (other than those in the Regulations) : Pseudomonas aeruginosa and Aeromonas hydrophila. These bacteria are indicators of poor \"Good Manufacturing Practices (GMP)\". The Canadian Food Inspection Agency and other health officials could test for these bacteria when the manufacturer is out of compliance and/or has been involved in food borne outbreaks.\nStandards for microbiological quality are based upon data collected through Canada wide surveys on these products, reviews of international data, and consultation with bottled water associations, scientific experts and other clients involved in this industry. Based upon these consultations, Health Canada is bringing the microbiological standards in line and harmonizing with international standards. The Food and Drugs Act and Regulations are available on the Department of Justice's Web site.\nThe federal responsibility for the regulation of bottled water sold in Canada is shared by Health Canada and the Canadian Food Inspection Agency. Health Canada establishes health and safety standards for bottled water and develops labelling policies related to health and nutrition. The Canadian Food Inspection Agency develops standards related to the packaging, labelling and advertising of these products and handles all inspection and enforcement duties.\nFederal laws set stringent national standards for bottled water. In addition to these laws, provinces and territories are free to establish additional requirements for their own jurisdictions.\nAs part of its enforcement role, the Canadian Food Inspection Agency can inspect bottled water products, labels and establishments (conveyances, equipment etc.) involved in the sale, manufacture and distribution of bottled water. In addition, some provincial and municipal ministries and agencies may inspect bottled water.\nYes. While bottled water is regulated federally as a food (see Question 2), the tap water distributed by municipalities is regulated by the appropriate province or territory. However, Health Canada is involved in the development of the Guidelines for Canadian Drinking Water Quality. These Guidelines are developed through the Federal-Provincial-Territorial Committee on Drinking Water which includes members from the provinces, territories and Health Canada's Healthy Environment and Consumer Safety Branch. They contain guidelines for microbiological, chemical, physical and radiological contaminants. For each contaminant, the Guidelines establish the maximum acceptable concentration of the substance that can be permitted in water used for drinking. They are used by the provinces and territories as the basis for their own drinking water standards.\nMore information on the Guidelines for Canadian Drinking Water Quality.\nYes. All bottled water sold in Canada must comply with the Food and Drugs Act and its Regulations.\nThe Food and Drugs Act and Regulations do not require bottled water operators to have a license to start bottling water commercially. However, the provinces and territories may have regulations, guidelines and policies that apply to the bottling of water. It is recommended that provincial and municipal authorities be contacted regarding standards and licensing requirements for bottling water.\nCurrently there are two categories of bottled water defined in the Food and Drug Regulations. The first, spring or mineral water, is bottled water that originates from an underground source which is not part of a community water supply and is fit for human consumption at its point of origin. Mineral water generally contains a larger amount of dissolved mineral salts than spring water. Spring or mineral water may not be subjected to any treatment that would modify the original composition of the water. It may be treated by the addition of carbon dioxide for carbonation, ozone for disinfection during the bottling process and fluoride for the prevention of dental carries.\nBottled water that is not labelled as spring or mineral water may be from any source and can be treated to make it fit for human consumption or to modify its original composition. The label of these bottled waters must show how they have been treated. When a water has been treated by distillation it must be labelled as \"distilled water\". If carbon dioxide has been added for effervescence, the label must identify the product as \"carbonated water\". When a water has been treated by a method other than distillation to reduce its dissolved mineral content to less than 10 parts per million, the product must be identified as \"demineralized water\".\nOzone may be added to spring or mineral water during the bottling process as a disinfectant to inhibit the growth of harmful microorganisms. Ozone is also effective in removing objectionable odours and flavours because it breaks down into oxygen which improves taste and other qualities.\nYes, except for mineral water or spring water. It is possible for a bottled water to be produced from municipal tap water that has undergone a treatment process to lower the mineral content and/or remove chemicals such as chlorine.\nAll bottled water must carry the following basic labelling information:\nThe following additional information must be on the label spring and mineral water:\nThe label of bottled water, other than spring or mineral water, must also include a description of any treatment the water has undergone.\nFor more information on the labelling of bottled water and other foods, please consult the Canadian Food Inspection Agency's Guide to the Labelling and Advertising of Food\nIn the case of a spring or mineral water, the label is required to provide information regarding the geographic location of the underground source of the water. There is currently no regulatory requirement that other types of bottled water declare the source of the water used for their manufacture. However, consumers can contact the bottled water manufacturer for detailed information on the source.\nBottled water has an excellent safety record in Canada. At the present time, no waterborne disease outbreaks have been associated with drinking bottled water in Canada. Health Canada is confident that the current bottled water regulations, as well as the general provisions of the Food and Drugs Act, are adequate to ensure the safety of bottled water products in Canada. Nevertheless, Health Canada is presently reviewing these regulations to update current requirements to incorporate new scientific knowledge, to harmonize them with the standards of other governments and international agencies and to bring the Regulations in line with the Guidelines for Canadian Drinking Water Quality.\nYes, there can be. Bacteria are found in most bottled waters sold for drinking purposes. Bottled water is usually disinfected to remove harmful microorganisms but this treatment is not intended to sterilize the water. Sterile water is normally reserved for pharmaceutical purposes, such as in contact lense solutions.\nA bottled water could potentially cause illness if the water used for its production was untreated or inadequately treated to remove any disease-causing organisms it contained. This risk would also exist if the same water was used in a community water system for delivery from the tap. A bottled water manufactured from an inadequately treated source would be in contravention of the Food and Drugs Act.\nQuality standards for bottled water and tap water are similar. Both bottled water and municipally distributed tap water that meet or exceed their required health and safety standards, are considered to be safe.\nThere is more to bottled water than an attractive label. When choosing a bottled water, examine the bottle and label for date of manufacture or manufacturing code, chemical analysis, treatments applied, company contact information and the location or type of water source.\nWhen travelling and uncertain of the source or quality of a bottled water, avoid bottled water that has not been disinfected or carbonated.\nExamine the bottles to be sure that the seals are not broken and the water is clear and free of debris. Report suspicion of any tampering or extraneous material to the store manager, the Canadian Food Inspection Agency and/or regional health officials.\nWhen buying 18 L carboys for use with a cooler, purchase those with no spill caps which ensure that water is not spilled and air does not enter the bottle when its placed on the cooler.\nHealth Canada recommends that people whose immune systems have been weakened by disease, surgery, or therapy, consume bottled water that has been disinfected in some manner to eliminate harmful bacteria (e.g., ozonation). These individuals should contact their physician for advice on the types of water to consume and how they should treat their drinking water.\nAll bottled water meeting or exceeding the federal standards are comparable from a health and safety perspective for the general population. A consumer's choice of bottled water is likely to depend on their needs and taste preferences.\nFor more information on the types of water for infants, please visit our website\nBottled water should be stored in a clean, cool and dry environment and out of direct sunlight.\nOnce water bottles have been opened, Health Canada recommends that you consume the content and refrigerate any leftover. The 18 L carboys of bottled water should be dispensed through a refrigerated water cooler that is kept clean to avoid contamination. See question 27 for guidance on how to clean and maintain your water cooler.\nYes. Large quantities of bottled water can be stored in a basement or other cold storage area in case of problems with municipal supplies or an emergency situation. The area should be a clean environment away from cleaning or chemical products and out of direct sunlight. Although manufacturers give bottled water a two year shelf-life, Health Canada suggest that you replace water stored for emergency use after one year.\nWhen preparing water for emergency use, use clean bottles or containers that are made/intended for food use. The container and cap should be cleaned with hot soapy water and rinsed well. The container can then be filled with clean potable water. Visit Is Your Family Prepared? for advice on how to treat and store water safely.\nIn the case of all single use bottled water (except 18L carboys):\nIn the case of 18L bottles used with a dispenser,\nYes. The plastic containers used for bottled water are regulated as food packaging materials. Manufacturers are responsible for ensuring that their bottles comply with requirements set out in Division 23 of the Food and Drug Regulations.\nMost plastic bottles used in the sale of bottled water in Canada are made of polyethylene terephthalate (PET or PETE) or polyethylene (PE), which does not contain Bisphenol A. Large jugs (18 L bottles) and some sport bottles can be made of polycarbonate plastic (PC) which may contain small amounts of Bisphenol A. As a result of the use of polycarbonate water bottles, minute quantities of Bisphenol A can potentially leach out into the water or food and consumers may be exposed to small amounts of Bisphenol A through their normal daily diet. The Food Directorate of Health Canada has conducted a review of all the data available on the migrational and toxicological characteristics of Bisphenol A as well as other pertinent information (e.g. use patterns) and concluded that the dietary exposure to Bisphenol A from food packaging sources, including PC water bottles, does not pose a health risk to consumers.\nHealth Canada does not recommend the reuse of single-use bottles because the reuse poses a potential microbiological risk if not cleaned properly. Studies on reusing single-use bottles have found that depending on the source of the water used and the general hygiene of the user, the growth of bacteria in the bottle can vary from negligible to potentially hazardous. Health Canada suggests that people use wide-necked bottles that can be thoroughly washed with hot soapy water between uses.\nFrequently, the concerns regarding the re-use of single-use plastic bottles for drinking water have focussed on the safety of the plastic under these conditions. There have been claims that polyethylene terephthalate (PET or PETE) plastic used in single use water bottles breaks down when used repeatedly releasing cancer-causing chemicals. However, Health Canada has seen no scientific evidence to suggest that reusing PET bottles will contribute harmful levels of chemicals and toxins to the water. Health Canada has also concluded that the levels of Bisphenol A detected in water bottled in polycarbonate do not pose a health concern.\nHealth Canada is not aware of any substantiated evidence to suggest that freezing bottled water causes the release of toxic chemicals from the plastic. Any possible migration of chemicals from the plastic water bottles is actually expected to be lower under freezing temperatures.\nCleaning your water cooler:\nNote: Clean your bottled water cooler before every bottle change.\nUnplug cord from electrical outlet of cooler.\nRemove empty bottle (carboy).\nDrain water from stainless steel reservoir(s) through faucet(s).\nPrepare a disinfecting solution by adding one tablespoon (15 mL) household bleach to one Imperial gallon (4.5 L) of water solution.\nSome companies suggest using one part vinegar to three parts water solution to clean the reservoir of scale before cleaning with bleach. Check your manual.\nNote: Other disinfecting solutions may be suitable. Please check with your water cooler supplier/manual.\nPour the bleach/other disinfection solution into the resevoir.\nWash reservoir thoroughly with bleach solution and let stand for not less than two minutes (to be effective) and not more than five minutes (to prevent corrosion).\nDrain bleach/disinfection solution from reservoir through faucet(s).\nRinse reservoir thoroughly with clean tap water, draining water through faucets, to remove traces of the bleach/disinfection solution.\nDrip Tray (located under faucets):\nLift off drip tray.\nRemove the screen and wash both tray and screen in mild detergent.\nRinse well in clean tap water and replace on cooler.\nWash hands with soap and warm water before handling. If you choose to use clean protective gloves (ex. latex), discard or disinfect after each use and prior to reuse.\nNote : Protective gloves should never replace proper hand washing and hygiene.\nWipe the top and neck of the new bottle with a paper towel dipped in household bleach solution (1 tablespoon (15 ml) of bleach, 1 gallon (4.5 L) of water). Rubbing alcohol may also be used, but must be completely evaporated before placing the bottle in the cooler\nRemove cap from new bottle without touching the surface of the opening to avoid any contamination. Place new bottle on cooler.\nAdapted from instructions provided by Ken Orom, Calgary Board of Education, and, Calgary Health Services.\nMicrobiological Quality of Bottled Water\nMs. Hélène Couture\nBureau of Microbial Hazards\nHealth Products and Food Branch\nSir Frederick Banting Research Center\nOttawa, Ontario K1A 0L2\nTel: (613) 957-1742\nFax: (613) 952-6400\nChemical Quality of Bottled Water\nBureau of Chemical Safety\nHealth Products and Food Branch\n1st Floor East\nSir Frederick Banting Research Center\nOttawa, Ontario K1A 0L2\nTel: (613) 957-1700\nFax: (613) 990-1543\nTap Water (Drinking Water), Drinking Water Treatment Devices\nWater Quality Program\nHealthy Environments and Consumer Safety Branch\n123 Slater Street, 5th Floor\nOttawa, Ontario K1A 0K9\nTel: (613) 952-6750\nFax: (613) 952-2574\nLabelling, Compliance Activity and Inspection\nCanadian Food Inspection Agency (www.inspection.gc.ca)\nBottled Water Industry\nMs. Elizabeth Griswold\nCanadian Bottled Water Association\n70 East Beaver Creek Road\nRichmond Hill, Ontario L4B 3B2\nTel: (905) 886-6928\nFax: (905) 886-9531\nMr. Adam Koven\nCanadian Association of Ice Industries\n5 Cataraqui Street\nKingston, Ontario K7L 5C6\nTel: (613) 546-2128\nFax: (613) 546-0333\nMunicipal Drinking Water\nMr. T. Duncan Ellison\nCanadian Water and Wastewater Association\nUnit 11, 1010 Polytek Street\nOttawa, Ontario K1J 9H9\nTel: (613) 747-0524\nFax: (613) 747-0523","In our haste to ban or regulate unsustainable and environmentally damaging materials and chemicals, we may overlook dangers posed by their substitutes. In light of the scientific evidence regarding the fate, persistence and toxicity of microplastics in the marine environment, many countries have banned the sale of rinse-off cosmetics containing plastic microbeads to prevent their release to the environment. However, the wider lifetime environmental impacts of the potential substitutes have not been considered, and care must be taken so that the environmental costs of using them do not potentially outweigh the benefits resulting from the bans. In this study, we use life cycle assessment to compare the environmental performance of a wide range of potential alternatives. The study investigates the quantities of these materials required and the human health and environmental impacts of their manufacture, transport and inclusion in cosmetics. We highlight that the long-term environmental and human health effects of their disposal are unknown and are thus excluded from the life cycle assessment. In support of the responsible replacement of plastic microbeads in cosmetics, we identify several alternatives that will perform better, as well as substitutes that could pose additional risks and have undesirable effects.\nThis is a preview of subscription content\nSubscribe to Nature+\nGet immediate online access to the entire Nature family of 50+ journals\nSubscribe to Journal\nGet full journal access for 1 year\nonly $8.25 per issue\nAll prices are NET prices.\nVAT will be added later in the checkout.\nTax calculation will be finalised during checkout.\nGet time limited or full article access on ReadCube.\nAll prices are NET prices.\nThe data that support the findings of this study are available from the corresponding author on request.\nZitko, V. & Hanlon, M. Another source of pollution by plastics: skin cleaners with plastic scrubbers. Mar. Pollut. Bull. 22, 41–42 (1991).\nGregory, M. R. Plastic ‘scrubbers’ in hand cleansers: a further (and minor) source for marine pollution identified. Mar. Pollut. Bull. 32, 867–871 (1996).\nFendall, L. S. & Sewell, M. A. Contributing to marine pollution by washing your face: microplastics in facial cleansers. Mar. Pollut. Bull. 58, 1225–1228 (2009).\nCole, M., Lindeque, P., Halsband, C. & Galloway, T. S. Microplastics as contaminants in the marine environment: a review. Mar. Pollut. Bull. 62, 2588–2597 (2011).\nBeach, W. J. Skin cleaner. US patent US3645904A (1972).\nLeslie, H. A. Review of Microplastics in Cosmetics (Institute for Environmental Studies, 2014).\nDefra Implementation of the Environmental Protection (Microbeads) (England) Regulations 2017 Impact Assessment Report No. Defra2083 (HMSO, 2017).\nCole, M. et al. Microplastic ingestion by zooplankton. Environ. Sci. Technol. 47, 6646–6655 (2013).\nManagement of Marine Debris UNEP/CMS/Resolution 11.30 (United Nations Environment Programme, 2016).\nThe Environmental Protection (Microbeads) (England) Regulations 2017 UK SI no. 1312 2017 (HMSO, 2017).\nAnnex XV Restriction Report — Microplastics ECHA/PR/19/03 (European Chemicals Agency, 2019).\nWaste Minimisation (Microbeads) Regulations 2017 Vol. 2017/291 (New Zealand Government, 2017).\nProduct Catalog Vol. LK_Katalog_AEF_V6_2017 (Lipoid Kosmetik AG, 2017).\nKEMI Förslag till nationellt förbud mot mikrokorn av plast i kosmetiska produkter (Proposal for a National Ban on Plastic Microbeads in Cosmetic Products) Report No. 2/16 (Kemikalieinspektionen, 2016).\nBryce, E. The race to reinvent microbeads and avoid a plastic disaster. Wired (1 December 2018).\nOSPAR Welcomes Cosmetics Industry Microplastics Pledge (OSPAR, 2015); https://www.ospar.org/news/ospar-marine-litter\nPlastic in Cosmetics—Are We Polluting the Environment Through Our Personal Care? (United Nations Environment Programme, 2015).\nHoward, G. From BPA to BPZ: A Toxic Soup? (CHEMTrust, 2018).\nDiquat Dibromide (International Union of Pure and Applied Chemistry, 2009); http://sitem.herts.ac.uk/aeru/iupac/Reports/256.htm\nScheebaum, M. V. Biomonitoring of Herbicide Impact Using Aquatic Macrophytes: Laboratory and Field Studies for the Evaluation of Potential Toxicity on Aquatic Plants. PhD thesis, Albert-Ludwigs-Univ. (2006).\nWoods, J. S., Veltman, K., Huijbregts, M. A. J., Verones, F. & Hertwich, E. G. Towards a meaningful assessment of marine ecological impacts in life cycle assessment (LCA). Environ. Int. 89–90, 48–61 (2016).\nSonnemann, G. V. et al. Medellin Declaration on Marine Litter in Life Cycle Assessment and Management (FSLCI, 2017); https://fslci.org/medellindeclaration\nDowns, C. A. et al. Toxicopathological effects of the sunscreen UV filter, oxybenzone (benzophenone-3), on coral planulae and cultured primary cells and its environmental contamination in Hawaii and the U.S. Virgin Islands. Arch. Environ. Contam. Toxicol. 70, 265–288 (2016).\nMoulite, M. Hawaii bans sunscreens that harm coral reefs. CNN (3 July 2018); https://edition.cnn.com/2018/07/03/health/hawaii-sunscreen-ban/index.html\nHou, J. et al. Toxicity and mechanisms of action of titanium dioxide nanoparticles in living organisms. J. Environ. Sci. 75, 40–53 (2019).\nHeller, A., Jarvis, K. & Coffman, S. S. Association of type 2 diabetes with submicron titanium dioxide crystals in the pancreas. Chem. Res. Toxicol. 31, 506–509 (2018).\nKershaw, P. Exploring the Potential for Adopting Alternative Materials to Reduce Marine Plastic Litter (UNEP, 2018).\nRegional Action Plan for Prevention and Management of Marine Litter in the North-East Atlantic (OSPAR, 2014).\nTransforming Our World: The 2030 Agenda for Sustainable Development Report No. A/RES/70/1 (UN General Assembly, 2015).\nEnvironmental Management — Life Cycle Assessment — Principles and FrameworkISO 14040:2006 (ISO, 2006).\nGouin, T. J. A. et al. Use of micro-plastic beads in cosmetic products in Europe and their estimated emissions to the North Sea environment. Int. J. Appl. Sci. (SOFW J.) 141, 40–46 (2015).\nHerrmann, I. T. & Moltesen, A. Does it matter which life cycle assessment (LCA) tool you choose? A comparative assessment of SimaPro and GaBi. J. Clean. Prod. 86, 163–169 (2015).\nPonsioen, T. What’s New in SimaPro 8? (PRé Sustainability B.V., 2013); https://pre-sustainability.com/articles/publication-whats-new-in-simapro-8\nGolsteijn, L. ReCiPe (PRé Sustainability B.V., 2012); https://www.pre-sustainability.com/recipe\nRumble, J. R. CRC Handbook of Chemistry and Physics 99th edn (CRC Press/Taylor & Francis, 2018).\nAlibaba Sourcing Solutions Products by Category (Alibaba, accessed 17 September 2019); https://www.alibaba.com/Products?spm=a2700.8293689.201703.9.28f465aakiCEJ7\nMurphy, F., Ewins, C., Carbonnier, F. & Quinn, B. Wastewater treatment works (WwTW) as a source of microplastics in the aquatic environment. Environ. Sci. Technol. 50, 5800–5808 (2016).\nSources, Fate and Effects of Plastics in the Marine Environment Report No. 90 (GESAMP, 2015).\nSherrington, C., Darrah, C., Hann, S., Cole, G. & Corbin, M. Study to Support the Development of Measures to Combat a Range of Marine Litter Sources (Eunomia, 2016).\nDris, R., Gasperi, J., Rocher, V., Saad, M. & Tassin, B. Microplastic contamination in an urban area: a case study in Greater Paris. Environ. Chem. https://doi.org/10.1071/EN14167 (2015).\nMato, Y. et al. Plastic resin pellets as a transport medium for toxic chemicals in the marine environment. Environ. Sci. Technol. 35, 318–324 (2001).\nEAC Environmental Impact of Microplastics: Fourth Report of Session 2016–17 Report No. HC 179 (House of Commons, 2016).\nHulme, P., Davis, K. & Hickey, P. CSOs – the past, the present and the future. In CIWEM Urban Drainage Group Annual Conference 2014 (CIWEM, 2014); http://ftp2.ciwem.org/Document%20Tracker.xlsx\nSussarellu, R. et al. Oyster reproduction is affected by exposure to polystyrene microplastics. Proc. Natl Acad. Sci. USA 113, 2430–2435 (2016).\nA Threat to Our Rivers: Plumbing Misconnections (Thames 21); https://www.thames21.org.uk/plumbing-misconnections\nCBD Impacts of Marine Debris on Biodiversity: Current Status and Potential Solutions Technical Series No. 67 (Secretariat of the Convention on Biological Diversity and the Scientific and Technical Advisory Panel, GEF, 2012).\nAndrady, A. L. Microplastics in the marine environment. Mar. Pollut. Bull. 62, 1596–1605 (2011).\nTanaka, K. et al. Accumulation of plastic-derived chemicals in tissues of seabirds ingesting marine plastics. Mar. Pollut. Bull. 69, 219–222 (2013).\nGregory, M. R. & Andrady, A. L. in Plastics and the Environment (ed. Andrady, A. L.) Ch. 10, 379–401 (John Wiley and Sons, 2003).\nSeltenrich, N. New link in the food chain? Marine plastic pollution and seafood safety. Environ. Health Perspect. 123, A34–A41 (2015).\nWe thank J. Hunt, Marine Litter Policy Lead in the Marine Environment Strategy Team, Department for Environment, Food and Rural Affairs (Defra), UK, for identifying the policy need that the work presented here aimed to address.\nThe authors declare no competing interests.\nPeer review information Nature Sustainability thanks Olwenn Martin and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nAbout this article\nCite this article\nHunt, C.F., Lin, W.H. & Voulvoulis, N. Evaluating alternatives to plastic microbeads in cosmetics. Nat Sustain 4, 366–372 (2021). https://doi.org/10.1038/s41893-020-00651-w"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:6c2b91ab-e967-482f-b36d-189808e8b209>","<urn:uuid:d56c91d2-29bf-45da-860d-9808b6c14312>"],"error":null}
{"question":"What similarities and differences can be observed between the management of exotic species in Canada and Vancouver's approach to supporting native bird populations?","answer":"Canada's management of exotic species and Vancouver's bird population initiatives show contrasting approaches to biodiversity. At the national level, Canada deals with numerous exotic species introductions, both accidental (through ships and canals) and intentional (for food and decoration), with an estimated 800 of 5,000 plant species being exotic. In contrast, Vancouver takes a more protective approach towards native species, implementing a bird strategy aimed at enhancing native bird populations. This involves careful planning of green spaces, maintaining understory vegetation, and recognizing the importance of regional connectivity since birds move freely between the city and surrounding areas like the Fraser River delta, ocean, and forests.","context":["There are currently between 1.5 to 1.8 million named species in the world, about half of which are insects. The largest group of insects is the beetles (the order Coleoptera), with 300 000 species. In contrast, there are only 4 500 species of mammals.\nThese numbers are just for the known and named species. No one knows how many species are still to be discovered. Estimates for the total number of species on the planet range from three million to 100 million, though most generally accepted estimates are between five and 20 million. This range of estimates emphasizes how little we presently know about biodiversity. Some groups are better known and more completely identified than others; new mammal species are rarely found, but an seemingly endless stream of new insect species are found wherever one looks. It would be almost impossible to identify all the species of bacteria (most of which would be new) in a kilogram of earth from an average temperate forest.\nDespite this large number of species currently inhabiting the planet, almost all species that have ever existed are now extinct. No one knows how many species have existed in the 3.8 billion years that life has been on this planet, but the current number is a small proportion of that.\nAn estimated 140 000 species live in Canada, only half of which have been identified. A list of some of the major groups is given in Mosquin et al. 1995, and some of the groups are given here. Most of the larger organisms (mammals, birds, trees) have been almost completely identified, and it's the smaller creatures that account for most of the unidentified species; over one fifth of all species in Canada are insects. Estimates of how many species of the more obscure groups, such as the nematodes, are little more than guesses.\nExotic species are constantly being introduced to new locations around the world, and Canada is certainly no exception. In many cases, the introduced species die out, leaving no permanent impact. Other times, though, they can grow out of control and directly interfere with native ecosystems, perhaps even dominating them and driving native species to extinction. A few of the better-known examples of successful exotic species in Canada include Dutch elm disease, purple loosestrife, zebra mussels, sea lampreys and starlings. The number of exotic plant species is staggering; it is estimated that 800 of the 5 000 plant species in Canada are exotic introductions and successful enough to maintain populations in the wild!\nIntroductions of exotic species can be accidental or intentional. Accidental introductions of aquatic species often occur when species arrive with ships, either attached to them or carried in the bilgewater, or new canals are built, allowing species to make their way around barriers that previously stopped their spread. These are both aquatic types of accidental introductions; accidental land introductions are rarer in Canada, usually arriving in imported goods.\nIntentional introductions are usually for food or decorative purposes. Plants and sport fish are most commonly introduced in Canada. Most tree species found on city streets and a surprising number of agricultural species that seem native, such as apples and potatoes, are actually introduced. Game fish have been introduced over a large part of the country, and are still introduced annually into many lakes to maintain levels for sport fishing.\nThe Great Lakes have been the site of many of Canada's introductions of exotic species. The cargo of ships along with the ships themselves often carry new species, and the docks and canals created to allow the ships access to the Great Lakes have also allowed new species to bypass the falls, rapids and stretches of land that once prevented their spread.\nFlowers, butterflies, birds, and other pleasing organisms are typically used to symbolize biodiversity. Large mammals such as the lynx and panda are especially popular, and the term \"charismatic megafauna\" has been used to describe them. Other, less attractive species, such as slugs, fungi or worms are rarely mentioned despite their importance in the functioning of ecological systems. The vast majority of species belong to these less charming groups; there are more Canadian species of earthworms (Oligochaeta) than mammals, and there are more Canadian species of clams and mussels than birds.\nThe more popular, large, or economically-important a group is, the more is known about it, including its distribution. Canada is such a large country, and so sparsely settled for the most part, that intensive surveys are virtually impossible to perform for the entire country (see the Patterns of Biodiversity section for more information on biological surveys).\nThe ideal distribution maps would be a regular grid with the abundance of every species in every grid space clearly marked. Unfortunately, none of the maps that are in this section are of this type, and there are extremely few of these sorts of distribution maps anywhere in the world. When they do exist, they are usually for small areas; the resources needed for proper large-scale surveys are large.\nWhen the distribution maps in this section have large solid areas (known as area maps), the species isn't found everywhere in the indicated area. The area instead represents where the species is likely to be found when the conditions are right, which is most of the indicated range. The actual shape of the areas is still determined as much as possible by actual records. More well-known species tend to have these sorts of maps.\nDot maps, formed by scattered dots, are usually sparse due to insufficient records. Each dot represents a location where the species was found, but areas that don't have dots have often simply not been searched yet. These maps show where a species definitely is, but where else it may be or where it is not found is uncertain. Although the species are found elsewhere, their pattern of distribution isn't known well enough to guess where, and so only actual records are shown. Less popular or hard to identify species have these sorts of maps unless an intensive effort has been made to obtain a more complete distribution.","Birders can carry out some pretty outlandish quests when they put their minds to it. In 2015, Noah Stryker set off on a Big Year to see how many birds he could see before year’s end. Forty-one countries later he had seen 6,042 of the slightly more than 10,000 species in the world. Years earlier and as a graduate student, my thesis advisor the late Jamie Smith told me how the list of birds heard while on the loo in his field camp on tiny Mandarte Island in British Columbia began to level off. The solution? Cut a window in the side of the outhouse thereby boosting the bird count.\nThere might seem no justifiable reason to spend my time trying to see 150 species of birds in Vancouver by Canada’s 150th birthday on July 1. And I would have agreed at the outset when on a frozen, ice filled bleak January 1, 2017 I started my quest. Except I had a secret mission.\nA few years ago, Vancouver City Council approved a bird strategy developed by the Vancouver Bird Advisory Committee that I chair. Among the aims of the strategy was the enhancement of the city’s native birds. The 150 challenge was an opportunity to see how many birds could be seen within the city limits – with some effort, I might add – and where we could make improvements. Not too surprising was that some species were present in very low numbers. I had to search a long time for a pied-billed grebe – finally located in Beaver Lake and later on Trout Lake. The two pairs might be the only nesting grebes in the city where lakes are few. Similarly, despite large numbers of species of shorebirds on the neighbouring Fraser River delta, the mudflats they prefer are just thin slivers along the Fraser River within the city limits. The city has many trees and there are large tracts of wild forest in Stanley Park and a few other parks. The need for understory is an important feature for birds. Few birds use isolated trees but add several trees, some understory shrubs and a pond, and the bird numbers soar.\nCareful planting and siting of green space provides more opportunities for us to share some time with our birds. However, the city can only do so much. Vancouver is fortunate to be located near the Fraser River delta, the ocean, and forested. Birds freely move in and out of the city and we need to not lose sight of the fact that a bird strategy needs to become region wide to ensure continuity.\nMy tally by mid-April was 108 species. While writing this blog, I got a comment that someone in the Okanagan was at 139 species. The 150 challenge is within their reach. For me, another 50 species is possible, but 35-40 is most likely. I am restricting my count to within the city limits of Vancouver. Spring migration is just gaining steam when the numbers of species should grow. I am daily watching the Rare Bird Alert and eBird for unusual sightings. So far, the tufted duck alluded me (duck 4:me 0). So did the snow bunting and Say’s phoebe but I saw a California scrub jay on the third attempt.\nFrom May 6 to 13 we will be running Bird Week with over 40 events for all levels of birding skills. The bigger event is coming in 2018 when Vancouver hosts the International Ornithological Congress and we launch the first Vancouver International Bird Festival. You can visit the web sites and I will be writing more about these events in subsequent posts, but mark your calendars and plan to visit Vancouver. Hopefully, I will have seen my 150 species by then."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:c116f996-8350-4296-8d2b-85e8b6d31bc3>","<urn:uuid:4d3f6617-1a58-4551-ae3d-5ebe0182b0a4>"],"error":null}
{"question":"Which affects model airplane control more - servo quality or elevator trim?","answer":"Servo quality has a more direct impact on control precision - better servos provide stronger, smoother control with good centering qualities, while lower quality servos result in imprecise, inconsistent performance. The elevator trim, while important, mainly provides downforce for pitch stability during level flight and its effectiveness varies with airspeed, making it a secondary factor compared to the fundamental control precision provided by quality servos.","context":["Do not confuse this with the balance point of your model!\nDownthrust is when the motor on the nose of a model airplane is mounted angled slightly down. The idea is for the propeller to pull the model forward and down a little bit. This is done to balance out the tendency of many model airplanes to climb when more power is applied.\nDucted fans may run into the same issue, but there may not be much you can do about it. Pusher propellers work in reverse and may need upthrust instead.\nTesting the Downthrust\nIntuitively, it is very easy to understand the need for downthrust on a motor mount. Just try it!\nFly your model airplane straight and level. You want the lowest throttle setting that will fly the model at a comfortable cruise speed. Wait a few seconds for the model to stabilize. Then push the throttle stick all the way forward. Don’t yank it there, but move it smoothly over a span of a couple of seconds. Watch carefully what happens to the model.\nDid it dive to the ground? Imagine you are coming in for a landing and you have to do an emergency go around. You give it full power and it dives for the runway. That is really bad. You have too much downthrust. Do not fly the model again until the problem is fixed.\nDid the model start climbing when you applied more power? For most model airplanes, this is not the ideal behavior. In a sport model or trainer a modest climb is not too bad. That is how I have most of my own model airplanes set up. But a steep climb so you are at risk of stalling the model is definitely a no no.\nFinally, did the model stay level, and just smoothly accelerated? Wow. A model that does that is a rare sight. You have my permission to jump up and give your spotter a high five. Yes, land the model first.\nCG and Downthrust\nAirplanes are only stable in pitch if there is a downforce on their tails when flying level. When the elevator stick is in neutral, the elevator will be raised a little bit. There is a very important consequence from this. Model airplanes always pitch up when they fly faster.\nThis is easy to understand. When the airplane speeds up, there is more air flowing past the elevator. It now works better, so it presses down harder. Voila! The nose goes up.\nFind the right balance point for your model before you try and adjust the downthrust. Use whatever test you like, such as the dive test. The balance point is a lot more critical. You also definitely do not want to be adjusting both of these at the same time.\nThis is also the reason why you should not use a programmable mix from the throttle to the elevator. Do yourself a favor and keep these two forces as separate as possible.\nGiven the complex relationship between the balance point and downthrust, you may be wondering if it is possible to perfectly adjust the amount of downthrust. In other words, is it possible to perfectly trim any model so that moving the throttle stick never causes a pitch change?\nI have to say no, for a simple reason. The elevator responds to changes in the airspeed of the airplane. The downthrust is reacting to changes in the propeller’s RPM. If you throttle up or down too quickly, the thrust from the propeller will be out of balance with the airplane’s pitch trim. This is because the RPM can change more quickly than the airplane can change speed.\nAlso, while doing aerobatic maneuvers the airspeed of the model is going to vary. Again, the two forces will be out of balance with each other.\nNote that I am assuming that the propeller blast is not hitting the elevator directly. This can happen even if the motor and the horizontal stabilizer are lined up vertically. You see, the downthrust can cause the prop blast to reach the tail too high to be of help.\nIf the prop blast hits the horizontal stabilizer directly, you may have an easier time trimming the model.\nHaving said all of that, the goal should not be find the perfect trim for your model, but to find the best trim that works for you.\nWhich brings me back to something I have said before. The best way for you to trim your model is by you flying it. Wait for typical weather conditions and do the maneuvers you typically do.","Once you master basic RC flight and start progressing to bigger, faster and more maneuverable airplanes, aerobatics start to enter your aviation mindset. Sure, anyone can pull back on the stick to perform a loop, but what’s really needed to become a skilled pilot is understanding how to execute maneuvers properly and how to fly them smoothly. Here are some flight and setup tips to make your learning curve less steep and more enjoyable. No one is born an expert, but it doesn’t take long to start flying like one!\nWHERE TO START?\n1 When it comes to aerobatics, you have a choice of style. There’s sport (for the fun of it) aerobatics, precision and pattern aerobatics, competition fun-fly aerobatics and 3D unlimited and freestyle. Even the most accomplished champion had to start somewhere and for all of us, it’s sport aerobatics that teaches us the basics. The best way to learn the ropes is to go out and attend some contests. Unlimited, 3D throwdowns and “Huck Fests” are amazing and you’ll get a good feeling for how the advanced guys set up their equipment. For really amazing, close-to-home events, nothing beats competition fun-fly events! This is down-on-the-deck aerobatics and very exciting. Many events now are enjoying an increase in the use of electric-powered airplanes, so you’ll feel right at home.\nPICK A PLANE\n2 For a plane to be a good choice for aerobatics, it needs to have a good power-to-weight ratio and light enough to have a pilot-friendly wing-loading for slow landing and stall speeds. There are hundreds of good planes to choose from and the best place to start looking is at your local flying field. Chat it up with the more experienced pilots and see what’s popular. For electrics, the field is wide open for size and power systems and most if not all today are plug and play. Just stick with what works and experiment a little and fine-tune the plane to suit your needs and style of flying.\n3 With a good transmitter and receiver, better-than-average servos are the next important ingredient. A servo has to be both strong enough for increased flight loads on the control surfaces, plus it has to be smooth and have good centering qualities. Sloppy servos may be less expensive, but your flight performance won’t be precise or consistent. If you want the best in power, speed and centering, consider digital servos.\n4 Often overlooked even by experienced pilots, the balance point of each airplane has to be correct for proper control and performance. There’s a range of balance for every wing type and you should always start in the center of the range for your first few test flights. The position of the Center of Gravity (CG) is also very important is it affects the pitch stability of your plane. As the CG is moved aft, the plane becomes more sensitive and more responsive and less elevator throw is needed. Moving the CG forward makes the plane less sensitive to pitch inputs and requires more elevator deflection especially during flare for landing.\n5 Evaluate your airplane and radio equipment. It is possible to learn aerobatics with a basic no-frills radio, but you will get better quicker if you have a programmable radio that makes control setup and servo throws and mixes easy. There are many 6- and 7-channel radio systems to pick from and they give you the flexibility you need for advanced performance. Remember, you don’t need the most expensive airplane to fly aerobatics, but you so have to set it up properly.\nGET IT STRAIGHT\nSetting up your models’ control surfaces with an accurate throw/deflection meter makes adjustments quick and accurate.\n6 Whether you’re building a kit or an ARF, for your airplane to fly straight, you have to build it straight. This starts on the workbench and goes a long way in minimizing unwanted trim adjustments. Make sure your plane’s wing and horizontal stabilizer and level with each other and square to the fuselage centerline. Measure the distance from the wingtips to the tail. The distance should be the same on both sides. Also, check for warps or twists in the tail or wing. Fit any misalignments to make sure you have a true airframe.\nControl linkage setup and proper hinge installation are key to precise control response.\n7 For maximum control authority, keep the linkage attachment points close to the center of the servo (short output arm) and further way from the hinge at the control surface (long control horn.) Also, bump up your standard-size pushrod and clevis hardware to 4-40 size. Use solder clevises at the servo arm and use a jam-nut at the clevis to lock it into place. Use clevis keepers to prevent them from popping off the control horns during flight. When it comes to setting control throws, start with the recommended amounts stated in the instructions and then test fly to confirm they feel right. You want only as much control throw at maximum deflections to complete your desired maneuvers. Anything more makes it harder to fly smoothly and can lead to over controlling your plane.\n8 This is where precision is created. With proper setup, you maximize your servo’s ability to control the plane by minimizing slop and play in the linkage that connects the servos to the control surfaces. By increasing the leverage of the servo, you also minimize the chances of flutter which can cause your plane to crash. Airplanes that are constantly in need of retrimming usually have a substandard control linkage setup. Always use strong control horns and attach them securely. Hinging too should be smooth and bind free. Don’t use CA cloth hinges, use larger pinned hinges and install them properly to minimize the hinge line gap between the fixed and movable surfaces.\nPOWER AND PERFORMANCE\n9 If your plane has a motor size range, select the more powerful power system. You want a good power-to-weight ratio to help execute vertical maneuvers and extend up-lines. You also want to match your motor, battery pack and speed control so everything works together without overheating or drawing too much current at full power. Make sure there’s adequate airflow to keep you controller cool and attach it securely so it does not move around during flight. This can strain the wires and connectors not to mention damage the controller itself. If you are using a programmable speed control, be sure to disable the propeller brake function and select a Soft Start setting for the throttle response.\nPICKING A PROPELLER\n10 Start with the recommended propeller but don’t be afraid to experiment. In general terms, electric motors can turn larger diameter propellers than equivalent 2-stroke engines. Also, propeller pitch for aerobatics should be lower than sport flying props. A lower pitch is like a lower gear in a transmission, it gives you more acceleration but less speed. Higher pitches give slower acceleration but a higher ultimate speed. Aerobatics also require good climb performance so the lower pitch is desired. Diameter changes affect current draw more than pitch changes do, so be sure to test each prop. Use a Watt meter and find a prop that gives you maximum performance while keeping the current draws at safe levels for your battery pack and speed control. Also, always be sure to properly balance your propellers to minimize vibration.\nENTRY AND EXITS\n11 All aerobatic maneuvers are built on a solid foundation of straight-and-level flight. Whether you are beginning or ending your maneuver upright or inverted, you should concentrate on keeping the wings level and the flight path parallel to the centerline of the runway. Even with knife-edge flight, you need to be straight with wings vertical. If you are not properly lined up, when you pull or push into the vertical, it will be angled one way or the other requiring you to make corrections.\nPRECISION & PLACEMENT\n12 All maneuvers are made up of segments of straight lines and radius turns. A loop is one continuous radius line that begins and ends at the same place. A square loop is made up of four equal length straight lines with four ¼ loops (one in each corner). Precision is the ability to make all the segments the same size and radius. When it comes to placement, you want to center your maneuvers in front of yourself with equal entry and exit segments. This is known as using the aerobatic box to full advantage. An excellent reference for various aerobatic maneuvers is John Glezellis’ Aerobatics Made Easy DVD series available at AirAgeStore.com.\nThat’s it! As with anything worth doing, it takes a great deal of practice to perfect your piloting skills and to fine-tune your aerobatic performance. Take it one step at a time and learn from each flight. Understand how to correct your mistakes and don’t get ahead of yourself. Move on to more complicated and challenging manuevers only after you’ve mastered the basics in all types of weather and wind conditions. Before you know it, you’ll be the show pilot everybody wants to copy! Fly often and remember to have fun."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:133a3704-672c-4d13-a27b-ee111af72f3a>","<urn:uuid:9c82938a-c014-4798-8352-c10b8e7c486c>"],"error":null}
{"question":"What are the different approaches being studied to address physical inactivity in Chicago's minority communities versus elderly Latino populations?","answer":"In Chicago's communities, researchers are examining environmental factors like violent crime rates, park space, grocery store availability, and walkability indexes to understand their impact on physical inactivity. For elderly Latino populations, researchers are taking a cultural approach by implementing the Bailamos dance program, which incorporates traditional dances like salsa and cha-cha-cha in twice-weekly classes at senior centers, as dancing is already familiar and culturally relevant to this population.","context":["The prevalence of physical inactivity and obesity in adults were linked to violent crime rates in Chicago’s urban Black and Hispanic communities, according to research to be presented at the American Heart Association’s Epidemiology, Prevention, Lifestyle & Cardiometabolic Health Conference 2021.\nIn the U.S., violence is a major threat to public health in communities with people from diverse racial and ethnic groups in large urban areas. These communities also have disproportionately higher rates of murder, armed robbery, aggravated assault and other types of violent crime. This study examines the effects of violence on physical activity and obesity in adults in communities with increased rates of violence.\n“This research is timely because exposure to violent crime is not often considered or accounted for in studies on racial or ethnic disparities in obesity and physical activity, and our results add much-needed insight into this area. To achieve racial equity in health, we must understand and address barriers to maintaining a healthy lifestyle,” said the study’s lead author Chelsea R. Singleton, Ph.D., an assistant professor in the department of kinesiology and community health at the University of Illinois at Urbana-Champaign.\nResearchers examined 2018 data from the City of Chicago’s police records to calculate the violent crime rate per capita; the Centers for Disease Control and Prevention’s community profile data for 2018 to assess physical activity and obesity prevalence among adults; and 2018 U.S. census bureau information on socio-demographics to define racial composition and median household income. They accounted for additional variables, including the amount of public park space in the census tract, the number of grocery stores and the walkability index of the communities. Walkability data was obtained from the U.S. Environmental Protection Agency’s National Walkability Index, which reflects sidewalk connectivity, pedestrian-oriented intersection density and land use for businesses. A higher walkability score means that the community environment allows for more walkable trips in a given area.\nThe analysis found:\n- In urban Chicago, Black communities had significantly higher rates of violent crime, and adults in those communities had higher rates of physical inactivity and obesity compared to adults in Hispanic or white communities.\n- Across all communities, violent crime was associated with higher levels of physical inactivity and obesity among adults.\n- As violent crime rates increased, the prevalence of physical inactivity and obesity increased in adults in non-Hispanic Black communities, even after accounting for differences in median family income, availability of grocery stores and the amount of park space.\nSingleton notes their results highlight an important question researchers need to address. “We found violent crime was not associated with physical inactivity or obesity in adults within Chicago’s predominantly white communities, yet violent crime rates were also lower in white communities compared to communities of color. We are interested in whether future, prospective research will show associations among the violent crime rate, physical inactivity and obesity. Additionally, information about adults in white communities with higher violent crime rates would be valuable.”\nResearchers believe future studies could combine epidemiological and geographical data to evaluate the role of violent crime rates in maintaining healthy lifestyle behaviors at both the individual and community levels. Future research could also map violent crime in public spaces, particularly those designated for exercise and other physical activities (e.g., trails, parks and recreation centers) to better understand the impact of violent crime on physical activity levels.\nThe key limitation of the study is its design—it is an ecological study, an analysis of environmental data, so no research subjects (people) were involved. Researchers note that also due to the study design, they cannot directly conclude that exposure to violent crime is associated with physical inactivity and obesity risk among Black and Hispanic adults in Chicago or other urban areas, only that areas with higher violent crimes rates also have a higher prevalence of adults who are not physically active and who are obese.\n“We can only conclude that predominantly Black and Hispanic census tracts with high violent crime rates have a higher prevalence of adults who are not physically active and who are obese. The study design must be considered when interpreting the findings,” said Singleton.","A group of Latino seniors soon will be stepping out on the dance floor in the name of medical research.\nA study being coordinated by a University of Illinois at Chicago professor will see if doing traditional dances such as salsa and cha-cha-cha can spark more physical activity in older Latinos and improve their health.\nAlthough doctors have long known exercise is important for staving off chronic disease, researchers of the dance study contend many older Latinos won't buy into just any physical activity, often for cultural reasons. Experts also say older Latinos often are at a high risk for developing disabilities.\n\"Exercising is just not a very familiar concept, but when you look at dancing, it's family parties, going out as young adults … this is just something they have done and it's a part of their culture,\" said David Marquez, UIC associate professor of kinesiology and nutrition.\nThe study will follow 166 Latinos age 65-74 for four months in twice-weekly dance classes at nine local senior centers, community centers and park buildings, and the same number in a control group.\nSeveral of the seniors will then be trained as instructors to lead the program for four more months to determine if participants — who will be evaluated for balance, mobility and cognitive improvement — will keep up with the dancing. A $1.7 million National Institutes of Health grant is funding the study.\nMarquez and Miguel Mendez, owner of Dance Academy of Salsa in Chicago, developed a four-stage program, called Bailamos, which includes merengue, cha-cha-cha, bachata and salsa. Bailamos means \"we dance\" in Spanish.\nMendez said the two wanted to use the popular dances as a form of therapy for the seniors, teaching basic but progressively more difficult steps they could follow without feeling pressured.\n\"How do you get people to move around?\" asked Marquez. \"Having something people are interested in and really enjoy, we hypothesize that will get people to come to the dance classes,\" he said.\nKyriakos Markides, an expert on aging at the University of Texas Medical Branch in Galveston and a consultant on this study, noted that Latinos have significant health problems. Although they tend to live about two years longer than whites, they are at greater risk for diabetes, obesity, depression, cognitive impairment and disability, he noted. Causes could include poverty, lack of health insurance, physically demanding jobs and isolation as they age. Studies have also shown the population tends to be more inactive.\n\"If you can get people to do it, it's got to be beneficial both physically and emotionally,\" said Markides, Annie & John Gnitzinger distinguished professor of aging studies at the university. \"We know physical and social activity are important for quality of life but they're also important in slowing down the negative effects of diseases like diabetes.\"\nCenters for Disease Control statistics show that Latinos are the largest and fastest-growing minority in the country nation. In 2010, 13 percent of the country's population was 65 or older, and Latinos made up 7.1 percent of this age group. Census 2010 data for Cook County showed that 9.2 percent of the elderly population is Latino. The life expectancy for Latinos is 80.6 years and for whites 78.1.\nDr. Patricia Grady, director of the National Institute of Nursing Research at NIH, noted the importance of studies that could help decrease illness and improve quality of life.\n\"The Bailamos program is an innovative and culturally appropriate intervention intended to increase physical activity among Latino seniors,\" noted Grady. \"Research has shown that there are multiple health benefits of physical activity for older adults.\"\nAida Giachello, professor, Feinberg School of Medicine's department of preventive medicine at Northwestern University, who is not involved in the study, said the research is important and timely because of the growing Latino population and its economic and health disparities.\n\"This is very significant because once the evaluation results are completed, this model can be transferred and adopted in nursing homes or community programs,\" said Giachello. \"I think what Dr. Marquez is doing is commendable and has a tremendous amount of potential to reach out to the elderly.\""],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:78a63d2d-03be-4b14-9f7c-d3cdbf9d3328>","<urn:uuid:810213c1-c0a2-47ba-9e2d-2468bf65b633>"],"error":null}
{"question":"What are the key differences between blockchain's approach to preventing duplicate transactions and traditional accounting's method of maintaining data integrity?","answer":"Blockchain prevents duplicate transactions through a network of miner nodes that solve proof-of-work mathematical problems and must reach majority consensus to validate transactions, ensuring the same data package cannot be used multiple times. Traditional accounting, particularly through double-entry bookkeeping, maintains data integrity by recording each transaction twice (as debit and credit), creating a self-checking system that reveals errors and inconsistencies through mathematical balancing rather than distributed consensus verification.","context":["Most people who have a general overview of cryptocurrencies will know the fact that the technology they are built upon is decentralized and uniquely secure is intrinsic to how they work. They will also know that this technology is called Blockchain. They will most probably be aware that Blockchain is a ‘peer-to-peer’ ‘digital-ledger’ system. Beyond that, a slightly smaller percentage of observers will be aware the transactions are verified through ‘mining’ and that this mining process involves computers solving complex mathematical problems. Also, that the cryptocurrency ‘miners’ are rewarded for their endeavor with new cryptocurrency can be considered relatively common knowledge.\nHowever, beyond this very broad overview of Blockchain’s role in cryptocurrencies, most don’t really understand the finer details of how this quite ingenious system works and what potentials it has. That is what we are going to explain here in as straightforward a way as possible.\nWhat Does Blockchain Do?\nBlockchain technology fulfills two core functions in a decentralized way, which requires no central authority or management:\n- Recording the entire history of a digital ‘object’ or piece of information\n- Ensuring that record is indisputable\nBlockchain achieves this through a combination of three pillars:\n- Cryptography for security\n- A decentralized ledger system for record keeping\n- Incentivisation of the p2p community to service and maintain the first two pillars\nTwo parties wish to exchange a digital data package. This could be a unit of cryptocurrency or a patient’s medical records or anything in between. Both parties have two keys – a public key and a private key connected to their own unique data and identity. The combination of these two keys constitutes a secure digital identity reference. This digital signature authenticates ownership and the combination of two digital signatures provides a strong authentication of ownership and the consent to transfer of ownership.\n2. Decentralized Ledger System\nThe role of the decentralized ledger system is to bear reliable witness to a transaction that transfers ownership. One key strength of Blockchain technology is that it provides a huge number of witnesses (essentially all other users) to corroborate a single event (the ownership transfer of a data package). This corroboration is achieved through mathematical verification rather than visually recording it.\nThe first step in this process is the sender of a data package making the announcement through their private key that they wish to transfer it to the intended recipient. This announcement is then attached to the recipient party’s public key.\nA block of data is then formed. This contains the digital signature formed by the combination of the two keys, the time of the transaction and any other relevant information (which will vary depending on the kind of data being sent (eg.) a cryptocurrency unit. This is then broadcast to all of the ‘witnesses’ that form the decentralized ledger. These witnesses are called ‘nodes’ and are part of the system by virtue of the fact they also own or transfer the same kind of data packages at some other moment in time. This could mean they are also owners of the same kind of cryptocurrency or whatever other kinds of data the particular Blockchain supports.\nBut why are witness nodes available to perform the mathematical verification that confirms a transaction takes place? In the case of open Blockchains such as those which cryptocurrencies are built on, the answer to this question is ‘mining’. A miner offers the computer processing power to run the network. To provide the mathematical verification that corroborates a transaction is incentivized by economic reward. In other words, Bitcoin miners are paid in Bitcoin. Self-interest services the public need.\nThe same data package must be verified as unique. The possibility that the same data, (eg.) a Bitcoin, is involved in multiple transactions at the same time must be eliminated. The nodes create and maintain the network that means the same data package can’t be duplicated by solving proof-of-work mathematical problems. New data blocks are then either accepted or rejected by each miner node. A majority of miner nodes must all agree a data block is valid for it to be added to the Blockchain.\nWhat constitutes a valid transaction and what is required for it to be verified differs between Blockchains. The exact verification process and incentives for node witnesses and miners can also vary between Blockchains. The technology is still being experimented with and used for a variety of different and new applications. The best solution to validation, verification and locking the information immutably into the ledger will vary between Blockchain applications.","Accounting Evolution: Past, Present and Future\nAccounting is the process of tracking financial information, and providing a system for recording, verifying, analyzing, and reporting transactions. Accounting has been around for centuries and has evolved over time to meet the needs of different societies and businesses. In this blog, we will explore some of the major milestones in the history of accounting and how they have shaped the modern profession.\nThe Origins of Accounting\nThe earliest evidence of accounting dates back to ancient civilizations in Mesopotamia, Egypt, China, India, Greece, and Rome. These cultures used various methods of writing, counting, and measuring to keep track of their trade activities, taxes, debts, inventories, and wealth. For example:\n- In Mesopotamia (around 3300 BC), clay tablets were used to record transactions using cuneiform script.\n- In Egypt (around 3000 BC), papyrus scrolls were used to record transactions using hieroglyphs.\n- In China (around 2000 BC), bamboo strips were used to record transactions using Chinese characters.\n- In India (around 600 BC), palm leaves were used to record transactions using Sanskrit script.\n- In Greece (around 500 BC), wax tablets were used to record transactions using Greek alphabet.\n- In Rome (around 100 BC), wooden tablets were used to record transactions using Latin alphabet.\nThese ancient accounting systems were mainly based on single-entry bookkeeping, which means that only one side of a transaction was recorded. For example, if a merchant sold goods for cash, he would only record the cash received and not the goods delivered. This method was simple but prone to errors and fraud.\nThe Development of Double-Entry Bookkeeping\nThe most significant innovation in accounting history was the invention of double-entry bookkeeping in medieval Europe. Double-entry bookkeeping means that every transaction is recorded on two sides: debit and credit.\nFor example, if a merchant sold goods for cash, he would record both the cash received (debit) and the goods delivered (credit). This method provides a more accurate and complete picture of a business's financial position and performance.\nThe originator of double-entry bookkeeping is widely considered to be Luca Pacioli (1447-1517), an Italian mathematician and friar who published a treatise on accounting called Summa de Arithmetica in 1494. Pacioli described the principles and rules of double-entry bookkeeping as well as various accounting topics such as journals, ledgers, trial balances, financial statements and auditing.\nPacioli's work was influenced by earlier scholars such as Leonardo Fibonacci (1170-1250) who introduced Arabic numerals to Europe; Ibn Khaldun (1332-1406) who discussed economic concepts such as profit, loss and taxation; and Benedetto Cotrugli (1416-1469) who wrote about double-entry bookkeeping before Pacioli but did not publish his work until later.\nPacioli's treatise was widely disseminated across Europe and became the standard reference for accounting practice for centuries. Double-entry bookkeeping enabled merchants, bankers and traders to keep track of their complex business transactions more efficiently and reliably. It also facilitated the development of commerce, capitalism,\nand the industrial revolution.\nThe Modernization of Accounting\nAs business activities became more diverse, globalized and regulated in the modern era,\naccounting also had to adapt to new challenges and opportunities. Some of the key developments in accounting history include:\n- The emergence of professional accountants: As accounting became more specialized and sophisticated, there was a need for qualified experts who could provide reliable advice and assurance on financial matters. The first professional accounting bodies were established in Scotland in 1854 followed by England in 1880. These bodies set standards for education, ethics and examination for their members who became known as chartered accountants or certified public accountants.\nThe creation of accounting standards:\nAs different countries developed their own accounting practices and rules, there was a need for harmonization and comparability across borders. The first international accounting standards were issued by International Accounting Standards Committee (IASC) in 1973 followed by International Financial Reporting Standards (IFRS) by International Accounting Standards Board (IASB) in 2001. These standards provide guidelines for preparing financial statements that are consistent, transparent and comparable across different jurisdictions and industries.\nThe expansion of accounting functions:\nAs businesses faced more complex and dynamic environments, there was a need for more diverse and advanced accounting functions beyond financial reporting. These include:\nThis involves providing information and analysis to managers for planning, controlling and decision-making purposes. Management accounting covers topics such as budgeting, costing, variance analysis, performance measurement and balanced scorecards.\nThis involves examining and verifying the accuracy and reliability of financial statements And other information prepared by accountants. Auditing can be internal or external depending on who performs it. Internal auditors are employed by the organization to assess its risks, controls and compliance with policies and regulations. External auditors are independent professionals who provide an opinion on whether the financial statements present a true and fair view of the organization's financial position and performance.\nThis involves calculating and reporting the tax liabilities of individuals or organizations based on their income, expenses, assets, liabilities and other factors. Taxation also involves providing advice on tax planning, saving and compliance with tax laws and regulations.\nThe adoption of accounting technology:\nAs technology became more accessible and sophisticated, there was a need for leveraging it to improve accountin"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:470ef686-3806-4f1f-8395-e36a5410ebb5>","<urn:uuid:7e47a487-c84c-4e6e-87fe-c2c8d1703fd3>"],"error":null}
{"question":"Hey! Can anyone tell me whether the British Parliamentary Papers Index includes more content types than the papers from the Battle of Gettysburg? 🤔","answer":"Yes, the British Parliamentary Papers Index is more comprehensive, as it indexes three different types of content: House of Commons Sessional Papers, House of Lords Sessional Papers, and Hansard's Debates. In contrast, the Battle of Gettysburg content discussed only includes telegraph reports and military communications about the battle itself.","context":["- Your selections:\nDatabase Results for Your Selections\nResults 151 - 175 of 869\nScholarly interdisciplinary reference work on the religion of Hinduism in its divergent and common forms. Articles are organized thematically in sections and alphabetically within sections.Expand\nThe index offers an alphabetically organized listing of all topics.\nBrill's New Jacoby (BNJ) is a collection of commentary on the fragments of 856 Greek historians whose works have survived only in fragments. BNJ supersedes F. Jacoby's monumental 18 volume, Die Fragmente der griechischen Historiker, Parts I-III.Expand\nEach author has the Greek text with facing English translation; new, critical commentary; a brief encyclopedia-style entry, and a select bibliography.\nSubjects: Classics, Biological Sciences\nFull-text articles on all aspects of the ancient world, including persons, places, institutions, events, artifacts, technical terms, ideas and concepts, and interaction between Greek and Roman culture with others (Semitic, Celtic, Germanic, etc.).Expand\nMany articles are illustrated with maps, site plans, genealogical tables, and black-and white photographs. Brill's New Pauly Online includes the entire text of Metzler's Der Neue Pauly and also all volumes of the English language Brill's New Pauly.\nSubjects: Art and Architecture, Classics\nBritish History Online is the digital library containing some of the core printed primary and secondary sources for the medieval and modern history of the British Isles.Expand\nCreated by the Institute of Historical Research and the History of Parliament Trust, we aim to support academic and personal users around the world in their learning, teaching and research. With so much material now being published on the world wide web in either un-reviewed form, or in fee-based services, the IHR has shown its commitment to promoting the study of history by publishing these priceless resources for their historical value, cross-searchable, in one place and free of charge.\nHighlights include sources relating to national and local government (excluding Parliament), and the development of British law. Sources relating to the churches, religious houses and other religious groups. Records of commerce and trade, taxation, population and property. Sources relating to art and architecture, science and the history of ideas. Historical and topographical sources relating to specific places. Parliamentary history: Sources relating to proceedings and legislation in the two Houses of Parliament at Westminster and sources for metropolitan London and other urban centres.\nSubjects: History Early Modern 1450-1800, History Middle Ages 400-1450, History Modern 1800-\nBOPCRIS (British Official Publications Collaborative Reader Information Service) aims to aid researchers interested in British Official Publications over the period 1688 -1995 by providing a Web-based bibliographic database (including Parliamentary papers).Expand\nThe BOPCRIS database enables researchers to search and browse for relevant documents, read abstracts, and view detailed consistent subject indexing of key documents so they can assess whether they need to see the full document. The database also offers digitized full-text versions of a limited number of documents.\nSubjects: History Early Modern 1450-1800, History Modern 1800-\nThis archive of digitized microfilm has a wide range of items to interest those researching territories colonized by Britain including: America, India, Africa, the West Indies, New Zealand, Australia and Melanesia.Expand\nDocuments include records made by the East India Company and the records of missionaries from the Society for the Propagation of the Gospel. The collection contains a range of sources that relate to both politics and history, these range from the Parliamentary Labour Party papers and the Anti-fascist newsletters of the International Transport Workers' Federation to the records of the Communist Party of Great Britain, and on to The British Union of Fascists’ newspapers and secret files.\nSubjects: African American Studies, African Studies Sub-Saharan, Area Studies, English and American Literature, History Early Modern 1450-1800, History Modern 1800-, History US, History World, Latin American and Caribbean Studies, Political Science, Religion, Archives and Primary Sources\nBritish Parliamentary Papers Index\nIndexes House of Commons Sessional Papers, House of Lords Sessional Papers, and Hansard's Debates.Expand\nSubjects: Government Information\nBritish Periodicals Collection I consists of more than 160 journals that comprise the UMI microfilm collection Early British Periodicals, the equivalent of 5,238 printed volumes containing approximately 3.1 million pages.Expand\nTopics covered include literature, philosophy, history, science, the fine arts and the social sciences.\nSubjects: Archives and Primary Sources, Art and Architecture, English and American Literature, History Modern 1800-, History World, Philosophy\nThis database provides online access to collections on the history of the Atlantic world from the British Archives. It includes materials such as journals, correspondence, official records and personal papers spanning two centuries of Britain's colonial, commercial, missionary and literary relations with Africa and the Americas.Expand\nSubjects: African American Studies, African Studies Sub-Saharan, History US, History World, Latin American and Caribbean Studies, Archives and Primary Sources\nBrockelmann Online consists of both original print volumes of Geschichte der arabischen Litteratur (GAL) by Carl Brockelmann (1868-1956), as well as the three supplement volumes, including the indexes.Expand\nIt is full-text searchable. The “Brockelmann” has become an indispensible research tool for the Islamic world in general and the Middle East in particular. Brockelmann’s Geschichte der arabischen Litteratur offers bio-bibliographic information about works written in Arabic and their authors, with an emphasis on the classical period. The multi-volume reference work is divided in chronologically organized sections, which are subdivided by literary genre. Individual entries typically consist of a biographical section and a list of the author’s works in manuscript and print, with references to secondary literature.\nSubjects: Area Studies, Classics, History Early Modern 1450-1800, History Middle Ages 400-1450, History Modern 1800-\nThe BuildingGreen Suite of online tools provides information related tosustainable building--from energy efficiency and recycled-content materials to land-use planning and indoor air quality.Expand\nThe BuildingGreen Suite includes GreenSpec Directory, Environmental Building News newsletter, and the High Performance Buildings Database of green building and LEED case studies. Each article, product listing, and case study is cross-listedamong the suite products and includes related content and information sources.\nSubjects: Environmental Sciences\nBusiness Source Complete is a scholarly business database that provides full-text access to many peer-reviewed business related journals, including disciplines such as marketing, management, MIS, POM, accounting, finance and economics.Expand\nIndexing and abstracts for the many important scholarly business journals as far back as 1886 are included. In addition to the searchable cited references provided for more than 1,200 journals, Business Source Complete contains detailed author profiles for the 20,000 most-cited authors in the database. Additional full-text, non-journal content includes financial data, books, monographs, major reference works, conference proceedings, case studies, investment research reports, industry reports, market research reports, country reports, company profiles, SWOT analyses and more.\nCAB Direct is the most thorough and extensive source of reference in the applied life sciences, incorporating the leading bibliographic databases CAB Abstracts and Global Health.Five (5) simultaneous users.ExpandFive (5) simultaneous users.\nSubjects: Public Health, Biological Sciences, Health, Science and Technology, Environmental Sciences\nCairn provides a comprehensive collection of French language periodicals in the social sciences and humanities. This database represents a broad collaboration between several European publishing houses (Belin, De Boeck, La Decounverte and Eres).Expand\nFrench Studies bouquet, covers topics in General Interest, History, Literature, and Philosophy, and includes such publishers as PUF, Belin, Vrin, La Découverte, and Les Belles Lettres. Several journals of interest are Revue française d'histoire des idées politiques, Dix-septième siècle, Le Français aujourd'hui, and Médiévales.\nSubjects: History Early Modern 1450-1800, History Modern 1800-, French Literature and Language\nCambridge Companions Complete Collection presently contains the Cambridge Companions to literature, philosophy, religion, classics and culture; more collections in subject areas will be added soon.Expand\nThe collection is updated with new Companions on publication.\n4-5 Simultaneous usersWalk in users: OK; 4-5 simultaneous users\nSubjects: African American Studies, African Studies Sub-Saharan, Anthropology, Area Studies, Art and Architecture, Classics, Economics, English and American Literature, Ethnic Studies, History Early Modern 1450-1800, History Middle Ages 400-1450, History Modern 1800-, History US, History World, Latin American and Caribbean Studies, Law, LGBT Studies, Linguistics, Mathematics, Multidisciplinary Databases, Philosophy, Political Science, Psychology, Religion, Sociology, Womens and Gender Studies\nCambridge Histories Online provides immediate full-text access to more than 300 volumes published since 1960 across 15 subject areas. Users can create accounts and save searches, place bookmarks, and adjust search results.Expand\nSubjects: African American Studies, African American Studies, Area Studies, Classics, Economics, History Ancient -400, History Early Modern 1450-1800, History Middle Ages 400-1450, History Modern 1800-, History US, History World, Multidisciplinary Databases, Philosophy, Political Science, Reference Sources, Religion, Theater Studies\nContains published crystallographic data on organic, organometallic, and metal complex structures. Also contains the Brookhaven Protein Data Bank files.Expand\nSubjects: Chemistry, Science and Technology\nWhen it is complete, Canadian Poetry will comprise essentially the complete canon of English-language Canadian poetry from the seventeenth century up to the early twentieth.Expand\nCurrently the collection contains more than 12,000 poems by 142 poets drawn from over 500 printed sources, many of them rare and inaccessible outside Canada.\nSubjects: English and American Literature\nCaribbean Abstracts is the online version of the original print index (of the same title) produced at the Royal Netherlands Institute of Southeast Asian and Caribbean Studies.Expand\nThis catalog contains abstracts in English of selected books and articles in the humanities and social sciences published since 1989. The publications are selected for their importance or the topicality of the subject. The Caribbean includes all islands of the Caribbean archipelago, as well as the mainland territories of Belize and the Guianas. Publications on Caribbean migrant communities in other regions are excluded.\nSubjects: Latin American and Caribbean Studies\nA fully searchable full text of historical newspapers from the islands of the Caribbean, principally the English speaking Caribbean.Expand\nSubjects: African American Studies, History US, Latin American and Caribbean Studies, News Sources\nThe CGP is the finding tool for federal publications that includes descriptive records and provides direct links to those that are available online. Users can search by authoring agency, title, subject, and general key word, or click on \"Advanced Search\" for more options.Expand\nThe catalog offers you the option to find a nearby Federal depository library that has a particular publication.\nFor help using this resource, contact Chris Palazzolo, Political Science and International Documents Librarian.\nSubjects: Government Information\nCovers all aspects of the Catholic faith and lifestyle, and includes over 370,000 index citations of articles and reviews published in Roman Catholic periodicals, Papal documents, church promulgations, and books about the Catholic faith that are authored by Catholics and/or produced by Catholic publishers.Expand\nPrimary legal sources and CCH explanations on securities, banking, capital changes, trade regulation, government contracts, information technology, energy, transportation, and products liability and safety.Expand\nSubjects: Business, Economics\nLegislation, regulations, caselaw, administrative decisions, forms, and commentary related to the following fields: human resources management, payroll, pension and benefits, employment law, workplace safety, workers' compensation, food and drugs, and health law.Expand\nSubjects: Business, Health\nComprehensive, up-to-date tax information from the IRS and state agencies, plus explanations, analysis, and practice aids from CCH.Expand","The decisive battle has been fought to-day, and the enemy repulsed with terrific loss. At daylight Lee’s right wing batteries opened upon our left, and shortly after those of his centre followed.\nAfter half an hour’s cannonading, doing but little damage to us, the fire slackened and only occasional shots were exchanged. Shortly afterwards the enemy’s left, composed entirely of infantry and sharpshooters, made an attack on our right wing so sudden and importunely that our skirmishers and front line were driven back from their entrenchments, but by the aid of the batteries in the rear and the bravery of the 12th corps, we regained the first position, capturing a considerable number of prisoners. Several hours of ominous silence followed this repulse. At 1 o’clock the enemy fired two shots, apparently the signal for the grandest artillery fight ever witnessed on this continent. Before a moment elapsed it is estimated at least 80 guns opened upon us. Our batteries returned the fire, and for more than one hour it seemed impossible that man or beast could live. The range as exhibited on the two previous days was wanting on this occasion, most of their shells exploding far in the rear of our front, and generally missing our batteries. Under cover of this Lee advanced his columns of infantry from their covers and made several desperate attempts to carry the lines by assault, but each successive attempt repelled with terrific havoc to them. Some of our batteries, whose ammunition being expended and the men exhausted, ceased to fire, and on the approach of the reserve batteries withdrew to the rear.\nThe enemy, on seeing the batteries withdrawn, and mistaking this for a retreat, made a rapid infantry charge upon the hill and obtained position in our lines, cutting to pieces and almost annihilating the small infantry supports, but before they had time to rejoice at their imaginary success, the breech batteries poured in a deadly fire of canister. The infantry reserve joined on either flank of the gap, charged them and added greatly to their destruction. They were completely surprised, and hundreds threw down their guns and asked for quarter. Nearly the entire brigade of Gen. Dick Garnett surrendered, and Garnett himself was wounded and barely made his escape.\nLongstreet was mortally wounded and captured. He is reported to have died in one hour afterward.\n– – –\nThis telegraphed report of an as-yet-unnamed battle near Gettysburg, Pennsylvania, was probably received, but not published by the Daily Democrat and News until July 6 as this newspaper did not print on the Fourth of July nor the next day, as it was a Sunday. Even so, the telegraph shortened the usual delay in news of the War by a week or more.\nIt’s important to note, however, that quick news doesn’t always mean accurate facts: As official reports later showed, Lieutenant General Longstreet* was unwounded. It was Brigadier General Garnett who suffered a fatal wound and died on the field.\nThe Brigadier General wasn’t the only one to fall in what would later be known as Pickett’s Charge, a bloody fight between an estimated 6,500 Union troops and 15,000 Confederate soldiers on the third day of the Battle of Gettysburg.\nUnion losses during this bloody battle, including the dead and the wounded, those missing in action or taken prisoner, totaled about 1,500. Confederate losses were over 6,000; roughly half those men were from Major General George Pickett’s division.\nThe evening of Pickett’s Charge, General Lee regrouped and waited for Major General Meade to attack, while heavy rains began to fall. Several small skirmishes took place on July 4th, but no further major battles, and by evening, General Lee had started to move his troops south.\nThe supply wagon train filled with Confederate wounded was reported to be 14 miles long.\n(posted by Amy D.)\n*Longstreet, Garnett, Pickett, and Lee all served with the Confederate forces. Major General Meade served with the Union.\nFor telegraph information leading up to the battle of Gettysburg, please click here."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:facc6c44-312c-4ad6-a8ec-e9efbc0a57c7>","<urn:uuid:a3f5157e-ed9a-4a6b-bf7a-ceb226113f30>"],"error":null}
{"question":"What is the key difference between the traditional approach of memorization through repetition and the method of loci when it comes to memory enhancement?","answer":"The key difference is that traditional memorization relies primarily on rote learning (ramming facts into your head), while the method of loci is based on creative visualization and spatial memory. As explained in Document 1, people mistakenly thought memory operated primarily by rote, but memory is actually an imaginative process. The method of loci, detailed in Document 2, enhances memory by having people mentally associate information with specific physical locations and create vivid visual images, making use of spatial memory and familiar environments to efficiently recall information.","context":["Ever wandered into a store and quickly realized you’ve forgotten what you need? Or been introduced to someone and forget their name by the time they walk away?\nDon’t worry; you’re not alone. A 2007 survey conducted at Trinity College in Dublin found that one-third of British people under age 30 can’t even remember their own phone numbers. Although this forgetfulness happens to the best of us, if you’re one to blame your \"bad memory,\" you should know that your refusal to train your memory is the reason it’s gone awry.\nIn his book, How to Develop A Super Power Memory, memory training specialist Harry Lorayne says \"there is no such thing as a bad memory\" and that \"there are only trained and untrained memories.\" Although there are a number of science-backed strategies when it comes to making connections and enhancing your memory, when you really think about the underpinnings of a strong memory, it has everything to do with creativity.\nBasically, saying that you have a bad memory is like saying you're not creative.\nIn the book Moonwalking With Einstein, author Joshua Foer (also a record-holding memory champion) writes about his quest to improve his own memory by studying \"mental athletes.\" At one point, he writes about Tony Buzan, an educational consultant, who says the following about memory:\nIn our gross misunderstanding of the function of memory, we thought that memory was operated primarily by rote. In other words, you rammed it in until your head was stuffed with facts. What was not realized is that memory is primarily an imaginative process. In fact, learning, memory, and creativity are the same fundamental processes directed with a different focus.\nIf your ability to remember something and create something both rely on your ability to associate information with meaning, then the two use the same part of your brain. Think about it this way, what is the best way to remember someone’s name? We’re often told to use mnemonics to memorize larger pieces of information, such as a verse, a formula, a list, or a name.\nBut to enhance your memory, instead of coming up with devices that have no association in your brain, pretend you’re working on a creative project and come up with a story about the names, list, or whatever it is you’re trying to remember. These exercises might sound ridiculous and time-consuming, but they actually train the brain to get better. Like every other function in your body, the brain is no different when it comes to training to get stronger and healthier.\nDale Carnegie’s leadership training program teaches professionals to associate names with a personality trait, occupation, or even a visual cue to help you tell your story. Carnegie says:\nPaint a mind picture of the person whose name you wish to remember doing something that reminds you of the person’s name. Have the face and body of the person you wish to remember in the picture so that, when the picture comes to mind, you get both the face and the name.\nThe better your visualization—the more creative you are—the deeper ingrained that memory is in your brain.\nAccording to Luca Lampariello who speaks over 10 languages, context is king when it comes to building memory. Our brains don’t remember isolated facts. Instead we remember the stories—the context—behind those facts. When you look at random pieces of information, it makes no sense in your brain, but when you attach a story to it, your brain is more likely to retain that information.\nStudies say humans remember pictures and images more than they do text. Creativity is all about coming up with these visualization techniques to be the foundation of your memory mnemonics. When you’re learning a new language, try to come up with visualizations for the new foreign word you’re learning. When trying to remember a list—even something as boring as a shopping list—come up with a bizarre story to help you remember that list. For instance, think of a long hallway and have different items on that list pop up during your walk down the hallway in strange ways.\nFoer says in the New York Times:\nThe point of memory techniques is to take the kinds of memories our brains aren't that good at holding on to and transform them into the kinds of memories our brains were built for. [They create] memorable images for your palaces: the funnier, lewder and more bizarre, the better. When we see in everyday life things that are petty, ordinary, and banal, we generally fail to remember them . . . But if we see or hear something exceptionally base, dishonorable, extraordinary, great, unbelievable, or laughable, that we are likely to remember for a long time.\nThink back at the strongest memories you have and there’s a good chance those memories are linked to some kind of strong emotion or feeling. For instance, you probably have a memory of a time when something really funny or really exciting happened. Those emotions are exactly what made the facts surrounding that time or day memorable.\nA study from 1969 found that when it comes to memorization, motivation is not a factor. Instead, it's feelings and senses that make the difference. In the study, researchers James Jenkins and Thomas Hyde gave two groups of students the same list of words. One group was instructed to memorize the list for an upcoming test and the other was told there would be no test. Additionally, one group was told to make a mental note when the letter \"E\" appeared in a word while the other group was asked to determine whether the word was pleasant or not to them. The researchers found that telling students they have an upcoming test didn’t affect their memorization skills, but asking them to determine pleasantness in a word allowed them to make associations with it and hence, had a larger impact on their memory abilities.\nLastly, once you have it all down, don’t forget that you need to repeat, repeat, and review. There is no point in learning something new if you don’t repeat it and get it into your memory.\nThere are a number of tricks you can follow like eating berries, exercising, drinking coffee, and meditating—all proven to help you enhance memory—but when it comes to the bedrock of training your brain and defeating forgetfulness, memory training comes down to unlocking creativity and your ability to turn all of life into a string of bizarre stories.","Method of loci\nThe method of loci (loci being Latin for \"places\") is a method of memory enhancement which uses visualizations with the use of spatial memory, familiar information about one's environment, to quickly and efficiently recall information. The method of loci is also known as the memory journey, memory palace, or mind palace technique. This method is a mnemonic device adopted in ancient Roman and Greek rhetorical treatises (in the anonymous Rhetorica ad Herennium, Cicero's De Oratore, and Quintilian's Institutio Oratoria). Many memory contest champions claim to use this technique to recall faces, digits, and lists of words. These champions' successes have to do with brain structure, intelligence, however more to do with using spatial memory and the use of the method of loci.\nThe term is most often found in specialised works on psychology, neurobiology, and memory, though it was used in the same general way at least as early as the first half of the nineteenth century in works on rhetoric, logic, and philosophy. John O'Keefe and Lynn Nadel refer to:\n'the method of loci', an imaginal technique known to the ancient Greeks and Romans and described by Yates (1966) in her book The Art of Memory as well as by Luria (1969). In this technique the subject memorizes the layout of some building, or the arrangement of shops on a street, or any geographical entity which is composed of a number of discrete loci. When desiring to remember a set of items the subject 'walks' through these loci in their imagination and commits an item to each one by forming an image between the item and any feature of that locus. Retrieval of items is achieved by 'walking' through the loci, allowing the latter to activate the desired items. The efficacy of this technique has been well established (Ross and Lawrence 1968, Crovitz 1969, 1971, Briggs, Hawkins and Crovitz 1970, Lea 1975), as is the minimal interference seen with its use.\nThe items to be remembered in this mnemonic system are mentally associated with specific physical locations. The method relies on memorized spatial relationships to establish, order, and recollect memorial content. It is also known as the \"Journey Method,\" used for storing lists of related items, or the \"Roman Room\" technique, which is most effective for storing unrelated information.\nMany effective memorisers today use the \"method of loci\" to some degree. Contemporary memory competition, in particular the World Memory Championship, was initiated in 1991 and the first United States championship was held in 1997. Part of the competition requires committing to memory and recalling a sequence of digits, two-digit numbers, alphabetic letters, or playing cards. In a simple method of doing this, contestants, using various strategies well before competing, commit to long-term memory a unique vivid image associated with each item. They have also committed to long-term memory a familiar route with firmly established stop-points or loci. Then in the competition they need only deposit the image that they have associated with each item at the loci. To recall, they retrace the route, \"stop\" at each locus, and \"observe\" the image. They then translate this back to the associated item. For example, Ed Cooke, a World Memory Champion Competitor, describes to Josh Foer in his book Moonwalking with Einstein how he uses the method of loci. First, he describes a very familiar location where he can clearly remember many different smaller locations like his sink in his childhood home or his dog's bed. Cooke also advises that the more outlandish and vulgar the symbol used to memorize the material, the more likely it will stick.\nMemory champions elaborate on this by combining images. Eight-time World Memory Champion Dominic O'Brien uses this technique. The 2006 World Memory Champion, Clemens Mayer, used a 300-point-long journey through his house for his world record in \"number half marathon\", memorising 1040 random digits in a half-hour. Gary Shang has used the method of loci to memorise pi to over 65,536 (216) digits.\nUsing this technique a person with ordinary memorisation capabilities, after establishing the route stop-points and committing the associated images to long-term memory, with less than an hour of practice, can remember the sequence of a shuffled deck of cards. The world record for this is held by Simon Reinhard at 21.19 seconds.\nThe technique is taught as a metacognitive technique in learning-to-learn courses. It is generally applied to encoding the key ideas of a subject. Two approaches are:\n- Link the key ideas of a subject and then deep-learn those key ideas in relation to each other, and\n- Think through the key ideas of a subject in depth, re-arrange the ideas in relation to an argument, then link the ideas to loci in good order.\nThe Rhetorica ad Herennium and most other sources recommend that the method of loci should be integrated with elaborative encoding (i.e., adding visual, auditory, or other details) to strengthen memory. However, due to the strength of spatial memory, simply mentally placing objects in real or imagined locations without further elaboration can be effective for simple associations.\nA variation of the \"method of loci\" involves creating imaginary locations (houses, palaces, roads, and cities) to which the same procedure is applied. It is accepted that there is a greater cost involved in the initial setup, but thereafter the performance is in line with the standard loci method. The purported advantage is to create towns and cities that each represent a topic or an area of study, thus offering an efficient filing of the information and an easy path for the regular review necessary for long term memory storage.\nThe technique is also used for second language vocabulary learning, as polyglot Timothy Doner described in his 2014 TED talk. The method is further described in Anthony Metiver's book \"How to learn and memorise German vocabulary\". What the author suggests is creating a memory palace for each letter of the German alphabet. Each memory palace then shall include a number of loci where an entry (a word or a phrase) can be stored and recalled whenever you need it.\nApplicability of the term\nThe designation is not used with strict consistency. In some cases it refers broadly to what is otherwise known as the art of memory, the origins of which are related, according to tradition, in the story of Simonides of Ceos and the collapsing banquet hall. For example, after relating the story of how Simonides relied on remembered seating arrangements to call to mind the faces of recently deceased guests, Stephen M. Kosslyn remarks \"[t]his insight led to the development of a technique the Greeks called the method of loci, which is a systematic way of improving one's memory by using imagery.\" Skoyles and Sagan indicate that \"an ancient technique of memorization called Method of Loci, by which memories are referenced directly onto spatial maps\" originated with the story of Simonides. Referring to mnemonic methods, Verlee Williams mentions, \"One such strategy is the 'loci' method, which was developed by Simonides, a Greek poet of the fifth and sixth centuries BC\" Loftus cites the foundation story of Simonides (more or less taken from Frances Yates) and describes some of the most basic aspects of the use of space in the art of memory. She states, \"This particular mnemonic technique has come to be called the \"method of loci\". While place or position certainly figured prominently in ancient mnemonic techniques, no designation equivalent to \"method of loci\" was used exclusively to refer to mnemonic schemes relying upon space for organization.\nIn other cases the designation is generally consistent, but more specific: \"The Method of Loci is a Mnemonic Device involving the creation of a Visual Map of one's house.\"\nThis term can be misleading: the ancient principles and techniques of the art of memory, hastily glossed in some of the works, cited above, depended equally upon images and places. The designator \"method of loci\" does not convey the equal weight placed on both elements. Training in the art or arts of memory as a whole, as attested in classical antiquity, was far more inclusive and comprehensive in the treatment of this subject.\nSpatial mnemonics and specific brain activation\nBrain scans of \"superior memorizers\", 90% of whom use the method of loci technique, have shown that it involves activation of regions of the brain involved in spatial awareness, such as the medial parietal cortex, retrosplenial cortex, and the right posterior hippocampus. The medial parietal cortex is most associated with encoding and retrieving of information. Patients who have medial parietal cortex damage have trouble linking landmarks with certain locations; many of these patients are unable to give or follow directions and often get lost. The retrosplenial cortex is also linked to memory and navigation. In one study on the effects of selective granular retrosplenial cortex lesions in rats, the researcher found that damage to the retrosplenial cortex led to impaired spatial learning abilities. Rats with damage to this area failed to recall which areas of the maze they had already visited, rarely explored different arms of the maze, almost never recalled the maze in future trials, and took longer to reach the end of the maze, as compared to rats with a fully working retrosplenial cortex.\nIn a classic study in cognitive neuroscience, O'Keefe and Nadel proposed \"that the hippocampus is the core of a neural memory system providing an objective spatial framework within which the items and events of an organism's experience are located and interrelated.\"\nIn a more recent study, memory champions during resting periods did not exhibit specific regional brain differences, but distributed functional brain network connectivity changes compared to control subjects. When volunteers trained use of the method of loci for six weeks, the training-induced changes in brain connectivity were similar to the brain network organization that distinguished memory champions from controls.\nFictional portrayals of the method of loci extend as far back as ancient Greek myths. The method of loci also features prominently in the BBC series Sherlock, in which the titular main character uses a \"mind palace\" to store information. In the original Arthur Conan Doyle stories, Sherlock Holmes referred to his brain as an attic. In Hannibal Rising by Thomas Harris, a detailed description of Hannibal Lector's memory palace is provided. The My Little Pony: Friendship is Magic season eight episode \"The Maud Couple\" features the character Mudbriar, who possesses a mind palace in reference to Sherlock. In the CBS series The Mentalist the titular main character Patrick Jane also claims the use of this technique in memorizing specific information and details.\n- Jusczyk, P.W.; Klein, R.M., eds. (August 1, 1980). The Nature of Thought: Essays in Honor of D. O. Hebb. Hillsdale, New Jersey: Psychology Press. pp. 217–218. ISBN 0898590345.\n- e.g. in a discussion of \"topical memory\" (yet another designator) Jamieson mentions that \"memorial lines, or verses, are more useful than the method of loci.\" Alexander Jamieson, A Grammar of Logic and Intellectual Philosophy, A. H. Maltby, 1835, p112\n- O'Keefe, John; Nadel, Lynn (December 7, 1978). The Hippocampus as a Cognitive Map'. Oxford: Oxford University Press. ISBN 978-0198572060.\n- Carlson, Neil R. (2010). Psychology the science of behaviour. Pearson Canada Inc. p. 245. ISBN 9780205645244.\n- \"The Roman Room Technique\". AcademicTips.org. Retrieved October 24, 2013.\n- Foer, Joshua (March 16, 2005). \"Forget Me Not: How to win the U.S. memory championship\". Slate. Retrieved October 24, 2013.\n- \"1997 World Memory Championships\". Mind Sports Worldwide. Archived from the original on May 14, 2011. Retrieved October 24, 2013.\n- \"Memory Town System for Languages - Memory Techniques Wiki\". mt.artofmemory.com.\n- Raz, A.; Packard, M. G.; Alexander, G. M.; Buhle, J. T.; Zhu, H.; Yu, S.; Peterson, B. S. (2009). \"A slice of π : An exploratory neuroimaging study of digit encoding and retrieval in a superior memorist\". Neurocase. 15 (5): 361–372. doi:10.1080/13554790902776896. PMC . PMID 19585350.\n- \"5 Minute Speed Cards\". World Memory Statistics. World Memory Sports Council. Retrieved October 24, 2013.\n- \"Learning How to Learn: Powerful mental tools to help you master tough subjects - Coursera\". Coursera.\n- Dalgleish, Tim; Navrady, Lauren; Bird, Elinor; Hill, Emma; Dunn, Barnaby D.; Golden, Ann-Marie (12 February 2013). \"Method-of-Loci as a Mnemonic Device to Facilitate Access to Self-Affirming Personal Memories for Individuals With Depression\". Clinical Psychological Science. 1 (2): 156–162. doi:10.1177/2167702612468111.\n- Galinsky, Douglas Boin/Karl. \"Rhetorica ad Herennium Passages on Memory\". www.utexas.edu.\n- \"Quintilian on Memory - Art of Memory Blog\". artofmemory.com. 25 November 2010.\n- Bremer, Rod (September 20, 2011). The Manual - A guide to the Ultimate Study Method (USM). Cambridge, United Kingdom: Fons Sapientiae Publishing. ISBN 978-0956990709.\n- \"New Project: Use Memory Techniques to Learn Brazilian Portuguese - Art of Memory Blog\". artofmemory.com. 18 December 2010.\n- Finger, Stanley (October 11, 2001). Origins of Neuroscience: A History of Explorations Into Brain Function. Oxford: Oxford University Press. p. 333. ISBN 978-0195146943.\n- \"\"In the First Place, in the Second Place\" - Art of Memory Blog\". artofmemory.com. 12 January 2015.\n- Donor, Timothy. \"Breaking the language barrier\". Retrieved 19 February 2015.\n- Frances Yates, The Art of Memory, University of Chicago, 1966, p1-2\n- Stephen M. Kosslyn, \"Imagery in Learning\" in: Michael S. Gazzaniga (Ed.), Perspectives in Memory Research, MIT Press, 1988, p245; it should be noted that Kosslyn fails to cite any example of the use of an equivalent term in period Greek or Latin sources.\n- John Robert Skoyles, Dorion Sagan, Up From Dragons: The Evolution of Human Intelligence, McGraw-Hill, 2002, p150\n- Linda Verlee Williams, Teaching For The Two-Sided Mind: A Guide to Right Brain/Left Brain Education, Simon & Schuster, 1986, p110\n- Elizabeth F. Loftus, Human Memory: The Processing of Information, Lawrence Erlbaum Associates, 1976, p65\n- For example, Aristotle referred to topoi (places) in which memorial content could be aggregated - hence our modern term \"topics\", while another primary classical source, Rhetorica ad Herennium (Bk III) discusses rules for places and images. In general Classical and Medieval sources describe these techniques as the art or arts of memory (ars memorativa or artes memorativae), rather than as any putative \"method of loci\". Nor is the imprecise designation current in specialized historical studies, for example Mary Carruthers uses the term \"architectural mnemonic\" to describe what is otherwise designated \"method of loci\".\n- Gutman, Sharon A. (December 1, 2007). Quick Reference Neuroscience For Rehabilitation Professionals. Thorofare, New Jersey: SLACK Incorporated. p. 216. ISBN 978-1556428005.\n- Maguire, E. A.; Valentine, E. R.; Wilding, J. M.; Kapur, N. (2002). \"Routes to remembering: The brains behind superior memory\". Nature Neuroscience. 6 (1): 90–95. doi:10.1038/nn988. PMID 12483214.\n- Parasuraman, Raja; Rizzo, Matthew (February 13, 2008). Neuroergonomics. New York: Oxford University Press. p. 139. ISBN 978-0195368659.\n- Hassabis, D.; Chu, C.; Rees, G.; Weiskopf, N.; Molyneux, P. D.; Maguire, E. A. (2009). \"Decoding Neuronal Ensembles in the Human Hippocampus\". Current Biology. 19 (7): 546–554. doi:10.1016/j.cub.2009.02.033. PMC . PMID 19285400.\n- Dresler, M.; Shirer, W. R.; Konrad, B. N.; Wagner, I. C.; Fernández, F.; Czisch, M.; Greicius, M. D. (2017). \"Mnemonic Training Reshapes Brain Networks to Support Superior Memory\". Neuron. 93 (5): 1227–1235. doi:10.1016/j.neuron.2017.02.003. PMC . PMID 28279356.\n- Zielinski, Sarah. \"The Secrets of Sherlock's Mind Palace\". Smithsonian. Retrieved 6 March 2018.\n- Harris, Thomas (2006). Hannibal Rising. United States: Delacorte Press. pp. 1–2,167,178–179. ISBN 0385339410.\n- Martinez-Conde, Susana (April 26, 2013). \"Neuroscience in Fiction: Hannibal Lecter's Memory Palace\". Scientific American.\n- Yates, Frances A. (1966). The Art of Memory. Chicago: University of Chicago Press. ISBN 0226950018.\n- Brown, Derren (2007). Tricks of the Mind. London: Transworld publishers.\n- Spence, Jonathan D. (1984). The Memory Palace of Matteo Ricci. New York: Viking Penguin. ISBN 0-14-008098-8.\n- Carruthers, Mary (1990). The Book of Memory. Cambridge University Press. ISBN 0521716314.\n- Carruthers, Mary (1998). The Craft of Thought. Cambridge University Press. ISBN 0521795419.\n- Rossi, Paolo (2000). Logic and the Art of Memory. University of Chicago Press. ISBN 0226728269.\n- Bolzoni, Lina (2001). The Gallery of Memory. University of Toronto Press. ISBN 0802043305.\n- Bolzoni, Lina (2004). The Web of Images. Ashgate Publishers. ISBN 0754605515.\n- Dudai, Yadin (2002). Memory from A to Z. Oxford University Press. ISBN 0198520875.\n- Small, Jocelyn P. (1997). Wax Tablets of the Mind. London: Routledge. ISBN 0415149835.\n- Carruthers, Mary; Ziolkowski, Jan (2002). The Medieval Craft of Memory: An anthology of texts and pictures. University of Pennsylvania Press. ISBN 0812218817.\n- Dann, Jack (1995) The Memory Cathedral: A Secret History of Leonardo da Vinci: Bantam Books 0553378570\n- Foer, Joshua (2011). Moonwalking with Einstein: The Art and Science of Remembering Everything. New York: Penguin Press. ISBN 978-1-59420-229-2.\n- Mnemonic Training Reshapes Brain Networks to Support Superior Memory, Neuron, 8 March 2017.\n- Lyndon, Donlyn; Moore, Charles W. (1994). Chambers for a Memory Palace. Cambridge, Massachusetts: The MIT Press."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:fe35ff4e-8377-4e50-b7cf-36541e5a0c24>","<urn:uuid:682158d0-42d6-4ee5-8623-d560e951d562>"],"error":null}
{"question":"I'm curious about brush maintenance - what's the proper way to clean and store both varnish brushes and artist brushes after use? Please explain!","answer":"For varnish application, foam brushes are typically disposed of after each coat. However, when using artist brushes, they must be thoroughly cleaned after use - for watercolor and gouache, rinse with lukewarm water to remove all paint residue. For acrylics or oils, it's crucial to thoroughly clean off solvents as they can dissolve the glue inside the ferrule, leading to hair loss. All brushes should be stored with the hair side up in a dry place protected from moths. They should never be left soaking in water or medium as this will bend the hairs and alter the shape of the hair tuft. Additionally, allowing dry paint to accumulate near the ferrule significantly reduces the brush's lifespan.","context":["Brushing On Varnish\nBy Sal Marino\nWhile not very easy to apply by spray application, oil based varnish is one of the easiest finishes to apply by brush. Because varnish sets-up slowly it gives the user plenty of time to brush and spread it out evenly on to the surface. It's hard to spray because it has a tendency to run if applied too heavy. I firmly believe that any film finish can be sprayed successfully if thinned out enough, but varnish is one of the last finishes I would want to spray. Over many years of testing, I have come to realize that brushing is the best way to apply oil based varnish.\nBefore applying varnish by brush, you should know a little more about how long it takes for each coat to set-up and how long before you can apply the next coat along with how it reacts to temperature and humidity and some other facts. Oil based varnish is much higher in solids than some other film finishes like lacquer. Therefore, it should only take a few coats of varnish to build a film significant enough to protect the surface of what you are finishing. After the surface has been sealed, it usually only takes about three coats to give you enough protection.\nOne very important factor when applying varnish is how the temperature effects the speed at which it cures. You should not apply varnish in temperatures lower than 65 degrees. If you apply varnish in lower temperatures it may take several days, even weeks for it to cure. Room Temp. (approx. 70 to 75 degrees) is good for applying varnish. Hotter temps. will make the varnish cure quicker, but the solvent in the varnish will evaporate quicker, making the varnish set-up quickly and you may have a problem getting the varnish to flow out properly. This could result in brush marks, bubbles and an uneven film. When working in temperatures higher than 75 degrees, try not to work on large surfaces.\nSome Tips For Preparation\nTry to set aside a room or part of your work shop to apply your varnish. This room should be as dust free as possible. Do not do any other woodworking, (especially sanding) in this area. If you are going to set aside an area of your shop instead of using a different room, it would be a good idea to also surround this area with heavy plastic sheeting. Before applying the varnish, wet mop the floor, this prevents you from kicking up any dust when you walk around. I always place clean craft (brown) paper under the piece I will be varnishing. Once the surface has been prepared properly you are ready to brush on your varnish.\nChoosing A Brush\nThere are a number of high quality brushes that can be used for brushing on clear topcoats. The best for shellac and lacquers are natural hair (like badger) or china bristle brushes. While any of these brushes will do a great job when applying varnish, there is a much less expensive alternative. A polyfoam brush. That's right, the disposable type. Oil based varnish is classified as a cold finish. This means the solvent use is not as strong as evaporative finishes like shellac and lacquer. Alcohol and lacquer thinner will melt a foam brush but the mineral spirits, solvents or turpentine used in most oil based varnishes will not harm a foam brush. Foam brushes are especially useful for novices who have a hard time getting brush marks out when applying a finish. If used properly, you can get excellent results. I always have a good supply of 1\",2\" and 3\" foam brushes in my shop. They are very inexpensive, so I use one for each coat and then throw it away.\nApplying The Varnish\nYou don't need a special sealer to seal the wood. Special sealers like sanding sealers will not do any better of a job of sealing the wood than the finish itself. Sealers only make the first coat easier to sand, thus speeding up production time. Also, if you use the wrong type of sealer, you may have adhesion problems. The best sealer for your first few coats should be the varnish itself. Take some of the same varnish you are planning to use as your finish and thin it down 50 percent ( this is a 1 to 1 ratio) with mineral spirits or gum turpentine. This will be your sealer. It will do a good job of sealing the wood and you won't have to worry about contamination problems. Pour some varnish through a paper paint strainer or stocking into another can or jar, then add the same amount of mineral spirits into the varnish. Stir well and strain a second time into a deep dish or bowl. It's best to work out of an open bowl or dish so you can easily dip your brush into it. Now, dip the foam brush into the mixture until the brush has been loaded slightly past the bevel on the foam brush. Lift the brush up and let the excess drip back into the dish. Next, brush on the first coat with the grain making sure not to leave any puddles or drips. Allow the sealer coat to dry overnight and then sand with 320 grit paper. Remove the dust with a vacuum, or tack cloth. If you are working on very porous woods, apply a second sealer coat following the previous steps.\nIt's a good idea to also thin out your coats of varnish a little. You can reduce your varnish 20 to 25 percent 4 parts varnish to 1 part mineral spirits or gum turpentine or 3 parts varnish to 1 mineral spirits or gum turpentine. This will not effect the strength of the varnish, it will only make it flow better and allow time for air bubbles that form when brushing to pop. The only drawback is that you will have to add a few more coats because less will remain on the surface once the varnish has dried. Prepare the varnish by mixing and straining in the same way you prepared the sealer. Use a foam brush and load it in the same manner as the sealer. Apply the varnish to the surface by brushing either with or against the grain initially. The main idea is to get it on the surface doing as little brushing as possible. Once on the surface take one light pass with the tip of the brush moving with the grain. Overlap each pass slightly, then leave the varnish alone, do not do a lot of brushing, this will make the solvent evaporate quicker and the varnish will set up too quickly and not have enough time to flow out. Let the varnish dry overnight, and then sand with 320 grit sandpaper and remove dust using vacuum or tack cloth. When sanding, if the varnish starts to clog the paper, it has not dried enough. If the varnish turns to powder, it is dry enough to sand and apply the next coat. Continue to apply 2 to 3 more coats of varnish using the same process. If you are going to rub out the finish (by wet sanding) after it has cured, you may want to apply at least a total of 6 coats ( not including sealer coats). This is because if there is not enough varnish left on the surface, you may cut through the finish into the raw wood in some spots.\nOnce you have applied the last coat, let the finish cure for several weeks before you are ready to use it or rub it out. Varnish does not need much maintenance. If you wish, you may apply a coat of paste wax or liquid polish from time to time.\nIf you want to rub out the varnish finish and are not familiar with the process, I am planning to put this info up on my homepage soon. However, if you need this information sooner, or have any other questions, please contact me by e-mail.","Classification of paint brushesThe artist brush is the most basic tool that the artist uses to realize his concept on paper, canvas, cloth and many other surfaces.\nIt is structurally divided into three main parts: the handle(for holding), ferrule (or capsule)that holds the hairs and the hair tuft, which is the part of the tool that interacts both with paint/ink and the surface.\nThe handle is generally made of solid wood from deciduous trees (oak, birch, beech, etc.), sometimes synthetic materials (e.g., plastic) are used. There are exclusive brushes with ivory handles, some minerals, precious stones etc.\nThe hair tuft of the brush is formed in certain size and a predetermined shape, with natural hair (tiger, pony, goat, badger, columns, bristles, etc.) or synthetics.\nThere is a division of the hands and the exhibition of the hair (ie the length of the hair in the hair bundle). Typically, the major manufacturers of brushes produce three main types of hair in the hands of the exhibition of the same size: an elongated, standard and shortened. This differentiation allows different work during a session of painting. Brush with a short exhibition conveniently perform short strokes or lighten the exact right place, washing off the ink particles from the texture of the paper. Brush standard averaged exhibitions usually performed most of the work. This brush is universal. And finally, the elongate brush insert is intended for drawing the long thin lines of constant width.\nThe most universal brush type is the \"cartrige\". The ferrule, or \"cartrige\" is a round metal part that holds the bristles together and attached to the handle of the brush. The base coat is fixed with adhesive and sealant is further applied. The ferrule must adhere to the handle, they are bonded with a special machine. When some hairs are constantly falling off, this usually means that there was not enough glue, or it was badly applied.\nThe shape of the cartridge brushes can be divided into two main groups: round (the cross section of the cartridge at its end iscircular) and flat (the cartridge is where it holds the hairs).\nround bristle brushRound brushes - perhaps the most versatile of all the brushes. Depending on the tilt and pressure it is possible to apply paint and draw lines as constant and variable width in a particular area, to carry out broad, thick strokes or exactly the opposite - the thinnest line with its very tip.\nThe round brush is ideal for drawing as it can be used for fine detailing.\nA specific example of round brushes are the font and line brushes. gives a definite shape, ensures the safety of the hair bundle and connects it with a pen. It is usually metal but may be made of other materials. Most are easy to use, practical, reliable and attractive\n|flat brush, parts of the brush|\nThey are very convenient for representing the pictorial elements in nature such as land, grass, leaves, as well as architectural compositions of urban landscapes and anything that requires a relatively precise outlines. This brush is characterized by that the capsule is flattened at the end, allowing the hair to form the desired flat plane - hence the name. Thick, long hair flat brush allows you to keep a sufficiently large amount of paint.\nType of flat brushes, that have a shorter hair tuft, giving extra firmness to the stroke. This ensures high quality flat strokes. Thanks to the short hair, with the help of contour brush it is very convenient to make the outlines of drawing or, for example, graphic works and can also be used to create smooth color transitions.\nThis brush type, a variety of the flat brush, got it's name due to the special shape of the hair bundle - a dome or oval shaped. It is versatile and it allows, depending on the position and inclination towards the working surface (canvas etc.)to perform strokes, characteristic for both circular (at a perpendicular position to the surface), and for the flat brush.\nThis is a kind of round brush, that has\nvery long hair. This form and the length of the hair bundle the main purpose of the brush (hence the name) is the ability of thin straight lines without reloading. Very handy brush for graphic works, but using to its full potential in the process of painting, it takes much practice and experience.\nFan brushFan brushes are a kind of flat brushes that got its name due to the special shape of the beam, that looks like a fan. Due to this they are very convenient to perform sophisticated color mixing, soften the contours and create contrasting textures . Just one swab allows you to put a lot of fine lines, for example, when drawing grass, etc. Working with it requires a certain skill.\nThere are three main characteristics that must be considered when assessing the quality of a brush. You can easily separate the quality brushes from cheap low-quality fakes. There are three main characteristics that make a good artist brush:\n1. The ability of the hair bundle to form a sharp tip. This indicator is very important, because in the creative process one often needs to apply very precise strokes, fine lines, very fine detail etc. If your brush will is not able to perform these tasks, because edge of the beam is too thick or the hairs are scattered among themselves, then it simply hinders your work.\nWhen pulled up from the painting surface, its hairs should retake their original form and be ready for the next stroke.\n2. The elasticity of the hair bundle. For quality brush strokes, it is important that the hairs have certain elasticity. This allows various effects to be achieved, that cannot be easily done in another way, e.g. scrubbing in etc.\n3. Control of the fluidity of paint. When working with dilluted paint, it should flow from the hair brush with the desired consistency, in a controlled manner.\nFew tips for choosing a brush.\nFirst determine which technique you are going to work in, meaning what paint are you going to use. The thicker the paint, the harder the hair of the brush should be. For watercolor, the brush from squirrel hair is suitable. For gouache - synthetic. For acrylic - synthetic, Kolinsky brushes, for tempera - synthetic, bristle, badger, oil - bristle, and other synthetics. Of course, your choice should mostly be based on personal experience and preference.\nEach brush has different size and mark-making properties. Typically, the size is denoted by a number indicating the diameter of the ferrule. However, this marking is not always universal, especially if the brush manufacturers are from different countries. Therefore, choosing a brush pay attention not only to the number, but gauge the size \"by eye\" to see if the brush suits your needs.\nSo, now that you have decided on the material and the size of the brush, you should check its quality. Start with the ferrule. It should be evenly and firmly fixed to the handle, not hanging out and not sliping or moving on it. Such a brush will be uncomfortable for work, it will be very difficult to perform the desired shape and stroke with a predetermined pressure. In addition, during painting will the brush will shed hair that are left on your canvas/paper.\nIt is also important to check the elasticity of the brush. Press it against your palm, to see how hard/soft it the hair is, and if the brush retains its shape after applying pressure on it.\nMoisten the brush with water ( wait for a while until the hair is fully saturated) and lightly tap your finger on the handle. A good round brush tip is to sharpen. Try now without departing from the surface to draw Arabic numeral \"eight\" (if there is paper,try on it, if not - on your palm). If the line has a smooth, uniform thickness without breaks and spray, that means you have a good quality brush. Additionally, note the individual hairs in the bundle. The hair should have natural ends and not be cropped.\nCare for your brushes.\nBrushes require careful attitude. Rough handling (e.g., roughly pressing a watercolor brush in the bottom of the water tank) can easily lead to damaged brush, sometimes even making it unusable.\nDo not leave the brush in the water or medium, this will bend the hairs and change the shape of the hair tuft. After finishing work, the brushes should be thoroughly cleaned of paint and put in a container (jar, vase, round box, etc.) for storing brushes, with the hair side up. The accumulation of dry paint on the hair of the beam near the ferrule and inside it, significantly reduces the life of the brushes, so after painting session always thoroughly clean the brush.\nAfter working with watercolor and gouache, rinse the brushes well with lukewarm water, making sure that there is no paint residue left.\nIf you are working in acrylic or oils, keep in mind that poorly washed solvent will accumulate inside the ferrule, and it can dissolve the glue inside, which leads to hair loss and a worthless brush.\nKeep the brush in a dry place protected from moths to avoid damage to the hair, and in order for the hair tuft to retain its shape.\nAll brushes differ, depending on the type of their hair. Here are some examples of the most common hair types.\nNatural fiber brusehs\nBest group of brushes for watercolor. Squirrel hair retains its shape, does not break, perfectly absorbs and releases moisture, and are easy to clean under running water. Squirrel brush allows you to draw smoothly without uneven strokes and overlay blurry spots. This brush is indispensable when working in glazes. The very soft hair allows you to overlay one layer of paint on top of the dried one, without damaging it. However, this brush will not work if you want to leave a strong brushstroke by pressing the paint in the texture of paper/canvas, as squirrel hair is relatively soft.\nThe best squirrel hair is obtained from the Siberian squirrel.\nThose use natural hair pig. • Top quality bristle brushes are those that are made from the hair of Chinese angkingskogo hog. These hair bristles are hard and resilient. It is perfect for use with thick paint (eg, acrylic, oil).\nHowever, if you decide to work with a bristle brush in watercolor, be advised that this hair is hard enough to not only destroy the paint layer (operating glaze), but also damage the basic structure of the paper! So please be careful and cautious.\nBrushes made of synthetic hair\nThanks to the innovation in technologies a unique synthetic fiber has been created - polyester.Special technology allows it to have the necessary form, identical to the hairs from natural origin. Synthetic hair show excellent quality -fluidity, elasticity, rigidity, all of the qualities needed for artist brushes.\nIt is important to note that the synthetic brushes, unlike natural, are of different stiffness (designation: stiffness icons 1 to 4) and the elasticity depending on the purpose. Very hard (4, exceeds the elastic stiffness of the hair bristles), hard (3 is similar to bristles) and average (2, similar in stiffness with column) synthetics superbly suits working with acrylic and oil, and the soft synthetics (1) are suitable for gouache.\nNote that the synthetic hair brushes are readily available for purchase and are much cheaper than their natural counterparts, although still inferior to them in some ways."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:a3176c52-91ea-4851-8621-5ca4c07cfb8a>","<urn:uuid:14a2444f-ebfe-4e5d-ae1d-9908c73c42e9>"],"error":null}
{"question":"Could you compare and contrast the research focus areas of Dr. Si Kao and Dr. Duan Hongbo in relation to their work on perception and modeling?","answer":"Dr. Si Kao focuses on perception research, particularly how people's perception of future events affects their memory of past experiences. His research revealed that events seem more distant when unlikely to reoccur. Meanwhile, Dr. Duan Hongbo specializes in energy-economy-environmental (3E) integrated modeling, developing models like E3METL and CE3METL to analyze climate policies and energy technological diffusion. While both conduct modeling work, they operate in different domains - psychological perception versus environmental systems.","context":["How Our Perceived Future Influences Our Past\nVivid memories and emotions about things affect our perceptions of when they took place, but can the chance of them happening again also influence how we regard the past?\nBy Si Kao, PhD Candidate, Department of Marketing, CUHK Business School\nThose of us who watched the Hollywood hit movie, La La Land, which focuses on a bitter-sweet love affair between a musician and an aspiring actress, may have been reminded of experiences we had with a former boyfriend or girlfriend.\nWhen talking to colleagues and friends, they all agreed that their past experiences with their former partners – whether taking a vacation in Bali, for example, or going out for dinner, having a fight, or just a conversation – feel particularly remote now. Why is this?\nResearch has shown that the vividness of a past trip and the emotions associated with it can influence people’s perceptions of when it took place. But can our perceived future also influence our past memories?\nThe findings of my published paper, “Looking Forward and Looking Back: The Likelihood of an Event’s Future Reoccurrence Affects Perceptions of the Time It Occurred in the Past”, in collaboration with Visiting Professor Robert Wyer and Associate Professor Dai Xianchi from the Department of Marketing at CUHK Business School, may provide some useful answers to these questions.\nLooking at a past trip and its possible reoccurrence\nIn our research, we considered another determinant – the likelihood of the event occurring again. Specifically, we suggested that people consider past events to be more distant if the chance of their reoccurrence is low.\nA past vacation in Bali, for example, seems more remote to people if they will never go there again, rather than if they are likely to return next year.\nWe based such a hypothesis on two assumptions: First, that people believe a future event to be more imminent if there is high – rather than low – likelihood of it occurring. Second, people’s perceptions of when a former event will reoccur have a positive influence on their judgments about when it previously took place.\nMore than 1,300 people took part in six studies. In one of the studies, we asked participants to recall the most recent New Year’s Eve they spent in a location other than their hometown or place of residence and write down where they went, the people they were with and what they did.\nWe also asked them to rate how long ago it felt since the event occurred, using a scale from 1 (feels like yesterday/feels very close) to 9 (feels very far away/feels very distant); the likelihood of them returning there and meeting the same people, also on a scale from 1 (not at all feasible) to 9 (very feasible); and things such as how vivid and emotional their memories were.\nAs a result, we found evidence that participants regarded a past event that was unlikely to reoccur as being more distant.\nIn another study, participants were questioned about a past vacation and how distant it felt to them today; they were also asked to estimate the number of years before they would take another trip to the same place.\nWe found that participants who were made to think that the future holiday was unlikely ended up estimating a longer period till when they would make a return trip. They also estimated the time since their past vacation to be longer when they felt it was very unlikely that the future holiday would reoccur.\nIn the final study, involving fixed events in the future, took place about 12 months before the 2016 Rio Olympics. We manipulated participants’ judgments about how long it was until the start of the next Summer Games by first focusing their attention on either a near or a distant sports event.\nHalf of the participants were asked to think first about the coming NBA finals – only three weeks away at the time – which led them to regard the approaching Olympics as relatively distant.\nThe other half was asked to think first about the 2018 FIFA World Cup in Russia – then three years away – which led them to view that the forthcoming Olympics as closer.\nParticipants were then asked to indicate how long it seemed since the last Olympics had taken place.\nLogic might suggest that the more imminent the next Olympics are then the more time that has elapsed since the previous Games. But people’s answers contradicted this: The farther away participants felt the next Olympics to be, the farther away they also felt the last Olympics to be.\n“People may want to distance themselves from a past event that was desirable, but was nevertheless unlikely to occur again, to reduce feelings of wistfulness.”\nSo What Did We Discover?\nResearch has shown that people’s memories and appraisal of the past can be influenced by their current goals, motivations, and beliefs.\nHowever, to our knowledge few studies have investigated whether people’s perceptions of the future can affect their judgments of the time an event occurred in the past.\nOur studies found that a past event is perceived to have occurred more recently if it is likely – rather than unlikely – to reoccur.\nThe results suggest an event’s likelihood of reoccurring influences the perception of the time it will occur in future and that this perception, in turn, affects judgments about when the event took place in the past.\nIt should be noted that we focused our studies almost exclusively on positive events, not things that could stir up negative feelings, such as a trip to the dentist or a car accident.\nAlso special or extraordinary personal events, such as a wedding, were not examined. Based on the current findings, we speculate that the observed effect may be reduced, for both negative and special events, because people rarely anticipate or consider they will reoccur in the future.\nOther explanations may also explain our current findings. One interesting idea is that people may want to distance themselves from a past event that was desirable, but was nevertheless unlikely to occur again, to reduce feelings of wistfulness. Such an alternative suggestion is not incompatible with the findings of our research.\nPractical Implications of Our Findings\nOur research could prove useful to marketers and retailers in future advertising campaigns because of the crucial role that memory and time perception play in shaping consumers’ decisions.\nOne study showed that under some circumstances a person’s desire to do something – perhaps take an exotic holiday – becomes stronger if they feel it is a long time since they last indulged.\nYet our research highlighted a previously ignored subtlety of marketing. Often great efforts have been taken to emphasize just how easy it is to do something, but the studies suggested that doing so could also backfire: when people perceive they are highly likely to indulge in something again they may feel more closely connected to these past events and are not so drawn to try them again in future.\nOur studies also indicated that some form of vicious cycle may negatively affect the effectiveness of consumers who try to self-regulate their behavior – specifically those who choose to set their own rules or constraints. Such consumers may want to manage their finances, for example, by limiting the frequency and amount they spend on holidays, or deliberately buy fewer cigarettes each time they run out as part of efforts to stop smoking.\nSuch self-imposed limits could be effective, but our studies suggest that they may also backfire. As the constraints on consumers imply a reduced likelihood of the event – such as taking holidays or smoking cigarettes – in the future, people may feel the last time they indulged in the activity to be more distant, and therefore finding it more alluring. So the restrictions these people give themselves may actually make it easier for them to justify their indulgent behaviors – and encourage them to do it again.\nCoordinator, MPhil and PhD in Marketing Programme","Name： DUAN Hongbo\nTitle： Associate Professor\nEnergy-Economy-Environment(3E) Integrated Modelling\nClimate Policy Analysis\nEnergy Technological Diffusion\nDr. Duan’s research interests focus on energy-economy-environmental (3E) integrated modeling, climate policy analysis and energy technological diffusion and marketing. His projects include establishment and development of 3E-integrated model at both global and national levels, analysis of long-term energy demand and CO2 emissions, integrated assessment of China’s climate vulnerability and damages, policy exploration of clean energy technological diffusion, and energy sustainable development.\nDr. Duan is the leading developer of the famous 3E-integrated model, E3METL and CE3METL, which are important members of CEMF model set (China Energy Modeling Forum); He has published over 50 peer-reviewed papers, several of which have been reported by famous international medias, like Advances in Engineering, IAEE newsletter, VerticalNews and Atlas of Science, etc., and one of which enters the 2018 outstanding research list of “Change the World, One Article at a Time” initiated by Springer Nature. He was invited to write a review article on IAM-based multi-model comparison analysis for Environmental Research Letters. His research has been financially supported by many key governmental sectors, such as National Natural Science Foundation of China (NSFC), Ministry of Education, Ministry of Science and Technology, Development Research Center (DRC) of State Council, Chinese Academy of Sciences (CAS).\nPh.D., Institute of Policy and Management, Chinese Academy of Sciences, 2013; Main field of research: Energy & Environmental Economics, Dissertation: “Modeling Energy, Economy and Environmental Integrated Systems and Renewables’ Diffusion”\nM.S., Kunming University of Science and Technology, 2010; Main field of research: Ordinary Differential Equations (ODE) in Mathematics\nB.S., Hulun buir University, 2007; Main field of study: Applied Mathematics\nDepartment of Economics, University of Kansas (KU) (Jan. 2018 to Jan. 2019), Visiting Scholar\nCollins College of Business, University of Tulsa (TU) (Nov. 2018), Visiting professor\nCenter for Energy & Environmental Research (CEEP), Institutes of Science and Development. (2013.7 to present), Research fellow\nSchool of Economics and Management, University of Chinese Academy of Sciences (UCAS) (2017.11 to present), Associate Professor\nSchool of Economics and Management, University of Chinese Academy of Sciences (UCAS) (2016.7 -2017.11), Assistant Professor\nSchool of Economics and Management, University of Chinese Academy of Sciences (UCAS), (2013.7 -2016.7), Affiliated Post Doctorate\n“Artificial selection vs. natural selection: Which causes the Matthew effect of science funding allocation in China”, with Xiao Wang, Gupeng Zhang and Jianing Dong, Science & Public Policy, 2020. (to appear)\n“Coronavirus: Limit short-term economic damage”, with Shouyang Wang and Cuihong Yang, Nature, 578: 515-515. http://doi.org/10.1038/d41586-020-00522-6.\n“Modeling deep decarbonization-Robust energy policy and climate action”, with Joeri Rogelj, Jason Veysey and Shouyang Wang, Applied Energy, 2020, 262: 114517.\n“How does the collaboration with dominant R&D performers impact new R&D employees’ innovation performance in different cultural contexts? A comparative study of American and Chinese large firms”, with Gupeng Zhang and Xiao Wang, Technological Forecasting & Social Change, 2019, 148: 119728.\n“Nonlinear globalization threshold effect of energy intensity convergence in Belt and Road Countries”, with Zhilin Huang and Hong Zhang, Journal of Cleaner Production, 2019, 230: 117750.\n“The global warming-rise target from 2C to 1.5C: The challenge for China”, with Shouyang Wang. Management World, 2019. (to appear)\n“Multi-renewable management: Interactions between wind and solar within uncertain technological ecological system”, with Yongqiang Du, Bingjie Song, Tsvetan G. Tsvetanov and Yingying Wu. Energy Conversion and Management, 2019, 187: 232-247.\n“Integrated benefit-cost analysis of China’s optimal adaptation and targeted mitigation”, with Gupeng Zhang, Shouyang Wang and Ying Fan, Ecological Economics, 2019, 160: 76-86.\n“Review on robust climate economic research: Multi-model comparison analysis”, with Gupeng Zhang, Shouyang Wang and Ying Fan, Environmental Research Letters, 2019, 14(3): 033001.\n“Role of policy synergies in achieve China’s IND targets”, with Jianlong Yang, Journal of Environmental Economics, 2018, 2: 11-27.\n“Ownership unbundling of natural gas transmission networks in China”, with Jichang Dong, Siying Sha, Xiuting Li and Wei Dai, Journal of Cleaner Production, 2018, 195: 145-153.\n“China’s energy and climate targets in the Paris agreement: Integrated assessment and policy options”, with Jianlei Mo, Shouyang Wang and Ying Fan, Economic Research Journal, 2018, 9: 168-181.\n“Examining the absolute advantage of energy technology between China and the developed countries”, with Gupeng Zhang, Shouyang Wang, and Qianlong Zhang Energy, 2018, 162: 1223-1233.\n“Potential impacts of China’s climate policies on energy security”, with Shouyang Wang, Environmental Impact Assessment Review, 2018, 71: 94-101.\n“Cross-national interaction and learning: A two-dimensional technological diffusion framework”, with Shouyang Wang, Ying Fan, Gupeng Zhang, Journal of Business Research, 2018, 89: 57-66.\n“Achieving China’s energy and climate policy targets in 2030 under multiple uncertainties”, with Jianlei Mo, Shouyang Wang and Ying Fan, Energy Economics, 2018, 70: 45-60.\n“The impacts of climate policies on energy security in carbon-restrained China”, with Shouyang Wang, IAEE Energy Forum, 2017, 4: 42-43.\n“Balancing China’s climate damage risk against emissions control costs”, with Gupeng Zhang, Shouyang Wang and Ying Fan. Mitigation and Adaptation Strategies for Global Change, 2018, 23(3): 387-403. doi: 10.1007/s11027-017-9739-y.\n“The timing of China’s carbon peaking under an uncertain future”, with Jianlei Mo, Ying Fan and Shouyang Wang, IAEE Energy Forum, 2017, 1: 49-51.\n“Network stability, connectivity and Innovation output”, with Gupeng Zhang and Jianghua Zhou, Technological Forecasting & Social Change, 2017, 114: 339-349.\n“Role of endogenous energy efficiency improvement in global climate change mitigation”, with Gupeng Zhang, Ying Fan and Shouyang Wang, Energy Efficiency, 2017, 10(2): 459-473.\n“Impacts of transaction costs on cost-saving effect of carbon trading market in China”, with Lianbiao Cui and Jinhua Xu, Management Review, 2017, 29(6): 23-31.\n“Emissions and temperature benefits: The role of wind power in China”, Environmental Research, 2017, 152: 342-350.\n“Regional opportunities for China to go low-carbon: Results from the REEC Model”, with Lei Zhu, Gürkan Kumbaroglu and Ying Fan, The Energy Journal, 2016, 37: 223-252.\n“Investigating Determinants of Inter-Regional Technology Transfer in China: A Network Analysis with Provincial Patent Data”, with Gupeng Zhang and Jianghua Zhou. Review of Managerial Science, 2016, 10(2): 345-362.\n“How will diffusion of PV solar contribute to China’s emission-peaking and climate response?”, with Gupeng Zhang, Lei Zhu, Ying Fan and Shouyang Wang, Renewable & Sustainable Energy Reviews, 2016, 53: 1076-1085.\n“China’s carbon reduction structure: An analysis based on endogenous energy efficiency improvement”, with Gupeng Zhang, Ying Fan and Shouyang Wang, Journal of Management Science in China, 2016, 19(7): 10-23.\n“Analysis of cost and technological diffusion of carbon capture and storage for China: Based on Chinese energy economic endogenous technological model”, with Lei Zhu and Ying Fan, System Engineering: Theory & Practices, 2015, 35(2): 333-341.\n“Modelling the evolutionary paths of multiple carbon-free energy technologies with policy incentives”, with Lei Zhu and Ying Fan. Environmental Modeling & Assessment, 2015, 20(1): 55-69.\n“CO2 mitigation potential of CCS in China – An evaluation based on an integrated assessment model”, with Lei Zhu and Ying Fan, Journal of Cleaner Production, 2015, 103: 934-947.\n“Small worldliness, Chinese culture and firm innovation performance: An empirical study based on patent collaboration data of China”, with Gupeng Zhang and Jianghua Zhou, Asia Journal of Technology Innovation, 2015, 23(2): 189-204.\n“How do prolific inventors impact firm innovation in ICT: Implications from patent co-inventing network”, with Gupeng Zhang and Xiaofeng Lv, Technology Analysis & Strategic Management, 2014, 26(9): 1091-1110.\n“A cross-country study on relationship between diffusion of wind and photovoltaic solar technology”, with Lei Zhu and Ying Fan, Technological Forecasting & Social Change, 2014, 83: 156-169.\n“Optimal carbon tax pathways in carbon constrained China-A logistic-induced energy economy hybrid model”, with Lei Zhu and Ying Fan, Energy, 2014, 69: 345-356.\n“Review on the integrated assessment model of energy-environment-economy for the global climate change”, with Lei Zhu and Ying Fan, Journal of System Engineering, 2014, 29(6): 852-868.\n“Modeling for new energy endogenous technological change under action of emission reduction policies”, with Ying Fan, Minglei Liu and Lei Zhu, Journal of System Engineering, 2014, 29(6): 763-770.\n“What is the most cost-effective policy of CO2 targeted reduction: an application of aggregated economic technological model with CCS?”, with Ying Fan and Lei Zhu, Applied Energy, 2013, 112: 866-875.\n“China’s wind technology development: The diffusion patterns and policy analysis”, with Lei Zhu and Ying Fan, Journal of Management Science, 2013, 26(4): 113-120.\n“International crude oil market trend analysis and price forecast in 2012”, Qiang Ji, Lei Zhu, Jianlei Mo, Jinhua Xu and Ying Fan, Bulletin of Chinese Academy of Sciences, 2012, 27 (1): 44-49.\n“Existence of weak nontrivial homoclinic orbits for second order impulsive differential equations”, with Hui Fang, Boundary Value Problems, 2012 (138), doi: 10.1186/1687-2770-2012-138.\n“Existence of weak solutions for second-order boundary value problem of impulsive dynamic equations on time scales”, with Hui Fang, Advances in Difference Equations, 2009. Article ID 907368,16 Pages doi: 10.1155/2009/907368.\nBook chapter: China’s Energy Revolution in the Context of the Global Energy Transition, Chapter 2: “China’s Energy Consumption Revolution”. Beijing: China Development Press, 2019.\nBook: Energy System Integrated Modeling: Decarbonization with Policy Driven, Co-edited with Ying Fan, Beijing: Science Press, 2017.\nBook: Special Report: Research on low-carbon management and development, Co-edited with Ying Fan and Lei Zhu, in Report on Advances in Management Science and Engineering, Beijing: China Science and Technology Press, 2014.\nBook chapter: Chapter 15: Evolution of energy technologies with policy incentives, in China Energy Security: A Perspective of Management Science, Beijing: Science Press, 2013.\nKeynote Speech “China IAM-based Multi-model Comparison Analysis”, the 6th Annual Conference of China Energy Modeling Forum (CEMF), Beijing, Jan. 8, 2020.\nInvited Parallel speech “Multi-model Analysis on China’s Challenges Facing 1.5C Warming Limit”, Summit Forum of Energy Strategy and Innovation Development, China University of Petroleum, Beijing, Dec. 4, 2019.\nInvited Parallel speech “Estimating China’s historical climate-economic relationship”, the 2nd Symposium of Environmental Economics and Policy Assessment, Xiamen University, Xiamen, Nov. 19-20, 2019.\nInvited session presentation “Non-standard scheme and diffusion of electric vehicles”, 2019 INFORMS Annual Meeting, Seattle, United States, Oct. 20-23, 2019.\nSession Chair of “Energy Materials & Technology” and IAAM Scientist Medal Lecture “Uncertain entanglements between photovoltaic and wind technology penetrations”, Stockholm, Sweden, Oct. 9-11, 2019.\nDiscussant and session presentation “Multi-renewable Interactions: Relationships, equilibrium stabilization and short-term forecasts”, the 24 Annual Meeting of the European Association of Environmental and Resource Economists (EAERE), Manchester, UK, Jun. 26-Jun. 29, 2019.\nSession Chair of “Energy Market, Technology Innovation and Economy Decarbonization”, 2019 China Conference of the Chinese Economist Society (CES), Dalian, China, Jun. 7-9, 2019.\nInvited panelist for the 38th International Energy Workshop (IEW2019), Jun. 3-5, Paris, France, 2019.\nSession Chair and oral presentation “How will globalization and financial development contribute to energy consumption reduction”, 2019 International Conference on Energy Finance (ICEF), Kunming, China, May 18-19, 2019.\nPanelist and plenary speech “Assessing China’s energy and economic risks under the 2-degree warming-limit target: Multi-model comparison analysis”, International Symposium for Global Climate Integrated Assessment Model, Beijing, China, May 13, 2019.\nInvited talk “Potential interactions between proliferation of wind and solar PV within uncertain technology ecological system”, Stockholm Environment Institute (SEI) at Tufts University, Boston, USA, Oct. 2, 2018.\nInvited talk “Valuing China’s economic impacts of climate change”. University of Maryland, College Park, Maryland, USA, Sep. 24, 2018.\n“Entanglements between diffusion of wind and solar within uncertain technology ecological system”, the 2018 United Stated Association for Energy Economics (USAEE/IAEE) North American conference, Washington DC, USA, Sep. 23-26, 2018.\nKeynote speech “E3METL: attempts to bridge top-down and bottom-up modeling framework”, Annual meeting of China Energy Modeling Forum (CEMF), Beijing, Nov. 11-12, 2017.\n“The potential relationships between China’s climate policies and energy security”, 2017 Research Links: Workshop on emission control and sustainable energy in green port development, Dalian, China, Aug. 26-28, 2017.\n“Policy effectiveness assessment of China’s optimal adaptation and targeted mitigation”. The 36th IEW International Meeting, College Park, Maryland, USA, Jul. 12-14, 2017.\n“The timing of China’s carbon peaking under an uncertain future”. The 1st IAEE Eurasian Conference, Baku, Azerbaijan, Aug 27-31, 2016.\nSession Chair and presentation “Mitigation and adaptation: Climate damage risk in China”. The 2nd IEEE Symposium on Analytics and Risk, Beijing, China, July 8-9, 2016.\nKeynote speech: “Cooperativity for reaching China’s carbon-peaking and non-fossil energy development targets”. The 2st Energy Environment & Modeling Frontier Forum, Nanjing, China, May 13-14, 2016.\n“Quantifying China’s warming benefit of substituting wind power for coal-fired electricity in mitigating climate change”. IEEE Symposium on Analytics and Risk, Beijing, China, Aug 14-15, 2015.\nStaff for the 4th IAEE Asia Conference, and with oral presentation “How will emissions trading scheme affect the China’s long-term evolution of carbon-free technologies?” Beijing, China, Sep 19-21, 2014.\n“The role of endogenous energy efficiency improvement (EEEI) in climate change abatement”. The IEW2014 International meeting, Beijing, China, Jun 4-6, 2014.\nStaff for “Theoretical advances and empirical lessons on emission trading schemes”. Beijing, China, Oct 10-11, 2013.\n“Optimal carbon taxes in carbon constrained China-a hybrid energy economy environmental model.” The 36th IAEE International Conference, Daegu, Korea, Jun 16-20, 2013.\n“What’s the most cost-effective policy of CO2 targeted reduction?” The 4th ICAE Annual Meeting, Suzhou, China, Jul 5-8, 2012.\n“How Policies Stimulate the Evolution of Carbon-free Energy Technologies?” INFORMS International Meeting, Beijing, China, Jun 24-27, 2012.\n—Invited Referee for Nature, Nature Climate Change, Energy Journal, Energy Economics, Energy Policy, Computers & Industrial Engineering, Technological Forecasting & Social Change, Energy Conversion & Management, Applied Energy, China Economic Review, Environmental Research, Emerging Markets Finance and Trade, Mitigation & Adaptation Strategies for Global Change, Frontiers for Energy Research, Resources, Conservation & Recycling, Energy, Environmental Research Letters, Technovation, Sustainability, Energies, Journal of International Economics, Resources Policy, Science of the Total Environment, Journal of International Economics, Production & Operations Management, Regional Environmental Change, Global Food Security, Journal of Cleaner Production, Environmental & Resource Economics, Energy Informatics, International Journal of Revenue Management, PNAS, International Journal of Energy Sector Management, Corporate Social Responsibility and Environmental Management, Journal of Environmental Management, Journal of Business Research, Elsevier Science & Technology Books, and many Chinese leading journals.\n—Associate editor of International Journal of Revenue Management\n—Managing guest editor of Applied Energy.\n—Managing guest editor of Mitigation and Adaptation Strategies for Global Change.\n—Managing guest editor of Regional Environmental Change.\n—Guest editor of Energy Economics.\n—Editorial board members of many other journals, like Energy Informatics, Sustainable Futures, JENR.\n—Advisory board member of Cambridge Scholar Publishing Book Series: Economics.\n—Member of Scientific Advisory Board for IAAM.\n—Member of IAEE, EAERE and INFORMS."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:bab67f99-011f-4226-b67e-50993f4486fa>","<urn:uuid:0f1a5f24-4b63-4206-8c47-d73dc21ccf13>"],"error":null}
{"question":null,"answer":null,"context":null,"question_categories":[],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":null,"error":"Conversation generation aborted while generating turn 1: It is impossible to generate a valid next turn even after 3 trials."}