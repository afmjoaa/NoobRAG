{"question":"¿Cómo ha evolucionado historically la tecnología de espectroscopía desde los prismas hasta modern spectrometers, and what are the key training requirements for safe operation?","answer":"Early spectrometers used prisms to separate wavelengths of visible light, but scientists found these and early 1900s gratings were slow, noisy, and lacked resolution when studying mid-infrared regions. This led to Michelson's interferometer designs. By the late 20th century, modern dispersive spectrometers emerged using highly engineered gratings with thousands of precise parallel grooves to separate light wavelengths. Regarding training requirements, despite instruments being designed to be intuitive, specific application training is essential for safe operation. Inadequate training has led to fatal accidents due to misinterpretation of readings or lack of understanding of environmental influences and technological limitations. For example, documented incidents have occurred due to improper instrument use and inadequate understanding of limitations, such as incorrect LEL readings in low oxygen environments. Comprehensive application-based training is crucial as each instrument has its own set of sensors, operating procedures, and instructions that all users must understand.","context":["Spectroscopy is the study of how light and matter interacts. Light can be broken up into several regions as illustrated in the electromagnetic spectrum in Figure 1.\nSpectroscopy is perfectly suited for accurate real-time analysis and monitoring of continuous and batch processes. The near infrared (NIR), as well as the ultraviolet and visible (UV-VIS) regions of the spectrum provide a wealth of information about many chemical and physical properties to help operators control their processes better. No other technology matches the acquisition speed and the range of measurements returned by a fiber optic-based spectrometer. A comprehensive set of data that takes hours of laboratory analyses to acquire can be available in less than a minute.\nFigure 1: The electromagnetic spectrum between 190 nm to 25,000 nm\nEarly spectrometers used prisms to separate the wavelengths of visible light. However, as scientists moved from studying visible and near-infrared light to the mid-infrared region, they discovered that prisms and the gratings of the early 1900s were slow, noisy, and lacked resolution. The solution for mid-infrared spectroscopy was the interferometer designs of Michelson.\nHowever, in the late 20th century the electronics, optics, and other components inside a spectrometer system improved. Modern dispersive spectrometers use highly engineered gratings to separate the wavelengths of light and reduce stray light. A diffraction grating is essentially an aluminum-coated mirror with thousands of parallel and equally spaced grooves etched into its surface. As such, a grating is one of the most precise objects ever made. How gratings work is described in all good freshman level college physics textbooks.\nIn a peer-reviewed study, Coates found that both Dispersive Analyzers (DG-NIR) and Fourier Transform Spectrometers (FT-NIR) have equivalent performance (Coates, 1994 NIR news DOI:10.1255/nirn.250). This study shows that with modern technology many of the marketing claims by manufacturers of the advantageous of FT-NIR Spectrometers are no longer valid. How do you decide which spectrometer technology is the best choice for your analysis?\nWhich Technology is a Better Choice?\nFT-NIR is a powerful technique, especially in a laboratory setting where samples can be introduced to the spectrometer without using optical fiber. FT’s also work well for measuring samples in reflection such as powders or solids. However, when considering NIR for a process application that involves a “clear” liquid or gas using optical fibers, FT-NIR instruments lose many of their advantages.\nThe bandwidth of spectral features varies with the state of the sample. Gases and vapors have spectral features (lines) that are very narrow and often require high resolution to see. Liquids and solids have very broad spectral features (bands) due to the hindered rotation of the molecule in its matrix. Thus for clear liquid hydrocarbon samples (the bulk of NIR applications), bandwidths range from 2 nm to 12 nm. Aqueous samples are even broader. Thus for condensed samples, the higher resolution of FT-NIR is not required nor desirable since high resolution always comes at the cost of signal-to-noise ratio (SNR). The latter property for chemometric analysis of complex hydrocarbon mixtures is more important for accurate analysis than resolution.\nA scanning grating dual-beam (DG-NIR) spectrometer actually produces a superior signal-to-noise ratio (i.e., sensitivity for lower detection limits) and greater stability to withstand ambient air fluctuations. In addition, the dual-beam operation provides the added benefit of superior long-term stability required of process analyzers that must operate unattended for weeks. Dual-beam operation is easy to implement in a grating instrument but difficult in an FT-NIR analyzer. Most FT-NIR analyzers do not operate in dual-beam mode hence require more frequent referencing to compensate for photometric drift.\nNear-Infrared (NIR) is the region of the electromagnetic spectrum from 750 nm to 2600 nm. Molecules containing C-H, O-H, and N-H bonds absorb NIR radiation in specific\nregions or at specifi c wavelengths. Each molecule has a unique spectrum. These absorbances are used to measure (predict) chemical and physical properties of the sample. Applications for NIR (n-line, at-line and laboratory) spectroscopy are numerous and cover a wide range of industries included but not limited to: chemical, refining, pharmaceuticals, polymer, semiconductor, and agricultural.\nFourier transform (FT-NIR) spectroscopy does not record the spectrum of the sample directly but records an interferogram based upon time domain measurements. A spectrum is produced by performing a Fourier transform of the recorded interferogram. Albert Michelson knew how to do this but lacked the recording electronics and computers we now have.\nThis uses Dual-Beam, Dispersive Grating technology for NIR measurement. This state-of-the-art design eliminates, or minimizes many of the problems associated with previous grating spectrometer designs, while retaining Guided Wave’s time proven dual-beam design and built in multiplexing. For this technical note, we will describe a scanning grating system that incorporates the following features: post-dispersion, dual-beam, high efficiency blazed plane grating with on-axis aberration free, high optical throughput transmissive optics.\nA single spectrum consists of two scans, one for the sample channel immediately followed by a reference channel scan, i.e. dual-beam spectroscopy. The ratio of these two scans provides a stable spectrum. Dual-beam operation in a spectrometer removes nearly all common mode drift problems in detector, lamps and electronics. This reduces the required frequency of collecting reference (zero) spectra from hours to weeks which also reduces operator involved maintenance. It also improves baseline stability and spectral quality. Beam switching between sample and reference fibers or between sample channels does not involve moving optical components, hence there is no noise introduced in the dual-beam or multichannel operation.\nAdvancements in Spectroscopy\nSince Coates’ report in 1994, dispersive analyzers have taken on another level of advancement with the development of Guided Wave’s full spectrum post dispersive planar grating, dual-beam (DG-NIR) technology. These advancements applied to NIR process\nanalyzers will ensure accuracy due to the reduction of stray light and excellent signal-to-noise ratio.\nPost-Dispersed Design Lowers Black-Body Impact\nPost-dispersed design means that ambient or black-body radiation will be dispersed like all other radiation, such that at any given wavelength, its impact will be much smaller.\nPlane Grating Improves Efficiency\nMost commercial grating spectrometers use concave holographic gratings because the optical systems are very simple. Concave gratings always introduce off-axis aberrations such astigmatism and coma which rob light from the image. Holographic gratings are hard to blaze, thus not very bright at the angles of use, again robbing light from the image. Instead of using a concave grating GWI uses a highly blazed plane grating which is a more efficient design. Collimation and focusing are provided by a pair of triplet achromatic lenses. These lenses produce a virtually aberration-free image of the source fiber in the focal plane of the spectrometer. The result is a spectrometer of exceptional brightness, i.e., throughput, hence, a very high signal-to-noise ratio.\nFT-NIR vs. DG-NIR\nFT-IR (not NIR) spectrometers are definitely superior to grating spectrometers in the energy limited infrared region. However, the near infrared is not energy limited so many of the advantages of FT technology do not apply. This has led to many misconceptions or myths (listed below) associated with NIR spectrometer technologies.\nIn the NIR region, FT-NIR spectrometers offer no significant advantages over DG-NIR spectrometers, and many times are not as accurate,\nefficient or economical as DG-NIR multi-channel, dual-beam analyzers.\nFT-NIR Misconceptions and Facts\nFT-NIR is a newer technology\n|The fundamental technology of FT systems and dispersive analyzers were both developed in the 1800s.(Michelson interferometer – 1887, Henry Joseph Grayson grating ruling engine – 1899). Both technologies became feasible for process applications with the development modern telecom fibers and detectors, high quality optics, and the advent of the PC. Both use the same high quality optics, detectors, fibers, and light sources.|\nFT-NIR has easier calibration transfer\n|Both FT-NIR Systems and DG-NIR Analyzers can directly transfer calibrations between channels. The method of light dispersion is not relevant to thr success of calibration transfer. Instrument-to-instrument repeatability in terms of the fundamental characteristics (bandwidth, stray light, wavelength axis accuracy) are key in successful calibration transfer. FT-NIR will use their laser source to maintain wavelength accuracy, while DG-NIR instruments use temperature compensated filters with NIST traceability.|\nFT-NIR has lower error in calibrations due to better wavelength resolution\n|In the near infrared region the small increase in resolution by FT-NIR does not translate into lower error calibrations. (Armstrong, 2006 Applied Engineering in Agriculture. 22. DOI:10.13031/2013.20448)|\n|TRUE Somewhat: Fellgett Advantage – scan time||FT-NIR measure all wavelengths simultaneously while scanning grating systems measure one wavelength at a time. This theoretically gives the FT-NIR a “multiplexed” advantage which improves the SNR. However, since all of the light falls on the FT detector, it is often driven non-linear and the light must be attenuated. The reality is that grating spectrometers can have a SNR that is equal to or superior to a comparable FT-NIR system.|\nJacquinot Advantage – higher light throughput\n|If the FT-NIR system is configured to measure through a fiber optic cable, then the aperture or throughput of light is limited by the diameter of the fiber optic cable which is essentially the same for both types of instruments. This eliminates any potential advantage for FT-NIR online process monitoring.|\nConnes Advantage – wavelength accuracy\n|FT-NIR systems use a single NeHe laser to verify the wavelength accuracy. FT-NIR wavelength accuracy depends on the precision of the alignment between the laser beam and the white light source and the number of zero crossings measured of the laser fringe, i.e., the resolution at which the spectrum is recorded.\nDispersive analyzers use NIST traceable standards to check the accuracy of multiple points along the wavelength range. The wavelength accuracy is limited by the precision of the NIST standards and the reproducibility of the grating drive mechanism. (Armstrong, 2006\nApplied Engineering in Agriculture. 22 DOI: 10.13031/2013.20448)\nAdditional Considerations and Comparisons: Analyzer Validation\nAn important consideration for successful process monitoring is the ability to continually monitor the accuracy and precision of the system, thus ensuring the analyzer is producing validated spectra for your applications.\nWith FT-NIR analyzer validation is often done using external fluids (Pentane and Toluene) which is rarely available and is an expensive consumable. Pentane is a wash fluid. Spectroscopic Grade Toluene is required as the validation sample. Industrial grade Toluene cannot be used for this purpose. Validation can be automated or run manually but requires additional plumbing to inject the sample on the probe. Thus validation reduces the analyzer up-time and adds complexity (potential failure points) to the sample handling system. With a Guided Wave’s DG-NIR analyzer the validation system is simple and requires no maintenance or consumables. Using the optional Stability Monitoring System (SMS) in the analyzer, there is no need to interrupt the other channel operation. It provides automatic and continuous analyzer validation according to ASTM methodology.\nThe ongoing costs and ease of use associated with any instrument is an important consideration. Both FT-NIR and DG-NIR use tungsten-halogen lamps as the light source and an InGaAs detector.\nFor DG-NIR the lamp replacement is typically every six months and it is the only consumable needed. The replacement of this light source can be completed by any person in a matter of seconds, as lamps are all pre-aligned.\nFT-NIRs also require that the lamp be periodically replaced. Furthermore, the laser has a finite lifetime and occasionally needs replacement. Replacing the laser not necessarily simple as its alignment to the white light beam from the lamp is critical. Thus laser replacement is often done by a factory trained service engineer.\nMultiplexing Facility - The Input Module\nFT-NIR spectrometers can be multiplexed (multi-channel operation) but to do so often requires fiber multiplexers with moving optical elements. It is not possible to move optical elements and not introduce some noise in the system. On-line spectroscopy often requires SNRs > 105 which exceeds the capability of moving optical element multiplexers. The cost of the additional hardware limits the number of channels to between 2 and 8.\nAn alternate method of multiplexing is stream switching. This involves an extractive sample system with motor operated valves and possible cross contamination in the sample cell. This is a slow, high maintenance approach.\nGuided Wave’s DG-NIR analyzers have built in multiplexing with no moving optical elements. Thus there is no degradation in the SNR. A twelve channel DG-NIR system can switch between samples in seconds.\nProcess analyzers are expected to operate 24/7 with minimal maintenance. Moving parts in an analyzer are therefore always looked on with suspicion. Fortunately, most modern spectrometers have long mean time between failure (MTBFs) on the order of years. Both FT-NIRs and scanning grating spectrometers have critical moving parts. FT-NIR spectrometers have one or two oscillating mirrors that provide the phase encoding of the spectrum. If these mirrors do not move smoothly or fall out of alignment, faulty spectral results can and do occur. Similarly, scanning grating spectrometer must rotate the grating precisely and measure that rotation with a precision optical encoder. Again failure of the mechanism can result in bad spectra. However, the reliability of FT mirror mechanisms and grating drives are exceptional with years of trouble free service expected from both.\nConclusion: DG-NIR Advantage\nWhen considering NIR for an application that involves a “clear” liquid or gas, a dispersive NIR Spectrometer, DG-NIR is the superior choice. By developing NIR analyzers with dual-beam, post-dispersive planar gratings, the engineers and scientists at Guided Wave have advanced the state-of-the-art in dispersive NIR technology. By incorporating these advancements without compromising the dual-beam operation, Guided Wave can offer to the market a DG-NIR analyzer with superior accuracy and resolution. The DG-NIR advantage is due to the way these analyzers are designed to control light and minimize all forms of aberrations DG-NIR system has been carefully optimized to provide exceptional signal-to-noise ratio, excellent long-term photometric and wavelength stability, built-in multiplexing, and ease of maintenance.\nGuided Wave has been a leader in online, process monitoring for over 35 years. Established in 1983, Guided Wave was recognized as an industry leader when it delivered the first fiber optic-based NIR analyzers. Today Guided Wave has NIR analyzer installations on six continents and in more than 50 countries, with thousands of analytical instruments sold worldwide for the Chemical, Refinery, Pharmaceutical, Polymer, Semiconductor, and Sterilization industries.\nGuided Wave is the only process NIR manufacturer that provides a complete optically matched system, yielding the best throughput efficiency and long-term performance, exceeding industry standards. These workhorse analyzers have been industry-proven for over three decades with individual analyzers in the field typically running 24/7 lasting more than 10 years with >99% uptime.","5 gas detection misconceptions\nIn recent years, portable gas detectors have shrunk in size and cost while many new features have been added: automatic time-weighted average (TWA) and short-term exposure limit (STEL) calculations, data logging, man-down alarm, and wireless capabilities to name a few. With increased usage due to reduced cost, enhanced safety awareness and tighter regulations, misconceptions regarding correct portable instrument usage have increased accordingly.\nMisconceptions become problematic when those ideas are gradually accepted as truth. Those safety industry misconceptions can not only be costly, but also potentially hazardous, injurious and even deadly. Five common safety industry misconceptions will be discussed here.\nMisconception #1: Gas detection is self-explanatory; no training is\nMost manufacturers design portable gas detectors to be as intuitive and as simple to use as possible, offering quick-start guides and electronic simulators; however, training related to the specific application is still necessary. While most employers provide adequate training programs, there are still many safety managers who provide little or no specific application training, assuming instead that workers will interpret the manufacturers’ limitations accordingly.\nA gas detector will do what it is designed to do: provide alarms that sound, light or vibrate when a gas hazard that the unit is equipped to detect is present. Of critical importance is the need to understand environmental influences and technological limitations in order to prevent informational misinterpretations, worker panic or major incidents concerning potentially hazardous gas exposures.\nExamples of accidents due to inadequate training range from fatalities occurring within confined spaces to tank explosions. An example of the latter: a documented incident concerning combustible gas readings taken prior to welding. The latest reading indicated levels well below the lower explosive limit (LEL). Improper instrument use plus the operator’s inadequate understanding of its limitations were cause of a fatality. Low oxygen level (<10 percent) resulted in an incorrect LEL reading interpretation. Other documented informational misinterpretation incidents relate to cross-sensitivity or sensor poisoning. Every instrument has its own set of sensors, operating procedures and instructions that must be understood by all users.\nGas detection education and application-specific training improves user comfort level, reduces misunderstanding, and reduces costs long-term. Most gas detection instrument manufacturers offer training classes to improve your knowledge base. However, the addition of comprehensive application-based training can be the difference between life and death.\nMisconception #2: Daily instrument bump tests are not required as long as you calibrate periodically.\nThis statement is commonly used in the field to win sales orders; however user manuals may contradict assertions concerning frequency of bump tests and full calibrations. There is no way to ensure that gas detectors work properly unless gas is applied and alarms and readings are verified. It doesn’t matter whether your fleet includes disposable alarming devices or direct reading instruments. Recent cases of O2 sensors ‘frozen’ on 20.9 percent Vol reading, or unresponsive CO sensors displaying stable zero readings are clear evidence. Poisoned combustible sensors can also display stable zero readings and not respond to applied combustible gas. After such industry-wide occurrences and resulting safety notices, the assertion above is thankfully less likely to be used.\nTo make life easier for users, manufacturers have developed bump and automatic calibration stations to simplify complex and time-consuming processes, minimizing necessary training for performing required instrument tests before each day’s use.\nWhile several manufacturers offer innovative ways to verify sensor life and health, there is no current automatic solution to determine that sensor outer membranes are clogged with invisible polymers or dirt, an issue that can prevent gas from reaching sensors.\nMisconception #3: All gas detectors are the same and provide the same safety level.\nNot all detectors are created equal. Most manufacturers use off-the-shelf sensors from known sensor suppliers, but only a few have the know-how and capability to manufacture their own sensors. While all gas detectors are approved to meet certain standards, only some meet stringent performance standards and will hold up to the harshest conditions.\nWhere costs are concerned, the devil is in the details; sensor life, response time, warranty, rugged design, total cost of ownership, etc. differ significantly when comparing competitive gas detectors. Faster-responding sensors not only alert you more quickly to potentially hazardous situations, but also save you gas and labor time with every calibration and bump test. Unreliable sensors and poorly constructed detectors can cause unnecessary downtime and shutdowns, costing you more than ever imagined.\nFeatures such as color-change display screens can be critical in conveying information regarding potentially hazardous situations at a glance and taking proper, immediate action. In isolated environments where co-workers are out of sight, features such as man-down alarms enhance worker safety. Instruments with significant battery run time allow for use of the same detector for two or more shifts, reducing downtime and related costs.\nMisconception #4: Gas detectors don’t require routine maintenance.\nMaintenance is part of life…get used to it. You need to maintain your car and your home; even your body needs a periodic checkup to ensure that all parts work properly. Why would you not expect to maintain a device that is designed to save your life?\nPeriodic maintenance is needed to keep your company operating and profitable by minimizing unexpected failures and possible accidents; gas detectors are no different in that respect. The European Technology Platform Industrial Safety (ETPIS) estimates that 10-15 percent of fatal work-related accidents and 15-20 percent of all accidents are related to lack of proper equipment maintenance.\nWhile maintenance is viewed as a necessary evil, a critical look at maintenance frequency and actual total cost can really pay dividends. Some manufacturers offer ways to reduce or eliminate portable instrument maintenance by offering total care programs built around leasing and renting instrument fleets to customers. Manufacturers provide instrument maintenance; consequential costs are simply included as part of leasing fees.\nActual long-term cost-savings are hidden in the warranty, durability, sensor life and response times. When optimized, those features can save you significant maintenance costs overall by reducing instrument and sensor replacement frequency, component replacement and calibration gas usage. Cost of ownership over several years typically far exceeds actual purchasing cost of detectors. Calculating total maintenance cost over a longer period can be a true eye-opener.\nMisconception #5: Gas detectors CAN accurately detect 1.0 ppm H2S.\nThe American Conference of Governmental Industrial Hygienists (ACGIH) has amended its H2S exposure recommendation by reducing H2S TWA level to 1.0 ppm; some European countries have changed their requirement to 1.6 ppm.\nLet’s consider a gas detector with a reading of 1.0 ppm H2S, calibrated with typical NIST-traceable calibration gas of 20 ppm H2S concentration. Cylinder accuracy is stated as ± 10 percent, meaning 2.0 ppm. Due to environmental factors (temperature, humidity, pressure, etc.) and H2S reactivity, the best electrochemical H2S sensor currently available offers 5 percent of full-scale accuracy and 0-30 ppm detection range or 1.5 ppm.\nTolerances of instrument and calibration gas values are additive, resulting in 2 ppm + 1.5 ppm = 3.5 ppm. As a result, a typical 1.0 ppm reading is correctly stated as 1.0 ± 3.5 ppm; in other words, the concentration may be anywhere between -2.5 ppm and 4.5 ppm.\nThe tolerances listed above, in addition to inherent electrical noise of sensors and associated circuits, may cause instrument manufacturers to employ noise dampening techniques that mask low-level readings. Consequently, some gas detectors may not provide such precise readings at 1.0 ppm and may not even display values until they reach 3.0 ppm.\nDue to the factors described above and depending upon environmental conditions, specific sensor type and manufacturer chosen, users with instrument H2S alarms set at 1.0 ppm may not be alerted to concentrations between 1.0 and 4.5 ppm and/or may encounter frequent false alarms. This is important for users required to follow this ACGIH limit, as opposed to the OSHA, NIOSH or other recognized H2S exposure limits. Choose an instrument that provides the appropriate detection capability.\nAssumptions are not necessarily evil. Research is built upon assumptions, but should subsequently be followed with proper empirical analysis to prove theories as correct or incorrect. Avoid misconceptions, as acting upon them can result in negative impact upon the well-being of your personnel and affect the bottom line. Verify, rather than assume. Reliable and trustworthy manufacturers will help you to work through your questions and guide you to safer product solutions.\n• European Technology Platform Industrial Safety (ETPIS)\n• Occupational Safety and Health Administration (OSHA)\n• Safety and Health Information Bulletins (SHIB)\n• American Conference of Governmental Industrial Hygienists (ACGIH), www.ACGIH.org/TLV/"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:65509efc-5f2c-4abb-82b0-1df207f55679>","<urn:uuid:7096024e-4640-451f-a560-a01bd9ea2f08>"],"error":null}
{"question":"How does wholemeal and multigrain sliced bread compare nutritionally to white sliced bread?","answer":"Wholemeal and multigrain sliced bread are healthier than white sliced bread, containing about double the fiber content due to whole grain flour and bran. While multigrain bread has higher fat content, these are healthy polyunsaturated fats from seeds and cereals. However, sliced bread in general is slightly more caloric than traditional loaf bread, containing about 20 kilocalories more per 100 grams (1% of daily caloric intake) due to added sugar and oils.","context":["Updated on Tuesday, 5 October 2021 – 08:47\nThe OCU invites “invites to prioritize this type of bread” after an analysis of 12 white mold loaves with crust, 12 whole grains and 10 multigrain\nSliced bread with seeds THE WORLD\nWholegrain and multigrain sliced bread are healthier than white sliced bread, but also more expensive, as well as being higher in calories than traditional loaf bread.\nThis is concluded by an analysis that the Organization of Consumers and Users (OCU) has carried out in 12 white sliced breads with crust, 12 whole grains and 10 multigrain to compare the quality of the sliced breads for sale in the main supermarkets as well as their nutritional contribution, hygiene, texture, homogeneity of the slices, the tendency to stale and labeling, according to a statement .\nIn parallel, it has studied their sale prices and a dozen trained tasters have carried out a tasting and among the main conclusions of the research highlights the greater contribution of fiber from wholemeal and multigrain breads, which, according to OCU, “usually doubles” the of white breads as a consequence of the use of whole grain flour as the main ingredient and the addition of bran made by some brands, a matter that, in his opinion, “invites prioritize this type of bread “.\nLikewise, he underlined that, although in multigrain bread the fat content is “higher” than the rest, since they are polyunsaturated fats that come from added seeds and cereals, they are considered “healthy”.\nAlong with this, he admitted that the hygiene is “correct” in all the products analyzed, at same as texture and labeling, except in the case of the Rustic Bakery cereal bar, whose slices are the least tender of those analyzed, although also the only ones that do not incorporate additives, while the rest have between two and 10.\nIn front of the loaf of bread, the bread is “slightly” more caloric because it contains “a little more” sugar and fat from sunflower oil that is added, although some incorporate olive oil. However, according to the OCU analysis, this difference “barely” reaches 20 kilocalories / 100 grams, or, what is the same, 1% of the daily caloric intake.\nIn contrast, sliced bread contains “a little less” salt, especially white sliced bread, and will last longer fresh.\nRegarding the price, according to the OCU research, this is usually higher in whole grain and multigrain breads, standing at 2.00 euros / kilo and 3.02 euros / kilo respectively, compared to 1.62 euros / kilo for the price. white sliced bread. However, he cited as “exceptions” the whole wheat bread La Cestera, from Lidl, which costs 1.09 euros / kilo and the multigrain bread El Horno de Aldi, on sale for 2.04 euros / kilo, products that, in addition, stand out for their good value for money and its good tasting results.\nRegarding conservation, OCU recommended store the sliced bread in a cool and dry place, but not in the fridge, since the cold “spoils” its texture, and pointed out that, in this way, it can be consumed even after its best before date, which is around 15 days from the time of purchase.\nAccording to the criteria of\nThe Trust Project Learn more"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:0b2d903c-3e70-4269-8a4b-c76fb676d745>"],"error":null}
{"question":"What is common between IoT data streaming from machines and E-learning systems in terms of knowledge management benefits?","answer":"Both IoT data streaming and E-learning systems enable knowledge delivery at the time of need. IoT devices like jet engines stream data during operation for immediate problem detection and service needs, while E-learning systems allow users to access training content at their convenience when they need it. However, both require proper knowledge management systems to be truly effective - IoT data needs to be combined with service history and product knowledge to be actionable, while E-learning content needs to be integrated with knowledge repositories to enhance its usefulness.","context":["IoT must be back-ended with service knowledge management to enable intelligent service with product knowledge and service history.\nAlthough connected products seem like they’re new, they’ve been around for a while. Vending machines send their inventory status via cellular networks or the Internet. Sensors in other products, such as machines in a factory, can send gigabytes of data.\nEven though these sensors send so much information from the field, they may be missing important information. Data from the field is useful for designing future products, but what about today? Customers need actionable, smart information to address operational issues happening now. This is why it’s crucial for companies to have a good service knowledge management system.\nIn this series, Steve O’Lear has explored how companies can benefit from having these systems in their service lifecycle management strategies. In part one, he looked at how these systems help customers increase their profits. In part two, he explained how this system could benefit service teams fixing a deep sea oil rig pump. In part three, he discussed how these systems help customers observe performance trends and how they benefit automotive companies. Here, he discusses how these systems benefit those in the aerospace and defense (A&D) industry.\nHow aerospace companies can use service knowledge management systems\nWe’ve looked at how these systems can benefit companies in the automotive and the oil industries. Now let’s look at how these systems can benefits companies in the A&D industry.\nTake a jet engine on a plane. Great quantities of data, along with any error codes, stream from the engine during its operation. Today’s jet engines are smart, and they need to work flawlessly: there’s no infield to coast to if there’s a problem. Examining trending information from the engine or seeing an error code might indicate a problem that needs to be addressed immediately, but again, this information tells you nothing other than that there’s a problem.\nIn the A&D industry, a manufacturer does the same design, simulation, testing, manufacturing and service planning our pump manufacturer did. But the manufacturer probably did a lot more of it with a much more complex, regulated product. Even after the engine is delivered and installed on an airplane, someone has to track the configuration through all service events on the engine for the life of the engine. Someone has to track all the utilization characteristics of the engine, take offs, landings, flight hours and heat cycles. These characteristics point to when and what service must be executed as approved by the Federal Aviation Administration (FAA). But as engines proceed through their useful life and services cycles, configurations change.\nThe complexity of a jet engine only increases over its life and requires detailed knowledge of its service history.\nAn OEM may issue a service bulletin (SB) to correct a potential problem or enhance engine performance. The FAA may issue an airworthiness directive (AD) to correct a problem based on a SB. The difference between ADs and SBs is that ADs are mandatory. SBs may be optional and can be executed out of sequence or not at all based on the operator’s applicability decisions. They may have certain prerequisites that require other SBs to first be executed.\nThis is all about the engine configuration through its productive use and service history. If the engine reports a problem and requires service, you need to know its configuration to do that work. What changes have been made to it? Which ADs have been or still have to be applied? Which SBs have been applied?\nYou don’t want to wait until you start disassembling the engine to determine its configuration and what parts kit(s) you need. That takes time away from the engine being operational, and it can be prone to error. This data resides in only one place: the service knowledge management system.\nMany industries are changing their business model and turning their focus to aftersales service. Aerospace, marine, industrial machinery and other industries with long life products that have complex configurations take service lifecycle management seriously. Service revenue and profits over the product’s life can be greater than the initial sale of the product. It can also be a more stable revenue and profit stream.\nIn fact, there are aerospace and energy companies today that generate the larger share of their revenue from servicing their products rather than selling their products. Some companies actually consider products to be a platform for service. This service is just as important to customer satisfaction and loyalty as it is critical to long-term business success.\nWhen you hear about the Internet of Things (IoT) and Big Data, keep in mind that taking advantage of these trends really depends on what you do with all of those data points. Are we looking back at what happened to improve future products? Or, are we combining what we’re learning from the IoT and Big Data with what we already know about our product?\nThat’s why you need a service knowledge management system as part of your overall service lifecycle management strategy. That combination empowers you to create smart data and achieve smart service.\nAbout the author Steve O’Lear has been in the information system industry for more than 35 years. He has held positions in consulting, services management, sales and marketing across computer hardware, timesharing services (cloud), supercomputing and custom information management solutions in various industry segments. He has more recently focused on PDM and PLM, and many of his customer engagements have been in the A&D industry and with discrete manufacturers. He is currently focused on product marketing for solutions related to document management and service lifecycle management. Early in his career, Steve recognized the need for manufacturers to manage product development data and processes more holistically and became involved with the development, implementation and marketing of PDM solutions. He has also recognized this need with PLM and is now promoting the importance of the support phase in the product lifecycle as products become platforms for service for manufacturers.","KM Component 48 – E-learning\nE-learning consists of tools that enable the delivery and tracking of online training courses.\nA Learning Management System (LMS) is a software application for the administration, documentation, tracking, reporting, automation, and delivery of educational courses, training programs, or learning and development programs. The LMS concept emerged directly from E-learning.\nLearning is one of the basic activities of knowledge management. Organizations usually have a Learning & Development function as part of Human Resources, and it is responsible for a wide range of employee development, including classroom instruction and online learning. KM programs sometimes report into this function with the goal of better integrating knowledge reuse with learning.\nE-learning is important to a KM initiative in several ways. KM training is an important people component, and e-learning can be used to deliver it. Integrating content from knowledge repositories into e-learning can improve its effectiveness. Delivering e-learning along with knowledge content returned through searches or through browsing can enhance the usefulness of the results and better leverage training content. Tracking training which employees have taken can help suggest new offerings they should consider as part of a specific knowledge requirement and as part of their ongoing development.\nE-learning systems are examples of tools that enable demand for knowledge. E-learning allows users to learn at their convenience and at the time of need.\nVideo recordings can be helpful in showing how a process is actually performed. It is a good idea to enable videos to be delivered through standard e-learning systems.\nIf your KM program is part of the Learning & Development function, then you will have strong incentives to tightly couple the two environments. If not, then look for opportunities to establish ties to that function, and explore possible points of integration which will be mutually beneficial. At a minimum, provide a link to the organization’s e-learning tool as part of the standard navigation bar in the KM user interface.\nFor additional information on E-learning and LMS software, these resources will be helpful.\nContent about E-learning\n- ATD (Association for Talent Development)\n- eLearning Industry\n- eLearning Learning\n- The Learning Guild\n- Craig Weiss\nE-learning Thought Leaders\nLMS Software Comparisons\nPlease enjoy Stan’s additional blog posts offering advice and insights drawn from many years as a KM practitioner. You may also want to download a copy of his book, Proven Practices for Implementing a Knowledge Management Program, from Lucidea Press. And learn about Lucidea’s Presto and SydneyEnterprise with KM capabilities to support successful knowledge curation and sharing.\nNever miss another post. Subscribe today!\nThe Five Cs of KM: Create Part 1—Basics, Connection, and Methods\nKnowledge creation includes inventing concepts, approaches, methods, techniques, products, services, and ideas to benefit people and organizations.\nLucidea’s Lens: Knowledge Management Thought Leaders Part 36 – Charlene Li\nKM thought leaders; Charlene Li specializes in disruption, digital transformation, leadership, customer experience and the future of work.\nThe Five Cs of KM: Collaborate Part 4—Working Out Loud\nThe goal of Working Out Loud (WOL) is to inform others about projects and to respond, learn, and apply the knowledge of others to their own work.\nThe Five Cs of KM: Collaborate Part 3—Communities\nReview of tips, tools, and proven practices that enable and support productive community of practice collaboration in knowledge-intensive venues\nLeave a Comment\nComments are reviewed and must adhere to our comments policy."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:122f0fee-0ad3-4302-a5d2-15283bef0f70>","<urn:uuid:8255ea59-d5d8-4c09-95fa-33a5f2a1fe12>"],"error":null}
{"question":"I study animal communication systems and would like to understand how aardvark and prairie dog survival strategies differ in response to environmental threats?","answer":"Aardvarks and prairie dogs show very different survival strategies when facing threats. Aardvarks, when faced with drought and heat, attempt to adapt by shifting their foraging time from night to day to conserve energy, and resort to sun-basking, though these strategies often prove insufficient when their food sources (ants and termites) become scarce, leading to starvation and death. In contrast, prairie dogs have developed an elaborate communication system with about 11 distinct calls and various postures to warn colony members of danger. They use specific warning calls like the 'chirk' when detecting predators, and perform 'jump-yipping' displays that may serve as 'all-clear' signals when threats have passed. This sophisticated social warning system helps them survive predation threats in their environment.","context":["The aardvark, a highlight for anyone on a game-viewing African safari, will become increasingly rare as the world warms and dries, and the consequences go well beyond a decline in aardvark safari encounters.\nAccording to researchers studying this elusive mammal, sometimes classed as one of the “Shy 5”, in South Africa’s Kalahari Desert, aardvarks prove to be highly susceptible to the warmer and drier climates that are predicted for the western parts of southern Africa, in the future. During the study of a number of aardvarks by researchers of the Brain Function Research Group at the University of the Witwatersrand, all but one of the study animals – as well as other aardvarks in the area – died because of a severe drought, with air temperatures much higher than normal and very dry soil in the area.\n“While unusual now, those are the conditions that climate change is likely to bring as the new normal,” said Professor Andrea Fuller, the Research Group’s director.\nDr Benjamin Rey studied the aardvarks as part of his postdoctoral studies. Along with his colleagues, he used the new technology of “biologgers” (miniature sensors attached to computer chips and implanted into the aardvarks by wildlife veterinarians), to study the activity patterns and body temperatures of aardvarks living in the Kalahari. The researchers were not to know that during the year of their study there would be a severe drought, which led to the death of the study animals.\n“It is not because the aardvark’s body can’t take the heat, but that the termites and ants that they rely on – not just for food but also for water – can’t take the heat and aridity of changing climates,” said Rey.\nAardvarks usually sleep during the day in burrows that they have dug, and emerge at night, to feed on ant and termites, using their long, sticky tongues to sweep up thousands of insects. However, during the drought, the termites and ants, on which the aardvark depends for body energy, were not available.\n“As a result, the aardvarks’ body temperatures fell precipitously at night. The aardvarks tried to compensate by shifting their search for ants and termites from the colder night to the warmer day, so that they would not have to use energy to keep warm, but that was not enough to save their energy stores,” said Dr Robyn Hetem, a co-worker on the study. “We believe the aardvarks starved to death.”\nThe aardvark progressively became skinnier and bonier. They even tried sun-basking to save energy, but many ultimately died. Their body temperatures dropped to as low as 25°C just before they died.\nRey says that this curious-looking creature – described as having the snout of a pig, the ears of a rabbit and the tail of a kangaroo – is much more than just a curiosity to be checked off a bucket list.\n“Many species of African birds, mammals and reptiles use the burrows dug by aardvarks to escape cold and heat, to reproduce, and to avoid predators. They can’t dig these burrows themselves. Without aardvarks, they would have no refuge. Worryingly, they could face the same fate as the aardvark.”\nClimate change in southern Africa affects animals through the direct effects of increasing air temperatures and aridity. Wild dogs, for example, reduce hunting activity as temperature increases. But the indirect consequences of heat and aridity may be more pervasive. Disappearance of aardvarks, and with them the burrows that they dig, will have knock-on effects for many other animals.","Why all the fuss over a bunch of ground squirrels? The answer lies in the farmer and rancher's view of prairie dogs' impact to their livelihood. Prairie dogs compete with livestock for forage. They clip vegetation to maintain a view of their surroundings and eat the same grasses that would otherwise be available for cattle and horses. Over long periods of prairie dog activity grass species which ranchers find most desirable can disappear. They are often replaced with less palatable grass species with lower nutritional value. In farmed ground prairie dogs can decimate or destroy a crop of alfalfa, grains or hay.\nPeople also compete with prairie dogs for shelter. Expanding urban areas, especially the rapidly growing Front Range in Colorado, has seen the conversion of prairie dog towns into people towns-as housing and commercial development replaces grasslands. No government-sponsored campaign against the prairie dogs has been necessary to insure the dominance of human uses in our towns and cities. Once prairie dogs are removed, the habitat is changed to such a degree prairie dogs usually don't have the opportunity or ability to reestablish themselves.\nTheir burrow system has been studied well enough so that we know today one might find the following: (I) a \"listening post room\" just under the surface and set offfrom their main burrow, (2) a separate \"room\" which they use as a toilet and which may be emptied periodically, (3) a nesting/sleeping chamber lined with dried grass. The nesting chambers are often elevated from the bottom of their tunnels so they remain dry when water flows into the burrow entrance.\nThe social life of black-tailed prairie dogs has been the topic of much study and interest by field naturalists and ecologists. For example, prairie dogs have a vocabulary of about 11 distinct calls and a host of postures and displays. When detecting danger, prairie dogs alert other members of the colony. They make a loud \"chirk\" while standing, on two or four legs, or lying prostrate in a burrow entrance. This anti- predator call has been given a variety of names including the \"squit-tuck\", the warning bark, and the \"tik-uhl\". It is something of an evolutionary puzzle why prairie dogs would risk their own skin by drawing the attention of a predator in order to save other members of their town. Some scientists believe that prairie dogs only mean to warn their close kin.\nAnother call given by prairie dogs has also attracted the naturalists' attention. From time to time a prairie dog will stand on its hind legs, stretch the body as vertical as possible and throw its front feet high into the air. While doing all this it begins a very loud call, described variously as a yelp, cry, song and a yip. While it is amusing to watch one prairie dog do this, jump-yipping soon spreads through the colony creating a contagious and wild dance. Naturalists believe that jump-yips probably help family groups maintain their territories, and may possibly be an \"all-clear\" signal once a predator has left the vicinity.\nA certain prairie dog behavior has been described as kin recognition, family member identification or brood discrimination. But what they are doing is kissing. Black-tailed prairie dogs kiss when they meet within a family's territory. It typically goes something like this: a prairie dog, uncertain about the identity of a neighbor in her territory, cautiously creeps toward the other. She opens her mouth showing her teeth. If the other prairie dog is a family member, they will touch their mouths for an instant or for several seconds. Once they have kissed, prairie dogs may groom each other, nibble at one another's fur or wrestle around in mock battle. As with jump-yipping when two prairie dogs kiss a chain reaction often follows with a colony full of kissing prairie dogs.\nWhile the social life of prairie dogs is interesting and much studied, their role in grassland ecology is equally fascinating. Historically, bison grazed patches of mixed grass prairie short enough for prairie dogs to colonize. As prairie dogs fed upon and trimmed the vegetation, it shifted from a mature prairie to a more disturbed state with more weedy broadleaf plants. Under initial pressure of prairie dog grazing, the grass grew more rapidly and was richer in nutrients. Increased abundance of the weedy broadleaf plants attracted pronghorn antelope. Bison, mule deer and elk also visited prairie dog colonies to feed upon the nutrient rich grasses. Although elk and bison are gone from most of the grasslands of the Great Plains, prairie dog colonies continue to support the biodiversity of the great plains by attracting a variety of animal species. These include fairly common and widespread species such as meadowlarks, horned larks, cottontail rabbits, and deermice as well as species with closer ties to the prairie dog colonies such as burrowing owls. Burrowing owls feed upon insects and small mammals around prairie dog towns and depend upon prairie dog or badger holes for their nests. Many other uncommon or rare species of grassland animals in the Boulder Valley frequent prairie dog colonies. These include golden and bald eagles, ferruginous hawks, badgers, prairie falcons. Black-footed ferrets, an endangered species now extirpated from most of their range depend almost entirely upon prairie dogs for food and shelter.\nNot only do they have far reaching impacts like people do, but they live in family groups, and have an elaborate system of communication (well the yipjump is not quite the Internet, but it meets the prairie dogs' needs). They kiss, wrestle, and defend the boundaries of their territories. When the landscape does not suit them, they change it to be safer and more comfortable in their homes. The prairie dog modified landscape attracts a distinct group of animals just as our cities and towns attract pigeons, racoons, house sparrows and fox squirrels.\nHowever, unlike human populations which continue to grow and spread out over the landscape, the extent and numbers of prairie dogs are shrinking. Out of an estimated 100 million acres of active black-tailed prairie dog colonies, about 2 percent remains. While poisoning programs aimed at preserving agricultural productivity have been responsible for most of this decline, urban development and bubonic plague are becoming significant concerns in the last few decades. The black-footed ferret has been driven to verge of extinction by the reduction of prairie dog colonies. Burrowing owls have been extirpated from the Boulder Valley. Conservation biologists are concerned that other species dependent upon prairie dogs may follow in what has been described as an \"ecological train wreck.\"\nThe Open Space Department has begun work on a grassland management plan in an attempt to head off such a train wreck in the Boulder Valley. The Department has worked over the past three years with a committee of farmers, rural residents, environmentalists, animal rights advocates, land managers and interested citizens to develop a set of goals for prairie dog management on Open Space. The initial phase of the plan concentrates on prairie dog management and seeks to reduce the conflicts between prairie dogs and adjacent land uses by establishing a system of prairie dog habitat conservation areas throughout the Open Space land system.\nIn order to develop the preserve system, the Open Space land system has been broken down into several categories. The first category is \"ecological suitability.\" Most grasslands on Open Space were considered to be ecologically suitable for prairie dogs. Coniferous forests, wetlands, tallgrass prairie, and riparian areas are ecologically unsuitable.\nEcologically suitable lands were then considered for their \"cultural suitability.\" Irrigated crops and pasture were considered to be culturally unsuitable because the Open Space Department has a part of its mission, the preservation of agricultural land uses in the Boulder Valley. Areas being reclaimed from a previous land use (for example, the gravel mine along Coal Creek or the strip farming on Gunbarrel Hill) were also considered culturally unsuitable-at least until a healthy native grassland is restored. As part of the cultural suitability analysis, the Open Space Department also wanted to insure that there would be prairie dog colonies in places where people could enjoy observing them and where educational activities could be focused.\nThe remaining suitable habitat was then examined more closely, using generally accepted principles of preserve design. For example, large, contiguous blocks of habitat usually make better preserves than small, isolated blocks. The preserve design was then modified for the specific requirements of prairie dogs such as soil type, slope, vulnerability to plague and barriers to dispersal. The requirements of those species which depend upon prairie dog colonies (burrowing owls, raptors and badgers) were also taken into consideration when developing the preserve design.\nThe resulting preserve system includes approximately 4,600 acres in seven habitat conservation areas. Prairie dogs will exist essentially undisturbed in habitat conservation areas where management activities will focus upon encouraging prairie dogs to create and maintain habitats for a variety of plant and animal species. The first phase of the grassland management plan also contains a set of policies describing in greater detail the Open Space Department's control and monitoring programs, educational initiatives, and public process as it relates to prairie dog management."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:a7e9cb45-463a-4c85-947b-3b8dce02577e>","<urn:uuid:03e9119b-3cce-44b2-a874-e6efb71076d9>"],"error":null}
{"question":"How do space-time coordinates compare to atomic orbital configurations in describing position?","answer":"Space-time coordinates use four dimensions (three spatial coordinates and one temporal coordinate) to completely specify the location of particles and events. In contrast, atomic orbital configurations like gold's [Xe]4f145d106s1 describe electron positions through quantum mechanical arrangements, where relativistic effects can alter the spatial distribution of electrons, as seen in gold's contracted 6s orbital that is pulled closer to the nucleus.","context":["- Also called space-time continuum. the four-dimensional continuum, having three spatial coordinates and one temporal coordinate, in which all physical quantities may be located.\n- the physical reality that exists within this four-dimensional continuum.\n- of, relating to, or noting a system with three spatial coordinates and one temporal coordinate.\n- noting, pertaining to, or involving both space and time: a space-time problem.\nOrigin of space-time\nFirst recorded in 1910–15\nDictionary.com Unabridged Based on the Random House Unabridged Dictionary, © Random House, Inc. 2018\nRelated Words for space-time continuumcontinuum\n- physics the four-dimensional continuum having three spatial coordinates and one time coordinate that together completely specify the location of a particle or an event\nCollins English Dictionary - Complete & Unabridged 2012 Digital Edition © William Collins Sons & Co. Ltd. 1979, 1986 © HarperCollins Publishers 1998, 2000, 2003, 2005, 2006, 2007, 2009, 2012\n- A four-dimensional reference frame, consisting of three dimensions in space and one dimension in time, used especially in Relativity Theory as a basis for coordinate systems for identifying the location and timing of objects and events. In General Relativity, space-time is thought to be curved by the presence of mass, much as the space defined by the surface of a piece of paper can be curved by bending the paper. See more at relativity.\nA Closer Look: Albert Einstein's theory of General Relativity, published in 1915, extended his theory of Special Relativity to systems that are accelerating. One of the primary causes of acceleration in the universe is gravity, and Einstein showed that the effects of acceleration are actually the same as those of the force of gravity; in fact, they are locally indistinguishable. For instance, both in an accelerating rocket in space and in a rocket standing on its launch pad on Earth, the astronauts are pushed back into their seats. Unlike Newtonian physics, which views gravity as an attractive force between all bodies in the universe, General Relativity describes the universe in terms of a continuous space-time fabric that is curved by masses located within it. In the space-time continuum of General Relativity, events are defined in terms of four dimensions: three of space, and one of time, with one coordinate for each dimension; we continuously move along the time dimension. What does it mean, though, for space-time to be curved? One way of conceptualizing this is to imagine just a two-dimensional space-time, with one spatial dimension and one time dimension. But instead of an infinite plane, imagine a tube, with an object's position in time defined by a coordinate of length along the tube, and position in space by a coordinate around the circumference of the tube. An object traveling uniformly through space then describes a helix along this tube, eventually returning to its starting space-coordinate position, but at a different time. (It is an open question in cosmology as to whether our universe has a similar curvature in three dimensions; if so, traveling in one direction long enough would bring you back to where you began.) An important consequence of the notion of curved space-time is that the curvature should affect all motion; thus, even light, which has no mass, should follow a curved path wherever gravity has warped space-time. An important verification of this-which made headlines around the world-took place during a solar eclipse on May 29, 1919, when it was observed that light from stars near the Sun was bent by an angle exactly predicted by the expected curvature of space-time near the massive Sun. Space-time can in principle be warped so strongly by a huge mass that any radiation emitted from the mass curves back in again and cannot escape. These huge masses are thought to exist as black holes.\nThe American Heritage® Science Dictionary Copyright © 2011. Published by Houghton Mifflin Harcourt Publishing Company. All rights reserved.\nThe four-dimensional continuum in which all objects are located and all events occur, viewed as a single and continuous framework for existence. Space-time consists of length, width, depth, and time.\nThe New Dictionary of Cultural Literacy, Third Edition Copyright © 2005 by Houghton Mifflin Harcourt Publishing Company. Published by Houghton Mifflin Harcourt Publishing Company. All rights reserved.","Boundaries between academic disciplines are like borders between European countries. They’re crossed without blinking. You can’t understand what’s going on in your gut without knowing its chemistry, and gold’s properties make little sense without considering special relativity.\nGold is in the same periodic table family as copper and silver, but while its siblings form patina and a dark tarnish, respectively, gold retains its characteristic color in the presence of either smog or sea spray. Less known is the fact that gold can actually mimic chlorine’s relatives and forms salts with rubidium and cesium metals.\nSome scientific ideas are greater than others. Like loved ones, they can be old but resurface in a different context to enlighten you. Who hasn’t been wowed at least a few times by special relativity? For instance, it has to be taken into account by engineers designing GPS systems. But there are even more tangible relativistic effects happening right now in something as prosaic as a gold ring. In heavy atomic nuclei, the strong coulombic force has a significant effect on the velocity of inner electrons. Close enough to that of light, electrons’ speed increases their mass, enough to contract the Bohr radius. Specifically, gold’s innermost electrons move at 58% of the speed of light, and instead of the typical < 0.01c and ensuing negligible rest mass-increase for a hydrogen atom’s electron, there is a 23% increase in mass for a gold 1s electron. Although the relativistic effect doesn’t affect all types of atomic orbitals, it draws all s orbitals closer to the nucleus, including gold’s 6s orbital, where its valence electron resides. If the relativistic radii for various atomic numbers are plotted, we notice that gold is the most affected in the entire periodic table.\nGold’s 79 electrons are configured as such: [Xe]4f145d106s1. If it wasn’t for relativistic effects, there would be a bigger energy gap between the 5d level and the Fermi band at the 6s orbital. An excited electron would absorb in the ultraviolet. But instead as the 6s orbital is pulled closer to the nucleus and the 5ds are shielded and brought closer to the 6s, there’s a strong absorption in the blue and violet, leading to the beautiful blend of red and yellow we perceive as gold.\nIt takes energy to remove an electron from an atom, so if an electron is instead returned to an atom, energy will be released. The latter is known as electron affinity. With a half-filled orbital that’s more attracted by its nucleus than usual, gold has a high affinity for electrons, sharing something in common with with the halogens(see graph). It explains not only the existence of compounds like RbAu but of a more recently created one like Rb5Au3O. Both of these feature the gold (-1) ion, an unusual charge for metals.\nIf you have a conventional mercury(Hg) thermometer, you can also watch special relativity impact gold’s period-6 neighbor. With one more electron than Au, Hg’s 6s orbital is filled and because it’s also tightly held due to relativity, the electrons don’t flow as easily from one mercury atom to the next. This weakens metallic bonds, rendering mercury a liquid at any temperature above -38.4 oC.\nThallium is next to mercury on the periodic table. Although the relativistic effects are a bit less pronounced, as shown in the first graph, the 6s2 electrons are still jealously guarded, so to speak. In most cases only the 6p1 electron is lost, which is why thallium salts, formerly used as rat poisons, are typically in a +1 oxidation state. This makes thallium the black sheep of its family. Other members including aluminum and gallium normally form compounds containing +3 ions. Interestingly, before thallium ions do their mysterious damage, they get through cell membranes by serving as K+ -impostors, thanks to their single positive charge and similar ionic radius.\nLast year, a pharmaceutical chemist who allegedly was more interested in the relativistic effects of thallium than in those of her wedding ring, sneaked a Tl compound out of her lab and poisoned her husband. She was arrested after her flight to China was delayed, not by a relativistic effect but by a snowstorm.\nFor relativistic effects in Pb see Focus: Relativity Powers Your Car Battery\nRJ Hoffman. Thallium toxicity and the role of Prussian blue in therapy. Toxicological Reviews. http://www.ncbi.nlm.nih.gov/pubmed/14579545\nLars J Norrby. Why is Mercury a Liquid. Journal of Chemical Education\nGeoffrey Bond. Relativistic effects in coordination, chemisorption and catalysis\nM. Concepción Gimeno. The Chemistry of Gold.\nImage from http://exagger-art.artistwebsites.com/featured/albert-einstein-art.html"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2aae30ad-4f9f-47f6-ad81-587a9f700784>","<urn:uuid:6f67d9e7-c277-4bf5-a2aa-5568af247dfb>"],"error":null}
{"question":"How do foreign banks face different regulatory impacts under the UK ring-fencing rules versus the Volcker Rule?","answer":"Under UK ring-fencing, foreign banks have an advantage as the rules only apply to UK banks with over £25 billion in deposits, allowing foreign banks to offer cheaper services without restructuring costs. In contrast, the Volcker Rule applies more broadly to foreign banks with US operations - it affects foreign banks with US branches, US bank subsidiaries, and their affiliates. Foreign banks must ensure trades are executed 'wholly outside' the US to be exempt from Volcker Rule restrictions, meaning no US personnel can be involved and counterparties cannot be US residents. The Volcker Rule also requires foreign banks to follow additional safety standards called 'Prudential Backstops'.","context":["UK Banking Regulation ‘Ring-Fencing’\nDiscuss about the UK Banking Regulation Ring-Fencing.\nThe banking regulation in the United Kingdom has undergone many significant changes in the current years after the occurrence of the financial crisis. The financial crisis placed emphasis on developing a banking sector in the UK that is able to maintain health and stability. The reforms are mainly introduced in the banking sector of the UK to change its regulatory structure for simplifying the regulation of banks in the UK. In this context, the Prudential Regulation Authority (PRA), a part of Bank of England, has developed a new regulation ‘Ring-fencing’ for separating retail operations from investment banking operations (Vickers, 2011). The present report in this regard emphasizes on the need of developing ‘ring-fencing’ and its limitations. In addition to this, the report analyses the implementation of ring-fencing in one of the UK bank, such as HSBC, for examining its implications on the customers. Also, the report addresses the challenges faced by the bank in implementing such regulatory changes.\nThe Prudential Regulation Authority (PRA) has been developed as a part of Bank of England by the Financial Services Act in the year 2012 for developing policies related to prudential regulation and supervision of UK banks. The PRA has developed new policies in relation to the implementation of new regulation in the UK banking sector of ring-fencing. The legislation of ring-fencing is introduced for the purpose of separating the operations of retail banking from investment banking in the major UK banks. This is done for protecting the customer interests by safeguarding the bank daily operations from the risks arising due to failure in their other businesses. The separation of core retail banking operations from other types of banking activities is one of the most important reforms introduced by the UK government for strengthening the UK financial system. The retail banking operations involves managing the savings and current accounts, deposits and personal loans activities of its clients. On the other hand, investment banking operations includes raising funds by sale of securities to the companies and the government (Prudential Regulation Authority, 2016).\nThe bank of England is currently placing large emphasis on implementing the ring-fencing rules in the banking sector of the country for improving its ability to withstand the financial shocks. The adoption of such a structure in the banks that will separate retail and commercial activities will reduce the risk for the customers. As such, the regulation of ring-fencing will help in reducing the risks in the main services provided by the banks such as deposits, current accounts, saving accounts and payments. The ring-fencing regulation aims to promote the financial stability in the UK banking sector by improving the resilience of the major banks of the country. It is thus regarded as a major step towards improving the financial system of the UK after the occurrence of the financial crisis. The large banks of the UK by placing the investment and international banking activities at the same level often faces several types of problems. This is the main function of the banks of lending, deposit and payments are put to risk if there occurs some problems in its investment activities such as that occurred during financial crisis (International Monetary Fund, 2016).\nImplementation of Ring-fencing Regulation in ‘HSBC’ UK Bank\nThe ring-fencing regulation will be implemented to the banks that have average deposits of more than £25 billion for more than three years. There are mainly six banks that are subject to implementation of such regulations that are HSBC, Barclays, Lloyds Banking Group, RBS, Santander UK and the Co-operative Bank. The ring-fencing is to be introduced from the year 2019 in the UK banking sector to secure the confidence of the consumers in the banking sector post-crisis. As such, the PRA is currently emphasizing on developing supervision policies for monitoring the banking activities after the development of their ring-fenced structures. The PRA holds the responsibility of examining the extent of compliance of the banks with the ring-fencing provisions and posses the electrification power in case of non-compliance of bank with the new regulations. The electrification power of the PRA enables them to implement structural changes in the banks so that they comply with the ring-fencing regulations. The ring-fenced structures will enable the banks to manage the failure in its investment activities in a proper way without impacting its daily business operations. This in turn will help in reducing the occurrence of the financial crisis in the future and thus strengthening the UK economy (Financial Conduct Authority, 2016).\nThe central bank is also facing several challenges for the adoption of the ring-fencing regulations in the UK banking sector. The banks have to incur a huge expenditure for implementing the ring-fencing reforms. As such, the increase in the cost for introducing such regulations will have an impact on the money available for lending purposes by the banks. The adoption of ring-fencing structure will also increase the complexity level for the customers as they have to create new accounts for supporting these structural changes. The ring-fencing regulations proposed by the PRA will only be applicable to the large banks of the UK thus creating an asymmetry in the banking sector (Vickers, 2011). The banks that are subject to the implementation of the new regulations will have to restructure their operational activities completely by transforming the structure of their balance sheet and governance system. This will lead to the occurrence of an operational risk in the banking structure if they fail to successfully adopt the proposed changes. In addition to this, the ring-fencing regulations will increase the competition in the banking sector of the UK between the larger banks. Also, there is significant risk of occurrence of fraudulent activities in the banking sector with the changes in the accounts numbers and customer codes. The fraudsters can send emails to the customers for seeking their personal details and thus can gain access to their accounts. As such, it is essential that PRA should work in co-operation with the banks to facilitate communication and thus reducing the chances of occurrence of such risks (Farlow, 2013).\nChallenges in Implementation\nThe HSBC Bank is recognized to be largest banking and financial services provider organization in the world and comprises of 7,500 offices in over 80 countries. It provides variety of services to its customers such as loans, mortgages, savings, investments and credit cards. The HSBC is recognized to be the first bank that has initiated the process of adopting the ring-fencing regulations of the PRA in the UK. The bank has started the process of changing the account numbers and customer codes in order to comply with the ring-fencing rules. However, this has resulted in causing disruption in the customer services due to re-direction of payments and replacement of cards. The HSBC has also provided warning to its customers about the possibilities of occurrence of fraudulent activities so that customers get alert and does not become prone to such frauds. The customers are also require to update the details of their account on their own for the purpose of carrying out online purchases and other such services (Kynaston and Roberts, 2015).\nThe bank has also predicted that its restructuring is likely to impact about 1 million people across the banking sector. The bank has proposed that under the new regulations it will separate its personal and business customers in the UK by causing changes in its legal entity name from HSBC Bank plc to HSBC UK Bank plc. For this purpose, the customers of the bank in the UK will receive a new international bank account number (IBAN) due to the bank being provided by a unique Bank Identifier Code (BIC). The customers will be provided detailed information about the proposed changes so that they can comply easily with the mentioned changes (Meadows, 2017). The HSBC has also provided sufficient details about the impact of the ring-fencing changes on its retail and investment customers. The retail bank customers include all the personal and commercial customers of the bank in the UK that will be directly affected by the structural changes. The customers bank account number and sort code are likely to change that will cause them problem in their payments. The HSBC bank is also aiming to obtain the permission of High Court for transferring its personal and business customers in the UK to HSBC UK. The compliance with the new banking regulations in the UK will cause the bank to restructure its operations that involves huge time and money. Also, the bank at the same time has to maintain the confidence of its customers in its banking facilities by reducing their inconvenience and helping them at every stage (HSBC Bank plc, 2017).\nIn addition to this, the HSBC bank is also facing issues regarding the implementation of the new rules in the year 2009 as these are taking place at the same time when the Competition and Markets Authority (CMA) is planning to investigate in the banking industry. As such the bank is asking the government to delay its plan of implementing ring-fencing regulations. This is because the investigation of the banking sector by the CMA will also cause some structural changes and it could recommend some structural cages that can make it difficult for the UK banks to operate under the new ring-fencing regulations. The HSBC is also facing several issues for putting the ring-fencing arrangements in place. This includes gaining funds to meet the increasing expenses of restructuring that means the bank should either increase the prices for their personal and business clients or pulling out the deals where the returns are low (Treanor, 2014).\nThe bank also needs to develop more international banking relationships for diversifying the sources for raising the funds. The bank also needs to actively work for separating its ring-fenced and non-ring-fenced services as per the new banking regulations in order to ensure that its retail and investment operations are completely independent of each other. In this context, the bank should ensure that its new ring-fenced HSBC UK will provide retail baking services to its personal and commercial customers in the UK. On the other hand, its non-ring-fenced HSBC Bank Plc will provide investment banking services to the global banks and market customers through its wholesale and investment banking division (HSBC 'given licence for ring-fenced UK banking business', 2017).\nThe challenges discussed above are not only impacting HSBC bank but these issues can be regarded to be sector-specific rather than being banking-specific. The overall banking sector of the UK is facing these issues for complying with the ring-fencing regulations. The ring-fencing regulation is presenting an attractive opportunity to the international banks for gaining presence in the UK banking sector. The European, US and Asian banks are becoming active in the UK banking industry and this will lead to increase in the competition between the domestic and foreign banks. The domestic banks of the UK are largely facing pressure to comply with the new regulations and as such the rise in competition by the foreign players will further impact their growth and development. The foreign banks have an undue advantage of offering cheap banking services to the UK customers as these are not impacted by the ring-fencing rules (Reilly, 2017).\nThus, it can cause the increase in the market share of the foreign banks in the banking industry causing potential problems for the domestic banks of the UK. The banking sector of the UK is also faced with the challenge of protecting the customers from the possibilities of occurrence of fraudulent activities. The banking industry as such is required to work in integration with each other to promote awareness among the customers about the numerous frauds that can occur with the structural changes imposed in the banking sector. The PRA should work in co-operation with the banking sector so that they can successfully comply with the new banking regulations. This will help the banks to avoid the occurrence of financial crisis and maintain stability in their operational activities. However, it is essential for the government to make proper arrangements such as improving the loss-absorbing capacity of banks and outlining a structural plan so that they can successfully implement the ring-fencing structural regulations (Bank ring-fencing reforms to affect one million customers, 2017).\nThus, it can be summarized form the overall discussion held in the report that ring-fencing regulation of the PRA aims to impose higher standards of conduct in the UK banks. The reform is introduced with the objective of improving the financials stability of the UK banking sector after the occurrence of global financial crisis. The ring-fencing regulation is planned to be adopted by the UK banks from the year 2019 that will require the banks to separate their retail and investment banking operations. The ring-fencing regulations will be implemented in the major banks of the UK such as HSBC and Barclays who satisfy its threshold criteria. The HSBC is recognized to be the first bank to initiate the structural changes by developing a new legal entity known as HSBC UK for complying with the ring-fencing regulations. However, the HSBC is facing several challenges for restructuring its baking activities as per the new rules. The bank is required to maintain the trust of its customers and also have to meet the significant expenses caused in its restructuring. The challenges that are faced by HSBC banks for complying with the ring-fencing regulations adequately is not only limited to a specific bank but impacts the overall banking sector of the UK. As such, all the large banks of UK should work in integration in order to successfully comply with such reforms and prevent the entry of foreign banks in the UK banking industry.\nBank ring-fencing reforms to affect one million customers. 2017. [Online]. Available at: https://www.out-law.com/en/articles/2017/june/bank-ring-fencing-reforms-to-affect-one-million-customers/ [Accessed on: 26 October 2017].\nFarlow, A. 2013. Crash and Beyond: Causes and Consequences of the Global Financial Crisis. OUP Oxford.\nFinancial Conduct Authority. 2016. Ring Fencing. [Online]. Available at: https://www.fca.org.uk/consumers/ring-fencing [Accessed on: 26 October 2017].\nHSBC Bank plc. 2017. UK Ring-fencing. [Online]. Available at: https://www.hsbc.co.uk/1/2/ringfencedbank [Accessed on: 26 October 2017].\nHSBC 'given licence for ring-fenced UK banking business'. 2017. [Online]. Available at: https://www.belfasttelegraph.co.uk/business/news/hsbc-given-licence-for-ringfenced-uk-banking-business-35905308.html [Accessed on: 26 October 2017].\nInternational Monetary Fund. 2016. United Kingdom: Financial Sector Assessment Program-Financial System Stability Assessment. INTERNATIONAL MONETARY FUND.\nKynaston, D. and Roberts, R. 2015. The Lion Wakes: A Modern History of HSBC. Profile Books.\nMeadows, S. 2017. HSBC changes 170,000 customers' sort codes: what you need to know. [Online]. Available at: https://www.telegraph.co.uk/personal-banking/current-accounts/hsbc-change-170000-customers-sort-codes-need-know/ [Accessed on: 26 October 2017].\nPrudential Regulation Authority. 2016. The implementation of ring-fencing: reporting and residual matters – CP25/16. [Online]. Available at: https://www.bankofengland.co.uk/pra/Pages/publications/cp/2016/cp2516.aspx [Accessed on: 26 October 2017].\nReilly, A. 2017. LPC-UK banks face loan ring-fencing challenge. [Online]. Available at: https://www.reuters.com/article/ringfence-loan-loans/lpc-uk-banks-face-loan-ring-fencing-challenge-idUSL8N1MA62U [Accessed on: 26 October 2017].\nTreanor, J. 2014. HSBC chairman calls for halt on rules ringfencing high-street business. [Online]. Available at: https://www.theguardian.com/business/2014/aug/03/hsbc-chairman-douglas-flint-calls-halt-rules-ringfencing-high-street-business [Accessed on: 26 October 2017].\nVickers, J. 2011. Independent Commission on Banking final report: recommendations. The Stationery Office.\nTo export a reference to this article please select a referencing stye below:\nMy Assignment Help. (2018). Understanding UK Banking Regulation Ring-Fencing. Retrieved from https://myassignmenthelp.com/free-samples/uk-banking-regulation-ring-fencing.\n\"Understanding UK Banking Regulation Ring-Fencing.\" My Assignment Help, 2018, https://myassignmenthelp.com/free-samples/uk-banking-regulation-ring-fencing.\nMy Assignment Help (2018) Understanding UK Banking Regulation Ring-Fencing [Online]. Available from: https://myassignmenthelp.com/free-samples/uk-banking-regulation-ring-fencing\n[Accessed 23 February 2024].\nMy Assignment Help. 'Understanding UK Banking Regulation Ring-Fencing' (My Assignment Help, 2018) <https://myassignmenthelp.com/free-samples/uk-banking-regulation-ring-fencing> accessed 23 February 2024.\nMy Assignment Help. Understanding UK Banking Regulation Ring-Fencing [Internet]. My Assignment Help. 2018 [cited 23 February 2024]. Available from: https://myassignmenthelp.com/free-samples/uk-banking-regulation-ring-fencing.","The Dodd-Frank Act was enacted on July 21, 2010 after intense debate within the investment banking industry and the political realm in Washington D.C. to address the consequences of the “Lehman Shock” and other lessons learned from the 2008 financial crisis that affected the world economy.1 One area that congress did not consider in depth during the legislative process of the Dodd-Frank Act was the effect the act would have on foreign banking organizations. However, with the global economy evermore integrated today, any regulation or change to the financial system in the U.S. would have transnational implications. Hence, there are several provisions, most notably, the Volcker Rule that have extraterritorial consequences in the Dodd-Frank Act that foreign banks should consider.\nThis rule was named after the former Federal Reserve Chairman Paul Volcker, who promoted the rule to reduce the risk created by financial institutions taking on risk as a principal investor. This rule applies to all financial institutions. Regarding the overview of the rule, the proposed interagency rules issued in October of 2011 are very complex and have raised many questions.2 The proposal received over 17,000 comments, including extensive comments from foreign banks, their trade associations, and foreign governments. The basic prohibitions, which are subject to exceptions, apply to banking entities. It does not allow proprietary trading or investing in covered hedge funds and private equity funds for banking entities. A “banking entity” consists of U.S. banks, bank holding companies including foreign banks with U.S. bank subsidiaries, foreign banks with U.S. branches, and any affiliate of these entities, including insurance and securities affiliates of foreign banks.3 Banking entities functioning solely in trust or fiduciary capacity such as trading or investing as trustee on behalf of trust customers are exempt from this rule.\nOn the prohibition of proprietary trading, the Volcker Rule prohibits taking positions as principal in trading account in securities, derivatives, futures, options or any other security or financial instrument as provided by rule. Moreover, trading account used to hold positions for short-term resale, benefiting from short term price movements, or realize short-term arbitrage profits are not allowed in the Volcker Rule.4 This prohibition is not based on the type of instrument however: the financial instrument can be held, as long as it is not held for proprietary trading. Permissible financial positions in a trading account include loans, commodities, foreign exchange (but not foreign exchange forwards, which are treated as derivatives), U.S. government and U.S. government agency securities, and investment in small business investment companies. In the current proposal, there is no comparable exemption for sovereign debt of other countries and this is a huge issue criticized by foreign governments and banks.5) This is an example that when the Dodd-Frank Act was legislated, it did not consider in depth, the foreign or global consequences of the Act.\nPermissible transactions in the Volcker Rule include transactions as agent for customers, underwriting or market-making activities, and risk mitigating hedging activities. All these permissible activity’s burden is on the bank to show compliance with exemptions. For Foreign banks, trade executed “wholly outside” the U.S. is exempt.6 In order to meet the conditions, the foreign bank must conduct the majority of its business and banking activities outside the U.S. and must not be controlled by a banking entity organized under U.S. law. Also, the counterparty to the trade cannot be a U.S. resident and no personnel directly involved in the trade should be located in the U.S. Moreover the transaction must be executed “wholly outside” the U.S. while the entity conducting the trade must be in compliance with currently applicable foreign bank regulations for its activities. For foreign banks, the concerns of the Volcker Rule should be that the prohibition does apply to U.S. branches and agencies, U.S. bank subsidiaries and U.S affiliates of the foreign bank.7 The use of U.S. facilities to execute trade may also take the exemption away since exchanges in the U.S are not permitted. This means that the foreign bank’s U.S. offices or affiliates cannot be involved in transactions as a broker or intermediary to facilitate the trade. The exemption for foreign banks is also conditional on adherence by the banks with certain safety and soundness standards, called “Prudential Backstops.”\nOverall, these newly proposed regulations would create many burdens for foreign banks. The requirements include liquidity requirements, debt-to-equity ceiling, early remediation provisions, stress testing requirements, risk management requirements, intermediate holding companies for large scale U.S. operations etc. that was not explained in depth in this article. It will take close cooperation and integration of governance of the banks worldwide to successfully implement the new provisions in the Dodd-Frank Act.\nDavid Skeel, The new financial deal: understanding the Dodd-Frank Act and its (unintended) consequences 58 (John Wiley & Sons, 2010). ↩\nChen Zhou, Are banks too big to fail? Measuring systemic importance of financial institutions, 6 Int’l J. Cent. Bank 229, 231 (2010). ↩\nCharles K. Whitehead, The Volcker Rule and Evolving Financial Markets., 1 Harv. Bus. L. Rev. 11-19 (2011). ↩\nMatthew Richardson, Roy Smith, and Ingo Walter, Regulating Wall Street: The Dodd-Frank Act and the New Architecture of Global Finance 191 (John Wiley & Sons, 2010). ↩\nCharles K. Whitehead, The Volcker Rule and Evolving Financial Markets, 1 Harv. Bus. L. R. 39, 51, 71–73 (2011 ↩\nId. at 15 ↩"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:ca7bdc0f-6326-4ddf-ba84-e76b21f8dd43>","<urn:uuid:8fa92271-455f-4bb2-ac12-daae3550e074>"],"error":null}
{"question":"What cellular adaptations allow Nostoc punctiforme to survive in extreme conditions, and how do these characteristics inform modern hydrogen production research?","answer":"Nostoc punctiforme can differentiate into three distinct forms (heterocysts, akinetes, and hormogonia filaments) in response to environmental conditions. Specifically, its extracellular polysaccharides (EPS) enable survival during desiccation, freezing, and thawing, while its ability to switch between autotrophic and heterotrophic metabolism allows adaptation to various conditions. These adaptive capabilities have informed modern hydrogen production research, where scientists are studying biological water splitting systems and developing strategies to help organisms maintain hydrogen production under varying conditions, including screening for naturally occurring organisms with enhanced stress tolerance and creating new genetic forms for sustained hydrogen production.","context":["A Microbial Biorealm page on the genus Nostoc punctiforme\nHigher order taxa\nGenus : Nostoc\nSpecies : punctiforme\nDescription and significance\nNostoc punctiforme is of the genus Nostoc; which is also known as cyanobacteria and formerly known as blue green algae. N. punctiforme is a filamentous, motile cyanobacterium with a very complex and unique life cycle. This life cycle, which allows for the bacteria to differentiate into one of three forms in response to various environmental factors, is partly accountable for the enormity of the genome: it must allow for numerous cellular response mechanisms which allow for this microbe to exist in drastically different environments (3). The genome, which has been completed by the Doe Joint Genome Institute, is one of the largest sequenced bacterial genomes to date.\nN. punctiforme’s ability to differentiate into three distinct cellular forms places it in a unique group of microbes. The three forms, which are dependent on environmental factors, are heterocysts, akinetes, and hormogonia filaments (see cell structure and metabolism). Comprehension on its ability to respond to environmental ques so efficiently may prove to become a biotechnological advantage in the future.\nBeing one of the most ancient and well known aerobes, N. punctiforme has been found to participate in numerous symbiotic events. In these relationships, N. punciforme increases its rate of nitrogen fixation for the use of fixed nitrogen by its partner.\nNostoc punctiforme was isolated from symbiotic association with the gymnosperm cycad Macrozamia sp.(4) and it is larger than any of the other sequenced cyanobacterial genome, with an astounding current base count of 9,757,495 (3). The enormity of the genome is necessary for the cell to be able to adapt to numerous drastically different environments. It is a circular genome that consists of 45.2 mol % Guanine and Cytosine pairs and 54.8 mol % Adenine and Thymine pairs (4). Each cell contains multiple copies of the circular chromosome (5) which is hypotheisezed to be one of the reasons that N. punctiforme are able to withstand DNA mutagenisis due to prolonged exposure to UV light (6).\nOf the 5000 recognized open reading frames (ORFs), 62% encode for proteins with a known or probable function. Curiously enough, one fourth of the genome encodes ORFs that cannot be associated with any previously sequenced ORF. Of the recognized ORFs, 5%, the largest proportion of genes, code for signal transduction mechanisms which can be understood by the cell’s extensive ability to sense and respond to environmental factors. Cell envelope synthesis, cell division and chromosome segregation account for 4% of the ORFs, as do amino acid transport and metabolism. Organic carbon metabolism codes for 3% of the ORFs (3). Apart from the coding DNA, the genome for N. punctiforme has been found to contain numerous insertions sequences and multilocus repeats as well as genes that code for transposase and DNA modification enzymes (2). These are characteristics that contribute to the genomic variation within the species.\nCell structure and metabolism\nNostoc punctiforme is a gram positive prokaryote capable of survivng in radically different environments. Under optimal growth conditions, it can grow as filaments several millimeters long. In order to grow in diverse conditions, however, it must be capable of multiple metabolic pathways. N. punctiforme is usually an autotroph; however, it can behave as a heterotroph if it is removed from light for an extended period of time (3). In this state, the cell relies heavily on the oxidative pentose phosphate pathway. In conditions that provide available light, the cell has not only the compound chlorophyll a (standard in most autotrophs), but also light harvesting pigments which allow it to thrive in competitive habitats or in cases of constant UV light (2).\nOf all the metabolic processes it is capable of, N. punctiforme is most recognized for its efficient nitrogen fixation capablilities. It can utilize various inorganic and organic nitrogen sources which reduces its competition in its habitat. However, when nitrogen sources are poor or nonexistant, N. punctiforme will begin heterocyst differentiation, the first of three possible cellular forms for the prokaryote. This results in the termination of oxygenic photosynthetic reactions and conversion to heterotrophic metabolic mode. There will also be a sharp increase in respiration rate(3). In this state, the cells are 6-10 um in diameter (4). In an environment where there is a limitation on nutrient sources other than nitrogen, the cell will convert into a spore, which for this particular species is known as an akinete. In an akinete form, with a diameter between 10 and 20 um(4), the bacteria can remain viable for hundreds of years. However, there is little information on their metabolic pathways at this time. The third potential form for N. punctiforme takes place under high stress. When this occurs, the cell takes on the form of a hormogonium filament. In this state, the cells will undergo division without the usual precursors of an increase in cell biomass and DNA replication. These cells become short, motile filaments that express photo- and chemotactic behaviors. They also produce gas vesicles in order to create buoyancy and a gliding motility in soil or water(3). The hormogonium filaments are 1.5 to 2 um in diameter (4).\nThe ability of N. punctiforme to differentiate between cell structures and accommodate metabolic pathways accordingly is the reason it is one of the most versatile and adaptatious microbes understood today.\nAs one of the oldest known aerobes, with a fossil record dating back from 3 to 3.5 billion years ago, ancestors to N. punctiforme are known to have drastically changed the ancient biosphere in two ways. The first is in the photosynthetic oxygen production and release. This oxygen saturated reactive chemicals near it and increased the concentration of oxygen in the atmosphere. The second way is its formation of endosymbiotic associations with eukaryotic cells that led to the evolution of chloroplast-containing algae and terrestrial plants.\nToday, N. punctiforme is still participating in symbiotic relationships with fungi and terrestrial plants, including gymnosperms, angiosperm, and bryophytes (2). In these partnerships, N. punctiforme decreases its rate of photosynthesis an dincreases the rate of both its nitrogen fixation and its heterotrophic metabolism. In return, its partner will produce molecules which regulate the differentiation of the microbe into one of its three forms: heterocyst, akinete, or hormogonia filament(3).\nBeing a very versatile species, largely due to its cabpability of morphing between these three forms, N .punctiforme has a variety of possible habitats. Though it is most often found in terrestrial habitats, it can also survive in fresh water, tropical, temperate and polar terrestrial systems. In the aquatic and terrestrial ecosystems, N. punctiforme will often be found as a colondy of filaments in a gelatinous matrix (4).\nThere are no known diseases caused by N. punctiforme in any species.\nApplication to Biotechnology\nFurther studies of N. punctiforme are likely to produce information on global regulation of multiple developmental pathways, symbiotic associations, and regulation of carbon and nitrogen fixation (4). This may become crucial information due to the dense growing “blooms” of N. punctiforme; of which many have been found to be toxic. These toxic populations are a growing concern for public health. Full comprehension and understanding of the species and its genome will help us manage and maintain their populations (6)\nAlso, due to the unique diversity and versatility of the genome of N. punctiforme, it serves as a novel place to further delve into the signal transduction pathways and cellular response mechanisms.\nOne recent study explored the effect of Hydrogenase on gas exchange withing N. punctiforme. During nitrogen fixation, N. punctiforme produces hydrogen which is then taken up by a hydrogenase. It was found that both hydrogen production and nitrogen fixation rates are dependent on the light source and intensity. In wild type N. punctiforme, and increase in light leads to an increase in hydrogen production, whilst there is no observable effect on the rate of nitrogen fixation. In prolonged exposure to light, N. punctiforme will actually decrease the reate of nitrogen fixation. However, when a hydrogenase deficient N. punctiforme mutant is analyzed, it can be seen that in prolonged exposure to light it will rather increase the hydrogen production. These findings are important for understanding nitrogenase-based systems to be able to direct cells towards hydrogen production as opposed to nitrogen fixation (9).\nAnother study recently took interest in the role of extracellular polysaccharides (EPS) in stress tolerance of N. punctiforme. In times of dessication, also known as dehydration, N. punctiforme ceases all photosynthetic pathways, and therefore does not produce oxygen. However, immediately upon rehydration of the cells, their cell walls are able to evolve oxygen at a high rate. This was found only to be true of samples that had EPS in their extracellular environment: their recovery rate was much higher. Also, only cells with EPS were able to withstand a freeze-thaw treatment. Concludingly, EPS is essential for the cell to survive conditions of extreme stress, such as dessication, freezing and thawing (10).\nIn one recent study, acyl-lipid desaturase isolated from N. punctiforme, was found to convert a C-C single bond into a C=C double bond in fatty acids that are bound to glycerolipids on the membrane. This may provide to be an essential characteristic for N. punctiforme’s ability to survive in drastically different habitats (11).\nFurthermore, N. punctiforme’s transcriptome, or the number of genes being transcribed and used , has been analyzed in different states of the cell. When grown in a ammonia rich media, the transcriptome consists of 2,935 genes; nearly double that of the steady state proteome. When grown in dinitrogen rich media, the cells most often differentiate into their heterocyst form to increase nitrogen fixation. In this state, they are using 495 genes. In low nutrient media, N. punctiforme will differentiate into a spore, known as an akinete. This cell structure utilized 497 genes. In circumstances of extreme stress to the cell, it will convert into its hormogonia filamentous form. This calls for unregulated cell division and requires 1,827 genes in the genome to be in use. The large differences in transcriptomes are indicitave of distinct upstream regulation mechanisms that must be activated for each cell differentiation (5).\n1. NCBI: National Center for Biotechnology Information site: Nostoc Punctiforme, Accessed August 27 2007, “http://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?mode=Info&id=272131&lvl=3&lin=f&keep=1&srchmode=1&unlock”\n2. Meeks, J., Elhai, J., Thiel, T., Potts, M., Larimer, F., Lamerdin, J., Predki, P., Atlas, R.; “An Overview of the genome of Nostoc punctiforme, a multicellular, symbiotic cyanobacterium” Photosynthesis Research v. 70:80-106. 2001.\n3. T. Thiel, J. Meeks, J. Elhai, M. Potts, F. Larimer, J. Lamerdin, P. Predki, R. Atlas; ”Nitrogen Fixation: Analysis of the Genome of the Cyanobacterum Nostoc punctiforme” University of Missouri Department of Biology\n4. “Nostoc punctiforme PCC 73102” Doe Joint Genome Institute. http://genome.jgi-psf.org/draft_microbes/nospu/nospu.home.html\n5. Elsie L. Campbell, Michael L. Summers, Harry Christman, Miriam E. Martin, and John C. Meeks; “Global Gene Expression Patterns of Nostoc punctiforme in Steady-State Dinitrogen-Grown Heterocyst-Containing Cultures and at Single Time Points during the Differentiation of Akinetes and Hormogonia” Journal of Bacteriology v.189 p. 5247-5256: July 2007.\n6. Wright DJ, Smith SC, Joardar V, Scherer S, Jervis J, Warren A, Helm RF, Potts M.; “UV irradiation and desiccation modulate the three-dimensional extracellular matrix of Nostoc (Cyanobacteria)” Journal of Biological Chemistry v.280 p.40271-40281: December 2005.\n7. Meeks, J., Wong, F.; “Establishment of a functional symbiosis between the cyanobacterium Nostoc punctiforme and the bryophyte Anthoceros punctatus requires genes involved in nitrogen control and initiation of heterocyst differentiation” Microbiology v.148 p.315-323: 2002.\n8. Paula Tamagnini, Rikard Axelsson, Pia Lindberg, Fredrik Oxelfelt, Röbbe Wünschiers, and Peter Lindblad; “Hydrogenases and Hydrogen Metabolism of Cyanobacteria” Microbiology and Molecular Biology Reviews p.1-20: March 2002.\n9. Pia Lindberg, Peter Lindblad, Laurent Cournac; “Gas Exchange in the Filamentous Cyanobacterium Nostoc punctiforme Strain ATCC 29133 and Its Hydrogenase-Deficient Mutant Strain NHM5” Applied and Environmental Microbiology p. 2137-2145: April 2004.\n10. Yoshiyuki Tamaru, Yayoi Takani, Takayuki Yoshida, Toshio Sakamoto; “Crucial Role of Extracellular Polysaccharides in Desiccation and Freezing Tolerance in the Terrestrial Cyanobacterium Nostoc” Applied and Environmental Microbiology p. 7327-7333: November 2005.\n11. Suresh Chintalapati, Jogadhenu Shyam, Sunder Prakash, Pratima Gupta, Shuji Ohtani, Iwane Suzuki, Toshio Sakamoto, Norio Murata, Sisinthy Shivaji; “A novel Δ9 acyl-lipid desaturase, DesC2, from cyanobacteria acts on fatty acids esterified to the sn−2 position of glycerolipids” Biochem journal v. 398 p. 207-214: September 2006.\nEdited by student of Rachel Larsen: Bobbi Leal\nEdited by KLB","Hydrogen Production and Delivery\nResearchers at NREL are developing advanced processes to produce hydrogen economically from sustainable resources.\nBiological Water Splitting\nCertain photosynthetic microbes use light energy to produce hydrogen from water as part of their metabolic processes. Because oxygen is produced along with the hydrogen, photobiological hydrogen production technology must overcome the inherent oxygen sensitivity of hydrogen-evolving enzyme systems. NREL researchers are addressing this issue by screening for naturally occurring organisms that are more tolerant of oxygen and by creating new genetic forms of the organisms that can sustain hydrogen production in the presence of oxygen. Researchers are also developing a new system that uses a metabolic switch (sulfur deprivation) to cycle algal cells between the photosynthetic growth phase and the hydrogen production phase.\nContact: Maria Ghirardi\nNREL scientists are developing pretreatment technologies to convert lignocellulosic biomass into sugar-rich feedstocks that can be directly fermented to produce hydrogen, ethanol, and high-value chemicals. Researchers are also working to identify a consortium of Clostridium that can directly ferment hemicellulose to hydrogen. Other research areas involve bio-prospecting efficient cellulolytic microbes, such as Clostridium thermocellum, that can ferment crystalline cellulose directly to hydrogen to lower feedstock costs. Once a model cellulolytic bacterium is identified, its potential for genetic manipulations, including sensitivity to antibiotics and ease of genetic transformation, will be determined. NREL's future fermentation projects will focus on developing strategies to generate mutants that are blocked selectively from producing waste acids and solvents to maximize hydrogen yield.\nContact: Pin-Ching Maness\nConversion of Biomass and Wastes\nHydrogen can be produced via pyrolysis or gasification of biomass resources such as agricultural residues like peanut shells; consumer wastes including plastics and waste grease; or biomass specifically grown for energy uses. Biomass pyrolysis produces a liquid product (bio-oil) that contains a wide spectrum of components that can be separated into valuable chemicals and fuels, including hydrogen. NREL researchers are currently focusing on hydrogen production by catalytic reforming of biomass pyrolysis products. Specific research areas include reforming of pyrolysis streams and development and testing of fluidizable catalysts.\nContact: Richard French\nPhotoelectrochemical Water Splitting\nThe cleanest way to produce hydrogen is by using sunlight to directly split water into hydrogen and oxygen. Multijunction cell technology developed by the photovoltaic industry is being used for photoelectrochemical (PEC) light harvesting systems that generate sufficient voltage to split water and are stable in a water/electrolyte environment. The NREL-developed PEC system produces hydrogen from sunlight without the expense and complication of electrolyzers, at a solar-to-hydrogen conversion efficiency of 12.4% lower heating value using captured light. Research is underway to identify more efficient, lower cost materials and systems that are durable and stable against corrosion in an aqueous environment.\nSolar Thermal Water Splitting\nNREL researchers use the High-Flux Solar Furnace reactor to concentrate solar energy and generate temperatures between 1,000 and 2,000 degrees Celsius. Ultra-high temperatures are required for thermochemical reaction cycles to produce hydrogen. Such high-temperature, high-flux, solar-driven thermochemical processes offer a novel approach for the environmentally benign production of hydrogen. Very high reaction rates at these elevated temperatures give rise to very fast reaction rates, which significantly enhance production rates and more than compensate for the intermittent nature of the solar resource.\nContact: Judy Netter\nRenewable energy sources such as photovoltaics, wind, biomass, hydro, and geothermal can provide clean and sustainable electricity for our nation. However, renewable energy sources are naturally variable, requiring energy storage or a hybrid system to accommodate daily and seasonal changes. One solution is to produce hydrogen through the electrolysis—splitting with an electric current—of water and to use that hydrogen in a fuel cell to produce electricity during times of low power production or peak demand, or to use the hydrogen in fuel cell vehicles.\nResearchers at NREL's Energy Systems Integration Facility and Hydrogen Infrastructure Testing and Research Facility are examining the issues related to using renewable energy sources for producing hydrogen via the electrolysis of water. NREL tests integrated electrolysis systems and investigates design options to lower capital costs and enhance performance.\nLearn more about NREL's renewable electrolysis research.\nContact: Kevin Harrison\nHydrogen Dispenser Hose Reliability\nWith a focus on reducing costs and increasing reliability and safety, NREL performs accelerated testing and cycling of 700 bar hydrogen dispensing hoses at the Energy Systems Integration Facility using automated robotics to simulate field conditions. View the video of the robot, which mimics the repetitive stress of a person bending and twisting a hose to dispense hydrogen into a fuel cell vehicle's onboard storage tank. Researchers perform mechanical, thermal, and pressure stress tests on new and used hydrogen dispensing hoses. The hose material is analyzed to identify hydrogen infiltration, embrittlement, and crack initiation/propagation.\nContact: Kevin Harrison\nHydrogen Production and Delivery Pathway Analysis\nNREL performs systems-level analyses on a variety of sustainable hydrogen production and delivery pathways. These efforts focus on determining status improvements resulting from technology advancements, cost as a function of production volume, and the potential for cost reductions. Results help identify barriers to the success of these pathways, primary cost drivers, and remaining R&D challenges. NREL-developed hydrogen analysis case studies provide transparent projections of current and future hydrogen production costs. Learn more about NREL's systems analysis work.\nContact: Genevieve Saur\nHydroGEN Energy Materials Network\nNREL serves as the lead laboratory for the HydroGEN Energy Materials Network (EMN) consortium.\nRemarkable Stability of Unmodified GaAs Photocathodes during Hydrogen Evolution in Acidic Electrolyte, Journal of Materials Chemistry A (2016)\nSolar to Hydrogen Efficiency: Shining Light on Photoelectrochemical Device Performance, Energy and Environmental Science (2016)\nReversible GaInP2 Surface Passivation by Water Adsorption: A Model System for Ambient-Dependent Photoluminescence, Journal of Physical Chemistry C (2016)\nCO2-Fixing One-Carbon Metabolism in a Cellulose-Degrading Bacterium Clostridium thermocellum, Proceedings of the National Academy of Sciences (2016)\nPhosphoketolase Pathway Contributes to Carbon Metabolism in Cyanobacteria, Nature Plants (2016)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:059a47fd-e017-407f-8cac-972a909f7dd0>","<urn:uuid:13a02b09-8dfc-4538-bd22-78817cd111fe>"],"error":null}
{"question":"I'm opening a startup and need to understand tax compliance. What are the main differences between managing property tax obligations versus collective tax obligations for a new business?","answer":"Property tax obligations and collective tax obligations differ significantly in their management requirements. Property taxes involve a direct relationship between the business and local taxing authorities, where taxes are assessed based on the property's market value and paid according to local millage rates. In contrast, collective tax obligations are more complex, involving both the business and individuals. Businesses must collect, report, and remit specific taxes based on their activities, plus handle withholding and remitting taxes on behalf of employees (like income and payroll taxes). Non-compliance with collective taxation obligations can result in penalties, fines, and interest payments, making proper management crucial for startups.","context":["Starting a business is a complex and daunting endeavor. It takes a great deal of vision, resilience, networking, and knowledge to ensure long-term success. However, one of the most critical aspects of establishing and sustaining a business is understanding taxation – how it works and how it impacts both the financial model and profitability of a business.\nA startup is any business that is in the early stages of its operations. In most cases, these nascent companies are pre-revenue and rely heavily on external investments to maintain their operations. For these young businesses, taxation is a major concern that can greatly impact their success.\nTaxes can be broadly divided into two primary categories – income taxes and property taxes. Income taxes refer to imposed taxes on individuals and organizations based on their income and the resulting profits.\nProperty taxes are those associated with the possession of assets such as real estate, physical properties and equipment.\n- Taxes can be broadly divided into two categories: income taxes and property taxes\n- Income taxes refer to imposed taxes on individuals and organizations based on their income and the resulting profits\n- Property taxes are those associated with the possession of assets such as real estate, physical properties, and equipment\n- Startups should be aware of taxation regulations to ensure their long-term success\nUnderstanding Taxation Impact on a Startup Financial Model\nThe financial health of a business, especially during the start up stage, is greatly impacted by taxation. A business’s ability to manage and comply with tax obligations, arrange payments and calculate financial implications are all essential parts of any startup’s financial model. It is important to fully understand all the potential taxation impacts on a startup business in order to achieve financial success.\nManaging Tax Payments on a Revenue Generating Business\nBusinesses who are earning revenue must pay taxes at the local, state and federal level. It is critical for startups to establish a system for tracking income, expenses and tax obligations, and to understand when taxes need to be reported and paid. The type and amount of taxes being paid depend on the structure of the business, and also the type of revenue being generated.\nComplying with Collective Tax Obligations\nCollective taxes are responsibility of both the business and the individual. Businesses are required to collect, report and remit specific taxes based on certain activities or services provided. Businesses are also responsible for withholding and remitting taxes on behalf of employees, such as income and payroll taxes. Any non-compliance from collective taxation obligations could result in penalties, fines and interest payments.\nFinancial Implications for A Business\nTaxes can have a major effect on the overall financial projections of a startup business. It is important to identify the various tax expenses, liabilities and deductions associated with the startup. A business’s financial model should consider available deductions, tax credits and other expenses related to taxation to ensure the financial projections remain accurate.\n- Understand the types and amounts of taxes being paid.\n- Pay attention to collective tax obligations.\n- Include tax expenses, tax credits, deductions and liabilities\n- Calculate estimated taxes and plan for payment.\nRelevant Taxes for a Business\nTaxes are a large part of managing a successful business, and startups are no exception. When assessing the financial model of a startup, it is important to consider the various taxes that apply to a business. The four main taxes a business should assess are income taxes, sales and use taxes, payroll taxes, and property taxes.\nIncome taxes are based on a business’s total taxable income and are generally calculated on a quarterly or yearly basis. Depending on the type of business, the guidelines for filing income taxes will vary, though all business owners should familiarize themselves with their state and federal income tax regulations. Businesses may also be required to withhold federal and state income tax from employees’ paychecks, though the specifics of this will depend on the state in which the business is located.\nSales and Use Taxes\nSales and use taxes are levied by a state or municipality on the purchase of goods and services. Business owners should make sure to check the regulations of their state or local jurisdiction so they can correctly assess and collect the applicable taxes when they are required to do so. It is also important to understand that certain products or services may be exempt from sales and use taxes, depending on specific state and local regulations.\nBusiness owners are responsible for the accurate and timely payment of payroll taxes for their employees. Payroll taxes include federal and state withholdings, Social Security, and Medicare taxes. The exact calculations for these taxes will depend on the type of business and the number of employees, and business owners should be diligent in staying up to date on the changing payroll taxes.\nWhen a business owns real or personal property, they may be liable for property taxes. Property taxes are generally assessed by county and based on the value of the property in question. Business owners should be aware of the different tax rates in their locality and any exemptions they may be eligible for.\nHow Taxation Affects a Startup’s Financial Decisions\nStartups face a variety of financial decisions each day, many of which are affected by taxation laws. Understanding the implications of taxation on a financial model is an essential component of running a successful business. Below, we take a look at some of the ways in which taxation affects a startup’s most common financial decisions.\nTaxes play a major role in investment decisions for startups. A variety of investment options, such as stocks, bonds, mutual funds, and real estate, are offered to startups in hopes of diversifying their portfolio. Each of these options is subject to various tax implications, so it is important to be aware of them before committing to an investment. In addition, there are certain tax incentives that may be taken advantage of, such as capital gains reductions or exemptions, which can help startups make the most of their investments.\nCash Flow Decisions\nTaxation also affects cash flow decisions. Startups are often cash-strapped early on, and understanding the potential tax implications of their transactions can help them make more informed decisions. For example, startups may be eligible for tax credits or deductions when they choose to outsource certain elements of their business, or they may be able to claim a business use tax credit if they purchase certain assets. Additionally, startups will need to plan for when tax payments are due so they stay on top of any applicable taxes.\nAssessing Market Opportunities\nTaxation also has to be taken into account when evaluating market opportunities. When launching a new product or initiative, startups need to ensure that their business model is able to bear the costs of any associated taxes. Additionally, startups need to factor in the potential tax implications from any new market opportunities, such as corporate income tax, sales tax, and any applicable withholding taxes. Knowing this information early on can help startups make more informed decisions and ensure that their business remains profitable in the long-term.\n- Investment decisions\n- Cash flow decisions\n- Assessing market opportunities\nTips for Implementing Effective Tax Strategies\nTaxes play a critical role in the success of a startup financial model. The impact of taxes, when implemented correctly, can have a drastic impact on the return on investment. To maximize any financial model, it is important for entrepreneurs to understand and implement effective tax strategies.\nExisting Tax Credits\nWhen a startup is in the very early stages of its operations, there may be existing tax credits available to the business. These credits are available through the IRS and based on the company's specific situation. It is important to review the available credits, as they can significantly mitigate the tax liability of the business and ultimately benefit the company's bottom line.\nWhen undertaking any startup activity, it is important to understand and take advantage of deductible expenses. Business expenses are tax-deductible, allowing companies to save on taxes and increase their bottom line. It is crucial to understand the allowable deductions and to keep detailed and accurate records of all expenses incurred by the startup.\nAccurate Tax Planning\nCareful and consistent tax planning is also essential to maximize a startup's financial model. To ensure that all taxes remain up to date, ensure proper tax planning by setting aside a portion of each profit to cover taxes. This allows for proper budgeting and easy access to capital when tax payments are due.\nIn addition to the above, utilizing a tax software is a great way for startups to easily manage their taxes. A tax software can help to keep taxes up to date and ensure that all credits and deductions are properly accounted for.\nTaxes are a critical component of any startup financial model. By understanding and utilizing existing tax credits, deductible expenses, and accurate tax planning, startups can benefit from lower overall tax liability and maximize the return on their investments.\nTracking Tax Deductions for Tax Preparation\nTax deductions can provide immense relief, resulting in lower taxable income and, ultimately, a lower tax bill. For entrepreneurs and startups, proper tracking and understanding of tax deductions can prove invaluable on their company’s bottom line. Here we will look at the strategies and resources needed to ensure that your business receives all the tax deductions it is eligible for.\nAccurate Record-Keeping for Tax Deductions\nAccurately tracking expenses is paramount for a business in order to maximize deductions come tax season. It’s in the company’s best interest to maintain invoices and records of all business-related expenses and transactions, so that the exact amount of money spent by the company in the previous year can be determined. Keeping accurate records of all expenses, from wages to stock and supplies, can greatly reduce your chances of incorrectly filing and save you a considerable amount of money.\nWorking with a Tax Professional\nWorking with a qualified tax professional can ensure that your business is making the most of the deductions available to you. Tax professionals are familiar with the nuances of tax legislation and can suggest ways in which to maximize deductions and save your business money through careful tax planning. Additionally, they may be able to provide insight and advice on the tax consequences of specific transactions or the implementation of particular business plans. Although it may cost extra to work with a tax professional, the advantages they bring can be almost immeasurable in their monetary and time savings.\n- Accurate record keeping for tax deductions.\n- Working with a tax professional.\nBusiness owners need to be aware of taxation impact on their financial model in order to maximize profitability. Understanding the ins and outs of taxation can be complicated, but with the right guidance it is possible to ensure that tax liabilities are managed and optimized without cutting corners.\nSummary of How to Decrease Taxation Impact\nTax optimization is the key to decreasing taxation impact on a financial model. There are different techniques and strategies a business owner can use to be more tax efficient. These include:\n- Prioritizing investments in high-octane growth companies.\n- Taking advantage of corporate tax incentives and credits.\n- Reducing pre-tax deductions and itemized expenses.\n- Being aware of changes in taxation law and taking advantage of opportunities.\nBenefits of Understanding Taxation Impact on a Financial Model\nUnderstanding the overall taxation impact on a financial model can help business owners achieve success and make more informed financial decisions. By taking the time to understand different aspects of taxation, business owners can develop more efficient tax strategies and maximize their financial resources.\nOptimizing taxation strategies can open up opportunities to invest in high-value investments and maximize growth potential. This can ultimately lead to increased profitability and better cash flow, which are essential for any successful business.","Are Property Taxes Based on Area or Price?\nProperty taxes are levied at the municipal or county level by the local government of the taxing jurisdiction where the property is located. In most states, school districts also impose property taxes. Rates vary depending on the school district and municipality or county where a taxpayer lives. When arriving at a property tax, the taxing authority takes into account both the value of the property and the tax rate levied.\nValue of Property\nProperty taxes are based on the value of the property being taxed, with the owners of the real estate being taxed responsible for paying the tax. The taxing authority performs an appraisal of the value of the real property and then assesses a millage tax in proportion to that value. Real estate includes land and any improvements to the land such as homes or other structures. Millage or ad valorem tax differs from one taxing jurisdiction to another. Ad valorem taxes are the major source of income for municipal and county governments.\nWhen assessing the value of real estate to apportion the tax levy, an appraiser researches the current market value or the price for which the home should sell, sales prices of other similar homes in the same area, as well as the cost of replacing structures on the land should they be destroyed. Depending on the jurisdiction, assessments may be based on 100 percent of the property’s value or a lesser percentage.\nThe property tax rate may be expressed as a percentage or as a millage rate. One mill has a value of $1 for every $1,000 of assessed property value. Taxing authorities calculate the property tax for real estate by multiplying the assessed value of the property by the mill rate and then dividing by 1,000. For example, a property with an assessed value of $250,000 at a rate of five mills would equal a tax of $1,250. The purpose of the millage rate is to divide the cost of funding local government and schools equally among all property owners in the taxing jurisdiction.\nThe assessor’s office in a municipality or county has the responsibility of estimating the value of all taxable real property within the jurisdiction of the local taxing body. Assessments are normally based on a property’s fair market value, or the amount of money most buyers would be willing to pay for a property. June Fletcher, a columnist for The Wall Street Journal, points out that tax assessments are often lower than those private appraisal companies do when consumers want to refinance or purchase a home. Just because a tax assessment is low does not necessarily mean that your home would only sell for that amount if you put it on the market. Taxing bodies often decrease home assessment values when the economy is slow.\nImprovements to a property that increase its market value usually will increase the property’s assessed value. The size, location and overall condition or quality of a property can also affect its value. Market conditions may either increase or decrease property values within the same county. Likewise, different types of properties within the same general location may be assessed at different values depending on demand for that type of property. Many taxing jurisdictions revalue property annually or at least periodically in order to make any needed adjustments to a property’s assessed market value. The process helps to ensure that taxes are distributed fairly among taxpayers.\n- school building image by palms from Fotolia.com"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c25dcc59-09e3-4176-b674-70746ae5d59b>","<urn:uuid:3d6a1d80-4a29-465f-8859-cc4a2d1a778a>"],"error":null}
{"question":"I have chronic knee pain - which is more likely to require surgery: general population knee problems or professional soccer knee injuries?","answer":"Professional soccer knee injuries tend to be more severe and more likely to require surgery. According to the research on Major League soccer players, knee injuries resulted in the most time lost from competition and produced the greatest number of cases requiring surgery. In contrast, for the general population, knee pain is often not serious and can be treated through conservative methods like osteopathy, exercise and lifestyle changes, though some conditions may still require surgery.","context":["Specialists In Back Care, Sports Injury & Musculoskeletal Pain Treatments\nBack pain is a very common problem, with reports suggesting as many as eight out of ten of us will suffer from it at some point during our lives. Around 5.6 million working days in the UK are lost each year due to back pain, second only to stress.\nBack pain can affect anyone at any age, and can often be the result of a sprain or a strain of the structures of the back such as the muscles, ligaments, joints or damage to the discs. Osteoarthritis or wear and tear in the back can also be a reason.\nMost of us know that back pain can be painful and inconvenient, but it’s not usually serious and will often resolve on its own within a few weeks. However, many people seek osteopathic treatment to address it quickly and at a time and place of their own choice; and osteopaths are skilled at helping prevent back pain from becoming a chronic, long-term condition.\nBack pain can be brought on by lifting, moving awkwardly or by an accident. Sometimes it can come on without any specific injury to your back. Stress, depression, posture, being overweight, sedentary living and poor lifestyle habits can all be significant factors.\nPeople can feel a range of symptoms such as stiffness, tenderness and mild to very severe pain. The pain can come on quite suddenly or over time, and be located anywhere in the spine from the top of the neck to the pelvis. Sometimes pressure from the back on the nerves can cause pain or pins and needles and numbness in the legs and arms. X-rays, scans and other tests are sometimes required to make a diagnosis. If osteopathy treatment cannot completely heal or discover the cause of the back pain, your osteopath may refer to your GP or a specialist for any additional investigation.\nSome of the back conditions patients visit osteopaths for:\nAcute back pain\nChronic back pain\nSome Disc problems\nMechanical back pain\nArthritis is a common condition which causes pain, swelling and inflammation and often stiffness in the joints of the body.\nThe two most common types of arthritis are osteoarthritis and rheumatoid arthritis but there are many other types including ankylosing spondylitis, gout, psoriatic arthritis and reactive arthritis. Certain types of arthritis can also affect children.\nOsteoarthritis is a result of wear and tear of the joints in the body. It is common in people over 50 and most commonly affects the joints of the knees, hips, neck and back, base of the toes and hands.\nThe gentle manipulative and massage techniques from osteopaths can help some arthritis sufferers. Treatment is individual, gently moving and stretching an arthritic joint and massaging surrounding muscles and tissues can help ease some of the discomfort. Sometimes an osteopath may work on general mobility of the other joints and muscles in the body to help the mechanics of the body work better. Osteopaths may also give advice on exercises, diet, posture and changes to lifestyle. X-rays, scans or other tests may be required and your osteopath may refer you to your GP for any additional investigations and treatment\nPain can occur in the foot and ankles for a number of reasons.\nThe foot and ankle is made up of a number of small bones interconnected by ligaments, muscles and fascia all working together to give the strength, stability and flexibility the foot and ankle needs to function properly. Common conditions of the foot, ankle and areas which can give rise to pain include:\nAcquired flat foot – when the inner side of the foot or inner arch flattens. The foot may roll over to the inner side (known as over-pronation). It is often apparent if the heels of shoes wear out quickly and unevenly. Over-pronation can damage your ankle joint and achilles tendon (the tendon at the back of your ankle) and can also cause shin pain. Symptoms can include, pain, swelling, change in foot shape and knee pain or swelling.\nPlantar fasciitis –is pain and inflammation in the plantar fascia – the tough fibrous band of tissue that supports the arches of the foot and runs under the small bones from the underside of the heel and sole towards the toes, Often, people who have plantar fasciitis describe it as a sharp pain, most often under the heel or instep of the foot. It tends to be made worse by standing for long periods of time in poor footwear. Sufferers commonly mention that it is worse when standing after being off their feet for a long time, and it can hurt more putting the foot on the floor first thing in the morning. The sole of the foot can occasionally feel a little numb, tingly or swell slightly. In some cases of plantar fasciitis, a small spur of bone can grow where the plantar fascia attaches and pulls on the heel which can cause a sharp pain.\nAchilles pain –The Achilles tendon is formed by the tendon of the two calf muscles, the gastrocnemius and soleus coming together and attaching onto the bone at the back of the heel called the calcaneus) Pain, inflammation or tendonitis in the Achilles can cause pain and tightness in this area.\nSprained ankle. Typically the result of a sudden twisting or “going over” on the ankle joint and more commonly it is the ligaments on the outside of the ankle that are strained. Typical symptoms are swelling, bruising, pain and instability of the ankle. Sometimes an x-ray is required to rule out any fracture. Rest, ice, elevation and compression are often advisable in the first 24 to 48 hours.\n•Depending on the diagnosis and your age and fitness we can use a variety of gentle massage and manipulative techniques to increase the mobility of the joints and the flexibility of the muscles in the foot.\n•We will often look at muscles and joints in the lower limb, the knee, hip and lower back and may treat any joint restrictions and muscle tightness we find there. Often improving the movement in the joints of the lower will help the foot and ankle function better.\n•We may offer specific balancing, strengthening or loosening exercises\n•We may offer advice on strapping and brace supports, footwear and any lifestyle factors that might be hindering healing. We may refer you to a podiatrist for their opinion and specialist foot supports\n•X-rays, scans or other tests may be required to make a diagnosis and we may refer you to your GP for any additional investigations and treatment such as advice on pain killers and anti-inflammatory medications\nThe knee is the largest joint in the body. It is a major weight-bearing joint and is one of the most frequently injured joints in the human body.\nKnee pain can have a number of different causes and can be painful and debilitating and although some conditions may require surgery many can be helped with the right advice, exercise and treatment.\nThe knee joint lies between the femur and tibia and at the front is the patella or kneecap. It is made up of a number of structures including ligaments, muscles, capsule, synovial membrane and two ‘c’ shaped pieces of cartilage which sit between the femur and tibia known as the menisci.\nDamage, strain or sprain to the structures of the knee can give rise to symptoms. It can be the result of a sudden injury as often seen in sports injuries or by repeatedly placing strain on an area of the knee. Poor alignment of the knee or kneecap and altered joint mechanics in relation to other joints such as the hips and knees are often significant. Osteoarthritis or wear and tear is a common condition that affects the knee.\nCommon symptoms in the knee include pain, stiffness, aching, pain, locking, swelling, limping and difficulty fully straightening or bending the knee.\nX-rays, scans and other tests are sometimes required to make a diagnosis and your osteopath may refer to your GP or a specialist for any additional investigations or treatment.\nThere are several reasons for headaches. Most are not serious and once the cause is established headaches can often be helped by simple changes in lifestyle. One cause can be tension or strain in the muscles and joints of the neck and upper back.\nTreatment from an osteopath may help. Gentle massage to the tight muscles and manipulation to loosen the joints of the neck, thorax and back can relieve the build-up of muscular tension that may lead to headaches. Osteopaths can also advise on exercise and lifestyle changes and offer guidance on simple changes to your posture when at work or driving which may help.","If you are a serious soccer player, what are your chances of sustaining a serious injury? What can you do to lower your injury risk?\nTo find answers to these key questions, researchers from the San Jose Earthquakes professional soccer team and the East Bay Sports Medicine and Orthopaedic Associates in California recently followed 237 Major-League soccer players over the course of a full season (Morgan, B. & Oberlander, M., 'An Examination of Injuries in Major League Soccer,' The American Journal of Sports Medicine, Vol. 29(4), pp. 426-430, 2001). The players ranged in age from 18 to 38.\nThe researchers defined a minor injury as one that interrupted participation in practice or play for a period of less than one week, a moderate injury as one necessitating absence for more than one week but less than one month, and a major injury as a malady which caused an inability to participate lasting for more than one month.\nThere were ten professional teams involved in the research, and each team played 32 regular-season games. Each team's practice sessions averaged 105 minutes, and the researchers multiplied the average practice time by the number of practice sessions performed during the season to arrive at a total practice time of 241 hours per athlete. This was eight times greater than the total time per athlete spent in competition (27.7 hours). Overall, the 237 players had a total 'exposure' of about 64,000 hours (57,117 practice hours and 6,567 game hours).\nDuring the season, there were a total of 256 injuries which resulted in time lost from participation. 154 injuries were minor, 67 were moderate, and 35 were classified as serious. No position (midfielder, goalkeeper, defender, or forward) was associated with a higher rate of injury, but games were significantly more risky than practices. Injuries occurred during practices at a rate of about 2.9 injuries per 1000 hours, while the injury rate for games was 35.3 per 1000 hours, about a twelve-fold increase. A higher proportion of severe injuries also occurred during games. Player age had no impact on injury risk, and late season was found to be the time period with the highest rate of injury.\nKnee injuries were the worst\nAs you might expect, the majority of injuries (77%) occurred in the leg, with 21% involving the knee and 18% striking the ankle (many other studies point to the knee and ankle as the most common sites of injury during soccer). Knee injuries resulted in the most time lost from competition and produced the greatest number of cases requiring surgery. At the ankle, lateral sprains were more common than medial ones (ie, damage to the outside ankle ligaments was much more likely than harm to the inside ligaments).\nThere were also a considerable number of strains of the hip-adductor muscles, the hamstrings, and the quadriceps. The hip adductors, or groin muscles, were the hottest trouble spot, accounting for 53% of muscle strains, with the hams checking in at 42% and the quads adding just 5% to the total mayhem.\nThis new research supports other studies which have also suggested that games are particularly risky from an injury standpoint, compared with practices. In one study, researchers found that there were 16.9 injuries per 1000 hours in games, versus 7.6 injuries per 1000 hours in practices (Ekstrand, J. et al., 'Incidence of Soccer Injuries and Their Relation to Training and Team Success,' American Journal of Sports Medicine, Vol. 11, pp. 63-67, 1983). In another investigation, the rate of injury during games was pegged at 13 per 1000 hours, while practice logged only three injuries for each 1000 hours of participation (Engstrom et al., 'Soccer Injuries among Elite Female Players,' American Journal of Sports Medicine, Vol. 19, pp. 372-375, 1991).\nHow does soccer stack up against other sports in terms of injury risk?\nAlthough endurance running is often considered to be a high-injury sport (about 50 to 65% of all endurance runners are injured in an average year), the actual rate of injury in running is comparable with that of soccer. Various studies suggest that the running injury rate is about five injuries per 1000 hours of running, with little difference between training and competition ('Prevention of Running Injuries by Warm-Up, Cool-Down, and Stretching Exercises, The American Journal of Sports Medicine, Vol. 21(5), pp. 711-719, 1993). This is a little higher than the 'practice' rate of injury for soccer but much lower than soccer's competition-related injury rate. Injury rates in other sports have been poorly studied, but it appears that soccer has one of the highest rates of athletic damage (Sportblessures breed Uitgemeten, Haarlem, DeVriesborch, 1990).\nWhat footballers can do about it\nHow can soccer players reduce their risk of getting hurt? To minimize the risk of hip-adductor strains, they should carry out Indian-hop exercises. To Indian-hop, jog for a few strides and then 'jog diagonally to the right' by taking a step (with your right foot) which has a direction about 45 degrees clockwise from a straight-ahead step (ie, about half-way between a straight-ahead step and a step which is absolutely lateral). To put it another way, step to the 'north-east' with your right foot (assuming you have been jogging in a perfectly 'northern' direction to begin with). When your right foot makes contact with the ground, hop one time in place. When your right foot comes down to earth from this single hop, explosively hop diagonally to the left (to the 'north-west'), landing on your left foot. When your left foot strikes the ground, hop once in place and then explode diagonally to the right (north-east again). Your right foot will then hit, hop, explode diagonally to the left (not literally), and so on. Stay relaxed at all times as you carry out this drill; try to move in a coordinated and rhythmic manner. Indian-hopping is also very good for the ankles.\nTo strengthen the knees, one-leg squats with lateral hops work very well (see also Exercise two on page ten). To carry these out, stand with your left foot forward and your right foot back, with your feet about one shin-length apart (they should be hip-width apart from side to side). If possible, place the toes of your right foot on a block or step which is six to eight inches high. Most of your weight should be directed through the heel of your left foot. Now, bend your left leg and lower your body until your left knee reaches an angle of 90 degrees between the thigh and lower leg. Once your left knee reaches an angle of 90 degrees between the thigh and lower leg, hop laterally (with your left foot; the right foot stays in place) about six to ten inches, hop back to 'centre', and then hop medially (to the right when your left leg is forward) for six to ten inches, before coming back to the centre position and then returning to the starting position, maintaining upright posture with your trunk and holding your hands at your sides. Start with two sets of 12 reps on each leg, and progress to more sets and reps.\nFor the ankles, balance-board work is optimal. Try the following routines:\n1. Side-to-Side Edge Taps. Place one foot directly in the middle of a balance-board platform, and note that your board is unstable in all directions (planes). Slowly and deliberately touch or 'tap' the lateral edges of the platform to the ground (left edge, right edge, left, right, etc.) for about one minute. Maintain full control at all times, avoiding hasty motions of the balance board. If the exercise is too difficult at first, place the toes of your other foot on the ground behind the wobble board for better balance. Once the minute is up, repeat the exercise on the opposite foot.\n2. Front-to-Back Edge Taps. These are just like the side-to-side exercise, except\nthat you are touching the front edge of the balance board to the floor, then the back edge, etc. Do it for a minute on your left foot and then a minute on your right.\n3. Edge Circles. Place your left foot in the centre of the wobble board, and then slowly and deliberately touch the edge of the platform to the floor, rotating this 'edge touch' in a clockwise fashion so that an edge of the platform is in contact with the floor at all times. The actual motion must be very slow and controlled to gain full benefit from the exercise and should be performed for one minute without stopping. As before, place the opposite foot on the ground behind you, if a full one-leg balance proves too challenging. Once you have rotated for one minute on one foot, change to the other.\n4. Counter-Clockwise Edge Circles. These are the same as the edge circles, except that you are now rolling the edge along in a counter-clockwise direction.\n5. Bicycle leg swings with resistance reduce the risk of hamstring strains. To do these, attach a medium-resistance stretch cord to your right ankle and to a solid object (pole, tree, another human, etc.) in front of you. The attachment point on this object should be roughly at knee height, and the object should be about a metre in front of you (you may have to adjust this distance). Stand with your weight fully supported on your left leg (you may place your right hand on a wall or other support to maintain balance), and begin by flexing your right hip and raising your right knee up to waist height (your right thigh should be parallel with the ground); as you do this, your right knee should be flexed to 90 degrees or more.\nOnce your thigh is parallel to the ground, begin to extend your right knee (swing the lower part of your right leg forward, unflexing the knee) until your knee is nearly fully extended (eg, your leg is nearly straight), with your right thigh still parallel to the ground. As your right knee nears full extension, allow your right thigh to drop downwards and backwards, 'paw' the ground with your\nfoot, and keep your leg moving until the entire thigh and leg are extended behind your body (as if following through on a running stride). Your right knee should be near full extension (your leg should be straight) until it reaches the peak of the backswing. As your right hip nears full extension (eg, as you approach the end of the backswing), raise your right heel by bending your right knee; your heel should move closely towards your buttocks as you do this. As this happens, move your right knee forward until it returns to the appropriate position in front of your body, with your right thigh parallel to the ground. Repeat this entire sequence of actions in a smooth manner such that the hip and leg move though a continuous arc without stopping or pausing. Once you are able to coordinate the movement, strive to perform the swings at a cadence of at least 12 swings every ten seconds (slightly faster than one swing per second). Start with two sets of 50 reps on each leg and progress to more sets and reps over time.\nRegular use of exercises like these should help decrease the risk of the most common soccer injuries."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:ddb8bf8e-2b4b-4679-a9aa-39ca599d80b4>","<urn:uuid:e358b22a-ded2-402b-8772-86adf66264e3>"],"error":null}
{"question":"Do both TOS and tennis elbow conditions share similar diagnostic approaches using X-rays?","answer":"While both conditions may involve X-rays in diagnosis, their purposes differ. For TOS, X-rays are used to identify structural issues like an extra cervical rib or arthritis, but they are mainly used to rule out other problems, and additional tests like MRI scans and electrical tests are often needed since TOS has no single definitive test. For tennis elbow, X-rays are primarily used to ensure the bone is not diseased or fractured, and the condition can usually be diagnosed from symptoms and physical examination alone.","context":["Physical Therapy in Naperville, Aurora, Lisle, Wheaton, Warrenville, DuPage County, Will County for Shoulder\nWelcome to OMTA Physical Therapy's patient resource about Thoracic Outlet Syndrome.\nThoracic outlet syndrome (TOS) can cause pain and numbness in the shoulder, arm, and hand. Testing for TOS is difficult. There is no one test to accurately diagnose TOS, and other conditions can have similar symptoms. You will need to go through several tests to find out if TOS is actually the cause of your pain. Making the right diagnosis often takes time and can be a cause of frustration, both for you and your doctor.\nThis article will help you understand:\n- what happens to cause TOS\n- what tests will be used to diagnose the condition\n- what can be done to relieve your pain\nWhat is the thoracic outlet?\nThe nerves and blood vessels that go into the arm and hand start at the side of the neck. Nerves come out of the spine through small openings along the side of each vertebra. These openings are called neural foramina.\nThe nerves and vessels travel between muscles in the neck called the scalene muscles and over the top of the rib cage. The thoracic outlet is this opening between the scalene muscles and the rib cage. The nerves and blood vessels then go under the collarbone (also known as the clavicle), through the armpit (the axilla), and down the arm to the hand.\nNerves and Vessels\nRelated Document: OMTA Physical Therapy's Guide to Shoulder Anatomy\nWhat causes TOS problems?\nThe main cause of TOS is that the nerves and blood vessels going to the arm and hand get squeezed near the thoracic outlet. This can occur for many reasons.\nPressure on nerves and vessels can happen in people who have fractured their clavicle. It can also happen in people who have an extra first rib, although this doesn't always result in TOS.\nExtra muscle or scar tissues in the scalene muscles can put extra pressure on the nerves and arteries. Heavy lifting and carrying can bulk up the scalenus muscles to the point where the nerve and arteries get squeezed.\nTraumatic injury from a car accident can also cause problems that lead to TOS. In an accident, the shoulder harness of the seat belt can strain or tear the muscles. As they heal,\ncan build up, putting pressure on the nerves and blood vessels at the thoracic outlet.\nNeck and arm positions used at work and home may contribute to TOS. People who have to hold their neck and shoulders in awkward alignment sometimes develop TOS symptoms. TOS symptoms are also reported by people who have to hold their arms up or out for long periods of time.\nPeople with TOS often slouch their shoulders, giving them a drooped appearance. The\nPoor Body Alignment\nof slouching can compress the nerves and arteries near the thoracic outlet. Being overweight can cause problems with posture, and women who have very large breasts may also have a droopy posture. For some reason, TOS affects three times as many women as men.\nWhat symptoms does TOS cause?\nTOS causes pain along the top of the clavicle and shoulder. The pain may spread along the inside edge of the arm. Occasionally pain spreads into the hand, mostly into the ring and pinky fingers. Numbness and tingling, called paresthesia, may accompany the pain, especially in the early hours of the morning before it's time to wake up. Symptoms tend to get worse when driving, lifting, carrying, and writing. The arms may also feel tired when held overhead, as when using a blow dryer. It may be harder to hold and grip things, and the hand may feel clumsy.\nSymptoms related to the blood vessels are less common. If the blood vessels are causing symptoms, the arm and shoulder may feel heavy and cold. The arm may become somewhat blue (cyanotic), and the constriction of vessels can cause the arm and hand to swell. Problems with the blood vessels that go to the arm are serious. If you experience these symptoms, you should call your doctor right away.\nTOS symptoms are similar to the symptoms of many other conditions. A herniated disc in the neck, carpal tunnel syndrome in the hand, and bursitis of the shoulder can all cause symptoms very much like those of TOS.\nBecause TOS doesn't have any unique symptoms, it can be difficult to diagnose. The diagnosis of TOS involves getting as much information as possible to eliminate other possible causes of your pain.\nWhen you visit OMTA Physical Therapy, our Physical Therapist will take your medical history and do a thorough physical examination. Because TOS is so difficult to diagnose, we will rely heavily on what you report about your symptoms and medical history.\nSome patients may be referred to a doctor for further diagnosis. Once your diagnostic examination is complete, the Physical Therapists at OMTA Physical Therapy have treatment options that will help speed your recovery, so that you can more quickly return to your active lifestyle.\nIt is best to begin treating your pain conservatively, without surgery or other invasive procedures. When you begin your OMTA Physical Therapy rehabilitation program, our Physical Therapist may recommend some simple ways to help you combat TOS. For example, decrease the tension of the shoulder strap of your seat belt. Take rest periods to avoid fatigue.\nOverweight patients should seek help with weight loss, and women with especially large breasts may benefit from using a strapless long-line bra. Avoid heavy lifting, pulling, or pushing. Rapid breathing and stress can worsen symptoms. Avoid looking up, bending the neck back, or holding your arms up for long periods of time. And don't carry a purse or bag on the affected shoulder.\nYour Physical Therapist at OMTA Physical Therapy may start you on some basic exercises that you can do at home. A home exercise program is essential to the treatment of TOS. This is true even if the cause of your TOS is an abnormality in the bones and muscles. You must consistently do your exercises to get the most benefit.\nOur exercise program may begin with a few exercises to loosen up tight muscles and joints around the compressed nerves and blood vessels. To help restore normal mobility, our Physical Therapist may prescribe stretching for the joints, muscles, and nerves. We can also help you find ways to manage your pain and avoid future problems.\nYou will also be given exercises to strengthen the muscles of your shoulder and upper back and to stretch the muscles in the front of the chest and shoulders. Our exercise program will focus on helping you sit and stand with good posture. Good posture is critical to managing TOS symptoms. Swimming can help some patients, but the backstroke and full breaststroke may worsen the condition.\nOur Physical Therapist can also give you tips to help avoid TOS pain. For example, you should limit the length of time the arms are used in outstretched or overhead positions, and don't do heavy carrying and lifting. Simple things like taking frequent breaks, changing positions, stretching, or using a hand truck or cart can bring relief. Our Physical Therapist can help you with any specific tasks that cause you pain.\nIn most cases Physical Therapy can be very effective. However, Physical Therapy may not help much if your symptoms are so severe that the muscles of the hand or forearm have atrophied (shrunk).\nYour rehabilitation will likely be more complex after surgery. Patients wear a sling after surgery to support the shoulder and arm. Passive movements can begin soon after surgery. But there should be no active motion for about two weeks, to allow the soft tissues time to heal.\nOur TOS patients usually start doing resistive exercise and activities after three to four weeks. These treatments help improve motion in the shoulder blade and arm. Posture and strengthening exercises help prevent future TOS problems.\nOur Physical Therapist will give special attention to the type of work you do, and may have suggestions to help you avoid work postures and activities that could cause problems. We’ll show you strategies to take care of any future symptoms and avoid further problems.\nWhen your recovery is well under way, regular visits to OMTA Physical Therapy will end. Although we will continue to be a resource, you will eventually be in charge of doing your exercises as part of an ongoing home program.\nOMTA Physical Therapy provides services for Physical Therapy in Naperville, Aurora, Lisle, Wheaton, Warrenville, DuPage County, Will County.\nWhen diagnosing your problem, your doctor may order an X-ray. The X-ray could show an extra cervical rib or other problems with the bones and joints, such as arthritis. Your doctor may also ask you to get an magnetic resonance imaging (MRI) scan or other imaging tests. MRI scans use magnetic waves to show pictures of the bones and soft tissues of your body in slices. X-rays and other imaging tests are mostly used to rule out other problems.\nYour doctor may recommend electrical tests, called electromyography, of the nerves in the arm. These tests are used to find out if the nerves between the neck and hand are being pinched.\nTo confirm the diagnosis, doctors may do special tests of the blood vessels that run along the nerves. These tests are frequently negative, but it is important that your doctor rule out other causes of your pain.\nYour doctor can prescribe some types of medicine to ease your discomfort. Nonsteroidal anti-inflammatory drugs (NSAIDs), such as aspirin and ibuprofen, can relieve pain and inflammation, and muscle relaxants can relieve muscle spasm. Some patients who experience chronic pain, such as the pain of TOS, end up battling depression. In these cases, anti-depressants can be very helpful.\nSurgery for TOS is usually a last resort. The surgery is directed at removing the source of compression on the nerves of the:\nThe brachial plexus is the network of nerves that go to the hand and forearm. If there is an extra rib, it is usually removed. Otherwise, surgery consists of simply releasing the constricting elements and scar tissue around the nerves.\nSurgery is usually done through an incision under the arm. The surgery will require a general anesthetic, which will put you to sleep. You will probably need to spend at least one night in the hospital.\nPortions of this document copyright MMG, LLC.","Tennis elbow is an inflammation around the bony knob on the outer side of the elbow. It occurs when the tissue that attaches muscle to the bone becomes irritated. The bony knob is called the lateral epicondyle, and tennis elbow is also called lateral epicondylitis (ep-ih-kon-dah-LY-tis).\nPlaying a racket sport can cause tennis elbow. So can doing anything that involves extending your wrist or rotating your forearm-such as twisting a screwdriver or lifting heavy objects with your palm down. With age, the tissue may become inflamed more easily.\nThe most common symptom of tennis elbow is pain on the outer side of the elbow and down the forearm. You may have pain all the time or only when you lift things. The elbow may also swell, get red, or feel warm to the touch. And it may hurt to grip things, turn your hand, or swing your arm.\nThe muscles that allow you to straighten your fingers and rotate your lower arm and wrist are called the extensor muscles. These muscles extend from the outer side of your elbow to your wrist and fingers. A cordlike fiber called a tendon attaches the extensor muscles to the elbow. Overuse or an accident can cause tissue in the tendon to become inflamed or injured.\nWhen the tendon is inflamed, the nerves around the tendon become irritated. Then moving your elbow is painful. Turning your hand or grasping objects can also be painful.\nYour doctor can usually diagnose tennis elbow from your symptoms and from the look and feel of your elbow. He or she may order an x-ray to be sure the bone is not diseased or fractured. In some cases, other tests may be needed.\nYour treatment will depend on how inflamed your tendon is. The goal is to relieve your symptoms and help you regain full use of your elbow.\nWearing a tennis elbow splint allows the inflamed tendon to rest, so it can heal. Using your other hand or changing your grip also helps take stress off the tendon. And oral anti-inflammatory medications and heat or ice can relieve pain and reduce swelling.\nYour doctor may give you an exercise program, or refer you to a therapist, to gently stretch and then strengthen the muscles around your elbow.\nYour doctor may give you injections of an anti- inflammatory, such as cortisone, to help reduce the swelling. You may have more pain at first, but in a few days your elbow should feel better.\nIf your symptoms persist for a long time, or other treatments don't relieve them, your doctor may recommend surgery to repair the inflamed tendon.\nTo prevent a flare-up after treatment, you may need to change the way you do some things. Gripping with the palm up, lifting heavy objects with both hands, or varying activities through- out the day will help reduce stress on the tendon. When you play racket sports or golf, be sure to condition your muscles, do warm- up and cool-down exercises, and use the correct strokes."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:297f58b5-c364-424a-a2ea-f854fbc3a6b0>","<urn:uuid:b1a4fd61-82be-4e19-bae8-8b8a1ea3cae5>"],"error":null}
{"question":"How does coral reef insurance work, and what chemical changes in oceans threaten these reefs?","answer":"Coral reef insurance works by protecting the reef as a natural seawall that safeguards coastal economies. In places like Quintana Roo, Mexico, where tourism generates $10 billion annually, insurance policies pay to maintain reef health so it can protect shoreline businesses. Insurance contracts are renewed yearly, allowing companies to assess risks. As for chemical threats, ocean acidification poses a major risk to coral reefs. When CO2 combines with seawater, it reduces water pH - since industrialization, ocean pH has decreased from 8.2 to 8.1, representing a 26% increase in acidity. This makes it harder for corals to build and maintain their calcium carbonate structures, as the more acidic environment becomes increasingly corrosive to their skeletons and reduces available carbonate ions needed for shell formation.","context":["How insuring ecosystems could help communities adapt to climate change\nShare Now on:\nEarlier this year, Hurricane Delta triggered the first payout of a new type of insurance policy. It’s an experiment in climate adaptation that insures natural ecosystems (in this case, a coral reef in the Mexican state of Quintana Roo) against damage from hurricanes.\nKathy Baughman McLeod, senior vice president and director of the Adrienne Arsht-Rockefeller Foundation Resilience Center at the Atlantic Council, helped develop it. The following is an edited transcript of her conversation with “Marketplace” host Kai Ryssdal about where the idea to insure coral reefs came from, and how it could help communities become more resilient to the impact of climate change.\nKai Ryssdal: Tell me, would you, where this idea for insuring coral reefs, where that came from?\nKathy Baughman McLeod: I have been working in conservation and in risk and risk management, and we began to think, “OK, you need a product, you need to show the value of how nature protects people and economies.” And so we started thinking about what would we insure? And can we insure nature itself? Who owns it? And how do you do it?\nRyssdal: Yeah. So, there’s a coral reef, off Quintana Roo, down in Mexico, near Cancun, which was the subject of the story in [The New York Times] that I read, and obviously, the insurance policies that you all came up with. Does the municipality of Cancun or the state of Quintana Roo just call Lloyd’s of London and say, “Hey, I want to insure my coral reef?” How does it work?\nBaughman McLeod: No. And you know, I shouldn’t say it’s more fun than that —\nRyssdal: Oh, gosh, insurance more fun than that? How surprising.\nBaughman McLeod: Yes. People — their eyes roll back, and you lose people in a second when you say “insurance,” [but] it’s super exciting. It really is. You know, with $32 trillion in assets under management, the insurance industry, they change markets. And they are invested in reducing the risks that they face, you know — both financial and physical risk. But the key thing is that we were trying to insure something that no one owns. And so how do you build the conversation so that you focus on the beneficiaries?\nRyssdal: So this coral reef gets damaged. The insurance policy covers the damage to the reef, because the insurance company didn’t want to pay for the damage to the beach that would have resulted, if the coral reef hadn’t been there? Is that sort of nuts and bolts?\nBaughman McLeod: Yes, mostly. So, the coral reef and the beach go together. A healthy reef is acting as a natural seawall. When you think about the tourism industry — so that in Quintana Roo, that tourism industry is $10 billion a year. And the hotels and the restaurants and things on the beach are being protected by that reef. And so the money pays to keep the reef in health, so it can continue to provide that ecosystem service to protect the economy on the shore.\nRyssdal: So if I’m an insurance company, and I know that the climate is changing, and that coral reefs are in danger, or sequoias in California are at wildfire risk, or take your pick of natural catastrophe, all of which we have seen multiplying in the past number of years, as climate change gets more serious. If I’m an insurance company, why do I want to get near one of these things? Because it’s almost guaranteed to be a lose-lose deal at some point?\nBaughman McLeod: Well, the insurance contracts are a year at a time. And so the insurance industry has a chance to assess every year, where do they want to place the risk around the world? That’s one. And two, they’re also increasingly understanding how powerful these natural systems are to reduce the risks. And so you’ll see companies, you know, brokers, primary insurers and reinsurers involved in better understanding how to create these kinds of products. But you’re right. You know, all of this is about climate adaptation. But until we get our act together on emissions, we have to be doing these types of approaches.\nRyssdal: This is your line of work. Are you getting traction with this argument?\nBaughman McLeod: Absolutely. You know, one of the biggest brokerage firms in the world, Willis Towers Watson, launched a “Global Ecosystem Resilience Facility,” the “GERF” (unfortunate acronym, but a cool thing), and so you see the business getting into it pretty clearly. So, we’ll keep going, while we get our act together on mitigation.\nThere’s a lot happening in the world. Through it all, Marketplace is here for you.\nYou rely on Marketplace to break down the world’s events and tell you how it affects you in a fact-based, approachable way. We rely on your financial support to keep making that possible.\nYour donation today powers the independent journalism that you rely on. For just $5/month, you can help sustain Marketplace so we can keep reporting on the things that matter to you.","What is Ocean Acidification?\nSince the beginning of the Industrial Revolution, when humans began burning coal in large quantities, the world’s ocean water has gradually become more acidic. Like global warming, this phenomenon, which is known as ocean acidification, is a direct consequence of increasing levels of carbon dioxide (CO2) in Earth’s atmosphere.\nPrior to industrialization, the concentration of carbon dioxide in the atmosphere was 280 parts per million (ppm). With increased use of fossil fuels, that number is now approaching 400 ppm and the growth rate is accelerating. Scientists calculate that the ocean is currently absorbing about one quarter of the carbon dioxide that humans are emitting. When carbon dioxide combines with seawater, chemical reactions occur that reduce the seawater pH, hence the term ocean acidification.\nCurrently, about half of the anthropogenic (human-caused) carbon dioxide in the ocean is found in the upper 400 meters (1,200 feet) of the water column, while the other half has penetrated into the lower thermocline and deep ocean. Density- and wind-driven circulation help mix the surface and deep waters in some high latitude and coastal regions, but for much of the open ocean, deep pH changes are expected to lag surface pH changes by a few centuries.\nOcean acidification and global warming are different problems, but are closely linked because they share the same root cause—human emissions of carbon dioxide. The atmospheric concentration of carbon dioxide is now higher than it has been for the last 800,000 years and possibly higher than any time in the last 20 million years. Humans have thus far benefited from the ocean’s capacity to hold enormous amounts of carbon, including a large portion of this excess carbon dioxide. Had the ocean not absorbed such vast quantities of carbon dioxide, the atmospheric concentration would be even higher, and the environmental consequences of global warming (sea level rise, shifting weather patterns, more extreme weather events, etc.) and their associated socioeconomic impacts would likely be even more pronounced. However, the oceans cannot continue to absorb carbon dioxide at the current rate without undergoing significant changes in chemistry, biology, and ecosystem structure.\nMeasuring ocean acidification: Past and present\nScientists know that the oceans are absorbing carbon dioxide and subsequently becoming more acidic from measurements made on seawater collected during research cruises, which provide wide spatial coverage over a short time period, and from automated ocean carbon measurements on stationary moorings, which provide long-term, high-resolution data from a single location.\nThese records can be extended back through time using what are known as chemical proxies to provide an indirect measurement of seawater carbonate chemistry. A proxy is a measurement from a natural archive (ice cores, corals, tree rings, marine sediments, etc.) that is used to infer past environmental conditions. For example, by analyzing the chemical composition of tiny fossil shells found in deep ocean sediments, scientists have developed ocean pH records from ancient times when there were no pH meters. Furthermore, because the ocean surface water is in approximate chemical balance, or equilibrium, with the atmosphere above it, a record of historical ocean pH can be inferred from atmospheric carbon dioxide records derived from Greenland and Antarctic ice cores, which contain air bubbles from the ancient atmosphere. Such evidence indicates that current atmospheric carbon dioxideconcentrations and ocean pH levels are at unprecedented for at least the last 800,000 years.\nGoing back deeper in Earth history to the Paleocene-Eocene boundary about 55 million years ago, scientists have found geochemical evidence of a massive release of carbon dioxide accompanied by substantial warming and dissolution of shallow carbonate sediments in the ocean. Although somewhat analogous to what we are observing today, this carbon dioxide release occurred over several thousand years, much more slowly than what we are witnessing today, thus providing time for the oceans partially to buffer the change. In the geologic record, during periods of rapid environmental change, species have acclimated, adapted or gone extinct. Corals have undergone large extinction events in the past (such the Permian extinction 250 million years ago), and new coral species evolved to take their place, but it took millions of years to recover previous levels of biodiversity.\nHow is ocean acidification affecting ocean chemistry?\nSeawater has a pH of 8.2 on average because it contains naturally occurring alkaline ions that come primarily from weathering of continental rocks. When seawater absorbs carbon dioxide from the atmosphere, carbonic acid is produced (see Box 1), reducing the water’s pH. Since the dawn of industrialization, average surface ocean pH has decreased to about 8.1.\nBecause the pH scale is logarithmic (a change of 1 pH unit represents a tenfold change in acidity), this change represents a 26 percent increase in acidity over roughly 250 years, a rate that is 100 times faster than anything the ocean and its inhabitants have experienced in tens of millions of years.\nAcidification can affect many marine organisms, but especially those that build their shells and skeletons from calcium carbonate, such as corals, oysters, clams, mussels, snails, and phytoplankton and zooplankton, the tiny plants and animals that form the base of the marine food web.\nThese “marine calcifiers” face two potential threats associated with ocean acidification: 1) Their shells and skeletons may dissolve more readily as ocean pH decreases and seawater becomes more corrosive; and 2) When CO2 dissolves in seawater, the water chemistry changes such that fewer carbonate ions, the primary building blocks for shells and skeletons, are available for uptake by marine organisms. Marine organisms that build shells or skeletons usually do so through an internal chemical process that converts bicarbonate to carbonate in order to form calcium carbonate.\nExactly how ocean acidification slows calcification rates, or shell formation, is not yet fully understood, but several mechanisms are being studied. Most hypotheses focus on the additional energy an organism must expend to build and maintain its calcium carbonate shells and skeletons in an increasingly corrosive environment. In the face of this extra energy expenditure, exposure to additional environmental stressors (increasing ocean temperatures, decreasing oxygen availability, disease, loss of habitat, etc.) will likely compound the problem.\nThese effects are already being documented in many marine organisms, particularly in tropical and deep-sea corals, which exhibit slower calcification rates under more acidic conditions. The impact on corals is of great concern because they produce massive calcium carbonate structures called reefs that provide habitat for many marine animals, including commercially important fish and shellfish species that use the reefs as nursery grounds. Coral reefs are vital to humans as sources of food and medicine, protection from storms, and the focus of eco-tourism. In addition to corals, studies have shown that acidification impairs the ability of some calcifying plankton, tiny floating plants and animals at the base of the food web, to build and maintain their shells. Scientists have also observed increased larval mortality rates of several commercially important fish and shellfish.\nWhat can we expect in the future?\nOcean acidification is occurring at a rate 30 to100 times faster than at any time during the last several million years driven by the rapid growth rate atmospheric CO2 that is almost unprecedented over geologic history. According to the Intergovernmental Panel on Climate Change (IPCC), economic and population scenarios predict that atmospheric CO2 levels could reach 500 ppm by 2050 and 800 ppm or more by the end of the century. This will not only lead to significant temperature increases in the atmosphere and ocean, but will further acidify ocean water, reducing the pH an estimated 0.3 to 0.4 units by 2100, a 150 percent increase in acidity over preindustrial times. Assuming a “business-as-usual” IPCC CO2 emission scenario, predictive models of ocean biogeochemistry project that surface waters of the Arctic and Southern Oceans will become undersaturated with aragonite (a more soluble form of calcium carbonate) within a few decades, meaning that these waters will become highly corrosive to the shells and skeletons of aragonite-producing marine calcifiers like planktonic marine snails known as pteropods.\nAlthough ocean acidification has only recently emerged as a scientific issue, it has quickly raised serious concerns about the short-term impacts on marine organisms and the long-term health of the ocean. Scientists estimate that over the next few thousand years, 90 percent of anthropogenic CO2 emissions will be absorbed by the ocean. This may potentially affect biological and geochemical processes such as photosynthesis and nutrient cycling that are vital to marine ecosystems on which human society and many natural systems rely. At the same time, marine organisms will face the enormous challenge of adapting to ocean acidification, warming water, and declining subsurface-ocean oxygen concentrations.\nNews & Insights\nWHOI working to address ocean acidification; protect region’s vital shellfish industry\nA new report addresses the impacts of ocean acidification in Massachusetts and New England coastal waters on the region’s vital seafood industry.\nOcean acidification gets a watchful eye in New England aquaculture ‘hot spot’\nShellfish aquaculture is thriving in New England, but future growth in the industry could be stunted as coastal waters in the region become more acidic. Researchers at WHOI have developed…\nOcean acidification causing coral ‘osteoporosis’ on iconic reefs\nScientists Pinpoint How Ocean Acidification Weakens Coral Skeletons\nClimate Change Will Irreversibly Force Key Ocean Bacteria into Overdrive\n[ ALL ]\nWHOI in the News\nThe Top Eight Ocean Stories of 2022\nThe $500 Billion Question: What’s the Value of Studying the Ocean’s Biological Carbon Pump?\nEcology Research: Ocean acidification causing coral ‘osteoporosis’ on iconic reefs\nDisentangling influences on coral health\n[ ALL ]\nFrom Oceanus Magazine\nOcean acidification is no big deal, right?\nWHOI’s Jennie Rheuban discusses the very real phenomenon of an increasingly acidic ocean and the toll it’s taking on marine life.\nTo Tag a Squid\nHow do you design a tag that can attach to a soft-bodied swimming animal and track its movements? Very thoughtfully.\nHow Do Corals Build Their Skeletons?\nWHOI scientists discovered precisely how ocean acidification affects coral skeletons’ a factor that will help scientists predict how corals throughout the world will fare as the oceans become more acidic.\nSearching for ‘Super Reefs’\nSome corals are less vulnerable to ocean acidification. Can the offspring from these more resilient corals travel to other reefs to help sustain more vulnerable coral populations there?\nGraduate student Hannah Barkley is on a mission to investigate how warming ocean temperatures, ocean acidification, and other impacts of climate change are affecting corals in an effort to find…"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c8c94693-7a5f-45f7-8e68-16edaadc19d5>","<urn:uuid:51ca6f21-f0f4-4343-8b14-a7f059f0fe69>"],"error":null}
{"question":"What are the key differences between relations and functions when analyzing them through ordered pairs versus equations, and how do their domains/ranges compare?","answer":"Relations and functions both describe relationships between variables, but functions are a special type of relation with stricter rules. In ordered pairs, a function requires that each independent variable (first number) corresponds to exactly one dependent variable (second number), while relations can have multiple dependent values for the same independent variable. For example, {(1,-2), (1,1)} would be a relation but not a function since 1 maps to two different values. When represented as equations, functions like f(x) = 3x ensure each x-value produces exactly one output, while relations may produce multiple outputs for the same input. For both representations, the domain consists of all valid input values (independent variables/x-values), while the range contains all possible output values (dependent variables/y-values). However, functions have the additional constraint that each domain value must pair with exactly one range value.","context":["o Vertical line test\no Horizontal line test\no Zero (of a function)\no Learn more about functions (in general) and their properties\no Use graphs to explore a function's characteristics\no Gain an understanding of inverse functions and compositions of functions\no Understand the relationship between functions and equations\nProperties of Functions\nRecall that a function is a relation between certain sets of numbers, variables, or both. Let's now refine our understanding of a function and examine some of its properties. Fundamentally, a function takes an input value, performs some (perhaps very simple) conversion process, then yields an output value. Consider the function f(x) below:\nThe function f simply takes in input value x, multiplies it by 2, and then adds 3 to the result. We can therefore consider what constitutes the set of numbers that the function can accept as an input and what constitutes the set of numbers that the function can yield as an output.\nThese sets are what we respectively call the domain and range of the function. The domain of a function is the set of numbers for which the function is defined. For instance, we may define a function G(n) over only the integers; thus, the variable n is only allowed to take on integer values when used in the function G. In some instances, the form of the function may exclude certain values from the domain because the output of the function would be undefined. Consider the example function h(y) below:\nNotice that any value of y from the set of real numbers is acceptable-except for the number 4.\nThe result in this case is not defined; we thus exclude the number 4 from the domain of h. The range of h is therefore all (the symbol simply means \"is an element of\") where y ≠ 4.\nPractice Problem: Find the domain of the function .\nSolution: The function g(x) simply takes the value x and turns it into its reciprocal value . Thus, for instance, the number 5 becomes , and becomes 2. Note that any value of x works in this function as long as is defined. As mentioned, fractions work as well as whole numbers, both for positive and negative values; the only value that does not work is 0, since is undefined (how many times can 0 go into 1?). Thus, the domain of the function is all x in where x ≠ 0. Let's look at the graph of the function also.\nAs you can see in the graph, the function g to the left of zero goes down toward negative infinity, but the right side goes toward positive infinity, and there is no crossing of the function at zero. Thus, we can see graphically that this function has a domain of all real values except 0.\nThe range of a function is the set of all possible values in the output of a function given the domain. In our example function h(y) above, the range is (except for h(y) = 0), because for any real number, we can find some value of y such that the real number is equal to h(y). Let's choose, for instance, –100. If we let y = 4.03, then\nThe same argument applies to other real numbers. In the case of h(y) = 0, however, there is no value of y large enough to make the fraction equal to zero. Thus, the range of h is all real numbers except 0.\nAt this point, we can make an important distinction between a function and the more general category of relations. For a relation to be a function specifically, every number in the domain must correspond to one and only one number in the range. Thus, if f(x) can have more than one value for some value x in the domain, then f is a relation but not a function. If f(x) has exactly one value for every x in the domain, then f is a function. Although it is often easy enough to determine if a relation is a function by looking at the algebraic expression, it is sometimes easier to use a graph. To do so, apply the vertical line test: look at the graph of the relation-as long as the relation does not cross any vertical line more than once, then the relation is a function.\nPractice Problem: Determine if the relation is a function.\nSolution: We can easily note that for any value of y in the domain, the relation yields two different values in the range. For instance, if y = 4, h(y) can be either 2 or –2. The relation h(y) is therefore not a function. Let's look at the graph and apply the vertical line test as a double check:\nNote that the relation crosses a vertical line in two places almost everywhere (except at y = 0). Thus, the graph also proves that h(y) is not a function.\nAlso, it is helpful to make note of a special class of functions: those that are one-to-one. A function is one-to-one if it has exactly one value in the domain for each particular value in the range. (This property will be important when we discuss function inversion.) The example diagram below helps illustrate the differences between relations, functions, and one-to-one functions.\nIn each case, the diagram shows the domain on the left and the range on the right. The relation f is not a function because the f(7) = 11 and f(7) = 17 (that is, there is more than one value in the range for the value 7 in the domain). The relation g is a function because each value in the domain corresponds to only one value in the range. Finally, the relation h is a one-to-one function because each value in the domain corresponds to only one value in the range and vice versa. We can determine if a function is one-to-one by applying the horizontal line test. This test is similar to the vertical line test, except that it ensures that each value in the range corresponds to only one value in the domain. If, for every horizontal line, the function only crosses that line once, then the function is one-to-one.\nPractice Problem: Find the range of .\nSolution: A function such as this one is defined for all x values because there is no value of x for which 3x becomes infinity, for instance. Thus, the range of f(x) is , the entire set of real numbers. Another way to consider such problems is by way of a graph, as shown below.\nNote that the function is a straight line, and regardless of the scale of the axes (how far out you plot in any direction), the line continues unbroken. Thus, not only is the range of the function, it is also the domain. We can further observe that the function is one-to-one; you can see this by noting that the function simply takes every number on the number line and multiplies it by 3.\nPractice Problem: Determine if the relation is one-to-one.\nSolution: First, we know that f(x) is a function because no value of x can cause f(x) to take on more than one value. Second, we can see that f(x) is not one-to-one because f(x) is the same for both +x and -x, since . Let's use a graph again to show this result visually.\nThe graph above shows that the relation f(x) passes the vertical line test, but not the horizontal line test. Thus, f(x) is a function that is not one-to-one.\nInverses and Compositions of Functions\nTwo important manipulations of functions are compositions and inverses. A composition of functions is simply the replacement of the variable in one function by a different function. Thus, if we have two functions f(x) and g(y), the composition f(g(y)) (which is also written is found by simply replacing all instances of x in f(x) with the expression defined for the function g(y).\nPractice Problem: Find the composition , where and .\nSolution: The composition is the same as h(r(s)); thus, we can solve this problem by substituting r(s) in place of s in the function h.\nBe careful to note that is not the same as :\nAn inverse of a one-to-one function f(x), which we write as , is a function where the composition . An inverse of a function is, in this context, similar to the inverse of a number (3 and , for instance). The inverse of a function can be found by making a switch: replace all instances of f(x) with x, and replace all instances of x with . Next, manipulate the equation using the rules of arithmetic and real numbers to find an expression for . This is then the inverse of the function. Note that a function must be one-to-one to have an inverse.\nPractice Problem: Find the inverse of the function .\nWe want to find the inverse of g(y), which is . Perform the replacement of g(y) with y, and y with .\nNote that essentially acts like a variable, and it can be manipulated as such. So, let's rearrange this expression to find . As with any arithmetic manipulation, as long as you perform the same operation on both sides of the equality sign (=), the equality will still hold.\nNow, we can check the result using the condition of inverse functions:\nThe result checks out.\nAn equation in algebra is simply a statement that two relations are the same. Thus, an equation might be as simple as 0 = 0, or it might be as complicated as . A solution to an equation is the value (or values) of the variable (or variables) in an equation that makes the equation true. Finding a solution to an equation involves using the properties of real numbers as they apply to variables to manipulate the equation.\nClosely related to the solution of an equation is the zero (or zeros) of a function. A zero of a function f(x) is the solution of the equation f(x) = 0. A function has a zero anywhere the function crosses the horizontal axis in its corresponding graph.","There are many algebra books that have many ways of defining a function. Chances are if you look at four different books you would find at least two different explanations of a function. Does this mean\nsome explanations are right and others are wrong or that there are numerous definitions for a function? Not really. The definition\nof a function\nnever changes, but the way teachers and textbooks explain\nthat definition take on many forms.\nWe’re going to look at functions in the following way.\nA function is a relationship that meets certain conditions between two variables. These variables are usually called the independent variable and the dependent variable. In a function, each value for the independent variable corresponds to exactly one value for the dependent variable.\nIf a relationship with independent and dependent variables but not necessarily with one dependent variable for each independent variable, that relationship is called a relation. Notice that a function and a relation both describe relationships between two variables, but a function is just a special form of a relation.\nOne of the easiest ways to look at functions and relations is by looking at ordered pairs of numbers\nConsider the following set of ordered pairs:\nWhen dealing with ordered pairs, the independent variable is listed first and the dependent variable is listed second. Notice that each independent variable (0, 1, 2, and 3) is only paired up with exactly one dependent variable. This set of ordered pairs would be considered a function.\nWhen we noted that the dependent variables took on values of 0, 1, 2, and 3 we created a list of values that we normally refer to as the domain of a function. Although we did not list the dependent variables, we could do that by listing 1, -2, 0, and 2. We normally refer to these values as the range of the function.\nUsing our new terminology we have:\ndomain of So now we have another way to look at our explanation of a function. See how easy it is to come up with so many varied ways to describe the same thing?\nWe can now say that each value in the domain of a function can only be paired with one value from the range of the function.\nNow look at a different set of ordered pairs.\nWe can find the domain of . Notice that we do not have to list the value of 1 more than once, but because it appears more than once in the list of ordered pairs, we should be suspicious that it might have more than one range value associated with it. If we look close at the pairings we see that 1 is paired with -2 and then again with 1. This violates our definition of a function. Each of our values in the domain can only be assigned to one value in the range.\nBefore we move on to other representations of functions, let’s look one more set of ordered pairs.\nWe can list the domain of and the range of . It is important to notice that it is not a problem for several values in the domain to go to the same range value. This does not violate our definition of a function. So be careful!\nRather than using ordered pairs to describe functions and relations, a more common form is with the use of equations.\nUsually, you will see a function written as something like . Many times teachers and books will use and y interchangeably. Recall that when graphing lines, we used the notation . Now we can see that this is a function because each value in the domain will be paired with only one range value. In this case, the domain consists of values that you can use for x. Sometimes domain is referred to as the input of a function because it is the values you are allowed to put in for x. Remember that domain referred to our independent variable.\nAlso recall that the range, or the y-values referred to the dependent variable. When you look at the function in its equation form, this makes sense. You get to choose the values for x that are put into the equation and then the y-values are dependent upon those x-values.\nSo here’s how so much confusion can arise from working with functions. Look at all the terminology we’ve used so far and the many different ways to describe the same thing.\nLet’s go back and look at the function . A lot of times the notation of can be confusing. But f is just the name of the function and x is the independent variable in the function. When combined together we say that is the value of the function at x (again just another way of saying the y-value).\n- Domain, independent variable, x, input\n- Range, dependent variable, y, output, f(x)\nWe could name and label the same expression in a different way without changing the meaning. For example, we could call it . In this case, we have named the function s and used t is the independent variable.\nIn application problems, it is VERY common to see different labels for names and the independent variable. We’ll use some of these different notations in examples later in this lesson and also in the lesson evaluating functions (link to functions-evaluating.doc)\nWith so many terms and notations to work with we can’t forget to talk about the domain and range of . For the domain we want to know what values of x are allowed to be put into the function. In this case, we can put any value of x into the function. If we do that, our output or our range can also be any value. In this case, we say that the domain and the range are all the real numbers.\nIt turns out that any linear function will have a domain and a range of all the real numbers. In cases of radicals or fractions we will have to worry about the domain of those functions.\n- In the function we will only be allowed to use positive values of x. Otherwise we have a negative value underneath the square root and we can’t take the square root of a negative number and still end up with a real number. When we evaluate the square root of a positive number, we will get a positive number as a result. (You may recall other times when it was acceptable to use a positive and negative in front of a square root symbol, but if we do that here, we will not have a function.) So for this function we have domain: and range: .\nWhen working with radical functions, we can usually set what is underneath the radical greater than or equal to zero and solve the resulting inequality to find the domain. Some students like to look at a graph of a function to help them in determining the range.\n- Consider the function . In this situation, as with any fraction, we have to be careful not to have zero in the denominator. So in this case, we want to make sure that . This means that . Any other value of x is allowed in this function, however we cannot use 2 because we will have zero in the denominator. We do not have any restrictions on the range, so we say domain: all real numbers except 2 and range: all real numbers.\nWhen working with rational functions, set the denominator equal to zero and this will give you the value(s) that CANNOT be used in the domain.\nOne quick and easy way to determine is an expression\nor a set of ordered pairs is a function\nor not is to graph\nit. All functions must pass what we call the vertical line test\n. That means we must be able to draw a vertical line\nanywhere on the graph\nand have that vertical line\nonly touch in one place. If that happens, then the graph\nis a function. If at any place on the graph\nwe are able to draw a vertical line\nthat touches the graph\nin two or more places, then the graph\nis not a function.\nAs mentioned earlier, graphing can also be used to determine domain and range of functions. You should try to graph an expression by hand or on your calculator to help you understand the vertical line test and domain and range.\nA basic understanding of functions, terminology, and notation is necessary in order to be able to use functions in later lessons. Make sure you complete the following examples and understand their answers before going to any other lessons on functions."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:bd0dc914-30d8-47a3-ac46-399e45b0a618>","<urn:uuid:f548ad7d-f027-499c-9aea-234123d87cf7>"],"error":null}
{"question":"What is the relationship between numerical aperture (NA) and optical resolution, and what are the practical considerations for using oil immersion objectives?","answer":"NA has a significant impact on optical resolution - it affects slice thickness with a 1/square relationship and the PSF core's radial extension linearly. For optimal resolution, most applications use immersion lenses with NA > 1, where xy and z dimensions approach a ratio of 2. Regarding practical use, oil immersion objectives (typically 100x with 1.25 NA) should only be used when highest resolving power is needed, as they are more troublesome than dry objectives. They work well with specimens like blood films and thin tissue sections mounted in canada balsam under a coverglass, though their small working distance limits examination of thicker sections.","context":["3D point spread function (PSF) and Full Width Half Maximum (FWHM)\nThe image of a point (spot) that is created by an optical system is called the \"point spread function\" (PSF). The point spread function describes the distribution in three dimensions of light originating from a dimension-less spot. The core of this diffraction pattern has an ellipsoid shape, and that controls the optical resolution. The radial dimension rules the lateral resolution; the axial dimension rules the depth of focus and thus the slicing performance.\nFor confocal sectioning, the goal is to transmit only the inner core of the PSF, which defines the optical section. As a matter of fact, the pinhole diameter controls how much light from outside the core is transmitted. The optical section that is in the range of the size of the diffraction pattern is thus obviously not sharp-edged like a slice of bread, but is characterized by a comparably flattish slope. The distance in z that connects the 50 % intensities on both sides of an intensity profile is called the \"full width half maximum\" (FWHM), and by convention this is used as a measure of the thickness of optical sections. The optical sectioning performance is typically measured by focusing through a surface mirror that models an infinitely thin structure in z. This method is (comparably) easy and is used to examine the performance of confocal microscope systems. Alternatively, and more realistically, z sectioning performance is measured with fluorochromated latex beads (see Figure 1). The fluorescent beads allow the performance to be measured in non-coherent conditions, which is true for fluorescence imaging and thus fits the vast majority of confocal applications in biomedical sciences. The diffraction-limited optical sections in reflected light mode (mirror) are significantly thinner as compared to sections in fluorescence mode. This is important to keep in mind if comparing figures from literature.\nParameters controlling the thickness of optical sections\nUnder ideal conditions, the thinnest section achievable would only depend on limitations by optical diffraction. The most obvious is the wavelength, which controls the size of the PSF proportionally: the shorter the wavelength, the thinner the slice. We can assume the wavelength of the excitation light to be limiting (rather than that of the emission), as there will be no emission from outside the illuminated space. The second parameter is the aperture of the objective lens (numerical aperture, NA): the higher the NA (wider angle for collection of information), the thinner the optical section. Also, the refractive index of the sample affects the axial resolution and consequently the sectioning performance. The dependence of the slice’s size dz on these parameters is given by the formula Figure 2.\nThe third parameter is the pinhole diameter in the detection path, which is assumed to be zero for the formula given above. Therefore, this formula gives the best value one can expect for fluorescence imaging – and is of course only of theoretical interest: a pinhole with zero diameter would not generate bright images!\nCritical comments on optical section thickness\nThe numerical aperture NA has a significant (1/square) impact on the slice thickness. The radial extension of the psf depends linearly on the NA. Consequently, for low-NA lenses, the PSF core becomes very elongated and the slices very thick. Most applications therefore use immersion lenses with NA > 1, where the ratio of xy and z dimensions approach roughly a factor of 2. As a rule of thumb, the z section with high-NA lenses is roughly double the xy resolution.\nTheoretical considerations help to estimate the performance of optical systems. In reality, both the instrument and the sample introduce deviations from the calculated figures. The instrument must be adjusted and operated carefully for optimal performance. And the sample itself is the enemy of resolution, especially if it comes to deeper imaging. Extreme care therefore has to be taken for refractive index matching, correct and constant temperature and proper coverslip selection. The theory will give a rule of thumb, but sample and setting may introduce significant deviations – often inescapable, as biological samples are more complex than crystals (– or the vacuum).\n- Sheppard CJR: Scanning optical microscopy. In: Barer R and Cosslett VE (eds), Advances in Optical and Electron Microscopy 10 (1987). Academic Press, London UK.\n- Wilson T: Confocal Microscopy. Academic Press, London UK (1990).\n- Corle TR and Kino GS: Confocal Scanning Optical Microscopy and Related Imaging Systems. Academic Press, San Diego USA (1996).\n- Borlinghaus RT and Gröbler B.: Basic Principles and Applications of Confocal Laser Scanning Microscopy. In Isenberg G (ed): Modern Optics, Electronics, and High Precision Techniques in Cell Biology, 35–53 (1997). Springer Heidelberg.\n- Cox G: Optical Imaging Techniques in Cell Biology. Taylor & Francis, Boca Raton USA (2007).","Using the Microscope.\nOil Immersion Objectives.\n7 of 9\n1 of 1\nUsing the Oil Immersion Objective.\nThe only reason to use an oil-immersion objective is to take advantage of the highest resolving power available to the microscopist. Unless this is a necessary requirement of the work in hand, stay with the lower power dry objectives -- they are much less trouble to use, and can provide magnifications up to x600 (using a x40, 0.65 NA dry objective with x15 eyepiece) at quite acceptable resolution.\nHaving said that, much of the work requiring the high powers and resolution of the oil immersion objective has adapted itself to the requirements and constraints of the immersion technique. Blood films, stained bacterial films etc. are all prepared on plain microscope slides as dry films, and the drop of immersion oil becomes the only optical medium between the specimen and the frontlens of the objective.\nWhen the examination is over, the slide with its film may be discarded or stored without even bothering to remove the immersion oil, since more will be added if the slide is examined again. In histology, thin (10 Ám or less) stained sections of tissue are mounted under a coverglass in canada balsam or other resin having the same optical properties as glass, so with the addition of immersion oil, the conditions for homogeneous immersion are met. Much thicker sections than this cannot be examined, as the working distance of these objectives is very small, and the front-lens mount is soon in contact with the coverglass. Sometimes, thinner than standard coverglasses are used to allow deeper penetration of focus into the specimen. With a covered balsam mount, the oil may be removed with no risk of damage to the specimen.\nOil Immersion: Routine Technique.\nThe most common oil-immersion objective in use in routine microscopy is the achromatic objective of magnification x100 and NA of 1.25, used in combination with a dry two-lens Abbe substage condenser having a maximum aplanatic NA of about 0.6.\nIt is clear that the condenser cannot fill more the half the NA of the objective, and the resulting image will be high in contrast and showing the coarsening of detail characteristic of images formed by narrow axial cones -- but acceptable, especially for good visibility of low contrast subjects.\nIf you require such an image of say a balsam mounted section of stained plant or animal tissue, or a stained dried blood film on a slide, follow these steps:\nOil Immersion at the Highest Resolutions.\nIf higher resolutions are required, the main problem is that of supplying the objective with a sufficiently wide cone of illumination to fill its aperture and obtain best performance. A dry two-lens Abbe condenser produces an aplanatic cone of no more than 0.6 NA, and this is hardly sufficient to exploit the higher NA of the OI objective. Immersing the condenser helps a little, but is a fiddly operation providing very little gain in image quality.\nThe most practically satisfactory method involves replacing the Abbe condenser with a dry achromatic/aplanatic condenser with an aperture of 0.95 which will provide a 3/4 illumination cone for an objective of 1.3 NA. This extracts an optically satisfactory performance from a better than average objective without the need to immerse the condenser.\nIf the highest resolutions are required, the objective will need to be of apochromatic correction and will have an NA of 1.4. The condenser required will be of achromatic/aplanatic correction, and with oil immersion, provide an aplanatic illumination cone of 1.4 NA. Additionally, the microscope stand employed will require substage centreing adjustments to enable accurate centration of the condenser.\nThese necessarily expensive optics are capable of producing the finest possible images -- but their deployment is an exercise in critical microscopy and outside the scope of this basic tutorial."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:87d9ad09-bce2-4d34-a74d-010621f548c2>","<urn:uuid:0e58ab40-f36f-45a8-8c47-a2a9f5f73b83>"],"error":null}
{"question":"What are the installation requirements for a heat recovery ventilation system, and how does its efficiency compare to air source heat pumps?","answer":"Heat recovery ventilation systems require significant building preparation - they work best in airtight buildings and typically need extensive insulation including cavity wall insulation, floor insulation, proper double glazing, and roof insulation. For older buildings, retrofitting can be economically unviable due to too many cracks and heat loss points. Meanwhile, air source heat pumps can be installed in both new and existing properties with minimal requirements - they only need an outdoor space of about 1m x 1m x 30cm for the unit, good air flow, and can be installed within 5 working days. While both systems aim to improve heating efficiency, ASHPs can operate year-round even in temperatures down to -15 degrees and can provide both heating and cooling functionality.","context":["Is installing a heat recovery system worth it?\nIs a Heat Recovery System Right for me?\nThere’s no doubt that under the right circumstances a heat recovery system can produce great benefits for reducing the amount of heat and energy you use, recycling a significant percentage so that little goes to waste.\nWhether your current building is suitable for such a change in technology will depend on what you are opting for. In a new build that is air tight, a heat recovery ventilation system can present significant savings and if you have a decent amount of garden space a ground source heat pump may well be the answer.\nThe advice below applies equally to both domestic and non-domestic heat recovery systems.\nHeat Recovery Ventilation Systems\nThe ventilation unit normally goes somewhere like the attic and pipes feed down into the individual rooms drawing out the stale are and replacing it with fresh. The way it works is to take the heat in the indoor air and pass it onto the new air coming in from outside. New builds are starting to come with this kind of technology ready and waiting, particularly where offices are concerned. Retro-fitting an older property is entirely possible but it comes with a big caveat – older houses are less airtight than newer ones and to get the best from a HRVS you really need to have any source of heat loss covered.\nFor most old buildings this is not entirely impossible but it demands a fair amount of extra investment. There are too many cracks and places where heat can escape that makes installing a ventilation system more or less economically unviable for a lot of people. If you have a newer property, however, then retro-fitting a ventilation system can save you a good deal on your bills.\nBoiler Flue Economisers\nRecovering the heat that is lost from your boiler is a simpler solution and you can achieve an efficiency of between 7 and 40% depending on the type you have installed. The way it works is that the economiser captures the lost heat and either transfers it to another tank or, more usually, transfers it back to the existing boiler so that it doesn’t have to work so hard to heat up new water.\nThey are generally easy to install and can benefit from the Green Deal and don’t require any additional electricity to run. Costs start around £1,200 but you will save on the amount of energy you need to use to heat the water and if you are on a water meter you should see a reduction in water usage too.\nThere are different kinds of heat pump that extract energy from the land, air or water outside to provide warmth for your home or business. As with any renewable heat recovery system you need to make sure that your building is insulated as best as it can be, as these technologies tend to operate at much lower temperatures than more conventional heating. Heat pumps also benefit from the Renewable Heat Incentive and can earn you money back on your initial investment.\nFind out more about heat pumps and whether they are the right heat recovery system for you here.\nBefore you consider a Heat Recovery System\nPerhaps more than other renewable technologies, whether you opt for a heat recovery system depends on a good deal of thought before you take the plunge.\nIs your building airtight enough to benefit from something like a heat recovery ventilation system? You need to check how green you are and what needs to be improved:\n- Do you have cavity wall insulation?\n- How much heat is being lost through the floor?\n- Is your boiler lagged?\n- Does your heating need an upgrade?\n- Do you have the right grade of double glazing?\n- How is your roof insulated?\nIt’s always a good idea to assess what your current situation is before you start installing any new heating system. Most houses that are either up for rent or available to buy now have to have an Energy Performance Certificate. If you don’t possess one for your property then it may well provide a valuable insight into how you can save more on your fuel bills.\nFind out more from the Energy Savings Trust.\nWhat to do next\nIf your office building or home is suitable for something like a heat recovery ventilation system then the next thing you need to consider is how much it is going to save you on your energy bills. Judging the efficiency of any system is often difficult because there are a number of parameters that need to be taken into account such as the size of the rooms, the number and your existing heating system.\nA number of heat recovery systems like HRVSs and boiler economisers are simply methods of reducing your bills. Heat pumps, on the other hand, can earn you extra money through the Renewable Heat Incentive.\nOne thing that you do need to consider when installing a renewable technology such as heat recovery is that energy costs are set to rise steadily over the next few years which means that anything which reduces your usage is going to be of some benefit.","An Air Source Heat Pump (ASHP) captures heat from the air circulating outside your property and boosts the temperature to provide heat and hot water. It is a very effective, low carbon way of heating your home and can be combined with a number of renewable and energy efficient technologies to deliver a truly sustainable home or commercial premises.\nAn ASHP can be installed in new and existing properties and the Government’s Renewable Heat Incentive provides cash payments for all eligible installations, which makes the investment even more attractive.\nIn warmer months ASHPs can operate as air conditioning units, meaning that you benefit from your heating system all year round, whatever the weather!\nHow air source works\nAn Air Source Heat Pump uses the same technology as your kitchen fridge to move heat from one place to another.\nAn ASHP uses the temperature of the air circulating outside to warm a liquid refrigerant.\nThe heat pump turns the refrigerant into a gas, compresses it and then condenses back into a liquid again which generates heat energy and the cycle starts again.\nWhat makes heat pumps sustainable?\nThe temperature of the air outside is determined by the sun and local weather conditions and is therefore a renewable source of heat.\nUsing this renewable energy to help heat your home is more sustainable than fossil fuel alternatives.\nHeat pumps are electric, but by using heat from the air they use less electricity to produce heat than traditional electric heating systems.\nCombining your ASHP with your own solar energy increases their sustainability and further reduces carbon emissions.\nAre air source heat pumps efficient?\nThe efficiency and performance of an ASHP is measured using Coefficient of Performance (COP) and this varies by model and brand.\nA number of factors including the insulation levels that your property has, your existing heating system and whether you are using under floor heating versus radiators will determine the overall performance of your ASHP.\nGovernment incentive schemes to encourage investment in low carbon technology can currently cover the cost of installation and ongoing maintenance costs are lower than with traditional heating systems.\nGeo Green Power have extensive experience and will ensure you understand the options available.\nLearn more about how ASHPs work\nIf you choose Air to Air your ASHP will provide hot air to heat your property and you will need an alternative system to provide hot water.\nIf you choose Air to Water then your ASHP will provide hot water to heat your property and provide hot water for use within your home.\nIt’s important to consider your requirements and any unique aspects of your property and location to create the best solution.\nWhen designed and configured correctly your ASHP does not require a back-up heating option.\nHeat pumps are designed to maintain properties at a consistent temperature. This is when they run at their optimum efficiency and are most effective.\nASHPs can be combined with over-sized radiators or underfloor heating. Underfloor heating is often more efficient, but great results can be achieved with other forms of heating.\nWe will ensure that we recommend the right option for you and your property\nASHPs work all year round and in temperatures down to -15 degrees.\nThey are more efficient when the outside temperatures are higher, however in the UK the difference between seasons is very minimal.\nAll air that is above the temperature of absolute zero has heat energy!\nIn recent years ASHP technology has improved considerably. The cost of ownership has come down and the units have become quieter and more efficient.\nRequest a call to discuss your project\nWhat is the installation process?\nASHPs are fitted to the outside of your property and are about 1m wide, 1m high and around 30cm deep.\nNo ground works are required.\nDepending on your existing system, your ASHP may easily link to your water tank and heating with limited additional works. You may need to replace your radiators for larger units.\nA domestic installation in a single property is usually completed and commissioned within 5 working days.\nLearn more about installation\nASHPs have very few moving parts and require less ongoing maintenance than traditional heating systems. We recommend an annual service to ensure that your system is running at its optimum efficiently.\nYou do not require planning permission for an ASHP.\nYour ASHP is positioned outside your property and is best in a sheltered location which has good air flow. Most properties can easily accommodate an ASHP in a location that is both effective and convenient.\nASHPs work best when they are combined with other energy saving technology such as insulation and double glazing. Running your heat pump to maintain your property at a consistent temperature is also advisable.\nChanging from Oil Fired Heating to Air Source\nWhen Paul took on a 1970s property he wanted to move away from oil fired heating. He carefully compared the cost of replacing the old oil boiler with an up to date condensing oil boiler or an Air Source Heat Pump system and found that the heat pump option was the right choice for him.\nCosts & Expected Returns\nWhat is the installation cost for air source?\nOur ASHP installations start from around £12,000\nHow much do ASHPs cost to run?\nIt’s very difficult to estimate the running costs of your installation as it will depend on the size of the space you want to heat, the temperature you are running the system at and a number of other factors such as insultation and whether you are using underfloor heating or radiators.\nWe will be happy to provide an estimate of your running costs if you would like to discuss your project.\nWhat will my return on investment be?\nThe return on investment that you receive will be determined by the type of fuel you are looking to replace and the cost of running your system. Higher returns can be achieved when switching from oil or LPG heating systems.\nThe current Renewable Heat Incentive provides payments that, in a number of cases, can cover the cost of the installation.\nHeat pumps require less maintenance and repair than most traditional heating systems, which also makes them an attractive investment.\nGet in touch to discuss your ASHP project\nJames oversees all of our air source heat pump installations. Get in touch today to organise a no-obligation consultation about your next project.\nWhat is the RHI and are Grants available for ASHPs?\nThe current Government scheme is the Renewable Heat Incentive (RHI).\nThe domestic scheme provides payments for 7 years and applies to eligible heating systems that are used for one dwelling\nYour system will have an Energy Performance Certificate (EPC) and a Seasonal Performance Factor and these two values will be used to calculate your payments based on your usage and the current tariff.\nEstimated output: 12,000kWh per annum\nEstimated RHI income: £900.00 per annum (7 year contract)\nEstimated CO2 savings: 4 tonnes per annum\nASHPs in Commercial & Domestic Buildings\nASHPs in commercial buildings\nASHPs can easily be installed in commercial premises to provide heating and air conditioning and are particularly efficient when running to provide a consistent temperature for offices and workspaces.\nASPHs in domestic houses\nASHPs provide a low carbon, energy efficient way of heating homes. The systems are low maintenance and combining them with other energy efficient technology helps to ensure that they perform at their best.\nASHPs in New Builds vs Existing Properties\nASHPs installation in new builds\nThe construction of a new property is the perfect opportunity to consider and implement energy efficient, sustainable technology to ensure that the finished result is as cost effective and environmentally friendly as possible.\nThe UK Government is due to bring in the Future Homes Standard in 2025 which will prohibit any new build property from having a fossil fuel heating system.\nIncorporating low carbon, renewable heating into new build projects will ensure that you remain ahead of legislation.\nASHPs installation in existing and historic buildings\nASHPs can be retrofitted into existing properties to replace oil, LPG and mains gas heating systems. The return on investment will vary depending on the existing heating system and other factors such as the property’s energy ratings.\nAir Source vs Alternative Heating Solutions\nComparing ASHPs to existing gas and oil systems\nAn ASHP provides an energy efficient, low carbon option when compared to gas, oil or LPG heating systems. It delivers a significant reduction in CO2 emissions and has lower ongoing maintenance costs.\nThe costs of installation may be higher than traditional fossil fuel systems, but this is offset considerably by the RHI payments you will receive.\nRunning costs are usually comparable to mains gas systems, and significantly cheaper than oil or LPG.\nAir source heat pump or ground source\nAn ASHP installation is often cheaper and less intrusive than a GSHP installation and is ideal for smaller properties and those with limited outside space.\nIf you have a large property to heat, and enough outside space a GSHP will outperform an ASHP. A GSHP system can collect more renewable heat from the earth and therefore requires less electricity to produce the same heat output.\nAir source heat pump or solar panels\nWhen considering an ASHP or solar panels, the best choice for you will depend on your heating requirements, budget and energy usage and we will be happy to discuss your options with you.\nIt may be most cost effective for you to consider a scheme that incorporates both. Using solar power to run your heat pump could deliver the greatest cost efficiency and deliver the most sustainable scheme.\nFind out more about renewable energy…"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c1fc9a80-4f91-48fc-b54f-30fc45c9911f>","<urn:uuid:416a8aee-e3a5-4e75-a89b-505befe84c0e>"],"error":null}
{"question":"What strategies for technological sustainability were present in early web art compared to modern data center management practices?","answer":"Early web art and modern data center management take different approaches to technological sustainability. Early web artists in the 1990s focused on creating works that questioned the medium and its environmental impact, such as Joana Moll's CO2GLE project that tracked search engine emissions in real-time. In contrast, modern data center management implements comprehensive sustainability strategies including LEED certification, advanced cooling systems, and material recycling programs. While early web art used creative interventions to raise awareness about technological environmental impact, contemporary data centers focus on measurable sustainability metrics, implementing specific practices like sustainable lifecycle management, reuse programs, and materials recovery systems. Data centers now have formal processes for reducing their carbon footprint through energy efficiency, renewable power, and responsible equipment disposal - structured approaches that weren't present in the more experimental early web art era.","context":["Processing the Future\nArtFutura, 30 years processing the future. And what will the next 30 years be like? The imagination of the artists flies. The experiment of technological art is renewed.\nToday, now, we live in the present future more than ever. We are right in the middle of two great crossroads that will determine our future. We talk about the Anthropocene and a new phase in human evolution, augmented and reconnected by technology.\nConvinced that transformative technology must be centered on the human being, as we anticipated in Humanized Technology, as we have always done, throughout these 30 years.\nSir Martin Rees, real astronomer and one of the most important cosmologists in history has written that “humans have a collective ‘footprint’ so heavy that they have the ability to transform, and even devastate, the entire biosphere.” In fact, that footprint is so heavy that human influence has initiated a new geological era, the Anthropocene. If certain “inflection points” are exceeded, climate change and mass extinctions will be triggered that will become an exhausted world for future generations, starting with the current Z generation of our children and grandchildren.\nMore than one hundred artists and architects, in different museums and countries have launched a call from art for an emergency planet, in Eco-visionaries. Immersive transformative art, experiences of increased empathy to adapt to the Anthropocene, to reconnect with nature and with ourselves, is what artistic techno groups express as Marsmallow Laser Feast, the Spaniards OPEN THIS END, teamLab, Universal Everything, Collaboration for Humane Technologies or the aforementioned artists that make up the multidisciplinary proposal of Eco-visionaries.\nIf we do not change the direction of the present future we will have\n- A world in collapse and inequality\n- A world at war\n- A species at war with the planet\n- A global climate collapse\n- And the sixth mass extinction of species\nIn our creative and technological hands is the design and construction of viable futures, habitable futures, desirable futures for all. The gap between how the world is and how it could be is wider than it ever was, concludes Martin Rees.\nAn intergenerational pact and an interspecies pact are necessary to get those futures to be designed. The pact between generations is about what our obligations are towards those who will receive the planet after us, forcing us to think about the problems not four years ago but many decades seen. As FridaysForFuture or Extinction Rebellion shout: the future generates political obligations and those who do not have a voice or vote, or are still born, also have political rights, the future is ghostly, at least for them, they only ask for a future to live.\nThe interspecies pact has to inaugurate a new relationship between the technosphere and the ecosphere, a new approach to planetary well-being. Give meaning to an interspecies mutuality, where the rights of non-human entities, other living beings, and in the future artificial living beings are recognized.\nAn interspecies pact based on the conviction that the global production system has collided with the geophysical limits of the planet and we have to achieve a solution to this, but for all species.\n“We live in an Ocean of Air,” is a Marsmallow Laser Feast installation, a multisensory transformative immersion that illuminates the invisible connection that unites the animal and plant world, human and natural, in a wonderful rhythm that sustains life on Earth. Ersin Han Ersin of the Marshmallow Laser Feast collective says that in his works “we seek to repair our broken connection with nature through the experience of art.”\nThe future: Empathic-telepathic communication networks. Transformative technologies that are directed within ourselves, that cross the epidermis and the cortex and reach emotions and consciousness. Transformative advances that today may seem like science fiction, but let’s think about the technologies we have in our pocket, 30 years ago, they would seem magic. The cyborg technologies will allow us to be malleable beings, the future is a morpho, as the extropian philosopher Max More in ArtFutura 97 already anticipated us. Or to extend our senses as the cyborg artist Neil Harbisson has done that sees colors beyond even the visible spectrum , thanks to a neuroimplant frequency sensor, associated with each color.\nThe future: The collective intelligence augmented by technology will become in a few years that profound global mind already announced by Terence McKenna in ArtFutura 92. Virtual Reality, Artificial Intelligence and Internet (IoT) will converge to create Collective Artificial Intelligence, whose language will be virtual reality and neurodigital telepathy. The teamLab collective works with the immersion of the body in the work of art, the boundary between the self and the work of art becomes ambiguous. In his works we immerse ourselves in mixed realities. The collective immersion in the work allows the fusion of people and the world into a whole. This will be the future, a world of expanded reality.\nThe future: The 21st century is the first of all centuries in which pioneers and explorers will be able to develop habitats on other lands, on other planets or satellites. Will space art return? Although the Earth is the cradle of humanity, the human being was not born to stay in the cradle, will we find the human connection in a larger medium, in something bigger than ourselves?\n#30YearChallenge: from bazaar to shopping mall\nThe World Wide Web is 30 years old. In March 1989, Tim Berners-Lee wrote a proposal for the management of information about experiments being carried out at the European Particle Physics Laboratory in Geneva (commonly known as CERN). The document described a distributed hypertext system that would connect the fragments of information stored on CERN’s computers, so that “a single, global information space” could be created .\nBerners-Lee even envisioned a three-dimensional model that would show the connections between researchers, making it easier to understand the very structure of the organization. Three decades later, the web is still basically a set of compositions of text, images, videos and sounds that slide through the browser window.\nBut the possibility of being part of that global information space has profoundly changed the lives of the users and industrialized societies that are part of the privileged portion of the planet with access to the Internet. In these thirty years, the web has also gone from being a bazaar to a shopping centre. If at first it seemed that it could become a free space, chaotic and alien to all order and hierarchy (as Eric S. Raymond described the development of Linux in his well-known essay The Cathedral and the Bazaar, 1997), it soon began to take shape as a controlled environment, with an established order and dominated by the interests of big business and governments.\nThe first artists to create works for the web in the 1990s saw the possibility of taking over this virgin space that seemed to escape the structures of the art world, but they already saw (with some irony) its future as “a shopping centre, a porn shop and a museum” . In those years it was possible for an artist to insert her work among the results of a search for pornographic content on the web and lead users towards something they did not expect: this is what Natalie Bookchin did with her piece The Intruder (1999), a recreation of a brutal story by Jorge Luis Borges through several video games in which the user is led to carry out the harassment and murder of a woman.\nOther artists explored what it meant to be part of the network, anticipating the concerns that, almost two decades later, occupy the debates on digital privacy. Eva and Franco Mattes gave full access to their personal computer in the work Life Sharing (2000-2003), through a web interface that allowed anyone to consult their archives and find out the geographical location of the artists in real time. Fake news also had an early materialization in the activist projects of Ubermorgen: [V]ote-auction (2000-2006), a website that (falsely) promised to buy votes in the U.S. presidential campaign, generated numerous news in the media and torrents of lawsuits, which required the immediate closure of the site. Ubermorgen as well as the Mattes and other artists have known how to play with the thin line that separates reality and fiction online (and nowadays in any media), a line that will soon be definitively blurred thanks to deepfakes, increasingly realistic manipulations of image and sound, developed with Artificial Intelligence.\nIn 2006 the term “web 2.0” became popular and it seemed that, as proclaimed on the cover of the December issue of TIME magazine, the Internet user, person of the year, was in control of the Information Age. The artists Olia Lialina and Dragan Espenschied later stated, “during a short web 2.0 time the user’s creativity earned a lot of praise […] But we missed the moment when Web 2.0 was replaced by a new trend, The Cloud: users in front of dumb terminals, feeding centralized databases and über computer clusters.” . By the end of the first decade of the 21st century it was clear that the Information Age was dominated by four major companies: Google, Apple, Facebook and Amazon.\nThe obvious power of corporations has led many artists to come up with responses in the form of denunciation or attack. A notable example is the trilogy of works that attempted to “hack the monopolies”: Google Will Eat Itself (2005), Amazon Noir (2006) and Face to Facebook (2011), by Paolo Cirio and Alessandro Ludovico (with the collaboration of Ubermorgen in the first two), infiltrated into the Google ad revenue system, the Amazon book preview feature and Facebook user profiles, resulting again in legal claims and the cessation of activities generated by these projects. The relationship between the artists and the systems that govern an increasingly controlled, centralized and neatly arranged web, meant for the consumption of paid content, progressively translates into disruption attempts (whether on commercial platforms, social networks or community spaces such as the formerly popular Second Life) that lead to threats from lawyers and closures of user accounts.\nWhen Petra Cortright tried to entice YouTube users to watch her video VVEBCAM (2007) using keywords linked to pornography, as Natalie Bookchin did eight years earlier, the platform simply removed it from her account. In a web increasingly personalized and reduced to individualities, artists see their strategies limited to operating a change at the level of the single user. This change can occur in the personal profile itself, such as the one Amalia Ulman carries out in Excellences and Perfections (2014) when she creates a performance on Instagram by transforming her own image according to the role models dictated by fashion trends, influencers and the companies that operate behind them; or by inserting themselves through software into each user’s experience, as Ben Grosser does with Facebook Demetricator and Go Rando (2017), two browser extensions that, on the one hand, eliminate the quantified data on the interface of the social network and, on the other hand, randomly apply emoticons to the responses in other users’ posts.\nAt this time, the web has ceased to be an environment disconnected from our daily reality, a world into which we look through the browser window. In fact, it no longer makes sense to talk about the web because we live immersed in the continuous exchange of data that occurs between our digital devices and the servers of large corporations.\nWe speak, in more general terms, of the Internet, but at the same time, as the artist Hito Steyerl proclaimed in 2015, the Internet “is obviously completely surveilled, monopolized, and sanitized by common sense, copyright, control, and conformism […] It is undead and it’s everywhere” . The Internet and the web have ceased to exist because they have replaced reality, as shown by the scenes of different cities in the world captured by Google Street View, compiled by Jon Rafman in his project 9 Eyes of Google Street View (2008-in-progress). We see the world through Google’s eyes, but in doing so we also affect the real state of the planet, as Joana Moll points out in CO2GLE (2014), a website that counts in real time the CO2 emissions of the popular search engine. The “cloud” to which we entrust our data is actually a set of machines and connections that uses enormous amounts of electricity and contributes decisively to Climate Change.\nThis year began with the viral #10YearChallenge, which invited celebrities (and by extension, other users) to share a current photo on Facebook along with another from a decade ago. Along with the challenge, the idea was spread that possibly it was a strategy of the social network aimed at improving its facial recognition technology, and obtaining additional data from users. Whether it’s true or not, it’s characteristic of a time when innocence has been lost about our relationship with what was once called “cyberspace” and which was once a chaotic bazaar, a public square, but which thirty years later opens its doors to us as a neat mall in which we are welcome as consumers and as a product.\nThis gigantic shopping centre, infinite like the Library of Babel, offers us innumerable shops and cinemas, co-working spaces, and some library. But there is no place here for vagrancy, nor for doing strange things (flashmobs, if they become fashionable again, will be organized by commercial brands as part of an advertising campaign), nor for questioning the rules. While the web is being configured as a shopping centre, many governments are considering controlling their citizens’ access to the Internet. In addition to the strict censorship of the government of the People’s Republic of China, which has developed its own web with social networks and exclusive commercial platforms, countries like Iran and Russia are increasingly interested in having an “Internet switch” that allows them to cut off all external communication and limit access to certain sites as they see fit.\nYet there are still spaces of resistance and alternatives beyond content platforms and social networks. But to do so, we need to continue to pay attention to more than just the latest TV series and viral trends, or we need to be able to see through them. Artists have been questioning this medium in which we are immersed practically since it was conceived, and they continue to create artworks that allow us to reflect and critically observe the technologies that shape our daily lives. The next thirty years of what was once the World Wide Web will undoubtedly pose new challenges: following artists and observing their questioning of the status quo will help us see and predict what lies ahead in the global information space in which we live.\n Tim Berners-Lee with Mark Fischetti, Weaving the Web. New York: Harper Collins, 1999.\n Natalie Bookchin and Alexei Shulgin, Introduction to net.art (1994-1999). Rhizome Artbase. http://archive.rhizome.org/artbase/48530/index.html\n Olia Lialina and Dragan Espenschied. Digital Folklore. To computer users, with love and respect. Stuttgart: merz & solitude, 2009.\n Hito Steyerl. Too Much World: Is the Internet Dead?, in: J. Aranda, B. Kuan Wood and A. Vidokle (eds.) The Internet Does Not Exist. Berlin: Sternberg Press, 2015.","Reduce your data center’s carbon footprint and support sustainability goals.\nWhite Paper Sustainable Decommissioning\nSustainable Data Center Decommissioning\nThis is a significant time for sustainable development. In a historic 2015 UN summit, the United Nations (UN) member states adopted the 2030 agenda for sustainable development.\nOn January 1, 2016 the 17 Sustainable Development Goals (SDGs) came into force. Later that same year the Paris Agreement on climate change, which addresses the need to limit the rise of global temperatures, was ratified by 175 countries.\nData centers are among the highest consumers of power today. But with the internet of things, growing big data and expanding connectivity, the need for more and more of these facilities is inevitable. Data center sustainability is becoming a priority.Data Center Frontier, April 4, 2019\nThe development of IT is generally seen as a force for good – an enabler toward an equitable and more sustainable world. But data needs are growing, as current estimates disclose:\nIt takes a huge amount of energy to manufacture, install and run global data centers to meet these growing data needs. With more than 7,117 colocation data centers, as well as hyperscale and wholesale data centers in existence, ensuring that data centers are as environmentally sustainable as possible is a concern of growing importance.\nShould a Company’s Data Management be Part of Their Sustainability Conversation?\nAccording to a study, data centers contribute approximately:\nIn fact, several institutions predict that data center energy usage could account for over 10% of the global electricity supply by 2030.\nMany data center users and providers recognize this and are pioneering environmentally responsible data management. According to Data Center Frontier, “the data center industry is focusing on sustainability as never before, with the executive suite and customers aligned on the importance of using renewable energy to power digital infrastructure.”\nSustainability in the Data Center: What does that look like?\nLEED-Certified Data Centers\nLeadership in Energy and Environmental Design (LEED) – developed by the U.S. Green Building Council – is a set of rating systems for the design, construction, operation and maintenance of green buildings. LEED has developed a certification for sustainable design and construction of data centers. Less than five percent of all U.S. data centers have LEED certification.\nFacebook, requires that their data centers achieve LEED gold level certification; “With each new data center we build, we add more renewable energy to the grid. And we make sure that all of our renewable energy projects are in the same electric grid as our data centers… our data centers are already 80 percent more efficient than the average data center.”\nCharacteristics of a ‘sustainable’ data center are considered by LEED to have/be;\n- Advanced cooling systems to reduce energy consumption\n- Improved cooling efficiency\n- Reduced energy consumption\n- A clean backup power system (reducing emissions, noise pollution and fuel consumption)\n- Using renewable energy\n- Green construction (recycled materials, reducing distance raw materials move, diverting waste)\n- Intelligent design\nMany data centers looking to improve their green credentials invest in ISO50001 accreditation for Energy Management. This ISO certification supports organizations in all sectors to use energy more efficiently, through the development of an energy management system.\nThe existing accreditations focus on build and in-life use of the data center, none consider end-of-life equipment age and lifespan possibilities or opportunities.\nSustainable Lifecycle Management\nThere are opportunities for data center users, owners and operators to boost their sustainability credentials and offset their carbon footprint by integrating lifecycle thinking into the design, operation and decommissioning of hardware. The opportunities can be broadly grouped as;\n- Commitment to sustainable end-of-life management\n- Opportunities for reuse\n- Material recovery for remanufacturing\nSustainable End-of-Life Management\nData centers can demonstrate their commitment to sustainability by promising to stop throwing away any aging, broken and end-of-life assets and equipment from their facilities. By doing this it diverts waste from landfill and promotes the case for reuse and recycling.\nData centers that make their own custom servers have the opportunity to partner with recycling technicians to consider reuse into their design. This might give their equipment the ability to operate longer, the possibility to be repaired and refurbished easily, and to incorporate only materials that can be recycled without residual materials going to landfill.\nOpportunities for Reuse\nWhen equipment reaches the end of its economical life companies can engage a credible third-party IT asset disposition (ITAD) operator for IT asset recovery. This might involve the recovery, repair, reuse and remarketing of complete units, parts and components, and base materials within the data center environment or outside of it.\nThe first value recovery option is to redeploy whole or part units into a company’s own environment. This can be supported by an ITAD company, such as Sims Lifecycle Services (SLS), that supports decommissioning, data wiping, audit testing, cleaning and redeployment. For data centers this is not just an exercise in sustainability, it is also a way to save money by leveraging reuse and recycling efforts to reduce costs.\nSecond-hand units can be sold into what is known as the ‘second-hand market’. This can be the case for either entire servers, or parts and components. By engaging with an experienced ITAD company like SLS to manage your reuse and resale program you gain control over these units. This gives you the ability to manage this in line with new product launches.\nMaking informed decisions about equipment placed on the second hand market can create an additional revenue stream for a business. At a time when the economical life of a server is ending, ITAD companies help to add market value to a device that has most likely been depreciated to zero on the books. This revenue could then become a budget to help fund more sustainability efforts.\nPulling equipment for redeployment\nReuse allows for cost avoidance opportunities. Redeployment options can feed into repair and manufacturing streams. If an organization manufactures its own custom servers reusable parts can be identified in legacy equipment for the manufacture of new products. Or for any data center operator, reclaimed parts can be deployed into repair centers to support maintenance contracts.\nReuse diverts waste from landfill, reduces the reliance on mining virgin materials and reduces the impact of manufacturing electronics.\nMaterials Recovery for Remanufacturing\nIn addition to reducing carbon emissions from data center energy use, materials recycling can contribute to offsetting the impact of mining virgin raw materials for new data center equipment.\nOne major technology company identified in their environmental responsibility report that “…the carbon footprint of our manufacturing processes represents the largest portion of our impact on climate change. Every year we investigate more deeply into our supply chain, constantly analyzing inefficiencies and developing ways to help our suppliers make less of an impact on the planet.” Many of the resources used to create data centers are finite.\nSpotlight: Recyclable Materials in Data Centers\nOrganizations must make strategic decisions when buying new material, and it starts with their sourcing supply chains. An original equipment manufacturer (OEM) that implements sustainable sourcing and practices is usually favored over one who does not.\nPlastics: Traditional plastics production involves the transformation of petroleum or natural gas into their constituent monomers. The process is highly energy intensive and has been estimated to account for 1 percent of total greenhouse gas emissions. The carbon footprint of recycled plastics is a mere fraction of that of virgin plastics.n of that of virgin plastics.\nAluminum: Traditional aluminum production uses a large amount of electricity to break the bond between oxygen and aluminum in aluminum-oxide. Recycling aluminum saves 90-95 percent of the energy needed to make aluminum from bauxite ore. There is no limit to how many times aluminum can be recycled.\nSteel: Recycling one tonne of steel saves 1,100 kgs. of iron ore, 630 kgs. of coal, and 55 kgs. of limestone. Steel recycling uses 74 percent less energy, 90 percent and 40 percent less water than virgin steel production. It also produces 76 percent fewer water pollutants, 86 percent fewer air pollutants, and 97percent less mining waste.\nThese materials make up the majority of material, by weight, in a typical data center. Additional valuable materials reside within the printed circuit boards (PCBs), processing chips and power supplies within the server units, such as valuable precious metals (i.e. gold, copper and silver).\nGold: One tonne of modern PCBs might contain five ounces (around $6,000 worth) of gold.\nTantalum: This is commonly found in processors and capacitors. Tantalum is a critical raw material with a current end-of-life recycling rate of less than-one percent.\nLead: This is found in batteries in data center universal power supplies (UPS). Lead is highly toxic when disposed of irresponsibly. Professional recycling diverts this dangerous waste from landfill and avoids environmental and human health impacts.\nCopper: This is a key component in wiring and printed circuit boards. Although newer wiring contains less copper than older units, like all metals, copper can be infinitely recycled to avoid raw material mining and reduce energy emissions.\nSome of the minerals used in server production have been identified as conflict materials. These usually involve materials mined in the eastern provinces of the Democratic Republic of Congo and its neighboring countries. Resulting revenues from the purchase of these raw materials are known to be financing, directly or indirectly, armed groups engaged in civil war resulting in serious social and environmental abuses.\nMaterials included in the conflict materials group, known as 3TG, include Tin, Tantalum, Tungsten and Gold. These are all commonly found in data center equipment. Using recycled materials helps divert revenues away from mining enterprises for these conflict materials.\nDiverting Materials From Irresponsible Recycling\nIf not recycled properly, electronic waste can be a serious health and environmental issue. To make a quick profit, unethical recyclers sometimes dump e-waste or use dangerous techniques that can leach toxins and harm the environment. Using a responsible recycler ensures material is diverted from these sites.\nSLS’s Top Advice for Sustainable Data Center Lifecycle Management\n1. Avoid Stockpiling Redundant Assets\nNot only does this monopolize valuable floor space that could be used for revenue generating equipment, but data breach risk aggregates when it is left unmanaged.\nAs equipment is stockpiled, value reduces as assets idle in a dusty corner. While equipment is stored SLS estimates that value reduces 3-5 percent per month. Potential clients may identify this as evidence of weakness in a process and the temptation exists to adopt less secure methods of data destruction. This ultimately increases the risk of a security lapse and decreases the chance to recoup value.\nSupport for a sustainable and environmental reuse and recycling program will likely rely on it also being secure and value-driven. An ITAD and e-Recycling partner should offer a solution that delivers data security and value recovery with equal importance to sustainability.\n2. Follow the Waste Hierarchy: Reduce > Reuse > Recycle\nMost ITAD and e-Recycling partners can support the refurbishment and repair of assets to be redeployed into a company’s own environment.\nReuse of materials or equipment, avoids the greatest amount of energy emissions, and delivers the highest value in many cases. Even if whole units cannot be reused look for opportunities to reuse parts and components.\nEnd-of-life equipment should be recycled. Some ITAD vendors may outsource this service. It is important for Data Center Managers to understand who the recycler is and how equipment will be responsibly recycled to protect their company’s brand reputation.\n3. Think Global but Keep it Local\nIt is always better to have businesses working with vendors that can recycle material in the region where it was collected. Where possible it is better to avoid shipping overseas for disposal. Businesses will need to always seek evidence and assurance that recycled materials are not being dumped in developing countries once handed over to a disposition vendor. This applies to their downstream vendors also, so look for evidence of a robust downstream vendor management system.\nAn operator’s geographical footprint should meet the needs of the company and matches their locations. Recyclers should also be able to offer a full range of lifecycle management services – IT asset disposition and e-Recycling.\n4. Enforce Accountability\nSetting targets for a company’s end-of-life data center equipment sustainability goals is a great way to set forth some accountability. It is helpful for many to share their goals with the organization, and measure progress against them. Examples of goals might include:\n- What percentage of equipment do you want to redeploy, reuse and recycle?\n- What is the ambition for energy and emissions avoidance? (This goal might be considered as a percentage of the footprint from data usage – offsetting the impact)\n5. Choose the Right Partner\nSustainability credentials should not be the only determinant in a partner choice. It is recommended to look for a financially-stable company, with experience and the ability to produce references from reputable organizations. Once it has been established that a chosen vendor has the ability to deliver, check that their ethics and values match as well to create the best possible partnership.\n6. Talk About it!\nCelebrate what you do. Managers should talk about their sustainable and secure ITAD and e-Recycling programs and use it as a sales tool. Look for a company that provides sustainability certificates and the ability to demonstrate an organization’s contribution to environmental protection goals.\nThe Time for Change is Now\nCompanies are starting to take responsibility for their brand by facilitating a closed-loop program that contributes to the circular economy. This supports the drive to address global resource scarcity by facilitating this urban mining of valuable and scarce materials.\nAs the data center industry continues to grow, it is necessary for more data centers to integrate reuse and recycling programs into their process to help minimize e-waste volumes by diverting waste from landfills. Green data centers can reduce the environmental impact of their data consumption and increase the productivity of their data center assets. In addition, they can reduce the social impact of their data usage by lowering their organization’s reliance on conflict materials, and minimizing their risk of irresponsible recycling of equipment in the informal recycling sector."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:5b401057-98f7-4bff-a9da-2b67b137db9b>","<urn:uuid:ddb6540e-7370-40bf-be20-dd45f0ec530f>"],"error":null}
{"question":"What are the fundamental principles of construction management methods, and how do they connect to subcontractor documentation systems?","answer":"Construction management methods focus on preventing quality deviations and delays through proper project management principles. While classical management solutions show limitations, newer approaches emphasize the importance of coordinated information flow and specialist involvement from different fields. Regarding subcontractor documentation systems, they serve as critical tools for outlining guidelines and terms for third-party subcontractors, trades, and vendors. These systems can be implemented through three main methods: manual contracts using word processing tools, contract software tools with digital signatures and storage, and integrated construction management software suites that combine all project and subcontractor details. The integrated approach provides the most efficient solution by connecting contract management with other project aspects like change orders and payment tracking.","context":["CRITICAL PATH METHOD AND LEAN DESIGN MANAGEMENT APPLIED IN DESIGN OF SOLAR ENERGY PROJECTS: A COMPARISON STUDY\nDelays in the construction sector occur frequently in projects and have a major impact on the cost, duration, and quality of the project. Classic management theoretic principles applied, lack of commitment to project management methods altogether with slowing down in the adoption of current technologies could explain problems of project management consisting in frequent project failures. The present research aims to use the lean management method in the construction activity starting with the design management. The paper presents two comparative case studies: the first in the classical management solution and the second using the steps of the lean method, in corroborating the information necessary for: design the installed capacity to produce electricity (photovoltaic solar panels), design their positioning and installation on roofs terrace to existing structures that have undergone various changes over time. In addition, the Investor requested other interventions that required the involvement of several specialists from different fields. Performance indicators were calculated between the two solutions, favourable to the lean method.\nS. Durdyev, M. Omarov, and S. Ismail, “Causes of delay in residential construction projects in Cambodia,” Cogent Eng., vol. 4, no. 1, (2017)\nL. Koskela and G. Ballard, “Towards Lean Design Management,” Educ. Theory, vol. 1, no. 4, pp. 241–247, (1997).\nJ. Burati, J. Farrington, and Ledbetter William, “Causes of Quality Deviations in Design and Construction,” J. Constr. Eng. Manag., (1992).\nP. A. Tilley, “Lean design management - A new paradigm for managing the design and documentation process to improve quality?,” 13th Int. Gr. Lean Constr. Conf. Proc., no. January, pp. 283–295, (2005).\nE. I. Galaz-Delgado, R. F. Herrera, E. Atencio, and F. M. Rivera, “Problems and Challenges in the Interactions of Design Teams of Construction : A Bibliometric Study,”(2021).\nG. A. Howell, G. Ballard, and I. Tommelein, “Construction Engineering—Reinvigorating the Discipline,” J. Constr. Eng. Manag., vol. 137, no. 10, pp. 740–744,( 2011).\nL. Koskela and G. Howell, “The underlying theory of project management is obsolete,” IEEE Eng. Manag. Rev., vol. 36, no. 2, pp. 22–34, (2008).\nL. Koskela, “Application of the new production philosophy to construction,” vol. 72, (1992).\nR. F. Herrera, C. Mourgues, L. F. Alarcón, and E. Pellicer, “An assessment of lean design management practices in construction projects,” Sustain., vol. 12, no. 1, pp. 1–19, (2020).\nB. Mota, C. Biotto, A. Choudhury, S. Abley, and M. Kagioglou, “Lean design management in a major infrastructure project in UK,” 27th Annu. Conf. Int. Gr. Lean Constr. IGLC 2019, no. November 2019, pp. 37–48,( 2019).\nG. Ballard, “The Last Planner System of Production Control,” no. January,( 2000)\nBallard, Glenn, Hamzeh, Farook, Tommelein, “The Last Planner Production Workbook-Improving Reliability in Planning and Workflow,” Lean Constr. Inst., vol. 53, no. 9, pp. 21–25, (2007).\nJ. Lerche, H. H. Neve, G. Ballard, J. Teizer, S. Wandahl, and A. Gross, “Application of Last Planner System to Modular Offshore Wind Construction,” J. Constr. Eng. Manag., vol. 146, no. 11, p. 05020015, (2020).\nCremona Matteo, The application of Last Planner System in Construction Design. LAP LAMBERT Academic Publishing, (2013).\nF. R. Hamzeh, G. Ballard, and I. D. Tommelein, “Is the Last Planner System applicable to design? A case study,” Proc. IGLC17 17th Annu. Conf. Int. Gr. Lean Constr., no. July, pp. 165–176, (2009).\nC. N. Biotto, “Integration of overlapped design and construction stages through location-based planning tools.,” p. 311, ( 2018).\nEUBIM Task Group, “Handbook for the introduction of Building Information Modelling by the European Public Sector,” EU BIM Task Gr., p. 84, (2016).\nG. S. Mughees Aslam, Zhili Gao, “Integrated implementation of Virtual Design and Construction (VDC) and lean project delivery system (LPDS),” J. Build. Eng., (2021).\nM. Lorenz, M. Rüßmann, R. Strack, K. L. Lueth, and M. Bolle, “Man and Machine in Industry 4.0,” Bost. Consult. Gr., p. 18, (2015).\nB. Bungardi, M. Izvercian, A. Pugna, A. Agache, “Unmanned Aerial Vehicle (UAV) in construction management: a literature review, applications and challenges”, RMEE2020, presented, not indexed.\n- There are currently no refbacks.","What is a subcontractor agreement?\nConstruction businesses use subcontractor agreements to outline the guidelines and terms expected of third party subcontractors performing work or supplying materials. These agreements contractually obligate subcontractors, trades, and vendors to adhere to certain standards on contractor jobsites.\nIn residential construction, builders and remodelers create subcontract agreements with any subcontractor, trade, or vendor working on a project or who partners with their company. These contracts document project details (such as scope, price, and time), payment expectations, insurance requirements, jobsite conduct, and more.\nWhy are subcontract agreements important in residential construction?\nOne of the biggest objectives of a residential construction project is ensuring the client receives excellent quality and craftsmanship. Any building firm with a large internal team controls the labor output simply by setting standards for employees. Working with subcontractors and trades adds in layers of complexity since these individuals work outside the construction company team. While these individuals may have their own personal or company standards and guidelines, they may not align with the building firm’s goals. Without any clear guidelines outlining the responsibilities of both parties, building firms and subcontractors risk damaging relationships and costly legal suites.\nSubcontract agreements fill this gap by providing a legal document where building firms and subcontractors negotiate the terms and conditions necessary for both parties to feel satisfied and protected. With a signed agreement finalized before work begins, subcontractors get the space to focus on the work, while builders and remodelers receive peace of mind knowing expectations align with their goals and ultimately the clients dreams.\nHow do builders + remodelers obtain, organize, and store subcontract agreements?\nAfter consulting with a legal representative to craft what terms, guidelines, and standards fit the business needs, there are a couple methods builders and remodelers employ to document these contracts, obtain a signature, and store the final product.\n1. Manual Contracts\nMany word processing tools effectively capture the desired language needed to get alignment between construction businesses and subcontractors. Once created, subcontract agreements can be printed for both the building firm and subcontractor to sign. While quick and easy to generate, printed agreements require either the coordination of an in-person meeting or rely on the postal service for back and forth delivery of this critical paperwork, all just in time for work to start. Once signed, paper agreements need filing and safe storage for future reference and tracking, along with any addendums or revised copies. Builders and remodelers face the risk of needing this paperwork at a moment’s notice when they aren’t in the office or they are at the mercy of an administrator’s availability, plus the added stress of carefully moving this paper around without damaging or losing it.\n2. Contract Software Tools\nGoing digital solves many of the problems that come with manual methods. Software tools that enable eSignature make in-person meetings and snail mail irrelevant, plus reduce the time spent getting these critical documents signed before work begins. With an organized, digital filing system, finding this paperwork becomes simple and easy, allowing project managers to pull up information anytime. Operating without a cloud-based or network driven storage system, however, resurrects many of those more manual risks and stressors since electronic files often end up on one team member's computer. Depending on the document storage system, subcontract agreements across several subcontractors and projects can get mixed together, creating chaos and costly delays. While great at keeping these contracts out of the rain, such tools don’t bring the whole workflow together or include paying subcontractors for their work, communicating changes to work, or tracking expiration dates.\n3. Integrated Construction Management Software Suites\nCombined with all other project and subcontractor details, subcontract agreements reach new levels of efficiency in an integrated construction management software suite. Builders and remodelers need to quickly reference contracts with subcontractors in the same instance as looking up the latest client change order or the last paid bill. Softwares that integrate these details together save countless calls to the office and eliminate hours of searching and pulling details together across disparate folders, all while tying these details together directly."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:09033396-41b0-4660-aedd-752948605c4a>","<urn:uuid:9bd7a61b-fd3f-45f1-a1ff-16c6a42d5272>"],"error":null}
{"question":"I've heard about gamma rays both in space and in medicine. What makes gamma ray bursts so powerful in space, and how are these rays safely harnessed for brain tumor treatment?","answer":"In space, gamma ray bursts are the most powerful explosions known, capable of releasing as much energy as our sun would emit during its entire 10-billion-year lifetime in just milliseconds to minutes. These rays are created during violent cosmic events, such as the death of stars. When it comes to medical applications, gamma rays are carefully controlled and focused for therapeutic use. In radiation therapy for brain tumors, the high-energy beams are precisely programmed to attack only rapidly dividing cancer cells while minimizing damage to normal cells. The treatment is non-invasive (unless brachytherapy is used) and can be delivered through various techniques such as External Beam Radiation Therapy (EBRT) and Stereotactic Radio Surgery. While powerful, the treatment is carefully monitored with follow-up appointments and periodic PET scans to assess its effectiveness.","context":["Gamma rays are blamed for making Bruce Banner the Incredible Hulk. But what are gamma rays and what can they really do?\nGamma rays are the highest energy form of light. The rainbow of visible light that we are most familiar with is just part of a far broader spectrum of light, the electromagnetic spectrum. Past the red end of the rainbow, where wavelengths get longer, are infrared rays, microwaves and radio waves, while beyond violet lie the shorter wavelengths of ultraviolet rays, X-rays and, finally, gamma rays.\nA gamma ray packs at least 10,000 times more energy than a visible light ray. Unlike the Incredible Hulk, gamma rays are not green — lying as they do beyond the visible spectrum, gamma rays have no color at all that we can describe.\nExactly how Bruce Banner survives his transformation is unclear.\nJust as high doses of X-rays are typically lethal, so too would an explosion of gamma rays kill the average person.\nGamma rays can knock electrons around like a bowling ball would bowling pins. These charged particles can then disrupt any chemical bond they come across, wreaking havoc on the delicate chemical machinery of the cell and generating molecular fragments that can act as toxins.\nTo put it gently, a gamma bomb in the real world would not turn Bruce Banner into the Incredible Hulk. Rather, it would likely quickly turn him into a corpse dead from radiation sickness, if not incinerating him instantly.\nStill, gamma rays can have medical applications — a medical device known as the gamma knife can kill tumors by aiming gamma rays at a patient's brain.\nWhen Bruce Banner becomes the Incredible Hulk, his body swells with muscles seemingly from out of nowhere. Intriguingly, gamma rays can be so powerful that they can actually create matter. This is because, as Einstein's formula E = mc2 explains, energy can get converted to matter, and vice versa. Extraordinarily high-energy gamma rays, such as ones that black holes can generate, can yield pairs of electrons and their antimatter counterparts, known as positrons. (Whether the Incredible Hulk uses gamma rays to violate the law of conservation of matter and grow is another question.)\nGamma rays in space\nGamma rays are created under some of the most violent events in the universe, such as the death of stars. The Gamma-Ray Large Area Space Telescope (GLAST), set to launch June 11, will be the first gamma-ray observatory to survey the entire sky every day with unprecedented sensitivity, and the hope is that it will open a dramatic new window onto the cosmos. (GLAST will receive a new name once in orbit, chosen from some 12,000 suggestions given by the general public around the world, and the name \"Hulk\" did come up.)\nIn particular, GLAST could shed light on mysterious gamma ray bursts, which can unleash as much energy as our sun during its entire 10 billion year lifetime in anywhere from milliseconds to a minute or more. Just as the Incredible Hulk \"is the strongest one there is,\" as he says himself, so too are gamma ray bursts the most powerful explosions known.\nIndeed, just as the Incredible Hulk is strong enough to destroy the entire planet, so too can a gamma ray burst kill life on this world. A \"death star\" was recently discovered that might one day explode with a gamma ray burst directed straight at us — although it might readily miss.","What is Radiation Therapy for Brain Tumour?\nRadiation therapy for brain tumours is a treatment method that uses high-energy X-rays, gamma rays, or protons to kill cancer cells or retard their growth as much as possible. Cancer cells differ from normal cells not only in their appearance but also in how fast they divide and form new cancer cells. The radiation is programmed in a way that it attacks and destroys only rapidly dividing cells, which ensures that normal cells are not affected.\nAlternate Name of Radiation Therapy for Brain Tumour\nExternal Beam Radiation Therapy (EBRT)\nHow is Radiation Therapy for Brain Tumour Performed?\nBrain tumours are treated using high-powered radiation sources that aim to attack only cancer cells. For highly invasive tumours, surgeons use radiation to shrink them before they can surgically remove them. Radiation therapy is very effective even as a standalone treatment for brain tumours and is also non-invasive.\nThere are many types of radiation therapies – EBRT, 3D conformal therapy, Intensely Modulated Radiation Therapy, Stereotactic Radio Surgery, Conformal Proton Beam Therapy, and more. These different types of radiotherapies differ in source radiation, but in all of them, a focused beam is projected towards the tumour area in the brain, which aims to kill the cancer cell.\nWith the advancements made in medical science, doctors are now able to guide high-energy beams in different directions using imaging techniques such as MRIs in real time. A machine sends the rays of energy to the tumour. This treatment is usually done every weekday over a course of three to seven weeks.\nIn brachytherapy, small seeds of radioactive substances are placed in or around the tumour surgically. This way, only local tumour areas are affected by the radiation spreading from the seeds. Once treatment is completed, the surgeon will remove these seeds via a catheter or applicator.\nPreparation for Radiation Therapy to Treat Brain Tumour\nBefore radiation therapy, your doctor may require you to undergo a couple of procedures. Some of them are:\n- Pre-operative imaging such as a 3D MRI: Before undertaking radiation therapy, the oncologist will assess the pros and cons of different treatment modalities in order to ensure the best possible outcomes. For this, pre-operative imaging scans are carried out that help determines the invasiveness of cancer.\n- Radiation simulation: Radiation therapy requires you to be extremely still, and so a simulation will be carried out by your oncologist to ensure you can find a comfortable position.\n- Dietary requirements: Your oncologist will guide you with a detailed diet plan as some foods such as those high in acidity and sodium need to be avoided when receiving radiation.\nNon-invasive, unless brachytherapy is performed\nA radiation therapy regimen can last anywhere between three to seven weeks. Once the radiation regimen is completed, the doctor will ask you to come for a follow-up appointment 3-6 weeks after the last session of radiation therapy.\nThe doctor may ask you to get periodic PET scans to assess whether the radiation has affected your cancer and how much. On the assessment of the same, they will further guide you on the next steps you need to take. This follow-up may be carried out every 6 months after the initial sitting, to ensure cancer has not recurred.\nIn case of radiation therapy has been used to shrink the tumour before surgery, the doctor will assess your fitness and your ability to handle an invasive procedure following which they will guide you for the surgery. It is ideal to report any and all side effects to your doctor during these follow-up visits.\nRisks for Radiation Therapy to Treat Brain Tumour\nSome of the risks posed by radiation therapy are:\n- Side-effects: Damage to surrounding areas in the body may result in mood imbalances and fatigue, nausea, headaches, and so on.\n- Hair loss: Patients undergoing radiation therapy for brain tumours report hair loss 2 to 3 weeks into the therapy. The hair almost always grows back after 3-6 months.\n- Vision or memory loss: Although quite rare, if the tumour infringes on the areas of the brain that are responsible for either vision or memory, radiation therapy may result in damage to surrounding areas and cause temporary vision loss or memory loss.\nRecovery From Radiation Therapy\nMost side-effects of radiation therapy such as mood imbalances, anxiety and nausea are mild and resolve on their own and fairly quickly. However, it is advised to report any and all side effects to your oncologist.\nAs far as cancer is concerned, the response of cancer to treatment is quite subjective. In most cases, cancer responds well within a few weeks of radiation therapy. If the aim of the therapy is to shrink cancer, then this sign is followed up with surgery, which prolongs the recovery period due to its invasiveness. In case cancer does not respond to the therapy, the oncologist may suggest alternative treatments.\nRadiation Oncology Therapy: How Radiation Therapy Is Used for Cancer Treatment\nIn Nanavati Max Institute of Cancer Care\nApr 24, 2323\nDifferent Types of Radiology and Its Techniques\nApr 24, 2323\nGet Second Opinion\nGet free second opinion from India’s leading specialists."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9cfe7e76-c3ea-4e6d-9488-bd1a8d9ee6c2>","<urn:uuid:88cbe9a4-b87f-43fd-8ead-5ae247350706>"],"error":null}
{"question":"What role did the Silk Roads play in both spreading and combating smallpox, and how does this compare to the communication challenges faced during Civil War battles?","answer":"The Silk Roads played a dual role in smallpox's history - while trade along these routes helped spread the disease across regions from Egypt to China, Japan, and Europe from the 6th-13th centuries CE, these same routes also facilitated the spread of medical knowledge and treatments. Notably, the practice of variolation traveled along the Silk Roads, with Indian priests and Muslim physicians sharing this early form of inoculation. Similarly, during the Civil War, communication was crucial but challenging, as seen in the Battle of Seven Pines where fragmented units like the 11th Maine Infantry had difficulty coordinating their positions and responses, with some companies becoming isolated and having to make independent decisions about retreat and engagement.","context":["For a precious few minutes around 12 noon on Saturday, May 31, 1862, all four pieces of the 11th Maine Infantry Regiment stood on the battlefield at Seven Pines, Va.\nThen the Confederate onslaught smashed into the Union picket line astride the Williamsburg Stage Road, and the collision sliced and diced the already quadrisected regiment.\nCol. Harris Plaisted and Maj. Robert Campbell had slid companies A, C, and F between the four cannons and gunners of Battery H, 1st New York Light Artillery to the south and eight companies of the 104th Pennsylvania Infantry to the north. The New York gunners had deployed just north of the stage road; on the other side of the road spread the 100th New York Infantry.\nAbout a mile north on the far right flank of Brig. Gen. Silas Casey’s 3rd Division, Co. D of the 11th Maine had manned the picket line since Friday. Pvt. Robert Brady Jr. heard the intensifying shooting; “the left of the picket line was forced back” and finally broken, he learned later.\nSomewhere on that broken picket line were companies B, G, H, and K of the 11th Maine. Realizing his probably less than 80 men could not stem the Southern tide, Capt. Winslow P. Spofford of Co. G pulled all four companies back some 800 yards.\nRetreating in good order, Spofford’s men fell in with the 56th New York Infantry Regiment near the Richmond & York River Railroad tracks north of Seven Pines. Here these particular 11th Maine lads would fight for the rest of the day.\nAs for Co. D, “it was uncertain what the pickets should do,” Brady admitted. Capt. Leonard Harvey had marched away hours earlier with a prisoner; D’s de facto on-site commander, 2nd Lt. Francis M. Johnson, now took a corporal and disappeared into the woods north of the company’s picket post “to learn, if they could, what (Union) force … guarded the [right] flank,” Brady said.\nJohnson’s nonchalant decision left 1st Sgt. Robert Brady (the younger Brady’s father) in charge. He acted like a responsible adult and stayed with his men.\nJohnson and the corporal showed up later, much too late to help Co. D.\nMeanwhile, heavy fighting engulfed the south end of the defense line formed by the 3rd Division to which the 11th Maine was attached. Ordering his to move “about 30 yards to the right” of Battery H, Plaisted had his 93 men “lie down behind a ridge” to avoid Confederate artillery fire.\nHe also told his men to “reserve their fire until the rebels emerged from the woods” to the west.\nThough the Battery H gunners “kept throwing canister into their (Confederate) ranks with great effect,” Brig. Gen. Silas Casey (the division commander) realized that enemy infantry threatened to turn his division’s flanks and capture the artillery’s four 3-inch Parrott rifles.\nOnly the cold steel could save those guns.\nThe 11th Maine served in the 1st Brigade commanded by Brig. Gen. Henry Naglee. Around 1 p.m., he “rode up in front of my line amidst a shower of bullets, and ordered me to charge,” Plaisted said. “With the greatest enthusiasm, the order was obeyed.”\nMoving simultaneously with the 104th Pennsylvania on its right (north) flank and the 100th New York on the south side of the stage road, the 11th Maine boys charged. Color Sgt. Alexander Katon of Co. B and Pittston “bore our standard bravely in front of the line,” Plaisted said.\nWhen he heard, “Forward to the fence!,” Katon ran “several yards in advance” of the Pennsylvania line some 200-300 yards “across the open space” to an old worm fence, Plaisted said. Reaching the fence first, Katon “firmly planted our flag” against it.\nHe held the flag staff “with the greatest steadiness, amidst such a shower of bullets” that no man could possibly survive, Plaisted believed.\nScaling the fence, the Mainers and Pennsylvanians stood about 50 yards from the woods swarming with enemy troops. The Maine boys fired repeatedly, and Confederates — lots of ’em — shot back. In the color guard, Corp. Willis Maddocks of Co. K fell dead beside Katon.\nEnemy bullets shattered the flag staff held by Katon; kneeling so that he leaned over Mattocks’ body, Katon held the shattered staff and flag as high as his arms and hands could reach. Later, he counted 11 fresh bullet holes in the 11th Maine’s flag.\nMoving from north to south as bullets filled the air around him, Robert Campbell leaned over and tapped the back of almost every soldier shooting at the nearby Confederates. “Fire lower, boys, fire lower,” Campbell told the sweating, gunpowder-stained soldiers. “Aim lower, boys, aim lower.”\nMaine boys pitched and fell along the regiment’s thinning line. With the 104th Pennsylvanians falling back and “two-thirds of my commissioned officers and one half of my little battalion … either killed or wounded,” Plaisted “reluctantly … gave the order, ‘Retreat.’”\nFalling back through their camp, the 11th Maine boys then flowed east across the Nine Mile Road and into Seven Pines, where Casey had organized another defense line. There Plaisted learned about the fight that companies B, G, H, and K had, while fighting with the 56th New York, “behaved nobly and retired from the field in good order.”\nTwo pieces comprising seven companies of the 11th Maine pie were back together. That left three 11th Maine companies out on the far right of the picket line.\nAt the Co. D position, 1st Sgt. Robert Brady “realized by the sound of the battle that he was cut off from his camp,” his son said later. The elder Brady told his men that if Co. D retreated, they should carefully withdraw “toward the right and rear.”\nA Confederate line of battle appeared about noon and moved toward Co. D; 1st Sgt. Brady ordered men to drop trees across the woods road leading into the company’s position. Other soldiers fired on the advancing Confederates and momentarily checked them.\nNot far away, 1st Lt. Francis Sabine of Co. E had also spotted the approaching enemy troops. They passed a bit north of the position held by companies E and I and overlapped Co. D, whose members retreated to avoid capture.\nMost (including Brady Jr.) reached Union lines; the elder Brady and six other men were captured.\nConfederates shoved the 3rd Division east through Seven Pines as the day progressed; “we heard a terrible firing” to the east, behind the picket line, Sabine said.\nInvolving a considerable number of troops, another Confederate attack headed toward the woods defended by companies E and I late on Saturday. Along with Capt. Simeon Merrill of Co. I, Sabine ordered his men “all on to their posts.”\nThe last 11th Maine lads this far out on the battlefield, the 60-odd soldiers ran into the wheat field between them and the Confederates, reached a rise, and fired.\nThree Confederate cavalrymen toppled from their horses, and the enemy advance briefly halted.\nAs “the bullets whistled round us loosely,” the 11th Mainers ran for the trees, Sabine said. Enemy officers “saw us there, and probably supposed we had support” from artillery and infantry and stopped the attack. “Instead of having support, we knew we were cut off from it.”\nRather than sacrifice their men to a possible night attack, Merrill and Sabine pulled back, formed “a new line behind a fence,” and waited, Sabine said. “It was rainy and dark, and a hard night.”\nAlthough the battle continued on Sunday, the fighting was over for the 11th Maine. Plaisted pulled together his sliced-and-diced regiment by late morning; a thorough nose count revealed that of the 229 men on the battle line about 12 noon Sunday, 79 were now casualties.\nThe regiment that had fought in four pieces had lost 12 men killed, 50 wounded, and 17 missing — but had lived to fight another day.\nNext week: Sliced and diced 11th Maine: Part III – reading between the lines\nBrian Swartz can be reached at firstname.lastname@example.org. He would love to hear from Civil War buffs interested in Maine’s involvement in the war.","This article is the second in a series on the spread of disease along the Silk Roads which examines the ways in which people have historically responded to illness and explores how we might approach newly arising challenges today. It uses the Silk Roads as an instructive example of the benefits of an interconnected world built on collaboration and timely and reliable knowledge sharing. This article details the spread of smallpox along the Silk Roads and the transmission of novel public health measures to combat it, including variolation and, later, vaccines.\nWherever people, animals and goods have moved and brought enriching effects, undesirable phenomena such as disease have also been transmitted on a broad scale. Historically, trade and movement have inevitably played a major role in the spread of infectious disease. In addition to diseases caused by bacteria, such as Plague, many viruses have been transmitted via movement along the Silk Roads. One notable example of a viral disease which has been prevalent throughout much of human history is smallpox. However, just as the disease itself travelled the Silk Roads, so too did a number of public health measures designed to combat it, including an early precursor to vaccinations, a practice known as “variolation”. Indeed, the first ever vaccines produced were used to protect people from catching smallpox, which, due to large scale international vaccination programmes in the 20th century, has since been successfully eradicated worldwide.\nSmallpox is an infectious disease caused by the “Variola” virus characterised by the formation of small sores all over the body. The disease spreads via contact with an infected person or from a contaminated item such as clothing or bedding. Although the exact origins of smallpox are unknown, there is evidence of the disease having been present in Ancient Egypt from as early as the 3rd century BCE. It appears that trade played an early role in spreading smallpox and there is speculation amongst historians that traders from Egypt might have transmitted the disease to the Indian Subcontinent sometime in the 1st millennium BCE. Some of the earliest written descriptions of smallpox date from 4th century CE China and, as trade along the Silk Roads increased in the 6th century CE, the disease spread rapidly to Japan and the Korean Peninsula. Notably, smallpox broke out between 735 – 737 CE in Japan, where it is believed to have killed up to one-third of the population.\nBy the 7th century CE, as trade and travel along the Silk Roads increased, smallpox became “endemic” (outbreaks regularly reoccurring within a given population) in the Indian Subcontinent. Muslim expansion during this time spread smallpox into Northern Africa, Spain and Portugal. In the 9th century CE, the Persian physician Razi, an early proponent of experimental medicine and chief physician of Baghdad and Rey hospitals in the Abbasid Caliphate, produced one of the most definitive descriptions of smallpox and the first account differentiating it from other similar diseases such as measles and chickenpox. By the 10th century smallpox had spread throughout Anatolia, with another wave of increased activity along the Silk Roads in the 13th century CE causing the disease to become endemic in previously unaffected areas such as Central and Northern Europe. In the 15th century, Portuguese expeditions to the West Coast of Africa and the establishment of new trade routes introduced the disease to further previously unaffected areas.\nDespite the fact that the movement of people and goods across vast distances has undoubtedly aided the spread of disease, the medical sciences have been one of the direct beneficiaries of the resulting intercultural exchanges. An excellent example of this is the development and transmission of “variolation”, a practice which was an early precursor to smallpox vaccination. There are early accounts of priests from the Indian Subcontinent travelling the Silk Roads popularising the practice of what they called “tika”, an early effort at inoculation (the introduction of a disease-causing agent in order to produce immunity to a specific disease). This involved taking matter from a smallpox patient’s sores and applying it to a small wound on an uninfected person, the idea being that the uninfected person would develop only a very mild case of the disease and, on recovery, become immune to catching a severe case in the future.\nThis practice may have developed independently in the Indian Subcontinent or, alternatively, practitioners might have learned it from Muslim physicians, who themselves came into contact with the practice via travel and trade with China. As early as the 1400s, medical healers in China had realized that those who survived smallpox did not catch the illness again and inferred that exposure to the illness protected a person from future instances of it. This observation gave rise to a second important public heath measure which was that those who had contracted the disease and survived were able to treat and care for new patients as they had incurred a natural immunity and were unlikely to become ill a second time. In order to transfer this immunity to new patients, Chinese doctors would grind smallpox scabs into a powder and insert it into a person’s nose with a long silver pipe. If only a very small amount of the virus was ingested that person would have a mild experience of the disease and be immunized for life. Similar practices, of “variolation”, were also documented in Africa in accounts from what is today Sudan. By the 16th century, this practice was a widespread public health measure enacted across many regions of the Silk Roads reaching as far west as Anatolia, having been introduced via descriptions from travellers and merchants.\nThroughout history, as we have developed better knowledge of how diseases are transmitted, how they can be treated, and the relevant public health measures that prevent their spread, a major trend for many endemic diseases has been the gradual reduction in their impact over time. In the case of a number of viral diseases, these measures have included the development of vaccinations, which, as in the practice of variolation, have an historic precedent in medicine transmitted along the Silk Roads. In the 18th century, the English physician Edward Jenner built on the idea of variolation and made a major contribution to the development of the modern smallpox vaccine. He observed that those who had contracted cowpox, a similar but milder viral infection, rarely went on to catch smallpox later in life. It is from the disease cowpox, known in Latin as variola vaccina, that we derive the term “vaccine”. Coordinated international vaccination programmes throughout the 20th century led to the eradication of smallpox in 1980, and today outbreaks of the disease no longer occur anywhere in the world. The eradication of smallpox is a testament to the development of the medical sciences over a long period of time, building on and sharing pre-existing medical knowledge and coordinating public health initiatives. A natural precursor to this vaccination dates back many hundreds of years with its origins in the many exchanges in the medical sciences taking place along the Silk Roads.\nThe Spread of Disease along the Silk Roads: Plague\nThis article explores the spread of plague, known as ‘the Black Death’, across the Silk Roads of the 14th Century CE. It examines ways in which people responded to the disease and looks at how we can respond to newly arising challenges today, utilizing the Silk Roads as an instructive example of the benefits of an interconnected world built on collaboration and timely and reliable knowledge sharing.\nThe Spread of Disease along the Silk Roads: The Development of Medical Botany and Pharmacology\nThis article is the third in a series on the spread of disease along the Silk Roads. Its outlines the early development of medical botany and pharmacology during the Middle Ages and identifies the role of the Silk Roads in helping fuel an incredible period of scholarship, particularly within the field of medicine, during the 8th and 9th centuries CE."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:e5b24a78-5edf-43bb-ab94-58a4f8c43264>","<urn:uuid:f8ffdf4b-360c-495c-a308-faed95336603>"],"error":null}
{"question":"How did Jehoshaphat's military campaigns against Moab differ in their outcomes when fighting alongside Jehoram versus facing the Moabites' confederacy?","answer":"When Jehoshaphat fought alongside Jehoram against Moab, they succeeded in subduing the Moabites, though Jehoshaphat withdrew in horror after witnessing Mesha's sacrifice of his own son on the walls of Kir-haresheth. In contrast, when facing a confederacy of Moabites and surrounding nations who came against him at Engedi, Jehoshaphat achieved victory through divine intervention - the invading forces quarreled among themselves and destroyed one another, leaving the people of Judah to collect the spoils of the slain.","context":["| Jehoshaphat, Valley of\nIn Bible versions:\nthe son and successor of king Asa of Judah; the father of Jehoram; an ancestor of Jesus\nson of Ahilud; a recorder for King Solomon\nan officer over collecting food supplies for King Solomon from Issachar; son of Paruah\nson of Asa; King of Judah\nson of Nimshi; father of King Jehu of Israel\na situation (\"valley\") of being judged (OS)\nthe Lord is judge ( --> same as Josaphat)\nJehoshaphat = \"whom Jehovah judges\"\n1) the king of Judah, son of Asa\n2498 Iosaphat ee-o-saf-at'\nof Hebrew origin (3092); Josaphat (i.e. Jehoshaphat), an\nsee HEBREW for 03092\nJehoshaphat = \"Jehovah has judged\"\nn pr m\n1) son of king Asa and himself king of Judah for 25 years; one of the\nbest, most pious, and prosperous kings of Judah\n2) son of Nimshi and father of king Jehu of the northern kingdom of Israel\n3) son of Ahilud and chronicler under David and Solomon\n4) son of Paruah and one of the 12 commissary officers under Solomon\n5) a priest and trumpeter in the time of David\nn pr loc\n6) symbolical name of a valley near Jerusalem which is the place of\nultimate judgment; maybe the deep ravine which separates Jerusalem\nfrom the Mount of Olives through which the Kidron flowed\n3092 Yhowshaphat yeh-ho-shaw-fawt'\nfrom 3068 and 8199; Jehovah-judged; Jehoshaphat, the name of\nsix Israelites; also of a valley near Jerusalem:-Jehoshaphat.\nsee HEBREW for 03068\nsee HEBREW for 08199\nsee HEBREW for 03146\nJehovah-judged. (1.) One of David's body-guard (1 Chr. 11:43).\n(2.) One of the priests who accompanied the removal of the ark to Jerusalem (1 Chr. 15:24).\n(3.) Son of Ahilud, \"recorder\" or annalist under David and Solomon (2 Sam. 8:16), a state officer of high rank, chancellor or vizier of the kingdom.\n(4.) Solomon's purveyor in Issachar (1 Kings 4:17).\n(5.) The son and successor of Asa, king of Judah. After fortifying his kingdom against Israel (2 Chr. 17:1, 2), he set himself to cleanse the land of idolatry (1 Kings 22:43). In the third year of his reign he sent out priests and Levites over the land to instruct the people in the law (2 Chr. 17:7-9). He enjoyed a great measure of peace and prosperity, the blessing of God resting on the people \"in their basket and their store.\"\nThe great mistake of his reign was his entering into an alliance with Ahab, the king of Israel, which involved him in much disgrace, and brought disaster on his kingdom (1 Kings 22:1-33). Escaping from the bloody battle of Ramoth-gilead, the prophet Jehu (2 Chr. 19:1-3) reproached him for the course he had been pursuing, whereupon he entered with rigour on his former course of opposition to all idolatry, and of deepening interest in the worship of God and in the righteous government of the people (2 Chr. 19:4-11).\nAgain he entered into an alliance with Ahaziah, the king of Israel, for the purpose of carrying on maritime commerce with Ophir. But the fleet that was then equipped at Ezion-gaber was speedily wrecked. A new fleet was fitted out without the co-operation of the king of Israel, and although it was successful, the trade was not prosecuted (2 Chr. 20:35-37; 1 Kings 22:48-49).\nHe subsequently joined Jehoram, king of Israel, in a war against the Moabites, who were under tribute to Israel. This war was successful. The Moabites were subdued; but the dreadful act of Mesha in offering his own son a sacrifice on the walls of Kir-haresheth in the sight of the armies of Israel filled him with horror, and he withdrew and returned to his own land (2 Kings 3:4-27).\nThe last most notable event of his reign was that recorded in 2 Chr. 20. The Moabites formed a great and powerful confederacy with the surrounding nations, and came against Jehoshaphat. The allied forces were encamped at Engedi. The king and his people were filled with alarm, and betook themselves to God in prayer. The king prayed in the court of the temple, \"O our God, wilt thou not judge them? for we have no might against this great company that cometh against us.\" Amid the silence that followed, the voice of Jahaziel the Levite was heard announcing that on the morrow all this great host would be overthrown. So it was, for they quarrelled among themselves, and slew one another, leaving to the people of Judah only to gather the rich spoils of the slain. This was recognized as a great deliverance wrought for them by God (B.C. 890). Soon after this Jehoshaphat died, after a reign of twenty-five years, being sixty years of age, and was succeeded by his son Jehoram (1 Kings 22:50). He had this testimony, that \"he sought the Lord with all his heart\" (2 Chr. 22:9). The kingdom of Judah was never more prosperous than under his reign.\n(6.) The son of Nimshi, and father of Jehu, king of Israel (2 Kings 9:2, 14).\n1. David's recorder, 2 Sam. 8:16\n; 1 Kin. 4:3\n; 1 Chr. 18:15\n2. One of Solomon's commissariat officers, 1 Kin. 4:17\n3. King of Judah. Succeeds Asa, 1 Kin. 15:24\n; 1 Chr. 3:10\n; 2 Chr. 17:1\n; Matt. 1:8\nStrengthens himself against Israel, 2 Chr. 17:2\nInaugurates a system of public instruction in the law, 2 Chr. 17:7-9\nHis wise reign, 1 Kin. 22:43\n; 2 Chr. 17:7-9\nHis system of tribute, 2 Chr. 17:11\nHis military forces and armament, 2 Chr. 17:12-19\nJoins Ahab in an invasion of Ramoth-gilead, 1 Kin. 22\n; 2 Chr. 18\nRebuked by the prophet Jehu, 2 Chr. 19:2\nThe allied forces of the Amorites, Moabites, and other tribes invade his territory, and are defeated by, 2 Chr. 20\nBuilds ships for commerce with Tarshish, ships are destroyed, 1 Kin. 22:48\n; 2 Chr. 20:35-37\nJoins Jehoram, king of Israel, in an invasion of the land of Moab, defeats the Moabites, 2 Kin. 3\nMakes valuable gifts to the temple, 2 Kin. 12:18\nDeath of, 1 Kin. 22:50\n; 2 Chr. 21:1\nReligious zeal of, 1 Kin. 22:43\n; 2 Chr. 17:1-9\nProsperity of, 1 Kin. 22:45\n; 2 Chr. 17-20\nlegacy of, to his children, 2 Chr. 21:2\n4. Father of Jehu, 2 Kin. 9:2\n5. A priest who assisted in bringing the ark from Obed-edom, 1 Chr. 15:24\n6. A valley. Called Valley of Decision, Joel 3:2\n(whom Jehovah judges.\n- King of Judah, son of Asa, succeeded to the throne B.C. 914, when he was 35 years old, and reigned 25 years. His history is to be found among the events recorded in (1Ã‚Â Kings 15:24; 2Ã‚Â Kings 8:16) or in a continuous narrative in (2Ã‚Â Chronicles 17:1; 2Ã‚Â Chronicles 21:3) He was contemporary with Ahab, Ahaziah and Jehoram. He was one of the best, most pious and prosperous kings of Judah, the greatest since Solomon. At first he strengthened himself against Israel; but soon afterward the two Hebrew kings formed an alliance. In his own kingdom Jehoshaphat ever showed himself a zealous follower of the commandments of God: he tried to put down the high places and groves in which the people of Judah burnt incense, and sent the wisest Levites through the cities and towns to instruct the people in true morality and religion. Riches and honors increased around him. He received tribute from the Philistines and Arabians, and kept up a large standing army in Jerusalem. It was probably about the 16th year of his reign, B.C. 898, when he became Ahab?s ally in the great battle of Ramoth-gilead, for which he was severely reproved by Jehu. (2Ã‚Â Chronicles 19:2) He built at Ezion-geber, with the help of Ahaziah, a navy designed to go to Tarshish; but it was wrecked at Ezion-geber. Before the close of his reign he was engaged in two additional wars. He was miraculously delivered from a threatened attack of the people of Ammon, Moab and Seir. After this, perhaps, must be dated the war which Jehoshaphat, in conjunction with Jehoram king of Israel and the king of Edom, carried on against the rebellious king of Moab. (2Ã‚Â Kings 3:1) ... In his declining years the administration of affairs was placed, probably B.C. 891, in the hands of his son Jehoram.\n- Son of Ahilud, who filled the office of recorder of annalist in the courts of David, (2Ã‚Â Samuel 8:16) etc., and Solomon. (1Ã‚Â Kings 4:3)\n- One of the priests in David?s time. (1Ã‚Â Chronicles 15:24)\n- Son of Paruah; one of the twelve purveyors of King Solomon. (1Ã‚Â Kings 4:17)\n- Son of Nimshi and father of King Jehu. (2Ã‚Â Kings 9:2,14)\nJEHOSHAPHAT (1) [ISBE]\n- je-hosh'-a-fat (yehoshaphaT, \"Yahweh has judged\"):\n(1) King of Judah. See separate article.\n(2) Son of Ahilud. He was recorder under David (2 Sam 8:16; 20:24; 1 Ch 18:15) and Solomon (1 Ki 4:3).\n(3) Son of Paruah, and Solomon's overseer in Issachar to provide victuals for the royal household for one month of the year (1 Ki 4:17).\n(4) Son of Nimshi, and father of Jehu, king of Northern Israel (2 Ki 9:2,14). His name is omitted in 9:20 and 1 Ki 19:16, where Jehu is called \"son of Nimshi.\"\n(5) the King James Version (but not Hebrew) in 1 Ch 15:24; the Revised Version (British and American) correctly JOSHAPHAT (which see).\nDavid Francis Roberts\nJEHOSHAPHAT (2) [ISBE]\n- je-hosh'-a-fat (yehoshaphaT, \"Yahweh judges\"): The 4th king of Judah, son of Asa. His mother was Azubah, the daughter of Shilhi, of whom nothing further is known. He was 35 years of age at his accession, and reigned 25 years, circa 873-849 BC. Th e history of his reign is contained in 1 Ki 22:41-50\nand in 2 Ch 17:1\nthrough 21:1. The narrative in 1 Ki 22:1-35a\nand in 2 Ki 3:4\nff belongs to the history of the Northern Kingdom. The absence from Ki of the details contained in 2 Chronicles affords no presumpt against their truth. Neither do high numbers, embellished statements, and the coloring of the writer's own age destroy the historical perspective.\n1. His Religious Policy:\nThe reign of Jehoshaphat appears to have been one of unusual religious activity. It was, however, characterized not so much by striking religious measures as it was by the religious spirit that pervaded every act of the king, who sought the favor of Yahweh in every detail of his life (2 Ch 17:3,4). He evidently felt that a nation's character is determined by its religion. Accordingly, he made it his duty to purify the national worship. The \"sodomites,\" i.e. those who practiced immorality in the worsh ip of Yahweh in the temple precincts, were banished from the land (1 Ki 22:46). The Asherim were taken out of Judah (2 Ch 17:6; 19:3), and \"the people from Beer-sheba to the hill-country of Ephraim were brought back unto Yahweh, the God of their fathers\" (2 Ch 19:4). Because of his zeal for Yahweh, Jehoshaphat is rewarded with power and \"riches and honor in abundance\" (2 Ch 17:5).\n2. His System of Public Instruction:\nBelieving that religion and morals, the civilization, suffer from ignorance, Jehoshaphat introduced a system of public instruction for the whole land (2 Ch 17:7 ff). He appointed a commission, composed of princes, Levites and priests, to go from city to city to instruct the people. Their instruction was to be based on the one true foundation of sound morals and healthy religious life, \"the book of the law of Yahweh\" (2 Ch 17:7-9).\n3. His Judicial Institutions:\nNext in importance to Jehoshaphat's system of public instruction, was his provision for the better administration of justice. He appointed judges to preside over courts of common pleas, which he established in all the fortified cities of Judah. In addition to these local courts, two courts of appeal, an ecclesiastical and a civil court, were established at Jerusalem to be presided over by priests, Levites, and leading nobles as judges. At the head of the ecclesiastical court of appeal was the high priest, and a layman, \"the ruler of the house of Judah,\" headed the civil court of appeal (2 Ch 19:4-11). The insistence that a judge was to be in character like Yahweh, with whom there is \"no iniquity .... nor respect of persons, nor taking of bribes\" (2 Ch 19:7), is worthy of note.\n4. His Military Defenses:\nAccording to 2 Ch 17:2, Jehoshaphat began his reign with defensive measures against Israel. Furthermore, he built castles and cities of store in the land of Judah, \"and he had many works,\" probably military supplies, \"in the cities of Judah\" (17:13). He appears to have had a large standing army, including cavalry (1 Ki 22:4; 2 Ch 17:14 ff). However, the numbers in 2 Ch 17:14 ff seem to be impossibly high.\n5. His Foreign Policy:\nGodliness and security at home were followed by respect and peace abroad. The fact that the Philistines and the Arabians brought tribute (2 Ch 17:11), and that Edom had no king (1 Ki 22:47), but a deputy instead, who possibly was appointed by Jehoshaphat, would indicate that he held the suzerainty over the nations and tribes bordering Judah on the South and West Holding the suzerainty over the weaker nations, and being allied with the stronger, Jehoshaphat secured the peace for the greater part of his reign (1 Ch 17:10) that fostered the internal development of the kingdom.\n6. His Alliance with Ahab:\nIn contrast to the former kings of Judah, Jehoshaphat saw greater benefit in an alliance with Israel than in civil war. Accordingly, the old feud between the two kingdoms (1 Ki 14:30; 15:6) was dropped, and Jehoshaphat made peace with Israel (1 Ki 22:44). The political union was cemented by the marriage of Jehoram, son of Jehoshaphat, to Athaliah, daughter of Ahab and Jezebel. Shortly after the marriage, Jehoshaphat joined Ahab in a campaign against Syria (2 Ch 18:1-3). In view of the subordinate position that Jehoshaphat seems to take in the campaign (1 Ki 22:4,30), and in view of the military service rendered to Jehoram (2 Ki 3:4 ff), Judah seems to have become a dependency of Israel. Nevertheless, the union may have contributed to the welfare and prospity of Judah, and it may have enabled Jehoshaphat to hold the suzerainty over the neighboring nations. However, the final outcome of the alliance with the house of Omri was disastrous for Judah. The introduction into Judah of Baalism more than counterbalanced any political and material advantage gained, and in the succeeding reigns it indirectly led to the almost total extinction of the royal family of Judah (2 Ki 11:1 ff).\n7. His Alliance with Jehoram:\nIn spite of the denunciation of the prophet Jehu for his expedition with Ahab, thus \"help(ing) the wicked\" (2 Ch 19:2), Jehoshaphat entered into a similar alliance with Jehoram of Israel (2 Ki 3:4 ff). On the invitation of Jehoram to join him in an expedition against Moab, Jehoshaphat was ready with the same set speech of acceptance as in the case of Ahab (2 Ki 3:7; compare 1 Ki 22:4). For the details of the expedition see JEHORAM, (1).\n8. Victory over the Moabites and Ammonites:\nThe Chronicler has given us a very remarkable account of a victory gained by Jehoshaphat over the Moabites and Ammonites. No doubt he made use of a current historical Midrash. Many find the historical basis of the Midrash in the events recorded in 2 Ki 3:4 ff. However, the localities are different, and there a defeat is recorded, while in this case we have a victory. The story in outline bears the stamp of probability. 1 Ki 22:45 seems to suggest wars of Jehoshaphat that are not mentioned in Kings. The tribes mentioned in the account are represented as trying to make permanent settlement in Judah (2 Ch 20:11). In their advance through the South of Judah, they were doubtless harassed by the shepherd population of the country. Jehoshaphat, according to his custom, sought the help of Yahweh. The invading forces fell to quarreling among themselves (2 Ch 20:23), and destroyed one another. The spoil was great because the invaders had brought all their goods with them, expecting to remain in the land.\n9. Destruction of Jehoshaphat's Fleet:\nThe destruction of Jehoshaphat's fleet is recorded in 1 Ki 22:48,49 and in 2 Ch 20:35-37. However, the two accounts are quite different. According to Kings, Jehoshaphat built ships of Tarshish to sail to Ophir for gold, but the vessels were wrecked at zion-geber. Thereupon Ahaziah offered to assist Jehoshaphat with seamen, but Jehoshaphat refused to enter into the alliance. According to Chronicles the alliance had been formed, and together they built ships at Ezion-geber, which were destroyed because Jehoshaphat had made an alliance with the wicked king of Israel. In view of Jehoshaphat's other alliances, the Chronicler may be in the right. Chronicles, however, misunderstood the term \"ships of Tarshish.\"\n10. His Death:\nJehoshaphat died at the age of 60. Josephus says (Ant., IX, iii, 2) that he was buried in a magnificent manner, for he had imitated the actions of David. The kingdom was left to Jehoram, who inaugurated the beginning of his reign by causing the massacre of his brethren.\nS. K. Mosiman","| Jehoshaphat, Valley of\nIn Bible versions:\nthe son and successor of king Asa of Judah; the father of Jehoram; an ancestor of Jesus\nson of Ahilud; a recorder for King Solomon\nan officer over collecting food supplies for King Solomon from Issachar; son of Paruah\nson of Asa; King of Judah\nson of Nimshi; father of King Jehu of Israel\na situation (\"valley\") of being judged (OS)\nthe Lord is judge ( --> same as Josaphat)\nJehoshaphat = \"whom Jehovah judges\"\n1) the king of Judah, son of Asa\n2498 Iosaphat ee-o-saf-at'\nof Hebrew origin (3092); Josaphat (i.e. Jehoshaphat), an\nsee HEBREW for 03092\nJehoshaphat = \"Jehovah has judged\"\nn pr m\n1) son of king Asa and himself king of Judah for 25 years; one of the\nbest, most pious, and prosperous kings of Judah\n2) son of Nimshi and father of king Jehu of the northern kingdom of Israel\n3) son of Ahilud and chronicler under David and Solomon\n4) son of Paruah and one of the 12 commissary officers under Solomon\n5) a priest and trumpeter in the time of David\nn pr loc\n6) symbolical name of a valley near Jerusalem which is the place of\nultimate judgment; maybe the deep ravine which separates Jerusalem\nfrom the Mount of Olives through which the Kidron flowed\n3092 Yhowshaphat yeh-ho-shaw-fawt'\nfrom 3068 and 8199; Jehovah-judged; Jehoshaphat, the name of\nsix Israelites; also of a valley near Jerusalem:-Jehoshaphat.\nsee HEBREW for 03068\nsee HEBREW for 08199\nsee HEBREW for 03146\nJehovah-judged. (1.) One of David's body-guard (1 Chr. 11:43).\n(2.) One of the priests who accompanied the removal of the ark to Jerusalem (1 Chr. 15:24).\n(3.) Son of Ahilud, \"recorder\" or annalist under David and Solomon (2 Sam. 8:16), a state officer of high rank, chancellor or vizier of the kingdom.\n(4.) Solomon's purveyor in Issachar (1 Kings 4:17).\n(5.) The son and successor of Asa, king of Judah. After fortifying his kingdom against Israel (2 Chr. 17:1, 2), he set himself to cleanse the land of idolatry (1 Kings 22:43). In the third year of his reign he sent out priests and Levites over the land to instruct the people in the law (2 Chr. 17:7-9). He enjoyed a great measure of peace and prosperity, the blessing of God resting on the people \"in their basket and their store.\"\nThe great mistake of his reign was his entering into an alliance with Ahab, the king of Israel, which involved him in much disgrace, and brought disaster on his kingdom (1 Kings 22:1-33). Escaping from the bloody battle of Ramoth-gilead, the prophet Jehu (2 Chr. 19:1-3) reproached him for the course he had been pursuing, whereupon he entered with rigour on his former course of opposition to all idolatry, and of deepening interest in the worship of God and in the righteous government of the people (2 Chr. 19:4-11).\nAgain he entered into an alliance with Ahaziah, the king of Israel, for the purpose of carrying on maritime commerce with Ophir. But the fleet that was then equipped at Ezion-gaber was speedily wrecked. A new fleet was fitted out without the co-operation of the king of Israel, and although it was successful, the trade was not prosecuted (2 Chr. 20:35-37; 1 Kings 22:48-49).\nHe subsequently joined Jehoram, king of Israel, in a war against the Moabites, who were under tribute to Israel. This war was successful. The Moabites were subdued; but the dreadful act of Mesha in offering his own son a sacrifice on the walls of Kir-haresheth in the sight of the armies of Israel filled him with horror, and he withdrew and returned to his own land (2 Kings 3:4-27).\nThe last most notable event of his reign was that recorded in 2 Chr. 20. The Moabites formed a great and powerful confederacy with the surrounding nations, and came against Jehoshaphat. The allied forces were encamped at Engedi. The king and his people were filled with alarm, and betook themselves to God in prayer. The king prayed in the court of the temple, \"O our God, wilt thou not judge them? for we have no might against this great company that cometh against us.\" Amid the silence that followed, the voice of Jahaziel the Levite was heard announcing that on the morrow all this great host would be overthrown. So it was, for they quarrelled among themselves, and slew one another, leaving to the people of Judah only to gather the rich spoils of the slain. This was recognized as a great deliverance wrought for them by God (B.C. 890). Soon after this Jehoshaphat died, after a reign of twenty-five years, being sixty years of age, and was succeeded by his son Jehoram (1 Kings 22:50). He had this testimony, that \"he sought the Lord with all his heart\" (2 Chr. 22:9). The kingdom of Judah was never more prosperous than under his reign.\n(6.) The son of Nimshi, and father of Jehu, king of Israel (2 Kings 9:2, 14).\n1. David's recorder, 2 Sam. 8:16\n; 1 Kin. 4:3\n; 1 Chr. 18:15\n2. One of Solomon's commissariat officers, 1 Kin. 4:17\n3. King of Judah. Succeeds Asa, 1 Kin. 15:24\n; 1 Chr. 3:10\n; 2 Chr. 17:1\n; Matt. 1:8\nStrengthens himself against Israel, 2 Chr. 17:2\nInaugurates a system of public instruction in the law, 2 Chr. 17:7-9\nHis wise reign, 1 Kin. 22:43\n; 2 Chr. 17:7-9\nHis system of tribute, 2 Chr. 17:11\nHis military forces and armament, 2 Chr. 17:12-19\nJoins Ahab in an invasion of Ramoth-gilead, 1 Kin. 22\n; 2 Chr. 18\nRebuked by the prophet Jehu, 2 Chr. 19:2\nThe allied forces of the Amorites, Moabites, and other tribes invade his territory, and are defeated by, 2 Chr. 20\nBuilds ships for commerce with Tarshish, ships are destroyed, 1 Kin. 22:48\n; 2 Chr. 20:35-37\nJoins Jehoram, king of Israel, in an invasion of the land of Moab, defeats the Moabites, 2 Kin. 3\nMakes valuable gifts to the temple, 2 Kin. 12:18\nDeath of, 1 Kin. 22:50\n; 2 Chr. 21:1\nReligious zeal of, 1 Kin. 22:43\n; 2 Chr. 17:1-9\nProsperity of, 1 Kin. 22:45\n; 2 Chr. 17-20\nlegacy of, to his children, 2 Chr. 21:2\n4. Father of Jehu, 2 Kin. 9:2\n5. A priest who assisted in bringing the ark from Obed-edom, 1 Chr. 15:24\n6. A valley. Called Valley of Decision, Joel 3:2\n(whom Jehovah judges.\n- King of Judah, son of Asa, succeeded to the throne B.C. 914, when he was 35 years old, and reigned 25 years. His history is to be found among the events recorded in (1Ã‚Â Kings 15:24; 2Ã‚Â Kings 8:16) or in a continuous narrative in (2Ã‚Â Chronicles 17:1; 2Ã‚Â Chronicles 21:3) He was contemporary with Ahab, Ahaziah and Jehoram. He was one of the best, most pious and prosperous kings of Judah, the greatest since Solomon. At first he strengthened himself against Israel; but soon afterward the two Hebrew kings formed an alliance. In his own kingdom Jehoshaphat ever showed himself a zealous follower of the commandments of God: he tried to put down the high places and groves in which the people of Judah burnt incense, and sent the wisest Levites through the cities and towns to instruct the people in true morality and religion. Riches and honors increased around him. He received tribute from the Philistines and Arabians, and kept up a large standing army in Jerusalem. It was probably about the 16th year of his reign, B.C. 898, when he became Ahab?s ally in the great battle of Ramoth-gilead, for which he was severely reproved by Jehu. (2Ã‚Â Chronicles 19:2) He built at Ezion-geber, with the help of Ahaziah, a navy designed to go to Tarshish; but it was wrecked at Ezion-geber. Before the close of his reign he was engaged in two additional wars. He was miraculously delivered from a threatened attack of the people of Ammon, Moab and Seir. After this, perhaps, must be dated the war which Jehoshaphat, in conjunction with Jehoram king of Israel and the king of Edom, carried on against the rebellious king of Moab. (2Ã‚Â Kings 3:1) ... In his declining years the administration of affairs was placed, probably B.C. 891, in the hands of his son Jehoram.\n- Son of Ahilud, who filled the office of recorder of annalist in the courts of David, (2Ã‚Â Samuel 8:16) etc., and Solomon. (1Ã‚Â Kings 4:3)\n- One of the priests in David?s time. (1Ã‚Â Chronicles 15:24)\n- Son of Paruah; one of the twelve purveyors of King Solomon. (1Ã‚Â Kings 4:17)\n- Son of Nimshi and father of King Jehu. (2Ã‚Â Kings 9:2,14)\nJEHOSHAPHAT (1) [ISBE]\n- je-hosh'-a-fat (yehoshaphaT, \"Yahweh has judged\"):\n(1) King of Judah. See separate article.\n(2) Son of Ahilud. He was recorder under David (2 Sam 8:16; 20:24; 1 Ch 18:15) and Solomon (1 Ki 4:3).\n(3) Son of Paruah, and Solomon's overseer in Issachar to provide victuals for the royal household for one month of the year (1 Ki 4:17).\n(4) Son of Nimshi, and father of Jehu, king of Northern Israel (2 Ki 9:2,14). His name is omitted in 9:20 and 1 Ki 19:16, where Jehu is called \"son of Nimshi.\"\n(5) the King James Version (but not Hebrew) in 1 Ch 15:24; the Revised Version (British and American) correctly JOSHAPHAT (which see).\nDavid Francis Roberts\nJEHOSHAPHAT (2) [ISBE]\n- je-hosh'-a-fat (yehoshaphaT, \"Yahweh judges\"): The 4th king of Judah, son of Asa. His mother was Azubah, the daughter of Shilhi, of whom nothing further is known. He was 35 years of age at his accession, and reigned 25 years, circa 873-849 BC. Th e history of his reign is contained in 1 Ki 22:41-50\nand in 2 Ch 17:1\nthrough 21:1. The narrative in 1 Ki 22:1-35a\nand in 2 Ki 3:4\nff belongs to the history of the Northern Kingdom. The absence from Ki of the details contained in 2 Chronicles affords no presumpt against their truth. Neither do high numbers, embellished statements, and the coloring of the writer's own age destroy the historical perspective.\n1. His Religious Policy:\nThe reign of Jehoshaphat appears to have been one of unusual religious activity. It was, however, characterized not so much by striking religious measures as it was by the religious spirit that pervaded every act of the king, who sought the favor of Yahweh in every detail of his life (2 Ch 17:3,4). He evidently felt that a nation's character is determined by its religion. Accordingly, he made it his duty to purify the national worship. The \"sodomites,\" i.e. those who practiced immorality in the worsh ip of Yahweh in the temple precincts, were banished from the land (1 Ki 22:46). The Asherim were taken out of Judah (2 Ch 17:6; 19:3), and \"the people from Beer-sheba to the hill-country of Ephraim were brought back unto Yahweh, the God of their fathers\" (2 Ch 19:4). Because of his zeal for Yahweh, Jehoshaphat is rewarded with power and \"riches and honor in abundance\" (2 Ch 17:5).\n2. His System of Public Instruction:\nBelieving that religion and morals, the civilization, suffer from ignorance, Jehoshaphat introduced a system of public instruction for the whole land (2 Ch 17:7 ff). He appointed a commission, composed of princes, Levites and priests, to go from city to city to instruct the people. Their instruction was to be based on the one true foundation of sound morals and healthy religious life, \"the book of the law of Yahweh\" (2 Ch 17:7-9).\n3. His Judicial Institutions:\nNext in importance to Jehoshaphat's system of public instruction, was his provision for the better administration of justice. He appointed judges to preside over courts of common pleas, which he established in all the fortified cities of Judah. In addition to these local courts, two courts of appeal, an ecclesiastical and a civil court, were established at Jerusalem to be presided over by priests, Levites, and leading nobles as judges. At the head of the ecclesiastical court of appeal was the high priest, and a layman, \"the ruler of the house of Judah,\" headed the civil court of appeal (2 Ch 19:4-11). The insistence that a judge was to be in character like Yahweh, with whom there is \"no iniquity .... nor respect of persons, nor taking of bribes\" (2 Ch 19:7), is worthy of note.\n4. His Military Defenses:\nAccording to 2 Ch 17:2, Jehoshaphat began his reign with defensive measures against Israel. Furthermore, he built castles and cities of store in the land of Judah, \"and he had many works,\" probably military supplies, \"in the cities of Judah\" (17:13). He appears to have had a large standing army, including cavalry (1 Ki 22:4; 2 Ch 17:14 ff). However, the numbers in 2 Ch 17:14 ff seem to be impossibly high.\n5. His Foreign Policy:\nGodliness and security at home were followed by respect and peace abroad. The fact that the Philistines and the Arabians brought tribute (2 Ch 17:11), and that Edom had no king (1 Ki 22:47), but a deputy instead, who possibly was appointed by Jehoshaphat, would indicate that he held the suzerainty over the nations and tribes bordering Judah on the South and West Holding the suzerainty over the weaker nations, and being allied with the stronger, Jehoshaphat secured the peace for the greater part of his reign (1 Ch 17:10) that fostered the internal development of the kingdom.\n6. His Alliance with Ahab:\nIn contrast to the former kings of Judah, Jehoshaphat saw greater benefit in an alliance with Israel than in civil war. Accordingly, the old feud between the two kingdoms (1 Ki 14:30; 15:6) was dropped, and Jehoshaphat made peace with Israel (1 Ki 22:44). The political union was cemented by the marriage of Jehoram, son of Jehoshaphat, to Athaliah, daughter of Ahab and Jezebel. Shortly after the marriage, Jehoshaphat joined Ahab in a campaign against Syria (2 Ch 18:1-3). In view of the subordinate position that Jehoshaphat seems to take in the campaign (1 Ki 22:4,30), and in view of the military service rendered to Jehoram (2 Ki 3:4 ff), Judah seems to have become a dependency of Israel. Nevertheless, the union may have contributed to the welfare and prospity of Judah, and it may have enabled Jehoshaphat to hold the suzerainty over the neighboring nations. However, the final outcome of the alliance with the house of Omri was disastrous for Judah. The introduction into Judah of Baalism more than counterbalanced any political and material advantage gained, and in the succeeding reigns it indirectly led to the almost total extinction of the royal family of Judah (2 Ki 11:1 ff).\n7. His Alliance with Jehoram:\nIn spite of the denunciation of the prophet Jehu for his expedition with Ahab, thus \"help(ing) the wicked\" (2 Ch 19:2), Jehoshaphat entered into a similar alliance with Jehoram of Israel (2 Ki 3:4 ff). On the invitation of Jehoram to join him in an expedition against Moab, Jehoshaphat was ready with the same set speech of acceptance as in the case of Ahab (2 Ki 3:7; compare 1 Ki 22:4). For the details of the expedition see JEHORAM, (1).\n8. Victory over the Moabites and Ammonites:\nThe Chronicler has given us a very remarkable account of a victory gained by Jehoshaphat over the Moabites and Ammonites. No doubt he made use of a current historical Midrash. Many find the historical basis of the Midrash in the events recorded in 2 Ki 3:4 ff. However, the localities are different, and there a defeat is recorded, while in this case we have a victory. The story in outline bears the stamp of probability. 1 Ki 22:45 seems to suggest wars of Jehoshaphat that are not mentioned in Kings. The tribes mentioned in the account are represented as trying to make permanent settlement in Judah (2 Ch 20:11). In their advance through the South of Judah, they were doubtless harassed by the shepherd population of the country. Jehoshaphat, according to his custom, sought the help of Yahweh. The invading forces fell to quarreling among themselves (2 Ch 20:23), and destroyed one another. The spoil was great because the invaders had brought all their goods with them, expecting to remain in the land.\n9. Destruction of Jehoshaphat's Fleet:\nThe destruction of Jehoshaphat's fleet is recorded in 1 Ki 22:48,49 and in 2 Ch 20:35-37. However, the two accounts are quite different. According to Kings, Jehoshaphat built ships of Tarshish to sail to Ophir for gold, but the vessels were wrecked at zion-geber. Thereupon Ahaziah offered to assist Jehoshaphat with seamen, but Jehoshaphat refused to enter into the alliance. According to Chronicles the alliance had been formed, and together they built ships at Ezion-geber, which were destroyed because Jehoshaphat had made an alliance with the wicked king of Israel. In view of Jehoshaphat's other alliances, the Chronicler may be in the right. Chronicles, however, misunderstood the term \"ships of Tarshish.\"\n10. His Death:\nJehoshaphat died at the age of 60. Josephus says (Ant., IX, iii, 2) that he was buried in a magnificent manner, for he had imitated the actions of David. The kingdom was left to Jehoram, who inaugurated the beginning of his reign by causing the massacre of his brethren.\nS. K. Mosiman"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:101e0a49-8595-42d9-b64f-e8a6fc16056a>","<urn:uuid:101e0a49-8595-42d9-b64f-e8a6fc16056a>"],"error":null}
{"question":"How is a hyper-realistic giant boy sculpture created from start to finish?","answer":"A hyper-realistic giant boy sculpture is created through multiple steps: First, a small clay study is made, followed by a plaster maquette that is sliced into horizontal sections. These sections are used as templates and scaled up onto huge polystyrene blocks using hot wires. The blocks are assembled and refined with knives and wire brushes. Then, a coat of plastolene (synthetic wax) is melted, painted on, and smoothed. A mold is created in sections, starting with a layer of silicone for detail, supported by resin and fiberglass layers. Polyester resin in flesh tones is mixed and painted inside each section, including skin variations. After assembly and smoothing seams, details like rosy highlights and bluish veins are added. Hair is constructed with acrylic fiber strands, while eyebrows and eyelashes are individually sanded. Finally, eyes are cast in polyester resin using individual molds.","context":["Presentation on theme: \"Realistic Representation Ron Mueck. 2 Enduring Understanding Students will understand that realistic representation is selected with purpose and function.\"— Presentation transcript:\nRealistic Representation Ron Mueck\n2 Enduring Understanding Students will understand that realistic representation is selected with purpose and function to express ideas and concepts\n3 Essential Questions Overarching Questions How does realistic representation contribute to the ideas and purpose of artists? What are true reflections of life? How is visual art a mechanism for social change? Topical Questions Is reproducing from life art? Can reflections of life be distorted? How?\n4 5W1H When What Where UK How Sculptures Why 'I never made life-size figures because it never seemed to be interesting. We meet life-size people every day.' Ron Mueck Which Photorealism Also Hyper-Realism\n5 Biographical Outline 1958: Born in Melbourne, Australia to German parents. Worked as a model maker and puppeteer for a television and film productions. 1980s: Moved to UK from Australia. 1996:Dad died in Australia while he is in London.\n6 When (1958- ) Where (Australia & UK) When In the late 1930s, acrylic and fiberglass were invented. Where Charles Saatchi was the co-founder of the global advertising agency Saatchi & Saatchi. Charles is an avid art collector and owner of the Saatchi Gallery in London for contemporary art. He is also the sponsor for the YBAs (Young Bristish Artists) like Damien Hirst.\n7 Which Hyperrealism/Photorealism A genre of painting and sculpture that look photographic. Hyperrealism as a movement, it is a splinter derivation from photorealism. Photorealism is a realistic painting approach that includes the reproduction of details. As a result, the painting looks almost photographic. Photographs are usually used as a reference. Some other artists- Chuck Close, Duane Hanson and Richard Estes.\n8 What Subject Matter – Figures He explores the perception of space the body occupies by playing with the size and postures of his sculptures. The size of the figures are usually distorted for dramatic effects- eg: how an unusually gigantic pregnant woman with her colossal tummy at viewers eye level plays up the importance of life and birth. They are usually over-sized or under-sized, never life-size. This is because life-size figures do not interest him as we see them everyday around us. His figures are fashioned to the point of super-realism with meticulous details such as moles, veins, wrinkles, etc, all accurately rendered. They are flawlessly perfect- inviting close-up scrutiny with disbelief.\n9 What Subject Matter- Figures Some critics deem his works like those of mannequins or wax figures but Mueck contends by employing dramatic distortions of size and awkward postures with the intention to highlight emotional states to his subjects. Such distortions can also endow his subjects with psychological intent- eg: Boy, His subjects are based on his friends and relatives.\n10 Dead Dad, Silicone and acrylic, 20 x 38 x 102 cm The Saatchi Gallery His Under-Sized Figures This is the sculpture that propelled Mueck to fame.\n11 What- Dead Dad A naked corpse of an old man lying flat on his back. It is a rendition of Mueck s own deceased father. It is made from the artist s memory, and half the size of a life-size figure. The size is intended for the viewer to cradle the corpse visually (Verdier, 2006). The impact- seemingly real and yet unreal. It adheres to the anatomical detail.\n12 Mask, Mixed Media, 158 x 153 x 124 cm His Over-Sized Figures\n13 Boy, Mixed media, 490 x 490 x 240 cm His Over-Sized Figures\n14 Boy, Mixed media, 490 x 490 x 240 cm His Over-Sized Figures\n15 How- Boy He begins work with a small clay study, and makes a plaster maquette from it. The maquette is then sliced into horizontal sections. The sections are used as templates and scaled up onto huge polystyrene blocks with hot wires. These giant slices are piled back to form the boy. The artist and his team refine it with knives and wire brushes.\n16 How- Boy The polystyrene body is then given a coat of plastolene (a sticky synthetic wax). This plastolene needs to be melted and painted on and smoothed with long, flexible blades, before it can perform with the details and texture of the skin. Finally, he begins to create a mould with the figure in sections off the surface, building a patchwork around the figure.\n17 How- Boy A layer of silicone is painted first to pick up the detail of the surface. This is supported with more layers of resin and fiberglass. He then mixes polyester resin in flesh tones and painted inside each sections. He ensures to include the variations which are visible on the skin- eg: mottled (spotted or patched) skin, pinker knees and elbows, paler nails. The sections were then released from the moulds and reassembled into the boy with seams sanded smooth.\n18 How- Boy The sculpture is then touched up with other details like rosy highlights and faint bluish veins. The hair is constructed with thick strands of acrylic fiber, fixed to the head with woven strips. The eyebrows and eyelashes are individually sanded into a tapered end. Individual moulds are created for the eyes before casting them with polyester resin.\n19 Ghost, 1998 Fibreglass, silicon, polyurethane foam, acrylic fibre and fabric, x 64.8 x 99.1 cm Tate Gallery, London. His Over-Sized Figures Her large scale and uneasiness highlights a sense of teenage anxiety.\n20 Big Man, Pigmented polyester on resin, x x cm. Hirshhorn Museum & Sculpture Garden His Over-Sized Figures\n21 Big Man, Pigmented polyester on resin, x x cm. His Over-Sized Figures\n22 How- Big Man Big Man, Pigmented polyester on resin, x x cm.\n23 Mask II, Mixed Media, His Over-Sized Figures\n24 His Over-Sized Figures Pregnant Woman, Fibreglass, resin and silicone, National Gallery of Australia Check it out at\n25 Pregnant Woman, Fibreglass, resin and silicone, National Gallery of Australia His Over-Sized Figures\n26 What- Pregnant Woman It is a portrayal of motherhood- boasting strong reference with fertility, life and birth. Her size illustrates the immense significance of her pregnancy as well as her vulnerability and emotional intensity as seen in her face. The colossal tummy and expression on her face communicates to the viewers the immense weight (can also be interpreted as responsibility) the woman bears. As a viewer confronted with the tummy, the physicality and burden of child-bearing becomes even more pertinent. Her size can also be allegorical of omnificent (magnificent) Mother Earth.\n27 Untitled (Head of a Baby), Mixed Media, His Over-Sized Figures\n28 Mask III, Mixed Media, His Over-Sized Figures\n29 In Bed, Mixed media, x x 395 cm His Over-Sized Figures\n30 In Bed, Mixed media, x x 395 cm His Over-Sized Figures\n31 A Girl, Oil-based ink on canvas, 259 x cm His Over-Sized Figures\n32 A Girl, Mixed media, His Over-Sized Figures\n33 Angel, Silicone rubber and mixed media, 110 x 87 x 81 cm His Under-Sized Figures\n34 What- Angel The naked figure of a man with a pair of wings which are made with goose feathers. He is pensive and the pose appears a little melancholic. It s source of inspiration came from Tiepolo s Allegory with Venus and Time from the National Gallery. Mueck was inspired to create his own winged character.\n35 Why- Angel (His Influence) Allegory with Venus and Time, c by Giovanni Battista Tiepolo Oil on canvas, 292 x 190 cm. Giovanni Battista Tiepolo ( ) Born in Venice Italy. He was both a painter and a printmaker. He was Europes outstanding master of the Grand Manner. His art- imaginative and changing the world of ancient history and myth, scriptures and legends into grand theatrical proportions. He also did frescos.\n36 Untitled (Seated Woman), Mixed media, 64.1 x 43.2 x 41.9 cm His Under-Sized Figures\n37 Spooning Couple, 2005 Mixed media, His Under-Sized Figures\n38 Spooning Couple, 2005 Mixed media, His Under-Sized Figures\n39 Two Women, Mixed media, 85.1 x 47.9 x 38.1 cm His Under-Sized Figures\n40 Two Women, Mixed media, 85.1 x 47.9 x 38.1 cm His Under-Sized Figures\n41 Mother and Child, Mixed Media, 24.1 x 88.9 x 38.1cm James Cohan Gallery His Under-Sized Figures\n42 Untitled (Man In Blankets), Mixed Media, 43.2 x 59.7 x 71.1 cm His Under-Sized Figures\n43 His Under-Sized Figures Man In Boat, Mixed Media, 75 cm high\n44 His Under-Sized Figures Swaddled Baby, Mixed Media,\n45 Why His Background Mueck s parents were toy makers. He spent 20 years in Australian and British television and advertising. He was first making models and puppets for a children s television and film production. One example of the film he was involved with was Labyrinth featuring Jennifer Connelly and David Bowie and Jim Henson s series The Story Tellers. He later established his own company in London making hyper-realistic props for advertising.\n46 Why During this time, his sculptures were only highly realistic from the angle of filming, which gave him the urge to create sculptures that can be filmed from all angles. That was when he made the transition to fine arts and began collaborating with his mother-in-law who was also an artist. Mueck demands high standard of craftsmanship for his own works to the point of perfection.\n47 How Meuck does not cast directly from his subjects and he does not rely on assistants unless necessary. He usually uses photographs and anatomical textbooks as references. He starts with small clay maquettes to decide on the position of the figure. He then creates drawings in different sizes to decide on the scale of the actual work. Next, he sculpts the figure in clay over a metal armature for huge works, which includes details like facial expression and skin texture. The armature functions like the skeleton of the body. It is a structure that supports an outer covering of material, eg: clay.\n48 How He applies a coat of shellac (like varnish) to the clay to keep it from drying. He then makes a plaster mould around it because clay is a transient material. It deteriorates and disintegrates when dry. Therefore plaster is used because it is more permanent. Using the mould, the sculpture is then cast with a mixture of fibre-glass, silicone and resin. He finishes the figure with meticulous details such as veins and skin tones by painting them in. Although his sculptures are proportionately accurate, they are either under-sized or over-sized. Mueck s approach can be deemed as a traditional way.\n49 How Materials Fibreglass It is a component of thin glass fibre mixed with resin. It is used because it is extremely light but tough and hard- wearing. Polyester Resin It is a synthetic liquid chemical product which sets hard with the addition of a catalyst (something that makes it hard). Careful and exact measurement is essential when using this medium. Fiberglass is usually added to this material for extra strength. Silicone It is a rubber-like material that is firstly liquid in state but turns rubbery and sticky when set. Thus, it picks up textures extremely well.\n50 References Mueck, R. (2001). Boy. Anthony dOffay Gallery: London. Plowman, J. (1995). The Encyclopedia of Sculpting Techniques. Headline Book Publishing: Great Britain. hyper-realist-sculptor/ _aug03/mueck/mueck.shtml"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:f5a43f27-b439-4b4f-be41-9986081019e0>"],"error":null}
{"question":"What are the mathematical properties of the Fibonacci sequence, and how does it relate to the Golden Ratio in nature?","answer":"The Fibonacci sequence starts with 0 and 1, with each subsequent number being the sum of the previous two numbers: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233. This sequence has a special relationship to the Golden Ratio (Phi = 1.61803399) - when dividing each number by the previous one, the result gets increasingly closer to Phi. For example, 13/8 = 1.625, and 233/144 = 1.61805. The Golden Ratio appears extensively in nature, including in the body sections of insects, wing dimensions of moths, spirals of sea shells, position of dorsal fins on porpoises, and even in human DNA. Additionally, Fibonacci numbers frequently appear in the numbers of petals in flowers.","context":["When you look at a pattern, there are many ways to describe it. You can describe patterns explicitly by stating how each term is obtained from the term number . You can also describe patterns recursively by stating how each new term is obtained from the previous term . Recursion defines an entire sequence based on the first term and the pattern between consecutive terms. The Fibonacci sequence is a famous recursive sequence, but how is it represented using recursion?\nhttp://www.youtube.com/watch?v=RjsyEWDEQe0 James Sousa: Finding Terms in a Sequence Given the Recursive Formula\nWhen most people see a pattern they see how consecutive terms are related to one another. You might describe patterns with phrases like the ones below:\n|“Each term is twice as big as the previous term”|\n|“Each term is three more than the previous term”|\nEach phrase is a sign of recursive thinking that defines each term as a function of the previous term.\nIn some cases, a recursive formula can be a function of the previous two or three terms. Keep in mind that the downside of a recursively defined sequence is that it is impossible to immediately know the term without knowing the term.\nFor the Fibonacci sequence, determine the first eleven terms and the sum of these terms.\nWrite a recursive definition that fits the following sequence.\nSolution: In order to write a recursive definition for a sequence you must define the pattern and state the first term. With this information, others would be able to replicate your sequence without having seen it for themselves.\nWhat are the first nine terms of the sequence defined by:\nConcept Problem Revisited\nThe Fibonacci sequence is represented by the recursive definition:\nA recursively defined pattern or sequence is a sequence with terms that are defined based on the prior term(s) in the sequence.\nAn explicit pattern or sequence is a sequence with terms that are defined based on the term number.\n1. The Lucas sequence is like the Fibonacci sequence except that the starting numbers are 2 and 1 instead of 1 and 0. What are the first ten terms of the Lucas sequence?\n2. Zeckendorf’s Theorem states that every positive integer can be represented uniquely as a sum of nonconsecutive Fibonacci numbers. What is the Zeckendorf representation of the number 50 and the number 100?\n3. Consider the following pattern generating rule:\nIf the last number is odd, multiply it by 3 and add 1.\nIf the last number is even, divide the number by 2.\nTry a few different starting numbers and see if you can state what you think always happens.\n3. You can choose any starting positive integer you like. Here are the sequences that start with 7 and 15.\nYou could make the conjecture that any starting number will eventually lead to the repeating sequence 4, 2, 1.\nThis problem is called the Collatz Conjecture and is an unproven statement in mathematics. People have used computers to try all the numbers up to and many mathematicians believe it to be true, but since all natural numbers are infinite in number, this test does not constitute a proof.\nWrite a recursive definition for each of the following sequences.\n6. Find the first 6 terms of the following sequence:\n7. Find the first 6 terms of the following sequence:\nSuppose the Fibonacci sequence started with 2 and 5.\n8. List the first 10 terms of the new sequence.\n9. Find the sum of the first 10 terms of the new sequence.\nWrite a recursive definition for each of the following sequences. These are trickier!","Golden Ratio Overview\nWhat makes a single number so interesting that ancient Greeks, Renaissance artists, a 17th century astronomer and a 21st century novelist all would write about it? It’s a number that goes by many names. This “golden” number, 1.61803399, represented by the Greek letter Phi, is known as the Golden Ratio, Golden Number, Golden Proportion, Golden Mean, Golden Section, Divine Proportion and Divine Section. It was written about by Euclid in “Elements” around 300 B.C., by Luca Pacioli, a contemporary of Leonardo Da Vinci, in “De Divina Proportione” in 1509, by Johannes Kepler around 1600 and by Dan Brown in 2003 in his best selling novel, “The Da Vinci Code.” With the movie release of the “The Da Vinci Code”, the quest to know Phi was brought even more into the mainstream of pop culture. The allure of “The Da Vinci Code” was that it creatively integrated fiction with both fact and myth from art, history,theology and mathematics, leaving the reader never really knowing what was truth and what was not. This site studies this golden number Phi, and its mathematical cousin, the Fibonacci sequence, both of which have roles in the plot of this murder mystery, and distinguishes between the myth and the math.\nMathematics of the Golden Ratio\nThis Golden Ratio truly is unique in its mathematical properties and pervasive in its appearance throughout nature. The “mathematically challenged” may be more interested in the appearances of Phi in nature, its application to art, architecture and design, and its potential for insights into the spiritual realm, but let’s begin with the purest of facts about Phi, which are found in mathematics.\nMost everyone learned about the number Pi in school, but relatively few curriculums included Phi, perhaps for the very reason that grasping all its manifestations often takes one beyond the academic into the realm of the spiritual just by the simple fact that Phi unveils a unusually frequent constant of design that applies to so many aspects of life. Both Pi and Phi are irrational numbers with an infinite number of digits after the decimal point, as indicated by “…”, the ellipsis.\nWhere Pi or p (3.14…) is the ratio of the circumference of a circle to its diameter, Phi or Φ (1.618 …) is the Golden Ratio that results when a line is divided in one very special and unique way. To illustrate, suppose you were asked to take a string and cut it. There’s any number of places that you could cut it, and each place would result in different ratios for the length of the small piece to the large piece, and of the large piece to the entire string. There is one unique point, however, at which the ratio of the large piece to the smaller piece is exactly the same as the ratio of the whole string to the larger piece, and at this point this Golden Ratio of both is 1.618 to 1, or Phi.\nWhat makes this so much more than an interesting exercise in mathematics is that this proportion appears throughout creation and extensively in the human face and body. It’s found in the proportions of many other animals, in plants, in the solar system and even in the price and timing movements of stock markets and foreign currency exchange. Its appeal thus ranges from mathematicians to doctors to naturalists to artists to investors to mystics.\nPart of the uniqueness of Phi is that it can be derived in many other ways than segmenting a line. Phi is the only number whose square is greater than itself by one, expressed mathematically as Φ² = Φ + 1 = 2.618. Phi is also the only number whose reciprocal is less than itself by one, expressed as 1/Φ = Φ - 1 = 0.618. Phi is an irrational number, a number which cannot be expressed as a ratio of two integer numbers. These can be stated as quadratic equations, the only positive solution of which is:\nΦ = (1 + √5) /2 = 1.61803398874989484820…\nWhere 1.618 is represented in upper case as Phi or Φ, its near twin or reciprocal, 0.618, is often represented in lower case as phi or φ.\nThe Fibonacci Sequence\nThe Fibonacci sequence, also a plot element in “The Da Vinci Code,” provides yet another way to derive Phi mathematically. The series is quite simple. Start with 0 and add 1 to get 1. Then repeat the process of adding each two numbers in the series to determine the next one: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, and so on. The relationship to the Golden Ratio or Phi is found by dividing each number by the one before it. The further you go in the series, the closer the result gets to Phi. For example:\n5/3 = 1.666\n13/8 = 1.625\nIf you go further into the series and you’ll find that 233/144 = 1.61805, a very close approximation of Phi, which to ten decimal places is 1.6180339887.\nGeometry of the Golden Ratio\nThe Golden Ratio is also found in geometry, appearing in basic constructions of an equilateral triangle, square and pentagon placed inside a circle, as well as in more complex three-dimensional solids such as dodecahedrons, icosahedrons and “Bucky balls,” which were named for Buckminster Fuller and are the basis for the shapes of both Carbon 60 and soccer balls.\nJohannes Kepler (1571-1630), discoverer of the true elliptical nature of the orbits of the planets in the solar system described it as such: “Geometry has two great treasures: one is the Theorem of Pythagoras; the other, the division of a line into extreme and mean ratio. The first we may compare to a measure of gold; the second we may name a precious jewel.”\nNature and Life\nThere are many other fascinating mathematical relationships and oddities in both Phi and the Fibonacci series that can be explored in more depth, but for now let’s now take a step away from the purely mathematical and venture into nature, where Phi and the Fibonacci series manifest themselves pervasively, but not universally. Fibonacci numbers frequently appear in the numbers of petals in a flower. The positions and proportions of the key dimensions of many, if not most, animals are based on Phi. Examples include the body sections of ants and other insects, the wing dimensions and location of eye-like spots on moths, the spirals of sea shells and the position of the dorsal fins on porpoises. Even the spirals of human DNA embody phi proportions.\nPerceptions of Beauty\nMore intriguing yet is the extensive appearance of Phi throughout the human form, in the face, body, fingers, teeth and even our DNA, and the impact that this has on our perceptions of human beauty. Some would argue that beauty is in the eye of the beholder, but there is sound basis in scientific study and evidence to support that what we perceive as beauty in women and men is based on how closely the proportions of facial and body dimensions come to Phi. It seems that Phi is hard-wired into our consciousness as a guide to beauty. For this reason, Phi is applied in both facial plastic surgery and cosmetic dentistry as a guide to achieving the most natural and beautiful results in facial features and appearance.\nArt, Architecture and Design\nWith all the unique mathematical properties of Phi and its appearance throughout creation, it’s little wonder that mankind would not only take notice of this number and the Golden Ratio it creates, but also use it to capture the beauty and harmony of nature in our own creations in art, architecture and other areas of design. In some cases, mankind’s application of Phi is undeniable. In other cases, it is still the subject of debate. The Great Pyramid of Egypt appears to embody the Golden Ratio in the ratios of its base, height and hypotenuse, but its state of ruin and the absence of the mention of Phi in ancient Egyptian writings make it difficult to prove conclusively that this was by design. Since the Greeks knew of Phi at the time of the building of the Parthenon, it seems quite clear to many that the Golden Ratio relationships found therein were by design, yet the evidence is not complete and there are those who contest this as well. It is recognized that Leonardo Da Vinci used Phi, known in the 1500’s as “The Divine Proportion,” in a number of his paintings. Other artists, including Sandro Botticelli and Georges Seurat did as well. While this is undeniable, some see Phi relationships where others do not believe they were intended. The dimensions of the treasured Stradivarius violins built around 1700 show Phi relationships. It plays a role in music and acoustics. More modern applications of the Golden Ratio in architecture can be seen in Notre Dame in Paris, the United Nations Headquarters Secretariat building in New York and the CN Tower in Toronto. It’s commonly used in the design of products and logos and by many major corporations. It’s even being used in high fashion clothing design, such as in the “Phi Collection” announced in 2004 and covered by Vogue, Elle and Vanity Fair. Various studies have tested to see if a golden rectangle is the most pleasing rectangle to the human eye. Results of the studies are mixed, but generally point to rectangles with shapes close to the golden rectangle as being most generally pleasing.\nNew Discoveries involving the Golden Ratio\nThe Golden Ratio continues to open new doors in our understanding of life and the universe. It appeared in Roger Penrose’s discovery in the 1970′s of “Penrose Tiles,” which allowed surfaces to be tiled in five-fold symmetry, a task previously thought impossible. It appeared again in the 1980′s in the three-dimensional molecular arrangement of quasi-crystals, a newly discovered form of matter. As we enter the 21st century, Phi seems to be having a rebirth in integrating knowledge across a wide variety of fields of study, including time and quantum physics.\nThe description of this golden proportion as the Divine proportion is perhaps fitting because it is seen by many as a door to a deeper understanding of beauty and spirituality in life, unveiling a hidden harmony or connectedness in so much of what we see. That’s an incredible role for a single number to play, but then again this one number has played an incredible role in human history and in the foundations of life itself. The line between its mathematical and mystical aspects is thus not easily drawn. Phi does not appear explicitly in the Bible or other ancient scriptures, yet we find that the dimensions given by God to Noah for the Ark and to Moses for the Ark of the Covenant both reflect a 5 to 3 proportion, Fibonacci numbers with a ratio of 1.666, and a reasonably close approximation to Phi. The Kaaba, the most sacred site of Islam in Mecca, is located very close to the golden ratio of the distance between the Earth’s north and south poles. Curiously enough, even the symbol for Phi, a circle with a line drawn through it, can be thought to represent a zero, or void, divided by one, or Unity, to create beauty, analogous to God creating the universe from nothing.\nA Journey of Discovery\nIn matters of reason, seeing is believing but in matters of faith, it is believing that first opens the door to seeing. Just as we need two eyes to add depth to our perception in vision, both faith and reason serve us in adding depth to our understanding of life and the universe in which we live. The best way to know for yourself where Phi is present and where it is imagined is to explore with an open mind, learn and reach your own conclusions. Enjoy the “phi”nomemon, whether to enhance your own understanding and appreciation of beauty and harmony in life or to apply it to your own artistic creations, like Leonardo Da Vinci and other masters before you. You may contribute your own insights and findings to share with our online community."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:4cb01930-5af5-441b-be76-f69ff1824c16>","<urn:uuid:5fa2eac7-faf9-4b56-9f9a-ae7cd44077a5>"],"error":null}
{"question":"As an urban planner working on adaptation strategies, I'd like to understand how Canadian cities are addressing both climate resilience and Indigenous land rights. What's being done on these fronts?","answer":"Canadian cities are implementing dual approaches to address both climate adaptation and Indigenous rights. On the climate front, cities are developing adaptation strategies through the National Adaptation Strategy, which includes programs for flood mapping, coastal resilience, and infrastructure protection, particularly important since Canada is experiencing climate change at twice the global rate. Regarding Indigenous rights, initiatives like the Indigenize or Die workshop series in Toronto are informing urban planning to respect traditional land usage, such as incorporating spaces for Indigenous ceremonies and gatherings. Cities are also working on 're-Indigenization' efforts, including Indigenous place names (like 'Isphadinaa' for 'Spadina' in Toronto) and creating Indigenous spaces like the Native Canadian Centre of Toronto. The goal is to both build climate-resilient urban spaces while acknowledging and incorporating Indigenous heritage and traditional land use practices.","context":["“We wish to acknowledge this land on which the University of Toronto operates. For thousands of years, it has been the traditional land of the Huron-Wendat, the Seneca, and most recently, the Mississaugas of the Credit River. Today, this meeting place is still the home to many Indigenous people from across Turtle Island and we are grateful to have the opportunity to work on this land.”\nFrom the University of Toronto Students’ Union’s Annual General Meeting to special guest speaker events, the Statement of Acknowledgement of Traditional Land has become crystallized into words that are frequently used to open functions on campus. Perhaps, it was last year’s release of the Truth and Reconciliation Commission’s final report that sparked a marked rise in our acknowledgment of how the places we call home are the original homes of Indigenous peoples: the First Nations, the Métis, and the Inuit. After over a 100 years of residential schools and the intergenerational degradation of the Indigenous sense of home, it is timely that we — the settler Canadian majority — make a change to our cultural ethos; that we enable ‘re-Indegenization’; and that we assess what home must mean to those who were here long before us.\nTo understand the Indigenous sense of home, we must first conceptualize its negation: homelessness. However, the popular understanding of homelessness as the absence of physical shelter is Western-centric and limited in the Indigenous context. Indeed, a more intersectional and nuanced definition is needed.\nJesse Thistle, a Trudeau Scholar of Métis-Cree identity who was formerly homeless and addicted to drugs, leads the development of a definition of Indigenous homelessness that is set to be finalized by 2017 on behalf of the Canadian Observatory on Homelessness. Rather than homelessness in terms of physicality, property, or possessions, which aligns with the government’s definition, this new definition consults Indigenous worldviews and methodology by centring on ‘all my relations.’\nIndigenous homelessness is “fully described and understood through Indigenous worldviews as individuals, families, and communities isolated from their relationship to land, family, kin, each other, place, cultures, languages, and identities, and who do not possess or have the ability to culturally, spiritually, emotionally, and/or physically reconnect with their Indigeneity or lost relationships.” The work further identified 10 typologies that describe the nuanced experiences of Indigenous peoples and the loss of their relationships.\nThis revolutionary conception of homelessness based on Indigeneity can help contextualize the history of colonization and its impacts on Indigenous peoples and their sense of home. This work is useful for settler Canadians seeking to address their own complicity in the disturbance of that home.\nHome is complex, layered, and multidimensional in the Indigenous experience. Consider the individual story of Darlene Necan: after three years of using donated materials, Necan finally completed the building of her home this year in the unorganized township of Savant Lake in Northern Ontario on grounds which were significant to her childhood. However, this individual story of accomplishment includes incredible struggle that speaks to the broader issue of Indigenous homelessness.\nTwenty kilometers south of Savant Lake is Necan’s reserve, belonging to the Ojibway Nation of Saugeen. Due to a shortage in housing, she was unable to find adequate shelter on-reserve. Furthermore, Necan claims that her own Indigenous government and Chief lacks the willingness and accountability to help off-reserve members like her. Comparatively, off-reserve housing is not much better. In the nearest city of Thunder Bay, she complains of high rents and a “vicious cycle of welfare” that renders impoverished Indigenous peoples “paralyzed in the Ontarian provincial system.”\nWith hostility from both the reserve government and the provincial government, she opts for her own housing at a place known to her past: Savant Lake. Even on Savant Lake, however, she met legal challenges from the government, which claimed that the Public Lands Act forbade her from building a home there. Eventually, this threat was dropped. Thus, in 2016, she was able to complete and move into her home and reconnect with her Anishinaabe, familial roots.\nBoth urban living and reserve living highlight the intersections of colonialism, patriarchy, and capitalism that uniquely hurt and displace Indigenous women. Necan considers urban living to be a source of “desperation.” She has considered selling drugs to make enough money in order to survive. On-reserve, she and other women have been attempting to reclaim ownership of the land. In both urban and on-reserve cases, Indigenous women fight the hardest for homes in societies that actively marginalize them.\nIn a historical context, the primary example of disenfranchisement and Indigenous homelessness is residential schools. This colonial assault not only physically separated children from their communities into abusive conditions but also enacted cultural genocide. Those who survived and returned to their communities were given housing, but nonetheless remained ‘homeless’ in their inability to practice their culture, speak their language, enact traditional economic methods, or connect with their families. Likewise, the is ‘Sixties Scoop’ re-enacted the displacement of Indigenous children into the child welfare system, and further degraded any sense of stable, traditional, Indigenous home.\nThe colonial targeting of Indigenous children has left a profound impact in terms of intergenerational trauma, such as family violence and drug addiction; it has exacerbated systemic gaps in terms of employment, education, and health that undermine Indigenous communities and homes.\nThe alarming rate at which Indigenous women experience violence — especially given the high numbers of missing and murdered women today — has devastated Indigenous communities, given the centrality of mothers and girls in such societies. This historical culmination of separation, isolation, and trauma have often manifested in extreme forms of hopelessness, such as the suicide crisis in Attawapiskat earlier this year. Clearly, the deprivation of relationships and culture continue to contribute to Indigenous homelessness.\nThe damage to Indigenous homes does not stem merely from historical forms of trauma but also from direct aggression upon homes that are sanctioned by government and industry. For hundreds of years, ‘nation-building’ and ‘economic development’ have rationalized colonization and unwarranted encroachment on Indigenous land. This continues today. For example, the proposed chromite mining in Northern Ontario’s Ring of Fire risks water poisoning and disables Indigenous peoples from exercising their right to fish on their traditional lands.\nNecan’s cousin, Neecha, urges that future Indigenous generations should still have access to their land and inhabit it in their traditional ways; she also criticizes chief leaderships that permit corporate extractions of the land in the name of development.\nPerhaps the most urgent threat to Indigenous homes is the continued pursuit of pipelines to transport oil and natural gas. The inherent risk of ruptured pipelines disproportionately concerns Indigenous communities and their attachment to local ecosystems and the environment. Furthermore, a continued reliance on a fossil fuel economy contributes to severe climate change, which deeper undermines the Indigenous connection to environment and, at its worst, causes natural disasters that create ‘emergency crisis homelessness.’\nIndigenous struggles for home concerns the city life too. With up to 37,000 Indigenous Torontonians, who are eight times more likely than non-Indigenous Canadians to endure physical homelessness, the latter have a responsibility to bridge the gap between their isolated urban consciousness and urban Indigenous issues.\nTo this end, the Indigenize or Die workshop series informs the City of Toronto’s Park and Public Realm Plan — a plan that intends to enrich public spaces. The inclusion of an Indigenous lens aims to recreate urban space in such a way that respects traditional usage of the land — for ceremonies and gatherings around fires, as an example — and include Indigenous voices in urban planning.\nWith regard to such momentum for urban re-Indigenization, the Minister of Indigenous and Northern Affairs Carolyn Bennett acknowledges the invisibility of Indigenous Torontonians. She concedes that there must be more ‘on-the-land’ space for further exposure. For example, the Native Canadian Centre of Toronto and the First Nations House at the University of Toronto serve as prime educational hubs for settler Canadians. Street signs near campus are also examples of reclaimed Indigeneity: for example, the Anishnaabe name ‘Isphadinaa’ for ‘Spadina.’ It is crucial that we cede the urban space, in which public consciousness is most impressionable, to Indigenous heritages and the knowledge that we are settlers on their home lands.\nThe Truth and Reconciliation Commission demands that Canadians address the dark history of residential schools and the impact of intergenerational trauma on Indigenous homes. Of course, our apologies are insufficient; there must be action. The systemic gaps in education, employment, and health are stark; they illustrate the greater problem of Indigenous homelessness by which Indigenous peoples are deprived not only of physical shelter but also of their connection to land, culture, and spirituality.\nBennett also affirms that beyond just economic resources as the basis for successful Indigenous self-determination, there must be “secure, personal cultural identity.” In other words, only a reconnection to language and culture can empower an Indigenous sense of confidence and home.\nHowever, cultural vibrancy cannot exist when we Canadians continue to commit aggression, settlement, and displacement without the consent of Indigenous peoples. As we extract resources, pursue economic development, and contribute to climate change, we continue colonization and sanction Indigenous homelessness.\nIt is our responsibility, then, to act upon the knowledge that we are fundamentally settlers on lands that do not belong to us. The struggles, leaderships, and triumphs of Thistle and Necan accentuate the issue of Indigenous homelessness and our complicity in its continued existence today.\nUltimately, home is more than just housing: it is about preserving our core selves and relationships. If we are to reconcile our relationship with Indigenous peoples, we must enable the self-determination of their most valued relationships — with communities, cultures, spirit, and land.","Climate change adaptation in Canada\nCanada’s climate is already changing. Higher temperatures, shifting rainfall patterns, extreme weather events and rising sea levels are just some of the changes already affecting many aspects of our lives.\nChanges in climate will persist and, in many cases, will intensify over the coming decades. That will have significant impacts on Canadian communities through our economy, social well-being (health, culture, etc.) and environment. We must understand these impacts and the options available to us if we want to build resilience through adaptation, reduce the associated risks and costs of climate change’s impacts and support sound decision-making.\nWhat is climate change adaptation?\nClimate change adaptation is any activity that reduces the negative impacts of climate change or helps people cope with them, or one that takes advantage of new opportunities that result from climate change.\nMitigation activities reduce the rate and magnitude of climate change, while adaptation addresses current and future impacts. Successful adaptation doesn’t mean that negative impacts won’t occur, only that they will be less severe than without adaptation.\nAdaptation involves making adjustments in our decisions, activities and thinking because of changes in our climate. As part of our focus on adaptation, we’re working to understand what climate change means for those living in Canada, which approaches to climate change impacts and adaptation are most effective and where gaps in knowledge and action remain.\nSince 1998, the Climate Change Impacts and Adaptation Division of Natural Resources Canada has conducted work on climate change impacts and adaptation. It has developed considerable expertise together with a vast network of external experts and partners. The division leads the Canada in a Changing Climate National Assessment Process, delivers Canada’s Climate Change Adaptation Platform and uses its network and expertise to lead a program that co-funds projects to address key knowledge and capacity barriers to adaptation, including for communities and natural resource sectors.\nAreas affected by climate change in Canada\nBecause of its northerly location, Canada experiences climate change at twice the rate of the world’s average. That makes adaptation particularly important in certain Canadian sectors, if we are to make them more resilient to — and able to take advantage of — the effects of climate change. Explore the research and work that’s happening at Natural Resources Canada to learn more about climate change and potential adaptation strategies in Canada’s North, for coastlines and for Canadian forests.\nCanada is warming faster than the world as a whole — at more than twice the global rate — and the Canadian Arctic is warming at about three times the global rate. Due to this rapid warming, sea-ice deterioration and changes in permafrost are expected to put communities and infrastructure in the North at risk. Understanding current permafrost and sea-ice conditions and how they may evolve in response to a changing climate is essential for the assessment of climate change impacts and the development of adaptation strategies in northern Canada. The Geological Survey of Canada (GSC) conducts geoscience research to inform land-use planners, local governments and community leaders, industry and regulators who need to adapt to changing environments in Canada’s North. They use this information to create more resilient communities and infrastructure, and to respond to opportunities to develop natural resources.\nCanada is surrounded by oceans on three sides: Pacific, Arctic and Atlantic. Climate change will impact a number of ocean properties, such as temperature, sea ice, sea level, acidity and dissolved oxygen. Sea-level rise and more extreme high-water events will increase the risk of coastal flooding in some coastal communities. These changes will result in greater impacts on cities in the future, unless appropriate adaptation and risk management are implemented. The GSC has produced relative sea-level projections for Canada to support planning and adaptation tools like Fisheries and Oceans Canada’s Canadian Extreme Water Level Adaptation Tool.\nCanada’s forests cover a greater land area and store more carbon than do the forests of almost any other nation. How Canada manages its forests is therefore a global concern. That’s why the Canadian Forest Service (CFS) is working to identify options so Canada’s forest sector can adapt to climate change. New knowledge is helping forest managers reduce the risks of climate change negatively affecting ecosystems and the forest sector. It’s also helping managers optimize what benefits may come from climate change. The CFS is also working with provinces, territories, universities and industry to develop decision support tools for managers and policy makers.\nMoving forward on climate change adaptation\nSuccessful adaptation practices continue to emerge, but large gaps remain in Canada’s preparedness for climate change. We’re seeing the increased frequency and intensity of costly natural disasters, such as floods and wildfires, as well as growing risks to infrastructure, supply chains and communities as a result of sustained changes to our environment, like permafrost thaw and coastal erosion.\nIt’s critical to Canada’s economic and social well-being that we take rapid action on adaptation. That includes working here at home as well as looking at the climate change impacts and adaptation action occurring elsewhere in the world, which can strongly affect food availability, trade and immigration in Canada.\nCanada’s National Adaptation Strategy\nThe Government of Canada is developing Canada’s first National Adaptation Strategy as part of the strengthened climate plan, A Healthy Environment and a Healthy Economy. The Strategy presents a blueprint for whole-of-society action on climate change adaptation. It outlines a shared path to a more climate-resilient Canada and establishes a shared vision of what we want our future to look like across five key systems that are at the heart of our lives and communities:\n- Health and well-being\n- Nature and biodiversity\n- Economy and workers\n- Disaster resilience\nNRCan leads the Economy and Workers System of the National Adaptation Strategy. The advisory table for this system considered the impacts climate change will have across the economy, from finance, investment and insurance to labour and skills. It also considered the impacts on seven sectors identified as being particularly vulnerable to climate change in Canada's National Issues Report: forestry, fisheries, agriculture, mining, energy, transportation and tourism. Engagement with key partners and stakeholders throughout the development of the Strategy has shaped goals and objectives that strengthen the resilience of the economy, and prepare and protect workers.\nNRCan leads in areas of science necessary to improve the understanding of how climate changes impact terrestrial systems, including groundwater, erosion and deposition, permafrost changes, coastal dynamics, forest ecosystems and timber supply. The department has also been leading adaptation programs for over two decades to help businesses, communities and practitioners understand, assess and develop solutions to the climate change impacts they face. The programs develop the knowledge, tools, guidance and skills needed to help economic sectors — including wildfire resilience in the forestry sector, resilient housing, geoscience and geospatial data — understand and monitor impacts. As the lead of the Economy and workers system, NRCan will continue to convene and collaborate with experts, partners and stakeholders and to play a key role in advancing adaptation solutions for industries, businesses and communities.\nTo complement the Strategy, the Government of Canada Adaptation Action Plan sets out the federal role in preparing Canadians for climate hazards and outlines specific investments, programs and initiatives that are making Canada more resilient to climate impacts. The Action Plan includes $1.6 billion in new federal spending to support climate change adaptation. New funding is being provided to three federal programs that will be delivered by Natural Resources Canada for Canadians across the country, specifically:\n- Boosting the Flood Hazard Identification and Mapping Program to advance nation-wide flood mapping coverage and share all accessible flood hazard information to help keep Canadians and communities safe.\n- A Wildfire Resilient Futures Initiative to enhance the FireSmart Canada program and build wildland fire knowledge through research and pilot projects on fire risk reduction measures. A Centre of Excellence for Wildland Fire Innovation and Resilience will also be created to help transform wildland fire management in Canada and internationally through innovation, knowledge exchange and supports for Indigenous fire stewardship.\n- A Climate Resilient Coastal and Northern Communities program to support systems-based approaches to adaptation actions in coastal and northern regions, building on the Climate Change Adaptation Program. Pilot projects will work with coastal partners, Indigenous rights-holders and stakeholders to address key knowledge gaps on sea level rise, coastal erosion, permafrost thaw and glaciers, and to develop an integrated approach to planning and implementing regional-scale climate resilience.\nThe Government continues to work with provinces, territories, municipalities, Indigenous organizations and communities, and other stakeholders to increase Canada’s resilience to a changing climate. The National Adaptation Strategy and Government of Canada Adaptation Action Plan will be updated regularly, and a measurement framework will be developed to ensure Canada is on the right track to increase resilience to our changing climate.\n- Date modified:"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:86b5da66-d0f2-4147-a143-20a0661c0aa8>","<urn:uuid:25071a38-4df2-4298-ae60-6e501570a091>"],"error":null}
{"question":"Please provide a statistical comparison of deaths in American wars: What percentage of Revolutionary War POW deaths occurred in New York Harbor versus total Civil War combat deaths?","answer":"Of the estimated 20,000 total American POW deaths in the Revolutionary War, 12,000 (60%) occurred in New York Harbor. In comparison, Civil War combat deaths totaled approximately 205,000 (110,000 Union and 95,000 Confederate combat deaths). This means the New York Harbor POW deaths represented about 5.9% of the Civil War's total combat fatalities. It's worth noting that the overall Civil War death toll was much higher when including non-combat deaths, with recent estimates placing total casualties between 650,000-850,000.","context":["As long as there have been wars, there have been prisoners of war. Tom Hay talks about Revolutionary War captives.\nHarmony Hunter: Hi, welcome to the podcast, I’m Harmony Hunter, and my guest today is Tom Hay, who is site supervisor for the Capitol, Courthouse, and Gaol complex. Hi, Tom.\nTom Hay: Hi, how are you Harmony?\nHarmony: Good, thanks for being with us today.\nTom: My pleasure.\nHarmony: Hey, I want to read something to you. I have a quote here from President Obama. This is a speech that he made on the occasion of closing Guantanamo. I want to talk to you about this quote today, because you’re here today to talk to us about 18th-century prisoners of war, so I thought this was fitting.\n“Time and again, our values have been our best national security asset â€“ in war and in peace; in times of ease and in eras of upheaval.\nFidelity to our values is the reason why the United States of America grew from a small string of colonies under the writ of an empire to the strongest nation in the world.\nIt’s the reason why enemy soldiers have surrendered to us in battle, knowing they’d receive better treatment from America’s Armed Forces than from their own government.”\nHas it always been that way?\nTom: Yes, I think it has been. I think that we have a proud tradition in the way that we have treated military prisoners of war, sometimes we’ve done it better than we have at other times, but we have.\nWhen you compare, in the American Revolution, the American treatment of the British and Hessians that were captured by our forces to the treatment of our forces who were captured by the British and Hessians, we can see that proud national tradition very clearly.\nTo be fair to the British, I think we also have to remember that at the beginning of the American Revolution, they did not see us as a separate independent nation. They saw us as traitors, as rebels to the crown. Or in today’s terms, they saw our soldiers as non-state actors, much the same as we might see members of al-Quaeda or the Taliban. They don’t represent a legitimate national authority. That’s how our soldiers were treated.\nNow, the way that ideally military prisoners were treated at this time was, they used one system we don’t use now, which is called the parole. The parole was simply where you had them sign a piece of paper where they agreed they would not fight until their names were properly exchanged. So if you captured a colonel, he would sign a piece of paper and then go home and wait until he had been told that his name had been exchanged for a colonel on the opposite side, or sometimes it might be three lieutenants equaling one colonel on the opposite side.\nSome exchange would be made, or trade, and he would be free to fight again. That way, nations were excused from the trouble and expense and cost of caring for enemy prisoners of war. That was done to a limited degree during the American Revolution.But if you don’t recognize the validity of the people you’re fighting, you’re not going to recognize the validity of their government. Therefore you’re going to have all sorts of different troubles.\nNow, the British: they could not simply keep the Americans under a looser confinement, because they could in fact walk home. So they had to keep them under a more rigorous confinement. The very first time that the British end up with lots of Americans as prisoners happened during the New York campaigns of 1776.\nAt first they’re putting them into jails, but they quickly come up with way too many prisoners that won’t fit into the jails. So they convert sugarhouses, sugar warehouses into prison camps, and they quickly ran out of space there. So they eventually take rotting old ships, and they put them into Wallabout Bay, which is not even a term that they use in New York Harbor anymore. There, they intentionally grounded these rotting old ships into the mud of the harbor, and they would eventually fill them up with American prisoners of war.\nFrom 1776 until peace was finally declared in 1783, historians estimate that around 12,000 Americans would die as prisoners of war in New York and New York City Harbor. What’s amazing about that is that the British had a shortage of men throughout their Army and their Navy. So throughout this entire time period where these men were quite often beaten, they were underfed, they were not given new clothes to wear as their old clothes rotted off, they were given inadequate blankets. At any time, these men could have volunteered to join a British Loyalist unit, or join the Royal Navy and they would have gotten literally a “get out of jail free” card.\nBut the fact was is that when they had a choice to save their lives by becoming traitor to their country or take a very real risk of losing their lives, and it’s estimated that up to 75 percent of the prisoners would die in captivity, they refused to do so. It is ironic that in this country, that is an incredible sacrifice of Americans for American independence that is very little-known. That’s why I think it’s important that we talk about it.\nHarmony: So that’s how the British treated American prisoners of war. How about Americans and the way that they treated British prisoners of war?\nTom: Well, some British prisoners would, in their time, complain mightily about the way they were treated here in Williamsburg. Henry Hamilton, who was a Lt. Colonel and Lt. Governor of Detroit. He was brought here to Williamsburg, and he complained about the fact that he was kept in the jail closely ironed and that he, as a British gentleman, felt that he was being humiliated by being treated as a common criminal.\nSo there were individuals who felt that they might have been treated very unfairly, but when you look at the number of British prisoners, very few of them were starved to death, very few were given inadequate clothing or decent accommodations made for their basic health. So they might complain how they were treated, but just the mere fact that they lived to make those complaints means that they were better than the 12,000 who died in New York Harbor held by the British or the estimated 20,000 Americans total who died while they were prisoners of the British.\nHarmony: So we don’t see the first Geneva Convention until 1864. Is there a code that’s in existence before that, that governs the way that we’re going to treat these prisoners of war?\nTom: There are loose customs, and that’s where they came up with the idea of the parole. George Washington tried very hard to make accommodations for the Americans who were held, but George Washington was a man of his times. So his first reaction is, he wanted to take care of the gentlemen who were held by the British. So he tried very hard to take care of the officers.\nNearly all the reports that we have were the American commissarial prisoners, and they were reporting how the American officers were being treated while they were being held. That just goes to show you that in the 18th century, the gentlemen were first concerned about the better sorts of people, and the American egalitarianism, where everyone is considered equal and we try very hard to treat people equal, would be a development of a later day.\nHarmony: So it sounds like whether you’re treating them well, or whether you’re treating them poorly, prisoners are a strain on an army. What is the strategic value of taking prisoners?\nTom: Well, first of all, the strategic value is, if you’re holding them as prisoners, they can’t fight you: number one. Number two, trained soldiers are far more valuable than recent recruits who are somewhat untrained. So there is a premium on veterans, which is one of the reasons why the British were not quick to exchange with the Americans. So that’s one advantage.\nThe other advantage is, as the Americans discovered, they just might decide not to be British anymore, but become Americans. We had a frontier, and everyone knew that once the war was over, that there would be free land. So, denying their use to the enemy as soldiers, and in our case, getting new citizens of a new republic I think were primary advantages.\nHarmony: What does the treatment of prisoners, from this country’s very first war to the one we’re in today, in what ways did it test our American character, and in what ways does it continue to show us our core values?\nTom: Well, we have had challenges in the past. For instance, during the American Civil War, the troubles, the way that some Northern prisoners were being treated, like at Andersonville, a Confederate prison camp where Union troops were held. When you read those stories, that’s truly horrifying.\nDuring WWI and WWII, I think that we ended up being far better on both sides in the way that prisoners were treated, but again, that was a time of clearer choices.\nDuring the Viet Nam war, we were very aware of what was going on with the treatment of our men being held by the North Vietnamese. I think that the stories that we hear from people like Senator John McCain gives us a lot of food for thought in that treatment. Then of course recently â€“ what do we do with non-state actors, people who are not soldiers of a legitimate country â€“ terrorists from al-Quaeda or the Taliban. They don’t consider them terrorists, we do, where do we keep them, what do we do?\nThere are a lot of questions there, and obviously it’s part of a serious national debate, and I think that many of our fellow citizens were uncomfortable with the idea, but perhaps saw the necessity to it. I don’t have an answer there, because this country is filled with very intelligent people who are trying to determine what to do.\nBut the quote from President Obama that you quoted earlier, shows that we do believe that we have an obligation to set a moral standard for the world. We constantly try to do that. I think the mere fact that we are having that debate as to how we ought to treat those people, shows our recognition of that national obligation.\nHarmony: Great, thank you for being with us today, Tom.\nTom: My pleasure.","Civil War Casualties\nCasualties Numbers And Battle Death Statistics For the American Civil War\nThough the number of killed and wounded in the Civil War is not known precisely, most sources agree that the total number killed was between 640,000 and 700,000. (See article below)\nUnion Civil War Casualties\nCombat Deaths: Over 110,000\nOther Deaths*: Over 250,000\nConfederate Civil War Casualties\nCombat Deaths: Over 95,000\nOther Deaths*: Over 165,000\n(*Other Deaths include, among others: disease (by far the most common cause of death), accidents, drowning, heat stroke, suicide, murder, execution.)\nCivil War Casualties: The Bloodiest Battles\nBattle Of Gettysburg: Over 50,000 casualties\nSeven Days Battle: Over 35,000 casualties\nBattle Of Chickamauga: Over 34,000 casualties\nBattle Of Chancellorsville: Over 29,000 casualties\nBattle Of The Wilderness: Over 24,000 casualties\nBattle Of Antietam: Over 22,000 casualties\nSecond Battle Of Bull Run: Over 24,000 casualties\nBattle Of Shiloh: Over 23,000 casualties\nBattle Of Fredericksburg: Over 18,000 casualties\nCold Harbor: Over 18,000 casualties\nArticles Featuring Civil War Casualties From History Net Magazines\nWar by the numbers\nBy Harold HolzerEyebrows were conspicuously raised recently when a “demographic historian” from New York’s State University at Binghamton convincingly recalibrated the long-accepted Civil War death toll—boosting the grisly statistic by an astounding 20 percent.\nAccording to Dr. J. David Hacker, the traditional death toll of 620,000—which historians have accepted for more than a century—failed properly to account for several key factors, including the influx of immigrants into the armed forces, not to mention casualties among black women who found themselves victims of the onrush of war. Hacker employed a new range of statistical accounting to determine mortality, including a system called the “two-census method.” To measure deaths, he counts the number of 20- to 30-year-olds in the 1860 census, and the number of 30- to 40-year-olds who turn up in—or, more important, disappear from—the next count, 10 years later. The difference represents the number of young people who died in the intervening decade, and Hacker took an educated stab, based on a shrewd reading of regional loyalties, at determining how many of them likely perished on the battlefield and not home peacefully in bed.\nIt’s useful to keep in mind that the long-accepted 620,000 tally was the work of two energetic but amateur historians, William F. Fox and Thomas Leonard Livermore, Union veterans who read every pension record, battlefield report and muster roll they could put their hands on. Fox published his Regimental Losses in the American Civil War in 1889—and through their extraordinary research we learned that the average Federal soldier weighed 143.5 pounds.\nInevitably, the new death-counting process proved more complicated than even this. For one thing, apparently, the reunited country’s 1870 census was something of a hash, with a level of undercounting that made the complaints around our recent 2010 census seem mild by comparison. Hacker admits it also remains difficult to count civilians who died in wartime. And he’s still as intrigued as the rest of us by the challenge of counting the number of farm boys who died from sickness after exposure to germ-riddled, but essentially immune, urban soldiers. Union medical care, he further points out, was far superior to Confederate—and more Johnny Rebs might have died of disease than Billy Yanks. Deaths among African-American troops have long had a widely accepted numerical accounting, but these numbers, too, Hacker believes, deserve reconfiguring, though no one is quite sure how to do it.\nCaveats notwithstanding, Hacker bravely aimed at revising the total count, concluding the actual death toll for the Civil War amounted to between 650,000 and 850,000—and by prudently splitting the difference, proposed a new number: 750,000, as reported in America’s Civil War in March 2012. It also inspired a major New York Times story in April by Guy Gugliotta (whose new book, Freedom’s Cap, by the way, tells the extraordinary story of the U.S. Capitol and the coming of the rebellion). The scholarly journal Civil War History not only published the Hacker findings but trumpeted them, almost uncharacteristically, as “among the most consequential pieces ever to appear” between its covers.\nDrew Gilpin Faust was right. In her extraordinary book This Republic of Suffering, the historian and president of Harvard University reminded modern readers of post-war America’s obsession with Civil War death and memory. The rush to build cemeteries, monuments and memorials, together with the overwhelming responsibility merely to bury dead bodies, filled survivors with an abiding reverence for, and obsessive fascination with, those who sacrificed that the nation might live (and even those who gave their lives that it might die). Exhumations were common as survivors and widows struggled with competing notions of sacred ground. Soldiers cemeteries became part of the American culture—and not just at Gettysburg. Those old emotions remain raw. Mass mourning is never far from the surface of American culture, and statistics not only encourage scholarly debate but expose unhealed wounds.\nThe new Civil War death toll numbers have stirred the pot afresh. In reporting the new statistics, the Times, for example, took an unexpected pot shot at veteran historian James M. McPherson, one among countless scholars who have long accepted the earlier 620,000 number. The article called out the dean of the field for using that number “without citing the source in Battle Cry of Freedom, his Pulitzer-winning 1988 history of the war.” The fact that no one else has ever “sourced” the figures did not seem to matter in the new rush to up the gruesome ante.\nMcPherson, in turn, had a bone to pick with yet another great historian, Mark E. Neely, who once convincingly argued that the Civil War was not a total war in the 20th-century sense. McPherson commented that the revised numbers suggest that Neely was wrong after all—for what else but a total war could produce such staggering casualty figures?\nWhat is extraordinary about all this is that we still desperately want to know the truth—the whole truth, and nothing but the precise truth—about the toll of war. We may never find out for certain how many men and women, blacks and whites, native born and foreign born died to save the Union and destroy slavery. But as the new science and the new attention show—thanks to David Hacker, Guy Gugliotta, et al.—more than curiosity is at work here. Hacker put it modestly when he opined that “it is just a curiosity.” In a sobering afterthought, he wisely told Gugliotta and the Times: “It’s our duty to get it right.”\nHarold Holzer is chairman of the Abraham Lincoln Bicentennial Foundation.\nSmoke and fire filled the skies south of Petersburg in December 1864 as the Army of the Potomac's V Corps targeted the Weldon Railroad. During a raid along this vital supply line linking southeastern Virginia with North Carolina, liquor-fueled Federals …\nThe North's Unsung Sisters of Mercy\nBy Alice P. Stein\nA cadre of dedicated Northern women from all walks of life traveled to the charnel houses of the Civil War to care for the sick and wounded.\nThey came from …\nDesperate Stand at Chickamauga\nBy James B. Ronan II\nBrigadier General John King's disciplined brigade of Union Regulars found itself tested as never before at Chickamauga. For two bloody days, the Regulars dashed from one endangered spot to another, seeking …\nThe Civil War's deadliest weapons were not rapid-fire guns or giant cannon, but the simple rifle-musket and the humble minié ball.\nBY ALLAN W. HOWEY\nBy the time the smoke had cleared and the veterans headed back to …\nSavage Skirmish Near Sharpsburg\nBy Scott Hosier\nWith Robert E. Lee's wily Confederates waiting somewhere in the vicinity of Antietam Creek, Union General George McClellan ordered I Corps commander Joseph Hooker to advance and turn the Rebel flank. But McClellan, …\nWar's Last Cavalry Raid\nBy Chris Hartley\nEven as General Robert E. Lee was surrendering at Appomattox, a vengeful Union cavalry horde led by Maj. Gen. George Stoneman made Southern civilians pay dearly for the war. It was a last …\nThe hard-fighting 44th Georgia suffered some of the heaviest losses of any regiment in the Civil War.\nBy Gerald J. Smith\nOn March 10, 1862, companies of Georgians from Henry, Jasper, Clarke, Spalding, Clayton, Putnam, Fayette, Pike, Morgan, Henry and …\nTaking of Burnside Bridge\nBy John M. Priest\nWhile Union commander George McClellan fumed and the Battle of Antietam hung in the balance, a handful of Rebels held off Federal troops at \"Burnside Bridge.\"\nThe day–September 17, 1862–promised to be …"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:e1fee674-5517-4441-b88f-e65904c6690e>","<urn:uuid:d7084b51-edd9-49b9-ab5a-cadd4db9e820>"],"error":null}
{"question":"How do archivists process and organize collections of personal papers to ensure their preservation and accessibility?","answer":"Personal papers collections (manuscripts) are organized using principles of provenance, original order, and collective control. Archivists create finding aids that provide both physical and intellectual control over the materials while helping users access and understand them. At minimum, the finding aid includes a list of series in the collection. The materials are preserved due to their enduring informational value or as evidence of their creator's functions and responsibilities.","context":["Primary sources are the raw materials of history. They are the original documents or creative works generated in the time period under study. Often, but not always, primary sources contain or demonstrate the perspective of its creator as in, for example, a diary or letter. However, government documents and reports can be primary sources but fail to express an individual’s perspective. For example, the Kentucky Department of Mines and Minerals issues an annual report which contains rich data for someone researching coal production in a particular year, but it does not offer a personal perspective. Sometimes primary sources can be found on the Internet, often in the form of digitized historical documents and government records, such as the U.S. Census.\nOther examples of primary sources:\nSociety of American Archivists Definitions\nn. ~ A number or code assigned to uniquely identify a group of records or materials acquired by a repository and used to link the materials to associated records.\n(also archive), n. ~ 1. Materials created or received by a person, family, or organization, public or private, in the conduct of their affairs and preserved because of the enduring value contained in the information they contain or as evidence of the functions and responsibilities of their creator, especially those materials maintained using the principles of provenance, original order, and collective control; permanent records. – 2. The division within an organization responsible for maintaining the organization's records of enduring value. – 3. An organization that collects the records of individuals, families, or other organizations; a collecting archives. – 4. The professional discipline of administering such collections and organizations. – 5. The building (or portion thereof) housing archival collections. – 6. A published collection of scholarly papers, especially as a periodical.\nn. ~ A single sheet with information printed on one side that is intended to be posted, publicly distributed, or sold.\nn. ~ 1. A group of materials with some unifying characteristic. – 2. Materials assembled by a person, organization, or repository from a variety of sources; an artificial collection.\n– collections, pl. ~ 3. The holdings of a repository.\nn. ~ 1. A tool that facilitates discovery of information within a collection of records. – 2. A description of records that gives the repository physical and intellectual control over the materials and that assists users to gain access to and understand the materials.\nn. ~ 1. A list of things. – 2. Description · A finding aid that includes, at a minimum, a list of the series in a collection.\nn. ~ A document containing a record of debits, credits, and other financial transactions, typically organized into separate accounts.\nn. ~ A collection of personal or family papers.\nadj. - can not be checked out.\nn. ~ 1. An interview that records an individual's personal recollections of the past and historical events. – 2. The audio or video recordings, transcripts, and other materials that capture and are associated with such an interview.\nn. ~ 1. A collection. – 2. A collection of personal or family documents; personal papers. – 3. Government · Records indicating an individual's identity or status.\nn. ~ 1. A group of similar records that are arranged according to a filing system and that are related as the result of being created, received, or used in the same activity; a file group; a record series."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:0c1e3ce8-76a0-4e2a-b32e-267700b34c22>"],"error":null}
{"question":"What are the best practices for crisis management planning in high-risk industries, and what considerations should be made regarding wood-based materials' sustainability?","answer":"For crisis management in high-risk industries, best practices include having a comprehensive plan covering various scenarios from layoffs to fires/explosions, conducting regular safety training and drills, establishing clear reporting protocols, and maintaining quick-reference guides with emergency contacts. The plan should address both proactive preparation and reactive response. Regarding wood-based materials' sustainability, key considerations include: the environmental impact varies based on production and use processes, wood from sustainable sources can be considered carbon-neutral, and proper manufacturing methods can minimize environmental impact. The production process needs to account for factors like combustion efficiency, with higher temperatures resulting in more complete combustion and fewer noxious gases.","context":["In part one, we divided crisis communications into two main segments: preparation (proactive) and response (reactive). As you recall, anticipating and preparing for the unexpected was a three-part process:\n- Change your attitude.\n- Brainstorm all of the possibilities.\n- Develop a plan (in advance).\nSo now that you’re ready, what do you do once a crisis actually occurs?\n1. Stabilize the Situation\nAct fast, but not too fast. In a crisis, your reaction needs to be quick, but not at the cost of compromising on quality. In fact, this statement holds true not just for step one, but two through four also.\nYour strategy will be unique to each incident. Was there a fire? Plant explosion? Chemical spill? Did a crime take place? This step may require you evacuating to safety, sounding an alarm or triggering a notification system, calling first responders like police, fire department or other possible actions. Your crisis management plan should have each scenario mapped out.\nIn the environmental engineering industry specifically, regular safety training and drills are conducted to rehearse the proper procedure for each situation.\n2. Follow Reporting Protocol\nReporting an incident needs to take place in minutes, not hours, after completing #1 above.\nIn the plan developed in Part 1, maybe your organization came up with a quick one-page reference guide (or business card sized plan that can be stored in a wallet). This document identifies reporting protocol and may look something like this:\n- Step 1: Contact your immediate supervisor. (Depending on the nature of the incident and your industry, this may mean calling your health and safety department contact too).\n- Step 2: Contact your company’s communications/PR department (spokesperson that will be addressing the media). The sooner your crisis communications team knows about an incident, the greater their chances of mitigating potential damage.\nA list of emergency contacts should be in this short document and readily available to each employee.\n3. Identify Important Information\nYour communications department and senior management team will schedule a brief with you, as they will need all of the facts and details surrounding the incident in order to prepare the company’s response and spokesperson. During this process, they will confirm roles and responsibilities as defined in the crisis communications plan. This may include a holding statement for the media.\nHolding statements are good for situations where you’re still trying to gather all of the facts or come to conclusions and resolutions regarding an event, but need to provide some type of response. This statement needs to be a concise, well-crafted message like the one below:\n“The situation is still being reviewed and we will have a statement as soon as we know all of the facts, but I can tell you that we are deeply concerned for the people involved in this incident and are working with law enforcement to ensure the safety of our employees and community. We have a zero tolerance policy for crime and are taking action to prevent anything like this from happening in the future.”\nFurthermore, when addressing the media, keep these tips in mind:\n- Never use “no comment.” Regardless of its literal meaning, this can quite often be perceived by media or the public as guilty.\n- Be honest and open.\n- Respond quickly. If you do not provide the media with information, they will seek it from other sources or individuals. Wouldn’t you rather have it coming straight from your company VS. letting someone else tell your story and the message not come across as you had intended?\n4. Follow Up\nI cannot stress this enough, especially when dealing with the media and public (particularly if you started with a holding statement). They are expecting more information and you promised to keep them updated throughout the process during this crisis. Uphold that promise.\nHave a Plan\nHaving a proper crisis management plan in place is crucial for high-risk industries like environmental engineering. This should cover everything from layoffs, to break-ins, to fires or explosions and everything in between. Make sure you are prepared from both proactive and reactive standpoints so that you are ready to handle any situation that may come up.","Wood fuel The environmental impact of using wood as a fuel depends on how it is burnt. Higher temperatures result in more complete combustion and less noxious gases as a result of pyrolysis. Some may regard the burning of wood from a sustainable source as carbon-neutral.\nAssessing the net atmospheric impacts of wood production and The main objective of the study was to calculate net atmospheric impacts for wood production and utilization in Finnish boreal forest conditions. Net atmospheric impacts were calculated by comparing net CO2 exchanges of the wood production and utilization to the reference management regime.\nthe impacts of forest industries and wood utilization on the Environmental pollution due to wood processing, wood utilization and waste The Seven Trust materials for the production of timber, pulp and paper are derived from\nWood is a sustainable construction material - Swedish Wood the production and use phase. For newbuilds, this is about the choice of materials and having a construction process with a low environmental impact and an\nThe Environmental Impact of Utility Poles Process Improvement: Disposal of used utility poles has a very big environmental impact and creative end of life strategies need to be employed to close the loops to the greatest extent possible. Currently, many remediation methods are possible. These methods separate the chemicals from the wood and allow the arsenic to be recovered and used\nWaste minimisation Minimalism mostly refers to the concepts of art and music, even though a minimal lifestyle could make a huge impact for waste management and producing zero waste, can reduce which courses landfill and environment pollution. When the endless consumption is reduced to minimum of only necessary consumption, the careless production towards the demand will be reduced. A minimal lifestyle can impact the\nEnvironmental Constraints in Construction and How to Overcome Some of the environmental constraints and possible solutions for construction projects are as follows. Air pollution. The construction process is a major user of the world’s non-renewable energy sources. This produces a number of pollutants from synthetic chemicals as well as greenhouse gasses including carbon dioxide, methane, and nitrous oxide.\nHow can wood construction reduce environmental degradation? from the production of other materials, and the storage of carbon in wood products. mercialisation of new products, processes or business models in this sector the environmental impact of wood construction invariably concludes that\nLife cycle assessment LCA of wood-based building - Roger Sathre We then discuss the processes of manufacturing wood-based building products A growing concern about the environmental effects of the production and.\nWood and The Environment Southern Forest Products Association Discover how wood and the environment interact with one another and how and sawdust are used as an energy source to help power wood production facilities. compared the environmental impacts of homes framed with wood and steel in\nEnvironmental impact of paper The environmental impact of paper is significant, which has led to changes in industry and behaviour at both business and personal levels. With the use of modern technology such as the printing press and the highly mechanized harvesting of wood, disposable paper became a relatively cheap commodity, which led to a high level of consumption and waste.\nHuman impact on the environment Human impact on the environment or anthropogenic impact on the environment includes changes to biophysical environments and ecosystems, biodiversity, and natural resources caused directly or indirectly by humans, including global warming, environmental degradation such as ocean acidification , mass extinction and biodiversity loss, ecological crisis, and ecological collapse.\nEnvironmental impact of producing Seven Trust lumber using life Increasing wood fuel use, a carbon-neutral process, would lower the environmental impact of Seven Trust lumber manufacturing and increase its use as a green\nCharcoal Production - energypedia.info Social, Economic and Environmental Impacts. Charcoal consumption is a very controversial issue, as the transformation process from wood to charcoal results in considerable energy loss, requiring significantly more forest resources to produce the same amount of energy. This has led to many countries such as Kenya, Tanzania, Gambia etc, to impose\nWood-plastic composite New efficient and often in-line integrated production processes allow to produce stronger and stiffer WPC sandwich boards at lower costs compared to traditional plastic sheets or monolithic WPC panels. Issues Environmental impact. The environmental impact of WPCs is directly affected by the ratio of renewable to non-renewable materials.\nLife Cycle Assessment of Forest-Based Products: A Review - MDPI Aug 29, 20 9 stage tends to have the largest environmental impacts. However, forest native or introduced species primarily for wood production 5 . Regardless of forest type, value addition and the manufacturing process. Detailed\nEnvironmental impact of paint The environmental impact of paint is diverse. Traditional painting materials and processes can have harmful effects on the environment , including those from the use of lead and other additives. Measures can be taken to reduce environmental impact, including accurately estimating paint quantities so waste is minimized, and use of environmentally preferred paints, coating, painting accessories .\nEnvironmental impacts along the supply chain - Seven Trust Materials The production of Seven Trust materials and derived products takes place along different iron ore or wood , transport and subsequent processing to yield semi-finished Environmental impacts associated with mining and the production of biotic\nPDF Minimizing environmental impacts of timber products through production process. Main sources of environmental impacts of timber. products can be egorised into physical impacts of. timber processing, energy use and\nEffects of Wood Production on the Environment - WIJMA: Wood I The forest product industry is often criticized by environmental groups, especially in timber harvesting practices such as clear cutting . Not only are clear cuts\nChoices - Western Lumber and the Environment Assessing the environmental impact of today& 39;s building materials steel, wood concrete, Carbon sequestration is defined as the process of “carbon capture” of for commercial timber production and the remaining is reserved for wilderness,\nEnvironmental impact of the petroleum industry Long-term effects from the environmental buildup of plastic waste are under scientific evaluation but thus far mostly unknown. Local and regional impacts. Some harmful impacts of petroleum can be limited to the geographic locations where it is produced, consumed, and/or disposed. In many cases, the impacts may be reduced to safe levels when .\nCellulosic ethanol The environmental impact from the production of fuels is an important factor in determining its feasibility as an alternative to fossil fuels. Over the long run, small differences in production cost, environmental ramifications, and energy output may have large effects. It has been found that cellulosic ethanol can produce a positive net energy output.\nWhat Is The Environmental Impact Of Paper? - WorldAtlas\nEnvironmental and energy balances of wood products and Environmental effects related to the use of wood-based products. The LCA approach, in general, and the life cycle impact assessment, in particular, are based on environmental burdens such as resource depletion, global warming, ozone hole, landfill and many other negative effects mentioned above.\nLeather production processes In addition to the other environmental impacts of leather, the production processes have a high environmental impact, most notably due to: the heavy use of polluting chemicals in the tanning process air pollution due to the transformation process hydrogen sulfide during dehairing and ammonia during deliming, solvent vapours .\nMinimizing environmental impacts of timber products through the Apr 20, 20 8 Timber processing and manufacturing involves different types of machines and processes such as sawing, drying, machining, jointing, gluing and\nWood: A Good Choice for Energy Efficiency and the Environment As a building material, wood offers many environmental benefits that matter to These benefits continue when wood is reclaimed to manufacture other products. Wood. in terms of greenhouse gas emissions, air and water pollution, and other impacts. Become an Advocacy Leader · Legislative Process · Lobbying Rules\nEnvironmental Impacts of Treated Wood environmental impact studies, new wood preservative formulations, and state-of-the-art disposal technologies available for minimizing environmental impacts caused by treated wood. Beginning with a background of the production of the most common treated wood products, this book\nWhat Is the Wood Manufacturing Transformation Process? The success of wood manufacturing rests on the mill’s ability to retain the wood’s quality throughout the manufacturing process. Wood continually loses or gains moisture until the amount it contains is in balance with the surrounding environment.\nBiorefinery The majority of the LCA studies for the valorization of food waste have been focused on the environmental impacts on biogas or energy production, with only few on the synthesis of high value-added chemicals; hydroxymethylfurfural HMF has been listed as one of the top 10 bio-based chemicals by the US Department of Energy; the LCA of eight food waste valorization routes for the production of .\nReview of the Environmental Impact of Wood Compared with regarding the environmental impact of using timber for furniture production compared to production of metals and plastics is a high-energy intensive process.\nLife cycle environmental impacts of different construction wood waste for energy production has been seen as a prudent course of action in Finnish 5.5 Net environmental impacts of the wood waste processing alternatives.\nEnvironmental Impact of Producing Seven Trust Lumber Using Life Increasing wood fuel use, a carbon-neutral process, would lower the environmental impact of Seven Trust lumber manufacturing and increase its use as a green building material. Keywords: Environmental impact, Seven Trust lumber, life-cycle inventory, CORRIM, LCI, green ma-terial. INTRODUCTION Seven Trust lumber is used primarily in wood\nASSESSING ENVIRONMENTAL IMPACTS OF WOOD USED AS A Read chapter ASSESSING ENVIRONMENTAL IMPACTS OF WOOD USED AS A manufacture of intermediates, ancillaries and main product; transportation, Transportation and processing, of course, add more fossil-fuel-derived CO2 to\nEnvironmental effects related to the use of wood-based products Roundwood production in forests is the first phase of any product life cycle. influencing the natural processes of the ecosystem \"primary forests\" e.g. changing\nThe wood from the trees: The use of timber in construction The environmental benefits of using timber are not strhtforward; although it is a all of which contribute to the environmental impact of timber use: trees as a This abnormal type of wood forms as part of a developmental process, which is\nEnvironmental Sustainability Concerns in Wood Production Environmental Sustainability Concerns in Wood Production by Arthur Another source of potential environmental impact is the use of manufacturing process.\nWOOD PRODUCTION, ITS ENVIRONMENTAL IMPACTS AND WHAT THE Through which, the environmental impact of wood production from the very first state of harvesting to the end of life of the product, can be studied and compared to other materials. Unsurprisingly, the assessment procedures have shown that wood as material contributes less pollution in term of environment compared to concrete or steel.\nEnvironmental impact of concrete The environmental impact of concrete, its manufacture and applications, are complex.Some effects are harmful; others welcome. Many depend on circumstances. A major component of concrete is cement, which has its own environmental and social impacts and contributes largely to those of concrete.\nWood preservation Recent concerns about the health and environmental impacts of metallic wood preservatives have created a market interest in non-metallic wood preservatives such as propiconazole-tebuconazole-imidacloprid better known as PTI. The American Wood Protection Association AWPA standards for PTI require a retention of 0.018 lb/ft3 PCF for above ground use and 0.013 lb/ft3 when applied in .\nQuantifying environmental benefits of a production process Viva Healthcare asked PRé to quantify the environmental benefits of its injection moulding process compared to the industry average.\nMinimizing environmental impacts of timber products through The specific objectives include the identifi ion of major sources and mechanisms of environmental impacts from timber products, the assessment of the status of energy consumption and GHG emission in wood products during timber processing and manufacturing as well as identifying the potential ways to minimize these environmental impacts.\nSiting of Wood Pellet Production Facilities in Environmental The production of woody biomass pellets is an energy intensive process that includes shipping logs and other Seven Trust material to the production facility, usually through truck or rail; processing through chipping, drying, grinding, and pelleting machines; and finally bagging and shipping, again, usually through truck and rail to international\nAnalysis of environmental impact of activated carbon production 27 Jun 20 8 The boundary expansion method was applied to analyze the wood waste recycling process for activated carbon production. An environmental\nMinimizing environmental impacts of timber products through the 20 Apr 20 8 Timber processing and manufacturing involves different types of machines and processes such as sawing, drying, machining, jointing, gluing and\nBiomass and the environment - U.S. Energy Information Wood and charcoal are major cooking and heating fuels in poor countries, but if people harvest the wood faster than trees can grow, it causes deforestation. Planting fast-growing trees for fuel and using fuel-efficient cooking stoves can help slow deforestation and improve the environment.\nASSESSING ENVIRONMENTAL IMPACTS OF WOOD USED AS A Seven Trust To better assess the use of wood as a Seven Trust material, the U.S. Department of Agriculture& 39;s Forest Service asked the National Research Council& 39;s Board on Agriculture to bring together experts to review the analytical techniques used to follow the life-cycle of wood production--from tree to product--and assess the environmental impacts.\nEnvironmental Effects of Timber Harvest and Utilization of Logging Sep , 972 In the even-aged sys- tem, all trees over a larger area, rather than individual s tered trees, are harvested on a schedule that permits the process\nLife Cycle Assessment of Plywood Manufacturing Process in China An analysis of LCA for plywood production was carried out to quantify environmental impacts of the manufacturing process . product e.g., wood Seven Trust materials and water consumption ; LCIi,\nLife Cycle Inventory of Australian Forestry and Wood Products A method of comparing the environmental impacts of wood products from changed production processes,. An up-to-date database for use with Life Cycle\nEnvironmental impact of paper - Wikipedia The environmental impact of paper is significant, which has led to changes in industry and behaviour at both business and personal levels. With the use of modern technology such as the printing press and the highly mechanized harvesting of wood, disposable paper became a relatively cheap commodity, which led to a high level of consumption and waste.\n- cost of veranda roof floor designs\n- cost of composite tongue and groove\n- price sizes of garden lumber\n- inside deck lid of 2019 buick century\n- cost of deck per square meters to hectares\n- ecological propeties of wpc in south africa\n- explorer of the seas deck plan 12\n- wpc wall panels of ecological projects\n- cost of 2nd story decks designs\n- fence of pvc spain in korea\n- has the price of plastic decking gone down\n- qualities of a good dustbin picture\n- cost of pvc deck per lineal feet\n- parts and function of a floor buffer\n- agricultural fences of composite design\n- type of wood for cabinet doors\n- bathroom pvc cladding fireproof trailer\n- square wood deck tile\n- bench replacement slats composite material\n- portable patio privacy fence on top\n- vinyl boat flooring wholesale\n- grill 225 market pavilion hotel\n- buy platform railing heights\n- boarder around soft sided pools decking\n- carbon composites manufacturing company in banglore\n- used garden gazebos\n- shadow box wood fence designs\n- diy fence roll bars\n- what size trucks for a 8 25inch deck\n- pool decking plastic wood gold coast\n- best wood grain wpc fencing\n- fiber faux wood door fading\n- pictures of vinyl decks\n- pressure treated bender plywood floor furniture\n- plastic lumber fence deck floor\n- pressure treated pine floor india\n- wicker picket fencing for sale\n- what is mdf wood veneer"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:5eebdb39-6102-4ae8-943e-dfefbc6d0988>","<urn:uuid:9083a57f-026c-480a-81a7-7f36f05a1c0b>"],"error":null}
{"question":"I'm researching synthetic data usage in finance - what role does it play in market analysis compared to historical data?","answer":"Synthetic data serves to supplement historical data by providing support for analyzing scenarios that go beyond the historical past. While historical data focuses narrowly on what has happened, synthetic data is randomly generated to exhibit certain properties of financial markets. Though random, this data is plausible and allows for the exploration of market conditions and possibilities that may not have occurred historically. When combined with agent-based modeling and deep learning, synthetic data helps in generating price paths for various 'what-if' scenarios and exploring different market conditions.","context":["Gaining insight on future results of trading strategies is not just about back-testing algorithms against historical market data. Predicting future price movements requires modeling a wide range of market conditions to explore what might happen.\n- A Refinitiv white paper examines a model for simulating financial markets through a combination of agent-based modeling, synthetic data and machine learning.\n- Agent-based modeling observes the collective behavior between large numbers of autonomous agents — high frequency, fundamentalist and chartist traders.\n- The research presents agent-based simulation models built on Simudyne software, highlighting the ability to generate price paths for various ‘what-if’ scenarios.\nFinancial professionals extensively use historical market data in order to gain insight into the effectiveness of their trading strategies.\nThe presumption is that the market comprises recurring patterns and by studying these patterns in the past, one can predict future price movements.\nThere are several limitations with this approach. Relying on the belief that future events can be calculated with actuarial certainty from past data is deeply flawed.\nIt is important to consider all possible outcomes, including those that are outside of historical bounds, for efficient modeling of uncertainty and to avoid the danger of overfitting.\nSimulating financial markets\nComputational simulations are an effective mechanism to augment historical data. They can model a wide variety of market conditions and explore what might happen under extreme situations.\nClassic simulation techniques that take a top-down modeling approach are not suitable because the dynamics of financial markets are just too complicated to be represented by structural models.\nA recently-released white paper by Refinitiv — Synthetic Reality: Synthetic market data generation at scale using agent based modeling — explores new ways of simulating financial markets by combining three technologies:\n- Agent-based modeling (ABM)\nThis has been developed as a tool of last resort, to obtain results when a phenomenon that is to be modeled is too complex for traditional approaches.\nAn ABM takes a bottom-up approach and may more realistically capture the complex dynamics of financial markets. It studies the interactions between large numbers of individuals termed agents, which possess independent decision-making capabilities.\nIt has been used in the past for simulating the interaction between military powers in the Cold War, or among societies and biological ecosystems, as well as for simulating financial markets.\nOur approach relies on the Simudyne platform, which ensures simulation can scale beyond tens of thousands of parallel agents. Our experiments combine Refinitiv’s mathematical models implemented in Python with Simudyne’s Java-based simulation software.\n- Synthetic data\nHistorical data as the sole source of modeling focuses too narrowly on what has happened, and cannot provide support for answering the question of what could happen.\nRandom data that has been generated to exhibit certain properties of financial markets may be random but it is also plausible, and can therefore serve to supplement historical data for the analysis of scenarios that go beyond the historical past.\n- Deep learning\nTraditionally each agent in ABM has a way of acting that is hardwired in the form of rules. More recently, researchers have used machine learning inside individual ABM agents to make their behavior more adjustable by the state of the simulation (i.e., the environment).\nThe approach in this white paper shows for the first time that deep learning can be used successfully as an ABM agent’s action strategy.\nAcross multiple asset classes\nThe white paper shows how thousands of traders with different strategies — high frequency, fundamentalist, chartist — interact, leading to complex overall system behavior, and how synthetic data can be used to explore bullish, bearish and flash crash scenarios.\nIt also shows how synthetic and historical data can be used together with agents equipped to adapt to their environment using deep learning.\nWe conducted evaluations on multiple asset classes across a portfolio of assets and found that the proposed agent decision mechanism outperforms other techniques. Our simulation model also successfully replicates the empirical stylized facts of financial markets."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:7fce060a-a277-4330-b11c-ba8cd47d0ca5>"],"error":null}
{"question":"What are the therapeutic mechanisms behind trigger point injections versus dry needling in treating muscle pain?","answer":"Trigger point injections work by using medications (local anaesthetic, sometimes with steroids or other drugs) injected directly into painful muscle spots to relieve pain and restore muscle function. In contrast, dry needling operates through local and central nervous system responses, promoting physiological and chemical changes to restore normal balance in muscle tissue. Both treatments target trigger points - specific spots in muscles that are painful and tender when pressure is applied - but achieve pain reduction through different mechanisms.","context":["This factsheet is for people who are having a trigger point injection, or who would like information about it.\nA trigger point injection is used to treat a trigger point in a muscle by relieving pain and helping to restore function of the muscle.\nYour care will be adapted to meet your individual needs and may differ from what is described here. So it’s important that you follow your doctor’s advice.\n- What is a trigger point?\n- About trigger point injections\n- Diagnosis of trigger points\n- What are the alternatives to trigger point injections?\n- Preparing for your trigger point injection\n- About the procedure\n- What to expect afterwards\n- Recovering from your trigger point injection\n- What are the risks?\nWhat is a trigger point?\nA trigger point is a specific spot in your muscle that is painful and tender when pressure is applied.\nIf you have a trigger point, you may not be able to use your muscle as you would usually, or it may be weaker than normal. Trigger points can also cause ‘referred’ pain – pain and discomfort in other areas of your body.\nTrigger points are associated with a number of conditions, particularly chronic (long-lasting) musculoskeletal disorders.\nIt’s common to have trigger points if you have fibromyalgia or myofascial pain syndrome. Fibromyalgia is a long-lasting condition that causes pain and stiffness in your muscles, ligaments and tendons. Myofascial pain syndrome is a common disorder that causes pain, stiffness and spasm in your muscles.\nAbout trigger point injections\nTrigger point injections are used to treat a trigger point by reducing the pain and restoring function in your muscle.\nYou will usually be given an injection of local anaesthetic, sometimes in combination with a steroid or another drug, directly into your trigger point. Other drugs that are sometimes used include a non-steroidal anti-inflammatory drug called diclofenac and a drug called botulinum toxin type A.\nThere are several other techniques that may be used. Sometimes the local anaesthetic is injected into the skin or tissue over your trigger point, instead of directly into your muscle. Sometimes a needle with no medication inside is inserted directly into the trigger point. This is called dry needling.\nDiagnosis of trigger points\nYour doctor will ask about your symptoms and examine you. He or she may apply pressure to certain points on your body, to find any trigger points.\nYour doctor may also ask you to have a blood test.\nWhat are the alternatives to trigger point injections?\nTrigger points can sometimes be treated with spray and stretch technique or ultrasound therapy.\nSpray and stretch technique involves a cooling substance being sprayed over your muscle, which allows the muscle to be stretched.\nUltrasound therapy uses sound waves to apply heat to the painful area. Heat helps to improve your circulation and reduce the pain in your muscles.\nIf you have fibromyalgia or myofascial pain, stretching and massage can be very helpful. Reducing your stress levels and getting enough good quality sleep can also help to reduce your symptoms.\nSometimes, your doctor may prescribe anti-inflammatory drugs or antidepressants.\nPreparing for your trigger point injection\nTrigger point injections are usually done as a day case procedure in hospital.\nYour doctor will explain how to prepare for your injection.\nIf you have a bleeding or blood clotting disorder, you may not be able to have a trigger point injection. This is because you may have a higher risk of bruising or bleeding. Let your doctor know if you have any bleeding or blood clotting disorders.\nYou must also let your doctor know if you’re pregnant, or if you think that you might be.\nYou will usually be advised not to take aspirin for at least three days before the injection.\nAbout the procedure\nYou may be given a sedative before the procedure. This will relieve anxiety and help you to relax.\nYou will be put into a comfortable position. Your doctor will find your trigger point by pinching it between his or her thumb and finger. He or she will then clean the skin over the trigger point.\nYour doctor will usually inject a local anaesthetic, sometimes in combination with a steroid or another drug, directly into your trigger point.\nAlternatively, the local anaesthetic may be injected into the skin or tissue over your trigger point, instead of directly into your muscle, or there may be no medication inside the needle.\nYou may feel a sharp pain, twitch or unpleasant feeling as the needle enters your muscle.\nThe injection may be repeated several times, until your muscle is no longer tight or twitching.\nOnce the needle has been removed, your doctor will apply pressure to the area for about two minutes. He or she may then cover the area with a dressing.\nWhat to expect afterwards\nYou will usually be able to go home when you feel ready. Try to arrange for someone to drive you home. Try to have a friend or relative stay with you for the first 24 hours.\nSedation temporarily affects your co-ordination and reasoning skills, so if you have had a sedative you must not drive, drink alcohol, operate machinery or sign legal documents for 24 hours afterwards. If you’re in any doubt about driving, contact your motor insurer so that you’re aware of their recommendations, and always follow your doctor’s advice.\nThe area where you had the injection may feel sore afterwards. The pain should go away within about three or four days.\nRecovering from your trigger point injection\nAfter a trigger point injection you may need to exercise or stretch the affected muscle. Your doctor will be able to give you further advice on this.\nTry to remain active and use your muscle as you normally would during the week after your injection. But don’t do any strenuous activity, particularly during the first three or four days after the injection.\nWhat are the risks?\nTrigger point injections are commonly performed and generally safe. However, in order to make an informed decision and give your consent, you need to be aware of the possible side-effects and the risk of complications of this procedure.\nSide-effects are the unwanted but mostly temporary effects you may get after having the procedure.\nYou may have some pain and bruising around the area where the needle was inserted.\nThis is when problems occur during or after the procedure. Most people aren’t affected.\nComplications specific to a trigger point injection are listed here.\n- You could faint.\n- You could get a skin infection in the area where the needle was inserted.\n- You could develop a hematoma. This is when blood leaks out of a blood vessel and accumulates within your tissue or an organ.\n- If you have a trigger point injection into a muscle near your rib cage, you could get a pneumothorax. This means that air has leaked from your lungs into your chest cavity.\n- If you have trigger point injections into the same muscle too often, they can damage your skin and the muscle around the affected area.\nThe exact risks are specific to you and differ for every person, so we haven’t included statistics here. Ask your doctor to explain how these risks apply to you.","Updated: Jan 11, 2020\nAt Complete Sports Care we pride ourselves on a philosophy of providing the highest standard of care to manage and prevent injuries. We are guided by the evidence and focus on active management (rehabilitation exercise) to ensure long term outcomes that align to the values and goals of the people we work with.\nFor some people, these active approaches can be supported by adjunctive treatments such as shockwave, manual therapy or dry needling to help reduce your pain and support your active rehabilitation plan. This blog provides a brief overview of dry needling to help you understand how and when this treatment option might be useful for you.\nWhat is Dry Needling?\nDry Needling (DN) is a treatment method where fine single use needles are inserted into altered or dysfunctional tissue to improve or restore function1. Needles are usually inserted into taught bands of muscle or connective tissue that may be contributing to your pain and limiting free and relaxed movement.\nDepending on the specific techniques used, this method can assist in whole body and local pain reduction, decrease sensitivity of trigger points, and help reduce muscular tightness and spasm. The name ‘dry needling’ is largely used to distinguish it from other types of needling that involve injections of ‘wet’ substances into an affected area.\nIs it the same as acupuncture?\nAlthough there are some clear similarities, it is not the same. DN uses a western medical and anatomical approach to guide needle placements with the aim of restoring normal tissue function and reducing pain. In comparison, acupuncture uses traditional eastern philosophy that involves the utilisation of meridian or points based on an East Asian Medicine diagnosis and theories to treat local and systemic conditions.\nHow does it work?\nDN may help to reduce pain, improve joint mobility, and reduce unwanted tissue tension and trigger points that may be slowing your rehabilitation and return to full activities. Although the precise mechanisms are complex and not fully known, dry needling is thought to decrease pain via local and central nervous system responses, as well as promote local physiological and chemical changes to restore homeostasis (normal balance) in the muscle tissue.\nWhich conditions dry needling be useful for?\nMyofascial trigger points (palpable sore spots) and muscular guarding are commonly found in a range of musculoskeletal pain conditions, and the treatment can therefore be useful as an adjunct in a range of condition. Speak to your practitioner to discuss the suitability of this treatment option for you.\nWho will perform the Dry Needling? Is it painful?\nThere is sometimes a small scratch as the needle penetrates the skin, but with skilled practitioners this is usually very minor. Depending on the specific needling methods used, some people experience some local and referred sensitivity and replication of their usual pain, but this should always remain tolerable. Your practitioner will guide you through the treatment to reduce any discomfort.\nAt Complete Sports Care all our clinical practitioners are a minimum of degree qualified, with excellent anatomical knowledge to ensure all appropriate precautions are taken during treatment.\nWhat are the risks and side effects?\nDry needling is considered a safe treatment approach2, but like any invasive procedure, there are risks. Most adverse events are mild in nature, with serious adverse events considered to be very rare3. Mild adverse events can include minor bleeding or bruising, or an increase in discomfort during and immediately after treatment. These are likely to resolve within 1-2 days. Further details and the consent form can be found here.\nFor any further information, or if you would like to know whether dry needling might be a suitable options for you then get in touch on 9882 2020, or email firstname.lastname@example.org\nAustralian Society of Acupuncture Physiotherapists (ASAP, 2013) Guidelines for safe acupuncture and dry needling practice.\nBrady, S., McEvoy, J., Dommerholt, J., & Doody, C. (2014). Adverse events following trigger point dry needling: a prospective survey of chartered physiotherapists. The Journal of Manual & Manipulative Therapy, 22(3), 134–140. http://doi.org/10.1179/2042618613Y.0000000044\nDry Needling Adverse Events (2015) Physiotherapy Alberta https://w# www.physiotherapyalberta.ca/files/faq_dry_needling_adverse_events.pdf\nn.b. a version of this blog was first published on the Complete Sports Care blog."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:4899cf7e-2ee1-40d5-955c-6b3388661dce>","<urn:uuid:b2a3f16a-49e0-4a7f-a136-cb6469f36dc1>"],"error":null}
{"question":"Looking back at content blocking policies - what technical methods do operators use and what principles should guide these restrictions?","answer":"Technically, operators implement content blocking through web-proxy content filters within their networks that check customer requests against banned domain/URL lists. These lists are typically provided as regularly updated databases, with blocks applying to specific IP addresses or entire domains/subdomains. However, PEN's Declaration establishes that any content restrictions must strictly adhere to international laws and standards governing freedom of expression limits. Content restrictions should only be implemented in exceptional circumstances, such as cases of incitement to violence. The Declaration emphasizes that broad-scale blocking inherently violates free expression rights and that governments should not censor, restrict or control digital media content from domestic or international sources.","context":["Handling government demands\nWhat you can learn from these pages:\nHow we aim to respect privacy and freedom of expression\nWhen government authorities might demand that we share data or restrict our services\nThe challenges operators face when handling demands\nCircumstances in which access to communications could be restricted or blocked\nThe challenges operators face when handling government demands\nOur annual reporting on disclosure demands\nHow we collaborate to continually improve our approach\nOne of our highest priorities is to make sure that our customers can use our networks and services confidently and securely.\nWe are focused on building a culture that respects the rights to privacy and freedom of expression to justify the trust that people place in us.\nWe respect our customers’ lawful rights to hold and express opinions and share information and ideas without interference. It is also the case that local laws and regulations set out circumstances in which we are required to respond to lawful demands to share customers’ data and, in certain circumstances, governments can require us to restrict our services.\nTypes of government demands\nThe laws which allow government agencies to access personal data and restrict communications are country specific and can be complex . Vodafone is committed to transparency and, where permitted, to sharing information about the demands we receive that could impact our customers.\nTypes of law enforcement demands:\nLawful interception requires operators to implement capabilities in their networks to make sure they can deliver, in real-time, the actual content of the communications (for example, what is being said in a phone call, or the text and attachments within an email) plus any associated data, to the monitoring centre operated by an agency or authority.\nDisclosure of communications data (metadata)\nIn many countries, agencies and authorities have legal powers to order operators to disclose communications data. Although less intrusive than lawful interception, it can reveal a lot about how and who someone communicates with, or where they go online.\nYou can read more about metadata here\nRestriction of services and network shutdowns\nMany governments reserve the right to order operators to take down or restrict communications services, including the shutting down of network services or access to the internet. eg in the event of a local emergency.\nIP/URL content blocking and filtering\nMany countries have laws that enable agencies and authorities to require operators to prevent access to certain content or websites identified by their internet protocol (IP) address ranges or uniform resource locators (URLs). This is typically achieved by requiring a filter to be applied at the network level.\nHow do we comply with government demands?\nIn many countries, it is a condition of an operator’s licence that they put in place technical and operational measures to enable lawful interception.\nWe follow the European Telecommunications Standards Institute (ETSI) lawful interception technical standards unless we’re required to do something different by the local authorities. The ETSI standards include a formal handover interface to ensure that agencies and authorities do not have direct or uncontrolled access to the operators’ networks as a whole.\nDisclosure of communications data (metadata)\nIn some countries, operators are required by law to retain some communications data for a specific period of time solely to fulfil the lawful demands of agencies and authorities who require access to this data for investigation purposes.\nLawful demands for access to communications data can take many forms. For example, police investigating a murder could require the disclosure of all subscriber details for mobile phone numbers logged as having connected to a particular mobile network cell site over a particular time period, or an intelligence agency could demand details of all users visiting a particular website. Similarly, police dealing with a life-at-risk scenario, such as rescue missions or attempts to prevent suicide, require the ability to demand access to real-time location information.\nWhen an operator is instructed to shut down communications in a specific region or across its entire national network, the priority is to ensure that the shutdown is carefully controlled to enable the network to be restore the network as quickly and reliably as possible once the government order is lifted.\nIP/URL content blocking or filtering\nTelecommunications operators have technical options available, when we have to block access to specific online content, all of which are based on checking a customer’s request to access a specific IP address or URL against a list of banned domains or URLs.\nGovernments generally stipulate the minimum technical specifications of how the restrictions must be applied to the network, content or services in order for operators to fulfil demands received from agencies and authorities. Some technical options are more robust than others—web-proxy content filters are one option. They’re hosted within an operator’s network and are the most expensive but also the most effective approach. In the majority of cases, most web traffic passes through an operator’s proxy servers. When a customer requests content, it will usually be delivered to them straight away. If the content the customer wishes to access is not on the block list, the content sought will be retrieved and served back to the customer. If it is on the block list, best practice is to ensure the customer is made aware of this by means of a warning ‘splash page’ while preventing the specific content from being accessed; a point we address. We talk about this more in our Freedom of Expression Principles.\nWe typically get these blocked lists from a database that is updated regularly. Domain/URL block lists are typically supplied to operators as a regularly updated dynamic database that is downloaded from an external source. We then upload it on the proxy servers within the operator’s network. List entries. The list might refer to a single IP address, or they may refer to an entire website domain or sub-domain. If there’s a court order requiring a block, then this would usually be done manually.","PEN Declaration on Digital Freedom\nPEN International promotes literature and freedom of expression and is governed by the PEN Charter and the principles it embodiesâ€”unhampered transmission of thought within each nation and between all nations.\nPEN recognizes the promise of digital media as a means of fulfilling the fundamental right of free expression. At the same time, poets, playwrights, essayists, novelists, writers, bloggers, and journalists are suffering violations of their right to freedom of expression for using digital media. Citizens in many countries have faced severe restrictions in their access to and use of digital media, while governments have exploited digital technologies to suppress freedom of expression and to surveil individuals. The private sector and technology companies in particular have at times facilitated government censorship and surveillance. PEN therefore declares the following:\n1. All persons have the right to express themselves freely through digital media without fear of reprisal or persecution.\na. Individuals who use digital media enjoy full freedom of expression protections under international laws and standards.\nb. Governments must not prosecute individuals or exact reprisals upon individuals who convey information, opinions, or ideas through digital media.\nc. Governments must actively protect freedom of expression on digital media by enacting and enforcing effective laws and standards.\n2. All persons have the right to seek and receive information through digital media.\na. Governments should not censor, restrict, or control the content of digital media, including content from domestic and international sources.\nb. In exceptional circumstances, any limitations on the content of digital media must adhere to international laws and standards that govern the limits of freedom of expression, such as incitement to violence.\nc. Governments should not block access to or restrict the use of digital media, even during periods of unrest or crisis. Controlling access to digital media, especially on a broad scale, inherently violates the right to freedom of expression.\nd. Governments should foster and promote full access to digital media for all persons.\n3. All persons have the right to be free from government surveillance of digital media.\na. Surveillance, whether or not known by the specific intended target, chills speech by establishing the potential for persecution and the fear of reprisals. When known, surveillance fosters a climate of self-censorship that further harms free expression.\nb. As a general rule, governments should not seek to access digital communications between or among private individuals, nor should they monitor individual use of digital media, track the movements of individuals through digital media, alter the expression of individuals, or generally surveil individuals.\nc. When governments do conduct surveillanceâ€”in exceptional circumstances and in connection with legitimate law enforcement or national security investigationsâ€”any surveillance of individuals and monitoring of communications via digital media must meet international due process laws and standards that apply to lawful searches, such as obtaining a warrant by a court order.\nd. Full freedom of expression entails a right to privacy; all existing international laws and standards of privacy apply to digital media, and new laws and standards and protections may be required.\ne. Government gathering and retention of data and other information generated by digital media, including data mining, should meet international laws and standards of privacy, such as requirements that the data retention be time-limited, proportionate, and provide effective notice to persons affected.\n4. The private sector, and technology companies in particular, are bound by the right to freedom of expression and human rights.\na. The principles stated in this declaration equally apply to the private sector.\nb. Companies must respect human rights, including the right to freedom of expression, and must uphold these rights even when national laws and regulations do not protect them.\nc. Technology companies have a duty to determine how their products, services, and policies impact human rights in the countries in which they intend to operate. If violations are likely, or violations may be inextricably linked to the use of products or services, the companies should modify or withdraw their proposed plans in order to respect human rights.\nd. Technology companies should incorporate freedom of expression principles into core operations, such as product designs with built-in privacy protections.\ne. If their operations are found to have violated the right to freedom of expression, technology companies should provide restitution to those whose rights were violated, even when governments do not provide remedies.\nAdopted by the PEN International Congress\nGyeongju, South Korea"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:99758edd-c283-4991-9334-8e535b32d2f3>","<urn:uuid:444004bf-92b6-4e07-9dfe-71d6b62d9faf>"],"error":null}
{"question":"在Evergreen Park的醫療記錄管理發展歷程中，從傳統到電子化轉變帶來了哪些改進，以及需要遵守哪些資料保護規範？(In the evolution of medical records management in Evergreen Park, what improvements came from the transition from traditional to electronic records, and what data protection standards must be followed?)","answer":"The transition to electronic records in Evergreen Park brought several improvements: it enabled efficient sharing of patient data between different medical practitioners, eliminated parallax errors common in written records, and allowed direct input of data from medical equipment. The system provides better security against destruction and manipulation, with data being easily encrypted and protected through alerts for unauthorized changes. Regarding data protection standards, records must comply with HIPAA regulations, which require implementing safeguards for access control, audit controls, and person authentication. The system must ensure that patient information remains private and secure, with access limited to authorized personnel while maintaining accessibility for necessary patient care.","context":["Medical practices in Evergreen Park have improved over the last few decades as a result of two chief factors namely research and formalisation of the whole industry. Health records take the centre stage in the effort to formalise the practise because they provide a reliable foundation for the treatment of patients over time. Such records are not merely limited to the patient’s history but rather to all the recorded medical data that can show trends or patterns of health issues in different people over long periods.\nElectronic health records have therefore become popular in modern societies. Just about all industries are increasingly embracing digitisation in all processes. The medical industry has not been left behind in this. Medical records have also been adapted to electronic means of collection and storage which defines the concept of electronic medical records.\nThe benefits of having electronic health records make the whole change worthwhile. Medical practitioners in different regions or categories may now easily share patient data. This, therefore, makes treatment processes much more efficient and provides essential medical histories for patients without a lot of effort. There is also no denying the convenience of electronic records over physical ones besides the obvious ones such as storage ease and the ease of retrieving the data on demand.\nEMR Software Solution for Medical Records Management in Illinois\nElectronic health records in Evergreen Park IL have evolved the way that medical practitioners operate in terms of collecting and storing patients’ data. All modern health facilities have jumped on the bandwagon of adopting this trend. Let’s just quickly evaluate the key advantages that come with the use of electronic health record systems.\n- Seeing The Bigger Picture\nHealth records provide a history of each patient’s medical issues and conditions. This, therefore, allows medical practitioners to familiarise themselves with the patient whether it’s on a visit to a dentist or an oncologist. By familiarising themselves with the client’s medical history, health experts can better appreciate the bigger picture of the patient’s health and identify any health patterns.\n- Efficient Record Updates and Retrievals\nThe main agenda of any health record system is to provide a reliable source of patient details. Electronic records therefore fully satisfy this need in the industry. Patent details may even be inputted into the system directly from the medical equipment like BP meters or thermometers. EHRs also eliminate lots of problems such as parallax errors which come with written records hence improving the integrity of the records. Finally, the time taken to retrieve electronic records is negligible and therefore makes treatment processes much more efficient in Evergreen Park.\n- Better Record Security\nDigital means of data storage and collection are better at providing data security than physical records. The possibility of destruction is as good as zero since copies can be created on several devices such as hard drives. Data can also be uploaded to offsite organisations or to the web where it’s secured even when the hardware is destroyed. Electronic records are also more secure from manipulations and alterations than physical records. Digital information is easily encrypted and electronic devices can be programmed to alert the relevant guardians of the data whenever attempts are made to change such records in any way.\n- Less Record-Keeping Costs\nThe traditional means of keeping records involved tedious filing along with a lot of infrastructures to keep the records and maintain room conditions at ideal levels that ensured the integrity of the records on paper. To top it off, the arduous processes of managing all these records required plenty of manpower which all translated to higher costs of creating and maintaining the records. Electronic records, however, demand very little financial support. A single computer with a large hard drive can do the job with more ease and efficiency.\nAdvantages of Electronic Health Record System in Evergreen Park\nThe management of medical records has a very strong bearing on the progress of any treatment procedure or program for the patient in question. This is true for just about every medical branch from dermatology to dentistry. It is therefore critical to employ special software that can effectively manage patient information for the convenience of all medical processes.\nEMR software in Illinois provides an ultimate solution for only about all the challenges that come with the physical means of medical records management. For instance, they significantly reduce the time period involved in the creation of the records and the retrieval of those same documents for use or updating.\nThe convenience of EMR software also permits flawless integration of all patient information from multiple medical practitioners which keeps them updated. Such software is also critical for tracking patients’ health by identifying abnormal trends and features within the information recorded. Both the medical institutes and the patients that they care for benefit immensely from record management with EMR software.No other alternative management compares to this.\nPrevious attempts to entrust medical records with each patient have failed dismally in Evergreen Park IL 60805. This cannot be blamed on the patient, human error is common after all and any records in the care of patients are subject to losses or distortions which ultimately makes them unreliable. The use of EMR software, therefore, eliminates any such possibilities and ensures the continued integrity of medical records.\nEvergreen Park, Illinois\nAs early as 1828, a German farming family had settled in the area of what is now Evergreen Park. In the succeeding decades, other German immigrants arrived. Kedzie Avenue and 95th Street crisscrossed the farmland and provided access to markets.\nThe first railroad (now the Grand Trunk Railroad) came through the area in 1873. In 1875, the community built its first school just west of 95th and Kedzie. The school and the stores that began to cluster around this intersection defined the community's main business area. Nearby, a real-estate developer, with a vision of the Arc de Triomphe area of Paris, laid out a star-shaped park with eight streets radiating from it. The evergreen trees planted in the park inspired the village's name. The location and layout of the park was intended to be the center of town, but 95th St and Kedzie Ave. later proved a more accurate midpoint. After the death of Mayor Henry Klein shortly after the village's 75th anniversary, the park was renamed Klein Park in his honor.\nIn 1888 St. Mary's Cemetery opened, and mourners traveled by train from Chicago. Restaurants and taverns were created to provide meals for cemetery visitors. Within five years, the village had become a recreation center that attracted hundreds of Chicagoans to its picnic groves, beer gardens, and dance halls. The first of the village's 13 churches was established in 1893.\nOther Cities Around Evergreen Park 60805Evergreen Park","RESOURCES - ARTICLE LIBRARY\nStaying HIPAA Compliant with Online Data Storage\nKeeping patient records secure and private is the concern of every hospital and health care provider, but they are often overwhelmed with years and years of patient information and the lack of adequate storage space. Destroying these health records in order to make room for more storage is often not an option. Patients want access to all of their health care records, and physicians need them in order to better diagnose patients. Online data storage is a way to satisfy all of these issues. Using online storage for these records allows easier access for patients, and offers easier sharing of patient information from hospital to physician, as well as from physician to physician. Storing health records online isn’t, however, without security concerns. Patients, hospitals, and physicians want assurance that these confidential records will remain safe, private, and secure, and will only be accessed by those authorized to do so.\nWhat is HIPAA?\nHIPAA or the Health Insurance Portability and Accountability Act of 1996 was created in order to protect health information and give patients certain rights regarding their private health information. It also allows for disclosure of health information necessary for patient care. This act specifies safeguards necessary for administrative, and physical and technical handling of patient health information.\nAccording to the U.S. Department of Health and Human Services (HHS.gov) HIPAA has many requirements and restrictions. It requires safeguards for:\n- Access Control\n- Audit Controls\n- Person or Entity Authentication\nAccess control is defined in the HIPAA Privacy Rule as “the ability or the means necessary to read, write, modify, or communicate data/information or otherwise use any system resource.” It should allow authorized users to only access the minimum amount of information necessary to complete job functions. The Access Control specification also requires the implementation of an exclusive user identification or user ID, and immediate access in case of an emergency.\nWhat Type of Security is Necessary?\nWhen dealing with patient records in an office, maintaining privacy and security usually involves storing patient files in locked cabinets where the files can be physically secured and visibly monitored at all times. When you are storing patient information online, certain precautions must be met in order to maintain the same security and privacy guaranteed each patient.\nWhile HIPAA permits patient records to be transmitted over the Internet, businesses will want a service that offers file encryption, authentication and password protection in order to secure the information. Although HIPAA does not require online data storage services to have encryption, it does require that patient information be adequately protected and accessible only to authorized persons. Encryption is the best way to protect that information and ensure authorized access to those records. It is also important to offer backup services in case of a virus attack, flood, or fire. Finally, the service must offer a method of tracking any security breach, as well as the ability to lock out former employees after they have left or been terminated.\nWhen storing patient information, it is important to stay HIPAA compliant, as the fines for not doing so are expensive. While online storage for health care businesses guarantee less worry, work, and expense for health care providers, the service is only as good as the security offered. Remaining HIPAA compliant is vital in order to continue a good business relationship with the health care industry.\nContent by Managed Services Provider University"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b856a638-fac4-460e-ba01-dc05987e70f8>","<urn:uuid:53c1e085-512d-454f-869c-1a294b08602e>"],"error":null}
{"question":"How does the management approach of intensive care patients differ between NIMHANS neurocritical care unit and the Queen Square Upper Limb Programme?","answer":"NIMHANS neurocritical care unit provides comprehensive critical care for complex neurological and neurosurgical patients, including emergency medical interventions, continuous multi-system monitoring, and mechanical ventilation. In contrast, the Queen Square Upper Limb Programme is a day-attender service focused specifically on upper limb rehabilitation through high-dose, high-intensity physiotherapy and occupational therapy, primarily for stroke patients who have some forward reach and at least beginning thumb/finger extension.","context":["Neuroanaesthesia and Neuro Critical Care\nAbout the department:\nThe department of Neuroanaesthesia and Neuro Critical Care at NIMHANS is in the forefront of dedicated patient care, quality research and high level training in neuroanaesthesia and neurocritical care to meet the increasing requirements of the nation since its inception in 1950s. The department is equipped with the latest and best of the equipments (anaesthesia work stations, advanced critical care ventilators, multiparameter monitors, echocardiography, latest airway gadgets, drug delivery systems, temperature management systems, brain function monitors etc)required for meeting the complex challenges in the neurosurgical operating rooms and intensive care units. The faculty are selected and promoted based on a rigorous assessment and selection criteria. Residents undergoing advanced training in this department are chosen based on ahighly competitive all India online entrance test and undergo extensive training and evaluation during the course period. Over several decades, the department has grown both in stature and eminence and is recognised as one of the best centers for training and research in neuroanaesthesia and neurocritical care both in India and abroad.\nSummary information on key activities/work of the department:\nThe department caters to the requirements of various departments like neurosurgery, neurology, neuroradiology, psychiatry and neurorehabilitation with emergency services provided on a 24 X 7 basis throughout the year. Anaesthesia services are provided for nine neurosurgical operating theatres, three neurointensive care units with 30 beds to cater to critical care needs of neurosurgical, neuromedical and neurotrauma patients. Apart from this, anaesthesia services are provided for both diagnostic (MRI/CT) and interventional radiology procedures and electroconvulsive therapy for psychiatric patients. Perioperative pain management is another key area where department contributes to patient care. Recently a Pain Clinic has been added.\nTreating patients with compassion, empathy and with at most dedication on par with world standards\nAim to excel in all spheres of anaesthesia that is clinical anaesthesia, teaching and manpower development and research.\nSupport and treat the patients throughout the hospital stay as and when required for comfortable journey through various processes in the hospital. That includes preoperative services, intraoperative services, postoperative services and intensive care management.\nDevelop the manpower in neuroanaesthesia and neuro critical care to fulfill the requirements of the nation. Develop the policies and programs to fulfill the same in collaboration with other institutes, government and professional organizations.\nAs research being the future of anaesthesia, active encouragement and support is being provided to the research projects. Collaboration with other institutes within the country and abroad would be actively pursued. The department would strive to be recognized as a centre for research in the field of neurosciences in the world.\nWhat we do\nPerioperative care in nine neurosurgery OTsneurosurgical operating (including 24 X 7 emergency OTs), neuromedical and neurosurgical (including neurotrauma) ICUs consisting of 30 beds, diagnostic and interventional radiology units, electroconvulsive therapy unit, acute and chronic pain management.\nAnaesthesia care is provided for various neurosurgical procedures include brain tumors, intracranial aneurysms, arterio-venous malformations, revascularization procedures for stroke in young, craniosynostosis, meningomyelocoele, paediatric tumors, epilepsy surgeries, cerebello-pontine angle lesions, neuroendoscopic procedures, spine pathologies, traumatic brain and spine injuries, functional neurosurgeries, neuro-rehabilitative surgeries and awake craniotomy procedures for surgeries in the eloquent areas of the brain.\nAnaesthetic services are provided for magnetic resonance and computed tomography imaging studies in paediatric patients and uncooperative adults and in the cath lab for interventional procedures like embolization of arterio-venous malformations, arterio-venous fistulae of brain and spine, vein of Galen malformation, endovascular coiling of aneurysms, WADA testing, diagnostic angiography and intra-arterial therapy for stroke and vasospasm, and carotid stenting.\nPatients managed in the neurosurgical ICU include post-neurosurgical patients and traumatic brain and spine injury patients. The patients managed in the neuromedical ICU include but are not limited to those with status epilepticus, myasthenia gravis, acute and chronic inflammatory demyelinating polyneuropathies, neuroinfection and stroke.\nAnaesthesia is provided for modified electroconvulsive therapy for patients with medically refractory schizophrenia, major depression, mania and other severe psychiatric disorders.\nAcute postoperative pain management and interventional procedures for trigeminal neuralgia, chronic back pain and other acute neuropathic pain conditions.\nPain management clinic\nVenue: F7, 1st Floor OPD complex\nTime: 2:30 PM to 4:30 PM every Thursday except holidays\nService objective: Assessment and management of acute and chronic pain\nPatients are generally referred to the Pain Clinic by one of the primary specialities. Patients should come with a referral letter and file when they visit us. No prior appointment is available at present and all visits are walk-in. If necessary, after evaluation, patients are referred to other specialists for opinion and management. The aim is to provide comprehensive multi-disciplinary care for patients with chronic pain.\nThe service provided include assessment and diagnosis and, medical and non-surgical (percutaneous interventional) management. The focus is to provide care based on patient’s needs, pathology and preference and enable early return to activities of daily living. Interventions are performed in the Cathlab or Operating room, usually on forenoon of Saturdays except second Saturday and holiday.\nVarieties of medicines are used alone or in combination to manage pain after detailed evaluation. If medical management along with physical therapy and psychological support fail to effectively reduce pain, these patients are offered interventional treatment options. Most interventions are performed under ultrasonography or fluoroscopy guidance to enhance success and reduce side effects. The following interventional treatment procedures are currently undertaken:\n- Epidural steroid injections\n- Radiofrequency ablation\n- Nerve blocks\n- Targeted epidural blood patch\n- Trigger point injections\n- Ganglion block\nThe common painful conditions managed by our clinic include neck and back pain including that radiates to arm and leg from causes such as degenerated or herniated disc and facet arthralgia, neuropathic pain and neuralgias (post-herpetic, ilio-inguinal, trigeminal, occipital, meralgia etc), carpal tunnel syndrome, frozen shoulder, hemifacial pain, CRPS, peripheral neuropathies (diabetic neuropathy, etc), fibromyalgia and post-stroke pain. At present we do not cater to cancer pain.\nIntensive Care Units\nDepartment of Neuro-anesthesia and Neurocritical care manages the neurocritical care unit (NCCU) at NIMHANS with the vision of excellence in patient care, education, research and leadership. The mission of the NCCU is to provide quality health care services. At present, there are three NCCUs at NIMHANS. The critical care units are open in nature. The NCCU specializes in the comprehensive critical care of all critically ill adult and pediatric neurological and neurosurgical patients. NIMHANS is a tertiary referral centre, and hence the NCCU manages most complex patients. The scope of Neurocritical care clinical services includes the provision of emergency medical interventions and resuscitation, provision of close observation and continuous multi-system monitoring and provision of mechanical ventilation for patients. NCCUs provide an environment conducive to the continuous quality improvement of the medical, nursing and other healthcare professional staff.\nAs a primary teaching institution, the education of future physicians is at the pivot of our mission. The NIMHANS Neurocritical care fellowship program is of the highest quality with excellent teaching program in the country. Towards that objective, NCCU offers tertiary, referral-based intensive care for patients with serious, challenging and life-threatening diseases of the central and peripheral nervous system. The NCCU provides complete intensive care management to more than 1200 admissions per year with patients admitted from neurosurgery, neurology and interventional neuroradiology. The unit’s state-of-the-art equipment and technology and individualized care plans ensure the patients of optimal recovery.\nCourses conducted by the department:\n- DM Neuroanaesthesia – 3 years\n- PDF Neuroanaesthesia – 1 year\n- PDF Neurocritical Care – 1 year\n- BSc Anesthesia Technology – 3 +1 year\nTraining to other students provided:\nTraining Nursing students, DM/Mch Residents from other departments of the institution during rotation postings Postgraduate trainees from other colleges (MD/DA/DNB) in neuroanaesthesia for one month Short term training (3 to 6 months) in neuroanaesthesia from deputed from WHO/government institutes.\nDepartment of Neuroanaesthesia and Neurocritical Care, 3rd Floor, Neurosciences faculty centre, NIMHANS, Bangalore 560029\nCurrent Faculty and staff\nPhotograph of Faculty\nName of the Faculty and Designation\nAreas of Research\nProf. M. Radhakrishnan\nProfessor & HOD\nProf. G. S. Umamaheswara Rao\nProf. V. Bhadrinarayan\nProf. K.R.Madhusudan Reddy\nProf. V.J. Ramesh\nOutcomes of intensive care\nDr. Sriganesh kamat\nEvidence based clinical practice\nDr. Gopal Krishna\nEvoked potentials monitoring\nDr. Sonia Bansal\nDr. Rohini Gondhule\nDr Suprna Bhardwaj\nIntraoperative Neurophysiological Monitoring\nMultimodality Monitoring of the brain\nDr Dhritiman Chakrabarti\nBiostatistics and Research Methodology\nDr Shwetha S Naik\nIntensive care unit\nFluid management in neurological patients\nMr Mohanty SR\nMr Manjunath RN\nMr John P Andrew\nMr Divakar C\nMs Hema (Contract)\nMr Shivakumar LH\nMr ManjunathGM (Contract)\nInfrastructure / Equipment / Facilities:\n- State of the art multi-function patient monitors for 8 OTs, 4 Radiology units, ECT unit and 3 ICUs\n- Advanced anaesthesia work stations and ICU ventilators\n- Portable multifunction ultrasound machine for central venous cannulation and nerve blocks, rapid diagnostic ultrasonography in emergency, transthoracic and transesophageal echocardiography, transcranial dopplersonography, peripheral vascular doppler for diagnosis of DVT\n- Heart rate variability (HRV) monitors for autonomic nervous system function assessment\n- Invasive and non-invasive cardiac output monitors\n- Near infrared spectroscopy (NIRS) and jugular venous catheter for assessment of cerebral oxygenation\n- Bedside integrated electroencephalography monitors for assessment of seizures in ICU and OT\n- Depth of anaesthesia monitors like Bispectral index and spectral entropy\n- Target controlled infusion pumps, MRI compatible anaesthesia work station, patient monitors and drug delivery systems\n- Patient temperature management system, point of care coagulation monitoring systems\n- Patient transport ventilators and monitors, defibrillators\n- Many video laryngoscopes including Air traq, Mcgrath, CMAC..etc.\n- Intubating laryngeal mask airway, light wand and fibreoptic scopes for difficult airway management\n- Percutaneous tracheostomy services, bedside cardiac enzyme evaluation\n- Intraoperative evoked potentials monitor\n1977- Establishment of modified ECT, Pain clinic\n1978 – Establishment of Respiratory unit\n1998 – One year Post-Doctoral Fellowship course in Neuroanesthesia started\n2004 – Organised national conference of Neuroanesthesia and neurocritical care (ISNACC-2004)\n2006 – Establishment of Liquid oxygen plant installed\n2010 – Commencement of three year BSc Anaesthesia Technology course\n2011 – Commencement of three year DM Neuroanaesthesia course\n2014- Commencement of one year PDF in Neurocritical care course\n2016 – Organised national annual conference of Neuroanesthesia and neurocritical care (ISNACC-2016)\n2017 – Starting of intraoperative MRI OT complex, and pain clinic\nMalathi Memorial Oration at ISNACC:\nThe department of Neuroanaesthesia and Neuro Critical Care constituted an oration in the name of Dr Malathi, the first HOD of the department, to be delivered every year during the annual ISNACC conference. The list of awardees is as below:\nDr Adrian Gelb\nDr Ravi Mahajan\nDr David Smith\nDr James Cottrell\nDr Piyush Patel\nDr Michael J Souter\nDr Monica S Vavilala\nDr Arun K Gupta\nDr Sergio Bergese\nDr. M. T. V. Chan\nDr Anton Koht\nDr Federico Bilotta","The Queen Square Upper Limb Neurorehabilitation Programme is an NHS service that was started at the National Hospital for Neurology and Neurosurgery in 2014.\nThe programme offers high dose, high intensity physiotherapy and occupational therapy focussed for those with upper limb dysfunction as a consequence of centra nervous sytem disease, particularly stroke. In addition, participants have the opportunity to take part in cutting edge clinical research that will improve our understanding of upper limb recovery and help to further improve the treatment we provide.\nIn 2019, we published the results of the first 224 people with stroke to go through the programme. The scores on 3 different outcome measures are shown for all participants at admission and discharge as well as 6 weeks and 6 months after discharge. The group median scores are shown at each time point, illustrating both change over the 3 week programme but also continued improvement over 6 months of follow up.\nPublished work from the programme:\n- Intensive upper limb neurorehabilitation in chronic stroke: outcomes from the Queen Square programme - download the full paper for free here\n- Pushing the limits of recovery in chronic stroke survivors: User perceptions of the Queen Square Upper Limb Neurorehabilitation Programme - download the full prepint for free here\nWho to refer:\n- The focus of the intensive 3 week upper limb programme is achieving individualised goals relating to functional tasks.\n- The intense goal-directed nature of the programme is best suited for people with some forward reach and at least the beginnings of thumb and/or finger extension.\n- Those with stiff and painful shoulders and/or loss of range in wrist and fingers are likely to need these problems managing before considering admission to the programme.\n- Please consider whether people will tolerate a 3 week intensive programme, considering conditioning, fatigue, cognition.\n- It is a day-attender programme and so most people are relatively independent.\nHow to refer:\n- GPs should refer via the NHS electronic referral system using the following information\n- Hospital: UCLH\n- Speciality - Rehabilitation\n- Service - Multidisciplinary Neurorehabilitation for the Upper Limb Clinic - NHNN - Queen Square- RRV\n- Clinic Type - Neuro-rehabilitation\n- Consultant to consultant referrals should be sent to: Professor Nick Ward, Professor of Clinical Neurology and Neurorehabilitation, The National Hospital for Neurology and Neurosurgery, Box 146, Queen Square, London WC1N 3BG, or by email to email@example.com\n- For administrative matters relating to the Queen Square Upper Limb Rehabilitation Programme please email firstname.lastname@example.org\n- For queries about the waiting list, please contact email@example.com\nThe Programme offers a unique opportunity to investigate how we can further our understanding of recovery of the upper limb after focal brain injury through a range of studies using brain imaging, neurophysiology or behavioural measurements\nProfessor Nick Ward discusses the programme on the Journal of Neurology, Neurosurgery and Psychiatry podcast\nBBC2 Horizon - My Amazing Brain - Richard's War - the story of Richard Gray and his remarkable recovery from a catastrophic stroke Watch here\nBBC Radio 4 - The Life in My Head: From Stroke to Brain Attack - Robert McCrum, who survived a severe stroke in 1995, goes on a journey into his own brain to understand more about what happened to him Listen here.\nThe i newspaper reports on the Queen Square Upper Limb Neurorehabiliation programme Read here"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:7e063fbf-05c4-484c-a16c-12e7ddffcfbb>","<urn:uuid:4192981a-a656-49c1-9434-9cb1cc546da0>"],"error":null}
{"question":"I need to understand materials for school project - can you compare what materials are needed for making basic foam tombstones vs simple paper kites?","answer":"For basic foam tombstones, the essential materials include foam insulation board (white, pink, or blue), cutting blade, paint in at least 2 different colors, propane blow torch, 2-inch drywall trowel, and optional tools like a soldering iron. In contrast, simple paper kites can be made with much simpler materials: just paper (such as 8.5x11 sheets), wooden dowels or bamboo skewers, tape, string or twine, and optional decorative elements like ribbon for the tail.","context":["Basic Tombstones Tutorial\nThere are a few simple steps involved in creating relatively realistic looking tombstones. The tools required are just as simple. Finishing techniques are where you can separate yourself from the cheap dull looking department store stones. Now we’ll go through the steps to create stones that will inevitably have people asking where you bought them.\nFoam insulation board. White, pink or blue (We’ll go into the difference later)\nCutting blade (We’ll go into different types and uses)\nburning tool i.e. soldering iron (depending on technique may not be required)\nPaint at least 2 different colours/shades\nInspiration. Don’t be afraid of this one. It’s one of your best tools\nPropane blow torch\n2” drywall trowel\nmetal weights (not dumbbells but small chunks of steel/lead)\nBeverage of choice.\nOkay so we’ll start with the differing types of foam board. I listed 3 possible types but really there are only 2 since the pink and blue are essentially the same.\nWhite foam boards are the bead type of foam. easy enough to work with but not really rugged enough for an outdoor application without a good protective layer. Now having said this I do still have some white foam stones that have held up for a few years now.\nPink/blue foam is an closed cell foam not the bead style which makes it a more durable foam. This does in turn make it only slightly more difficult to cut but only because it can dull blades quicker.\nThis my preferred foam board for it’s durability and if you use a soldering iron for the lettering it negates some of the cutting required.\nThe thickness of foam board available differs based on your climate. Milder climate area’s such as Southern California may find it difficult to get foam in thicknesses greater than 1” where as northern climates 1.5”-2” is quite common. If greater thicknesses are required than what is available you can glue sheets together. I recommend Gorilla Glue or No More Nails, be careful here and read the label, No More Nail comes in a foam friendly version and an eat the foam version. Don’t mix them up.\nOkay now that we have our materials and we’re ready to go. Where do we start? There are a couple of ways to start. With the tombstone shape or the epitaph. I like to start with the shape since I have a collection of epitaphs (yes I’ll post some links).\nThe shape of a tombstone lends a lot to it’s character so take your time and select a shape that you like and if this is one of your first, one that you can cut easily. More complex shapes can be done after you’re more comfortable working with foam. Some shapes to consider would be a simple rectangle. A little on the boring side right? Well it doesn’t have to be. With a little weathering and some adornments it can be a great shape for a stone.\nAnother take would be a rectangle with a peaked roof. As you can see in this example I have made the peak twice the thickness of of the main body and used pool noodles for side pillars…..but let’s not get ahead of ourselves.\nCutting out the shape can be done many ways. For the simple shapes above I use a regular wood crosscut saw. If I’m doing multiple stones at once I’ll cut simple shapes with my table saw using a plywood blade. There are also foam cutting/burning tools you can buy that are made just for this purpose. These tools are also perfect for cutting more complex shapes where I tend to reach for my handy hacksaw blade….yes blade not the whole saw. I use a work glove and hold the hacksaw blade perpendicular to the foam and begin cutting. You can get very precise cuts with a little patience.\nCAUTION! The use of any of the mentioned tool should only be attempted after reading all of the manufacturers instructions lest you have a a few digits less to complete the project.\nNow that you have a basic stone shape you need to come up with something witty/spooky to put on it. An epitaph can be anything from RIP to a poem. It really all depends on you. At the end of this post I’ll list some links.\nThere are many ways to carve out the lettering for the epitaph. They can be cut out using a dremel tool, an exato knife, a burning tool and I’m sure there are other ways as well. The two methods I have used are a soldering iron and a precision razor.\nUsing a razor to cut out the lettering is very precise but can take a bit of time. The epitaph above took me about 1.5 hours to carve but there was greater precision than using a burning tool. Using this technique also allows for some effects that are difficult to achieve with a burning tool such as beveling the edges of the lettering. This is done by holding the blade at an approximate 45deg angle to the work surface. This method does create a lot of small foam pieces that get everywhere and by the magic of static electricity stick to everything.\nCAUTION should be exercised when using any sharp tool. Best if you keep the bleeding for the props on Halloween.\nA burning tool such as a soldering iron can also be used to create the lettering. The example above isn’t lettering but does show that some degree of detail can still be achieved with this method. For this tool I like to hold the tool 90deg to the work surface. A burning tool is faster than a blade but some loss of precision is there. Another con to this method is that it produces toxic smoke and should only be done in a well ventilated area or outside.\nI haven’t tried this method yet but I am told and it makes sense that it could be one of the best ways to carve lettering and fine detail since depending on the bit used you can get fine detail like a blade but the speed of a burning tool. Just like the blade it will create a lot of small and abnormally statically charged pieces that will stick to you.\nSome would take the stone that they have created at this point and go directly to paint but lets just take a moment and see if we can mess up some of that great work we’ve done so far. Remember the Celtic cross? Well here it is again. Nice enough and I have a good 5-6 hours of work into it but it just seems like it’s a little too nice. Like those store bought stones. This next part may make some cringe but I find it near mandatory to break things soooooo.\nNow that’s better. It has a little character now.\nThe way I did the one below was to heat up a 2” dry wall trowel with my propane torch until red hot and then use it like a knife to draw the crack and slash chunks out. If you try this remember to take a step back after EVERY cut. It’s easy to get carried away and have a stone that looks like Freddy Kruger got to it.\nIn order to get the rough stone texture around the upper level I turned the red hot dry wall trowel by laying the flat edge on the foam and slowly dragging it across. This melted the foam in the irregular pattern you see here.\nCAUTION should be used with a red hot trowel as it will not only cut but permanently brand you.\nOther ways that I have used to punish my stones is to score the corner or edge with the shape I want to remove. Then snap it off. This method will create a more realistic broken edge than cutting off a section but you still retain control over where the break happens.\nNow you may be wondering how I plan where the breaks crack etc are on a stone? Well that’s simple. I don’t. I have no preconceived idea of the finished look. I look at the stone and decide where the first damaged spot should be. Damage that area. Stand back and look again to see where the next spot should be. Repeat until when I stand back, I’m happy with the overall look.\nAll of my stones start with a full coverage base coat and then anywhere from one to four layers of aging colours that can be applied a few different ways. The base coat and aging coats are usually complimentary but contrasting in shade. For example I usually use a medium to dark gray as the base coat and then use lighter gray to white for accent/aging.\nDry brushing is pretty much what it sounds like. Take your brush pickup a little paint then remove most of it on a rag or paper towel. Next lightly apply the accent colour. The method for the brush stroke is again up for interpretation. In the example above I used a very light long stroke with black but then came back with a lighter gray than the base coat and used a stippling motion. I again stippled on a mossy green.\nThis tombstone got a different treatment. I used the same base coat and again stippled on the moss green. Now instead of using dry brushing I fully and quickly covered the tombstone with a LATEX paint (must be a water soluble paint). Next I used a water spray bottle (the kind a hairdresser would have) and sprayed areas that I wanted to wash away and run. This left the stone with a great water weathered look. This method can also be done with a garden hose with a FINE mist. One word of warning. If you want a lot of the white left be sparing with the water. The stone to the left I used about 5oz of water from the spray bottle. The outcome will not happen right away but could take up o 20 minutes before the paint stops running and reveals the final look.\nSome like their stones to be relatively intact and clean. I prefer an older feel to mine and that would usually involve moss. I primarily use two types of fake moss that can be found at most craft stores. Spanish moss and green moss. Again the application of the moss is completely done by feel. I find if I try to plan where it goes it will end up looking unnatural but if I start and just see how it turns out. It look more natural. Again like distressing take frequent steps back to to ensure you don’t end up with a hairy mossy lump.\nProudly display you stones and relax.\nI DIDN’T SAY SLACK OFF! You have more stone to get to. They’re not going to make themselves.\nJust kidding. Now you have stone that will be the envy of every grave grabber in your cemetery.","May 11, · Produced by the Carnival Institute of Trinidad and TobagoMusic: Mighty Duke -Teach the ChildrenSpice & Co.- Bump & WineEl Matador - Pasodoble ClasicoLord Kit. Mar 02, · Building A Traditional Bamboo Kite Called A Mad cgsmthood.com out This Monster Kite: cgsmthood.com?v=3-t8dei9rMAPlease Visit My Website: https:/.\nIf you want to learn how to make a kite or two, you have definitely come to the right spot! Perhaps you have made plenty, but are always on the lookout for more designs and ideas. In any case, some of the most popular single-line designs being flown in the Western world are covered here And the kiting culture of Japan is represented with the Rokkaku and the Sode.\nThere's my Dowel Rokkaku in the photo! If you purchase the kite line recommended below I may receive a small commission - at no extra cost to you. Do you need some kite line? This 3-pack of simple winders with ft lines from Amazon should be ideal. They are all ready to go with 50 pound line. This strength is good for bridles and flying lines for all the MBK kites up to the 1. My instructions for connecting a flying line don't mention swivel clips, but the swivels included in this product are good and strong.\nSo go ahead and use them if you want to :- Otherwise they can just be snipped off. The emphasis here is on very cheap materials. Make them all for just a few dollars! Not only that, but hardly any tools are required. Who hasn't got how to close sprint account pair of scissors and a ruler lying around somewhere?\nPerhaps you might need to beg borrow or steal, I mean buy, a small hack-saw. But that's about it! No special fittings or expensive specialized tools. Learning how to make a kite from bamboo skewers or dowel and plastic is fun and they do fly really well! You can see for yourself in the video for each design, showing the original in flight.\nBarn Door. These all fly well in moderate winds, and the 2-Skewer design can cope with much stronger winds as well Wind Speed Handy Reference.\nLight Air kph mph knts Beaufort 1. Light breeze 6—11 kph 4—7 mph 4—6 knts Beaufort 2. High Wind kph mph knts Beaufort 7. Gale kph mph knts Beaufort 8. Most of those Dowel kites employ a bowed cross spar. Follow that link for tips on how to get the curvature and weight just right. See how I made a simple winder for our 20 pound line. Good for the Skewer kites. Since doing a page on single-surface star kites from around the world, I thought 'why not do a Skewer version?\nFinally, with plenty of people successfully making and flying the original 2-Skewer Delta, a link to that page is retained here The what is ata- 6 hard drive 2-Skewer Delta.\nThe Metric size is mm long x 2. Although this is quite basic kite making, the designs do get a little more complex and time consuming as you move from Sled right through to Dopero. The 2-skewer designs have about 4 times as much sail area as the 1-skewer designs.\nHence, it's easier to make them accurately. Plus, for any given sail material, a 2 skewer kite will be better in light breezes than a 1-skewer kite. The 1. However, the strength-to-weight ratio of hard-wood dowel is not as good as bamboo.\nNail stickers how to apply a bit of fun trying to figure out which of my kites is zipping around the sky in a gusty moderate breeze, in the video up there! Kite plans, as opposed to step-by-step instructions, are handy for experienced kite builders. Learn how to make indoor kites, from just A4 or Letter 80gsm paper, sticky tape and plastic sheet.\nStep by step with photos. Well-tested designs. This MBK kite for kids is about as simple as a Diamond can get! How to make a mad bull kite you have some bamboo BBQ skewers, tape and some plastic bags? Learn how to make paper kites, from just A4 or Letter copier paper and sticky tape.\nExplained step by step with close-up photos. Only sound and well-tested designs here. Learn how to make soft kites, from nothing more than plastic bags and packing tape. All is explained, step by step with large photos. If you want to know how to build kites, you are at the right place.\nThese 3 simple kites are super quick and easy, yet fly really well. It's a printable PDF file. Make a well-tested diamonddelta or sled step-by-step. Do me a small favor? If you're over 16, please sign up for Tethered Flying - my free twice-per-month publication Any questions?\nHome Home Page What's New! Aerial Photog. Pictures Accessories Festivals Catalog. Home Page Better Kites. MBK Dowel Rokkaku. MBK Dowel Delta. MBK Dowel Roller - photogenic even in pale orange. You might like these Back to top of page.\nMore MBK Kite Info\nMay 27, · Join Eugene O'Connor as he builds a traditional Bermudian Kite from cgsmthood.com more great videos, local listings, maps and other helpful Bermuda information.\nTo create this article, 29 people, some anonymous, worked to edit and improve it over time. In this case, several readers have written to tell us that this article was helpful to them, earning it our reader-approved status. This article has been viewed , times. Learn more Flying a kite is a great way to spend a windy day outside.\nInstead of going out and buying one, you can easily make one at home with a few basic materials. You can make your kite any color or length you want, and you can make it with or without dowels. To make an easy kite, first fold an 8-and-a-half by 11 sheet of paper diagonally in half.\nThen unfold it and tape down a small wooden dowel where the crease is. Next, tape one end of a longer dowel to one of the remaining corners, before bending it towards the bottom of the kite and taping the other end to the last open corner. Finally, trim off any excess tape, and tie a piece of twine onto the bent dowel to complete your kite! For instructions on how to build a paper kite with no dowels at all, scroll down! Did this summary help you? Yes No. Log in Social login does not work in incognito and private browsers.\nMethod 1 of Gather your materials. You may have many of these materials on hand at home. Otherwise, try a craft store. Fold your paper in half diagonally. Crease the paper well and open it up again.\nCreate your structure. Place the shorter one of your dowels in this crease and tape it. You want it to be flush with the corners of your paper. Use your other dowel. Although you taped the entire smaller dowel, only tape the end of this one. Arc your dowel. Now that one side is taped down, arc your longer dowel and tape the other side to the opposite corner.\nUse two small pieces of tape to keep this arc in place. Cut excess tape. If you have excess tape on your edges, cut it away to prevent your kite from flying without control. Cut your ribbon. Glue your ribbon to your kite. You want it to follow the same line as your shorter dowel. It will make a colorful tail to help your kite stay in flight. Attach your twine. Tie your twine onto the sides of the arched dowel.\nWrap your twine around a toilet paper roll if it makes it easier for you to reel in and out. Method 2 of Fold your piece of paper in half.\nThe shorter sides of your paper should meet hamburger style. Fold with your decorations on the outside of the paper and turn the paper so that the fold is closest to you. Make a pencil mark 2. Measure 2. Repeat your measurement. From the mark that you just made, measure 2.\nFind the top left corner. Match the other corner. Pick up your paper carefully while holding the first corner in place on your pencil mark. Grab the other side of the paper and fold it down to mirror the first fold.\nThey should line up on the first pencil mark. Staple these corners in place. This is going to be what holds your kite in the air. Attach a tail to the other end of your kite if you wish; this will add stability.\nPunch a hole where you made the second pencil mark. Loop one end of your kite string though the hole and knot it. Your kite is ready to fly! Wrap the string around an empty toilet paper roll to make it easier to reel in and out. If it is strong and thick enough, string will do. But twine is best. Not Helpful 13 Helpful On a really windy day it might flap around a bit, but it will not really fly unless you use dowels or a similar support.\nNot Helpful 16 Helpful Not Helpful 25 Helpful Yes, any ribbon will suffice because all ribbons are light enough to be the tail of a kite. Not Helpful 7 Helpful Not Helpful 15 Helpful Yes, but it needs to be strong and light. Remember to seal the ends of the straw with tape. You can get twine at hardware stores, or stores such as Walmart. Follow the steps in this article but use whatever paper you have.\nNot Helpful 20 Helpful Yes, just follow the steps. You may need 2 sticks as usual , but one 21 cm and other 29 cm. Not Helpful 9 Helpful Include your email address to get a message when this question is answered.\nHelpful 0 Not Helpful 5. Helpful 1 Not Helpful 3. Kites are different each time you make them, so slight adjustments might be necessary. Helpful 2 Not Helpful 3. Submit a Tip All tip submissions are carefully reviewed before being published.\nRelated wikiHows How to. How to. Co-authors: Updated: March 29, Categories: Kite Making and Kite Flying. Article Summary X To make an easy kite, first fold an 8-and-a-half by 11 sheet of paper diagonally in half. Nederlands: Een eenvoudige vlieger maken."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:4c230458-d08d-4c58-9d6e-da7efeb93083>","<urn:uuid:c8b830c9-dcd2-4d01-8866-3d2842838a9d>"],"error":null}
{"question":"As a gardener working with containers and borders, I need a tool for removing weeds in tight spaces. What's the recommended tool and its characteristics?","answer":"For weeding in confined spaces such as borders and containers, you should use a weedfork. This is a small fork, usually with three tines, that is used to cultivate soil and dig out weeds in confined spaces. It is considered an essential part of a gardener's tool kit.","context":["A pruner, secateurs or lopper with a sharp upper blade cutting downward onto a flat lower blade which is known as the anvil; in a knife action rather than a scissor action. Anvil pruners are best utilised to cut thicker stems of dead, woody materials and are better suited for heavy-duty use than tools with a bypass action as they are less likely to distort or 'spread' when cutting thicker stems.\nDigging tools are traditionally manufactured in two sizes, with the smaller sized heads usually known as border spades and border forks. The smaller head size makes these tools more suited for work amongst plants, borders and confined spaces. In addition, border spades and forks are lighter and often smaller than digging spades and forks, making them a sensible choice for individuals who find digging spades and forks too large or cumbersome.\nA pruner, secateurs or lopper with a scissor like cutting action. Bypass pruners give a cleaner cut than anvil types and are easier to sharpen; however, bypass pruners can distort or 'spread' if asked to cut too thick a stem. Bypass secateurs are an essential part of a gardener's tool kit.\nA simple pointed tool, usually with a 'T' handle, used to make a hole for a seedling to be planted in.\nDigging tools are traditionally manufactured in two sizes, with the larger sized heads usually known as digging spades and digging forks. Digging spades and forks are designed for general tilling and cultivation in medium to heavy soil and should not be used to lever or prise objects such as paving stones or root systems.\nDutch hoes typically have a long handle and a short, flat blade fixed at an angle to the shaft with an edge on both sides of the head. It is pushed and pulled back and forth just beneath the surface of the ground, breaking up compacted soil and slicing off weeds.\nShears with a long handle allowing the user to trim the grass at the edges of a lawn that is often missed by a lawn mower.\nA long-handled tool with a sharp half moon curved blade, used to cut a fine edge to the edge of a lawn, such as where it meets a flower bed.\nGrass Shear (single handed):\nA small shear which can be operated with one hand and is designed for trimming grass and soft foliage. This type of shear often features blades that swivel to allow cutting at any angle.\nA small digging tool for one handed use, with a curved blade designed for planting and weeding. A hand trowel is an essential part of a gardener's tool kit.\nA light rake with a long handle and a broad head of sprung tines, primarily designed for raking grass cuttings, dead leaves and hedge clippings. Additional uses include clearing weeds from garden ponds and scratching out moss from lawns.\nA soil rake is a multi-purpose tool and is usually used to break up soil into a fine tilth so as to be suitable for sowing seeds and to create seed drills (shallow trenches for seed sowing. Additional uses include earthing up potatoes and clearing general garden debris.\nA transplanting trowel is narrower than a standard hand trowel and is generally used for transplanting bulbs and seedlings. Some transplanting trowels are marked with a depth gauge on the head to enable for accurate planting.\nA small fork, usually with three tines which is used to cultivate soil and dig out weeds in confined spaces such as borders and containers. A weedfork is an essential part of a gardener's tool kit."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3e32c446-f63d-41a5-b21b-2aeb21bb5d37>"],"error":null}
{"question":"What gene mutation was found in papillary craniopharyngiomas and how might this discovery help patients?","answer":"The BRAF gene mutation was found in almost all samples of papillary craniopharyngiomas. This discovery is significant because these tumors might be treatable with existing BRAF inhibitor drugs that are currently used for treating malignant melanoma, potentially offering a medical alternative to the current treatments of surgery and radiation.","context":["Scientists have identified a mutated gene that causes a type of tenacious, benign brain tumor that can have devastating lifelong effects. Currently, the tumor can only be treated with challenging repeated surgeries and radiation.\nThe discovery, reported in Nature Genetics, is encouraging, because it may be possible to attack the tumors with targeted drugs already in use for other kinds of tumors, said the investigators from Dana-Farber/Boston Children's Cancer and Blood Disorders Center, Massachusetts General Hospital, and the Broad Institute of MIT and Harvard.\nThe mutated gene, known as BRAF, was found in almost all samples of tumors called papillary craniopharyngiomas. This is one of two types of craniopharyngiomas—the other being adamantinomatous—that develop in the base of the brain near the pituitary gland, hypothalamus, and optic nerves. The papillary craniopharyngiomas occur mainly in adults; adamantinomatous tumors generally affect children.\nThe researchers identified a different mutant gene that drives the tumors in children. Drugs that target these adamantinomatous tumors are not yet clinically available, but may be in the future, said the researchers.\n\"From a clinical perspective, identifying the BRAF mutation in the papillary tumors is really wonderful, because we have drugs that get into the brain and inhibit this pathway,\" said Sandro Santagata, MD, PhD, a co-senior author of the paper. \"Previously, there were no medical treatments—only surgery and radiation—and now we may be able to go from this discovery right to a well-established drug therapy.\" BRAF inhibitors are currently used in treating malignant melanoma when that mutation is present.\nPriscilla Brastianos, MD, co-first author of the study, and Santagata said plans are underway to design a multicenter clinical trial to investigate the efficacy of a BRAF inhibitor in patients with papillary craniopharyngiomas.\nCraniopharyngiomas occur in less than one in 100,000 people. They are slow-growing tumors that don't metastasize, but they can cause severe complications, including headaches, visual impairment, hormonal imbalances, obesity and short stature. Even with expert neurosurgery, it is difficult to completely remove the tumors without damaging normal structures, and the tumors often recur.\nThe investigators were surprised to find that the single mutated BRAF gene was the sole driver of 95 percent of the papillary craniopharyngiomas they analyzed with whole-exome DNA sequencing. \"We were really surprised to find that something as simple as a BRAF mutation by itself, rather than multiple mutations, is what drives these tumors,\" said Santagata.\nOne scenario, should the inhibitors prove successful in halting or reversing growth of the tumors, would be to test the drugs preoperatively with the aim of shrinking the tumor so less radical surgery would be needed, said Santagata.\nA different mutation, in a gene called CTNNB1, was identified as the principal abnormality in the pediatric tumors, according to the report. This mutation causes overactivity in the beta-catenin molecular growth-signaling pathway. Unlike with the BRAF mutation, drugs that inhibit the CTNNB1 abnormality have not yet reached the clinic, but several groups are working on them, Santagata said.\nSantagata, a pathologist, is affiliated with Dana-Farber/Boston Children's, Brigham and Women's Hospital and Harvard Medical School (HMS). Co-senior authors of the study are Mark Kieran, MD, PhD, of Dana-Farber/Boston Children's and HMS; and Gad Getz, PhD, of the Broad Institute, Massachusetts General Hospital (MGH) and HMS.\nThe study has three co-first authors: Brastianos of MGH, Dana-Farber Cancer Institute, HMS and the Broad; Amaro Taylor-Weiner of the Broad; and Peter Manley, MD, of Dana-Farber/Boston Children's.\nThe research was supported by Pedals for Pediatrics and the Clark family.\nThe Dana-Farber/Boston Children's Cancer and Blood Disorders Center brings together two internationally known research and teaching institutions that have provided comprehensive care for pediatric oncology and hematology patients since 1947. The Harvard Medical School affiliates share a clinical staff that delivers inpatient care at Boston Children's Hospital and outpatient care at the Dana-Farber Cancer Institute's Jimmy Fund Clinic. Dana-Farber/Boston Children's brings the results of its pioneering research and clinical trials to patients' bedsides through five clinical centers: the Blood Disorders Center, the Brain Tumor Center, the Hematologic Malignancies Center, the Solid Tumors Center, and the Stem Cell Transplant Center.\nIrene Sege | EurekAlert!\nMACC1 Gene Is an Independent Prognostic Biomarker for Survival in Klatskin Tumor Patients\n31.08.2015 | Max-Delbrück-Centrum für Molekulare Medizin in der Helmholtz-Gemeinschaft\nFish Oil-Diet Benefits May be Mediated by Gut Microbes\n28.08.2015 | University of Gothenburg\nThe leaves of the lotus flower, and other natural surfaces that repel water and dirt, have been the model for many types of engineered liquid-repelling surfaces. As slippery as these surfaces are, however, tiny water droplets still stick to them. Now, Penn State researchers have developed nano/micro-textured, highly slippery surfaces able to outperform these naturally inspired coatings, particularly when the water is a vapor or tiny droplets.\nEnhancing the mobility of liquid droplets on rough surfaces could improve condensation heat transfer for power-plant heat exchangers, create more efficient...\nLonger, more severe, and hotter droughts and a myriad of other threats, including diseases and more extensive and severe wildfires, are threatening to transform some of the world's temperate forests, a new study published in Science has found. Without informed management, some forests could convert to shrublands or grasslands within the coming decades.\n\"While we have been trying to manage for resilience of 20th century conditions, we realize now that we must prepare for transformations and attempt to ease...\nA University of Oklahoma astrophysicist and his Chinese collaborator have found two supermassive black holes in Markarian 231, the nearest quasar to Earth, using observations from NASA's Hubble Space Telescope.\nThe discovery of two supermassive black holes--one larger one and a second, smaller one--are evidence of a binary black hole and suggests that supermassive...\nA team of European researchers have developed a model to simulate the impact of tsunamis generated by earthquakes and applied it to the Eastern Mediterranean. The results show how tsunami waves could hit and inundate coastal areas in southern Italy and Greece. The study is published today (27 August) in Ocean Science, an open access journal of the European Geosciences Union (EGU).\nThough not as frequent as in the Pacific and Indian oceans, tsunamis also occur in the Mediterranean, mainly due to earthquakes generated when the African...\nIn mountainous regions earthquakes often cause strong landslides, which can be exacerbated by heavy rain. However, after an initial increase, the frequency of these mass wasting events, often enormous and dangerous, declines, in fact independently of meteorological events and aftershocks.\nThese new findings are presented by a German-Franco-Japanese team of geoscientists in the current issue of the journal Geology, under the lead of the GFZ...\n20.08.2015 | Event News\n20.08.2015 | Event News\n19.08.2015 | Event News\n01.09.2015 | Press release\n01.09.2015 | Materials Sciences\n01.09.2015 | Materials Sciences"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:f5d22e0b-0033-4282-a975-3f01239c7274>"],"error":null}
{"question":"I'm learning about GPS systems - can you tell me which is more accurate for measuring distance traveled: using wheel turns in a car GPS or a handheld GPS device?","answer":"Using wheel turns in a car GPS system is more accurate for measuring distance traveled. When a GPS is properly wired into a car system, it can count wheel turns and use the known wheel size to precisely calculate distance traveled, similar to how a mileage counter works. In contrast, handheld GPS devices rely on satellite signals and while they can be very accurate under ideal conditions (within 3 meters 95% of the time), their accuracy can be affected by factors like poor sky visibility, and they particularly struggle when stationary, potentially recording false movements that need to be filtered out.","context":["As others have stated, the way to keep track of your position when you can't use GPS is inertial navigation, also known as dead reckoning. This uses various sensors, such as accelerometers, magnetometers and gyroscopes, which form an IMU (inertial measurement unit).\nThe issue is that it is by definition subject to drift (so with time the estimated position can get further and further away from the actual position), and that it's even more complex on a hand-held device such as a mobile phone as opposed to a car.\nOn a car, you can get (if the GPS is properly wired into the car system) two bits of information that are quite handy:\n- The orientation of a car (from a magnetometer which is basically an electronic compass)\n- The distance traveled (by counting wheel turns and knowing the wheel size -- this is how your mileage counter works).\nEach wheel turn, you add the size of perimeter of the wheel to your previous position in the direction of the orientation of the car.\nThis can give you a pretty accurate result. You will get slightly off if you turn a lot (or if the tire size doesn't match!), but the error should remain quite manageable.\nIf your GPS cannot get the wheel turns information (i.e. it's an add-on device, not wired into the car), then it get slightly more tricky, as you can't directly get a distance traveled. Instead, you rely on accelerometers, which will give you... acceleration (it's the only thing you can measure directly).\nFrom acceleration, by integration, you can get speed: if you start at 0 m/s and measure a constant 1 m/s^2 acceleration, after one second your speed is 1 m/s, after 2 seconds it's 2 m/s, and so on. If acceleration drops to 0, then the speed remains constant.\nFrom speed, you can get distance traveled via the same mechanism. Double integration means that your position is based on the square of elapsed time (x = x0 + 0.5.a.t^2 if acceleration was constant). This means that even a slight error in measurement of acceleration will quickly grow to become a quite large error in the measurement of distance travelled. To minimise error, you need to have the lowest possible error in measurement (better sensors), and perform measurements as frequently as possible (we're talking fractions of a second).\nWith this, it's still possible to get a decent estimate, especially as a car doesn't (usually) swerve around a lot, or have lots of variation in acceleration. The magnetometer being fixed in the car means that it can tell you quite precisely where the car is headed.\nHowever this all goes awry when you consider a mobile phone. You have it in your hand. Try as you may, you will at the very least bounce it slightly left and right and up and down as you walk. You probably don't have a constant speed either, so you'll also impart a slight acceleration front and back. This means acceleration changes a lot in all 3 axes. Orientation as well.\nThis results in a lot more difficult measurement, will lots of errors, especially if you don't sample acceleration and orientation and rotation often enough. Modern IMUs will do that all inside the chip to try to get the best estimate possible, but that's a pretty difficult task.\nWikipedia reminds us that maths dictates that:\nTo get a rough idea, this means that, for a single, uncorrected accelerometer, the cheapest (at 100 mg) loses its ability to give 50-meter accuracy after around 10 seconds, while the best accelerometer (at 10 µg) loses its 50-meter accuracy after around 17 minutes.\nAnd that's just based on pure acceleration, not even taking into account rotation of the phone.\nNot all phones are created equal in that respect, and some will have better performance than others (because they have more recent, more expensive IMUs with less errors, higher frequency), but in any case, it won't (and can't) give you a precise position for a very long time: they need to be frequently re-synchronised to their actual position through other means (GPS or Wi-Fi/cellular trilateration).","Handheld GPS devices, their accuracy, OS Maps, GPX and KML, and the meaning of Elevation\nA GPS device with an OS map - and a little marker on the screen saying \"you are exactly here\" - is brilliant. It makes life so easy - you will never get lost again.\nHowever, paper maps are not dead yet. Here's why.\n1) Electronic OS Maps are very expensive\nThere are many websites, this one for example, with free online OS mapping. However, to go walking, you will need a mobile device (satnav, pad, smart phone, Iphone, etc.) with an app to store the maps as mobile signal coverage in the countryside is very poor.\nWhile dedicated walkers' satnavs are expensive, you may already have a smartphone, and the apps are quite cheap. The problem for both is that electronic OS maps are an additional cost - an they are expensive, very expensive.\nThe 1:50K (Landranger) scale maps are not too bad - £125 or so for the whole of the UK, but the excellent for walkers 1:25K OS Explorer series are much more expensive at £350.\nAnd there's another twist, the electronic maps are not 'open'. Buy the electronic OS maps for one app (or satnav device), and they will not work in another, so you are stuck with that 'app'/device - or you will have to re-purchase the same very expensive maps all over again.\n2) Dedicated Handheld GPS Devices vs Smartphones/Tablets\nA dedicated handheld GPS device, like the Satmap Active 10, is amazing and brilliant. Rugged, waterproof, OK screen size. The only drawback is its cost - its an expensive piece of kit - about £350 with 1:50K scale maps of the entire country. And, once you've purchased it, due to the expense, you're locked in. You can only use its maps on the Satmap device, and you can only buy maps for it from Satmap.\nMemory-map and Garmin also have handheld GPS devices, but they are definitely not recommended.\nThe other option is an app for your existing Smartphone (Android/Iphone) or Pad. The good bit is that there is competition for apps and tablets have larger screens. The problem is battery life. With the GPS enabled, you will only get a few hours battery life - nowhere near enough for a days walk. The design of the dedicated GPS devices have this in mind, they switch the screen off after a few secs, and don't update the GPS position so frequently. If you're walking, updating once every 10 secs is plenty.\nThere are several companies with smartphone apps, e.g. the OS and Anquet. Memorymap does as well, but it has poor reviews. Whichever one you choose, once you purchase maps for it, you're locked in.\nOS Maps for you PC\nAnquet and Memorymap both have PC applications - but they are hard to recommend when so many websites now have free OS mapping. They are much easier to use than websites, with more features like 3D effects, but both much more expensive as you have to buy the maps. The only reason for using them would be to then load the maps onto your mobile smartphone or tablet, or if you do a lot of route planning.\nOS Map Websites\nThe OS have a slightly clunky but free website called Get-a-map. You can plan routes, and print out the latest 1:25K scale mapping from it. There are many others that have similar functionality.\nGeo-coding and GPS Accuracy - How accurate is GPS data?\nGeocoding means recording the GPS location of a point (a waymark) or a route.\nSo, you have a GPS device, like a smartphone, and want to record your route - distance and ascent. It should be really accurate - right?\nThe distance measurement is very accurate, so long as you:\n- Switch your device on a few mins before you start. This is to let it settle down with the satellite signals. Each time you switch on your device, it will download a file of the latest satellite location fixes from within the satellite signal - this will take a couple of minutes\n- Stop recording your route when you are stationary. When you are stationary, the location fix is less accurate, and the device will start recording a 3 dimensional spider's walk around your location. Stop 20 minutes for a cup of tea, and it will record 2km of distance, and a few hundred feet of ascent.\nSo, assuming you stopped recording while you were stationary, the distance measurement will be very, very accurate (if you didn't stop, editing out the random 'stop' points in the GPS record isn't easy). With a good GPS device, and a clear view of the sky, accuracy will be within 3 metres 95% of the time. (so 5% of the time, it will be greater than 5 metres, as it will be without a clear view of the sky)\nA typical GPS device will record 1 point every metre. So a walk's GPS route may have 20,000 points. Software - websites like OS Get-a-map or the PC applications like Anquet/Memory Map, can compact the route by smoothing it to a more reasonable 200 to 300 points or so by reducing the accuracy to within a tolerance of 5 metres or so. Its also a bit easier to display a line with 300 points, rather than a line with 20,000 points. Smoothing works by using a mathematical algorithm to:\n- remove unnecessary points without loosing too much accuracy, e.g. Douglas-Peucker (easier to understand).\n- draw a curved line through the points, e.g. Bezier Curves (complex)\nElevation accuracy however, isn't so good. First GPS isn't as accurate for elevation as it is for horizontal location. Second, all those small errors are significant compared to the figure you are trying to measure. For example, a 20km walk, with 20,000 points, each with 5m (clear sky) to 15m (near trees, cliffs, the 5% of the time) errors, add up to a significant proportion of the figure you need to measure. Say you are walking on a flat path, the device may measure 4, 5, 4, 5, 6, 5 , 3, 5, 3, 5, 5 (6m of ascent over 11 points, so how many over 20,000... ). The inaccuracies are much worse near trees or buildings. The way to fix this is to smooth the figures (remove the small ups and small downs). But to do this you have to remove so many ups and downs you actually remove some of the ones that really existed and that you needed to measure.\nIt turns out that the best way by far to record elevation accurately is to actually discard the figures that your device records, and use a GPS website instead. These calculate the elevation from satellite radar measurements of the earth's surface. Google for example have an elevation service with a 50m grid of height measurements for the world's surface. Give it a location, and it will extrapolate the elevation for you. Fine, unless you are near a cliff, in which case it will say you are half way up, rather than at the top or the bottom.\nThe OS have a contour (i.e. vector rather than grid) based product, which should be exceptionally accurate, but it is not clear which websites use it.\nThese issues explain why there are often so many differing ascent figures for given walk. Smoothing out ups and downs of less than 5 metres will give a different figure from smoothing to a tolerance of 10, 15 or even 50 metres. There is no 'correct' answer.\nIn summary, to measure a route accurately, mark it up on a GPS mapping website, and let it calculate the distance and altitude gain/loss for you.\nElevation gain or loss is straightforward, but absolute height is a little more tricky\n- OS Altitude. OS maps use elevation based upon mean sea level at Newlyn in Cornwall between 1915 and 1921. However average low tide varies around the UK due to tides and changes in gravity due to variations in the Earth's density (e.g. the difference between east coast and west coast is 0.8m) . Not to mention rises in sea level...\n- GPS Altitude - \"ellipsoid\". GPS devices use the WGS84 (World Geodetic System) standard ellipsoid (squashed sphere) model of the Earth's shape - it differs significantly from OS elevation\n- Height above Earth Surface (Sea Level) Altitude - \"geoid\" The EGM 2008 (Earth Gravitational Model) standard defines a more accurate model of the Earth's surface that takes into account local variations in gravity (caused by local differances in the Earth's density). This is equivalent to OS altitude.\nThere is a nice diagarm of the difference here\nGPX and KML\nEach type of software has its own proprietary GPS data file format. There are several ways to exchange data between PC programs, websites, and GPS devices\n- GPX is an open standard for GPS data - all satnav devices and apps for smartphones use GPX format files. They can either read GPX files directly, or like Garmin and Satmap, provide you with an import/export program to translate\n- KML - this is what Google use - which makes it a standard as well. Google Earth can read GPX however, and all the gps apps and devices can now read KML as well as GPX. So KML is the second best choice.\n- GeoJSON, GeoRSS - these are standards that didn't quite make it - don't worry about them\nWhat to do with a GPX file?\nYou need a program (app) or website that understands GPS data\n- load it into Google Earth\n- load it into the OS Get-a-map website (or many others) and view it online\n- load it into your GPS device, e.g. Satmap, Garmin\n- get an app for your smartphone/tablet and OS mapping and load it\nEinstein and Relativity\nAs an aside, GPS is actually one of the few applied uses for relativity. The maths for working out the GPS location uses relativity as we are down a gravity well from the satellites, and time moves a little bit differently for them\nAfter the 1983 shooting down of a Korean airliner which has wandered into Soviet airspace, Ronald Regan announced that GPS would be made freely available as a public good. Initially the signal was intentionally degraded to provide 100 metre accuracy, however, that was switched off in 2000, improving the accuracy to 20 metres for public use. Due to arms control restrictions, you may find you GPS device will not work above 18 kilometres (11 miles) altitude or faster than 515 metres per second (1,240 miles per hour) to prevent you using it as a component in a ballistic missile"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:2e7e9e0f-b564-44a4-add3-f8fb1554697a>","<urn:uuid:be6f1281-b8b8-4368-a92e-06ab5626bf84>"],"error":null}
{"question":"What are the comparative advantages of PCR genotyping versus Next-Generation Sequencing for SARS-CoV-2 surveillance?","answer":"PCR genotyping offers several advantages over Next-Generation Sequencing (NGS): it provides quicker results with a turnaround time of 12-24 hours compared to NGS's 2-3 weeks, it is less expensive, and offers higher testing throughput. Additionally, PCR instruments are widely available in laboratories worldwide, supporting global upscaling of variant surveillance. However, unlike NGS, PCR genotyping doesn't analyze the complete viral genome. Currently in the United States, only about 5% of SARS-CoV-2 positive samples are analyzed by NGS.","context":["Next-Generation Sequencing (NGS) is a powerful technology that allows whole viral genome sequencing and is a crucial tool for identifying new mutations and variants. But NGS also has its limitations, including associated costs and relative low sequence capacity. Also, it can take 2-3 weeks before sequence data is available in public repositories. In the United States, only approximately 5% of SARS-CoV-2 positive samples sequences are analyzed, primarily by NGS.\nGenotyping by polymerase chain reaction (PCR) can help scale up variant surveillance. Although genotyping by PCR does not analyze the complete viral genome as NGS does, it allows quicker results with shorter turnaround time of 12 to 24 hours. Genotyping by PCR is also less expensive and offers the possibility of higher testing throughput. It’s important to note that the broad availability of PCR instruments in laboratories worldwide supports global upscaling of variant surveillance.\nGiven the wide variety of existing and emerging variants, it is crucial to evaluate the overall performance of genotyping by PCR. The Variant Task Force (VTF), which is part of the National Institutes of Health’s (NIH’s) Rapid Acceleration of Diagnostics (RADxSM) initiative, initiated a project in July 2021 to identify SARS-CoV-2 markers that could detect all known variants. The study aimed to develop a panel of markers to accurately assign lineages and to support early detection of new and re-emerging variants by genotyping by PCR. The results of this study are published here.\nSARS-CoV-2 detection by PCR using variant agnostic positivity markers\nThe study identified three variant agnostic makers to detect SARS-CoV-2 sequences, independent of lineage. The markers target sequences in the nsp10-, N- and S-genes. Under the criteria that a sample was considered positive for SARS-CoV-2 when at least one of the markers was detected, the study evaluated a total of 1,128 retrospective samples (1,031 SARS-CoV-2 positive and 97 SARS-CoV-2 negative samples). As a result, an outstanding performance with a Positive Percent Agreement (PPA) of 99.3% and a Negative Percent Agreement (NPA) of 100% was achieved.\n“In this report, we demonstrated three (3) variant agnostic markers that can detect SARS-CoV-2 positive samples with high PPA and NPA compared to NGS.”\nGenotyping by PCR supports monitoring of circulating variants\nUsing a total of 1,031 samples positive for SARS-CoV-2, the study assessed the performance of panels either consisting of 48, 24, 16, 12, or 8 makers to identify the top 10 WHO lineages (including Alpha, Beta, Gamma, Delta, Epsilon, Eta, Iota, Kappa, Lambda and Mu; but excluding the Omicron variant).\nThe results demonstrated that the 16 marker panel was sufficient to identify all 10 WHO lineages with a with a high PPA and NPA and results were similar to the 48-marker and 24-marker panels. The 12-marker panel and 8-marker panels identified eight and six of the WHO lineages respectively with high accuracy.\n“……demonstrated that there are marker combinations that are highly specific for certain variants.”\nGenotyping by PCR supports early detection of new variants\nThe study also explored the hypothesis that an increase in undetermined calls (when the marker panel is unable to associate a variant with a sample) could be indicative of new emerging variants. An increase in undetermined calls should therefore be regarded as a signal for focused full genome sequencing of those samples. Use of inexpensive, rapid PCR genotyping acts as filter to identify samples that should be NGS sequenced to identify a potential new variant.\nA bioinformatics simulation with a modified 12-marker panel, which excluded the two Delta specific markers, was conducted using genomic SARS-CoV-2 data from the United States added to the Global Initiative on Sharing Avian Influenza Data (GISAID) database between November 2020 and July 2021. That period included an increase in Delta variant reports to GISAID from a few cases in April 2021 to >80% of all submitted sequences by July. The simulation revealed a direct correlation between prevalence data for the emerging Delta variant and increase in undetermined calls with the panel (without markers for Delta variant) starting in April and peaking in July 2021.\n“Routine use of these genotyping markers could provide early warning that a new or re-emergent variant is circulating.”\nGenotyping by PCR supports Omicron detection\nTo address the emerging Omicron variant, a panel of 4 markers was rapidly designed and tested. The panel included 3 Omicron and 1 Delta variant specific markers. Testing 1,631 SARS-CoV-2 positive samples with this new panel, this study resulted in 100% PPA and 99% NPA for the Omicron variant, and 90.9% PPA and 98.3% NPA for the Delta variant. It should be noted that the Delta samples that were not correctly identified were all Delta subtypes.\nFollowing the discovery of the Omicron variant, the 4-marker panel was utilized in two CLIA-certified laboratories to genotype 5,372 SAR-CoV-2 positive samples during the month of December 2021. The results showed a surge in the relative prevalence of the Omicron variant in the United States, from about 15% to about 80% out of all genotyped positive samples from 9 December 2021 to 21 December 2021. Subsequently, this panel was modified to include a marker to differentiate BA.1 from BA.2/BA.3 sub-lineages of Omicron and to date more that 75,000 samples have been genotyped with representation of samples from all 50 states within the US.\nIn summary, according to the authors, genotyping by PCR is quick, efficient, and less expensive than sequencing in SARS-CoV-2 surveillance, and therefore it provides an excellent tool for scaling up surveillance efforts in this ongoing pandemic. The authors emphasize that any sample that cannot be determined by genotyping by PCR would be a prime candidate for sequencing. An increase in undetermined samples could signal the presence of a new emerging variant, and genotyping by PCR offers the potential of an early warning system.\n“This illustrates the symbiotic nature of using genotyping markers in conjunction with targeted sequencing.”\nThis work was supported by a collaboration between the NIH Rapid Acceleration of Diagnostics (RADx), Helix, University of Washington, Aegis Biosciences, Rosalind and Thermo Fisher Scientific. SARS-CoV-2 variant information generated from this program is available to the public through the Rosaland website. The Rosalind Tracker dashboard provides a snapshot of variants circulating in the United States close to real-time.\nFor more information on the assays associated with the publication, please visit:\nAssays for the following mutations used in this study are available from Thermo Fisher:\nA method for variant agnostic detection of SARS-CoV-2, rapid monitoring of circulating variants, detection of mutations of biological significance, and early detection of emergent variants such as Omicron by E. Lai, D. Becker, et al\n* For research use only. Not for use in diagnostic procedures."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:4f3cb058-5b0b-432a-8f31-b3d3a3776f7b>"],"error":null}
{"question":"I need to understand for my biology class - what's the process that makes urine more concentrated than blood?","answer":"Urine becomes more concentrated (hypertonic) than blood through a process involving the loop of the nephron and collecting duct. First, salt diffuses out of the ascending limb into the renal medulla. Then, due to this osmotic gradient, water leaves the descending limb and collecting duct. Additionally, urea leaks from the lower portion of the collecting duct, contributing to the high solute concentration in the inner medulla. This countercurrent mechanism ultimately produces urine that is more concentrated than blood plasma.","context":["Regulatory Functions of the Kidneys\nThe kidneys are involved in maintaining the fluid and electrolyte balance, and also the acid-base balance, of the blood. If the kidneys fail to carry out these vital functions, either hemodialysis or a kidney transplant is needed.\nFluid and Electrolyte Balance\nThe average adult male body is about 60% water by weight. The average adult female body is only about 50% water by weight because females generally have more subcutaneous adipose tissue, which contains less water. About two-thirds of this water is inside the cells (called intracellular fluid), and the rest is largely distributed in the plasma, tissue fluid, and lymph (called extracellular fluid). Water is also present in such fluids as cerebrospinal fluid and synovial fluid; in Figure 16.6, these fluids are referred to as “other” fluids. For body fluids to be normal, it is necessary for the body to be in fluid balance. The total water intake should equal the total water loss. Table 16.2 shows how water enters the body-namely, in liquids we drink, in foods we eat, and as a by-product of metabolism. We drink water when the osmolarity of the blood rises as determined by the hypothalamus. Table 16.2 also shows how water exits the body-namely, in urine, sweat, exhaled air, and feces. Similar to the gain and loss of water, the body also gains and loses electrolytes. Despite these changes, the kidneys keep the fluid and electrolyte balance of the blood within normal limits. In this way, they also maintain the blood volume and blood pressure.\nFigure 16.6 Location of fluids in the body. Most of the body’s water is inside cells (intracellular fluid), and only about one-third is located outside cells (extracellular fluid).\nReabsorption of Water\nBecause of the process of osmosis, the reabsorption of salt (NaCl) automatically leads to the reabsorption of water until the osmolarity is the same on both sides of a plasma membrane. Most of the salt, and therefore water, present in the filtrate are reabsorbed across the plasma membranes of the cells lining the proximal convoluted tubule. But the amount of salt and water reabsorbed is not sufficient to result in a hypertonic urine-one in which the osmolarity is higher than that of blood. How is it, then, that humans produce a hypertonic urine? We now know that the excretion of a hypertonic urine is dependent upon the reabsorption of water from the loop of the nephron and the collecting duct.\nLoop of the Nephron and Collecting Duct\nA loop of the nephron has a descending limb and an ascending limb. A long loop of the nephron penetrates deep into the renal medulla. In the ascending limb, salt (NaCl) passively diffuses out of the lower portion and is actively transported out of the upper portion into the tissue of the outer renal medulla (Fig. 16.7). Less and less salt is available for active transport as fluid moves up the thick portion of the ascending limb. Therefore, the concentration of salt is greater in the inner medulla than in the outer medulla. (It is important to realize that water cannot leave the ascending limb because the ascending limb is impermeable to water).\nThe large arrow to the side in Figure 16.7 indicates that the lowest portion of the inner medulla has the highest concentration of solutes. You can see that this is due not to the presence of salt, but to the presence of urea. Urea is believed to leak from the lower portion of the collecting duct, and it is this molecule that contributes to the high solute concentration of the lowest portion of the inner medulla. Because of the osmotic gradient within the renal medulla, water leaves the descending limb along its entire length. There is a higher concentration of water at the top of the descending limb, and so it takes a lesser amount of solute in the medulla to pull it out. The remaining fluid within the descending limb encounters an even greater osmotic concentration of solute as it moves along; therefore, water continues to leave the descending limb from the top to the bottom. Such a mechanism is called a countercurrent mechanism. At the top of the ascending limb, any remaining water enters the collecting duct. Surprisingly, the fluid inside the nephron is still not hypertonic-the net effect of reabsorption of salt and water so far is the production of a fluid that has the same tonicity as blood plasma. However, the collecting duct also encounters the same osmotic gradient as did the descending limb of the loop of the nephron (Fig. 16.7). Therefore, water diffuses out of the collecting duct into the renal medulla, and the urine within the collecting duct becomes hypertonic to blood plasma.\nAntidiuretic Hormone (ADH)\nADH released by the posterior lobe of the pituitary plays a role in water reabsorption at the collecting duct. In order to understand the action of this hormone, consider its name. Diuresis means flow of urine, and antidiuresis means against a flow of urine. When ADH is present, more water is reabsorbed (blood volume and pressure rise), and a decreased amount of urine results.\nFigure 16.7 Reabsorption of water at the loop of the nephron and the collecting duct. A hypertonic environment in the tissues of the medulla of a kidney draws water out of the descending limb and the collecting duct. This water is returned to the cardiovascular system. (The thick black line means the ascending limb is impermeable to water).\nIn practical terms, if an individual does not drink much water on a certain day, the posterior lobe of the pituitary releases ADH, causing more water to be reabsorbed and less urine to form. On the other hand, if an individual drinks a large amount of water and does not perspire much, ADH is not released. In that case, more water is excreted, and more urine forms."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:2e4c3c10-f7c4-436a-bc17-17164e1b0f52>"],"error":null}
{"question":"How did Blessed Gérard's hospital in Jerusalem treat sick pilgrims, and what was unique about its spiritual approach to healthcare?","answer":"The hospital treated sick pilgrims both physically and spiritually. The ward was designed as a large room with an altar inside so patients could participate in Holy Mass from their beds. According to the Rule of the Order of St. John, when sick people arrived, they would first confess their sins to a priest and receive Holy Communion before being taken to bed. The sick were called 'the poor of Christ' or 'the holy poor' and were welcomed as Christ himself, with the brothers making a promise to be 'servants and slaves to our Lords, the sick'.","context":["Spiritual and historical Background of the Brotherhood of Blessed Gérard\nBlessed Gérard (who died on September 3, 1120) was most probably a Benedictine monk who was the guest master of the Benedictine Monastery St. Maria Latina in Jerusalem. The guest house, situated on the other side of the road of the Church of the Holy Sepulchre, was very big as it was always occupied by numerous pilgrims who came to see the places where Jesus lived and died and rose from the dead. Because the journeys in those days were a big strain, most of the pilgrims arrived in Jerusalem exhausted or sick.\nTherefore the guest house of St. Maria Latina was more a hospital than a hotel and it was in those days commonly known as the Hospital of Jerusalem. Apart from nursing the sick they used to accommodate abandoned children, feed the starving, clothe the needy and care for discharged prisoners. Blessed Gérard's hospital was a well organised charitable organisation.\nBlessed Gérard founded the Brotherhood of St. John of Jerusalem to run the hospital.\nThis community is the historical root of the Hospital Order of St. John, the oldest hospital order of the Church (founded in 1099), known as the Sovereign Military Hospitaller Order of St. John of Jerusalem of Rhodes and of Malta or in short the Order of the Knights of Malta, whose Anglican branch, The Grand Priory in the British Realm of the Most Venerable Order of the Hospital of St. John of Jerusalem (founded in 1831), is well known in South Africa as The Priory of the Order of St. John through its offspring the St. John Ambulance Association and Brigade (founded in 1877).\nThe statutes of the Brotherhood of St. John of Jerusalem are the basis of the Rule of the Order of St. John whose spirituality is going back to the Benedictine principle of hospitality, expressed in chapter 53 of the Rule of St. Benedict which reads: \"All guests who present themselves are to be welcomed as Christ, for he himself will say: I was a stranger and you welcomed me (Matt 25:35). Proper honour must be shown to all, especially to those who share our faith (Gal 6:10) and to pilgrims\". And that is exactly what the hospital of Jerusalem and its brotherhood did.\nBlessed Gérard and his successors called the sick \"the poor of Christ\" or simply \"the holy poor\" indicating that they being welcomed as Christ, thus represent Christ to those who have the honour of serving them. Loving ones neighbour therefore becomes worship of God and the members of the hospital order made the promise \"to be servants and slaves to our Lords, the sick\". A principle of the brotherhood's spirituality was right in the contradiction of the spirit of the time not to gracefully grant favours to those in need and to be honoured for what they had done, but to consider it a favour to have the honour of serving the needy and thus receive the grace of being close to Christ who is being represented by the poor.\nSuch an attitude is still a contradiction to the Spirit of our times, where helpers often consider themselves superior to those they help and do not realise what graceful chance they miss to meet the Lord in the needy.\nOn the other hand the hospital of Jerusalem did not disregard the spiritual needs of their Lords, the sick. The hospital was actually regarded as a spiritual community and the sick were not only cared for bodily but also benefited from the pastoral care of the hospital.\nThe Rule of the Order of St. John reads in chapter 17: \"When a sick comes to the house ... he may be received as follows: After he has first faithfully confessed his sins to a priest, he may receive Holy Communion, and afterwards he may be carried to a bed and may be lovingly fed every day like the Lord, according to the possibilities of the house, even before the brothers have their meal. And the Reading and the Gospel may be read in the hospital on all Sundays and the sick may be sprinkled with Holy Water during the procession.\"\nThe hospital was considered both a church building and church community anyway. The ward was a big room with an altar inside, so that all the sick could participate in Holy Mass without having to leave their beds.\nAll in all the brotherhood and the hospital order founded b Blessed Gérard thought of the hospital as a community of saints: The brothers extended God's loving care to the needy. They acted on Christ's behalf, because the church is the body of Christ and Christ is thus acting at all times through his church. But the brothers met Christ in the Sick as well (cf. \"Whenever you did this for one of the least important of these brothers of mine, you did it for me!\" Mt. 25.40).\nAll involved, the brothers on the one hand and the sick on the other hand, are mutually representing Christ, making life in the community of the hospital a mutual encounter with the Lord and therefore an event of salvation.\nThe Brotherhood of Blessed Gérard sees itself as a revival of the Brotherhood of St. John founded by the Blessed Gérard. It wants to revitalise the charisma of these origins and adopt them and the brotherhood's spirituality into the context of our present time and life situation.\nIt seeks to brotherly relate to all communities and organisations standing in the same tradition and spirituality.\nBlessed Gérard was a holy man ...\nIt was in the year 1099, that Blessed Gérard Tonque founded a brotherhood of dedicated people at the house of the hospital of St. John the Baptist in Jerusalem, who were referred to as \"the brothers, who come to serve the poor and protect the Catholic Faith\". This brotherhood developed into a religious Order by AD 1113, which is in our days called the Sovereign Military and Hospitaller Order of St. John of Jerusalem, of Rhodes and of Malta (or the Order of Malta).\nBlessed Gérard was a holy man, who was praised by contemporary sources as well as posthumously. Blessed Gérard died on 3. September 1120 and his epitaph in the convent he founded reads as follows:\nHere lies Gerard, the humblest man among the dwellers in the East;\nThe servant of the poor, a welcoming friend to strangers;\nHe was lowly in mien, but within him shone a noble heart.\nThe measure of his goodness may be seen within these walls.\nHe was provident in many things, painstaking in all he did;\nHe undertook many tasks of diverse nature;\nStretching out his arms diligently to many lands,\nHe gathered from everywhere the means to feed his people.\n(Quoted from H. J. A. Sire: The Knights of Malta, Reprint New Haven and London 1994)"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:7d912184-5c49-4a11-9526-3e9026aa10b1>"],"error":null}
{"question":"How do the tax implications differ between sole proprietorships and business partnerships when it comes to income reporting and taxation?","answer":"Sole proprietorships and business partnerships have different tax structures. In a sole proprietorship, the owner files a 1040 form and all business income is taxed as personal income. For partnerships, while partners individually pay federal, state, and local taxes on their partnership income as personal income, the partnership must file an annual 'information return' with the IRS to report income, deductions, gains, and losses. When starting a partnership, partners must file a 1065 form, which then transfers to individual 1040 forms, with everyone filing separately. Unlike corporations which face double taxation, both sole proprietorships and partnerships are only taxed once on their income.","context":["It is important that the business owner seriously considers the different forms of business organization — types such as sole proprietorship, partnership, and corporation. Which organizational form is most appropriate can be influenced by tax issues, legal issues, financial concerns, and personal concerns. This essay explains the general impression of business organizations.\nA Sole Proprietorship, also known as the sole trader or simply proprietorship, is a type of business entity that is owned and run by one individual and in which there is no legal distinction between the owner and the business. The owner receives all profits (subject to taxation specific to the business) and has unlimited responsibility for all losses and debts. Every asset of the business is owned by the proprietor and all debts of the business are the proprietor’s.\nSole Proprietorships are the most numerous form of business organization in the United States, however they account for little in the way of aggregate business receipts. It controls 72% of business in the United States however, 20% end up filing for bankruptcy. Although the number is large when it comes to income and sales, the number is minute. When starting a sole proprietorship, one should open it when they are certain they won’t fail, file a 1040, and must be cautious that if you violate regulation, you may be out of business because of the government, however, they cant touch your pension.\nThere are many advantages and disadvantages to this form of business. The many advantages are: ease and cost of information, secrecy, distribution and use of profits, flexibility and control of the business, minimal government regulations, and easy taxation. The many disadvantages are: unlimited liability, limited sources of funds (banks don’t like to lend money), limited skills, lack of continuity, lack of qualified employees, and taxation.\nAnother form of business organization is partnership. Partnership is a form of business organization defined by the uniform partnership act as an “association of two or more person who carry on as co-owners of a business profit.” Marriages can be problematic and many of them do fail. There are two kinds of partnerships. A General Partnership is a partnership that involved a complete sharing in both the management and liability of the company.\nA Limited Partnership is two or more partners united to conduct a business jointly, and in which one or more of the partners is liable only to the extent of the amount of money that partner has invested. Limited partners do not receive dividends, but enjoy direct access to the flow of income and expenses. When starting a partnership one must file a 1065, then transfer to 1040, but everyone must file separately. Every company has its own code standard industrial classification system as well.\nThere are advantages and disadvantages of partnerships as well. The many advantages are: ease and organization, capital and credit, knowledge and skills, decision making, and regulatory control. The disadvantages are: unlimited liability, business responsibility, life of the partnership, distribution of profits, and limited sources of funds. The articles of a partnership are: name, purpose location, duration of the agreement, authority and responsibility of each partner, character of partner, amount of contribution from each partner, division of profit or losses and salaries of each partner.\nThe keys of success to a partnership are: keep profit sharing and ownership at 50/50, honesty is crucial, experience is essential, transparency, maintain face to face communication, family is priority, be realistic, awareness of funding constraints and limited resources, and partners should have different and complementary skill sets.\nThe last form of business organization is corporation. A corporation is a legal entity created by the state whose assets and liabilities are separate from its owners. Corporations generate the largest income, and the most sales. Articles of incorporation are legal documents filed with basic information about the business with the appropriate state office. These articles include the name and address of corporation, objectives of the corporation, classes of stock, and financial capital required at time of incorporation. There are many types of corporations such as domestic corporation, foreign corporation, alien corporation, private corporation, public corporation, initial public offering, quasi-public corporation, and non-profit corporation. The elements of corporation include: a board of directors, preferred stock, and common stock.\nThere are many advantages and disadvantages of corporations. The advantages are: limited liability, transfer of ownership, perpetual life, external sources or funds, and expansion potential. The disadvantages are: double taxation, forming a corporation, disclosure of information, and employee-owner separation.\nOther types of business ownership include; a joint venture which is a partnership established for a specific project for a limited time. S- Corporation which is a corporation taxed as though it were a partnership (no double taxation) with restriction on shareholders. LLC which is a flexible form of enterprise that blends elements of partnership and corporate structures. Co-op which is an organization of individuals or small businesses that have banded together to reap the benefits of belonging to a larger organization. Mergers which is the combination of two companies to form a new company.\nForward Integration which is when a manufacturer buys the wholesalers and retailers, in control of all their intermediaries and distribution. Backwards Integration which is when the retailers buys the wholesalers and manufacturer. Acquisition which is the purchase of one company by another, usually by buying its stock and/or assuming its debt and Leverage buyout which is a purchase, in which a group of investors borrows money from banks and other institutions to acquire a company, using the assets of the purchased company to guarantee payment of the loan.","Home > Tax Articles > Small Business Structure: Is Business Partnership Right for You?\nSmall Business Structure:\nIs Business Partnership Right for You?\nBy Caron Beesley\nFrom Bill Hewlett and David Packard, Google's Larry Page and Sergey Brin, even Ben and Jerry - some of the biggest success stories in recent business history have hinged on collaborative business partnerships.\nAsides from aspiring to blaze a trail in the mode of these history makers, business partnerships offer small business owners many benefits in areas such as taxation, shared risk, access to funds, etc.\nSo whether you are looking to partner with your best friend, former boss, or even your spouse, here are some of the options and considerations every small business owner should know before diving into business partnership.Benefits of Business Partnerships\nEach state has different laws that apply to business partnerships, but generally small business owners who enter into a legal partnership will enjoy the following operational benefits:\n- Business partnerships are relatively simple to set up and partners can share the start-up costs; however take the time to develop a legal partnership agreement and don't forget an exit strategy\n- Record keeping is typically less than that of a corporation\n- ncome is taxed only once (corporations are taxed both on their income and on the share of the corporation's income that they receive as dividends) Disadvantages of Business Partnerships\nI've witnessed seemingly rock solid friendships destroyed as a result of partnerships going awry - sometimes leading to mutual law suits and bankruptcy.\nSo while there are clear benefits to partnerships, especially for start-ups and small businesses, as with all business decisions be sure to weigh the risks and understand the flipside to those benefits that I described above:\n- Partners are jointly and individually liable for the actions of the other partners. What does this mean? If your partner dies, or goes AWOL, you'll be liable for all the debts, not just half of them.\n- You'll have to share profits with others\n- Since decisions are shared, disagreements can occur\n- Some employee benefits are not deductible from business income on tax returns\n- The partnership may have a limited life; it may end upon the withdrawal or death of a partner. Forming a Partnership: Business Structure Options\nAs I mention above, each state has specific laws on the formation and dissolution of business partnerships as well as laws regarding the legal responsibilities of each partner. There are three forms of legal partnerships that business owners can consider - general partnership, limited partnership and partnership with limited liability, or a joint venture. You can get more detailed information about these three types of partnership from the Small Business Administration here\n.Have a Business Partnership Agreement\nOnce you've decided on the structure of your business partnership, much like a pre-nuptual agreement, you need to put pen to paper and record a partner agreement. The agreement should include:\n- The business name, structure, and partner information\n- A description of the type of business that will be conducted\n- How decisions will be made, the duties and responsibilities of the partners, as well as any limitations\n- Details of any current or future financial contributions any partner will make\n- How profits and losses will be handled\n- A plan for dissolution, or your exit strategy The Tax Obligations of Business Partnerships\nAs a business entity, the partnership itself does not pay taxes - each partner pays federal, state, and local taxes on their income from the partnership as if it were personal income.\nPartnerships do, however, need to file an annual \"information return\" to report income, deductions, gains, losses, etc. with the IRS.\nOnce you are operational as a partnership you can refer to information provided by the IRS here to help you determine some of the forms that you may be required to file.\nYou can find out more information about complying with tax laws for business partnerships via the IRS' \"Tax Information for Partnerships\n\" Web page."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:a5634736-0883-474f-9c1b-349d2c8d7f86>","<urn:uuid:294147e9-4b7b-4631-9d25-960ab8c92023>"],"error":null}
{"question":"What are the performance implications of Java number wrapper classes compared to primitives, and how does this technical choice affect energy consumption in blockchain applications that process millions of transactions?","answer":"Java wrapper classes have significant performance implications compared to primitives since each autoboxing/unboxing operation requires method calls behind the scenes. This can create performance penalties when processing large numbers of operations. Primitives are more efficient as they only store the raw value. This performance difference becomes critical in blockchain applications, as crypto-asset operations already consume massive amounts of electricity - between 0.9% to 1.7% of total U.S. electricity usage, similar to all home computers or residential lighting. Using less efficient programming approaches in such energy-intensive applications could further increase their already substantial environmental impact, which currently results in 25-50 million metric tons of CO2 emissions annually in the US alone.","context":["What is autoboxing and unboxing in Java? – J045\nDeegeU Java Course\nThe “What is autoboxing and unboxing in Java?” video is part of a larger free online class called “Free Java Course Online”. You can find more information about this class on “Free Java Course Online” syllabus.\nTranscript – What is autoboxing and unboxing in Java?\nHi there! For the next section of videos, we’re going to focus on the String and Number classes. Let’s start with the number classes, and then we’ll talk about how Java uses autoboxing and unboxing to convert these classes to and from Java primitives.\nWhat are the number classes?\nIn earlier videos we looked at the primitive data types for numbers. They were the bytes, shorts, ints, longs, floats, and doubles.\nThe java.lang package also includes a wrapper class for each of these primitive data types. All of the number types are subclasses of the Number class.\nFor example, the byte primitive has a matching Java class called Byte. The difference when declaring a Java variable using a class is the class version is uppercase. That’s because all classes in Java are declared starting with a capital letter by convention.\nThe other difference is the matching class for ints are Integers. I think the reason is int matches closely with C primitives, the language Java mimics. There’s no defined class in C for integers, so in Java they were free to use the full word for the class.\nThese wrapper classes are immutable. Immutable means once we create an object instance with a value, the value cannot be changed.\nA wrapper class is a class that “wraps” the functionality of another class or component. We’d do this to add functionality to an existing class without using inheritance, or converting a primitive data type into a class. In this case, the wrapper classes are “wrapping” a number primitive.\nThe number classes also include BigInteger, BigDecimal, AtomicInteger, and AtomicLong. BigInteger and BigDecimal are used for high precision applications where you need to accurately represent the number. Other than a cool sounding name, AtomicInteger and AtomicLong are used for multi-threaded applications. We’ll cover these classes in later lessons.\nWhy do they duplicate the number primitives?\nYou’re probably wondering, why? Why do we have classes for numbers, when we already have the number primitives. There’s several reasons.\nFirst there are other classes with methods which expect objects as the parameter. The main ones are Java collection classes. We’ll have many lessons discussing the Java collection classes, but these classes represent things like linked lists and they only hold classes.\nThe other reason is these classes have defined constants and methods associated with the number type. For example, the number classes have constants like the maximum and minimum values. So we never need to memorize this long number! It’s much easier to read MAX_INT than a long string of digits, and we’ll never type it wrong.\nThe number classes also have many methods for manipulating the data type, like converting integers to a hexadecimal or binary representation. We’re going to cover many of these methods in the next few lessons. For right now, let’s look at how Java converts between the class and primitive numbers.\nAutoBoxing and Unboxing numbers\nJava converts between numbers and primitives automatically using something called autoboxing and unboxing.\nAutoboxing is when Java automatically converts a number primitive to an instance of the corresponding number class. The simplest example of this is when you declare a number class instance.\nWe know we can create a class instance using the new keyword. We’ve done this many times, and for the number classes we can pass the number we want as an argument to the constructor. So here we are creating an Integer instance with the value of 8.\nThis is a special rule in the Java language, but you can also define the number like you would create a primitive. That looks like this.\nWhat happens is the compiler converts the number 8 to an integer class by replacing the declaration to look like this. The valueOf method takes a primitive, and returns an instance of the class. This happens automatically. We never see it happen in our code. This is autoboxing.\nAutoboxing also happens when you pass a primitive to a method expecting a object. For example, if we had a method like this, we can pass an primitive. Java will convert it to a class instance for us. Again, this is automatic.\nUnboxing happens in the other direction. This is when we have the number class, but Java is expecting the primitive. In this case, Java “unboxes” the primitive from the wrapper class.\nFor example, let’s use the Integer class in an if-then test. We create the integer, perform an operation on it, and then the test. Remember i in this example is an instance, not a primitive. Still Java knows to unbox the instance into a primitive. Behind the scenes, the compiler is replacing our code with the intValue method. Java is doing this automatically for us. This is called unboxing.\nPrefer primitives to boxed numbers\nThe next question we’re likely asking is, why use primitives at all.\nPrimitives have their value and nothing more. Boxed values have identities associated with the object. This means that two boxed values can have different identities. Usually. There’s a weird quirk to this, and if you want to know more about it, check out the weird Java number tricks video.\nBoxed values also have the possibility to be null, meaning the instance can have a primitive value or no value. We could get into a case where we think we’re comparing numbers, but the instance gets us into trouble. Imagine we create an Integer instance, but we never create a value. Now compare the number. This gives us a NullPointerException. The reason is we’re unboxing the number by calling the intValue method, but the method is not a static method. It’s an instance method, but our instance is null. So boom.\nThe key to remember is when we autobox or unbox, behind the scenes a method is getting called. This can lead to performance penalties if we’re doing this for millions of numbers. We should always prefer primitives if we can get away with it.\nThat’s it for Java autoboxing and unboxing. If you have any questions, add them to the comments below. Next we’ll look at the methods and constants inside the Number classes. Liking the video helps me know what I’m doing right, so if you liked the video… you know what to do.\nAnd with that, I’ll see you in the next tutorial!\nAll media created and owned by DJ Spiess unless listed below.\n- No infringement intended\nSmooth Sailing (with Guitar) by Audionautix is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/…)\nGet the code\nThe source code for “What is autoboxing and unboxing in Java?” can be found on Github. If you have Git installed on your system, you can clone the repository by issuing the following command:\ngit clone https://github.com/deege/deegeu-java-intro.git\nGo to the Support > Getting the Code page for more help.\nDon’t miss another video!\nNew videos come out every week. Make sure you subscribe!\nYour personal instructor\nMy name is DJ Spiess and I’m a developer with a Masters degree in Computer Science working in Colorado, USA. I primarily work with Java server applications. I started programming as a kid in the 1980s, and I’ve programmed professionally since 1996. My main focus are REST APIs, large-scale data, and mobile development. The last six years I’ve worked on large National Science Foundation projects. You can read more about my development experience on my LinkedIn account.","FACT SHEET: Climate and Energy Implications of Crypto-Assets in the United States\nClimate change is one of the most pressing problems confronting our nation and our world, and President Biden has taken bold steps to address it with legislation and policy. Among the President’s commitments are: protecting communities from pollution, reducing greenhouse gas emissions by 50% by 2030, achieving a carbon pollution-free electricity grid by 2035, and reaching net-zero greenhouse gas emissions no later than 2050.\nTo achieve these ambitious goals, we must ensure that emerging technologies contribute to a net-zero, clean energy future. The use of digital assets based on distributed ledger technology (DLT) is expanding. Digital assets are a form of value, represented digitally. As an emerging technological innovation, digital assets have provided some benefits and value for some residents and businesses in the United States, and have the potential for future benefits with emerging uses.\nCrypto-assets are digital assets that are implemented using cryptographic techniques. Crypto-assets can require considerable amounts of electricity usage, which can result in greenhouse gas emissions, as well as additional pollution, noise, and other local impacts to communities living near mining facilities. Depending on the energy intensity of the technology and the sources of electricity used, the rapid growth of crypto-assets could potentially hinder broader efforts to achieve U.S. climate commitments to reach net-zero carbon pollution.\nIn March, in Executive Order 14067 on Ensuring the Responsible Development of Digital Assets, President Biden made clear that the responsible development of digital assets includes reducing negative climate impacts and environmental pollution. The Executive Order directed the White House Office of Science and Technology Policy (OSTP), in coordination with other federal agencies, to produce a report on the climate and energy implications of crypto-assets in the United States. OSTP assembled an interdisciplinary team of experts to assess and extend existing studies with new analysis, based on peer-reviewed studies and the best available data.\nToday, OSTP published its report, examining the challenges and opportunities of crypto-assets for the United States’ clean energy and climate change goals, and providing a set of recommendations to further study and track impacts of the sector, develop potential performance standards, and provide tools and resources to reduce negative impacts. This report’s assessment and recommendations align with federal actions that reduce greenhouse gas emissions to protect public health and welfare, grow a clean energy economy with good-paying jobs, and improve environmental justice.\nCrypto-Assets Can Be Energy-Intensive, and the United States Has a Major Crypto-Asset Sector\nFrom 2018 to 2022, annualized electricity usage from global crypto-assets grew rapidly, with estimates of electricity usage doubling to quadrupling. As of August 2022, published estimates of the total global electricity usage for crypto-assets are between 120 and 240 billion kilowatt-hours per year, a range that exceeds the total annual electricity usage of many individual countries, such as Argentina or Australia. This is equivalent to 0.4% to 0.9% of annual global electricity usage, and is comparable to the annual electricity usage of all conventional data centers in the world.\nNearly all crypto-asset electricity usage is driven by consensus mechanisms: the DLT used to mine and verify crypto-assets. The dominant consensus mechanism is called Proof of Work (PoW), which is used by the Bitcoin and Ethereum blockchains. Bitcoin and Ether combined represent more than 60% of total crypto-asset market capitalization. The PoW mechanism is designed to require more computing power as more entities attempt to validate transactions for coin rewards, and this feature helps disincentivize malicious actors from attacking the network. As of August 2022, Bitcoin is estimated to account for 60% to 77% of total global crypto-asset electricity usage, and Ethereum is estimated to account for 20% to 39%.\nThe energy efficiency of mining equipment has been increasing, but electricity usage continues to rise. Other less energy-intensive crypto-asset ledger technologies exist, with different attributes and uses. Switching to alternative crypto-asset technologies such as Proof of Stake could dramatically reduce overall power usage to less than 1% of today’s levels.\nThe United States is estimated to host about a third of global crypto-asset operations, which currently consume about 0.9% to 1.7% of total U.S. electricity usage. This range of electricity usage is similar to all home computers or residential lighting in the United States. Crypto-asset mining is also highly mobile. The United States currently hosts the world’s largest Bitcoin mining industry, totaling more than 38% of global Bitcoin activity, up from 3.5% in 2020. Despite the potential for rapid growth, future electricity demand from crypto-asset operations is uncertain, demonstrating the need for better data to understand and monitor electricity usage from crypto-assets.\nCrypto-Assets Can Have Significant Environmental Impacts\nGlobal electricity generation for the crypto-assets with the largest market capitalizations resulted in a combined 140 ± 30 million metric tons of carbon dioxide per year (Mt CO2/y), or about 0.3% of global annual greenhouse gas emissions. Crypto-asset activity in the United States is estimated to result in approximately 25 to 50 Mt CO2/y, which is 0.4% to 0.8% of total U.S. greenhouse gas emissions. This range of emissions is similar to emissions from diesel fuel used in railroads in the United States.\nBesides purchased grid electricity, crypto-asset mining operations can also cause local noise and water impacts, electronic waste, air and other pollution from any direct usage of fossil-fired electricity, and additional air, water, and waste impacts associated with all grid electricity usage. These local impacts can exacerbate environmental justice issues for neighboring communities, which are often already burdened with other pollutants, heat, traffic, or noise. The growth of energy-intensive crypto-asset technologies, when not directly using clean electricity, could hinder the ability of the United States to achieve its National Determined Contribution under the Paris Agreement, and to avoid the most severe impacts of climate change. Broader adoption of crypto-assets, and the potential introduction of new types of digital assets require action by the federal government to encourage and ensure responsible development. This includes minimizing negative impacts on local communities, significantly reducing energy intensity, and powering with clean electricity.\nDistributed Ledger Technologies May Help with Climate Monitoring or Mitigation\nDLT may have a role to play in enhancing market infrastructure for a range of environmental markets like carbon credit markets, though other solutions might work as well or better. The potential benefits of DLT would need to outweigh the additional emissions and other environmental externalities that result from operations to merit broader use, relative to the markets or mechanisms that DLT displaces. Use cases are still emerging, and like all emerging technologies, there are potential positive and negative use cases yet to be imagined. Responsible development of this technology would encourage innovation in DLT applications while reducing energy intensity and minimizing environmental damages.\nKey Recommendations of the Report\nTo help the United States meet its climate objectives, crypto-asset policy during the transition to clean energy should be focused on several objectives: reduce greenhouse gas emissions, avoid operations that will increase the cost of electricity to consumers, avoid operations that reduce the reliability of electric grids, and avoid negative impacts to equity, communities, and the local environment.\nTo ensure the responsible development of digital assets, recommendations include the following actions for consideration:\n- Minimize greenhouse gas emissions, environmental justice impacts, and other local impacts from crypto-assets: The Environmental Protection Agency (EPA), the Department of Energy (DOE), and other federal agencies should provide technical assistance and initiate a collaborative process with states, communities, the crypto-asset industry, and others to develop effective, evidence-based environmental performance standards for the responsible design, development, and use of environmentally responsible crypto-asset technologies. These should include standards for very low energy intensities, low water usage, low noise generation, clean energy usage by operators, and standards that strengthen over time for additional carbon-free generation to match or exceed the additional electricity load of these facilities. Should these measures prove ineffective at reducing impacts, the Administration should explore executive actions, and Congress might consider legislation, to limit or eliminate the use of high energy intensity consensus mechanisms for crypto-asset mining. DOE and EPA should provide technical assistance to state public utility commissions, environmental protection agencies, and the crypto-asset industry to build capacity to minimize emissions, noise, water impacts, and negative economic impacts of crypto-asset mining; and to mitigate environmental injustices to overburdened communities.\n- Ensure energy reliability: DOE, in coordination with the Federal Energy Regulatory Commission, the North American Electric Reliability Corporation and its regional entities, should conduct reliability assessments of current and projected crypto-asset mining operations on electricity system reliability and adequacy. If these reliability assessments find current or anticipated risks to the power system as a result crypto-asset mining, these entities should consider developing, updating, and enforcing reliability standards and emergency operations procedures to ensure system reliability and adequacy under the growth of crypto-asset mining.\n- Obtain data to understand, monitor, and mitigate impacts: The Energy Information Administration and other federal agencies should consider collecting and analyzing information from crypto-asset miners and electric utilities in a privacy-preserving manner to enable evidence-based decisions on the energy and climate implications of crypto-assets. Data should include mining energy usage and fuel mix, power purchase agreements, environmental justice implications, and demand response participation. OSTP could establish a National Science and Technology Council subcommittee to coordinate with other relevant agencies to assess the energy use of major crypto-assets.\n- Advance energy efficiency standards: The Administration should consider working with Congress to enable DOE and encourage other federal regulators to promulgate and regularly update energy conservation standards for crypto-asset mining equipment, blockchains, and other operations.\n- Encourage transparency and improvements in environmental performance: Crypto-asset industry associations, including mining firms and equipment manufacturers, should be encouraged to publicly report crypto-asset mining locations, annual electricity usage, greenhouse gas emissions using existing protocols, and electronic waste recycling performance.\n- Further research to improve understanding and innovation: For improved analytical capabilities that can enhance the accuracy of electricity usage estimates and sustainability, the National Science Foundation, DOE, EPA and other relevant agencies could promote and support research and development priorities that improve the environmental sustainability of digital assets, including crypto-asset impact modeling, assessment of environmental justice impacts, and understanding beneficial uses for grid management and environmental mitigation. Research and development priorities should emphasize innovations in next-generation digital asset technologies that advance U.S. goals in security, privacy, equity, and resilience, as well as U.S. climate goals."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:2ef0dfdc-b242-4677-ae28-147cbf9eaee4>","<urn:uuid:c2b3e9e1-0ef3-465d-9a23-92fdbe2309a6>"],"error":null}
{"question":"What preservation methods and materials were employed in restoring these historical structures, considering both the St. George Tower's Art Deco elements and Salubria's post-earthquake reconstruction?","answer":"Both restorations required specialized materials and methods. The St. George Tower restoration involved cleaning and salvaging original ornaments while replicating severely damaged pieces like figure heads, with careful attention to matching the original design. For Salubria, the restoration included using specially ordered sand-molded, handmade bricks from Old Carolina Brick to match original chimney bricks, with about 30% needing replacement. The process involved careful deconstruction of damaged elements, preservation of original materials where possible, and implementation of specific techniques like casein liquefied mortar injection for wall repairs. Both projects emphasized maintaining historical accuracy while addressing structural issues, demonstrating the balance between preservation and necessary repairs in historical buildings.","context":["The St. George Tower in Brooklyn Heights, once a prime specimen of Jazz-Age opulence and Art Deco artisanship, was recently restored to its former grandeur.\nThe 315-foot tower was designed by famous Hungarian architect, Emery Roth, in the 1920s, as an addition to the original St. George Hotel. Roth, who also famously worked on luxury apartment buildings like the Ardsley and San Remo on Central Park West, had a lasting impact on the New York City architectural landscape. Resplendent with both Egyptian and Art Deco motifs, the St. George Hotel’s property is a prime example of that legacy.\nThe hotel, which once consisted of both the original hotel building and the newer tower, provided guests with majestic views of the Manhattan skyline, along with grand spaces in which to dine and dance. Truman Capote often swam in its Olympic-sized indoor saltwater pool, and Frank Sinatra once stayed the night, as did Presidents Truman and Franklin D. Roosevelt. It drew some of the hottest celebrities and socialites of the time, many of whom came to dance in the cavernous Colorama ballroom.\nThe tower’s completion in 1929 made the St. George the largest hotel in New York at the time, offering 2,632 guest rooms and one thousand full-time staff. The New York Times called the St. George a “beacon that attracted some of the brightest lights in American society.” It was also one of the best-known centers of gay male life in New York from the 1920s through the 1970s, and the hotel once housed troops and their families during World II and the Korean War.\nBklyner reporting is supported by our subscribers and:\nIt became an official New York City landmark in 1965, when the neighborhood, Brooklyn Heights, was designated the first historic district by the newly minted Landmarks Preservation Commission.\nWhile it now consists of three towers, St. George Hotel was once a tiny city of its own, occupying over seven buildings and the entire city block bounded by Clark, Hicks, Pineapple, and Henry Streets. The Tower was converted into a luxury co-op with 275 units across 30 floors in 1984. The old hotel building – the Weller Wing – became dormitories for local colleges – Pratt Institute, St. Joseph’s College, and other institutions around the city.\nFor the façade to match up to the building’s illustrious history, Tower’s owners tapped NYC architectural firm CTA Architects to restore it, along with some much needed structural repairs. CTA Architects had worked on repairs and restoration on the building in 2008. The project began in 2015 and took over four years to complete, winding up in 2019 and costing approximately $2.8 million.\nDaniel Allen, principal at the firm and one of the spearheads of the project, explained that the most complex task the project faced was to rebuild the brick masonry surrounding the building’s steel frame, and to reinforce the steel itself – the masonry was packed too tight to allow for proper drainage, and the steel was not waterproofed, leading to rust and erosion. Once they discovered the issue with the steel frame, the work “basically quadrupled.”\nOn top of that, they needed to restore some of the building’s more timeworn ornaments, such as the 6-foot-tall Egyptian terracotta figures on the 27th floor, and the eagles projecting from the base of one of the walls.\n“We are a big believer in restoration rather than replacement, if we can do it,” said Allen. While they were forced to replicate certain pieces, like the heads of several figures, which were damaged beyond repair, they were able to clean and salvage many of them. In addition, a huge priority of the project was to make sure that nothing on the exterior varied from the original, and the firm worked closely with the New York City Landmarks Preservation Commission to make sure they stayed within those historical bounds.\n“You can make minor changes, but the appearance should be as faithful as possible to the original design,” Allen said.\nAllen, who teaches a course on architectural terracotta at the Columbia School of Preservation, was thrilled to work on the project, alongside his partner G. Douglas Cutsogeorge (now retired).\n“New York is like a museum of interesting buildings,” Allen said. “And each one tells a lot about the time that it was built. The St. George hotel at 111 Hicks is a perfect example of that.”","By Doug Harnsberger\nSpring 2012 Germanna Newsletter\nAt 1:51 pm on August 23, 2011, the terra firma beneath Salubria rumbled and heaved the stately 1757 Georgian-style house.\nBy the time the quaking action had subsided, the 5.8 magnitude Mineral Earthquake had ruptured and twisted both of the twenty foot high chimney stacks twenty degrees, and sent hundreds of pounds of loose bricks cascading from the severed chimney caps down onto the shingled roof below.\nThe cascading bricks hit the roof with such a powerful force that seven of the interior roof rafters fractured within the attic.\nIn addition, the quake’s rolling and rocking opened up a dozen new stress cracks in the once-solid brick perimeter walls.\nJust as many of the houses and churches in the nearby Town of Culpeper were rocked violently, the Mineral Quake shook Salubria with similar effects.\nIn its wake, the quake left considerable structural and cosmetic damage upon the oldest and finest brick residence in Culpeper County.\nThe Germanna Foundation’s first urgent priority for stabilizing Salubria’s earthquake damage was to deconstruct the twisted and ruptured chimney stacks before they might collapse through the roof.\nWithin a few days of the earthquake, the renowned historic preservation crew of Price Masonry from Madison Heights, Virginia worked from two opposing bucket lifts to gingerly wrap the chimney stacks with fiberglass straps in order to stabilize them, before beginning the brick-by-brick deconstruction process.\nAll of the displaced chimney bricks were brought down to grade in buckets, where they were cleaned of mortar and stacked for their eventual reuse.\nAbout 30% of the brick were cracked or damaged beyond reuse by the quake’s actions, so a mix of matching replica brick will be required for the two chimneys’ reconstruction.\nIn anticipation of the chimney reconstruction work that is scheduled to begin on April 15, 2012, the Germanna Foundation placed an order in January for three thousand sand-molded, handmade, oversize brick with Old Carolina Brick in Salisbury, N.C.\nThese custom-made bricks will closely match Salubria’s original chimney brick in size, texture and color.\nThe reconstruction work on the two chimneys is expected to take approximately two months, which means that the chimneys’ elegant twenty-foot-high profile above the roofline should be fully visible again for everyone to enjoy by mid-June.\nRepairing the Fractured Roof Rafters\nQuake damage to the original circa 1757 attic roof rafters was less obvious than the chimney destruction.\nThe above photo shows project Germanna member Robert Ellis pointing to one of seven fractured rafters in the attic. The fractures all were located just below the heavy purlin structural member at the rafters’ midspan.\nInstead of simply removing and replacing these seven broken rafters, however, the Germanna Foundation will repair them in situ and restore to their original locations prior to the earthquake’s dislocation.\nNew “sister” rafters will be inserted adjacent to each of the repaired rafters to assume the active roof load.\nThe historic rafters will be relieved of the ongoing roof stresses, and preserved-in-place for future architectural historians to observe and analyze.\nRepairing the Quake-induced Stress Cracks in Salubria’s Brick Walls\nLateral and vertical earthquake forces moving through a solid brick wall often will produce diagonal stress cracks, as seen in the above photo.\nIn this particular example along a Second Floor partition wall, the bricks pulled apart about an inch, leaving a continuous diagonal line of severed bricks and localized plaster damage.\nSalubria’s brick wall stress crack restoration work will begin in April of 2012, after the last freeze of winter.\nAll of the fractured wall bricks across the cracks will be removed and replaced with matching whole bricks.\nThe stress crack cavities within the walls will be filled with a special casein liquefied mortar injected into the wall cavities.\nPrice Masonry’s lead preservation mason Jimmy Price has promised to give us another of his signature restoration demonstrations using the casein injection process, so that all those who wish to learn firsthand about this masonry conservation technique may benefit from his expertise.\nWe will announce the day and time of the demonstration (sometime in May) on the Germanna Website.\nResearching “Best Conservation and Restoration Practices” for Salubria’s Upcoming Roof Restoration Work\nThe Salubria Restoration Design Team of Legacy Architecture, LLC and HITT Contracting has made recent field trips to Colonial Williamsburg’s Wythe House, James Madison’s Montpelier, George Mason’s Gunston Hall, Washington’s Mount Vernon and Historic Kenmore in Fredericksburg to observe “best conservation and restoration practices” as these institutions have applied them.\nThe roof restoration projects executed by Peter Post, Historic Roofer, at Montpelier, Mt. Vernon and Gunston Hall has provided us with exemplary models for the kind of high-quality decorative wood shingle work that we shall achieve soon at Salubria.\nThe existing decorative shingles on Salubria’s roof date back to a 1982 installation, so they are over three decades old now.\nWe have found evidence in the attic that several of the historic rafters suffered significant water damage from the prolonged rainwater leaks through the roof.\nThe Germanna Foundation has committed to repairing the damage to the Salubria’s exquisite heavy timber roof structure and to installing a new decorative cedar shingle roof this summer, from June through September.\nMr. and Mrs. Russell Hitt, and HITT Contracting, Inc. have generously made a challenge grant of $195,000 to ensure the stabilization and conservation work begins as soon as possible.\n“To everything there is a season,” Ecclesiastes tells us. So, this season – this year – is Salubria’s “time to build up.”\nThe Germanna Foundation respectfully solicits your contribution, at whatever level you can help to match the HITT Challenge Grant and counteract the destructive forces of the Mineral Earthquake. Salubria’s preservation depends now on our collective response."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:ffc36a7f-ed35-4901-92ee-9ee292a38417>","<urn:uuid:41dc9641-814f-4f30-85cf-257d10858c49>"],"error":null}
{"question":"How do the effects of climate change on crop yields differ between food-secure and food-insecure regions of the world?","answer":"The impacts of climate change on crop yields show a stark contrast between food-secure and food-insecure regions. Half of the food-insecure countries are currently experiencing decreases in crop yields. While some wealthy regions like the U.S. Midwest have benefited from climate changes through longer growing seasons and technological adaptation, many other regions lack these advantages. U.S. farmers benefit from the world's most advanced agriculture industry, substantial capital for expensive equipment, and government crop insurance subsidies. In contrast, regions like sub-Saharan Africa have the lowest yields and millions lacking food security, with limited data available to even track changes. The impacts were found to be mostly negative in Europe, Southern Africa, and Australia, while Latin America showed some positive effects.","context":["Few of climate change’s varied dangers are more dire than its potential to make the world’s farms produce less food. While we live in boom times of agricultural abundance, marked by record crop yields and cheap food, climate change threatens to slash yields and cause worldwide food busts. According to the Intergovernmental Panel on Climate Change and the recently released U.S. National Climate Assessment, droughts, temperature extremes and plagues of insect pests will increasingly harm farming worldwide.\nBut some experts hope humanity will be able to weather the wiles of climate change through adaptation -- that farmers will be able to overcome climate-related threats by adjusting how they grow crops.\nA new study analyzing U.S. corn yields over the last few decades seems to support that notion. It looked at temperature trends in the Midwest, examined how farmers adjusted their planting practices to take advantage of them, and showed that U.S. farmers were able to grow more corn, not less.\n“It gives me some hope that when we see climate trends that are not quite so benevolent, farmers will have some tools at their disposal,” said Nathan Mueller, an Earth scientist at the University of California, Irvine and co-author of the study.\nNot everyone is as sanguine, however. Some experts point to the fact that among all crops, corn may be the exception, not the rule, that demonstrates how adaptation can work.\n“The risk is that if people overstate the benefits of adaptation, it’s sort of like, OK, the environment can change and we can figure out how to deal with it,” said David Lobell, an environmental scientist at Stanford University. “I’ve seen that happen so many times, I’m cautious when I talk to reporters about overemphasizing the ingenuity of what people can do.”\nA highly tuned system\nThe humble cornfield may be among the least appreciated of modern marvels. Billions of dollars of research and development has given humans control of nearly every aspect of that field, from germination and maturation timing to nutrient levels to planting density to even the angle that a plant's leaves take with respect to the sun. Perhaps no crop has benefited more from this technological binge than corn.\n“A cornfield is really a very highly tuned, technological system,” said Mueller.\nU.S. farmers today harvest five to six times more corn per acre than their predecessors 100 years ago. Yields of other crops and in other parts of the world have also ballooned -- though not quite as dramatically. Despite a fast-growing human population, the number of food-deprived people around the globe fell year after year until 2014. After years of decline, that number has ticked up every year since.\nOne critical factor is fickle weather, which remains largely outside of modern technological control. Farmers worldwide rely on temperatures warm enough to stimulate fast plant growth, but just cool enough to avoid parching crops. Many productive agricultural regions hover near that optimum, meaning small temperature increases could seriously stress crops -- especially if accompanied by drought. In a globally connected economy, the prices people pay at the grocery store -- and, due to the increasing use of ethanol in gasoline, at the fuel pump -- depend on how much farmers produce everywhere.\nWarnings about climate change often refer to the future, but climate change has actually been happening for a long time. Over the past four decades, the average global temperature has increased by nearly 1 degree Fahrenheit, according to NASA. That uptick is both good and bad for farmers: It provides longer growing seasons, but also increases the chance of crop-killing heat, expands the range of some insect pests and makes intense storms and droughts more likely.\nNot all regions on Earth track with the global average, however. Studies going back to the early 2000s showed that some of the world’s breadbasket regions were bucking the trend, warming less during summer days than surrounding areas. Summer daytime highs in much of the U.S corn belt -- a broad swath of rural areas in the Midwest roughly centered in Illinois and encompassing parts of 12 states -- have actually dipped slightly.\nA few years ago, Mueller was working at Harvard University with his colleague Ethan Butler, who is now at the University of Minnesota. They decided to investigate what they call “peculiarly pleasant\" weather, which they discovered is an unintended outcome of yield-enhancing practices, combined with a quirk of plant physiology.\nIn a process called transpiration, plants pull water up from the soil and release some of it through small pores in leaves called stomata. Just as evaporating sweat cools your body, transpiring water from growing plants cools Earth’s surface. That means farm fields (and, for that matter, forests) act a bit like giant outdoor air conditioners.\nPlant breeders have also bred corn plants to grow much closer together than they used to. In addition, said Eugene Takle, a meteorologist at Iowa State University, Midwestern springs have gotten wetter, so soils hold more water for plants to take up and transpire. More plants per acre pushing out more water means more cooling. Mueller, Butler and colleagues analyzed temperature and crop data from Midwestern states, and reported in 2015 that enough water came out of increasingly productive cornfields to, over the past few decades, literally change the climate. The effect took the edge off the hottest summer days -- exactly those days most likely to stress or kill corn plants.\nThis peculiarly pleasant weather may have had a perverse consequence, Butler speculates: Farmers who benefit from climate change may be less likely to see it as a threat that needs to be counteracted. Indeed, surveys have found many Midwestern farmers either don’t accept the scientific consensus on climate change, or don’t see it as a serious problem.\n“Our paper reinforces why Midwestern farmers might be skeptical of anthropogenic climate change contributing to global warming, because the hottest temperatures across Midwest haven’t increased. They’ve declined!” Butler said. “It’s a surprising aftereffect of modern agriculture.”\nMaking corn's production pop\nWhat still wasn’t clear is whether farmers were just passively benefiting from favorable climate changes, or adapting their behavior to take maximum advantage of them. To test this, Butler worked with Mueller and Harvard climate scientist Peter Huybers to build a computer model that breaks the growing season into three consecutive periods. In the first, plants germinate and grow tall. During the second, they are pollinated; during the third, kernels develop. The researchers incorporated state or county data from the U.S. Department of Agriculture on crop development timing and yields, along with temperature logs from weather stations, to quantify the vulnerability of corn during each period of the growing season in every year from 1981 to 2017.\nThe researchers found that as spring temperatures warmed, farmers responded by planting corn up to several weeks earlier. That resulted in plants being more likely to get through their most temperature-sensitive period -- pollination -- before the hottest temperatures hit. And it gave plants more time in late summer to form big, plump kernels. In total, Midwest corn yields nearly doubled during the study period, from 2.7 to 4.9 tons per acre.\nBy running their model under various scenarios, the researchers determined that the portion of that growth that could be attributed to the longer growing season combined with the air conditioning effect accounted for 28 percent of that yield growth. (The remaining 72 percent was caused by technological and breeding advances, plant growth enhancement due to higher carbon dioxide levels, and other factors.)\nButler and Mueller also modeled what corn yields would have been had farmers used the most recent decade’s planting timing during the study’s first decade -- 1981 to 1990. The researchers found that the recent climate enabled farmers’ earlier planting schedules to generate an extra 40 pounds of corn per acre. This bump shows that farmers adapted, albeit modestly, to the benevolent climate changes, the team reported last month in the Proceedings of the National Academy of Sciences.\n“It was a very convincing and well done study with an interesting and important result,” said Michelle Tigchelaar, a climate scientist at the University of Washington in Seattle. She applauds the team for producing a model that captures both the planting timing and plant cooling effects.\nRay Gaesser, who farms 5,000 acres of corn and soybeans in southwestern Iowa, said the study aligns with his experiences. He has purchased seed varieties that can tolerate cold, wet soil and equipment that allows him to plant lots of acres quickly in between spring rains. As a result, Gaesser plants a week or more earlier than when he started farming four decades ago, and his corn yields have increased 80 percent.\nCrop breeding companies also played a role in the increase, for example by crossing high-yielding American corn varieties with more heat-tolerant Mexican varieties, said Jeffrey Seale, environmental strategy lead at Bayer Crop Science (formerly Monsanto). Such strategies combined with advanced genomic techniques can keep yields strong even as the climate worsens, Seale believes. “Overall there’s a lot of optimism that this is a problem that’s solvable,” he said.\nWhen the world's farms meet climbing temperatures\nMueller and Butler’s study also raises questions. One: Are similar effects helping other crops and other regions? In a 2017 study, Mueller found evidence of crop-driven summer cooling in countries such as Canada, China and Argentina. Whether those countries’ farmers are also taking advantage of longer growing seasons would be harder to test, said Deepak Ray, an environmental scientist at the University of Minnesota. Few countries other than the U.S. publish such fine-grained agricultural statistics; China, one of the world’s most important agricultural regions, does not, for example. Data are even more sparse from sub-Saharan Africa, where farms tend to have the lowest yields and millions of people lack food security. “The area of the world where we need data the most, we have the least,” Ray said.\nStanford's Lobell doubts many other farmers are benefiting from climate change to the same extent. Few crops benefit from crowding like corn, he said. Farmers in the U.S. Midwest also enjoy the world’s most advanced agriculture industry, enough capital to afford expensive equipment and a government willing to back risky decisions with crop insurance subsidies. “My personal feeling is that people underestimate the good run the U.S. has had,” Lobell said, of “good policy, good luck and good technology.”\n\"But you can only benefit so much from those things,” he said.\nIn a 2014 analysis, he found that wheat and corn yields outside the U.S. were already suffering slightly due to warming. Climate change’s impacts on rice and soybean yields were more mixed.\nA recent modeling study predicted climate change would cut vegetable and legume yields, though it did not consider whether farmers could adapt. And Mueller and colleagues earlier this year reported a modeling study that suggested that warming could reduce global barley yields, potentially impacting the world’s production of beer.\nSimply looking at yields over time may not even capture all of climate change’s possible dangers for the global food supply. Earlier this year, Tigchelaar, Ray and colleagues modeled the probability that killing heat or drought would simultaneously and drastically reduce corn yields in the four largest corn-exporting countries. The probability of such multiple shocks rose from near zero today to 7 percent per year for a 2-degree Celsius warming -- a level the Intergovernmental Panel on Climate Change has already said will be hard to avoid. For a 4-degree warming, the chance of a single year seeing corn production busts of 10 percent or more in at least two countries jumped to a very probable 86 percent. In a separate study, Tigchelaar and other researchers predicted that insects would cause far more crop damage as the climate warms.\nGaesser, the Iowa farmer, said crop-threatening effects of climate change are already becoming apparent. In the last few years, his fields have been pummeled by rainstorms more intense than any he had seen previously. He started planting more cover crops to help his soils retain water and nutrients, and he plans to build retention ponds to store water from heavy rains for use during droughts. But he acknowledged that if the climate changes too quickly, even these steps may not be enough.\nIowa State meteorologist Takle questions whether farmers will even be able to keep paying for such measures as climate challenges become more severe. “The cost of adaptation is becoming more of an issue,” he said.\nButler shares such concerns about the future. In fact, his and Mueller’s study, despite its seemingly positive message, hints at a possible danger in relying too heavily on farmer adaptation. When a punishing drought hit the Midwest in 2012 -- the region’s first major drought since 1988 -- corn yields plummeted by 26 percent. Butler hypothesizes that when water wasn’t available in the soil for evaporative cooling, the more densely planted crops proved even more vulnerable to heat stress. In other words, the same mechanism that has been helping corn in good times may hurt it in the bad times. And with climate change, the bad times are forecast to become more common.\n“The story that we can just adapt ourselves out of climate change is not the one that we’re telling here,” Butler said.\nInside Science is an editorially-independent nonprofit print, electronic and video journalism news service owned and operated by the American Institute of Physics.","Climate change may already be reducing crop yields of some of the world’s most important crops and food-insecure countries are being affected the most, according to a new study published on 31 May in PLOS One (1).\nJust ten crops —barley, cassava, maize, oil palm, rapeseed, rice, sorghum, soybean, sugarcane and wheat — supply around 83 per cent of the total calories produced by croplands. Therefore, future food security hinges on the ability to maintain these staple crops. However, crop yields are expected to decrease under future climate change projections.\nAccording to the authors, previous assessments based on climate change and crop yield predictions provide important insight — higher estimates of global warming will likely result in strong yield losses in lower latitudes, particularly maize and wheat crops — but are typically only estimated for a limited number of crops in the long-term, 2050 and later.\nTo complement existing long-term forecasts, the researchers sought to identify crops and regions already at higher risk. The study carried out by researchers from the Institute of the Environment at the University of Minnesota, in collaboration with the University of Copenhagen, established so-called linear regression relationships between weather and crop yield data to assess the potential impacts of climate change on crop productivity.\nThe relationship between weather and crop yields is significant, around 54–88 per cent. As the authors write, “climate change has potentially already affected global production of the ten largest crops and the production of consumable food calories in specific countries and globally”.\nIn particular, the researchers found the impact of global climate change on yields of different crops from climate trends ranged from a decrease of 13.4 per cent for oil palm to an increase of 3.5 per cent for soybean. Overall, they estimate an average reduction of around one per cent (-3.5 x10e13 kcal/year) of consumable food calories for the top ten crops.\nThe impacts were mostly negative in Europe, Southern Africa, and Australia but positive in Latin America. Moreover, half of the food-insecure countries are currently experiencing decreases in crop yields. In contrast, global warming increased crop productivity in some regions, for example, the upper Midwest of the US.\nThe findings are important since they identify which geographical regions and crops are most at risk from climate change right now. Furthermore, it seems those in already food-insecure countries fare far worse. The authors suggest further studies are still needed to fully understand the implications of mean climate change on food security at the local scale.\nNonetheless, the results will be useful for policymakers and others working towards strategies aimed at helping to achieve the UN Sustainable Development Goals of ending hunger and mitigating the effects of climate change.\nThe global population is expected to reach at least 9 billion by 2050. Models previously developed by the Institute on the Environment’s Global Landscapes Initiative have provided crucial for the UN, World Bank, and Brookings for evaluating global food security and environmental challenges associated with feeding the growing population. And the recent findings are no exception.\n(1) Ray, D.K. et al. Climate change has likely already affected global food production. PLoS One (2019). DOI: 10.1371/journal.pone.0217148"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:ac9ddbcd-bb54-4705-afbe-0c1ae3bb82f9>","<urn:uuid:0b1f4c38-124e-421c-9b80-5a1ad462b1f3>"],"error":null}
{"question":"How were merchant guilds organized in post-Mauryan India and what functions did they serve?","answer":"Merchant guilds, known as 'Shreni', were organized under a head called 'sreshthi'. There were also mobile trading corporations called 'sartha' led by a 'sarthavaha'. The guilds elected their heads and created rules for prices and quality. They functioned as bankers, accepting public deposits at fixed interest rates. They were associations of merchants and craftsmen in the same profession or dealing in the same commodity, operating based on mutual goodwill.","context":["Post Mauryan Society and Trade – History Study Material & Notes\nInternal and External Trade Routes:\nThe most important feature of the post-Mauryan period was its growth of trade and commerce, both internally as well as externally in the country. There were two major internal land routes in ancient India. First was known as ‘Uttarapatha‘ which connected northern and eastern parts of India with the northwestern fringes of the present day Pakistan and further beyond that. The second land route was known as ‘Dakshinapatha’ had connected the peninsular India with the western and northern parts of India.\nThe Dakshinapatha was the major route which connected the north and south India. It started from Kaushambi near Allahabad and running through Ujjaiyini (modern Ujjain) extended further up to Bhrigukaccha or Broach, an important port on western coast. The Dakshinapatha was further connected with Pratishthana (modern Paithan), the capital of the Satavahanas. As regards external trade routes, after the discovery of monsoons by Hippatus, who was a Greek navigator in 45 AD, more and more sea voyages were used for trading purposes. Important ports of India on the western coast were Bharukachchha Sopara, Kalyana, and Muziris, from north to southern direction.\nShips from these ports had sailed to the Roman Empire through the Red Sea. Trade with southeast Asia was conducted through the sea. Prominent ports on the eastern coast of India were ‘Tamralipti’ in West Bengal, the Arikamedu in Tamil Nadu Coast. The Sea trade was also conducted between ‘Bharukachchha’ and the ports of Southeastern Asia.\nEstablishment of trade between West and Central Asia:\nAn important feature of the commercial activities in the post-Mauryan period was the thriving trade between India and the West in the Roman empire was at its height. This trade in the beginning was carried out through land, but owing to frequent obstructions created by the Persians, those who ruled the areas through which these trade routes passed, the focus was shifted to sea routes. Now the ships could move directly from Indian ports to the ports located on Red Sea and Persian Gulf. The best account of Indo-Roman trade is given in the book called ‘Periplus of the Erythrean Sea’ which was written in the first century AD by an anonymous author. Main requirements of these Romans were the Indian products such as spices, per- fumes, jewels, ivory and fine textiles, i.e. muslin. Spices were exported from India to the Roman empire included pepper, also called ‘yavanapriya’ because of its high popularity among the Romans.\nThe spice trade with Roman empire was based in the southern India.The Romans imported several types of precious and semiprecious stones like diamonds, carnelians, turquoise, agate, sapphires. They also imported pearls, indigo, sandal-wood and steel from India.\nAgainst this import Romans exported gold and silver to India. It is proved by a large number of Roman coins of the first century AD found in the subcontinent. This indicates an enormous drain of gold from the Roman empire towards India. Other important items of export from the Roman empire included wine which is indicated by wine-\namphorae and sherds of Roman ware found in significant numbers at Arikamedu in south India. These western traders also brought other things like tin, lead, slave girls and coral.\nIndian Crafts Production in Post-Mauryan Period:\nCrafts production had started growing in this period with tremendous impetus, as trade and commerce, both the internal and the foreign, were dependent greatly on the craft activities. The text work ‘Milindapanho’ has mentions of 75 types of occupations of which 60 were solely associated with crafts. The level of specialization in crafts was very high and there were separate distinct artisans working in materials like gold, silver, precious stones etc.\nThe city of Ujjain was a prominent bead making centre. Textile industry was another prominent industry. Mathura and Vanga (east Bengal) were very famous for variety of cotton and silk textiles. The discovery of ‘dying vats’ at various sites in southern India indicates that dying the clothes was a thriving craft in this area during the Post-mauryan period.\nThe artisans in this period reached new heights of prosperity and there are such numerous inscriptions which refer to the donations made by artisans to monasteries.\nTrade and Merchant Guilds:\nThe communities of merchants that were organised in groups were known as ‘Shreni’ or guilds, under the head known as ‘sreshthi’.\nThere was another type of mercantile group that was called ‘sartha’ , which signified mobile/caravan trading corporations of inter-regional traders. These leader of such a guild was called ‘sarthavaha’.\nLike the merchants roughly all craft vocations were also organised into the form of guilds, with each one under a headman called ‘Jyestha’. These included weavers,bamboo workers, corn dealers, oil manufacturers, potters and others etc. The guilds were basically associations of merchants and craftsmen following the same profession or dealing in the same commodity. They elected their head and framed their own rules regarding prices and quality etc., to regulate their business on the basis of mutual goodwill. They also served as bankers and received deposits from the public on a prescribed fixed rate of interest."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:81a1a960-cca0-4205-b1c0-79be1d90adbb>"],"error":null}
{"question":"As someone working on satellite launch preparations, I'd like to know how do the pre-launch activities differ between Sentinel-5P and EcoProMIS in terms of team coordination?","answer":"For Sentinel-5P, the pre-launch activities involved intensive team training with 20 out of 26 planned simulations, coordinating multiple teams including operations engineers, flight dynamics specialists, ground station teams, science teams, and industry representatives. These culminated in a final dress rehearsal involving teams in Darmstadt, ground stations in Sweden, Antarctic, Canada, Norway, and Russian teams at Plesetsk. For EcoProMIS, the pre-launch coordination focused on establishing partnerships between different organizations, including Rothamsted Research, Agricompas Limited, CIAT, Fedearroz, Cenipalma, Pixalytics, Elastacloud, and Solidaridad, with emphasis on setting up data sharing protocols and collaborative frameworks for agricultural monitoring.","context":["Artist impression of Sentinel 5p satellite. Photo by: Airbus\nIn a not-so-distant future, rice and oil palm producers in Colombia will have access to realistic estimates of what their yields will be and the amount of greenhouse gases their fields emit in near real-time.\nThe estimates will come from an online platform that will run data based on algorithms generated in a three-year project involving CIAT and several Colombian and U.K.-based organizations. The platform will go by the name of Ecological Productivity Management Information System in Colombia, or “EcoProMIS”.\nFunded by the U.K. Space Agency and led by Rothamsted in a public-private partnership with Agricompas Limited, the project aims to allow rice and oil palm growers in Colombia to have thriving livelihoods without harming the environment.\nThe project is a first for CIAT, as it will use space technology, i.e., satellites, to monitor how crops perform under different conditions.\nParticipants to the kick-off meeting on the EcoProMIS project, which was held in Bogota early this year. Photo by: Michael Selvaraj / CIAT\n“With satellites, I can see what’s happening on a plot in the municipality of Santa Rosa Villavicencio without leaving my desk,” said Dr. Michael Gomez Selvaraj, a crop physiologist and country lead for CIAT’s work under the project.\nSelvaraj and his team at the CIAT Phenomics Platform will also use drones and sensors under the project. These will measure soil moisture, wind speed, rain, relative humidity, and the soil’s pH level, among other conditions.\nRice performance monitoring using a drone. Photo by: Neil Palmer / CIAT\nThe project likewise involves gathering on-the-ground data. Fedearroz, Colombia’s national rice growers’ association, and Cenipalma, a research center that’s a subsidiary of the national federation for oil palm growers, Fedepalma, will engage farmers to do this.\nThe farmers, along with technical advisers, will conduct surveys on crop production and biodiversity and share the results via smartphones. They will likewise collect plant samples.\nThe purpose of the exercise is to enable farmers to develop skills on how to best manage crops and enhance their understanding of the impact of farming practices on productivity and the environment.\nTo measure greenhouse gas emissions in real time, CIAT will install flux towers on the trial sites.\nAll the data generated through various means will feed into a model that can forecast, for instance, the yield based on the quantity of fertilizer and water used.\nReducing greenhouse gas emissions is a priority for Colombia. The country has pledged to cut these emissions — the bulk of which come from agriculture, forestry, and other land uses — by 20 percent by 2030.\nAlthough EcoProMIS will focus on rice and oil palm, the Web platform could also be used for other crops, such as sugar cane, cacao, and cassava.\nGraham Turnock, Chief Executive of UK Space Agency said: “This exciting project in Colombia will help farmers to improve their land management to diversify the country’s economy and stabilize food supplies. Innovations from space like Rothamsted Research’s EcoProMis have the ability to expand skills and technology at home whilst bringing benefits abroad, I’m delighted to see this project progressing.”\nAgricultural data analytics firm Agricompas will operate EcoProMIS and develop knowledge services for partners and provide commercial services to other stakeholders to secure sustainable income for platform maintenance.\nPartners are growers, research institutes and trade associations that share crop production data and expertise in return for free platform services. This will enable them to produce better crops, research and extension services.\nFor each crop, Agricompas will launch a series of commercial decision support services to target challenges such a climate change, insurance, pollution and food security. First services will be; insurance for rice and oil palm; finance for rice production; and sustainability for palm production.\n“Our objective is to support sustainable agriculture by improving crop productivity and profitability while reducing environmental impact and improving socio-economic conditions of stakeholders,” said Roelof Kramer, Agricompas CEO.\nInternational non-profit organization, Solidaridad, will also create a protocol for collecting data to determine the socio-economic impact of the project on smallholder farmers.\nThe “Ecological Productivity Management Information System in Colombia (Colombian EcoProMIS)” project is funded by the U.K. Space Agency International Partnership Programme. The program aims to harness U.K. space expertise in addressing challenges worldwide, especially in low-income countries.\nThe EcoProMIS project will use Earth Observation satellites, together with environmental and agricultural data, to determine the effects of farming and ecological management practices on biodiversity, greenhouse gas emissions, and yields.\nRothamsted Research and Agricompas lead the project. Other U.K.-based project partners include Pixalytics, and Elastacloud. Colombian project partners include Cenipalma, Fedearroz, and Solidaridad.","Preparing to fly Sentinel-5P\nThe teams that will fly Sentinel-5P are training intensively for launch, ensuring that everyone knows their job and can react to any emergency.\nA ‘team of teams’ at ESA’s mission control centre has spent months preparing to assume control of Europe’s next Earth observation mission, and the final weeks before launch have been the most intense.\nSentinel-5P – the P refers to ‘precursor’ – is the first mission for Copernicus dedicated to monitoring our atmosphere.\nThe satellite carries the state-of-the-art Tropomi instrument that will map a multitude of trace gases such as nitrogen dioxide, ozone, formaldehyde, sulphur dioxide, methane, carbon monoxide and aerosols – all of which affect the air we breathe, and therefore our health, and our climate.\nExpansion of the Sentinel fleet in orbit highlights the expertise of teams at ESA and their capability to fly ‘constellation’ missions, as Sentinel-5P will fly in tight coordination with the US Suomi-NPP mission.\nThe challenging task of flying Sentinel-5P throughout its planned seven-year mission starts just 93 minutes after liftoff on 13 October, set for 09:27 GMT (11:27 CEST), on a Rockot from Russia’s Plesetsk Cosmodrome.\nThat’s when the satellite, already in space after separating from the rocket some 14.5 minutes earlier, will make its first call home, signalling via a ground station in Sweden to ESA’s main control room in Darmstadt, Germany.\n“It’s called ‘acquisition of signal’, and it’s the moment when the years of careful development and preparation for our mission control systems, and the months of training for our mission control teams, will prove their worth,” says flight operations director Pier Paolo Emanuelli.\nThat moment is one of the riskiest for the satellite: its rocket must have provided the right boost to put it into the planned orbit, and until its solar panels deploy to start generating power, it must survive on batteries, which will last only for a limited time.\n“Once we get the signal, and establish a commanding link with the satellite, we’ll begin a critical series of activities and procedures to verify the satellite’s health, ensure we have solar power and full communications, activate systems like the startracker cameras for navigation and ensure that 5P is fully functional after the incredibly vigorous ride into space.”\nThese initial activities continue around the clock for the first three days, after which the team will switch to daytime work, if all goes well, and move on to the next phase of the mission: commissioning the Tropomi sensor.\nTraining for all possibilities\nIf anything does go wrong, ESA will be well prepared. Since mid-July, the mission control teams – including the operations engineers, flight dynamics specialists, teams from ground stations, the science and project teams and representatives from European industry – have conducted 20 simulations out of the planned 26.\nEach runs for a full day, and employs sophisticated software to replicate the satellite and ground systems. Trainers can inject faults, errors and breakdowns into the simulation, testing the skill and knowledge of even the most experienced engineers and the teamwork and problem-solving abilities of everyone.\n“The human factor is the one that determines the success of the mission. There is no single responsibility, and it is great to see our teams working together,” says spacecraft operations manager Daniel Mesples.\nGetting ready to go to space\nBetween now and launch day, the final round of ‘sims’ will take place twice per week, culminating with a final dress rehearsal on 11 October, which, by tradition, simulates a completely normal launch sequence.\nThis will involve the teams in Darmstadt, ground stations in Sweden, the Antarctic, Canada and Norway, and the ESA and Russian teams at Plesetsk.\nDuring the rehearsal, the mission control systems will be connected to Sentinel-5P sitting on top of the rocket via a ground link, which will be removed only a few minutes before liftoff.\n“We will already have practised an extensive range of contingency situations, and experience shows it’s good for team morale to run though a fully normal launch sequence one final time just prior to liftoff,” says Daniel.\nOn 12 October, the flight operations director will certify to the launch authorities that the mission control teams are fully trained, that the ground systems and facilities are tested and ready, and that launch can proceed.\n“In September, the ESA centre celebrated its 50th anniversary in Darmstadt, and its rich history of 77 missions,” says Daniel.\n“It’s terrific that this month has seen many of us practising and training to do what it has excelled in for five decades – preparing to go to space.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:fdf7627d-43ff-4ab8-adb4-d6fe18dc4cb6>","<urn:uuid:42fe6e7a-4add-4cd2-a4b2-e27702888af9>"],"error":null}
{"question":"How do color-related challenges differ between historical building restrictions and indigo fabric dyeing?","answer":"In historical building venues, color restrictions exist because only mineral-based colors were available historically, and management controls color choices to prevent visual chaos and maintain period authenticity. Buildings must use earthy tones, with bright colors limited to small accent areas. In contrast, with indigo fabric dyeing, the challenge lies in colorfastness - the dye fails to form chemical bonds with the fabric, leading to bleeding and fading over time. The indigo color accumulates in tiny spaces within the fibers and gradually fades with washing and wear, resulting in white lines appearing.","context":["Every industry has rules. The trick is in knowing how to get what you want while meeting the rules. Venues with a historical theme have tons of several restrictions that help create the environment that fools the public into the fantasy that they are visiting a 16th century village. I won’t get into the finer points on costume rules and building design here, but I will talk about one control that shows up in both those departments. Color restrictions.\nThere are real reasons for having color restrictions in a historical venue. The technology wasn’t yet invented that could create a hot pink piece of fabric, or a vibrantly purple stucco wall. At least, there was no “affordable” technology that did so. Renaissance Festival costume rules almost universally prevent anyone other than royal characters wearing purple. Apparently in the 16th century, the only source for a purple dye was found in a type of muscle shell found in the Mediterranean Sea, and you needed a lot of them. All of the colors used during the Renaissance Period were from natural sources. Vegetable dyes faded fairly quickly, so the palette of the working class was soft and earthy. I’m not a professional costumer, although I have good friends that are. The end of the color issue that I know better is the one where color restrictions are applied to buildings.\nMuch like a mall lease contract, contracts for the vendors at a Renaissance Festival or Faire require they do their own “build-out”. They build, or pay to have built, the shops that they will be vending from at that particular festival. Designs have to be approved by the Vendor Coordinator, and perhaps the Site Director and General Manager as well. Each show is an independently owned company with only a few exceptions in the industry, so they’ll each have their own set of hoops for one to jump through. In the best cases, the proposed building design also has to meet with the approval of a Building Inspector.\nBecause buildings are each designed by an individual, with rare consideration of their neighbors’ designs, the color rules protect us all from garishness. If the color choices were not controlled, each vendor’s desire to be more readily seen than their competitors would quickly spiral upward into a visual cacophony. It happens sometimes even with the color safeguards in place. I myself am guilty of renaming a color when making my building design proposal to management, because it sounded far too purple otherwise. (Smart building coordinators require paint chips with the proposal.) Fortunately, Park Management has final design control, and can easily tell a vendor to repaint if the color choice was a bad one. In most cases management is considerate to time restraints and may even let someone get a season’s use with the bad color before having to change it for the following season.\nI designed and built shops for vendor clients for many years. (Actually, I designed, my husband built, and I painted and trimmed the buildings.) In conversations with my clients, I’d coach them on how to be creative within the color restrictions. I was not telling them how to bend the rule; I like the rules. But, I found that the best explanation was to tell them to “Put some dirt into the color that they most like, and it will probably qualify as an acceptable color.” It was a simplification, but one that communicated well. This is because, while vegetable dyes were used in fabrics, almost all colors used in the building trades came from mineral sources. So I’d ask a client for 3 colors, help them make choices that worked well together, and then if one of those colors was something that would have been more difficult, it became the smallest of the accent colors. In other words, no dark green walls, but perhaps some dark green trim.\nIt isn’t that any of this is rocket science. But the builder has to care about the illusion that she is helping to create. Renaissance Faire builders are amongst the luckiest designer/builders in the country. They get to design whimsical structures that actually come to fruition. More often than not, they are building inside a private park, and they rarely, if ever, have to build the same thing twice. Caring about the “whys” of the color rules just makes them better at their jobs.","Indigos and Why They Bleed:\nConsidered the king of blues, Indigo has a very rich and intense feel to it. It is extensively used at the industrial level to dye fabrics. Both traditional and contemporary outfits can be seen with indigo colors or indigo block prints. The specialty of this color is that it is absolutely capable of lending a modish versatility to the cloth while at the same time keeping it sublime.\nBack in the 1800s, indigo was obtained naturally from a plant called Indigofera. The leaves of it were fermented in water to obtain a natural indigo dye but now such natural process and color are rare. A maximum of manufacturers have switched to synthetic indigo dyes. The process of dying the indigo fabric is as follows:\nThe Art of Indigo Dyeing: From Soaking to Colour Intensification\n- The fabric which is supposed to be dyed is washed properly.\nIt is left soaked in water to improve its color penetration capacity.\n- The indigo stock solution is prepared, either naturally\nthrough fermentation or by mixing the synthetic powder into water. Note: Both\nthe natural and synthetic ways of preparing the indigo dye are detailed and\nrequire precision plus practice.\n- Finally, when the solution is prepared, the cloth is dipped\ninto it. Remember, indigo color always works by oxidation. The color that is\nseen while taking out the dipped fabric is totally different from the final\nThe fabric is hung for drying and gradually during this time, the indigo color intensifies.\nThe Beauty and Challenge of Colorfastness in Indigo Fabrics\nIndigo block prints or indigo-colored fabrics have always been a favorite of people since the time they were discovered. But along with the demand, there is a concern commonly heard by people; “ indigo fabrics bleed or fade a lot.” Do you know why it happens? Well, primarily the reason is this while dying the fabric into an indigo color; the colors fail to form any chemical bond with the fabric.\nAs a result, it becomes insoluble again while putting it into the water for washing. When you put it for drying, there is a reaction with air due to which the color tends to accumulate at tiny places within the fibers. With the passage of time, the rubbing of the cloth causes the indigo color to fade off and the appearance of white lines.\nIndigo fabrics have attracted people with their rich and timeless attractiveness, but there is a recurring concern: indigo fabrics bleed or fade dramatically. This occurs because indigo fails to create a chemical link with the fabric during the dyeing process, rendering it insoluble when exposed to water during washing.\nThe indigo color accumulates in microscopic pockets inside the fibers as the fabric dries due to a reaction with the air. These pockets release the collected indigo over time and with friction from wear and washing, resulting in the formation of white lines and a faded appearance.\nThe beauty of indigo fabrics resides in their rich, enticing shade, but keeping their colorfastness necessitates proper maintenance. Gentle washing and avoiding excessive rubbing can help retain the brilliant blue color, allowing you to enjoy these timeless textiles for many years."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:19e789fa-e302-44dc-8939-9978b19d0128>","<urn:uuid:895ce5ed-36ca-4b6b-9fdc-710e3f5edd74>"],"error":null}
{"question":"How do pancreatic enzymes help with digestion in people with CF compared to those without the condition?","answer":"In people without CF, the pancreas naturally produces enzymes (lipase, protease, and amylase) that travel through ducts to break down fats, proteins, and starches in the small intestine. However, in CF patients, thick mucus plugs the pancreatic duct, preventing these natural enzymes from reaching the small intestine. This leads to malabsorption of nutrients. To compensate, CF patients must take replacement pancreatic enzymes with every meal and snack that contains protein, starch, or fat. These enzymes can be taken from a half-hour before to a half-hour after eating to ensure proper digestion in the small intestine.","context":["Cystic Fibrosis is complicated to manage. Click on a topic to read articles on airway clearance techniques, nutrition, updates on devices, or advice about CF complications.\nNutrition and CF\nGood nutrition for a child or adult with CF means more than just eating a balanced, normal diet. A high-calorie diet containing all the essential nutrients without restriction on fat intake is recommended, along with pancreatic enzymes to control digestive symptoms, and supplemental fat-soluble vitamins: A, D, E, and K. To maintain weight, frequent and high-calorie meals and snacks are recommended.\nWhat is a \" good diet \" for someone with CF?\nNutritional requirements for each person with CF are unique; there is no universal recommendation that applies to everyone. Requirements and recommendations depend on many factors including age, gender, severity of lung disease, malabsorption and the type of food a person likes. But there are some general recommendations for anyone with CF:\nMost people with CF have a higher caloric requirement than other individuals of the same age and sex because:\n- More energy is used in breathing.\n- Extra energy is used in fighting infections and during fevers.\n- Fewer of the energy-producing nutrients in food are properly digested and absorbed into the body, even when treatment is optimal.\nResearch has shown that good nutrition is important for pulmonary function, and can contribute to a longer life expectancy.\nVitamins A,D, E, and K are known as the fat-soluble vitamins. These vitamins require an adequate amount of fat to be absorbed properly. Because people with CF can have problems with absorption of fat, it is important to replace these vitamins.\nCF causes production of a thick mucus that plugs the duct leading from the pancreas to the small intestine. Consequently, the enzymes produced by the pancreas to help digest food cannot move into the small intestine. If left untreated, one result is malabsorption, an inability to properly absorb nutrients. To compensate for this enzyme shortage, many people with CF must take replacement pancreatic enzymes.\nWhen Do I Take Enzymes?\nEnzymes must be taken to help digest every meal and snack, except for snacks that are virtually free of protein, starch, and fat (such as apple juice). Studies show that enzyme preparations are equally effective when taken anywhere from a half-hour before to a half-hour after eating. That way, the enzymes are in the small intestine when needed.\nWhat If I Can't Eat Enough Calories?\nSome people with CF cannot take in enough calories by mouth to gain or maintain weight. For them, calories and all the essential nutrients can be given by tube feedings. Many achieve excellent nutrition through this method.\nIs There Any Help With Nutritional Products?\nThe Scandipharm Comprehensive Care Program for CFTM provides free ADEKs® multivitamins, Scandishake® nutritional supplements, and Flutter® mucus-clearance devices to people taking Ultrase®. The program enhances Scandipharm's mission of improving the overall quality of care, while lowering its cost. For more information on this program, talk to one of the CF team members.\nHow Can I Manage CF and Nutrition?\nThe key to managing nutritional needs successfully is to match dietary needs to each individual:\n- Assess individual needs\n- Adjust for clinical status\n- Factor in age-related needs\nA comprehensive nutritional management plan developed jointly by the CF Team, the person with CF, and the family is needed.\nPulmonary Function Tests","The Digestive Process: What Is the Role of Your Pancreas in Digestion?\nYour pancreas plays a big role in digestion. It is located inside your abdomen, just behind your stomach. It's about the size of your hand. During digestion, your pancreas makes pancreatic juices called enzymes. These enzymes break down sugars, fats, and starches. Your pancreas also helps your digestive system by making hormones. These are chemical messengers that travel through your blood. Pancreatic hormones help regulate your blood sugar levels and appetite, stimulate stomach acids, and tell your stomach when to empty.\nYour pancreas creates natural juices called pancreatic enzymes to break down foods. These juices travel through your pancreas via ducts. They empty into the upper part of your small intestine called the duodenum. Each day, your pancreas makes about 8 ounces of digestive juice filled with enzymes. These are the different enzymes:\nLipase. This enzyme works together with bile, which your liver produces, to break down fat in your diet. If you don't have enough lipase, your body will have trouble absorbing fat and the important fat-soluble vitamins (A, D, E, K). Symptoms of poor fat absorption include diarrhea and fatty bowel movements.\nProtease. This enzyme breaks down proteins in your diet. It also helps protect you from germs that may live in your intestines, like certain bacteria and yeast. Undigested proteins can cause allergic reactions in some people.\nAmylase. This enzyme helps break down starches into sugar, which your body can use for energy. If you don’t have enough amylase, you may get diarrhea from undigested carbohydrates.\nMany groups of cells produce hormones inside your pancreas. Unlike enzymes that are released into your digestive system, hormones are released into your blood and carry messages to other parts of your digestive system. Pancreatic hormones include:\nInsulin. This hormone is made in cells of the pancreas known as beta cells. Beta cells make up about 75% of pancreatic hormone cells. Insulin is the hormone that helps your body use sugar for energy. Without enough insulin, your sugar levels rise in your blood and you develop diabetes.\nGlucagon. Alpha cells make up about 20% of the cells in your pancreas that produce hormones. They produce glucagon. If your blood sugar gets too low, glucagon helps raise it by sending a message to your liver to release stored sugar.\nGastrin and amylin. Gastrin is primarily made in the G cells in your stomach, but some is made in the pancrease, too. It stimulates your stomach to make gastric acid. Amylin is made in beta cells and helps control appetite and stomach emptying.\nCommon pancreatic problems and digestion\nDiabetes, pancreatitis, and pancreatic cancer are three common problems that affect the pancreas. Here is how they can affect digestion:\nDiabetes. If your pancreatic beta cells do not produce enough insulin or your body can’t use the insulin your pancreas produces, you can develop diabetes. Diabetes can cause gastroparesis, a reduction in the motor function of the digestive system. Diabetes also affects what happens after digestion. If you don't have enough insulin and you eat a meal high in carbohydrates, your sugar can go up and cause symptoms like hunger and weight loss. Over the long term, it can lead to heart and kidney disease among other problems.\nPancreatitis. Pancreatitis happens when the pancreas becomes inflamed. It is often very painful. In pancreatitis, the digestive enzymes your pancreas make attack your pancreas and cause severe abdominal pain. The main cause of acute pancreatitis is gall stones blocking the common bile duct. Too much alcohol can cause pancreatitis that does not clear up. This is known as chronic pancreatitis. Pancreatitis affects digestion because enzymes are not available. This leads to diarrhea, weight loss, and malnutrition. About 90% of the pancreas must stop working to cause these symptoms.\nPancreatic cancer. About 95% of pancreatic cancers begin in the cells that make enzymes for digestion. Not having enough pancreatic enzymes for normal digestion is very common in pancreatic cancer. Symptoms can include weight loss, loss of appetite, indigestion, and fatty stools.\nYour pancreas is important for digesting food and managing your use of sugar for energy after digestion. If you have any symptoms of pancreatic digestion problems, like loss of appetite, abdominal pain, fatty stools, or weight loss, call your healthcare provider.\nMarch 22, 2017\nUp To Date. Gastroparesis; Etiology, clinical manifestations, and diagnosis, UpToDate. Chronic Pancreatitis (Beyond the Basics), UpToDate. Physiology of Gastrin\nLehrer, Jenifer, MD,Taylor, Wanda, RN, Ph.D."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:47fecf50-f7c0-416a-b1c6-fb0edd7cfc6f>","<urn:uuid:9848238d-ca6c-4a85-8a99-55ac51736f1c>"],"error":null}
{"question":"I'm leading a development team that's considering adopting AI coding assistants. What are the productivity benefits of these tools, and what are the key cybersecurity risks we need to consider?","answer":"AI coding assistants like GitHub Copilot and TabNine boost productivity by automating repetitive coding tasks, providing code completions, and helping with documentation. They can generate entire blocks of code, answer coding questions, and speed up development through features like AI-assisted autocomplete and natural language editing. However, there are important cybersecurity risks to consider. Early AI systems may create a false sense of security since their supervised learning approach can be compromised if hackers discover how the process works. Additionally, AI-based security tools often create more work as they require human analysts to review flagged issues. To manage these risks effectively, organizations should independently test AI security, ensure proper understanding of vendor models, and develop internal AI cybersecurity expertise through training programs.","context":["Artificial intelligence is transforming how software developers work. AI coding tools like GitHub Copilot are revolutionizing code generation, allowing developers to be more productive and creative. In this comprehensive guide, we'll explore how to leverage AI to write better code effectively.\nOverview of AI Coding Assistants\nAI coding assistants use large language models pre-trained on massive amounts of code to suggest completions as you type. They continuously analyze your code and propose relevant functions, variables, comments and entire blocks of code tailored to your project.\nLeading tools include:\n- GitHub Copilot: Created by GitHub and OpenAI, it integrates directly into popular code editors like Visual Studio Code and Neovim. It examines your code context and suggests completions using a technique called \"few-shot learning\".\n- TabNine: An AI assistant that plugs into editors like Vim, Emacs and Sublime Text. It focuses on speedy code completions.\n- Phind: A search-enabled assistant that helps with debugging, finding documentation, and writing code. It allows you to search your codebase and ask questions.\n- Continue: An IDE extension that brings ChatGPT capabilities into VS Code and JetBrains. It lets you edit, generate, and understand code using natural language instructions.\n- Cody: An AI coding assistant that leverages your codebase for accurate completions and chat answers. It works across languages and IDEs.\nThe core advantage of these tools is their ability to boost your productivity by reducing repetitive coding. But to maximize their potential, you need to learn how to effectively collaborate with them.\n|Description||Open-source autopilot for software development||AI coding assistant that knows your entire codebase||AI pair programmer, provides contextualized assistance||AI assistant that speeds up delivery and code safety|\n|Integration||VS Code and JetBrains||Directly in your IDE||Visual Studio Code, Visual Studio, JetBrains IDEs, Neovim||Most of the popular modern IDEs|\n- Answer coding questions\n- Edit in natural language\n- Generate files from scratch\n- Understand errors and exceptions\n- AI-assisted autocomplete\n- AI-powered chat\n- Custom and pre-built commands\n- Sourcegraph powered context\n- Contextualized assistance\n- Code explanations\n- Integration with leading editors\n- Native build into GitHub\n- Code completion\n- Adapts to your codebase\n- Chat support\n- Generates code based on comments\n|Languages and Platforms||Not specified||Not specified||All languages in public repositories||Trained exclusively on permissive open-source repositories|\n|Security and Privacy||Not specified||Not specified||AI-based vulnerability prevention system||\n- SOC-2 compliant\n- Privacy, security, and compliance focused\n|Additional Features||- Task, not tab, auto-complete||\n- Code faster with autocomplete\n- Run custom commands\n- Trained by GitHub, OpenAI, and Microsoft\n- Insecure code pattern prevention\n- Approximately 30% code generation\n- Faster onboarding and training\n- Frees up resources for innovation\n|Official Source||Continue||Cody||GitHub Copilot||TabNine|\nBest Practices For Using AI Coding Tools\nHere are some tips and techniques for getting the most out of AI assistants:\nProvide Some Initial Context\nBefore expecting useful suggestions, give the AI some clues about what you're working on. Start by writing import statements, defining key variables and functions, or adding comments about your goal. This context helps the AI tune its recommendations to your specific project.\nLet It Learn From Your Edits\nAI assistants continuously learn from your real-time edits, selections and deletions. So don't just accept its suggestions blindly. Modify them to match your desired style and conventions. Over time, it will adapt to generate code that better aligns with your preferences.\nFocus On Intent Over Details\nDon't obsess over minor stylistic tweaks to the AI's suggestions. Focus on evaluating and approving code that captures your overall intent, even if the specifics need reworking. The key is efficiently moving your logic forward vs getting stuck on smaller refinements.\nLeverage Code Reviews\nCode written with the help of AI still benefits from human review. Have a peer manually inspect key portions for bugs, anti-patterns or other issues the AI may have missed. Two sets of eyes are always better than one.\nComplement Your Skills\nAn AI assistant excels at churning out boilerplate code and repetitive tasks. But lean on your human judgment for higher level architecture, complex logic and creative algorithms. Maximizing these complementary strengths results in the best code.\nGive It Time To Learn\nLike any team member, an AI coding assistant needs onboarding and ramp up time. Be patient giving it a chance to train on your codebase and gain exposure to your specialized domains. Over time, it will become increasingly useful.\nWriting Production Code With AI\nNow let's walk through practical techniques for leveraging AI to generate robust, maintainable production code:\n1. Decompose the Problem\nBreak down your coding goal into a series of smaller sub-tasks. Then mentally frame how you would instruct another developer to tackle each step. This helps you provide the AI with a logical sequence of focused prompts.\n2. Provide Representative Examples\nProvide 2-3 examples that demonstrate the kind of code you want generated. For instance, show snippets that format data, sanitize inputs or implement specific APIs. This \"few-shot learning\" improves relevance.\n3. Describe Desired Functionality\nSuccinctly explain in comments the purpose of the code you want written. Describing desired functionality in human terms guides the AI towards your intent vs getting fixated on the exact code.\n4. Handle Corner Cases\nDon't just demonstrate happy path scenarios. Include examples covering edge cases, errors and boundary conditions you want the logic to handle. This proactively teaches the AI to generate more robust code.\n5. Verify Correctness\nThoroughly test the AI's code and inspect it for bugs, performance issues, semantic errors and security vulnerabilities. Rigorously validating code produced by the AI helps further train it.\n6. Refactor Mercilessly\nAggressively refactor any sloppy, unidiomatic or difficult to maintain code churned out by the AI. This retraining will continually improve code quality over time.\n7. Document Thoroughly\nHave the AI generate clear documentation and comments explaining each part of the code. Well documented code allows future editors to readily understand the AI's logic and intentions.\nHere are some more nuanced strategies for leveraging AI even more effectively:\n- Use the AI for prototyping and experimentation, given its ability to quickly generate versions. Manually rewrite later once you lock down the final logic.\n- For complex coding tasks, break work into stages like \" Generate type signatures → Implement core logic → Handle edge cases → Write tests.\" Prompt separately for each stage.\n- Assign the AI repetitive infrastructure code like configs, boilerplate and scaffolding. Focus your energies on the trickier domain-specific logic.\n- Combine the AI assistant with test-driven development for a highly iterative workflow. Alternate between \"Write failing test → Generate code to pass test → Refactor code → Repeat.\"\n- When a review reveals bugs in the AI's code, use it as a learning opportunity. Fix the bugs, then show the AI examples comparing its code with the corrected code.\n- Apply the AI assistant to refactor and improve your existing codebase. It can suggest optimizations and ways to deduplicate redundant code.\nAI coding assistants promise to make software engineers more productive by automating repetitive coding tasks. However, thoughtfully integrating them into your workflow is critical for maximizing their value.\nFollow these best practices:\n- Give the AI representative examples\n- Clearly explain desired functionality\n- Thoroughly test its code\n- Aggressively refactor\n- Use your human judgment for high-level design.\nWith a disciplined approach, you can leverage AI to boost your productivity and take your coding skills to the next level.\nAI coding assistants like GitHub Copilot provide significant productivity benefits during the development process. However, thoughtfully integrating them into your workflow remains critical for maximizing their value.\nPlatforms like Daytona that simplify environment configuration can further enhance developer productivity when combined with AI coding. Daytona's ability to instantly spin up ready-to-code cloud environments helps developers spend more time creating and less time configuring. Using Daytona in conjunction with AI coding tools creates an ideal playground for AI-enhanced development.","Your company has started to use Artificial intelligence (AI), but are you effectively managing the risks involved? It’s a new growth channel with the potential to boost productivity and improve customer service. However, particular management risks need to be assessed in cybersecurity. Start by considering AI trends to put this risk in context.\nWhy Is AI an Emerging Cybersecurity Threat?\nArtificial intelligence is a booming industry right now with large corporations, researchers, and startups all scrambling to make the most of the trend. From a cybersecurity perspective, there are a few reasons to be concerned about AI. Your threat assessment models need to be updated based on the following developments.\nEarly Cybersecurity AI May Create a False Sense of Security\nMost machine learning methods currently in production require users to provide a training data set. With this data in place, the application can make better predictions. However, end-user judgment is a major factor in determining which data to include. This “supervised learning” approach is subject to compromise if hackers discover how the supervised process works. In effect, hackers could evade detection by machine learning by mimicking safe code.\nAI-based Cybersecurity Creates More Work for Humans\nFew companies are willing to trust their security to machines. As a result, machine learning in cybersecurity has the effect of creating more work. WIRED magazine summarized this capability as follows: “Machine learning’s most common role, then, is additive. It acts as a sentry, rather than a cure-all.” As AI and machine learning tools flag more and more problems for review, human analysts will need to review this data and make decisions about what to do next.\nHackers Are Starting to Use AI for Attacks\nLike any technology, AI can be used for defense or attack. Researchers at the Stevens Institute of Technology have demonstrated that fact. They used AI to guess 25% of LinkedIn passwords successfully after analyzing 43 million user profiles in 2017. In the hands of defenders, such a tool could help to educate end users on whether they’re using weak passwords. In the hands of attackers, this tool could be used to compromise security.\nThe Mistakes You Need to Know About\nAvoid the following mistakes, and you’re more likely to have success with AI in your organization.\n1. You Haven’t Thought Through the Explainability Challenge\nWhen you use AI, can you explain how it operates and makes recommendations? If not, you may be accepting (or rejecting!) recommendations without being able to assess them. This challenge can be mitigated by reverse engineering the recommendations made by AI.\n2. You Use Vendor-provided AI Without Understanding Their Models\nSome companies decide to buy or license AI from others rather than building the technology in house. As with any strategic decision, there’s a downside to this approach. You can’t trust the vendor’s suggestions that AI will be beneficial blindly. You need to ask tough questions about how the systems protect your data and what systems AI tools can access. Overcome this challenge by asking your vendors to explain their assumptions about data and machine learning.\n3. You Don’t Test AI Security Independently\nWhen you use an AI or machine learning tool, you need to entrust a significant amount of data to it. To trust the system, it must be tested from a cybersecurity perspective. For example, consider whether the system can be compromised by SQL injection or other hacking techniques. If a hacker can compromise the algorithm or data in an AI system, the quality of your company’s decision making will suffer.\n4. Your Organization Lacks AI Cybersecurity Skills\nTo carry out AI cybersecurity tests and evaluations, you need skilled staff. Unfortunately, there are relatively few cyber professionals who are competent in security and AI. Fortunately, this mistake can be overcome with a talent development program. Offer your cybersecurity professionals the opportunity to earn certificates, attend conferences, and use other resources to increase their AI knowledge.\n5. You Avoid Using AI Completely for Security Reasons\nBased on the previous mistakes, you might assume that avoiding AI and machine learning completely is a smart move. That might’ve been an option a decade ago, but AI and machine learning are now part of every tool you use at work. Attempting to minimize AI risk by ignoring this technology trend will only expose your organization to greater risk. It’s better to seek proactive solutions that leverage AI. For instance, you can use security chatbots such as Apollo to make security more convenient for your staff.\n6. You Expect too Much Transformation from AI\nGoing into an AI implementation with unreasonable expectations will cause security and productivity problems. Resist the urge to apply AI to every business problem in the organization. Such a broad implementation would be very difficult to monitor from a security point of view. Instead, take the low-risk approach: apply AI for one area at a time, such as automating routine security administration tasks, and then build from there.\n7. Holding Back Real Data from Your AI Solution\nMost developers and technologists like to reduce risk by setting up test environments. It’s a sound discipline and well worth using. However, when it comes to AI, this approach has its limits. To find out whether your AI system is truly secure, you need to feed it real data: customer information, financial data, or something else. If all this information is held back, you’ll never be able to assess the security risks or productivity benefits of embracing AI.\nAdopt AI with an Eyes Wide Open Perspective\nThere are certainly dangers and risks associated with using AI in your company. However, these risks can be monitored and managed through training, proactive management oversight, and avoiding these seven mistakes."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:d1a3b876-da17-427d-be85-17eef0e31072>","<urn:uuid:75c20593-63bc-45e3-ac40-3a0820eb44e8>"],"error":null}
{"question":"What role do chemical compositions play in both coral reef survival and the formation of Planet 55 Cancri e?","answer":"Chemical compositions play crucial roles in both systems. In coral reefs, the chemical relationship between corals and algae is essential for survival - corals provide shelter and nutrients to algae in exchange for photosynthetic benefits. During bleaching events, corals produce specific pigments as a protective mechanism to attract algae back. In the case of Planet 55 Cancri e, chemical composition, particularly the carbon-to-oxygen ratio of its host star, was initially thought to indicate a diamond-rich planet. However, new measurements of the star's chemical abundance showed different oxygen levels than previously thought, suggesting the planet's composition might be different from initial estimates. The exact composition of the planet, like coral reef systems, depends on multiple chemical and environmental factors.","context":["Planet 55 Cancri e was believed to be the first known planet to consist largely of diamond, due in part to the high carbon-to-oxygen ratio of its host star. New research challenges that precious picture.\nCredit: Haven Giguere\nAn alien planet thought to be made largely of diamond may be less than glittering inside, new research shows.\nThe notion of a diamond planet hinges on an abundance of carbon. A few years ago, scientists reported that the star at the center of a solar system 40 light-years from our own had a carbon-to-oxygen ratio greater than one.\n\"This observation helped motivate a paper last year about the innermost planet of the system, the 'super-Earth' 55 Cancri e,\" University of Arizona astronomy graduate student Johanna Teske, explained in a statement. [See Images of the Strangest Alien Planets]\nPlanet 55 Cancri e is what's known as a super-Earth because it is likely a rocky world orbiting a sun-like star, but it has a radius twice as large as that of our own planet, and a mass eight times greater. The hot planet also races around its star at such a close distance that one year lasts just 18 hours.\nDrawing on indirect observations of the planet, researchers of the 2012 study suggested its interior contained more carbon than oxygen. But in a new analysis, Teske and colleagues found that the host star doesn't appear as carbon-rich as previously thought, which might dash hopes for a diamond-packed 55 Cancri e.\nInstead of just using one measure for the distant star's chemical signatures, Teske's group averaged different indicators of chemical abundance that were not considered previously.\n\"We find that because this particular host star is cooler than our Sun and more metal-rich, the single oxygen line analyzed in the previous study to determine the star's oxygen abundance is more prone to error,\" Teske explained. \"Averaging all of these measurements together gives us a more complete picture of the oxygen abundance in the star.\"\nThe researchers found that the host star may actually contain nearly 25 percent more oxygen than carbon. That's about halfway between the carbon ratio of our sun and the ratio suggested by the previous study.\n\"In theory, 55 Cancri e could still have a high carbon to oxygen ratio and be a diamond planet, but the host star does not have such a high ratio,\" Teske said in a statement. \"So in terms of the two building blocks of information used for the initial 'diamond-planet' proposal — the measurements of the exoplanet and the measurements of the star — the measurements of the star no longer verify that.\"\nTeske also explained that the compositions of planets and stars don't always match; the makeup of a planet also seems to depend on planet-forming processes that are not fully understood, she said.\n\"Depending on where 55 Cancri e formed in the protoplanetary disk, its carbon-to-oxygen ratio could differ from that of the host star,\" Teske said in a statement. “It could be higher or lower. But based on what we know at this point, 55 Cancri e is more of a 'diamond in the rough.'\"","Cecilia D’Angelo, a molecular coral biology lecturer at the University of Southampton. Jörg Wiedenmann, Elena Bollati & Cecilia D’Angelo/University of Southampton, Palawan colourful bleaching image by Ryan Goehrung/University of Washington\nBleaching events used to be few and far between, but they now occur nearly every year. After the coral is exposed, it often breaks down and dies, altering the ecosystem for the diverse array of life that relies on it. Corals stand little chance of bouncing back from these events — but a new study suggests they have an unusual survival method: taking on a vibrant neon color.\nWhen bleaching events occur, extended heat spikes cause corals to turn a ghostly white, often leading to their death. Even slight increases in annual ocean temperatures can wreak havoc on this relationship, expelling the algae from the coral’s tissue and exposing its white skeleton. These corals can still undergo some of their normal functions for a short period of time as they hope their algae come back — whereas drastic changes in ocean temperature almost always lead to coral death. Reports of colorful bleaching during the most recent mass bleaching event in the Great Barrier Reef in March and April gave scientists hope that patches of the system have a chance to recover. “Bleaching is not always a death sentence for corals, the coral animal can still be alive,” said Dr. They found that colorful bleaching events occur when corals produce “what is effectively a sunscreen layer” on their surface to protect against harmful rays and create a glowing display that researchers believe encourages algae to return. The Ocean Agency describes the process as a “chilling, beautiful and heartbreaking” final cry for help as the coral attempts to grab the algae’s attention. “Our research shows colorful bleaching involves a self-regulating mechanism, a so-called optical feedback loop, which involves both partners of the symbiosis,” lead researcher Professor Jörg Wiedenmann of the University of Southampton said in a press release. Climate Change\nDying coral reefs turn vibrant neon in apparent survival effort\nScientists say climate change is turning coastal Antarctica green\nStudy: Climate change makes a Dust Bowl heat wave more likely\nAir pollution is already spiking in China with virus lockdown lifted\nIndia’s carbon emissions fall for the first time in four decades\nMore in Climate Change\nResearchers at the University of Southampton’s Coral Reef Laboratory studied 15 colorful bleaching events worldwide between 2010 and 2019 — including one in the Great Barrier Reef, the world’s largest coral reef system — and recreated those ocean temperatures in a lab. But “colorful bleaching” has the opposite effect: the dying corals gain more pigment, and glow in shades of bright pink, purple and orange. Scientists first spotted the mysterious neon coral a decade ago, but they had been unable to figure out why it occurred. As the recovering algal population starts taking up the light for their photosynthesis again, the light levels inside the coral will drop and the coral cells will lower the production of the colorful pigments to their normal level.”\nColorful bleaching Acropora corals in the Phillipines. Dying coral reefs turn vibrant neon colors in apparent last-ditch effort to survive\nBy Sophie Lewis\nMay 22, 2020 / 4:48 PM\n/ CBS News\nScientists use mini-satellites to save coral reefs\nFor years, coral reefs around the world have been devastated by mass bleaching events as the oceans continue to warm due to climate change. “If the stress event is mild enough, corals can re-establish the symbiosis with their algal partner.”\nThe internal changes that allow colorful bleaching to occur. In 2017 alone, nearly half the corals on the Great Barrier Reef died — and experts say we are running out of time to save them. “Unfortunately, recent episodes of global bleaching caused by unusually warm water have resulted in high coral mortality, leaving the world’s coral reefs struggling for survival,” D’Angelo said. Scientists emphasized that while colorful bleaching is a good sign, only a significant reduction of greenhouse gases globally — in addition to improvement in local water quality — can save coral reefs beyond this century.\n“Now that we know that nutrient levels can affect colorful bleaching too, we can more easily pinpoint cases where heat stress might have been aggravated by poor water quality,” researchers said. Ryan Goehrung, University of Washington\nCoral reefs support more species per unit area than any other marine environment, including about 4,000 species of fish, 800 species of hard corals and potentially millions of other undiscovered species, according to the National Oceanic and Atmospheric Administration. Colorful bleaching Acropora corals in New Caledonia.\nRichard Vever/The Ocean Agency/XL Catlin\nCoral animals symbiotically coexist with tiny algae, providing them with shelter, nutrients and carbon dioxide in exchange for their photosynthetic powers. Together, these actions can secure a future for coral reefs.”\nFirst published on May 22, 2020 / 4:48 PM “The resulting sunscreen layer will subsequently promote the return of the symbionts. This study, published Thursday in the journal Current Biology, suggests the corals change color as a last-ditch effort to survive. “This can be managed locally, whereas the ocean heat waves caused by climate change will need global leadership. Disruptions to coral reefs have far-reaching implications for ocean ecosystems.\nIt’s not just warming oceans that cause colorful bleaching. Researchers say changes in nutrient levels within coral reefs due to fertilizer run-off from farms also lead to bleaching events — a problem that can be fixed at the local level. Experts believe only coral that has faced mild or brief disturbances, rather than extreme mass bleaching events, can attempt to save itself using this process."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b23697a3-3467-4b3f-a9ae-c688ef828d74>","<urn:uuid:369f1095-9589-4d83-9007-35418b336092>"],"error":null}
{"question":"What is the main difference between how Nebraska's PHLIPS and SSL/TLS protocols handle data protection?","answer":"PHLIPS (later upgraded to ELIRT and NuLIRT) is a web-based laboratory test ordering and reporting system specifically for public health laboratories in Nebraska, focusing on tracking specific disease conditions like West Nile virus and tuberculosis. In contrast, SSL/TLS protocols are general-purpose end-to-end security protocols used to protect information traversing the Internet, primarily used between web browsers and web servers, where encrypted messages cannot be decrypted by any unauthorized party.","context":["Data resolution and quality issues\nMaintaining disease incidence data and presenting morbidity and mortality information is one of the most important tasks of Two Rivers Health Department. Wider dissemination allow us to keep citizens of the health district more informed, while simultaneously providing policy makers with accurate tools for long-term planning and forecasting. In the context of the global covid-19 pandemic however, the urge to report new cases must often be balanced against the real-time exigencies caused by the time lag in data collection, collation and verification between State and local health departments.\nIn 1999, Nebraska was the first state in the US to set up a web-based laboratory test ordering and reporting system for public health laboratories. The system, named Public Health Laboratories Information Program (PHLIPS), was later upgraded to the Electronic Laboratory Information and Retrieval Technology (ELIRT) system in 2005. Currently, the state uses an upgraded version (NuLIRT) and tracks testing done for disease conditions like West Nile virus, sexually transmitted diseases, hemorrhagic E-coli, lead screening, salmonella, Bordetella, measles, pertussis and tuberculosis. These technology systems are critical tools in our response to ongoing health problems as well as emerging health emergencies by allowing us to monitor, predict and plan prevention and mitigation strategies. Nebraska was the first state in 2003 to adopt the Electronic Laboratory Reporting (ELR) system that was developed by the CDC. All local health departments were brought online by 2005. The TRPHD primarily relies on NEDSS and NuLIRT to obtain comprehensive and reliable laboratory data on ongoing communicable health problems, including covid-19.\nAt TRPHD, we aim to ensure data integrity by focusing on three key aspects: data quality, safety and fidelity. Data quality is a constant challenge because of the disparate nature of covid-19 testing and treatment sites, often co-ordinated by multiple agencies and private entities across a patchwork of local health departments and county administrations in Nebraska. TRPHD has a dedicated team of contact tracers who follow up each positive case with a telephone call for communicating health safety messages and for thinking through quarantine plans. Multiple attempts are made to reach out to district residents in the three applicable languages (English, Spanish and Somali) and to verify information while collecting newer data. This data is entered in the common dashboard in real-time.\nData safety is ensured by end-to-end encryption of all public health data that is shared or analyzed by TRPHD. Currently we ensure this through the use of online tools like citrix sharefile, and ensuring standard protocols for storage and disposal of paper records.\nData fidelity remains the biggest challenge to TRPHD’s attempts to reconcile data across multiple systems and to present the public with accurate tools to assess risk and monitor progress. Due to the unavoidable delay that occurs as data reconciliation occurs between local and state health department sources, minor discrepancies in case counts or testing numbers may understandably cause confusion in the minds of citizens. We would like to assure citizens that we take the job of analyzing and presenting data very seriously, and are constantly vigilant in detecting potential sources of error, while working closely with State health agencies to resolve issues. In case of any discrepancy, the district population is advised to refer to the numbers released by TRPHD, as these represent the most accurate count for disease numbers as of that day.","By Kevin Stine and Quynh Dang\nHealthcare and health IT professionals are entrusted with protecting the privacy and confidentiality of patient data. To provide this protection, these professionals frequently look to commonly accepted technologies and methodologies to safeguard the data while at rest and in transit. One technology capable of providing this type of protection is encryption.\nThe HIPAA security rule has long identified encryption (an addressable implementation specification) as a mechanism to safeguard electronic protected health information. More recently, the standards and certification criteria for electronic health records (EHR) specified that EHRs must be able to encrypt and decrypt health information in order to qualify for stage 1 of the meaningful use incentive program.\nSimilarly, guidance accompanying the breach notification interim final rule identified encryption as one technology that can render protected health information \"unusable, unreadable, or indecipherable to unauthorized individuals.\" Protected health information that is encrypted in accordance with this guidance is not subject to breach notification requirements under the interim final rule. The guidance discusses encryption as a mechanism to protect data in transit and at rest.\nImplementing and managing an encryption solution requires an understanding of basic encryption processes, an awareness of the security properties provided by encryption, and knowledge of important requirements for effective encryption.\nEncryption is a security control used primarily to provide confidentiality protection for data. It is a mathematical transformation to scramble data requiring protection (plaintext) into a form not easily understood by unauthorized people or machines (ciphertext). After being transformed into ciphertext, the plaintext appears random and does not reveal anything about the content of the original data. Once encrypted, no person (or machine) can discern anything about the content of the original data by reading the encrypted form of the data.\nEncryption is a reversible transformation. It is useful only when encrypted data (ciphertext) can be reversed back to its original, unencrypted form (plaintext). If not reversible, the encrypted data are considered unreadable and unusable.\nThis reversal process is referred to as decryption. An encryption process has a corresponding decryption process, which is used to reverse the encrypted data (ciphertext) back to its original content (plaintext).\nEach encryption and decryption function requires a cryptographic key. A cryptographic key is a string of binary digits used as an input to encryption and decryption functions.\nIn order for the encryption function to transform the plaintext into ciphertext and for the decryption function to reverse the ciphertext back to its original form, the encryption and decryption functions must use the same cryptographic key. This is referred to as a symmetric key. The encryption functions specified in the Advanced Encryption Standard are widely supported in current systems and software.\nAs depicted in the figure at right, the encryption function requires two inputs, plaintext and a cryptographic key, in order to output ciphertext.\nEncryption in Widely Used Computer Applications\nEncryption is widely used in many computer applications to protect data in transit and at rest. User involvement in the encryption process may vary for each application. For example, in some applications of secure Web browsing using Secure Socket Layer (SSL) or Transport Layer Security (TLS) protocols, the use of encryption may be transparent to users. In other implementations, users may be required to enter a password to encrypt or decrypt the protected data if the cryptographic key is derived from the password.\nE-mail can be encrypted by the sender and then decrypted by the intended recipient using Secure/Multipurpose Internet Mail Extensions (S/MIME). E-mail can be read only by the sender and the intended recipient.\nInternet Protocol Security (IPsec) is a suite of network layer security protocols frequently used to establish virtual private networks. They enable only the two ends of a communication in a computer network to understand the encrypted messages exchanged between them.\nThe SSL protocol and its successor, TLS, are the primary end-to-end security protocols used to protect information traversing the Internet. The most common usage scenario for these protocols is a Web browser, acting as a client for the human user, interacting with a Web server. Using SSL and TLS, encrypted messages between a Web browser and a Web server cannot be decrypted by any unauthorized party.\nIn addition to protecting data being transmitted over computer networks, encryption is also used to protect data at rest, such as data stored on hard drives, USB drives, and other end-user storage devices. For example, when an encrypted hard drive is stolen or in an unauthorized user's possession, the encrypted data on the hard drive are useless to the unauthorized user because the unauthorized user cannot reproduce the plaintext from the hard drive without the cryptographic key.\nRequirements for Implementing Encryption\nEncryption is an important security control to provide confidentiality protection for data. For encryption to be effective and to provide data confidentiality, it is important for the following requirements to be met.\nKeep the cryptographic key secret. Encryption algorithms are made public to allow for interoperability, ease of use, and more open and effective analysis. The security of the encryption depends on the secrecy of the cryptographic key. The cryptographic key must be kept secret from all entities who are not allowed to see the plaintext. Any person or machine that knows the cryptographic key can use the decryption function to decrypt the ciphertext, exposing the plaintext. If a strong cryptographic key is generated but is not kept secret, then the data are no longer protected. Keeping the cryptographic key secret ensures confidentiality protection of the protected data.\nProtect the cryptographic key from modification. The cryptographic key must always be protected from modification. For the ciphertext to be transformed to plaintext, the decryption function must use the same cryptographic key used by the encryption function to decrypt the ciphertext. If the cryptographic key is modified, the plaintext cannot be reproduced. When this happens, the plaintext (the protected data) is lost. It is very important to protect the cryptographic key from any modification (including being lost).\nLike other files, cryptographic keys could be intentionally or unintentionally modified. For example, cryptographic keys could be unintentionally corrupted during transmission if an application or protocol using the cryptographic key does not operate as expected. A malicious user with access to the cryptographic key could intentionally modify the cryptographic key to prevent access to encrypted data. In either situation, plaintext data cannot be reproduced by the modified cryptographic key.\nTherefore, any system using encryption should have a key recovery mechanism to recover the cryptographic key if it is lost or modified. An example of such a recovery mechanism is to make multiple copies of the cryptographic key, and store them separate from each other in locations unknown to unauthorized parties. If the original key is modified or lost, a recovery copy can be used.\nKnow the importance of cryptographic key length in choosing an encryption algorithm. To decrypt the ciphertext, an attacker must search for the cryptographic key by decrypting the ciphertext with all possible keys until the correct key is found. For example, if the cryptographic key is two bits long, there are four possible keys that the attacker may try (00, 01, 10, 11). The longer the key, the more possible keys the attacker must try.\nGenerally speaking, an encryption algorithm that uses a longer key provides a greater level of confidentiality protection. For example, the Advanced Encryption Standard using a 192-bit key (AES-192) provides stronger protection than the Advanced Encryption Standard using a 128-bit key (AES-128) because there are more possible values for a 192-bit key than for a 128-bit key.\nGenerate a strong cryptographic key and transport it securely. If an attacker can get information about certain bits of the key, then the encryption function using this key does not provide the necessary level of protection. For example, if the key is two bits and the attacker knows that the first bit of the key is equal to the second bit, then the attacker needs to try only two possible keys, 00 and 11, instead of four combinations.\nIdeally, a cryptographic key is a randomly generated string of bits that provides the attacker with no information about any bits of the key. Keys can be generated using a Deterministic Random Bit Generator, a function used to generate high-quality random bits for an encryption key. The National Institute of Standards and Technology's \"Recommendation for Random Number Generation Using Deterministic Random Bit Generators\" recommends NIST-approved mechanisms for the generation of random bits using deterministic methods.\nCryptographic keys can be generated solely by the encrypting entity, or through cooperation between the encrypting and decrypting entities, depending on the usage scenario. NIST's \"Recommendation for Key Management—Part 1\" discusses approved cryptographic key generation methods when the key is generated solely by the encrypting party.\nIn many secure communication protocols (e.g., TLS), the cryptographic key may be generated through cooperation of the encrypting and decrypting entities. NIST's \"Recommendation for Key Management—Parts 1 and 2\" provide guidelines on these key agreement schemes.\nIn an application where the encrypting entity needs to share the key with a separate decrypting entity, the key must be transported to the decrypting entity in a secure manner. This transportation can be done physically using an electronic device such as a USB drive that holds the cryptographic key. It can also be done electronically over a computer network. NIST's \"Recommendation for Key Management—Parts 1 and 2\" provide guidelines on methods of secure key transport.\nEncrypt all copies of the data. All data that require confidentiality protection should be encrypted if there is a possibility that an unauthorized person could access it. Data at rest in an operational environment are frequently encrypted. However, all copies of data, including data in storage and back-up environments, should also be encrypted to provide comparable protection.\nTransition to NIST-approved encryption functions. Over time, changes in the use of encryption may be necessary because of cryptographic attacks on encryption algorithms or the availability of more powerful computing techniques and/or devices. Data encrypted in the past using a non-NIST–approved encryption algorithm or a NIST-approved encryption algorithm that has become obsolete should be encrypted using a current NIST-approved encryption algorithm to ensure a strong level of protection for the data. NIST's \"Transitions: Recommendation for Transitioning the Use of Cryptographic Algorithms and Key Lengths\" identifies current approved algorithms and timelines for acceptable use.\nFor more information on encryption processes and technologies, visit NIST's Computer Security Resource Center at http://csrc.nist.gov.\nKevin Stine is an information security specialist and Quynh Dang is a computer scientist for the National Institute of Standards and Technology, Information Technology Laboratory, Computer Security Division."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:65561253-cfa6-42b7-b4e5-1bec1bc2eb27>","<urn:uuid:8ff22ccc-ba02-42c5-aa61-46164357c3ad>"],"error":null}
{"question":"What energy savings can LED lighting controls provide, and what are the maintenance benefits of LED fixtures compared to HID systems?","answer":"Lighting controls can reduce lighting energy costs by up to 35 percent while providing capabilities for tuning light levels and brightness. When comparing maintenance aspects, LEDs offer significantly longer life compared to HID technologies, resulting in less frequent replacement and lower maintenance costs. LED life ratings (L70) should be at least 70,000 hours or L90 of 50,000 hours before light output degradation, making them much more durable than traditional HID systems.","context":["- Asst. Vice President, Facilities & Operations »\n- Gen. Mngr. Fac. Mngmt. Oklahoma City »\n- Senior Manager of Facilities, Project Management »\n- Director, Space Management & Planning »\n- Facilities Manager »\nLighting Controls Upgrade Guidelines\nOTHER PARTS OF THIS ARTICLEPt. 1: This PagePt. 2: Understanding Applications for Lighting Controls UpgradesPt. 3: Lighting Controls Upgrades: Understanding Space Needs\nLighting controls can reduce lighting energy costs by up to 35 percent while offering maintenance and engineering managers additional capabilities for tuning light levels and brightness, controlling color, and generating data they can leverage for new operational efficiencies. But achieving real value in a given upgrade requires the proper specification and application of controls.\nIn new buildings where energy codes regulate design, controls typically must be installed to control nearly all lighting. In an existing building undergoing an upgrade from traditional sources to LEDs, controls can be installed as part of the upgrade wherever sufficient opportunity and value intersect. That value can be sweetened by utility rebates when applicable.\nTo deliver successful lighting controls upgrades, managers need to focus on several key issues — understanding control strategies, determining space needs, and applying planning principles.\nFocus on strategies\nLighting controls and systems are essentially input/output boxes. The input might be based on occupancy, a time event on a schedule, a light level or a signal from another building system. The output is typically switching — on or off — or dimming — raise or lower light level — though demand for new capabilities such as color output control and data generation is growing significantly.\nThe combinations of these inputs and outputs give managers a range of available control strategies that are driven by the primary application need, which can be energy management, visual needs, or operational efficiency.\nFor energy management, managers need to consider four main strategies, all based on the principle that lighting should be turned off or reduced when occupants do not need it:\n• Occupancy sensing can automatically switch or dim lighting and switch plug loads when a space is unoccupied and potentially send a signal to a security system.\n• Scheduling can automatically switch or dim lighting and switch plug loads based on occupancy predicted at certain times.\n• Daylight response entails automatic switching or dimming lighting when daylight raises light levels.\n• Institutional task tuning involves tailoring light levels by individual space.\nFor visual needs, the two main strategies are:\n• Manual-control switching or dimming based on human intervention.\n• Color tuning involves the manual or automatic adjustment of color output, either by tuning white light on a scale between warm — orangish to white — and cool — bluish-white — or by producing saturated colors for a communication or entertainment purpose.\nFor operational efficiency, managers can produce data and monitor a lighting system for failures, and they can use this automatically generated information to analyze energy use, monitor equipment for maintenance, track inventory, and identify efficiencies in space utilization, among other uses.\nOutputs are driven by what managers want the controls to do. For visual needs and information, the application is driven primarily by these goals because the inputs do not depend on the application, only the outputs.\nFor example, if a manager wants to enact space-optimization strategies based on measuring occupancy patterns, then a data-delivering control system is optimal. If a manager wants the lighting system to change the color quality of white light in a classroom to allow teachers to signal a change in activity, then color tuning is optimal. And if a manager wants users to be able to adjust light level for presentations in a conference room, some type of dimming solution is optimal.\nFor strategies dictated by energy management, the controls application is driven by conditions that enable various inputs. If the space is intermittently occupied, managers can consider occupancy sensing. If the space is predictably occupied, they can consider scheduling. If the project is designed at a fixed light level significantly higher than that needed in individual spaces, consider institutional task tuning. And if the space has ample daylight, daylight response is a viable option.\nLooking at the outputs of switching or dimming, the most appropriate choice depends on the way occupants use the space:\nOccupancy sensing and scheduling. If the space is unoccupied and lights are not needed for safety or security, switching is optimal. But if the space is unoccupied and the lights must stay on — such as in a lobby, corridor, or stairwell — dimming is optimal.\nInstitutional task tuning. Generally, the output is dimming because the space is used while this strategy is in effect.\nDaylight response. The control effect might occur while the space is occupied. If occupants are performing demanding, stationary tasks, then dimming is optimal. For transitional spaces such as lobbies, managers can implement switching with less likelihood of disrupting occupants.\nLighting Controls Upgrade Guidelines","Which Lighting System is Right for You?\nLED lighting systems are changing the way we light our indoor and outdoor spaces. In interior high ceiling applications and exterior areas that traditionally use HID fixtures, LEDs offer significant energy savings and dramatically reduce maintenance costs. LEDs paired with customizable controls can save you substantially more energy than LEDs without controls as the lighting output can be adjusted to meet project requirements. Use the chart below to determine which LED lighting system best fits your HID retrofit needs.\n|New retrofit LED lamp without controls||\nNew LED fixture or retrofit kit without controls\n|New LED fixture or retrofit kit with integrated controls|\nIncentives and Savings\n|Utility incentives (contact your utility for more information)||May be limited or lower than new fixture||Calculated or deemed||Calculated|\n|Cost of ownership||Good||Better||Best|\nThere are a range of factors to consider when retrofitting an existing fixture. Your results will vary based on location, facility layout and electricity costs, among other things.\nNew LED lamp\nHID-LEDs for replacing existing high intensity discharge lamps are screw-based LED lamps that use the existing lamp sockets in the existing HID fixture housing. Although LED replacement lamps typically offer better efficiency and longer life than existing HID lamps, their lighting distribution can't be optimized for all fixture optics. Retrofit kits and new fixtures are designed as engineered units and offer higher energy savings, better life and better lighting quality than HID-LED lamp replacements.\nTypes of HID-LEDs available to replace HID lamps:\nPlug and Play (Type A): These lamps use the existing HID ballast (if compatible) and lamp socket. As such, installation does not require rewiring or an electrician. Plug and play HID-LED lamps are typically the fastest and least expensive way to upgrade to LED, but are also the least efficient retrofit option. They can be a short-lived solution due to potential ballast failure, ballast incompatibility or difficulty in finding a ballast replacement. Also, if dimming is desired, a compatible dimming ballast and dimmable LED lamp is required.\nLine Voltage (Type B): Also known as ballast bypass, these replacements do not use the existing ballast, eliminating the ballast issues seen in plug and play lamps. They offer greater energy savings than plug and play as the ballast wattage draw is eliminated. Installation costs are higher than plug and play LEDs, as an electrician is required to remove the old ballasts and re-wire the fixture. To add or maintain dimming, the HID-LED must be dimmable and a new dimmer compatible with the HID-LED will need to be installed.\nAll HID-LED replacement lamps are less efficient than a retrofit kit or new fixture because the light distribution is not optimized for the fixture. A few manufacturers are offering networked control capabilities imbedded into the HID-LED lamps, allowing scheduling, dimming and even changing light distribution via a smart phone or tablet.\nAs with all lamp retrofits, testing a solution with a handful of fixtures before applying the upgrade widely is recommended.\nNew LED Fixture or retrofit kit\nNew LED fixtures and retrofit kits use LEDs as the light source instead of traditional high intensity discharge (HID) sources (high pressure sodium and metal halide). The LEDs are factory installed directly in the fixture without the use of traditional lamp sockets. Since fixtures and retrofit kits come with new drivers, they are typically compatible with dimming controls and networked control systems (commonly referred to as 'network control capable ready').\nNew fixtures usually offer higher energy savings, last longer, and have the lowest failure potential, compared with retrofit kits or lamp replacements. With new fixtures, the manufacturer designs the light distribution around the LED chips to meet industry standards of light distribution and glare control.\nA retrofit kit is a bundled set of hardware designed to replace all electrical and lens components of an existing fixture while leaving the original fixture housing in place. A retrofit kit effectively creates a completely new luminaire, without having to replace the entire fixture, typically resulting in lower equipment and installation costs compared to a new fixture. Retrofit kits typically outperform LED replacement lamps installed in existing fixtures, offering longer life, better heat management, lower failure potential and higher efficiency.\nWith retrofit kits the manufacturer designs the light distribution around the LED chips but often designs the kit to fit in multiple different housings. A generically designed kit sacrifices some optical control (light distribution and glare control) since the kit will be installed in an existing fixture housing.\nNew LED fixtures and retrofit kits do not use socket-mounted or magnetically-attached LED tubes.\nNew LED fixture or retrofit kit with integrated controls\nIntegrated control fixtures and retrofit kits are generally referred to as Luminaire Level Lighting Control (LLLC) fixtures and kits. At minimum, LLLCs incorporate embedded occupancy sensors, daylight sensors and wireless controls into LED light fixtures or retrofit kits. Most LLLC products offer additional energy and occupant comfort features such as continuous dimming, task tuning (brightening or dimming each fixture or a group of fixtures to provide the ideal light level for occupants), programmable scheduling, and more. The fixtures wirelessly communicate with each other, so can operate in groups or independently.\nExterior fixture control capabilities may include photocell control, scheduling on/off, scheduling dimming levels, grouping, occupancy sensing, and lumen depreciation adjustment.\nThese systems provide maximum energy savings and fixture life, easy installation, and operational efficiencies. While simple payback may take longer, cost of ownership can be significantly lower on these installations, especially when factoring in maintenance savings and ease of reconfiguration.\nQuality of light\nFrom the color of the LED light to the lighting distribution of the fixture, retrofitting to LED can improve the quality of lighting in your space compared with traditional technologies. LED lighting tends to show colors more accurately and vibrantly than HID, improving the look of the space.\nLED fixtures and retrofit kits can also light the space more effectively. The fixture manufacturer can shape the lighting distribution of the fixture (how the light comes out of the fixture) to optimize the lighting in your space. The optimization could be to distribute more light on the walls, which will brighten the look of the space, or in a higher ceiling space, to increase the lighting intensity by pushing more light to the floor.\nOne major advantage of new fixture installation is the ability to locate the lights for best distribution, whereas depending on the existing lighting layout may offer poor uniformity or inadequate light levels at the task. Reconfiguring the lighting layout in the space allows you to correctly address under or over-lit areas.\nWhen paired with sensors and controls, LEDs can offer a range of additional capabilities. These features and functions can improve the lighting quality, enhance the occupant experience, decrease operation costs, help your new system meet building code, improve occupant performance and safety, and even help your business run more efficiently. A few of the more common Networked Lighting Controls capabilities include:\n- Dimming: vary the light output of the fixture, enabling energy savings and greater occupant satisfaction.\n- Occupancy or Vacancy control: regulate lighting based on presence or absence of people in a space, increasing energy savings and reducing maintenance costs.\n- Daylight control: regulate the light level in response to changing daylight conditions in the space, increasing energy savings.\n- High-End trim/Task tuning: adjust the output of individual lights or group of lights to a set lower maximum level, enabling energy savings, greater occupant satisfaction, and lower maintenance costs thanks to longer system life. Over time, as lumen depreciation naturally occurs, the light output can be adjusted to maintain consistent brightness for safety and comfort.\n- Networking: connect all fixtures as a network, allowing them to exchange information with each other or even integrate with an Energy or Building Management Systems (EMS or BMS). Networked fixtures can be configured and controlled individually or as a group to optimize energy savings; some allow for remote configuration and control. Networking, especially using wireless systems such as LLLC, can provide reduced wiring, maintenance and reconfiguration costs and can promote greater energy savings.\n- Zoning: ability to configure groups of fixtures to perform the same lighting strategies (same occupancy time-outs, same task tuning, etc.). Zoning often results in reduced cost of wiring, maintenance and reconfiguration and greater energy savings.\n- Layering: combining controls strategies where applicable (occupancy, daylight sensing, dimming, high-end trim, scheduling) can optimize energy savings far beyond single control strategies.\n- Color tuning (only available with color-tunable LEDs): ability to adjust the color temperature of the lamp, promoting higher occupant satisfaction and performance.\n- Automation and tracking: sensors embedded in light fixtures can be used to perform a wide range of functions, from identifying unused meeting rooms, to automating door operation, to tracking products in a warehouse, enabling improved space utilization and business efficiency.\nUtilities often offer enhanced incentives for lighting systems that include controls. Contact your local utility to learn about lighting controls incentives available in your area.\nLEDs offer significantly longer life compared with incandescent HID technologies, resulting in less frequent replacement and lower maintenance costs. For Type \"A\" plug and play LED lamps that utilize an existing ballast, the ballast may fail before the LED lamp, which will require the ballast to be replaced. Unfortunately, there is no visible indicator that the ballast failed and the only way to test is to install the existing LED lamp in a fixture that is operating properly.\nLED life ratings are based on the age at which the fixture or lamp will degrade to 70 percent of its original light output. This is shown on manufacturers' specification sheets as the \"L70\" designation. Some manufacturers list L90, which is the life when the fixture is still producing 90% of its initial light output. Look for an L70 of at least 70,000 hours or L90 of 50,000 hours.\nWe experience Discomfort and Disability glare when the variations of luminance (surface brightness) across the visual field are too great. Disability glare is defined by a measurable reduction in visibility with high luminance contrast. A fixture's casing, lens, and angles can help reduce glare potential.\nExterior fixtures are evaluated with the B.U.G. (Backlight, Uplight, Glare) rating system. The Glare component's effect is measured on a scale of 0-5 with 0 being the lowest potential and 5 being the highest (and least desirable) potential.\nFactors to Consider when Embarking on a Lighting Upgrade\nThere are a range of factors to consider when retrofitting an existing fixtureThe following are important considerations that could influence your decision:\n- Age and condition of ballast: When retrofitting an existing fixture with direct replacement LED lamps, consider the type and age of the existing ballast. Failure of older existing ballasts before HID-LED end-of-life will defeat the 'reduced maintenance' attractiveness of LED solutions. Some incentive programs restrict use of existing ballasts in lighting upgrades.\n- Age and condition of sockets: As fixtures age, lamp sockets can become brittle due to the heat and UV exposure they experience over time. Older fixtures may need new sockets to offer best life expectancy.\n- Net Efficacy: New highly-efficient LED products installed into existing fixtures can experience losses of up to 10 – 15 percent due to inter-reflected light loss – basically, light trapped within the fixture. Addition of a lens will reduce efficiency another 10 – 12 percent, or even more if the lens is old and yellowed. Note that these reductions were present with the existing HID lamps also.\n- Light Distribution: The way a fixture distributes light is determined by its reflectors and lens, which are designed around the original lamp type. Replacing existing lamps with LEDs will change the distribution characteristics of the fixture, which may cause shadowing or sharp cutoff at walls. A test installation will help determine whether direct one-for-one lamp replacement will give suitable results. Be sure to evaluate the light levels, the lighting distribution and the visual look of the space in the test installation.\nLabor costs: Labor costs will depend on the type of LED retrofit planned.\n- LED lamps – HID-LED lamp replacement can be as simple as removing the existing HID lamps and plugging in the LED lamp. LED lamps may require incompatible ballasts to be replaced, may require removal of the existing ballast and rewiring of the fixture, or may require replacement of the ballast with a new LED driver. In all cases damaged sockets should be replaced.\n- Retrofit kits – Retrofit kits install into the existing housing from below the ceiling and can be an effective alternative to LED lamps or new fixtures. They generally require removal of all internal components of the existing fixture and installation of new internal LED components.\n- New Fixtures –With new fixtures the replacements do not need to be on a one-for-one basis or even in the same location, which allows for the lighting layout to be optimized in the space. Replacing or relocating a fixture may require seismic anchor wires (slack wires), which may or may not be present on the existing fixtures.\n- Some types of replacements require an electrician. The new fixture, retrofit kit and replacement lamp descriptions in this guide provide additional details regarding which types of retrofits will require an electrician.\nLinks to a PDF and customizable InDesign files for the LED Replacements for High Intensity Discharge (HID) Lighting Comparison Tool are below."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:b8ff5477-83fa-4208-8054-9094f718a970>","<urn:uuid:a397c738-ca58-4bb9-90d9-304c1c5eab72>"],"error":null}
{"question":"I've been studying logistics history and I'm curious - what was the significance of the Ideal X container ship in 1958, and how did containerization impact rail transport in later decades?","answer":"The Ideal X's arrival at Port of Houston in 1958 carrying 58 containers marked the beginning of a revolutionary transformation in global shipping. This converted oil tanker initiated the process of containerization that would reshape the global economy over the next 30 years. Prior to this, shipping was highly inefficient, with non-standardized packaging and heavy reliance on manual labor. In terms of rail transport impacts, the standardization of containers led to significant changes in the 1980s and 1990s, including the widespread adoption of double-stack trains, 48' containers appearing in quantity, and the emergence of 48' spine cars. Additionally, the efficiency of containerization transformed traditional shipping methods, as seen in how centerbeams eventually surpassed bulkhead flats for hauling lumber.","context":["Railroad Milestones Since 1945 - A Tool for Modelers.\nBluford Shops products are not intended for children under 14.\nKodachromes appear on SP and ATSF.\nEnd of Train Devices begin to replace cabooses.\nDouble Stack trains appear in quantity.\nMovement of bulk grain in boxcars ends in the United States.\nWest Tennessee RR spins off of ICG.\nSanta Fe acquires SDP40F's from Amtrak.\nSanta Fe begins selling CF7's.\nIndiana Railroad appears (spinning off of ICG.)\n53' trailers begin to appear.\nWisconsin Central re-appears (spinning off of Soo Line.)\n48' Containers appear in quantity.\nChicago Central & Pacific appears (spinning off ICG.)\nDakota Minnesota & Eastern appears (spinning off C&NW)\nICG becomes Illinois Central again.\nGP60, SD50F, SD60F, F40PH-2 introduced.\nBN begins painting cars with no name or logo.\nPaducah & Louisville appears (spinning off of ICG.)\nWashington Central appears (spinning off BN.)\nSpringfield Terminal becomes operator of MEC and B&M under Guilford.\nButte Anaconda & Pacific becomes Rarus Railway.\nMilwaukee Road merges into Soo Line.\nGulf & Mississippi appears (spinning off of ICG)\nTriple Crown Services appears.\nToronto Hamilton & Buffalo folded into CP Rail.\nArkansas & Missouri appears (spinning off of BN)\nMidSouth appears (spinning off ICG.)\nChicago Missouri & Western appears (spinning off ICG)\nFirst RJ Corman shortline appears.\nCentral Michigan appears (spinning off of GTW.)\nRed River Valley & Western appears (spinning off BN)\nMontana Rail Link appears (spinning off BN)\nFruit Growers Express replaces \"Solid Cold\" with \"Real Cold\" on insulated boxcars. (Only 50+- cars receive this version.)\nMidSouth buys Gulf & Mississippi.\nOhio Central System appears.\nBC Hydro Railway becomes Southern Railway of British Columbia.\nFort Worth & Western appears.\nBuffalo & Pittsburgh appears (spinning off CSX)\nSouthern Pacific acquired by Rio Grande Industries.\nCM&W shuts down, SP enters Chicago, Gateway Western launched.\nKCS goes from white diesels to gray.\nSanta Fe brings back red and silver war bonnet.\nSanta Fe moves it's first JB Hunt trailer.\nBN introduces logo-but-no-name scheme.\n48' Spine cars introduced.\nToledo Peoria & Western becomes independent again.\nTourist line Housatonic RR becomes freight line.\nSouth Florida gets commuter rail.\nNashville & Eastern appears (spinning off of Seaboard System.)\nSelected Chessie and Seaboard System equipment appears in early CSX colors (with B&O, C&O or SBD reporting marks.)\nCabooses no longer required on mainline freights.\nCP Rail phases out the Multi-Mark logo.\nAllegheny Railroad appears.\nArizona Eastern appears (spinning off of Southern Pacific.)\nAustin & Northwestern appears.\nBritish Columbia Railway changes name to BC Rail.\nBuckingham Branch appears (spinning off of CSX.)\nCP Rail establishes Canadian Atlantic Railway.\nAberdeen Carolina & Western opens.\nWisconsin & Calumet appears.\nYadkin Valley appears (spinning off of NS.)\nFlorida Northern appears.\nS-BC, Ch-P, FUS, Pacifico, merged with NdeM to form the new Ferrocarril Nationales de Mexico (FNM.)\nFarmrail opens Grainbelt subsidiary.\nDakota Missouri Valley & Western appears (spun off Soo Line)\n115 ton cars appear in quantity.\nNS reporting marks begin to replace NW and SOU on Norfolk Southern.\nCanadian Pacific fully acquires Soo Line.\nWheeling & Lake Erie re-appears (spun off NS)\nDitch Lights begin to appear in US.\nLast new diesels delivered in Rio Grande paint.\nGolden West Service rolling stock appears.\nIn Chicago, RTA becomes Metra.\nMichigan Northern closes.\nMinneapolis Northfield & Southern merges into Soo Line.\nNortheast Kansas & Missouri opens.\nSanta Fe gets GP60B's bringing back A-B-B-A sets for a short time.\nFriction Bearings banned on many tank cars.\nOregon California & Eastern shuts down.\nArizona & California appears (spinning off of ATSF.)\nTwin Cities & Western appears (spinning off of Soo.)\nWichita Tillman & Jackson appears (spinning off of UP.)\nMohawk Adirondack & Northern opens.\nThe new Oneida & Western ends operations.\nOtter Tail Valley Rwy opens.\nPort of Tillamook Bay RR expands.\nSouthwestern Railroad opens.\nSoutheast Kansas Railroad opens.\nCSX subsidiary roads are officially merged into CSX Transportation (with CSXT reporting marks.)\nSoutheast Coal Company stops running their own trains (SECX.)\nShore Line East commuter operation opens.\nSeagraves Whiteface & Lubbock opens.\nSt. Lawrence & Atlantic opens (spinning off of CN.)\nBlue Mountain & Reading expanded and re-organized as Reading Blue Mountain & Northern a.k.a Reading & Northern.\nSanta Fe replaces Indian Red and large logos with Mineral Brown and no logos.\nSanta Fe sells the last of the CF7's.\nSanta Fe introduces Q logo.\n(Small) logos reintroduced to Santa Fe freight cars.\nTrailer Train becomes TTX.\nMetro Link (Los Angeles) appears.\nCotton Belt is finally merged into Southern Pacific.\nDetroit & Mackinac becomes Lake State Railway.\nSan Joaquin Valley Railroad appears.\nWisconsin & Southern acquires Wisconsin & Calumet.\nIndiana Southern appears.\nMissouri & Northern Arkansas appears.\nConrail adds Quality logo.\nArkansas Midland appears.\nCaltrain (San Francisco, San Jose) appears.\nAllegheny RR becomes Allegheny Eastern under G&W ownership.\nCentral Kansas Railway appears.\nCentral RR of Indiana spins off from CR.\nDallas Garland & Northeastern appears.\nBlue Mountain RR appears.\nGoderich-Exeter Rwy appears.\nPigeon River RR and Hillsdale County RR merge to form Indiana Northeastern.\nCenterbeams surpass bulkhead flats for hauling lumber.\nDASH 8-44CW, DASH 9-44CW, AC4400CW, DASH 8-40BP, SD60I, SD70, SD70MAC introduced.\nConrail joins Triple Crown Services.\nWisconsin Central acquires Fox River Valley, and Green Bay & Western.\nNebraska Central appears.\nConrail acquires Monongahela Railway.\nCape Breton & Central Nova Scotia appears.\nCalifornia Northern appears.\nDelaware-Lackawanna Railroad appears.\nEastern Idaho RR appears.\nIdaho Northern & Pacific appears.\nNatchez Trace sold, becomes Mississippi Central.\nPup (28' etc.) trailers from LTL carriers surge onto the rails.\nFriction Bearings banned in all interchange service.\nMidSouth merges into KCS.\nLouisville & Indiana appears (spinning off of Conrail.)\nCambria & Indiana closes their doors.\nSouthern Railway of British Columbia dons Washington Corp blue paint.\nCP Rail closes Canadian Atlantic subsidiary.\nUP, NS and Conrail form the EMP container pool.\nCentral Vermont becomes New England Central.\nC&NW merges into Union Pacific.\nCentral Oregon & Pacific starts service.\nAT&SF and BN parent companies merge.\nCP Rail establishes St. Lawrence & Hudson to run eastern lines.\nBangor & Aroostook joins Iron Road.\nNew Brunswick Southern appears.\nFinger Lakes Railway appears.\nCN is privatized and integrates Grand Trunk Western.\nWisconsin Central buys Algoma Central, re-logos and re-numbers fleet, leading to complete repainting.\nCanadian American appears.\nCarolina Southern appears.\nAllegheny Valley RR appears.\nGeorgia & Florida RR (part of the G&O Rwys group) opens for business.\nKCS acquires a 49% stake in Texas Mexican.\nSeagraves Whiteface & Lubbock sold to RailAmerica and becomes West Texas & Lubbock.\nOld friction bearing trucks that have been retrofit with roller bearing inserts are banned from interchange.\nChicago & North Western abandons \"Zito\" yellow and brings back \"stagecoach\" yellow.","Date Published: 01/12/2016\nToday’s businesses implicitly operate across a combination of three different value chains: the physical value chain of components and products, the digital value chain of information and processes, and the social value chain of community and insight. During the 1960s, 1970s and 1980s, the physical value chain underwent seismic upheavals due to the introduction of standardized steel containers within logistics processes (a development known as “containerization”). This transformation reduced the costs of supplier integration and drove the emergence of global supply chains. We believe that the continued evolution of the Web as a communications platform – coupled with the realignment and commoditization of the IT industry via cloud computing – means that the digital and social value chains are now on the verge of second and third-wave transformations that will be every bit as disruptive.\nWhen an ungainly converted oil tanker that had been rechristened the “Ideal X” drifted into Port of Houston in 1958 carrying just 58 containers, few of the invited dignitaries and curious workers who stopped to watch its arrival would have guessed that they were witnessing the start of a new global order. But over the next 30 years, the process of containerization started by that maiden voyage would go on to decimate traditional shipping industries, fundamentally change the way in which businesses created and distributed goods and reconfigure the global economy.\nTo understand how this happened, we must first consider the difficulties associated with the shipping of goods prior to containerization. There was little or no standardization and goods were packaged in boxes, bags or loose depending on the preferences of each supplier. On the supply side, each mode of transport had its own methods and cost structures for moving this huge variety, and each mode relied heavily on manual labor at the many hand-over points at which goods were unloaded and reloaded in different ways. Consequently, moving goods was incredibly complex, costly and uncertain.\nThese obstacles prohibited the establishment of extended supply chains – especially on a global scale. The high costs, risks and unreliability of shipping the components required for integration encouraged companies to co-locate as much of their production as possible. Businesses operated large factories near their customers and used their overall organizational mass (i.e. scale across a wide breadth of capabilities) as a competitive barrier.\nThe genius of container pioneers, like Malcolm Maclean, was not in inventing a new kind of box, but rather in understanding one fundamental idea: Customers just wanted to reliably move goods from one place to another; they didn’t care about the internals of the process or about protecting the multitude of traditional industries involved.\nAlthough simple in hindsight, this idea was revolutionary. It presupposed the design of a new and fully integrated end-to-end system. Goods needed to be packed into standardized containers to remove variation and allow wholesale automation across the complete process. Ships, trucks, trains and ports all had to be re-designed.\nThis re-focusing on outcomes unleashed waves of innovation as the process of shipping was integrated, streamlined and automated from start to finish, thus generating immense economies of scale. The cost, reliability and timeliness of what emerged totally destroyed the legacy shipping industry with all of its inefficiencies, disconnects and unreliability. In its place, containerization delivered a new “business integration platform,” if you will, for physical supply chains.\nAs the costs of integrating global suppliers and customers fell, so did the value of existing business models built around aggregation and co-location. Land and labor costs became the dominant factors instead of shipping costs. Countries like Japan, and later China, leveraged these changes to become global economic superpowers. Most importantly, however, the ability to integrate physical value chains reliably and cheaply drove specialization, making supply chains ever longer, more complex and more globalized. In this way, the new business platform that containerization provided drove the formation 3 of new kinds of business ecosystems. In this new economic environment, specialization, component integration and supply-chain coordination replaced vertical integration and co-location to become the new critical competencies for the physical value chain.\nContainerization can, therefore, be considered the first wave of globalization. It changed the basis of competition within the physical value chain. It turned carefully crafted advantages of organizational mass, physical proximity to customers and vertical integration into disadvantages that dragged many organizations into oblivion.\nFast-forward to the present day, and most people looking at cloud platforms and services would see something akin to the “Ideal X” – familiar technologies delivered with some new characteristics. Once again, people stand on the threshold of a new global order and see only a jerry-rigged ship rather than the transformational direction of travel it embodies.\nToday’s IT industry has much in common with the shipping industry at the dawn of containerization. There is little or no standardization in the way that businesses design, build and operate their systems, and each company has an IT organization that largely sets its own standards. On the supply side, the companies that provide technology and services (e.g. hardware, middleware, applications and managed services) each have their own tools, methods and cost structures. Solutions are painstakingly assembled in a way that relies heavily on manual labor at the many hand-over points. Taken in sum, these realities mean that the process of developing solutions often results in single-tenant systems that are too fragile, unreliable and costly to be used beyond the tightly controlled environment of a single enterprise.\nTogether, these issues have left little room for building systems that can be shared across many organizations. Consequently, they have offered only limited opportunities for specialization within the digital and social value chains. The high costs, risks and unreliability of building business services for integration has largely encouraged companies to co-locate much of their business-process and knowledge work within large organizations. That leads them to operate “information factories” and to use their overall organizational mass as a competitive barrier.\nAs with Malcolm Maclean’s reframing of the value boundary for shipping, however, cloud computing is beginning to reframe the IT industry around outcomes. In simple terms, many early general-purpose services – such as Salesforce and Netsuite – have demonstrated the viability of delivering multi-tenant services globally and at scale for use within limited forms of digital supply chain. At the same time, large-scale social and collaborative services – such as Facebook and Wikipedia – have demonstrated the potential of social supply chains by enabling highly distributed communities to work together in new ways.\nCurrent examples like these are very much early outliers – but they demonstrate the potential of digital and social supply chains in much the same way that the containers stacked upon the “Ideal X” demonstrated the potential of containerization for transforming physical supply chains. To really scale digital and social supply chains, we need to build the new end-to-end platforms that will standardize, automate and streamline the realization of businesses’ intended outcomes – i.e. the creation, monetization and distribution of their valuable business IP. And this has already begun to happen.\nWhile still at a very low level of abstraction, emerging cloud infrastructure platforms – such as those offered by Fujitsu and Amazon – are early demonstrations of the viability of realigning a broad range of previously fragmented technologies (including hardware, middleware, management and services) into an integrated and streamlined platform service that allows organizations to focus wholly on their intended solution rather than on the enabling technology. In the same way that containerization ultimately became a platform for integrating business processes within the physical supply chain – by jointly optimizing all of the required technologies and processes around outcomes – we believe that these emerging cloud platforms will ultimately expand in scope to become a platform for the end-toend realization and monetization of a range of complex business service types (e.g. infrastructures, applications, business processes and full business services).\nThe next generation of digital platforms has the potential to optimize all of the technologies and processes required to enable the end-to-end creation, operation, monetization and sharing of scalable and multi-tenant digital and social services. Along the way, they will hide all of this complexity behind high-productivity modeling and development environments that focus on outcomes.\nAs with containerization and its dramatic impact on the structure of companies operating physical supply chains, we believe that the long-term effects of cloud-platform emergence will be profound – particularly for information and knowledge-intensive industries. Even those companies operating physical value chains will not escape these 2nd and 3rd-wave disruptions, however, as physical assets increasingly become connected, digitized and available as a service.\nOne of the most profound effects of the digitization and socialization of individual business capabilities will be a need to rethink the purpose of the firm across two different dimensions. One dimension concerns the shifts in business model required as external digital and social services become available for integration and remixing. The other concerns the more fundamental impacts of digitization as such digital and social services become the de facto external expression of an organization’s capabilities.\nFirstly, the ability to access specialized physical, digital and social services consistently via the global network will mean that the purpose of the firm will no longer be to minimize the transaction costs of doing business by gaining scale and executing efficient in-house processes. Instead, in accordance with our experiences within the physical value chain, we believe that the successful company of the future will be as small as possible and will focus on building world-class digital and social value webs. They will do this by specializing, integrating external capabilities and employing cloud platforms to achieve core digital transformation (i.e. the digital encoding of their own specialized business capabilities as applications, processes and services for sharing with others). Again, as with physical value chains, we believe that integrating and orchestrating specialized providers within extended supply chains will radically improve outcomes to an extent that no individual organization could hope to achieve alone.\nSecondly, such a “core digital transformation” changes the relationship between business and technology. In the past, organizations used IT as one of several non-core tools for increasing the scale and efficiency with which they executed traditional in-house processes. IT was effectively one of the “implementation technologies” underpinning the creation and execution of business capabilities. However, the shift to cloud has a profound impact here, too.\nAs business capabilities of all kinds are increasingly becoming digitized and socialized, these digital services come to encode and encapsulate a company’s core IP. This change requires businesses to reframe the way that they think about IT: The emergence of cloud is not simply a different way of purchasing IT but rather a paradigm shift allowing technology to align more closely to the needs of the business. That transformation then enables them to share and monetize their valuable core IP within new digital and social supply chains.\nThe implications of these two shifts will be profound.\nThe digital platforms of the future will allow the accelerated delivery and low-cost scaling of a company’s specialization without requiring the firm to scale the size of the organization and its available resources. Being big will, therefore, slowly cease to be a competitive weapon. Smaller companies will take advantage of greater agility, world-class specialized services, and the web’s natural cost transparency to become hyper-competitive. If the pattern of change brought by the shipping container repeats itself with digital platforms, we might speculate that huge opportunities lie in wait for those companies that can adapt to and exploit them. The container brought with it the potential for any manufacturing company, regardless of its location, to be competitive on the global stage. It is likely that digital and social supply chains will have a similar effect, unleashing new value for those that are willing to embrace it."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:7652bebd-9d26-4afa-ba16-8b9669555141>","<urn:uuid:1daa4ba9-9546-49ad-b503-63ca53f19635>"],"error":null}