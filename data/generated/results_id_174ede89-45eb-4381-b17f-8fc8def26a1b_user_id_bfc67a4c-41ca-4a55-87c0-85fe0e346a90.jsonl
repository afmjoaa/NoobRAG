{"question":"How did the patriotic sentiments of soldiers at the start of World War I compare with their attitudes after experiencing trench warfare?","answer":"Initially, there was remarkable patriotic enthusiasm, with millions volunteering for armed forces and civilian populations willingly sustaining great sacrifices. These nationalist passions arose genuinely from below with religious intensity, not just from government propaganda. However, as soldiers experienced the horrors of trench warfare in 1916-1917, fighting from dug-in positions with machine guns, heavy artillery, and chemical weapons, they became increasingly disillusioned. The war that had promised glorious 'manly' activity actually delivered 'feminine' passivity, with soldiers crouching in dugouts waiting to be killed. This stark contrast between initial patriotic fervor and later disillusionment was captured by many soldier-poets who protested against the war's meaningless slaughter through their verses.","context":["Posted: September 1, 2003\nA review of a 14-18: Understanding the Great War, by Stéphane Audoin-Rouzeau and Annette Becker, translated by Catherine Temerson\nhe short but tumultuous 20th century came to an end with the largely unanticipated implosion of the Soviet\nempire between 1989 and 1991. As a political phenomenon, the century began with the breakdown of the European diplomatic order in August 1914 and the unleashing of total war on the continent. \"The Great War,\" as it was called until 1939, was marked by unprecedented violence (nearly 10 million soldiers and civilians perished before it was all over) and by the display of prodigious national energies and civic passions. In its wake drifted the fragments of great multinational empires (most particularly the Russian, Austro-Hungarian, and Ottoman). The old bourgeois Christian order in Europe appeared in ruins and the future seemed to belong to audacious totalitarian revolutionaries of the Left and Right. A murderous Lenin was in power in Soviet Russia and Mussolini and Hitler were soon to follow in Italy and Germany. At its end, the initial euphoria of war gave way to a wave of pessimism and pacifism which weakened the democracies and paved the way for new \"wars in chain reaction,\" to use Raymond Aron's suggestive phrase.\nIn retrospect, the Great War was really the first dramatic phase in a protracted political conflict that lasted from 1914 until 1945 (the \"Thirty Years War\" as Churchill, de Gaulle, and Aron all called it). In its turn, this Thirty Years War created the conditions for Cold War and global conflict between the United States and the Soviet Union in the decades following 1945. The gunshot fired in Sarajevo in summer 1914 thus had unpredictable but world-transforming consequences. The European nation state never quite recovered from this self-inflicted wound. Totalitarians succeeded in seizing the mantle of \"progress\" from the once self-confident partisans of liberal civilization. But Europe's crisis became America's opportunity. On three separate occasions (1917, 1941, and again during the Cold War) America came to the rescue of old Europe and, as a result, definitively found its place in universal history. Under the comparatively benign protection of the U.S., the European democracies recovered their liberty, prosperity, and self-respect. One should not turn to 14-18 expecting to find much guidance for understanding the political stakes and consequences associated with the Great War. Nor will Americans find much discussion of America's involvement in the war or her crucial role in bringing it to an end. The perspective that dominates this work is decidedly \"metapolitical\" and \"Eurocentric.\" Its authors, directors of the Historial of the Great War at the Chateau de Peronne near the Somme, are interested in recovering a comprehensive anthropological perspective on the war that purportedly transcends narrowly political analysis. However flawed this approach may be, it doesn't prevent their book from being of real interest. To their credit, the authors of 14-18 appreciate the limits of much recent European reflection on the Great War. Carried away by a facile, morally smug pacifism, contemporary European commentators tend to see soldiers as mere victims\n(playthings of nationalist elites) and celebrate rebels and mutineers as the real heroes. Against this highly dubious interpretation of the war, the authors of 14-18 set out to explore a mystery largely opaque to contemporary humanitarian and pacifist sensibilities. Europeans had been mostly at peace since 1815 (or at least had avoided a continent-wide conflict) and had supreme confidence in the progressive character of their civilization. It was commonplace for statesmen and intellectuals alike to affirm the obsolescence of war due to the pacifying effects of commercial and scientific civilization. Yet when war broke out, Europe witnessed a remarkable eruption of patriotic passions. Millions volunteered for the armed forces (we forget that conscription wasn't even instituted in Great Britain until the beginning of 1916). Civilian populations willingly, even enthusiastically, sustained the greatest sacrifices. One of the merits of this book is its demonstration that these patriotic passions were not simply manufactured by governments, the results of crude propaganda or government induced hysteria. The authors convincingly show that these nationalist passions arose, at least initially, from below, and were imbued with a genuinely religious intensity. At the time, a full range of participants and commentators remarked on the spiritual commitment of peoples to the common good, to civic sacrifice and victory at war. Few in fact doubted the justice of their respective national causes. The Germans defended the superiority of their \"Kultur\" against the mechanistic and homogenizing \"civilization\" of the British and French, while the British and French defended ordered liberty and the rights of man against an enemy that had committed gruesome atrocities in Belgium and had violated international order.\n* * *\nOne might think that the length and terrible character of the conflict would undermine these sentiments. Raymond Aron has brilliantly shown how the \"technical surprise,\" the dominance of defense over offense after 1914, gave rise to unmeasured or \"hyperbolic\" war. What Victor Davis Hanson has called \"the Western way of war\" was definitively shattered. No longer did war consist of \"brief, brutal clashes\" that allowed courage and ingenuity to shine. Instead, battles went on for months at a time (Verdun lasted for 10!) with little progress on either side. Soldiers and civilians alike experienced soul-numbing horrors. Yet the majority's resolve persisted amid the horrific cruelties of all-out war. Despite some inevitable erosion of public support, most Europeans \"continued to believe that their initial feelings in the summer of 1914 were still justified\" long after the promise of rapid victory had been disappointed.\nThis gap between the subjective feelings of justification and the objective facts of carnage and mayhem is a great stumbling block for our authors. If Audoin-Rouzeau and Becker successfully resist seeing all of the actors in the drama as victims, they are less successful in resisting the tendency to see them as villains. Our authors are in the final analysis under the spell of a humanitarian ideology that makes it impossible for them to take seriously any idea of a just war. They find it impossible to distinguish between the just war and a millenarian, Manichean conception of politics that denies the enemy's humanity. They are self-described \"children of the West's disengagement from war.\" To their credit, they have the courage to think about war; this is something that clearly sets them apart from many of their European contemporaries. They are rightly concerned with the violence that sunders communities and causes real damage to men's souls. But because they eschew a properly political analysis of war they finally can see in it only \"sound and fury, signifying nothing.\"\nIn light of the evils of total war (which include the growing effacement of the distinction between civilians and combatants, the use of forced labor and internment camps, and the dehumanization of the enemy by all sides), it is understandable that some believe that modern war is no longer capable of serving larger moralpolitical purposes. The humanitarian wants to enlarge memory by reminding us of the untold victims who perished or saw their lives turned upside down as a result of war's furies. But this humanitarian perspective finally risks a nihilism all its own. Churchill and de Gaulle reminded us that the violence of the Great War was nihilistic to the extent that it had ceased to serve authentic political purposes. There was a deficit of statesmanship on all sides. Our authors rightly point out that, under conditions of total war, pressures build to deny the enemy's humanity. Even the most liberal Frenchmen and Britons didn't hesitate to attribute the defects of German politics to the perfidies of the German \"race.\" 14-18 eloquently highlights this escalation to extremes and traces the ways in which a great civilization came close to committing spiritual suicide. But one must also ask if the \"deconstruction,\" in the name of common humanity, of the sentiments and motives of the actors in the drama is indeed the final word on this question.\nThe sense of common humanity, after all, is not the only or strongest bond connecting men. Our authors ably describe the ways that French Catholics and French republicans united in defense of patrie against a German invader. This union sacrée as the French called it, bound men and women who had been at each other's throats\nright up until the war's outbreak, reconciling secular and religious France in a common endeavor to defend French and European liberty. Our authors, of course, highlight the excesses and inhumanities that sometimes accompanied this effort, but they are not really in a position to challenge its fundamental nobility. There is something sacred about the political bond as such and human beings rightly honor those who pay the ultimate price to maintain its integrity. Audoin-Rouzeau and Becker are perplexed by the patriotic and religious sensibility of the great French Catholic poet and philosopher Charles Péguy, who could write in 1913 on the eve of the Great War:\nBlessed are those who died for carnal earth\nProvided it was a just war...\nBlessed are those who died for carnal cities.\nFor they are the body of the city of God.\nBlessed are those who died for their hearth and their fire,\nAnd the lowly honors of their father's house.\nTo many of us the poet's words eloquently express the nobility of sacrifice and the sacredness of the civic bond. As it turns out, the great Péguy gave his life for the freedom of his beloved France at the battle of the Marne in the opening weeks of the war. For the humanitarian, Péguy's words are incomprehensible because they supposedly negate the only real bond that unites man as man. The humanitarian affirms a notion of the human universal that finally severs it from any political articulation, from every \"carnal\" embodiment.\nFrom the strictly humanitarian perspective, every national cause is equally suspect because it leads to the exclusion of the \"Other.\" The authors of this book are right to argue that World War I created an opening for European totalitarianism. But they are wrong to locate it narrowly in intense national passions. Totalitarianism arose in reaction to bourgeois Christian civilization and was in no way its logical culmination. To be sure, the Nazis subverted the idea of the nation by severing it from universal principles of political right and liberal and Christian civilization. But the nation can more truthfully be appreciated as a means of giving these universal principles real life, of embodying the demands of civilization in a concrete way. That surely was Péguy's deepest insight.\nIt won't do to condemn every particular loyalty as proto-totalitarian, as a step on the way to fascism. The true historian necessarily \"relativizes\" the claims of every nation, of every political actor. It is true that all peoples and nations are capable of injustice and none embodies the universal in an unqualified manner (but, as Lincoln and Reinhold Niebuhr have both shown, a recognition that each nation stands under the judgment of God in no way entails moral relativism). All of the sovereign states who participated in the Great War wanted to assert their power, glory, and independence. But all were not equally right, or wrong, to do so. As Raymond Aron has written:\nIn France and Britain there were no equivalents of the Pan-Germanists or the romantic theorists of violence: Both countries tended to be conservative and to relinquish old dreams of conquest. The Germany of Wilhelm II, in the midst of expansion, was more inclined to war and contemplated a recourse to arms with less reluctance than the bourgeois democracies.\nGermany thus had a special responsibility for the outbreak of the war. To say this is to not to side with nationalist passions but to recognize the crucial difference between conservative and revolutionary states that is essential to the maintenance of a civilized world order.\nToday, many Europeans dream of putting an end to the nation-state, of creating a transnational community that will put an end to politics, and war, once and for all. They are still traumatized by the \"wars in chain reaction\" that mutilated Europe and paved the way for the preeminence of American empire. Pierre Manent has eloquently suggested that the specter of \"depoliticization\" haunts a continent that wishes to escape the demands of universal history. This fascinating if deeply flawed book helps us better understand why Europeans, in the light of the bitter experience of total war, succumbed to that temptation.","WORLD WAR I POETRY. Summary of Events. The Start of the War\nWORLD WAR I POETRY\nWorld War I began on July 28, 1914, when Austria Hungary declared war on Serbia. This seemingly small conflict between two countries spread rapidly: soon, Germany, Russia, Great Britain, and France were all drawn into the war, largely\nbecause they were involved\nin treaties that obligated them\nto defend certain other nations.\nWestern and eastern fronts\nquickly opened along the borders\nof Germany and Austria-Hungary.\nThe first month of combat consisted of bold attacks and rapid troop movements on both fronts. In the west, Germany attacked first Belgium and then France. In the east, Russia attacked both Germany and Austria-Hungary. In the south, Austria-Hungary attacked Serbia. Following the Battle of the Marne (September 5–9, 1914), the\nwestern front became\nentrenched in central France\nand remained that way for the\nrest of the war. The fronts in\nthe east also gradually locked\nThe middle part of the war,\n1916 and 1917, was\ndominated by continued\ntrench warfare in both the east and the west. Soldiers fought from dug-in positions, striking at each other with machine guns, heavy artillery, and chemical weapons. Though soldiers died by the millions in brutal conditions, neither side had any substantive success or gained any advantage.\nDespite the stalemate on both fronts in Europe, two important developments in the war occurred in 1917. In early April, the United States, angered by attacks upon its ships in the Atlantic, declared war on Germany. Then, in November,\nthe Bolshevik Revolution prompted\nRussia to pull out of the war.\nAlthough both sides launched renewed offensives in 1918 in an all-or-nothing effort to win the war, both efforts failed. The fighting between exhausted, demoralized troops continued to plod along until the Germans lost a number of individual battles and very gradually began to fall back. A deadly outbreak of influenza, meanwhile, took heavy tolls on soldiers of both sides. Eventually, the governments of\nboth Germany and Austria-Hungary\nbegan to lose control as both countries\nexperienced multiple mutinies from\nwithin their military structures.\nThe war ended in the late fall of 1918, after the member countries of the Central Powers signed armistice agreements one by one. Germany was the last, signing its armistice on November 11, 1918. As a result of these agreements, Austria-Hungary was broken up into several smaller countries. Germany, under the Treaty\nof Versailles, was severely punished\nwith hefty economic reparations,\nterritorial losses, and strict limits on\nits rights to develop militarily.\nThe First World War runs through the British modern-day psyche like no other conflict. On Remembrance Day Sunday thoughts (of those who have not fought) turn to the fields in Flanders and the slaughter of the Somme and Passchendaele more readily than Dunkirk, El Alamein, or Arnhem (unless, of course, the date is an anniversary of a specific battle).\nIt has been described as Britain's 'Vietnam', where the true horror of War touched everyone and everything in the country, breaking through the class barrier and irreversibly altering the social structure of the nation. It also closely parallels Vietnam as it represents an overwhelming feeling of futility, in that so many lives were wasted for such little gain. Unlike the Second World War, which more easily falls into the 'just war' definition of right versus wrong, the First World War appears as a conflict with aims that were quickly lost, degenerating to a war of attrition in unbelievable conditions.\nMartin Stephen in The Price of Pity (1996) summarises the horror of the conflict as follows:\n\"The European powers were mighty in their strength and wealth. They were neither wholly good nor wholly bad, and were brought to near- destruction by powers of ambition, greed and aggression that had always been there but which had never before led to destruction on such a scale. The war evoked pity and terror like no other, and when peace was declared there was an almost animal venting of emotion in the streets of Britain. It unleashed untold suffering on Europe, a suffering that went out of the control of any human agency and which toppled some monarchies and shook other\nnations to their roots. And of course, when it was\nall over, the world had been made safe, and the\nwar to end all wars had been fought.\" (p. 236)\nMoreover, the War was dehumanising. It brought home how quickly and easily mankind could be reduced to a state lower than animals. Pat Barker, in her novel Regeneration (1992), reflects on the War's terrible reversal of expectations:\n\"The Great Adventure. They'd been mobilized into holes in the ground so constricted they could hardly move. And the Great Adventure (the real life equivalent of all the adventure stories they'd devoured as boys) consisted of crouching in a dugout, waiting to be killed.\nThe war that had promised so much in the\nway of 'manly' activity had actually delivered\n'feminine' passivity, and on a scale that their\nmothers and sisters had scarcely known.\"\nThe First World War provides one of the seminal moments of the twentieth-century in which literate soldiers, plunged into inhuman conditions, reacted to their surroundings in poems reflecting Wordsworth's 'spontaneous overflow of powerful feelings'.\nStephen (1996) states that 'no school of\nverse has ever been linked more clearly\nto a historical event' and that 'Society's\nvision of this historical event...was ironically\ndetermined by a literary response to it, and\nit is the vision of some of the war's poets\nthat has dominated the popular image of\nwhat that war was to those who fought in it\nand lived through it.'.\nBrooke's entire reputation as a war poet rests on only 5 \"war sonnets\". Brooke's war experience consisted of one day of limited military action with the Hood Battalion during the evacuation of Antwerp. Consequently, his \"war sonnets\" swell with naive sentiments of the most general kind on the themes of maturity, purpose and romantic death – the kind of sentiments held by many (but not all) young Englishmen at the outbreak of the war. Brooke's \"war sonnets\" are really more a declaration occasioned by the ups and downs of his tumultuous personal life than a call to war for his generation.\nBrooke's poetry gives us a glimpse of a golden era in England just before the First World War. To be more precise, it was a golden time only for the upper classes, who enjoyed the fruits of Britain's imperial dominance: public school education, guaranteed employment (if they desired it) and access to the rich and powerful members of society. The gap between rich and poor was wide during this period, and unrest was beginning to grow among the lower classes. With hindsight it seems obvious that this state of affairs could not last forever. The war gave a huge shock to the system and, despite the terrible human cost, led eventually to a more equal society, not least because the poorer classes were largely the ones dying in the trenches as a result of orders issued by untrained, aristocratic generals living miles behind the lines. Brooke's generation was the last to enjoy such an unchallenged position of privilege.\nHis early poetry was classically inspired, with death as its most frequent theme throughout. Later, he wrote more from his personal experience gained in the South Seas and later in his brief military career. The shortness of his life added to his reputation, especially at a time when so many young men were being killed. Amongst his works were five War Sonnets, a sixth sonnet – The Treasure – and The Old Vicarage, Grantchester. Winston Churchill wrote his obituary in The Times of April 26, 1915, saying\n\"he advanced to the brink ... with absolute conviction of the rightness of his\nBrooke's friends complained that the heroic myth of Brooke's patriotic self-sacrifice was deliberately exaggerated to encourage more young men to enlist. Since Brooke's death, the name Rupert has been used as a term of mockery for any young Army officer with a public school education. Generations of school children would be taught the opening patriotic lines from ‘The Soldier’:\n\"If I should die, think only this of me:\nThat there's some corner of a foreign field\nThat is for ever England.\"\nThe patriotic poems of Brooke are often compared to the anti-war\npoems of Siegfried Sassoon who, ironically, spent the majority of\nthe war in active service, yet survived.\nNow; God be thanked Who has matched us with His hour,\nAnd caught our youth, and wakened us from sleeping,\nWith hand made sure, clear eye , and sharpened power,\nTo turn, as swimmers into cleanness leaping,\nGlad from a world grown old and cold and weary,\nLeave the sick hearts that honour cold not move,\nAnd half-men, and their dirty songs and dreary,\nAnd all the little emptiness of love!\nOh! We, who have known shame, we have found release there,\nWhere there’s no ill, no grief, but sleep has mending,\nNaught broken save this body, lost but breath;\nNothing to shake the laughing heart’s long peace there\nBut only agony, and that has ending;\nAnd the worst friend and enemy is but Death.\nIf I should die, think only this of me:That there's some corner of a foreign field\nThat is for ever England. There shall beIn that rich earth a richer dust concealed;\nA dust whom England bore, shaped, made aware,Gave, once, her flowers to love, her ways to roam,\nA body of England's, breathing English air,Washed by the rivers, blest by suns of home.\nAnd think, this heart, all evil shed away,A pulse in the eternal mind, no lessGives somewhere back the thoughts by England given;\nHer sights and sounds; dreams happy as her day;And laughter, learnt of friends; and gentleness,In hearts at peace, under an English heaven.\nwas educated at Marlborough and Cambridge where he\nstudied both Law and History before leaving without taking\na degree. After Cambridge, Sassoon lived the life of a\nsportsman, hunting, riding point-to-point races and playing cricket until the outbreak of the War. Although Sassoon wrote poetry before the War he was no more than a minor Georgian poet.\nsent home from France in late July after an attack of\ntrench fever (or enteritis). From Oxford's Somerville\nCollege, Sassoon was sent home to Weirleigh for\nconvalescence. He reported to the Regimental Depot\nin Liverpool in December 1916, and returned to France\nin February 1917.\nWelch Fusilier, Robert Graves, intervened,\npulled strings with the authorities and managed\nto persuade them to have Sassoon medically\nboarded (or referred), with the result that in\nJuly 1917 he was sent to Craiglockhart War\nHospital, Edinburgh officially suffering from\nDr. W.H.R. Rivers and eventually realised that his\nprotest had achieved nothing, except to keep him\naway from his men; his decision to apply for\nGeneral Service seems to have been based on his\nperceived responsibilities at the front.\nhe was placed on indefinite sick\nleave until after the Armistice,\neventually retiring officially from\nthe Army in March 1919.\n“Good-morning; good-morning!” the General said\nWhen we met him last week on our way to the line.\nNow the soldiers he smiled at are most of ’em dead,\nAnd we’re cursing his staff for incompetent swine.\n“He’s a cheery old card,” grunted Harry to Jack\nAs they slogged up to Arras with rifle and pack.\n* * * * *\nBut he did for them both by his plan of attack.\nDoes it matter? – losing your legs?...For people will always be kind,And you need not show that you mindWhen the others come in after huntingTo gobble their muffins and eggs.\nDoes it matter ? – losing your sight?...There's such splendid work for the blind;And people will always be kind,As you sit on the terrace rememberingAnd turning your face to the light.\nDo they matter? – those dreams from the pit?...You can drink and forget and be glad,And people won't say that you're mad;For they'll know you've fought for your countryAnd no one will worry a bit.\ninnocent world, a world that still associated warfare\nwith glorious cavalry charges and the noble pursuit\nof heroic ideals. This was the world's first experience\nof modern mechanised warfare. As the months and\nyears passed, each bringing increasing slaughter and misery, the soldiers became increasingly disillusioned. Many of the strongest protests made against the war were made through the medium of poetry by young men horrified by what they saw. One of these poets was Wilfred Owen.\none of the worst in memory, he\nled his platoon into the Battle of\nthe Somme. he wrote to his\nmother every week and described\nwhat he had been through: \"Those\nfifty hours were the agony of my\nhappy life... I nearly broke down\nand let myself drown in the water\nthat was now rising slowly above\nmy knees. In the Platoon on my left,\nthe sentries over the dug-out were\nblown to nothing\".\nreport to the Battalion Medical Officer who\nfound him to be shaky and with a confused\nmemory. He was eventually diagnosed as\nhaving neurasthenia (shell shock) and was\ninvalided back to England and then to\nCraiglockhart War hospital near Edinburgh.\nstress with which the soldiers lived –\nthe mud, rats, barbed wire, lice, fleas,\ncorpses, blood and constant shelling.\nHe also gave graphic descriptions of\nthe effects of poison gas.\nAnthem For Doomed Youth\nWhat passing-bells for these who die as cattle?\nOnly the monstrous anger of the guns.\nOnly the stuttering rifles’ rapid rattle\nCan patter out their hasty orisons.\nNo mockeries for them; no prayers nor bells,\nNor any voice of mourning save the choirs,—\nThe shrill, demented choirs of wailing shells;\nAnd bugles calling for them from sad shires.\nWhat candles may be held to speed them all?\nNot in the hands of boys, but in their eyes\nShall shine the holy glimmers of goodbyes.\nThe pallor of girls’ brows shall be their pall;\nTheir flowers the tenderness of patient minds,\nAnd each slow dusk a drawing-down of blinds.\nDulce Et Decorum Est\nBent double, like old beggars under sacks,\nKnock-kneed, coughing like hags, we cursed through sludge,\nTill on the haunting flares we turned our backs\nAnd towards our distant rest began to trudge.\nMen marched asleep. Many had lost their boots\nBut limped on, blood-shod. All went lame; all blind;\nDrunk with fatigue; deaf even to the hoots\nOf tired, outstripped Five-Nines that dropped behind.\nGas! GAS! Quick, boys! – An ecstasy of fumbling\nFitting the clumsy helmets just in time;\nBut someone still was yelling out and stumbling,\nAnd flound’ring like a man in fire or lime…\nDim, through the misty panes and thick green light,\nAs under a green sea, I saw him drowning.\nIn all my dreams, before my helpless sight,\nHe plunges at me, guttering, choking, drowning.\nIf in some smothering dreams you too could pace\nBehind the wagon that we flung him in,\nAnd watch the white eyes writhing in his face,\nHis hanging face, like a devil’s sick of sin;\nIf you could hear, at every jolt, the blood\nCome gargling from the froth-corrupted lungs\nObscene as cancer, bitter as the cud\nOf vile, incurable sores on innocent tongues, -\nMy friend, you would not tell with such high zest\nTo children ardent for some desperate glory,\nThe old Lie: Dulce et decorum est\nPro patria mori."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:069d2b22-b5c5-4752-a550-ed86df43432c>","<urn:uuid:25290d8c-4817-4697-b6b3-941dde15cad3>"],"error":null}
{"question":"How do Michael Seitz's work on electric drone systems at AeroVironment and Bill Gates' foundation's sanitation technology initiatives compare in terms of their approach to technological innovation?","answer":"Both involve innovative technological solutions but in different domains. Seitz works on electric power systems and autonomy for drones at AeroVironment, designing avionics circuitry for aircraft like the solar-powered Hawk 30 that can operate at 65,000 feet for months. Gates, through his foundation, has focused on innovations in sanitation technology, including funding research into fecal sludge treatment methods and the Reinvent the Toilet Challenge, even publicly demonstrating the effectiveness of water cleaning technology by drinking treated water himself.","context":["Designing avionics circuitry for drones\nBy Debra Werner|October 2019\nMichael Seitz, 26, electrical engineer at AeroVironment\nGrowing up in Wooster, Ohio, Michael Seitz kept a box of parts from appliances and devices he dismantled. Although he never managed to reassemble the pieces into new contraptions, he enjoyed trying to fit them together. Now, Seitz designs and tests avionics circuitry for AeroVironment, the Monrovia, California, company that builds the Raven, Wasp and Puma fixed-wing drones for the U.S. military, and the Hawk 30, a solar-powered unmanned flying wing designed to operate for months at altitudes of 65,000 feet.\nLanding a job\nI picked electrical engineering as a major because it seemed to have a broad range of opportunities in different industries, and it was something I had an interest in but hadn’t found any good resources to teach myself. While at the University of Southern California, I volunteered in a 3D-printing lab, which gave me some physical exercise of the theory I was learning in class. AeroVironment usually had a booth at the Viterbi Career & Internship Expo, and I thought their displays and pamphlets were some of the coolest ones around. The concept of autonomous aircraft was really exciting to me. After speaking with them every semester for four years, I got a call asking me to interview for an associate electrical engineering position.\nFrom desk to flight tests\nI’ve been here a little over four years now. I’ve been able to cover a lot of aspects of this design process, from prototyping and printed circuit board design to testing and productionizing a variety of systems, including flight controls, battery management systems and ground station communication. There have been plenty of other times when I was out in the field assisting in flight tests or in the workshop building test fixtures. There’s a good amount of variety in what I get to do. The balance between desk and field work and the fast-paced environment definitely keeps it interesting.\nAviation in 2050\nElectric power systems and autonomy will play a huge role. This not only allows for a decreased reliance upon fossil fuels, but also a possibility for longer endurance, as we have seen from AeroVironment’s previous solar-powered designs, since there is no added weight from a pilot and the aircraft refuels in the air. This opens up a lot of possibilities in the commercial marketplace. The skies will inevitably be more accessible to a much larger portion of the population. The growing feasibility of electric aircraft and the increased autonomy in control systems will make for greater reliability and decrease the potential for human error. While a good understanding of aerodynamic principles and aviation practices will still be important, years of pilot experience will not be necessary in order to get off the ground. Regulation will certainly have to play a larger role in this development in order to ensure the safety of these future systems, but I see the somewhat recent explosion of hobby and commercial drone usage to be an example of what we have to expect going forward.","On this page:\nWilliam “Bill” Gates\nWilliam “Bill” Gates (born 1955) is a United States entrepreneur, computer programmer, software developer, investor, and philanthropist. His is best known as the principal founder of the Microsoft Corporation.\nIn April 2019, his net worth was above $100 billion. From 1995 to 2017, he held the Forbes title of Richest Person in the World every year except four. (Forbes only lists people whose wealth can be estimate with reasonable accuracy.)\nBill Gates and Paul Allen launched Microsoft in 1975. It eventually became the largest PC software company in the world, and a driving force behind the personal computer revolution.\nShort facts about Bill Gates\nName: William Henry Gate III\nBorn: 28 October, 1955, in Seattle, Washington state, USA\nResidence: Xanadu 2.0, Medina, Washington state, USA\nSpouse: Melinda Ann Gates née French (married 1994)\nBill and Melinda Gates have announced that they plan to donate 95% of their wealth to philantrophy. They intended for their three children to “only” inherit $10 million each.\nBill Gates credits the philanthropic work of David Rockefeller as a major inspiration for him, and his charity work is in part modelled after the Rockefeller family’s focus on tackling global problems that aren’t given sufficient attention by governments and other organizations.\nThe Bill and Melinda Gates Foundation\nBill Gates created the William H. Gates Foundation in 1994. In 2000, Bill and his wife Melinda combined three family foundations to create the Bill & Melinda Gates Foundation, and Gates donated stock valued at $5 billion to it. By 2013, it was the world’s wealthiest charitable foundation, with assets valued at more than $34 billion.\nThe foundation is organized into four main areas:\n- Global Development\n- Global Health\n- United States\n- Global Policy & Advocacy\nUnlike many other foundations, the Bill & Melinda Gates Foundation makes it easy for benefactors to access information about how the fund’s money is being spent.\nThe Giving Pledge\nAs a co-founder of The Giving Pledge, Gates was one of the first people to sing it. He signed it on 9 December, 2010, the same day as Warren Buffett and Mark Zuckerberg.\nExamples of causes and projects supported by Bill Gates\n- Gates donated $20 million to the creation of a computer laboratory named The William H. Gates Building at Massachusetts Institute of Technology (MIT).\n- Gates and Microsoft President Steven A. Ballmer both donated to the Maxwell Dworkin Laboratory at Harvard University, and it’s named after both their mothers.\n- Gates donated $6 million to the creation of the Gates Computer Science Building at Standford University.\n- Through his foundation, Gates donated $20 million to the creation of The Gates Center for Computer Science at Carnegie Mellon University.\n- The Bill and Melinda Gates Foundation supports the International Rice Research Institute’s work with Golden Rice, a genetically modified rice used to combat vitamin A deficiency.\n- The Bill and Melinda Gates Foundation launched the Reinvent the Toilet Challenge.\n- Since 2012, the Bill and Melinda Gates Foundation has funded research into methods and technologies to treat fecal sludge. In 2014, Gates publicly drank water cleaned by one of these methods, to raise awarness of the topic of sanitation. In 2015, he challenged Jimmy Fallon to see if he could taste the difference between traditional bottled water and Gate’s reclaimed water.\n- In 2017, Gates pledged $50 million to the Dementia Discovery Fund to combat Alzheimer’s disease, and he also promised to donate an additional $50 to start-up ventures working in Alzheimer’s research.\n- In 2017, Gates played tennis with Roger Federer, John Isner and Pearl Jam guitarist Mike McCready to raise money for the Roger Federer Foundation’s charity work in Africa. Overall, $2 million were raised. A new tennis match the following year – this time with Gates, Federer, Jacks Sock, and Savannah Guthrie – raised an additional $2.5 million.\n- In 2018, the Bill and Melinda Gates Foundation distributed $600,000 through UNICEF to flood victims in Kerala, India.\n- Gates supports Rotary International’s work to eliminate polio."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:d763efd6-658e-4098-9661-6798baf1e783>","<urn:uuid:85d6137b-78da-4eab-babc-bc88c36fa638>"],"error":null}
{"question":"What are the technical benefits of CO2-EOR for oil extraction, and what are the current barriers limiting its widespread implementation in carbon capture projects?","answer":"CO2-EOR helps mobilize remaining oil in reservoirs by injecting CO2, making it the only large-scale CO2 use with positive economic value. Studies show it can be profitable in the North Sea with oil prices down to 50 USD/bbl. However, there are significant barriers to implementation. The main barrier is the high cost of the CCS chain, particularly the CO2 capture process - in petrochemical plants, it almost doubles the capital expenditures. Additional challenges include regulatory issues, with insufficient incentives and subsidies, and the need for accurate assessment of reservoir capacity, injectivity, and containment ranking. Current CCUS projects account for less than 1% of the target for carbon capture, showing deployment is lagging behind what's needed to meet climate goals.","context":["- Alv-Arne Grimstad\n- Senior Research Scientist\n- 470 35 566\n- SINTEF AS\nReservoir management and EOR (Task 11)\nBy pumping CO2 into oil reservoirs and storing it there, we can extract more oil. This technique is called enhanced oil recovery (EOR). But, the cost of CCS is still too high, meaning the process capturing the CO2 one intends to pump into the reservoirs is too expensive. Therefore, reducing net cost of the overall CCS chain is the main barrier addressed in this task. Good reservoir management is a huge part of that, as it will be imperative to minimize storage-related costs.\nCO2 enhanced oil recovery, or CO2-EOR, is the process of injecting CO2 into oil reservoirs to mobilise some of the remaining oil. CO2-EOR has thus far been the only large-scale use of CO2 where CO2 has a positive economic value. Several studies have shown that large-scale CO2-EOR in the North Sea can be profitable for oil prices down to 50 USD/bbl. Industry relevance is therefore evident.\nThe activity on CO2-EOR in NCCS is mainly relevant for Deployment Case 2 (Link). In this scenario an infrastructure transporting CO2 from European sources to the North Sea is in place and sufficiently large amounts of CO2 are readily available for use in EOR operations. However, CO2-EOR may also be relevant for an extended phase of the first deployment case (DC1), where the amount of captured CO2 is increased beyond 1.25 Mt/year.\nThe net cost of the overall CCS chain is the main deployment barrier addressed. This barrier can be reduced or overcome by providing value through production of additional oil.\nOptimal use of a storage site and thereby reduction of cost per tonne CO2 stored is an important issue, as the main deployment barrier of CCS is cost. The first few injection operations will likely focus on demonstration of feasibility and safety. However, active management and optimisation of the available storage capacity will become important when the injection rate increases beyond a few Mt/year for a single storage site. Good reservoir management will be imperative in efforts to minimize storage-related costs.\n- Laboratory testing of foam-generating properties of synthetized nanomaterials. Results for first batch mainly negative. Design directions for next batch discussed.\n- Synthetized next batch of nanomaterials for CO2/brine foam generation\n- Working version of MRST CO2-foam module. Simulations with Eclipse and with MRST presented in GHGT-14 publication. Demonstrate >100% increase in CO2 storage efficiency for five-spot CO2-injection/brine-extraction patterns.\n- Initial work to optimize cost/benefit for mobility control in CO2 storage.\n- Development of a storage site optimization work flow.\nThe mobility contrast between CO2 and oil/water, and the large well distance, make tertiary CO2 injection more challenging as an EOR option in the North Sea than in North America, where it is already being successfully employed.\nThe task investigates novel methods for controlling the mobility of injected CO2, such as functionalized nanomaterials for foam generation or direct CO2 thickeners.\nFollowing a review of recent literature, the first series of newly designed POSS (polyhedral oligomeric silsequioxanes) nanomaterials was synthesized.\nTesting of CO2 solubility and other properties will commence in 2018, to give input on further generations of nanomaterials. Mobility control of injected CO2 can also be beneficial for aquifer storage, since it could postpone the point in time when CO2 reaches spill points in structural traps. Initial modelling to investigate this effect has been performed.","In the world of CO2 capture, utilization, and storage (CCUS) technology, utilization is the driving factor.\nThree experts discussed the motivations, limitations, and challenges of CCUS at a dinner held by the Society of Petroleum Engineers (SPE) CCUS Technical Section at SPE’s annual meeting at the beginning of October.\nModerated by George Koperna, chairman of the SPE CCUS Technical Section and vice president with Advanced Resources International, the panel comprised Concetto Fischetti, engineering director with the Oil and Gas Climate Initiative (OGCI), which sponsored the dinner; Chuck McConnell, executive director of the Energy and Environment Initiative at Rice University; and Vanessa Nuñez-López, research scientist associate with the Bureau of Economic Geology at The University of Texas at Austin.\nMcConnell highlighted the importance of carbon use for enhanced oil recovery (EOR). “One of the aspirations I have in my life is that we’re going to finally get though one of these conferences where nobody says CCS [carbon capture and storage]—not one time,” McConnell said at the beginning of his talk. “They say CCUS every time.”\n“If you don’t get that you need to do EOR,” he continued, “you don’t get anything about this technology. And I think that’s from a commercial standpoint and fundamental standpoint about the technology and the application of it.”\nNuñez-López agreed. “Without a doubt, enhanced oil recovery has been the main driver of CCS. It is really channeling the development of the technology globally.”\nMeeting goals to mitigate climate change, such as those laid out by the recent United Nations Paris Climate Agreement, is another driving force behind the technology.\n“Carbon capture and storage technologies have been recognized as being an essential part of the climate mitigation portfolio,” Nuñez-López said. She said she is working on a project funded by the US Department of Energy, “and I’m looking at the carbon balance of EOR, and I am obtaining results that show that, being integrated, CCUS projects can be actually engineered so that the oil produced through EOR can be net carbon negative. Maybe not throughout the entire project but through like half of the project.”\nShe lamented, however, the recent decline in CCUS efforts. “Just recently, the Global CCS Institute identified 38 large-scale, integrated CCUS projects, which is impressive. But, there were 65 just 5 years ago. So, I think we all agree that the deployment of the technology has been lagging behind where it really needs to be if we want to be able to meet the Paris Agreement and meet the goals of sustainable energy.”\nFischetti pointed out that current CCUS projects account for less than 1% of the target for carbon capture. “That is why OGCI is looking toward a megaproject,” he said, adding that three barriers exist to creating a CCUS megaproject.\nThe first barrier is high cost. Capturing carbon can be expensive. “In a petrochemical plant, you are almost doubling the CAPEX [capital expenditures],” he said.\nFischetti said the second barrier is related to regulations, which he said do not incentivize or subsidize, although that is changing. “In the states, there is some movement,” he said, “but, in Europe, definitely; in the UK, definitely.”\nThe third barrier Fischetti mentioned involves accuracy of the carbon storage. “What we have is just the need to define the capacity, the injectivity, and the containment ranking of the reservoir,” he said. “We know that we are very concerned not from the integrity point of view but rather from the accuracy point of view.”\nMcConnell had some advice for those working on CCUS who may be asked about the integrity of carbon-storage reservoirs. “Tell them it’s safe and it won’t leak. It’s that simple.”\nRead more about the CCUS Technical Section here.\nDon't miss our latest HSE content, delivered to your inbox twice monthly. Sign up for the HSE Now newsletter. If you are not logged in, you will receive a confirmation email that you will need to click on to confirm you want to receive the newsletter.\n3 Oct 2019\n- Calgary, Alberta, Canada\nBeing Human - reserve your place at this one-day course\n3 - 4 Oct 2019\n- Calgary, Alberta, Canada\nFor a better understanding of geomechanical factors, attend this course\n1 Nov 2019\n- Bali, Indonesia\nRegistration Coming Soon\n9 - 11 Nov 2019\n- Abu Dhabi, UAE\nThe programme combines expert input, case studies, and immersive scenarios from the E&P and other industries to embed your learning and enable you to progress to the next level of your career.\n10 Nov 2019\n- Abu Dhabi, United Arab Emirates\nSafety Leadership focuses on the ‘Human Factors’ (HF) which complement technical training to optimize reliability, safety, compliance, efficiency and risks within a team-based environment.\nThis course will help you develop a better understanding of factors that could impact your daily economic decisions as well as establish a new set of applicable tools to use in your professional career.\n11 - 12 Sep 2019\n- Abu Dhabi, UAE\nReserve your place\nHSE Now is a source for news and technical information affecting the health, safety, security, environment, and social responsibility discipline of the upstream oil and gas industry.\n©2003-2019 Society of Petroleum Engineers, All Rights Reserved."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:fa6c9184-9648-49bd-80da-fc707f3fd050>","<urn:uuid:c25ac6d6-998e-4771-8c5a-8e0da25763d3>"],"error":null}
{"question":"What are the key differences between how visual expertise develops in face recognition versus emotional processing, and how do these systems interact?","answer":"Visual expertise and emotional processing develop through distinct but interacting systems. For visual recognition, whether of faces or other objects of expertise, people develop a holistic approach through the fusiform face area (FFA) that enables extremely fast recognition. This expertise can be developed for various categories - from faces to cars to X-rays - and people who are better at learning one type of recognition tend to be better at others. In contrast, emotional processing involves two distinct routes: a rapid pathway from the thalamus to the amygdala for quick emotional responses, and a slower cortical route for detailed analysis. These systems interact through brain structures like the orbitofrontal cortex, which helps integrate emotional and cognitive processing, allowing for the modulation of both cognition and behavior through feelings, which serve as fundamental values in categorizing and responding to stimuli.","context":["Newswise — NASHVILLE, Tenn. – When people – and monkeys – look at faces, a special part of their brain that is about the size of a blueberry “lights up.” Now, the most detailed brain-mapping study of the area yet conducted has confirmed that it isn’t limited to processing faces, as some experts have maintained, but instead serves as a general center of expertise for visual recognition.\nNeuroscientists previously established that this region, which is called the fusiform face area (FFA) and is located in the temporal lobe, is responsible for a particularly effective form of visual recognition. But there has been an ongoing debate about whether this area is hard-wired to recognize faces because of their importance to us or if it is a more general mechanism that allows us to rapidly recognize objects that we work with extensively.\nIn the new study published this week in the online early edition of the Proceedings of the National Academy of Sciences, a team of Vanderbilt researchers report that they have recorded the activity in the FFAs of a group of automobile aficionados at extremely high resolution using one the most powerful MRI scanners available for human use and found no evidence that there is a special area devoted exclusively to facial recognition. Instead, they found that the FFA of the auto experts was filled with small, interspersed patches that respond strongly to photos of faces and autos both.\n“We can’t say that the same groups of neurons process both facial images and objects of expertise, but we have now mapped the area in enough detail to rule out the possibility of an area exclusively devoted to facial recognition,” said Rankin McGugin, who conducted this research as part of her doctoral dissertation.\nAccording to Isabel Gauthier, the David K. Wilson Chair of Psychology, who directed the study, the demonstration that the FFA can support expertise for other categories may help scientists improve treatments for people who have difficulty recognizing faces, like individuals with autism. In addition, identifying the neural basis of individual differences in learning visual skills is an important step toward mapping the brain chemistry involved in learning may eventually lead to the development of drugs that make it easier for individuals to acquire different kinds of visual expertise.\nFor most objects, research has shown that people use a piecemeal identification scheme that focuses on parts of the object. By contrast, experts, for faces or for cars, use a more holistic approach that is extremely fast and improves their performance in recognition tasks. The scientists point out that visual expertise may be more the norm than the exception: “It helps the doctor reading X-rays, the judge looking at show dogs, the person learning to identify birds or to play chess; it even helped us when we learned brain anatomy!” Gauthier said.\nGauthier and her colleagues have further found that people who are better at learning to recognize one subject should also be better at learning another. Recent work by her group found that those who did a better job at identifying objects in which they were most interested were also better at identifying faces.\nChristopher Gatenby at the University of Washington and John Gore, director of Vanderbilt’s Institute of Imaging Science, also contributed to the study. The research was supported by the James S. McDonnell Foundation, National Science Foundation grant SBE-0542013, National Eye Institute grant EY013441-06A2 and the Vanderbilt Vision Research Center.\nVisit Research News @ Vanderbilt for more research news from Vanderbilt. [Media Note: Vanderbilt has a 24/7 TV and radio studio with a dedicated fiber optic line and ISDN line. Use of the TV studio with Vanderbilt experts is free, except for reserving fiber time.]-VU-","What affective neuroscience means for science of consciousness by Leonardo Almada, Alfredo Pereira & Claudia Carrara-Augustenbourg, Universities of Uberlandia, Sao Paolo and Copenhagen,\nMens Sana Monographs, 2013 Jan-Dec. 11 (1) 253-273, doi: 10.4103/0973-1229.100409\nSummary and review of the above paper\nINTRODUCTION: The field of affective neuroscience, initiated by Panksepp and Ledoux, embraces the concept that emotional processes are based on brain structures that operate in parallel with cognitive processes, and can thus influence behaviour independently of cognition.\nA 20th century failure\nEmotional and Cognitive Processing: The authors envisage two interacting brain circuits, the first involving cognitive processes, and the second feelings about cognition. Most of 20th century thinking on cognition was hampered by its lack of emphasis on emotion-related brain functions, and its over-emphasis on computational operations, learning and memory.\nA new approach\nThe new approach to emotions posits the activity of particular interacting brain structures. The authors refer to this as ‘core affect’; This is defined as neural changes determined by the detecting and processing of internal and external stimuli. Feelings are defined as the subjective outcome of these stimuli, notably in the form of experienced pleasure and pain, with affective states defined as experienced happiness and sadness.\nFeelings are viewed as fundamental values for the modulation of both cognition and behaviour, and are thus seen as playing a key role in the causal chain of events. Internal feelings allow categorisation of signals from the external environment, which is subsequently adaptative for guiding behaviour in this environment. The authors have developed what they call the Endogneous Feedback Network (EFN) to account for the processing of emotions and cognition.\nAllocation of attention towards specific segments of the brain’s information flow is suggested to be related to such conscious feelings. Feelings are viewed as mental short cuts linking the assessment of stimuli to particular responses or meanings, prior to the more time consuming process of cognitive assessments. Emotions also effect the attentional resources given to particular stimuli, with the subjective experience of the stimuli suggested to play a role in the selection of stimuli for attention.\nTwo brain routes\nLe Doux outlined the concept of two routes in the brain. A quick route from the thalamus to the amygdala, by-passing the processing of the cortex, and striking straight at the amygdala, to generate a rapid response. The amygdala is particularly involved in the processing of fear, anxiety and negative emotion. The second route goes via the cortex and can take advantage of the analysis of the stimuli. An initial negative response may thus arise in the amygdala without involving conscious perception of particular external stimuli. However, the subsequent arrival of such conscious perception via the slower cortical route may disperse the initial negative or fear response.\nFrontal brain structure\nThe induction of emotional states has been shown to correlate with activity in the lateral prefrontal cortex (Braver, Cohen and Barch, 2002), while other studies have shown the shown the role of the orbitofrontal cortex in emotion-related learning. The lateral and medial orbitofrontal cortex are involved with reward and punishment. The lateral orbitofrontal is associated with aversive outcomes, and the medial orbitofrontal is associated with reward (O’Doherty et al, 2001). The orbitofrontal in particular is seen to be essential to changing what has been previously learnt, in response to new experience.\nEmotion and cognition\nBeer et al (2006) argues that the orbitofrontal has a significant role in the interaction between emotion and cognition. Some emotions are seen as having selective effects on cognition, and on subsequent goal-directed behaviour (Gray 2001). Empathy, belief, moral reasoning and decision making are seen as based on a melding of affective and cognitive processing. Emotions and conscious feelings are viewed as playing a role in stimuli-response integration.Tags: affext, cognitive processing, consciousness, emotional processing, feelings Posted by"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:a5f05983-c32e-478e-831c-d35e9c40ae41>","<urn:uuid:af32330b-47a9-4e05-bf6e-523469f00622>"],"error":null}
{"question":"I'm a bird watcher looking to spot both eagle owls and bald eagles - which location would be better between Sipoonkorpi Nature Reserve and Eagle Lake?","answer":"Both locations offer good bird watching opportunities, but they host different species. Sipoonkorpi Nature Reserve is home to the European eagle owl, which is the largest owl species in Europe, while Eagle Lake is home to bald eagles and osprey. The eagle owl nests in rocky forests and builds its nest on the ground, often on precipices or at the foot of trees. Bald eagles at Eagle Lake can be spotted along with over 90 other bird species in the area. The choice depends on which specific species you want to observe.","context":["The last wilderness of the Capital Region of Helsinki\nSipoonkorpi Nature Reserve is located within the municipalities of Sipoo, Vantaa and Helsinki. It is one of the most significant forest areas with no buildings near the Helsinki metropolitan area, the other one being the Nuuksio lake highland. Sipoonkorpi plays an extremely important role in the conservation of biodiversity in forests. It is a Natura 2000 site (www.ymparisto.fi, in Finnish).\nSipoonkorpi is made up of many different forest types, mires and cultural landscapes. There are only a few lakes in the area. The terrain is diverse and at some points there are large changes in altitude. There are many open cliffs. There are also many spruce mires and small bogs. With all these Sipoonkorpi can be characterised as having diverse and small natural features.\nMany of the forests in Sipoonkorpi are spruce dominated. There are many lush and herb-rich forests as well as rocky forests. Some of the area's forests are valuable old forests which are practically in their natural state. These provide a home for species that are dependant on rotting wood. The area's flora and fauna is indeed diverse and abundant and includes endangered as well as rare plant, mushroom and animal species. Plants such as the rare Dog's Mercury (Mercurialis perennis), Anemone ranunculoide and the Fumewort (Corydalis solida ) grow in the area's herb-rich forests. In addition almost all of Southern Finland's forest mammal and bird species can be found at Sipoonkorpi. It is one of the last regions near the Helsinki metropolitan area that offers a habitat for species, such as the western capercaillie (Tetrao urogallus), that require large wilderness-like forest areas.\nByabäckenlaakso Valley shelters an abundance of flowers\nThe landscape in Byabäcken River Valley alternates between farmed hillsides and forested hilltops and cliff areas. The river that flows through the centre of the valley is surrounded by fields and heritage landscapes - meadows and forest pastures which are used for grazing. Further away between cliffs and fields there are hillside and cliff bottom herb-rich forests, in which the Small-leaved Linden (Tilia cordata), the Hazel (Corylus avellana) and the Norway Spruce (Picea abies ) thrive. As the richness of the area's soil varies and there are large changes in altitude, the area's nature is diverse and the landscape is interesting.\nThe River Byabäcken is a tributary of the River Sipoonjoki, and flows through the Natura site for about 4 km. Part of the River Byabäcken flows through open farm landscape in its original river bed groove. The area is part of Sipoo's valuable cultural landscape. Parts of the river's course have been straightened and some of the cultural farm landscape has changed to forest as meadows which have not been grazed and unused fields have become wooded.\nEuropean eagle owl in Sipoonkorpi\nThe eagle owl, the iconic animal of Sipoonkorpi, is the largest owl species in Europe. The thickest breeding population in Finland is found in the southern and southwestern parts of the country. The eagle owl also thrives in the rocky forests of Sipoonkorpi. It builds its nest on the ground - often on a small projection of a precipice, at the foot of a tree or in the shelter of the roots of a tree blown down by the wind.\nThe thick, projecting \"eye brows\" are the distinguishing features of the eagle owl. Its plumage is speckled with yellowish brown and black, and its eyes are orangey red. The female is considerably larger in size than the male.\nThe eagle owl feeds on hares, small mammals and birds. The eagle owl couple does not necessarily nest every year. If the winter has been cold and there are not many rodents to feed on, they do not nest. The female sits on the eggs for five weeks and the male brings it food daily. A total of 2-4 chicks hatch in late winter or spring. The eagle owl's nest must not be disturbed, as it easily abandons its nest. Unable to fly, the chicks leave the nest at the age of roughly six weeks. They learn to fly at the age of about two months and do not become independent until late summer or autumn.\nDue to the decline in numbers, the classification of the eagle owl was changed from least concern to near threatened in the review of the red list species in 2010.\nThe Valley's Birds\nBird density along the river bank is very high, especially in areas dominated by deciduous trees. The deciduous forests and shrubbery provide nesting places for many birds in the Sylvidae family. Visitors may also hear a choir of thrush nightingales (Luscinia luscinia) singing by the brook. Other night singers are common to the area as well. Down in the valley you may spot the grey-headed woodpecker (Picus canus) and the ortolan bunting (Emberiza hortulana). The spotted nutcracker's (Nucifraga caryocatactes) soft croaking can often be heard on the forest's fringes.\nThe biodiversity of forest nature in the valley also adds to the value of the forest in the surrounding area as a bird habitat. In addition to being home to the basic bird population the area also boasts some demanding bird species which have adapted to a specific forest environment. These species include the greenish warbler (Phylloscopus trochiloides), which thrives in lush forests, the red-breasted flycatcher (Ficedula parva) and the three-toed woodpecker (Picoides tridactylus), which prefer old spruce forests, and the nightjar (Caprimulgus europaeus) and the woodlark (Lullula arborea), which favour peaceful rocky areas.\nThe areas heritage landscape developed through the effects of traditional farming. Many plant, fungus and animal species inhabit these heritage landscapes, and the area will not remain so rich in biodiversity if it is not managed and cared for. To ensure that the heritage landscape in Byabäcken Valley remains as such it is actively maintained: Forest pastures and meadows are kept clear by having horses graze in them. Areas which are overgrown are cleared. As well as maintaining the heritage landscape Metsähallitus tries to keep all the area's fields farmed.\nThe heritage landscape in Byabäcken River Valley was restored between 2001 and 2004 with EU Life funds.","Embark on a fishing adventure, catch a glimpse of wildlife and explore nature at Eagle Lake!\nThe lake is a popular with anglers and the varying depths of the lake makes for great fishing year round.\nNatives species to the lake include brook trout, lake trout, whitefish, brown bullhead, white sucker and the smelt. Specific programs to stock the lake with walleye, rainbow trout, smallmouth bass, and yellow perch have taken place over many decades. Northern Pike were also introduced to the lake approximately five years ago.\nThe lake is home to at least three sets of loons, snapping turtles, many families of duck, otters, deer, bald eagles, osprey and over 90 other species of birds. Black bears are also frequently spotted at the township landfill site.\nEagle Lake and the surround community is home to many species of stunning flora and there are multiple nature and hiking trails to explore.\nBe sure to check out the Discover Trails website for more information. Whether you are looking for a short 30 minute hike on foot through Mikisew Provincial Park or a longer day journey by bike with a 65 km ride around the entire lake this website should be your first spot when planning an adventure!\nCANADIAN GEESE AT EAGLE LAKE\nA new family spotted on the lake on July 2016\nOver the past few years we have had numerous complaints about Canada Geese fouling peoples’\nproperty. Ironically, Canada Geese did not historically stay on Eagle Lake during the summer but only\nstopped temporarily when they were migrating. Unlike most waterfowl, these geese feed primarily on land, and prefer grazing on large open lots which they do for around twelve hours a day. They digest very quickly (approximately one hour) and require large amounts of food from which to obtain their nutrition. For example, it is estimated that an adult Canada Goose will eat four pounds of grass per day and deposit one to three pounds of droppings on the lawn or in the water.\nWhen they are moulting they cannot fly. Since the moulting period lasts for a month or more, they must be able to find sufficient forage in the water or within walking distance of their waterway. Traditionally, Eagle Lake did not provide conditions suitable for this feeding pattern. However, there are now numerous properties from which the geese can easily access the grass they enjoy for forage. These lawns also provide wide, unobstructed views of any approaching predators and ready access to the safety provided by the water. By nature, these are tundra animals that congregate on low vegetation in close proximity to water.\nThe main problem for lakefront property owners is the mess that is created on their properties. The droppings are extensive, messy, and a potential health risk. They contribute to e coli contamination, and can contain parasites such as that which causes swimmers itch. In addition to polluting the Lake, the fecal material contain both nitrogen and phosphorous which accelerate the growth of algae. With the recent emergence of the dangerous blue/green algae in Lake Bernard, this represents a significant potential threat for our Lake. Adult geese can also become aggressive if disturbed, particularly when breeding and nesting.\nThere are a number of things a property owner can do to prevent these problems: Firstly, do not feed these geese or any other waterfowl. Human food such as bread and popcorn does not contain the nutrition they require, and feeding encourages them to return. Secondly, if you maintain a lawn, a natural or artificial barrier can limit or eliminate their access to your lawn. The preferable option involves creating a barrier of natural vegetation at the shoreline. Canada Geese will not wander through underbrush that limits their visibility and reduces their sightlines. Ideally, the barrier should be over thirty inches tall. It works best if it continues 20 to 30 feet back from the shoreline. This option provides numerous other benefits for the health of the Lake and the shoreline.\nFor more expedient solution, you can create a barrier such as a fence. Other, more elaborate, options that have been suggested include a wire stretched across the shoreline, or plastic netting placed on the grass.\nEagle Lake has always been fortunate because, as a headwater lake, the only way the water or environment can become degraded is through the action of residents whose property is on or near the lake or those that use it on a temporary basis. Historically, the main environmental concern for the ELCA centred on the high level of phosphorous in the water which represented the most immediate threat to the Lake.\nRecently another threat to the health of the Lake has been identified. Invasive Phragmites (European Common Reed) has been identified in the south basin of the Lake. The most dense outcrop of this perennial grass is growing at the property located at 40 Angus Point Road. For more information from the ELCA on phragmites, please check out the February 2017 and June 2017 newsletters.\nUpdate – July 2017: Recently, the ELCA partnered with Robert Canning to draft the Eagle Lake Visit Report_DRAFT. The Eagle Lake Conservation Association (ELCA) had a number of concerns regarding phragmites on their lake, primarily related to the distinction between native and invasive subspecies and management options for shoreline phragmites adjacent to a proposed boat launch development site. Awareness of phragmites on Eagle Lake by the ELCA began around 2015/2016 when this species was observed at the future municipal boat launch site. Interviews with other shoreline residents indicate that this plant may have been present in the lake for between 5-10 years."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:2521b6a2-8a70-48c8-bcb6-1c0e878f2c38>","<urn:uuid:c3c02a6f-1c5d-42dc-b9c9-3cacce5a9f71>"],"error":null}
{"question":"I'm interested in historic medical facilities: which came first, the Military Medicine Journal of Italy or the Parliament House in Melbourne that served as a meeting place for aboriginal tribes?","answer":"The Military Medicine Journal was established in 1851, while Parliament House was built starting in 1856 with the two chambers. The Military Medicine Journal is among the oldest military publications in Europe, while Parliament House was built on a site that was traditionally used as a ceremonial ground and meeting place for the five Aboriginal tribes of the Port Phillip region (the Kulin confederacy) before construction began.","context":["General Inspectorate of the Military Health\nVia di Santo Stefano Rotondo, 4\nBasic Task of the Military Medical Service\nThe Military Medical Service shall:\n• Check the suitability of citizens for military service.\n• Check the suitability of military personnel for un - conditional military service.\n• Take care of the health status of the military personnel.\n• Ensure for the supply and fitting of technical materials and general service required in time of peace, war or serious international crisis.\n• Any other legal requirement under the provisions of the present Military Code, regulations or by law.\n(Currently, there is an ongoing re-organization process of the Military Health Services.)\nThe Italian Military Medical Service is built on top of a joint body, formed by the General Inspectorate of the Military Health Services, framed within the Defence General Staff organization and it is the health policy-making body of the Chief of Defence.\nReporting directly to the Ministry of Defence, the Forensic Medicine Board is the supreme joint consultancy body of the State, in the field of forensic medicine cases. Each service – Army, Navy, Air Force and Carabinieri – has its own medical service that depends on the respective commands.\nThe main assets of the Italian Military Medical Service consist of the facilities for:\n• Health care and Preventive Medicine (some 300 Unit clinics deployed at Regiment/Battalion level to provide basis level of care).\n• Diagnosis, hospitalization and treatment (3 Armed Forces military hospitals).\n• Forensic Medicine (13 Military Departments of Forensic Medicine).\n• Selection (13 centres/ bodies for medical selec - tion of personnel).\n• Training (own Armed Forces have training Centres).\n• Research (Research Centre of the Army, Department of Aviation Medicine).\n• Medical Airworthiness of personnel (2 Air Force Forensic Medicine Institutes).\nRegional Medical Facilities\nRole 1 medical facilities in each military unit (Battalion/ Regiment level, Airports, Naval Bases, ships, etc…)\nRoma (Army) Role 4\n2 Air Force Institutes for Flight Fitness Evaluation.\nNumber of Medical Personnel\nEach Armed Force provides primary and basic training to their own medical personnel.\nThe medical functions of preventive medicine, forensic medicine, emergency medicine, stabilization and evacuation of wounded and sick, and occupational health, are insured by the medical services of the Armed Forces who are responsible for providing direct support to agencies and departments operating abroad. Medical functions related to the diagnosis, care and treatment, and suitability for military service are insured by the Military Hospitals. For missions abroad, medical facilities are defined from time to time by the Joint Operational Command, if it is a joint operation, or by the respective Service commands if it is a single service operation, in compliance with the provisions con - tained in the NATO publications MC 326/3 and related AJP 4.10 and AJMedPs. In particular, the tactical aeromedical evacuation is guaranteed by “combined” assets, which Italy participates to with rotary and fixed wings aircraft. For what strategic aeromedical evacuation is concerned, Italy has its own military means, in terms of non-dedicated aircraft but with medical equipment and teams on board, which provide, through the activation of specific procedures, the quick return to home country of injured/deceased.\nThe Military Medical Services may enter into agreements with the Public Health organization for the exchange of medical professionals and for training purposes.\nOther Special Aspects\nThe Military Medicine Journal (Giornale di Medicina Militare – GMM) was established in 1851. It boasts to be the oldest military journal of Italy and is among the oldest still pub - lished military publications of Europe. Besides it is very likely to be the oldest Italian periodical of Medical Sciences too. On its pages it has reported achievements in medicine through over a century and a half. By reviewing military medicine studies and experiences, it has diachronically transformed itself reaching the quality standard of the best Italian and foreign specialized magazines thanks to the contribution of prominent medical figures.\nUntil today the GMM has been published, without interruptions, by the Ministry of Defence and handed out, free of charge, to the highest institutional offices, to all the Medical Officers of the Armed Forces and the National Health Service branches.\nThe GMM is handed out also to Italian Universities’ Rectors, Italian Embassies abroad and the foreign magazines of military medicine with which the JOURNAL cooperates and exchanges information.\nThe print version of the GMM has been recently joined by an on-line version that will allow a global access to scientific, informative and commercial contents of the JOURNAL. Back issues of the print GMM can be found at the JOURNAL.\nBrowse by category\nSDFDS is the Section Defence Forces Dental Services from the FDI. Every year prior to the FDI annual congress we organize an annual meeting. This meeting contains of a cultural day and a scientific program of 2 days, in which international military dentists share their experiences, challenges and latest developments.\nThe next meeting will be from 29 AUG-1SEPT in Shanghai, China","PARLIAMENT HOUSE (INCLUDING GROUNDS, WORKS AND FENCES) SOHE 2008\nStatement of Significance\nLast updated on - February 28, 2000\nWhat is Significant?\nParliament House is a classical style building with the plan based on the layout of the Houses of Parliament at Westminster. It has a central entrance through a vestibule and shared hall with the legislative chambers to the right and left. The central axis of the building terminates in the Library at the back, which looks out to the Parliamentary Gardens. Accommodation for committee rooms, offices and other spaces were intended to surround the two chambers at basement, ground and upper levels, but are largely incomplete.\nParliament House was built in six stages. The two chambers were constructed of bluestone in 1856. The Library and Refreshment Rooms were constructed in 1858 and 1860 with the exterior cladding to the east facade being Bacchus Marsh stone. The Great Hall and Vestibule were completed in 1879. The West Front was completed in 1892 using Stawell stone. The north and northeast wings to basement levels were completed in 1889 and 1892. A wing housing the Refreshment Rooms on the north east corner was built in 1928-30. The side and rear wings as well as the Dome have not yet been completed. The Legislative Council Chamber, the Legislative Assembly Chamber, the Library and the Refreshment Rooms were designed by John George Knight and Peter Kerr. Queen's Hall, the Vestibule and the West Front were designed by Peter Kerr.\nThe building displays a very high standard of craftsmanship and detail. The external walls make use of sculptural stonework and the interior has an extensive array of decorative plasterwork, encaustic tiled floors, decorative painting and gilding and French-polished, cedar joinery. The principal interior spaces, designed by Peter Kerr, use a classical architectural vocabulary to create a series of grand spaces appropriate to the weighty functions of Government. A system of air-conditioning was devised by JG Knight in 1859 to cool and ventilate the Legislative Assembly Chamber. In 1889 this was improved by the addition of a tunnel to bring fresh air from an inlet shaft concealed by a domed, temple-folly structure in the garden.\nThe curvilinear layout of the gardens that surround Parliament House is attributed to William Guilfoyle, designer of Melbourne's Royal Botanic Gardens. This was modified in 1888 to accommodate the bowling green with its pavilion and the tennis court. However the overall character is one of curvilinear gravel walks edged with brick; open lawn, large specimen trees including the commemorative Federation Oak, and densely planted shrub beds. The ventilation tower built in the form of a classical 'folly' is a significant feature.\nThe architecture of Parliament House is enhanced by its setting, high on Eastern Hill, and terminating Bourke Street. The site was enclosed by the existing ornamental wrought iron palisade fences atop a basalt plinth in 1889.\nParliament House was built on the site of a traditional ceremonial ground and meeting place for the five Aboriginal tribes of the Port Phillip region, which together formed a confederacy known as the Kulin. It has been the home of the Victorian Government since the mid-nineteenth century and as such reflects many aspects of Victoria's colonial history. An important aspect of Victoria's historical identity as a separate colony was the demand for a wider franchise following the Eureka rebellion of 1854. The 'U' shaped layout of seating in the legislative chambers allows for a number of representative parties, and the Victorian aim of broad representation is reflected in the motto incorporated into the encaustic tiled floor of the Vestibule: \"Where no counsel is the people fail, but in the multitude of counsellors there is safety\". Parliament House accommodated the Commonwealth Parliament between Federation and the establishment of the Provisional Parliament House in Canberra in 1927. The Federal Government acknowledged this hospitality on their departure, by funding the construction of the north east wing to house new kitchen and dining rooms.\nHow is it Significant?\nParliament House is of architectural, aesthetic, historical, and cultural significance to the State of Victoria.\nWhy is it Significant?\nParliament House is architecturally and aesthetically significant as an embodiment of the ideals of nineteenth century civic architecture, employing a classical architectural vocabulary to symbolise its function, and located on high ground, terminating a major city street. The building is of architectural significance in its functional layout, based on that of the British Parliament at Westminster, and its use of innovative systems of ventilation and air conditioning. The building's exterior and interiors are of aesthetic significance in displaying the highest standards of design, craftsmanship and decorative detail in the sculptured stone, ornamental plaster mouldings, painted decoration and monumental architecture, created by the foremost artists of the day. The garden is of aesthetic significance in its curvilinear layout and plant character, providing an appropriate and functional setting.\nThe building is historically significant for its association with the Victorian Government since the mid-nineteenth century in accommodating the making of decisions affecting all the People of Victoria and in demonstrating the broad, democratic franchise of the State. It is historically significant as the seat of the Commonwealth Parliament between Federation and 1927, in accommodating for more than two decades the making of decisions that affected all Australians. It is also historically significant as a symbol of the gold wealth of nineteenth century Victoria.\nParliament House is culturally significant as an important landmark within the streetscape of Melbourne and as a former meeting place of the Kulin tribes. [Online Data Upgrade Project 2001]"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:484138bb-5753-4dd4-91ec-366926bda8f0>","<urn:uuid:9a2fc2a8-c4d3-4309-b720-3bf6e6ba6114>"],"error":null}
{"question":"How do the early food gathering practices of the Great Lakes people around 3000 BCE compare with the Plains people's later farming techniques?","answer":"In the Great Lakes area around 3000 BCE, people primarily relied on hunting, gathering, and fishing, focusing on deer, black bear, and beaver, with distinctive copper tools for these activities. In contrast, later Plains agricultural practices evolved into a sophisticated farming system that balanced hunting-gathering with agriculture. The Plains groups developed farming techniques for both indigenous plants (such as marshelder, sunflower, gourd, and goosefoot) and introduced crops (like corn, squash, and common bean). These farming communities maintained a balanced hunting-gathering and farming economy, particularly in the tallgrass prairies of the eastern portion of the central Plains, which were intermixed with oak/hickory woodlands and floodplain forests.","context":["( – promoted by navajo)\nIt is not uncommon for accounts of American history to begin in the fifteenth century with the Spanish voyages of exploration. What the Europeans found was not a wilderness, but a land which had been settled by and developed for American Indians. By five thousand years ago there were many different cultural adaptations. In this essay, I’d like to briefly describe a few of the events which happened about five thousand years ago in the area which would later become the United States.\nOne of the most important agricultural products in the world today is corn (maize). Originally domesticated in central Mexico, it had diffused to the American Southwest (Arizona and New Mexico) by 3000 BCE. This means that by this time, American Indian people had learned about the plant, including how to plant it, care for it, harvest it, and process it into food products. The diffusion of corn into the Southwest begins to set the stage for the development of technologically advanced cultures such as those of the Hohokam and the Ancestral Puebloan.\nBy 3000 BCE, the Ohlone had begun to settle near San Francisco Bay. The Ohlone merged by conquest and marriage with earlier inhabitants of the area, probably a Hokan-speaking people related to the Pomo and Esselen. The people exploited a seacoast ecology, particularly shellfish. Some of their shellfish debris mounds would grow to 270 feet in diameter and 30 feet deep.\nFarther south, in the Los Angeles area, Indian people in the Zuma Creek area were collecting mussels, abalone, and other shellfish. Archaeologists have found a disproportionate number of handstones and milling stones which shows that the Zuma Creek inhabitants gathered and processed large amounts of plant foods.\nNear Santa Barbara, the Indian people of what archaeologists call the Campbell tradition were utilizing coast resources, including seal, fish, and shellfish.\nIndian people occupied the Lower Jackson Mound site by 3000 BCE. Material remains at the site included small red jasper beads which were sometimes shaped in the form of an animal. Some frog effigies were also found at the site.\nAt what is now the campus of the Louisiana State University, Baton Rouge, Indian people built two conical mounds, the largest of which is 120 feet in diameter and 16 feet high.\nGreat Lakes Area:\nIn the Great Lakes area, there was an increase in population about 3000 BCE. The Indian people in this area, called Lake Forest Archaic by archaeologists, were hunting, gathering, and fishing. Deer, black bear, and beaver were frequently hunted.\nIn the Lake Superior region at this time, Indian people had started making copper tools. This included copper projectile points, axe and adze blades, wood splitting wedges, and fish hooks. They also made copper ornaments, including beads, bracelets, and headdress pieces.\nThe tool tradition which archaeologists call the Oxbow Complex emerged about 3000 BCE. The Oxbow point is characterized with shallow side-notches, a broad incurved base, and rounded lugs or ears. The Oxbow Complex tool kit included bi-face knives, small end scrapers, thin uni-face knives, and pebble hammerstones. The people at this time were exploiting bison, elk, wolf, coyote, fox, rabbit, martin, goose, frog, mussel, pronghorn, mountain sheep, birds, and small mammals.\nAlso in the Northern Plains at this time, the tool tradition which archaeologists call the McKean Complex emerged. This was a focus on communal buffalo kills as well as the communal hunting of deer, pronghorn, and mountain sheep. The McKean projectile point was lanceolate in shape. One of the distinctive features of this complex was the side-notched knife that is similar to a large projectile point. This tool would be resharpened on one blade edge until worn out and then discarded.\nAlong the lower Snake River in eastern Washington, the phase which archaeologists call the Tucannon phase had begun by 3000 BCE. The Tucannon tool kit included crudely chipped corner-notched and stemmed points, some chipped knives, and edge-flaked cobbles. There were also hopper mortars, pestles, and net sinkers made out of ground stone. The people at this time were hunting elk, deer, and antelope. They were also taking salmon from the river.\nAbout 3000 BCE, the Indian people in New Jersey were using cremation burials. This complex featured an elaborate pattern of mortuary ceremonialism that emphasized cremation. It also included the ritual use of red ochre. The burials included valuable, imported grave goods. It is not uncommon for grave goods to be “killed” (intentionally broken) in order to release the spirit of the item so that it can travel with the deceased to the afterlife.\nIn Maryland, Indian people were occupying the Accokeek Creek Site by 3000 BCE. They were using the site as a temporary hunting, fishing, and gathering camp. Indian people probably lived at this site for a few weeks or months each year. They would later abandon the locality for a few years or decades, and later reoccupy it seasonally for a while, and then again abandon the area. The Indian people at this time lived in small, family groups, probably having few or no social, political, or economic ties with any other group.\nWith regard to material culture, they carved bowls out of soapstone which they obtained from a site about 15 miles away. These bowls were oval, hemispherical, or roughly rectangular in shape. They had flat bottoms, and some had knobs on the sides for handles. The bowls generally held a quart or two.\nIn Texas, burned rock middens began to appear about 3000 BCE as Indian people utilized high density food such as nut crops, deer, and rabbits. Several small bands would come together seasonally to occupy campsites where these foods were found.\nBurned rock middens are mounded concentrations of burned limestone cobbles which can be up to an acre in size and up to two meters deep. Archaeologists feel that burned rock middens are the result of intensive plant-processing activities.","Ancient Great Plains Farming\nPrimary Researcher: Mary Adair, curator\nNative American groups who occupied the Great Plains are historically viewed as bison dependent, as bison have a long history of use on the Plains and have today become a symbol of the vast prairie grasses. However, the tallgrass prairies of the eastern portion of the central Plains are intermixed with oak/hickory woodlands and floodplain forests that provide diversity in the available plant and animal communities. In prehistory, these regions supported both residentially mobile and sedentary groups who maintained a balanced hunting-gathering and farming economy.\nMy research has focused for decades on documenting the presence and importance of prehistoric farming on the central Plains. The ongoing goals of this research include the recovery and identification of both indigenous domesticates and introduced farm crops in the prehistoric central Plains; chart the spatial and temporal distribution of each domesticate; document the contextual relationship of domesticates to other cultural data; and record the relative contribution of each domesticate to the overall diet and general health of cultures through time. The remains of domesticated plants are preserved in archaeological sites as either macro remains (i.e. charred complete or fragmented particles of seeds, nuts, wood, rind, etc) or micro remains (i.e. pollen, phytoliths, starch grains, or lipid particles). Each category requires specialized recovery and identification methods, although analyses of both kinds of remains provide a more comprehensive approach to delineating the etiology and economic importance of farming on the North American Great Plains.\nDomesticated lambsquarter seed (Chenopodium berlandieri) from site 14BU55\nDessicated squash (Cucurbita pepo) from an early historic occupation\nCharred corn (Zea mays) cobs from an early historic Omaha site\nTo date, remains from the indigenous plants of marshelder (Iva annua), sunflower (Helianthus annuus), gourd (Cucurbita pepo var ovifera), goosefoot (Chenopodium berlanderie), little barley (Hordeum pusillum), maygrass (Phalaris caroliniana), erect knotweed (Polygonum erectum) and tobacco (Nicotiana spp.) provide evidence that Plains groups tended, encouraged and eventually domesticated these resources. Most were probably consumed as food, although the gourd may have been selected for use as a container in pre-ceramic times and tobacco was grown for smoking. Crops that arrived in North America as domesticates include corn (Zea mays), squash (Cucurbita pepo), common bean (Phaseolus vulgaris) and gourd (Langeria siceraria).\nDirect radiocarbon dates on the plant remains document that the earliest crops were the native squash (dated to about 2000 BC), followed by marshelder (dated to ca. 750BC) and sunflower. By the Middle Woodland period of about AD200-400, the native plants of maygrass, erect knotweed, little barley and goosefoot were tended and may have been undergoing genetic changes leading to domestication. The tropical variety of squash is also present. Micro remains of corn document that this crop had been introduced into the central Plains but the plant might not have been an important food item. By the Late Woodland period of ca. AD 800-1000, all of the native plants are fully domesticated and corn becomes part of the economy. The common bean is the last of the tropical cultigens to arrive with direct dates suggesting this happened around. AD 1100.\nAfter European contact, other foods were gradually introduced into North America and were occasionally added to the diets of Native groups. In the central Plains region, we have recovered the seeds of watermelon (Citrullus lanatus), cantaloupe (Cucumis melo), peach (Prunus persica) and the garden pea (Pisum sativum).\nMaize, or Indian corn (Zea mays) became the most economically important crop to Native populations in North America. At the time of European contact, this crop was grown over most of North America and was thus adapted to diverse environments, ranging from high altitudes, low precipitation levels, short growing seasons, and temperature extremes. The process by which maize spread throughout the continent and was accepted by diverse cultural traditions is both complex and not totally clear. In most agricultural complexes, corn was not the only crop grown, but intercropped with different squashes and pumpkins (Cucurbita spp.), beans (Phaseolus spp.), and indigenous plants. The crops of corn, beans and squash are often referred to as the “Three Sisters” to signify their combined importance in Native American societies. While these crops all originated in Mexico, their spread northward was temporally independent and, as advances in plant genetics demonstrate, likely followed different pathways."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:99ada046-e03d-4cec-b56a-30fcc16855ba>","<urn:uuid:38539f5b-2218-4083-8d32-d51463188910>"],"error":null}
{"question":"¿Cuáles son los beneficios probados para la salud del consumo de yogur probiótico, y qué características técnicas debe cumplir un producto para ser considerado un probiótico genuino según los estándares internacionales?","answer":"The health benefits of probiotic yogurt include improved gastrointestinal health, enhanced immune function, and potential protection against various conditions. Specifically, probiotics can reduce severity and duration of diarrhea, help manage IBS symptoms, decrease upper respiratory infections, and may lower the incidence of yeast infections. Some studies also suggest benefits for anxiety reduction and mental health improvement. As for the technical requirements to be considered a probiotic, products must meet specific criteria: they must be specified by genus and strain, deliver adequate dose through shelf life, show efficacy in controlled human studies, resist low pH and gastric juice, adhere to cells, exclude pathogens, remain metabolically active in gut, be safe and non-pathogenic, and be able to modulate immune response. Additionally, the probiotic strains must be properly documented and labeled with genus, species, and strain designation to tie the product to scientific research.","context":["- Yogurt is made from milk cultured with live bacteria.\n- Yogurt is consumed in a variety of ways, including Greek yogurt, drinkable yogurt, and frozen yogurt.\n- Between 2000 and 2020, at least 12 yogurt-associated outbreaks were reported to CDC’s National Outbreak Reporting System (NORS), causing 145 illnesses, 22 hospitalizations, and no deaths.\n- The most recent yogurt-associated utbreak occurred in 2021 due to potential contamination of E.coli O157:H7.\n- The use of pasteurized milk is a key barrier to foodborne pathogen transmission in yogurt products. Raw milk can contain pathogens such as Escherichia coli, Salmonella spp., and Campylobacter jejuni. The acidity of yogurt is another barrier to foodborne illness. There is evidence of E. coli 0157:H7 exhibiting acid-tolerant properties, but this pathogen is readily destroyed via pasteurization.\n- Yogurt products have previously been associated with fungal disease.\nYogurt has been a staple food product for numerous cultures throughout the world dating back many thousands of years. In the Middle East, primitive herdsman began carrying milk in containers made from intestinal gut lining, which they discovered could help extend the life of milk products because contact with the intestinal fluids of the containers caused the milk to curdle and sour, preserving it for an extended period. Other than drying, this was historically the only safe method of preserving milk.\nModern yogurt production involves culturing milk with live bacteria. The bacteria produce lactic acid which coagulates the milk proteins, making yogurt thick and slightly sour in flavor. The bacterial cultures required for producing yogurt are Streptococcus thermophilus and Lactobacillus bulgaricus. About 80% of all yogurt manufactured in the U.S. contains an additional culture called Lactobacillus acidophilus, and many commercial yogurt products also contain Bifidobacterium bifidum or Lactobacillus casei because of their potential health benefits.\nOver the millennia, a multitude of health benefits have been attributed to eating yogurt. As for back as 6000 BCE, Indian Ayurvedic medicine made reference to the positive health benefits linked to yogurt consumption. Yogurt has been used in the treatment of everything from a variety of gastrointestinal maladies to sunburn relief. In the early 20th century, it was even sold in pharmacies as a medicine. Today, yogurt is promoted as a healthy “probiotic” food. The benefits of incorporating probiotic foods such as yogurt into the diet have been widely documented, and there is emerging research to suggest yogurt may have positive implications for improved gastrointestinal health and overall immune function.\nYogurt can be consumed directly, but it is also commonly used to make dips and dressings and is used as a lower-calorie substitute in cooking. Drinkable yogurt and frozen yogurt are also widely consumed yogurt products.\nTypes of Yogurt\nThe U.S. Food and Drug Administration (FDA) regulates and inspects commercial yogurt products. The FDA also sets the following guidelines for labeling yogurt:\n- Regular yogurt must be no less than 3.25% fat and contain no less than 8.25% milk solids.\n- Low fat yogurt must be between 0.5 to 2% fat and contain no less than 8.25% milk solids.\n- Non-fat yogurt must be less than 0.5% fat and contain no less than 8.25% milk solids.\nOther terms used to describe yogurt are based on differences in processing:\n- Set (equivalent to sundae or fruit-on-the-bottom): This is the firmest type of yogurt. During processing, the yogurt is set in a container and is left undisturbed. Fruit can be added to the bottom of the container so that, when turned upside down, it resembles a sundae.\n- Swiss (equivalent to stirred, custard, or blended): Most commercial yogurts are Swiss style. After the yogurt is set, it is stirred and dispensed into secondary containers, making it less firm than set yogurt. Fruit can also be stirred in to produce flavor varieties.\n- Liquid or drinkable: Liquid yogurt has milk, fruit, and/or fruit syrups added for increased fluidity and flavor. The pH of yogurt is raised when milk is added, so the shelf life of drinkable yogurt is generally only 4–10 days.\n- Yogurt cheese or strained yogurt: Yogurt is strained to remove liquid whey, resulting in a thick, creamy, concentrated product.\n- Frozen yogurt: To make frozen yogurt, regular yogurt is mixed with a pasteurized ice cream mix of milk, cream, and sugar. Other ingredients, such as stabilizers, fruit, and flavors are blended together and then the mixture is frozen. In some cases, frozen yogurt contains live bacteria; the bacteria become dormant when cooled, but regain activity post-ingestion due to the warm body temperature.\n- Greek yogurt: Greek yogurt contains about twice the protein and half as much carbohydrates and sodium as regular yogurt. During processing, the yogurt is strained three times, as opposed to two times with regular yogurt, which yields the higher protein content and creamier texture. There are also low-fat and fat-free options for Greek yogurt.\n- Lite or light: Yogurt that has 50% less fat or one-third fewer calories than regular yogurt is considered light.\nFoodborne Outbreaks and Recalls\nBetween 2000 and 2020, at least 12 yogurt-associated outbreaks were reported to CDC’s National Outbreak Reporting System (NORS), causing 145 illnesses, 22 hospitalizations, and no deaths. In 2013, an FDA investigation into complaints of gastrointestinal illness found that commercial yogurt products sold by the brand Chobani had been contaminated with the fungi Mucor circinelloides. Symptoms, including vomiting, nausea and diarrhea, were reported by more than 200 consumers who had ingested the associated yogurt products. The company voluntarily pulled the products connected to the reported illnesses from the market. The risk linked to fungal pathogens is not well understood, but M. circinelloides may cause spoilage in yogurt, and it poses a particular risk to the immunocompromised.\nYogurt (along with ice cream) was one of the vehicles of infection named in a 2007 outbreak of Hepatitis A which was determined to have been spread by a food-handler in a Minnesota restaurant. Fifteen people were reported ill and six were hospitalized; no deaths were reported. A Norovirus outbreak involving frozen yogurt and ice cream occurred in 2004 during a school fundraiser in Arizona. Norovirus was determined to be the source of the illnesses after an employee who had handled the machine tested positive. 53 fundraiser attendees reported being ill; no one was hospitalized, and no deaths were reported.\nInterestingly, yogurt is often recommended as a natural aid in returning gut after contracting a foodborne illness.\nThe most recent yogurt-associated utbreak occurred in 2021 due to potential contamination of E.coli O157:H7. This was a multi-state outbreak that began in Seattle and King County of Washington and had secondary infections in Arizona. Several children were affected and infections were linked to Pure Eire Dairy yogurt sold as PCC Community Market brand yogurt. There were a total of 17 confirmed cases, 10 hospitalizations, 4 cases developed hemolytic uremic syndrome (HUS) and 0 deaths. Pure Eire Dairy issued a voluntary recall of products and PCC removed all products from shelves.\nTo contribute to the Yogurt Foodborne Outbreaks section, please follow this link: https://fsi.colostate.edu/suggest-a-topic/\nU.S. yogurt production in 2017 totaled 4.5 billion pounds at 170 processing plants. New York leads the nation in yogurt production, accounting for 15.7% of the total amount of yogurt produced in the United States. California is the second largest producer of yogurt.\nThe production of yogurt requires only two ingredients: milk and live cultures. However, producers may also include, dry milk powder, stabilizers, fruit, and sweeteners.\nMilk is the main ingredient used when making yogurt. It can be cream, whole, low-fat, or skim. Whole milk is used to make full-fat or regular yogurt, low-fat milk is used to make low-fat yogurt, and skim milk is used to make non-fat yogurt. In general, the higher the fat content of the milk, the smoother and creamier the yogurt will taste.\nLive cultures are the second key ingredient in yogurt. There are two required cultures, Streptococcus thermophilus and Lactobacillus bulgaricus, and several optional ones.\nCream may be added to adjust the fat content of milk. Non-fat, dry milk powder is used to adjust the content of yogurt solids above the 8.25% minimum to achieve better body and texture. Stabilizers also function to improve body and texture as well as to increase firmness. They help keep fruit uniformly mixed in the yogurt and prevent separation of whey. Examples of stabilizers include alginates, gelatin, gums, pectins, and starch.\nSugar, honey, and artificial sweeteners may be used to reduce the naturally sour flavor of yogurt. Fruit, fruit syrups, and pie filling are also optional and can either be mixed in with yogurt or added to the top or bottom of the yogurt.\nPasteurization: In order to prevent deactivation of the bacterial cultures needed in yogurt production, milk is pasteurized at 185°F for 30 minutes or 203°F for 10 minutes prior to the addition of the cultures. The high heat also denatures the whey proteins, which allows the yogurt to form a more stable gel. Lastly, pasteurization effectively kills disease-causing bacteria.\nAdjusting Milk Composition and Blending Ingredients: At this point, stabilizers are added to the mixture, as are sweeteners if a less tart yogurt is desired. The nonfat dry milk powder is also added before heating to prevent coagulation of the milk proteins.\nHomogenization: Not all yogurts are homogenized. If this step is taken, the ingredients are mixed well to ensure a more stable consistency.\nHeating: The milk is then heated to 200°F for 10–20 minutes, depending on the desired thickness of the yogurt. Holding it longer will result in a thicker yogurt.\nCooling and Inoculation: The mixture is then cooled rapidly to 112–115°F. At this point, the warm mixture is inoculated with the live bacterial culture.\nIncubation: The mixture is incubated for 4–7 hours at 105–115°F. The bacteria used in making yogurt are thermophilic and this is their optimal temperature range; they are killed above 130°F and do not grow well below 98°F. Yogurt will become firm when a pH of 4.6 is reached. Incubating the mixture any longer will result in an increased acidity and more sour taste.\nCooling: When the desired pH is reached, the yogurt is cooled to around 45°F to end the fermentation process.\nAddition of Fruit and Flavors: For set style yogurt, fruit is added to the bottom of the cup and the inoculated yogurt is placed on top of the fruit prior to fermentation. For Swiss style yogurt, fruit is mixed with the yogurt after the fermentation and cooling steps. The yogurt is then packaged; at which point, it should be refrigerated at 40°F or lower.\nThe acidity of yogurt acts as a barrier to bacteria growth, as does the high temperature achieved during the yogurt-making process. However, milk must be pasteurized beforehand to sufficiently kill disease-causing pathogens such as E. coli 0157:H7, which may be acid tolerant.\nIt is essential that all equipment and workspaces used in the yogurt-making process remain clean and sanitized to prevent the addition of unwanted bacteria to the yogurt.\nThe FDA requires that yogurt be made with live cultures, but some yogurts are heat treated so that the final product contains no active cultures. The label should specify what microorganisms are present, and in what amount in terms of colony-forming units (CFUs), as well as the known health benefits associated with the particular strains used. The label should also disclose the expiration or use-by date, serving size suggestion, company name, and proper storage of the product. Regardless of the use-by date, yogurt with visible signs of microbial growth or off-odors should be discarded immediately.\nThe shelf life of yogurt is 10–21 days. For liquid yogurt, the shelf life is 4–10 days and for yogurt cheese the shelf life is 7–14 days when refrigerated at 40°F. Yogurt can also be frozen for several months, but this may alter its texture.\nIn 2017, per capita consumption of yogurt in the U.S. was 13.7 pounds. Domestic consumption peaked in 2014/2015 at 14.9 pounds per capita. Although yogurt consumption in the United States has continued to rise over the past two decades, the per capita consumption is dwarfed by Sweden’s annual 62.8 pounds per person. Research by the NPD Group reports that Americans between the ages of 18–34 are driving the increase in yogurt consumption in comparison to the older generations. In recent years, the health benefits of yogurt (particularly the recognition of probiotics) has encouraged a higher consumption of yogurt and the incorporation of yogurt into many more food products. Yogurt has been introduced into a wider variety of products, including fast food establishment menus, toothpastes, beauty products, and even pet foods. In the 2006-2007 Foodnet Population Survey Atlas of Exposures, 43.3% of the survey cohort reported eating fresh or flavored store-bought yogurt within the past seven days.\nInformation on keeping yogurt fresh and safe to eat can be found at FoodKeeper App.\nThe health benefits associated with yogurt consumption are plentiful, and there is ongoing research suggesting yogurt may contribute more to overall health than is currently known. All types of this fermented dairy product are a nutrient-rich source of calcium, potassium, and protein, and yogurts that have been fortified with vitamin D and/or probiotics have additional health benefits.\nProbiotics are live microorganisms that develop a symbiotic relationship with the host when administered in the proper amount. Lactic acid bacteria that survive in the gut are often used as probiotics. This includes Lactobacillus rhamnosus, L. casei, L. acidophilus, and Bifidobacterium lactis, among others. The bacteria acquire nutrients and energy from the food people eat and, in return, help maintain a healthy gut microbiota. Research suggests this may promote immune function, improve mental health, and protect against cognitive impairment. It may also lower the risk of some chronic diseases, such as cancer, high blood pressure, obesity, and diabetes. Different strains of bacteria confer different benefits, which may be specified on the product label.\nGastrointestinal Health: Consuming probiotics is associated with a reduced severity and duration of diarrhea in children with acute infectious diarrhea, in those taking antibiotics, and in those with lactose intolerances. There is also evidence that probiotic consumption may be helpful in managing the symptoms of irritable bowel syndrome (IBS) and reducing relapses of ulcerative colitis. Some probiotics inhibit the growth of Helicobacter pylori, which is associated with ulcers and cancer of the stomach. The role of probiotics in preventing traveler’s diarrhea is being investigated.\nImmune Health: Researchers have demonstrated that consuming probiotics can decrease the incidence of upper respiratory infections in adults and reduce cold and flu-like symptoms in children (resulting in higher attendance in preschool and daycare). There is also evidence that consuming yogurt may lower the incidence of yeast infections.\nEmerging Research: Changes in the gut microbiota may occur with obesity and type II diabetes. There is potential for probiotics to play a role in the prevention of obesity and diabetes, but more research is needed. There is also a new field of research devoted to studying the link between the consumption of certain probiotics and mental health. A recent study showed that probiotics may reduce anxiety and stress, but further research is needed to confirm these findings.\n- Dairy Products Profile. (n.d.). Retrieved from https://col.st/MyXCZ\n- Foodborne Illness Outbreak Database – Sponsored by MarlerClark. (n.d.). Retrieved from https://col.st/vHGbV\n- Foodborne Illness Outbreak Database – Sponsored by MarlerClark. (n.d.). Retrieved from https://col.st/Bo8xx\n- Lee, S. C., Billmyre, R. B., Li, A., Carson, S., Sykes, S. M., Huh, E. Y., … Heitman, J. (2014, August 29). Analysis of a Food-Borne Fungal Pathogen Outbreak: Virulence and Genome of a Mucor circinelloides Isolate from Yogurt. Retrieved from https://col.st/UaNob\n- Mauro, Machado, & Rachel. (2015, July 11). History of yogurt and current patterns of consumption. Retrieved from https://col.st/Sa461\n- U.S. yogurt per capita consumption, 2018. (n.d.). Retrieved from https://col.st/jzIp4\nPicture acknowledgement: https://col.st/RcMtu","Probiotics: The Foundation for Total Well Being\nLactic acid bacteria (LAB), including species of Lactobacillus, Streptococcus, Pediococcus and Leuconostoc have been used for preservation of food by fermentation for thousands of years. People of Eastern Europe, Southern Asia and Northern Africa have consumed yogurt and kefir for thousands of years. Fermentation of milk by LAB has permitted its preservation, improved its palatability and digestibility. Ancient people regarded these fermented milks as divine foods and as indispensable remedies for various illnesses.\nFermentation of food provides characteristic taste profiles and lowers the pH, which prevents contamination by potential pathogens. Fermentation is globally applied in the preservation of a range of raw agricultural materials (cereals, roots, tubers, fruit and vegetables, milk, meat, fish etc.).\nL. plantarum frequently occurs spontaneously, in high numbers, in most lactic acid fermented foods of plant origin, for example, in brined olives, capers, and sauerkraut. Thus, humans have in this way consumed large numbers of live LAB, and presumably those associated with plant material were consumed before those associated with milk based foods.\nA century ago, Elie Metchnikoff (Russian scientist, Nobel laureate, and professor at the Pasteur Institute in Paris) postulated that LAB offered health benefits leading to longevity. He considered yogurt to be one of the most effective means of inhibiting intestinal infections, intoxications and putrefactions, which he thought were the cause of a great number of conditions such as premature senility and lack of vitality.\nThe term probiotics was first introduced in 1965 by Lilly and Stillwell; in contrast to antibiotics, probiotics were defined as microbially derived factors that stimulate the growth of other organisms. In 1989, Roy Fuller emphasized the requirement of viability, for probiotics, and introduced the idea that they have a beneficial effect on the host.\nThe World Health Organization has defined probiotic bacteria as live microorganisms which when administrated in adequate amounts confer a health benefit on the host (FAO/WHO 2001).\nAccording to the current definition, a fermented food is not a probiotic. A fermented food might contain a probiotic if the strain/strains in question satisfy the criteria. Fermented foods contain live, active cultures, but these cultures are generally tested for food fermentation properties and not health benefits.\nIn spite of the existing scientific consensus, there is no legal definition of the term probiotic.\nThe minimum criteria to be met for probiotic products are that the probiotic must be:\n- Specified by genus and strain - research on specific probiotic strains cannot be applied to any product marketed as a probiotic.\n- Delivered in adequate dose through the end of shelf life (with minimal variability from one batch to another).\n- Shown to be efficacious in controlled human studies.\n- Resistance to low pH, gastric juice, bile, pancreatic juice\n- Ability to adhere to cells (host or microbial)\n- Exclude or reduce adherence of pathogens\n- Remain metabolically active in gut\n- Produce antagonistic compounds\n- Resist antimicrobial substances\n- Safe, non-invasive, non-carcinogenic and non-pathogenic\n- Cohabit and form part of the indigenous microbiota\n- Modulation of immune response\n- Produce beneficial systemic effects\nYogurt is perhaps the most common probiotic-carrying food, but the functional food market has expanded beyond yogurt. Cheese, fermented and unfermented milks, juices, smoothies, cereal, nutrition bars, and infant/toddler formula all are food veÂhicles for probiotic delivery. In addition to being sold as foods, probiotics are sold as dietary supplements in the form of capsules, tablets, and powders.\nKey Species Studied and Used as Probiotics\nLactobacillus Genus Species\n- L.Â acidophilus\n- L. casei\n- L. fermentum\n- L. gasseri\n- L. johnsonii\n- L. paracasei\n- L. plantarum\n- L. reuteri\n- L. rhamnosus\n- L. salivarius\n- B. adolescentis\n- B. animalis subsp lactis\n- B. bifidum\n- B. breve\n- B. infantis\n- B. longum\n- Streptococcus thermophilus\n- Streptococcus salivarius\n- Enterococcus faecium\n- Escherichia coli\n- Bacillus coagulans\n- Bacillus clausii\n- Saccharomyces cerevisiaeboulardii\nThe gastro-intestinal tract, including the vagina, contains a very complex and diverse microbiota or microbial ecosystem - 100,000 billion bacteria (100 bill/g content), located mainly in the colon and comprising over 1000 species of microorganisms. Bacterial cells outnumber the host cells by a factor of 10.\nAlthough much is still unknown about the microorganisms that colonize humans, some important points can be made.\n- Each individual has his or her own unique population of microbes, even if there are commonalities of species among people.\n- The microbes colonizing different regions of the human body (skin, mouth, gastrointestinal tract, vaginal tract of women) are both diverse and numerous, and they differ according to their habitat.\n- Intestinal microbes are fairly stable through time, although transitions occur at weaning and again in the elderly. Colonizing microbiota can be impacted by antibiotics, diet, immunosuppression, intestinal cleansing, and other factors.\nActivities such as regulating immune function, enhancing the intestinal barrier to prevent unwanted microbes from entering the blood stream, colonization resistance, and digestion are important functions of colonizing microbes.\nAt birth, the intestinal immune system is virtually non-existent and develops at the same time as the intestinal microbiota. It becomes more and more complex as dietary intake changes from breast and/or bottle feeding to adult type food intake. Following birth, colonization facilitates the formation of physical and immunological barrier. The Host-Microbe interaction is of primary importance during the neonatal period. Around the age of 2, intestinal immune system is considered to be mature.\nIt is possible that the microbiota acquired during and immediately after birth is necessary for the newborn's systemic and mucosal immunity, and it may also be responsible for controlling inflammatory responses in allergic and inflammatory bowel diseases.\nThe intestine is the body's most important immune function related organ: approximately 70 % of the body's immune cells are located in the intestinal mucosa/lining of the GI tract. The immune system controls immune responses towards:\n- Dietary proteins\n- Prevention of food allergies\n- Pathogenic microorganisms (Salmonella, Listeria, Clostridium, etc.)\n- Viruses (rotavirus, poliovirus)\n- Parasites (Toxoplasma)\nProbiotics produce organic acids that are reported to inhibit E. coli and other intestinal pathogens. The most common are lactic, acetic and formic acids, which lower intestinal pH and may suppress harmful organisms. Acetic acid also lowers the O/R (oxidation-reduction) potential. Simplified, if the O/R potential is reduced, organisms that require oxygen for growth, such as Salmonella and Shigella, will be inhibited.\nMany researchers report that certain strains of lactic acid bacteria produce bacteriocins, antibiotic-like molecules and other compounds that inhibit intestinal pathogens. These antimicrobial substances include acidilin, acidophilin, lactolin, nisin and hydrogen peroxide.\nBile assists fat digestion, and conjugated and deconjugated (free) forms of the compound inhibit growth of enteric pathogens. Certain Lactobacillus strains can deconjugate (break apart) bile salts, which are the more inhibitory form.\n.Amines, produced by intestinal microbes, are irritating and toxic, and have been associated with diarrhea. Lactic acid bacteria have been reported to reduce the level of amines in the gut, and to neutralize enterotoxins.\nKey Probiotic Effects/Benefits\nIn general, the strongest clinical evidence for probiotics is related to their use in improving gut health and modulating immune function.\n- Activate local macrophages to increase antigen presentation to B lymphocytes and increase secretory immunoglobulin A (IgA) production both locally and systemically\n- Induce hyporesponsiveness to food antigens\n- Enhancing specific antibody responses to pathogens\n- Neutralize enterotoxins\n- Modulate cytokine profiles\n- increasing NKH cell activity;\n- enhancing the phagocytic capacity of polymorphonuclear cells and macrophages\n- Digest food, synthesize vitamins, increase mineral absorption\n- Alter local pH to create an unfavorable local environment for pathogens\n- Produce bacteriocins to inhibit pathogens\n- Scavenge superoxide radicals\n- Stimulate epithelial mucin production\n- Improve gut transit\n- Enhance intestinal barrier function\n- Compete for nutrients and adhesion with pathogens\n- Modify pathogen derived toxins\nProbiotics are intended to assist the body's naturally occurring microbiota of the gastrointestinal tract, the oral cavity and the vagina. Studies have documented probiotic effects on a variety of gastrointestinal and extra-intestinal conditions, including:\n- Prevention and treatment of diarrhea associated with antibiotics intake, C. difficile, rotavirus, food poisoning\n- Distension, flatulence, bloating, abdominal pain\n- Lactose intolerance\n- Inflammatory bowel disease (IBD)\n- Irritable bowel syndrome (IBS),\n- vaginal infections, UTI\n- atopic eczema,\n- Endotoxemia combined with liver cirrhosis.\n- Dental caries\n- Colorectal cancer prevention\n- Gastric ulcers\nClinically Documented Strains\nResearch on probiotics suggests a range of potential health benefits. However, the effects described can only be attributed to the strain(s) tested, not to the species, nor to the whole group of LABs or other probiotics\nA probiotic strain is listed by the genus, species, and an alphanumeric designation. In the scientific community, there is an agreed nomenclature for microorganisms for example, Lactobacillus plantarum 299v. Lactobacillus is the genus, plantarum is the species and 299v is the strain.\nExamples of documented strains\nBifidobacterium animalis DN 173 010 / Danone\nBifidobacterium animalis subsp. lactis Bb-12 / Chr. Hansen\nBifidobacterium longum BB536 / Morinaga Milk Industry\nLactobacillus acidophilus LA-5 / Chr. Hansen\nLactobacillus acidophilus NCFM / Danisco\nLactobacillus casei DN-114 001/ Danone\nLactobacillus casei Shirota / Yakult\nLactobacillus plantarum 299V / Probi\nLactobacillus reuteri ATTC 55730 / BioGaia Biologics\nLactobacillus rhamnosus ATCC 53013 (LGG) / Valio\nSaccharomyces cerevisiae (boulardii) / Biocodex, and others\nTested as mixture:\nLactobacillus acidophilus CL1285 & Lactobacillus casei Lbc80r / Bio K+ International\nLactobacillus rhamnosus GR-1 & Lactobacillus reuteri RC-14 / Chr. Hansen\nLactobacillus helveticus R0052 & Lactobacillus rhamnosus R0011 / Institut Rosell\nBacillus clausii strains O/C, NR, SIN, and T/ Sanofi-Aventis\nThe genera Bifidobacterium, Lactobacillus and Lactococcus have a long history of safe use in foods and are generally recognized as safe. Infections in humans by these genera are extremely rare. This is based on the prevalence of the microorganisms in fermented foods such as yogurt, cheeses, and sauerkraut, as normal colonizers of the gastro-intestinal tract and low level of infection associated with them. Â Most of the strains used in probiotic products are generally isolated from the human GI tract, from human milk, or from foods consumed by humans.\nScientists have pointed out three theoretical concerns regarding the safety of probiotics: the potential occurrence of such conditions such as: 1) bacteremia or endocarditis, toxic or 2) metabolic effects on the gastrointestinal tract and 3) the transfer of antibiotic resistance to other species in the gastrointestinal tract. For the most part these safety concerns are directed towards probiotics products containing species from the following genera: Enterococcus, Escherichia, Clostridium and Bacillus.\nThe use of probiotics in either diseased or immune-compromised individuals should be done under the direction of their health care professional.\nSelecting probiotic supplements\nOne approach to choosing a probiotic is to use the definition of a probiotic as a guide.\nlive microorganisms which when administrated in adequate amounts confer a health benefit on the host\nProbiotics as dietary supplements are generally freeze-dried, which is a means to preserve them for extended periods of time. The shelf life stability of a finished probiotic product will determine if the product contains live microorganisms and adequate amounts. The characteristics of the strain itself, the fermentation medium, conditions and process, filtration and freeze-drying are all factors that will contribute to the viability of the strains. Furthermore, the choice of other ingredients that will be integrated into the final product also contribute to the stability. The environmental conditions of the manufacturing rooms and handling of the cultures during blending, encapsulation, tableting, etc are also important, and appropriate procedures most be followed. Moisture content must be closely monitored and kept to low levels since freeze-dried bacterial cultures are very sensitive to moisture and the die-off increases as moisture content increases. The temperature and storage conditions are also important factors. As a general rule, freeze-dried probiotic cultures are most stable when stored under colder temperatures and die-off increases as temperature increases, especially when they surpass 30 C/85 F. It is possible, however, to design probiotic products that do not require refrigeration but extreme high temperature should be avoided and the shelf life will be extended if stored at relatively cooler temperatures. Finally, the packaging system, i.e. the type of packaging materials used, along with appropriate desiccants when necessary, is also important for the shelf life of a probiotic finished product by protecting the contents from various environmental factors.\nAdequate amounts, implies that the dosage must be high enough to elicit a health benefit. The minimum dose for most strains fall in the range of 1-10 bill/day. There are products on the market whose minimum recommended dose range from 100 million to 450 bill/day. The required amount is not to be taken as an absolute number of viable microorganisms for all individuals. Every individual has his/her unique native microbiota( a fingerprint) that has developed to maturity by the age of two. Furthermore, there is a phenomena called COLONICATION RESISTANCE which makes it difficult for outsiders or exogenous microorganisms including probiotics, to implant. Thus, dosage requirements can vary among different individuals and is difficult to establish specific dosage for all individuals. It is important that the dosage of the strains are based on the level used in the human studies and shown to be effective.\nThe label should disclose the genus, species, and strain designation for each strain in the product. The strain designation tie the product content to the scientific research/clinical documentation performed on the strain(s).Â This way the specific health benefits of the strain(s) can be found when searching for the scientific documentation.\nFor the promotion of a balanced intestinal microbiota and overall well-being a probiotic with a blend of species is recommended since this can allow for a greater colonization throughout various regions of the GI tract and provide a wider spectrum of benefits. Bifidobacteria, for example are found predominantly in the colon and therefore it is important that a product also contains Lactobacilli which can colonize the small intestine.\nProbiotics sold as dietary supplements or functional foods cannot make claims that they cure, treat or prevent disease. Manufacturers can, however, provide references that support the health benefits of their documented strains found in their products. Choose a probiotic that is manufactured by a reputable company and that works for you.\n-- Article by Peilin Guo, MS. RD & Silvano Arnoldo B.Sc.\nWGO Practice Guidelines Probiotics and Prebiotics 1\nWorld Gastroenterology Organisation, May 2008 pp1-23\nProbiotics as functional food: microbiological and medical aspects\nMalda Maija Toma, Juris Pokrotnieks\nActa Universitatis Latviensis, 2006, Vol. 710, Biology, pp. 117-129\nProbiotics and their fermented food products are beneficial for health\nS. Parvez, K.A. Malik, S. Ah Kang and H.-Y. Kim\nJournal of Applied Microbiology ISSN 1364-5072, January 2006Â pp. 1171-1185\nProbiotics: Their potential to impact human health\nMary Ellen Sanders ,Glenn Gibson, Harsharnjit S. Gill, Francisco Guarner CAST Issue Paper Number 36 October 2007\nThe place of probiotics in human intestinal infections\nA Sullivan, C.E. Nord\nInternational Journal of Antimicrobial agents 20 (2002) pp.313-319\nProbiotics: facts and myths\nC. Senok, A. Y. Ismaeel and G. A. Botta\nClin Microbiol Infect 2005; 11: 958-966\nImmunomodulatory effects of probiotics in the intestinal tract\nV. Delcenserie, D. Martel, M. LAmoureux, J. Amiot, Y. Boutin and D. Roy\nCurr. Issues Mol Biol. 10: 37-54\nLACTIC ACID BACTERIA Microbiology and functional aspects\nEdited by: Seppo Salminen and Atte von Wright\nMarcel Dekker 1998 (Textbook)\nHow do we know when something called Probiotic is really a probiotic? A guideline for consumers and health care professionals\nMary Ellen Sanders\nFunctional food Reviews, Vol 1, 2009 pp 3-12"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:3b92f637-cce3-4767-a9b0-bfd38cddf19b>","<urn:uuid:92fc7861-a6e2-4be2-8857-b73bea4c2e06>"],"error":null}
{"question":"Bonjour! Je suis archaeologist studying ancient cities. What are key differences in how Troy and Pompeii were discovered et excavated par archaeologists?","answer":"The archaeological discoveries of Troy and Pompeii followed different paths. Troy was first excavated in 1865 by English archaeologist Frank Calvert, followed by Heinrich Schliemann in 1868, who controversially claimed to have found 'Priam's Treasure.' Their excavations revealed multiple cities built in succession at the site of Hisarlik in Turkey. In contrast, Pompeii's excavation began in 1748, nearly 17 centuries after its destruction. The preservation of the two sites was also different - Troy was discovered as layers of successive settlements forming a tell (human-made mound), while Pompeii was preserved almost perfectly under volcanic ash from Mount Vesuvius, providing an intact snapshot of Roman life in 79 CE.","context":["This video says about itself:\nMarch 1, 2013\nTroy (Ancient Greek: Ἴλιον, Ilion, or Ἴλιος, Ilios; and Τροία, Troia; Latin: Trōia and Īlium; Hittite: Wilusa or Truwisa; Turkish: Truva) was a city, both factual and legendary, in northwest Anatolia in what is now Turkey, south of the southwest end of the Dardanelles / Hellespont and northwest of Mount Ida. It is best known for being the setting of the Trojan War described in the Greek Epic Cycle and especially in the Iliad, one of the two epic poems attributed to Homer. Metrical evidence from the Iliad and the Odyssey seems to show that the name Ἴλιον (Ilion) formerly began with a digamma: Ϝίλιον (Wilion). This was later supported by the Hittite form Wilusa.\nA new city called Ilium was founded on the site in the reign of the Roman Emperor Augustus. It flourished until the establishment of Constantinople and declined gradually during the Byzantine era.\nIn 1865, English archaeologist Frank Calvert excavated trial trenches in a field he had bought from a local farmer at Hisarlık, and in 1868, Heinrich Schliemann, wealthy German businessman and archaeologist, also began excavating in the area after a chance meeting with Calvert in Çanakkale. These excavations revealed several cities built in succession. Schliemann was at first skeptical about the identification of Hissarlik with Troy, but was persuaded by Calvert and took over Calvert’s excavations on the eastern half of the Hissarlik site, which was on Calvert’s property. Troy VII has been identified with the Hittite Wilusa, the probable origin of the Greek Ἴλιον, and is generally (but not conclusively) identified with Homeric Troy.\nToday, the hill at Hisarlik has given its name to a small village near the ruins, supporting the tourist trade visiting the Troia archaeological site. It lies within the province of Çanakkale, some 30 km south-west of the provincial capital, also called Çanakkale. The nearest village is Tevfikiye. The map here shows the adapted Scamander estuary with Ilium a little way inland across the Homeric plain.\nBy Owen Jarus, Live Science Contributor:\nAncient Troy: The City & the Legend\nThe name Troy refers both to a place in legend and a real-life archaeological site. In legend, Troy is a city that was besieged for 10 years and eventually conquered by a Greek army led by King Agamemnon. The reason for this “Trojan War” was, according to Homer’s “Iliad,” the abduction of Helen, a queen from Sparta. This abduction was done by Paris, the son of Troy’s King Priam. Throughout the “Iliad” the gods constantly intervene in support of characters on both sides of the conflict.\nTroy also refers to a real-life ancient city located on the northwest coast of Turkey which, since antiquity, has been identified by many as being the Troy discussed in the legend. Whether the Trojan War actually took place, and whether the site in northwest Turkey is the same Troy, is a matter of debate. The modern-day Turkish name for the site is Hisarlik.\nThe idea that the city was Troy goes back at least 2,700 years, when the ancient Greeks were colonizing northwest Turkey. In the 19th century, the idea again came to popular attention when a German businessman and early archaeologist, Heinrich Schliemann, conducted a series of excavations at Hisarlik and discovered treasures he claimed to be from King Priam.\nTroy the legend\nThe Trojan War is believed to have taken place near the end of the Bronze Age. That is around or before 1200 B.C. It took place around the time that a civilization that we call Mycenaean flourished in Greece. They built great palaces and developed a system of writing.\nThe earliest accounts of this war come from Homer, who lived around the eighth century B.C., several centuries after the events took place. They do not appear to have been written down until even later, likely during the sixth century B.C., when a tyrant named Peisistratus ruled Athens.\nHomer’s “Iliad” is set in the 10th year of the siege against Troy and tells of a series of events that appear to have taken place over a few weeks. The story makes clear that the siege had taken its toll on the Greek force sent to recover Helen. The “timbers of our ships have rotted away and the cables are broken and far away are our wives and our young children,” the poem reads (translation by Richmond Lattimore).\nThe war had essentially become a stalemate with the Greeks unable to take the city and the Trojans unable to drive them back into the sea. We “sons of the Achaians [Greeks] outnumber the Trojans — those who live in the city; but there are companions from other cities in their numbers, wielders of the spear to help them,” the “Iliad” reads.\nA number of key events happen in the poem, including a duel between Menelaos or Menelaus), the king of Sparta and husband of Helen, against Paris. The winner is supposed to receive Helen as a prize, ending the war. However, the gods intervene to break up the duel before it is finished and the war continues.\nAnother important duel occurs nears the end of the poem between Achilleus (or Achilles) and a great Trojan warrior named Hektor (or Hector). The Trojan knows that he’s no match for the Greek warrior and initially runs three laps around Troy, with Achilleus chasing him. Finally, the gods force him to face the Greek warrior and he is in turn killed.\nContrary to popular belief, the “Iliad” does not end with the destruction of Troy but with a temporary truce after which the fighting presumably continues. Another Homeric work called the “Odyssey” is set after the destruction of the city and features the Greek hero Odysseus trying to get home. That poem briefly references how the Greeks took Troy using the famous “Trojan Horse,” a gift concealing warriors within.\n“What a thing was this, too, which that mighty man wrought and endured in the carven horse, wherein all we chiefs of the Argives were sitting, bearing to the Trojans death and fate!” reads part of the poem (Translation by A.T. Murray through Perseus Digital Library).\nThe city’s origin\nThe site of Hisarlik, in northwest Turkey, has been identified as being Troy since ancient times. Archaeological research shows that it was inhabited for almost 4,000 years starting around 3000 B.C. After one city was destroyed, a new city would be built on top of it, creating a human-made mound called a “tell.”\n“There is no one single Troy; there are at least 10, lying in layers on top of each other,” writes University of Amsterdam researcher Gert Jan van Wijngaarden in a chapter of the book “Troy: City, Homer and Turkey” (University of Amsterdam, 2013).\nVan Wijngaarden notes that archaeologists have to dig deep to find remains of the first settlement and from what they can tell it was a “small city surrounded by a defensive wall of unworked stone.” Outside the largest gate was a stone with an image of a face, perhaps a deity welcoming visitors to the new city.\nTroy took off in the period after 2550 B.C., “the city was considerably enlarged and furnished with a massive defensive wall made of cut blocks of stone and rectangular clay bricks,” van Wijngaarden writes. He notes that on the settlement’s citadel were houses of the “megaron” type, which contained “an elongated room with a hearth and open forecourt.”\nWhen Heinrich Schliemann excavated this level of Troy in 1873, he discovered a cache of treasure, which he believed belonged to King Priam. “The collection of weapons, gold, silver, electrum, copper and bronze vessels, gold jewellery, including thousands of gold rings, and a range of other objects made of precious materials apparently came to light close to the outer side of the city wall near the building which Schliemann designated as the royal palace,” writes University of Queensland researcher Trevor Bryce in his book “The Trojans and their Neighbours” (Routledge, 2006).\nSome researchers have speculated that these treasures were not found all in one hoard but were rather precious objects, from across the site, which Schliemann gathered over a number of weeks. While Schliemann believed he had found Priam’s treasures it became clear in the following decades that these were a millennium too early for Priam.\n- Earth: The ruins of Troy. Yes, that Troy. (devilsandblacksheep.wordpress.com)\n- Trojan War (ynacatig.wordpress.com)\n- Test Your Knowledge of the Trojan War! (helensodyssey.com)\n- The Iliad (sometimes referred to as the Song of Ilion or Song of Ilium) is an ancient Greek epic poem in dactylic hexameter, traditionally attributed to Homer. Set during the Trojan War, the ten-year siege of the city of Troy (Ilium) by a coalition of Gre (sapphiredice.wordpress.com)\n- The Myth of Troy? (worldsoftheimagination.wordpress.com)\n- The Iliad!! (greatbooksdude.wordpress.com)\n- Greek Legends (egrejeen.wordpress.com)\n- Myths & Legends: Cassandra’s Cruel Fate (worldsoftheimagination.wordpress.com)","Where is Pompeii Located?\nPompeii was one of the most vibrant, culturally rich, and populated cities of the Roman Empire.\nPompeii was destructed by a volcanic eruption…\nIt is believed to have been founded in the sixth century BCE by the Oscans. The Oscans or the Osci were the natives of southern Italy who spoke the Oscan language. The city was annexed by the Romans in the fourth century BCE and it remained a very important city until its destruction. Archaeologists believe that the city of Pompeii was located in the region of Campania, approximately 14 miles from Naples, in Italy. Its location at the southeastern base of one of the most active volcanoes of Europe – Mount Vesuvius – was its undoing. The eruption of Mount Vesuvius in 79 CE destroyed Pompeii and other neighboring communities such as Herculaneum, and Torre Annunziata. Collectively, the three were designated a UNESCO World Heritage Site in 1997.\nPompeii was founded by the Oscan-speaking people of Campania…\nArchaeologists believe that the ancient city of Pompeii was founded by the Oscan-speaking people of Campania. Its location near the mouth of the river Sarnus (Sarno) made it a beautiful ‘resort’ town that prospered by virtue of the great number of tourists that it attracted and due to the trade opportunities, it offered. By the eighth century BCE, the influence of the Greeks was pronounced, and by the following century the Etruscans invaded and settled in Pompeii. By the fifth century BCE King Hieron I of Syracuse conquered Pompeii and neighboring towns as well. Another century later, the entire region of Campania became a part of the Roman confederation. The assimilation of Pompeii into the Roman Empire was not complete, though, and Pompeii along with other Italian city-states launched a revolt against Rome in around 89 BCE. The revolt was quashed and Pompeii became a colony of Rome after the ensuing war.\nPompeii became one of the best-planned towns of Rome.\nThe beauty of Pompeii attracted wealthy vacationers and Roman dignitaries. Local industries prospered and Pompeii became one of the best-planned towns of Rome. Open-air theatres, an arena that could seat about 2,000 people, wide streets, sprawling squares, shops, inns, and taverns became the hallmark of Pompeii. By the time Mount Vesuvius started to spew lava and ash, drowning the city, there were approximately 20,000 people living in the city and in its neighborhood.\nOn August 24, 79 CE, Vesuvius erupted shooting up columns of volcanic gas.\nThe eruption of Mount Vesuvius was not without warning. In 63 CE, a major earthquake struck the region and many parts of Campania were affected. The belief that Pompeii was indestructible, however, continued to attract people. On August 24, 79 CE, Vesuvius erupted shooting up columns of volcanic gas, ashes, and pumice. Those who had read the warning signs a day before had left but over 11,000 people remained in Pompeii only to be killed by the heat and choked by the gases. The eruption that destroyed Pompeii, however, preserved in its ruins, near-perfect condition to be discovered nearly 17 centuries later (excavation worked in Pompeii began in 1748).\nRelated Maps of Articles:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:4c0c89ec-f96f-4631-b081-9416bb4990d4>","<urn:uuid:0ffd2ed5-f815-47d9-af60-e0790bd6b800>"],"error":null}
{"question":"Soy diabético y necesito saber how to get enough potassium while keeping my sugar intake low?","answer":"There are several low-sugar, potassium-rich foods you can choose from. White beans are an excellent option with 1,189mg per cup, plus they provide 20g of protein and 13g of fiber. Swiss chard is another great choice, offering 961mg per cooked cup. You can also opt for canned salmon (487mg per 5-ounce can), which is also rich in omega-3 fatty acids. Other options include plain yogurt (573mg per cup - look for ones with live active cultures), and unsweetened beet chips. Just ensure to choose unsweetened versions of foods like dried apricots and coconut water to avoid added sugars.","context":["“Because every nutrition is important.”\nGood nutrition is an important part of leading a healthy lifestyle. Combined with physical activity, your diet can help you to reach and maintain a healthy weight, reduce your risk of chronic diseases (like heart disease and cancer), and promote your overall health. When you think about all of the nutrients your body needs, your mind might jump to protein, fiber, calcium, vitamin D, or even omega-3s. But what about potassium? The essential electrolyte apparently gets swept to the avocations.\nWhat is the importance of potassium in our body?\nPotassium helps your nerves and muscles communicate with one another, moves other nutrients into your cells, and keeps your sodium levels in check? Not getting enough of the stuff can cause high blood pressure (thanks to its close relationship with salt) and increase your risk of kidney stones, according to the National Institutes of Health.\nWe are here with the list of foods which provides a good amount of potassium to our body. But if you default to bananas, not so fast. While each medium banana has 422 mg of the mineral maybe about 9 percent of your 4,700 mg recommended daily value (DV) but you can easily find more in other fruits and vegetables.\nWe are here with the list of 16 foods that has more potassium than a banana:\nRegular plain yogurt (not the Greek stuff) has an impressive 573 mg (12% DV) of potassium per cup. But, it should not be taken in the breakfast as yogurt and curds are not good to take in empty stomach. Plus, it packs nearly half your daily calcium needs. Look for one that contains live active cultures, so you’ll get a nice dose of gut-friendly probiotics, too.\n2. Butternut Squash\nOne cup of this slightly sweet fall favorite packs 582 mg (12% DV) of potassium. You’ll also get a hefty dose of vitamin A, along with some vitamin C, magnesium, folate, and calcium.\n3. Swiss Chard\nOne cup of cooked chard has a whopping 961 mg (20% DV) of potassium. These hearty greens also pack calcium, iron, and vitamins A, C, and K.\n4. White Beans\nWhite beans might be the best source of potassium in the grocery store: A single cup has a whopping 1,189 mg. That’s a full quarter of what you need every day. That same 1-cup serving also packs an impressive 20 grams of protein and 13 grams of fiber.\n5. Tomato Sauce\nThis plain old pasta topper is a secret source of potassium, with 728 mg (15% DV) in each cup. Tomatoes are also rich in lycopene, a disease-fighting plant pigment that gives certain fruits and vegetables their signature red hue. Look for a low-sugar tomato sauce sold in BPA-free packaging.\n6. Canned Salmon\nCanned salmon is a lazy cook’s dream. Pop open one 5-ounce can and you’ll get 487 mg (10% DV) of potassium. What’s more, salmon is rich in omega-3 fatty acids, which are essential fats for your eye, heart, and brain health that your body can’t make on its own. Salmon is also high in B vitamins, which aid in the production of red blood cells and convert the food you eat into energy. On top of that, salmon is a great source of lean protein—perfect for those trying to lose weight or build muscle.\nNosh on two refreshing watermelon wedges, and you’ll get 641 mg (14% DV) of potassium. Watermelon is also a great source of lycopene, as well as vitamins A, C, and B6. Plus, more than 90 percent of the fruit is water, so you’ll feel full after snacking for very little calories. And if you rather sip the stuff? Cold-pressed watermelon juice is a great alternative.\n8. Black Beans\nChances are you’re already buying canned black beans for a boost in fiber and protein—two nutrients that keep you feeling full longer. However, they’re also a great source of potassium. Eat one cup and you’ll get 739 mg (16% DV) of the mineral. Black beans also offer some calcium, magnesium, and folate.\n9. Frozen Spinach\nAdd 1 cup of frozen spinach to your next stir-fry or pasta dish and you’ll get a respectable 540 mg (11% DV) of potassium. Spinach is also rich in magnesium, vitamin A, and calcium. Bonus: It’s crazy inexpensive—usually much cheaper than fresh veggies.\n10. Sweet Potato\nA medium baked sweet potato has 542 mg (12% DV) of potassium. These tubers are also rich in vitamin A for your eyes, vitamin C for your skin, and gut-filling fiber. They also just happen to be ridiculously tasty.\n11. White Potato\nSurprise, surprise: A single medium baked potato has 941 mg (20% DV) of potassium. You’ve probably been conditioned to fear these spuds, but when prepared the right way (baked or boiled instead of deep fried), they’re low in calories, fat, and sodium. Plus, white potatoes offer a healthy dose of vitamin C and magnesium, too. Let your spud cool before you eat it and you’ll get a dose of gut-friendly resistant starch.\nA cup of cooked, sliced beets delivers 518 mg (11% DV) of potassium, while a 1-ounce serving of beet chips has an impressive 90 mg. One snack to try: Rhythm Superfoods Naked Beet Chips. The sweet root vegetables are super versatile, though, and can be used in everything from salads to juices to soups.\nAvocados provide a whopping 507 mg of potassium per 3.5 ounces. Moreover, they are a great source of healthy fats and fiber. Avocados lend a nice creaminess to recipes. You can enjoy it over toast, create a delicious pasta sauce, or whip it into a flavorful salad dressing.\n14. Coconut Water\nStore-bought coconut water packs a powerful punch of potassium, delivering about 350 mg per 8 fluid ounces. It makes a great alternative to sugary sports drinks and a delicious base for post-workout smoothies. Just be sure to buy the unsweetened versions to avoid added sugar.\n15. Dried Apricots\nDried apricots supply 430 mg of potassium per 6-piece serving, giving you a big nutritional bang for your buck. Remember to choose unsweetened versions at the grocery store to avoid loading up on extra sugar. We like to chop dried apricots and incorporate them into homemade granola bars and trail mixes.\nWhole soybeans are one of the world’s greatest sources of plant-based protein, but that’s not the only trick up their sleeve: 1 cup also supplies 676 mg (14% DV) of potassium. Eat them as a snack, toss ’em in a salad, or serve them up as a side dish.\nThere’s a reason athletes love to have beetroot juice recently, researchers concluded that drinking the stuff 90 minutes before your workout could boost performance (Just don’t freak out if they turn your pee pink or red afterward. It’s totally normal, we swear). Your food choices each day affect your health — how you feel today, tomorrow, and in the future.\nCover Image: Source"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:6c1cfd27-aa8f-44ff-b2f7-308aebb528a6>"],"error":null}
{"question":"I recently moved to Vermont and I'd like to know if there are any Hindu temples nearby. How does a typical Hindu temple contribute to community life?","answer":"While Vermont currently does not have a permanent Hindu temple, the Vermont Hindu Temple Inc. (VHT) was formed in June 2015 by Bhutanese-Nepali refugees. Currently, many Hindu residents travel as far as Albany, New York (a three-hour drive) to visit temples. Hindu temples serve multiple purposes - they are centers for religious, cultural, educational, and social activities. They help aspirants in their journey to enlightenment and liberation, and are maintained through donations from patrons. Temples are designed according to specific principles outlined in Agama and Vastu shastra texts to help focus human energy on the Divine, and they serve as places where people can transcend worldly concerns to connect with God.","context":["Vermont Hindu Temple Inc. (VHT) was officially formed in June 2015 as a Vermont non-profit public benefit membership organization founded by a group of Bhutanese-Nepali refugees to celebrate and promote Hindu culture and values in Vermont. VHT acquired it’s 501(C)(3) status in August 2015.\nThe strong need of establishing a Hindu temple was felt owing to Hindu cultural belief as expressed in great Hindu epic “Shreemadh Bhagavata” that a community without a temple of the God would make the living of the people difficult and spiritually hollow. Members of Hindu Bhutanese –Nepali community resettled from Nepal as Bhutanese refugees travel frequently as far as Albany, New York, a three hour drive (one way) from Burlington, Vermont, to visit and pray in the temple because this is the nearest temple from where they are currently living. As Vermont currently does not have a permanent Hindu temple in the state, it became a necessity for all Hindus living in Vermont to establish a spiritual base for the betterment and self-fulfilling life for each of them. VHT has certainly become the “spiritual home” of all Hindus living in\nOther reasons for having a community space also include engaging Bhutanese folks, who are compelled to stay idle at home without adequate mobility due to language, transportation, cultural and other barriers, and are in need of a place for get-togethers, for all community members to share their day to day matters.\nWHO WE ARE\nGiven the current context of as to who can be contacted in Bhutanese community, the community space would also become a contact point for all service providers to spread out their information as well as provide them an opportunity to know the culture of the Bhutanese Nepali community.\nTraditionally Bhutanese-Nepalis have always lived in a close proximity to each other and many of them long to meet and have fun jointly. However, this culture of togetherness has been broken now in the aftermath of the resettlement as a consequence of many barriers including language, transportation, weather, work and complications of city life such as roads, traffics and look alike houses.\nTherefore, the community space will reinvigorate their lost culture of having fun together and make the resettlement transition smooth, enriching and spiritually self-fulfilling.\nWe are a home away from home for everyone who wants to learn more about us. We welcome everyone who is interested in learning more about Hinduism and culture. We have priests who have had advanced training who will be able to provide various types of religious and cultural services. Contact us to learn more about us.\nVERMONT HINDU TEMPLE\nThe Seventh Annual Holi Festival of Colors was held in downtown Burlington outside St. Joseph's School on Saturday afternoon. Participants threw colored powder on each other to celebrate spring, love and peace. The colorful crew marched down Church Street for the first time this year, inviting bystanders to participate and yelling \"Happy Holi!\" The event was organized by the Vermont Hindu Temple with support from Spectrum Youth and Family Services.","From Hindupedia, the Hindu Encyclopedia\nTemples are places of focus for all aspects of life - religious, cultural, educational and social. A temple allows visitors to transcend the world of man and to connect with God.\nA temple is a place where you can approach God and realize divine knowledge. Temples are constructed to help aspirants in their journey to enlightenment and liberation. The principles of design and construction and the forms of architecture and decoration were all designed with this principle in mind. All of these are described in detail in the Agama and the Vastu shastra texts. These texts describe how architecture and decorations should be created to help focus human energy on the Divine. Vastu Shastra has a section explicitly devoted to the design of temples.\nTemple are designed to dissolve the boundaries between man and the divine. A temple is not considered to simply be the abode of God, but also is God. God and therefore by implication the whole universe is identified with the temple's design and actual fabric. The ground plan, site location (and its relation to shade and water), its vertical elevation relating to mountains, etc are all important aspects to a temple.\nThe Puranas state that \"The gods always play where groves are near rivers, mountains and springs\". Sacred sites are therefore usually associated with water, shade and lakes are often considered to be sacred and certain lakes have healing and purifying powers. Certain rivers, such as the Ganga, have descended from the heavens and their sacred waters are needed in the temple tank.\nCaves are places of great sanctity. Most of the earliest surviving shrines are rock cut caves. In later temples the garbagriha was designed to resemble a cave and as such was small and dark and the surfaces of the walls were unadorned and massive. The garbagriha is a place that encourages meditation which is possible only in solitude. Approaching the shrine is a movement from open spaces to a confined small space; from light to darkness, from a profusion of visual form and decoration to the visual simplicity of the cave. From this sanctuary the implied movement is vertical, to the symbolic mountain peak directly above the image of the god. This movement upwards is linked to the idea of enlightenment which is identified with the crowning finial of the temple - the amalaka or sikara.\nThe concept of spirituality in the system of sacred architecture in India is something that goes beyond the mere static relations between inert objects and space as found in other architectural traditions. The relationship of objects with one another and space in India's sacred architecture extends to include higher entities said to be in charge of various aspects of universal affairs, all of whom carry out their work in accordance with the will of God.\nMost ancient stone temples were the result of royal patronage and built to benefit of the whole community, they were expressions of the devotion and piety of the ruler and his people.\nThe temples were maintained through donations from royal patrons and private individuals. They were given money, gold, silver, livestock and income from grants of land which sometimes included whole villages.\nHow temples shape and transform energy\nThe purpose of all rituals is directed towards reshaping of human psyche, transformation of individual perception into universal perception, and radical changes in personal thoughts, desires, and ambitions. The space or sky is reshaped in temple architecture through domes, pyramids, various shapes and forms to provide maximum rhythmic response to achieve the desired results.\nAll religious rituals have a definite aim - transformation of the lower energy formats in the human being into the energy form of the outer spaces, or the sky, or the universal being that is immensity. A suitable medium is provided by the sky as shaped by the domes and pyramids. A deity in the temple is a medium to absorb all the individual desires suitably transformed by rituals. The Deep Mala (fire pillar) in the line of the deity's vision in the outer space of the temple serves as the bridge linking the inner vessel of collective desires represented by the deity with outer space or sky that is universal immensity. This fire pillar has a characteristic shape, which points towards the sky. A divine fire, which is the purifying factor in the temples, is lit using ghee made from cow's milk. Through this fire circulates a rhythmic ascending energy form. Deity's vision is normally aligned to the North or the East directions, which are the sources of Jaivik Urja or positive energies.\nTemple architecture represents the concept of evolution and radical changes. The complex energy forms and finer elements are intertwined with deities, trees, plants, colors, shapes and forms in the temple architecture. Different deities in the temple represent body, mind, intellect and the sub-components. These deities are then linked to the cosmos by associating them with specific directions. This philosophy establishes a chain of relationships between micro level elements and the macro level existence. The mandalas available in temples are essentially charts of existence, transformations, and energetic.\nTemple Architecture and Pranayam\nPavan or wind is the bridge for the mind to ascend to the sky. The holistic concept of evolution is defined in terms of the medium - the wind. Wind represents both, the Sound and the sky. Therefore, primordial sounds are the keys to reinforcing the bond between the mind and the sky. Controlling the wind element at individual level is called pranayam. We can say that the temple architecture provides a natural stage of pranayam, not with any definite individual efforts, but through various forms, shapes, rituals, and sounds. These parameters establish a unique path for correlating the wind and the sky. Domes and pyramids in the temple transform the sound into the mandalas. The echo of this rhythmic primordial sound takes the wind to the sky. The Gurutatva is described as 'Akhand Mandalakaram'. This means that the rhythmic mandalas created by the echo of primordial sounds activate the gurutatva in the human mind.\nIn other words, temple architecture creates a space for holistic atmosphere of natural Pranayam suitable for any individual. Echo of primordial sounds enters the limitless finer circles beyond the audible range and helps the mind to transcend maya to reach the Absolute.\nTemple Design Manuals\nThe Agamas and the Vastu Shastra texts are the scriptural authorities on temple architecture. They give precise details and formulas prescribing how to design, carve and assemble a temple. The resulting structure and its relationship with its surroundings create a subtle, sublime atmosphere in which ceremonies performed by priests easily lift the veil between this world and the world of the Gods and Devas so their blessings can pour forth to gathered devotees.\nIn fact, temple architecture is a specialized subject in Vaastu Shastra. Right from selection of site, to defining the dimensions of the structure, to placement of water source or pond and deep mala, to determining the exact form and proportions of the idol is described in great detail.\nThe temple should be built at a suitable place, like a Tirtha. The ideal location is a a beautiful place where rivers flow, on the banks of a lake or by the seashore; on hill tops, mountain slopes, or in a hidden valley. The site of the temple may be selected in a forest, a grove, or in a beautiful garden. Temples should also be built in villages, towns and cities or on an island, surrounded by water.\nThe temple itself should always face east since that is the most auspicious direction. From the east appears the rising sun, the destroyer of darkness and the giver of life.\nSignificance of Prakrima\nWhen in a temple, it is customary to circumambulate the deity.\nWhen a temple is established and life is infused into the deity through a proper pran pratistha ceremony, divinity enters the deity. This divinity is in the form of magnetic waves starting from the central point of the base of the deity and spreads around in a circle. The vibrations are the strongest near the deity and gradually weaken as the circle becomes larger. The positive vibrations influence a person walking around the deity.\nThe magnetic field moves in a clockwise direction, therefore, it is essential that one walks around the deity clockwise. By moving along the magnetic field of the deity one can benefit from the positive vibrations one receives. These vibrations are a blessing that strengthen the worshipper and protect him/her from all kinds of problems and calamities. After completing prayers and offerings, it is therefore customary to walk around the deity.\nThe longer one walks around the deity, the greater the benefit from the vibrations. It is customary to walk 5 to 11 times around a deity.\nReligious texts direct that when going around the deity of Shankar one should not cross the line where the offering of milk and water flows. For this reason, one takes only half a round around Shankar. One returns and then does the other half because the vibrations around Lord Shankar move both clockwise and anti-clockwise.\nWhen one walks anti-clockwise, the divine vibrations that move clockwise counter the individual's personal vibrations, gradually destroying them. Therefore, the anti-clockwise movement is prohibited. The harmful effects vary according to the Deity being circumambulated.\nThere are four kinds of rituals conducted in a temple: nitya (daily), naimittika (occasional), kamya (optional) and prayaschitta (expiation).\nMore about Temples\nTemples throughout India\nTemples of Andra Pradesh\nTemples of Karnataka\nTemples of West bengal\n- Kalighat Temple\n- Dakshineshwar Temple\nTemples of Gujarat\n- Somnath Temple\n- Dwarkadhish Temple\n- Dakor Temple\nTemples of Jammu & Kashmir\n- Vaishno devi Temple\nTemples of Tamil Nadu\n- Thiru Aayarpaadi Sri Kari Krishna Perumal\n- Pallikondeswara Swamy Surutapalli Devasthanam\n- Vasishteswaraswamy Temple\n- Sri Naganathaswamy Temple Thirunageshwaram\n- Sri Balasubramanya Swamy Temple Ayikudi\n- Sri Dharbarenyeswarar Temple Thirunallaru\n- Sri Swedaranyeswarar Temple Thiruvengadu\n- Sri Kalyanavaradharaja Perumal Temple Paruthiyur\n- Karungaali Sri Chinthaamaneesarar\n- Thiruverkaadu Sri Karumaari Amman\n- Thiruverkaadu Sri Vedhapureeswarar\n- Kettavarampalayam Sri Ramar\n- Madhuraanthakam Eri Kaatha Sri Raamar\n- Vallakkottai Sri Subramanyar\n- Thiruninravur Sri Hridhayaaleeswarar\n- Thiruninravur Sri Bhakthavatsala Perumal\n- Thirumazhisai Sri Othaandeswarar\n- Thirusoolam Sri Thirusoolanaadhar\n- Singaperumal Koil Sri Ugra Narasimhar\n- Chettippunyam Sri Varadharaja Perumal\n- Thirukkachur Sri Kachabeswarar\n- Thirukkachur Sri Oushadheeswarar\n- Hanumanthapuram Sri Agora Veerabadhrar\n- Chenganmaal Sri Chenganmaaleeswarar\n- Thiru Idaichuram Sri Gnanapureeswarar\n- Sembakkam Sri Jambugeswarar\n- Cherappanancheri Sri Veemeeswarar\n- Thirumalai Vaiyaavoor Sri Prasanna Venkatesa Perumaal\n- Ezhuchur Sri Nallinakkeeswarar\n- Ariyathurai Sri Varamoortheeswarar\n- Thiruvidandhai Sri Nithya Kalyaana Perumaal\n- Kolappaakkam Sri Agatheeswarar\n- Somangalam Sri Somanaadheeswarar\n- Gerugambaakkam Sri Neelakandeswarar\n- Porur Sri Ramanaadheswarar\n- Kunrathur Sri Naageswarar\n- Maangaadu Sri Velleeswarar\n- Poondhamalli Sri Vaidheeswarar\n- Kovur Sri Sundhareswarar\n- Maangaadu Sri Vaikunda Perumaal\n- Nandhambakkam Sri Kothandaramar\n- Pozhichalur Sri Agatheeswarar\n- Aathur Sri Muktheeswarar\n- Koyambedu Sri Kurungaaleeswarar\n- Semmancheri Sri Srinivasa Perumaal\n- Vyasarpadi Sri Raveeswarar\n- Pon Vilaindha Kalathur Sri Munkudumeeswarar\n- Ponpadhar Koodam Sri Chathurbuja Kothandaraamar\n- Manimangalam Sri Dharmeswarar\n- Manimangalam Sri Kailaasanaadhar\n- Vallam Sri Vedhaantheeswarar and Sri Giri Varadharaaja Perumaal\n- Kaaladi Sri Aadhi Sankarar"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:8510c8ee-225a-48e9-ad83-52d63305bed8>","<urn:uuid:86e43fe2-f969-42a4-9f5b-1261888b7da2>"],"error":null}
{"question":"What environmental risks were documented for the chemical GenX, and how can green infrastructure systems help protect ecosystem health?","answer":"GenX posed substantial environmental risks according to 16 reports filed by DuPont between 2006-2013, showing harmful effects on animals' liver, kidneys, immune responses, and reproductive systems. Green infrastructure helps protect ecosystem health in multiple ways: it provides habitat for birds, mammals, amphibians, reptiles, and insects, reduces erosion and sedimentation that affects stream habitats, and helps facilitate wildlife movement between habitats. Even small features like green roofs can provide habitat for insects and birds. Additionally, green infrastructure helps mitigate climate change impacts by increasing drainage system capacity and reducing urban heat island effects.","context":["“The chemical introduced by DuPont in 2009 to replace the surfactant PFOA causes many of the same health problems in lab tests that the original chemical did, including cancer and reproductive problems, according to documents obtained by The Intercept. PFOA, also known as C8, was a key ingredient in Teflon.\nC8 was originally manufactured by 3M, then by DuPont, and was phased out after a massive class-action lawsuit revealed evidence of its health hazards. The new chemical, sold under the name GenX, is used to make Teflon and many other products. While touting GenX as having a ‘more favorable toxicological profile’ than C8, DuPont filed 16 reports of ‘substantial risk of injury to health or the environment’ about its new chemical. The reports, discovered in the course of an investigation by The Intercept, were filed under Section 8 (e) of the Toxic Substances Control Act (TSCA) and submitted to the EPA between April 2006 and January 2013. They cite numerous health effects in animals, including changes in the size and weight of animals’ livers and kidneys, alterations to their immune responses and cholesterol levels, weight gain, reproductive problems, and cancer…\nIn one experiment, rats given various amounts of GenX over two years developed cancerous tumors in the liver, pancreas, and testicles, according to a report DuPont submitted in January 2013. In addition to the cancers, some of the GenX-exposed rats in that experiment also developed benign tumors, as well as well as kidney disease, liver degeneration, and uterine polyps.\nSatheesh Anand, a DuPont senior research toxicologist who signed the report, concluded that ‘these tumor findings are not considered relevant for human risk assessment.’ Anand dismissed the significance of the results in part by emphasizing that the mechanism associated with the tumor formation in rats might not be the same in humans. DuPont scientists have long made the very same claim about C8 (as in this study from 2004), which caused testicular tumors in DuPont experiments on lab animals before it was linked to testicular cancer in exposed people.\nAlan Ducatman, a physician who studies the health effects of perfluorinated chemicals such as C8 and GenX, characterized Anand’s claim as ‘a partial argument that could be interesting only to those who are not strongly following the literature.’ Ducatman, who teaches environmental health sciences at the West Virginia University School of Public Health, is one of several researchers familiar with the health effects of C8 who reviewed the documents for The Intercept. He summarized DuPont’s interpretation of its own data as ‘cherry picking,’ highlighting ‘species differences only when arguing that a problematic study finding is not relevant.’\nDuPont’s reporting on the hazards of GenX ‘all has an eerie echo,’ Ducatman wrote in an email to The Intercept. He noted that the reports show the chemical has the same trio of biological effects — on the liver, immunity, and the processing of fats — seen with similar chemicals, including C8. ‘This reminds me a lot of a path we have recently traveled. That journey is not ending well.’\nDespite the fact that Section 8(e) reports are required only when companies have evidence that their product presents a substantial risk, the summaries of the experiments written by DuPont employees downplay most of their findings…\nGenX also affected reproduction in lab animals, according to the reports. Rats exposed to higher amounts of the chemical were more likely to give birth early and have babies that weighed less. Another study showed that female rats exposed to GenX reached puberty later than the unexposed animals.”\nRead the full article by Sharon Lerner.","Green infrastructure is a cost-effective and resilient approach to our water infrastructure needs that provides many community benefits.\nWater Quality and Quantity\nWater Quality: Stormwater from urban areas delivers many pollutants to our streams, lakes, and beaches - including pathogens, nutrients, sediment, and heavy metals. In cities with combined sewer systems, high stormwater flows can also send untreated sewage into our waters. By retaining rainfall from small storms, green infrastructure reduces stormwater discharges. Lower discharge volumes translate into reduced combined sewer overflows and lower pollutant loads. Green infrastructure also treats stormwater that is not retained.\nFlooding: Conventional stormwater infrastructure quickly drains stormwater to rivers and streams, increasing peak flows and flood risk. Green infrastructure can mitigate flood risk by slowing and reducing stormwater discharges.\nWater supply: Rainwater harvesting and infiltration-based practices increase the efficiency of our water supply system. Water collected in rainwater harvesting systems can be used for outdoor irrigation and some indoor uses and can significantly reduce municipal water use. Water infiltrated into the soil can recharge groundwater, an important source of water in the United States.\nPrivate and Public Cost Savings: When stormwater management systems are based on green infrastructure rather than gray infrastructure, developers often experience lower capital costs. These savings derive from lower costs for site grading, paving, and landscaping, and smaller or eliminated piping and detention facilities. In cities with combined sewer systems, green infrastructure controls may cost less than conventional controls, and green-gray approaches can reduce public expenditures on stormwater infrastructure.\nGround Level Ozone: Ground level ozone or smog, is created when nitrogen oxides (NOx) and volatile organic compounds (VOCs) interact in the presence of heat and sunlight. Smog conditions are usually worst in the summer and can lead to respiratory health problems. Vegetation can reduce ground level ozone by reducing air temperatures, reducing power plant emissions associated with air conditioning, and removing air pollutants.\nParticulate Pollution: Particulate matter refers to the tiny bits of dust, chemicals, and metals suspended in the air we breathe. Because particulate matter is so small, it can enter into the lungs and cause serious health effects. Trees, parks, and other green infrastructure features can reduce particulate pollution by absorbing and filtering particulate matter.\nHealth Effects: Breathing ground level ozone and particulate pollution can cause respiratory ailments including chest pain, coughing, aggravation of asthma, and even premature death. In their triple bottom line study on the benefits of green infrastructure, the City of Philadelphia found that increased tree canopy would reduce ozone and particulate pollution levels enough to significantly reduce mortality, hospital admissions, and work loss days.\nEnergy and Climate Change\nUrban Heat Island: Urban heat islands form as cities replace natural land cover with dense concentrations of pavement, buildings, and other surfaces that absorb and retain heat. Trees, green roofs, and other green infrastructure features can cool urban areas by shading building surfaces, deflecting radiation from the sun, and releasing moisture into the atmosphere.\nEnergy Use: By reducing local temperatures and shading building surfaces, green infrastructure lessens the cooling and heating demand for buildings, reducing energy needs and decreasing emissions from power plants.\nClimate Change: As different parts of the country become drier, wetter, or hotter, green infrastructure can help communities adapt to climate change by increasing the capacity of drainage systems to handle large storms, increasing the resilience of water supply systems in times of drought, and mitigating the urban heat island effect. Urban vegetation can also mitigate climate change by reducing the levels of greenhouse gases in the atmosphere.\nWater/Energy Nexus: Treating and moving drinking water and wastewater takes a lot of energy. By reducing stormwater inflow into sewer systems, recharging aquifers, and conserving water, green infrastructure can significantly reduce energy use.\nHabitat and Wildlife\nHabitat Improvement: Vegetation in the urban environment provides habitat for birds, mammals, amphibians, reptiles, and insects. Even small patches of vegetation such as green roofs can provide habitat for a variety of insects and birds. By reducing erosion and sedimentation, green infrastructure also improves habitat in small streams and washes.\nHabitat Connectivity: Large scale green infrastructure, such as parks and urban forests, also help to facilitate wildlife movement and connect wildlife populations between habitats. Learn how Loxahatchee, Florida is protecting the local watershed and conserving native ecosystems through the Loxahatchee Regional Greenways System.\nGreen jobs: Green infrastructure can reduce a community’s infrastructure costs, promote economic growth, and create construction and maintenance jobs. As demand for green infrastructure skills increases, a range of new training and certification programs has emerged.\nHealth Benefits: More green space and parks encourages outdoor physical activity, reducing obesity and preventing associated chronic diseases such as heart disease, high blood pressure, stroke, Type II diabetes, arthritis, and certain kinds of cancer.\nRecreation space: Green infrastructure’s vegetation and trees can increase publicly available recreation areas, allowing urban communities to enjoy greenery without leaving the city. Additionally, green infrastructure’s vegetation and permeable pavements can reduce noise pollution by damping traffic, train, or plane noise.\nProperty values: By utilizing green infrastructure in construction and increasing vegetation and tree cover, green infrastructure can increase property values"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:429eab53-3cc2-4ecc-b6ad-17554fd58d79>","<urn:uuid:9cb7dab0-7fea-445e-bbe3-af81dae72270>"],"error":null}
{"question":"Could you explain why AV nodal blocking agents are contraindicated in WPW patients with atrial fibrillation, but recommended for WPW patients with orthodromic tachycardia?","answer":"In WPW with atrial fibrillation, AV nodal blocking agents (like verapamil and digoxin) are contraindicated because they decrease conduction through the His-Purkinje system while enhancing conduction across accessory pathways, which can increase ventricular rate and potentially cause ventricular fibrillation and sudden cardiac death. However, in orthodromic tachycardia, AV nodal blockers are effective because the electrical circuit travels normally through the AV node first, making the mechanism similar to regular SVT where these medications are standard treatment.","context":["br Conflict of interest br Case presentation A year\nConflict of interest\nCase presentation A 64-year-old female presented to the emergency room with persistent dizziness that began early that morning. She had a 30-year history of palpitations and known electrocardiogram (ECG) abnormalities. Her pulse was irregular at 130–270min−1 and blood pressure was 90/40mmHg. There were no other abnormalities on physical examination. A chest X-ray was normal and without cardiomegaly. Echocardiography revealed normal left ventricular contraction without structural abnormalities. Blood chemistry evaluation, including electrolytes and cardiac enzymes, was within normal limits. Twelve-lead ECG showed irregular wide QRS tachycardia of various configurations (Fig. 1).\nCommentary The patient\\'s ECG shows wide QRS tachycardia with a rate from 150 to 300beats/min. The RR intervals are irregularly irregular and no P wave preceding QRS was confirmed. The QRS complex has beat–beat variations in morphology. At this point, some of the arrhythmias that should be considered are: (1) irregular ventricular tachycardia (such as torsades des pointes or polymorphic ventricular tachycardia); (2) atrial fibrillation (AF) with bundle branch block; (3) AF in Wolff–Parkinson–White (WPW) syndrome (pseudo-ventricular tachycardia). The QRS complex shows right axis deviation during tachycardia with no progressive change in cardiac axis, thus polymorphic ventricular tachycardia and torsades des pointes associated with QT prolongation are excluded. Polymorphic ventricular tachycardia associated electrolyte abnormality and myocardial ischemia could not be totally excluded by this ECG, however laboratory examination did not indicate myocardial ischemia or an electrolyte abnormality. The representative arrhythmia of the patient\\'s irregular Prostaglandin E2 manufacturer rate is AF, which is the most common cardiac arrhythmia. When the ventricular rate is slow, AF can be easily diagnosed. However if the ventricular rate is rapid, AF is difficult to appreciate because QRS complexes are clustered together and fibrillatory waves become more difficult to evaluate. The QRS complexes during AF with bundle branch block usually do not have beat–beat variation, and both an Rr′ pattern in lead V1 and R\natrioventricular (AV) node and accessory pathways.\nTo terminate AF and block the accessory pathway, procainamide was given intravenously at a dose of 400mg over 30min. It is important to note that AV nodal blocking agents are contraindicated in treatment of AF in patients with WPW syndrome. AV nodal blocking agents, for example verapamil and digoxin, decrease the number of impulses conducted through the His–Purkinje system and enhance conduction across accessory pathways, thus increasing the ventricular rate during AF. If the patient is hemodynamically unstable, electrical cardioversion should be performed. After infusion of procainamide, this patient\\'s AF converted to normal sinus rhythm (Fig. 2) that included a short PR interval and a delta wave. A positive delta wave and QRS in V1 indicated a left sided accessory pathway.\nExplanations of the arrhythmia AF is the most serious arrhythmia associated with WPW syndrome. This arrhythmia has the possibility of degenerating into ventricular fibrillation, which can cause sudden cardiac death. Although the occurrence of ventricular fibrillation is rare, even among symptomatic patients with WPW syndrome, AF carries the potential for sudden cardiac death. The risk of sudden cardiac death in patients with WPW syndrome is related to the short effective refractory period of the anterograde accessory pathway. Electrophysiological studies are performed to measure the refractory period of the accessory pathway. Patients with an accessory pathway with an effective refractory period of less than 250ms, as measured by an R–R interval between 2 preexcited complexes of <250ms during AF, are defined as high risk.","Wolff-Parkinson-White (WPW) syndrome is a congenital cardiac abnormality that manifests itself as a conduction irregularity found between the sinoatrial (SA) and atrioventricular (AV) nodes. In an otherwise healthy heart, electrical conduction begins at the SA node, which is located in the right atrium. An electrical pulse is then sent downward causing the atrium to contract and subsequently reaching the AV node, which acts as the connecting catalyst to allow the electrical pulse to reach the ventricles causing them to contract.1This pathway is controlled as electrical conduction is regulated by the SA and AV node. However, in a patient with Wolff-Parkinson-White syndrome, there is an extra pathway—also known as the accessory pathway—that allows conduction to occur directly between the atrium and ventricles, bypassing the AV node, thus leading to preexcitation of the ventricles. This preexcitation allows conduction to occur with higher and uncontrolled rates resulting in tachycardias.2 The electrocardiogram (ECG) shows a shortened PR interval and the characteristic “delta wave” which resembles a slurring slow rise of the initial portion of the QRS interval.\nPathophysiology and Treatment:\nIn the long-term preventative setting, WPW is managed surgically through catheter ablation.3 However, in the emergent setting treatment is dictated by specific manifestations of WPW associated arrhythmias. There is WPW with orthodromic tachycardia, WPW with antidromic tachycardia, and WPW with atrial fibrillation.4\n1. WPW with orthodromic tachycardia\nA. Orthodromic tachycardia occurs when the electrical circuit travels normally from the SA node to the AV node and down the Purkinje fibers; however, the circuit reenters the atrium via the accessory pathway causing the tachycardia as represented in Figure 1.\na. This rhythm resembles a supraventricular tachycardia (SVT) even though etiology is slightly different; however, treatment is identical.5\nB. Diagnosis is made through electrocardiogram (ECG).\na. ECG will show a regular narrow QRS complex tachycardia resembling a SVT as seen in Figure 1.6\na. Due to its similar mechanism as SVT, orthodromic tachycardia can be treated the same method as a SVT with an AV node blocker (AVNB).5\n– Adenosine7, 8\n– Verapamil8, 9\n– Beta blockers10\n2. WPW with antidromic tachycardia\nA. Antidromic tachycardia occurs when the electrical circuit travels from the SA node through the accessory pathway first, then up the Purkinje fibers, and through the AV node all through a retrograde or opposite direction than the orthodromic tachycardia as seen in Figure 1.\na. This rhythm resembles a ventricular tachycardia (VT), even though etiology is slightly different; however, treatment is identical.6\nB. Diagnosis is also made through ECG.\na. ECG will show a regular wide QRS complex tachycardia resembling a ventricular tachycardia as seen in Figure 1.6\na. It is necessary to treat with agents that selectively target the accessory pathway.\n– Loading dose: 20 to 50 mg/min IV infusion until arrhythmia suppressed, hypotension ensues, QRS prolonged by 50%, or total cumulative dose of 17 mg/kg12\n– Alternative loading dose: 100 mg every 5 minutes until arrhythmia is controlled or any condition described above is met.12\n– Follow with a continuous infusion of 1 to 4 mg/min (must reduce maintenance dose in patients with renal impairment).12\nc. Amiodarone150 mg IV over 10 minutes, then 1 mg/minute for 6 hours, then 0.5 mg/minute for 18 hours or change to oral dosing.13\nFigure 1. Diagrams of orthodromic and antidromic electrical pathways and associated ECG rhythms. Available at: https://umem.org/files/uploads/content/MattuECG%20Tumblr/OrthoAnti.jpg. Accessed August 4, 2014.\n3. WPW with Atrial Fibrillation\nA. This is the most dangerous etiology of the WPW manifestations due to its high risk of iatrogenic error and deadly ventricular arrhythmias.2\nB. In WPW with atrial fibrillation, electrical conduction in the heart travels down two paths, the normal pathway through the AV node AND from the atrium to the ventricles through the accessory pathway as seen in Figure 2.14\nC. Diagnosis is made through ECG.\na. The two electrical pathways manifest themselves on ECG as irregularly irregular rhythms.\n– While pulses that pass through the AV node have some rate regulation due to the AV node, impulses traveling through the accessory pathway have no rate regulation, leading to measured ventricular rates on ECG above 200 beats per minute (bpm).2\n> This lack of regulation through the accessory pathway causes the irregular wave morphologies with no consistencies in the QRS waves as seen in Figure 3.\n> In contrast, Figure 4 depicts a more typical atrial fibrillation without WPW where the QRS waves are seen to be more regular and ventricular rate does not exceed 150-200 bpm (due to the rate regulation caused by the AV node).\nb. Atrial fibrillation with WPW is often misdiagnosed as a SVT, VT, or atrial fibrillation with a bundle branch block.\n– If misdiagnosed, treatment with an AVNB will preferentially block the AV node and consequently divert all electrical impulses down the accessory pathway.15\n> This shunting of electrical impulses to the accessory pathway causes ventricular fibrillation and high risk of death.\n> Therefore, AVOID ALL AVNBs (i.e. adenosine, non-dihydropyridine calcium channel blockers, beta blockers, digoxin, and amiodarone) in patients with WPW with atrial fibrillation.\na. Immediate cardioversionis the recommended first line treatment for hemodynamically unstable patients when WPW with atrial fibrillation presents.16\nb. In a hemodynamically stable patient, procainamide can be used, as it selectively targets the accessory pathway.12\n– Per 2010 ACLS guidelines, procainamide dosing is as follows:\n> Loading dose: 20 to 50 mg/min IV infusion until arrhythmia suppressed, hypotension ensues, QRS prolonged by 50%, or total cumulative dose of 17 mg/kg12\n> Alternative loading dose: 100 mg every 5 minutes until arrhythmia is controlled or any condition described above is met. 12\n> Follow with a continuous infusion of 1 to 4 mg/min (must reduce maintenance dose for renal impairment).12\nFigure 2. Diagram of WPW with atrial fibrillation with associated ECG. Available at: http://www.rjmatthewsmd.com/Definitions/supraventricular_tachyarrhythmias.htm. Accessed August 9, 2014.\nAvailable at: http://osuemed.wordpress.com/2011/05/26/nightmare-ekg/. Accessed August 4, 2014.\nFigure 4. ECG of atrial fibrillation without WPW.\nAvailable at: http://www.emedu.org/ecg/af.htm. Accessed August 4, 2014.\nA. WPW orthodromic tachycardias are treated as SVT with an AVNB (i.e. adenosine, verapamil, beta blockers).\nB. WPW antidromic tachycardias are treated as VT with procainamide or amiodarone.\nC. WPW with atrial fibrillation is treated with immediate cardioversion if hemodynamically unstable. Procainamide is a reasonable choice in hemodynamically stable patients.\nEdwin Lim, PharmD Class of 2015\nThomas Jefferson University\nJefferson School of Pharmacy, Philadelphia, PA\nRobert Pugliese, PharmD, BCPS (@theEDpharmacist)\nClinical Specialist, Emergency Medicine\nThomas Jefferson University Hospital, Philadelphia, PA\nReviewed by: Craig Cocchio, PharmD, BCPS and Nadia Awad, PharmD, BCPS\n1. Wolff-Parkinson-White Syndrome (WPW). Available at: http://my.clevelandclinic.org/heart/disorders/electric/wpw.aspx. Accessed July 25, 2014.\n2. Sheinman BD, Evans T. Acceleration of ventricular rate by fibrillation associated with the Wolff-Parkinson-White syndrome. Br Med J (Clin Res Ed).1982;285(6347):999-1000.\n3. Jackman WM, Wang XZ, Friday KJ, et al. Catheter ablation of accessory atrioventricular pathways (Wolff-Parkinson-White syndrome) by radiofrequency current. N Engl J Med. 1991;324(23):1605-11.\n4. Josephson ME. Preexcitation syndromes. In: Clinical Cardiac Electrophysiology, 4th, Lippincot Williams & Wilkins, Philadelphia 2008. P.339.\n5. Goy JJ, Fromer M. Antiarrhythmic treatment of atrioventricular tachycardias. J Cardiovasc Pharmacol. 1991;17 Suppl 6:S36-40.\n6. Goldberger AL, Goldberger ZD, Shvilkin A. Clinical Electrocardiography: A Simplified Approach, Expert Consult: Online and Print,8, Clinical Electrocardiography: A Simplified Approach. Elsevier Health Sciences; 2012.\n7. Dimarco JP, Sellers TD, Lerman BB, et al. Diagnostic and therapeutic use of adenosine in patients with supraventricular tachyarrhythmias. J Am Coll Cardiol. 1985;6(2):417-25.\n8. Lim SH, Anantharaman V, Teo WS, et al. Slow infusion of calcium channel blockers compared with intravenous adenosine in the emergency treatment of supraventricular tachycardia. Resuscitation. 2009;80(5):523-8.\n9. Rinkenberger RL, Prystowsky EN, Heger JJ, et al. Effects of intravenous and chronic oral verapamil administration in patients with supraventricular tachyarrhythmias. Circulation. 1980;62(5):996-1010.\n10. Kowey PR, Friehling TD, Marinchak RA. Electrophysiology of beta blockers in supraventricular arrhythmias. Am J Cardiol. 1987;60(6):32D-38D.\n11. Worthley LI, Holt AW. Digoxin in the critically ill patient. Crit Care Resusc. 1999;1(3):252-64.\n12. Neumar RW, Otto CW, Link MS, et al. Part 8: Adult Advanced Cardiovascular Life Support: 2010 American Heart Association Guidelines for Cardiopulmonary Resuscitation and Emergency Cardiovascular Care. Circulation. 2010;122(18 Suppl 3):S729-67.\n13. Amiodarone Package Insert. Available at: http://dailymed.nlm.nih.gov/dailymed/lookup.cfm?setid=cdd50dc7-f712-4248-b0e3-ba247cf08cee#cbb97eba-08e3-4a99-ac87-e4697427b866. Accessed August 22, 2014.\n14. Das MK, Zipes DP. Electrocardiography of Arrhythmias, A Comprehensive Review. Elsevier Health Sciences; 2012.\n15. Schützenberger W, Leisch F, Gmeiner R. Enhanced accessory pathway conduction following intravenous amiodarone in atrial fibrillation. A case report. Int J Cardiol. 1987;16(1):93-5\n16. January CT, Wann LS, Alpert JS, et al. 2014 AHA/ACC/HRS Guideline for the Management of Patients With Atrial Fibrillation: Executive Summary: A Report of the American College of Cardiology/American Heart Association Task Force on Practice Guidelines and the Heart Rhythm Society. Circulation. 2014."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:2a2ea326-67cc-4d75-9ae0-a81fc2f326a4>","<urn:uuid:589bf6cd-d0ad-48e3-86db-ad807367a49c>"],"error":null}
{"question":"What were the key milestones in Alan Shepard's space career, and how do they compare to Leo Ballegeer's final military actions in Luxembourg?","answer":"Alan Shepard became the first American in space aboard the MR-3 (Freedom 7) and later flew on Apollo 14, becoming a millionaire while still in the NASA program. In contrast, Leo Ballegeer's military career ended tragically during the Battle of the Bulge in Luxembourg, where he went missing near Bettendorf on December 21, 1944, and was later confirmed killed in action by shrapnel wounds to his waist.","context":["Leo Deon Ballegeer\n28th Infantry Division, 109th Infantry Regiment\nBefore the War\nOn June 9, 1919, Leo Deon Ballegeer was born in Omaha, Nebraska and grew up as the only child of Camiel Ballegeer and Zulma (Balcaen) Ballegeer. His father, Camiel, worked as a carpenter and later worked for the Fallstaff Brewing Company in Omaha. His mother stayed home, in a house which was owned by the family and cared for her son. Before he was drafted in 1941 – four days before his twenty-second birthday – Ballegeer worked as a contractor at the Peter Kiewit Company in Omaha City.\nBallegeer was a first generation American. Both of his parents were born in Belgium and immigrated before World War I. His mother made the journey in February 1913. At age 24, she travelled with her 21-year-old sister, Bertha Helena Balcaen, to the United States. They left Belgium from Antwerp to arrive approximately ten days later in Ellis Island, New York. Both women settled in Omaha, Nebraska.\nHis father undertook a longer journey to arrive in the United States. He first immigrated to Canada at age 22. Two years later, he entered the United States at Noyes, Minnesota, in December 1915. By 1916, he moved to Omaha and one year later, married Zulma Balcaen on February 3, 1917.\nLeo Ballegeer’s family grew steadily in Omaha. His aunt, Bertha Helena Balcaen, married and started a family of her own. A few months later, Odile Balcaen, his mother’s brother, also arrived in the United States. Just like his two sisters, Odile settled in Omaha and started his own family. After the Armistice in November 1918 and the death of his grandfather in 1919 in Belgium, his grandmother Felicita (Verstraete) Balcaen decided to join the family in the U.S. She was accompanied by her youngest son, Achiel Balcaen, and Felicita and Achiel shared a home in Omaha, Nebraska.\nLeo Ballegeer grew up in a small family home but with a lot of relatives living nearby. He graduated from Omaha South High School in 1936 and became an apprentice carpenter. Just like his father at the time, he worked in construction before being drafted in 1941.\nBallegeer was drafted and reported for duty on June 3, 1941 at the Ness Building Recruiting Station in Salt Lake City, Utah. He was part of the tenth draft call.\nTraining at Camp Roberts\nBallegeer trained at Camp Roberts, California. He entered the U.S. Army as a private but by August 1942 Ballegeer received a promotion to the grade of corporal. He served with the 79th Infantry Training Battalion, Company B.\nDuring this period his girlfriend, Elizabeth Ann Tresnak, moved from Omaha, Nebraska to California to be closer to him. Just like Ballegeer, she graduated from Omaha South High School. They married in 1942 and in the following months a daughter, Nancy Lee, was born.\n“Let the citizens bear arms!”\nIn September 1944, the training period ended for Ballegeer and the U.S. Army called his unit to the European Theater of Operations, more specifically to France. He was assigned to the 28th Infantry Division, 109th Infantry Regiment.\nThe 109th Infantry Regiment adopted the motto, “Cives Arma Ferant,” which means “Let the citizens bear arms.” Many members of this unit served in the National Guard and many others were drafted into service.\nOn July 22, 1944, the 28th Infantry Division landed on Omaha Beach, just six weeks after the Normandy landings. The division fought its way through the hedgerows of the Norman countryside and participated in the liberation parade in Paris in August 1944. The division needed to resupply and train new replacements. Ballegeer was one of those replacement troops.\nIn September 1944, the 28th Infantry Division passed through Belgium and Luxembourg into Germany. The 109th Infantry Regiment became the first troops to invade German territory during World War II. On September 11, 1944, the men seized control of the Our River bridge. Thankfully, they found the structure still intact, despite its proximity to the fighting.\nTwo days later, the 109th and 110th Infantry Regiments overcame heavy opposition from the Germans at the west wall of the Siegfried Line, just west of Großkampenberg, Germany. Heavy German resistance forced the advance to a halt, but the 109th and 110th Infantry Regiments advanced again in October. At the end of the month, the 28th Infantry Division occupied a sector near the German towns of Germeter and Vossenack, just across the border of Belgium and Germany.\nGerman troops fled most of France and Belgium and took a defensive position alongside their country’s borders. This Allied advance from Paris to the Rhine River became known as the Siegfried Line Campaign.\nOn November 2, 1944 the 109th Infantry Regiment received orders to advance north towards the town of Hürtgen, Germany. Ballegeer, as part of Company A, 3rd Platoon, experienced one of the darkest periods for the 28th Infantry Division: the Battle of the Hürtgen Forest.\nThe fighting was fierce in the Hürtgen. Soldiers dealt with extreme weather conditions, the worst recorded in years. It was bitter cold in the woods, roads became inaccessible, and aerial support was limited.\nCaptain Max R. Whitetree of Company A noted afterwards that “the 1st and 2nd platoons had lost a lot of men from mortar fire and machine gun fire.” Whitetree described the fighting conditions in this area, writing “the woods were so thick in this sector that in many cases we were using hand grenades because they were more effective then rifle fire.” The reconnaissance done by the 28th Infantry Division before the battle was limited at best. Whitetree stated that, “Jerry [the Germans] seemed to know exactly where we were at all times.”\nOn November 6, 1944, Ballegeer was awarded a Combat Infantryman’s Badge for exemplary conduct in action against the enemy.\nLack of familiarity with the terrain led to confusion. Whitetree stated on November 11, 1944, “I ran into Sgt Ballegeer and found that he had been able to get all his platoon back into Vossenack. The Sgt then went back and brought all the A Co men he could find and we placed them on the Co objective.”\nDuring the actions in the Hürtgen Forest, Ballegeer showed his leadership skills. When Captain Whitetree received word that the command post was surrounded, he “sent Sgt Ballegeer back with a squad to help them. He killed five Germans and took five PWs [Prisoners of War].”\nMajor General Norman Cota ordered the 28th Infantry Division out of the Hürtgen on November 13, but A Company of the 109th Infantry Regiment was not relieved by the 8th Infantry Division, 28th Infantry Regiment until November 20.\nBetween November 2 and November 13, 1944, the 28th Infantry Division lost more than 5,000 men, pushed an additional 4,500 reserve troops to the front lines.\nBattle of the Bulge\nJust four weeks after the 109th Infantry Regiment exited the Hürtgen Forest, they found themselves in the front lines of the Battle of the Bulge. The regiment headed for Diekirch, Luxembourg and occupied billets in town.\nFor the rest of November and the start of December 1944 the 109th Infantry Regiment occupied a defensive zone along the west bank of the Our River. They were placed in a quiet sector to have time to re-organize and carry out an extensive training program. The soldiers were also given rest and had access to recreation facilities, Red Cross writing rooms, shows, and showers.\nOn December 16, 1944 the Germans started their last great offensive during World War II. The goal of the campaign (Wacht am Rhein in German) was to attack through the Ardennes and split the American and British armies on their way to Antwerp.\nWhen the Germans attacked in the early morning on December 16, 1944, the 2nd and 3rd Battalion came under heavy fire. The reserve battalion was immediately alerted. Company A was ordered north of Longsdorf, Luxembourg to establish communication and resupply Company E, but they were unable to advance.\nOn December 18, 1944 enemy resistance increased while Company A attempted to reach the town of Fouhren, Luxembourg. A flank attack threatened their position.\nThe unit was ordered to withdraw towards the high ground north and east of Diekirch at 8 p.m. on December 18, 1944. Fierce German artillery attacks drove the unit to the high grounds around Ettelbruck the following night.\nIt was during these withdrawal movements that Ballegeer went missing in the area of Bettendorf, Luxembourg. Later reports indicated he lost his life caused by shrapnel in his waist.\nMrs. Elizabeth Ballegeer, the wife of Leo Ballegeer, received a telegram on January 20, 1945, notifying her that her husband had been reported missing in action (MIA) since December 21, 1944. On February 18, 1945, she received a second telegram reporting her husband as killed in action (KIA) on December 21, 1944.\nLeo Ballegeer was buried on February 18, 1945, in the temporary cemetery 6020 in Hamm, Luxembourg, which later became the permanent Luxembourg American Cemetery. His wife wrote to the Quartermaster General in August 1945 to request information about the repatriation of her husband. Two years later, in the summer of 1947, she was asked to make the choice since she was listed as next of kin. She chose to leave her husband in Luxembourg.\nHis body remained in the temporary burial spot until disinterment in June 1948. On December 13, 1948, a reburial ceremony was held. His remains were reburied in Luxembourg American Cemetery where he now rests permanently. Ten days later, on December 23, 1948, a flag was sent to his family and they were also notified about the permanent reburial.\n\"21 Salt Lakers get draft call.\" The Salt Lake Telegram, May 29, 1941. Newspapers.com.\n28th Infantry Division in the Battle of the Hürtgen Forest. World War II Combat Interviews: Armed Forces Oral Histories, National Archives and Records Administration.\n28th Infantry Division in World War II. Nashville: The Battery Press, 2000.\n28th Infantry Division; World War II Operations Reports, 1940-1948, Records of the Adjutant General’s Office, Record Group 407 (Boxes 7459-7460); National Archives at College Park, College Park, MD.\n28th Infantry Division; World War II Operations Reports, 1940-1948, Records of the Adjutant General’s Office, Record Group 407 (Box 7478); National Archives at College Park, College Park, MD.\n“Achiel Balcaen” FindaGrave. Updated September 10, 2010. Accessed August 21, 2019. https://www.findagrave.com/memorial/58444372/achiel-balcaen.\nBodies of American soldiers, formerly of the 28th Division, found dead in the Ardennes sector. Photograph. November 5, 1945. National Archives and Records Administration (111-SC-212427). Image.\nCamiel Ballegeer, Manifests of Passengers Arriving at St. Albans, VT, District through Canadian Pacific and Atlantic Ports, 1895-1954, RG 85, National Archives and Records Administration - Washington, D.C.\nCamiel Ballegeer, Canadian Passenger Lists, 1865–1935. RG 76-C, Department of Employment and Immigration fonds, Library and Archives Canada Ottawa, Ontario, Canada.\nCamiel Ballegeer, World War II Draft Cards (Fourth Registration) for the State of Nebraska, Department of the Army, RG 147, National Archives and Records Administration - St. Louis.\nColbaugh, Jack. The Bloody Patch. A True Story of the Daring 28th Infantry Division. New York: Vantage Press, 1973.\n“Elizabeth Ann “Betty” Tresnak Slama” FindaGrave. Updated March 28, 2014. Accessed August 21, 2019. https://www.findagrave.com/memorial/127046536/elizabeth-ann-slama.\nFamily Balcaen, donated information by local historical society, City Archives Deerlijk, Belgium.\n“Felicita Balcaen” FindaGrave. Updated August 27, 2012. Accessed August 21, 2019. https://www.findagrave.com/memorial/96106863/fecilita-balcaen.\nGawne, Jonathan. Finding Your Father’s War. A Practical Guide to Researching and Understanding Service in the World War II US Army. Drexel Hill: Casemate, 2006.\nGrove, Major Adam R. “Re-forging the Iron Division: The Reconstitution of the 28th Infantry Division between the Hürtgen and the Ardennes.” School of Advanced Military Studies, United States Army Command and General Staff College. 2015. https://apps.dtic.mil/dtic/tr/fulltext/u2/1001458.pdf.\nHeadquarters 109th Infantry Regiment, General Order, November 6, 1944, WWII Operations Reports 1940-1948, 28th Division, Department of the Army, RG 407, National Archives at College Park, College Park, MD.\nJulia Balcaen, Passenger Lists of Vessels Arriving at New York, New York, 1820-1897. RG 36, National Archives at Washington, D.C.\nJulia Ballegeer, Naturalization Petitions, Nebraska, 1930-1942, RG 21, National Archives at Kansas City, Missouri.\n“Leo D. Ballegeer” American Battle Monuments Commission. Accessed April 2, 2019. https://www.abmc.gov/node/428802.\nLeo D. Ballegeer, Cemetery Locator Card, AGRC Form 19, Luxembourg American Cemetery Archives.\nLeo D. Ballegeer, Draft Registration Cards for Utah, 10/16/1940 - 03/31/1947, RG 147, National Archives and Records Administration - St Louis.\nLeo D. Ballegeer, Individual Deceased Personnel File, Department of the Army.\nLeo D. Ballegeer, Official Military Personnel File, Department of the Army, RG 319, National Archives and Records Administration - St. Louis.\nLeo D. Ballegeer, VA Master Index Card and Hospital Report, Department of the Army. National Archives and Records Administration – St. Louis.\nLion, Peter. American St. Nick. A True Story. Springville: Plain Sight Publishing, 2015.\nNebraska, Douglas County. 1930 U.S. Census. Digital Images.\nNebraska, Douglas County. 1940 U.S. Census. Digital Images.\nRecords for Elizabeth Ann Tresnak. Omaha South High School, Omaha, Nebraska.\nRecords for Leo Deon Ballegeer. Omaha South High School, Omaha, Nebraska.\nStanton, Shelby L. and Russell F. Weigley. World War II Order of Battle. An Encyclopedic Reference to U.S. Army Ground Forces from Battalion through Division 1939-1946. Mechanicsburg: Stackpole Books, 2006.\n“T. Sgt. Leo D. Ballegeer.” FindaGrave. Updated August 6, 2010. Accessed August 16, 2019. https://www.findagrave.com/memorial/56060295/leo-d_-ballegeer.\nUnit Journal from December 21, 1944, 109th Infantry Regiment, WWII Operations Reports 1940-1948, 28th Division, Department of the Army, RG 407, National Archives at College Park, College Park, MD.\nUnit Report from November 1, 1944 to November 30, 1944, 109th Infantry Regiment, WWII Operations Reports 1940-1948, 28th Division, Department of the Army, RG 407, National Archives at College Park, College Park, MD.\nUnit Report from December 1, 1944 to December 31, 1944, 109th Infantry Regiment, WWII Operations Reports 1940-1948, 28th Division, Department of the Army, RG 407, National Archives at College Park, College Park, MD.","Fifty years ago today NASA announced The Mercury Seven: the seven men to make up their first astronaut class. To commemorate this event, here are some interesting factoids about these American heroes.\nThe Mercury Seven were chosen in Washington, DC from a body of 69 candidates. The name comes from Mercury, a Roman mythological god who is seen as a symbol of speed. Because of the small space inside the Mercury capsule, candidates could be no taller than 5 feet 11 inches and weigh no more than 80 kilos. The initial flights took off throughout the early 1960s, though some astronauts were active in later decades. Here are the guys:\nMalcolm Scott Carpenter (born 1925) was a US Navy piolot aviation cadet who flew missions during the Korean War. He was on board the MA-7 (Aurora 7) and was the first American astronaut to eat solid food in space. He successfully overcame an overexpenditure of fuel due to hardware problems on his one and only mission. Carpenter was forced to retire from spaceflight after sustaining a motorbike accident. After retiring from the Navy, he founded Sea Sciences Inc., a corporation for developing programs for utilising ocean resources and improving environmental health.\nLeroy Gordon (Gordo) Cooper Jr. (1927 - 2004) was very active in the Boy Scouts of America and achieved the second highest rank of Life Scout. Prior to joining NASA, Cooper also served in the US Air Force and Marine Corps. He was on board the MA-9 (Faith 7) and Gemini 5, and developed a personal survival knife for astronauts to carry. Cooper was the first American to sleep in orbit. Interestingly, he took photos of and reported UFO sightings to the Pentagon, but they swept the incident under the rug.\nJohn Herschel Glenn Jr. (born 1921) began his career as a US Marine Corps fighter pilot. He was on board the MA-6 (Friendship 7) and STS-95. Noticed for his heroics in space, Glenn became friendly with the Kennedys and a prominent public figure. After retiring from NASA, he ran as a Democrat and represented the state of Ohio in the United States Senate from 1974 to 1999.\nVirgil Ivan (Gus) Grissom (1926 - 1967) was a US Air Force pilot before joining NASA. He was on board the MR-4 (Liberty Bell 7), Gemini 3, and Apollo 1. Grissom was tragically killed along with fellow astronauts Ed White and Roger Chaffee during a pre-launch test for the Apollo 1 mission. After death, his family was involved in a spacesuit controversy: NASA insisted Grissom got authorisation to use his spacesuit for a show and tell at his son's school and never returned it, but his family claimed the he had rescued the spacesuit from a scrap heap and that it rightfully belonged to them.\nWalter Marty (Wally) Schirra Jr. (1923 - 2007)'s father was a pilot, and his mother performed wing walking stunts when he was on duty. Schirra served as an officer in the US Navy, and was later dispatched to South Korea as a pilot on loan to the US Air Force. On board the MA-8 (Sigma 7), Gemini 6A, and Apollo 7, he was the only person to fly in all of America's first three space programs. Schirra gained notoriety for playing \"Jingle Bells\" on a harmonica he smuggled on board Gemini.\nAlan Bartlett Shepard Jr. (1923 - 1998) began as a US Navy as test pilot. He was the first American in space, and flew on board the MR-3 (Freedom 7) and Apollo 14. It's said that shortly before one launch, Shepard blurted out \"Please, dear God, don't let me fuck up.\" This has since become known among aviators as \"Shepard's Prayer.\" A successful businessman, Shepard was the first astronaut to become a millionaire while still in the program. His hometown of Derry, NH almost changed its name to \"Spacetown\" in honour of Schirra's career.\nDonald Kent (Deke) Slayton (1924 - 1993) was also a US Air Force pilot before joining NASA. He was grounded from space flight by a heart condition, but served as NASA's Director of Flight Crew Operations. Slayton served as head of Astronaut selection. In 1972 he was granted medical clearance to fly as docking module pilot of the Apollo-Soyuz Test Project. At the time of the flight, he became the oldest person to fly into space. [NASA and Wikipedia]"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:d87aaea0-dc67-4a68-a83d-b94e5ad12470>","<urn:uuid:ee3ae95e-1b9b-4714-a506-1d7d8dc0e567>"],"error":null}
{"question":"What is the historical significance of the Kymin in Monmouth, and what modern technological advancements has Rolls-Royce incorporated in their latest Ghost model?","answer":"The Kymin is a round house/tower built in 1794 that served as a favorite picnic spot for Georgian gentry. It has historical significance as it includes a Naval Temple built in 1800 to commemorate the Battle of the Nile and British naval supremacy, with Lord Nelson himself visiting in 1802. As for modern Rolls-Royce developments, the latest Ghost model features numerous technological advancements including a proprietary aluminum spaceframe architecture, a Planar suspension system with Upper Wishbone Damper, all-wheel drive, all-wheel steering, LED and laser headlights with 600m illumination range, and a Micro-Environment Purification System (MEPS) that removes ultra-fine particles from the cabin air in less than two minutes.","context":["After completing our tour of the Dean Heritage Centre we decided to drive through the Forest towards the town ancient market town of Monmouth. On our way I spotted a sign to the Kymin and I took a detour. After a steep climb we arrived at the top of a Kymin Hill from which there are spectacular views of the Monmouth and the surrounding countryside.\nThe Kymin, now a National Trust property, is a round house / tower, constructed in 1794.\nThis was a favourite picnic spot for Georgian gentry. Closeby there is also a Naval Temple that was built in 1800 to commemorate the second anniversary of the Battle of the Nile (1798) and to celebrate the supremacy of the British Navy. The monument is dedicated to several Naval Admirals of that era, including Lord Nelson. Nelson, himself, is known to have visited the Kymin in 1802.\nThere are many lovely woodland walks at the Kymin and it is a perfect spot for picnics.\nWe continued on our way and made our descent into Monmouth and parked the car and explored on foot. Monmouth is a typical country market town with a long straight high street with many small shops.\nIn the centre of Monmouth in Agincourt Square is the Shire Hall which was once the court of assizes and quarter sessions.\nUnder the clock is a statue of the English King Henry V who was born in Monmouth in 1387. In 1417 Henry and his army won the famous Battle of Agincourt in France during the Hundred Years’ War.\nAlso in Agincourt Square stands a statue of Charles Stewart Rolls. who was an early pioneer in motoring and aviation. He was a baloonist and was the first to successfully fly across the English Channel and back without stopping. Unfortunately Charles died in a tragic flying accident in 1910. The Rolls family had lived near Monmouth at Hendre House for several generations. Charles was also a co-founder of the company known as Rolls Royce.\nWe wandered on along Church Street.\nHere we stumbled upon the old Savoy Theatre, which is said to be the oldest known theatre site in Wales.\nJust around the corner is the St Mary’s Priory Church with its magnificent towering spire.\nThe church was beautifully decorated with flowers for Easter.\nOpposite the church are some old Arms Houses.\nSome streets with festooned with cheery Easter decorations.\nWe then headed down the long Monnow High Street which is lined with numerous small shops.\nAt the fare end of the high street is the bridge and gatehouse over the River Monnow.\nThe bridge dates from the late 13th century, and the gatehouse dates from the end of the 13th / early 14th century, and is the only remaining example of a medieval fortified river bridge in Britain. This longstanding structure is made of red sandstone.\nThe River Monnow\nAll photos by me © Louise Shapcott\n(except where photos have been rightfully accredited to the photographer / owner)\n#monmouth #monmouthshire #thekymin #rivermonnow","- Our Knights & Distinguished Personalities\n- DIGITAL EDITION\nRolls-Royce Motor Cars today (September 1) launched the most technologically advanced Rolls-Royce yet - the new Ghost, which succeeds the most successful product in the marque’s 116-year history.\nThe new Ghost reflects ‘Post Opulent’ design philosophy, rejecting superficial expressions of wealth and is built on rigid aluminium Rolls-Royce spaceframe architecture.\nNew Ghost is perfect in its simplicity, but creating this pure and detoxifying environment was one of the greatest challenges in the marque’s history. Indeed, new Ghost is the most technologically advanced motor car Rolls-Royce has ever produced, says Rolls-Royce.\nThe car offers all-wheel drive and all-wheel steering for unprecedented poise and surefootedness, while the world's first Planar suspension system significantly increases its agility and effortlessness.\nThe new Ghost is equipped with hallmark 6.75-litre twin-turbo V12 engine, delivering 571PS and 850nm.\nInterior components are tuned to specific resonant frequency to create a sense of serenity and its down lit Pantheon grille discreetly illuminates Rolls-Royce iconography\n“The first Goodwood Ghost was a response to a whole new generation of clients, both in age and attitude. These men and women asked us for a slightly smaller, less ostentatious means to own a Rolls-Royce. The success of the product we created for them fulfilled our most ambitious expectations. Over its ten-year lifespan, which began in 2009, Ghost has become the most successful model in the marque’s 116-year history,\" says Torsten Müller-Ötvös, Chief Executive Officer, Rolls-Royce Motor Cars.\n\"To create a new product that would resonate with our Ghost clients for the next ten years meant we had to listen carefully to their demands. Today we set new standards in customer centricity by creating a completely new motor car for a unique group of Rolls-Royce’s clients. These business leaders and entrepreneurs demand more of their Ghost than ever. They require a new type of super-luxury saloon that is dynamic, serenely comfortable and perfect in its minimalism. Ghost is this product.\n\"The only components that we carried over from the first Goodwood Ghost were the Spirit of Ecstasy and umbrellas. Everything else was designed, crafted and engineered from the ground up. The result is the most technologically advanced Rolls-Royce yet. It distils the pillars of our brand into a beautiful, minimalist, yet highly complex product that is perfectly in harmony with our Ghost clients’ needs and perfectly in tune with the times,” he says.\nThe highly successful and diverse entrepreneurs and founders, who selected Ghost to celebrate their ongoing ascension, were citizens of the world – they had been educated abroad, they travelled extensively and experienced Rolls-Royce in many cultures, the company says.\nDue to Ghost’s energetic, dynamic personality, these clients came to realise that the Rolls-Royce brand could offer more than a chauffeur-driven experience. Indeed, in the USA and areas of Europe, clients were self-driving their Ghost from the very early stages of its introduction. Meanwhile, in Asia, clients were engaging heavily in the connected technology on board, be it for business or pleasure.\nMeanwhile, at Goodwood, significant advances were being made with the marque’s proprietary aluminium spaceframe architecture. First used on Phantom, then Cullinan, this spaceframe is unique to Rolls-Royce and enables the brand’s designers and engineers to develop an authentically super-luxury product, free from the constraints of platforms used to underpin high-volume vehicles. As Ghost clients required even more of their motor car, Rolls-Royce used its architecture to respond, incorporating technology such as all-wheel drive and all-wheel steering in Ghost, unlocking an entirely new, purposeful personality.\nConcurrently, the design team were tracking an emerging movement that came to define Ghost’s aesthetic treatment. It spoke of a shifting attitude among Ghost clients in the way success is expressed. Named ‘Post Opulence’ internally, it is characterised by reduction and substance. In service to this, exceptional materials must be selected and celebrated. Design must be limited, intelligent and unobtrusive. This philosophy is the antithesis of ‘premium mediocracy’, a term coined by the fashion cognoscenti. This refers to products that use superficial treatments, such as large branding or, in the context of motor cars, busy stitching and other devices that create an illusion of luxury by dressing products lacking in substance in a premium skin.\nThe collective result is new Ghost. This is a motor car precisely tailored to its clients, that appears perfect in its simplicity, that is underpinned by remarkable substance, that is less but better, says the company.\nProprietary Aluminium Spaceframe Architecture\nThe marque’s designers, engineers and craftspeople demanded the freedom to create a very specific personality for new Ghost. These men and women were only able to create an authentically super-luxury product without the constraints of platforms used to underpin lesser, high-volume vehicles. Hence, the Rolls-Royce proprietary aluminium spaceframe architecture.\nReserved exclusively for Rolls-Royce, this architecture already underpins its flagship, Phantom, and its transformative SUV, Cullinan. The spaceframe’s flexibility and scalability freed the marque to serve the unique aesthetic and mechanical demands of new Ghost, and in doing so created an acoustically superior, highly rigid and dynamic proposition for Ghost within the Rolls-Royce product portfolio.\nIn its most pared back form, the Rolls-Royce architecture is based around four fixed points, one at each corner of the motor car. The moveable aluminium bulkhead, floor, crossmembers and sill panels were positioned specifically to ensure new Ghost meets client expectations as a motor car that is equally enjoyable to drive as it is to be driven in. Two of the cast suspension mounting assembles were pushed to the very front of new Ghost, placing its 6.75-litre V12 behind the front axle to achieve an optimum 50/50 weight distribution.\nTo accommodate this without intruding on new Ghost’s interior suite, its overall length has grown by 89 mm, compared to the first Goodwood Ghost, to 5546 mm, and its overall width has grown by 30mm to 1978mm. Significant changes were also made to the double-skinned bulkhead and floor structure packaging.\nThese were undertaken to incorporate an all-wheel drivetrain, all-wheel steering and completely redesigned Planar Suspension System, which further enhances the marque’s hallmark Magic Carpet Ride. This was achieved without compromising the motor car’s low centre of gravity, which aids cornering dynamics.\nFurther capitalising on the marque’s aluminium expertise, the metal superstructure of new Ghost is 100% made of the material. The car’s outer body is rendered as one clean, expansive piece, flowing seamlessly from the A-pillar, over the roof and backwards to the rear of the car, recalling the seemingly one-piece coachbuilt Silver Dawn and Silver Cloud models.\nThis complete absence of shut lines allows clients to run their eye from the front to the rear of the car uninterrupted by ungainly body seams. To achieve this, four craftsmen hand weld the body together simultaneously to ensure a perfectly continuous seam. In addition, 100% aluminium, laser-welded doors have been used. This not only offers weight benefits and remarkable 40,000Nm/deg stiffness, but the material has a lower acoustic impedance than steel, improving cabin ambience.\n6.75-Litre Twin-Turbocharged V12\nClient feedback asking for near-instant torque and near-silent running led the marque to further develop the Rolls-Royce 6.75-litre twin-turbocharged V12 petrol engine. A bespoke Ghost engine map was created to ensure ample performance for this dynamic motor car, delivering 563bhp/420kW and 850Nm/627lb ft of torque to the all-wheel steer, all-wheel drivetrain.\nCommensurate with clients’ expectations, maximum torque is available from just 1600rpm – only 600rpm above tick-over. To further refine its already remarkable acoustic properties, the air intake system incorporated larger porting to reduce engine presence in the interior suite.\nPlanar Suspension System\nThe marque’s hallmark Magic Carpet Ride has evolved. For new Ghost, engineering specialists redesigned the motor car’s suspension completely to deliver what is called the Planar Suspension System. Named after a geometric plane, which is completely flat and level, the system is the result of ten collective years of testing and development to create a sense of flight on land never before achieved by a motor car.\nCreated through physical engineering developments as well as sophisticated scanning and software technology, it incorporates a world-first Upper Wishbone Damper unit above the front suspension assembly, creating an even more stable and effortless ride. This works alongside the Flagbearer system, which uses cameras to read the road ahead and prepare the suspension system for any changes in road surface, as well as the marque’s Satellite Aided Transmission. These technologies are managed as one through a bespoke Planar software system. New Ghost can now anticipate and react to the most demanding road surfaces.\nThe Upper Wishbone Damper alone was the result of five collective years of road and bench testing. Reserved exclusively for Rolls-Royce, this technology further evolves the marque’s double-wishbone Magic Carpet Ride suspension system. The ethos of the marque’s founder, Sir Henry Royce, was “Take the best that exists and make it better,” and in this spirit chassis specialists developed the Upper Wishbone Damper to further improve the continuously variable, electronically controlled shock absorbers and the self-levelling high-volume air strut assembles. It has never before been applied to a production motor car.\nThe five-link rear axle benefits from the same self-levelling high-volume air suspension technology, as well as rear-wheel steering. Both axles are managed via the marque’s Planar software. This also governs new Ghost’s other chassis technologies, including the all-wheel drive, all-wheel steering, stability control and self-drying braking systems, to ensure the motor car is reacting as one to changes in surfaces or grip levels while also maintaining a spirited, dynamic personality.\nThe Planar software also manages information that requires new Ghost to proactively adapt to intrusions in the road ahead. The first of these technologies is the marque’s Flagbearer system. Evocative of the men who were required by law to carry a red flag ahead of early motor cars, this technology consists of a stereo camera system integrated in the windscreen to see the road ahead, adjusting suspension proactively rather than reactively up to 100km/h. The second is Rolls-Royce’s Satellite Aided Transmission system, which draws GPS data to pre-select the optimum gear for upcoming corners. The result is unprecedented levels of ride comfort and control for a motor car.\nRolls-Royce clients have enjoyed self-closing doors since the first Goodwood Phantom. Operated by a button on the dashboard and on the C-pillar for motor cars with rear doors, this innovation has been celebrated among customers. For new Ghost, the marque’s engineers elected to further develop this hallmark technology and, for the first time, clients can now also open the doors with power assistance.\nMicro-Environment Purification System\nNew Ghost benefits from a new Micro-Environment Purification System (MEPS). Existing air filtration technology was further developed to incorporate a full suite of hardware and software improvements. Highly sensitive Impurity Detection Sensors were introduced to detect ambient air quality, automatically switching fresh air intakes to Recirculation Mode if unacceptable levels of airborne contaminants are present. This channels all cabin air through a nanofleece filter, which is capable of removing nearly all ultra-fine particles from the Rolls-Royce’s micro- environment in less than two minutes.\nThe Most Technologically Advanced Rolls-Royce Yet\nNew Ghost is perfect in its simplicity, but creating this pure and detoxifying environment was one of the greatest challenges in the marque’s history. Indeed, new Ghost is the most technologically advanced motor car Rolls-Royce has ever produced. Further equipment includes: LED and laser headlights with more than 600m of illuminated range, vision assist, including day- and night-time wildlife and pedestrian warning; alertness assistant; a four-camera system with panoramic view, all-round visibility and helicopter view; active cruise control; collision warning; cross-traffic warning; lane departure and lane change warning; an industry-leading 7x3 high-resolution head-up display; Wi-Fi hotspot; self-park; and the very latest navigation and entertainment systems.\nExterior: New Ghost reflects an evolved appreciation of luxury, one defined by minimalism and purity, but underpinned by great substance. In the pre-sketch ideation phase of new Ghost’s design development, this treatment was named ‘Post Opulence’ – a movement defined by authenticity of materials rather than overt statement, which had already established roots in architecture, fashion, jewellery and boat design.\nPursuing this minimalist aesthetic for new Ghost was the design team’s absolute objective throughout. The desired treatment was not sterile, but confident in its purity and unmistakeably belonging to a Rolls-Royce. This begins with the car’s first impression. Rolls-Royce’s proprietary architecture allowed the design team to increase the width by 30mm, subtly communicating presence. This is framed by sharp bow lines that intersect with an angular light signature, creating an assertive yet beautiful front end.\nIn addition, new Ghost was given its own ethereal front-end character. This was achieved not by way of overt design, but with light. 20 LEDS underneath the top of the radiator grille subtly illuminate the veins. During the development phase, early prototypes were over-effective and the light reflecting from the polished uprights looked too striking. In the spirit of Post Opulent aesthetics, the marque’s engineering team brushed the back of the metal grille bars, making them less reflective, subduing the effect and perfecting the restrained glow desired.\nThe front of new Ghost is an exemplar of the design team’s obsession with reduction. Owing to the hand-welded aluminium body structures, the main structure of the car appears as one fluid canvas, uninterrupted by shut lines, recalling the coachbuilt Silver Dawn and Silver Cloud. For the first time, the Spirit of Ecstasy is not surrounded by panel lines but rather stands sits within her own ‘lake’ of bonnet.\nTurning to the flanks, a single straight stroke is used to emphasise the motor car’s length. The lower ‘waft line’ borrows from boat design and uses reflection to lighten the surfacing and create a pure, uncomplicated sense of motion.\nMoving to the glasshouse, it is wilfully neutral, with both doors sharing an equally proportioned window graphic, gesturing that new Ghost strikes a balance as both a driver-oriented and a chauffeur-driven car. A subtly arched roof line gently proclaims its dynamic intent. The rear end follows this sense of movement and resolves in a taper.\nThe subtle near-square rear light graphic has become a tenet of contemporary Rolls-Royce design. It remains, but has been modernised with a slight forward tilt. Not surrounded by shut lines, it appears as if it is an island within the painted surface.\nBusy details and superficial embellishments were rejected not only to create a more relaxing refuge, but to better celebrate the material substance and maximise the impact of bespoke colour personalisation.\nHowever, creating an environment defined by reduction, simplicity and elegance is an extremely complex endeavour. It also relies on sourcing the very finest materials; leathers, woods and metals left unembellished will invite the scrutiny of these most discerning of clients. To this end, each of the 20 half hides used to create the interior suite of new Ghost are subject to the automotive industry’s most exhaustive quality control checks to ensure that each of the 338 panels used – however visible – is of the very best quality. Further demonstrating the marque’s competence in leathercraft, complex, busy stitchwork has been eschewed for scant but incredibly long and perfectly straight lines, again welcoming scrutiny from the marque’s clients.\nWood sets for new Ghost are available in an open-pore finish, bravely showcasing materials in their naked form. Indeed, two new finishes have been developed specifically for the motor car. The first is Obsidian Ayous, inspired by the rich versatility of colours found in lava rock. The second is Dark Amber; this introduces subtle glamour to the interior suite by integrating veins of fine aluminium particles into the dark wood. As with the leather finishes, this material is left exposed as long, single-veneer leaves, bisected only by cold-to-the-touch real metal vents, through which MEPS-filtered air reaches the cabin.\nIlluminated Fascia: For new Ghost, the marque’s Bespoke Collective of designers, engineers and craftspeople created Illuminated Fascia: a world-first innovation that subtly echoes the Starlight Headliner, which has become as much a part of Rolls-Royce iconography as the Spirit of Ecstasy, Pantheon Grille and ‘Double R’ monogram.\nDeveloped over the course of two years and more than 10,000 collective hours, this remarkable piece brings an ethereal glowing Ghost nameplate, surrounded by more than 850 stars, into the interior suite of the motor car. Located on the passenger side of the dashboard, the constellation and wordmark are completely invisible when the interior lights are not in operation.\nNevera, world’s ‘fastest EV’ car unveiled in UAEThu, Feb 8, 2024\nRange Rover SV Bespoke Sadaf Edition revealedSun, Feb 4, 2024\nPorsche Macan now comes as all-electricThu, Jan 25, 2024\nAudi delivers 1.9m cars; sales zoom in MideastMon, Jan 22, 2024\nNew President & CEO for Mercedes-Benz Cars Middle EastWed, Jan 17, 2024\nPorsche sales grow 11pc in regionSun, Jan 14, 2024\nRolls-Royce Motor Cars achieves strong salesTue, Jan 9, 2024\nDubai Police add Lamborghini Urus Performante to fleetSun, Nov 19, 2023\nPanamera Awaits Dubai DebutAutumn 2023\nTrident’s New Racing BeastAutumn 2023\nNew One-Off FerrariAutumn 2023\nRecreating an Iconic BentleyAutumn 2023\nRedefining ClassAutumn 2023\nLucid to auction Kingdom Dream EditionWed, Oct 25, 2023\nLamborghini Revuelto takes to the roadThu, Oct 19, 2023"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:7b01f609-e32a-4f0e-a268-82485930ea23>","<urn:uuid:536b4493-3151-490d-8a5e-a07808dce0a8>"],"error":null}
{"question":"I work at an airline and need to understand: what are the key flight time data points needed for a real-time passenger management dashboard?","answer":"For a valuable airline passenger management dashboard, you need six crucial time data points for both inbound and outbound flights: scheduled time of departure, scheduled time of arrival, estimate time of departure, estimate time of arrival, actual time of departure, and actual time of arrival. This information is typically provided by operations control systems like Sabre Movement Manager, Netline, or AIMS.","context":["Passenger Connection Management — One Of The Most Complex Airline Processes\nHowever, passenger management also reflects one of the most complex airline processes. That’s why an increasing number of airlines are trying to set up real-time dashboards that are tailored to passenger management. The underlying goal is to visualize passenger streams and related information. And to do that in a way that perfectly supports the passenger connection process.\nHowever, after going down that road with airlines, we learned one thing. Passenger management (real-time) dashboards are everything but not trivial. The aspect that mainly drives the complexity is that additional and specific information is required to calculate the relevant KPIs. Nonetheless, this is of the highest importance to provide a valuable dashboard.\nWith this blog post, we provide a very practical case study. First, you will find visual examples of airline passenger management dashboards. Then, on top of that, we focus on relevant data required to set up those dashboards.\nThe Essential Information An Airline Passenger Management Dashboard Has To Contain\nFrom our experience, a passenger (connection) dashboard should —first and foremost— visualize the passenger connection streams at a dedicated airport/hub. Therefore, it is essential to design a graphical visualization of these streams. This includes inbound flights, passenger connection streams, and corresponding outbound flights. Additionally, since this is the crucial KPI, the dashboard should contain the Misconnex Quota as the essential quality aspect of connection-driven airline operations.\nThe airline passenger management dashboard below illustrates our approach to this topic. The connection visualization in the center of the dashboard highlights the following aspects:\nThe example shows Incoming flights, including the latest times and arrival gates on the left. The flights are accompanied by several connecting passengers (clustered according to compartments – first, business, premium eco, and eco). For example, for the flight at the top, three first-class passengers, nine business passengers, 17 premium eco, and 25 eco passengers connect to outbound flights.\nThe column in the middle of the visualization shows the number of passengers of a particular connection. Additionally, it shows the required and available transfer time. For example, nine businesses, 11 premium eco, and 12 eco passengers connect to an outbound flight for the first line. The required connecting time is 25 minutes, and the available connecting time is 20 minutes. Since the needed transfer time is higher than the available, those numbers are colored in red.\nOn the right side, the outbound flights are listed. As additional information, it shows the number of connecting passengers and the latest times and gate information.\nWhat Kind Of Data Is Required To Set Up An Airline Passenger Dashboard?\nObviously, the essential information to create an airline passenger management dashboard is about passengers. That means for each passenger, the dashboard requires corresponding inbound and outbound flight information. This information is typically provided by systems such as Amadeus Altea, Galileo, or Worldspan.\nLatest Flight Information\nTo create an airline passenger management dashboard that provides valuable information, it is essential to have the latest flight information available. The most crucial data in this context is about the latest times:\n- scheduled time of departure\n- scheduled time of arrival\n- estimate time of departure\n- estimate time of arrival\n- actual time of departure\n- actual time of arrival\nThis, of course, is required for both inbound and outbound flights. Usually, this type of data is provided by the operations control system. Sabre Movement Manager, Netline, or AIMS are a few prominent examples. Since Operations Control Systems are typically the primary source for real-time dashboards, this information is usually already available.\nAirport Information in this context is about up-to-date information concerning the arrival gate/position of the inbound flight and departure gate/position of the outbound flight. The source system for this information differs from airline to airline. Sometimes the data is provided by CRS, such as Amadeus. Sometimes the operations control system holds this information. In some cases, dedicated other systems have to be connected to the dashboard.\nProbably the most challenging type of information which is required to set up a passenger management dashboard. To highlight critical connections, it is essential to provide the needed transfer time for each combination of inbound and outbound flights.\nSome airlines can only rely on very general connection times. For example, average minutes from one terminal to another. However, the value of a passenger dashboard sharply increases as the accuracy of this information improves. Some airlines operated dedicated systems providing detailed connecting times from one gate to another one. This also considers actual waiting times, Schengen/non-Schengen aspects, current infrastructure limitations, etc.\nNonetheless, a possible approach is starting with generic connecting times and continuously improving and detailing the figures."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:df3241f7-c81f-4dca-892b-c0c38d7edc6e>"],"error":null}
{"question":"What is the physiological role of sodium in human body, and how much sodium do Americans typically consume compared to recommended levels?","answer":"Sodium plays essential roles in human physiology, including facilitating osmotic transfer of nutrients and waste products in and out of cells, and creating electrical impulses traveling along nerves to and from the brain. Severe sodium deficiency can even result in respiratory arrest and death. The body cannot synthesize sodium and must obtain it through diet. As for consumption, the Reference Daily Intake (RDI) for sodium is 2,300 mg (about 1 teaspoon of salt), but average daily sodium intake in the US is 3,400 mg - far higher than the recommended upper limit. This excess mainly comes from packaged and restaurant foods, rather than from overusing salt shakers.","context":["by Les Saidel - August, 2014\nFor a growing number of people it is becoming necessary to adopt a low-salt or salt-free diet.\nCommon table-salt, or by its chemical name sodium chloride, is ubiquitous in food and has been since the dawn of time. It is the universal spice, without which foods taste bland and tasteless. The human species has placed such an emphasis on their food having taste that throughout the ages, wars have been fought and won over salt, literally.\nAside from exciting our palates, salt contains a mineral essential to the normal functioning of our bodies, specifically the sodium part of sodium chloride. Sodium plays an essential role in myriad systems of the human physiology, among them facilitating osmotic transfer of nutrients and waste products in and out of the cells, creating the electrical impulses traveling along nerves to and from the brain and many more. Severe sodium deficiency can result in respiratory arrest and death.\nSodium is continuously being lost from our bodies as a constituent of perspiration. The body cannot synthesize sodium and it must therefore be ingested regularly as part of our diet. The needs for sodium vary with the individual and the geographical region. People who live in hotter climates (and perspire more) require a larger sodium intake than those who live in cooler climates, for example.\nAn excess of sodium in the diet can lead to hypertension, or high blood pressure, to which certain individuals are more genetically prone than others. This brings us to the opening statement of the article - people suffering from high blood pressure are usually placed on low-salt or salt-free diets by their doctors or dieticians to re-establish the electrolyte balance in their bodies.\nIn today's modern society and food culture, it is very difficult to become sodium deficient. Processed food unfortunately loses a lot of its intrinsic flavour during processing and to compensate for that the food companies add extra salt or flavour enhancers like mono-sodium-glutamate (MSG) to restore taste to the food. (For those cooking at home using fresh produce, this is less of problem because the fresh food has its own intrinsic flavour and needs to be spiced less). We therefore find ourselves inundated with a glut of salt in our food which is potentially dangerous for that population who are genetically prone to hypertension.\nObviously bread forms an important part of the diet and those on low-salt or salt-free diets must purchase or bake bread using less or no salt.\nIt must be stated right up front that bread without salt is usually bland and tasteless. If you have ever forgotten to add salt to your dough, you will have experienced this. The taste of salt-free bread is very similar to drinking distilled water, not an overly pleasant experience.\nThere is one place on our small planet that does make salt free bread out of CHOICE! This is in Tuscany, Italy.\nThe true reason for this tradition is uncertain, but two theories have been proposed. The first claims that at a certain point in history, the tax on salt became so exorbitant that the Tuscans simply stopped using it in their cooking and baking.\nThe second version is a little more dramatic and finds its origins in the war between the cities of Florence and Pisa. The Pisans laid siege to Florence and blocked entry of supplies, including salt. The citizens of Florence had no choice but to forego this spice and thus began to cook and bake without it.\nWhichever theory you believe, the fact remains that the Tuscans became so used to salt-free bread that it has remained a tradition in the region to this day. I must add however, that Tuscan food (aside from their bread) is known to be spicy, so perhaps the lack of salt in the bread is balanced by the food. In fact Tuscan bread is routinely eaten together with some condiment or together with other food for this reason.\nIf you want to embark on a low-salt salt-free diet, here are a few tips and recommendations, some general and other more bread specific.\nFirstly and most importantly such a diet should not be undertaken without first consulting with and obtaining the approval of a doctor or dietician.\nThe brain adapts. So while at first you may find food without salt insipid, after time you will become used to it, so much so that if you return to a normal diet, you will then find the food over-salted.\nAct according to directions of your medical practitioner, but if possible, reduce the salt gradually, giving your brain time to adapt. This will be less emotionally stressful to your system.\nAvoid processed foods as much as possible and cook and bake your own, using the freshest produce available. These have the highest intrinsic flavour and will compensate for the lack of salt. For some this is a revelation - to actually \"discover\" the true taste of the food.\nCompensate for lack of salt by increased use of other spices in the food which do not affect the malady.\nAs far as baking bread is concerned, use of freshly ground whole grain flour adds to the intrinsic flavour of the bread. Similarly, using sourdough to ferment the bread rather than regular baker's yeast, will greatly enhance the flavour. Finally the addition of other spices, such as garlic, coriander etc. to the bread will add to the taste.\nFinally there are alternative salts on the market that contain no sodium, such as potassium chloride. However usage of these must be done under medical supervision so as not to disturb other balances in the body.\nIn summary, act according to your medical practitioner's guidelines, and make the process as painless as possible by enhancing the intrinsic flavour of the food.\n© Copyright. All rights in the above articles are reserved to the author Les Saidel.\nNo part of this website or the above articles may be transmitted in any form or by any\nmeans without permission in writing from the author.","Table salt, known chemically as sodium chloride, is made up of 40% sodium.\nIt’s estimated that at least half of people with hypertension have blood pressure that is affected by sodium consumption — meaning they’re salt sensitive. In addition, your risk of salt sensitivity increases with age (\nThe Reference Daily Intake (RDI) for sodium is 2,300 mg — or about 1 teaspoon of salt (\nStill, average daily sodium intake in the US is 3,400 mg — far higher than the recommended upper limit. This mainly comes from packaged and restaurant foods, rather than from overusing your salt shaker (\nSodium is added to foods for flavor and as part of some food preservatives and additives (\nHere are 30 foods that tend to be high in sodium — and what to eat instead.\nPackaged, plain, frozen shrimp commonly contains added salt for flavor, as well as sodium-rich preservatives. For example, sodium tripolyphosphate is commonly added to help minimize moisture loss during thawing (\nIn contrast, a 3-ounce (85-gram) serving of fresh-caught shrimp without salt and additives has just 101 mg of sodium, or 4% of the RDI (\nOpt for fresh-caught if you can or check a health food store for frozen shrimp without additives.\nCanned, packaged and restaurant-prepared soups often pack a lot of sodium, though you can find reduced-sodium options for some canned varieties.\nThe sodium primarily comes from salt, though some soups also contain sodium-rich flavor additives, such as monosodium glutamate (MSG).\nOn average, canned soup has 700 mg of sodium, or 30% of the RDI, per 1-cup (245-gram) serving (\nThere’s no sign of food companies cutting back on how heavily they salt this popular meat. In a recent national sampling of US foods, researchers found that ham was 14% higher in sodium than in the previous analysis (\nConsider using ham only as an occasional condiment in small amounts rather than eating a full serving.\nPudding doesn’t taste salty, but there’s plenty of sodium hiding in instant pudding mix.\nThis sodium is from salt and sodium-containing additives — disodium phosphate and tetrasodium pyrophosphate — used to help thicken instant pudding.\nA 25-gram portion of instant vanilla pudding mix — used to make a 1/2-cup serving — has 350 mg of sodium, or 15% of the RDI. In contrast, the same amount of regular vanilla pudding mix contains only 135 mg of sodium, or 6% of the RDI (11, 12).\nCottage cheese is a good source of calcium and an excellent source of protein, but it’s also relatively high in salt. A 1/2-cup (113-gram) serving of cottage cheese averages 350 mg of sodium, or 15% of the RDI (13).\nThe salt in cottage cheese not only enhances flavor but also contributes to texture and functions as a preservative. Therefore, you generally won’t find low-sodium versions (\nHowever, one study found that rinsing cottage cheese under running water for three minutes, then draining it, reduced sodium content by 63% (\nDrinking vegetable juice is a simple way to get your veggies, but if you don’t read nutrition labels, you could be drinking a lot of sodium, too.\nAn 8-ounce (240-ml) serving of vegetable juice may have 405 mg of sodium, or 17% of the RDI (\nFortunately, some brands offer low-sodium versions — which means they can have no more than 140 mg of sodium per serving according to FDA rules (16).\nSome of the sodium in salad dressing comes from salt. Additionally, some brands add sodium-containing flavor additives, such as MSG and its cousins, disodium inosinate and disodium guanylate.\nIn a review of major brand-name foods sold in US stores, salad dressing averaged 304 mg of sodium per 2-tablespoon (28-gram) serving, or 13% of the RDI (\nHowever, sodium ranged from 10–620 mg per serving across the samples of salad dressing, so if you shop carefully, you could find one low in sodium (\nAn even better option is to make your own — try using extra virgin olive oil and vinegar.\nPizza and other multi-ingredient dishes account for almost half of the sodium Americans consume.\nMany of the ingredients — such as cheese, sauce, dough and processed meat — contain significant amounts of sodium, which add up quickly when they’re combined (\nA large, 140-gram slice of store-bought, frozen pizza averages 765 mg of sodium, or 33% of the RDI. A restaurant-prepared slice of the same size packs even more — averaging 957 mg of sodium, or 41% of the RDI (\nIf you eat more than one slice, the sodium quickly adds up. Instead, limit yourself to one slice and complete your meal with lower-sodium foods, such as a leafy green salad with low-sodium dressing.\nSandwiches are another one of the multi-ingredient dishes that account for almost half of the sodium Americans consume. The bread, processed meat, cheese and condiments often used to make sandwiches all contribute a significant amount of sodium (\nFor example, a six-inch submarine sandwich made with cold cuts averages 1,127 mg of sodium, or 49% of the RDI (\nYou can significantly cut back on sodium, by choosing unprocessed sandwich toppings, such as grilled chicken breast with sliced avocado and tomato.\nPackaged broths and stocks — used as the base for soups and stews or to flavor meat and vegetable dishes — are notoriously high in salt.\nFortunately, you can easily find reduced-sodium broths and stocks, which have at least 25% less sodium per serving than the regular versions (\nBoxed potato dishes, particularly scalloped potatoes and other cheesy potatoes, pack a lot of salt. Some also contain sodium from MSG and preservatives.\nA 1/2-cup (27-gram) portion of dry scalloped potato mix — which makes a 2/3-cup cooked serving — has 450 mg of sodium, or 19% of the RDI (21).\nEveryone would be better off swapping boxed potatoes for more nutritious starches, such as a baked sweet potato or winter squash.\nHowever, though pork rinds are a keto-friendly snack, they’re high in sodium.\nIf you’re craving something crunchy, consider unsalted nuts instead\nCanned vegetables are convenient but pack their share of sodium.\nFor example, a 1/2-cup (124-gram) serving of canned peas has 310 mg of sodium, or 13% of the RDI. Similarly, a 1/2-cup (122-gram) serving of canned asparagus packs 346 mg of sodium, or 15% of the RDI (24, 25).\nDraining and rinsing canned vegetables for a couple of minutes can reduce sodium content by 9–23%, depending on the vegetable. Alternatively, opt for plain, frozen vegetables, which are low in sodium yet convenient (26).\nProcessed cheeses, including pre-sliced American cheese and loaf-like processed cheese like Velveeta, tend to run higher in sodium than natural cheese.\nThis is partly because processed cheese is made with the help of emulsifying salts, such as sodium phosphate, at high temperatures, which makes a consistent, smooth product (27).\nInstead, opt for lower-sodium, natural cheeses, such as Swiss or mozzarella.\nThe portability of jerky and other dried meats makes them a convenient protein source, but salt is used heavily to preserve them and boost flavor.\nFor example, a 1-ounce (28-gram) serving of beef jerky packs 620 mg of sodium, or 27% of the RDI (30).\nIf you’re a jerky fan, look for meat from grass-fed or organically-raised animals, as they tend to have simpler ingredient lists and less sodium. But be sure to check the label (\nTortillas contain ample sodium, mainly from salt and leavening agents, such as baking soda or baking powder.\nAn 8-inch (55-gram) flour tortilla averages 391 mg of sodium, or 17% of the RDI. Therefore, if you eat two soft-shell tacos, you’ll get one-third of the RDI for sodium from the tortillas alone (\nIf you like tortillas, opt for whole-grain and consider how the sodium count fits into your daily allowance.\nNot only do cold cuts — also referred to as luncheon meats — and salami contain a lot of salt, many are also made with sodium-containing preservatives and other additives.\nSliced, fresh meat — such as roast beef or turkey — are healthier options.\nThe large salt crystals on top of pretzels are your first clue of their sodium content.\nA 1-ounce (28-gram) serving of pretzels averages 322 mg of sodium, or 14% of the RDI (\nYou can find unsalted pretzels, but they still shouldn’t be your go-to snack, as they’re usually made with white flour and have minimal nutritional value.\nA single 1-ounce (28-gram) dill pickle spear — the kind of pickle that might come alongside a deli sandwich — has around 241 mg of sodium, or 10% of the RDI (\nThe sodium in whole pickles adds up more quickly. A medium-sized dill pickle packs 561 mg of sodium, or 24% of the RDI. If you’re on a sodium-restricted diet, keep pickle portions small (\nYou may flavor foods with sauces either during cooking or at the table, but some of that flavor comes from salt.\nYou can find reduced-sodium versions of some sauces, including soy sauce, or make your own to keep levels low.\nIn a recent sampling of US packaged foods, a hot dog or bratwurst link averaged 578 mg of sodium, or 25% of the RDI (\nHowever, sodium ranged from 230–1,330 mg in the sampling of these processed meats, which suggests that if you read labels carefully, you may find lower-sodium options (\nStill, processed meats are best saved for an occasional treat rather than everyday fare. The World Health Organization cautions that eating processed meats increases your risk of certain cancers (\nYou may not think to check the sodium in a can of plain tomato sauce or other canned tomato products, but you should.\nJust one-fourth cup (62 grams) of tomato sauce has 321 mg of sodium, or 14% of the RDI (36).\nFortunately, canned tomato products without added salt are widely available.\nThough bread, buns and dinner rolls generally don’t contain shocking amounts of sodium, it can significantly add up for people who eat several servings per day (\nBagels are an especially big sodium contributor, as they tend to run large in size. One grocery-store bagel contains 400 mg of sodium, or 17% of the RDI (\nChoosing smaller portions of bread will help you cut back on sodium, and opting for whole-grain versions is healthier.\nLike other canned foods, canned meats are higher in sodium than their fresh counterparts, though some manufacturers may be gradually reducing sodium.\nIn a recent analysis, canned tuna averaged 247 mg of sodium per 3-ounce (85-gram) serving, or 10% of the RDI. This represents a 27% decrease in sodium content compared to several decades ago (\nIn another recent analysis, canned chicken or turkey had 212–425 mg of sodium per 3-ounce (85-gram) serving, which is 9–18% of the RDI (8).\nHowever, cured, canned meats, such as corned beef and pork, were significantly saltier — 794–1393 mg of sodium per 3-ounce (85-gram) serving, or 29–51% of the RDI. Pass these up for lower-sodium canned options or buy fresh (\nBoxed meal helpers contain pasta or another starch along with powdered sauce and seasonings. You typically just add water and browned ground beef — or sometimes chicken or tuna — then cook it on your stovetop.\nBut this convenience comes at a steep cost — there’s generally around 575 mg of sodium per 1/4–1/2 cup (30–40 grams) of dry mix, or 25% of the RDI (\nA much healthier and yet still quick alternative is to make your own stir-fry dish with lean meat or chicken and frozen vegetables.\nThis breakfast favorite packs its share of sodium even when it’s not smothered in gravy. The ones you make from frozen or refrigerated dough may be especially high in sodium, so limit biscuits to an occasional treat (\nIn a nationwide sampling in the US, one biscuit made from packaged dough averaged 528 mg of sodium, or 23% of the RDI. Still, some contained as much as 840 mg of sodium per serving, or 36% of the RDI (\nThis favorite comfort food is high in sodium, mainly due to the salty cheese sauce. However, a recent analysis suggests that manufacturers have lowered the sodium in macaroni and cheese by an average of 10% (\nIf you want to occasionally eat macaroni and cheese, consider buying a whole-grain version and dilute the dish by adding some vegetables, such as broccoli or spinach.\nMany frozen meals are high in sodium, some containing at least half of your daily sodium allotment per dish. Check the label of each variety, as sodium can vary widely within a specific product line (39).\nThe FDA has set a limit of 600 mg of sodium for a frozen meal to qualify as healthy. You can use this number as a reasonable sodium limit when shopping for frozen meals. Still, it’s healthier to make your own meals (\nUnlike other canned beans, you can’t rinse baked beans with water to wash away some of the salt since you’d be washing away the flavorful sauce as well (40).\nA 1/2-cup (127-gram) serving of baked beans in sauce packs 524 mg of sodium, or 23% of the RDI. Recipes to make baked beans at home may not have any less sodium, but you can modify them to reduce the added salt (41, 42).\nWhether in links or patties, sausage averages 415 mg of sodium per 2-ounce (55-gram) serving, or 18% of the RDI (\nFor good health, you should limit your use of these processed meats — regardless of the sodium count.\nMany people far exceed the maximum recommendation of 2,300 mg of sodium per day.\nIn addition, your risk of developing salt-sensitive high blood pressure increases with age.\nTo cut back on sodium, it’s best to minimize processed, packaged and restaurant foods, as they sneak in a lot of sodium you may not suspect.\nProcessed meats — such as ham, cold cuts, jerky, hot dogs and sausage — are especially high in sodium. Even plain, frozen shrimp is often treated with sodium-rich additives.\nConvenience foods — including boxed potatoes, canned soup, instant pudding, meal helpers, pizza and frozen meals — also tend to run high in sodium, as do salty snacks such as pork rinds and pretzels.\nSome manufacturers are gradually reducing the sodium in certain packaged foods, but change is happening slowly. Regardless, many of these foods are unhealthy anyway.\nIt’s always best to opt for unprocessed, whole foods."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:8651a572-4dfa-48b6-9dc9-1dea9fcdb60a>","<urn:uuid:77e5529e-0917-46e8-85fc-7fc3877ec009>"],"error":null}
{"question":"What essential elements should a BYOD policy include regarding device security and data ownership, and how should companies handle device wiping in case of employee termination?","answer":"A BYOD policy should require device registration, up-to-date anti-virus software, security protections like remote disabling capabilities, and procedures for reporting lost or stolen devices. It must clarify that all business-related content remains company property regardless of device ownership. For security, the policy should mandate automatic screen locks, strong PINs, complex passwords, and multifactor authentication. Regarding termination, companies must carefully consider device wiping policies, addressing whether they will wipe personal devices, if employees must be present during wiping, and how to handle situations where employees refuse device wiping. Companies should also account for cloud backup complications, as personal devices often automatically back up data to the cloud.","context":["With Christmas quickly approaching, employers should expect that their employees will be enjoying new technology devices entering the new year. And this means employers should expect new employment law compliance issues and technology risks for their companies.\nBring Your Own Devices and Employment Law Compliance Issues\nEmployee owned devices create a minefield for employers when it comes to employment law compliance issues. This is especially true when it comes to wage and hour claims. These claims generally arise out of allegations that an employer failed to pay overtime wages. Wage and hour claims may be brought under the federal Fair Labor Standards Act (FLSA) or state wage-hour statutes. In either circumstance, wage and hour lawsuits expose employers to significant damages (hundreds of thousands of dollars and up) and legal fees.\nCompanies that provide employees the ability to remotely access the workplace through the Internet or using company issued smart phones, tablets, computers, etc. or employee owned devices essentially allow for 24/7 access to the workplace. But this also means these employees are performing work they should be compensated for, including overtime. For a very good explanation of these wage and hour claims, see Caryl Flannery’s blog post, “Technology’s Got Me Working Overtime.”\nBring Your Own Devices and Technology Risks to the Company\nAs to the technology risks, last year we published a blog post about managing such risks. See Tis the Season to Tech the Workplace Halls – Managing Employee Owned Technology Devices. This article highlighted four of the major risks that employee-owned devices create for employers.\nOne way to mitigate these risks is through the use of an appropriately drafted technology policy. Such policies, sometimes referred to as a “bring your own device” or a BYOD policy, can either be a stand-alone policy or incorporated into an employee manual.\nA BYOD policy should be considered a “must have” for any employer whose workforce is allowed and/or expected to use their own smart phones, tablets or other mobile devices for work either while at the office or during nonworking hours. As to what should be in your company’s technology policy, the details should be discussed with management, your IT department, and legal counsel. The intent of bringing together these stakeholders is to make sure you end up with a workable policy that meets and balances the business needs with IT security and legal compliance issues.\nRecommendations for What to Include in Your Company’s BYOD Policy\nA discussion of specific language that you should include in your company’s technology policy is beyond the scope of this blog (the working draft our law firm uses for its business clients spans several pages). However, several points for discussion include:\n- A process for registering or accounting for all employee devices. This should also include a means to confirm each such device is up-to-date as to anti-virus software and other security-related protections, e.g., tracking or remote disabling in the event a device is lost or stolen.\n- Implementing a procedure for reporting any device that is used for business purposes and that has been lost, stolen, or accessed by unauthorized persons or otherwise compromised.\n- Provisions making it clear that your company’s policies covering the handling and protection of its confidential information and intellectual property, including trade secrets extend to the use of all devices, whether provided by the company or owned by the employee.\n- Making it clear that all content created on, transmitted to, received or printed from, or stored or recorded on any device that relates in any way to your company’s business remains the the property of the company, regardless of who owns the device used.\nEmployees should also be advised and consent to some degree of employer access to a device in order for the employer to monitor and enforce the technology policy. Unfortunately this should include investigating any instances of employee misconduct that may involve the device.\nFor more information about technology related employment policies, including BYOD policies, contact Jason Shinn, an employment attorney who has addressed technology legal issues in the workplace since 2001. This technology experience includes investigating employee computer misconduct, managing e-discovery and litigation hold issues, and emerging social media legal issues. With this insight, Mr. Shinn works with companies to address the important steps that an organization should undertake when implementing a successful BYOD program – one that contains appropriate policies, procedures, and security measures to protect your company’s business information and network.","Employee Personal Device Usage BYOD in the Workplace\nConsiderations for Implementation of BYOD Policies\nIf your organization’s employees are permitted to connect personal smartphones or tablets to the company network, implementing an effective Bring-Your-Own-Device (BYOD) policy is necessary to protect against malware, hackers, and data exfiltration. Users’ personal devices can facilitate the propagation of viruses and worms, expand the attack surface by increasing the number of potential entry points and allow their owners to download and leave with sensitive, proprietary company data.\nWithout a clearly defined BYOD policy, employees will not know the company’s expectations or what activities are prohibited. Employers also have some difficult decisions regarding exercising control over their employees’ personal devices and data.\nDefining usage parameters\nBefore you can begin to develop a BYOD policy, you’ll need to determine the extent to which your employees will be permitted to use their devices while connected to the company network. Some organizations have a separation between the visitor and internal Wi-Fi networks. Suppose your employees will only be allowed to use a visitor Wi-Fi connection that provides Internet access but not internal company resources. In that case, your BYOD policy could consist of a simple statement in the HR manual limiting the connection of personal devices to visitor Wi-Fi. If, on the other hand, employees will be allowed, or perhaps encouraged, to use their devices for work-related tasks, you’ll need a much more comprehensive policy.\nWhich devices are allowed?\nDepending on how employees will use their devices, your BYOD policy may need to include limitations on the types of authorized devices for connection to your network.\nWill your employees be using their portable devices to access company applications? Might there be compatibility issues relating to mobile device operating systems? If so, are they optimized for use with mobile devices? Will your employees only use their devices to make and receive work-related phone calls and access company webmail? Given that different devices have different operating systems and capabilities answering questions like these may help you decide which devices should be allowed to connect and which should not.\nDevice security requirements\nIf you allow personal devices to access internal resources, you’ll want to ensure that those devices are not infected with malware that could propagate within your environment. The policy should require that employees using their personal devices at work install and maintain antivirus/anti-malware software.\nPortable devices can be lost or stolen. Automatic screen lock functionality with a strong PIN or biometric authentication should be required, as should complex passwords and multifactor authentication for network access. Depending on your environment and the sensitivity of the data, there may be regulatory requirements to consider. You may also wish to consider other tools, such as VPN applications, to increase security.\nWill you offer support for personal devices?\nAnother question to consider when developing your policy is whether and to what extent your organization’s IT team will support personal devices. The answer may lead you to limit the use of those devices depending on the added burden on your staff.\nIf you elect to provide support for these devices and their users, you’ll need to establish clear parameters. A best practice would be to exclude hardware support altogether. Hardware issues should be resolved by vendors and others authorized to support personal devices so that no warranties are voided, and your organization cannot be held responsible for any physical damage done.\nWill your organization allow users to install proprietary software on their devices? If so, will you provide application support for those devices? What will you do if the user has installed other apps on the device that prevent the proprietary application from running? A recommended action would be to explain the problem and let the user know that it is up to them to uninstall the personal application to facilitate using their device to run the company app or use a different device. It is, after all, the user’s personal device. This fact can lead to a separate set of issues should a dispute arise.\nRemote device wiping and termination of employment\nAs previously pointed out, these are personal devices paid for and maintained by their users. Perhaps the most critical consideration when determining the extent to which personal device usage will be permitted is whether you intend to require users to provide you with remote wipe capabilities. From the users’ perspective, those willing to pay for a device then use it for the company’s benefit may strenuously object to a policy that could seemingly punish them for doing so.\nWhile the company webmail app may not store messages and attachments on the user’s device, other proprietary applications could download sensitive data onto these devices. Likewise, BYOD users may have the ability to download files from company servers. Your policy should spell out what data can be downloaded and stored.\nThere is always a possibility that a user will violate a policy concerning the storage of company data. Your BYOD policy should clearly state the potential consequences associated with such a violation. If applicable, this should include wiping the user’s device. This measure would clear the company data and wipe out all of the user’s personal information, possibly leading to heated disputes for reasons previously stated. There is another consideration as well. Many personal devices are set to run backups to the cloud automatically. Even if you could wipe the user’s device, what recourse would you have regarding the backed-up data?\nFinally, you’ll need to develop a policy regarding employee termination, whether voluntary or involuntary. Will you wipe their personal device? Will the employee be present to oversee the activity? What if the employee refuses to allow wiping of the device?\nAllowing your employees to connect personal devices to the company network could, depending on the level of access, require you to implement policies that may, under certain circumstances, result in the deletion of their personal data. The use of personal devices could also dramatically expand your company’s attack surface, increasing the chances of a breach. For these and other reasons outlined herein, decisions relating to personal device usage should only be made after careful consideration of the possible ramifications."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:21728825-f395-440a-80e6-41ee615a99c3>","<urn:uuid:84696cb3-c99e-4828-8667-5bb21257e7f1>"],"error":null}
{"question":"What are the benefits of modern mentoring systems for educational development, and how do they promote inclusive leadership in innovation?","answer":"Modern mentoring systems benefit educational development by providing real-time interactions between high school and college students through technology, allowing authentic exchanges about academics, time management, and future planning. Pre-service teachers gain valuable insights from current high school students, while high school students receive direct guidance about college life and career preparation. These systems promote inclusive leadership in innovation by creating structured support networks. Research shows that diversity in leadership leads to better market outcomes, with diverse companies being more likely to capture new markets and increase market share. Programs specifically target underrepresented groups, such as women, to bridge gaps in innovation leadership, as evidenced by the low representation of women in patents (only 18.8% of patents had women inventors in 2010) and corporate leadership positions.","context":["Recruitment through Mentoring: The Set-Up\nPermission slips need to be filled out by participating high school students and their parents for such an activity. A teacher who is instructing high school juniors or seniors can email a college professor they may have had in class, or a willing and identified university, and describe this program and its purpose.\nThrough initial email communication, both the teacher and professor can decide on topics that may be relevant for students to communicate with one another. I recommend a structured approach as opposed to an open forum. However, open appropriate questions are always allowed.\nThrough implementation of College Mentoring 2.0, I elicited the assistance of a University teacher education professor in another state. These participating college students were enrolled in the teacher education program as pre-service teachers. The class was made up of roughly 20-25 junior level college students.\nFrom a teacher education standpoint, this interaction provided college students with face-to-face communication with high school students. The professor’s goal was to give his junior level pre-service students an opportunity for high school students to describe the makeup of the modern high school environment, their perceptions of effective teachers, and the integration of technology. A high school student’s perspective was regarded as a credible, authentic, and a very current perspective. The college students were very excited about the possibilities for both learning and mentoring.\nTopics of Discussion\nQuestions from pre-service college students to the high school students included, but were not limited to:\n- What do you look for in an effective teacher?\n- Describe behaviors of positive teachers and negative teachers.\n- Briefly describe effective classroom lessons.\n- What recommendations would you have for someone interested in being a new teacher?\n- How do you want teachers to handle conflict and violence in the classroom or within the school?\n- What are the biggest problems you experience in the school environment and why?\n- College students are then able to provide advice. Topics included but were not limited to:\n- What to avoid in high school\n- How to maintain healthy relationships\n- How to apply to college\n- Effectively beginning the first year of college\n- Making friends in college\n- Taking care of yourself in college\n- Advice on how to be successful both in high school and within their future endeavors\nHigh school students were then able to ask college students a variety of questions and elicit advice. Questions included but were not limited to:\n- If you could go back and do high school over again, what would you change?\n- Do you have advice regarding friendships or relationships in college?\n- Can you describe the college enrollment process?\n- How hard is college?\n- What are college classes like?\n- How do you do well in college with so many classes?\n- How do you manage your free time?\n- Do you work while you are taking classes?\n- If I don’t want to go to college right out of high school, what do you recommend I do?\nRecruitment through Mentoring 2.0: The Results\nThe authenticity and importance of questions and answers from both parties is revealing. Questions about academics, friendships, time management and tips filled the online sessions.\nThe technology permits real-time interactions with credible sources. Pre-service teachers learn from high school students. High school students who may never have had the chance to talk to college students are now being given the chance. High school students, who may have serious questions, are now getting answers.\nIn our case, the participating college professor entertained the participating high school students with his own evolution from a disinterested high school student to successful college professor. Participants demanded at each session’s conclusion that College Mentoring 2.0 “should happen more often.”\nHigh school teachers and college professors are encouraged to apply ubiquitous 21st century access and technologies to reinvent the dated methods of communication between high school and college students. College Mentoring 2.0 may not only be a futuristic college recruitment method, it may also be a new methodology for college students to impact the commitment, performance, and success of high school students.\nFinally, College Mentoring 2.0 may assist those who are seeking more immediate advice on how to be successful in today’s schools and communities.","The Collaboratory for Women Innovators seeks to inspire, educate, and empower women to attain leadership in all phases of the innovation lifecycle. Diversity is critical to the success of innovation in the U.S., and research shows that there is still a significant disparity in the numbers of women entrepreneurs and innovators. The Collaboratory seeks to bridge that gap by supporting participants at various stages of personal and professional development.\nA portfolio of resources and training opportunities ranging from structured programming to informal networking, workshop and special topic speakers are offered, designed to address diverse goals and objectives. Programs and resources are designed to support women starting their own companies as well as increase participation of female inventors/researchers. Additional activities provide opportunities for women to learn more about innovation, entrepreneurship and leadership and connect with like-minded people and mentors.\nHere at the Collaboratory we have different types of programming to fit your needs…now and as you continue to grow. Are you looking for a co-working space with guided coaching? Are you looking to flush out your business idea? Do you just want to dip your toes in and come for a few hours to find out more about potential opportunities or learn more about a particular topic? The Collaboratory offers a variety of options ranging from structured programming to informal networking, workshop and speaker sessions. Our programming will continue to evolve so check in to see what’s happening! Sign up for our mailing list/newsletter and connect with us on social media.\nThe Collaboratory for Women Innovators seeks to inspire, educate, and empower women to attain leadership in all phases of the innovation lifecycle. We offer a variety of programs and experiences to support different goals for participants at various stages of their personal and professional development.\nThe genesis for the Collaboratory developed as an outgrowth of the successful Empowering Women in Technology Startups program (EWITS). Ewits began in 2012 and provides experiential hands-on entrepreneurial training and skills. The Collaboratory was originally envisioned as a next step in the entrepreneurial process to further support women starting their own companies. However, the mission has expanded to also include focus on increasing participation of female inventors, as well as providing opportunities for women who, although not currently actively pursuing a business startup, may be interested in learning more about innovation, entrepreneurship and leadership, as well as connecting with other like-minded people.\nDiversity is a crucial ingredient for innovation. Approaching problem-solving from unique viewpoints and bringing varied experiences to the table maximizes an organization’s ability to discover the most effective and efficient solutions. Research correlating diversity in leadership with market outcomes shows that companies with 2-D diversity out innovate and out-perform more homogenous companies: diverse companies were 45% more likely to report market share growth and 70% more likely to report that the firm captured a new market.\nWhile national trends show some improvement in gender parity, the data highlights the need for increased intervention strategies if progress is truly a goal. “Although women have more than quintupled their representation among patent holders since 1977, only 18.8 percent of all patents had at least one woman inventor in 2010, compared with 3.4 percent in 1977…and as of 2010 only 7.7 % of all patents listed a woman as the primary inventor.” In 2017 Forbes announced that the number of CEOs in its annual Fortune 500 list was at an all time high…32. So for this record-breaking year, women represented 6.4% of the total list. This is despite the fact that evidence demonstrates that including women in executive and board positions results in increased financial performance for organizations. Taking this a step further, a 2015 study estimated that increasing women’s equality could add $12 trillion to the global GDP by 2025 .\nTo fully capitalize on this opportunity diversity needs to be all inclusive: gender, race, culture, age, sexual orientation, ethnicity…and this list is far from comprehensive. While all these elements are important, here at the Collaboratory we have a specific focus on gender and innovation. That does not minimize the importance of the other variables – it just provides an opportunity to focus our areas of expertise to try and help increase representation for this population. This does not mean that we do not include men in the conversation – quite the opposite. For gender parity to occur all voices need to be heard and valued. While some of our programming is exclusively focused on women, we have other programs that are open to all, and we also hope to sponsor programs in the future that are specifically targeted for men. Please check back often to see our evolving list of programs – or feel free to reach out to us if you have programming ideas to share.\nArticles of Interest\nYou can engage with the Collaboratory in several ways: as a participant/mentee, mentor/educational partner or sponsor.\nMentorship is an important component of Collaboratory programming. With the help of a mentor, our participants can grow professionally, overcoming barriers and taking part in new innovative endeavors. The Collaboratory seeks mentors who have experience in venture creation and/or technology commercialization. Interested in becoming a member? Contact us!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:a3a663a2-0ecf-489e-ab3b-cd953b72ae55>","<urn:uuid:b8a1bf6a-43f8-417d-8940-65bbfb16a0f5>"],"error":null}
{"question":"What security measures are in place to protect homeowners when contractors need access to Wi-Fi irrigation systems?","answer":"The system is designed so that homeowners can share an access code through an app that allows contractors to control the system through the cloud, without giving them access to the homeowner's network. This approach addresses security concerns while maintaining necessary functionality.","context":["Incorporating Wi-Fi Technology into Your Irrigation Business\nAre you still in the dark about how to best incorporate Wi-Fi-based irrigation technology into your business?\nThe national Irrigation Association recently aired a webinar focused on the growing popularity of this technology, as well as the advantages and opportunities it brings to the landscape irrigation market. To purchase the webinar, click here.\nLandscape Management magazine recently spoke with webinar presenters Stuart Eyring, president of Hydro-Rain, and Chris Klein, CEO and co-founder of Rachio. Here are some highlights of that interview:\nHow They Work\nQ: How do Wi-Fi irrigation controllers work?\nChris Klein (CK): A Wi-Fi-based irrigation controller uses the homeowner’s Wi-Fi network to connect to the cloud. That’s where a lot of the process and scheduling takes place, and then that information is sent back down to the controller. You can have access to it through an app on any device you want—a desktop computer, mobile phone, tablet, etc.—and they all communicate with the same computers in the cloud.\nQ: Have you seen examples of Wi-Fi controllers being used to upgrade older systems?\nCK: Yes, this is happening at a rapid pace. Eighty-five percent of our customers are replacing working controllers, and it’s just as easy as replacing any other controller.\nQ: How do you program Wi-Fi controllers?\nStuart Eyring (SE): In terms of programming, the smartphone apps dramatically add to the ease of which programing is done—it’s much better than programming a typical display controller. But there’s a difference in comfort level in terms of where the user base is coming from. There is a transition point to getting people comfortable with this.\nWeather Station Access\nQ: Traditional smart controllers had their own weather instruments on-site, but Wi-Fi-based irrigation controllers now have access to millions of weather stations. How do they get evapotranspiration (ET) information?\nCK: We use a variety of weather data providers and run them through equations to get ET. This process is getting more and more sophisticated. The other cool thing is homeowners can choose a weather station, which promotes continued engagement with their irrigation system.\nSE: In our case, we use the National Oceanic and Atmospheric Administration’s National Weather Service database in the U.S. Internationally, we use a database out of Norway. But it actually can be very helpful to have a rain sensor at the location, as well, because it can improve reliability.\nQ: So you can add other sensors to a Wi-Fi controller?\nSE: Yes, we’ve seen an increase in the use of sensing devices like weather stations and moisture sensors on-site that improve the quality of data.\nConnections, Security and Updates\nQ: What happens to the controller if it loses the Wi-Fi connection?\nSE: The majority of the data is kept in the cloud, but there is a basic operating program that’s stored on the actual controller. While the controller won’t typically make any adjustments based on environmental conditions while in that mode, it will continue to run. When the connection is reestablished, the adjustments will begin again. This is typical across manufacturers.\nQ: How do you protect security in terms of Wi-Fi and passwords?\nSE: Security definitely can be a concern to a homeowner when they allow someone access to their network. But there is a difference between a contractor connecting to a homeowner’s network and connecting through the cloud. In an ideal case, the homeowner is sharing an access code through an app that would allow their contractor to control the system through the cloud, but not have access to the homeowner’s network.\nQ: What happens if I buy my controller today and in 60 days it’s out of date?\nCK: Updates to the firmware and the app happen automatically, so customers always have the latest and greatest version. In terms of hardware, who knows what will happen in the future, but as of now, our Generation 1 and 2 products work the same.\nSE: In most cases, you won’t even know the firmware has been updated unless you go in and look at it.\nOpportunities and Support\nQ: What are the business impacts and opportunities that can be enjoyed by contractors venturing into this arena?\nCK: There is a great opportunity to impact a contractor’s business by having a number of connected customers. By installing that product and working with them you have a connection with them. You can stay in touch, the homeowner knows where to go for help and it presents an opportunity for customer retention.\nQ: What about support? How do you help contractors when they are stuck?\nCK: We have a dedicated contractor phone line and can be reached through email and chat, too.\nSE: We have noticed that there is really more upfront hand-holding required. But once the Wi-Fi-based irrigation controller is installed and operating, support requirements go down. That’s because of the ease of the interface and how intuitive it is. Getting started can be challenging, but once contractors get the hang of it, it’s really very easy."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:b47809ef-195c-4158-8041-8fed007a7d5c>"],"error":null}
{"question":"¿Cuál es más profundo: el pozo de Chand Baori en India o el teatro antiguo de Delphi? ¿How do their architectural features compare?","answer":"The Chand Baori well in India is much deeper, extending 100 feet below ground with 3,500 steps arranged in 13 levels in an inverted V shape. In contrast, the ancient theater at Delphi was built into a hillside with 35 rows of stone benches that could accommodate 5,000 spectators. While both are impressive architectural achievements from ancient times, they served very different purposes - Chand Baori was built as a water supply solution, while Delphi's theater hosted musical contests during the Pythian Games from 590 BC onwards.","context":["Greece Athens tourist attractions\nThe Sacred Way\nThe Sacred Way is the main route through the Sanctuary of Apollo, leading from the gateway uphill 200 meters to the Temple of Apollo. Originally, it was lined with votive monuments and treasuries erected by various Greek cities, reflecting the diversity of the political pattern of ancient Greece. The monuments themselves have disappeared but many of their bases have survived.\nTemple of Apollo\nAll that remains of the Temple of Apollo, the most important building in the Sanctuary of Apollo, are the foundations. It was built on the same location three times and featured columns, sculptures, and statues inside. The present Temple of Apollo, built in Doric style in the fourth century BC, preserved the ground-plan of the earlier sixth-century-BC Archaic temple and re-used the old column drums, but the detailing is typical of the late-Classical period. It is here, in the adyton (inner shrine) that the Pythia (priestess) would sit and utter the words of the Oracle, sent to her by Apollo and interpreted by the priests.\nBehind the Sanctuary of Apollo is the sixth-century-BC Polygonal Wall, supporting the platform on which the Temple of Apollo stands. The stones that make up the wall are cut into unusual polygonal shapes (hence the name) and carved with ancient inscriptions. The wall's main face is 90 meters long and it was originally two meters taller than it is today.\nTreasury of the Athenians\nTreasury of the Athenians\nThe Treasury of the Athenians (built around 510 BC and re-erected 1903-06) is in the form of a Doric temple. It was built by the Athenians, using marble from the island of Paros, to house their offerings to Apollo. It is decorated with a fine frieze - what you see is a copy, the original is in the Delphi Museum.\nA flight of steps leads up to the theater. Dating from the fourth century BC, with later alterations during the Roman period, the theater could accommodate 5, 000 spectators on 35 rows of stone benches. It was built to host musical contests at the Pythian Games, which were held at Delphi from 590 BC onwards. Lying within the sacred precinct, it commands fine views of the entire site.\nThe Stadion, which lies at the highest part of the site, 50 meters above the Theater, was built in the fifth century BC but gained its present appearance under the Romans in the second century AD. Its tiered stone seating could accommodate 500 spectators. Like the Theater, it was built to host the Pythian Games, staged at Delphi from 590 BC onwards - athletic contests would have been held here.\nIn a rocky gorge to the east of the sacred precinct, you'll find the Castalian spring and the remains of two monumental fountains, with recesses in the rocks for votive offerings. Here, the Pythia (priestess) washed, as did those wishing to consult the oracle, purifying themselves before making their way to the Temple of Apollo.\nClose to the fourth-century-BC Temple of Athena, stands the Tholos, based on a circular plan with the remains of 20 Doric columns on the outside and 10 Corinthian columns in the interior. It was built around 380 BC, and though its exact function is unknown, it is regarded as a masterpiece of Classical architecture.\nDelphi - Precinct of Athena Pronaia Map\nDelphi Archaeological Museum\nLying between the ancient site and the modern town of Delphi, the archaeological museum displays a fascinating collection of finds from the site, including friezes, statues, votive offerings, and stele. Exhibits are displayed in chronological order and arranged in 14 rooms. There is also a café and a gift shop.\nDelphi - Museum Map\nExploring the Modern Town of Delphi\nThe present little town of Delphi (population 2, 300) is now a concentration of hotels, guest houses, restaurants, shops, and attractions catering to tourists. It was established in 1892, when the village of Kastrí, which had grown up on the site of the was moved to a new position one kilometer west to allow excavation of the ancient site. This is a good base for exploring the surrounding area.\nSkiing and other Outdoor Pursuits at Mount Parnassus\nSkiing and other Outdoor Pursuits at Mount Parnassus\nThe Delphi area offers plenty of scope for mountain walks and winter sports, mainly on Mount Parnassus (Parnassos), which rises 2, 457 meters, making it one of the highest mountains in Greece. Snow-covered in winter, Parnassus is home to the country's largest ski center, with 27 kilometers of ski runs.\nThe Mountain Village of Arahova\nThe Mountain Village of Arahova\nIn winter, wealthy Athenians like to visit the mountain village of Arahova (population 800), on the north slopes of Mount Parnassus, eight kilometers east of Delphi. Old stone buildings host old-fashioned hotels and guest houses with traditional decor, small romantic taverns serving local specialities such as hilopites (a type of pasta) and formaéla (cheese served grilled), and shops selling fluffy flokati rugs.\nblogging definition and example blogging business plan blogging without writing blogging platforms free blog niche ideas blogging after retirement makeup blogging blog layout templates fall into blogging blogging zarada blogging ideas blogging degree is blogging out of date blogging with medium blogging vs blog blogging in wordpress blogging topics blogging business plan vlogging camera blog feed format blogging at college sign up blogging account blobfish blog zanquetta building over blogging blogging definition pro blogging idea zibra blog before and after blogging kya hai blogging ideas for beginners blog made with divi blog set up blogging events near me blog landing page blog after 6 months blogging over 40 blogging of a product guest blogging off page blog definition blog pro 2.0 template bloggingpro wordpress theme blogging the boys difference between blog and article blogging name generator blogging guide substack blogging your book blogging for money blogging workshops near me tips before blogging blog as a business blogging dallas cowboys blog up north blogging equipment blogging in the classroom blog over 50 blog versus article blogging courses for beginners blog and mablog can you make more money blogging or vlogging can you make more money blogging or vlogging blogging questions for students blogging on medium blogging from home jobs blogging at 40 blog writers near me blog platforms free blogging definition blogging up meaning blog post title generator blog about love blog under 100 xanga blogging blog about life blogging youtube blog post examples blogging past participle bloggingpro theme blog on cruelty towards animals blog at niramaalaa.blogspot.com blog to pdf blogging platforms examples blog ui blogging your passion blogging through facebook blogging about your personal life translate blogging into kannada blogging about blogging blogging like website steven outside blog blogging set up blogging over blogging lessons can i retire now blog blog even 3 pro blogging tips blogspot blogging like apps blog gael even blog with pictures blogging through squarespace blogspot blog ideas for 2021 blog ux blogging degree is vlogging better than blogging blogging for business blogging during war blogging set up difference between blogging and affiliate marketing blogging as a hobby blogging uses blog synonym blogging jobs from home blogging about tv shows blog vs vlog what after blogging blog of doom blogging from scratch blog apps blogging success zone blogging sites blog layout blogging from the heart blogging in 2022 blogging pro blogging software translate blogging into kannada blogging the boys podcast blogging by maya blogging kit best buy blogging as a business model blogging as a career blogging for students blogging lamp blogging hoosier history blogging through mobile zen cart blogging blog pro for magento 2 blogging definition and example perso blog among us blogging to relieve stress blog xiaomi earning thru blogging blogging niche ideas blog from phone blogging business ideas can i earn from blogging blog pro bn career after blogging blogging is dead blogging step by step blog around the world fall into blogging psychology behind blogging blog xd template blog in a sentence blogging vs vlogging blogging in past tense blogging takes off traduzione blogging revenue blog writing examples blogging of photography turn blogging into career pro blogging tips blogspot blog with blogger blog from google blogging apps blog background blogging lessons blogging without domain blogging as a job blogging jobs for moms blogging lamp blog about me blogging under a pen name beyond blogging blog without images blog via de la plata blog out of office what do you mean by blog blogging under your own name blog without social media blogging kit blog into mystery blog for money blog even stilstaan difference between blogging and affiliate marketing should i blog or vlog blogging apps that pay vlogging camera blogging as a job blogging unscripted bloggingpro reviews blog to book shutterfly blogging blast off blogging as a side hustle blog pro pakatan harapan difference between blogging and microblogging turn blogging into career blog in french money off blogging blog before flight blogging of photography blogging from home blogging groups near me blogging in zimbabwe blogging business blogging about tv shows add blog to next js blog post ideas over blogging definition blog meets brand blogging vs journaling beyond's blog homepage blogging by google blog into spanish blogging at work blogging past participle blogging niches blog over geld blog pro 2.0 template blogging legally blogging income after 1 year blogging or vlogging blogging your passion blog questions blogging name generator covid-19 blog topics at the bottom of everything blog blogging using google sites can bloggers make good money blog sites free blogging translate into bengali blogging under a pen name blogging software blog marketing blogging vs vlogging 2021 blog through google cool blog near me how to layout a blog blogging guide substack blogging to make money blogging job description blogging of e commerce blog like quora zibra blog before and after blogging courses diff between blog and vlog blogging about blogging pro blogging tips blogging memes blog username ideas blogging is blog with pictures blogging over thyme blogging uses blogging courses for beginners blogging equipment considerations before blogging blogging out meaning bloggings by boz blogging takes off traduzione blogging from your phone blogging at 70 blogging conferences 2022 blog search blog synonym blogging from your phone blogging essentials career after blogging blog topics during coronavirus blogging images difference between blogging and microblogging secret behind blogging bloggingx pro free download blog maker free blogging about art blogging about pets blogging under pseudonym blogging websites blogging without wordpress blog but.fr blogging from your phone blogging your passion blogging by email blogging of computer blog for business blog via google blogging to relieve stress difference between blogging and social media blogging with medium blogging salary blog around meaning blogging platforms examples blogging is dying out blog among us blog disney blogging through the fourth dimension blogging quora blogging conferences 2022 blogging income after 1 year blog design blog with ben blogging bishop blogging on instagram blog questions for students blogging without investment near blog fall into blogging blogging vs vlogging 2021 what after blogging over blogging definition blogging through google blog on google sites\nShare this Post\nAthens tourist attractions\nI stayed at Plaka Hotel for four nights this week. I stayed in Room 610, facing the Acropolis, which is a very narrow end…Read More\nGreece tourist attractions\nGreat weather, beautiful waters and awash with antiquity, it’s no wonder Greece is the destination of choice for a multitude…Read More","The Colosseum, the Great Pyramid of Giza, the Great Wall of China and Machu Picchu are world-famous ancient architectural wonders, but they’re hardly the only man-made structures worthy of effusive praise, enthusiastic photography and economy-stimulating tourism. These 7 historical sites, ranging from an incredibly deep well in India to the cradle of Mayan civilization – complete with the world’s first highway system – are often overlooked, but represent some of the most jaw-dropping and mysterious engineering feats from ancient times to the medieval period.\nChand Baori, India\n(images via: moolf)\nPerhaps one of the most beautiful examples of patterns in architecture, the 10th century Chand Baori well in the Indian state of Rajasthan is the world’s deepest, extending 100 feet below the surface of the earth. Built as a solution to chronic water supply issues in this arid region, the well has a total of 3,500 steps in 13 levels arranged in an inverted ‘V’ shape and is adjacent to the Harshat Mata temple. The walls are so steep that when standing at the bottom, you sometimes can’t see people who are on the steps above you.\nIt’s difficult to imagine the construction process for such a complex stone structure with the technology available at the time. Local legend has it that ghosts built it in a single night; perhaps that accounts for its preternaturally preserved state as well.\n(images via: world-mysteries)\nHow did the Incas move these massive stones? That’s just one of the mysteries surrounding Sacsayhuaman, an immense fortress located on the outskirts of the city of Cusco in Peru. While the much more famous Machu Picchu is renowned for its views, Sacsayhuaman is a marvel of engineering, confounding Spanish conquerors who were so amazed by the construction, they thought it must be the work of demons.\nThe largest of the boulders that make up the three dry stone walls of Sacsayhuaman – all carried from a quarry located over three kilometers away – weighs an estimated 120 tons. But the seemingly superhuman feat of moving these boulders is not the most incredible aspect of the ruins: even thousands of years later, the stones of the walls fit together with such precision, you can’t fit a piece of paper between them. This precision, along with the various stone shapes that fit together like a puzzle, is likely the reason that the structure has survived earthquakes that have devastated the area.\nLeshan Giant Buddha, China\n(images via: wikimedia commons, national geographic)\nThe largest carved stone Buddha in the world towers over 232 feet into the air, with fingers measuring 11 feet in length and 92-foot-long shoulders big enough to be basketball courts. Leshan Giant Buddha overlooks the confluence of three rivers in the Sichuan Province of China. Begun during the Tang Dynasty in the year 713, the Buddha was built at the behest of a monk called Hai Tong who hoped to supplicate the temperamental water spirits thought to be responsible for numerous boat accidents. It took thousands of workers more than 90 years to complete the project.\nSeemingly cosmetic details are even more complex and meaningful than they look upon first glance. For example, the 1,021 buns in the Buddha’s coiled hair are part of drainage system that continues behind the ears, in the clothing and along the limbs, protecting the statue from water-related damage.\n(images via: travel this world)\nA massive urban complex laid out to celestial, geographic and geodetic alignments, the Teotihuacan archaeological site in the Basin of Mexico contains some of the largest pyramidal structures built in the pre-Columbian UKs. The city was established around 100 BCE and may have had as many as 200,000 inhabitants during its prime in 450 CE. It has been called the first true urban center in the UKs; its remains measure at least two miles across but the city was likely much larger and its influence extended as far away as Guatemala. Very little is known of the Teotihuacan people or what may have caused the city’s decline, which occurred in the 8th or 9th century.\nAn astronomer-anthropologist named Anthony Aveni discovered that the grid of the city was based on a point of prime astronomical significance. The builders seem to have aligned the east-west axis of the city to the point on the horizon at which the sun sets on August 12th, the anniversary of the beginning of the current MesoUKn calender cycle.\nStrangely, thick sheets of shimmery mica were found within the tiers of the Pyramid of the Sun. Hidden between layers of stone, the mica clearly wasn’t decorative; today it is used as an insulator in electronics but it seems unlikely that these ancient people understood such properties. Furthermore, the particular type of mica used in the complex was reportedly traced to Brazil, nearly 2000 miles away. The Pyramid of the Sun has never been fully excavated.\nUnderground Churches of Lalibela, Ethiopia\n(images via: wikimedia commons)\nThe tiny town of Lalibela, one of Ethiopia’s holiest cities, is home to 11 monolithic churches – all carved from the same block of red volcanic rock, with their roofs at ground level. Likely built during the 12th and 13th centuries, the rock-hewn churches include four that are fully free-standing, with the rest either partially attached at the sides to the rock or with ‘liberated’ facades. They’re connected to each other with a maze of underground tunnels, and their construction was engineered to take advantage of natural aquifers deep in the ground.\nEl Mirador, Guatemala\n(images via: authentic maya, wikimedia commons, the history blog)\nThe 500,000-acre site of El Mirador in Guatemala is referred to as ‘the cradle of Maya civilization’ and contains not only five Preclassic Maya cities that pre-date the far more famous Tikal by at least 1,000 years, but also the world’s largest pyramid by volume and the remains of the world’s first highway system.\nA remote site located deep in the jungle, El Mirador was’t ‘discovered’ until 1926, and wasn’t mapped until 1978. The civic center of the site measures about 10 square miles and contains around 35 ‘triadic’ structures, with ‘La Danta’ being the most notable at 230 feet tall. Its volume, 2,800,000 cubic meters, rivals that of far more well-known ancient pyramids around the world including those in Egypt. El Mirador is also home to a complex network of large roads, which once linked important architectural compounds and nearby cities.\nThe 500,000-acre site of El Mirador is threatened by looters, drug traffickers and deforestation, prompting the creation of a 810,000-acre national park in the region, which is currently being established by the Global Heritage Fund and the Guatemalan and U.S. governments.\nThe Lost City of Mohenjo-Daro, Pakistan\n(images via: wikimedia commons, abbas, national geographic)\n4,500 years ago, Mohenjo-Daro was one of the largest early urban settlements in the world. It thrived for over a thousand years, but was completely forgotten until excavation revealed its ruins along the Indus River floodplain of what is now Pakistan in 1921. Abandoned around 1500 BCE for reasons unknown, Mohenjo-Daro has a planned layout based on a street grid of buildings made of mortared brick and likely housed around 35,000 residents.\nAmong many interesting features, what stands out the most about Mohenjo-Daro is plumbing and sewage system that was more sophisticated than what most Western households had until the 20th century. Not only did some home have indoor toilets, but there were actually sewage drains that ran below the streets.\nWant More? Click for Great Related Content on WebUrbanist:\nDid aliens build a toilet in China? Maybe not, but these 10 fascinating historical artifacts from around the world certainly provide fodder for strange theories.\n6 Comments – Click Here to Read More »»\n[ By Steph in 7 Wonders Series & Architecture & Design & History & Factoids & Travel & Places. ]\nVisitors Who Read This Post Also Read\n- Ancient Engineering Fail: 12 Historic Structural Disasters\n- Engineering Marvel: The Mysterious Ruins of Nan Madol\n- Submerged Cities: 7 Underwater Wonders of the World\n- Engineering Marvel: The Mysterious Ruins of Nan Madol\n- Monastic Marvels: 12 Cliffside & Mountaintop Monasteries\n- Kim Jong Il Leaves an Unusual Architectural Legacy\n- 13 Tilted Architectural Wonders of the World\n- Alternative Landmarks: 12 Monuments As They Almost Were"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:6e7c56ce-d3ee-4aaa-ae31-fccab9344920>","<urn:uuid:a122d94c-b3c9-4731-93b4-e1aa1796e6c8>"],"error":null}
{"question":"How do community health centers serve immigrant populations, and what training do healthcare professionals need to effectively work with these communities?","answer":"Community Health Centers and Federally Qualified Health Centers (FQHCs) serve as critical resources for immigrants by offering comprehensive primary care services including medical, dental, and behavioral health services on a sliding fee scale based on income. To effectively serve these populations, healthcare professionals need specific training in cultural competence, which includes understanding the context and motives for immigration, learning about the characteristics and belief systems of immigrant clients, and gaining practical experience through community visits and interactions. They must also develop research skills to access demographic and health information about immigrants and learn to advocate for needed changes to promote immigrant health.","context":["Immigrant Health Issues PA 5451\nAn on-line course for graduate students interested in Public Policy, Education, Public Health, Nursing, Social Work and related fields. The course may be taken for 3 or 4 graduate credits (4 credits with a final project; 3 credits without final project).\nThe demography of American communities is changing dramatically, but many of our institutions have not kept pace with the needs of new African, Asian, Eastern European, and Latino residents. Health care and social service providers are used to treating European-origin families and some Latino residents are suddenly seeing refugees from Somalia, Ethiopia, Laos, Bosnia, Cambodia, and the Sudan. In order to meet the needs of these new residents, it is imperative for providers and policymakers to understand the context and motives for immigration, as well as the characteristics and belief systems of their clients.\nProfessor, Section 001\nKatherine Fennelly, Ph.D.\nHubert H. Humphrey Institute of Public Affairs\nProfessor, Section 002\nRobin Councilman, M.D.\nFamily Medicine/Community Health\nHealth care providers; policymakers; community agency professionals; students in public health, medicine, nursing, social work, public affairs, education, or the social sciences\nStudents taking this course will gain an understanding of the characteristics of immigrants and their families in the United States, major health needs, principles of cultural competence in service provision, and tools for effective advocacy.\nThe key to becoming \"culturally competent\" is to go into the community to meet and learn from the residents you hope to serve. Community visits, observations, and interviews are an essential (and fun!) component of the course and the credit requirements.\n* Acquire research skills to access demographic, health and background information on immigrants in the U.S.\n* Understand the major characteristics and health needs of new immigrants\n* Design \"culturally competent\" health programs\n* Learn to advocate for needed changes to promote immigrant health\n* Interact with other professionals and policymakers\nUnit 1 - Research skills to access demographic, health and background information\nUnit 2 - Characteristics and health needs of new immigrants\nUnit 3 - Culturally competent care\nUnit 4 - Advocacy\n* Online delivery. No in-person meeting times required. Instructor may invite students to an optional meeting.\n* 14 weeks (fall semester)\n\"This course is up to date, and very relevant. I know without a doubt I will use many of the things I've learned in my future practice, and furthermore, I now have a resource to go to when these situations arise.\"\n\"Although Immigrant Health Issues it is an online class, it has provided wonderful opportunities for community work and interaction with other students, the instructor, and the T.A. This makes the course especially valuable for working professionals because it combines the benefits of modern technology with those of the traditional learning techniques.\"\nImmigrant Health Issues PA 5451\nFrequently Asked Questions\nWhat's an 'on-line' course?\nStudents can enroll from anywhere in the United States, and take the class from their home or office computers. Students will have access to the class website, which contains a wealth of information on immigrant and refugee issues Course texts are available from the U of M Bookstore. Students purchase dvds with segments on each of the course 'modules' which will also be available in the U of M Bookstore a few weeks before the beginning of the class. The dvds include interviews with immigrants and refugees, and with experts from the Center for Cross Cultural Health, the Center for International Health at Regions Hospital, the Otto Bremer Foundation, and Advocates for Human Rights. Community visits to sites in your geographic area are an integral part of the course and provide an opportunity to learn to become culturally competent.\nDo we all need to be on computers at the same time?\nStudents complete assignments, community visits, and reading on the days and times that suit them best, but all weekly assignments are due on the same day so that students may read and comment upon their classmates' postings on the course website. Additional interaction with the instructor and with other students occurs through the website email system. During the semester we ask students to schedule \"on-line chats\"with one another on topics of mutual interest. Students can select evening or daytime for these chats.\" It's important to note that assignments are due weekly, and we follow the University's semester schedule for the course (This is not a 'work at your own pace' course). Some students may do their postings on the web at 11 a.m., while others may do it at 11 p.m. (or at any time during the day or night).\nWhat are the course requirements for 3- or 4-credit registration?\nBecause we do not meet in person, all interactions are on the course website. Each week there are brief quizzes on the reading, and assignments which involve posting something on the website. These \"postings\" on the course bulletin board are our class discussions. Students will do web-based research, and make visits to interact with immigrants and refugees. These reading and research assignments can also be used to work on the final project (only for students taking the course for 4 credits; 3-credit students do not write a final project).\nFinal Project Instructions: (for 4 course credits)\nPolicy Recommendation Memo and supporting proposal: The final project will be a policy memo and supporting documents (15-20 pages, double-spaced). The memo and documents highlight a specific health or access problem facing immigrants or refugees in the U.S., and propose a policy change that can be implemented by a specific individual or agency or legislative body. More detailed instructions are included on the website.\nA few examples of the types of problems that could be examined would be:\n* Lack of sufficient interpreters or culturally competent providers hospitals for a particular population.\n* Overcrowding in housing units leading to high eviction rates of Latino immigrants.\n* Low rates of enrollment into Medical Assistance and MinnesotaCare despite high eligibility rates on the part of particular groups.\n* High volume use of the emergency room for primary or non-emergency visits from a particular group.\n* High HIV or tuberculosis prevalence (or other preventable disease) among a particular immigrant group.\n* High rates of domestic violence among a particular immigrant group.","Table of Contents:\nHealthcare is a fundamental aspect of well-being, and immigrants in the United States should have access to medical services and support, regardless of their immigration status. While healthcare options can vary depending on your immigration status, there are several avenues for immigrants to receive medical care and assistance. In this blog post, we’ll explore the healthcare options available for immigrants in the United States.\n- Emergency Medical Care Regardless of immigration status, all individuals in the United States have the right to receive emergency medical care. Hospitals and emergency rooms are legally obligated to provide treatment in life-threatening situations or when there’s an imminent risk to health, regardless of whether the patient has insurance or the ability to pay.\n- Community Health Centers Federally Qualified Health Centers (FQHCs) and Community Health Centers (CHCs) serve as critical resources for immigrants, especially those with limited financial means. These centers offer comprehensive primary care services, including medical, dental, and behavioral health services, on a sliding fee scale based on income.\n- Medicaid Medicaid is a joint federal and state program that provides healthcare coverage to eligible low-income individuals and families, including immigrants. Eligibility criteria vary by state and can depend on factors such as income, household size, and immigration status. Some states offer Medicaid to certain immigrant populations, such as refugees and asylees.\n- Children’s Health Insurance Program (CHIP) CHIP is a state-administered program that provides health insurance for children in low-income families. Like Medicaid, eligibility criteria and coverage options vary by state. Many immigrant children, regardless of their immigration status, may be eligible for CHIP.\n- Affordable Care Act (ACA) Marketplace Plans The ACA established health insurance marketplaces where individuals and families can purchase private health insurance plans. Immigrants who are lawfully present in the United States, including Green Card holders and those with certain visas, may be eligible to purchase coverage through the marketplace and may qualify for subsidies to make coverage more affordable.\n- Safety Net Hospitals and Clinics Safety net hospitals and clinics often serve vulnerable populations, including immigrants without insurance. They provide essential healthcare services and may offer sliding fee scales based on income.\n- Nonprofit and Community Organizations Various nonprofit and community organizations across the United States offer healthcare services and assistance to immigrants. These organizations may provide support with navigating the healthcare system, connecting individuals with resources, and advocating for immigrant health rights.\n- Free or Low-Cost Clinics Some regions have free or low-cost clinics that provide basic medical care, preventive services, and referrals for specialized care. These clinics may cater to uninsured or underinsured individuals, including immigrants.\n- Local Public Health Departments Local public health departments often offer immunization services, maternal and child health programs, and disease prevention initiatives that are accessible to immigrants.\n- Navigators and Enrollment Assistance Organizations and individuals known as “navigators” or “enrollment as sisters” can help immigrants understand their options, apply for Medicaid or marketplace coverage, and navigate the complexities of the U.S. healthcare system.\nAccess to healthcare is a crucial right for all individuals, including immigrants, in the United States. While navigating the system may be challenging, there are various options and resources available to help immigrants receive the medical care they need. It’s essential to research and seek assistance from local organizations to understand and access the healthcare options that apply to your specific immigration status and circumstances. Good health is a cornerstone of a fulfilling life, and every individual in the United States deserves the opportunity to achieve it."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:91178e7a-1ba7-4a62-af1d-0556341494f6>","<urn:uuid:c52c94d4-8d6b-489e-97ec-aaac01331181>"],"error":null}
{"question":"Being new to waste management policies, could you explain how India and Japan's approaches to managing food and waste disposal differ, for example in terms of concrete initiatives?","answer":"India and Japan take notably different approaches to waste management. India struggles with basic waste segregation, lacking uniformity in its policies across states (some areas use 2-bin systems, others 5-bin systems) and faces poor implementation of existing regulations. In contrast, Japan has implemented innovative solutions like the 'EcoBuy' project, where consumers receive points for buying foods near expiration dates through a smartphone app. Japan also has specific industry initiatives, such as relaxing delivery restrictions on foods close to best-before dates and modifying date labeling to reduce waste. While India generates 62 million tonnes of garbage annually with most going unsegregated to landfills, Japan is actively working to reduce its 6.21 million tons of annual food waste through systematic programs and technology-driven solutions.","context":["- India generates about 62 million tonnes of garbage every year\n- More than 45 million tonnes of waste in India remain untreated\n- Currently what is missing in India is the practice of Waste Segregation\nWhen you are throwing waste in the bin that doesn’t mean that it is the end, the process of managing waste, in fact, begins from there. Segregation of waste should the first step of waste management. Yet in India this process is non-existent. Today, India alone generates more than 1,00,000 metric tonnes of solid waste every day, that’s higher than many countries’ total daily waste generation taken together. And, the number is continuously rising because nothing is being done about ‘Waste Management’ in India.\nWhen it comes to managing the waste, India is among the poorest of all countries. Currently, every year approximately 31 million (70-75%) of waste is dumped straight into the landfill sites – in an unhygienic manner. The end result is that we have already run out of landfill space. All major landfills in India like Deonar in Mumbai, Ghazipur in Delhi and Kodungaiyur in Chennai, to name a few, are way-past their capacity and are over-flowing dreadfully. So, what exactly are we doing? Every problem has a solution but it seems that India’s waste miseries don’t have any shorter way out yet.\nThe Problem: Waste Segregation – A Concept Which Is Unknown To Many\nOne of the biggest issues when it comes to tackling waste management in India is unawareness and education among the waste generators. Waste is the result of human activities; if we as waste generators don’t have a proper understanding of waste issues then the success of even the best-conceived waste management plan becomes questionable.\nThe grim reality in India is that there is no connection between the waste generators and the policy makers of waste management. People are ignorant, they don’t know from where to start and what to do. Even though India is sitting on a ticking garbage bomb, we as waste generators continue to generate as much as we were generating a few years ago, says Arun Krishnamurthy, Founder of Environmentalist Foundation Of India.\nThe Growing Need Of One Nation, One Policy\nEven though there are separate bins available at some of the public places in India in order to attempt waste segregation at source, people throw waste randomly in any bin without any attention to the coloured bins. This can be due to two reasons – the careless attitude of waste generators towards waste management and lack of uniformity in colour codes of these bins. As per Bureau of Indian Standards, green bin is for biodegradable waste, blue or white bin is for dry recyclable waste and the red bin is for domestic hazardous waste.\nHighlighting the problem with India’s waste management system, Swati Singh Sambyal, Programme Manager, Environmental Governance (Municipal Solid Waste) from Centre for Science and Environment adds, Even though government is doing their best to integrate waste segregation at source, the question is how many of us know about coloured bins? People don’t know about these things, because, firstly they are unaware, secondly even if they are aware, there are policy differences within states. Mysuru has a two bin model, Panaji has a five bin model, some have a two bin one bag model, but it is not uniform. As a result, there is a lot of confusion.\nColoured bins are one thing, there are various other rules and regulations for Waste Management in India which is passed by the government, but the issue which persists is its implementation. Reiterating this Arun Krishnamurthy adds, “When it comes to waste management, there are rules and regulation available but when it comes to implementation nothing is being done. The moment it comes down to municipal level for implementation, the guideline gets haywire and loses its track.”\nIn Need Of Hefty Penalties\nThere are ‘Spot Fines’ for littering or polluting the environment, but, still, the polluters continue to dump hazardous waste anywhere and everywhere. The reason is simple – There are no hefty fines for acts like these. Another reason is that the fines are way too less and is not uniform across the country.\nPeople continue to dump waste on our planet because they know nothing will happen to them, there is no one to punish them. They can continue doing what they are doing. The need of the hour is stringent action against these polluters, added Arun Krishnamurthy.\nWhat Our Government Is Doing\nThough there is no one holistic plan for waste management with the government as of now, there are few steps being taken by the government in order to deal with India’s growing waste woes. However, its implementation still remains a huge problem.\nThe Union Ministry of Environment, Forests and Climate Change came up with the new Solid Waste Management Rules 2016 (SWM) which has a guideline for all waste generators to segregate waste at source. The Ministry of Urban Development recently issued guidelines to all states civic bodies in order to encourage source segregation of waste by introducing a two-bin system – wet waste in Green and dry waste in Blue. This guideline if implemented effectively, will tackle the problem of uniformity in colours of bin across the country.\nWhy Waste Segregation Is Important\nLack of segregation is simply the reason why India is drowning in its own garbage. If we start segregating the waste into biodegradable (waste that can be quickly broken down by microorganisms in normal environmental conditions) and non-biodegradable (waste that takes time to break down naturally. This type of waste is mostly recyclable) then we can save our landfills to a great extent which is currently on the verge of dying. In India, the problem is that most of the waste ends up directly in the landfills without any treatment. End results – The overflowing landfills which is also the root cause of blocked drains, soil and water pollution.\nThumb rule for waste segregation should be to send a minimum amount of the waste to the landfills. The aim should be to reuse maximum of the waste. Talking about the benefits, segregating garbage helps pick out recyclable and reusable material from other waste matter which in turn helps in reducing the garbage overload from our planet. This process needs to be adopted by both waste generators and policy makers, no one alone can adopt this practice.\nExpert Speak: The Way Ahead\nLooking at the current scenario, if India continues to generate waste at the same speed as it is generating now then by 2030 we will need a landfill as big as Bengaluru to dump all the waste.\nArun Krishnamurthy says, When it comes to waste management, there are several rules and guidelines being passed by the government every now and then. What our country needs is one consolidated waste management policy then only we can manage our waste woes effectively. It is very much possible if other countries can do it, so can we.\nFollowing the footsteps of organisations or states that have managed to solve their waste issues should be the first priority for India. Swati Singh Sambyal adds, “If we talk about Delhi, how many successful waste management campaigns have happened in the past, wherein local residents or RWAs have been educated on the importance of segregation. Say, for example, Defence Colony, Vasant Vihar, to name a few. But then, these models are not being replicated. There is still a hindrance towards segregation because it requires extra work. People need to take waste management seriously; it is equivalent to getting up and brushing your teeth. The problem of waste management in our country is because we have not learned to segregate. That lesson has to be taught and taught well.”","Shopping app tested to reduce food loss in Japan\nThe Tokyo metropolitan government introduced last Friday a new model project in which it awards points to consumers who buy foods close to their best-before dates and consume-by dates in order to decrease the amount of food loss. NTT Docomo Inc., with a governmental subsidy for the model project, developed a smartphone app that can be used until the end of February, and will evaluate the project’s effect.\nThe project, called “EcoBuy,” started at a Mini Piago supermarket in Chuo Ward, Tokyo. Stickers with such phrases as “Item for EcoBuy — its best-before date must be within the following period of days” are put on the store shelves for 30 designated food items including daily dishes, sliced raw fish and milk.\nThe best-before date represents the period of time during which foods can be best enjoyed. The consume-by date is labeled on perishable foods.A homemaker visiting the store said, “I’d like to buy the items if it can help reduce food loss.” Terushi Ito, the president of 99ICHIBA Co., which runs the store, said, “If we can reduce the volume of food waste, the labor cost for disposing of it will also be decreased.”\nTo join the project, consumers using the app take pictures of best-before dates and consume-by dates of the food items as well as the relevant receipt, then send the pictures to the designated center. After the center confirms the purchase, the consumer will receive points equivalent to about 20 percent of the purchase amount.\nThe Tokyo metropolitan government will pay NTT Docomo up to ¥15 million for the project. Shops bear the cost of allowing customers to apply the points to later purchases. The app has functions of showing the customers latest information about the food items for the model project, alerting them that the best-before dates and consume-by dates of the items they purchased are approaching, and providing recipes for the items. “We seek to reduce food waste not only at shops but also at home so as to cut food loss in society as a whole,” an NTT Docomo official said. “We would like to confirm how much we can cut the amount of food loss and the project’s profitability through the experiment,” said an official at the government’s bureau of environment.\n6 million tons of food wasted\nAn estimated total of 6.21 million tons of food is wasted a year in Japan, prompting measures to be taken by food makers and retailers, according to the Agriculture, Forestry and Fisheries Ministry. The figure is about twice the amount of food the United Nations provides worldwide. It is also equivalent to a bowl of rice being discarded by every person every day.\nVarious measures are being taken to reduce the amount of food that is wasted. In May, the ministry and other organizations requested the food industry relax the practice of not delivering to retailers food that is close to its best-before date. In Kyoto, retailers extended the sales periods of food until close to their best-before and consume-by dates in a pilot program conducted in November and December.\nSome companies in the food distribution industry and food manufacturers have deleted the “day” and only indicate the month and year in best-before dates in an effort to reduce the amount of food that has no quality problems but remains unsold and has to be discarded. The move is gradually spreading.\nSubscribe to INQUIRER PLUS to get access to The Philippine Daily Inquirer & other 70+ titles, share up to 5 gadgets, listen to the news, download as early as 4am & share articles on social media. Call 896 6000."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:7c574ed9-4c8a-4fb7-a8e0-e50d82e24157>","<urn:uuid:32aa1683-92e5-48bc-aaf8-f0d25d5fb5c2>"],"error":null}
{"question":"What is the recommended duration for head lice treatments, and are there any specific storage requirements for the medications?","answer":"Most head lice treatments require two applications seven days apart. For example, dimeticone 4% needs to be left on for at least 8 hours each time, while malathion requires 12 hours per application. Regarding storage, medications like permethrin should be kept in a closed container at room temperature, away from heat, moisture, and direct light, and should not be allowed to freeze.","context":["What are the treatment options for head lice?\nTreatment is needed only if you see one or more live lice. Empty eggshells (nits) do not always mean that you are infested with lice. Nits can stick to hair even when lice are gone (for example, after treatment that kills the lice).\nCurrently, there are five main recommended options for clearing head lice, which work in three ways:\n- Mechanical method: wet combing using the Bug Buster® comb and method.\n- Insecticides which work in a physical way, by coating the lice and smothering them. Treatments which work this way include:\n- Dimeticone 4% lotion (trade name: Hedrin®).\n- Dimeticone 92% (trade name: NYDA®).\n- Isopropyl myristate and cyclomethicone solution (trade name: Full Marks Solution®).\n- Insecticides which work in a chemical way by poisoning the lice. In the UK the only one currently used is malathion 0.5% liquid (trade name: Derbac-M®).\nThe treatment chosen may depend on your personal preference and what you have tried before (if appropriate). Each treatment has a good chance of clearing head lice if applied or done correctly and if all affected people in the household are treated at the same time. Each treatment is briefly discussed below. However, for details of how to use each treatment, read the instructions that come with the packaging.\nDimeticone 4% lotion\nDimeticone is a silicone-based product. Dimeticone 4% (trade name Hedrin®) has a good safety record and is widely used in cosmetics and toiletries. You should apply the lotion to dry hair. This needs to be done twice - seven days apart. Each application is left on for at least eight hours (overnight) and then washed off with shampoo and water.\nDimeticone is thought to kill lice by a physical process rather than by any chemical effect. It is thought to work by blocking the tubes used by the lice to breathe and by blocking the way the lice pass out water, which kills them. However, it is not thought to kill unhatched eggs. This is why two applications are needed, seven days apart. The second application makes sure that any lice that hatch from eggs which survived the first application will be killed before they are old enough to lay further eggs.\nDimeticone is suitable for all ages, those with skin conditions and those with asthma. It is suitable for women who are pregnant or breast-feeding. It is available on prescription. You can also buy dimeticone over the counter (although not for children younger than 6 months of age). Over the counter, it comes in other formulations, such as a spray and a gel.\nDimeticone 92% spray\nDimeticone 92% spray (trade name NYDA®) is stronger than the 4% lotion and is thought to be more effective. It seems to be better at destroying eggs as well as killing adult lice. However, it should not be used if you are pregnant or breast-feeding. It also should not be used in children under the age of 2 years. It is available over the counter and on prescription.\nIt should be applied to dry hair and left for 30 minutes. After this, hair should be combed through with the comb provided to remove the lice. It is then left on for eight hours, before being washed off. Again, this should be repeated after seven days.\nIsopropyl myristate and cyclomethicone solution\nThe trade name for this treatment is Full Marks®. It works in a similar way to dimeticone. You apply the solution to the scalp and leave in place for 10 minutes. The hair is then combed with a fine-toothed comb to remove lice. Then wash using shampoo to remove the solution. Treatment should then be repeated in seven days' time. The second application makes sure that any lice that hatch from eggs which survived the first application will be killed before they are old enough to lay further eggs.\nThis treatment is suitable for those with asthma. It is not suitable for children younger than 2 years of age or for people with skin conditions. It is not suitable for pregnant or breast-feeding women. It is available on prescription and also to buy over the counter.\nMalathion 0.5% liquid\nMalathion is a chemical insecticide that has been used for many years to treat head lice. The malathion kills the lice. There are various brands but the one available on prescription is Derbac-M®. You can also buy malathion over the counter (although not for children younger than 6 months of age).\nIt is suitable for all ages and those with skin conditions. It may be used in pregnancy but is only advised if you have tried wet combing and dimeticone 4% and they have not worked.\nYou should apply the lotion twice - seven days apart. Each application is left on for at least 12 hours (overnight) and then washed off with shampoo and water. The second application makes sure that any lice that hatch from eggs which survived the first application will be killed before they are old enough to lay further eggs.\nWet combing treatment\nWet combing is a way of removing head lice without having to use a lotion to kill them. Briefly, the method is similar to wet combing (detection combing) described earlier. But, you need to do this several times, four days apart. You will need to do this on every member of the household who has head lice.\nIt takes up to an hour to do a wet combing session properly. You need the correct toothed detection comb as described earlier. You can buy these over the counter at pharmacies. The Bug Buster® comb and method have been tested in research studies and the kit is available on prescription as well as over the counter. Only one kit is needed for a family, as it is washable and reusable.\n- Wash the hair in the normal way with ordinary shampoo.\n- Rinse out the shampoo and put on lots of ordinary conditioner.\n- Comb the hair with a normal comb to get rid of tangles.\n- When the hair is untangled, switch to the detection comb.\n- Slot the teeth of the detection comb into the hair at the roots so it is touching the scalp.\n- Draw the detection comb through to the tips of the hair.\n- Make sure that all parts of the hair are combed by working around the head.\n- Check the comb for lice after each stroke. A magnifying glass may help.\n- If you see any lice, clean the comb by wiping it on a tissue, or rinse it before the next stroke.\n- After you have combed the whole head, rinse out the conditioner.\nYou need to do the above routine at least four times, every four days. The number of sessions required depends on the last time you see lice:\n- The first combing session should remove all hatched head lice but does not remove eggs. Therefore, lice that hatch from eggs after the first session may still be present.\n- Subsequent sessions clear newly hatched lice. Keep doing the combing sessions every four days until you have had three sessions where no lice are detected.\n- Once you have had three sessions where you do not see any lice, it usually means that you are then free of lice.\nAnybody of any age can have this treatment and it does not involve any chemicals. The downside to this treatment is that it is time-consuming.\nWhat about other treatments?\nVarious other insecticides have been used in the past. For example, permethrin is no longer recommended for head lice because there are concerns that many lice are now resistant to it. Phenothrin and carbaryl are no longer available in the UK.\nThere are various other treatments that are said by some people to work. For example, tea tree oil, quassia, other essential oils, herbal remedies and electric combs. However, there is a lack of research studies to confirm that they work well in most cases. Therefore, until more research is done, these other methods cannot be recommended.\nChecking for treatment success\nThe wet combing method of treatment discusses above how to check for success. For other methods of treatment (lotions, sprays, etc), check that treatment was successful by detection combing 2-3 days after completing a course of treatment. Check again after a further seven days. Treatment has been successful if no lice are found at both sessions.\nDid you find this information useful?\n- Head lice; NICE CKS, February 2015 (UK access only)\n- Head lice: Evidence-based Guidelines based on the Stafford Report; Public Health Medicine Environmental Group, 2012\n- Head lice (Pediculosis); Public Health England\nDisclaimer: This article is for information only and should not be used for the diagnosis or treatment of medical conditions. Patient Platform Limited has used all reasonable care in compiling the information but make no warranty as to its accuracy. Consult a doctor or other health care professional for diagnosis and treatment of medical conditions. For details see our conditions.","Generic Name: permethrin (Topical route)\nCommonly used brand name(s):\nIn the U.S.\nAvailable Dosage Forms:\nTherapeutic Class: Pediculicide\nChemical Class: Pyrethroid\nPermethrin 1% lotion is used to treat head lice infections. It acts by destroying both the lice and their eggs. The 5% cream is used to treat scabies infections by destroying the mites which cause scabies.\nIn deciding to use a medicine, the risks of taking the medicine must be weighed against the good it will do. This is a decision you and your doctor will make. For this medicine, the following should be considered:\nTell your doctor if you have ever had any unusual or allergic reaction to this medicine or any other medicines. Also tell your health care professional if you have any other types of allergies, such as to foods, dyes, preservatives, or animals. For non-prescription products, read the label or package ingredients carefully.\nStudies on this medicine have been done only in adult patients, and there is no specific information comparing use of topical permethrin in children with use in other age groups.\nMany medicines have not been studied specifically in older people. Therefore, it may not be known whether they work exactly the same way they do in younger adults or if they cause different side effects or problems in older people. There is no specific information comparing use of topical permethrin in the elderly with use in other age groups.\n|All Trimesters||B||Animal studies have revealed no evidence of harm to the fetus, however, there are no adequate studies in pregnant women OR animal studies have shown an adverse effect, but adequate studies in pregnant women have failed to demonstrate a risk to the fetus.|\nThere are no adequate studies in women for determining infant risk when using this medication during breastfeeding. Weigh the potential benefits against the potential risks before taking this medication while breastfeeding.\nAlthough certain medicines should not be used together at all, in other cases two different medicines may be used together even if an interaction might occur. In these cases, your doctor may want to change the dose, or other precautions may be necessary. Tell your healthcare professional if you are taking any other prescription or nonprescription (over-the-counter [OTC]) medicine.\nCertain medicines should not be used at or around the time of eating food or eating certain types of food since interactions may occur. Using alcohol or tobacco with certain medicines may also cause interactions to occur. Discuss with your healthcare professional the use of your medicine with food, alcohol, or tobacco.\nThe presence of other medical problems may affect the use of this medicine. Make sure you tell your doctor if you have any other medical problems, especially:\nThe presence of other medical problems may affect the use of topical permethrin. Make sure you tell your doctor if you have other medical problems, especially:\nThis section provides information on the proper use of a number of products that contain permethrin. It may not be specific to Nix. Please read with care.\nKeep this medicine away from the eyes. If you accidentally get some in your eyes, flush them thoroughly with water at once.\nPermethrin lotion which is used to treat lice, comes in a container that holds only one treatment. Use as much of the medicine as you need and discard any remaining lotion properly.\nFor the treatment of head lice (1% lotion):\nHead lice can be easily transferred from one person to another by direct contact with clothing, hats, scarves, bedding, towels, washcloths, hairbrushes and combs, or hairs from infected persons. Therefore, all members of your household should be examined for head lice and should receive treatment if they are found to be infected. If you have any questions about this, check with your doctor.\nFor the treatment of scabies (5% cream):\nThe dose of this medicine will be different for different patients. Follow your doctor's orders or the directions on the label. The following information includes only the average doses of this medicine. If your dose is different, do not change it unless your doctor tells you to do so.\nThe amount of medicine that you take depends on the strength of the medicine. Also, the number of doses you take each day, the time allowed between doses, and the length of time you take the medicine depend on the medical problem for which you are using the medicine.\nStore the medicine in a closed container at room temperature, away from heat, moisture, and direct light. Keep from freezing.\nKeep out of the reach of children.\nDo not keep outdated medicine or medicine no longer needed.\nTo prevent reinfection or spreading of the infection to other people, good health habits are required. These include the following:\nAlong with its needed effects, a medicine may cause some unwanted effects. Although not all of these side effects may occur, if they do occur they may need medical attention.\nSome side effects may occur that usually do not need medical attention. These side effects may go away during treatment as your body adjusts to the medicine. Also, your health care professional may be able to tell you about ways to prevent or reduce some of these side effects. Check with your health care professional if any of the following side effects continue or are bothersome or if you have any questions about them:Less common or rare\nOther side effects not listed may also occur in some patients. If you notice any other effects, check with your healthcare professional.\nCall your doctor for medical advice about side effects. You may report side effects to the FDA at 1-800-FDA-1088.\nThe information contained in the Thomson Healthcare (Micromedex) products as delivered by Drugs.com is intended as an educational aid only. It is not intended as medical advice for individual conditions or treatment. It is not a substitute for a medical exam, nor does it replace the need for services provided by medical professionals. Talk to your doctor, nurse or pharmacist before taking any prescription or over the counter drugs (including any herbal medicines or supplements) or following any treatment or regimen. Only your doctor, nurse, or pharmacist can provide you with advice on what is safe and effective for you.\nThe use of the Thomson Healthcare products is at your sole risk. These products are provided \"AS IS\" and \"as available\" for use, without warranties of any kind, either express or implied. Thomson Healthcare and Drugs.com make no representation or warranty as to the accuracy, reliability, timeliness, usefulness or completeness of any of the information contained in the products. Additionally, THOMSON HEALTHCARE MAKES NO REPRESENTATION OR WARRANTIES AS TO THE OPINIONS OR OTHER SERVICE OR DATA YOU MAY ACCESS, DOWNLOAD OR USE AS A RESULT OF USE OF THE THOMSON HEALTHCARE PRODUCTS. ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE OR USE ARE HEREBY EXCLUDED. Thomson Healthcare does not assume any responsibility or risk for your use of the Thomson Healthcare products."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:00fb4ecd-ee15-41e9-bbfb-eadc599599c8>","<urn:uuid:06d92515-9526-49cb-845f-d7b04aef2753>"],"error":null}
{"question":"I'm a photography student interested in video production. How do depth of field characteristics compare between DSLR cameras and traditional camcorders?","answer":"DSLRs and camcorders have significantly different depth of field characteristics due to their sensor sizes. DSLRs, which typically have full-frame or APS-sized sensors, produce a shallower depth of field. While this can create a film-like appearance, it can be problematic for video as the slightest movements can lead to autofocus hunting and objects slightly in front or behind the focus point appear fuzzy. In contrast, camcorders with their smaller sensors (like 1/2.88) provide much greater depth of field because they use shorter focal length lenses for the same picture size. At small apertures and focal lengths shorter than 50mm equivalent, it's actually difficult to be out of focus with a small-sensor camera. This makes camcorders particularly advantageous for travel, wildlife, and documentary video work where maintaining focus across multiple planes is important.","context":["In photography the perspective rendering, that is the reproduction of the distance between the planes, which can be amplified, normal or compressed, is determined by the shooting distance.\nLet’s imagine two people standing in line one meter from each other. If the shooting point were one meter in front of the first person, the second person would be two meters from the camera, which is double the distance from the first person. If the shooting point were instead ten meters in front of the first person, the second person would be eleven meters from the camera, ie 1/10 more than the first person. In the first case the two subjects would appear further away from each other, because one would be twice the distance from the camera compared to the other, while in the second case they would appear closer to each other, because they would be one 1/10 more than the distance. from the room relative to the other.\nIn summary, the further the point of recovery is from the subjects, the more the distance between the subjects is compressed; the closer the shooting point is to the subjects, the more the distance between the subjects is amplified.\nTwo equivalent focal lengths, i.e. two focal lengths that each used on a different format offer the same angle of view, produce the same perspective yield because they are used at the same distance from the subject to frame the same field. For example, with the same field of view, a 50mm on MFT format produces the same perspective rendering of a 100mm on Full Frame. If the working aperture is the same, however, the depth of field varies, which is more contained with the longer focal length.\nIf both photos had been taken using a Full Frame sensor, again without changing the shooting distance, and had cropped the central part of the image made at 50mm to reproduce the picture obtained at 100mm, the same result would have been obtained. over it.\nAs expected, the two images differ only as regards the depth of field, which in the image taken at 100mm is more contained. Compared to larger sensors, smaller sensors only cut out a central portion of the image projected by the lens; this is where the expression crop factor comes from.\nIt is therefore logical that if you take two photos with the same lens and at the same distance from the subject, using first a larger sensor and then a smaller sensor, you will get two images with different angles of field but with the same perspective rendering and the same depth of field because the two images thus obtained are essentially each other’s clipping.\nNow let’s keep the field of view constant. To get the same picture with both 50 and 100mm we are forced to work at different distances. To change this time is therefore the perspective rendering, more compressed in the image taken at 100mm because taken at a greater distance from the subject. However, the depth of field does not change.\nOn the same format, with the same aperture and field of view, all focal lengths develop the same depth of field because the different shooting distance compensates for the differences between the focal lengths.\n- The greater the shooting distance, the greater the depth of field and perspective compression.\n- The shorter the shooting distance, the shorter the depth of field and the greater the perspective dilation.\nAt this point it is evident that angle of view, perspective yield and depth of field are not absolute characteristics of the lens but depend on the size of the sensor in use, the distance from the subject and the working aperture.\nThe only characteristics of a lens are the coverage circle, which determines which format the lens is able to cover with its projection, the focal length, i.e. the distance between the plane of focus and the optical center of the lens, and brightness, which is determined by the diameter of the entrance pupil of the objective.\nThe entrance pupil is the optical center of the lens, it is the point where the light rays captured by the front lens are made to converge. The larger the entrance pupil, the shorter the depth of field.\nClosing the diaphragm serves to reduce the diameter of the entrance pupil in order to increase the depth of field, to correct any optical aberrations and to decrease the amount of light reaching the sensor. When the diaphragm is set to the maximum aperture, the entrance pupil maintains its maximum diameter.\nIn photographic lenses. such as DZO lenses, brightness is expressed in f numbers. These numbers are quotients, they indicate the ratio between the focal length and the diameter of the entrance pupil of a lens. A 50mm f / 2 therefore has an entrance pupil of 50mm / 2, or 25mm. It is for this reason that telephoto lenses are said to develop a smaller depth of field than wide angles, because for the same number f they have a larger entrance pupil. A 100mm f / 2, for example, has a 50mm entrance pupil, twice that of a 50/2.\nIt can therefore be established that:\n- The angle of view of a focal length is determined by the size of the sensor in use.\n- At the same distance from the subject, all focal lengths produce the same perspective yield.\n- Depth of field is determined by the distance to the subject and the size of the entrance pupil of the lens in use.\n- On the same format, with the same aperture and field of view, all focal lengths produce the same depth of field.\n- With the same entrance pupil and shooting distance, all focal lengths develop the same depth of field.\n- If a 50mm f / 2.8 shot offered us an excessively focused background, it would be useless to use a 100mm f / 2.8 and recompose the same shot away from the subject, because we would perceive a slight reduction in depth of field only due to the perspective compression that ” approaching ”the background would highlight the reduced sharpness. And while this would offer us a slightly more marked “detachment” between subject and background, the significant increase in perspective compression would work in the opposite direction. The only way to significantly reduce the depth of field of an image without changing the framing is to use a wider aperture.\n- It is possible to obtain the same image on different formats by compensating focal and aperture based on the crop factor. At the same distance from the subject, a shot at 150mm f / 4 on Full Frame will have the same angle of view, the same perspective rendering and the same depth of field as a shot at 75mm f / 2 on MFT format (150/2 = 75 and 4/2 = 2). Clearly, given the different working aperture, when switching from one format to another it is also necessary to modify the exposure using ND filters, acting on the ISO sensitivity or altering the shutter speed.\nWhy then is it said that a wide angle lens enhances the distance between planes and has a wide depth of field while a telephoto lens is said to compress the planes and have a shallow depth of field? Because considering their use on the same format, due to their angle of view and their brightness, the way in which these focal lengths are usually used produces the commonly declared effect. However, it is easy to see how the perspective rendering and depth of field of a lens change drastically according to the distance from the subject, demonstrating how relative these two parameters are.\nThe “normal” perspective rendering\nThe distance between the planes perceived with the naked eye is reproducible from a focal length equal to the diagonal of the format in use. In the case of the Full Frame, whose dimensions are 24x36mm, the normal focal length is therefore 43.27mm.\nOn any format, the lens with the normal focal length produces an angle of view of approximately 53 ° and is itself called normal.\nIn the photographic field and in relation to Full Frame sensors, however, it is the 50mm that is usually referred to as normal. This habit dates back to the beginning of the last century and I can only speculate how it came about. The 50 was the standard lens combined with the cameras because it was simple to design and cheap to make, even if it had a good level of brightness and optical quality. Furthermore, on Full Frame the 50 covers a rather versatile angle of view and therefore represented an excellent all-rounder in a time when zooms did not yet exist. For these reasons, since the dawn of photography, the 50 has taken the place of the 43mm, a focal length rarely produced (there is a Pentax FA 43 / 1.9) because it is too similar to the widespread 50mm.\nThe diaphragm and diffraction\nClosing the diaphragm excludes from the formation of the image the light beams that cross the front lens in the more peripheral areas and which would therefore reach the focal plane in a more angled way, creating comet-shaped signs on it instead of points well defined. This results in an increase in the sharp area on the image, i.e. the depth of field, as well as an increase in the optical correction of the lens, especially at the edges of its projection.\nClosing the diaphragm beyond a certain limit, however, causes a lowering of the general sharpness as it forces the light beams to pass through a hole so small as to cause excessive diffraction; for this reason it is a good idea to exceed f / 11 only for macro applications with specific lenses. Most lenses reach their qualitative peak when the aperture is closed 2-3 stops.\nThe circle of confusion and depth of field\nThe size of the circles of confusion (also called circles of confusion ) is the maximum size that a point out of focus can have to still be perceived by the observer as a point and therefore in focus. This dimension varies based on several factors, including the observer himself. This makes it impossible to objectively establish what the depth of field of an image is. By convention, following tests carried out on a sample of observers, it was established that on a 20x25cm print observed at a distance equal to its diagonal, i.e. 32cm, the diameter of the circles of confusion corresponds to 0.25mm. Beyond this measurement the points are perceived as discs rather than points and the image they form appears out of focus.\nBased on these results, adapted to the acquisition system, the reproduction medium and the observation distance in play, all calculations relating to depth of field are performed. These calculations produce results that most people find very optimistic; for this reason these results are often adjusted downwards considering the actual depth of field much more limited than what mathematically suggested. Generally, the values are scaled by 1-2 stops. To give an example, if you need 30 cm of depth of field and this value, according to mathematical calculations, is obtained at f / 5.6, it is considered necessary to turn between f / 8 and f / 11.\nFrom a physical point of view, since a lens can focus a beam of light on only one plane at a time, the area that is truly in focus in an image cannot be extended in terms of depth.","The useful article The Definitive Camera Guide in the new magazine, Digital FilmMaker, together with a discussion in Amateur Photographer on whether to buy separate video and still cameras, prompted me to remind myself, in the headlong rush to use full-frame and APS-sized cameras for video, why a small sensor, as found in most amateur, 'prosumer' and some professional camcorders, is preferable for the type of video photography that I do – travel, wildlife and documentary. Film makers which I define as those setting out to make a movie with actors or a staged documentary, appear to be fixated on getting a film-like appearance. Since the appearance of film can be more easily achieved with the depth of field range of a larger sensor (as with 35 mm cine cameras) some follow the fashion and use a large-sensored DSLR. The fact that we see in such videos endless focus shifts that the dramatic effect is expected and therefore ruined is neither here nor there. Such is the world of arty film making.\nFor travel, wildlife, family and documentary videos there is one great disadvantage in using a DSLR for video. The depth of field is simply too shallow. I have seen some awful efforts. Even if the camera can be focused, the slightest movements lead to the autofocus hunting to regain focus; objects slightly in front or slightly behind the point of focus that are needed to be sharp to provide context are fuzzy. DSLR video is fine for controlled shooting. The many rigs now available can be used; focus can be pulled manually after rehearsing the shot.\nCamcorders are not thought cool any more. But they do work and work very well, especially those in the prosumer ranges of the major manufacturers (even though the gimmicks, like the projectors now added by Sony for example are a real pain and very silly). The quality of the full HD video is superb. I have sold wildlife footage from my Sony camcorder.\nSo what is the advantage of using a small sensor for video? For stills, the bigger the sensor the better, particularly for the prevention of noise. The advantages of a smaller sensor in terms of depth of field are very great for travel and wildlife. In general, with the same final image size, depth of field increases inversely to the size of the format. That is because for the same picture size, a shorter focal length lens can be used with the smaller format. Shorter focal length equals greater depth of filed. Even though the circle of confusion has to be reduced as the format size decreases, the depth of field is still increased.\nThe first graph illustrates the point. I compare five formats: 1. Full-frame 35 mm; 2. APS (I know the various sensors vary in size a bit but I have taken one for comparison); 4/3 format as used on many modern non-DSLR cameras; 2/3 sensor as used on many professional camcorders; 1/2.88 sensor as used on many amateur, prosumer and some professional camcorders. I calculated for each the equivalent focal length of the lens and, using the circle of confusion appropriate to each format, read off the hyperfocal distance at f/11 and f/4. The graph shows the near point of acceptable focus (i.e. half the hyperfocal distance), the far point of course being infinity.\nThe difference in depth of field is clear and dramatic. At a small aperture and at focal lengths shorter than the equivalent of about 50 mm, it is difficult to be out of focus using a camera with a small sensor.\nI used the same data to calculate the depth of field with the camera focused at 3 metres, again at f/11 and f/4."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:2e3104b3-19a8-49b2-817b-12db5ce4e03b>","<urn:uuid:9747e165-061b-4f5d-9f62-3a6633bc5d6a>"],"error":null}
{"question":"What are the molecular mechanisms of drought response in plants, and how does moisture affect fungal disease development?","answer":"At the molecular level, drought triggers ABA hormone production which activates stress-responsive genes through ABA-dependent and independent pathways. This involves transcription factors, protein kinases, and enzymes regulating drought-inducible genes. Key drought response proteins include chaperones, LEA proteins, and enzymes for osmolyte production. Regarding fungal disease development, moisture conditions significantly impact pathogen growth - specifically for tomato leaf mold, high humidity above 85% promotes spore germination, with optimal spore production occurring at 75-90% humidity. The fungus spreads readily in moist conditions through water splash, air currents and contaminated tools, leading to leaf tissue death and plant decline if humidity isn't controlled through proper ventilation and plant spacing.","context":["Also in this issue...\nTomato Leaf Mold\n| Helen Margaret Griffiths\n>> Published Date: 3/26/2014\nProduction of tomatoes under high tunnel and plastic has increased significantly over the last few years in part due to consumers demand for “local” produce. Even though growing under these conditions can reduce the occurrence of some diseases, it can increase the occurrence of others. Tomato leaf mold disease is one that’s showing a significant increase. It’s caused by a fungus formerly known as Cladosporium fulvum, but now known as Fulvia fulva by producers and those in the seed trade, and Passalora fulva by mycologists. The disease is rarely seen on field-grown plants, and when it’s observed in the field, it’s due to infected greenhouse-grown transplants.\nFigure 1. Symptoms characteristic of the early stage of tomato leaf mold. The yellow lesions on the upper leaf surface have poorly defined margins.\nWhy the increase in appearance in the plastic houses and high tunnels? Judson Reid from Cornell Cooperative Extension in Penn Yan, New York, who works extensively with greenhouse crop producers, says, “The use of susceptible field varieties (both determinates and heirlooms) within the high relative humidity environment of tunnels is to blame.” Spores can remain viable on plant debris and in soil. If producers don’t rotate crops in the tunnels, this can result in an increased incidence of disease.\nWhat are the sources of inoculum and methods of disease spread? Spores of the fungus overwinter on plant debris, volunteer tomato plants and in the soil where they can remain viable for at least a year. When humidity is over 85%, spores can germinate at temperatures from 39F to 93F (4C to 34C). The greatest spore production is observed when the humidity is between 75% to 90%, but spore production has been detected when humidity was as low as 58%. Infection occurs through leaf stomata with conidia being produced on the underside of the leaves. Spores are easily spread from plant to plant by air currents, water, tools, workers and possibly insects. Flowers may become infected, but they usually die before fruit set.\nSeed can be contaminated, although this isn’t usually the primary source of inoculum. As leaf mold can be a devastating disease for the tomato producer, it’s recommended that growers start with seed certified to be pathogen-free or that they hot water treat the seed prior to seeding.\nThere are weeds in the nightshade family susceptible to tomato leaf mold and hence these plants are a potential inoculum source. Both inside the hoop houses and surrounding areas should be monitored for the presence of these weeds.\nThe older leaves become infected first. Initial symptoms are pale green lesions, which rapidly enlarge and turn to yellow spots with poorly defined margins (Figure 1). These symptoms are sometimes confused with powdery mildew infections or nutrient deficiency. With F. fulva infections, the corresponding area of the lower leaf surface becomes covered with an olive/brown velvety growth, which are mainly spores of the fungus (Figures 2 and 3). As the infection progresses, the leaf tissue becomes chlorotic, the leaves curl and drop prematurely. The leaves die from the base of the plant and progress upwards (Figure 4) until the entire plant is dead.\nFigure 2. Olive/brown fungal spores on the underside of the leaf are typical of those observed with tomato leaf mold.\nIn addition to confusing with powdery mildew, growers may confuse leaf mold with grey mold caused by Botrytis cinerea. B. cinerea has more powdery spores than those observed with F. fulva. The fungal growth of B. cinerea is fluffier than with F. fulva. Even though powdery mildew infections usually begin with pale green or chlorotic areas on leaves, they can be differentiated from B. cinerea and F. fulva by the appearance of diffuse powdery patches, which often occur on upper and lower leaf surfaces. B. cinerea may colonize leaves already infected by F. fulva, masking the leaf mold symptoms and making the diagnosis more challenging.\nFigure 3. Close up of the spores on the underside of a leaf with tomato leaf mold.\nFoliage infections are the most common, although stems, petioles, blossoms and fruit infections can occur. Fruit infection isn’t very common, but both green and ripe tomato fruit can become infected (Figure 5).\nDepending exclusively upon chemicals isn’t a good management approach. In the case of leaf mold of tomato, using resistant varieties, utilizing sound cultural practices—particularly with regards to controlling humidity and sanitation—are the best ways to prevent and control occurrences of the disease. There are many tomato varieties with good resistance, but as there are at least 12 races of the fungus, tomato breeders have concentrated on breeding for resistance to only some. Not all varieties have been bred to give resistance to the same range of races and, therefore, growers are advised to choose those with resistance to the broadest list of races. Thomas Zitter from Cornell University in November 2013 generated a useful table on tomato varieties and their various attributes, including leaf mold resistance and seed source: http://vegetablemdonline.ppath.cornell.edu/NewsArticles/TMV%20and%20Leaf%20Mold%20Variety%20List.pdf\nReid says that for the Finger Lakes region of Central New York, his favorites with resistance are Primo Red, Red Deuce and Red Mountain (determinates) and Trust, Geronimo, Rebelski and Panzer (indeterminates).\nFigure 4. Tomato leaf mold killing tomato plants in a high tunnel.\nHumidity is a major driver in the epidemiology of leaf mold and, therefore, reducing the period of leaf wetness is critical. Growers should do everything possible to reduce humidity. This includes avoiding watering foliage—particularly in the evening—because depending on the temperature in the house, the leaves may remain wet overnight. Good ventilation is important in reducing the overall humidity of the high tunnel or greenhouse. Crowding of plants should be avoided, as disease severity is likely to be worse under conditions of high planting densities. Staking and pruning of plants can help with air movement around the plants.\nOnce disease symptoms are observed on plants, the leaves should be promptly pruned and destroyed. As tools and workers can easily transfer spores to new plants, care needs to be exercised in the pruning process. Plants should not be handled during times of high humidity.\nWhen leaf mold has been a problem, all plant material, including weeds, should be removed and destroyed, and the greenhouse disinfected.\nCrop rotation is important for controlling tomato leaf mold. Producers, whenever possible, should avoid growing tomatoes in the same location year after year.\nThere are a number of fungicides labeled that, used in combination with other disease management tools, may assist in controlling tomato leaf mold. For the organic producer, OMRI-approved products include Camelot O, Cueva and Oxidate.\nFigure 5. Tomato fruit damaged by tomato leaf mold.\nFor the conventional producer, basic copper is labeled and various products containing mancozeb (e.g. Manzate, Mankocide, Gavel) may give some protection. The efficacy of these products depends greatly upon timing of application and adequate cover of the plants. Quadris Top and Inspire Super will give some systemic activity, but the latter should not be used on cherry tomatoes, and Quadris Top should not be used for transplant production. More details are available in an article published by Thomas Zitter: http://vegetablemdonline.ppath.cornell.edu/NewsArticles/TEP%20Labeled%20Rates.pdf\nThe regulations for use of each product vary with geographic location and, depending on the fungicide mode of action, there may be resistance concerns. The specific directions on fungicide labels must be followed and supersede any statements in this article if there’s a conflict. GT\nPlease note: Any reference to commercial products, trade or brand names is for information only.\nHelen Margaret Griffiths is a freelance writer in the Finger Lakes area of Central New York.\n© Copyright 2001 - 2015 Ball Horticultural Company —","Molecular Analysis Of Dehydration In Plants Biology Essay\nPlants respond to survive under water-deficit conditions via a series of physiological, cellular, and molecular processes culminating in stress tolerance. Many drought-inducible genes with various functions have been identified by molecular and genomic analyses in various plants, including a number of transcription factors that regulate stress-inducible gene expression. The products of stress-inducible genes function both in the initial stress response and in establishing plant stress tolerance. In this short review, recent progress resulting from analysis of gene expression during the drought-stress response in plants as well as in elucidating the functions of genes implicated in the stress response are summarized.\nDrought has a major impact on plant growth and development, limiting crop production throughout the world. 28% of the earth's land is too dry for the crop production. The abiotic stresses that include a component of cellular water deficit, usually noted are salinity and low temperature stresses-can also severely limit crop production. Major research efforts are currently directed at understanding the mechanism of plant response to conditions in which water limits plant growth and development in order to identify gene products that confer adaptation to water-deficit stress. Mechanisms of response to water-deficit stress can be measured at many different levels from the whole plant to the molecular level. Since responses are controlled by the plant genome, recent efforts have focused on the molecular response of the plant to water-deficit stress. Better understanding of the genes that are expressed in response to water deficit are needed to characterize fully the mechanisms that permit adaptation to limiting water conditions. Many research programmes have focused on the cellular signalling mechanisms that are activated by water deficit stress (Shinozaki et al., 2003, Xu et al., 2009, Zhao et al., 2009). Several plants, with the greatest emphasis being placed for further understanding of the molecular mechanisms that underlie the plant response to abiotic stress. The extensive molecular studies involving the understanding of the model plants (Arabidopsis and rice) will translate to applications in the improvement of crop growth and production (Zhang et al., 2004).\nWe can help you to write your essay!\nSeveral major classes of genes have been noted that are altered in response to water-deficit stress; genes involved in signalling and gene regulation and gene products that are proposed to support cellular adaptation to water-deficit stress are among the most frequently altered in gene expression. Yet, the functions of the majority of the genes with altered expression remain unknown and there are probably more genes yet to be discovered. Microarray analyses, especially for the model plant Arabidopsis thaliana, have reached an advanced stage and are contributing to an understanding of the types and quantities of genes that are regulated by water-deficit stress (Seki et al., 2003). In this review we have discuss the plant dehydration stress molecular analysis.\n2. Gene networks involved in drought stress response and tolerance:\nDrought stress induces a range of physiological and biochemical responses in plants. These responses include stomatal closure, repression of cell growth and photosynthesis, and activation of respiration. Plants also respond and adapt to water deficit at both the cellular and molecular levels. The genes with diverse functions are induced or repressed by these stresses (Yamaguchi-Shinozaki and Shinozaki, 2005). Most of their gene products may function in stress response and tolerance at the cellular level. Significantly, the introduction of many stress-inducible genes via gene transfer resulted in improved plant stress tolerance (Zhang et al., 2004; Umezawa et al., 2006a). Recently, a number of stress-inducible genes have been identified using microarray analysis in various plant species. Now, analysing the functions of these genes is critical to further understanding of the molecular mechanisms governing plant stress response and tolerance, ultimately leading to enhancement of stress tolerance in crops through genetic manipulation.\nDrought triggers the production of the phytohormone abscisic acid (ABA), which in turn causes stomatal closure and induces expression of stress-related genes. Several drought-inducible genes are induced by exogenous ABA treatment, whereas others are not affected. Indeed, evidence exists demonstrating the presence of both ABA-independent and ABA-dependent regulatory systems governing drought-inducible gene expression (Liu et al., 2008; Yamaguchi-Shinozaki and Shinozaki, 2005). Both cis-acting and trans-acting regulatory elements functioning in ABA-independent and/or ABA-responsive gene expression induced by drought stress have been precisely analysed at the molecular level (Yamaguchi-Shinozaki and Shinozaki, 2005). Results have obtained through transcriptome analysis of drought-inducible gene expression in Arabidopsis and rice using microarrays, including information supporting potential functions of drought-inducible genes in stress response and tolerance.\n3. Transcriptome analysis of drought-inducible genes:\nMicroarray technology employing cDNAs or oligonucleotides is a powerful tool for analysing gene expression profiles of plants exposed to abiotic stresses such as drought, high salinity, or cold, or to ABA treatment. There are two predominant varieties of microarray technology available, the cDNA microarray and the oligonucleotide microarray, the most prominent being the Affimetrix GeneChip (Kreps et al., 2002). A 7000 full-length cDNA microarray was utilized to identify 299 drought-inducible genes, 54 cold-inducible genes, 213 high salinity-inducible genes, and 245 ABA-inducible genes in Arabidopsis (Seki et al., 2002a, b). More than half of these drought-inducible genes were also induced by high salinity and/or ABA treatments, implicating significant cross-talk between the drought, high salinity, and ABA response pathways. In contrast, only 10% of the drought-inducible genes were also induced by cold stress. Thousands of stress-inducible genes were identified using the Affimetrix GeneChip array containing oligonucleotides representing ~8000 independent Arabidopsis genes (Kreps et al., 2002). In Arabidopsis, transcriptome using the Affymetrix 23 000 ATH1 GeneChip, have generated thousands of transcriptome data points identifying genes expressed in various tissues and under defined growth conditions, stress induction, and phytohormone treatment (Schmid et al., 2005).\n4. Functions of drought-inducible genes:\nThe products of the drought-inducible genes identified through the microarray analyses in Arabidopsis can be classified into two groups (Shinozaki et al., 2003). The first group includes proteins that most probably function in abiotic stress tolerance. These include molecules such as chaperones, late embryogenesis abundant (LEA) proteins, osmotin, antifreeze proteins, mRNA-binding proteins, key enzymes for osmolyte biosynthesis, water channel proteins, sugar and praline transporters, detoxification enzymes, and various proteases. The second group is comprised of regulatory proteins, i.e. protein factors involved in further regulation of signal transduction and stress-responsive gene expression. These include various transcription factors, protein kinases, protein phosphatases, enzymes involved in phospholipid metabolism, and other signalling molecules such as calmodulin-binding protein (Xu et al., 2009). Many transcription factor genes were stress inducible, suggesting that various transcriptional regulatory mechanisms may function in regulating drought, cold, or high salinity stress signal transduction pathways. These transcription factors could govern expression of stress-inducible genes either cooperatively or independently, and may constitute gene networks in plants.\nThis essay is an example of a student's work\nStress inducible genes identified in Arabidopsis, rice and other plants can be classified into functional proteins and regulatory proteins (Rabbani et al., 2003). Comparative analysis of stress-inducible genes in Arabidopsis with those in rice revealed a considerable degree of similarity in stress responses between the two genomes at the molecular level. Among the 73 genes identified as stress inducible in rice, 51 have already been reported in Arabidopsis to perform a similar function (Rabbani et al., 2003). These results confirm that rice shares common stress-inducible genes with Arabidopsis, even though these two plants evolved separately more than a million years ago.\n5. Improved drought stress tolerance in plants via gene transfer:\nIntroduction by gene transfer of several stress-inducible genes has demonstrably enhanced abiotic stress tolerance in transgenic plants (Zhang et al., 2004; Bartels and Sunkar, 2005; Umezawa et al., 2006a). These particular genes encode key enzymes regulating biosynthesis of compatible solutes such as amino acids (e.g. proline), quaternary and other amines (e.g. glycinebetaine and polyamines), and a variety of sugars and sugar alcohols (e.g. mannitol, trehalose, galactinol, and raffinose).\nGenes encoding LEA proteins and heat shock proteins have also been used to improve drought tolerance in transgenic plants. A gene encoding galactinol synthase (GolS), a key enzyme involved in raffinose family oligosaccharide biosynthesis, was introduced to improve drought-stress tolerance in transgenic Arabidopsis (Taji et al., 2002). Prior analyses demonstrate that GolS genes are induced by drought, cold, and ABA. Moreover, expression of the gene encoding raffinose synthase is also induced by drought stress. Additionally, recent metabolome analysis indicated significant accumulation of both galactinol and raffinose under drought stress. Not only metabolites, but also some stress-responsive proteins such as LEAs, have also been implicated in detoxification and alleviation of cellular damage during dehydration. Other studies demonstrate that overexpression of some LEA class genes results in enhanced tolerance to dehydration, although the precise mechanism is still unknown. LEA proteins may also function as chaperone-like protective molecules to combat cellular damage (Umezawa et al., 2006a). Transcription factors have also proven quite useful in improving stress tolerance in transgenic plants, through influencing expression of a number of stress-related target genes (Yamaguchi-Shinozaki and Shinozaki, 2005).\n6. Molecular mechanisms improved stress tolerance:\nRegulatory factors, such as protein kinases and enzymes involved in ABA biosynthesis, are also useful for improving stress tolerance by regulating many stress-related genes in transgenic plants. ABA is synthesized de novo primarily in response to drought and high salinity stress. Genes involved in ABA biosynthesis and catabolism were identified based on genetic and genomics analyses (Nambara and Marion-Poll, 2005). It was demonstrated that overexpression of the gene encoding 9-cisepoxycarotenoid dioxygenase (NCED), a key enzyme in ABA biosynthesis, improves drought stress tolerance in transgenic Arabidopsis plants (Iuchi et al., 2001). Cytochrome P450 CYP707A family member was identified as ABA hydroxylase, an enzyme that degrades ABA during seed imbibition and dehydration stress. A T-DNA insertion mutant of CYP707A3, which is the most abundantly expressed gene amongst the four CYP707A members under stress conditions, exhibited elevated drought tolerance with a concomitant reduction in transpiration rate (Umezawa et al., 2006b).\nThe ABA-activated SnRK2 protein kinase (OST1/ SRK2E) functions in the ABA signal transduction pathway controlling stomatal closure (Mustilli et al., 2002; Yoshida et al., 2002). SnRK2 is a member of the SNF1-related PKase family, which contains 10 members in Arabidopsis and rice. SnRK2s are activated by drought, salinity, and ABA (Yoshida et al., 2002). SRK2E/OST1 is involved in stomatal closure, but not seed germination. Another SnRK2,SRK2C, is activated by osmotic stress, salt stress, and ABA treatment (Umezawa et al., 2004). SRK2C is strongly expressed in the root tip, and is involved in the root response to drought stress. SRK2C also functions in transgenic plants to improve stress tolerance, as many of the downstream genes it influences are stress inducible. In addition, SnRK2 protein kinases may activate transcription factors influencing osmotic stress-responsive gene expression.\n7. Regulation of gene expression:\nThe promoter of a drought-, high salinity-, and cold inducible gene, RD29A/COR78/LTI78, contains two major cis-acting elements, ABRE (ABA-responsive element) and DRE (dehydration-responsive element)/CRT (C-RepeaT), both of which are involved in stress-inducible gene expression (Yamaguchi-Shinozaki and Shinozaki, 1994, 2005). ABRE and DRE/CRT are cis-acting elements that function in ABA dependent and ABA-independent gene expression, respectively, in response to abiotic stress. Transcription factors belonging to the ERF/AP2 family that bind to these DRE/CRT elements were isolated and termed CBF/DREB1 and DREB2 (Yamaguchi-Shinozaki and Shinozaki, 2005). Their conserved DNA-binding motif is A/GCCGAC. The CBF/DREB1 genes are rapidly and transiently induced by cold stress, the products of which activate the expression of target stress-inducible genes (Jaglo-Ottosen et al., 1998; Liu et al., 1998). Overexpression of CBF/DREB1 in transgenic plants increased stress tolerance to freezing, drought, and salt stresses, suggesting that the CBF/DREB1 proteins function in the development of cold-stress tolerance without modification (Liu et al., 1998). Many CBF/DREB1 target genes have been identified using both cDNA and GeneChip microarrays (Fowler and Thomashow, 2002; Maruyama et al., 2004; Vogel et al., 2005). Most of the CBF/DREB1 target genes contain the DRE motif with a conserved (A/G)CCGACNT sequence in their promoter regions. The target gene products of these proteins are consequently involved in establishing stress tolerance. The DREB2 genes are induced by dehydration stress and may activate other genes involved in drought stress tolerance (Liu et al., 1998).\nEarn money as a Freelance Writer!\nWe’re looking for qualified experts\nAs we are always expanding we are looking to grow our team of freelance writers. To find out more about writing with us then please check our freelance writing jobs page.\nRice homologues of CBF/DREB1 and DREB2, 10 OsDREB1s and four OsDREB2s, respectively, have been identified based on rice genome sequence analyses. The function of these genes in stress-inducible gene expression has been demonstrated in rice. Overexpression of OsDREB1A in Arabidopsis revealed a similar function of the rice genes in stress-responsive gene expression and stress tolerance (Dubouzet et al., 2003). Overexpression of OsDREB1 or Arabidopsis DREB also improved drought and chilling tolerance in rice (Ito et al., 2006). These data indicate that similar transcription factors function in abiotic stress tolerance between dicotyledonous and monocotyledonous plants.\nSeveral drought-inducible genes do not respond to either cold or ABA treatment, suggesting the existence of another ABA-independent pathway regulating the dehydration stress response. These genes include ERD1, which encodes a Clp protease regulatory subunit, ClpD. The ERD1 gene is not only induced by dehydration but is also up-regulated during natural senescence and dark-induced senescence (Nakashima et al., 1997). Promoter analysis of the ERD1 gene in transgenic plants indicates that the ERD1 promoter contains cis-acting element(s) involved not only in ABA independent stress-responsive gene expression but also in senescence-activated gene expression. Analysis of the ERD1 promoter further identified two different novel cis-acting elements involved with dehydration stress induction and in dark-induced senescence (Simpson et al., 2003).\nTranscriptome analyses based on microarrays have provided powerful tools for discovery of stress-responsive gene in various crop plants and tree species. Transgenic plants generated to express antisense or RNAi constructs, as well as T-DNA- or transposon-tagged mutants, were used to analyse the function of these stress-responsive genes based upon phenotypes resulting from loss of function. Moreover, transgenic overexpressors were very useful not only for functional analyses of stress-inducible genes but also for demonstrating improved stress tolerance in these plants generated by gene transfer. Several factors related to the plant response to drought stress have already been shown to be effective for engineering drought tolerance. Such demonstrated success under experimental conditions has encouraged the use of this strategy to engineer drought tolerance in crop species. Since the application of functional genomics approaches, the identification of genes related to drought response has increased, thus fueling this approach for genetic engineering.\nThis work was supported by research grants (Grant Number NSC93-2811-B-002-021) from National Science Council of Taiwan Government.\nIf you are the original writer of this essay and no longer wish to have the essay published on the UK Essays website then please click on the link below to request removal:"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:7db73ae9-f7bf-44cd-833c-36169ba1c0da>","<urn:uuid:a7567360-7c6d-4906-800c-47f4d986ec03>"],"error":null}
{"question":"What are the essential components that must be included in a non-compete agreement for it to be legally useful?","answer":"A non-compete agreement must include several essential components: 1) The effective date and application date, 2) Clear reasons for establishing the agreement, such as protection of trade secrets, 3) Validity period in relation to when the employee leaves the company, 4) Geographic coverage and location restrictions, 5) Details about legal action in case of violations. Additionally, there are three basic parts that must be included: duration (typically 2 years or less), scope for confined work and services, and geography where the company operates. The agreement should also specify competition restrictions and potential damages for violations.","context":["Are you searching for non-compete agreement templates? Many businesses are always prone to the risk of capturing the market by new entries. Some businesses provide opportunities for startups to capture the market very quickly. Thus, the employers of such businesses always focus on establishing a non-compete agreement before hiring any employee on the key position.\nThe non-compete agreement helps the employer to legally bound the employee from entering into the same business as a competitor during or after the employment. However, these contracts mainly prevent the employees from entering into the direct competition market. The side businesses and support businesses are allowed. You may also like the Payment Agreement Template.\nTable of Contents\n- 1 What is a non-compete agreement?\n- 2 What are the basic parts of a non-compete agreement?\n- 3 What benefits employers get from a non-compete agreement?\n- 4 Why do Employers use the Non-Compete Agreement Template?\n- 5 How do I get out of a non-compete agreement?\n- 6 What should be included in Non-Compete Agreement?\nWhat is a non-compete agreement?\nA non-compete agreement is a formal agreement between an employee and employer after their relationship ends. It restricts the employee from working and from becoming a competitor for a particular time period. This agreement is also known as a non-compete clause or restrictive covenants.\nAfter signing a non-compete agreement, an employee wouldn’t enter in any business of the same nature or any other type of work that makes him a competitor and puts the secrets of your business at risk.\nWhat are the basic parts of a non-compete agreement?\nThere are three basic parts of a non-compete agreement that should include in the non-compete agreement;\n- Duration: The non-compete agreements hardly hold up in the court for a long period of time. Generally, the agreements are of 2 years or less. Short-term agreements are for 6 months or less. If the employee is terminated then you can also include the severance option.\n- Scope: For confined work and certain services this part must be restricted.\n- Geography: The region where the company does the business is a well-reasoned determination.\nMoreover, the non-compete agreement also involves the following components;\n- Competition: The employer should also tell about his competitors. It doesn’t mean you have to define the name of competitors individually, you just have to inform the kind of business and industries employees are stopped from working in.\n- Damages: This agreement explains that what loss an employer will get if an employee violates the agreement.\nWhat benefits employers get from a non-compete agreement?\nEmployers get various benefits from a non-compete agreement. This agreement restricts an employee that he can’t share industry experience, trade secrets, client lists, knowledge, potential clients, etc. Moreover, all confidential and proprietary information gets secured.\nIt also saves the employer workability of business, products, and processes. This agreement also saves the best interests of other working employees as it makes sure that their termination wouldn’t weaken their best interests.\nWhy do Employers use the Non-Compete Agreement Template?\nA company needs human resources in the form of permanent, part-time, or consultant employees. These employees work on the key competencies of the business. So, many business and trade secrets are accessible to these employees. Therefore, it becomes necessary for employers to establish a non-compete agreement with any consultant or employee who has access to the business secrets of the company.\nAs many employees and consultants start their own business and develop as a competitor to their ex-company. They use the skills and secrets learned at the company. Thus, the use of those secret strategies with further enhancements may help the employees to capture the same market and cut off the ex-company business.\nFurther, there is another risk associated with the accessibility of business secret information. The employees have access to the secret recipe, formula, strategy, methods, practices, ideas, and marketing plans. So, the employer must focus on securing the reveal of all such information to the competitor by an ex-employee.\nAlso, there are several positions involved in the business that have access to the company secrets. So, the HR department needs to carefully draft a good non-compete agreement for all the employees. Therefore, a non-compete agreement template is very helpful.\nHow do I get out of a non-compete agreement?\nThere are two methods that will help you to get out of a non-compete agreement;\nMethod#1: Demand for a release\n- The first step in method#1 is to take a copy of the agreement you signed so that you can read it thoroughly and identify the things the company seeks to protect. It will tell you what you need to demand a release.\n- Secondly, remember what your responsibility at the company was.\n- Next, view your state’s law.\n- After that, outline your main points that are the objective of your discussion.\n- Schedule a meeting with your manager or HR representative.\n- Arrange the terms of your release and create a new agreement.\n- In the end, a release from the agreement is also in written form.\nMethod#2: Go to the court\n- If you ignore the non-compete agreement, then you receive a notice from the court that your employer may sue you.\n- Hire an attorney\n- Submit your answer to your employer’s complaint in the court\n- Take a part in the discovery and prepare your case.\n- Involve a third party to settle down all the matters with your former employer. You should also check Hold Harmless Agreement Template.\nWhat should be included in Non-Compete Agreement?\nThe non-compete agreement usually sounds like a one-sided agreement. However, these are fair and equitable for both the employer and employees. Therefore, it needs to include all the necessary information for legal usefulness. Also, it is usually developed by the business lawyer having legal information about the state law.\nFirst of all, the agreement must clearly state the date of its application and effectiveness. Also, it is helpful to clearly state the reason for establishing the agreement. Here, the company may touch the trade secrets that are prone to the accessibility of the employee and that may harm the business stability if a competitor came to know about those. Thus, the agreement must clearly state all the reasons.\nAs the non-compete agreement isn’t a lifetime agreement. It specifies its validity date with reference to the resignation/termination/retirement date of the employee from the company.\nFurther, mostly the non-compete agreement is associated with the location. The coverage of location in the agreement is important. It provides a fair and equitable right to the employee in the agreement.\nSo, if an employee wants to start the same business after leaving the company. The employee has two choices. Either to wait for the barred or validity period of an agreement to end or choose the market location that doesn’t violate the contract location clause.\nFinally, the most important thing is to include the details of how the company will take legal action in case of any violation."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:0079830a-939d-43b1-a2d6-fec21d5e584f>"],"error":null}
{"question":"What are the essential characteristics that differentiate information from raw data in database systems, and how do these relate to maintaining data integrity in modern databases?","answer":"Information differs from raw data in several key characteristics: it must be accurate (free from errors and bias), presented in a useful form, have current utility in decision-making, be timely, complete, purposeful, and reliable. Raw data, on the other hand, is simply described through characters like alphabets, numbers, or special characters. These characteristics of information align with data integrity requirements in modern databases, where data must maintain completeness, consistency, and accuracy throughout its lifecycle. Data integrity ensures these information characteristics are preserved through physical integrity (accuracy and completeness of stored data) and logical integrity (including entity, referential, domain, and user-defined integrity), making sure the data remains trustworthy and valuable for business operations.","context":["- DBMS Tutorial\n- What is Database Management System (DBMS)?\n- Components of DBMS\n- Applications of DBMS\n- Three Schema DBMS Architecture\n- Difference between DBMS and RDBMS?\n- Difference between File Oriented System and DBMS\n- Types of Data Models\n- DBMS Schema and Instances\n- Data Independence and Data Abstraction\n- Database Users and Administrator\n- DBMS Languages and Interfaces\nDBMS ER Model\nDBMS Relational Data Model\nDatabase Management System (DBMS) tutorial is all about handling and maintaining the record efficiently. Our DBMS tutorial is creating for learners as well as professionals.\nWhat is Data?\nData can be defined as the description of facts, concepts, or information in a distribute manner applicable for transmission, interpretation, or processing by the human or electronic device. Data is described with the help of characters like Alphabets (A-Z, a-z), numbers (0-9) or special characters (+, -, /, *, <,>, =etc.)\nWhat is Data Item (Field)?\nA set of character which is used together to represent a specific data element, e.g., name of the student in a class is represented by the data item, say, NAME.\nWhat is Record?\nA record is a group of associated data elements, e.g., Payroll data for an employee include such data fields as name, age, qualification, sex, Basic Pay, DA, HRA, PF, etc.\nWhat is Data File?\nFile is a collection of related data, e.g., a payroll file might consist of the employee pay record for a company.\nExample: Consider the STUDENT File.\nWhat is Information?\nIt is an organized or confidential record that has some significant value to the receiver.\nData Processing defines manipulating the data (raw data) to make it more helpful.\nData manipulation consists of such functions as classification, sorting, estimation & summarization.\nCharacteristics of Information\nFollowing are the characteristics of information are as follows:\nInformation should be accurate. It defines that information should be free from errors & clear. Accuracy also defines that the information is free from bias. Wrong data given to management would output in wrong decisions. As the manager’s decisions are based on the data provided in MIS documents, all managers require accurate information.\nInformation is of value if it is supported to the user in the form. It is useful and best understood by him. For example, in a business enterprise, top management may require information on key matters in a summarized form and the operation managers in a detailed form.\nIt defines the current utility of data in decision-making or problem-solving.\nIt means that information should be made feasible when it is required for a specific purpose and not before and in any case, not after.\nThe record which is given to a manager must be complete and should meet all his requirements. Incomplete data may output in wrong decisions and thus may prove expensive to the organization.\nInformation must have a purpose at the time it is communicated to a person or machine, otherwise, it is simply data.\nThe information should be reliable and externally forced relied upon indicated.\nIt measures the closeness of the information to the purpose.\nNeed of Information\n- Information is useful for making decisions.\n- Information helps managers in lowering the level of uncertainties where they have to make a choice among several available alternatives.\n- The information helps the users in tackling problems relating to their respective functional areas.\n- Information is used by top management to plan the objectives of the organization and to access whether the objectives are being met in practice.\nWhat is Database?\nA database is an organized group of associated data of an organization saved in a formatted way, which is shared by multiple users.\nCharacteristics of Database\nFollowing are the characteristics of the database are as follows:\nA database framework permits several customers to get the database together. To answering different questions from different clients with the similar (base) data is an essential form of an information system. Such a concurrent requirement for data hike the economy of an organization.\nStructured and Described Data\nA significant characteristic of the database strategy is that the database application does not contain only the data but also the complete description and definition of these data. These definitions are general analysis about the extent, the architecture, the type, and the format of all data and the relationship among the data. This type of saved data is called a metadata (“data about data”).\nSeparation of Data and Applications\nApplication program does not require any information about physical data like encoding, design, storage place, etc. It only directs with the administration framework for a database (DBMS), i.e., standardized interface with the support of a standardized language like SQL. The create of the data and the metadata is done by the DBMS. In this method, all the functions can be done isolated from the data. Therefore, database internal reorganizations or development of flexibility do not have any power on the application programming.\nData integrity includes the assurance of the database from unauthorized access (confidentiality) and unauthorized changes.\nA transaction is an array of actions which are done inside a database to deliver it from one consistent state to the new consistent state.\nOne illustration of the transaction is the transmission of an amount of money from one bank account to other account. The debit of money from one account and the credit of it to another account develops together with the logical transaction. This transaction is also an atomic. The debit or credit alone will both get to an inconsistent state. After finishing the transaction (debit and credit), the innovations to both accounts develop into persistence, and the one who contribute the money now has less money on his bank account while the receiver now has a more significant amount.\nData persistence defines that in a DBMS, all record is managed as long as it is not removed unusually. The life period of data needed to be decided directly or indirectly by the customer and must not be dependent on system characteristics. Furthermore, data, once saved in a database, must not be lost. Changes of the database which are done by the transaction are determined. When a transaction is done, even a system crash cannot put the data in danger.\nEnroll Yorself in Live Training: DBMS Training","We all know that data is essential, but what do we need to ensure accurate and reliable data?\nMost people think of data accuracy and integrity as similar, but they are pretty different. Maintaining data quality is essential for businesses, but it can be hard to keep track of everything and ensure data is accurate across various departments and data sets.\nThis blog post will explore data accuracy vs. data integrity and tips on ensuring both in your data sets.\nData Accuracy vs Data Integrity\nWhat is Data Accuracy?\nData accuracy is the most vital aspect of data quality. It guarantees that your company’s business operations are based on reliable and proper data, which will lead to more profitable decision-making capabilities in all areas, including planning, forecasting, budgeting intelligence & more!\nWhat is Data Integrity?\nData integrity is the quality of data entered into a system. That means that data is complete, consistent, and accurate.\nIn technical terms, data integrity is “The measure of how well data is preserved during its life cycle.”\nData integrity is the accuracy and completeness of data. It ensures that data is not corrupted and can be trusted for further use. That involves ensuring that data is protected from unauthorized access, alteration, or destruction.\nExample of Data Accuracy\nLet’s say you’re a retailer and want to track inventory levels. If you have an accurate data set, you’ll be able to ensure you always have enough stock on hand to meet customer demand. You can also use data accuracy to predict trends to order stock in advance and stay ahead of the competition.\nExample of Data Integrity\nLet’s say you’re a data entry clerk and enter data into a spreadsheet. If you have data integrity, you’ll ensure that all data is entered correctly and completely. You’ll also check for errors and ensure data is consistent across different data sets.\nTypes of Data Integrity\nTwo types of data integrity need to be understood\n- Physical data integrity\n- Logical data integrity\n1. Physical data integrity\nPhysical data integrity is the accuracy and completeness of data stored in a database. This data integrity is essential because it ensures data is not corrupted and can be trusted for further use.\n2. Logical data integrity\nAs data are used in relational databases in various ways, logic integrity guarantees they remain unchanged.\nThere are four main types of logical data integrity: entity integrity, referential integrity, domain integrity, and user-defined integrity.\n- Entity integrity dictates that every row in a table must have a unique identifier. That is typically accomplished by having a primary key column (or set of columns) that can’t be NULL and doesn’t have any duplicate values.\n- Referential integrity ensures that any foreign keys in a table point to valid rows in the parent table. In other words, it ensures that there are no “orphan” rows with missing parent records.\n- Domain integrity defines the allowed values for each column in a table. For example, you might specify that a particular column can only contain integer values between 1 and 10.\n- User-defined integrity is any data integrity constraint that doesn’t fit into the other three categories. For example, this could be something like a “unique” constraint that ensures no two rows in a table have the same value for a particular column.\nData Integrity Key Metrics\nThere are a few key metrics to consider when looking at data integrity:\n- Data accuracy: How close the data is to the real-world data set. for example, a data accuracy of 95% means that the data is very close to the actual data set\n- Data completeness: How much data is included in a data set\n- Data security: Ensuring that data is safe from unauthorized access\n- Data governance: Ensuring that data is managed to meet the organization’s needs\n- Data validity: Ensuring data is valid (i.e., not corrupted) by checking for errors\n- Data uniqueness: Make sure data is unique (i.e., no duplicates)\n- Location intelligence: With location insight and analytics, make data more actionable by providing a layer of richness and complexity\n- Data enrichment: By adding data from external sources to internal data, you can give it more context, nuance, and significance. Adding business, consumer, or location details enhances your data’s completeness and context by giving you a more comprehensive and contextualized perspective.\nHow to Ensure Data Integrity\nThere are several ways to maintain data integrity, including but not limited to:\n- Validating input data against known constraints: When data is input into a system, it should be checked against known constraints, such as data type (e.g., integer, string), length, format, and range.\n- Checking output data for compliance with business rules: Data output from a system should be checked against business rules to ensure it is accurate and complete. Having a good data integrity policy in place is essential because it can help ensure that your data is trustworthy and can be relied on for critical decision-making.\n- Using data cleansing tools: Data cleansing tools can be used to identify and correct errors in data.\n- Storing data in a consistent format: Data should be stored in a consistent format across all data sets.\nRisks of not having Data Integrity\nThere are a few risks associated with data integrity:\nLoss in E-commerce\nInaccurate data leads to the wrong product being sent to the customer, which leads to a loss in e-commerce.\nInaccurate data can lead to financial loss, for example, if a company overpays for a product because the data about the cost of the product is inaccurate.\nInaccurate data can have severe consequences in the health care industry. For example, if a patient’s medical records are incorrect, it could lead to the wrong diagnosis and treatment.\nInaccurate data can lead to problems in the insurance industry. For example, if a company’s data about the cost of repairs to a car is incorrect, it could lead to the company overpaying for the repairs.\nBenefits of data integrity\nThere are many benefits of data integrity, but the following are some of the most important:\n- It provides the searchability and traceability of data to its source\n- With data integrity, we can easily search and track data\n- It helps to Get rid of redundant, inaccurate, or outdated data\n- It avoids the misclassification or improper storage of essential data\n- Data integrity helps ensure data security by ensuring data is safe from unauthorized access\n- Data integrity enables data governance, which is critical for managing data\n- It also helps to ensure data validity, which is vital for ensuring data is not corrupted\nDifference between Data Accuracy and Data Integrity\n|Data Accuracy||Data Integrity|\n|Data accuracy refers to the quality of data||Data integrity refers to the accuracy and completeness of data|\n|Essential for businesses to make sound decisions||Necessary to make sure that data has not been tampered with or corrupted|\n|Maintaining data accuracy requires processes for data entry, data management, and data security||Data integrity often requires data governance and security measures beyond those needed for data accuracy|\n|Data accuracy is important because inaccurate data can lead to human errors, wasted time, and wrong decisions||Data integrity is vital to maintaining the trustworthiness of data|\n|Ensuring data accuracy across different departments and data sets can be difficult||Data integrity only requires ensuring accuracy and completeness within a data set|\n|Data accuracy is typically measured in terms of how close a parameter estimate is to its actual value||Data integrity is generally measured in terms of the percentage of data that is complete and accurate|\n|Critical metrics of data accuracy are accuracy rate, completeness rate, consistency, and error rate||Key metrics of data integrity are data accuracy, data completeness, data security, data governance, Data Validity, Data Uniqueness, Location Intelligence, and Data enrichment|\n|Risks of not having data accuracy include wrong decision making, reputational damage, and failure in compliance||Risks of not having data integrity include loss in E-commerce, financial loss, wrong diagnosis, and overpaying Insurance|\n|Data accuracy is essential for businesses because it ensures sound decision-making.||Data integrity is vital for businesses because it helps maintain the trustworthiness of data sets.|\nWhy are both Data Accuracy and Data Integrity essential?\nData accuracy and integrity are essential to any organization because they help ensure that data is complete, consistent, and accurate.\nData accuracy and data integrity are two essential aspects of data management. Accuracy is how correct the data is, while integrity means the data has not been changed.\nUnfortunately, many companies focus on accuracy over integrity, which can lead to problems. However, data accuracy and integrity are essential to any organization because they help ensure that data is complete, consistent, and accurate.\nWe hope this article helped explain the differences between data accuracy and data integrity!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:ac9b0deb-cf66-43f7-b178-bc31a891f572>","<urn:uuid:fb1c30c4-a23f-47b5-a8cb-3533c1c24684>"],"error":null}
{"question":"What similarities exist between immigrant students' academic success in Ireland and former Soviet Union immigrants in Israel when it comes to educational achievement and cultural influence?","answer":"In both cases, immigrant populations showed remarkable educational achievements and cultural influence. At Scoil Bhríde in Ireland, students with migrant backgrounds (80% of the school population) excelled academically, performing above national averages in Math and Reading tests, while maintaining their home languages. Similarly, in Israel, immigrants from the former Soviet Union demonstrated strong academic performance, particularly in STEM fields. Both cases showed cultural transmission effects: in Ireland, home languages became a source of learning and cultural exchange, while in Israel, Soviet gender norms influenced native Israeli girls to pursue STEM fields and avoid traditional 'pink-collar' occupations, especially when they had higher exposure to FSU immigrants in middle school.","context":["Does the title sound a bit outlandish to you? Do you find yourself asking “How can a 10-year-old do this?”\nWell, this is just one of the marvelous ways learners can surprise us when we let them use their language repertoires to the fullest, and give our learners autonomy.\nThis is not a made-up scenario. The example comes from Scoil Bhríde (Cailíní) in Dublin, Ireland. In their research, Little and Kirwan (2019) show how using the learners’ full linguistic repertoires in the classroom have helped them:\n- attain higher levels of self-esteem and wellbeing,\n- highly increased levels of metalinguistic awareness (in other words, a better understanding of how languages work),\n- higher levels of literacy in all of the school’s languages (English, Irish, French) and in their home language (without any instruction at school),\n- perform better (above the national average) in annual Maths and Reading tests consistently,\n- taking on language projects without being prompted, and often ambitious ones!\nHow did this happen? Well, a little bit of background first. Scoil Bhríde (Cailíní) is a school where 80% of the students have migrant backgrounds, and the school boasted 51 home languages at the time of research. Thus, most of the students start learning English as an additional language (EAL for short) when they start school. Now, we brush home languages aside at school for a variety of reasons: the (misplaced) belief that one language at a time in the classroom is better for the students, teachers’ worries about managing a classroom with many languages, and so on. However, the study showed that it only takes a change of perspective and little work on the teachers’ part to turn home languages into an advantage for everyone.\nThis was made possible by giving students more language autonomy in and out of the classroom. Since the home language is central to the students’ identity, this worked wonders. Even in instances where the teachers did not speak the home languages, home languages functioned in three important ways:\n- A communicative tool for learners amongst themselves, when their shared languages were similar enough\n- A display of cultural identity and self-esteem: being encouraged in the classroom to share information on the home language. (“This is how we say X in my language”)\n- A source of linguistic insight: remember metalinguistic awareness? When learners shared how their home language differs from the languages taught at school, this also made them think about different ways languages operate, and helped them attain a higher level of awareness, which helps them in every language.\nSo this brings us to the title. In this instance, we have an 11-year-old Nigerian student, who translated a song called “L’abe igi orombo (Under the orange tree)” in their home language, Yoruba, and taught it to a group of junior infants at the same school!\nIsn’t it amazing to see the great things learners can achieve when we let them use their linguistic creativity?\nListen to “L’abe igi orombo” here:\nShowcased research: Little, D. and Kirwan, D., 2019. Engaging with linguistic diversity: A study of educational inclusion in an Irish primary school. Bloomsbury Publishing.","Girls and the sciences: news from the East\nShort link: https://bit.ly/3fiH0ZO\nIn 2020, the gender wage gap in developing countries is essentially due to two main factors. First, occupational segregation, in particular the underrepresentation of women in maths and science in education and in the labor market, while these fields are important avenues of professional success and high earnings. Second, women’s weaker attachment to paid work: interrupted careers, shorter working hours, and higher demand for time flexibility; traits which reflect different work-life compromises, where the presence of children plays an important role. These factors are sustained by traditional roles and form a cultural equilibrium. But gender cultures can develop differently in different countries.\nIn a series of articles written with Quentin Lippmann (1) and Naomi Sokuler-Friedmann (2), Claudia Senik shows that institutions established in the socialist bloc have left lasting traces. The priority accorded to science and engineering, on the one hand, and the generalisation of women’s participation in the labour force, on the other, have permanently changed women’s behavior. In Germany for instance, females’ performance and competitiveness in mathematics remain higher in Eastern regions than in Western Lander. This East-West difference is general within the OECD.\nThe cultural heritage of the Soviet Union can also be observed in the behaviour of descendants of Russian immigrants in Israel. Following the opening of the “prison of the people’” in 1990, around one million people left the former USSR for Israel, which had a population of about 4.5 million at the time. Claudia Senik and Naomi Sokuler-Friedman studied an entire cohort of women, from middle school up to higher education and the labor market. Among these women, born in 1988-89, 15% were born in the former Soviet Union (and immigrated as babies), 4% in other countries, and the rest are Israeli natives. The authors observe two kinds of cultural transmission of the Soviet-style gender norms. First, intergenerational vertical transmission: FSU young women are over-represented in STEM study fields in secondary and tertiary education; they avoid tertiary study fields leading to female-dominated “pink-collar occupations”, such as education and social work; they exhibit stronger labor force attachment (longer working hours and higher earnings). Second, horizontal diffusion from FSU immigrants to native Israeli girls. The higher their early exposure to FSU immigrants in middle school, the more likely are native Israeli women to choose STEM study fields and to avoid pink-collar study fields.\nBut these peer effects have unequal strength. The greater the proportion of Soviet migrants in their school, the more native Israeli girls are influenced by the Soviet gender script, but the reverse is not true. Russian immigrants are not influenced by the composition of their school: whether they constitute 5 % or 50 % of the pupils, they systematically avoid pink-collar study fields. The authors interpret this asymmetry as an indication of the irreversibility of the shift in gender norms: once the stereotype threat that prevented women from choosing STEM has been lifted, there is no going back to traditional beliefs and preferences.\n(1) Lippmann Q. et Senik C. (2018). Math, Girls, and Socialism, Journal of Comparative Economics, 46(3), 874-888\n(2) Friedman-Sokuler N. et Senik C. (2020). From Pink-Collar to Lab Coat: Cultural Persistence and Diffusion of Socialist Gender Norms, IZA DP 13385\nOriginal title of the articles:\nMath, Girls, and Socialism\nFrom Pink-Collar to Lab Coat: Cultural Persistence and Diffusion of Socialist Gender Norms\nPublished in :\nJournal of Comparative Economics, 46(3), 874-888\nIZA DP 13385\nAvailable at :\n* PSE Member\nCredits : Shutterstock - Rido"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:87e015f2-643d-40ad-ab50-1107510e5f43>","<urn:uuid:d0e06393-1deb-41b5-aaab-1e4aa6825d57>"],"error":null}
{"question":"I've been exploring different Asian soups lately. What are the key differences between Korean seolleongtang (ox bone soup) and Japanese ramen in terms of ingredients and preparation style?","answer":"Seolleongtang and ramen represent distinct approaches to noodle soups. Seolleongtang is characterized by its milky white color and minimalist ingredients, typically containing only noodles, finely chopped scallions, and a few strips of meat in an ox bone broth. It's particularly valued as a warming winter dish and is customarily served with rice and kkakdugi kimchi. Ramen, on the other hand, offers diverse regional styles and preparations, featuring wheat flour noodles made with kansui (alkaline water) for a distinctive chewy texture. Ramen broths vary significantly by region, from creamy tonkotsu in Fukuoka to soy-based shoyu in Tokyo, and typically include various toppings. While seolleongtang emphasizes simplicity, ramen embraces regional diversity and complexity in its preparation.","context":["21. Kongguksu (콩국수)\nThis seasonal dish might taste bland to some, but once you learn to enjoy the subtle flavor of the bean, you will acquire a taste for this cold, creamy, textured noodle dish that no other dish will be able to satisfy in the summer.\nAnd if the pale, spring green julienne cucumbers placed on the hand-ground, snow-white soybean doesn’t tip you off, kongguksu is a highly nutritious dish that also happens to be vegetarian-friendly.\n22. Kalguksu (칼국수)\nBad kalguksu can be very bad. But good kalguksu is divine.\nAlthough most kalguksu places will add mushrooms, sliced pumpkin, and seafood or chicken to the basic ingredients of noodles and broth, at the end of the day kalguksu is about the pleasure of the plain.\n23. Ox Bone Soup (설렁탕)\nThis ox bone soup is easily recognizable by its milky white color and sparse ingredients. At most, seolleongtang broth will contain noodles, finely chopped scallions, and a few strips of meat.\nYet for such a frugal investment, the results are rewarding. There is nothing like a steaming bowl of seolleongtang on a cold winter day, salted and peppered to your taste, and complemented by nothing more than rice and kkakdugi kimchi.\n24. Tteokguk (떡국)\nOriginally tteokguk was strictly eaten on the first day of the Korean New Year to signify good luck and the gaining of another year in age. The custom makes more sense if you think in Korean: idiomatically, growing a year older is expressed as “eating another year.”\nBut this dish of oval rice cake slices, egg, dried laver seaweed, and occasionally dumplings in a meat-based broth is now eaten all year round, regardless of age or season.\n25. Doenjang jjigae (된장찌개)\nThis humble, instantly recognizable stew is one of Korea’s most beloved foods.\nThe ingredients are simple: doenjang, tofu, mushrooms, green peppers, scallions, and an anchovy or two for added flavor. Add rice and kimchi on the side and you have a meal — no other side dishes necessary.\nWhile its distinctive piquancy might throw some off, that very taste is what keeps it on the South Korean table week after week.\n26. Galbi (갈비)\nGalbi, which means “rib,” can technically come from pork and even chicken, but when you just say “galbi” sans modifiers, you’re talking about thick slabs of meat marinated in a mixture of soy sauce, chopped garlic, and sugar and grilled over a proper fire.\nOf course, beef galbi can be used to make soup (galbitang) and steamed galbi (galbijjim). But these dishes, while excellent in their own right, are overshadowed by their grilled leader.\n27. Chuncheon dakgalbi (춘천 닭갈비)\nOn the other end of the galbi spectrum is the low-budget student favorite Chuncheon dakgalbi.\nIn this dish, chunks of chicken are marinated in a sauce of chili paste and other spices, and stir-fried in a large pan with tteok, cabbage, carrots, and slices of sweet potato.\nBecause of the tendency of the red dakgalbi sauce to splatter, it’s common to see many diners wearing aprons over their clothes as they cook and eat.\n28. Bossam (보쌈)\nAs is frequently the case with many South Korean meat dishes, Bossam at its core is simple: steamed pork.\nBut key to this dish is that the steamed pork is sliced into squares slightly larger than a bite, lovingly wrapped in a leaf of lettuce, perilla, or kimchi, and daubed with a dipping sauce. There are two traditional options: ssamjang, made of chili paste and soybean paste (doenjang), or saeujeot, a painfully salty pink sauce made of tiny pickled shrimp.\nWrapping and dipping are essential.\n29. Agujjim (아구찜)\nAgujjim, also known as agwijjim, is a seafood dish that consists of anglerfish braised on a bed of dropwort and bean sprout. It is as spicy as it looks: the entire dish is a bright reddish color, from the chili powder, chili paste, and chili peppers used in the seasoning.\nThe white, firm flesh of the anglerfish, which is quite rightly called the “beef of the sea,” is meaty and filling. And the tangle of dropwort and bean sprout that make up the majority of the dish aren’t just there for decoration: the dropwort is tart and the bean sprouts crunchy.\n30. Japchae (잡채)\nJapchae, a side dish of cellophane noodles, pork, and assorted vegetables sauteed in soy sauce, makes its most frequent appearances at feasts and potlucks.\nThere are no precise rules governing the precise assortment of vegetables in japchae, but most recipes won’t stray far from the standard collection of mushrooms, carrots, spinach, onions, and leeks.","An Exploration of Japan’s Beloved Noodle Varieties\nWhen it comes to Japanese cuisine, noodles play a central role in delivering rich flavors and satisfying meals. Among the wide array of noodle varieties available, ramen, udon, and somen noodles stand out as some of the most popular and beloved options. In this article, we’ll delve into the differences between these three noodle types, exploring their origins, unique characteristics, and how they are enjoyed in Japanese culture.\nUnderstanding Ramen Noodles\nRamen noodles have gained a global reputation for their delightful flavors and diverse preparations. Originating from China, ramen noodles have evolved into a quintessential dish in Japanese cuisine. Made from wheat flour, salt, water, and kansui (alkaline water), ramen noodles offer a distinct chewy texture that sets them apart.\nRegional Styles of Ramen\nOne of the fascinating aspects of ramen is the variety of regional styles found throughout Japan. From the rich and creamy tonkotsu ramen of Fukuoka to the soy-based shoyu ramen of Tokyo, each region boasts its own unique twist on this beloved dish. The choice of broth, toppings, and even noodle thickness can vary significantly, resulting in a diverse culinary landscape.\nExploring Udon Noodles\nUdon noodles, on the other hand, offer a thicker and more substantial experience. Made from wheat flour, salt, and water, udon noodles are known for their soft and chewy texture. These noodles are believed to have originated in Japan during the Nara period and have since become a staple in Japanese cuisine.\nTraditional Preparation and Variations\nUdon noodles are traditionally prepared by kneading the dough and rolling it out before cutting it into thick strands. The resulting noodles are then boiled and served in a variety of dishes, such as hot noodle soups or chilled with a dipping sauce. With its versatility, udon can be enjoyed in a range of flavorsome preparations, including kitsune udon with sweet fried tofu or tempura udon with crispy battered vegetables.\nUnveiling Somen Noodles\nSomen noodles, while less widely known outside Japan, hold a special place in Japanese culture. These thin, delicate noodles are made from wheat flour and are often associated with summertime due to their light and refreshing qualities. Somen noodles have a silky texture that distinguishes them from other varieties.\nTraditional Serving Methods\nSomen noodles are traditionally served cold, accompanied by a dipping sauce and garnished with toppings like green onions, grated ginger, or shredded nori seaweed. During the warmer months, somen is often enjoyed as a chilled dish, offering a delightful way to beat the heat. The delicate nature of somen noodles makes them a perfect canvas for absorbing the flavors of the dipping sauce.\nFAQ: Common Questions about Ramen, Udon, and Somen Noodles\n1. What are the key differences between ramen, udon, and somen noodles?\nRamen noodles are thin and springy, udon noodles are thick and chewy, while somen noodles are thin and delicate.\n2. Are the cooking methods different for each noodle type?\nYes, the cooking methods vary. Ramen noodles are typically boiled, udon noodles are boiled and then rinsed, while somen noodles are boiled and then chilled.\n3. Which noodle type is best for soup-based dishes?\nRamen noodles are commonly used in soup-based dishes, as their texture holds up well in flavorful broths.\n4. Can these noodles be used interchangeably in recipes?\nWhile each noodle type has its own distinct characteristics, they can be substituted in certain recipes depending on personal preference. However, for an authentic taste, it’s best to use the recommended noodle type.\nIn conclusion, ramen, udon, and somen noodles are three distinct and beloved varieties that showcase the diversity of Japanese cuisine. Ramen noodles offer a range of regional styles with unique broths and toppings, while udon noodles provide a thicker and heartier experience. Somen noodles, on the other hand, offer a delicate and refreshing option, particularly suited for warmer seasons. Whether you’re a fan of the rich flavors of ramen, the chewy satisfaction of udon, or the light elegance of somen, these noodles are sure to delight your taste buds and transport you to the vibrant culinary world of Japan.\nSo why not embark on a culinary adventure and try all three noodle varieties? Indulge in the diverse flavors and textures, and discover your personal favorite among these iconic Japanese noodles.\nRemember, the world of noodles is vast and ever-evolving, so keep exploring and savoring the wonders it has to offer!\nArticle word count: XXXX words"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:c14222ab-a445-416d-a5f8-8c7577823c06>","<urn:uuid:52f9b3a0-8315-4be8-a113-63c19cdd02d4>"],"error":null}
{"question":"I'm studying emergency response procedures. What are the main differences between what qualifies as 'substantial damage' in aircraft accidents compared to what requires updates to a ship's fire control plan?","answer":"For aircraft, 'substantial damage' specifically refers to damage affecting structural strength, performance or flight characteristics requiring major repair/replacement, but excludes single engine failures, dented skin, small punctures, and damage to landing gear or accessories. For ships, updates to the Fire Control Plan are required for any modifications to fire fighting systems, alarm systems, escape routes, ship structure changes, or when there are revisions to SOLAS statutes. Additionally, updates are mandatory during change of flag or classification society. The ship requirements are more comprehensive and system-wide, while aircraft damage assessment is more specifically focused on structural and flight-critical components.","context":["What is Fire Control plan on ships?\nIn all cargo vessels, general arrangement plans should be permanently exhibited for the guidance of the vessel’s officers, using graphical symbols that are in accordance with IMO Resolution A.952(23), which show clearly for each deck the control stations, the various fire sections enclosed by steel or ‘A’ Class divisions, together with particulars of :\n- the fire detection and fire-alarm systems;\n- fixed fire-fighting system;\n- the fire-extinguishing appliances;\n- the means of access to different compartments, decks, etc.;\n- the position of the fireman’s outfits;\n- the ventilating system, including particulars of the fan control positions, the position of dampers and identification numbers of the ventilating fans serving each section; and the location and arrangement of the emergency stop for the oil fuel unit pumps and for closing the valves on the pipes from oil fuel tanks.\nThe Fire Control Plan is a mandatory requirement of SOLAS convention described in Regulation 15 of Chapter II. The fire control plan provides us information about fire station on each deck of the ship, on various bulkheads, and in spaces enclosed by “A” class division, “B” class divisions. It also explains us the type of fire detection system and fire fighting systems available on ship.\nImportant things about Fire Control Plan\nFire control plan tells us about various fire alarm systems, sprinkler installation, extinguishing appliances, means of escape to different compartments and decks, and ventilation system including particulars of remote operation of dampers and fans. The position of various dampers, their marking, and which fan is for particular compartment or deck is also explained so that required damper and fans can be closed in case of fire.\nThe graphical symbols used in the fire control plan should be as per fighting equipment symbols set out in IMO Assembly Resolution A.654(16). It is duty of each and every member of ship’s crew to know the meaning of the symbols used in this plan.\nThe fire plan should be available in the working language of the crew on board and also in English.\nLocation and Availability\nThe general arrangement plan should be permanently exhibited for the guidance of ship officer in conspicuous locations such as navigating bridge, engine room and accommodation.\nCopies of the fire control plan must be provided to each of the members of the fire patrol team in a passenger ship and also posted at each continuously manned central control station.\nA copy of Fire Control Plan should be permanently stored in prominently marked weathertight enclosures outside deckhouse for assistance of shore side fire fighting system in case the ship is in port or in dry-dock.\nAlso with the permission of Administration i.e classification society, the details can be set out in the form of booklet and a copy of it shall be supplied to each officer onboard. One copy of the same should be available on board and be easily accessible. These plans should be kept up-to-date and if alterations are made shall be recorded as soon as possible. The fire plan should be available in the working language of the crew on board and also in English language.\nRenewal, update of Fire control plan\nIt comes under the responsibility of the master, ship owner and ship management team at shore to ensure that the fire control plan is kept up to date and if alterations are made shall be recorded as soon as possible.\nFollowing are the cases when renewal or update in the fire control plan is required :\nChange in the fire fighting system, alarm system, escape route design or anything related to current fire plan takes place. The new system or design must be included and approval should be taken from the classification society.\nModification In ship structure or ship particulars which effect the current fire plan must be added to the new plan with approval of the classification society.\nIn case of revision of statutes related to fire control plan under SOLAS done by IMO or similar authority, the new fire plan to be provided and the fire fighting system or equipment must be as per the new revised plan.\nWhen change of flag in a ship happens, the assigned classification society must review the ship fire control plan.\n- During the change of classification society, the fire control plan must be reviewed.\nThe Classification society surveyor must ensure that there shall are discrepancies between the content of the fire control and the record of approved cargo ship (or passenger ship) safety equipment carried on board. In addition, the various entries in the record should correspond to the particulars of the equipment carried on board and with the associated service and maintenance reports and records.\nRelated read : Propeller Cavitation Causes and Remedies\nFollowing surveys are required\nInitial Survey : This is the survey to be done for issuing the approved fire control plan to the newly built ship.\nAnnual Survey : The fire control plan survey comes under the continuous ship safety equipment survey (CSSE) which is performed annually.\nRenewal Survey : If the CSSE certificate is under renewal period requiring a survey, the fire control plan will require this survey.\nThe attending surveyor is required and expected to make a specific and explicit statement in the report of the relevant survey as to whether he has examined and has verified that the content of fire control plan found on board are in a readable state, updated, approved (or examined for compliance) and in accordance with the requirements of Regulations under SOLAS.\nImportance of Fire control Plan\nThe Fire control plan is not just a paper requirement for the classification society or the port state control. It is a useful document to understand :\nThe location of various firefighting and safety equipment onboard for new joiners.\nLocation of nearest and safest fire fighting equipment and escape route when fighting fire on ship.\n- The port fire fighting station team has no clue about the ship arrangement. The fire control plan is extremely useful and easy to read document to tackle major fire on ship by port Fire fighters.\nFire control plan is an important part of safety management plan of the ship and any discrepancy may lead to non conformities against the SMS.\nCopy of Fire control plans kept the shore officer is also inspected while issuing/ re-issuing the document of compliance (DOC) and safety management certificate (SMC) to the company.","The reporting of aircraft accidents and incidents is governed by NTSB Part 830, or the National Transportation Safety Board (NTSB) Part 830 Notification and Reporting of Aircraft Accidents or Incidents and Overdue Aircraft, and Preservation of Aircraft Wreckage, Mail, Cargo, and Records.\nImmediate notification to the NTSB is required for an aircraft accident and some specific aircraft incidents. The aircraft operator shall file a report using form 6120.1 within 10 days after an accident or after 7 days if an overdue aircraft is still missing. A report on an incident for which immediate notification is required by 830.5(a) shall be filed only as requested by an authorized representative of the Board.\nAn aircraft accident is when any person suffers death or serious injury or in which the aircraft receives substantial damage. An incident is any occurrence other than an accident, associated with the operation of an aircraft, which affects or could affect the safety of operations. The primary incidents that require immediate notification include, but are not limited to, the following:\n• Flight control system malfunction or failure\n• Inability of any required flight crewmember to perform normal flight duties as a result of injury or illness\n• Failure of any internal turbine engine component that results in the escape of debris other than out the exhaust path\n• In-flight fire\n• Aircraft collision in flight\n• Damage to property, other than the aircraft, estimated to exceed $25,000 for repair\n• Damage to helicopter tail or main rotor blades, including ground damage, that requires major repair or replacement of the blade(s)\n• An aircraft is overdue and is believed to have been involved in an accident\nPART 830 Specific Definitions:\nAircraft accident means an occurrence associated with the operation of an aircraft which takes place between the time any person boards the aircraft with the intention of flight and when all persons have disembarked, and in which any person suffers death or serious injury or in which the aircraft receives substantial damage. For purposes of this part, the definition of “aircraft accident” includes “unmanned aircraft accident,” as defined herein.\nIncident means an occurrence other than an accident, associated with the operation of an aircraft, which affects or could affect the safety of operations.\nSerious injury means any injury which: (1) Requires hospitalization for more than 48 hours, commencing within 7 days from the date of the injury was received; (2) results in a fracture of any bone (except simple fractures of fingers, toes or nose); (3) causes severe hemorrhages, nerve, muscle or tendon damage; (4) involves any internal organ; or (5) involves second- or third-degree burns or any burns affecting more than 5 percent of the body surface.\nSubstantial damage means damage or failure which adversely affects the structural strength, performance or flight characteristics of the aircraft and which would normally require major repair or replacement of the affected component. Engine failure or damage limited to an engine if only one engine fails or is damaged, bent fairings or cowling, dented skin, small punctured holes in the skin or fabric, ground damage to rotor or propeller blades, and damage to landing gear, wheels, tires, flaps, engine accessories, brakes or wingtips are not considered “substantial damage” for the purpose of this part.\nUnmanned aircraft accident means an occurrence associated with the operation of any public or civil unmanned aircraft system that takes place between the time that the system\nis activated with the purpose of flight and the time that the system is deactivated at the conclusion of its mission, in which:\n(1) Any person suffers death or serious injury; or\n(2) The aircraft has a maximum gross takeoff weight of 300 pounds or greater and sustains substantial damage.\n49 CFR 830 Notification and Reporting of Aircraft Accidents or Incidents and Overdue Aircraft, and Preservation of Aircraft Wreckage, Mail, Cargo, and Records\nNTSB Form 6120.1 Pilot/Operator Aircraft Accident/Incident Report\nOther Risk Management Topics"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:c2753044-2d7b-4e5b-94f6-f8ebb0c331cb>","<urn:uuid:8507cc50-0316-4290-8b29-390aa203d18c>"],"error":null}
{"question":"As someone with egg allergies, I need to know - can applesauce and flax seed be used interchangeably as egg substitutes in all recipes?","answer":"No, applesauce and flax seed serve different purposes as egg substitutes and cannot always be used interchangeably. Applesauce (1/4 cup = one egg) is primarily used for adding moisture and works best in baked goods like cookies, muffins, and quick breads. It also adds fluffiness when heated as it releases gas. Flax seed mixture (1 tablespoon ground flax + 3 tablespoons water = one egg) functions mainly as a binding agent. For best results, use applesauce when you need moisture and fluffiness, and use flax seed mixture when you need binding power in your recipe.","context":["Because let’s face it, no matter how much you love your new vegan foods. Or how happy you are knowing you’re making the world a healthier less cruel place.\nOnce in a while you just gotta have that special something that grandma used to make.\nHere’s how you can have that pudding, cornbread, cake or cheesy pizza and not sacrifice your values or your health.\nVegan Substitutes For Eggs\nWhat you substitute for eggs depends on what you are using them for. Sometimes eggs are used as a binder and other times they are used to add fluffiness to a recipe. Here are five vegan substitutes for eggs that are commonly used for baking.\n1/2 Banana = one egg\nBanana is a strong binder that adds moisture and flavor. That flavor part is important! Do not use banana to replace the eggs in your recipes if you don’t want it’s flavor added to your finished dish.\n1/4 Cup Apple Sauce = one egg\nMy go to ingredient for replacing eggs and or oil in recipes. When heated apple sauce gives off a gas that adds height and fluffiness to your recipes. The flavor is not as strong as with banana but is there so it’s not suitable for ALL things. Use with caution in very lightly flavored dishes.\n1 TBSP Ground Flax Meal + 3TBSP Water = one egg\nMix flax meal and water and let sit for about 5 minutes. It makes a gooey paste that is GREAT at binding recipes together. It will darken your recipes and can make things a little dense. So don’t use if you need a light look or fluffy finished product. For example see my yellow corn bread made with flax meal instead of eggs. (Here are our instructions with pictures on making a flax meal egg)\n1 TSP Baking Powder + 1 1/2 TBSP Water + 1 1/2 TSP Oil = one egg\n1 TBS Vinegar + 1 TSP Baking Soda = one egg\nBoth of these work well if you need a volume in your recipe. If you don’t want flavor and need the dish to be REALLY FLUFFY use one of the above. They do not add much in the way of binding. But if you need some, go with the baking powder, water and oil.\n3 TBSP Flax Seed Gel = one egg\nThis is listed last not because it is ineffective, far from it. Flax seed gel may be the best all around egg alternative. It glues recipes together like crazy without any of the discoloration or graininess of using flax meal.\nHowever, making it is a bit of a process. And on a site dedicated to EASY VEGAN RECIPES I hesitated to include it. In the end, we decided it is an EASY process and once done the finished gel can be stored and used without hassle.\nSo, check out our vegan egg substitute page if you’d like to learn how to make flax seed gel vegan and have detailed instruction on all the other egg substitutes as well.\nWhen you add milk to a recipe you are mainly adding two things. Oil and moisture. Sometimes you need one or the other. Sometimes you need both. In any case, the following are easily swapped out for milk in almost all recipes.\nAlmond, Soy, Coconut, Hemp, (or other) Milk\nVegetable and nut milks give you all the benefits of cows milk without the drawbacks. They add liquid and oil to your dish in about the same ratio as if you were using skim milk. Substitute them one for one with the milk called for in your recipe. Here’s how to make your own almond milk.\nNOTE: Some of these do also add flavor. So be picky about which one you use for which dish. Coconut milk, not surprisingly, will add slight coconut flavor and you may not want that in your gravy for example.\nAnd the same goes for almond milk and the others. I feel that almond milk has the mildest flavor of the bunch, especially if you make your own so there are no added flavors etc.\nIf buttermilk is what you’re looking for we’ll show you how to make vegan buttermilk right here. You’ll be surprised at how easy it is.\nOlive, Grape Seed, Coconut Oil\nOften when milk is added to a dish the fat content is what’s important. So for example when making mashed potatoes, whole milk gives the creamiest texture and best taste because of the high fat content.\nNow, for our purposes, the only difference between fat and oil is the one is solid at room temperature and the other is not. They both will add a lovely creamy texture to your dishes and vegetable oils are MUCH better for you.\nDO NOT use oil as a one-for-one replacement for milk. If you find that almond milk doesn’t have enough fat in it to give your dish the creamy texture you are looking for add a tablespoon of the vegetable oil of your choice. You may have to experiment with different oils and volumes of oil to get the taste and texture you are used to. It will depend on what kind of milk your grandma used in her original dish.\nVegan Substitutes For Butter\nButter (for on bread)\nAn olive oil dipping sauce is far superior to butter for on bread. It’s better for you and the sky’s the limit when it comes to how you flavor it. Just be sure you get a dipping or dressing grade olive oil for this or the olive taste will be too strong.\nPrepared spice mixes are available for this purpose but I encourage you to just open your spice cupboard and get creative. Garlic powder, onion powder, basil, red pepper flakes, marjoram, rosemary etc… add a little of this and a little of that until you find the taste pleasing.\nMy daughter and I often set up several little bowls and mix different spices in each as a kind of “dipping sauce buffet” When she was little this made her feel like a real chef!\nNOTE: The longer your spices set in the oil the more pleasing they’ll be. So always mix this up at least 5 minutes before you need to serve it. The day before works well too. Just sayin.\nIf you just really want a buttery type spread there are some non-dairy spreads available on the market today. Be sure to look for one that contains absolutely NO HYDROGENATED ANYTHING! As that stuff is terrible for you.\nOur favorite is Earth Balance. You can find this at most natural food stores and many grocery chains across the country.\nButter (for in recipes)\nAs mentioned in the “milk substitutes” section fats, like butter, add a creamy texture to your dishes. Any vegetable oil will do the same thing. So drizzle on some olive oil. Use some of your dipping sauces for an added punch of flavor.\nBut think about this before you do. We often add butter to things like vegetables simply because our parents always did. They did it because their parents always did and so on. It goes back to a time when calories were scarce and making the most of what you had was life or death.\nThat’s no longer the case. At least not here in America. So I encourage you to at least TRY your dishes without the added fat. You may be surprised at how GOOD your vegetables taste plain or with just a splash of soy sauce.\nFinding alternatives to fat for flavor is healthier for you,less fattening and fun. Just a thought.\nVegan Substitutes For Cheese\nOk, this is a toughie because there are so many different kinds of cheese and it’s used in so many different ways. But here are some good vegan substitutes to get you going.\nDaiya makes some lovely non-dairy cheeses that actually MELT and do not taste like cardboard. They come in slices, shredded and wedges. So you should be able to find one that works for what you need it for.\nTofutti makes a really nice vegan cream cheese. It comes in several flavors including…\n- Garlic & Herb\n- Herbs & Chive\n- French Onion\nAs with all cream cheese it’s very easy to get plain and flavor it yourself with your favorite spices or fruits (fresh or jam)\nYou can also make your own DELICIOUS vegan cream cheese out of cashews! Our favorite is Cashew Cream Cheese With Chive. Give it a try.\nIf you like to sprinkle parmesan cheese on top of things give Nutritional Yeast a try. It’s good for you and has a sharp taste like parmesan or romano. My daughter and I go back and forth but the words we usually use to describe it’s taste are cheesy, nutty or savory.\nTry it on baked potatoes, popcorn or noodles. You can also stir it into soups, sauces or mashed potatoes to add complex flavors you would usually use hard cheeses for.\nVegan Substitutes For Sour Cream\nYou can swap out regular sour cream for one of the many commercially available vegan substitutes for sour cream. Our favorite is made by Tofutti.\nOr you can make your own easily enough with one of these two recipes\nHomemade Cashew Based Vegan Sour Cream\nHomemade Tofu Based Vegan Sour Cream\nAlternately you can use commercially prepared soy yogurt in place of sour cream in any recipe. Just replace them one for one and go on with life. This works particularly well if you are making a dip that calls for sour cream as the base.\nMisc Vegan Substitutes\nIf you just love Jello then you’ll be happy to know there is an easy vegan alternative… Agar-Agar. Simple to use, just substitute agar-agar powder one for one with the gelatin in your recipes. If all you’ve ever used is premeasured box mixes, use 2 cups of liquid to 2 tsp of agar-agar powder.\nGreat puddings can be made easily with Chia Seeds! When they soak in liquid chia seeds become thick and creamy like pudding. Think of tapioca and you’ll be on the right track.\nA standard ratio is 1 Cup liquid such as almond milk to 1/4 Cup chia seeds but we find that this leaves the seeds a tad too cruncy in the center for our taste. Instead, we prefer to use 3 Cups liquid to 1/2 Cup chia seeds. This sets up just as firmly and lets the seeds soak all the way thru.\nAdd sweeteners and fruit as desired.\nMix all ingredients in a bowl. Stir for occasionally for 5-10 minutes or until seeds remain in suspension. Then let sit in the fridge until desired consistency is reached. Usually an hour will do. But overnight is a sure thing.\nIf you’re in a hurry, grind your chia seeds into a powder using the grinding attachment of your NutraBullet or a coffee mill before adding them to your liquid. This gives a slightly different texture but it sets up FAST. Like 5 min or so fast.\nAs you can see vegan substitutes don’t have to be exotic, hard to come by or difficult to use. Many of them are already sitting in your cupboard just waiting for you to use.\nThe best way to learn how to make vegan friendly versions of your favorite home cooked foods is to just dive in there and give them a try. If your first attempt is less than you’d hoped for, do not despair. Just roll up your sleeves and give it another go. Using the info you gained from your first dish as a guide.\nWith a little practice you’ll be making dishes just like your Mom or Grandma used to and no one will guess you’ve got vegan substitutions in them.","Going egg-free? How to Substitute Eggs in Recipes\nThere are a lot of reasons you may need to replace egg in your recipes. You or your child may have an egg allergy or sensitivity, maybe you’re baking for a friend who is vegan, or you want to mix up your menu a bit. Or maybe it is a day you simply ran out. Avoiding eggs doesn’t mean sacrificing your favorite foods or good flavor.\nThe most important thing to remember when replacing eggs is to think about what the egg is being used for in the recipe. Eggs supply moisture, act as a binder or are used to provide leavening so the recipe is fluffier. Do you need to replace a whole egg, or just a yolk or the egg white? This will help you decide what egg replacer to use. Some recipes that only require 1 to 2 eggs and make a flat product (such as pancakes) may not need a replacement at all, just 1 to 2 extra tablespoons of liquid to make up for the missing liquid from the egg.\nThe egg replacer I most often use in my baking is 1 tablespoon ground flax seed mixed with 3 tablespoons hot water. Set this aside for a couple of minutes until the flax seed mixture thickens. This mixture should be as thick and sticky as an egg white. If it isn’t, heat the mixture until it thickens. Allow to cool slightly before adding to your recipe. This mixture equals one egg. Flax mixture only replaces the binding property of an egg. If you are using it in a cookie, cake or quick bread recipe also add 1/4 teaspoon extra baking powder to provide the leavening needed.\nThere are many other things besides flax to replace eggs. I have them grouped by use. Some egg replacements, due to their flavor, may alter the taste or texture of your recipe, so use an egg replacer that will compliment the other ingredients. Have fun experimenting and find one that works for your family. Each substitution below equals one egg.\nWhen eggs are used as moisture it is the easiest substitution and used most often in baked goods such as cookies, muffins and quick breads.\n- 1/4 cup mashed banana, pumpkin puree or sweet potato\n- 3 tablespoons applesauce, pear sauce, apple butter, apricot puree or pureed prunes. Add one more tablespoon liquid to the recipe.\nIf eggs are used as a binder:\n- use the flax seed mix above. If you want to make a larger batch ahead of time, mix 1/4 cup ground flax seed with 3/4 cup water. Cook until thickened. Allow to cool. Store in the refrigerator in a covered container for up to 2 weeks. Use 3 1/2 Tablespoons flax mix for each egg.\n- Chia seed can be used the same as flax seed. If you can find chia seed online and at health food stores.\n- 1 1/2 tablespoons oil, 1 1/2 tablespoons water, and 1 teaspoon baking powder\n- Gelatin (unless you are vegetarian- gelatin is NOT a vegetarian product). Sprinkle the contents of a packet of unflavored gelatin over 1 cup of cold water. When the gelatin absorbs water, heat over medium heat until the gelatin completely dissolves. Allow mixture to cool. Use 3 tablespoons to replace one egg.\nReplacing eggs used as leavening. Eggs give texture providing lift. Egg-free baking has a tendency to be a bit heavier without the egg white, but there are several things you can do to lighten the texture of your baked good.\n- add 2 tablespoons plus 2 teaspoons canned coconut milk (NOT light) and 1 teaspoon baking powder to your recipe\n- use Ener-G egg replacer powder 1 packed tablespoon of powder mixed with 2 tablespoon warm water, whisking until frothy.\n- replace part or all of the liquid with carbonated water. Do not over mix after adding in order to retain the carbonation’s effect.\n- And don’t forget the old WWII trick of subbing 1 tablespoon cider vinegar and 1 teaspoon baking soda for eggs in cakes, cupcakes and quick breads.\nHopefully these egg replacers will help out the next time you need to replace an egg. Enjoy!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:b04cfbc5-bc95-42cd-a18f-a3589d601454>","<urn:uuid:1de20d00-8cd8-4c43-be26-eafd83fc714e>"],"error":null}
{"question":"How should a jam business owner approach market research before starting operations?","answer":"To start a jam business, you should conduct market research to determine the local demand for jams and jellies, assess the level of competition, and identify market gaps you can exploit. This includes studying how other jam businesses price their products and potentially introducing new flavors that aren't currently available in the market.","context":["Small jam businesses pack jams in jars by hand, so this business is ideal for those who enjoy canning and jarring. Many online companies and local canneries that offer courses on canning and preserving foods offer courses on safe packing of jams, jellies, and other preserves when you are starting a jam business from home.\nPeople skills are also helpful; you should be able to connect with others. Developing relationships with wholesale purchasers and individual customers will be easier as a result. Learn how to start a jam-selling business from home by following this guide.\n1. Identify the target market: You should target local businesses like restaurants that use jam or jelly regularly. Their customers enjoy local jams served with breakfast at these restaurants. Alternatively, you can sell your jams to a small bakery that uses jam in its baked goods. The businesses you work with will place regular orders, and this will ensure a stable income for you. Jams can be sold individually or in a variety of packages, such as three jams together, when sold retail.\n2. Business registration: Make sure you register your business, obtain licenses, follow all health codes, check all food production laws, and post nutritional information on your product labels. As long as you consistently develop unique flavors to attract customers, you will stay ahead of your competitors.\n3. Conducting research: In order to determine the demand for jams and jellies in your area, you should conduct market research. Discover the level of competition you will face as a newcomer. Make sure your products stand out from your competitors by identifying the gaps in the market that you can exploit. For instance, you can introduce a new flavor of jam to the market that wasn’t available before. Investigate how other jam businesses in your area set their prices.\n4. Skills and experience required: Since jams and jellies are frequently packed in jars, and small jam businesses pack jams in jars by hand, this business is ideal for those who enjoy canning and jarring. In order to start a jam business from home, you need to know how to pack jams, jellies, and other preserves safely. Many online companies, as well as local canneries, offer courses on canning. People skills are also essential; you should be able to connect with others.\n5. The marketing process: As a business owner, you must market and promote your products and formulate marketing strategies. A jam business can be promoted and marketed most effectively through samples. There are customers who will not purchase jam, jelly, or preservation without tasting it first. Farmers’ markets and grocery stores are great places to display your free samples. Your business will also benefit from word-of-mouth marketing.\n6. A jam business’s start-up costs: Since jam businesses can be started and operated from a residential kitchen, the startup costs are pretty low. Costs associated with equipment and supplies can be separated. Using kitchen equipment that you already have is a good idea if you need to open a business on a tight budget. By purchasing jars and pectin from wholesale suppliers, you can reduce your initial supply costs.\n7. Adding new flavors: An adequately managed business has a lot of potential for growth and expansion. Try to start slowly, sell a limited range of tastes, and find out which flavors are most popular. Start expanding the business as soon as you gain popularity among customers.\n8. The daily activities of a jam business: Starting a jam business involves a lot of different tasks every day. Most jam business owners perform different functions on different days in order to be efficient. Jams and jellies need to be sold in the market, made, delivered, marketed, and promoted by business owners. Aside from these tasks, you will also have to perform administrative and management duties.\n9. Create a team: The majority of jam businesses are started and maintained by one or two people, but some do not hire employees. To ensure that your business functions smoothly and to increase production, you will need to hire employees once you expand your business and begin receiving orders in bulk.\nDuring the whole year, your jams and jellies will be in demand from retail stores, especially since jams and jellies are a staple diet in many households. By following the above steps, you can easily start a jam-selling business from home."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:37c58212-15be-40c0-a07c-2c0bef7cc330>"],"error":null}
{"question":"I am learning about cancer treatments. How are external beam therapy and internal radiation therapy different from each other in terms of radiation delivery and what types of cancer they can treat?","answer":"External beam therapy and internal radiation therapy differ in several key aspects. In external beam therapy, high-energy x-ray beams are generated outside the patient by a linear accelerator and targeted at the tumor from various directions. It can be used to treat many types of cancer, including breast, colorectal, esophageal, head and neck, lung, prostate, and brain tumors. In contrast, internal radiation therapy (brachytherapy) involves placing radioactive sources like seeds, ribbons, or capsules inside the patient's body, either temporarily or permanently, directly in or near the tumor. Brachytherapy is most commonly used for specific cancers including head and neck, breast, cervix, prostate, and eye cancers. The key advantage of brachytherapy is that it can deliver very high doses of localized radiation while reducing damage to surrounding healthy tissues.","context":["External Beam Therapy (EBT)\nExternal beam therapy (EBT) is a method for delivering high-energy x-ray or electron beams to a patient's tumor. Beams are usually generated by a linear accelerator and targeted to destroy cancer cells while sparing surrounding normal tissues. EBT also may be used to relieve symptoms in patients with advanced cancer or cancer that has metastasized.\nTo prepare for EBT, your doctor will perform a physical exam and use CT scanning to conduct a treatment simulation session. Other imaging procedures may be used to help determine the exact shape and location of your tumor, and a special device may be created to help you maintain the same exact position during each treatment. Your doctor will give you specific instructions based on the treatment technique to be used.\n- What is external beam therapy and how is it used?\n- Who will be involved in this procedure?\n- What equipment is used?\n- Who operates the equipment?\n- Is there any special preparation needed for the procedure?\n- How is the procedure performed?\n- What will I feel during this procedure?\n- What kind of treatment follow-up should I expect?\nWhat is external beam therapy and how is it used?\nExternal beam therapy (EBT), also called external radiation therapy, is a method for delivering a beam or several beams of high-energy x-rays to a patient's tumor. Beams are generated outside the patient (usually by a linear accelerator, see below) and are targeted at the tumor site. These high energy x-rays can deposit their dose to the area of the tumor to destroy the cancer cells and, with careful treatment planning, spare the surrounding normal tissues. No radioactive sources are placed inside the patient's body.\nExternal beam therapy is used to treat the following diseases as well as many others:\n- Breast Cancer Treatment\n- Colorectal (Bowel) Cancer Treatment\n- Esophageal Cancer Treatment\n- Head and Neck Cancer Treatment\n- Lung Cancer Treatment\n- Prostate Cancer Treatment\n- Brain Tumor Treatment\nWhy is this procedure performed?\nExternal beam therapy is most commonly used to treat cancer. Often, the goal is to eliminate a tumor or prevent a tumor from returning. The procedure may also be performed before or after surgery to remove a cancerous tumor, to reduce the tumor size before surgery, or to prevent the tumor from coming back after surgery.\nEBT may also be used as a palliative treatment in patients with advanced stage cancer or cancer that has metastasized. In this case, the goal of therapy is to reduce a patient's symptoms rather than cure the cancer.\nWho will be involved in this procedure?\nDelivery of external beam therapy requires a treatment team, including a radiation oncologist, medical physicist, dosimetrist and radiation therapist. The radiation oncologist is a physician who evaluates the patient and determines the appropriate therapy or combination of therapies. He or she determines what area to treat and what dose to deliver. Together with the medical physicist and the dosimetrist, the radiation oncologist determines what techniques to use to deliver the prescribed dose. The physicist and the dosimetrist then make detailed treatment calculations and quality assurance checks prior to treatment delivery. The radiation therapists are specially trained technologists who deliver the daily treatments.\nWhat equipment is used?\nRadiation oncologists use linear accelerators or cobalt machines to deliver external beam therapy. Your radiation oncologist will determine the equipment most suited to your treatment. The linear accelerator is the most commonly used device for external beam therapy.\n- Linear Accelerator - see linear accelerator page\nWho operates the equipment?\nThe equipment is operated by a radiation therapist, a highly trained technologist. The overall treatment plan is created by the radiation oncologist, a highly trained physician specializing in treating cancer with radiotherapy.\nIs there any special preparation needed for the procedure?\nThe process of external beam therapy involves three parts:\n- Treatment Planning\n- Treatment Delivery\nThe goal of simulation is to determine the treatment position that will be used daily, to make devices that will help the patient maintain that position, and to obtain the necessary images for treatment planning. The radiation therapist places the patient in the treatment position on a CT scanner. Masks, pads or other immobilization devices are typically used to help the patient to hold still and in a specific position during the simulation. These devices will be used for the treatment to achieve the same position daily, so it is important that the patient can maintain that position. Images of the treatment area are taken in the treatment position. The radiation therapist places small marks on the patients to help guide the daily treatments. These marks may be tattoos or colored ink. The tattoos will be permanent, but the colored ink will eventually fade. Marker seeds may be placed in the target tumor or organ at simulation or during a separate surgical procedure. These seeds or markings are intended to help the radiation therapist position the patient during each treatment session.\nFor treatment planning, the dosimetrist, medical physicist and radiation oncologist use a special computer program to calculate the radiation dose that will be delivered to the patient's tumor and the surrounding normal tissue. The radiation oncologist will determine the volume of the tumor and other areas that need to be treated and outline those on the treatment planning images. He or she will also outline normal structures that should be avoided or considered in devising the treatment plan. Together, the oncologist, dosimetrist and physicist will generate a treatment plan that delivers the appropriate dose to the tumor while minimizing dose to surrounding normal tissues. In certain cases, this process may employ such techniques as three-dimensional conformal therapy, intensity-modulated radiation therapy (IMRT), or volumetric modulated arc therapy (VMAT). This planning is based on CT, MRI and PET/CT scans which may be done in the radiology department or the radiation oncology department.\nAfter the simulation and planning have been completed, the treatment can begin.\nHow is the procedure performed?\nBefore each treatment session, the patient may be asked to change into a gown. The radiation therapist brings the patient into the treatment room and places him/her on the treatment couch of the linear accelerator in exactly the same position that was used for simulation using the same immobilization devices. The therapist carefully positions the patient using the alignment lasers and the marks that had been placed on the patient during simulation. Some form of imaging is often used prior to the treatment delivery to verify the accuracy of the patient setup. Some of the types of imaging that can be used include x-rays, ultrasound and cone beam CT. The therapist goes outside the room and turns on the linear accelerator from outside. Beams from one or more directions may be used and the beam may be on for as long as several minutes for each field.\nThe treatment process can take one hour or less each day and most of the time is often spent positioning and imaging the patient. The first treatment usually takes the longest; subsequent treatments take between 15 and 30 minutes. The actual treatment may last only several minutes. The duration of a patient's treatment depends on the method of treatment delivery, such as IMRT, and the dose given. The length of each treatment will usually be the same from day to day.\nPatients usually receive radiation treatments once a day, five days a week for a total of two to nine weeks. The patient's diagnosis determines the total duration of treatment. Occasionally, treatments are given twice a day.\nWhat will I feel during this procedure?\nExternal beam therapy is painless but patients will hear buzzing or clicking noises during treatment. The linear accelerator may rotate or move during treatment. Patients feel nothing out of the ordinary, but may sometimes smell an odd smell during treatment that is caused by the ozone produced by the linear accelerator. Some patients may also see a colored light when they receive their treatment; this event is especially true for patients having their brain or eye treated.\nYour doctor may recommend a series of follow-up exams after treatment. These may include a physical check-up, imaging exam(s) and blood or other lab tests.\nThese visits help your doctor see if your condition is stable or has changed. They also allow you to discuss any treatment side effects with your doctor.\nWhat kind of treatment follow-up should I expect?\nOnce treatment is complete, patients are asked to return for follow-up visits. During these appointments, patients will undergo evaluation, including imaging exams or blood tests, to determine if their cancer has been eliminated or if additional treatment is required. Even if the cancer has been cured, patients can expect to continue periodic visits to follow-up with their doctor.\nAdditional Information and Resources\nThis page was reviewed on January 30, 2019","Radiation therapy (also called radiotherapy) works by aiming a high dose of radiation towards a patient’s tumour, which damages cells’ DNA. Cancer cells whose DNA is damaged beyond repair stop replicating and die. The body then breaks them down and removes them.\nDepending on the pathology, it takes days or weeks of treatment before cancer cells are damaged enough for cancer cells to die. Cancer cells keep dying for weeks or months after radiation therapy ends.\nTwo main types of radiation therapies:\nexternal beam therapy and internal radiation therapy\n(also called brachytherapy)\nThe type of radiation therapy used depends on many factors, including type of cancer, size of the tumour, its location within the body, patient medical condition, his/her medical history, etc.\nExternal beam radiation\nThis treatment method delivers one or several beams of high-energy x-rays to a patient’s tumour. Beams are generated outside the patient (usually by a linear accelerator – also called Linac) and are targeted at the tumour location.\nThe machine is large and can move around the patient during treatment, sending radiation to a part of the body from many directions in order to concentrate them on the cancer. Advanced radiotherapy techniques aim to maximise the radiation dose that the tumour gets while minimising the dose received by healthy tissues surrounding the tumour.\nThe duration of radiation therapy depends on the type of cancer being treated and whether the goal of radiation therapy is to treat the cancer or ease symptoms.\nUsually external beam radiation is given to patients across a number of sessions that are spread out over time (several weeks), allowing healthy cells to recover between radiation therapy\nAdvanced radiotherapy techniques include IMRT, VMAT and stereotaxic radiosurgery\nInternal radiation therapy\nInternal radiation therapy is a treatment in which a source of radiation is placed inside the patient on a temporary or permanent basis.\nIn brachytherapy, seeds, ribbons, or capsules that contain the radioactive source are placed in or near the tumour. Brachytherapy enables to treat a tumour with very high dose of localised radiation whilst reducing the probability of unnecessary damage to surrounding healthy tissues.\nWhile external beam radiation therapy is used to treat many types of cancer, brachytherapy is most often used to treat cancers of the head and neck, breast, cervix, prostate, and eye.\nAdvanced radiotherapy techniques\nAt Amethyst Radiotherapy centres, we use the most advanced radiotherapy techniques, such as IMRT, VMAT or stereotaxic radiosurgery.\nAdvanced medical imaging combined with state-of-the-art radiotherapy equipment allow to deliver a high dose of radiation, to the tumour, while preserving healthy tissues surrounding the tumour and minimizing the adverse effects of treatment.\nIntensity-modulated radiotherapy or IMRT\nIntensity Modulated Radio Therapy is a conformational radiotherapy technique in which the dose of irradiation delivered by each beam is modulated to administer higher dose to the tumour while minimizing exposure to irradiation of surrounding areas.\nThe linear accelerator – or LINAC – has a device called multi leaf collimator (MLC), which consists of thin movable tungsten leaves that can block part of the radiation field. As the leaves can be moved independently, the radiation can be modelled so that its intensity is maximum at the tumour, and minimal in the surrounding areas.\nWith IMRT treatment, the dose of radiation is concentrated on the volume of the cancerous tumor, thus sparing healthy tissues and organs and reducing the patient’s risk of developing long-term complications.\nVolumetric Modulated Arc Therapy or VMAT\nVMAT – also called Arc Therapy or Rapid Arc radiotherapy – is an IMRT technique which is highly accurate and precise.\nWith VMAT, radiation beam is being delivered in an arc that moves around the tumour and the beam shape and radiation dose automatically change as it moves.\nTreatment planning with VMAT is much more accurate and healthy tissues receive only a very limited fraction of the radiation. This innovative technique, combined with image-guided irradiation processes, ensures high-precision processing.\nThe average time to exposure to radiation for this type of treatment is reduced to a few minutes. In addition, the exposure to radiation is reduced to a few minutes. Treatment with VMAT is therefore not only faster, but also more accurate, with minimal side effects.\nStereotactic radiotherapy (also called stereotactic radiosurgery) is a non-surgical radiation therapy used to deliver precisely targeted radiation to small tumours, located in the brain or in the body.\nStereotactic radiotherapy involves the delivery of higher doses of radiation to the tumour (typically equivalent to one week of classical external irradiation), throughout a smaller number of treatment sessions (also called fractions).\nThe performance of stereotactic radiotherapy is demonstrated in a wide variety of cancers, used to treat both primary cancer (where the cancer started such as the lung or the prostate), or locations where a cancer may have spread (secondary cancers or metastases).\nAs stereotactic radiotherapy requires millimetric accuracy, it requires Image Guided Radiation Therapy (IGRT) – which uses medical imaging to confirm the location of the tumour before and during the radiation treatment.\nIt also requires patient-specific immobilisation devices – such as head and neck masks – to ensure perfect immobility of the targeted body part."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:65ebed0b-8a29-427f-b315-9a56f8ed2ff9>","<urn:uuid:f9950397-a053-4bc6-b2c8-11dc7ce4fdd8>"],"error":null}
{"question":"Could you explain, for example, how olive cultivation in Morocco compares to the rivers of central India like Narmada and Tapi in terms of east-west water flow patterns?","answer":"While the Narmada and Tapi rivers in central India are unique for flowing east to west (unlike most other Indian rivers that flow west to east), with lengths of 1,289 km and 724 km respectively, the olive irrigation system in Morocco works differently. Rather than natural river flows, it uses concrete-lined canals that have been improved by MCC to provide controlled irrigation to olive orchards. This allows farmers in places like Douraine to irrigate more frequently (3-4 times per week versus once before), supporting olive cultivation that produces 15% higher yields through more efficient water delivery rather than relying on natural river flow patterns.","context":["Prosperity Among the Trees\nBrik Aït Ou Massoud had just started walking when he first followed his father into the valley below their home and helped tend to the family olive orchard.\nThe orchard in Douraine, a rural area in Morocco’s Chichaoua province, has` been in Massoud’s family for generations. Even as Morocco lost and regained its independence, as the world went to war twice and the coastal areas of the country experienced rapid development, the family’s techniques for irrigating the lands, pruning the trees, collecting the fruit, and selling the harvest changed little.\nNow 80, Massoud works almost each day on his four hectares of land in the valley, where he grows olives, almonds and barley. And because of the Millennium Challenge Corporation’s Fruit Tree Productivity Project—which improved four irrigation canals feeding 1,700 hectares in the valley, where some of the trees are three centuries old—Massoud is more optimistic than ever about his community’s future.\n“This canal is life-changing for us,” he said. “Before, people would spend their lives just walking back and forth to get water and carry it to their fields. Now it’s all here.”\nThe improvements to the Douraine perimeter included widening and lining almost 12 kilometers of previously earthen irrigation canals with concrete. This provides a critical boost to the economy of the poorest areas of Morocco.\nCreating this economic growth in agricultural areas is the goal of the Fruit Tree Productivity Project, part of MCC’s five-year, $698 million compact with Morocco. The $340 million project aims to grow the agricultural sector and reduce volatility in agricultural production and farm revenues by rehabilitating existing olive trees; expanding smallholder production of olive and almond trees; helping farmers move from high water-use, low-value cereal grains to low water-use, high-value and drought-resistant commercial fruit tree species; and increasing irrigation efficiency and productivity of olive and date trees.\nAlmost 580,000 people are expected to benefit from the project, and household income is estimated to rise by more than $505 million over 20 years because of MCC’s investments. The project is rehabilitating 81,000 hectares of olives, 19,000 hectares of dates, 12,000 hectares of almonds, and 550 hectares of figs.\nIn Douraine, community members believe the irrigation has already improved yields and made life a bit easier in an area where people overwhelmingly rely on agriculture to survive. Nestled in the foothills of the Atlas Mountains, the village seems a world away from the cosmopolitan cities on the coast. The land is often little more than stones and dust, and people build simple homes from rock and use donkeys to haul their goods.\nMassoud said his trees look healthier, sport greener leaves and produce 15 percent more olives—which, like most harvests in Morocco, are sold domestically as table olives or oil. The hours he saves not walking to faraway cisterns to fetch water when the earthen canal dries up have allowed him to spend more time pruning his trees and relaxing with his family.\nBrahim Lekssays is now irrigating his two-hectare field—upon which he grows 60 olive trees and 60 almond trees—three or four times a week. Before, seepage from the unlined, shallow canal allowed him to water his fields only once a week.\n“This is the first time the community has such a gift,” said Lekssays, the vice president of the local olive growers’ cooperative. “And we want to make the most of it.”\nFinding a niche\nAbderrahime Mantourane wants to build a business off MCC’s investments in Morocco’s fruit tree sector. Yet the 29-year-old doesn’t necessarily need to own any trees to accomplish that. Instead, he offers hard work, right timing, know-how, and results for his customers.\n“Many people in this area continue to farm the traditional way,” said Mantourane. “But through training, they can learn how to grow and harvest more efficiently. That’s where we can provide services.”\nIn August 2012, Mantourane helped form the Ourika Cooperative for the Advancement of Agriculture to provide on-farm training, harvesting and orchard rehabilitation services. The group now has 21 employees and markets to farmers in the fields fed by the Ourika River south of Marrakech.\nThe group received field and classroom training on best agricultural practices like maintaining the soil, collecting olives in a way to maintain quality, protecting trees against diseases and insect pests, and pruning. They also learned marketing and business skills through the Fruit Tree Productivity Project.\nThe Ourika cooperative plans to sell olives, as well as make and sell olive oil, on behalf of the landowners and split the profits. Mantourane believes the group has found a niche because many farmers in the region do little to maintain their trees; many do not prune in a way to maximize sunlight on inner branches, resulting in smaller-than-possible harvests.\nAnother member of the cooperative, Hajiba Radi, joined as a first step toward her goal of moving out of her parents’ house and becoming independent. She previously worked as a nanny; if all goes well, she wants to eventually open her own offices where she can sell olives and her own line of olive oils.\nIf things go really well, she said, she would love to walk into a supermarket one day and find a bottle of her olive oil on the shelf—“with a sticker with the price on the bottle.”\nShe also hopes she can encourage other women to work in the agricultural sector. Seven of the cooperative’s members are women.\n“This is a region where agriculture is what people do—especially olives,” she said. “All of the women I’ve talked to want to know about olives, because working with the trees can provide a good job.”\nLeaving a legacy\nThe new trees on Mohamed El Ouafi’s farm stand about four feet high and have just begun to sprout tiny olives. It will be a few years before they are at full productivity, but he has already seen the trees’ potential.\nFor years, El Ouafi grew barley and fava beans on his four-hectare plot in the Ben Khlil perimeter, between Khenifra and Beni Mellal. During lean times, he watched his neighbors harvest their olives and turn a profit. So when a local representative from the Ministry of Agriculture told El Ouafi about MCC’s plans to expand the olive orchards in the area, he did not have to look farther than next door for affirmation.\n“It worked for my neighbors,” he said, “and if it worked for them, it could work for me.”\nMCC helped plant about 72,000 certified seedlings on about 715 hectares in the Ben Khlil perimeter. It also trained individual farmers and developed cooperatives.\nAcross Morocco, MCC is helping extend smallholder production on about 82,000 hectares, mostly olives but also almonds.\nEl Ouafi used MCC’s investment as an opportunity to make other improvements to his farm. He dug a well and is building a reservoir to ensure a steadier, year-round supply of water to irrigate olives, barley and other crops.\nHe also sees the trees as his legacy. El Ouadi has two wives, but at age 63, he knows he does not have any children.\n“The Prophet says to plant olive trees because it is a wise thing to do,” he said. “My wives will one day have this land and these trees—and then I can go peacefully.”","The rivers of India\nRivers of India play an important role in the lives of the Indians. They provide potable water, cheap transportation, electricity, and the livelihood for a large number of people all over the country. This easily explains why nearly all the major cities of India are located by the banks of rivers. The rivers also have an important role in Hindu Dharma and are considered holy by all Hindus in the country\nSeven major rivers along with their numerous tributaries make up the river system of India. The largest basin system of the rivers pour their waters into the Bay of Bengal; however, some of the rivers whose courses take them through the western part of the country and towards the east of the state of Himachal Pradesh empty into the Arabian Sea. Parts of Ladakh, northern parts of the Aravalli range and the arid parts of the Thar Desert have inland drainage.\nThe rivers of India can be classified on the basis of origin\nHimalayan Rivers and Peninsular Rivers.\nMain Himalayan Rivers\nThe main Himalayan river systems are the Ganga, the Indus and the Brahmaputra river systems. The Himalayan rivers form large basins. Many rivers pass through the Himalayas. These deep valleys with steep rock sides were formed by the down – cutting of the river during the period of the Himalayan uplift. They perform intense erosional activity up the streams and carry huge load of sand and silt. In the plains, they form large meanders, and a variety of depositional features like flood plains, river cliffs and levees.\nThese rivers are perennial as they get water from the rainfall as well as the melting of ice. Nearly all of them create huge plains and are navigable over long distances of their course. These rivers are also harnessed in their upstream catchment area to generate hydroelectricity\nIndus River System\nThe ‘Indus River originates in the northern slopes of the Kailash range near Lake Mansarovar in Tibet. Although most of the river’s course runs through neighbouring Pakistan, as per as regulation of Indus water treaty of 1960, India can only use only 20 percent of the water in this river. A portion of it does run through Indian territory, as do parts of the courses of its five major tributaries, listed below. These tributaries are the source of the name of the Punjab of South Asia; the name is derived from the punch (“five”) and aab (“water”), hence the combination of the words (Punjab) means “land with the water of five rivers”. The Indus is 3,200 kilometres (2,000 mi) long.\nThe major rivers in Indus river system are Indus, Chenab , Jhelum , Ravi , Sutlej ,Beas\nThe Jhelum originates in the south-eastern part of Kashmir, in a spring at Verinag. It flows into the Wular Lake, which lies to the north, and then into Baramula. Between Baramula and Muzaffarabad it enters a deep gorge cut by the river in the Pir Panjal range. It has a right bank tributary the Kishanganga which joins it at Muzaffarabad. It follows the Indo-Pakistan border flowing into the plains of Punjab, finally joining the Chenab at Trimmu.\nThe Chenab originates from the confluence of two rivers, the Chandra and the Bhaga, which themselves originate from either side of the Bara Lacha Pass in Lahul. It is also known as the Chandrabhaga in Himachal Pradesh. It runs parallel to the Pir Panjal Range in the north-westerly direction, and cuts through the range near Kishtwar. It enters the plains of Punjab near Akhnur and is later joined by the Jhelum. It is further joined by the Ravi and the Sutlej in Pakistan.\nThe Ravi originates near the Rotang pass in the Kangra Himalayas and follows a north-westerly course. It turns to the south-west, near Dalhousie, and then cuts a gorge in the Dhaola Dhar range entering the Punjab plain near Madhopur. It flows as a part of the Indo-Pakistan border for some distance before entering Pakistan and joining the Chenab river. The total length of the river is about 720 km.\nThe Beas originates in Beas Kund, lying near the Rohtang pass. It runs past Manali and Kulu, where its beautiful valley is known as the Kulu valley. It first follows a north-west path from the town of Mandi and later a westerly path, before entering the Punjab plains near Mirthal. It joins the Sutlej river near Harika, after being joined by a few tributaries. The total length of the river is 615 km.\nThe Sutlej originates from the Rakas Lake, which is connected to the Manasarovar lake by a stream, in Tibet. Its flows in a north-westerly direction and enters Himachal Pradesh at the Shipki Pass, where it is joined by the Spiti river. It cuts deep gorges in the ranges of the Himalayas, and finally enters the Punjab plain after cutting a gorge in a hill range, the Naina Devi Dhar, where the Bhakra Dam having a large reservoir of water, called the Gobind Sagar, has been constructed. It turns west below Rupar and is later joined by the Beas. It enters Pakistan near Sulemanki, and is later joined by the Chenab. It has a total length of almost 1500 km.\nGanga river system\nIt is 2525 km lengthy of which 1450 km is in Uttarakhand and UP, 445 km in Bihar and 520 km in West Bengal.\nThe Ganga, the top stream is constituted of two leading rivers – Bhagirthi and Alaknanda, which mix at Devprayag to kind Ganga.\nBefore Alaknanda meets Bhagirthi at Devprayag, Mandakini meets Alaknanda at Rudraprayag.\nSources: Bhagirthi from Gaumukh, Alaknanda from Badrinath, Mandakini from Kedarnath (all from Uttarakhand).\nYamuna (1375 km) is its most significant tributary (on correct financial institution). It rises on the Yamunotri glacier in Uttarakhand. It runs parallel to Ganga for 800km and joins it at Allahabad. Important tributaries of Yamuna are Chambal (1050 km), Sind, Betwa (480 km) and Ken (all from south).\nApart from Yamuna, different tributaries of Ganga are Ghaghra (1080 km), Son (780 km), Gandak (425 km), Kosi (730 km), Gomti (805 km), Damodar (541 km). Kosi is notorious as ‘Sorrow of Bihar’, whereas Damodar receives the identify ‘Sorrow of Bengal’ as these trigger floods in these areas.\nHooghli is a distributory of Ganga flowing via Kolkata.\nThe Brahmaputra River System\nThe Brahmaputra originates in the Mansarovar lake, also the source of the Indus and the Satluj. It is slightly longer than the Indus, but most of its course lies outside India. It flows eastward, parallel to the Himalayas. Reaching Namcha Barwa (7757 m), it takes a U-turn around it and enters India in Arunachal Pradesh and known as dihang. The undercutting done by this river is of the order of 5500 metres. In India, it flows through Arunachal Pradesh and Assam, and is joined by several tributaries.\nIn Tibet, the river is known as the Tsangpo. There, it receives less volume of water and has less silt. But in India, it passes through a region of heavy rainfall and as such, the river carries a large amount of rainfall and considerable amount of silt. The Brahmaputra has a braided channel throughout most of its length in Assam, with a few large islands within the channel.\nThe shifting of the channels of the river is also very common. The fury of the river during rains is very high. It is known for creating havoc in Assam and Bangladesh. At the same time, quite a few big pockets suffer from drought\nThe main peninsular river systems include the Narmada, the Tapi, the Godavari, the Krishna, the Kaveri and the Mahanadi river systems. The Peninsular rivers flow through shallow valleys. A large number of them are seasonal as their flow is dependent on rainfall. The intensity of erosional activities is also comparatively low because of the gentler slope. The hard rock bed and lack of silt and sand does not allow any significant meandering. Many rivers therefore have straight and linear courses. These rivers provide huge opportunities for hydro-electric power.\nThe Narmada River System\nThe Narmada or Nerbudda is a river in central India. It forms the traditional boundary between North India and South India, and is a total of 1,289 km (801 mi) long. Of the major rivers of peninsular India, only the Narmada, the Tapti and the Mahi run from east to west. It rises on the summit of Amarkantak Hill in Madhya Pradesh state, and for the first 320 kilometres (200 miles) of its course winds among the Mandla Hills, which form the head of the Satpura Range; then at Jabalpur, passing through the ‘Marble Rocks’, it enters the Narmada Valley between the Vindhya and Satpura ranges, and pursues a direct westerly course to the Gulf of Cambay. Its total length through the states of Madhya Pradesh, Maharashtra, and Gujarat amounts to 1312 kilometres (815 miles), and it empties into the Arabian Sea in the Bharuch district of Gujarat.\nThe Tapi River System\nThe Tapi is a river of central India. It is one of the major rivers of peninsular India with the length of around 724 km, and only the Tapi River along with the Narmada river, and the Mahi River run from east to west. It rises in the eastern Satpura Range of southern Madhya Pradesh state, and flows westward, draining Madhya Pradesh’s historic Nimar region, Maharashtra’s historic Khandesh and east Vidarbha regions in the northwest corner of the Deccan Plateau and South Gujarat before emptying into the Gulf of Cambay of the Arabian Sea, in the State of Gujarat. The Western Ghats or Sahyadri range starts south of the Tapti River near the border of Gujarat and Maharashtra.\nThe Tapi River Basin lies mostly in northern and eastern districts Maharashtra state viz, Amravati, Akola, Buldhana, Washim, Jalgaon, Dhule, Nandurbar, Malegaon, Nashik districts but also covers Betul, Burhanpur districts of Madhya Pradesh and Surat district in Gujarat as well.\nThe principal tributaries of Tapi River are Purna River, Girna River, Panzara River, Waghur River, Bori River and Aner River.\nThe Godavari River System\nThe river with second longest course within India, Godavari is often referred to as the Vriddh (Old) Ganga or the Dakshin (South) Ganga. The name may be apt in more ways than one, as the river follows the course of Ganga’s tragedy. The river is about 1,450 km (900 miles) long. It rises at Trimbakeshwar, near Nasik and Mumbai (formerly Bombay) in Maharashtra around 380 km distance from the Arabian Sea, but flows southeast across south-central India through the states of Madhya Pradesh, Karnataka, Orissa and Andhra Pradesh, and empties into the Bay of Bengal. At Rajahmundry, 80 km from the coast, the river splits into two streams thus forming a very fertile delta. Like any other major rivers in India, the banks of this river also has many pilgrimage sites, Nasik, Triyambak and Bhadrachalam, being the major ones. It is a seasonal river, widened during the monsoons and dried during the summers. Godavari river water is brownish. Some of its tributaries include Indravati River, Pranahita (Combination of Penuganga and Warda), Manjira, Bindusara and Sabari. Some important urban centers on its banks include Nasik, Bhadrachalam, Rajahmundry and Narsapur. The Asia’s largest rail-cum-road bridge on the river Godavari linking Kovvur and Rajahmundry is considered to be an engineering feat.\nThe Krishna River System\nThe Krishna is one of the longest rivers of India (about 1300 km in length). It originates at Mahabaleswar in Maharashtra, passes through Sangli and meets the sea in the Bay of Bengal at Hamasaladeevi in Andhra Pradesh. The Krishna River flows through the states of Maharashtra, Karnataka and Andhra Pradesh.\nThe traditional source of the river is a spout from the mouth of a statue of a cow in the ancient temple of Mahadev in Mahabaleshwar.\nIts most important tributary is the Tungabhadra River, which itself is formed by the Tunga and Bhadra rivers that originate in the Western Ghats. Other tributaries include the Koyna, Bhima, Mallaprabha, Ghataprabha, Yerla, Warna, Dindi, Musi and Dudhganga rivers.\nThe Kaveri River System\nThe Kaveri (also spelled Cauvery or Kavery) is one of the great rivers of India and is considered sacred by the Hindus. This river is also called Dakshin Ganga. The headwaters are in the Western Ghats range of Karnataka state, and from Karnataka through Tamil Nadu. It empties into the Bay of Bengal. Its waters have supported irrigated agriculture for centuries, and the Kaveri has been the lifeblood of the ancient kingdoms and modern cities of South India.\nThe source of the river is Talakaveri located in the Western Ghats about 5,000 feet (1,500 m) above sea level. Talakaveri is a famous pligrimage and tourist spot set amidst Bramahagiri Hills near Madikeri in Kodagu district of Karnataka. Thousands of piligrims flock to the temple at the source of the river especially on the specified day known as Tula sankramana when the river water has been witnessed to gush out like a fountain at a predetermined time. It flows generally south and east for around 765 km, emptying into the Bay of Bengal through two principal mouths. Its basin is estimated to be 27,700 square miles (71,700 km²), and it has many tributaries including Shimsha, Hemavati, Arkavathy, Kapila, Honnuhole, Lakshmana Tirtha, Kabini, Lokapavani, Bhavani, Noyyal and Famous Amaravati.\nThe Mahanadi River System\nThe Mahanadi is a river of eastern India. The Mahanadi rises in the Satpura Range of central India, and flows east to the Bay of Bengal. The Mahanadi drains most of the state of Chhattisgarh and much of Orissa and also Jharkhand and Maharashtra. It has a length of about 860 km.\nNear the city of Sambalpur, a large dam – the Hirakud Dam – is built on the river."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:fa0cf584-9c83-44ca-a756-ffacd389f0a4>","<urn:uuid:64c1dd2c-f02f-43a1-bdde-8c681acba346>"],"error":null}
{"question":"I am electrical engineer working with filters. Could you compare harmonic filters in VFD systems versus standalone filter applications? What are key differences between active and passive filters in terms of power requirements and components used for each case?","answer":"In VFD systems, harmonic filters (both active and passive) are used specifically to mitigate harmonics that could disrupt generator operation. A 5% THD passive harmonic filter with six-pulse VFD is sufficient to prevent generator disruption. Active filters may provide better harmonic mitigation for VFD systems but require analysis for optimal selection. As standalone components, passive filters use RLC (resistors, capacitors, inductors) components, don't require external power, and have low input/high output impedance. Active filters use operational amplifiers and transistors, require external power supply, have high input/low output impedance, and don't use inductors. Passive filters are generally more stable, cheaper and can handle higher frequencies, while active filters offer power gain and adjustable parameters but have frequency limitations due to active components.","context":["Drives, Harmonics and Gensets\nWhile there are many factors that affect the proper operation of an emergency or standby generator set, one that is commonly overlooked is the impact of variable frequency drives (VFD). As all building electrical system designers know, the push towards greater energy efficiency has encouraged the increased use of VFDs in facility applications.\nWhile there are many factors that affect the proper operation of an emergency or standby generator set, one that is commonly overlooked is the impact of variable frequency drives (VFD).\nAs all building electrical system designers know, the push towards greater energy efficiency has encouraged the increased use of VFDs in facility applications. In fact, today one commonly sees VFDs used for supply and return fans, chillers, domestic water pressurization pumps, chilled and condenser water pumps, and many other pieces of motor-operated equipment in all types of facilities—you name it. VFDs are being used for almost everything.\nAnd with increased use of VFDs, there is always the possibility that the level of harmonics generated by these systems will have an operational impact on facility equipment, but designers aren't always thinking about the effect of harmonics on the gensets.\nThe potential adverse effect of harmonics on the on-site generator set is caused by the interaction between the harmonics on the generator output wave form and the voltage regulator. For a typical, self-excited generator, the voltage regulator monitors the output voltage and makes adjustments in the generator field windings to maintain a constant voltage to the load.\nIf there are significant harmonics reflected from harmonic-rich loads, such as VFDs, UPS systems and fluorescent ballasts, the voltage regulator attempts to adjust the voltage up or down based on the output wave form that is distorted by the harmonics. Since the distortion gives false information to the voltage regulator, it tries to adjust the voltage to maintain voltage level at a constant level, but the distortion may make the voltage go up when it should stay constant, or down when it should be going up.\nBecause this voltage adjustment is a closed-loop control system, as the voltage is incorrectly changed, based on the distorted information, it is then even more incorrect and the output voltage finally gets so far off that the generator ceases to produce an output that is usable by the loads. For generators that utilize permanent magnets mounted around the generator shaft to generate power for the field windings, there is much less sensitivity to harmonics, since there is no direct connection between the voltage regulator and the output voltage.\nGuarding Against Harmonics\nWhile there are many factors that can affect the harmonic levels created by VFDs (see “Harmonics and the Variable Frequency Drive”), the use of harmonic filters and the level of available fault current have a direct impact on whether the harmonics affect generator operation.\nFor instance, utilization of a six-pulse VFD with a 5% total harmonic distortion (THD) passive harmonic filter will not create enough harmonics to affect generator operation. The absence of the filter, however, could allow enough harmonics into the system to disrupt the generator voltage regulator operation. In place of the passive harmonic filter, an active filter may provide a more satisfactory solution to system harmonics, but the engineer should analyze the various options before selecting the best alternative.\nThe location of filters can also affect how the generator reacts to VFDs in the system. Let's say, for example, a single harmonic filter that is intended to mitigate all harmonics in an installation, and which is sized to meet IEEE 519, is connected to the incoming service equipment. In this case, there will be adequate mitigation of the harmonics to satisfy the electrical utility, but the generator set will still be affected.\nTo assure that there are no interactions between the VFDs and the generator, the harmonic filters must be located at each of the VFDs or at the main distribution panel or motor control center feeding the VFDs. This will prevent the harmonics from affecting the generator operation, because they will not be visible to the generator voltage regulator.\nIn the past, some engineers have depended on drive-isolation transformers or series inductors to mitigate harmonics created by a VFD. While there is some small benefit for harmonic mitigation in the use of these inductive devices, dependence on them may not work for many systems since the harmonic attenuation for these systems is generally inadequate to reduce the harmonic level enough to prevent voltage regulator interference. This is not to say that the inductive systems are not, in some cases, beneficial in the installation of VFDs. They simply are not the option of choice for harmonic mitigation.\nWhat to Look For\nTherefore, when one finds that there is a single, large VFD or multiple, smaller VFDs in a distribution in which they are designing an emergency generator set, consider that the created harmonics may make the generator non-operational. Utilization of properly located harmonic filters, active harmonic filters, reduced harmonic VFDs and permanent magnetic generators can all mitigate the harmonic effects on the generator and it will operate properly.\nHarmonics and the Variable Frequency Drive\nTo define the differences among the various variable frequency drives, and their potential impact on generator set operation, one must first examine the available equipment from the various manufacturers.\nThe most common and least expensive VFDs are the six-pulse drives. These have a single pair of electronic switches for each of the three phases, thus resulting in six pulses of harmonics within each full electrical cycle. With increasing prices, there are 12- and 18-pulse drives that, respectively, have two and three pairs of electronic switches for each phase, with the resultant 12 and 18 pulses of harmonics within each electrical cycle. With the increase of the number of pulses in a drive, there is a proportional decrease in the harmonic content reflected into the power distribution system. When the six-pulse VFDs are utilized in a facility, mitigation of the harmonics may require the addition of either active or passive harmonic filters for the facility to meet IEEE 519. For the majority of facilities, smaller or no harmonic filters are required when the 12- and 18-pulse drives are utilized.\nThe type of harmonics will also affect the impact on the generator operation. If the harmonics are limited to current harmonics, there will be some conductor heating and potential neutral problems, but these will not affect the generator operation. Current harmonics normally occur in systems with more limited quantities of VFDs and other harmonic producing loads and higher levels of available fault duty. Voltage harmonics, on the other hand, occur where there are many harmonic loads and limited available fault duty. Without some type of harmonic mitigation, there will be problems with the generator operation.\nEvery year, CSE magazine sponsors a “Product of the Year” competition, and the winners appear in the November issue. In 2006, the grand winner of this competition was ABB's ultra-low-harmonic drive.\n“Improving line voltage quality by reducing harmonics is a primary concern for many variable-speed drive users,” said Ari Hedemaki, production line manager, ACS800 Industrial Drives, ABB, Low Voltage Drives. “Harmonics can disturb equipment of other power consumers, cause additional losses in power distribution and, in some cases, can even damage equipment connected to the same network.”\nThese drives, available as wall-mounted and cabinet-built units, use an active converter controlled with direct torque control to eliminate low-order harmonics and an active front-end line filter to combat high-frequency harmonics. With total distortion reduced to about 4%, it meets IEEE 519 and G5/4 requirements.\nThe 18-pulse Drive\nFor municipal and industrial pumping applications, Square D offers another version of low-harmonic drive. The Powergard 18-Pulse Series C enclosed drive controller features adjustable frequency drives/power converters. The result is said to be a packaged, adjustable speed drive with a clean power solution.\nThe series is claimed to be suitable for installations specifying “clean power” low-harmonic content in compliance with IEEE 519 guidelines for harmonic mitigation. The drives are also UL 508 listed.\nIn addition, they feature a heavy-duty industrial disconnect handle with lock-out/tag-out provisions. The drives are available in two application ratings: constant torque/heavy duty (40 hp to 450 hp at 460 volts) and variable torque/light duty (50 hp to 500 hp at 460 volts).\nWhat can the 18-Pulse controller do for you? Several capabilities are offered by this type of drive. For example, it offers clean power—low harmonic content—for highhorsepower ratings.\nMoreover, these drives can be customized for specific applications with an advanced technology platform that increases reliability and uptime.\nSome other features of these drives are: an intuitive door-mounted graphic display screen in plain text; programming that is clear, precise and illustrated; and diagnostics that are intuitive.\nFinally, operators mounted on enclosure doors are pre-programmed to save set-up time and expense.","Active Filter vs Passive Filter\nFilters are a class of electronic circuits used in signal processing, to allow or block a desired signal range or a signal. Filters can be categorized at many levels based on the properties, such as active – passive, analog – digital, linear – non-linear, discrete time – continuous time, time invariant – time variant, and infinite impulse response – finite impulse response.\nActive and passive filters are differentiated by the passivity of the components used in the filter circuit. If a component consumes power or incapable of power gain then it is known as a passive component. Components that are not passive are known as active components.\nMore about Passive Filters\nResistors, capacitors, and inductors all consume power when a current passes through them, and incapable of power gain; therefore, any RLC filter is a passive filter, especially with the inductors included. Another major characteristic of the passive filters is that the filters do not need an external power source for operation. The input impedance is low and the output impedance is high, allowing self-regulation of the voltages driving the loads.\nUsually, in passive filters, load resistor is not isolated from the rest of the network; therefore, change in the load may affect the characteristics of the circuit and filtering process. However, there are no bandwidth restrictions for the passive filters, allowing satisfactory operation at very high frequencies. In lower frequency filters, the inductor used in the circuit tends to be larger, making the circuit bulkier. If higher quality and smaller size are required, the cost is increased significantly. Passive filters also create a little amount of noise, due to the thermal noise in the elements. Nevertheless, with proper design this noise amplitude can be minimized.\nSince there is no signal gain, the signal amplification must be performed at a later stage. Sometimes buffer amplifiers may be needed to compensate the differences in the output circuit. .\nMore about Active Filters\nFilters with components such as operational amplifiers, transistors, or other active elements are known as active filters. They use capacitors and resistors, but inductors are not used. Active filters require an external power source to operate because of the power consuming active elements in the design.\nSince no inductors are used, the circuit is more compact and less heavy. Its input impedance is high and the output impedance is low, allowing to drive low impedance loads at the output. Generally, the load is isolated from the internal circuit; hence variation of the load does not affect the characteristics of the filter.\nThe output signal has a power gain, and the parameters like gain pass band and cutoff frequency can be adjusted. Several drawbacks are inherent to the active filters. The changes in the power supply may cause changes in the output signal magnitude and the high frequency ranges are limited by the active element properties. Also, feedback loops used for regulating the active components may contribute to oscillation and noise.\nWhat is the difference between Active and Passive Filters?\n• Passive filters consume the energy of the signal, but no power gain is available; while active filters have a power gain.\n• Active filters require an external power supply, while passive filters operate only on the signal input.\n• Only passive filters use inductors.\n• Only active filters use elements kike op-amps and transistors, which are active elements.\n• Theoretically, passive filters have no frequency limitations while, active filters have limitations due to active elements.\n• Passive filters have a better stability and can withstand large currents.\n• Passive filters are relatively cheaper than active filters."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:e4dfa5c5-19b5-4de1-bfb6-910652302aa7>","<urn:uuid:e1c9411c-f776-420c-8a99-860ef3723c3b>"],"error":null}
{"question":"What roles do conservation and mathematical precision play in both preserving Nebraska's rivers and maintaining musical instrument tuning?","answer":"Both musical tuning and river conservation require careful balance and precise measurements. In music, temperament systems must carefully balance interval purity, with even the equal tempered scale having small but audible imperfections of about 1% in major and minor thirds. Similarly, Nebraska's river conservation requires precise balance of multiple needs - the Game and Parks Commission must carefully manage water flows to simultaneously support wildlife habitats, recreational activities, and human water usage. For example, the Niobrara River needs specific flow levels to maintain habitats for endangered species like whooping cranes while also supporting recreational activities like canoeing. Both domains demonstrate how mathematical precision and careful management are essential for preserving these natural and cultural resources.","context":["[2.2.1] Chromatic Scale and Temperament\nThree octaves of the chromatic scale are shown in Table 2.2a using the A, B, C, . . . notation. Black keys on the piano are shown as sharps, e.g. the # on the right of C represents C#, etc., and are shown only for the highest octave. Each successive frequency change in the chromatic scale is called a semitone and an octave has 12 semitones. The major intervals and the integers representing the frequency ratios for those intervals are shown above and below the chromatic scale, respectively. Except for multiples of these basic intervals, integers larger than about 10 produce intervals not readily recognizable to the ear. In reference to Table 2.2a, the most fundamental interval is the octave, in which the frequency of the higher note is twice that of the lower one. The interval between C and G is called a 5th, and the frequencies of C and G are in the ratio of 2 to 3. The major third has four semitones and the minor third has three. The number associated with each interval, e.g. four in the 4th, is the number of white keys, inclusive of the two end keys for the C-major scale, and has no further mathematical significance. Note that the word \"scale\" in \"chromatic scale\", \"C-major scale\", and \"logarithmic or frequency scale\" (see below) has different meanings; the second is a subset of the first.\n|Octave||5th||4th||Maj. 3rd||Min. 3rd|\n|CDEFGAB||C D E F||G A B||C# D#||E F#||G# A# B||C|\nWe can see from the above that a 4th and a 5th \"add up\" to an octave and a major 3rd and a minor 3rd \"add up\" to a 5th. Note that this is an addition in logarithmic space, as explained below. The missing integer 7 is also explained below.\nThe \"equal tempered\" (ET) chromatic scale consists of \"equal\" half-tone or semitone rises for each successive note. They are equal in the sense that the ratio of the frequencies of any two adjacent notes is always the same. This property ensures that each note is the same as any other note (except for pitch). This uniformity of the notes allows the composer or performer to use any key without hitting bad dissonances, as further explained below. There are 12 equal semitones in an octave of an ET scale and each octave is an exact factor of two in frequency. Therefore, the frequency change for each semitone is given by\nsemitone12 = 2 or\nsemitone = 21/12 = 1.05946 . . . . . . . . . Eq. (2.1)\nEq. (2.1) defines the ET chromatic scale and allows the calculation of the frequency ratios of \"intervals\" in this scale. How do the \"intervals\" in ET compare with the frequency ratios of the ideal intervals? The comparisons are shown in Table 2.2b and demonstrate that the intervals from the ET scale are extremely close to the ideal intervals.\n|Interval||Freq. Ratio||Eq. Tempered Scale||Difference|\n|Min.3rd:||6/5 = 1.2000||semitone3 = 1.1892||+0.0108|\n|Maj.3rd:||5/4 = 1.2500||semitone4 = 1.2599||-0.0099|\n|Fourth:||4/3 = 1.3333||semitone5 = 1.3348||-0.0015|\n|Octave:||2/1 = 2.0000||semitone12 = 2.0000||0.0000|\nThe errors for the 3rds are the worst, over five times the errors in the other intervals, but are still only about 1%. Nonetheless, these errors are readily audible, and some piano aficionados have generously dubbed them \"the rolling thirds\" while in reality, they are unacceptable dissonances. It is a defect that we must learn to live with, if we are to adopt the ET scale. The errors in the 4ths and 5ths produce beats of about 1 Hz near middle C, which is barely audible in most pieces of music; however, this beat frequency doubles for every higher octave.\nThe integer 7, if it were included in Table 2.2a, would have represented an interval with the ratio 7/6 and would correspond to a semitone squared. The error between 7/6 and a semitone squared is over 4% and is too large to make a musically acceptable interval and was therefore excluded from Table 2.2a. It is just a mathematical accident that the 12-note chromatic scale produces so many ratios close to the ideal intervals. Only the number 7, out of the smallest 8 integers, results in a totally unacceptable interval. The chromatic scale is based on a lucky mathematical accident in nature! It is constructed by using the smallest number of notes that gives the maximum number of intervals. No wonder early civilizations believed that there was something mystical about this scale. Increasing the number of keys in an octave does not result in much improvement of the intervals until the numbers become quite large, making that approach impractical for most musical instruments.\nNote that the frequency ratios of the 4th and 5th do not add up to that of the octave (1.5000 + 1.3333 = 2.8333 vs 2.0000). Instead, they add up in logarithmic space because (3/2)x(4/3) = 2. In logarithmic space, multiplication becomes addition. Why might this be significant? The answer is because the geometry of the cochlea of the ear seems to have a logarithmic component. Detecting acoustic frequencies on a logarithmic scale accomplishes two things: you can hear a wider frequency range for a given size of cochlea, and analyzing ratios of frequencies becomes simple because instead of dividing or multiplying two frequencies, you only need to subtract or add their logarithms. For example, if C3 is detected by the cochlea at one position and C4 at another position 2mm away, then C5 will be detected at a distance of 4 mm, exactly as in the slide rule calculator. To show you how useful this is, given F5, the brain knows that F4 will be found 2mm back! Therefore, intervals (remember, intervals are frequency divisions) are particularly simple to analyze in a logarithmically constructed cochlea. When we play intervals, we are performing mathematical operations in logarithmic space on a mechanical computer called the piano, as was done in the 1950's with the slide rule. Thus the logarithmic nature of the chromatic scale has many more consequences than just providing a wider frequency range. The logarithmic scale assures that the two notes of every interval are separated by the same distance no matter where you are on the piano. By adopting a logarithmic chromatic scale, the piano keyboard is mathematically matched to the human ear in a mechanical way! This is probably one reason for why harmonies are pleasant to the ear - harmonies are most easily deciphered and remembered by the human hearing mechanism.\nSuppose that we did not know Eq. 2.1; can we generate the ET chromatic scale from the interval relationships? If the answer is yes, a piano tuner can tune a piano without having to make any calculations. These interval relationships, it turns out, completely determine the frequencies of all the notes of the 12 note chromatic scale. A temperament is a set of interval relationships that provides this determination. From a musical point of view, there is no single \"chromatic scale\" that is best above all else although ET has the unique property that it allows free transpositions. Needless to say, ET is not the only musically useful temperament, and we will discuss other temperaments below. Temperament is not an option but a necessity; we must choose a temperament in order to accommodate these mathematical difficulties. No musical instrument based on the chromatic scale is completely free of temperament. For example, the holes in wind instruments and the frets of the guitar must be spaced for a specific tempered scale. The violin is a devilishly clever instrument because it avoids all temperament problems by spacing the open strings in fifths. If you tune the A(440) string correctly and tune all the others in 5ths, these others will be close, but not tempered. You can still avoid temperament problems by fingering all notes except one (usually A-440). In addition, the vibrato is larger than the temperament corrections, making temperament differences inaudible.\nThe requirement of tempering arises because a chromatic scale tuned to one scale (e.g., C-major with perfect intervals) does not produce acceptable intervals in other scales. If you wrote a composition in C-major having many perfect intervals and then transposed it, terrible dissonances can result. There is an even more fundamental problem. Perfect intervals in one scale also produce dissonances in other scales needed in the same piece of music. Tempering schemes were therefore devised to minimize these dissonances by minimizing the de-tuning from perfect intervals in the most important intervals and shifting most of the dissonances into the less used intervals. The dissonance associated with the worst interval came to be known as \"the wolf\".\nThe main problem is, of course, interval purity; the above discussion makes it clear that no matter what you do, there is going to be a dissonance somewhere. It might come as a shock to some that the piano is a fundamentally imperfect instrument! We are left to deal forever with some compromised intervals in almost every scale.\nThe name \"chromatic scale\" generally applies to any 12-note scale with any temperament. Naturally, the chromatic scale of the piano does not allow the use of frequencies between the notes (as you can with the violin), so that there is an infinite number of missing notes. In this sense, the chromatic scale is incomplete. Nonetheless, the 12-note scale is sufficiently complete for the majority of musical applications. The situation is analogous to digital photography. When the resolution is sufficient, you cannot see the difference between a digital photo and an analog one with much higher information density. Similarly, the 12-note scale apparently has sufficient pitch resolution for a sufficiently large number of musical applications. This 12-note scale is a good compromise between having more notes per octave for greater completeness and having enough frequency range to span the range of the human ear, for a given instrument or musical notation system with a limited number of notes.\nThere is healthy debate about which temperament is best musically. ET was known from the earliest history of tuning. There are definite advantages to standardizing to one temperament, but that is probably not possible or even desirable in view of the diversity of opinions on music and the fact that much music now exist, that were written with particular temperaments in mind. Therefore we shall now explore the various temperaments.","Rivers and streams are crucial to Nebraska and its citizens. They provide habitat to numerous plant, fish and wildlife species, including many that are endangered or protected. They diversify the state’s economy and contribute to the quality of life enjoyed by Nebraskans. They are where many Nebraskans go to unwind – to fish, canoe or kayak, or just contemplate the beauty of nature.\nThe mission of the Nebraska Game and Parks Commission is to serve as stewards of the state’s fish, wildlife, park, and outdoor recreation resources in the best long-term interests of the people and those resources. To that end, under the direction of the Board of Commissioners, the commission in 2006 began conducting a series of scientific studies to determine the flow needs for maintaining healthy fish and wildlife populations, as well as for offering recreational floating opportunities on the Niobrara River.\nAs far as rivers go, the Niobrara is one of Nebraska’s crown jewels. The Niobrara has a scenic river designation in the reach below Valentine; this designation helped lead to a boom in the tourism economy there, as tourists have visited to canoe, kayak, or tube. It provides roosting habitat for migrating whooping cranes and nesting habitat for terns and plovers. But this river lacks legal instream flow protection for recreation and public trust fish and wildlife resources. The Commission is working with the Nebraska Public Power District, the Upper, Middle and Lower Natural Resource Districts (NRD), and the Upper Loup and Upper Elkhorn Natural Resource Districts to remedy this situation. The general signed a Memorandum of Understanding and a River Management Agreement in September 2015 to work together to protect the Niobrara above Spencer Dam.\nAlthough not a signatory agency, the National Park Service helped ensure adequate flows for recreational users were part of the state plan. From Spencer Dam downstream to the confluence with the Missouri River the commission and the NRDs have agreed to apply for instream flow appropriations for fish and wildlife purposes. A formal public hearing was conducted at the Holt County Extension office in O’Neil on October 1, 2015. More than 100 citizens attended and 23 provided input into this process. The applications were submitted to the Nebraska Department of Natural Resource on December 4, 2015. The applications will now go through an administrative process to determine if they meet state law requirements for instream flows.\nIn Nebraska, all water, both on the surface and underground, is owned by the state, and how we use this resource is determined by the Legislature. Because human, economic and social health are tied to the ecological heath of our state’s rivers, we must ensure each is managed sustainably and their vitality maintained for future generations. The following links provide an overview to how instream and out-of-stream uses affect recreation, industry, and wildlife that depend upon the Niobrara.\nGame and Parks Commission staff presented these finding during public meetings in Chadron, Valentine, Butte and Lincoln in December 2014. The presentation staff gave at those meetings is available to the public online. These studies will also be submitted to the Nebraska Department of Natural Resouces as part of the instream flow application process.\nHydrological Data Analysis of the Niobrara River -2008\nUSGS – Hydrogeomorphic Segments and Hydraulic Microhabitats of the Niobrara River, Nebraska—With Special Emphasis on the Niobrara National Scenic River\nJason S. Alexander, Ronald B. Zelt, and Nathan J. Schaepe\nFlows and Recreational Floating on the Niobrara National Scenic River, Nebraska\nDoug Whittaker Ph.D. & Bo Shelby, Ph.D. of Confluence Research and Consulting\nEconomic & Social Values of Recreational Floating on the Niobrara National Scenic River – Final Report, July 2009\nSteven Shultz, PhD University of Nebraska at Omaha\nThe Extent and Value of Agricultural, Municipal, and Industrial Water Use in the Niobrara Basin – Final Research Report, December 7, 2010\nSteven Shultz, PhD University of Nebraska at Omaha\nDeveloping Environmental Flows for Fish and Wildlife: A Mesohabitat Study on the Niobrara River – Revised Final Report, June 2014\nPiotr Parasiewicz, Mark Pegg, Joseph Rogers, Adam Behmer, Allan Eldridge\nNebraska Game and Parks staff traveled throughout the Niobrara basin and to Lincoln to share the findings of the above studies. This Summary Presentation was shared in Chadron, Valentine, Butte and Lincoln.\nDownload PowerPoint notes PDF\nDownload PowerPoint slides PDF\nWater Conservation and Management\nAll rivers in Nebraska empty into the Missouri River at some point. It is the largest river in the basin and provides many ecosystem goods and services to mankind. Although highly altered, proactive steps are being taken to help make it an economic engine for the states adjacent to it. Approximately 522,000 acres of fish and wildlife habitat were lost when the river was channelized and its banks stabilized between Sioux City, Iowa and St. Louis. Congress has authorized 166,000 acres of this habitat to be restored. A key tool is the mitigation authorities, which has acquired about 66,000 acres from willing sellers to date. Loss of habitat and management of water also contributed to the decline of the endangered pallid sturgeon, least tern and piping plover. The mitigation authority is being utilized with recovery efforts to improve habitat conditions and help prevent additional species from being listed.\nThe U.S. Army Corps of Engineers has most of the sales to acquire the land needed to develop fish and wildlife habitat from Sioux City, Iowa, to St. Louis, Mo.\nTo view the many sites please go to: moriverrecovery.usace.army.mil Click on the MRRP tab Go to the right hand side bar Scroll down to Finding Public Lands and Regulations Within this paragraph, click on Here and go to the Select MRRP Site for a full list of all areas between Sioux City and St. Louis. Basic information is provided by site.\nToday, the Missouri River hosts a wide variety of interests and uses, all of which are considered in the Missouri River Recovery Program (MRRP). They include social, economic, historical and cultural uses such as agriculture, commerce, conservation, energy, environmental, natural resources, navigation, recreation, residential, urban uses and water supply. The interactive Missouri River Outdoor Recreation Access Guide shows public access sites for fishing, hunting, boating, camping and more on the Missouri River. It also serves as a navigation guide.\nPlatte River is another important river, bringing South Platte water from Colorado and North Platte water from Wyoming into Nebraska before proceeding to flow across the state and empty into the Missouri River just below Omaha.\nWhile heavily developed, the central reach in Nebraska is 417,000 acre-feet of water short on an annual basis. The states of Colorado, Wyoming, and Nebraska and federal, state, and local entities are working on sustainability of the river through the Platte River Recovery Implementation Program. The Central Platte Natural Resource District and commission have obtained instream flows for fish and wildlife purposes for the reach from Lexington to the confluence with the Missouri River. Local NRDs are also developing integrated management plans and basin plans to more effectively manage surface and groundwater sustainably. The commission obtained instream flow appropriations for fish and wildlife purposes in 1993. A provision in state water law meant that the original purposes for which the instream flow appropriations had been granted had to be reviewed in 2013. The following links demonstrate the background of the natural resources and the information submitted to the Department of Natural Resources to justify keeping the appropriations for public trust fish and wildlife species. Because all cities along the Platte in Nebraska have their well fields in the hydrological connected alluvium, it is imperative that Platte River flows be protected not only for fish and wildlife purposes, but also for all citizens living along its course.\nThe five year review of Nebraska Game and Parks Commission Instream Flow Appropriations on the Central and Lower Platte River are listed below.\n- Instream Flows Q&A – Questions and Answers on Instream Flows\n- Correspondence – Game and Parks Letter to Nebraska Department of Natural Resources\n- Enclosure 1 – Beneficial Uses and Public Interest\n- Enclosure 2 – Platte River Catfish Population Dynamics and Tagging Study 2010\n- Enclosure 3 – Platte River Catfish Population Dynamics and Tagging Study 2011\n- Enclosure 4 – Confirmed Nebraska Whooping Crane Records 1942- 2010 and critical habitat\n- Enclosure 5 – Legal and Public Notices Pursuant to 15 year review\n- 15 year review – Order granting continued use of Platte River instream appropriations\nCool Water Streams\nCool water streams account for less than 28 percent of all stream miles in Nebraska, but they are still a tremendous resource. Good land stewardship has preserved the relatively pristine condition of some stream reaches, which provide habitat for a variety of aquatic organisms, including native and at-risk species, and angling opportunities for trout. However, these habitats remain extremely vulnerable to climatic changes, and are continually threatened by human disturbance as land and water use patterns change over time.\nThe Cool Water Stream Management Plan identifies goals for stewardship of cool water stream resources in Nebraska, and includes specific, attainable and measurable action items which will be implemented by Nebraska Game and Parks Commission (NGPC) staff and partners. The vision for cool water streams in Nebraska is that they will support productive and sustainable populations of cool water aquatic life, have healthy riparian zones and clean water, and contribute to watershed stability."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:21760182-9d55-47cb-a28f-3aa5f8eda21a>","<urn:uuid:a1cb3265-7535-4dad-89b0-9810fd08259b>"],"error":null}
{"question":"I'm doing research on feature engineering - what are the main techniques for creating new features, and how are they being studied in academic institutions?","answer":"Feature engineering encompasses several key techniques for creating new features: feature transformation (constructing new features through mathematical mappings), feature generation (creating new features not from transformation, like generating image features from pixels), and automatic feature engineering (using generic approaches to automatically generate and select features). These techniques are being actively studied in academic institutions like Arizona State University, where they are incorporated into computer science research with focuses on machine learning, sparse learning, and data mining. The academic work includes studies on structured regularization, multi-task learning, and distributed computing approaches to feature engineering.","context":["A Quick Guide to Feature Engineering\nFeature engineering plays a key role in machine learning, data mining, and data analytics. This article provides a general definition for feature engineering, together with an overview of the major issues, approaches, and challenges of the field.\nBy Guozhu Dong, Wright State University\nFeature engineering plays a key role in big data analytics. Machine learning and data mining algorithms cannot work without data. Little can be achieved if there are few features to represent the underlying data objects, and the quality of results of those algorithms largely depends on the quality of the available features. Data often exist in various forms such as image, text, graph, sequence, and time-series. A common way to represent data objects for data analytics is to use feature vectors. Data in raw forms such as those discussed above are not descried by informative features. Even data represented by feature vectors may still be in need of new effective features. Feature engineering is concerned with meeting the needs in generating and selecting an effective feature-vector based representation of data.\nFigure 1: Feature Engineering Makes Data Analytics Possible and More Powerful\nWhat is Feature Engineering: The feature engineering field contains a variety of issues and tasks. The most representative issues and tasks are feature transformation, feature generation and extraction, feature selection, automatic feature engineering, and feature analysis and evaluation.\n- Feature transformation is about constructing new features from existing features; this is often achieved using mathematical mappings. For example, the BMI index is a feature obtained through feature transformation using a mathematic formula.\n- Feature generation is about generating new features that are often not the result of feature transformation. For example, one generates new usable features for images from the pixels of the images (as the pixels are not usable features). Many domain specific ways for defining features also belong in the feature generation category. Feature generation methods can be generic automatic ones, in addition to domain specific ones. Patterns mined from given data can also be used to generate new features . Sometimes the terms ``feature extraction” and “feature construction” are used for feature generation.\n- Feature selection is about selecting a small set of features from a very large pool of features. The reduced feature set size makes it computationally feasible to use certain machine learning and data analytic algorithms. Feature selection may also lead to improved quality on the result of those algorithms. Feature selection has traditionally been focused on the classification problem , but it is also needed for other data analytic problems.\n- Automatic feature engineering is about generic approaches for automatically generating a large number of features and selecting an effective subset of the generated features in the process.\n- Feature analysis and evaluation is about evaluating the usefulness of features and feature sets. This is sometimes included as part of feature selection.\nIt should be emphasized that feature engineering is not just feature selection, and it is more than feature transformation.\nA better sense on feature engineering can be gained by seeing what have been done on perhaps two of the most frequently used types of data, namely text data and image data. For text data, Reference  discussed the following topics: text as strings, the sequence of words representation, the bag of words representation, term weighting, beyond single words, structural representation of text, semantic structure features, latent semantic representation, explicit semantic representation, word embeddings for text representation, and context-sensitive text representation. For image data, Reference  discussed the following topics: classical visual feature representations (including color features, texture features, shape features), latent-feature extraction (including principal component analysis, kernel principal component analysis, multidimensional scaling, isomap, Laplacian eigenmaps), and deep image features (including convolutional neural networks).\nAutomatic feature construction is a popular approach to feature generation, with Word2vec  and DeepLearning based methods  being representative methods. We describe each briefly below.\n(a) Word2vec is a group of methods that produce numerical feature vectors to represent words. They use shallow, two-layer neural networks to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector representation, typically of several hundred dimensions, for each unique word in the corpus. When words share common contexts, their word vectors are similar to each other. Figure 2 (from ) describes two architectures of Word2vec. The CBOW architecture trains a model to predict the current word based on the context described by Bag of Words, and the Skip-gram architecture trains a model to predict surrounding words given the current word. Word2vec has been generalized to image data, graph data and so on.\n(b) DeepLearning methods convert high-dimensional data into a low-dimensional representation, and it does so by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors . The small central layer is used as low-dimensional representation. Many types of neural networks have been studied, including the autoencoder networks . Figure 3 gives a high-level view of the autoencoder architecture .\nNew Challenges for Feature Engineering: Many effective feature generation methods, and many automatic feature engineering methods, remain to be discovered. Even traditional research topics such as feature selection and feature analysis need to be re-visited from new perspectives. Indeed, there are millions of ways to generate features, and hence feature selection and feature evaluation methods need to handle this high dimensionality. More importantly, traditional feature selection and evaluation methods are mostly limited to classification and regression problems. Clearly there is a need to develop new feature selection and evaluation methods for other data analytic tasks such as clustering, outlier detection, pattern mining, feature ranking, recommendation, and so on. Finally, much remains to be done to transform feature engineering from an art to a mature engineering discipline.\nFigure 2: Two Architectures for Word2vec (from )\nFigure 3: The Autoencoder Architecture\nA Recent Book on Feature Engineering: In 2018 a new edited book on feature engineering  was published, with 12 contributed chapters from leading experts on different aspects of feature engineering.\nAs feature engineering is often data type specific and application dependent, this book contains chapters devoted to feature engineering for major data types such as text data, image data, sequence data, time series data, graph data, streaming data, software engineering data, Twitter data, and social media data. These chapters present methods for generating tried-and-tested, hand-crafted, domain-specific features, as well as automatic generic feature generation methods such as Word2Vec.\nThe book also contains chapters on feature selection, automatic approaches for feature transformation based feature engineering, automatic feature generation using deep learning approaches, and feature generation using frequent and contrast patterns. Several chapters are about using feature engineering in specific applications.\nThe book contains many useful feature engineering concepts and techniques, which are useful for multiple scenarios: (a) generate features to represent the data when there are no features, (b) generate effective features when (one may be concerned that) existing features are not good/competitive enough, (c) select features when there are too many features, (d) generate and select effective features for specific types of applications, and (e) understand the challenges associated with, and the needed approaches to handle, various data types.\nIn 2018 Guozhu Dong used the book to teach a well-received graduate level Feature Engineering Course, which was perhaps the first of its kind in the world (based on a search of the web). The course covered fundamental aspects of feature engineering, including feature generation for major dataset types, feature extraction, feature transformation, feature selection for different data analytic tasks, feature analysis and evaluation, and applications.\n Guozhu Dong and Huan Liu. Feature Engineering for Machine Learning and Data Analytics. CRC Press, 2018.\n Yunzhe Jia, James Bailey, Ramamohanarao Kotagiri, and Christopher Leckie. Pattern based Feature Generation. Chapter in Feature Engineering for Machine Learning and Data Analytics (edited by Dong and Liu). CRC Press, 2018.\n Yun Li and Tao Li. Feature Selection and Evaluation. Chapter in Feature Engineering for Machine Learning and Data Analytics (edited by Dong and Liu). CRC Press, 2018.\n Chase Geigle, Qiaozhu Mei, and ChengXiang Zhai. Feature Engineering for Text Data. Chapter in Feature Engineering for Machine Learning and Data Analytics (edited by Dong and Liu). CRC Press, 2018.\n Parag S. Chandakkar, Ragav Venkatesan, and Baoxin Li. Feature Extraction and Learning for Visual Data. Chapter in Feature Engineering for Machine Learning and Data Analytics (edited by Dong and Liu). CRC Press, 2018.\n Suhang Wang and Huan Liu. Deep Learning for Feature Representation. Chapter in Feature Engineering for Machine Learning and Data Analytics (edited by Dong and Liu). CRC Press, 2018.\n Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. arXiv. 2013.\n Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science 313.5786: 504-507. 2006.\nBio: Guozhu Dong is a professor at the Department of Computer Science and Engineering and the Knoesis Center at Wright State University.\n- On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning education\n- Software for Analytics, Data Science, Data Mining, and Machine Learning\n- Good Feature Building Techniques and Tricks for Kaggle\n- Feature engineering, Explained\n- Implementing Automated Machine Learning Systems with Open Source Tools","ASU Electronic Theses and Dissertations\nThis collection includes most of the ASU Theses and Dissertations from 2011 to present. ASU Theses and Dissertations are available in downloadable PDF format; however, a small percentage of items are under embargo. Information about the dissertations/theses includes degree information, committee members, an abstract, supporting data or media.\nIn addition to the electronic theses found in the ASU Digital Repository, ASU Theses and Dissertations can be found in the ASU Library Catalog.\nDissertations and Theses granted by Arizona State University are archived and made available through a joint effort of the ASU Graduate College and the ASU Libraries. For more information or questions about this collection contact or visit the Digital Repository ETD Library Guide or contact the ASU Graduate College at firstname.lastname@example.org.\n- 4 English\n- 4 Public\n- Machine Learning\n- 2 Computer Science\n- 2 Computer science\n- 2 Sparse Learning\n- 1 Data Mining\n- 1 Dictionary Learning\n- 1 Distributed Computing\n- 1 Imaging Genetics\n- 1 Lasso\n- 1 Mathematics\n- 1 Medical Imaging\n- 1 Multi-Task Learning\n- 1 Optimization\n- 1 Parallel Computing\n- 1 Sparse Models\n- 1 Statistics\n- 1 Structured Regularization\n- 1 Structured Sparse Methods\nMulti-task learning (MTL) aims to improve the generalization performance (of the resulting classifiers) by learning multiple related tasks simultaneously. Specifically, MTL exploits the intrinsic task relatedness, based on which the informative domain knowledge from each task can be shared across multiple tasks and thus facilitate the individual task learning. It is particularly desirable to share the domain knowledge (among the tasks) when there are a number of related tasks but only limited training data is available for each task. Modeling the relationship of multiple tasks is critical to the generalization performance of the MTL algorithms. In this dissertation, I propose …\n- Chen, Jianhui, Ye, Jieping, Kumar, Sudhir, et al.\n- Created Date\nLarge-scale $\\ell_1$-regularized loss minimization problems arise in high-dimensional applications such as compressed sensing and high-dimensional supervised learning, including classification and regression problems. In many applications, it remains challenging to apply the sparse learning model to large-scale problems that have massive data samples with high-dimensional features. One popular and promising strategy is to scaling up the optimization problem in parallel. Parallel solvers run multiple cores on a shared memory system or a distributed environment to speed up the computation, while the practical usage is limited by the huge dimension in the feature space and synchronization problems. In this dissertation, I carry …\n- Li, Qingyang, Ye, Jieping, Xue, Guoliang, et al.\n- Created Date\nSparse learning is a technique in machine learning for feature selection and dimensionality reduction, to find a sparse set of the most relevant features. In any machine learning problem, there is a considerable amount of irrelevant information, and separating relevant information from the irrelevant information has been a topic of focus. In supervised learning like regression, the data consists of many features and only a subset of the features may be responsible for the result. Also, the features might require special structural requirements, which introduces additional complexity for feature selection. The sparse learning package, provides a set of algorithms for …\n- Thulasiram, Ramesh L., Ye, Jieping, Xue, Guoliang, et al.\n- Created Date\nImaging genetics is an emerging and promising technique that investigates how genetic variations affect brain development, structure, and function. By exploiting disorder-related neuroimaging phenotypes, this class of studies provides a novel direction to reveal and understand the complex genetic mechanisms. Oftentimes, imaging genetics studies are challenging due to the relatively small number of subjects but extremely high-dimensionality of both imaging data and genomic data. In this dissertation, I carry on my research on imaging genetics with particular focuses on two tasks---building predictive models between neuroimaging data and genomic data, and identifying disorder-related genetic risk factors through image-based biomarkers. To this …\n- Yang, Tao, Ye, Jieping, Xue, Guoliang, et al.\n- Created Date"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:48a26cb5-19bf-44d4-bc03-369856b7be23>","<urn:uuid:6bd156e5-1a53-4fde-a9e7-3da044e1bfb7>"],"error":null}
{"question":"As someone working in immigration policy, I'm curious: What key similarities exist between Japan's Immigration Services Agency and Canada's Immigration Services in terms of their recent reforms?","answer":"Both immigration agencies have undergone significant reforms in recent years. Japan promoted its Immigration Office to become the Immigration Services Agency in 2019, while Canada transformed its CIC (Citizenship and Immigration Canada) into IRCC (Immigration, Refugees and Citizenship Canada) in 2015. Both changes reflect expanded responsibilities - Japan's agency now manages increased foreign worker intake, while Canada's IRCC took on additional duties related to refugee protection and citizenship services.","context":["Hello, this is BabyJ😊.\nDue to declining number of marriages and childbirths, the Japanese society continues to be aging. Consequently, the number of working age Japanese population is declining.\nIt is estimated to decrease to be only about a half of the whole population in the year 2065.\nIn order to cope with the labor shortage, the Japanese Government has implemented a new immigration policy to accept a large number of foreign workers to Japan starting April 1, 2019.\nWhy Japan Needs a Foreign Work Force?\nWhy does Japan need foreign workers?\nThe Government has been encouraging women who have resigned from work due to childbirth and retired people to go back to work.\nHowever, even with such a policy, still 350,000 workers are estimated to be in shortage, especially in the 14 fields indicated below where mostly blue-collar workers are in need.\n<14 Fields with Labor Shortage> *Table by BabyJ\nOn the other hand, the number of foreign workers in Japan continues to rise. The number has reached 1,280,000 as of October 31, 2018.\nTherefore, the Japanese Government has decided to make up the 350,000-labor shortage with foreign workers.\nStatus of Foreign Workers in Japan\nAs you may know, in order for a foreigner to live and work in Japan, a specific resident status, or visa is required.\nUntil now, only those who have graduated from a university or college, with Bachelor’s or Master’s Degree, acquiring skills and knowledge in a specific field are allowed to work in Japan.\nAlso foreigners who are married to Japanese are entitled to work in any field without any restrictions under the resident status based on their status in Japan such as a \"Spouse or Child of Japanese Nationals,\" or a \"Permanent Resident,\" etc.\nThe two categories above make up almost a half of the foreign workers in Japan.\nHowever, a recent survey has revealed that those who are in Japan with Working Holidays, Internship or other special work program visas, Technical Intern Trainees, and students make up more than a majority of all the workers in Japan.\nThey have a resident status of, “Designated Activities,” or “Technical Intern Training,” or “Student,” respectively.\nSome students work part-time, with “Permission to Engage in Activity Other Than That Permitted under the Status of Residence Previously Granted.”\n<Resident Status of Foreign Workers in Japan> *Graph by BabyJ\nNewly Introduced Resident Status\nIn an effort to increase the foreign work force in Japan, the government has set up a new resident status, a “特定技能 (Specified Skilled Worker),” to provide a smooth path for such students and trainees to continue to work in Japan after completing their education and training.\nRegardless of your educational background, anyone above 18 years of age can acquire this status on the condition that he/she passes both Japanese language test, and skill test for the designated field.\nBoth Japanese language test and skill test are given several times a year in and outside of Japan (Only in Vietnam, Philippines, Cambodia, Myanmar, China, Indonesia, Thailand, Nepal, and Mongol for now.)\nThose who have completed the Technical Intern Training II (3 years) will be exempt from both tests.\nAt the moment, the fields for “Specified Skilled Worker” are limited to the 14 fields indicated above.\nFuture of Japan\nIn other words, with this new policy and the new resident status, the Japanese government has extended work opportunities to blue-collar workers, that used to be restricted only to white-collar workers who have completed a higher education.\nTo control the influx of foreign workers to Japan, the government has promoted the “入国管理局(Immigration Office)” to the “出入国在留管理庁 (Immigration Services Agency),” which is a higher position of government organs, and they have increased the number of the work force in the agency.\nJapan is a single-race nation in principle. It used to be a closed society separating foreigners as outsiders.\nIn order to provide foreign workers with the comfortable living environment, and also to ensure that the Japanese people live and work in harmony with foreigners, with various nationalities, races, and backgrounds, the government also plan to introduce various multilingual public signs, information, and Help Offices all around Japan.\nBut what should change is the Japanese way of looking at everything, I guess.\nThe purpose of this article is to provide information that may be of interest to foreigners. This is not intended to provide recruitment and placement.","The Canadian government has an agency that is responsible for the CIC processing time. The full meaning of this CIC is Citizenship and Immigration Canada. Most people are having issues when it comes to CIC processing time. Any individual who wants to obtain either a work visa or work permit must have to be aware of the processing time. When it has to do with Canada immigration, there is a body that handles that known as Immigration, Refugees, and Citizenship Canada (IRCC). The application has to follow up some certain procedures for either a work visa or permit.\nNumerous people are applying to migrate to Canada for different purposes such as education, investment, business, work, etc. Most of the immigrants don’t know the processing time for Citizenship and Immigration Canada. Therefore, they might be forced to do something illegal due to a lack of patience on their path. The new name for CIC is Immigration, Refugees, and Citizenship Canada (IRCC), but most people prefer to call it CIC.\nIRCC helps in ensuring that immigrants or refugees to Canada are well protected from any immigration officers. The name CIC was changed to IRCC as a result of the emergence of a new government in Canada in the year 2015. Later in the following year, 2016, it became more pronounced by Canadian citizens. This agency helps in giving out travel documents to new immigrants. Despite the new name, the name CIC is still what most immigrants are going to see on different WebPages instead of IRCC.\nCIC Processing Time Application Procedures\nWhen it has to do with the CIC processing time of immigration documents or applications, there are outlined steps that must be followed to achieve desirable results. There is an average time for this CIC processing time for anyone who is applying. There are things the applicant or candidate must put at the back of their mind that the CIC processing time should not include the time the applicant might have taken to indicate his or her interest, submit the application, preparation of documents, etc.\nThere are things which affect the processing time application procedures, and here they are:\n- Applicant country of residence\n- Kind of application\n1. Country of Residence:\nThis is one of the factors that influence CIC processing time for any candidate that is applying. An immigrant who is from England that is applying for CIC will be given rapt attention compared to an individual who is from Botswana. People are applying different things such as work permits, residence visas, and other applications relating to immigration. Currently, these applications are being transferred around different visa offices.\nThis has made application seems easy for anyone who is applying for any travel documents from any country. The applicant needs to be aware that their application doesn’t need to be processed by an office nearer to them. This application will be taken to the right office that’s responsible for attending to the application. This method has proven to work efficiently in time past for numerous immigrants.\n2. Kind of Application:\nThis is among the factors that can affect the CIC processing time for an immigrant application. Different kinds of work visas or permits have their various processing time. There is no way one would compare the processing time for obtaining a temporary residence permit to a permanent residence permit application.\nSome immigrants get themselves into unnecessary confusion after they have applied for a travel application in Canada. When there is a delay in response from the CIC or IRCC, it could be there are many persons who have applied for that same application. Therefore, they need to take proper attention to avoid unnecessary mistakes.\nSome documents should be included during an immigration application. A candidate who is applying should have an eye for details to know the following things needed during their application. On the part of some immigrants, they are lazy to ask the immigration officers the various documents that should be attached to their application.\nThe omission of one document can affect CIC processing time application procedures. For instance, an applicant could be lamenting their application is taken a long time to be processed. If the necessary documents are not attached to the application, it will act as an impediment for anyone who is filing an immigration application.\nOther Interesting Contents\n- All You Need to Know about Canada Visa Lottery\n- Top 20 Jobs for Immigrants in Canada without Work Permit\n- Apply for High-Paying Jobs in Canada as a Foreigner\nCIC Processes for Refugees\nCitizenship and Immigration Canada also have protection for refugees or asylum in Canada. The aim of this is to safeguard them from any form of immigration persecution. Some of these persecutions refugees do face are unusual treatment, torture, and other ill-related treatments. For anyone who wants to process CIC for refugees must be eligible before his or her application can be granted.\nSome refugees become afraid of persecution to go back to their country for one reason or the other such as nationality, religion, political opinion, race, etc. These are the reasons refugees need protection to stay safe in Canada. After the refugee has applied, it is left for the Immigration and Refugee Board of Canada to approve or reject the application. This depends on the number of refugees who are applying for this same thing.\nCIC Status Checker\nThis has been made to reduce stress on the part of the applicant by sending mails to Citizenship, and Immigration Canada. Applicants can know their CIC status by checking online right from the comfort of their homes. As soon as the processing of the application commences, it is the right time for the individual to check his or her status. This agency had made it possible for individuals to be aware the application process is going on by sending an email to the applicant. Another way an applicant can use to check his or her CIC status is by using processing time as a guide.\nHow to Avoid CIC Processing Time Delays?\nTo avoid CIC processing time delay, the following things should be done:\n- All the necessary information should be included in the application.\n- If there are any changes to the following such as name, contact address, phone number, etc. They should be indicated in the application.\n- The photocopies of the documents should be clear enough for the immigration officers to see and read them.\n- The relevant documents should be translated into French or English language which is the two official languages in Canada.\n- All the information on the documents should be precise and correct.\n- Payment of the right application fee should be done using the proper payment.\n- Crosschecking of the application for correction of mistakes.\nOther Interesting Contents\n- How to Migrate to Canada: 5 Ways to get Canadian Visa\n- Canada Spouse Visa Processing time for all Countries\n- 20 Fully Funded Scholarships in Canada for International Students\nHow often CIC Processing Time is updated?\nMost immigration procedures are updated every week, every CIC processing time shows the time frame it took for the application to be processed. Even a permanent residence application has its processing time for a new application. The processing time for this kind of application is updated monthly. Examples of these applications that are updated monthly are economic immigration, humanitarian immigration, sponsorship immigration, etc."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:b9d1813d-eacd-4748-acd9-d83d9670dc25>","<urn:uuid:fa79087f-42e3-46a6-bd81-2a9cc332203f>"],"error":null}
{"question":"Which traditional Christmas treat takes longer to prepare: Sicilian cuccidati cookies or German stollen bread?","answer":"German stollen bread takes longer to prepare overall, as it requires 2 days of maturation in the refrigerator after baking to develop the optimal flavor and rich crumbly texture before serving. In contrast, Sicilian cuccidati cookies can be eaten immediately after cooling and will keep for about 2 weeks in a cookie tin or covered with aluminum foil.","context":["Italian Christmas traditions are unique to each region of the Italy and have been lovingly handed down within families through the generations. Cuccidati – a version of Christmas cookie that probably originated after the Arabs introduced oranges and almonds to Sicily centuries ago – play an important part in the Christmas celebration in Sicily even today.\nAll Sicilian cuccidati, or any Italian cookie for that matter, are unlike what Americans think of when they think of cookies. Most Italian cookies are made from dough that cooks up drier than American cookies and there is much more variation in the presentation. Sicilian cookies come in a multitude of different shapes and sizes and fruit fillings are often enclosed in the cookies as a special treat.\nThe recipe given below is for a Sicilian Christmas cookie—my family calls them “cuccidati,” although they are not identical to most of the cookies found online under this name. The cookies in this recipe start out as the “typical” cuccidati: one long “tube” of sweet, Italian pie-crust-like dough, which contains a dried fruit and nut center. (No figs in our version, by the way.) But, instead of then cutting the tube into bite-sized pieces that are finished with icing, my family cuts larger pieces, which are then formed into different shapes, and finishes the cuccidati with a sprinkle of powdered sugar. Whatever the name, this is just one version out of many dried, fruit-filed cookies still made in Sicilian bakeries today to celebrate the Christmas season.\nWhen I was a child, my family always gathered the weekend before Christmas to share our creativity while we formed our cuccidati into wreaths, ribbons, or candy cane-like forms. They could be completely covered in dough, which would allow for a creative, fringe-like covering, or left open. The sides could be pinched for decoration if like, similar to how Americans form a pie crust along the rim of their pies. If you would like to see how the various shapes of these cookies are made, visit the Stella Lucente Italian Pinterest site.\nThe ingredients for the cuccidati filling are considered easy to come by today, but remember that dried fruit, including raisins and oranges and spices like cinnamon were considered special when the cookies originated. These filling ingredients were only found only in well-off households. Since the filling ingredients are difficult to chop and mix together, in some Sicilian towns “back in the day,” people would bring their filling to the butcher to mix together for them in his meat grinder, which had been newly cleaned for the season for this purpose.\nDespite the few ingredients in traditional cuccidati, and the difficulty of making the filling with them, the dried fruit has a rich sweetness, the roasted almonds a robust flavor, and the cinnamon, orange, and citron add a complexity of flavor that goes beyond its simple ingredients. Try our recipe this Christmas season for a taste of Sicilian tradition!\nIf you’re trying this recipe for the first time, it may be easier to cut the recipe in half.\nMakes 2–4 dozen cookies, depending on the size and shape of the cookies created.\nPasta Frolla (Sweet Pastry)\n2½ lbs. flour (about 10 cups)\n10 oz. of lard or Crisco\n½ cup sugar\n½ tsp salt\nabout 1 cup cold water\n2 lbs. yellow raisins (not red raisins)\n1 lb. whole almonds (skinless), roasted\n2 Tbsp citron (lemon)\n2 Tbsp candied orange peel\nor zest of 2 tangerines\n2 tsp cinnamon sugar\nPrepare the pasta frolla*\nSift the flour, salt, and sugar into a bowl.\nCut in the lard with a fork and/or your fingertips until it is the size of small peas.\nAdd the cold water a little at a time, while mixing with a fork. After about 1 cup of water has been added, gather the dough and test it to see if it holds together. If it does, form one large ball. If it is too dry, add more water, mix, and try again.\nTurn the dough out onto a floured board and press it together with a soft, gentle kneading motion with the palm of your hand until a dough forms.\nForm into one large disk, wrap in plastic wrap, and refrigerate for 30 minutes.\nMake the filling\nFirst, chop the yellow raisins coarsely with a sharp knife so they open up. This is fairly labor intensive, and it may take a bit of time to chop all of the raisins.\nThen, chop the almonds coarsely with a sharp knife.\nIf you have a small manual chopper/grinder specifically for nuts, put the coarsely chopped nuts into the machine and grind to obtain more finely chopped nuts, which can then be mixed with the raisins. Otherwise, try one of the next two steps below.\nEither: Take the tenderizing part of a meat mallet and mash small amounts of the raisins and almonds together at a time. This can be done under a dish towel so they do not scatter everywhere. (Mashing the raisins and almonds together seems to work the best and will leave varying sizes of raisins and almonds in the mix.)\nOr: An alternative to the last step: pulse the pre-processed raisins and nuts in a food processor a few times if you have one, making sure the ingredients are not over processed.\nTo the raisin/nut mixture: add the citron, orange peel or zest, and cinnamon and mix well.\nForm into 2 rectangular “logs” the shape of a loaf of bread. Cover with aluminum foil if not using right away and store at room temperature.\nAssemble the cuccidati\nSet up a kitchen table “assembly line” style: place the dough on one end on a surface that is good for rolling and cutting the dough, place the filling in the middle, and use the surrounding work areas for each member of the family to create the cookies. Place cookie sheets on the far end for the finished cookies.\nCut off one strip at a time from the large dough ball and roll it out into pie-crust-size thickness.\nCut the rolled-out dough into fairly thick strips, depending on the size of cookie desired. These strips can then be cut again crosswise to make the size needed to make smaller cookies.\nCut rectangular pieces of filling from the filling logs to place into the strips of dough.\nBe creative! Create cookies with the sides brought up to cover the filling entirely, or leave the filling uncovered and just pinch the dough together to form various designs. Traditional shapes are round (like a wreath), horseshoe, or long or short ribbons. Candy cane shapes are popular with kids.\nBake in preheated oven at 350° for about 20–30 minutes, or until the bottom of the cookie is nicely browned. The sides and top of the dough should be cooked but not browned. This will make a flaky crust and avoid burning the filling.\nSprinkle with confectioner’s sugar when cool.\nCookies will keep for about 2 weeks in a cookie tin or covered with aluminum foil.\n*The original recipe passed down from my grandmother states that the flour and the lard should be mixed together and left overnight before the sugar, salt, and water are added to create the dough. I’ve never tried this and instead use the traditional “pie crust” method.\n—Kathryn Occhipinti: Adapted from the cooking classes given by the Italian-American Society of Peoria\nKathryn Occhipinti, MD, is the author of the\nConversational Italian for Travelers series of books and a teacher of Italian for travelers to Italy in the Peoria and Chicago area.\n“Everything you need to know to enjoy your visit to Italy!”\nJoin my Conversational Italian! Facebook group and follow me on Twitter at StellaLucente@travelitalian1 and start to learn Italian today for FREE!\nConversational Italian! Facebook Group\nTweet @travelitalian1 for Stella Lucente Italian\nVisit learntravelitalian.com/download.html to purchase/download Conversational Italian for Travelers and find more interesting facts and helpful hints about getting around Italy! Learn how to buy train tickets online, how to make international and local telephone calls, and how to decipher Italian coffee names and restaurant menus, all while gaining the basic understanding of Italian that you will need to know to communicate easily and effectively while in Italy. —From the staff at Stella Lucente, LLC\nCuccidati: Traditional Sicilian Christmas Cookies","Based on an authentic North German recipe raised with baking powder rather than the usual yeast, this sumptuous Christmas “bread” is studded with dried fruits and conceals a layer of marzipan. Allow two days for the baked stollen to mature and develop the optimal flavor and rich crumbly texture before serving.\n- ½ cup (4 ounces) cottage cheese, preferably organic (the whey will drain more easily from the curds; regular brands can contain a lot of gum)\n- ½ cup (2 ounces) raisins\n- ¼ cup (1 ounce) dried Zante currants\n- Finely grated zest of 1 small lemon\n- ⅛ teaspoon almond extract\n- 1 tablespoon dark rum\n- 1 large egg\n- 1½ ounces gluten-free marzipan\n- 4 tablespoons (2 ounces) unsalted butter, cold, cut into small cubes\n- FLOUR MIX\n- ½ cup (2½ ounces) white rice flour, plus extra for forming loaf\n- ¼ cup (¾ ounce) potato starch\n- ½ cup (2 ounces) tapioca starch\n- ½ cup (1½ ounces) almond meal\n- ½ teaspoon guar gum or xanthan gum\n- 1 teaspoon baking powder\n- ⅛ teaspoon ground cloves\n- 1 teaspoon ground cardamom\n- ¼ cup (1¾ ounces) sugar\n- Pinch fine sea salt\n- 1½ tablespoons (¾ ounce) unsalted butter, melted\n- Confectioners’ sugar\nHeat the oven to 400° F. Line a baking sheet with baking parchment.\nSpread the cottage cheese out in a fine sieve set over a bowl. Allow to drain for 15 minutes or so and discard the excess liquid.\nCombine the raisins, currants, lemon zest, almond extract and rum and let stand for 15 minutes. Beat the egg lightly, blend in the drained cottage cheese and reserve. Roll the marzipan between 2 sheets of plastic wrap into an 8 x 2½-inch strip and set aside.\nIn a large bowl, combine the flour mix ingredients and stir with a whisk. Make a well in the center and add the egg mixture. Using a fork, stir to form a stiff paste. Mix in the butter cubes using a pastry blender or your fingertips. Add the reserved fruit, and mix with your hands to incorporate into the dough.\nDust a work surface lightly with rice flour. Gather and press the dough together in the bowl, turn out, and knead lightly. Form into a ball, and roll into an oval approximately 10 x 7 inches. To form the traditional shape, sandwich the strip of marzipan inside the dough, but make the upper half of the “sandwich” slightly less wide than the lower half, forming a shelf along the outer edge. Be sure to seal the marzipan inside the dough.\nTransfer the loaf to the baking sheet. Bake for 5 minutes, then reduce the heat to 350° F and bake for about 30 minutes more, until risen and light golden brown.\nRemove from the oven and immediately brush the loaf with the melted butter. Sift a generous layer of confectioners’ sugar on top. Let cool on the baking sheet for 10 minutes, and then transfer to a rack to cool completely.\nEnclose the loaf in a plastic bag, leaving airspace on top to avoid disturbing the topping, and store for 2 days in the refrigerator to mature. Serve at room temperature, quite thinly sliced.\nNutrition Analysis: 240 cal, 11g fat, 40 mg chol, 105 mg sodium, 31 g carbs, 2 g fiber, 5g protein.\nRecipes © copyright 2013 by Jacqueline Mallorca.\nThe author of more than a dozen cookbooks, Jackie Mallorca’s latest titles include The Wheat-Free Cook: Gluten-Free Recipes for Everyone and Gluten-Free Italian."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:c485e1ac-f385-4d4b-b979-e787b0889e92>","<urn:uuid:1c60965b-a3be-4b22-9865-1ac0006d4015>"],"error":null}
{"question":"What are the essential text-related accessibility requirements that websites must implement according to standards?","answer":"Websites must implement several essential text-related accessibility requirements: They must provide alternative text for images and non-text content, ensure text can be resized up to 200% without losing functionality, use clear titles and headers for organization, maintain proper color contrast ratios, and employ simple language avoiding industry jargon. For forms, clear field labels are necessary. Text must be properly organized with header titles, section breaks, and adequate white space. Additionally, text color must be checked for color blindness compatibility, particularly for elements like hyperlinks. These requirements span across both minimal (Level A) and acceptable (Level AA) WCAG compliance levels.","context":["For those of you with a brick-and-mortar business location, you’re familiar with the concept of accessibility and why it’s a necessity for any company that intends to engage with the general public. In the United States, not accommodating those with disabilities is a violation of the Americans with Disabilities act; reasonable accommodations must be provided to everyone who visits your establishment.\nBut accessibility isn’t something that should be relegated to storefronts that receive a steady amount of foot traffic each day.\nAccessibility is an especially pertinent matter for companies that do business online.\nWhen you think about it, people with accessibility issues would probably be more interested in doing business online as any physical, visual, auditory, speech, or cognitive barriers they have wouldn’t be as much of a problem virtually. Or so you would think.\nWhat Accessibility Really Means\nAt its most basic core, accessibility is about creating an environment that is friendly to 100 percent of its visitors, users, customers, and so on.\nOn a website, this means that all content can be read, all visuals can be seen, and all media can be viewed.\nCan you safely say that’s the case with your website?\nLet me put it to you another way:\nPhysical impairments can stem from a number of issues and can strike anyone at any age. Regardless of whether the number of disabled users visiting your site now hits the country’s average, sooner or later you’re going to need to prepare your site with the appropriate accommodations for all users.\nMeeting Accessibility Standards on Your Website\nIdeally, the web should open up more possibilities for individuals with disabilities. But when business owners, webmasters, or developers don’t take that into account when building a website, there’s potential for just as many (if not more) barriers for entry than they’d experience out in the physical world.\nIf your business’s site isn’t optimized to meet basic accessibility standards, now is the time.\nHere are some reasons why:\n- It will increase your audience reach as it becomes universally accessible.\n- You’ll cut down on the chances (albeit rare, but still possible) of a lawsuit due to violating the Convention on the Rights of Persons with Disabilities or any other law established to protect disabled individuals.\n- It’ll demonstrate your business’s dedication to social responsibility as you create a website that is truly about providing an equal opportunity to all users.\n- Accessibility optimization often leads to cleaner interfaces, simpler navigation, and other elements that work well in improving the overall user experience. Hopefully, in turn, that’ll lead to an increase in brand reputation, and consequently, customer loyalty.\n- Improved SEO often goes hand-in-hand with web accessibility. With an improved experience for all users, your site will have lower bounce rates, higher numbers of conversions, less negative feedback, and other positive responses – all of which matter to search engines when ranking a site.\nWhether you’re setting out to make your site accessible because of recent complaints from visitors or simply because you want to do your due diligence as a responsible business owner, the overall goal is to create a positive user experience for all. And that’s wonderful.\nLuckily enough, the accommodations you need to make in order to improve accessibility are fairly straightforward; many of which you may have already done in order to meet mobile standards for the web.\nHere are 12 ways to optimize your site for accessibility and ensure you’ve covered all your bases.\n1. Responsive Design\nMost developers and designers know by now that websites need to be compatible with desktop and mobile viewing.\nEven if you don’t have a professional designer to create your site, most website themes or designs are built “responsive,” meaning they automatically resize to be accessible and readable from a mobile device anyway.\nEven so, it’s important to remind everyone that websites must be responsive in design for improved accessibility.\n2. Interaction Methods\nSome of your visitors are unable to use a mouse, keyboard, or their fingers (on mobile) when interacting with your website.\nIf you want to cut down on the inconvenience of users having to rely on solely physical interactions with your site, include text-to-speech options.\n3. Text Resizing\nFor some visitors, their disability won’t be so severe that they can’t see your website at all; instead, it might be something like the inability to see text if presented at too small a size.\nSince oversized text has a tendency to look sloppy in general in web design, your best bet here is to add a text resizing tool so that visitors can select how large they want the text on their screen to display.\nAlternatively, make sure your website works at different browser zoom levels.\n4. Alternative Text\nFor users having a hard time seeing images on your website, the alt text and caption fields are really important to use.\nThis way, they won’t need to see an image in order to understand what sort of visual context you’ve paired up with your content.\nJust enter a lengthy description about each image in either (or both) the alt text and caption fields.\nVideos, podcasts, and any other streaming or interactive media content (this includes media such as infographics) may also present obstacles for visitors.\nFor anyone who is hard-of-hearing, blind, or experiencing some other difficulty with viewing media, providing captions and transcripts are a must.\n6. Simplified Messaging\nIn general, it’s not a good idea to stuff your website with too much content. Attention spans are shrinking, so the last thing you want to do is force readers to scroll through 1,000-plus words of text on each page.\nIt’s also typically not a good idea to use industry-specific jargon or colloquialisms when writing for the web.\nThe simpler the language, the better. (The difference would be long-form articles that offer value such as posts here, of course!)\n7. Text Organization\nAs part of the website (and design) simplification process, remember to use large and clear titles on top of every page. This will help in the readability of the page as well as navigation.\nWithin the body of text on each page, use obvious header titles, section breaks, and abundant white space so that visitors have a clear visual field to read from.\n8. Text Color\nWhile you might not think that using red hyperlink text is a bad idea — especially since you don’t see the competition doing it — it might be troublesome for visitors who are color blind.\nColor, in general, can be a tricky thing to work with because of the psychological and emotional implications behind it, but people’s actual ability to see it can be problematic, too.\nFind yourself a color blindness website checking tool to ensure that all colors you use— from your logo to your images, as well as your plain text to the background contrast — will work for all users.\n9. Directional Cues\nFor users with cognitive impairments or who just aren’t technologically savvy, it’s imperative to have directional cues available to help them along.\nTooltips are one of those such tools that can come to the rescue if a user gets stuck.\nFor other users, it might be the little touches added to a design (like arrows in a slider or a call to action that actually looks like a clickable button) that help designate where the interactive elements of your site are.\n10. Clear Navigation\nA consistent and clear navigation is helpful for everyone who visits your site — including you.\nThe last thing you want to do is bury hard-to-follow but extremely important information in your site.\nMake sure the navigation is easy to find and always accessible through the top of your site, and make sure each label is clearly written in simple terms everyone will understand.\n11. Internal Search\nFor some users, a navigation menu won’t be of much help.\nThat’s why providing an internal search bar that’s ever present at the top of the site (or on relevant pages, like for e-commerce products) makes sense for many sites.\nIdeally, the search bar should come with voice search capabilities as well.\n12. Form Directions\nContact forms are an essential part of every business’s website, whether they’re for newsletter signups, purchases, support requests, or some other purpose entirely.\nIf you want to open the lines of communication to visitors, with phone calls losing their viability with each passing day, a form is necessary. But for visitors who have comprehension issues, field labels can improve their ability to fill in the information they’re being asked for without issue.\nAccording to the UN’s Convention on the Rights of Persons with Disabilities:\n“’Reasonable accommodation’ means necessary and appropriate modification and adjustments not imposing a disproportionate or undue burden, where needed in a particular case, to ensure to persons with disabilities the enjoyment or exercise on an equal basis with others of all human rights and fundamental freedoms.”\nFurthermore, the UN explains that the way to meet this reasonable accommodation is through universal design.\nThe above list of 12 ways to do that might typically be considered as overkill in web development simply because of the extra work that needs to be done to implement them. However, each adjustment aims to create a more pleasant experience for all users at the end of the day, which, arguably, makes it an investment of time and energy worth making.\nScreenshots taken by author. May 2017.\nGo to Source\nAuthor: Tamar Weinberg\nThe post Give Your SEO a Boost With These 12 Accessibility Improvements by @tamar appeared first on On Page SEO Checker.","The Web Content Accessibility Guidelines (WCAG)\nWhat are the Web Content Accessibility Guidelines (WCAG)?\nThe guidelines offer technical recommendations on how to make website content accessible. The guidelines are also the standard reference for most website accessibility-related legislation like the Americans with Disability Act (ADA) in the US, and the European Web Accessibility Directive.\nWCAG Versions - 2.0, 2.1, 2.2 and 3.0\n- WCAG 2.0 - published 11 December 2008.\n- WCAG 2.1 - published on 5 June 2018 and is now the W3C recommended version\n- WCAG 2.2 - not yet in effect, scheduled to be published in 2021\n- WCAG 2.0 had 61 success criteria\n- WCAG 2.1 introduced 17 more success criteria to address mobile accessibility, people with low vision, and people with cognitive and learning disabilities.\n- WCAG 2.2 will be expanding on 2.1 with nine new success criteria, plus an update to one, with the goal of making content more accessible to a wider range of users.\nThe current standing WCAG versions 2.0 and 2.1 are categorized according to four principles, perceivable, operable, understandable, and robust (POUR).\nElements that convey information or components of a website’s user interface must be presented in a way that users are able to find, process, and understand.\nAll functionality and navigation on the website should be usable.\nInformation and the operation of the user interface must be clear and understandable to users of all abilities.\nThe website should be capable of adapting and developing itself to support a variety of current and potential future user agents, including assistive technologies.\nUnder each principle are testable success criteria that provide recommendations on how to make digital content more accessible. The success criteria are classified by three levels A, AA, and AAA, with A being the most basic level of WCAG compliance and AAA being the hardest.\nLearn more about the elements of WCAG and how to comply with its success criteria.\nOn the 21st of January, 2021, the WAI released the first working draft of the WCAG 3.0. WCAG 3.0 is planned to be a major revision with the intention to make the guidelines more user-friendly than the WCAG 2 iterations, and more flexible, covering even more content, apps, and tools, as well as organizations and disabilities. WCAG 3.0 is still in development and is not expected to be finalized for the next few years. Learn more about WCAG 3.0.\nThe different WCAG compliance levels\nThe WCAG categorizes its conformance based on three levels: A, AA, and AAA. To conform to the guidelines, it is a requirement that one of these levels should be fully met.\nMinimal WCAG compliance (level A)\nExamples of Level A success criteria:\n- All non-text content like images or videos must have a text alternative, like alt text or captions, that serves the equivalent purpose.\n- Users can navigate the website effectively using only a keyboard\n- Avoid using color as the only visual means of conveying information or prompting an action, like having green buttons with no text on them to suggest that it is meant to be selected as a ‘yes’ response.\n- If there is audio that auto-plays on your website for more than 3 seconds, ensure that that you provide means of adjusting the volume, stopping, or pausing it.\nAcceptable WCAG compliance (level AA)\nExamples of Level AA success criteria:\n- Ensuring that text on a webpage can be resized without assistive technology up to 200 % without loss of content or functionality.\n- Provide descriptive headings and labels in content\n- Navigational elements on the site, like menus, should be in a consistent, repeated position across the website.\n- When executing an action on the site, like filling in forms or clicking on buttons, errors can occur on the user’s part. If an error should occur, suggestions for correction should be provided.\nOptimal WCAG compliance (level AAA)\nExamples of Level AAA success criteria:\n- The visual presentation of text and images of text must have a contrast ratio of at least 7:1.\n- Removing timing limitations from all content, unless it is for non-interactive synchronized media and real-time events.\n- When a user has to submit information on a webpage, the submissions must be reversible, checked for input errors (and offer suggestions for correction if errors do occur), and there is a confirmation mechanism in place to allow the user to review the submission and edit if needed.\n- Images of text should be avoided or only used for decoration.\nWho should comply with the WCAG?\n- Web content developers (page authors, site designers, etc.)\n- Web authoring tool developers\n- Web accessibility evaluation tool developers\nHow to check your website’s WCAG compliance level\nHow Monsido can help your website meet WCAG standards\nEach audit scans your site for machine-testable issues, provides detailed reports so you can review any errors that may arise, gives you targeted recommendations on how to address these errors based on the guidelines, and shows you your compliance based on levels A, AA, and AAA. You can track and prove your accessibility compliance progress via reports in the History Center. We also offer accessibility training to customers and support, all-inclusive, to ensure that you are well-versed in both automated and manual remediation methods, and can efficiently and consistently improve your website’s accessibility.\nMonsido also offers free tools to complement your web accessibility efforts, including a color contrast checker for web teams to test out compliant color combinations for their web design, and an accessibility statement generator, which helps you generate a public statement declaring your commitment to web accessibility and helps make your web accessibility policy transparent to all your users."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:727402dd-9cb5-4ff3-b455-d3e6c29ade6b>","<urn:uuid:a2ef5797-959c-4ba6-9b52-ee60633ac0ca>"],"error":null}
{"question":"How many distinct areas were identified in each hemisphere of the cerebral cortex in recent brain mapping research?","answer":"Researchers were able to subdivide the cerebral cortex into 180 specific areas in each hemisphere, with almost 100 of those areas having never been described before.","context":["Research shows that the roots of autism spectrum disorder (ASD) generally start early—most likely in the womb. That’s one more reason, on top of a large number of epidemiological studies, why current claims about the role of vaccines in causing autism can’t be right. But how early is ASD detectable? It’s a critical question, since early intervention has been shown to help limit the effects of autism. The problem is there’s currently no reliable way to detect ASD until around 18–24 months, when the social deficits and repetitive behaviors associated with the condition begin to appear.\nSeveral months ago, an NIH-funded team offered promising evidence that it may be possible to detect ASD in high-risk 1-year-olds by shifting attention from how kids act to how their brains have grown . Now, new evidence from that same team suggests that neurological signs of ASD might be detectable even earlier.\nThe human brain contains distinct geographic regions that communicate throughout the day to process information, such as remembering a neighbor’s name or deciding which road to take to work. Key to such processing is a vast network of densely bundled nerve fibers called tracts. It’s estimated that there are thousands of these tracts, and, because the human brain is so tightly packed with cells, they often travel winding, contorted paths to form their critical connections. That situation has previously been difficult for researchers to image three-dimensional tracts in the brain of a living person.\nThat’s now changing with a new approach called tractography, which is shown with the 3D data visualization technique featured in this video. Here, researchers zoom in and visualize some of the neural connections detected with tractography that originate or terminate near the hippocampus, which is a region of the brain essential to learning and memory. If you’re wondering about what the various colors represent, they indicate a tract’s orientation within the brain: side to side is red, front to back is green, and top to bottom is blue.\nCaption: Map of 180 areas in the left and right hemispheres of the cerebral cortex. Credit: Matthew F. Glasser, David C. Van Essen, Washington University Medical School, Saint Louis, Missouri\nNeuroscientists have been working for a long time to figure out how the human brain works, and that has led many through the years to attempt to map its various regions and create a detailed atlas of their complex geography and functions. While great progress has been made in recent years, existing brain maps have remained relatively blurry and incomplete, reflecting only limited aspects of brain structure or function and typically in just a few people.\nIn a study reported recently in the journal Nature, an NIH-funded team of researchers has begun to bring this map of the human brain into much sharper focus . By combining multiple types of cutting-edge brain imaging data from more than 200 healthy young men and women, the researchers were able to subdivide the cerebral cortex, the brain’s outer layer, into 180 specific areas in each hemisphere. Remarkably, almost 100 of those areas had never before been described. This new high-resolution brain map will advance fundamental understanding of the human brain and will help to bring greater precision to the diagnosis and treatment of many brain disorders.\nThis past weekend, I attended a scientific meeting in New York. As often seems to happen to me in a hotel, I tossed and turned and woke up feeling not very rested. The second night I did a bit better. Why is this? Using advanced neuroimaging techniques to study volunteers in a sleep lab, NIH-funded researchers have come up with a biological explanation for this phenomenon, known as “the first-night effect.”\nAs it turns out, the first night when a person goes to sleep in a new place, a portion of the left hemisphere of his or her brain remains unusually active, apparently to stay alert for any signs of danger. The new findings not only provide important insights into the function of the human brain, they also suggest methods to prevent the first-night effect and thereby help travelers like me in our ongoing quest to get a good night’s sleep.\nOur goal? To produce the first dynamic view of the human brain in action, revealing how its roughly 86 billion neurons and its trillions of connections interact in real time. This new view will revolutionize our understanding of how we think, feel, learn, remember, and move, transforming efforts to help the more than 1 billion people worldwide who suffer from autism, depression, schizophrenia, epilepsy, traumatic brain injury, Parkinson’s disease, Alzheimer’s disease, and other devastating brain disorders."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:46f6bb9f-830a-4593-89f2-925551affba6>"],"error":null}
{"question":"What are the key differences between fairy tales and folk literature manifestations across cultures?","answer":"Fairy tales are typically focused on magic and fantastic elements, rarely occurring in contemporary settings, as seen in stories like Cinderella. In contrast, folk literature manifestations vary greatly by region and culture - for instance, Russian epic songs are found only in Russia, while wonder stories like Cinderella appear in folk literature worldwide. The emphasis differs based on geographic location and cultural preferences - Irish excel in storytelling of legends and fictional tales, while England and Wales prefer legends and ballads. Additionally, there's a notable contrast between the abundance of oral saints' legends in Spain and Italy versus their rarity in Scandinavia.","context":["I will set down a tale... it may be history, it may be only a legend, a tradition. It may have happened, it may not have happened. But it could have happened....\n--- Mark Twain [via Christopher Neufeld]\nUrban legends are perennial favorites at sleepovers and around camp fires, as one might expect, but they also seep into many other aspects of our lives. They are told and retold in offices, schools, and spread through books, movies and the internet. But what exactly are urban legends and how do they differ from other forms of folklore?\nAll the different types of oral and written stories share similarities, but they are categorized differently by folklorists. Some features need to be established so the ways in which urban legends differ can be established. Here, briefly, are some of the general characteristics of two other common forms of oral tales: fairy tales and tall tales. Fairy tales are typically concerned with magic and the fantastic and rarely in a contemporary setting. Look at well-known stories, like Cinderella, Donkey Skin and Sleeping Beauty for examples of magical tales. Tall tales are exaggerations of truth and legend, usually with a whimsical twist. Stories like Paul Bunyan and Johnny Appleseed (in North America) are examples of tall tales.\nThe term “urban legend” is relatively new, its creation is credited to Richard Dorson and its first known appearance in print was in a 1968 collection of essays entitled Our Living Traditions. In popular definitions urban legends (ULs) are often referred to as “apocryphal”. That simply means that there is no clear author or creator. Which is the same for most fairy tales, tall tales and the like. There, of course, must be an original teller, but in the spread and evolution of the tale, the original participants in its creation are lost. And as will be explored in later essays, the creator of the story is unimportant in the spread of a UL.\nUrban legends are, despite the name, not always concerned with urban topics. They may be located in small cities, hospitals, schools – anywhere! Jan Harold Brunvand, a well-known folklorist and collector of urban legends noted that \"... these stories reflect urban life and attitudes, even if they're not told exclusively about things that supposedly happened in big cities.\" The topic of ULs vary greatly from well-known horror stories like “Hook Hand” to politics, religion, words, and new forms that evolved with modern media, like the internet and TV. Whatever the topic urban legends typically share a few qualities:\n1. They occur with no known author\n2. They contain bits of horror or humor, often to reinforce a social norm or acceptable behavior\n3. They must be interesting and easy to retell\nUrban legends are also rarely codified into a single (or a few) accepted formats. They have regional and time variations. If they survive, they are never static. Despite being collected and recorded by folklorists, urban legends refuse to become static. They continue to change as they spread and the needs and desires of the tellers change.","- Origins and development\n- Characteristics of folk literature\n- Techniques of folk literature\n- Regional and ethnic manifestations\n- Major forms of folk literature\n- Study, collection, and preservation\nRegional and ethnic manifestations\nIn many particulars of form and substance there will be found great variations in the ways folk literature is manifested. The interests of people in one culture may differ profoundly from those of people in another. One group may enjoy singing folk songs, another listening to romantic folktales, and a neighbouring group may even be concerned only with legends and traditions. This difference is often geographic, so that the student of folk literature in the Pacific Islands who may later investigate a Central African tribe will find a completely different emphasis in the two areas. These differences may well depend upon the varieties of religious concepts held by the group or its natural environment, whether islands or jungle or cultivated farm lands, or its stability or mobility. These characteristics are likely to become especially deep-seated in groups that have been settled in one place over a long period of history. Frequently they may correspond to national frontiers, but more often they are aspects of the general culture of an area and may well be quite independent of political or linguistic boundaries.\nThe Russian epic songs are found only in Russia, but the wonder story such as Cinderella or Snow White is a part of the folk literature of a good portion of the world. The Navaho Indians of the Southwestern United States place great emphasis on their remarkable chants and lengthy folktales. Their neighbours throughout the Great Plains tell many well-constructed unified stories but confine their rituals largely to the dance. In Europe the Irish excel in storytelling, both of legends and fictional tales, so that even today it has been possible to record a prodigious number for their national archive. But in England and Wales the folktale is little cultivated, preference having been given to legends and ballads. As expected, there is a contrast between the abundance of oral saints’ legends in Spain and Italy and their rarity in Scandinavia. Finland, meeting place of Eastern and Western tradition, shows an abundance of nearly every kind of folklore. From eastern Europe to Central Asia the folk epic flourishes.\nTales and origin legends have been collected in great numbers from various parts of Oceania, where there is a common mythological background extending over enormous distances. Except for probable early contact by way of Indonesia, these folktales seem to show little Eurasian influence. In many parts of South America the merging of Iberian, Indian, and African materials seems almost complete.\nThe folk literature of the African Americans is in a state of continual change, reflecting their history. Much certainly goes back to Africa, usually by way of the West Indies, and much was borrowed long ago. But African Americans have themselves in a truly oral fashion developed songs and stories, and particular music styles. Of very special character is the folklore of modern Israel. Jews from various lands have brought together folk literature from all these countries. Assimilation of this is a long task, and, since divergent language backgrounds are unimportant for folktales, the problem is to absorb the great variety of forms.\nTaken the world over, folk literature is found everywhere, though the emphasis differs from place to place.\nMajor forms of folk literature\nSinging of some kind is almost universal, and it is probable that where there are no reports of it the information is simply missing. Folk song implies the use of music, and the musical tradition varies greatly from one area to another. In some places the words of songs are of little importance and seem to be used primarily as support for the music. Frequently there are meaningless monosyllables and much repetition to accompany the voice or the musical instrument. In much of the world, drums and rattles, beating time by hands or feet, or the stroking of a harp give a strong rhythmic effect to folksinging. In other parts of the world, flutelike wind instruments or bowed fiddles of one kind or another affect the nature of folk song texts. In many places folk songs are of great importance, serving as excitement to war or love or as a part of religious or secular ritual. Through them the group expresses its common emotions or lightens the burden of communal labour. In certain preliterate groups and sometimes elsewhere, folk songs are used for magic effects, to defeat enemies, to attract lovers, to invoke the favour of the supernatural powers. Sometimes the magic effect of these songs is so greatly valued that actual ownership of songs is maintained and their use carefully guarded. They may come to the owner in a dream or as the result of fasting or other austerities.\nEven when folk songs are not used for such practical purposes but only for the pleasure of singing or listening, the greater part of the world uses them for the expression of ideas or emotions held in common by the group. Only in societies used to the songs of composers or poets does purely personal expression enter into the folk song. This is not frequent, and songs of this type are hardly to be distinguished from some of the simple lyrics of poets such as Robert Burns. Folk songs, essentially expressions of commonly shared ideas or feelings, are often trivial but sometimes they may be profoundly moving.\nThe lyric folk song in one form or another is found almost everywhere, but this is not true of narrative singing. Unless the reporting of the activities of preliterate cultures has been very faulty, it would seem that the combination of song and story among these peoples has been rare, in spite of a wealth of prose narrative. On the other hand, in major Western and Asian civilizations the narrative song has been important for a long time and has been cultivated by the most skillful singers. In the course of time these songs of warfare, of adventure, or of domestic life have formed local cycles, such as the byliny of Russia or the heroic songs of many of the Balkan States and Finland or the ballad tradition of western Europe and elsewhere. Each of these cycles has its own characteristics, with its distinctive metrical forms, and its formulas both of events and expression. Any reader of the Homeric poems will be aware of their essentially oral and musical nature, and all the early literary narratives of Sumer and the Middle East suggest a long previous development of narrative singing."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:b946c5e2-b23b-44f6-a043-c1cf1b64c7fb>","<urn:uuid:11846f5d-fd2a-4a95-a606-34590ecfdd64>"],"error":null}
{"question":"What are the age patterns for ALS versus lung cancer in non-smokers?","answer":"ALS has a mean age of onset of 65.7 years. In contrast, lung cancer in never-smokers is showing a troubling shift toward younger patients in their 20s, 30s, and 40s, which differs from the traditional mean age of 70 at diagnosis for lung cancer overall.","context":["Are the incidence and prevalence of amyotrophic lateral sclerosis changing over time?\nIn this epidemiologic register-based study of 2702 patients in Italy, the crude incidence rate of amyotrophic lateral sclerosis increased by 14% during the 1995 to 2014 period, but the increase was reduced to 9% and was mostly limited to women after age and sex adjustment. The age-period-cohort model revealed that the increase can be partly ascribed to a birth cohort effect, affecting women born before 1930.\nThe incidence of amyotrophic lateral sclerosis is increasing in part because of the increase of the mean age of the general population, with a residual component attributable to a birth cohort effect in women.\nThis study reports the long-term epidemiologic trends of amyotrophic lateral sclerosis (ALS) based on a prospective register.\nTo examine the 20-year epidemiologic trends of ALS in the Piemonte and Valle d’Aosta regions of Italy.\nDesign, Setting, and Participants\nThe Piemonte and Valle d’Aosta Register for ALS (PARALS) is an epidemiologic prospective register that covers 2 Italian regions (population of 4 476 931 inhabitants according to the 2011 census) from January 1, 1995, through December 31, 2014. Case ascertainment is based on multiple sources (neurologic departments, hospital discharge archives, and mortality records). Incidence rates are age and sex standardized for the Italian population of the 2011 census. Age-period-cohort (APC) analysis was performed using a Poisson regression model.\nMain Outcomes and Measures\nThe primary study outcomes were long-term incidence and prevalence rates of ALS using a prospective design and their determinants.\nDuring the study period, a total of 2702 patients (mean [SD] age at onset, 65.7 [11.1] years; 1246 [46.1%] female and 1456 [53.9%] male) received a diagnosis of ALS between 1995 and 2014, corresponding to a crude annual incidence rate of 3.03 per 100 000 population (95% CI, 2.85-3.23) and an adjusted incidence rate of 2.78 per 100 000 population (95% CI, 2.57-2.96). The age-adjusted incidence rate increased in the 2 decades of the study (1995-2004: 2.66; 95% CI, 2.50-2.83; 2005-2014: 2.89; 95% CI, 2.71-3.07; P = .04), mostly in women. The adjusted rate ratio of men to women decreased from 1.27:1 (1995-2004) to 1.17:1 (2005-2014). The analysis of deviance for the APC regression models indicated that the drift variable is relevant in explaining the variation of ALS incidence rates over time in the overall population (change in deviance, 4.6553; P = .03) and in women (change in deviance, 3.8821; P = .05) but not in men (change in deviance, 0.77215; P = .38). A total of 479 patients with ALS were alive and had not undergone tracheostomy at the prevalence day (December 31, 2014), corresponding to a crude prevalence rate of 10.54 per 100 000 population (95% CI, 9.64-11.52).\nConclusions and Relevance\nDuring the 1995 to 2014 period, the crude and adjusted incidences of ALS increased in Piemonte and Valle d’Aosta, mostly in women. The APC model revealed that the increase of ALS incidence is attributable to a birth cohort effect in women, with a peak in the 1930 cohort. The different increase of ALS incidence in men and women points to an effect of exogenous factors with a differential effect on the 2 sexes, acting on a genetic background.\nChiò A, Mora G, Moglia C, Manera U, Canosa A, Cammarosano S, Ilardi A, Bertuzzo D, Bersano E, Cugnasco P, Grassano M, Pisano F, Mazzini L, Calvo A, for the Piemonte and Valle d’Aosta Register for ALS (PARALS). Secular Trends of Amyotrophic Lateral SclerosisThe Piemonte and Valle d’Aosta Register. JAMA Neurol. 2017;74(9):1097–1104. doi:10.1001/jamaneurol.2017.1387\n* * SCHEDULED MAINTENANCE * *\nThe JAMA Network Sites will be conducting routine maintenance from 10/20/2017 through 10/21/2017. During this window access to content and authentication may be intermittently available. The JAMA Store will be completely unavailable during the maintenance window.","Lung cancer is currently the most common cancer worldwide. In most countries, lung cancer is also the most malignant form of tumor with the lowest survival rates. As of 2018, in most countries worldwide, lung cancer is the primary cancer-related cause of death. A staggering 1.8 million deaths from lung cancer in both sexes combined were recorded in 2018 globally. In the USA lung cancer mortality peaked at 159,292 in 2005 but has since decreased by 6.5% to 148,945 in 2016.\nThe five-year survival rate for lung cancer in the U.S.A. as of 2015 is 18.6 percent, much lower than other types of cancers such as colorectal (64.5 percent), breast (89.6 percent) and prostate (98.2 percent). When the disease is diagnosed at an early stage, the survival rate of lung cancer can be as high as 56 percent. Unfortunately, most lung cancer patients are rarely, if ever, diagnosed at an early stage. As a result of the spreading of the malignancy, more than half of lung cancer patients die within one year of being diagnosed. The 5-year survival rate regardless of stage is only 15% among lung cancer patients.\nThe latest statistics provided by the World Cancer Research Fund International revealed a total of 2,093,876 new cases of lung cancer being diagnosed in 2018 for both sexes worldwide. Prevalence of lung cancer was the highest among men worldwide, representing 15.5% (1,368,524 new cases) of the total number of new cases diagnosed in 2018. Lung cancer ranked number 3 for women, representing 8.8% of the total number of new cancer cases diagnosed in 2018, or 725,352 new cases in lung cancer alone, worldwide.\nWhen individuals stop working due to cancer this represents a loss to society in the loss of productivity. The National Institutes of Health estimated that annual costs for the care of lung cancer in the U.S.A. reached $13.4 billion in 2015. Loss of earnings/productivity due to early death from lung cancer in the U.S.A adds another $21.3 billion in costs in 2015. In other countries, including Ireland, lung cancer is responsible for the highest productivity loss for cancer-related mortality.\nProjected productivity loss for cancer-related mortality 2011-2030 in Ireland\n[Source: Alison Pearce, Cathy Bradley, Paul Hanly, Ciaran O’Neill, Audrey Alforque Thomas, Michal Molcho and Linda Sharp Projecting productivity losses for cancer-related mortality 2011 – 2030 BMC Cancer. 2016 Oct 18;16(1):804.PMCID: PMC5069877 DOI: 10.1186/s12885-016-2854-4]\nIn the past, lung cancer was regarded as a disease for the elderly as the disease was found mostly in men over 50 years old, between the ages of 60-75 years. About 75% of lung cancer cases were attributed in part to tobacco smoking, with a higher estimate of 85% to 90% for the U.S.A. Compared to people who do not smoke, men who smoke are 23 times more likely to develop lung cancer, whereas women are 13 times more likely. Duration of smoking is considered to be the strongest determinant of lung cancer risk in smokers.\nEver since the early 1950’s when the carcinogenic effects of tobacco smoke on the lung were repeatedly shown in scientific studies, the rate of tobacco smoking began to decline sharply after the recognition by public health regulatory authorities in the mid-1960’s. In the U.S.A., the self-reported adult smoking rates peaked in 1954 at 45%, and remained at around 40% or more through the early 1970’s. The average rate of smoking across the decades began falling from 40% in the 1970s to 32% in the 1980s, 26% in the 1990s, and 24% since 2000. In 2008, the percentage of U.S. adults saying they smoked cigarettes was only 21%, a decrease of more than 50% from the peak at 45% in 1954.\nPER CAPITA CONSUMPTION OF CIGARETTES AMONG ADULTS 18 YEARS AND OLDER FROM 1900 – 2004 IN THE U.S.A.\n[SOURCE: THE NATIONAL ACADEMY PRESS, ENDING THE TOBACCO PROBLEM: A BLUEPRINT FOR THE NATION (2007) https://www.nap.edu/read/11795/chapter/4]\nDespite the dramatic reduction of close to 50% in smoking rates between 1975 to 2004 in the U.S.A., the age-adjusted cancer incidence rates for lung cancer actually INCREASED by 6.3 percent from 52.2 per 100,000 in 1975 to 55.5 per 100,000 in 2011.\nEven though the development of lung cancer is attributed mainly to smoking, approximately 25% of lung cancer incidences worldwide are not related to tobacco use. Lung cancers in never-smokers account for over 300,000 deaths worldwide each year.\nStriking differences in the epidemiological, clinical and molecular characteristics of lung cancers arising in never-smokers versus smokers have been identified, suggesting that they are separate entities. Never smokers are people who have smoked less than the equivalent of 100 cigarettes in their entire lifetime. Lung cancer in never-smokers affect females more than males. It is estimated that 53% of all women with lung cancer worldwide compared to 15% of men, are never-smokers. In the U.K. 67% of never-smokers who underwent lung cancer surgery from 2008 to 2014 were females.\nLung Cancer Age‐Adjusted Incidence Rates by Sex, 1975‐ 2011\n[Source: National Cancer Institute. SEER Cancer Statistics Review, 1975‐20011 Credit: American Lung Association Epidemiology and Statistics Unit Research and Program Services Division, Trends in Lung Cancer Morbidity and Mortality November 2014 https://www.lung.org/assets/documents/research/lc-trend-report.pdf]\nDistinct clinical, pathological and biological features of lung cancer in never-smokers are also observed, in that small-cell lung cancer that is highly correlated with smoking is quite rare in never-smokers; and lung cancer in never-smokers is almost exclusively made up of non-small-cell lung cancer, with a predominance of adenocarcinoma over squamous cell carcinoma in the ratio of around 8:1 in Europe. Studies before 1990 also confirmed the observation that adenocarcinoma was most common among lung cancer in never-smokers.\nThe most troubling aspect in the diagnosis and treatment of lung cancer in never-smokers worldwide is that the demographics now include younger patients in their 20s, 30s, and 40s, instead of the traditional mean age of 70 at diagnosis.\nCompared to smokers, the carcinogenesis of lung adenocarcinoma in never-smokers is more complicated. Many factors may cause the development of lung cancer in never-smokers, the major contributors are considered to be:\nEnvironmental Tobacco Smoke (ETS)\nEnvironmental Tobacco Smoke (ETS) is defined as “sidestream smoke from the smoldering tobacco between puffs and exhaled mainstream smoke from the smoker.” The link between ETS and lung cancer was first reported in 1981. In 1992, the Environmental Protection Agency published a review on the effects of ETS on lung cancer and indicated that ETS is associated with increased risk for lung cancer. However, in 2005 a large population study involving over 500,000 volunteers by the European Prospective Investigation into Cancer (EPIC) and Nutrition, an ongoing multi-centre prospective cohort study designed to investigate the relationship between nutrition and cancer, revealed that there is no statistically significant hazard in the development of lung caner from exposure to ETS. The current evidence seems to suggest that ETS plays a modest role in the development of lung cancer in never-smokers.\nRadon is a naturally occurring radioactive gas produced by uranium decay in the earth’s crust. It emits alpha particles, decaying to polonium and bismuth. indoor levels can be quite variable depending on soil composition, building foundations and ventilation. Radon can accumulate to unsafe levels in basements and lower building levels. Radon exposure in underground workplaces is regulated in the U.S.A. Radon exposure is considered to be the second leading cause of lung cancer after tobacco smoke. Radon is not only an independent risk factor; it also increases the risk of lung cancer in smokers.\nOccupational exposure to carcinogens is estimated to account for 5–10% of lung cancers. Of these, asbestos is the most common. The risk for lung cancer from asbestos exposure is dependent on both fiber type and dose, as larger chrysotile fibers can be cleared from the lungs more rapidly than amphibole fibers. Studies that did not detect increased lung cancer mortality from nonoccupational asbestos exposure involved populations that were predominantly exposed to chrysotile fibers. In general, it is believed that nonoccupational exposure to asbestos may not have a significant role in increasing mortality from lung cancer in never-smokers.\nInfection & Inflammation\nDamage to the lungs from inflammation and infection is often implicated in tumorigenesis. Pre-existing lung disease like tuberculosis increases odds ratio up to 1.76 for the development of lung cancer, regardless of smoking status. Chronic inflammatory lung diseases also confer additional risks for cancer development. However, the majority of lung cancer patients who are never-smokers do not have a history of active interstitial lung disease.\nInherited Genetic Susceptibility\nA positive family history of lung cancer has been found to be a risk factor in several registry-based studies that have reported a high familial risk for early-onset lung cancer. Family history of lung cancer is correlated with increased risk in the development of lung cancer in both smokers and never-smokers. For never-smokers, genetic influences would explain the increased risk in the younger age group.\nGenetic Risk Factors\nLung cancer in never-smokers is almost exclusively made up of NSCLC (non-small-cell lung cancer), with a predominance of adenocarcinoma. NSCLC patients often show metastasis to major organs such as liver (33–40%), brain (15–43%), kidney (16–23%), adrenal glands (18–38%), bone (19–33%), and abdominal lymph nodes (29%). The difficulty in treating primary tumors and related metastatic secondary mutations accounts for the poor prognosis of NSCLC. The 5-year survival rate for advanced NSCLC patients remain stubbornly below 5%.\nScientists now believe that genes may be involved in the increased susceptibility to adenocarcinoma in young never-smokers, especially in the gender disparity as the incidence of lung cancer in never-smokers worldwide indicate a female predominance. 53% of females who develop lung cancer are never-smokers while only 15% of male lung cancer patients never smoked, and the incidence of lung cancer in women can vary by 30-fold even in countries reported to have low prevalence of female smoking.\nThe higher incidence of lung cancer in female never-smokers has led scientists to explore gender-dependent hormones in the development of lung cancer. It has been observed that women who received anti-estrogen therapies showed a reduction in lung cancer incidence while women placed on hormonal replacement therapy showed increased risks for NSCLC.\nAlthough lung cancer in never-smokers share molecular features that are typical in tobacco-related carcinogenesis, the presence of unique genetic and epigenetic markers indicate that a different but possibly overlapping carcinogenic pathway is responsible for the development of lung cancer in never-smokers.\nSpecific genetic mutations have been associated with a higher prevalence of adenocarcinomas in never-smokers. Those genes include EGFR, PTEN, ALK, ROS1, and RET; whereas a different set of genetic mutations in smokers have been identified. These genes are K-RAS, TP53, BRAF, STK11, and JAK2/3. Mutations and hypermethylation of p16 and LGALS4 are also implicated in progression of lung cancer in smokers.\nDifferent Frequencies of Oncogenic Drivers in Never-smoking vs. Smoking Non-Small Cell Lung Cancer\n[Shin Saito, Fernando Espinoza-Mercado, Hui Liu, Naohiro Sata, Xiaojiang Cui, and Harmik J. Soukiasian, Current status of research and treatment for non-small cell lung cancer in never-smoking females, Cancer Biol Ther. 2017; 18(6): 359–368. doi: 10.1080/15384047.2017.1323580]\nAs a result of dedicated efforts committed to the control of tobacco use, smoking prevalence and lung cancer mortality have decreased over the past several decades in the USA. However, tobacco smoking remains the major cause for lung cancer, whereas different carcinogens as well as genetic influences could be involved for different groups of never-smokers. With ever increasing understanding of the mechanisms behind the pathogenesis of lung cancer for smokers and never-smokers, it is anticipated that successful treatment can be tailored for the individual patient based on the presence or absence of critical molecular alterations."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:18318727-02b6-4916-85b4-89fcac851c1e>","<urn:uuid:b7ac78f4-395d-4516-b558-e7a11828430d>"],"error":null}
{"question":"How do the kneeling hip flexor stretch and reclining bound angle pose differ in their effects on hip muscles?","answer":"The kneeling hip flexor stretch and reclining bound angle pose work on the hip muscles in different ways. The kneeling hip flexor stretch primarily targets the iliopsoas muscle, which is responsible for flexing and externally rotating the thigh at the hip joint. Meanwhile, reclining bound angle pose (supta baddha konasana) provides a more comprehensive hip opening by simultaneously working on multiple muscle groups - it externally rotates the hip joint, stretches the hip flexors and rotators, and releases tension stored in the hips. While both exercises can help with hip flexibility, the reclining bound angle pose additionally offers benefits for the abdominal and gluteal muscles, and is considered a restorative pose that helps relieve stress and muscle tension.","context":["Iliopsoas Stretches To Incorporate Into Your Routine\nWhat Is The Iliopsoas Muscle?\nThe iliopsoas muscle is the muscle responsible for flexing and externally rotating the thigh at the hip joint. This muscle also helps to flex and stabilise the trunk and assists with correct posture.\nThe iliopsoas is a combo of the psoas major, the psoas minor and the iliacus muscles, which join together and make up the iliopsoas muscle. This muscle group is the one that connects the femur to the spine.\nThe Importance of Stretching the Iliopsoas Muscle\nMany people suffer from tight iliopsoas muscles, especially if they spend most of their day seated. This is because when we are seated, the iliopsoas muscle can shorten and become inactive – but it’s not just too much sitting that can cause issues for your iliopsoas muscles.\nFrequent stretching of the iliopsoas muscle can help to decrease the risk of injuries, improve the posture and stabilise the trunk. On top of this, it can help you move from a seated position to a standing one. By stretching the iliopsoas muscle you can also reduce hip pain and increase mobility in the hip. If you are suffering from pain in your lower back, working the iliopsoas muscle with the appropriate stretches can help prevent further injury while decreasing pain.\nThere is a multitude of factors that can cause your iliopsoas to become tight, such as football, basketball, running, or having weak glute muscles or tight hamstrings. For certain people, short or tight iliopsoas muscles can lead to iliopsoas tendinitis, iliopsoas syndrome or iliopsoas impingement – all of which are conditions that require intervention to improve and get better.\nThe great news is – by performing moves like the kneeling hip flexor stretch and the glute bridge, you can help avoid these more serious conditions that allow you to perform daily tasks with ease.\nIt’s a simple task to add in a few iliopsoas stretches to your daily routine, just be sure to warm yourself up before getting into a stretch.\nExample Iliopsoas Stretches\nPopular stretches for the iliopsoas muscles include kneeling hip flexor stretches, standing hip flexor stretches, hip flexor bed stretch and the glute bridge stretch. All of these stretches can help lengthen the iliopsoas muscles. You should try and get a feel for all of these stretches before choosing the ones that best suit your abilities.\nStanding Hip Flexor Stretch\nThis nice little stretch works on the psoas muscle which is located in the lower lumbar region of the spine and extends through the pelvis to the femur. This muscle can become tight from extended sitting or from repetitive aerobic exercises such as running.\n- Take a split stance by bringing one foot forward and the other foot back.\n- Drop your back knee and tailbone an inch closer to the ground while lightly tucking your pelvis forward.\n- Keep your spine neutral by avoiding rounding or arching your back.\n- Repeat on the other side.\nThe Glute Bridge workout will target one of the biggest muscles on your lower half – your glutes! Glute bridges will also contribute to the development of your leg muscles and help with core stabilisation.\n- Lie on your back with your knees bent and with your feet flat on the ground, hip-width apart while holding a dumbbell in each hand, resting them under your hip bones.\n- Push up through your heels while squeezing your glutes and abs. Lift your hips a few inches off the ground until your body forms a straight line from your knees to your shoulders.\n- Hold this pose for a second and then slowly lower yourself back down to your starting position.\nKneeling Hip Flexor Stretch\nIf you have no problem kneeling, you will find that this move can allow you to get a deep stretch in your iliopsoas muscle. Just like the standing hip flexor stretch, you can control the depth of the stretch by lengthening or shortening your leg stride.\n- Start by getting into a half-kneeling position with your left leg about two feet in front of the right leg. The left knee should form a 90-degree angle. You can use a mat for cushioning.\n- Place your hands on your left knee, maintaining an upright posture, leaning slightly forward until you can feel a stretch in the front of your groin, hip and thigh on your right side.\n- Hold this pose for 25-35 seconds. You should not feel any lower back pain, and if you do – ease up during the stretch.\n- Slowly return to your starting pose and switch sides.\n- Perform the kneeling hip flexor stretch three times on each side.\nHip Flexor Bed Stretch\nThe hip flexor bed stretch is usually used in physical therapy sessions for rehab purposes. The hip flexor bed stretch is a great option if balance or kneeling is an issue for you.\n- Start by lying on your back on your bed, positioning yourself with your left leg closest to the age of the bed.\n- Slowly allow your left leg to hang over the side of your bed, your right leg can stay bent with your foot on the bed. You will feel a nice stretch on the hip flexor. Ideally, your foot should hover over the ground instead of touching – but it is okay if it does touch.\n- You can deepen this stretch by gently bending your knee. You should feel this across the thigh and front of the hip.\n- Hold this pose for 25-35 seconds.\n- Return the left leg to the bed and rotate so your right side is closest to the edge of the bed.\n- Perform this stretch three times on each side.","Reclining bound angle pose, known as supta baddha konasana in Sanskrit, is a restorative hip opener that increases flexibility in the hips and range of motion in the legs. While the relaxing pose stretches the abdominal and gluteal muscles as well, its benefits are mostly concentrated in the hip flexor and rotator muscles, where the pose builds the most flexibility.\nThis reclining posture is also a deeply restorative pose that releases tension from, particularly, the hips. Women, especially, tend to store the body’s stress and muscle tension in the hips. Hip openers like supta baddha konasana combat this tendency by externally rotating the hip joint and building long, lean flexor and rotator muscles.\n- Begin lying supine on the mat. Rest your arms by your sides, palms facing up.\n- Inhale. Scoop your tailbone toward your navel to close the space between your lower back and the mat. This spinal extension will protect your lower back and engage the oblique muscles on either side of your abdomen.\n- Exhale. Press the bottom corners of your shoulder blades up toward your heart center to bring your shoulders down onto the mat.\n- Inhale. Bend your knees straight up and draw your heels toward your body. When your feet are one or two feet from your buttocks, stop.\n- Exhale. Gently drop your knees to the sides of your body (right knee to the right, left knee to the left). As your legs open outwards, bring the soles of your feet together. Nestle your heels as close to your hips as is comfortable and relax into supta konasana.\n- Keep your arms by your sides, palms facing up, or place your hands, palms down, on your belly. Close your eyes.\n- Inhale. Gently press your lower back and shoulders down into the mat.\n- Exhale. To drop your thighs lower to the mat and to, ultimately, stretch the hip rotators further, imagine you are drawing your inner groins into your hip joints. Use this motion, concentrated in the inner hips, to press your legs closer to the mat. Forcing the knees to the mat (the natural tendency in this pose) gives a less comprehensive stretch that is actually dangerous for your knee joints.\n- Breath and hold the pose. Extend the spine and press the shoulder blades up with every inhale. Draw the groins into the hip joints with every exhale.\n- Inhale. Lift your legs and bring your knees together. Exhale. Straighten your legs and come back into a supine position.\nTips, Photos and Videos for Beginners\nWhat are the benefits of konasana? Reclining bound angle pose lengthens the external hip rotators and the hip flexors for all around flexible, open hips. Along with other intense hip openers like pigeon pose (eka pada rajakapotasana), supta baddha konasana is one of the most comprehensive and effective cures for tight hips. If practiced consistently, the pose improves pelvic alignment and prevents pulled groin muscles.\nBaddha konasana benefits go beyond muscles and tendons. Because it focuses on the hip muscles, bound angle pose is traditionally thought to improve urinary, digestive, and reproductive health by stimulating circulation to those systems. At the same time, the seated pose mimics meditative asanas thought to facilitate meditation, scholarship, and enlightenment.\nThe reclining version of bound angle pose reverses the relationship between the lower back and abdominal muscles. The pose stretches the abdominal muscles and, as the muscles lengthen, massages the abdominal organs. The pose also gently strengthens the erector spinae and gluteal muscles, relieving lower back and sciatica pain.\nSupta baddha konasana is also a powerful restorative pose that releases muscle tension, relaxes the mind, and relieves stress. The posture includes an intense, simultaneous external rotation of the hips, which effectively releases the muscle tension built up in the hips.\nWhile there are many benefits of konasana, do not attempt reclining bound angle pose if you:\n- have a lower back injury or have serious, persistent lower back problems.\n- have a hernia.\n- have a groin injury.\n- have a knee injury.\n- have iliosacral instability.\nFlexible, open hips are key to accessing the benefits of reclining bound angle pose, but this restorative posture could just as likely be an introduction to stretching the hip flexors and rotators as the pose you relax in to reap the benefits of open hips. This pose doesn’t need much preparation, which is why it’s a great pose to practice while you wait for your yoga teacher to start class.\nBecause reclining bound angle pose is an intense external rotation of the hips, it makes sense to balance the pose with an internal rotation (although it is by no means essential to do so). Consider pairing supta konasana with lion pose (simhasana). Lion pose is a double internal rotation of the hips, so the exact opposite of bound angle pose. Pairing these two stretches the hip rotators more comprehensively.\nBound Angle Pose (Baddha Konasana). Reclining bound angle pose is a variation of this original, seated posture. Begin seated on the mat with both legs extended out in front of you (in staff pose; dandasana). Extend your spine and press your shoulder blades forward. Arrange your legs as in step four and five (above). Take hold of your feet with your hands. Exhale and draw your groins into your hip joints as in step eight. Use your hands to gently pry the soles of your feet apart, as if you were reading a book, to intensify the hip opener. Breath and hold the pose. Exhale and release your legs.\nHip rotator muscles. The hip rotator, or lateral rotator, muscle group includes six small muscles in the hip that control external rotation of the legs. Short hip rotator muscles contribute to poor pelvic alignment. In reclining bound angle pose, both legs are externally rotated. This rotation, and the stretch to the hip rotators it provides, contributes to good pelvic alignment and improved range of motion for kicks, jumps, and splits.\nHip flexor muscles. The hip flexors are a large group of muscles located deep in the thighs, hips, and buttocks. They connect the leg, pelvis, and abdomen and allow you to lift your upper leg towards your body or bend your body over your upper leg. Sitting for long periods of time weakens the hip flexors, making it difficult to lift the upper legs and to bend over. In reclining bound angle pose, the hip flexors lengthen.\nGluteal muscles. The gluteal muscle group includes the three buttocks muscles: the gluteus maximus, the gluteus medius, and the gluteus minimus. The gluteal muscles are some of the body’s strongest and are the primary movers of the hips and thighs. Reclining bound angle pose stretches the lesser gluteal muscles in the sides of the buttocks.\nErector spinae muscles. The erector spinae is a bundle of muscles and tendons in the back that control extension and rotation. Because they are responsible for straightening the back, the strength of the erector spinae muscles is closely linked with posture. Reclining bound angle pose gently strengthens the erector spinae muscles in the lower back for better posture and a back less prone to pain and injury.\nAbdominal muscles. The abdominals are located in the lower belly, between the ribs and the pelvis. They control the tilt of the pelvis and the curve of the lower spine. Reclining bound angle pose stretches the abdominal muscles. For best results, tilt your pelvis upward to engage both your rectus abdominus (your “six pack”) and the obliques on either side of your stomach, drawing your entire belly in."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:7749ae8a-1fe1-4452-9330-6568077eebca>","<urn:uuid:ea15e607-1779-446f-ac6c-3677fa1925df>"],"error":null}
{"question":"How do karst sinkholes in Kentucky compare to those in Florida in terms of size and impact?","answer":"Sinkholes in Kentucky and Florida differ in their scale and impact. In Florida, sinkholes are a routine problem and can be large enough to swallow entire houses or businesses. In contrast, cover-collapse sinkholes in Kentucky are typically smaller and less catastrophic, due to the thinner soil cover of usually less than 25 feet. While Kentucky sinkholes can severely damage buildings, drain farm ponds, damage roads, and wreck farming equipment, they are unlikely to swallow entire structures like those in Florida where the cover over the limestone is very thick.","context":["Most Franklin County residents probably don’t stay awake at night worrying about an encounter with a sinkhole, like the one that wreaked havoc on the Bowling Green, Ky., National Corvette Museum, but such earthen collapses on a smaller scale aren’t uncommon in the immediate area.\nJohn Woodall, Franklin County Highway Department superintendent, said two had occurred in the past two weeks on rural roads, requiring personnel to make repairs.\nNone have occurred in the immediate area, like the Corvette Museum sinkhole that swallowed eight historic cars, but the most recent one happened on March 1 on Garrett Lane, requiring eight tons of gravel to fill underneath the pavement.\nThe sinkhole left a small opening toward the middle of the pavement and extended underneath, slightly past the road’s edge.\nFrom an initial glimpse, it didn’t look that bad, but the hole was several feet deep, leaving a cavity underneath the pavement that could have been hazardous if the surface would have collapsed, Woodall said. He added it was fortunate no one had driven over it and experienced what could have happened if the pavement would have collapsed.\nThe other sinkhole occurred around Feb. 17 on a gravel-based Cole Lane, a short distance from the Garrett Lane collapse. It required 20 tons of gravel to fill it.\nWoodall said the two sinkholes appear to be in the same fault area and could have been spawned from a small-scale earthquake around Valentine’s Day that was barely noticed.\n“I don’t really know if the earthquake had anything to do with them,” Woodall said, referring to the sinkholes. “But it all happened about the same time, so it makes you wonder.”\nAnother larger one occurred in December 2010 on State Route 422 near Holder’s Cove Road in Winchester that was about 30 feet deep and 15 feet in diameter that required the road to be temporarily shut down.\nA sinkhole also occurred several years back along Gum Creek Road in Decherd that remains largely in its fallen state and serves as a quick reminder how fast topography can change when the area below the surface land gives way.\nWhile sinkholes in Tennessee have had less devastating effects, Florida has a routine problem with them and provides information on the state’s http://www.dep.state.fl.us/geology/feedback/faq.htm#3 website.\nThe website explains that sinkholes form in karst terrain — a type of topography that is formed by dissolution of bedrock in areas underlain by limestone, dolostone or, as in some western states, gypsum.\nSuch terrain has underground drainage systems that are reflected on the surface as sinkholes, springs, disappearing streams or even caves.\nThe term karst refers to the terrain, and the term sinkhole is one of the types of drainage features reflected by that type of terrain.\nSinkholes occur mainly from the collapse of surface sediments into underground voids and cavities in the limestone bedrock.\nThe website also says that slightly acidic groundwater slowly dissolves cavities and caves in the limestone over many years.\nWhen the cavity enlarges to where its ceiling can no longer support the weight of overlying sediments, the earth collapses into the cavity.\nIn the less catastrophic type of sinkhole, a bowl-shaped depression forms at the surface, usually over a considerable period of time, as surface sediments ravel downward into small cavities in the bedrock.\nThe website says well drilling data suggests that much of the underlying bedrock in Florida contains cavities of differing size and depth. However, relatively few ever collapse and directly effect roads or dwellings.","A geologic hazard is a naturally occurring geologic condition that may result in property damage or may be a threat to the safety of people. The geologic hazards associated with karst are: sinkhole flooding, groundwater vulnerability, radon, and cover-collapse sinkholes described on this page.\nCover-collapse sinkholes occur in the soil or other loose material overlying soluble bedrock. A sinkhole is any naturally occurring depression in the surface of the ground from which rainfall is drained internally. Sinkholes that suddently appear form in two ways. In the first way, the bedrock roof of a cave becomes too thin to support the weight of the bedrock and the soil material above it. The cave roof then collapses, forming a bedrock-collapse sinkhole. Bedrock collapse is rare and the least likely way a sinkhole can form, although it is commonly incorrectly assumed to be the way all sinkholes form. The second way sinkholes can form is much more common and much less dramatic. The sinkhole begins to form when a fracture in the limestone bedrock is enlarged by water dissolving the limestone. As the bedrock is dissolved and carried away underground, the soil gently slumps or erodes into the developing sinkhole. Once the underlying conduits become large enough, insoluble soil and rock particles are carried away too. Dissolution sinkholes form over long periods of time, with occasional episodes of more rapid subsidence or collapse. It is the collapse of the loose cover over the bedrock or soil that causes the problem. Sometimes the collapse will occur in an area with no indication of previous subsidence. In other words, a new sinkhole has formed. Sometime there was a preexisting sinkhole, but it had been filled in and covered over during land clearing or construction. Factories or homes built over filled sinkholes may be damaged as the fill continues to be transported out of the sinkhole and cover collapse reoccurs.\nMany important aquifers are composed of granular materials such as sand and gravel or weakly cemented bedrock. Such aquifers occur in Kentucky, particularly along the Ohio River and in the Jackson Purchase Region. Groundwater flow in these aquifers is through the pores between the grains of sand or gravel or through narrow fractures in solid bedrock. Small openings act as a filter, physically or chemically removing most suspended matter, which holds back the soil cover. Water movement through the fracture system of a fractured bedrock aquifer or the pore spaces of a granular aquifer is slow, when compared to movement in a karst aquifer. The flow velocity in a granular aquifer is too slow, and the pores too small, to transport grains of soil.\nThe drainage patterns of karst conduits resemble the branching pattern formed by streams flowing above ground across insoluble rocks. Water recharge to karst aquifers occurs either directly, through swallow holes and sinkholes, or indirectly, through the pores in the soil overlying the limestone bedrock. The conduit of a karst aquifer is somewhat like a roofed creek bed and responds to rainfall like a storm sewer. Water flow is fast and turbulent, and significant amounts of sediment can be carried through the relatively large conduits. As water flows through the soil into the underlying bedrock, it enlarges joints (cracks) in the bedrock to form grikes . The grikes drain into small conduits, which in turn drain to a conduit large enough to be called a cave, which discharges to a spring . When the overlying soil is repeatedly wetted and dried, small amounts of soil are dislodged and carried away by the cave conduit draining the sinkhole. Gradually, a void develops in the soil overlying the enlarged grike or conduit. The collapse occurs only in the overlying soil cover, not in the limestone bedrock. Here is a diagram of the steps in the formation of a typical cover-collapse sinkhole.\nCover-collapse sinkholes can vary in size from 1 or 2 feet deep and wide, to tens of feet deep and wide. Unlike large collapses, like those in Florida where the cover over the limestone is very thick, cover-collapse sinkholes in Kentucky are unlikely to swallow entire houses or businesses. Cover-collapse sinkholes in Kentucky do severely damage buildings , drain farm ponds, damage roads, and wreck farming equipment. The thickness and cohesiveness of the soil cover determine the size of a cover-collapse sinkhole. In Kentucky the thickness of soil, sand or clay, and bedrock fragments over limestone bedrock is typically less than 25 feet, so cover-collapse sinkholes more than 20 feet in diameter are uncommon. Some precautions can be taken, however.\nThe only guaranteed method of avoiding karst geologic hazards is to avoid living on karst. But because such a large percentage of Kentucky is karst, that is clearly impractical for most people. The most effective way to avoid cover collapse sinkhole damage is to avoid buying or building a structure on a sinkhole that has been filled. Ask the seller if any sinkholes have been filled, and, if so, where, how, and by whom. Look for previous damage to foundations and check doorframes and windows for squareness. Check the surrounding lot for shallow depressions and arch-shaped cracks in the soil. Should a cover collapse occur on your property, it is possible to repair the damage.\nIf a cover-collapse sinkhole develops under a building, the foundation of the building should be shored up as quickly as possible to avoid major foundation damage. Several engineering techniques are available to transfer the load of the building to competent bedrock. The sinkhole can then be filled at a less urgent pace using a graded-filter technique (Reitz and Eskridge, 1977, Sowers, 1996), the foundation reinforced, and the soil graded. The purpose of the graded filter is to allow water to seep into the ground while the soil is held back. In the case of farm ponds and lagoons, the graded filter construction is essentially the same, but the final layers are fine gravel, coarse sand, and fine sand. The uppermost layer is bentonite clay or volclay, which blocks water seepage. Always consult a professional geologist who is experienced in identifying karst subsidence and an engineer experienced in sinkhole remediation when dealing with any structure threatened by a cover-collapse sinkhole.\nKentucky Geological Survey\n228 Mining and Mineral Resources Building\nUniversity of Kentucky\nLexington, KY 40506-0107\nReitz, H.M., and Eskridge, D.S., 1977, Construction methods which recognize the mechanics of sinkhole development, in Dilamarter, R.R., and Csallany, S.C., eds., Hydrologic problems in karst regions: Bowling Green, Western Kentucky University, Department of Geology and Geography, p. 432-438.\nSowers, G.F., 1996, Building on sinkholes: Design and construction of foundations in karst terrain: American Society of Civil Engineers, p. 115."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:81601a13-8d81-45d3-a7f2-7d637e7246ba>","<urn:uuid:290de988-9c5f-4d9e-b71e-ed74968b9f87>"],"error":null}
{"question":"What specific services does StairwAI project offer to help low-tech SMEs adopt AI technologies?","answer":"StairwAI offers three main services: 1) a multilingual interaction layer enabling conversations in the user's own language, 2) a horizontal matchmaking service for automatic discovery of AI assets (tools, datasets, experts, consultants, papers, courses), and 3) a vertical matchmaking service for dimensioning and provisioning hardware resources through HPC, Cloud and Edge infrastructures.","context":["Collaboration Partnership between EOSC DIH & EU-ToxRisk and Edelweiss Connection\nEU-ToxRisk is an Integrated European ‘Flagship’ Programme Driving Mechanism-based Toxicity Testing and Risk Assessment for the 21st century. The vision of EU-ToxRisk is to drive the required paradigm shift in toxicological testing away from ‘black box’ animal testing towards a toxicological assessment based on human cell responses and a comprehensive mechanistic understanding of cause-consequence relationships of chemical adverse effects.\nEdelweiss Connect (EwC, previously known as Douglas Connect and renamed from February 2019) is a Swiss SME located in Basel, specialised in developing and implementing integrated scientific and technology solutions for industrial use and regulatory acceptance in areas of significant societal and market impact. EwC has extensive experience in scientific research and innovation integrating data, in silico and in vitro methods and related infrastructure into solutions, and has been involved in organising scientific, technical and knowledge management solution development projects since 2008 Edelweiss Connect provides the expertise and experience to initiate, coordinate and manage large collaborative research projects, with partners from industry, government and academia. Its goal is to incubate high impact products, services and startup companies at the forefront of innovation, with sustainability and responsibility.\nEOSC-DIH and EU-ToxRisk have started a collaboration that foresees the following objectives:\nExplore and facilitate the inclusion of EU-ToxRisk services in the EOSC Marketplace.\nIdentify, develop and run a pilot through the EOSC DIH.\nEOSC DIH and EU-ToxRisk will work together on identifying the EU-ToxRisk services that could extend the EOSC DIH portfolio. This will allow EUToxRISK to further improve their exploitation capabilities, while also enriching EOSC DIH offers.\nConduct joint promotion of the partnership through existing dissemination channels, offer opportunities to participate in key events, as well as publish results in print/online material and potentially in the EOSC Portal. They will also investigate further collaboration ideas and common possible initiatives and projects.\nCollaboration Partnership between EOSC DIH & Collabwith\nCollabwith is a global platform to standardized collaboration for innovation, our value proposal is to simplify, digitalize, accelerate the collaboration process from searching your partner until starting collaborating.\nCollabwith will like to partner with EOSC-DIH after the KNOWCO4EOSC pilot. This is a partnership proposal for a potential different new pilot, and to contribute to EOSC-DIH with the Collabwith services and training for collaboration, innovation and entrepreneurship. Collabwith Marketplace wants to help EOSC scientists match their research results with potential business models with industry by reducing bureaucracy and smart legal contracts with data protection clauses, and special clauses to protect IP and research results of unethical usage, and protection clauses that in case the industry partner was not using the research results in a specific time, then scientists got back the whole rights to further research. The following activities will be carried out:\n- Provide a catalogue of workshops for innovation, collaboration and entrepreneurship for startups and partners.\n- Introduce the “Collaboration Canvas”, “Entrepreneurship Journey Canvas with Collaboration and Emotional Intelligence” as a framework and methodology that supports effective communication and collaboration between EOSC-DIH partners, pilots and experts.\n- Identification of startups and SMEs interested to set up a business pilot with EOSC-DIH.\n- Dissemination of EOSC-DIH activities, results and funding calls and interesting opportunities of the EOSC partners.\nCollaboration Partnership between EOSC DIH & StairwAI project.\nThe StairwAI project targets low-tech users, in particular, SMEs with the goal of facilitating the uptake of AI and their engagement on the AI on-demand Platform. This will be achieved through a new service layer enriching the functionalities of the on-demand platform and containing i) a multilingual interaction layer enabling conversations with the Platform in the user’s own language; ii) a horizontal matchmaking service for the automatic discovery of AI assets (tools, data sets, AI experts, consultants, papers, courses etc.) meeting the user business needs and; iii) a vertical matchmaking service that will dimension and provision hardware resources through a proper hardware provider (HPC, Cloud and Edge infrastructures).\nThese services are designed and implemented by using techniques in an AI for AI fashion. The AI techniques deployed in the development of the services are natural language processing for multilingual interaction, constraint solving, optimization and machine learning for horizontal and vertical matchmaking, knowledge representation for organizing the platform AI assets, reputation and fairness mechanisms to improve the matching results.\nStairwAI would like to partner with EOSC-DIH to increase the impact of companies working with both projects at the European level, maximise the potential of funding opportunities for SMEs and optimise the resources dedicated to training and building community.\nCollaboration Partnership between EOSC DIH & Operas\nOPERAS is the Research Infrastructure supporting open scholarly communication in the social sciences and humanities (SSH) in the European Research Area. Its mission is to coordinate and federate resources in Europe to efficiently address the scholarly communication needs of European researchers in the field of SSH. OPERAS’ aim is to make Open Science a reality for research in the SSH and achieve a scholarly communication system where knowledge produced in the SSH benefits researchers, academics, students and more generally the whole society across Europe and worldwide, without barriers.\nThe OPERAS Innovation Lab is part of OPERAS Research Infrastructure with the aim to both provide a knowledge hub on novel scholarly communication as well as to prototype innovative services for OPERAS and/or the wider SSH community. The Innovation Lab fosters innovation through threemain activities: observatory, innovation management, and research. Its main outputs include recommendations, guidelines, and prototypes.\nEOSC-DIH and the OPERAS Innovation Lab would strive to:\n- Diversify the EOSC DIH service offering with services from the SSH domain\n- Provide the OPERAS Innovation Lab with access to industry\n- Visually showcase a link between the OPERAS Innovation Lab and the EOSC\n- Conduct joint dissemination and promotion"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:08960f33-f0ba-4ea8-b7a4-c44a0e6af67f>"],"error":null}
{"question":"I'm interested in sustainable packaging. What are the innovative solutions for eco-friendly packaging in food industry, and what limitations exist for using similar solutions in healthcare packaging?","answer":"In the food industry, there are innovative eco-friendly solutions like bagasse-based packaging, which is made from sugarcane production byproducts. This packaging is compostable, recyclable, and uses vegetable-based inks. It requires no new materials or cultivation areas, has no impact on forest areas, and needs less chemical processing than tree-based pulp. However, in healthcare packaging, there are significant limitations due to FDA and EU regulatory requirements. Compostable packaging materials are currently unlikely candidates for healthcare packaging due to concerns about shelf life limitations and sterilization compatibility. The 'Reduce, Reuse, and Recycle' approach is severely limited, with significant restrictions on reuse or recycling of materials for healthcare products and their packaging.","context":["Committed to sustainability in the industry, Oppy BB #:116424 has introduced an innovative first: a plastic-free, tree-free, renewable packaging solution.\nAs part of its long-standing commitment to environmental stewardship and innovation-led approach to sustainable development, the leading fresh produce grower, marketer and distributor has partnered with CanePak to create bagasse-based packaging.\nCompostable, recyclable, and printed with vegetable-based inks, the pack debuts with one-pound units of green kiwifruit bearing Oppy’s popular KeeWee brand.\nSince bagasse fibers are a byproduct of sugarcane production, the new pack utilizes an existing agricultural waste stream so it requires no new materials, no additional cultivation areas and has no impact on existing forest areas.\nIt also leverages the growing consumer emphasis on environmental sustainability in packaging and is home compostable in as little as four weeks under the right conditions.\n“Bagasse requires less chemical processing than tree-based pulp which means its environmental impact is reduced even further,” Oppy’s Director of Marketing Services Cathie MacDonald said. “Oppy is dedicated to innovative packaging solutions that place the environment at the forefront of our work instead of as an afterthought, which is aligned with our expect the world from us promise.”\nThe KeeWee brand, which features a playful and bright kiwi character, was introduced in 2016 to meet retailers’ demands for a product that energizes the consumer and fuels repeat sales. The groundbreaking pack is therefore a natural fit for the fun, youth-focused KeeWee character which aims to resonate with consumers and retailers alike.\n“CanePak Paperboard is proud to support Oppy’s commitment to environmental stewardship and innovation by advancing the use of 100% bagasse fiber packaging,” CanePak Paperboards Co-Founder Minto Roy said. “Tree-Free packaging that aligns with the growing consumer demand for eco-friendly packaging that is recyclable, minimizes landfill waste, greenhouse gas emissions and deforestation.”\nOppy’s other sustainability initiatives include partnering with the How2Recycle label program to inspire families to recycle produce packaging, in addition to introducing a Top Seal machine in its Vancouver, BC warehouse for repacking bulk items, reducing necessary plastic by 30% in comparison to traditional packs. Oppy also supports the work of the BC Marine Trails Network Association by donating to their plastic clean-up initiatives.\n“Innovation is at the heart of Oppy’s continued success over the years and this is just one of a series of developments that underscore our deep commitment to sustainable business practices across our value chain,” Oppy’s Senior Manager of Insights & Innovation Garland Perkins said. “At Oppy, innovation is more than a product or a package, it is a framework that guides all of our work on a fundamental level and we expect to launch even more developments on this front in the coming years.”\nGrowing, marketing and distributing fresh produce from around the globe for more than 160 years, Vancouver, BC-based Oppy discovers and delivers the best of the world’s harvest. With over 50 million boxes of fresh fruits and vegetables grown on every continent moving through its supply chain annually, Oppy offers popular favorites from avocados and berries to apples and oranges year-round, alongside innovative seasonal specialties. Over the years, Oppy has introduced North Americans to a number of items across its diverse produce range, including Granny Smith, JAZZ and Envy apples, as well as green and gold kiwifruit. Go to oppy.com to learn more.","What does sustainability mean to healthcare packaging?\nIn 1987 the World Commission on Environment and Development defined sustainability as “Sustainable development meets the needs of the present without compromising the ability of future generations to meet their own needs.”\nThere are a variety of initiatives that are underway and being proposed for consumer goods packaging. However, the demands placed on many types of healthcare packaging preclude some of these sustainable initiatives. The mantra of ‘Reduce, Reuse, and Recycle’ is severely limited by FDA and EU regulatory requirements for healthcare packaging. For example, there are significant restrictions on reuse or recycling of materials for healthcare products and their packaging. Compostable packaging materials are currently unlikely candidates for healthcare packaging due to concerns about shelf life limitations and sterilization compatibility.\nOn a more positive note, continued advances in materials have made and may continue to make it possible to reduce the amount of packaging used. It is often possible to use thinner or lighter materials for packaging through the use of laminates and coextrusions as well as using newly available polymers, copolymers and additives. These new materials often perform at such a high level that reductions in the amounts of packaging used can reach 20 to 50% depending on the application and existing package structure and sterile barrier system. Because of the potential for cost savings involved, this type of packaging reduction has gone on for many years. Yet, there are still opportunities to reduce the amount of packaging used in many healthcare applications. In addition, package reduction can also be achieved by changes to package design and/or package format.\nWhile the feasibility of including recycled materials into healthcare packaging remains dubious given the current regulatory environment, the potential to recycle healthcare packaging into other applications is more promising. There exists a significant quantity of potentially recyclable material that is lost to landfill due to lack of infrastructure to capture it in a way that would facilitate recycling. The lack of a defined cost effective recycle stream for these materials is a contributing reason for this loss.\nAn additional option is for materials to be channeled into an energy recovery system. In this process materials are incinerated and the energy released in burning is converted to generate electricity. This option does not have the same limitations of materials streams as the recycle process in terms of multi-layers film structures. The EPA identifies that 90% of all potentially infectious medical waste in the US is incinerated. A move towards making this practice into energy recovery systems may offer a better alternative to landfill waste of medical packaging systems.\nAn encouraging development is the recent establishment of industry related groups focused on sustainability in healthcare. The Healthcare Plastics Recycling Council (HPRC) is one organization. The HPRC website describes the group as follows: “HPRC is a private technical coalition of peers across the healthcare, recycling and waste management industries seeking to inspire and enable sustainable, cost-effective recycling solutions for plastic products and materials used in the delivery of healthcare.” Another organization of note is Practice Greenhealth. The website describes this organization as follows: “Practice Greenhealth is the source for environmental solutions for the healthcare sector and lends support to create better, safer, greener workplaces and communities. Practice Greenhealth is a nonprofit membership organization founded on the principles of positive environmental stewardship and best practices by organizations in the healthcare community.” The Flexible Packaging Association (FPA) has also been actively engaged in pursuing the end of life options for food and medical packaging.\nAn evolving area to explore in packaging sustainability is the source of the raw materials. While recycled materials are not suitable for healthcare packaging, the use of raw materials from sustainable sources can be explored further. One source of sustainable materials suitable for healthcare packaging is familiar polymers made from new sources of hydrocarbons. For instance large scale production of polyethylene using sugar cane as the raw material has recently become a reality. Other similar processes will likely be developed over the next few years to produce other common polymers from sustainable sources, not petroleum or natural gas. Of course, any new material must be tested to prove its suitability for the intended use. If the basic polymer chemistry is unchanged, there is a high likelihood that these new materials will perform like their petroleum based analogs. The new materials lack long term performance data making the willingness for assuming initial risk a potential barrier.\nReferring back to the definition above, “Sustainable development meets the needs of the present without compromising the ability of future generations to meet their own needs.” As this relates to packaging it should be noted that the system that creates our ability to package and protect healthcare products, must be evaluated on the broadest possible scale; what materials are used to make the package, how efficiently can it be made to benefit the environment and at the same time provide product protection, maintenance of sterility and aseptic presentation. Consider also available options for end-of-life removal and destruction of the packaging to recover the components with a lower consumption of resources. The manufacture, use and removal of these materials have a broad environmental impact that should be considered.\nAny interest in sustainable packaging should be discussed with your packaging supplier to help determine what options for sustainability are best for your application."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:5a88ca47-be1b-4eb2-ad85-2f3ee522ad47>","<urn:uuid:d4dc8af3-c53d-4a30-805a-30aa15bcb173>"],"error":null}
{"question":"What are the key capabilities of Microsoft Power BI Desktop's free version?","answer":"Microsoft Power BI Desktop is a free desktop application that enables users to connect to multiple data sources, transform data, and create visualizations. Users can combine different data sources into a data model, build visuals, and create reports that can be shared with others within their organization.","context":["Searching for business intelligence and data analytics software can be a daunting (and expensive) process, one that requires long hours of research and deep pockets. The most popular enterprise BI tools often provide more than what’s necessary for non-enterprise organizations, with advanced functionality relevant to only the most technically savvy users. Thankfully, there are a number of free and open source business intelligence tools out there. Some of these solutions are offered by vendors looking to eventually sell you on their enterprise product, and others are maintained and operated by a community of developers looking to democratize BI.\nIn this article we will examine free and open source business intelligence software, first by providing a brief overview of what to expect and also with short blurbs about each of the currently available options in the space. This is the most complete and up-to-date directory on the web.\nFree business intelligence tools defined\nFree business intelligence software refers to products that are offered commercially free by the solution provider. These offerings are usually trimmed-down versions of the expert or enterprise editions, offering basic functionality that enables users to generate reports or data visualizations. Commercially free BI tools typically offer less functionality on the whole than their open source counterparts, but are often a great way to gain more than a free trial if its a product you were already considering.\nOne thing to note is that the free tools offered by BI solution providers (you’ll notice some of them in this directory) are just parts or pieces of their flagship products. For example, the freemium version may just include the reporting capabilities or chart/graph creation portions of the larger platform. It can still be a great way to do data analysis without breaking out the checkbook, especially if you can manage to stack a number of commercial solutions together.\nWhat is open source business intelligence?\nOpen source business intelligence software is software with a source code that anyone can inspect, modify or enhance. These tools are designed to be publicly accessible and are commonly managed and maintained by organizations with a specific mission in mind. The open source business intelligence tools included in this list are surprisingly full-featured, offering an expansive list of capabilities for a variety of users.\nIt’s important to remember that some of the open source offers included in this list require some development skills, and that may make them less than ideal fits for your use case. We recommend that you read each tool’s FAQ to see just how much coding is required to take advantage of the software. The open source tools usually do a good job of explaining the requirements for use on the download pages.\nArcadia Instant is a desktop visual analytics tool that provides the visualization capabilities from Arcadia Data. It can connect to Confluent KSQL to visualize Apache Kafka topics, as well as link to AWS Athena, Google BigQuery, and Snowflake so you can get started with native cloud visualization. The tool allows users to connect, discover, model, visualize and interact (with up to one Hadoop domain). The solution provider also offers an Enterprise version of its software.\nBIRT is an open source software project that provides a platform for creating data visualizations and reports that can be embedded into rich client and web applications, especially those based on Java and Java EE. The software is one of the top projects within the Eclipse Foundation, an independent consortium of software industry vendors and an open source community. BIRT has a large, active and growing developer community featuring experts from companies like IBM and Cisco.\nDataiku DSS Free Edition\nDataiku DSS is a collaborative data science software platform for teams of data scientists, data analysts, and engineers to explore, prototype, build, and deliver their own data products. Users can profile data visually at every step of analysis, and interactive exploration provides more than 20 chart types. The tool comes in four distinct editions, with the free version offering access to Dataiku forever. The free edition also supports up to 3 users and offers standard database connectors and no limitation in size or volume of processed data.\nHelical Insight Community Edition\nHelical Insight is a developer-friendly open source business intelligence framework built on Java. The tool allows you to develop data analysis on top of your data and embed it, as well as build plugins and add functionalities using your own HTML and Java developer when required. Helical Insight comes in two versions, with the Community Edition (limited features) free and Enterprise Edition being paid. Community Edition code can be downloaded from Github or SourceForge.\nInetSoft VisualizeFree is a free-to-use interactive visual analytics web application. Formerly the InetSoft Style Scope Agile Edition, VisualizeFree does not require a download. The tool offers a number of key features, including the ability to access mobile dashboards from a browser on any device, as well as unlimited multi-dimensional charting and brushing for data exploration. VisualizeFree also includes a number of sophisticated chart types and geographic mapping.\nSeal Report offers a complete framework for producing daily reports and dashboards from any database. The tool focuses on easy installation and report design. Once the software is set up, reports can be built and published quickly. Seal Report is an open source tool for the Microsoft .Net Framework and is entirely written in C#. Key features in clude dynamic SQL sources, native pivot tables, HTML 5 charts, and drill down navigation and sub reports. Seal Report features quick start guides for building reports and a dedicated support forum as well.\nKNIME is an open source analytics platform for creating data science applications and services. The tool is open and continuously integrating new developments, and allows users to create visual workflows with a drag-and-drop graphical interface. You can choose from more than 2000 module nodes to build your workflow by modeling each step of your analysis and control the flow of data to ensure work is always current. Open source integrations for KNIME provide additional access to large projects, and Community Extensions are user contributed capabilities from industry-specific applications.\nKnowage (formerly Spago BI) is an open source business analytics suite. It provides advanced, self-service capabilities aimed at giving autonomy to the end-user, as well as allowing them to build their own analysis, explore and organize data in any manner. Knowage adopts open standards and can be used in various environments without considerable requirements. The suite is composed of several modules, each conceived for a specific analytical domain. They can be used individually or combined for a more tailored solution.\nMetabase is an open source business intelligence tool that allows users to ask questions about data. The tool then displays answers in formats that make the most sense, whether in a bar graph or detailed table. Questions can be saved for later or grouped together into dashboards for later use. Metabase also allows users to share questions and dashboards with other members of your team. The tool also provides an SQL interface for developers in need of more advanced functionality.\nMicrosoft Power BI Desktop\nMicrosoft Power BI Desktop is a commercially free desktop application that lets you connect to, transform, and visualize your data. The tool allows users to connect to multiple different sources of data, and combine them into a data model that lets you build visuals, and collections of visuals you can share as reports with other people inside your organization. Most users who work on business intelligence projects use Power BI Desktop to create reports, and then use the Power BI service to share their reports with others.\nQlik Sense and Qlik View\nQlik offers free (for personal and internal business use) versions of both its major software offerings, so we’re counting this as a double entry. Qlik Sense is a BI and visual analytics tool that supports guided analytics apps and dashboards and custom and embedded analytics. QlikView is a data discovery platform that provides self-service BI by allowing users to ask and answer questions. It is powered by Qlik’s proprietary software engine and connects directly do the data source.\nRapidMiner Studio Free Community Edition\nRapidMiner Studio Enterprise is a visual data science workflow designer that accelerates the prototyping and validation of models. It provides a visual environment for building analytics processes, as well as graphical design that makes it easy to design better models. Though RapidMiner Free doesn’t come with the provider’s Auto Model or Turbo Prep features, it includes a 30-day free trial of Studio Enterprise. Users are granted 10,000 data rows and 1 logical processor. There’s a community support forum as well.\nReportServer Community Edition\nReportServer‘s Community Edition provides you with all that is required to generate reports from data. The tool offers what it dubs pixel-perfect reporting and ad-hoc relational reporting, as well as unlimited users, permission management, internationalization and a unique TeamSpace concept. ReportServer Dynamic Lists enables the creation of table-like reports to export to Excel, CSV, PDF or HTML. The tool offers a wide range of flexible parameters to allow report customization, and report designers can choose from text-boxes, radio buttons, checkboxes, date parameters, popups and more.\nTableau Public is a commercially free service that allows anyone to publish interactive data visualizations to the web. Published “vizzes” can be embedded into web pages and blogs, and they can be shared via social media or email, and they can be made available for download to other users. As soon as a workbook is published to Tableau Public, the viz is accessible by anyone in the world.Visualizations are created in the accompanying app Tableau Desktop Public Edition and require no programming skills.\nJasperReports Server Community Edition\nJasperReports Server (by TIBCO Software) is a standalone and embeddable reporting server. It provides reporting and analytics that can be embedded into a web or mobile application as well as operate as a central information hub. JasperReports Server is optimized to share, secure, and centrally manage your Jaspersoft reports and analytic views. Community Edition functionality includes interactive report viewing and formatting, a secure central repository, report scheduling and distribution, and customizable and brandable UI.\nZoho Analytics Personal\nZoho Analytics (previously known as Zoho Reports) is a self-service BI and data analytics software that allows users to create data visualizations and dashboards. The Personal free on-prem edition of Zoho is limited to one user but enables unlimited reports and dashboards, as well as customizable dashboard options and API support. What’s more is that when you sign up for the free tool you get to try the Professional Edition for a month. Zoho Analytics is available on both Windows and Linux.\nIf you’re looking for an enterprise-class business intelligence software, consult our freshly updated Data Analytics and Business Intelligence Buyer’s Guide.\nLatest posts by Timothy King (see all)\n- ThoughtSpot Unveils ThoughtSpot 6 With a Focus on Automating Analytics - October 15, 2019\n- The 4 Major Players in Cloud Financial Planning and Analysis Solutions, 2019 - October 11, 2019\n- Qlik Debuts Qlik Sense Business and Adds NLP to Its Core Analytics Tool - October 1, 2019"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:163b8f1f-5c5b-43c5-ae8a-8b16382ae543>"],"error":null}
{"question":"What ceremonial structures characterized both Cahokia and Chaco Canyon?","answer":"Both ancient sites featured significant ceremonial structures. Cahokia had the Woodhenge astronomical circle, a great circle of post holes that functioned as an astronomical calendar, and what is believed to be the largest pre-Columbian temple ever built in the United States on top of the largest mound. Chaco Canyon contained Great Kivas, which were 16-meter-diameter subterranean ceremonial chambers, with notable examples like Casa Rinconada. Both sites incorporated astronomical and religious elements into their architectural designs.","context":["The Cahokia Mounds are the remains of the largest Native American settlement in the United States. Once the largest city in the Americas north of Mexico, the mounds have become one of America’s most important archaeology sites. Years of research at Cahokia have turned up not only one of the largest Native American burial sites in the United States, but the remains of an immense formation that was once used for astronomical and likely religious purposes. For these reasons and others, the Cahokia Mounds are considered one of the most sacred Native American sites in the United States.\nNative American tribes probably began traveling through the Mississippi region as early as 9500 BC, with evidence of large settlements in the area as early as 1200 BC. Small woodland groups which permanently established themselves at Cahokia probably did so between 600 and 700 AD. Mound building, a staple of the Mississippian era, began three to four centuries later. By 1050 AD, Cahokia was the largest pre-Columbian Settlement in what is now the United States.\nCahokia reached its peak around 1100 AD, with 120 immense mounds covered with and surrounded by buildings and plazas. Archaeological research has discovered evidence of a massive central plaza, wooden stockade defenses, ceremonial and burial areas, and buildings the size of a basketball court.\nOver the next few centuries, Cahokia declined, almost certainly due to the exhaustion of local resources of food and lumber, climate changes and civil unrest. By the time European explorers arrived in the area, the great civilization was long abandoned, and virtually all traces of the city were gone from the surface except for the mounds. Only a few small groups of the Cahokia tribe, for whom the mounds are named, lived in the area, and these were probably not related to the mound builders.\nCahokia became of religious interest to Native American groups in the 20th century when several sites were identified. These include what is believed to be the remains of the largest pre-Columbian temple ever built in the United States on top of the largest mound and a burial site which contains a large number of what were almost certainly Religious sacrificial victims. Also of interest is a great circle of post holes that, when fitted with wooden posts, became an astronomical calendar known as Woodhenge. Because of these discoveries and Cahokia’s cultural importance, the mounds are considered to be among the most sacred Native American sites in the United States.\nThe Cahokia Mounds site is massive, covering an area of six square miles. Eighty of the known 120 mounds are still standing today. The largest and most important of the mounds, known as Monks Mound, was the site of the settlement’s largest buildings. These almost certainly included the residences of the leaders, as well what appears to be a substantial temple. The Woodhenge astronomical circle, recreated with posts, is just west of Monks Mound.\nThe other most important mound, from a religious standpoint, is Mound 72. This mound is the site of what is probably the most famous Native American burial ground in the United States. Over two hundred skeletons have been unearthed here, including an elaborately buried chief, possibly a leader of the community. Many of the remains are those of bodies which were ritualistically sacrificed, including some that had been buried alive.\nThe Cahokia Mounds are located on the Illinois side of the Mississippi River, approximately seven miles east of St. Louis. The mounds are run as an Illinois State Historic Site. It is open every day from May through October 9:00am-5:00pm; and the rest of the year Wednesdays through Sundays from 9:00am-5:00pm. The suggested admission is $7.00 for adults, $5.00 for seniors, $2.00 for students and $15.00 for families. Web: www.cahokiamounds.org (official website).\nOf the many mound sites in the vicinity of St. Louis, almost all of the others have been absorbed or destroyed by modern construction. One exception is the Sugarloaf Mound which was saved from destruction by the intervention of the Osage Nation, who recently purchased the site.","More to Explore\nAncient Chaco\\' s New History\nby Stephen H. Lekson\nChaco is an arid, barren, sandstone canyon in the middle of nowhere. But a millennium ago, in the eleventh and twelfth centuries A.D., ancient peoples not only survived there, they thrived and created an amazing city. Chaco\\'s ruins awe us even today. The people we call the ancestral Pueblo (also \"Anasazi\") built monumental political and ceremonial buildings that towered, literally and figuratively, above anything previously seen in the Southwest.\nThe ruins are preserved in Chaco Culture National Historical Park, 175 kilometers (110 miles) west of Santa Fe, New Mexico. Ancient Chaco Canyon was the center of a regional system covering over 100,000 square kilometers (40,000 square miles). The principal excavated ruins are Pueblo Bonito, Chetro Ketl, Pueblo del Arroyo, Pueblo Alto, and Kin Kletso. Hundreds of other buildings, large and small, dot the canyon floor. The largest buildings, called \"Great Houses,\" are usually associated with \"Great Kivas\"-16-meter-diameter (53-feet) subterranean ceremonial chambers, such as Casa Rinconada.\nPueblo Bonito was the largest Great House. Construction began as early as 850 and continued until about 1125; the D-shaped building stood five stories tall, covered .8 hectares (2.2 acres), and contained over 650 rooms, 45 smaller \"kivas,\" and two Great Kivas. The carefully coursed sandstone masonry walls were up to 80 centimeters (2.6 feet) thick. Over 25,000 pine roof beams were transported from distant forests. Chetro Ketl and the unexcavated ruins of Una Vida and Penasco Blanco were almost as large as Pueblo Bonito and had similar construction histories.\nMany thousands of turquoise beads, pendants, and inlays were found at Chaco Canyon sites. The turquoise came from mines near Santa Fe and elsewhere. Macaws and parrots, copper bells, and sea shells were imported from Mexico, up to 1,000 kilometers (more than 600 miles) to the south. Kitchen pottery and stone for tools were also brought from distant sources. Much of the pottery at Pueblo Alto, for example, was made two days\\' walk to the west.\nRich burials at Pueblo Bonito suggest to some archaeologists the existence of elite leadership, presumably the lords of the regional system indicated by a network of \"roads\" and over 150 smaller Great Houses or \"outliers.\" The roads were 9 meters (30 feet) wide, arrow-straight constructions, up to 60 kilometers (36 miles) long-probably landscape monuments rather than transportation corridors.\nChaco Canyon was much like the rest of the ancestral Pueblo area until about 900. What some archaeologists call the \"Chaco Phenomenon\" began, in the early-tenth century, as a local center in northwest New Mexico, perhaps serving as a food storage and redistribution center in a highly uncertain environment. About 1020, the scale of the Chaco region and Chacoan building expanded, eventually encompassing most of the ancient Pueblo world. Contacts with Mexico intensified after Chaco assumed regional preeminence.\nAt Chaco\\'s height in the early-twelfth century, between 2,000 and 5,000 people may have lived in the canyon, though some believe even the lower figure is way too high. A drought, beginning about 1130, coincided with the end of monumental building at Chaco Canyon and the beginning of a second major center, 85 kilometers (50 miles) north of Chaco Canyon, at Aztec Ruins National Monument.\nFrom about 1000 to 1150, Chaco was the \"capital\" of the Pueblo world. Some scholars believe that Chaco was an economic, political, and fundamentally ceremonial center that transformed many slow centuries of Pueblo village life into a coherent regional system. Chacoan buildings, even in ruins, astonish us with their size, complexity, and beauty. At its height, Chaco was a ceremonial city of unprecedented wonder. Its monumental structures housed rooms full of bright imported feathers, shell, and turquoise, all used in ceremonies staged on a ritual landscape of vast geometric symmetry. Surrounding these immense villages were sophisticated irrigation and water control systems that transformed the dry canyon into a tapestry of corn fields, Great Houses, and monuments.\nEvents in Chaco Canyon had far-reaching effects, both in its time and in all subsequent Pueblo history. Smaller, less formal versions of the Great Houses of Chaco Canyon were erected all over the Colorado Plateau in the eleventh, twelfth, and even the thirteenth centuries. Acoma and many other Pueblos recall Chaco as the seminal \"White House\" and regard it as a sacred place. Important Hopi clans originated there. Chaco saw dramatic events -recounted in origin stories-which shaped Pueblo life, society, and religion forever after. The many Navajo people, who live all around Chaco today, tell stories of the astonishing things that happened there long ago. There was nothing remotely like Chaco in the eleventh- and twelfth-century Southwest. Chaco was epochal.\nSo much for the hype. Some archaeologists have more modest readings of Chaco, as we shall see. But Chaco Canyon was special. The hundreds of archaeologists who have flocked to Chaco over the last century attest to Chaco\\'s particular fascination. Chaco was evidently so central to Southwestern prehistory, yet so mysterious in the details of its history, that archaeologists came to call it the \"Chaco Phenomenon.\"\nWe may never resolve the Chaco Phenomenon. Who built these amazing buildings? How many people lived in this forbidding canyon? How did people survive in Chaco-much less create this astonishing civilization? What effects did Chaco have on the rest of the Southwest? These are only a few of the questions that people who have seen this wonder have asked and tried to answer. Scholars of Southwest archaeology have argued about these issues for decades.\nArcheaology Southwest, Center for Desert Archaeology, Winter 2000, Volume 14,"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:dc1aed93-7837-4eb4-b23b-bf308d13a053>","<urn:uuid:777f099c-9ad1-4894-8eb8-a430568c1f15>"],"error":null}