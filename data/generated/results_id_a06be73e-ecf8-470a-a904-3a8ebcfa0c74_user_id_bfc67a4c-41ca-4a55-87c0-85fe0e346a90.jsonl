{"question":"¿Quién ganó la Palma de Oro en Cannes 2023? How is the winning film described?","answer":"Anatomy Of A Fall won the Palme D'Or at Cannes 2023. It was directed by French director Justine Triet. The film is a Hitchcockian mystery thriller about a woman, played by Sandra Hüller, who is accused of murder when her husband dies under suspicious circumstances.","context":["The first titles in the running for the 2023 European Film Awards have been revealed by the European Academy, including Cannes premieres Anatomy Of A Fall, How To Have Sex, The Old Oak and Firebrand.\nAnatomy Of A Fall won the Palme D’Or for French director Justine Triet at Cannes. The Hitchcockian mystery thriller is about a woman, played by Sandra Hüller, accused of murder when her husband dies of suspicious causes. Marie-Ange Luciani and David Thion produce.\nHow To Have Sex won the top Un Certain Regard prize at Cannes for debut UK filmmaker and Screen Star of Tomorrow 2021 Molly Manning Walker. The feature follows a group of teenage girls on a rite-of-passage clubbing holiday, shot in Greece. It is produced by Wild Swim’s Ivana MacKinnon and Emily Leo alongside Heretic’s Konstantinos Kontovrakis, who produced Triangle Of Sadness, which swept the board with four big wins at the 2022 ceremony.\nThe Old Oak is directed by UK filmmaker Ken Loach, made with his long-time creative partners writer Paul Laverty and producer Rebecca O’Brian of Sixteen Films. The film portrays the struggle of a landlord to hold onto a pub called The Old Oak as the only remaining public space where people can meet in a once thriving mining community, following the arrival of Syrian refugees who are placed in the village without any notice.\nFirebrand stars Alicia Vikander as Katherine Parr, the sixth wife of Henry VIII, played by Jude Law. Brazilian Karim Aïnouz directs, with Brouhaha Entertainment producing.\nFurther Cannes titles in the mix include Fallen Leaves, La Chimera, The Animal Kingdom, Blackbird Blackbird Blackberry, The Goldman Case and Close Your Eyes.\nEligible for the European Film Awards are European feature films which, among other criteria, had their first official screening between June 1 2022 and May 31 2023 and have a European director, with some stipulations for if a non-European director is Europe-based or has worked in the European film industry for five consecutive years or more.\nFurther titles will be announced in September.\nIn the coming weeks, the 4,600 members of the European Film Academy will start to watch and vote for the selected films. Based on the votes of the participating members the nominations in the feature film categories European film, director, actor, actress and screenwriter, as well as in the category European documentary, will be made public on November 7. The winners will be announced at an awards ceremony that takes place on December 9 in Berlin.\nAdditionally, an eight-member jury will decide on the award recipients for European cinematography, editing, production design, costume design, make-up and hair, original score, sound and visual effects. There will be no nominations in these categories.\nEFA Feature Film Selection 2023 – Part 1\n20,000 Species of Bees (Sp)\nDir. Estibaliz Urresola Solaguren\nDir. Christian Petzold\nAnatomy Of A Fall (Fr)\nDir. Justine Triet\nBehind The Haystacks (Greece-Ger-North Macedonia)\nDir. Asimina Proedrou\nBlackberry Blackbird Blackberry (Switz-Geo)\nDir. Elene Naveriani\nClose Your Eyes (Sp-Arg)\nDir. Víctor Erice\nFallen Leaves (Fin-Ger)\nDir. Aki Kaurismäki\nDirs. Sam H. Freeman, Ng Choon Ping\nDir. Karim Aïnouz\nHow To Have Sex (UK-Greece)\nDir. Molly Manning Walker\nDir. Marco Bellocchio\nLa Chimera (It-Fr-Switz)\nDir. Alice Rohrwacher\nSafe Place (Cro-Slovenia)\nDir. Juraj Lerotic\nDir. Marija Kavtaradze\nThe Animal Kingdom (Fr)\nDir. Thomas Cailley\nThe Goldman Case (Fr)\nDir. Cedric Kahn\nThe Happiest Man In The World (North Macedonia-Bos & Her-Belg-Slovenia-Cro-Den)\nDir. Teona Strugar Mitevska\nThe Old Oak (UK-Fr-Belg)\nDir. Ken Loach\nThe Teachers’ Lounge (Ger)\nDir. Ilker Çatak"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9ecd458f-8452-488c-a70d-ef3bb610a35b>"],"error":null}
{"question":"What specific technological and natural resources are managed in San Diego Airport and Amazon indigenous lands?","answer":"San Diego Airport manages technological resources including variable-speed escalators, 400-hertz aircraft power supplies, LED lighting systems, and solar panels generating one megawatt of electricity. The facility also employs white reflective roofing and drought-tolerant landscaping. In Amazon indigenous territories, the primary resources managed are rivers, forests, and mineral deposits, particularly gold. The Kayapo people specifically manage 23 million acres of rainforest and savanna, protecting their territory's biodiversity, water resources, and carbon storage capacity of approximately 1.3 billion metric tons, while fighting against illegal gold mining operations that contaminate water sources with mercury.","context":["The building team for San Diego International Airport’s Terminal 2 expansion and renovation didn’t set out to create the world’s first LEED Platinum-certified commercial airport terminal. The initial goal was LEED Silver, but with the cost of significant sustainable elements within budget, architects and other specialists at HNTB, AECOM, and general contracting joint venture Turner/PCL/Flatiron realized that they could aim higher.\n“It was obvious at an early date that we could reach Gold,” says Bob Bolton, the airport’s director of design and construction. “Near the end of the design phase, it started to look like we could get close to Platinum. We didn’t want to go public with that, though, because you never know if you can achieve it until you get official notification from the USGBC.” Guided by rigorous cost-benefit analyses, this Platinum project demonstrates that LEED’s top designation is within reach for airport terminal projects anywhere.\nIn large airport terminals, electric motors for escalators, baggage handling equipment, and pumps and air-handlers for HVAC systems are among the biggest energy hogs. Terminal 2 employs highly efficient variable-speed models for all of these systems. “We received special state approval to install new escalators that slow down when not in use,” Bolton says of the Otis escalators, which use sensors to determine when a person is approaching. (Importantly, the escalator increases speed before, not after, the person sets foot on the first step—a significant safety feature.)\nTwo other green systems are unique to airport applications: a 400-hertz aircraft power supply and pre-conditioned air supplies. Both feed airplanes at the gates from the building, eliminating the need for planes to run engines between arrival and departure. This both saves jet fuel and improves air quality.\nThe building was designed with rooftop solar panels in mind, but how they would be paid for wasn’t settled until later in the project’s development. The most cost-effective option turned out to be a 20-year power-purchase agreement with Borrego Solar Systems. The airport authority avoids spending capital to buy the panels, and its electric rates are locked in for the full 20-year term.\nThe solar array will provide one megawatt of electricity to the building, accounting for 12.5 percent of the terminal’s electrical needs for an estimated $4-8 million of savings over the next 20 years. Thanks to the energy-efficiency and solar power elements, the project received the maximum 17 points in LEED’s Energy and Atmosphere category.\nSome leading-edge green options did not make the cut. “We did an analysis very early in the program on geothermal and thermal energy storage for hot water, but the payback was not optimal,” says Tom Rossbach, a principal at HNTB. A rainwater capture system couldn’t be justified either, given San Diego’s low annual rainfall.\nSome systems, by contrast, were no-brainers: “Lighting controls can pay off in as [little] as nine months,” Rossbach says, and “LED lighting can pay for itself in one year.”\nOther notable green features include white reflective roofing, drought-tolerant landscaping, low-flow plumbing fixtures, and shaded fenestration for the large atrium, with darker glass near the top and lighter glass at the bottom to reduce heat gain.","“The Brazilian Amazon has the highest concentration of indigenous peoples in the world. Recently, the Brazilian government sent a bill to Congress to regulate commercial mining in indigenous lands.”\n“This work analyzes the risks of the proposed mining bill to Amazonian indigenous peoples and their lands. To evaluate the possible impact of the new mining bill, we consider all mining license requests registered in Brazil’s National Mining Agency that overlap indigenous lands as potential mining areas in the future. The existing mining requests cover 176 000 km2 of indigenous lands, a factor 3000 more than the area of current illegal mining. Considering only these existing requests, about 15% of the total area of ILs in the region could be directly affected by mining if the bill is approved. Ethnic groups like Yudjá, Kayapó, Apalaí, Wayana, and Katuena may have between 47% and 87% of their lands impacted. Gold mining, which has previously shown to cause mercury contamination, death of indigenous people due to diseases, and biodiversity degradation, accounts for 64% of the requested areas. We conclude that the proposed bill is a significant threat to Amazonian indigenous peoples, further exposing indigenous peoples to rural violence, contamination by toxic pollutants, and contagious diseases. The obligation of the government is to enforce existing laws and regulations that put indigenous rights and livelihoods above economic consideration and not to reduce such protections.”\nThis abstract comes from a study out of the Environmental Research Letters Journal published on October 9th, 2020. We encourage you to read the full article by clicking below.\nFollow Director Todd Moen as he journeys into the Kayapo Territory to discover the Kayapo Project’s sustainable enterprise with Untamed Angling\nIs there a way for us to get a glimpse of paradise on Earth without drastically altering its nature, without destroying it forever? A small Kayapo community which lives along the banks of the Iriri river might have the answer. Deep in the heart of the Menkragnoti Indigenous Territory, in one of the most isolated settlements of the tropical world lies Kendjam. A small village established only in 1993 by chief Pukatire who brought his followers away from the destructive influences of alcohol and the extractive industry with the goal to create a deeply traditional community.\nPukatire was once quoted as saying, “we only need the white man for three things: eyeglasses, flip-flops and flashlights”. And while he feels as strongly about preserving his people’s traditional lifestyles as he did when he founded Kendjam, he is now embracing a new sustainable model that can benefit his people without compromising the natural world upon which they depend. Today, thanks to a partnership between Associacao Floresta Protegida (Kayapo NGO), Untamed Angling and several Kayapo communities, outsiders can visit the rainforest in a way which is respectful to the environment and equitable towards the indigenous hosts. Instead of jeopardizing the traditional way of life of the communities, the project aims at further bolstering the pride the Kayapo take in their way of life, in their ancestral wisdom and profound knowledge of the surrounding environment.\nYou don’t need to travel to the heart of the Amazon in order to get an idea of what it is to be in the heart of the Amazon rainforest. Whether you’re interested in fly-fishing or not, this brilliant short documentary produced by Todd Moen, a filmmaker affiliated from CatchMagazine, will take your spirit far from the confines of your modern life and will give you a glance of a world and a culture that have managed to survive despite all the mounting threats.\nI hope that watching this video gave you a better sense of how important it is for us to do whatever we can to protect this jewel of biodiversity and traditional culture. Without their rivers, but more importantly, healthy rivers, the Kayapo would not be able to have access to the healthy fish stocks and other resources that their protected lands offer them generation after generation.\n“We want to tell the kubẽ (white men) to listen, to respect our rivers, our forests, our land for where there is mining it gets worse for us because we can get sick. The relatives who live where there is gold mining are already sick. There is a lot of mercury contamination, even fish. That’s why I don’t want to mine in my village”. – Oro Muturua (Kayapó leader)\nIf you want to help the Kayapo in their fight for the protection of these pristine waters from the poisonous pollution of gold mining consider supporting their fundraiser. The funds will support the establishment of two guard posts in order to further the protection of Kayapo’s border entrances at the shores of the Iriri and Xingu rivers.\nThe Spirit of Survival – Written by: LINDSAY RENICK MAYER from Global Wildlife Conservation Original Blog\nKayapo Indigenous People Call on World to Help Protect Amazonia Against Extractive Industry, Brazilian Government\nThat was how the Kayapo Indigenous people approached the illegal goldmining camp that had, for months, been destroying part of the Amazon rainforest, home to countless animals and plants, and polluting the nearby river in the Kayapo’s ratified territory of Bau.\nAs 17 Kayapo came upon the camp in mid-October, after traveling for two days by boat and then by foot, any noise would have been drowned out anyhow by the goldminers’ hydraulic machines. Their actions resulted in the peaceful removal of the trespassers from the land, which was accessible to these outsiders only by plane, and the complete dismantling of the camp.\n“The area the goldminers destroyed is very large and the streams are badly damaged,” said Bepmoro-I, from the village of Bau located in Bau Indigenous Territory. “It’s awful there. But we blocked off the airstrip and so now the streams and forest will begin to recover. If goldminers come back, we will go and remove them again.”\nKayapo wait with goldminers from the illegal “Novo Horizonte” illegal gold mine in the Kayapo Bau territory. The air strip supplied their camp and here the goldminers wait to be picked up by their employer.\nThis is not the first time the Kayapo have had to remove invaders from 23 million acres of their rainforest and savanna territory in the southeastern region of the Brazilian Amazon, an area the size of the state of Virginia. For more than 40 years, the Kayapo have fought off many outsiders looking to exploit their natural resources. They have done so with the partnership of multiple NGOs, including Conservation International, Environmental Defense Fund, and GWC partner, the International Conservation Fund of Canada.\nThe removal of the goldmining camp came against the backdrop of a Brazilian Federal Government that has been considering a bill this year that would effectively legalize goldmining and other extractive industries in Indigenous territories across Brazil. This marks the latest in an onslaught of threats to Brazil’s Indigenous People’s cultures, lives and land, and to the wildlife and ecosystems that they protect.\nA Message to the World\nThe Kayapo are anything but silent against the congressional bill, Proposed Law 191/2020, that could significantly weaken protection of Amazonia, and they want the world to know what is going on.\nMore than 6,000 Kayapo from 56 communities of the Bau, Capoto/Jarina, Kayapo, Las Casas and Mekragnoti Indigenous Territory, the Indigenous organizations Associação Floresta Protegida, Instituto Kabu and Instituto Raoni recently published a declaration expressing their opposition to the bill.\n“How could we be in favor of such an activity that profoundly negatively impacts our environment, society and communities?” the letter asks. “How could we deprive our children and grandchildren of a vital territory that supports our livelihoods, autonomy, customs and traditions, as guaranteed by the federal constitution? We appeal to all Brazilians and international society to support our struggle to protect our forest and demand that the government respect the federal constitution and our right to use our territories according to our customs; as well as the right of all people to an ecologically balanced environment.” [READ THE FULL STATEMENT FROM THE KAYAPO]\nBrazilian President Jair Bolsonaro introduced the proposed law in February of 2020 to open up demarcated territories to the extractive industries of mining, oil and natural gas. Two other proposed laws would have similar devastating effects: one aimed at the establishment of a general environmental licensing law, which would essentially allow industry to easily obtain licenses for environmentally damaging extractive activities easily—even through self-declaration (PL3729/2004); and another that would grant amnesty to invaders and in essence encourage deforestation and land-grabbing (PL 2633/2020).\n“I have long admired the great courage of the Kayapo and their undying commitment to protecting their traditional lands, ever since I first visited them in 1991 with Barbara Zimmerman to help her establish her long-running program to work with these amazing people,” said Russ Mittermeier, GWC Chief Conservation Officer, who has visited the Kayapo lands and other parts of the Xingu region a number of times over the past three decades. “If the Brazilian government opens indigenous territories such as those of the Kayapo and their neighbors to legal goldmining and logging, this could signal a death knell for the magnificent forests of Amazonia and the great and wonderfully diverse Indigenous Peoples who call it home. The vast forests of Amazonia are critical to the health of our planet, and the Kayapo and their fellow indigenous peoples are its most important guardians.”\nWe Won’t Give Up’\nThe Kayapo protect more than 2,000 kilometers of heavily threatened borders around their territory. Kayapo land represents the last large block of forest in the southeastern Amazon and stores an estimated 1.3 billion metric tons of carbon. It is hard to understate the critical importance of the Amazon rainforest—one of the world’s five designated High Biodiversity Wilderness Areas and home to one-quarter of Earth’s terrestrial biodiversity—to the health of the planet, and the critical role that the Kayapo and other Indigenous communities play in protecting it. An estimated 20 million Indigenous people from more than 350 Indigenous groups call the forests of Amazonia home and depend on their natural habitats and resources for their livelihoods and culture.\n(Photo by Antonio Briceno)\nYet the forests of Amazonia continues to come under serious threats. Deforestation in 2019 and 2020 was the highest it has been since 2008 and represents a doubling in forest loss over 2012. Amazonia has experienced some of its worst fire seasons in the last two years, a result of previous deforestation, primarily for the expansion cattle ranching and cattle feed crops (soybeans), leaving a drier local microclimate. The fires themselves are often purposely started to clear land for agriculture, mostly cattle and cattle feed for export to the United States, EU, China and other countries.\n“The Kayapo face today face what Native America Tribes faced in the mid-1800s: an infinitely more numerous and better armed capitalist society building along their borders and slavering to devour their land no matter the law,” said Barbara Zimmerman, director of the Kayapo Project for the International Conservation Fund of Canada and the U.S.-based Environmental Defense Fund. “The difference is timing: in the 21st century there exist indigenous rights, international media, the internet and NGO Indigenous allies. We are about to see whether these factors help the Kayapo to save themselves and a vast tract of Amazonia forest upon which their culture and livelihoods are based. If the Kayapo can win, if they can hold out, then I think that anything can be achieved in the conservation of our planet.”\nFor the Kayapo, beating these bills, which the Brazilian Congress could vote on as early as February, and continuing to protect the forests of Amazonia is going to depend on the willingness of the rest of the world to help safeguard this irreplaceable place. But no matter what, the Kayapo say that they are not going to give up.\nPhoto by Cristina Mittermeier\n“We won’t stop doing this work. We won’t give up. We are going to keep fighting,” Bepmoro-I said. “We would like the entire world to see our effort, the work of the Kayapo people to protect our land and our culture—and help us with the resources we need to continue protecting our land and rivers.”\nYou can help. Make a donation to the Kayapo Fund today at Kayapo.org"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:18242e47-f197-44dd-89e1-969015510d10>","<urn:uuid:15310b56-8748-49a6-97fa-fe7fd816c150>"],"error":null}
{"question":"How has the transformation of Mollicy Farm evolved from farmland to restored floodplain?","answer":"The transformation began in the 1960s when Mollicy Farm, a 20,000-acre tract in Morehouse Parish, was cleared for farmland and surrounded by a 30-foot levee. The U.S. Fish and Wildlife Service acquired the land in the 1990s as part of the Upper Ouachita National Wildlife Refuge. During the farming period, streams were filled in or straightened for fast field drainage. The restoration process began about five years ago with strategic levee cuts, followed by restoring the interior hydrology. Scientists reconstructed the natural stream paths with meandering curves and varying depths. The final phase involved reconnecting Mollicy Bayou to the Ouachita River for the first time in 40 years, after allowing new hardwood seedlings to stabilize the soil along the banks.","context":["Chris Rice of The Nature Conservancy stood on top of the last earthen dam holding back the Mollicy Bayou from its natural path that was blocked more than 40 years ago.\n\"This is the last cut to be made that will let Mollicy Bayou flow back into the Ouachita River,\" Rice said. \"It's the first time in 40 years it will flow like it used to from the upland pine woods. Now that's pretty cool.\"\nWhen U.S. Fish and Wildlife Service equipment engineers Mike Simmons, Ronnie Tidwell and Kyle Cheeseman make that final cut this week, it will mark the completion of what is believed to be the largest floodplain reconnection project in North America.\nScientists and engineers with The Nature Conservancy and the U.S. Fish and Wildlife Service painstakingly excavated the stream to its natural path, complete with meandering curves, sloped banks and varying depths.\nThat phase was complete last year following the reconnection of the smaller stream Shiloh Creek, but both ends of Mollicy Bayou were plugged until a new crop of natural hardwood seedlings could take root along the banks for soil stabilization.\n\"This is a functioning floodplain now,\" Rice said.\nMollicy Farm, a 20,000-acre tract located in the northwestern corner of Morehouse Parish, was cleared for farmland and surrounded by a 30-foot levee in the 1960s before the U.S. Fish and Wildlife Service acquired the land in the 1990s. It's part of the Upper Ouachita National Wildlife Refuge.\nThe partners first made strategic cuts in the levee about five years ago and then began to work on the interior hydrology of the project. When the floodplain was cleared for farmland the streams were filled in or straightened to allow fast drainage from the fields.\nBut the rushing waters from those straight flumes also flushed loads of sediment and nutrients directly into the Ouachita River, which harmed water quality.\nBy restoring the floodplain's natural interior plumbing the land will not only act as storage during floods, protecting populated areas like Ouachita Parish, but also act as a filter for the water when it drains back into the river following floods.\n\"Restoring the internal hydrology as it was intended to function will allow the water to recede from the floodplain much slower, which will allow the sediments and nutrients to be deposited over the entire floodplain instead of dumped into the river,\" said Brett Hortman, manager of the U.S. Fish and Wildlife's North Louisiana Refuge Complex.\n\"The sediment load into the river has already decreased over the past five years since the levee was first breached by a flood,\" Hortman said. \"We believe the hydrology and the new trees will accelerate the natural filtering process.\"\nIt's been a signature project, said Keith Ouchley, director of The Nature Conservancy of Louisiana and Mississippi.\n\"We've given it all the help we know how to put it on the right track and have high confidence that it will pay dividends for decades to come,\" Ouchley said. \"I can't wait to one day sit on the banks of the Mollicy Bayou under the shade of a cypress tree.\"\nSimmons, one of the equipment engineers who had to make the theoretic plans work on the ground, said he sometimes sits back and visualizes the way the floodplain once was and what it can be again.\n\"We just tried to put it back a little at a time,\" Simmons said. \"And it's coming back. I'd love to be able to see it in big timber again.\"\nThe Nature Conservancy and U.S. Fish and Wildlife Service have provided the palette. Now, the experts said, it's up to Mother Nature to finish the job.\n\"Once it's reconnected, we have to let nature take its course, and nature usually does a pretty good job,\" Hortman said.\nFollow Greg Hilburn on Twitter @greghilburn1."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:47033f16-d01e-42c1-87a9-ea45fce5e054>"],"error":null}
{"question":"What are the drawbacks of using waxes in stone conservation?","answer":"The main drawbacks of waxes in stone conservation are their tendency to soften at high temperatures and their propensity to collect dirt.","context":["Get this from a library! Alkoxysilanes and the consolidation of stone. [George Wheeler; Getty Conservation Institute.]. Stone is one of the oldest building materials, and its conservation ranks as one of the most challenging in the field. The use of alkoxysilanes in the conservation. Permalink: ; Title: Alkoxysilanes and the consolidation of stone / George Wheeler ; with annotated bibliography by.\n|Country:||Turks & Caicos Islands|\n|Published (Last):||8 December 2013|\n|PDF File Size:||7.53 Mb|\n|ePub File Size:||12.81 Mb|\n|Price:||Free* [*Free Regsitration Required]|\nWhen choosing a solvent and catalyst to increase the rate of polymerization, one should be aware of the fact that they must not react with the substrate and leave any soluble residue in it which can cause future decay. Inorganic and Metal-Organic Compounds, ” p. Applications were a few days apart In sone, a series of injections of an acryUc emulsion were made through the preexisting cracks to strengthen the inner part of the wall.\nOn the other hand, Rolf Wihr advocated the use of a spray system providing a continuous rain of consolidant and recirculating run off material in order to obtain a deeper penetration, based on observation that stone gradually becomes fully soaked in rain. Acrylic Polymers 13 1. The earliest attempt to use silicic ether or ethyl silicate for stone conservation was made in an meeting of the Royal Institute of British Architects by A.\nGetty Publications Virtual Library. A Long-Term Evaluation,” pp. The other drawbacks of waxes are their tendency to soften at high temperatures and to collect dirt. Since Brethane was a newly ztone consolidant, its use was recommended only for cases where decay was advanced.\nWhen necessary, absorbent pads of tissue should be placed below the treatment area to prevent excess silane al,oxysilanes running down. Zinsmeister, Conservation of Natural Stone: Alkoxysilane-based formulations have since become the material of choice for the consolidation of stone outdoors.\nThe alkoxy groups R of alkoxysilanes are capable of reacting with hydroxyl groups. A Review of Natural Stone Preservation.\nDon’t have an account? As a consequence of the combined effects of chemical and mechanical weathering, stone can lose its cohesion to such a degree that its physical survival is imperilled and a treatment is necessary to restore its integrity. Horie, Materials fo r Conservation: A certain optimism has resulted fix m increased research on the development of different techniques and the materials themselves.\nIt is the siUcon-oxygen-silicon linkages that produce the off and strengthening effect.\nAlthough the chemical affinity of silanes for calcareous stones is minimal, a network of polymer may fill the intergranular spaces of the stone and having a consolidating effect without forming a chemical bridge between the grains.\nSiliceous consolidants and alkaline earth hydroxides are the primary inorganic 3 materials used in stone consolidation. Inorganic and Metal-Organic Compounds,” in L. Several types of stone, brick and clay have reactive hydroxyl groups on their surface.\nDuring handling or application, a consolidant should not introduce health and safety hazards. Besides providing a short-term evaluation of the recommended consolidation treatment slaboratory programs have the advantage that test conditions can be adjusted to simulate the specific environment and climate to which the consolidant will be exposed.\nDetails Additional Physical Format: Restoration and Preservarinn Ethingen bei Boblingen: Preprints of the Contributions to the Paris Congress, September The amount of consolidant deposited in each of the treated masonry samples is determined to evaluate the effectiveness of the treatment and efficiency of the application procedures.\nThe product was reported to reduce the rate of stone decay very substantially. However, they do not penetrate easily into small pores due to the large size of their molecules and tend to accumulate near the surface.\nWhat makes the alkoxysilanes so attractive is that some of them, notably the alkylalkoxysilanes and silicic acid esters, meet several of the performance requirements.\nA study on the polmerization of methyl methacrylate in sandstone by P. After more than twenty years of experience, alkoxysilanes have so far proved to be effective for the consolidation of sandstones, limestones and earthen building materials. Furthermore, on-site testing sgone the rate of application, rate of consumption and exact application procedures to be used under field conditions, such as the number of applications and how the products are applied.\nChemical Structure of Alkoxysilanes 31 2. However, it is extremely difficult to control relative humidity in the field.\nAlkoxysilanes and the consolidation of stone\nThe following tests are recommended for a comprehensive and reliable laboratory evaluation of untreated masonry samples to determine the suitability of the masonry for consolidation treatment: Elsevier Science Publishers B. Partially polymerized ethyl snd is less expensive than monomeric ethyl silicate.\nAlkoxysilanes and the Consolidation of Stone. The stone consolidants reviewed in this chapter are divided into four groups according to their chemistry.\nFull text of “Alkoxysilanes consolidation of stone and earthen building materials”\nPreview this item Preview this item. The following tests are suggested for the laboratory evaluation of treated masonry samples: Ef oxies have been used to consolidate limestone, sandstone, and marble in addition to their use as adhesives, paints, varnishes, and constituents of synthetic mortars and concrete.\nThis phase of the test wall project is scheduled for completion in These products are in the form of one-pack materials.\nAuthors theorize that a slow dehydration takes place which takes the clay to a form that prevents the water-based curing mechanism from occuring. In response to this, numerous research projects, publications and conferences dealing with stone have been facilitating the worid wide exchange of information.\nThe deposition, that will bind stone particles together, can result from evaporation of alkoxysiilanes solvent or chemical reactions with the stone.\nTorraca indicates that Paraloid B shows a good resistance to the ageing process and its life span is years alkoxyailanes external exposure.\nThey polymerize by means of heating with o initiator, ultraviolet radiation or gamma radiation. Consolidation a nd Protectionp. Controversy in the publications over the use of alkoxysilanes on stone and adobe conservation might partially result from the lack of research and an incomplete understanding of the complexity of alkoxysilane chemistry."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:bb152eb2-2111-4bb1-8ced-33d20121485c>"],"error":null}
{"question":"What is the prevalence of dust mite allergies in industrialized countries, and what preventive measures can be taken to minimize exposure?","answer":"Allergic asthma has reached epidemic proportions in many industrialized countries, with studies showing that up to 80 percent of people with asthma are allergic to house dust mites. To prevent exposure, several measures can be implemented: using allergen-proof bed covers on mattresses and pillows; washing all bedding weekly in hot water and drying in sunlight; maintaining relative humidity at about 40% using air conditioners or dehumidifiers; regularly cleaning curtains and upholstered furniture; dusting with damp towels; and avoiding carpeting in favor of wood, tile, or vinyl flooring. Regular cleaning of washable stuffed toys is also recommended.","context":["Washington, Feb 16 (ANI)): Scientists have discovered a new molecule, which appears to play a role in damping down the body's allergic response to house dust mite.\nThe University of Nottingham's discovery shed light on how the body's immune system identifies and reacts to allergens, which could ultimately pave the way for developing new therapies or treatments for preventing allergies.\nThe molecule DC-SIGN, identified by a team of immunologists led by Dr Amir Ghaem-Maghami and Professor Farouk Shakib in the University's School of Molecular Medical Sciences, can be found on the surface of the immune cells which play a key role in the recognition of a major allergen from house dust mites called Der p 1, a leading cause of asthma in northern Europe.\n\"There has been a sharp increase in the prevalence of allergies over the past few decades and allergic asthma among children has reached epidemic proportions in many industrialised countries, including the UK,\" said Dr Amir Ghaem-Maghami.\n\"Despite improvements in patient care, mortality and morbidity of allergic asthma has remained high, and most therapies target symptoms rather than curing the condition.\n\"Many people with asthma are highly sensitive to airborne allergens such as those from house dust mite - in fact, many studies have shown that up to 80 per cent of people with asthma are allergic to house dust mite.\n\"A better understanding of how the interaction between allergens and the immune system triggers allergy is vital if we are to develop more effective and efficient treatments for this debilitating condition,\" he explained.\nAllergy is a disorder caused by the body's immune system reacting to harmless substances found in the environment, known as allergens. Believing itself under attack, the immune system produces an antibody called IgE, which eventually leads to the release of further chemicals (including histamine) by certain immune cells, which together cause an inflammatory response and the classic symptoms of allergy -itchy eyes, sneezing, runny nose and wheezing.\nThe Nottingham work has focused on the role of DC-SIGN, a receptor found on the surface of antigen presenting cells. These cells are among the first cells in the immune system that come into contact with allergens.\nThe team found that DC-SIGN binds to major allergen from house dust mite (Der p 1) and dogs (Can f 1) and seems to play a regulatory role in the allergic response to house dust mite allergens. The binding of allergen to DC-SIGN on antigen presenting cells seems to promote a mechanism that could dampen harmful immune responses to allergens.\nThis is opposite to the role of another allergen reception - the mannose receptor - that has previously been identified by the Nottingham group.\nThe discovery shows that DC-SIGN could potentially play a beneficial role in regulating immune responses to environmental allergens.\nThe finding has been published this week in the Journal of Biological Chemistry. (ANI)\nRead More: Lady Harding Medical College | Medical College | Amir Khas | Gsvm Medical College | Medical College Po | Amir Nagar | B.r.d Medical College | Mlb Medical College So | Medical Campus | Govt. Medical College | Madras Medical College | Kilpauk Medical College | Thanjavur Medical College So | Pariyaram Medical College | Calicut Medical College Mdg | Alappuzha Medical College | R.g.kar Medical College Po | Assam Medical College | Silchar Medical College | K.r.high School","What is Dust Mite Allergy?\nDust Mites are very small bugs which get mixed with house dust and feed on dead skin cells which are shed by people. These bugs tend to adapt to virtually all environments and feed on the dead cells that the people shed everyday. Dust Mites thrive in warm temperatures and hence can be found around the beds or in places where woolen clothes are kept. An individual may not be aware of this but it unknowingly inhales waste products of dust mites.\nDust mites may not affect for some time but with repeated exposure the immune system gets triggered and identifies the inhaled waste products of dust mites as invaders and starts to produce antibodies which release chemical called histamines which trigger symptoms like sneezing, watery and itchy eyes which are classic symptoms of Dust Mite Allergy.\nWhat Causes Dust Mite Allergy?\nDust Mite Allergy is caused by the action of the immune system of the body. When an individual is repeatedly exposed to waste products of dust mites which can be present anywhere in the house, the immune system starts to identify those substances as harmful for the body and starts producing antibodies to fight the substances. This results in releasing of chemical called histamine which in turn produces symptoms of itchy eyes, runny nose, and sneezing which are the classic presenting features of a Dust Mite Allergy.\nPeople may think that they have a clean house but in fact there may be places where some dirt is still there and that is the breeding ground for dust mites. This is especially under the carpeting, bedding, cushions which all hold moisture from the air and allow these bugs to thrive ultimately resulting in Dust Mite Allergy.\nWhat are the Symptoms of Dust Mite Allergy?\nThe symptoms caused by dust mite allergy may range from mild to severe. some of the symptoms of dust mite allergy are:\n- Runny nose\n- Postnasal drip\n- Itchy skin\n- Nasal congestion\n- Sinus pressure which at times may cause facial pain\n- Water eyes\n- Redness in the eyes\n- Scratchy throat\n- Persistent cough\n- Swollen eyes which may have a tinge of bluish discoloration just beneath the eyes\n- Trouble with sleeping due to the above symptoms.\nIn case if an individual has asthma and additionally has dust mite allergy then apart from the above mentioned symptoms he or she may also experience:\n- Chest pain or tightness\n- Problems with breathing\n- Problems with talking\n- Severe asthma attack.\nHow is Dust Mite Allergy Diagnosed?\nIn order to identify whether an individual has dust mite allergy, he or she needs to consult with an allergist who will perform a series of tests. The best test to identify whether an individual is allergic to dust mites is by performing a skin prick test. In this test, common allergens like products released by dust mites are used and applied to the skin in the arm or the back with tiny punctures made. If an individual has dust mite allergy, he or she will develop hives or a raised bump at the site where the allergen was applied. This will confirm the diagnosis of Dust Mite Allergy.\nWhat is the Treatment for Dust Mite Allergy?\nThere is no cure for allergies but symptoms can be controlled to a large extent by medications and allergy shots but limiting exposure to dust mites is the best way to avoid dust mite allergy. The medications which can used to treat the symptoms of dust mite allergy are:\n- Treating Dust Mite Allergy with Antihistamines: These medications block the production of histamines and hence relieve the dust mite allergy symptoms like nasal congestion, runny nose, and watery eyes. Some of the medications which come under this class are Allegra and Claritin\n- Nasal Corticosteroids: These medications are also quite effective in treating the symptoms of Dust Mite Allergy. Some of the medications which come under this class are Flonase and Nasonex\n- Decongestants: These medications help in clearing the congestion in the nose and get rid of the symptoms of dust mite allergy. Some of the medications under this class are Sudafed and Afrin.\nSome of the other medications that may be used for treatment of symptoms of Dust Mite Allergy are:\n- Cromolyn sodium\n- Allergy shots.\nNasal lavage is also quite an effective way in opening the nasal passages and help with the symptoms of Dust Mite Allergy. This can be done by rinsing the nasal passage with warm water mixed with salt on a daily basis.\nHow to Prevent Dust Mite Allergy?\nApart from these treatments there are also certain preventive measures that you can take top prevent Dust Mite Allergy. These methods are:\n- Use allergen-proof bed covers on the mattress and pillows.\n- Wash all bedding at least once a week. This includes bed sheets, pillow covers, blankets and bed covers. Let them dry in sunlight.\n- Use an air conditioner or dehumidifier so as to keep the relative humidity down to about 40%\n- Buy only washable stuffed toys and wash them often.\n- Dust frequently with a damp towel or mop\n- Clean curtains and upholstered furniture regularly\n- Avoid use of carpeting and try using wood, tile, or vinyl flooring."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:337a615b-e2b6-44a5-8f97-3042fd58e39b>","<urn:uuid:1e672ce7-2abd-428e-b9cf-db7a7440f6bc>"],"error":null}
{"question":"What are the main settings to adjust when calibrating a monitor?","answer":"There are four major components for optimizing monitor display settings: 1) Gamma, which controls the relationship between pixel values and brightness, affecting black to white transitions, 2) Brightness, which affects the intensity of light through each pixel and impacts how true colors appear, 3) Contrast ratio, which indicates the depth of darker colors, and 4) Color balance between the primary colors (red, green, and blue) which is crucial for displaying true colors. These can be adjusted through the monitor's built-in menu or calibration tools.","context":["Monitor calibration refers to the accuracy of the different properties of a display. The more accurate an image is to its true colors, the better. This is why the people working with professional arts, designing, video, and photography-editing use expert tools, both software and hardware, to calibrate their monitors and achieve the best possible results.\nThe Windows operating system comes with its own integrated display calibration tool known as “Display Color Calibration,” using which an end-user can calibrate all of their monitors to obtain colors as accurate as the real ones, and even have the same color output across the different display devices.\nHowever, in this post, we are going to discuss several online tools that you can use to calibrate the different settings of your display. These tools can be beneficial for both Windows and non-Windows users since they offer more precise images and context to adjust the various monitor settings.\nTable of contents\n- Monitor Display Calibration Settings\n- How to Calibrate a Monitor\n- Best Tools to Calibrate Your Monitor\n- Challenge Your Color Perception\n- Closing Words\nBefore we discuss our top picks for the tools to use to calibrate your monitors, let us have a look at how to calibrate them, and what settings to adjust.\nMonitor Display Calibration Settings\nThere are a few things that you will need to adjust on your display to get the most accurate results. Some of these settings can be adjusted from the Windows Settings app, while almost all of them can be adjusted from your monitor’s built-in menu using the buttons. We have discussed the “how” part in the section below.\nLet us have look at what settings need adjustment.\nBy definition, gamma is the relationship between the numerical value of a pixel (between 0 and 255) and the pixel’s brightness. The correspondence between this relationship defines how smoothly black transitions to white on a display device.\nThere are different standards of gamma, such as 1.8, 2.2, etc. This number defines the curvature of the transition from black to white. We say “curvature” because the numerical values of the pixel do not define the brightness of the pixel linearly.\nThe table below will help you understand the different gamma standards:\nIn layman’s terms, adjusting the gamma correctly would show black as true black (and not gray) and white as true white.\nBrightness is referred to as the intensity of light coming through each pixel. Adjusting the brightness correctly would display the true red, green, and blue colors, especially when seeing grayscale images and text.\nWhen talking about a display’s contrast, it means color contrast. A monitor’s contrast ratio indicates the depth of the darker colors, like blacks. The higher the contrast, the greater the depth of the color.\nAs an example, the human eyes perceive white text on black (and vice versa) as the best reading setting. This is because the contrast of the two colors complement one another and are greatly distinguished.\nA display has 3 primary colors: Red, green, and blue. These 3 colors make up all of the other colors on each pixel on your screen. The right balance between these colors would only display the true colors of an image on your screen.\nIt is not always necessary that the same numerical value of these colors will show true colors, as each display has a different setting. Therefore, adjusting them to obtain a result as close to the real colors is important to get the best accuracy.\nThese 4 are the major components of optimizing your monitor display settings, apart from the screen resolution and scaling. However, a monitor’s internal settings may offer more control, such as adjusting the display’s sharpness. If yours does, we suggest that you should also adjust those, but only after adjusting the major settings discussed above.\nLet us now see how to adjust your monitor’s settings.\nHow to Calibrate a Monitor\nThe Windows Settings app offers some display calibration, but those are very limited and not ideal to perform a real calibration. From the Settings app, you can only adjust the screen brightness and some other display settings, but not the gamma, color balance, or contrast ratio.\nHowever, the Windows OS does come with its display calibration tool.\nDisplay Calibration Tool\nThe Display Color Calibration utility lets you calibrate the display settings we discussed above that you can do manually with the help of images inside the tool. It offers a complete guide on how to adjust these settings to obtain an ideal calibration.\nUsing this tool alone might not be sufficient for professionals who want the best of the best display calibrations. Therefore, you can use this tool to adjust the display settings alongside the online tools listed below. This way, you can make the adjustments using Display Color Calibration while monitoring live results using the colors and images in those online tools.\nTo open Display Color Calibration, type in dccw in the Run Command box.\nWe also have a dedicated guide on how to calibrate your monitor using the Display Color Calibration tool.\nThe other option to calibrate your monitor is with the help of the On-Screen Display (OSD) menu. This is the menu you can bring up on your screen using the buttons imposed on the monitor itself.\nDepending on how advanced your monitor is, it gives you several options to adjust, including gamma, color balance, brightness, sharpness, etc.\nTogether with the OSD menu and the online tools listed below, you can calibrate your display to obtain the best accurate results possible.\nBest Tools to Calibrate Your Monitor\nWe have listed down these online tools in the order that offer the best calibration options to the least. However, all of these tools offer images and context that will help you calibrate your display accurately.\nLagom LCD Monitor Test Pages\nThe Lagom monitor test pages are a compilation of 13 different pages with comprehensive images that will assist you in adjusting the different display settings of your monitor. It has a separate page for adjusting the following list settings:\n- Display resolution\n- Clock and phase for VGA\n- Black levels\n- White saturation\n- Black and white gradient shifting\n- Color inversion\n- Monitor response time\n- Viewing angle\n- Contrast ratio\n- Subpixel layout\nOpen each of these pages and adjust the corresponding display setting from the Display Color Calibration tool in Windows or the OSD menu. If confused, read the instructions given on each page to obtain the optimal calibration results.\nMoreover, the creator of this website has also given a downloadable ZIP file that will download all the images which you can then carry with you when shopping for a display device.\nAll these things make Lagom test pages our number one pick.\nPhotoScientia Gamma Assessment\nThe PhotoScientia Gamma Assessment tool is a gamma adjustment tool only. It provides images according to the different standards, and then you can adjust your device’s gamma to those standards.\nTo adjust your gamma, we suggest that you use this tool as this website is fully devoted to gamma. Adjust the gamma setting using the OSD menu or the Display Calibration Tool until all the squares match up with their backgrounds as closely as possible.\nOnline Monitor Test\nThe Online Monitor Test offers a series of different color calibration pages. It has a different page for different kinds of tests, like one for grayscale colors and gradients, another for different colored gradients, and so on.\nIt has a total of 16 pages to test and adjust different color-related monitor settings. It can also be used to determine blacklight uniformity across the entire display, as well as individual primary colors, and their smoothness in transition in gradient pages.\nPhoto Friday is a basic single-page website that offers different images on the same screen to adjust your monitor’s brightness and contrast. This tool is used mostly by photographers to obtain real results after editing their images.\nHowever, Photo Friday cannot be used to calibrate your monitor as you cannot use it to balance the colors, nor can you adjust your gamma settings.\nW4ZT is an all-in-one monitor calibration tool that can be used to adjust your screen’s color balance, contrast, gamma, and brightness. However, the images and content on this website are limited compared to the tools we discussed earlier.\nSimply scroll down the page and use the different images to adjust your monitor’s settings accordingly. You will also find on-screen instructions on how the images should appear after calibrating your display.\nChallenge Your Color Perception\nBy now you should be able to calibrate all your monitors perfectly. However, one thing that you also may have noticed is that all of these calibration settings depend on your ability to perceive the different colors and contrasts. The calibration will hence only be as good as your ability to identify minor color variations.\nHence, if in doubt, we recommend that you take a quick test to check your color-identifying abilities.\nThe X-Rite Color Challenge is a color-hue test where you need to adjust the color boxes in the correct gradient.\nThe color boxes on both ends of the rows are fixed – move the boxes in the middle to attain the perfect color gradient. When you are done, click Score my test to get the results.\nThis test will help you judge your color-identifying abilities.\nIf you score low in the X-Rite Color Challenge, then we suggest that you ask someone else to calibrate your devices for you.\nIf you are a professional who needs picture-perfect monitor calibration, then we suggest that you opt for calibrating hardware which includes cameras and other sensors that automatically calibrate your monitor. These include hardware from Datacolor, Calibrite, and Wacom. Of course, this option will cost you."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:7e778c4b-8ccc-4479-982e-dbc4eb2ac171>"],"error":null}
{"question":"I'm studying environmental science - what are the eco benefits of real Christmas trees during their growing time, and what happens to them after disposal?","answer":"During their growing time, Christmas trees provide significant environmental benefits - each tree absorbs more than 1 ton of CO2 throughout its lifetime, and one acre of trees produces enough oxygen for 18 people's daily needs. After disposal, real Christmas trees are 100% biodegradable and widely recycled through over 4,000 available recycling programs, with about 93% of trees being recycled. These recycled trees serve multiple environmental purposes - they can be turned into mulch for landscaping and gardening, chipped for playground material and hiking trails, and used for beachfront erosion prevention and shoreline stabilization. Additionally, they can provide fish and wildlife habitat.","context":["Complete the cycle of your tree’s life by recycling it after the holidays! Each year, we offer curbside pick up and drop off locations for Christmas tree recycling.\nReal trees are biodegradable and recyclable, and during their years on farms, they actually support the environment. Each spring, growers plant one to three seedlings for each tree harvested.\nChristmas tree recycling programs are available nationwide, and many are quite creative. A farm-grown Christmas tree is 100% biodegradable, so it can be used for all kinds of things in nature, from mulch to erosion control.\nHere are some guidelines for recycling your tree and making sure it gets back into nature in a manner that supports the environment:\n1. Trees 6′ or taller should be cut in half\n2. Do not place your tree in a bag\n3. Remove all ornaments, lights, garlands and other decorations.\nKeep an eye on our Facebook page for drop off or curbside collection dates after the holidays! And if you live outside the Austin or San Antonio areas, be sure to look up tree recycling programs in your area.\nThe 2012 Christmas Season was one to remember at Papa Noël Christmas Trees. That was the year, a very special Papa Noël tree spread joy in America’s most famous home; The White House. We’re still basking in the glow!\n2012 NCTA Grand Champion\nWe were delighted that we were the winners of the 2012 National Christmas Tree Association’s Grand Champion Christmas Tree Competition. Our champion tree was selected out of 700 farms in 29 different states to present to First Lady, Michelle Obama. Since 1966 a panel of long-time growers and past winners unite each year to select the finest quality Christmas tree in America. The Grand Champion tree, which was grown on one of Papa Noël’s farms in North Carolina, traveled almost 400 miles and was the centerpiece for The White House holiday celebrations!\nWe Grow Quality Trees\nThe Grand Champion Christmas tree represents the quality we have aimed to provide those of you in Austin and San Antonio, Texas for the past 35 years. It is a distinct honor for all of us at Papa Noël to win such an honorable award and provide the tree that will journey all the way to Washington D.C., and into the Blue Room in The White House. As the tree prepares for its journey to The White House and the Christmas season approaches, we will keep you updated at papanoeltrees.com. Be sure to check back for updates and an exciting look back into the The White House’s Christmas celebrations!\nChristmas Traditions At The White House\nThe season would not be complete without the official White House Christmas tree. Each year a special tree is featured in the Blue Room of The White House and acts a the centerpiece for the President’s joyful celebration.\nThe First Tree\nIn 1889 the first recorded Christmas tree was lit in the The White House under President Benjamin Harrison. First Lady Caroline Harrison, an artist, facilitated the decorating of the tree and paved the way for a tradition that has since become the responsibility of our First Ladies. In 1929, First Lady Lou Henry Hoover decorated The White House’s first “official” tree and in 1961, First Lady Jacqueline Kennedy, began the tradition of selecting a theme for our Nation’s official tree. Since 1929, no Holiday season has gone without an official White House tree.\nSince 1966 The National Christmas Tree Association has provided the Blue Room of The White House with its official Christmas tree. Each year, The Grand National Champion is selected by a panel of judges and long time growers, who vote for the years finest quality tree. Over the years the official tree has acted as a symbol of unity for our country. After President Kennedy’s assassination, First Lady “Lady Bird” Johnson selected themes of comfort and nostalgia, while First Lady Laura Bush selected “Home For The Holidays” in 2001. Last year, First Lady Michelle Obama chose the theme, “Shine, Give, Share,” with a tree decorated to honor America’s servicemen and women.\nAfter hearing hundreds of times; “can I buy my tree before Thanksgiving”, we are finally able to say ‘yes’! The most common reason folks seem to want to buy their tree early is they want to jump into Christmas decorating right after the Thanksgiving meal.\nIt’s a tradition in many families to decorate the tree together and families always come together for Thanksgiving. Just yesterday a lady called asking if she could buy her tree the Saturday before Thanksgiving and when I said yes, she was absolutely thrilled. She said her daughters come home for Thanksgiving break and Christmas break but the parents didn’t want to wait until Dec 18th when the girls came home to decorate the tree, they wanted to do it at Thanksgiving. She went on to say that “since your trees are so lovely, we want to have it up as long as we can and buying it before Thanksgiving allows our daughters to join in the decorating.”\nWe have fresh shippments arriving Every Week until Christmas!\nFake Christmas trees are NOT better for the environment! 85% of fake trees are imported from China and are made of metals and plastics that contain PVCs. Because of the unacceptable levels of lead, California requires that they carry a warning label.\nMyths about Christmas Trees\nFrom the National Christmas Tree Association\nMYTH #1: Real Christmas Trees deplete our forests.\nBUSTED: Seriously, do people still believe this? To be completely accurate, in a few locations around North America, the Forest Service sells permits for people to harvest wild trees so that they can create fire breaks. Commercial Christmas trees (nearly half a billion) grow on family farms; each spring growers plant one to three seedlings for each tree harvested.\nMYTH #2: Real Christmas Trees aggravate allergies.\nBUSTED: A person may be allergic to tree pollen or even tree sap, but a real tree produces pollen in the spring, not in the fall and winter when it is dormant. Studies show that of the 50,000 species of trees, less than 100 have been shown to cause allergies.\nChristmas trees can collect pollens, dust, mold or other allergens as they grow. Of course, so can the artificial tree stored in the attic or basement. Whether you use a fresh Christmas tree from a farm, or an artificial tree stored in a box, if you are sensitive to dust or molds, you can simply hose the tree with water and let it dry before bringing indoors. At Papa Noel, we use a shaker to help remove allergens and dead needles so these don’t end up in your home!\nBUSTED: Whether you use a fake tree for the average 6-9 years or 20 years, it will end up in a landfill. Real trees are biodegradable and recyclable and during their years on farms, they actually support the environment.\nMYTH #4: Christmas trees are a fire safety hazard and frequently catch on fire.\nBUSTED: The reality is, a tree being accidentally ignited is EXTREMELY rare because they are rich in sap and water. As in 0.0004%. According to a report from the National Fire Protection Association, 28% of home fires involving a Christmas tree were started with fake trees.\nMYTH #5: Real Trees cost too much.\nBUSTED: You will find a wide range of prices for the size you want. If I spend $20 on a Christmas tree from a farm each year and you spend $300 on a fake tree, you’d have to use it for 15 years (way past the average) before I will have spent the same amount as you.\nMYTH #6: Real Christmas Trees have pesticides and chemicals on them.\nBUSTED: Myths such as this often get a foothold due to the disconnect that most people have with agricultural practices. Christmas tree farmers do not use chemicals in a “harmful” manner. Chemicals are used only when needed and only according to the specified instructions and regulations of the EPA, the USDA and the FDA. Christmas tree farmers live on their land and raise their families there. They would not engage in an activity that would put their families, employees or the people they sell their product to in harms way. To suggest otherwise is at best uninformed, and at worst, offensive.\nThere has never been a scientific research article suggesting that harmful levels of chemical residue exists on Christmas Trees, and in fact there have been studies looking for it. On the flip side, there have been studies showing a potential health danger of lead dust coming from plastic trees. The state of California requires a warning label on fake trees and wreaths. Watch this clip\nMYTH #7: Real Christmas Trees end up in landfills.\nBUSTED: Christmas tree recycling programs are available nationwide, and many are quite creative. A farm-grown Christmas tree is 100% biodegradable, so it can be used for all kinds of things in nature, from mulch to erosion control.\nMYTH #8: Real Christmas Trees are a hassle and a mess.\nBUSTED: It’s all relative. You may have to vacuum some needles. Does that mean you don’t vacuum normally? Vacuuming should be a regular household chore all year long, so if the tree drops some needles, you’re going to vacuum anyway right?\nThe time invested in buying and maintaining a farm-grown Christmas tree is nothing compared to what you get out of it. A good feeling. Memories. A home that “smells” like Christmas. Knowledge that you made a good environmental choice. That’s not a hassle, that’s a blessing.\nSelecting the best tree for you home is a very personal choice. Species, height and width can all come into play. At Papa Noel Christmas Trees we have selected the finest and freshest trees from the farms where we grow them. We also have chosen a select group of species especially for you. Visit your local Papa Noel lot this holiday season for help choosing your Christmas dream.\nThings to keep in mind\nBe sure you know what size (height and width) you need before heading to a Papa Noel Christmas tree lot.\nDecide where you will place the tree at home or office: against a wall, corner, center of the space. This will help you look for a shape to best fit the space.\nCheck out the species we offer (listed on this page) as these are the fir that retain their beauty in the Texas climate. If you know which species you like best, great, but if you want to try something different, ask a lot manager for assistance with options.\nAsk the lot manager about help with delivery and recycling.\nInvolve the whole family in the selecting and plan fun things for everyone to do during the trip.","The real versus artificial Christmas tree debate replays itself year after year. But the truth is, each option has its own place on the naughty-and-nice list.\nJust a few short decades ago, displaying a Christmas tree in your living room really only yielded one option: a real pine or fir tree. That all changed when a U.S.-based toilet bowl brush manufacturer, the Addis Brush Company, created an artificial tree from brush bristles in the 1930s, acting as the prototype for modern artificial trees.\nThe Pros and Cons of Artificial\nGuilt. Many have made it the sole reason to invest in an artificial tree. The thought of cutting down a new tree each year can put a damper on the holidays for some.\nAlso, cost, convenience and environmental impact are other reasons consumers opt for an artificial tree.\nGiven the current economic climate, artificial trees may be especially appealing for their investment value when compared with the recurrent, annual expense of a real Christmas tree. Their convenience is also appealing to consumers as they don't need watering, don't leave pine needles all over the floor and transportation from tree farm to home isn't an issue.\nBut many experts believe artificial trees actually have a greater negative environmental impact when all aspects of an their life cycle are considered.\nToday's artificial trees are typically manufactured with metal and polyvinyl chloride (PVC), a non-biodegradable, petroleum-derived plastic. In addition, many older varieties may contain lead, used as a stabilizer in the manufacturing process.\nDespite their PVC contents, artificial trees are non-recyclable and non-biodegradable, meaning they will sit in a landfill for centuries after disposal.\nFurthermore, approximately 85 percent of artificial trees sold in the U.S. are imported from China, according to the National Christmas Tree Association (NCTA), adding to their overall environmental footprint.\nThe Pros and Cons of Real\nApproximately 33 million real Christmas trees are sold in North America each year, according to the U.S. EPA. Luckily, about 93 percent of those trees are recycled through more than 4,000 available recycling programs.\nAlso known as \"treecycling,\" the act of recycling a Christmas tree is a leading reason many experts agree they are more environmentally friendly than their plastic counterparts.\nTreecycling is an easy way to return a renewable and natural source back to the environment instead of disposing it in a landfill, where decomposition rates are slowed due to lack of oxygen.\nChristmas trees are recycled into mulch and used in landscaping and gardening or chipped and used for playground material, hiking trails, paths and walkways. They can be used for beachfront erosion prevention, lake and river shoreline stabilization and fish and wildlife habitat.\nA single farmed tree absorbs more than 1 ton of CO2 throughout its lifetime. With more than 350 million real Christmas tress growing in U.S. tree farms alone, you can imagine the yearly amount of carbon sequestering associated with the trees. Additionally, each acre of trees produces enough oxygen for the daily needs of 18 people.\nIn order to ensure a healthy supply of Christmas trees each year, growers must use sustainable farming techniques. For each tree harvested, one to three seedlings are planted the following spring, ensuring a healthy supply of trees.\nAccording to the NCTA, the Christmas tree industry employs more than 100,000 Americans, an important economic consideration in the real versus artificial debate.\nBesides the aforementioned cons associated with real Christmas trees, they are farmed as agricultural products, meaning repeated applications of pesticides, herbicides and fertilizers may be used throughout their lifetime. The ideal tree would be raised organically, using integrated pest management techniques rather than chemicals.\nAnother con associated with real Christmas trees may depend on where you live. For climates where coniferous trees don't grow, that tree in your living room may have had to travel hundreds of miles to reach the lot, significantly impacting the environmental impact associated with travel. However, a tree trucked from a couple states away is still traveling thousands of miles less than one from overseas.\nAn Even Better Option\nGo one step further than the real versus artificial debate and consider a living, potted tree this Christmas. Though not feasible for everybody due to climate and land availability, living trees are brought into the home for about 10 days, then replanted after Christmas. If you don't have the land for replanting, your local parks department will likely accept your tree for planting after the holidays.\nSo what's the final word? Drumroll please... Real trees top our charts for holiday adornment. Even though they might shed needles on your floor, the investment in a U.S.-based product, the carbon-neutral nature of their production and their ease of recycling make them a clear winner.[holidaySearch type=\"recycle\" what=\"Christmas trees\" whatlabel=\"Christmas trees\"]\nLatest posts by Lori Brown (see all)\n- Real vs. Artificial Christmas Trees - November 29, 2010\n- Design Squad Awards Kids for Trash to Treasure Ingenuity - October 27, 2010\n- India Sets Up Green Tribunal to Try Environmental Crimes - October 21, 2010"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:49a03c06-4b34-49f9-9dd9-6619717d7392>","<urn:uuid:84a88cd2-fbc1-409f-a6c7-4a9ed616986b>"],"error":null}
{"question":"How have displays of integrity in sports evolved historically, and what has been the development and spread of steroid use in athletics?","answer":"Historically, there have been powerful displays of integrity in sports, from Bobby Jones' self-imposed penalty in 1925 to more recent examples like Armando Galarraga gracefully accepting an umpire's mistake that cost him a perfect game in 2010. Meanwhile, the use of anabolic-androgenic steroids (AAS) has evolved since their discovery in 1935, initially developed for medical purposes like treating breast cancer and bone loss. AAS use spread from Soviet Union and Eastern European weightlifting teams in the 1940s to the broader athletic community, leading to FDA approval in 1958. Despite regulatory efforts like the Anabolic Steroid Act of 1990 and its 2004 amendment, steroid use has increased across all levels of athletics, including high school sports.","context":["Too often these days, fans and athletes alike focus on the negative side of sports, whether it's a fight that breaks out on the field or trash talk off the field. Still, there are many instances of athletes taking the high road and doing the right thing. Here are five perfect examples of athletes showing us what sportsmanship is all about.\n1. Bobby Jones, 1925 U.S. Open Assessed himself a one-stroke penalty on the par-4 11th hole for an infraction nobody saw. He’d eventually lose in a playoff. Praised for his honesty, Jones said, “You might as well praise a man for not robbing a bank.”\n2. Armando Galarraga, Detroit Tigers, 2010 With Galarraga one out away from a perfect game, umpire Jim Joyce blew a call at first base to spoil the achievement. Rather than lash out, Galarraga defended Joyce and accepted his apology.\n3. Shawn Crawford, 2008 Beijing Olympics\nFinished fourth, but was awarded silver in the 200 meters after two runners were disqualified for lane violations. Feeling that Churandy Martin of the Netherlands Antilles had rightly earned the honor, Crawford sent him his medal.\n“I know the rules are the rules. You step on the line, you’re disqualified. But when you’re running clear in your position and aren’t impeding in someone’s race and you barely touch the line? I felt that Churandy was robbed. He worked four years just like I did, went to practice and trained — probably threw up at practice like I did — nights of breathlessness and days where you wanted to sleep. For four years... I felt like he ran a second-place race. He beat us by a considerable margin, from second to third. I know I put in the work, and I go out there and expect to earn my position. I felt like I was beat fair and square.\n“When Churandy received my medal, he was surprised. He couldn’t believe that I was doing it. At first, he didn’t accept it. He understood my argument, but wanted to make sure that’s really what I wanted to do. But he has so much respect for me now. We’re really cool, and it’s a great relationship. To me, that’s more important. If I had kept the medal, I probably never would have known much about him. Just ‘hi’ and ‘bye,’ every time I saw him. I like the relationship we have built. It’s important to have good relationships and friendships in life. You only live once, and it’s a short life.”\n4. Mallory Holtman and Liz Wallace, Central Washington University women’s softball, 2008 After Sara Tucholsky of Western Oregon University hit her first career home run, she injured her knee rounding first base. Holtman and Wallace carried her around the bases, so she could receive credit for a four-bagger.\n5. San Francisco Giants, 2010 NLDS After clinching victory in the series at Turner Field, the Giants broke off their victory celebration to join in a standing ovation for manager Bobby Cox, managing his last game of a 25-year run with the Braves.","Anabolic – androgenic steroids (AAS) are synthetic compounds structurally related to testosterone that promote muscle growth and have masculinizing effects. They have been used by athletes who attempt to gain an edge in competition.\nAAS were discovered in 1935 and have been researched and synthesized since then to help reduce the androgenic (masculinizing) effects. The original objective of the medical community was to produce a drug that could treat breast cancer, bone loss, burns and recovery from surgery.\nAAS has also been used by psychiatrists to treat mood disorders in order to improve mental alertness, mood elevation and concentration (Uzych, 1992). It is possible the German army of World War II was the first illegitimate user of anabolic steroids (Daigle, 1990).\nThe Spread of AAS\nThe spread of AAS use in the male athletic community has been rapid. In the 1940’s AAS were used by Soviet Union and Eastern European weightlifting teams. The United States pharmaceutical industry responded in 1958 with the first Food and Drug Administration approved AAS (Fair, 1988). The early beginning of AAS use by a small number of Olympic level athletes has seen a spread to the greater athletic community.\nThe United States has attempted to control AAS use with the addition of anabolic steroids to Schedule III of the Controlled Substance Act and the Anabolic Steroid Act of 1990. This act was amended by the Anabolic Control Act of 2004 which included prohormones as part of the list of controlled substances (Murphy, 2005).\nDespite a ban on AAS by all major sporting organizations the use of anabolic steroids has increased among all levels of athletes including those at high school (Inaba and Cohen, 2007).\nWhen Only Winning Matters\nAn attitude that accepts only winning is often promoted by coaches and parents who push their athletes to keep up with their competition (Center of Addiction and Substance Abuse, 2000). The athletic environment that condones the win- at – all – cost approach appears to encourage AAS use and provides a context that appears to accept this strategy as rational (Goldberg et al., 1996).\nGoldberg et al. (1996) found high school football players who were committed to winning at – all – cost were more likely to use anabolic performance enhancing steroids. The extreme motivation to achieve in professional sports may be due to considerable financial rewards. African American parents are four times more likely than Anglo Americans to see their adolescent athlete as an opportunity for economic gain (Edwards, 1992).\nThis may be due to the perception created by successful African American professional athletes in high profile sports. Although African Americans constitute 13 percent of the general population, recent NFL surveys revealed that the NFL is made up of approximately 68% African-American athletes, while 76 percent of the National Basketball League.\nSide-Effects of Anabolic Steroid Abuse\nPersonality assessment characteristics of steroid users have revealed notably higher levels of assertion and impulsiveness. The misuse of anabolic – steroids has been linked to:\n- Sleep disorders\n- Homicidal ideation (NIDA, 2005)\nAggression and violence is often cited as a major problem with male athlete AAS users due to the alteration of brain development for aggressive inhibition. Recent neuroscience research has highlighted the link between testosterone and approach – avoidance behavior. The use of AAS has also been associated with the exploitation of cigarettes and alcohol.\nAchieving No Matter What\nThe extreme motivation to succeed athletically by some athletes who use AAS can be defined as an attempt to achieve in a conventional environment using unconventional means. Bamberger and Yeager (1995) found over 50% of the Olympic and aspiring Olympic athletes they interviewed stated they would use banned performance enhancing substances if they were not discovered and could be guaranteed first place in each of their competitions despite the possibility of death in the next five years.\nThis attitude appears to continue with today’s successful athlete’s use of anabolic steroids. Both Lance Armstrong (Cycling) and Marian Jones (Sprinting) exemplify the extreme need to get to the top of their chosen sport. Both Armstrong and Jones reached the highest levels of their sport and revealed AAS use.\nThese revelations resulted in criminal consequences for Jones and on – going legal action against Armstrong with both having all sporting successes removed from the record books. Sport often represents the role of the individual within their community and family.\nThe Family Intensive Sport Environment\nSport has become an integral part of the family in the United States. The psychological development of many young people is enmeshed in the process of achieving success in sport. This phenomenon has been described as a family intensive sport environment (Helledt, 1990). The family invests both time and financial resources into the development of the family athlete, which can result in tremendous pressure on the athlete to succeed.\nThe adolescent athlete that is overwhelmed by controlling parents, demanding training schedules and the pressure of academic standards may withdraw from family relationships to a world outside of conventional thought.\nParents who pressure their adolescent athlete with criticism often have high expectations of their athletic performance (Goodger et al. 2007). The pressure felt by the adolescent athlete could result in cognitive exhaustion and AAS use.\nFinding Fulfillment Through Success\nAthletes often find the fulfillment of many psychological needs through their successes in sport. The use of AAS is attractive to those who want to gain an unfair advantage over their competition in an effort to gain the many rewards.\nThe consequences of AAS abuse range from criminal offenses to health related concerns. The male athlete who understands that rewards in life come from internal satisfaction will no longer seek unhealthy methods to obtain the external rewards. Internal rewards become acknowledged as the best one can do within the rules of the competition.\nCommunity Discussion – Share your thoughts here!\nWhat steps can we take as an addiction community to bring awareness to the dangers of steroid use and the importance of inward satisfaction in competitive sports?\n- Center for Addiction and Abuse National Commission on Sports and Substance Abuse (2000). Winning at any cost: Doping in Olympic sport. A Report Prepared for the U.S. Office of National Drug Control Policy, New York: Center for Addiction and Substance Abuse.\n- Daigle, R. D. (1990). Anabolic Steroids. Journal of Psychoactive Drugs 22 (1): 77-80.\n- Edwards, H. (1992). Are we putting too much emphasis on sports? Blacks in sports: The next 45 years. Ebony, Retrieved July 19, 2008 from http://findarticles.com/p/articles/mi_m1077/\n- Fair, J. (1988). Olympic weight-lifting and the introduction of Steroids: A statistical analysis of the world championship results, 1948-72. International Journal of the History of Sport.\n- Golberg, L, Elliott D., Clarke, G., MacKinnon, D., Moe, E., Zoref L., Green, C, Wolf, S, Greffrath, E. miller, D., & Harrison, L. (2007). Redefine the finish line: Overemphasis on athletic success may limit academic pursuits for African American children. The University of North Texas at Austin, Retrieved July 18, 2008 from http://www.utexs.edu/features/2007athletics/index.html\n- Helledt, J. (1990, September). Early adolescent perceptions of Parental pressure in the sport environment. Journal of Sport Behavior, 13(3), 135-144. Retrieved July 27, 2008, From PsycINFO database.\n- Karila T. (2004). Concomitant abuse of anabolic androgenic Steroids and human chorionic gonadotrophin impairs\n- National Institute on Drug Abuse (2000) Research Report Series: Anabolic Steroid Abuse. Retrieved July 29, 2008 from www.steroidabuse.org\n- Murphy, S. (2005). The sport psych handbook. Champaign, IL: Human Kinetics.\n- Uzych L. (1992). Anabolic steroids and psychiatric related Effects: A review. Canadian Journal of Psychiatry, 37, 23-28.\n- Yesalis, C., E, & Bahrke, M., S. (1995). Anabolic – androgenic steroids. Sports Medicine, 19, 326\nThe opinions and views of our guest contributors are shared to provide a broad perspective of addiction. These are not necessarily the views of Addiction Hope, but an effort to offer discussion of various issues by different concerned individuals.\nLast Updated & Reviewed By: Jacquelyn Ekern, MS, LPC on June 26th, 2015\nPublished on AddictionHope.com"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:e934603f-94b5-404b-a3c1-338a7480d753>","<urn:uuid:2d914447-ac2e-404d-87fe-2c2342c0b5bb>"],"error":null}
{"question":"Do the EDI programs at UBC and University of Calgary both involve community engagement?","answer":"Yes, both universities' EDI programs involve community engagement, but in different ways. UCalgary's Dimensions program includes Indigenous engagement as part of its broader institutional efforts, with Lauren Remple, a member of Alexander First Nation, helping to oversee the program. At UBC, community engagement is demonstrated through specific funded projects, such as Mario Brondani's project engaging the community as teachers in dental education and Chris Lee's Asian Canadian and Asian Migration Studies Program involving community participation in curriculum development.","context":["March 9, 2022\nEquity, Diversity and Inclusion program team expands\nThe University of Calgary is committed to making transformational change in how equity, diversity, inclusion and accessibility are woven into the research ecosystem. Key to this change is the national Dimensions Equity, Diversity and Inclusion (EDI) pilot program.\nDimensions is a unique federal tri-agency program that is designed to recognize Canadian post-secondary institutions that are working to increase research excellence, innovation, and creativity by advancing a more equitable, diverse, inclusive, and accessible institutional environment and embedding EDI across the research ecosystem.\nIn support of UCalgary’s commitment, Dr. William Ghali, vice president (research), and Dr. Malinda Smith, vice-provost and associate vice-president-research (EDI) and chair of the Dimensions executive committee, have announced a new member of the Dimensions team.\nLauren Remple has been selected as the Dimensions program manager for a one-year term.\nRemple joins the Dimensions team after spending seven years in human resources and student services roles at UCalgary. She has over a decade of experience in human resources, Indigenous engagement and EDI and is a proud member of Alexander First Nation.\n“Being part of the university is being part of something larger than yourself,” says Remple. “Every day I start work and think, ‘Maybe we can create something today that will change someone’s life, or the way someone sees the world’.” As program manager, Remple will oversee the operational aspects of Dimensions at UCalgary, and be the first point of contact for inquiries on the program.\nRemple will join Smith on the Dimensions executive committee and play a key role in the progress of Dimensions that will lead to the completion of the institution’s Dimensions award application in fall 2022.\nDimensions is based on a Charter with eight principles, which UCalgary committed to and signed in 2019. Following a national competition, UCalgary was then selected as one of 17 post-secondary institutions to participate in the Dimensions pilot program.\nKey to Dimensions is its recognition scheme, which builds on and extends international equity initiatives like Athena SWAN in the United Kingdom and Ireland, and SEA Change in the United States.\nUnlike EDI programs in other countries, the Dimensions EDI initiative is inclusive of, but not limited to, the Canadian employment equity designated groups including women, Indigenous Peoples, visible/racialized persons, persons with disabilities, as well as LGBTQ2S+ persons.\n“Dimensions is ambitious in its vision to foster transformational change, to advance research excellence, innovation and creative inquiry,” says Smith. “It does so on the one hand by removing obstacles and barriers that impede an equitable and inclusive academy and research ecosystem; and on the other hand, by proactively working to create equitable pathways and access to research funding and embed EDI throughout the research process. This includes research questions, design, methodology and data collection, analysis and interpretation, and the dissemination of results.”\n\"We cannot be complacent in our institutional efforts to improve equity, diversity and inclusion at UCalgary. Similarly, we must continue in our Indigenous engagement journey,” says Ghali. “Expansion of the Dimensions team is key to ensuring we are working toward tangible, positive change. Lauren will support UCalgary’s efforts to deepen our commitment to EDI, accessibility, and Indigenous engagement across the research ecosystem.”\nRemple is excited by the importance and possibilities of the EDI work and journey ahead.\n“The possibilities that excite me the most are looking at knowledge in a different way,” says Remple. “For so long, what we perceive as ‘knowledge’ has been created by a small group of people, and Dimensions will expand that. We can think about what knowledge is, who creates it, and learn about concepts offered by many different viewpoints.”\nThose wishing to get involved with Dimensions steering committee or the Equitable Pathways working groups can visit the Dimensions web page for more information.","UBC’s Equity & Inclusion Scholars Program funds innovative projects that seek to integrate equity, diversity, and inclusion (EDI) in teaching-related scholarship.\nProjects may include, for example, changes to curriculum, teaching practice, or programming within a faculty or school, all with the aim of placing EDI at the centre of teaching and learning. The following is a list of funded projects.\nProject Lead: Neil Armitage, Lecturer, Sociology\nDescription: The project will develop and provide a template for adapting, embedding and evaluating existing Community Building Education (CBE) provided by the Equity and Inclusion Office within first year Arts courses. The project will work with Teaching Assistants to adapt and facilitate CBE to Sociology and Anthropology contexts and will evaluate its impact on students’ sense of community.\nProject Lead: Mario Brondani, Associate Professor, Oral Health Sciences\nDescription: This collaborative and interdisciplinary project will engage the community as a teacher in an inclusive learning to address sexual diversity, and addiction and mental health within the undergraduate dental and dental hygiene curricula, and will respond to the following question: ‘To what extent can an interactive and open dialogue about mental illness and addiction, and Queer health foster a transformative learning on future health care professionals?\nProject Lead: Janice Stewart, Senior Instructor, Institute for Gender, Race, Sexuality & Social Justice\nDescription: The Learning With Strangers project aims to make visible the intersectional pedagogical architecture of faculty and student experiences of “belonging/inclusion” or “unbelonging/exclusion” --- complex, inspirational and/or difficult conversations and engagements with power/knowledge in University. Supporting inclusion in classrooms gets to the heart of what is most challenging about social justice teaching for both faculty and students.\nProject Lead: Tal Jarus, Professor, Occupational Science and Occupational Therapy\nDescription: In spite of compelling equity and patient care arguments to support diversifying participation in health education, students with disabilities experience significant barriers to their participation in those programs, in particular a lack of clarity on how to support them during fieldwork education. This project will explore the challenges and opportunities for creating inclusive and equitable fieldwork environments from the perspective of fieldwork and academic educators and students with disabilities.\nProject Lead: Chris Lee, Associate Professor, English/Asian Canadian and Asian Migration Studies\nDescription: This proposed project will enable the Asian Canadian and Asian Migration Studies Program (ACAM) to build an interdisciplinary undergraduate curriculum that integrates equity and inclusion in the context of community-engagement. Project funds will be used to build faculty capacity, enable student participation, develop and support new courses, and facilitate community involvement in curriculum development.\nProject Leads: Christine Goedhart, Science Educational Specialist, Botany & Skylight: Science Centre for Learning and Teaching; Karen Smith, lecturer, Microbiology and Immunology; Jared Stang, Lecturer, Physics and Astronomy; Jaclyn Stewart, Senior Instructor, Chemistry\nDescription: Project summary: We will enhance equity and intercultural understanding in introductory active-learning Biology, Chemistry, and Physics courses by: developing EDI measurement tools to inform teaching and conduct research; identifying the inequities and inequalities that currently exist, and; determining the characteristics of learning activities that support and engage all students."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:ccb7bc9b-1db0-4cb2-b9e8-8fa03d3e3999>","<urn:uuid:1fa96070-5c54-4ee6-bd72-2fad17346548>"],"error":null}
{"question":"How can one reach and explore the stunning Lake Sorapis in the Dolomites?","answer":"Lake Sorapis can only be reached on foot via a clear path. The lake is at high elevation, so hikers should proceed slowly. While there is an option to hike to the summit for those in great shape, visitors can still enjoy amazing views of the lake's turquoise waters, white Dolomite cliffs, and green hills without climbing the mountain. Note that the path is not recommended for those uncomfortable with heights.","context":["Ancient Romans loved the lakes in Northern Italy as much as we do—the scenery and mild climate are incomparable. For us, there’s the bonus of culture, cuisine, and heritage. There are the big-beauty lakes, such as Maggiore, Como, and Garda. But, there are also smaller, hidden lakes. All were formed by glaciers millennia ago. Prehistoric people inhabited the region and, much later, the Romans moved in. Magnificent churches, sanctuaries, and castles from the Middle Ages dot the landscape. Funiculars, narrow-gauge railroads, boats, and cable cars offer views. An Italy vacation in the Northern Lakes is sublime.\nShare this Graphic On Your Site\nWhether you stay or take a day-trip from Milan, Como is the most spectacular of the major lakes. Lush hillsides climb to snow-tipped Alps, and the lake is set prettily in the middle. Villages hug the shoreline, including elegant Bellagio and Varenna. The town of Como, on the southern tip, was founded by Julius Caesar as a resort town, and alpine fort, in the 1st century AD. The 18th century Austrian take-over passed along a vibrant café culture. You might also get a kick out celebrity-spotting. George Clooney’s family, and guests, creates a buzz.\nJust 34 miles from Milan’s airport, Maggiore has brilliant shores filled with camellias, azaleas, and verbena. (The Roman name for this lake was Verbanus.) Head to the village of Arona, and climb inside the statue of Cardinal Borromeo. Look at the lake through his eyes—he was their patron saint. At the center of the lake, the Borromean Islands are near the resort town of Stresa. Delight in the natural grottoes, and then explore 17th century Pallazo Borromeo and its gardens. Two miles west of Cannobio, a market town near Switzerland, you can hike to the dramatic gorge and tumbling waterfall of Orrido di Sant ‘Anna.\nDeep in the Dolomites, high-elevation Lake Sorapis is a vision. (If you’re a hiker in great shape, consider going to the summit.) The water color ranges from deep blue to milky turquoise. The white Dolomites, which resemble marble, rise from the lake, and green pastures surround all. With its towering pinnacles and lake vistas, this is one of the most spectacular spots in Europe. The lake can only be reached on foot, and there is a clear path. It’s quite high in elevation so if you choose to hike to the top, go slowly. (This path is not for anyone who is uneasy about heights.) You can be amazed by the lake without climbing the mountain, and the sheer white cliffs, green hills, and turquoise waters are a must for nature lovers.\nIn the center of the Franciacorta wine region and foodie heaven, local trattorias serve up fresh dishes with fish and sparkling wines. The area round Iseo has tall mountains, waterfalls, and a small island, Monte Isola. There are scores of fishing villages on the shores, giving this area a homey, less-glitzy, feel. The small town of Sulzano makes a good jumping-off place to explore the area around the lake. On the east bank, a road leads to the village of Cisiano, about three miles from Marone. Rock formations shaped like spires are topped by boulders and known by residents as “fairies of the forest.” This area is low-key and lovely.\nThe largest lake in Italy, Garda is between Venice and Milan on the edge of the Dolomites. Francis of Assisi founded his monastery on the largest island here, Isola del Garda, in 1220. Olive trees, hardy lemons, parasol pines, and the Canary Island Date Palm grow in abundance. Winds rush down the mountains in the morning, and go back up at night. There are so many types of winds, that they have different names. Go to Sirmione and discover Scaligeri Castle, one of the most complete and well-preserved castles in Italy, and then take a trail to prehistoric rock art.\nJust a short distance from Como and Maggiore, Lake Lugano is tucked near the Swiss border. The area is a dynamic mix of cultures, and more than a thousand types of exotic, flowering plants hug the shores. In the town of Lugano, oddly, is a statue of George Washington raised by a Swiss-Italian admirer in the 19th century. (Visit the Chiesa di Santa Maria di Loreto church.) There’s also an ancient olive grove near the lake called Sentiero dell’olivo. Explore, and find hidden grottoes.\nThe smallest lake, Orta is a hidden gem. The main town, Orta San Giulio, is romantic—picture winding lanes, rustic houses, and ancient walls. It’s also a literary hideout for notable poets and writers. The medieval village is known for Sacro Monte, a UNESCO-listed place of pilgrimage and worship, overlooking the lake. Isola San Giulio is an island in the lake, accessible only by boat, and home to the 12th century Basilica di San Giulio. Milan airport is 24-miles away, which makes this a nice, one-day trip from the city."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:8f3d979f-f866-4a46-af58-47d4b3b1ac23>"],"error":null}
{"question":"I'm researching post-conflict reconstruction efforts - what was the geographical expansion of Iraq's Emergency Operation for Development project in 2017?","answer":"The project expanded beyond the original areas of Salah Ad-Din and Diyala governorates to include more recently liberated cities, particularly Mosul in Ninawa Governorate and Ramadi in Anbar Governorate. The expansion also included support for communities hosting internally displaced persons (IDPs) across Iraq, including in Kurdistan.","context":["|Environmental Assessment - Sep 20, 2017||Updated environmental and social management framework (English) - SFG3631\nThe conflict in northern Iraq has unfolded at a time of severe fiscal crisis. In the last two decades,\nIraq has witnessed a dramatic fall in almost all human development indicators including poverty,\nhealth standards, life expectancy, and literacy. Extreme poverty is widespread, particularly in rural\nareas and a number of governorates. The government’s recovery strategy is to jump-start the\ndelivery of basic infrastructure and services and rehabilitate critical infrastructure in the liberated\nareas from the insurgency. In response to the request of the Government of Iraq, the World Bank’s\nsupport, through the proposed Emergency Operation for Development – Additional Finance\n(EODP-AF), is aimed at supporting the Republic of Iraq in the reconstruction of damaged\ninfrastructure and restoration of public services delivery in Targeted Municipal Areas.\nThe parent EODP is being implemented in urban agglomerations of Tikrit, Al- Dour, Al-Alam and\nAl Dhuluiya located in the Salah Al-Din Governorate as well as urban agglomerations of Jallawla,\nAs-Sadiya and Al-Azeem located in Diyala Governorate. In addition, suburban areas, villages and\ninfrastructure across open range land may also be included for project-financed activities. The\nEODP is already expanding its support to additional municipalities such as Ramadi and few others\nthat were liberated over the past year, where immediate support to reinstate services was much\nneeded. The EODP-AF would expand the support further to other liberated areas and extend to\nother priority sectors. This proposed EODP-AF would expand scope from electricity, water,\nsanitation and solid waste management, transport (roads and bridges) and health sectors to include\nagriculture, water resources and irrigation, municipal services and education. These newly\nintroduced sectors would address important segments of the society who are living in lagging\nregions, poor, with high unemployment and where women’s employment in the agriculture is a\nviable one. Similarly, the improvement of municipals services and the return of the younger\ngenerations to a modern schools and curriculum would rehabilitate them from the nearly two years\nof fierce conflict and extremist ideologies.\nGeographically, the expansion would go beyond today’s Salah Ad-Din and Diyala governorates\nto more cities that have been recently liberated in two additional governorates such as Mosul in\nNinawa, Ramadi in Anbar and few others. These cities have experienced enormous damage to all\naspects of from public and private assets whether in terms of infrastructure, services, housing or\nBeyond these areas that were directly affected by the conflict, the EODP-AF will also support\ncommunities who are hosting IDPs for the past three years to continue and improve their ability to\ndeliver services to IDPs. These communities are in many cities in Iraq including in Kurdistan\nThe common feature for all project interventions is the strict adherence to pre-existing footprints\nof buildings, structures and linear infrastructure, which was damaged, destroyed, sabotaged or\nstolen during combat activities and occupation by the terrorist groups.\n1.2 Project Development Objective and Rationale for Additional Finance\nThe Project development objective is to support the Republic of Iraq in the reconstruction of\ndamaged infrastructure and the restoration of public services delivery in Targeted Municipal\nAreas. The PDO for the EODP-AF is consistent with the PDO for the parent EODP but with an\nexpansion in geographical and sectoral coverage: to support the GoI in the reconstruction of the\ndamaged infrastructure and the restoration of public service delivery in Targeted Areas.\n1.3 Rationale for the updated ESMF\nAccording to the World Bank requirements for financing this project, the Project Owner prepared\nan Environmental and Social Management Framework (ESMF) that covers the entire scope of\npotential investment sub-projects. The ESMF for the parent EODP was prepared, consulted and\ndisclosed before any EOPD physical activities started. Given that a new scope has been added to\nthe original EODP, the ESMF needs to be updated to incorporate the new geographical and sectoral\nThis updated ESMF also includes a positive list of likely activities and investments to be financed,\nand a negative list of activities, equipment, and goods that will not be financed by the project due\nto their potential, negative environmental impacts.\n|Project Info. and Integ. Safeguards||Oct 5, 2017 - PIDISDSA23148\n|Appraisal Project Information||Oct 5, 2017 - PIDISDSA23148 (English)\n|Project Paper||Oct 17, 2017 - PAD2525\n1. This Project Paper seeks the approval of the Executive Directors to provide an additional\nloan in the amount of US$400 million to the Republic of Iraq for the Emergency Operation for\nDevelopment (EODP) - P155732 [IBRD-85200].\n2. Rationale for Additional Financing (AF). Fierce and protracted fighting in areas of Iraq\nnewly liberated from the Islamic State of Iraq and Syria (ISIS), especially Mosul, ISIS's last urban\nbastion in Iraq, resulted in extensive damages that have impacted service delivery and basic\ncommercial activities. In July 2017, Prime Minister Abadi announced the liberation of Mosul and\nthe need to start reconstruction of the area. The international community has expressed its\ncommitment to support the Government-led reconstruction of Mosul and other localities that have\nbeen recently liberated from ISIS, namely, the immediate implementation of recovery,\nreconstruction, and rehabilitation of priority infrastructure to restore delivery of public services.\nThe proposed AF will expand the EODP scope to include agriculture, irrigation, urban services,\nand education services, as well as additional cities that have been recently liberated from ISIS.\n3. The design of the proposed AF is similar to that of the EODP parent project. It is fully\naligned with the EODP’s intended scale-up strategy. Both are intended to complement the\nGovernment of Iraq (GoI) stabilization efforts through restoration of public service delivery via\nreconstruction and rehabilitation of priority infrastructure, and to pave the way for Iraq’s\nsustainable, inclusive and equitable development.\n4. The AF will finance geographic scale-up of the parent project activities to new\nmunicipalities liberated from ISIS in the Salah Ad-Din and Diyala governorates, as well as in\nthe governorates of Al Anbar (including Ramadi), Ninawa (including Mosul) and others that would\nbe identified at a later stage. Accordingly, the implementation responsibilities, which under the\nparent EODP were under central government institutions, will be expanded to subnational\n5. As part of this AF, the following changes will be introduced to specific components of\nthe EODP: (i) Component 1 on restoring electricity infrastructure and connectivity will be\nexpanded to the new geographic areas, and will include works and activities related to installation\nof the supplied equipment; (ii) Component 2 on restoring municipal waste, water and sanitation\nservices will be extended to the new geographic areas; (iii) Component 3 on restoring transport\ninfrastructure will be extended to the new geographic areas and will incorporate financing for a\nfeasibility study on the operation and maintenance of Mosul airport post reconstruction, as well as\nfinancing for the restoration of public transport terminals that serve intercity transport under a\nPublic Private Partnership (PPP) scheme; and (iv) Component 4 on restoring health services will\nbe revised in scope (the component will receive no additional funding and will focus, in addition\nto the supply ambulances and mobile clinics, on the repair of damaged hospitals and clinics instead\nof replacement of supply of mobile hospitals), since the Kuwait Fund for Arab Economic\nDevelopment has made available a grant of US$100 million to restore health services in areas\nrecently liberated from ISIS.\n6. The AF will also expand the sectoral scope of the EODP to address pressing education,\nagriculture/irrigation, and urban service delivery needs of communities in liberated areas.\nIn addition to activities that are currently planned or implemented in the four sectors covered by\nthe parent project (water and sanitation, electricity, transport, and municipal services), the AF\nwould finance the following activities in the new sectors:\n(i) Support for the restoration of agriculture productivity. The AF will finance provision\nand distribution of farm household starter packages; repair, reconstruction, and rehabilitation of\nkey knowledge and service centers for agriculture; and measures to promote access of farmers\nand farmer groups to knowledge, services and technologies. This will be achieved through\nacquisition and distribution of farm input vouchers1\nand establishment of a transparent and\nefficient local input supply industry, as well as support for institution building and increased\naccess of local agribusinesses to working capital.\n(ii) Support for emergency repair of water control hydraulic infrastructure and\nirrigation schemes. This would support emergency repair of the Falluja barrage and emergency\nrepairs to six irrigation schemes covering 72,000 hectares (ha). Typical emergency works would\ninclude repair of headworks, main canal water control structures, groundwater wells, small\ncanal bridges and culverts. This component would also include rebuilding of damaged Ministry\nof Water Resources (MoWR) offices and O&M offices including the procurement of machinery\nand equipment for operations and maintenance.\n(iii) Support for the restoration of education services in liberated and conflict-affected areas\nof Iraq. The AF will establish the foundation for additional developments in this sector through:\nrehabilitation, reconstruction, upgrading and equipping of education infrastructure;\ndevelopment and implementation of training and support programs for teachers and school\nleaders; and vocational training and social support for youth who are not engaged in education,\nemployment or training (NEETs).\n(iv) Support for restoring basic municipal infrastructure and services and preserving\ncultural heritage assets at the governorate levels. The AF’s support in this regard will entail:\nrehabilitating urban infrastructure to improve access to municipal, commercial and financial\nservices; upgrading sports facilities for youth and rehabilitation of selected cultural heritage\nassets; and enhancing the capacity of municipal authorities and service providers to deliver\n(v) The AF will also deepen and mainstream across project components key citizen\nengagement and community participation elements of the EODP design, including the\nsocial inclusion of Internally Displaced Persons (IDPs), returnees and other vulnerable groups;\nsupport to cultural heritage restoration as a means for dialogue and reconciliation; and fostering\ndecentralization by strengthening governorate-level implementation mechanisms for planning,\nimplementation and monitoring.\n7. Corresponding results indicators and targets have been adjusted in the Results Framework\nof the AF.\n8. Partnerships: The EODP has created opportunities for collaboration with development\npartners which will contribute to the Iraq’s recovery and reconstruction program. In addition to\nUnited Nations (UN) agencies, the governments of German (EUR500 million), Japan (US$300\n1 Electronic vouchers will be utilized by the project. To minimize the risk of failure of the various electronic systems\nand processes, a small pilot will be conducted prior to the launch of implementation to identify and rectify errors.\nmillion), and Kuwait (US$ 100 million) have contributed to these programsSome of the funds are\nbeing managed either through the UN system, ReFAATO or directly by the line ministries.\n|Executive Directors Meeting||Oct 31, 2017 - 121446\nMinutes of meeting of the Executive Directors of the Bank and IDA held on October 31, 2017 and record of approvals September 29 thru October 31\nThe Executive Directors considered the President’s Memorandum and Recommendation\n(R2017-0226, dated October 19, 2017) and approved the additional loan in the amount of US$400 million\nto Iraq for the Emergency Operation for Development Project on the terms and conditions set out in the\n|Board Report||Oct 31, 2017 - 120901\nExecutive Directors approved the additional financing in the amount of US$400 million to the\nRepublic of Iraq towards the Emergency Operation for Development Project (EODP) on the\nterms and conditions set out in the Memorandum and Recommendation of the President.\nDirectors supported the additional financing which aims to support Iraq in the reconstruction of\ndamaged infrastructure and the restoration of public services delivery in targeted municipal\nareas, in a direct effort to reinstate trust between the State and its citizens. They underscored the\nimportance of supporting the country and welcome the new focus on agriculture in particular.\nDirectors acknowledged the rapid response of the World Bank team in designing a\ncomprehensive strategic response plan to Iraq’s pressing needs against a backdrop of a fragile\nand high risk conflict environment, noting the complexity of the situation and need to maintain\nflexibility and adaptability.\nDirectors acknowledged the overall high risk of the project including security, fiduciary and\nsafeguards risks, and underscored the need to carefully monitor risk developments and execute\nmitigation measures. Directors stressed the importance of capturing lessons learned from the\nimplementation of the initial project, particularly in relation to the importance of sustained\nengagement with UN partners and other donors to strengthen the institutional capacity of Iraqi\nauthorities at all levels and increase government’s ability to restore essential services and\npromote inclusive recovery and reconstruction. They emphasized the need to ensure adequate\ncollaboration and complementarity among all actors. Directors noted that lessons learned from\nthis operation may be relevant for other FCS engagements. They also highlighted the\nexpectation of private sector engagement in reconstruction efforts going forward.\nDirectors welcomed the additional financing’s emphasis on gender, youth and employment,\nwhich are principal factors of the MENA Regional Strategy’s goals to rebuild a sustainable\nsocial contract and inclusive reconstruction."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:8231f68f-a0ec-44f6-b7e2-5cb715c6124d>"],"error":null}
{"question":"What are the comparative neurological impacts of West Nile Virus infection versus congenital Cytomegalovirus infection?","answer":"West Nile Virus and congenital Cytomegalovirus both can cause serious neurological conditions, but with different manifestations. WNV can cause encephalitis or meningitis in less than 1% of infected people, with some neurologic effects being permanent. For CMV, approximately 10% of infected infants will be symptomatic at birth with serious neurological conditions including microcephaly and cerebral palsy. Additionally, CMV can cause sensorineural hearing loss, which can be present at birth or develop later, and can be unilateral, bilateral, fluctuating, or progressive. Both infections can lead to severe outcomes, but CMV specifically targets developing fetuses with lasting neurological impacts.","context":["West Nile Virus\nMosquito Control and why it's Important to YOU\nWhat is West Nile Virus (WNV)?\nWNV is a mosquito-borne virus that can cause fever, encephalitis (inflammation of the brain), or meningitis (inflammation of the lining of the brain and spinal cord).\nHow do people get WNV?\nWNV is most commonly spread through the bite of an infected mosquito. WNV can be spread through blood transfusions, organ transplants, and from mother to baby during pregnancy, delivery, or breastfeeding but this is very rare. It is not transmitted from person to person, or from person to animal.\nWhat are the symptoms of WNV?\nMost people (70-80%) infected with WNV do not develop any symptoms.\nIf present, WNV symptoms usually appear 2-14 days after the mosquito bite. Approximately 1 in 5 people infected will develop a fever and possibly headache, body aches, joint pain, vomiting, diarrhea, or rash. Most people with these symptoms recover completely, but fatigue and weakness can last for weeks or months.\nLess than 1% of people infected will develop serious neurologic illness such as encephalitis or meningitis (inflammation of the brain or surrounding tissues). Recovery from severe illness may take weeks or months. Some of the neurologic effects may be permanent. Only about 10% of people who develop neurologic infection due to WNV will die.\nSerious illness can occur in people of any age. However, people over 60 years of age are at the greatest risk for serious illness. People with certain medical conditions, such as cancer, diabetes, hypertension, kidney disease, and people who have received organ transplants are also at greater risk for serious illness.\nSee your health care provider if you have symptoms of WNV.\nWho is at risk for WNV infection?\nAnyone living in an area where mosquitoes are infected with WNV is at risk. WNV has been detected in all states except Alaska and Hawaii. The risk of infection is highest for people who work outside or participate in outdoor activities because of greater exposure to mosquitoes.\nIs there a vaccine or treatment for WNV infection?\nThere is no vaccine or specific treatment for WNV infection.\nPeople with mild symptoms of WNV infection usually recover on their own. Over-the-counter pain relievers can be used to reduce fever and relieve some symptoms. People with severe illness usually need to be hospitalized to receive supportive treatment, such as intravenous fluids, pain medication, and nursing care.\nHow can I prevent WNV?Make you and your home a Bite-Free Zone to prevent WNV and other mosquito-borne diseases.\nWhat is the Chester County Health Department doing to prevent WNV?\n- Provides educational materials. Call 610-344-6490 to request materials.\n- Provides community education. Request a presentation or participation at a community event\n- Responds to complaints of standing water- Chester County Health Department enforces County regulations requiring property owners to dump and drain sources of standing water (ex. tires, pools, containers) which mosquitoes use for breeding. Citations may be issued for failure to comply.\n- Identifies bodies of water containing mosquito larvae.\n- Sets mosquito traps to collect and test adult mosquitoes for WNV – Traps are placed in highly populated areas, known mosquito breeding areas, and in areas where a resident has previously been identified as having a confirmed case of WNV infection. Traps are also placed in response to complaints from residents regarding high levels of mosquito activity.\n- Uses U.S. Environmental Protection Agency-approved products (Bti, Bs, or Methoprene) to kill mosquito larvae in bodies of standing water that cannot be drained.\n- Uses U.S. Environmental Protection Agency-approved products (Permanone or DeltaGard) to kill adult mosquitoes in areas that have high mosquito activity and multiple mosquito samples testing positive for WNV- Spraying is done as a last resort after exhausting all other mosquito control strategies.\n- The Chester County Health Department uses a truck-mounted sprayer to apply 1.5 ounces of the mosquito control product per acre of land. Sprays are conducted after sunset, when mosquitoes are most active and bees have returned to their hives. Sprayers are turned off near bodies of water and apiaries to protect aquatic life and bees. The Chester County Health Department also notifies beekeepers and residents who are listed as hypersensitive in a designated spray area prior to conducting a spray. People who are concerned about exposure to mosquito control products can reduce their potential for exposure by staying indoors with children and pets when their neighborhood is being sprayed. Because the mosquito control spray becomes inactive in just a few hours or with sunshine, it is not necessary to wash off outdoor furniture or playground equipment before use.\n- The Chester County Health Department is a member of the Environmental Protection Agency’s Pesticide Environmental Stewardship Program. This program requires participants to affirm that environmental stewardship is an integral part of their integrated pest management (IPM) practice, use current, comprehensive information regarding the life cycle of mosquitoes within their IPM program, educate the community on the benefits of IPM, and demonstrate a commitment to pesticide risk reduction activities.\n- Investigates reports of WNV illness in residents.\nHow can I find out when a mosquito control spray is being conducted in my neighborhood?\nThe Chester County Health Department notifies residents of sprays at least 48 hours ahead of time through the following channels:\n- News releases sent to the media, legislators, municipalities, etc.\n- Public Health Updates- E-mail updates that residents can sign up for.\n- Chester County Health Department website.\n- Chester County Health Department Facebook and Twitter.\n- Residents in a designated spray area who are listed as hypersensitive are contacted directly by the Chester County Health Department.\nPeople who are concerned about exposure to mosquito control products can reduce their potential for exposure by staying indoors when their neighborhood is being sprayed.\nFor more information, call 610-344-6752 or email firstname.lastname@example.org.\n- Prevent Mosquito-Borne Diseases\n- West Nile Virus Brochure- English, en Español\n- Zika Virus\n- Centers for Disease Control and Prevention- Avoid Mosquito Bites\n- Centers for Disease Control and Prevention- West Nile Virus\n- Penn State Extension- Pennsylvania Pesticide Hypersensitivity Registry\n- Penn State Extension- West Nile Virus\n- Pennsylvania's West Nile Virus Control Program","Understanding and Preventing CMV in the Educational Setting\nAuthor: Brenda Balch, M.D., AAP EHDI Chapter Champion for CT\nThe leading cause of sensorineural hearing loss is heredity, but many people don’t realize that the second most common cause of SNHL in children is due to Congenital Cytomegalovirus or cCMV. It is also the most common congenital viral infection in the United States and resulting hearing loss may be preventable. CMV can cause wide variation in hearing loss, including progressive loss.\nWhat is CMV?\nCytomegalovirus is a herpes virus that causes minimal to no symptoms in most people. In the U.S, by age 40, most of us have evidence of a past infection. CMV becomes a concern in primarily two scenarios – in a pregnant woman or in a severely immunocompromised individual. Most women are unaware of CMV and the risk of infection during pregnancy.\nWhat is the size of the CMV problem?\ncCMV is the most common congenital viral infection in the United States. Approximately 1 out of every 200 infants are infected with CMV prior to birth. With 3.8 million births in the US in 2018 we can assume that 19,000 children were infected with CMV.\nOf those who are infected, how many children end up with hearing loss?\n|Characteristics of CMV SNHL Hearing loss in infants/children with cCMV can be unilateral, bilateral, present at birth, late onset, fluctuating or progressive. ANY child with hearing loss could potentially have had it caused by cCMV!|\nOf the 1 in 200 infants infected with cCMV, approximately 10% will be “symptomatic” and have serious symptoms at birth that may include microcephaly, enlarged liver/spleen, cerebral palsy, cognitive impairment, vision loss and sensorineural hearing loss(SNHL). Another 10% – 20% of the 200 infected infants are “asymptomatic” will have or go on to develop SNHL. Using the 3.8 million births in the US in 2018 as an example again, 1900 would have had symptomatic CMV and 950-1900 would have had asymptomatic CMV that caused hearing loss.\nAudiological follow-up data for 860 children with congenital CMV. Dahle et al 2000, extrapolated by Walter 2017\n|Asymptomatic at birth, n=651||Symptomatic at birth n=209|\n|High frequency only||37.5%||12.9%|\n|Median age of delayed onset||44 months range (24-182)||33 months range (6-197)|\nCMV is so common! Can’t we test for it before it causes hearing loss and other problems?\n|If an infant is known to have passed the newborn hearing screen but has tested positive for CMV, the most recent JCIH statement recommends a full pediatric audiology evaluation by 3 months of age and then future monitoring “every 12 months to age 3 or at shorter intervals based on parent/provider concerns”.|\nPresently, most infants are not tested for cCMV at birth. Infants with obvious symptoms of cCMV are being tested, and in a few states with recent legislation, those infants who fail their newborn hearing screen are tested. If an infant is not tested for cCMV by 3 weeks of age, any positive test after 3 weeks of age may indicate an acquired infection rather than a congenital infection. It is therefore difficult to estimate what proportion of SNHL is due to congenital CMV in children outside of the newborn period.\nHow is CMV spread?\nCytomegalovirus is primarily spread through saliva, mucous and urine. Infants and young children are commonly shedding the virus. Small children have behaviors that are more likely to lead to the transmission of CMV. Women of child bearing age should be aware of the risks of congenital CMV and methods of prevention. Women who are pregnant or planning on becoming pregnant can take precautions that may reduce their risk of exposure to CMV. Clinical studies with antivirals for CMV and trials for a CMV vaccine are ongoing.\nHow can we prevent CMV?\nEducators who work with young children are at greater risk of contracting CMV and can help to prevent transmission of CMV by treating all body fluids as if they are infectious.\n- Wash hands frequently with soap and water, lathering for at least 15 seconds\n- Avoid kissing a child near the nose or mouth\n- Do not put things in your mouth that have been in a child’s mouth such as a pacifier, cups, utensils or food\n- Wear gloves for all contact with body fluids, and always wash hands after removing gloves\n- Use EPA approved disinfectants to frequently clean workplace surfaces that may be contaminated with body fluids\n- Do not use diaper wipes to clean potentially contaminated workplace surfaces\n- Disinfect small toys or objects that may have been contaminated with body fluids\nAny and all children, both in the classroom and in the home, or extended family setting, may potentially transmit CMV to a woman of childbearing age or a pregnant woman. It is prudent to use good hygiene precautions in all of these settings."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:6a411dea-76df-44fc-ae19-1e2dd7a03721>","<urn:uuid:89b2be35-1cce-4cd9-8eeb-0d24991e6917>"],"error":null}
{"question":"How does the zebra finch's significance in research compare between the VGP initiative and OHSU's studies?","answer":"In the Vertebrate Genomes Project (VGP), the zebra finch is highlighted as one of only three vocal learning bird orders among over 40 bird orders, with both male and female genomes being sequenced to create high-quality reference assemblies. Meanwhile, in OHSU's research, the zebra finch's importance stems from its unique genetic characteristics, particularly its mutated LDLR gene, making it a valuable model for studying cholesterol metabolism and viral interactions as part of a National Science Foundation-funded initiative where it serves as an important model in neurobiology.","context":["International Vertebrate Genomes Project releases first 15 new genomes\nMax Planck Society supports projects for high quality reference genomes\nThe international Vertebrate Genomes Project (VGP) is officially launched and releases 15 new reference genomes representing all five vertebrate classes – mammals, birds, reptiles, amphibians, and fish. These 15 genomes are the most complete versions of their species to date. The mission of the VGP is to sequence and assemble high quality, nearly error-free, and complete genomes of all 66,000 vertebrate species on Earth. The VGP data is currently being produced primarily by teams at three sequencing hubs: the Rockefeller University, USA, the Wellcome Sanger Institute, UK, and at the Max Planck Institute of Molecular Cell Biology and Genetics (MPI-CBG) in Dresden, Germany. Two of the 15 released genomes, a bat and a fish, have been sequenced and assembled in Dresden.\nWith its ambitious mission the VGP aims to address fundamental questions in biology, conservation, and disease including identifying species most genetically at risk for extinction and preserving their genetic information for future generations. The high-quality VGP genomes will become the main references for their species and will be stored in the Genome Ark, a digital open-access library of genomes.\nThe current Phase 1 of the VGP – the VGP orders project – aims to create reference assemblies of selected species representing all 260 vertebrate orders that have diverged from each other shortly after the last mass extinction 66 million years ago. Studying these ordinal level species will help scientists determine what type of species survived the previous extinction event that wiped out the dinosaurs. Those studies can also give insights into how other species could survive the current 6th mass extinction event and help identify genetic variants that might protect these species from total extinction. Amongst the 15 new genomes are critically endangered species like the platypus, and the Kakapo parrot. Other species include the zebra finch songbird and the Anna’s hummingbird, which like parrots, belong to the only three vocal learning bird orders among over 40 orders of birds. Also, two vocal learning bat species are part of this first data release.\nTo conduct the VGP, the umbrella G10K organization, from which the project arose, has convened over 150 experts from academia, industry, and government, from 12 countries, to develop high-resolution sequencing methods that both reduce costs and eliminate the errors that plague current reference genomes. Many current reference genomes are riddled with errors—parts of genes are missing, some are incorrectly assembled, and other genes are completely missing. Consequently, researchers are potentially working with incorrect gene sequences and structures hampering their genomic studies. The new VGP genomes eliminate most of these errors.\nThe MPI-CBG and in particular its bioinformatics researchers at the Center for Systems Biology Dresden (CSBD) is involved in the sequencing, assembly and annotation of the initial Phase I genomes of the VGP project with a focus on bats and fish. The Dresden scientists are part of the DRESDEN-concept Genome Center (DCGC) and have special expertise in using various long-read sequencing and long-range scaffolding technologies. The Dresden hub, led by Prof. Dr. Eugene Myers has contributed two genomes of the 15 released species: the greater horseshoe bat (Rhinolophus ferrumequinum) and the flier cichlid fish (Archocentrus centrarchus). In the future, about 10-20% of the VGP species are expected to be sequenced in Dresden. Prof. Eugene Myers, director at the MPI-CBG and founder of the CSBD says, “The advances in long-read sequencing is revolutionizing DNA sequencing. After a 10-year hiatus, this trend inspired me to return to genome assembly as I believe it implies that we will ultimately be able to produce near-perfect genome reconstructions. I think this capability is going to dramatically alter the landscape of genomics.”\nIn addition to the VGP, the MPI-CBG and the CSBD are actively engaged in synergistic international sequencing projects. The Bat1K project has the goal of sequencing all 1,300 bat species, many of which live unusually long or have near-perfect immune systems. Six bat genomes will be released in the near future, and another 25 species are being prepared to study aging, immunity, and vocal-learning in collaboration with the Bat1K consortium, which includes partners Sonja Vernes from the Max Planck Institute for Psycholinguistics in the Netherlands and Emma Teeling of the University College Dublin, UK. Another project is the Euro-Fish project, which aims to sequence almost all 600 species of fish swimming in European fresh waters. One of our main collaborators is Prof. Dr. Axel Meyer of the University of Konstanz. The Max Planck Society is funding the initial genomes from these synergistic projects. All the genomes will be sequenced to the high quality standard set by the VGP and will be placed in the Genome Ark repository, where one day all 66,000 vertebrates will be recorded.\nThe 15 genomes created through the VGP:\n1. Mammals (4 species)\n• Two bat species, Greater horseshoe bat (Rhinolophus ferrumequinum) and Pale spear-nose bat (Phyllostomus discolor), used as models for longevity and vocal learning\n• The Canada lynx (Lynx canadensis), once nearly extinct in the United States and now recovering\n• The duck-billed platypus (Ornithorhynchus anatinus), an egg-laying mammal with reptilian traits\n2. Reptiles (1 species)\n• A newly discovered turtle species from Mexico, Goode’s Thornscrub Tortoise (Gopherus evgoodei)\n3. Amphibians (1 species)\n• Two-lined caecilian (Rhinatrema bivittatum), a limbless amphibian that resembles a snake\n4. Birds (3 species, 4 genomes)\n• In addition to the kakapo (Strigops habroptilus), the VGP re-sequenced species from two other bird orders to represent the only three vocal learning birds among more than 40 avian orders\n• A male and female zebra finch (Taeniopygia guttata), the most commonly studied vocal learner\n• Anna’s hummingbird (Calypte anna), belonging to the smallest group of birds\n5. Fish (5 species)\nThese species represent a large diversity of traits and are used to study species evolution and adaptation:\n• Flier Cichlid (Archocentrus centrarchus), native to Central America\n• Eastern happy (Astatotilapia calliptera), also a cichlid fish Native to Lake Malawi, Africa\n• Climbing perch (Anabas testudineus), native to inland waters of Southeast Asia\n• Tire track eel (Mastacembelus armatus), native to rivers of Southeast Asia\n• Blunt-snouted clingfish (Gouania willdenowi), native to north Mediterranean coast, Syria to Spain\nAbout the MPI-CBG\nThe Max Planck Institute of Molecular Cell Biology and Genetics (MPI-CBG) is one of 84 institutes of the Max Planck Society, an independent, non-profit organization in Germany. 500 curiosity-driven scientists from over 50 countries ask: How do cells form tissues? The basic research programs of the MPI-CBG span multiple scales of magnitude, from molecular assemblies to organelles, cells, tissues, organs, and organisms.\nAbout the CSBD\nThe Center for Systems Biology Dresden (CSBD) is a cooperation between the Max Planck Institute of Molecular Cell Biology and Genetics (MPI-CBG), the Max Planck Institute for the Physics of Complex Systems (MPI-PKS) and the TU Dresden. The interdisciplinary center brings physicists, computer scientists, mathematicians and biologists together. The scientists develop theoretical and computational approaches to biological systems across different scales, from molecules to cells and from cells to tissues.\nAbout the DRESDEN-concept Genome Center (DCGC)\nThe DCGC is a joint sequencing center between the Technische Universität Dresden and the MPI-CBG. It is one of four DFG-funded German competence centers for next generation sequencing. The cooperative project is an amalgamation of employees of the TU Dresden and MPI-CBG, as well as of the CSBD, and the Center for Regenerative Therapies Dresden (CRTD). The center consists of three platforms focusing on long read sequencing technologies, single cell sequencing, and short read sequencing.\nAbout the Rockefeller University\nThe Rockefeller University is the world’s leading biomedical research university and is dedicated to conducting innovative, high-quality research to improve the understanding of life for the benefit of humanity. Our 82 laboratories conduct research in neuroscience, immunology, biochemistry, genomics, and many other areas, and a community of 1,800 faculty, students, postdocs, technicians, clinicians, and administrative personnel work on our 14-acre Manhattan campus. Our unique approach to science has led to some of the world’s most revolutionary and transformative contributions to biology and medicine. During Rockefeller’s 117-year history, 25 of our scientists have won Nobel Prizes, 23 have won Albert Lasker Medical Research Awards, and 20 have garnered the National Medal of Science, the highest science award given by the United States.\nAbout the Wellcome Sanger Institute\nThe Wellcome Sanger Institute is one of the world’s leading genome centres. Through its ability to conduct research at scale, it is able to engage in bold and long-term exploratory projects that are designed to influence and empower medical science globally. Institute research findings, generated through its own research programmes and through its leading role in international consortia, are being used to develop new diagnostics and treatments for human disease. To celebrate its 25th year in 2018, the Institute is sequencing 25 new genomes of species in the UK. Find out more at www.sanger.ac.uk or follow on Twitter @sangerinstitute.\nThe Vertebrate Genome Laboratory (VGL) at the Rockefeller University is a Resource Center specializing in ultra-High-Molecular Weight DNA (uHMW DNA) and long-read genomic technologies. The primary objective of the VGL is to generate at least one high-quality, phased, chromosome-level, annotated, reference genome assembly of all approximately 66,000 vertebrate species for the Vertebrate Genomes Project (G10K-VGP). The team is composed of four members, including a Director, two Research Support Specialists/Associates, and a Research Assistant. The VGL is equipped with four state of the art Pacific Biosciences Sequel™ sequencers, one Bionano Genomics Saphyr™ optical mapper, one 10x Genomics Chromium™ microfluidics platform, and all the necessary ancillary instruments for preparing uHMW DNA.\nProf. Eugene Myers\n+49 (0) 351 210 1900","New analysis unearths a genetic quirk in a small species of songbird along with its skill to hold a music. It seems the zebra finch is an incredibly wholesome chicken.\nA find out about printed as of late within the magazine Lawsuits of the Nationwide Academy of Sciences unearths that zebra finches and different songbirds have a low-density lipoprotein receptor (LDLR) gene unusually other than different vertebrates.\nThe serve as of LDLR, which is accountable for cell uptake of LDL-bound ldl cholesterol, or “dangerous ldl cholesterol,” has been regarded as conserved throughout vertebrates. OHSU scientists discovered that in relation to songbirds, key domain names for the serve as of the receptor have been missing.\nMutations like this are a genetic reason for critical excessive ldl cholesterol and excessive possibility for cardiovascular disease. Unusually, this has now not led to excessive ldl cholesterol for songbirds; they bring maximum in their ldl cholesterol in high-density lipoprotein, HDL, or “just right ldl cholesterol.”\n“Those songbirds appear to have tailored to the LDLR adjustments and feature advanced a wholesome ldl cholesterol profile, or ratio of low to excessive cholesterol,” mentioned senior writer Claudio Mello, M.D., Ph.D., professor of behavioral neuroscience within the College of Medication at Oregon Well being & Science College. “It means that songbirds can have some kind of coverage from heart problems.”\nIn flip, the invention may just result in a style to higher perceive—and in the long run enhance remedy—for heart problems in folks.\nThis unsuspected distinction genetic mutation additionally has implications for viral access, cell delivery programs and, doubtlessly, gene remedies.\nLDLR could also be the primary receptor for the G protein of vesicular stomatitis virus (VSV G) and are used to pseudotype, through coating, lentiviral vectors for gene manipulation in animals and gene treatment trials in people. The loss of key practical domain names in LDLR explains the low susceptibility of finches to lentiviruses, a circle of relatives that incorporates HIV, pseudotyped with VSV-G. But even so the consequences for bettering gene manipulation equipment in finches, this find out about illustrates the interesting co-evolution and interaction between viral access and fundamental cell delivery programs.\nThe genetic discovery is a part of a broader initiative to generate top of the range genome sequences in a lot of animals. It used to be additionally a part of a three-year initiative funded through the Nationwide Science Basis enabling researchers at OHSU and different establishments to check the genetic make-up of zebra finches, the most important style in neurobiology.\nTarciso A. F. Velho el al., “Divergent low-density lipoprotein receptor (LDLR) connected to low VSV-G-dependent viral infectivity and distinctive serum lipid profile in zebra finches,” PNAS (2021). www.pnas.org/cgi/doi/10.1073/pnas.2025167118\nOregon Health & Science University\nGenetic discovery in songbird supplies new insights (2021, April 26)\nretrieved 27 April 2021\nThis record is matter to copyright. Aside from any honest dealing for the aim of personal find out about or analysis, no\nsection could also be reproduced with out the written permission. The content material is equipped for info functions handiest."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:b93690a2-7353-4476-97af-2e4085b152c4>","<urn:uuid:02aa856c-dce4-4ab7-bd31-271bc0747c7d>"],"error":null}
{"question":"I need help understanding environmental management systems - what exactly is an EMS according to ISO 14001, and what internal/external factors influence it?","answer":"An Environmental Management System (EMS) consists of policies, processes, plans, practices, and records that define how a company interacts with the environment. It must be tailored to each company's specific legal requirements and environmental interactions. Regarding influencing factors, internal factors include strategies to conform to policies, relationships with staff and stakeholders, resources and knowledge, products/services, and other adopted standards. External factors that affect the EMS include government regulations, economic shifts in the market, events affecting corporate image, technological changes, and the needs of interested parties such as customers, suppliers, regulators, and neighbors.","context":["The idea that organizations must take a more proactive role in their environmental impact and social responsibility is nothing new, nor is it like to go away any time soon. Political platforms, external pressures from consumers, and corporate image all play a part in keeping the environment as a top of mind issue for most of today’s companies regardless of size.\nISO 14001 is one of the leading international environmental management system standards that was designed to help organizations minimize how their operations negatively impact the environment (i.e. cause damages to air, water, or the earth in general). But, what does that all mean, exactly? And how does certification help a company accomplish its economic goals? What’s involved and what does it all entail?\nBelow are 10 of the top resources on the web that we’ve aggregated to help you answer these questions and fill in the gaps for anyone who may be looking to get started with ISO 14001 certification\nISO 14001:2015 Why Is It so Important? - ISO.org\nWhat better place to start than straight from the source, or in this instance the governing body: On ISO 14001:2015, “It maps out a framework that a company or organization can follow to set up an effective environmental management system.’ ‘…Using ISO 14001:2015 can provide assurance to company management and employees as well as external stakeholders that environmental impact is being measured and improved.” Use the link to find out why this standard is considered to carry a hefty value. Watch…\nWhat is an Environmental Management System? – Advisera.com\nAccording to Advisera, an online ISO 14001 consultant, “An environmental management system, often called an EMS, is comprised of the policies, processes, plans, practices, and records that define the rules governing how your company interacts with the environment. This system needs to be tailored to your company because only your company will have the exact legal requirements and environmental interactions that match your specific business processes. However, the ISO 14001 requirements provide a framework and guidelines for creating your environmental management system so that you do not miss important elements needed for an EMS to be successful.” Read more…\nInvesting & Financial Analysis of ISO 14001 – Investopedia.com\n“ISO 14001 standards state that an organization must define and document the scope of its environmental management system. Adopters of the standards must also determine risks that need to be addressed to ensure that the system can achieve its intended outcomes. Firms must take clear and well-defined action to adhere to their stated environmental management system, and make prompt adjustments when needed.” Read more…\nWhat Are Some of the Potential Benefits of an EMS Based on ISO 14001? – EPA.gov\nWhy an Environmental Management System Does More Than Just Help the Planet – Smithers.com\n- Improvements in overall environmental performance and compliance.\n- Provide a framework for using pollution prevention practices to meet EMS objectives.\n- Promote predictability and consistency in managing environmental obligations.\n- More effective targeting of scarce environmental management resources.\nWe covered EMS certification and the benefits organizations can gain in addition to environmental improvements in a past post:\nThe History of ISO 14001 Through the Lens of the First Multinational Company to Earn Global Registration\n- Major enhancements to corporate image worldwide\n- Improving opportunities and access to markets in other countries for exporting goods and services\n- Noticeable growth in relationships with customers and in the public eye (corporate responsibility)\n- Meeting or exceeding regulations and keeping stakeholders happy\n“Since 1997, IBM has expanded its global ISO 14001 registration to include additional entities such as our research locations that use chemicals, several IBM organizations at a country level, as well as our Procurement and Supply Chain, and Global Asset Recovery Services organizations.” Read more…\n5 Steps to ISO 14001:2015 -Ramboll.com\n(by: Greg Roberts)\nA comprehensive report that breaks down the ISO 14001 standard a five of the most crucial steps toward successful certification and maximizing your environmental returns. Read more…\nThe Best and Worst Aspects of ISO 14001 – IvoryResearch.com\n(by: Mary Taylor)\nBefore you make an important investment, like the undertaking and implementation of an ISO 14001 EMS, it’s critical to look at all sides of the issue and fully understand the scope of what you’re getting into. Ivory Research does a nice job of laying out both pros and cons for ISO 14001. Read more…\n7 Steps in Handling Waste - Advisera.com (by: Strahinja Stojanovic)\nA key component to environmental safety contributions from an organization is how it handles waste, and although it may not always seem like it, there is more to waste management than just taking out the trash. In fact, many companies deal with hazardous materials on a regular basis, and if they are not disposed of properly, these materials can endanger both environmental and human health. Advisera.com appears on our list for the second time as they produced an excellent article breaking down how ISO 14001 relates to waste management and some common methods for meeting the requirement. Read more…\nISO 14001 EMS Checklist – ISO.org\nWho doesn’t love a checklist? ISO.org offers a streamlined and visually pleasing ISO 14001 checklist for small businesses. So, if your company is on the small to medium size you can gain a lot of insight and reassurance by downloading this document and go through it before your schedule an ISO 14001 audit. Download it here\nAs with most elements of a successful business, an effective and efficient environmental management system requires hard work and diligent practices. This post is not meant to be all encompassing of every possibility, as every organization has some needs that are unique to it and those might not be addressed here. The above collection of resources is meant to be a great jumping off point for those interested in educating themselves on the requirements, possible benefits, and actions needed to prepare themselves, and their companies, for possible ISO 14001 certification. All content is credited to the specific website and authors listed under each heading, and to the ISO 14001 standard itself.\nLearn how Smithers helps companies get certified to ISO 14001:2015","Understanding the Organization and its Context from an Environmental Perspective\nAs with any journey in life, there must be a clear understanding of where we are starting from and where we want to get to. ‘Sat Navs’ tell us our starting point when they work out our route, however as yet no one has come up with technology to do this in terms of ISO 14001. That is why defining 'Organizational Context' is so important in Environmental Management System standards.\nContext is a thorough determination and understanding of the internal and external issues that can impact an organisation and its objectives. It can be done for all organizations irrespective of size, industry or geographical location.\nPut more simply - the expected outcome of ISO 14001 is ‘improvement in environmental performance’. There are things that will help achieve this and some things that will get in the way. Defining the context is simply working out and understanding what these are.\nThe EMS is then focussed on maximizing the positives and mitigating the effects of the negatives. Alongside this is an evaluation of those people or organizations that also may be affected by or have an interest in what you do.\nA common criticism of previous versions of the standard was the lack of connection of environmental topics to everyday business operations. They were often viewed as separate and not relevant to many employees.\nThorough determination of organizational context helps solve this. By identifying internal and external issues, known as risks and opportunities they can be used to steer the strategic direction and purpose of the business. The management system then becomes a useful part of daily business and a relevant platform for employees to track their own performance.\nEnsuring that this clause is fulfilled helps connect high-level company strategic goals to the tasks and work across all levels and functions. Although the standard doesn’t prescribe the method for determining the context of the organization, there are some logical steps and milestones.\nWhat are internal and external issues?\nYour organization's internal context is the environment in which you aim to achieve your objectives. Internal context can include your approach to governance, your contractual relationships with customers, and your interested parties.\nInternal issues can include your:\nStrategies to conform to your policies and achieve your objectives\nRelationship with your staff and stakeholders, including partners and suppliers\nResources and knowledge (e.g. capital, people, processes and technologies)\nProduct or service\nOther standards, guidelines and models adopted by the organization\nTo understand your external context, consider issues that arise from your social, technological, environmental, ethical, political, legal and economic environment.\nExternal issues may include:\nGovernment regulations and changes in the law\nEconomic shifts in your market\nEvents that may affect your corporate image\nChanges in technology\nShould we care about other people’s opinions?\nIn simple terms, the requirement for identifying relevant interested parties means that you need to decide whose opinion about your company you should care about.\nInterested parties include direct customers, end users, suppliers and partners, regulators, and neighbours. Others could include people in the organization, owners/shareholders, and even society in general. These parties add value to the organization or are impacted by the activities within the organization. Identifying and meeting their needs is important to implementing an efficient and effective environmental management system. Their feedback can really help you to determine what can be improved in your organization, and how.\nYour interested parties include customers, partners, employees and suppliers. When developing your EMS, you only need to consider interested parties that can affect your:\nAbility to consistently provide a product or service that meets your customers' needs and any statutory requirements and regulations\nContinual improvement process\nAbility to enhance customer satisfaction through effectively applying your system\nProcess for ensuring you conform to your customers' requirements and any statutes or regulations that apply along with any other relevant commitments you have made.\nDo I have to document it all?\nOne of the areas misunderstood by many with this clause is related to documenting the results of this clause. ISO 14001 uses the term ‘maintain documented information’ whenever there is a formal requirement to document something. The standard does not make that declaration for this clause or its outputs.\nThe Annex of the standard (A4.1) details that the intent of this clause is to provide a ‘high level, conceptual understanding’ of the relevant and identified issues.\nOf course, clauses later in the standard building on the foundation set by this clause do require documenting such as the risks and opportunities that need to be addressed.\nWhen and if do I have to review it?\nISO 14001:2015 clause 9.3 (management review) states that the review process shall include consideration of changes in:\nExternal and internal issues that are relevant to the EMS\nThe needs and expectations of interested parties , including compliance obligations\nThese of course are the two main areas covered in ISO 14001:2015 clause 4 - ‘Context of the Organization’.\nThe introduction of this clause has ensured that the EMS is no longer dictated by the standards, internal/external auditors or the environmental manager. Instead, it is the top management in an organization that is empowered, providing them with the authority and flexibility to define their management system and ensure that it meets the expected outcome - an improvement in the environmental performance of their organization.\nAuthored by: Richard Walsh, NQA UK Principal Energy & Environment Assessor\nTo check out what ISO 14001 Environmental Management training NQA offers click here."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:e55be8d5-9d14-4e78-a7c2-8ac105af4786>","<urn:uuid:890b938f-cd39-429c-bb71-7e948686813c>"],"error":null}
{"question":"How do the use cases of Augmented Reality differ between architectural design and archaeological research?","answer":"In architectural design, AR is primarily used for project visualization, design collaboration, and real estate marketing. Architects can view their 3D models in real-time during the design stage, collaborate with others on the same model, and show clients how projects will look before construction begins. They can also create immersive VR home tours for potential buyers. In archaeology, AR serves different purposes - it helps researchers visualize and reconstruct historical sites by overlaying archaeological features onto contemporary landscapes. Systems like VITA (Visual Interaction Tool for Archaeology) allow users to explore excavation data remotely and collaborate by navigating, searching, and examining archaeological data. While architectural AR focuses on future constructions and design visualization, archaeological AR helps understand and recreate past structures and landscapes.","context":["Since the advent of Augmented Reality, one of the categories to most embrace this new technology has been that of architects and designers. The reason is quite simple, these professionals were already accustomed to design 3D projects and to deal with 3D models, so they have always had a lot of material to be fed to Augmented Reality software.\nEven tough 3D models have been at the base of almost any architectural and civil engineering project in the last two decades, for a long time many professionals have continued working with renderings, a bidimensional representation of the project made using the 3D models, basically pictures from a few different points of view of the 3D representation.\nOver time, this way of presenting the project has begun to hold out against professionals in the sector and their clients, essentially for two reasons. The first is that with the renderings many details and many sensations related to the visualization of the project are lost; the second is that with the renderings one renounces to fully exploit the potential of a 3D model, especially since BIM has begun to establish itself.\nWhy is Augmented Reality so important in Architecture?\nWith the Building Information Modeling (BIM) market valued at $5,205 million in 2019, and projected to reach $15,892 million by 2027 (Source: Allied Market Research), the use of digital content and the digitalization of assets management are strongly pushing nearly any mid-sized to large project towards a massive use of 3D, not only for design purposes, but also for visualization and building management.\nIn addition, the use of photogrammetry in relation to construction sites and the management of existing sites is also increasing, with an increasing use of drones for photogrammetric reconstructions. Even on everyday devices, including smartphones, tools such as Lidar are starting to arrive that will make photogrammetry and the ability to process accurate 3D reconstructions at low cost more and more accessible.\nThis abundance of 3D assets, with a lot of information embedded, play a vital role in the adoption of Augmented Reality for Architecture.\nAugmented Reality Use Cases in Architecture\nWe analyzed a few use cases where we believe Augmented Reality can have a strong impact on the way AEC professionals approach their Architectural projects.\nUse Case 1 – Project Design and Collaboration\nArchitects nowadays mostly work with 3D modeling software during the design stage of a project. While working with the model they may be able to check how they are progressing by viewing their project in Augmented Reality mode. This kind of view gives them better hints for the parts of the model to be improved and the AR visualization may even be done at a 1:1 scale.\nArchitectural visualization using AR in the design stage is also very useful when there is some degree of collaboration involved. The viewers may see the same 3D model, in real-time and discuss together what must be changed.\nUse Case 2 – Project Visualization\nProject visualization works very similarly to project design, the main difference is that now the project is complete and the client wants to see it even before the construction site is open.\nAugmented Reality visualization allows the Architect to show the client how the project looks like in a very realistic manner.\nThe 3D model can be used for AR visualization on an image, like a planimetry, or in the space, including the construction site where the model can be viewed at its actual size, including the interiors, allowing the user to walk inside the 3D model as if it were a real building. The use of stereoscopy would make the entire visualization experience more immersive.\nUse Case 3 – Marketing and Real Estate for Architectural Projects\nFor the purpose of selling or promoting a real estate property, Augmented Reality comes really handy because it allows the sales force to easily build a more immersive representation of a project that may exist just on paper because its construction has not yet begun.\nThat is when AR brochures prove to be very effective. The prospect can point their device to some marketing materials and explore the 3D models of the project. These models may even have some interest points that the prospect may touch to enable additional information, such as 360? views of the interiors, videos and audio explanations.\nAnother marketing application in the real estate field concerns the use of immersive VR home tour, allowing the potential buyers to get an idea of a property even before visiting it, helping them to shortlist for the real visit, only the properties that they liked the most.\nUse Case 4 – Site Management and Maintenance\nA last set of use cases for Augmented Reality in Architecture is related to all the activities that must be carried out while the construction process is ongoing and when the building is finally built.\nDuring the construction stage AR offers advanced tools to assess if structural clashes may arise and to assess if the construction process is progressing according to the schedule.\nWhen the construction stage is over, AR can be used to support workers in a set of tasks including operations and maintenance, like repairing pipelines, fixing air conditioners, replacing parts of the boiler and so on.\nHow to easily use Augmented Reality for Architecture\nAugmented Reality can support Architects and BIM Managers in a very wide array of activities, from project design to assets management.\nThe wide availability of 3D models and BIM models makes the adoption of Augmented Reality tools easier today than it was a few years ago. Also mobile devices are becoming more and more powerful and they come now with embedded hardware that can be useful to create photogrammetric reconstructions, like the Lidar.\nIn this scenario AR-media is the ideal tool to build and publish Augmented Reality projects for Architecture, as it requires no coding skills and allows the user to create AR content with a visual, simple workflow. Get a free AR-media account and start creating, you can rely on detailed tutorials and documentation available in the support section.","What is Augmented Reality? How does it work? What is the application of Augmented Reality in Daily life? Read further to know more about Augmented Reality.\nWhat is Augmented Reality?\nThe real-time integration of digital information with the environment of the user is known as Augmented Reality (AR). Users of Augmented Reality encounter a real-world environment with created perceptual information superimposed on top of it, as opposed to Virtual Reality (VR), which produces a completely artificial environment.\nUsing Augmented Reality, users can receive more information or have natural environments aesthetically altered in some way.\nThe main advantage of Augmented Reality is that it successfully combines digital and three-dimensional (3D) elements with how people perceive the real world.\nAR has several applications, from entertainment to aid in decision-making.\nThrough a device like a smartphone or glasses, Augmented Reality provides the user with visual elements, sound, and other sensory information.\nIn order to provide a seamless experience where digital information modifies the user’s view of the actual environment, this information is layered onto the device. A part of the natural world might be hidden or added to by the superimposed information.\nThe most well-known consumer Augmented Reality (AR) devices right now include Google Glass, smartphone games, and heads-up displays (HUDs) on automobile windscreens.\nBut a lot of other businesses, like healthcare, public safety, gas and oil, travel, and marketing, also use this technology.\nHow does Augmented Reality Work?\nThere are many ways to provide augmented reality, including through smartphones, tablets, and eyewear.\nAdditionally, contact lens-delivered AR is being developed.\nHardware elements like a processor, sensors, a display, and input devices are needed for the technology. This hardware is often already present in mobile devices, including sensors like cameras, accelerometers, GPS, and solid-state compasses.\nThis makes AR more approachable for regular people. For instance, a GPS can be used to determine the user’s location and its compass can be used to determine the direction of the device.\nMachine vision, object identification, and gesture recognition are potentially possible features of sophisticated augmented reality training programmes utilised by the military.\nBecause AR can be computationally demanding, data processing can be offloaded to another machine if a device is underpowered.\nIn order to connect animation or contextual digital information in the computer programme to an augmented reality marker in the actual world, augmented reality apps are created in specialised 3D programmes.\nWhen an AR app or browser plugin on a computing device receives digital data from a recognised marker, it starts to run the marker’s code and layer the appropriate image or images.\nDifferences Between AR and VR\nVirtual Reality (VR) is a virtual environment made using software and presented to users in a way that causes their brains to temporarily suspend disbelief so they can accept the virtual environment as the actual world.\nThe main way to enjoy Virtual Reality is via a headset that includes both sight and sound.\nThe main distinction between Augmented Reality (AR) and Virtual Reality (VR) is that while VR totally immerses users in a virtual environment, AR takes the existing real-world environment and overlays virtual information on top of it. AR inserts the user in a kind of mixed reality, whereas VR immerses them in a brand-new, virtual experience.\nThis is also accomplished using various tools. Virtual reality (VR) uses head-mounted displays (HMDs) to show users simulated visual and aural information. Less constrained AR devices often include smartphones, eyewear, projections, and HUDs in automobiles.\nPeople are placed into a 3D environment in Virtual Reality (VR), where they may move around and interact with the environment that is being built. However, AR keeps users rooted in the physical environment by superimposing virtual data as a visual layer on top of it.\nTechnology Used for Augmented Reality\nA CPU, display, sensors, and input devices are examples of hardware for Augmented Reality. These components, which frequently include a camera and MEMS sensors like an accelerometer, GPS, and solid-state compass, are present in contemporary mobile computing devices like smartphones and tablet PCs, making them excellent AR platforms.\nThe technologies diffractive waveguides and reflective waveguides are both employed in Augmented Reality.\nThe representation of Augmented Reality employs a variety of technologies, including optical projection systems, displays, handheld devices, and display systems that are worn on the human body.\nA head-mounted display (HMD) is a display gadget that is strapped to the forehead or put on a helmet. HMDs, overlay the user’s field of vision with images of the real environment and virtual things.\nIn order to match virtual information with the real environment and adapt to the user’s head motions, modern HMDs frequently include sensors for six degrees of freedom tracking.\nWithout the aid of specialised displays like monitors, head-mounted displays, or handheld gadgets, projection mapping enhances real-world objects and settings.\nDigital projectors are used in projection mapping to project graphics onto real-world objects.\nThe main distinction in projection mapping is the separation of the display from the system’s users. Projection mapping scales up to groups of people organically because displays are not connected to individual users, enabling collocated collaboration between users.\nMotion Tracking technologies used by contemporary mobile augmented reality systems include radio-frequency identification, accelerometers, GPS, gyroscopes, solid-state compasses, digital cameras, and/or other optical sensors (RFID).\nThese technologies provide various degrees of precision and accuracy.\nThe widespread use of mobile devices, particularly wearable ones, has increased the appeal of mobile augmented reality applications. They do, however, frequently rely on computer vision techniques that are computationally demanding and have strict latency requirements. Data processing is frequently wanted to be offloaded to a distant machine to make up for the lack of computing resources.\nTechniques include speech recognition systems, which turn a user’s spoken words into computer instructions, and gesture recognition systems, which analyse a user’s physical movements using either visual detection or sensors built into a peripheral device like a wand, stylus, pointer, glove, or other body wear.\nIn order to synthesise and position augmentations, the computer analyses the perceived visual and other data. The graphics that go with augmented reality are created by computers.\nA computer-generated image is used in augmented reality, which dramatically changes how the real world is displayed.\nAugmented reality will drastically alter people’s perspectives of the real world as technology and computers advance.\nAR content can also be shown on projectors. On a projection screen, a virtual item can be thrown by the projector, and the viewer can interact with it. Various things, including walls and glass windows, can serve as projection surfaces.\nApplications of Augmented Reality\nThe usage of AR has aided archaeological study. AR enables archaeologists to generate potential site configurations from existing structures by overlaying archaeological characteristics onto the contemporary landscape.\nEarly archaeological augmented reality applications have reused computer-produced representations of ruins, structures, landscapes, or even ancient individuals. Implementing a system like VITA (Visual Interaction Tool for Archaeology), for instance, will enable users to visualise and explore real-time excavation data from the comfort of their homes.\nEach user can work together by “navigating, searching, and examining data” for each other.\nAR can help with project visualisation.\nBefore a building is physically built on a site, computer-generated images of that building can be superimposed on a local view of that location.\nAdditionally, AR can be used at an architect’s office to produce animated 3D visualisations of their 2D designs.\nAR apps can improve architectural sightseeing by enabling people to virtually look through a building’s walls to observe its inside furnishings and layout.\nAR has been utilised in educational contexts to supplement a regular curriculum. It is possible to overlay text, graphics, video, and music on top of a student’s actual environment.\nTextbooks, flashcards, and other reading materials for school may have “markers” or triggers built in that, when scanned by an augmented reality device, provide the student with additional information presented in a multimedia manner.\nAR is utilised to replace paper manuals with digital instructions that are superimposed on the field of vision of the manufacturing operator, hence requiring less mental effort to operate.\nBecause operators have direct access to a machine’s maintenance history, AR facilitates efficient machine maintenance.\nSince digital instructions can be modified and distributed more easily than physical manuals, virtual manuals aid manufacturers in adapting to constantly evolving product designs.\nBefore making a purchase, customers can utilise a store’s internet app to see items like furniture in their homes.\nEntertainment and Gaming\nWith the use of Augmented Reality, users can animate their faces in fun and imaginative ways on social media or overlay a virtual game in a real environment.\nA route to the user’s destination can be added using augmented reality to a real-time view of a road. When utilised for navigation, Augmented Reality can also show details about nearby businesses.\nData that shows destination directions, travel times, weather information, and road conditions can be shown on a car’s windscreen.\nExamples of Augmented Reality\nA Target retail app feature, allows users to take a photo of a space in their home and digitally view an object, such as a picture on the wall or a chair, to see how it will look there.\nApple Measure app\nThe Measure app for Apple iOS functions similarly to a tape measure, allowing users to select two or more points in their environment and measure the distance between them.\nSnapchat filters use augmented reality to apply a filter or mask to the user’s Snap or picture.\nPokemon Go is a popular mobile augmented reality game that uses the player’s GPS to detect where Pokemon creatures appear in the user’s surroundings for them to catch.\nGoogle Glass is the company’s first commercial attempt at a glasses-based augmented reality system. Users can work hands-free with this small wearable computer.\nGoogle is also developing another pair of glasses for 2022 that will overlay a live transcription or translation of what someone else says in the text.\nThe United States Army employs augmented reality in an eyepiece known as Tactical Augmented Reality (TAR). TAR attaches to the soldier’s helmet and assists in locating the position of another soldier.\nAs the popularity and familiarity of apps and games like Pokemon Go or retail store AR apps grows, so does AR technology. The expansion of 5G networks, for example, may make it easier to support cloud-based augmented reality experiences by providing higher data speeds and lower latency to AR applications. Modern advancements in development, such as Google’s smart glasses that can live translate audio to text, will change the way people who speak different languages communicate. Because AR employs immersive technology, more opportunities and experiences across various platforms and media types are on the way.\nTo get more such notes Click Here\nArticle written by: Aryadevi E S\nLeave a Reply"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:198c3cde-2502-4934-9115-5a57c7f498e9>","<urn:uuid:80c53a5b-cf1f-44d0-a34a-e1cf0ff5ef5b>"],"error":null}
{"question":"I'm renovating my house and need safety advice: between unattended candles and damaged extension cords, which is considered a more serious fire hazard in residential settings?","answer":"Both are serious fire hazards. Unattended candles are explicitly highlighted as a major concern, being the 5th leading cause of house fires and accounting for 3% of home fire deaths, with multiple documented incidents of house fires caused by unattended candles. Damaged extension cords are listed as one of the unsafe home conditions that can lead to fire and are specifically mentioned in electrical safety guidelines requiring immediate replacement. However, electrical issues collectively (including damaged cords) cause more extensive damage, affecting over 24,000 homes annually.","context":["Candles are the 5th leading causes of house fires and account for 3% of all home fire deaths. Never leave a burning candle unattended.\nOklahoma City, OK. — An unattended candle was determined to be the cause of a fire Thursday morning at a southwest Oklahoma City home near Southwest 36th Street and Blackwelder Avenue.\nCrews responded to the fire at around 5 a.m., and they couldn’t find anybody inside the house. However, authorities said the homeowner arrived and confessed to firefighters that he had left an unattended candle burning in the front room.\nA dog that was in the home got outside safely and no injuries were reported.\nFlorence, S.C. — Florence Fire Department crews extinguished a house fire that was caused by an unattended candle.\nFirefighters arrived at a Rebecca Street home at 12:01 a.m. Tuesday morning and found smoke coming from the home.\nFirefighters had the fire under control in 20 minutes and no injuries were reported.\nThe Florence Fire Department responded with 3 Engine companies, a Ladder company and a Command Officer. In addition, the Florence Police Department and Florence County EMS were on scene.\nThe Red Cross assisted the homeowners.\nAjax, Ontario, Canada — A house fire broke out February 22 in Ajax’s Taunton Road and Westney Road area and an investigation determined the blaze was caused by a lit candle left unattended in an ensuite bathroom. No one was harmed in the fire.\n“Fire can happen anywhere, at any time. Most often, it strikes when we let our guard down. It is imperative that you keep candles away from anything that can burn. If you use real candles, please remember to blow them out when you leave the room,” Shelley Langer, Ajax Fire Prevention Inspector.\nThe Inspector also reminded residents of the importance of smoke alarms and carbon monoxide (CO) detectors.\nSmoke alarms should be installed on all levels of a home and outside all sleeping quarters.\nSmoke alarms should be tested every month and their batteries should be replaced yearly and should be replaced every ten years.\nCO detectors should be installed outside every sleeping area in a home with a fuel burning appliance, fireplaces, or with an attached garage. CO detectors should be tested every month and their batteries should be replaced yearly. CO detectors last about five to seven years.\nMiddleboro, MA —A seriously injured man was rescued a from a third-floor window of a veterans’ home that caught fire by a Massachusetts State Police trooper officials said.\nAt about 1:12 a.m. on Tuesday, the Middleboro Fire Department responded to a house fire at the Acorn Hill Home at 285 West Grove St.\nAccording to Fire Chief Lance Benjamino, first responders found heavy fire coming from the upper two stories of the domicile when they arrived.\nThe Middleboro Fire Department extinguished a fire at a home for veterans at 285 West Grove St. on Tuesday, Feb. 22, 2022.\nJohn Hagerty, a State Police trooper was passing by the home when he discovered the fire and he used a ground ladder to rescue a seriously injured man from the third floor, according to officials. The man was hospitalized as was Hagerty was also injured during the daring rescue and was taken to a local hospital for treatment.\nBenjamino said crews “mounted an aggressive exterior fire attack” then went inside to finish extinguishing the fire. It took about 45 minutes for firefighters to get the fire under control.\n12 other residents who live in the house were able to evacuate on their own, officials said. Two of those residents were evaluated on scene, but declined to be taken to the hospital.\nThe home is considered a total loss as a result of the heavy fire and smoke damage, meaning 13 residents were displaced who are being assisted by the American Red Cross.\nState Fire Marshal Peter J. Ostroskey and Benjamino said that the fire was caused by an unattended candle in a second-floor bedroom.\n“As we saw here, the flame from one small candle can cause a fire that destroys a home,” Benjamino said. “Given the early hour, we’re very lucky more people weren’t injured or worse. Always exercise caution with candles, and never leave a burning candle unattended.”\nFirefighters from the Lakeville, Raynham and Carver fire departments assisted the Middleboro crew and Wareham firefighters provided station coverage during the incident.\nCandles are the 5th leading causes of house fires and account for 3% of all home fire deaths so, here are some candles burning tips from the Mohawk Valley Trading Company:\n- Never leave a burning candle unattended.\n- Keep burning candles away from pets and children.\n- Keep burning candles away from flammable materials.\n- Remove paper label from candle before lighting.\n- Burn candles only in a draft-free environment away from open windows, fans, air ducts, etc. This will help to prevent any smoking or dripping.\n- Light the candle wick from the base of the wick, where the wick comes out of the candle. This allows the beeswax to be absorbed into the wick.\n- Keep the wick trimmed to about 1/4″ when burning.","Electrical Fire Safety Info for Cables and Other Home Equipment\nWhile electricity is a wonderful aspect of modern life, it can be dangerous if we aren't careful. Electrical fires are a common occurrence in the United States, causing injuries, claiming lives, and resulting in losses of property. The majority of electrical fires are a result of old wiring and faulty outlets. Damage to appliance cables and extension cords and plugs can also lead to electrical fires in the home. To help prevent electrical fires in your home, it’s important to check your electrical appliances and wiring on a routine basis. Use the following information and tips to learn more about electrical fires, hidden hazards, statistics, circuit interrupters, and safety tips.\nFacts and Figures\n- More than 15 million appliances have been recalled in the last five years for defects that could result in a fire, including 1,942 incidents reported.\n- According to data analyzed by the National Fire Incident Reporting System (NFIRS), more than 69,000 fires were reported from 2002 to 2009 associated with appliances.\n- Residential appliance fires result in an estimated 9,600 fires, 525 injuries, 25 deaths, and $211 million in property loss annually.\n- Nationwide, there is a home fire death an average of every three hours.\n- According to the National Fire Protection Association, fire statistics data reported that 85 percent of fire deaths occurred in the home in 2009.\n- Each year fires started in electrical systems or lighting equipment result in damage to more than 24,000 homes and cause 830 civilian injuries and 320 civilian deaths.\nHidden Electrical Hazards\nExamples of unsafe home conditions that can lead to fire include:\n- Damaged extension cords, electrical conductors or plug wires\n- Short or overloaded circuits\n- Use of modified, unapproved or faulty electrical equipment\n- Insufficient clearance between combustibles and electrical heating equipment\n- Loose electrical connections\n- Running power cables through walls to a wall-mounted TV. This problem is quickly remedied with a recessed power outlet. Learn how to hide your TV cables safely.\nWarning signs of electrical dangers include:\n- Reoccurring or frequent issues associated with blowing fuses in appliances or tripping circuit breakers\n- Tingling sensation as you touch an electrical appliance\n- Sparks coming from an outlet or discolored or warm wall outlets\n- A continuous burning smell from a room or appliance\n- Lights that frequently flicker or dim\nElectrical Circuit Interrupters\nElectrical circuit interrupters protect against fires by monitoring electrical current. These devices respond to overloads and short circuits, such as with circuit breakers. Electrical circuit interrupters help to save lives and make homes safer by eliminating a significant source of electrical-related fires. When an electrical switch is opened or closed, a discharge of electricity occurs across the circuit, known as arcs. When arcs are unintentional, such as when wires or cords are loose or damaged, sparking can result in possible igniting of combustibles. These devices do not interfere with smoke alarms and other appliances and do not affect power supply reliability.\nWhen you run cable through any building you first need to consider plenum spaces. Plenum spaces are the open spaces above the ceiling or below the floor that are used for air circulation. Technically, any ductwork is considered a plenum space too. These spaces are important for air circulation in any building, but they also bring some problems if you ever have a fire because of the high oxygen content and lack of fire barriers. Using untreated cables in plenum spaces can spread the fire to other areas very quickly and spread noxious smoke throughout the building. Plenum cables are coated with flame retardant and made using special plastics that don’t smoke nearly as much as other plastics to help prevent this problem. Any cable that you run through plenum spaces must be plenum rated, even wiring used for information transfer, like Cat5 wiring.\nThe problem with plenum cables is that they are often more expensive, which is why people can become tempted to cut corners and use non-treated cables. An easy and far less risky solution is to use unterminated bulk cable and terminate it yourself. This process is fairly straightforward with most types of wiring, but Cat5 wiring can be a bit more complicated. To help you out we’ve made a Cat5 wiring diagram that shows you what you need to do.\n- Consider having the wiring inspected in your home if you live in a house that is over ten years old. If the house is over forty years old, an inspection should be immediate.\n- Always follow the usage instructions on all appliances.\n- Fix electrical or appliance problems right away. Use caution if fuses blow often, switches become hot or cause shocks, or circuit breakers trip often.\n- Never place electrical cords under bedding or rugs. Sparks or heat from the wiring can cause a fire.\n- Check labels on lighting fixtures and lamps to ensure you are using the right size bulbs. Check the label on the fuse box to ensure you are using the right size fuses.\n- Always cover unused electrical outlets with plastic plugs.\n- If a cable or cord has three-prongs, always use it properly. Do not remove the extra prong as the appliance must be grounded to reduce the risk of electrical shocks.\n- Do not overhaul electrical outlets. Extension cords should not be used as permanent cords in homes.\n- Routinely check your electrical appliances, electronics and wiring for signs of damage.\n- Replace all damaged, old, or worn appliance cords immediately and do not use appliances with frayed wiring.\n- Keep electrical appliances away from wet counters and floors. Be cautious when using electrical appliances in the kitchen and bathroom.\n- Do not allow children to play with or around electronics or electrical appliances, including hair dryers, space heaters, and irons.\n- Keep curtains, clothes, and other combustible items at least three feet away from heaters.\n- Some electrical appliances are designed to be left “ON” all the time. Check the manufacturer directions if you are unsure.\n- Learn the wiring colors and be sure to follow all instructions given by the manufacturer when fitting a plug.\n- Use proper rated cable, such as plenum HDMI, plenum Cat5e, and plenum siamese cable.\n- U.S. Fire Administration: Fact sheet from the U.S. Fire Administration on home electrical fire prevention.\n- Electrical Appliance Fire Safety: Learn what to check for on your electrical appliances to prevent fires.\n- Home Fire Statistics: Facts and figures associated with home fire statistics, including information on civilian home fires and fire-related deaths.\n- Electrical Safety Foundation International: Series of articles relating to fires caused by electronics and appliances.\n- Microwave Ovens Safety Issues: Learn about the common dangers associated with microwave ovens and how to use them safely.\n- Building and Fire Safety: Tips on how to identify electrical safety problems with this electrical safety check list.\n- The Danger of Electrical Shock: Information on how to stay safe from electrical shocks, injury, and death.\n- Kitchen Fire Prevention: Tips on how to prevent kitchen fires from fireproofing dangerous electrical appliances.\n- Electrical Safety: List of electrical hazards that can lead to shocks, burns, fire, or electrocution.\n- Plug into Electrical Safety: Information on fire-related accidents that can be prevented by taking a few electrical safety precautions.\n- Electrical and Fire Safety: Fact sheet on how to prevent fires and burns from electrical appliances.\n- Electrical Fire Safety: Tips for preventing electrical fires in the winter seasons in the home and workplace.\n- Fire Safety Tips: List of important fire safety tips to use in your own home to keep your family safe."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2654f098-8bae-46c7-acd6-07763c77446d>","<urn:uuid:37293d77-7439-4009-a43f-ad96782b95d1>"],"error":null}
{"question":"What is the relationship between manufacturing sector size and GDP, and what are the major safety risks in modern industrial activities?","answer":"Manufacturing sector size strongly correlates with overall economic strength, with a 0.97 correlation coefficient between manufacturing and other sectors' contributions to GDP. The World Bank data from 171 countries shows that strength in manufacturing and other sectors tend to go together, with few exceptions. As for safety risks in industrial activities, particularly in construction, there are five major categories of incidents: falls from height (accounting for 49% of fatalities), slips/trips/falls on the same level (25% of non-fatal injuries), being trapped by collapsing/overturning objects (14% of fatalities), being struck by moving/falling objects (10% of fatalities), and injuries from handling/lifting/carrying (20% of non-fatal injuries).","context":["Nov 17 2018\nWhat does manufacturing as a whole look like as a worldwide economic activity? When a company starts up or shuts down a plant, or relocates production from one country to another, it is a data point. It may be a compelling story but it doesn’t tell you the aggregate effect of all manufacturing on a country or the world.\nBeyond anecdotes about particular companies or locations, the global data collected by the World Bank and the International Labor Organization confirm that (1) the size of the manufacturing sector is positively correlated with that of the rest of the economy, and (2) the share of manufacturing in advanced economies is holding its own in terms of value added but declining in terms of share of the labor force.\nWe apply our own analytics to datasets from publicly available sources.\nThe World Bank as a source of economic statistics\nGlobally, you cannot describe manufacturing by pictures or videos. You have to use numbers, and money is the only language usable to compare activities as different as making frozen lasagna and oil tankers. It’s tricky because currencies change in value over time and with respect to each other. Economists at institutions like the World Bank, however, have been dealing with these issues for decades. They use currency-independent ratios whenever possible; they convert all values into US dollars based on bank exchange rates, and they adjust for inflation to have numbers in constant dollars.\nCan the numbers be trusted?\nNational economic statistics are not equally trustworthy in every country. In democracies, they are usually compiled by agencies structured to be immune from political pressure and staffed by professional statisticians; in authoritarian regimes, they are tasked with producing the numbers the rulers want to see. We use these data not because they are perfect but because they are all we have, understanding the need for cautious inferences.\nGross Domestic Product (GDP) and Value-Added\nThe most common measure of a country’s economy is its Gross Domestic Product (GDP), the sum of the value added of all its businesses, defined as:\nThese external inputs are massive in manufacturing. The aggregate value-added of the manufacturing sector is its contribution to the GDP. This concept of value-added has nothing to do with customers’ willingness to pay but is widely used by governments, not only for GDP calculations but also as the basis for Value-Added Taxes (VAT).\nWhen a company outsources a process, it reduces its value-added by the corresponding spend. When it outsources to another country, it reduces its contribution to GDP by the same amount. A company’s value-added per employee is usable as a measure of productivity across multiple activities, unlike the pieces/operator metrics that are specific to a production line or a product. A caveat, however, is that it can be increased without any improvement in work methods or equipment if there is an opportunity to switch to a product with a higher value added.\nThe ILO as a source of employment data\nThroughout the world, communities vie to attract factories, to create jobs and stimulate the local economy. Conversely, factory closings dislocate communities and may even destroy them. Worldwide employment statistics reflect the aggregate effect of such events. They are available from the International Labor Organization (ILO), an arm of the UN.\nFirst, we consider the 10 largest economies in the world by their GDPs and by the contributions of their manufacturing sectors. The point is to establish whether the rankings match. The top 4 are the same in both rankings, with the US being #1 overall and China #1 in manufacturing. Among the next 6, Brazil and Canada lose their spots in manufacturing, replaced by Korea and Indonesia.\nThis raises the question of whether a country can be rich without a strong manufacturing sector or poor with one. To check this out, we plot the contributions to per-capita GDP of manufacturing versus all other sectors for the 171 countries in the World Bank database for 2016. With a few, small exceptions, a strong manufacturing sector goes hand-in-hand with a strong overall economy.\nWorld Rankings By GDP and Manufacturing Sectors\nFor the economy as a whole, WawamuStats posted a video on Youtube that shows the evolution of the top 10 countries. It shows, in particular, the dramatic rise of China, to #2 since 2010, primarily due to manufacturing. The 2016 ranking is as follows:\nWhen we rank countries by manufacturing’s contribution to GDP, we put China on top, while keeping the same top 4:\nManufacturing Versus Other Sectors\nThe following figure plots the contribution of manufacturing versus other sectors of the economy for the 171 countries or economic areas the World Bank provides data for in 2016. To keep small and large economies comparable, the figures are per capita.\nThe correlation coefficient is .97, indicating that strength in manufacturing and other sectors tend to go together. The outliers with a high per capita GDP and an undersize manufacturing sector both have populations of about 600,000. They are Macao, with revenues centered on gambling, and Luxembourg, a tax haven. The outliers with an oversize manufacturing sector and a low GDP are Ireland and Puerto Rico, respectively with populations of 5 million and 3.3 million.\nManufacturing Share of GDP Over Time\nIn advanced economies, journalists and politicians bemoan the “hollowing of manufacturing.” As manufacturing migrates to emerging economies with cheap labor, the sector is described as withering, with plenty of anecdotal evidence from plants and cities devastated by plant closures. But what do the numbers say? The following chart shows the value added of manufacturing as a percentage of GDP for the 6 largest economies in the World Bank data:\nIt has started to decline in China but is still substantially higher than for the other countries in the group, reflecting the fact that China is still an emerging economy. Except for a dip during the financial crisis, the share of manufacturing in the GDPs of Japan and Germany has been steady around 22% since 2002. By this measure at least, these countries have seen no hollowing of their manufacturing sectors. This share is lower for the US, France, and the UK, between 13% and 9%, and shows no decline since 2008.\nIf you go further in time, you see a decline in manufacturing’s share of GDP in Japan, Germany, the US, France, and the UK, but not in the past 16 years for Japan and Germany, and not in the past 10 years for the US, France, and the UK.\nManufacturing Employment Over Time\nThe horror stories about the hollowing of manufacturing in advanced economies, however, are not about GDP but about employment. This is a different story. Historical data on manufacturing employment is available for all of the GDP top 6 countries except China. This is unfortunate, given that China’s manufacturing sector is the largest in the world. We plot what we have for the other countries, and treat the special case of China separately.\nThe GDP Top 6 Except China\nThe following chart includes only data compiled by the ILO and used by the World Bank, that we can assume are consistent:\nHere we do see a continuing slow decline, and what is happening will become clearer below, when we look at value-added per manufacturing employee.\nThe Special Case of China\nAmong the six countries with the top GDP in 2016, China needs a separate treatment. Neither the World Bank nor the ILO provide data on manufacturing employment in China but only on industrial employment. Partial data can be retrieved from other sources, like the US Bureau of Labor Statistics (BLS), or the National Bureau of Statistics of China (NBSC), but we have no way to be sure it is computed the same way as the World Bank data. In particular, there is no consensus on the difference between industry and manufacturing.\nIndustry versus Manufacturing\nIn Europe, they are synonymous. Germany’s “Industry 4.0,” for example, is entirely about manufacturing. Americans, on the other hand, call any business an “industry,” even when it has nothing to do with manufacturing, as in “the entertainment industry” or “the insurance industry.”\nTo the World Bank, manufacturing is “the making of goods or wares by manual labor or by machinery,” and it does not include mining, oil & gas, or construction. The NBSC, on the other hand, does not have a separate category for manufacturing. They lump it under industry, with mining, logging, salt processing and the repair of industrial products.\nChina industrial employment since 1980\nIn 2016, according to the NBSC, China had 807 million “Economically Active” people. 776 million of them were employed, and 223.5 million in industry. The latest data cited by the BLS for manufacturing is of 99 million employees in 2009, exceeding all the five other top GDP countries combined. For this reason, we cannot ignore it. For 2009, the NBSC gives 211 million for industry, more than twice as many as the BLS estimates for manufacturing. Clearly, numbers based on industry and manufacturing do not belong on the same chart.\nThe chart shows the share of the industrial sector peaking around 2012. It does not, however, allow us to draw conclusions about the manufacturing subsector.\nValue-Added/Employee Over Time\nIn earlier posts, I discussed the merits of value-added per employee as a productivity metric. Whatever its shortcomings, however, I don’t see any alternative to it when trying to compare the performance of countries. The lack of available data on manufacturing employment in China prevents us including it in the following chart:\nValue-added per employee is highest in Japan and the US. It has been rising in the manufacturing sectors of all the advanced economies on the chart except the US since 2010. There are several contributors to this rise and we don’t have the means to quantify their relative influence.\nManufacturing in advanced economies has been focusing on high value-added products. It is where they have a comparative and sometimes even an absolute advantage over emerging economies. This is not only luxury goods. There are other products selling for prices that are high with respect to their external inputs. This includes leading-edge semiconductors and pharmaceuticals, aircraft, or agricultural machinery.\nMethods improvements in Japan in Japan, the US, and Europe have paid off for some companies. What they learned then enabled them to make savvy investments in equipment and technology that use less labor.\nWe work with incomplete and not perfectly clean data, but we can still draw the following conclusions:\n- The size of the manufacturing sector in an economy correlates positively to the sizes of the other sectors.\n- The share of manufacturing in the GDP of advanced economies is holding steady.\n- Manufacturing’s share of the labor force in advanced economies is slowly declining.\nIn the best possible future, today’s emerging economies all become advanced, while today’s advanced economies remain so. Manufacturing will not be the massive source of jobs that, up to now, people have taken primarily out of necessity. Employment in manufacturing will instead be the chosen profession of a minority. Others will work in services, attending to the needs and wants of other people. This future is by no means guaranteed. It may be a utopia but the alternatives are dystopic.","Health and safety in the construction industry: what are the major risks?\nThe construction industry has the second-highest rate of fatalities across all industries in the UK according to the Health and Safety Executive (HSE) website statistics for 2019. Sadly, 30 people lost their lives during 2019 due to incidents on the construction site. With another 54,000 non-fatal injuries reported across the year, there is clearly a way to go when managing health and safety in construction to prevent further deaths and long-term conditions.\nThe construction industry is the European Union’s (EU) largest industry employer. A workforce of 18 million contributes to 9% of the EU’s gross domestic profit (GDP). This large and diverse workforce provides an invaluable service to society and the prosperity of each nation. So, why are there still so many incidents on construction sites and how can health and safety regulations work to protect our workers?\nTargeting the big five\nFive main categories of incidents are most common on construction sites:\n- Falls from height – which account for 49% of fatalities and 18% of non-fatal injuries (a number that is 10% higher than the statistics for all other industries)\n- Slips, trips or falls on the same level – accounting for 25% of non-fatal injuries\n- Being trapped by something collapsing or overturning – responsible for 14% of fatalities\n- Being struck by a moving, falling or flying object – the cause of 10% of fatalities and 12% of non-fatal injuries\n- Injured while handling, lifting or carrying – making up 20% of non-fatal injuries\nBesides these main issues, construction workers are at an increased risk from other serious and life-changing conditions. Conditions such as chronic obstructive pulmonary disease (COPD), occupational deafness, hand-arm vibration syndrome (HAVS) and asbestosis are serious and life-altering conditions that can be caused, or aggravated, by conditions on-site.\nHealth and safety legislation should be at the core of all construction businesses and every project, it should be a top priority for senior managers and it should be embedded into every plan, decision and activity. Read on for ideas on how to manage health and safety in construction, to ensure that your staff are safe, give confidence to your clients, maintain a flawless health and safety record and save time and money through introducing safety measures.\nAssess and avoid\nRisk assessments should be the first stage of all construction projects. Health and Safety teams should be involved in early planning to ensure that the health of workers is embedded in the project.\nAvoidance is the first key step. Is an activity deemed as dangerous? Can it be carried out in an alternative way that would avoid the danger? Can working at height be removed altogether or can fabrication be carried out on the ground and the structure lifted to height for fixing? Are hazardous chemicals essential to work, or can alternatives be found?\nIf time constraints or pressure from third parties will impact on the health and safety of your staff, these issues should be flagged to all stakeholders. If the risk can’t be removed, then safe working conditions should be made available through the correct equipment or protocols. If the situation can’t be made safe, then the group must work to change the working parameters. Time or client pressures should never impact the safety of workers on site. No project is that important.\nHealth & Safety: Do you have the skills on-site?\nOnce all stakeholders have understood and accepted the risk parameters for a project, plans must be put in place to reduce these risks. The main aim, and the measure of success for all health and safety teams, is for a project to run with zero reported incidents.\nChecking the qualifications and experience of all workers and supervisors is key. Workers must be able to perform tasks confidently and competently to reduce the risk to themselves and others. Construction Skills Certification Scheme (CSCS) card checks are a must in the UK and a full log of each card, every contractor and their competency to carry out a task must be documented. Where gaps in knowledge or experience are identified, training should be provided or new staff must be brought in to complete the task. If this can’t be done, then the task must be changed to accommodate the experience of the workers on-site, even if this does add extra time or cost.\nEnsure that qualified Health, Safety and Environment (HSE) officers are known to all workers and that all staff know how and when to report a dangerous situation. Whether you have a dedicated HSE officer on-site, site office staff logging incidents or an app-based solution everyone can access, it should be easy for anyone to report an incident.\nMake protective equipment a habit\nOnce you have the right workers with the correct experience, they must have access to the correct personal protective equipment (PPE) to carry out the job safely. Every person on site should understand the risks to themselves and those around them and know which equipment they need to complete the task safely. Wearing the necessary equipment should become second nature and this only happens if everyone adheres to the rules, if the equipment is missing, there should be a quick and easy way for staff to request it.\nManagers should model the behaviours expected of their teams. Everyone should be free to stop work and demand equipment if it is not available and to request that others in their team do the same.\nHealth and safety regulations become a habit when the culture enables team members to express their views openly. This open and honest approach rewards workers who show behaviours that protect their own safety and those of their colleagues.\nThe right tools for the job\nThis theme carries through to the suitability of equipment too. All equipment, from pneumatic drills and saws to large earthmoving machinery, should be assessed for its suitability and safety. Does the equipment pose a risk to the worker? Can a different piece of equipment give the same result with lower risk? If we do have to accept a risk, does the equipment minimise the risk in the best way? What other checks do we need to ensure zero incidents whilst using this equipment?\nChecks don’t stop when the equipment is in use. Maintenance schedules are essential to ensuring the safety of equipment and safeguarding those using it. A robust service and maintenance routine should be developed, assigned to an individual or team and monitored throughout the project, with any checks or maintenance work documented.\nIdentify issues, make changes and demonstrate the impact\nSoftware really can be a health and safety professional’s best friend. A good software package will allow easy data collection from multiple team members in multiple locations. Everyone should be able to easily report a health and safety concern, record standard health and safety data and complete assessments. The best software allows for collection through app-based systems, enabling any smartphone to become a data collection device.\nOnce data is recorded, software should help you to easily identify hazards and other issues, show where changes were made, demonstrate the impact of these changes and allow for easy reporting to senior management. This comprehensive and closed-loop data flow is essential for demonstrating regulatory compliance and for providing evidence during inspections.\nBy moving to a continuous improvement culture, issues that are flagged and dealt with on one project can be carried through as health and safety requirements for the next project. This process ensures that the same problem isn’t repeated and that everyone learns from previous mistakes.\nThe PlanRadar app is the perfect tool for completing comprehensive digital health and safety assessments through straightforward templates. Fire assessments and other specific templates can be designed, saved and adapted for future projects. And, most importantly, where plants require no transmitting appliances, PlanRadar can be used in an offline mode, automatically syncing information as soon as the device is reconnected to a network. This functionality can be essential to COSHH assessments.\nWork hard, but rest hard too\nWhen time pressures are mounting and the threat of late fines are looming, it’s tempting to cut corners and push to get a project done, but this should never be at the expense of health and safety. If changing plans impact on people, equipment or procedures, then risk assessments should be re-evaluated.\nAlthough contractors should expect high standards of work and output from their workers, rest is important too. Workers need to be highly alert to their surroundings and to help keep everyone safe; this is especially important when working at height, with hazardous material or in confined spaces. Workers should be given adequate time to rest and relax and not be pushed to work overly long hours, even if they request the overtime. Distraction and tiredness lead to mistakes and they can be serious and even life-threatening.\nIf in doubt, shout\nBusinesses that excel at managing health and safety in construction have safety at the very core of their work and everyone on-site sees the benefit of the precautions, follows the rules and enforces them within their teams.\nCommunication is key here. When an individual understands why a rule is in place or why something has changed, they are more likely to comply and enforce compliance from others. When you have zero incidents during a project, reward the team for achieving this. If a near-miss has occurred, communicate this to the team, help them to understand why it happened and how further incidents can be avoided. Be clear about the changes you’ve made to make ensure the incident doesn’t occur again and how others can contribute to this adjustment.\nHealth and safety in the construction industry is the responsibility of everyone on site. Put simple procedures in place so that construction workers can quickly and easily report an issue and instil a culture of responsibility. If a trip hazard is identified, encourage workers to remove it. If someone is in danger, encourage others to act. Dealing with an issue when it arises can help reduce immediate incidents that may follow.\nHealth and safety rules should be repeated at daily briefings and reinforced throughout the day. There are many ways to remind workers of their health and safety obligations, perhaps through posters displayed around the site or reminders on software tools.\nSeveral of the bigger construction companies have powerful slogans to help educate and engage all team members on-site.\nBalfour Beatty’s award-winning golden rules highlight core expectations and they are simple and easy to remember:\nMeanwhile, Amey’s Target Zero encourages workers to speak out and talk about health and safety issues so that problems can be identified and dealt with quickly.\nWe have all the tools to achieve zero incidents and to make the construction site a safe place for all workers. By taking a thorough approach to identifying risk, developing a culture of open and honest communication and ensuring workers have access to the safest tools for the job, we can reduce injuries on-site and achieve zero incidents."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9cfe533b-ff1c-4488-924c-bfcd66f8de49>","<urn:uuid:2aab1c69-8b42-4bc3-b9a1-8f308e254433>"],"error":null}
{"question":"How do scientists study both the early universe and Pluto's surface features through the analysis of different types of radiation?","answer":"Scientists use different radiation-based methods to study both phenomena. For the early universe, astronomers observe the Cosmic Microwave Background Radiation at microwave wavelengths, looking for tiny temperature fluctuations and polarization patterns that provide information about conditions as early as 10-35 seconds after the Big Bang. For Pluto, scientists use visual imaging through instruments like the Ralph/Multispectral Visual Imaging Camera (MVIC) on the New Horizons spacecraft, which captures detailed images using backlighting from the sun to reveal surface features, atmospheric hazes, and geological formations like mountains and glaciers.","context":["Probing the Origins of the Universe with CMBR\nBen - Astronomers and Cosmologists seek to understand the origins of the universe, but as this was billions of years ago, we're left with very few clues as to what actually happened. One of the big clues is the Cosmic Microwave Background Radiation, as Cambridge University's Professor George Efstathiou explained.\nGeorge - The Cosmic Microwave Background Radiation is the remnant radiation from the Big Bang. And as the universe has expanded, this radiation has cooled and is now observed as a background radiation with a very low temperature of 2.3 degrees above absolute zero.\nBen - So it pervades the entire universe and shows us that at some point in the history, the universe, it was very hot and very dense.\nGeorge - That's right. The discovery of the microwave background was discovered in 1965 and once that radiation was discovered, it was really incontrovertible proof that the universe started off with a very hot dense state.\nBen - And how do we study it?\nGeorge - We can study it by observing the radiation at microwave wavelengths. So there have been a range of experiments and missions since the discovery of the background radiation. But it was realized very early on, as soon as the radiation was discovered, that the structure that now produces galaxies and classes of galaxies that we see in the universe today would have imprinted tiny little temperature variations on the background radiation. And so, physicists and astronomers started a search to discover these fluctuations and they were discovered in 1990 by the COBE satellite. These fluctuations are very important because we see them at the time that the universe became neutral. This occurred when the temperature of the background radiation was around 10,000 degrees and the universe was only 400,000 years old. So we see the universe as it was 400,000 years after the Big Bang. But the fluctuations that we see are faithful representations of what happened 10-35 seconds after the Big Bang and that's the real interest that cosmologists have in studying these fluctuations because it tells us about conditions in the ultra, ultra early universe.\nBen - And so, can we use it to work out some of the physics that was going on back then?\nGeorge - That's right. By studying these fluctuations, we can probe physics at ultra high energies, 10 to 15 orders of magnitude higher than the energies achievable by the large hadron collider. The physics of those high energies is really not at all well understood and it could be very, very different to the sort of physics that we know about. And that's what makes it really fascinating because a theory like that would have maybe 9 or 10 spatial dimensions. And these dimensions that we currently don't see, at those very early times, would've opened up. And so, it's possible that we could see signatures of higher dimensional physics by studying the background radiation.\nBen - Relatively recently, the Planck mission has set out to try and study this in more detail. What are you hoping to use that for?\nGeorge - Planck was successfully launched last year and it is by far the most sensitive space probe designed to study these fluctuations. So we will get much, much better images, of a higher angular resolution with much higher sensitivity that have ever been achieved before. And we're also measuring the polarisation. In this background radiation, the fluctuations are slightly polarised. They're polarised at a few percent level and Planck has sensitivity to polarisation so we expect to create an accurate picture of not just the distribution of temperature but the distribution of polarisation. That's very important because gravitational waves generated in the very early universe can produce a specific type of polarisation pattern which we may detect.\nBen - So what's the really big question that we're trying to answer?\nGeorge - We know very little about the physics of these very early times. What we would like to know is what actually happened very, very close to the Big Bang, not just in a sort of conceptual way, but actually really probe the structure of the Big Bang. What we think happened is that very close to the Big Bang, the universe underwent a period where it effectively expanded faster than the speed of light. So we called this a period of inflation. It's a very attractive theory. It is not a well-founded theory in terms of physics yet, so we don't understand the mechanism, we don't understand any of the details. What we hope to do with Planck is to get enough information that we can actually get a handle on what actually happened. Did the universe really go through an inflationary phase of expansion? What was the physics responsible for inflation? How were the fluctuations generated? What was the energy scale of inflation? These are the sort of questions that we hope to answer.\nBen - George Efstathiou on how we can use the leftover radiation from the Big Bang to probe the early universe.","September 17, 2015 – The latest images from NASA’s New Horizons spacecraft have scientists stunned – not only for their breathtaking views of Pluto’s majestic icy mountains, streams of frozen nitrogen and haunting low-lying hazes, but also for their strangely familiar, arctic look.\nPluto’s Majestic Mountains, Frozen Plains and Foggy Hazes: Just 15 minutes after its closest approach to Pluto on July 14, 2015, NASA’s New Horizons spacecraft looked back toward the sun and captured this near-sunset view of the rugged, icy mountains and flat ice plains extending to Pluto’s horizon. The smooth expanse of the informally named icy plain Sputnik Planum (right) is flanked to the west (left) by rugged mountains up to 11,000 feet (3,500 meters) high, including the informally named Norgay Montes in the foreground and Hillary Montes on the skyline. To the right, east of Sputnik, rougher terrain is cut by apparent glaciers. The backlighting highlights over a dozen layers of haze in Pluto’s tenuous but distended atmosphere. The image was taken from a distance of 11,000 miles (18,000 kilometers) to Pluto; the scene is 780 miles (1,250 kilometers) wide. Image Credit: NASA/JHUAPL/SwRI\nThis new view of Pluto’s crescent — taken by New Horizons’ wide-angle Ralph/Multispectral Visual Imaging Camera (MVIC) on July 14 and downlinked to Earth on Sept. 13 — offers an oblique look across Plutonian landscapes with dramatic backlighting from the sun. It spectacularly highlights Pluto’s varied terrains and extended atmosphere. The scene measures 780 miles (1,250 kilometers) across.\nCloser Look: Majestic Mountains and Frozen Plains: Just 15 minutes after its closest approach to Pluto on July 14, 2015, NASA’s New Horizons spacecraft looked back toward the sun and captured this near-sunset view of the rugged, icy mountains and flat ice plains extending to Pluto’s horizon. The smooth expanse of the informally named Sputnik Planum (right) is flanked to the west (left) by rugged mountains up to 11,000 feet (3,500 meters) high, including the informally named Norgay Montes in the foreground and Hillary Montes on the skyline. The backlighting highlights more than a dozen layers of haze in Pluto’s tenuous but distended atmosphere. The image was taken from a distance of 11,000 miles (18,000 kilometers) to Pluto; the scene is 230 miles (380 kilometers) across. Image Credit: NASA/JHUAPL/SwRI)\n“This image really makes you feel you are there, at Pluto, surveying the landscape for yourself,” said New Horizons Principal Investigator Alan Stern, of the Southwest Research Institute, Boulder, Colorado. “But this image is also a scientific bonanza, revealing new details about Pluto’s atmosphere, mountains, glaciers and plains.”\nNear-Surface Haze or Fog on Pluto: In this small section of the larger crescent image of Pluto, taken by NASA’s New Horizons just 15 minutes after the spacecraft’s closest approach on July 14, 2015, the setting sun illuminates a fog or near-surface haze, which is cut by the parallel shadows of many local hills and small mountains. The image was taken from a distance of 11,000 miles (18,000 kilometers), and the width of the image is 115 miles (185 kilometers). Image Credit: NASA/JHUAPL/SwRI\nOwing to its favorable backlighting and high resolution, this MVIC image also reveals new details of hazes throughout Pluto’s tenuous but extended nitrogen atmosphere. The image shows more than a dozen thin haze layers extending from near the ground to at least 60 miles (100 kilometers) above the surface. In addition, the image reveals at least one bank of fog-like, low-lying haze illuminated by the setting sun against Pluto’s dark side, raked by shadows from nearby mountains.\n“In addition to being visually stunning, these low-lying hazes hint at the weather changing from day to day on Pluto, just like it does here on Earth,” said Will Grundy, lead of the New Horizons Composition team from Lowell Observatory, Flagstaff, Arizona.\nCombined with other recently downloaded pictures, this new image also provides evidence for a remarkably Earth-like “hydrological” cycle on Pluto – but involving soft and exotic ices, including nitrogen, rather than water ice.\nPluto’s ‘Heart’: Sputnik Planum is the informal name of the smooth, light-bulb shaped region on the left of this composite of several New Horizons images of Pluto. The brilliantly white upland region to the right may be coated by nitrogen ice that has been transported through the atmosphere from the surface of Sputnik Planum, and deposited on these uplands. The box shows the location of the glacier detail images below. Image Credit: NASA/JHUAPL/SwRI\nBright areas east of the vast icy plain informally named Sputnik Planum appear to have been blanketed by these ices, which may have evaporated from the surface of Sputnik and then been redeposited to the east. The new Ralph imager panorama also reveals glaciers flowing back into Sputnik Planum from this blanketed region; these features are similar to the frozen streams on the margins of ice caps on Greenland and Antarctica.\nValley Glaciers on Pluto: Ice (probably frozen nitrogen) that appears to have accumulated on the uplands on the right side of this 390-mile (630-kilometer) wide image is draining from Pluto’s mountains onto the informally named Sputnik Planum through the 2- to 5-mile (3- to 8- kilometer) wide valleys indicated by the red arrows. The flow front of the ice moving into Sputnik Planum is outlined by the blue arrows. The origin of the ridges and pits on the right side of the image remains uncertain. Image Credit: NASA/JHUAPL/SwRI\n“We did not expect to find hints of a nitrogen-based glacial cycle on Pluto operating in the frigid conditions of the outer solar system,” said Alan Howard, a member of the mission’s Geology, Geophysics and Imaging team from the University of Virginia, Charlottesville. “Driven by dim sunlight, this would be directly comparable to the hydrological cycle that feeds ice caps on Earth, where water is evaporated from the oceans, falls as snow, and returns to the seas through glacial flow.”\n“Pluto is surprisingly Earth-like in this regard,” added Stern, “and no one predicted it.”\nIntricate Valley Glaciers on Pluto: This image covers the same region as the image above, but is re-projected from the oblique, backlit view shown in the new crescent image of Pluto. The backlighting highlights the intricate flow lines on the glaciers. The flow front of the ice moving into the informally named Sputnik Planum is outlined by the blue arrows. The origin of the ridges and pits on the right side of the image remains uncertain. This image is 390 miles (630 kilometers) across. Image Credit: NASA/JHUAPL/SwRI"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:73cd1977-8823-403f-af72-71c658a618b7>","<urn:uuid:d232a059-e93c-47f6-acec-93f42b457a55>"],"error":null}
{"question":"What are the key differences between protecting container plants versus in-ground plants from winter damage?","answer":"Container plants require more protection than in-ground plants because their roots are more exposed to cold temperatures being above ground. Container plants should be moved inside protective structures like garages or greenhouses, or have their containers wrapped in plastic, burlap, or blankets to reduce heat loss. Multiple containers can be grouped together and mulched for additional protection. In contrast, in-ground plants benefit from soil heat radiation and can be protected primarily through proper mulching, covering with sheets or blankets that don't touch the foliage, and ensuring proper watering before freezes since well-watered soil absorbs and reradiates more heat.","context":["Getting the Garden Ready for Winter\nAutumn is that glorious season when Mother Nature puts on her vivid display of colour, one last thrill before winter takes us into its icy grip. As temperatures decrease, growth slows, signifying the beginning of fall cleanup time in the garden.\nLift tender tuberose begonias, cannas and dahlias after the first frost. Dig carefully and remove the foliage leaving a stalk of four to six inches. Clean soil from the tuber and allow them to dry overnight in a protected area. Place in a cardboard box with newspapers between the layers and store in a cold, dry area. Check monthly to ensure that they are sound. If rot occurs, cut away the affected areas and repack in fresh paper.\nMost perennials do not need to be cut back. Leave ornamental grasses, Sedum and Echinacea to add winter interest and provide food and protection for the birds. The possibility of self-seeding is usually outweighed by the benefits. In addition, the leaves will help to trap snow that provides additional protection for the plant.\nCut back hostas to reduce potential sites where slugs can over-winter. True lilies should also be cut down and any mulch removed in an attempt to control the lily beetle.\nThe area around rose bushes should be cleared of all debris to reduce the potential of over-wintering diseases and pests. To protect tender roses, add eight to ten inches of compost of soil after the soil freezes to discourage mice from eating the buds. Do not scrape the soil from between the bushes because you can injure the roots. A local grower has suggested that the plants be covered with a tarp or white greenhouse plastic after applying the mulch. Provide winter protection for all roses, no matter how hardy they are, during the first two winters after planting.\nBe aware of the amount of rainfall throughout the season and, if needed, continue to water coniferous trees and shrubs until the ground freezes. Wrap shrubs that are prone to snow load damage. To provide a windbreak for tender shrubs put the posts into the ground now and add burlap at a later time. Do not allow the material to touch the plant and do not use plastic.\nFall is a perfect time to aerate, over-seed and fertilize your lawn. To help to prevent snow mould cut grass shorter.\nPonds and water features need to be winterized. Pumps should be cleaned and stored in a warm place. Keep ponds clear of fallen leaves, which will rot over the winter, creating gases and contaminants. If fish are kept in the pond over the winter, add a bubbler or heater to maintain an open area in the ice.\nAnnuals should be removed. Cut the tops off leaving the root ball intact particularly if there is a slope. This will help to stabilize the soil and prevent erosion while providing organic material.\nTake out the dead vegetable plants. Bury them or put them into the compost and turn over the vegetable garden.\nLeaves can be bagged and put out for collection by the city, composted at home or used as mulch. To encourage decomposition over the winter, chop using a lawn mower with a mulching blade or a leaf shredder. A thick mulch of leaves will protect your perennials when fluctuating temperatures in early spring cause plants to heave out of the ground.\nTake in garden ornaments and ceramic or clay pots that will not withstand the effects of the freeze thaw cycles and will be damaged or break if left unprotected. If you plan on filling your pots with fall or winter foliage be sure to use plastic, metal or fiberglass. Clean and store your tools and hoses.","Protecting Plants from Cold Temperatures\nAlthough we live in the South, winter sometimes brings cold temperatures that can cause severe damage to many of our landscape plants. A late freeze after the temperature rises in January or February could be more injurious than the same cold temperature in winter when these plants have become dormant and more resistant to changes in temperature. An example is the sudden drop in temperature in April 2007 that killed or severely damaged many plants. It is important to protect plants from these cold temperatures.\nTemperature Changes and Plant Damage\nA plant’s ability to withstand cold temperatures depends on plant species, and how low and how fast temperatures decrease. When temperatures gradually decrease, a plant can acclimate, or adjust itself, to withstand colder temperatures better. Sudden decreases in temperature cause more damage in fall or early winter than similar low temperatures well into winter. If temperatures increase during the winter months, some plants may break dormancy, or deacclimate, and begin leafing out or flowering. Plants that break bud dormancy become more susceptible to late frost because of their new, tender growth.\nCold injury can occur to all parts of the plant (flowers, fruits, leaves, stems, trunks, roots, and buds). Fruits and flowers are the least tolerant of cold injury because they have little ability to adjust or build up tolerance to colder temperatures. Leaf and stem tissues are injured and damaged when ice forms within the plant’s cells, which typically occurs during a rapid freeze. When this happens, the plant’s tissue dies; this is often characterized by plant parts turning brown and mushy. When the temperature drops slowly, ice sometimes forms between the walls of the plant’s cells. Hardy or cold-acclimated plants can often withstand this type of ice formation.\nWindy conditions can also cause plant damage by desiccation, or the drying out of the plant. Desiccation causes marginal or leaf-tip burn or totally brown leaves in severe cases. Desiccation occurs when a plant loses more water than it absorbs, or takes up, by its roots, especially when the ground is frozen.\nHomeowners in Mississippi can enjoy a wide variety of plants. They can increase their choice of available plants by careful selection based on growing conditions and location in the landscape. By planting a combination of tender and hardy plants and by protecting plants susceptible to cold temperatures, homeowners can have landscapes that survive cold temperatures.\nProtecting Plants from Cold Temperatures\nPlant and Site Selection\nThe best way to prevent cold injury to plants is to choose plants that tolerate the cold temperatures in your area. Mississippi is in USDA Cold Hardiness Zones 7b, 8, and 9a. Northern regions of Mississippi are in Zone 7b, which means that plants need to be hardy to 5–10ºF. Northwestern and central regions of Mississippi are in Zone 8a (10–15ºF); southern Mississippi is in Zone 8b (15–20ºF); and the coastal tips of Mississippi’s three coastal counties are in Zone 9a (20–25ºF). Select plants that meet the minimum cold-hardy requirements in your area; for example, if you live in Zone 8a, choose plants that are hardy to at least zone 8a and preferably zone 7 to ensure they can better withstand any sudden cold dips in temperatures in your area.\nIn addition to proper plant selection, proper site selection is essential. Assess your property to determine the location of the coldest and the warmest spots. During the winter, the coldest spots are often found on the north and northwest parts of the property and in low areas where cold air settles. The warmest spots are usually on the southern part of the property.\nAssessing the microclimates of your property is also important. Elevation, landform, soil properties, canopy cover, and proximity to structures or other plants determine a microclimate. You can help protect plants by placing cold-sensitive plants near the part of the house that receives southern exposure or near larger plants or other structures.\nMaintaining proper plant nutrition also helps protect your plants from cold damage. Proper nutrition of plants is critical. A plant that has been given the appropriate nutrition tolerates cold temperatures, withstands sudden temperature drops, and recovers from cold damage better than plants that are nutritionally deficient.\nFertilizing plants at the proper time of year is also vital. Fertilizing plants in the fall (after August or September) with a fertilizer high in nitrogen can result in a flush of new growth that is more susceptible to cold temperatures.\nDuring the fall and winter months, most plants enter a dormant period when they need less fertilization. Winterizing-formulated fertilizers, which are high in potassium and low in nitrogen, may be used.\nCanopies and Shade\nTree canopies can reduce cold injury from radiational freezes. Radiational freezes occur on calm, clear nights when temperatures drop because of radiational cooling or heat loss from the earth and from the surfaces of objects. Canopies help reduce radiant heat loss from the ground to the atmosphere by raising the minimum night temperature beneath them.\nPlants that grow in shaded areas are less susceptible to winter desiccation, or drying out, than plants that grow in open areas. But plants that prefer full sun do not do well in the shade and will be unhealthy and less tolerant of cold temperatures if you plant them in the shade.\nWindbreaks such as fences, buildings, and temporary coverings can help protect plants from cold injury. Windbreaks are most useful in reducing injury resulting from cold winds and advective freezes (freezes that occur when temperatures drop because of the invasion of cold air masses into the area).\nCovering and Heating\nProtect plants that are in containers either by placing them inside a protective structure (house, garage, greenhouse, or shed) or by placing a protective covering over them. Container plants are more susceptible to cold temperatures than a similar plant growing in the ground. Their roots are more exposed because they are above the ground. Roots that are damaged by cold temperatures may not show immediate signs of damage, but these plants will show signs of stress when temperatures increase.\nPush together container plants that are left outside, and mulch or cover them to decrease heat loss from the sides of the containers. Wrap the bases of the containers in plastic, burlap, or blankets to reduce heat loss.\nPlants that grow close to the ground are usually protected by heat radiating from the soil. Plants that are tall and more open do not receive this radiating heat and are not as protected from the cold.\nRemember to mulch the soil. Mulching protects the roots of plants and helps reduce heat loss, thus minimizing temperature fluctuations. As with shrubs and trees, protecting the roots is necessary for them to survive the cold and come back in the spring.\nCovering your plants helps protect them from frost as well as from extremely cold temperatures. Covers that reach the ground and do not come in contact with foliage form a layer of insulation from the cold temperature. To prevent foliage breakage, avoid having the covers (sheets, blankets) touch the foliage. Remember to remove these protective coverings from the plant canopy after cold temperatures have passed.\nWhen extremely cold temperatures are predicted, place a light bulb (60-watt is sufficient) or other heat source under the cover to provide heat. Be very careful when using a bulb or other heat source, which can be a potential fire hazard. Do not let the bulb or heat source come in contact with the plant or the cover. Remove the cover and provide ventilation during the day to allow release of the heat that is trapped by solar radiation. This precaution is critical when using plastic covers.\nWater Needs before and after a Freeze\nWatering plants before a freeze can help protect them from cold injury. Soil that is well watered absorbs more heat and then reradiates heat, helping to increase the elevated temperature around the plants. However, poorly drained soils result in plants that have weak and shallow roots, which are more susceptible to cold injury. Use mulch to help retain soil moisture.\nCheck the water needs of plants after a freeze. After very cold temperatures, water that is in the soil may still be frozen and unavailable to the roots. If plants are transpiring (losing water from their leaves) and water is unavailable to the roots, plants may dry out. To provide water for plants, apply water to thaw the soil and the ice.\nPruning in late summer or early fall can result in new growth that is more susceptible to cold injury; so avoid pruning at this time of year.\nWait to prune plants until new growth appears in late winter or early spring. Cold damage will be more apparent, making it easier to remove the damaged portions of the plant. Severely injured plants may not break bud in the spring and may take on an overall weak appearance. Branch tips are more likely to suffer cold injury than older wood.\nTo determine if wood has been injured by the cold, check the cambium layer (layer directly under the bark). To do this, carefully scratch through the bark layer or carefully slice through this layer with a knife. Healthy, undamaged cambial tissue will be green; damaged will be brown or black. Prune this wood below the discoloration.\nTo determine if your fruit plants have been damaged by the cold temperatures, wait several days after a freeze and remove several flower buds from the plants. Use a sharp knife or razor blade to cut a cross section of the bud’s top. If there is any discoloration in the bud, the bud has been damaged and will not produce fruit. Damage may be localized, however, and not all buds may have been damaged. Check several buds from different areas of the plant to get a better assessment of the damage.\nPlants can be protected from cold temperatures by proper selection, placement, and care. Healthy plants are more resistant to cold injury than plants that are weakened by disease, by insect damage, or by improper care. You can take measures to protect plants during sudden and prolonged exposure to cold temperatures.\nPublication 2303 (POD-02-19)\nRevised by Jeff Wilson, PhD, Assistant Professor and State Master Gardener Coordinator, North Mississippi Research and Extension Center.\nThe Mississippi State University Extension Service is working to ensure all web content is accessible to all users. If you need assistance accessing any of our content, please email the webteam or call 662-325-2262."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:38a4579a-ed89-411b-bb71-d3ef479c46f5>","<urn:uuid:cb84294a-c3a9-41fa-aa34-00ceac532c65>"],"error":null}
{"question":"I'm a gardening enthusiast - can you compare the recommended soil conditions for growing popcorn versus potatoes?","answer":"Both crops require specific soil conditions for optimal growth. Popcorn needs deep, fertile, well-drained soil in a sunny site, similar to sweet corn growing conditions. For potatoes, the soil should be loose and crumbly, neither too dry nor too wet, with an ideal soil temperature between 60°F to 70°F (16°C to 21°C). If the soil is too dry, it won't deliver water and nutrients properly to the potatoes, while excessive moisture can cause the potatoes to rot. Both crops need regular watering - popcorn requires deep, regular watering especially during tasseling, while potatoes need about an inch of water weekly.","context":["Popcorn has grown into a favorite snack food since the tasty confection of Cracker Jack hit the scene at the World's Fair in 1893. We eat it plain or buttered; seasoned with herbs, spices or cheese; rolled into sweetened balls; and candy-coated with chewy caramel and nuts. You can find this tasty treat at theaters, fairs and festivals as well as sporting events, street corners and department stores. It's even popping up in home gardens.\nThough popcorn (Zea mays) and sweet corn share similarities, one way they do differ is that the popcorn kernels have a hard moisture-sealed hull and a dense starchy filling. When perfectly cured popcorn is heated, the kernels explode, inside out, into popped corn.\nPopcorn varieties are not limited to kernels of yellow or white. Options also include blue, yellow, pink, or even mahogany. And while varieties do play a role in popcorn quality, taste and popability, the ultimate secret to this gourmet treat lies with how it is grown, harvested, dried and stored.\nPopcorn thrives in the same growing conditions as sweet corn, so choose a sunny site and deep, fertile, well-drained soil.\nIf you're growing popcorn and sweet corn in the same season, be sure to separate the two to avoid cross-pollination. You can do this either by distance (250 feet), planting time (sow two weeks apart) or maturity date (for example, a 90-day variety versus a 120 day variety). If a crossing does occur it may result in tough, starchy kernels in the sweet corn.\nSow seeds directly in the ground one to two weeks after the danger of frost has passed and soil temperature has warmed to 60 degrees. You can get a jump on the growing season by starting seeds in 2 inch pots and transplanting to the garden when seedlings reach about 3 inches tall.\nGive your corn a tight spacing, sowing seeds or transplanting seedlings every 6 to 10 inches -- depending on the variety -- with rows spaced from 12 to 18 inches apart. For full, well-filled ears, plant your popcorn in blocks of four to six rows rather than individual long rows. Corn is wind-pollinated, so the more plants you have, the more pollen you'll create. And that means there will be more kernels per ear to pop.\nPopcorn is a hungry feeder, especially when it comes to nitrogen. Give seeds a jump start with a smooth seed bed enriched with plenty of compost or aged manure. Follow up with a high nitrogen fertilizer every two to four weeks or a foliar spray every one to two weeks until the corn begins to tassel. Examples of a natural fertilizer high in nitrogen includes composted manure, fish meal, alfalfa meal, or other organic sources. Cut back on nitrogen if plants turn a dark shade of green.\nWater plants regularly and deeply, especially when the stalks begin to tassel, which is the feathery flower at the top that releases pollen. Be sure to thoroughly wet the entire root zone. (Mulching around plants with compost or composted manure will feed plants, conserve soil moisture, and help prevent weeds.) Water less frequently once the ears have filled out. This will help produce the best yields and quality of popcorn.\nEnjoying the harvest\nPopcorn is ready to harvest when the silks turn brown and the kernels are fully mature, firm, and well colored. Leave corn drying on the stalks as long as possible, allowing the kernels to dry down naturally on the ear. Harvest and husk the popcorn and allow the ears to cure further in a dry and well ventilated location. Drying time can vary from one week to several weeks or more, depending on weather conditions.\nYour ears of popcorn are ready to shell when the kernels come easily off the cob, which is when the moisture content is around 13 to 14 percent. Test the popcorn by popping a few kernels. (To pop, put kernels or an ear in a folded paper bag in the microwave.) If they pop, it’s time to shell the corn. To shell the corn, roll the kernels from the cob with your hand, pushing firmly with your thumb. Wearing a sturdy glove is helpful when shelling.\nStore the shelled corn in a moisture-proof, airtight plastic or glass container in a cool, dry location. Avoid storing popcorn in a warm location or in the refrigerator, which can dry out the kernels. A moisture loss of as little as three percent can render your popcorn unpopable. If kernels dry out, adding a tablespoon of water per quart of popcorn may help revive their popability. Shake or stir the kernels until the moisture is absorbed, close up the container, and try popping again in a few days.\nWhen properly cured and stored, popcorn can remain popable for a long, long time. In fact, researchers found ancient kernels of popcorn believed to be 1,000 years old in tombs on the east coast of Peru. And guess what--the kernels still popped!\nPopcorn is as much fun to grow as it is to eat. But as any popcorn connoisseur knows, there is definitely a difference in taste among types.\n'Tom Thumb', 'Robust' and other yellow types explode into golden yellow snowflake or butterfly-shaped popcorn with a corny taste that's light and airy. Yellow popcorn also yields up to 46 times its original volume.\nWhite popcorn varieties like 'Japanese White Hulless' burst into smaller snowy white mushroom-shaped popped kernels that are tender and slightly sweet. This type yields 35 to 40 times its kernel size.\nBlue, pink and multi-colored types like 'Calico' typically pop into deliciously tender nuggets with nutty undertones. Mahogany varieties such as 'Strawberry' take on the characteristics of white popcorn.\n-- Kris Wetherbee\n-- Kris Wetherbee","Knowing how long it takes to grow potatoes ensures you harvest the potatoes at the perfect time when the potatoes have fully grown and are most delicious. After all, potatoes are one of the most versatile ingredients, growing your own is a must!\nEarly varieties are generally ready for harvest after 90 days, but late varieties might take upwards of 110 days. Potatoes are root vegetables that are easy to plant, and they grow abundantly. There are over one hundred varieties of potatoes, with different shapes, sizes, colors, and also growth times.\nKeep reading to know how long different kinds of potatoes take to grow and how to give your growing potatoes the best conditions.\nHow Long To Grow Potatoes Before Harvesting?\nThe average time to grow potatoes before harvesting is two to four months; this time period is based on a wide variety of potatoes. The specific growth period for your potatoes heavily depends on the type of potato you are growing.\nWhen you are growing potatoes, the maturation time is an extremely important variable. Different potato varieties have their own Days To Maturity (DTM). It is recommended that you know what your specific potato type’s DTM is so you can be best prepared. The DTM begins from the day you plant the potatoes.\nThe varieties of potatoes can be placed into three categories: early varieties, mid-season varieties, and late varieties. See below for the average growth time range for each of the three varieties.\n|Early varieties||These varieties of potatoes take less than 90 days to mature. These potatoes are a good fit for almost any garden.||Irish Cobbler, Mountain Rose, Norland, Caribe, King Harry|\n|Mid-season varieties||These varieties of potatoes take around 100 days to mature.||Viking, Red Pontiac, Yukon Gold, Red LaSoda|\n|Late varieties||These varieties of potatoes take 110 or more days to mature. The potatoes in this category tend to grow heavy tubers, which store excellently.||Elba, Kennebec, Butte|\nMoreover, the new potatoes are ready to be harvested in 60 days or ten weeks. “New potatoes” are the potatoes that are harvested earlier than usual to have smaller-sized and more tender potatoes.\nIf you’re having trouble finding the variety you want locally, Johnny’s Selected Seeds is a great online source for acquiring quality seed potatoes.\nOther Factors That Affect Potato Growth Periods\nNow we know that planted potatoes need the following to reach their best growth potential.\n- Plenty of sunlight\n- Loose, fertile soil\n- An inch of water weekly\nOther factors also play a role in how long it takes for you to grow your potatoes:\n|Where you are planting your potatoes||Potatoes can be planted and grown in pots, containers, or even potato-specific “grow bags”; all of these need more attention and care and produce smaller yields. A raised bed in the garden is easier and more efficient for growing a large harvest of potatoes.|\n|Where you buy your seed potatoes||The seed potatoes are cut-up pieces of whole potato tubers that have sprouted and potatoes that can sprout. You must buy your seed potatoes from a farm store or a catalog, as those can be planted and grown. The potatoes in grocery stores are usually treated to not grow sprouts. Ensure you have healthy seed potatoes to begin with.|\n|What kind of tubers you choose to plant||You can work with tubers that have sprouted on them already. You can also pre-sprout the potato yourself by placing them on your kitchen counters and wait for them to sprout. The pre-sprouted potatoes take less time to harvest than non-sprouted potatoes.|\n|Positioning of seed potatoes in the soil||You must plant the seed potatoes eight to nine inches deep under fertile soil. Each potato piece should be at least 12 inches apart from each other from all sides. These measurements are important as it affects how fast and efficient the potatoes are in taking in nutrients and growing in time.|\n|Fertilizer||The amount and quality of fertilizer you use can also affect the potatoes’ harvest time. Which fertilizer to use depends on your potato variety type.|\nHow Late Can You Plant Potatoes?\nGenerally, you can plant potatoes early in the gardening season. You can plant them as early as you see that the frost is out of the soil. Frost’s action is where the soil water is frozen or thawed; either of these is not good for growing potatoes. If you don’t want any risk with the soil frost, however, you can plant your potatoes one or two weeks after the last spring frost.\nYou might want to plant potatoes later to avoid any soil frost, or maybe you want to plant potatoes later on for your own personal reasons. Whichever the reason, it is good to know how late you can plant your potatoes.\nThere is no “one rule” for how late you can plant your potatoes because it depends on multiple factors:\n|Ideal weather temperature||Potatoes grow best when the weather temperature is around 45°F (or 7 to 8 °C). Cooler weather is preferable for potatoes.|\n|Ideal soil temperature||The best soil temperature is 60°F to 70°F (or 16°C to 21°C). The minimum soil temperature to grow potatoes in is 50°F (or 10°C).|\n|Ideal soil quality||You want soil that is loose and crumbly, but not dried out. Soil that is too dry will not deliver water and nutrients properly to the planted potatoes. Soil that is too wet becomes sticky and is difficult to work with. Additionally, too much moisture will rot your planted potatoes.|\n|Days to Maturity (DTM)||You must know your potato type’s DTM. You can figure out how long the ideal weather and temperatures will last and compare that with the DTM. The lower the DTM is, the later you can plant your potatoes. The higher the DTM is, the earlier you must plant your potatoes.|\n|Water||The general rule of thumb for growing potatoes is giving them an inch of water weekly. Be aware of heavy rain seasons. You want to plant and harvest before too much water reaches your potatoes.|\nOverall, the soil’s condition, not necessarily the date and time, is in charge of the potato’s optimal growth and harvest time. You need to figure out how the season and the daily weather will affect the soil’s quality and determine how late you can plant your potatoes while keeping the DTM in mind.\nKeep in mind that if the weather gets too hot, the potatoes will not be in the best condition when it is harvest time. This is why it is better to avoid planting too late; the potatoes will begin to grow nicely, but as the summer heat and humidity rises, the affected soil will ruin your potato harvest.\nHere is a very general potato planting month schedule based on the area where you live:\n|Northern regions||Potatoes can usually be planted in early April and mid-April.|\n|Center regions||Potatoes can usually be planted in winter, from September to February.|\n|Southern regions||Potatoes can usually be planted in January and February.|\nOverall, it can take from less than 60 days to more than 100 days. Furthermore, loose and crumbly soil in a cooler temperature is what potatoes love to grow in.\nNow you should know how long it takes to grow potatoes before their harvest time. There is no one size fits all answer. This is because there are plenty of different kinds of potatoes, each with its growth period. Furthermore, many other factors affect the growth and harvest period too.\nSubscribe To My Newsletter\nGet The Latest Updates, News, and Special Offers from Redemption Permaculture!\nAs An Added Gift For Subscribing Receive A Free PDF Download Of My Book: From Home To Small Town Homestead – Over 180 pages"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b274680b-15fa-4d0b-b960-72317dd89778>","<urn:uuid:94673676-f707-4101-b819-e0979d566e8e>"],"error":null}
{"question":"I teach English as a second language. What's the connection between interaction in the classroom and language fossilization? ¿Cómo afecta la interacción al desarrollo del lenguaje?","answer":"While interaction generally promotes language development through negotiation of meaning and feedback, it has a complex relationship with fossilization. Interaction can help prevent fossilization by providing learners with opportunities to receive corrective feedback and notice gaps in their language knowledge. However, interaction isn't always positive - it can sometimes make input more complicated or overwhelming for learners, especially if interlocutors use lengthy paraphrases or complex definitions. Fossilization can still occur despite interactive learning opportunities, as it's a fundamental phenomenon in second language acquisition where certain linguistic features become resistant to change, particularly in adult learners. This can persist regardless of the amount of interaction or instruction received.","context":["The Interaction hypothesis is a theory of second-language acquisition which states that the development of language proficiency is promoted by face-to-face interaction and communication. The idea existed in the 1980s, but is usually credited to Michael Long for his 1996 paper The role of the linguistic environment in second language acquisition. There are two forms of the Interaction Hypothesis: the \"strong\" form and the \"weak\" form. The \"strong\" form is the position that the interaction itself contributes to language development. The \"weak\" form is the position that interaction is simply the way that learners find learning opportunities, whether or not they make productive use of them.\nSimilarly to Krashen's input hypothesis, the interaction hypothesis claims that comprehensible input is important for language learning. In addition, it claims that the effectiveness of comprehensible input is greatly increased when learners have to negotiate for meaning. This occurs when there is a breakdown in communication which interlocutors attempt to overcome. One of the participants in a conversation will say something that the other does not understand; the participants will then use various communicative strategies to help the interaction progress. The strategies used when negotiating meaning may include slowing down speech, speaking more deliberately, requests for clarification or repair of speech, or paraphrases.\nInteractions often result in learners receiving negative evidence. That is, if learners say something that their interlocutors do not understand, after negotiation the interlocutors may model the correct language form. In doing this, learners can receive feedback on their production and on grammar that they have not yet mastered. The process of interaction may also result in learners receiving more input from their interlocutors than they would otherwise. Furthermore, if learners stop to clarify things that they do not understand, they may have more time to process the input they receive. This can lead to better understanding and possibly the acquisition of new language forms. Finally, interactions may serve as a way of focusing learners' attention on a difference between their knowledge of the target language and the reality of what they are hearing; it may also focus their attention on a part of the target language of which they are not yet aware.\nPrimacy of interaction\nAlthough there are several studies that link interaction with language acquisition, not all researchers subscribe to the idea that interaction is the primary means by which language proficiency develops. In a survey of the literature on the subject, Larsen-Freeman and Long say that interaction is not necessary for language acquisition; they do say, however, that it helps in certain circumstances. Gass and Selinker claim that as well as interaction facilitating learning, it may also function as a priming device, \"setting the stage\" for learning rather than being the means by which learning takes place. In addition, Ellis notes that interaction is not always positive. He says that sometimes it can make the input more complicated, or produce amounts of input which overwhelm learners. According to Ellis, this can happen if interlocutors use lengthy paraphrases or give complex definitions of a word that was not understood, and he comes to the conclusion that the role of interaction in language acquisition is a complex one.\n- Johnson, Keith; Johnson, Helen, eds. (1999). \"Interaction Hypothesis\". Encyclopedic Dictionary of Applied Linguistics: A Handbook for Language Teaching. Oxford: Blackwell Publishers. p. 174. ISBN 978-0-631-22767-0.\n- Long, Michael (1985). \"Input and Second Language Acquisition Theory\". In Gass, Susan; Madden, Carolyn. Input in second language acquisition. Rowley, Mass: Newbury House. pp. 377–393. ISBN 978-0-88377-284-3.\n- Ellis, Rod (1984). Classroom Second Language Development: A Study of Classroom Interaction and Language Acquisition. Oxford, UK: Pergamon. p. 95. ISBN 978-0-08-031516-4.\n- Long, Michael (1996). \"The role of the linguistic environment in second language acquisition\". In Ritchie, William; Bhatia, Tej. Handbook of second language acquisition. San Diego: Academic Press. pp. 413–468. ISBN 978-0-12-589042-7.\n- Ellis, Rod (1997). Second Language Acquisition. Oxford Introductions to Language Study. Oxford, New York: Oxford University Press. pp. 47–48. ISBN 978-0-19-437212-1.\n- Richards, Jack; Schmidt, Richard, eds. (2002). \"Interaction Hypothesis\". Longman dictionary of language teaching and applied linguistics. London New York: Longman. p. 264. ISBN 978-0-582-43825-5.\n- Brown, H Douglas (2000). Principles of Language Learning and Teaching. White Plains, NY: Longman. pp. 287–288. ISBN 978-0-13-017816-9.\n- For an overview, see Gass, Susan; Selinker, Larry (2008). Second Language Acquisition: An Introductory Course. New York, NY: Routledge. pp. 353–355. ISBN 978-0-8058-5497-8.\n- Larsen-Freeman, Diane; Long, Michael (1991). An Introduction to Second Language Acquisition Research. London, New York: Longman. pp. 143–144. ISBN 978-0-582-55377-4.","Interlanguage fossilization is a phenomenon of second language acquisition in which second language learners develop and retain a linguistic system, or interlanguage, that is self-contained and different from both the learner’s first language and the target language. This linguistic system has been variously called interlanguage, approximative system, idiosyncratic dialects, or transitional dialects.\nAccording to Corder  this temporary and changing grammatical system, interlanguage, which is constructed by the learner, approximates the grammatical system of the target language. In the process of second language acquisition, interlanguage continually evolves into an ever-closer approximation of the target language, and ideally should advance gradually until it becomes equivalent, or nearly equivalent, to the target language. However, during the second language learning process, an interlanguage may reach one or more temporary restricting phases when its development appears to be detained;. A permanent cessation of progress toward the target language has been referred to as fossilization. This linguistic phenomenon, interlanguage fossilization, can occur despite all reasonable attempts at learning. Fossilization includes those items, rules, and sub-systems that second language learners tend to retain in their interlanguage, that is, all those aspects of interlanguage that become entrenched and permanent, and that the majority of second language learners can only eliminate with considerable effort. Moreover, it has also been noticed that this occurs particularly in adult second language learners’ interlanguage systems.\nSelinker suggests that the most important distinguishing factor related to second language acquisition is the phenomenon of fossilization. However, both his explanation that “fossilizable linguistic phenomena are linguistic items, rules, and subsystems which speakers of a particular native language will tend to keep in their interlanguage relative to a particular target language, no matter what the age of the learner or amount of explanation or instruction he receives in the target language” and his hypotheses on interlanguage fossilization are fascinating in that they contradict our basic understanding of the human capacity to learn. How is it that some learners can overcome interlanguage fossilization, even if they only constitute, according to Selinker, “a mere 5%”, while the majority of second language learners cannot, ‘no matter what the age or amount of explanation or instruction’? Or is it perhaps not that they cannot overcome fossilization, but that they will not? Does complacency set in after second language learners begin to communicate, as far as they are concerned, effectively enough, in the target language, and as a result does motivation to achieve native-like competence diminish?\nThe concept of fossilization in SLA research is so intrinsically related to interlanguage that  considers it to be a fundamental phenomenon of all SLA and not just to adult learners. Fossilization has received such wide recognition that it has been entered in the Random House Dictionary of the English Language (1987). Selinker’s concept of fossilization is similar to that of  and  all of whom attempted to explore the causes of fossilization in second language learners’ interlanguage.\nFossilization has attracted considerable interest among researchers and has engendered significant differences of opinion. The term, borrowed from the field of paleontology, conjures up an image of dinosaurs being enclosed in residue and becoming a set of hardened remains encased in sediment. The metaphor, as used in SLA literature, is appropriate because it refers to earlier language forms that become encased in a learner’s interlanguage and that, theoretically, cannot be changed by special attention or practice of the target language. Despite debate over the degree of permanence, fossilization is generally accepted as a fact of life in the process of SLA.\nMany researchers have attempted to explain this. Workers have attempted to discover: 1) why fossilization occurs  2) the precipitating conditions  3) what kind of linguistic material is likely to be fossilized  and 4) what type of learners are more prone to fossilize. However, there has been almost no investigation by SLA theorists on the possibilities of preventing or overcoming fossilization, and little explanation related to those adult second language learners who overcome one or more ‘areas of stability’ in interlanguage—those learners whose interlanguage does not fossilize, and who reach a high level of proficiency in the second language ).\nOne factor of obvious relevance is motivation, and studies have been conducted regarding motivation to learning second language, and the relationship of fossilization to the learner’s communicative needs. Arguments have emerged regarding adult learners’ general lack of empathy with target language native speakers and culture. According to, adults do not have the motivation to change their accent and to acquire native-like pronunciation. Unlike children, who are generally more open to target language culture, adults have more rigid language ego boundaries. Thus, adults may be inclined to establishing their pre-existing cultural and ethnic identity, and this they do by maintaining their stereotypical accent. Notwithstanding this, there is a lack of needed research, particularly regarding achievement motivation, especially considering that fossilization can be considered the most distinctive characteristic of adult SLA.\nThe text of this article is taken with permission from The Role of Achievement Motivation on the Interlanguage Fossilization of Middle-Aged English-as-a-Second-Language Learners by Dr. Zoran Vujisić (2007).\n- Nemser, 1971\n- Corder, 1971\n- Schumann, 1975\n- Omaggio, 2001\n- Selinker & Lamendella, 1980\n- Tarone (1976)\n- Sridhar (1980)\n- Adjemian, 1976\n- Corder, 1978\n- De Prada Creo, 1990\n- Nakuma, 1998\n- Schumann, 1976\n- Schumann, 1978a\n- Schumann, 1978b\n- Schumann, 1990\n- Naiman, et al., 1996\n- Seliger, 1978\n- Stern, 1975\n- Virgil & Oller, 1976)\n- Selinker & Lakshamanan 1992\n- Todeva, 1992\n- Scovel, 1969\n- Scovel, 1978\n- Scovel, 1988\n- Scovel, 2000\n- Selinker, Swain & Dumas, 1975\n- Virgil & Oller\n- Acton, 1984\n- Birdsong, 1992\n- Bongaerts, et al., 1997\n- Ioup, Boustagui, El Tigi, & Mosell, 1994\n- Gardner, 1988\n- Gardner & Smythe, 1975\n- Nickel, 1998\n- Ushioda, 1993\n- Guiora et al. (1972)\n- Guiora et al., 1972\n- Acton, W. (1984). Changing fossilized pronunciation. TESOL Quarterly, 18, (1), 71-85.\n- Adjemian, C. (1976). On the nature of interlanguage systems. Language Learning, 26,(2), 297-320.\n- Birdsong, D. (1992). Ultimate attainment in second language acquisition. Aneuaee, 68, (4), 706-755.\n- Bongaerts, T. (1999). Ultimate attainment in L2 pronunciation: The case of very advanced late L2 learners. In David Birdsong (Ed.), Second language acquisition and the critical period hypothesis. Mawah, NJ: Lawrence Erlbaum Associates.\n- Corder, S. P. (1971). Idiosyncratic dialects and error analysis. IRAL, 9, (2), 147-160.\n- Corder, S. P. (1978). Language-learner language. In J. C. Richards (Ed), Understanding second and foreign language learning (pp. 71–92). Rowley, MA: Newbury House.\n- Corder, S. P. (1981). Error analysis and interlanguage. Oxford: Oxford University Press.\n- De Prada Creo, E. (1990). The Process of fossilization in interlanguage. (Paper presented at the 9th annual meeting of the World Congress of Applied Linguistics, sponsored by the International Association for Applied Linguistics, Thessaloniki, Greece, April 15?25, 1990).(ERIC Document Reproduction Service No. ED 362 012).\n- Gardner, R. C. (1988). Attitudes and motivation. Annual Review of Applied Linguistics, 9, 135-148.\n- Gardner, R. C., & Smythe, P. C. (1975). Motivation and second language acquisition. Canadian Modern Language Review, 31, (3), 218-230.\n- Guiora, A., Beit-Hallahmi, B., & Brannon, R. (1972). The effects of experimentally induced changes in ego status on pronunciation ability in a second language: An exploratory study. Comprehensive Psychiatry 13, 421-428.\n- Han, Z. (2004). Fossilization in Adult Second Language Acquisition. Clevedon: Multilingual Matters.\n- Ioup, G., Boustagui, E., El Tigi, M., & Mosell, M. (1994). Reexamining the critical period hypothesis. Studies in Second Language Acquisition, 16, 73 – 98.\n- Naiman, N., Frohlich, M., Stem. H. H., & Todesco, A. (1996). The good language learner. Clevedon, Avon. England: Multilingual Matters.\n- Nakuma, C. (1998). A new theoretical account of “fossilization”: Implications for L2 attrition research. IRAL, 36, (3), 247-257.\n- Nemser, W. (1971). Approximative systems of foreign language learners. IRAL, 9, (2), 115-124.\n- Nickel, G. (1998). The role of interlanguage in foreign language teaching. IRAL, 35, (1), 1-10.\n- Omaggio, A. (2001). Teaching language in context. Proficiency oriented instruction. (3rd ed.). Boston, MA: Heinle & Hainle Publishers.\n- Schumann, J. H. (1975). Affective factors and the problem of age in second language acquisition. Language Learning 25, 205-235.\n- Schumann, J. H. (1976a). Second language acquisition research: Getting a more global look at the learner. In Brown, H. (Ed.), Papers in second language acquisition, language learning. Special Issue 4. Ann Arbor, MI: Michigan State University.\n- Schumann, J. H. (1976b). Second language acquisition: the pidginization hypothesis. Language Learning, 26, (2), 391-408.\n- Schumann, J. H. (1978a). Social and psychological factors in second language acquisition. In J. C. Richards (Ed.), Understanding second & foreign language learning (pp. 163–178). Rowley, MA: Newbury House Publishers.\n- Schumann, J. H. (1978b). The pidginization process: A model for second language acquisition. Rowley, MA: Newbury House.\n- Schumann, J. H. (1990). Extending the scope of the acculturation/pidginization model to include cognition. TESOL Quarterly, 24, 667-684.\n- Scovel, T. (1969). Foreign accents, language acquisition, and cerebral dominance. Language Learning, 19, (3 & 4), 245-253.\n- Scovel, T. (1978). The effect of affect on foreign language learning: A review of the anxiety research. Language Learning, 28, (1), 129-142.\n- Scovel, T. (1982), Questions concerning the application of neurolinguistic research to second language learning/teaching. TESOL Quarterly 16, 323-331.\n- Scovel, T. (1988). A time to speak: A psycholinguistic inquiry into the critical period for human speech. New York, NY: Newbury House/ Harper & Row.\n- Scovel, T. (2000). A critical review of the critical period research. Annual Review of Applied Linguistics, 20, 213-223.\n- Selinker, L. (1972). Interlanguage. IRAL, 10, (3), 209-231.\n- Selinker, L., & Lakshamanan, U. (1992). Language transfer and fossilization: The “Multiple Effects Principle”. In S. M. Gass, & L. Selinker (Eds.), Language transfer in language learning (pp. 197–216). Amsterdam: John Benjamins.\n- Selinker, L., & Lamendella, J. T. (1980). Fossilization in interlanguage learning. In K. Croft (Ed.), Reading on English as a second language (pp. 132–143). Boston. MA: Little, Brown and Company.\n- Selinker, L., Swain, M., & Dumas, G. (1975). The interlanguage hypothesis extended to children. Language Learning, 25, (1), 139-152.\n- Sridhar, S. N. (1980). Contrastive analysis, error analysis, and interlangauge. In Croft, K. (Ed.), Readings on English as a second language. Cambridge, Mass: Winthrop.\n- Stern, H. H. (1975). What can we learn from the good language learner? The Canadian Modern Language Review, 31, (4), 304-318.\n- Tarone, E. (1976). The phonology of interlanguage. In J.C. Richards (Ed.),Understanding second and foreign language learning: Issues and approaches. Rowley, MA: Newbury House Publishers.\n- Todeva, E. (1992). On fossilization in SLA theory. PALM (Papers in Applied Linguistics –Michigan), 7, 216-254.\n- Ushioda, E. (1993). Acculturation theory and linguistic fossilization: A comparative case study. CLCS Occasional Paper No. 31. Dublin, Ireland: Centre for Language and Communication Studies; 56pp. (ERIC Document Reproduction Services No. ED 368 172).\n- Vigil, N. A., & Oller, J. W. (1976). Rule fossilization: a tentative model. Language Learning, 26, (2), 281-295."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:d0be8cd4-dff0-4821-ba7a-001f6ee717b2>","<urn:uuid:2610bb28-e0ac-40ac-996d-c7da87f7c39c>"],"error":null}
{"question":"I am an ophthalmologist researching age-related vision disorders. What are the primary manifestations and pathophysiology of temporal arteritis that could lead to vision loss? How is early intervention crucial?","answer":"Temporal arteritis manifests with severe headache, pain when chewing, and tenderness in the temple area, potentially followed by sudden and permanent vision loss. Additional symptoms include shaking, weight loss, shoulder or hip weakness, and low-grade fever. The condition causes arteries to become inflamed and possibly obstructed, and may be caused by an impaired immune system. Early treatment with medication can help prevent vision loss in one or both eyes.","context":["Vision problems in maturing adults\nThere are several eye problems that are more common among people as they age, although they can affect anyone at any age. They include:\nPresbyopia: This is the loss of the ability to clearly see objects or small print at multiple distances. It is a normal process that happens slowly over a lifetime, but you may not notice any change until after age 40. Presbyopia is often corrected with reading glasses or bifocals.\nFloaters: These are tiny spots or specks that float across the field of vision. Most people notice them in well-lit rooms or outdoors on a bright day. Floaters often are normal, but can sometimes indicate a more serious eye problem, such as retinal detachment, especially if they are accompanied by light flashes. If you notice a sudden change in the type or number of spots or flashes you see, visit your eye doctor as soon as possible.\nDry eyes: This happens when tear glands cannot make enough tears or produce poor quality tears. Dry eyes can be uncomfortable, causing itching, burning, or (rarely) some loss of vision. Ironically, patients with dry eyes often complain of tearing because dry eyes are easily irritated, and the eye tries to wash out the irritation using excess tears. Your eye doctor may suggest the use of lubricating eye drops. In addition, using a humidifier in your home, nutritional supplements, such as flaxseed oil, medications to reduce inflammation as a cause, or special eye drops that simulate real tears may be recommended. In more serious cases of dry eyes, plugs may be used to block drainage ducts for tears.\nTearing: Having too many tears can come from being sensitive to light, wind, or temperature changes. Protecting your eyes by shielding them or wearing sunglasses can sometimes solve the problem. Tearing may also mean that you have a more serious problem, such as an eye infection, dry eyes, or a blocked tear duct. Your eye doctor can treat or correct all of these conditions.\nCataracts: A cataract is the progressive clouding of the natural lens within the eye. Since a healthy eye lens is clear like a camera lens, light has no problem passing through the lens to the back of the eye to the retina where images are processed. When a cataract is present, the light cannot get through the lens as easily and, as a result, vision can be impaired. Cataracts often form slowly. They do not cause pain, redness, or tearing in the eye. Some stay small and do not alter eyesight. If they progress to the point at which diminished vision impacts activities of daily living, cataracts can be removed by surgery.\nGlaucoma: This is a disease that can cause permanent and irreversible damage to the optic nerve. It is also often accompanied by an increase in the pressure in the eye. If not treated early, this can lead to permanent vision loss and blindness. Heredity, age and race are significant risk factors for glaucoma. Glaucoma is less commonly caused by other factors such as injury to the eye, severe eye infection, blockage of blood vessels or inflammatory disorders of the eye. Because most people with glaucoma have no early symptoms or pain, it is very important to get your eyes checked by an eye doctor regularly. Treatment may include prescription eye drops, oral medications, laser therapy or surgery.\nRetinal disorders: The retina is a thin lining on the inside of the eye made up of cells that collect visual images and pass them on to the brain. Retinal disorders interrupt this transfer of images. They include age-related macular degeneration, diabetic retinopathy, and retinal detachment. Early diagnosis and treatment of these conditions is important to maintain vision.\nCorneal diseases: The cornea is the clear, dome-shaped “window” at the front of the eye. It helps to focus light that enters the eye. Disease, infection, injury, and exposure to toxic agents can damage the cornea causing eye redness, watery eyes, pain, reduced vision, or a halo effect. Treatments include making adjustments to the eyeglass prescription, using medicated eye drops, or having surgery.\nEyelid problems: The eyelids protect the eye, distribute tears, and limit the amount of light entering the eye. Pain, itching, tearing, and sensitivity to light are common symptoms of eyelid problems. Other problems may include drooping eyelids, blinking spasms, or inflamed outer edges of the eyelids near the eyelashes. Eyelid problems often can be treated with medication or surgery.\nTemporal arteritis: This condition causes arteries to become inflamed and possibly obstructed. It can begin with a severe headache, pain when chewing, and tenderness in the temple area. It may be followed by sudden and permanent vision loss. Other symptoms can include shaking, weight loss, shoulder or hip weakness and low-grade fever. It may be caused by an impaired immune system. Early treatment with medication can help prevent vision loss in one or both eyes. Patients with symptoms should contact their doctor."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:c482fba8-4220-4d72-b78d-eb3dc20eae7b>"],"error":null}
{"question":"What are the mathematical implications of AES S-box attacks in cryptography, and what security risks do they pose for IoT device implementations?","answer":"In AES S-box attacks, the probability of success is high (0.939298096) when no bytes of epsilon are zero, allowing attackers to narrow down potential first-round-key candidates to 2^32 possibilities by analyzing each S-box independently. This mathematical vulnerability poses significant risks for IoT devices, particularly in user-accessible systems like commercial networked HVAC systems, wireless base stations, and connected cars. To mitigate these risks, IoT implementations require both software and hardware security measures, including IP protection, encrypted configuration bit streams, and protection against physical attacks, as software security alone has proven insufficient.","context":["Posted by: jmclaugh\nDate: 17 June 2010 20:19\nI'm afraid I don't follow all of dsds's post above.\ndsds, are you describing a modified attack model where the attacker does not need to know \\delta, but does need to be able to xor it with the post-round-1 state several times, and in which the attack's effectiveness is reduced if any bytes of \\epsilon are zero?\nIf so, you don't explain how the adversary can deduce the value of \\delta, so noting that you attack each S-box independently, I'd like to borrow some ideas from integral cryptanalysis to suggest the following:\nStep 0. Given any p, the adversary obtains p* by, as described, xoring the value \\delta with the state after the first round, and then reversing the first round.\nStep 1. Adversary chooses p.\n(The adversary not knowing \\delta may correspond to it being a block of data situated somewhere else on the target machine that the adversary had a limited amount of access to, I don't know. It doesn't sound very plausible if I'm honest.)\n2. For each of the sixteen S-boxes in turn:\n2.1 The adversary chooses a set of plaintexts p_i (0 <= i < 256) such that p_i has value i in the byte corresponding to the current S-box, and is identical to p in all other bytes.\n2.2 The adversary generates, for each p_i in turn, the corresponding p*_i by applying Step 0. (Obviously the adversary doesn't need to do this for a p_i that's already appeared as a p*_j for j != i)\n2.3 If all values \\alpha_i = (p_i \\oplus p*_i) are zero, the corresponding byte of \\epsilon was zero, this part of the attack fails and the adversary should move on to the next S-box.\n(However, the probability that none of the bytes of \\epsilon are zero is 0.939298096, so the odds are in the adversary's favour.)\n2.4 If this is not the case, precisely one of the values \\alpha_i will have been (p_i \\oplus p*_i) and (p_j \\oplus p*_j) for some p_j not equal to p_i or p*_i. (This follows from the fact that every non-trivial row, and every non-trivial column, of the AES S-box's difference distribution table has precisely one 4 in it.)\nKnowledge of this value is sufficient to identify the byte of \\epsilon corresponding to the current S-box, and to narrow down the number of possible candidates for the corresponding byte of the round key to four.\n2.5 This leaves us with the number of possible first-round-keys reduced to:\n(2^2)^(|s-boxes for which the attack succeeded|)*(2^8)^(|s-boxes for which the attack failed|). Since the attack will succeed for all the S-boxes with high probability, the adversary will probably be left with precisely 2^32 candidate values for the first round key.\nIn spite of all that, though, the overall attack model (adversary is able to modify interim data after the first round and then use oracle access to obtain the corresponding plaintext) does seem to give the adversary an enormous amount of power - is it really plausible? I have my doubts and rather suspect it isn't, but could be wrong...","Biggest security threats for embedded designers\nEmbedded system designers face a number of threats to the applications that they develop for the Internet of Things (IoT). One of the biggest threats comes from IoT devices that end-users can access, such as commercial networked HVAC systems, wireless base stations, power stations, network gateway systems, and avionics networking.\nAnother example is the connected car, including the advanced driver assistance system (ADAS) that encompasses intelligent, interconnected vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) systems. Since vehicles are fielded systems, they are accessible by people with malicious intentions. There can be serious consequences – up to and including loss of life – if, for instance, ADAS systems cannot ensure that V2V and V2I messages originate from a trustworthy source and are not modified between sender and receiver.\nWith these and other systems, software security, alone, has proven inadequate to protect user-accessible devices against known threats. What is needed is a combination of software and hardware security. For example, today’s FPGA SoCs can be used to implement a hardware security scheme that compliments the software and strengthens the system. Ideally, the hardware and software solution should combat three types of security breaches:\n1. Design security: This includes IP protection and ensuring that configuration bit streams are encrypted and protected. In addition, designs need to incorporate a method to ensure there is no overbuilding or cloning of the design possible.\n2. Hardware security: Designers also need to certify that user-accessible devices are resistant to physical attacks. For example, differential power analysis (DPA) attacks can extract keys and other vital device information.\n3. Data security: This element ensures that communications into and out of the device are authentic and secure.\nEmbedded system program managers and development teams must design these types of protections into their products while best leveraging the characteristics of the underlying platform. The result should be a robust protection network with no single point of failure. Some key methods for achieving this goal include:\n∑ Risk assessment: System penetration testing should be used for a detailed system evaluation, to assess critical system data/functions, discover vulnerabilities, enumerate threats, and outline the likelihood and consequence of system compromise.\n∑ Protection planning: Using risk assessments and any other compiled data, developers should seek to understand protection implementation costs and design options for mitigating identified system vulnerabilities and ensuring successful system verification and validation.\n∑ Attack scenario testing: This can include a black box approach, pitting experienced reverse engineers with state-of-the-art attack tools against a system in a deployed setting to reveal vulnerabilities that cannot otherwise be found during most other evaluation exercises.\n∑ DPA side-channel analysis and mitigation: Side-channel attacks are currently the most practical method for compromising cryptography implementations. It is important to regularly perform measurable, objective, and repeatable testing for resistance to side-channel attacks for applications where adversaries have the ability to observe side channels (i.e., power draw, timing, EM emanations) during on-device cryptographic operations.\nIn today’s cyber hacking world, it is essential for every public and private organization to proactively address security issues. Embedded system designers can help their customers in this area by creating secure designs that are protected from today’s rapidly evolving threats, including those posed by a rapidly growing ecosystem of interconnected, user-accessible hardware."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:9f2e6151-fe39-42a9-9142-5342db6c96e2>","<urn:uuid:6fd90e2b-d1fc-4ceb-b428-5c07dc11374d>"],"error":null}
{"question":"How do the standardization efforts of scripts compare between Qin Dynasty China and the Manchu writing system - what was the main motivation in each case?","answer":"In both cases, the standardization was politically motivated but had different specific aims. In Qin Dynasty China, Emperor Qin Shi Huang Di standardized the script (Xiao Zhuan) primarily to maintain control over his unified empire and ensure imperial edicts would be legible across all regions. For the Manchus, Nurhaci's motivation was different - he standardized the script because illiterate Chinese and Mongolians could understand their languages when read aloud, but Manchus could not do the same with their documents which were recorded by Mongolian scribes. The Manchu script was later further standardized by Dahai in 1632 through the addition of diacritical marks to reduce ambiguity.","context":["History of Chinese Calligraphy – The Origins of Calligraphy in Ancient China: The Unification of China and a Common Script in Qin Dynasty\nThe Qin Dynasty (秦朝/Qin Chao, Ch’in Ch’ao)\nIn 221 BCE, the state of Qin finally conquered the remaining states that had made up the Warring States and proclaimed the formation of a new dynasty under a centralized government. This essentially began the Imperial period of Chinese history, with the first Emperor Qin Shi Huang Di (秦始皇帝/Ch’in Shih Huang Di) effectively controlling the entirety of China.\nUnlike the Warring States or the Zhou that came before, the state was maintained not by a feudal system of lords, but by a more direct, and in many cases repressive, hierarchical system that fed directly back to the Imperial throne. China was for the first time unified under a single ruler who maintained direct control over almost every facet of society.\nInscriptions on Qin Stelae\nOne of the Emperor’s first official acts was the erection of stelae at important points throughout the China. A stele is essentially and inscribed stone monument. The Qin stelae were inscribed with politico-religious proclamations designed to cement the Emperor and the clan of Qin as the rightful rulers of all China. Although the Qin Emperor could clearly not be present in every corner of the Empire, claims of his legitimacy could be fixed permanently throughout his realm. Indeed, the written word had such force in early China that the simple acts of inscribing and installing these stelae were in and of themselves a cogent claim of legitimacy. Almost all Qin Stelae have been lost over time, and are available only as rubbings made in later dynasties.\nStandardization of Script and the Emergence of the Xiao Zhuan Calligraphy Script\nIn the Zhou and Warring States, textual variation was very common. Scribes in the various regions adopted different graphs for the same word, separated as they were by vast distances and, often, strict political border. The first Emperor knew that a single writing system would be needed if his unified empire was to be maintained. Imperial edicts and laws would need to be legible to officials in all corners of China. The result is what we now call Xiao Zhuan (小篆/Hsiao Chuan), or small seal script. For the first time, the entirety of China was provided with a single set of characters from which all written works were to be written.\nPolitical Control Through Words\nOf course, this tendency toward far greater standardization in the characters themselves was not without its dark side. The Warring States was a period of incredible philosophical and religious diversity, known for its Hundred Schools and the emergence of Taoism, Confucianism and Legalism. The emergence of technologies needed for the creation and maintenance of a considerable literary corpus allowed an incredible diversity of ideas to flourish within the independent states. The first Emperor believed that these texts could easily challenge the legitimacy of his reign. His own political, ritual, metaphysical and philosophical framework, based on Legalism could not be challenged. In order to quash dissent, Qin Shi Huang Di decreed that all written works not on a prescribed list be summarily burnt. It is only due to the concealment of many works, either by political dissidents or those lying in graves, as was the common practice, that archaeologists have been able to piece together the vast complexity of the literary tradition before the Imperial period.","Manchu/Lesson 12 - The Manchu Script\nHistory and orgin of the Manchu script\nThe current Manchu script is a Semitic script from the Aramaic group of Semitic scripts and is written vertically from left to right (although it was originally written horizontally from right to left). The script was spread east via Nestorian Christians and was adopted by the Sogdians, then the Uyghurs and the Mongols, before finally being adopted by the Manchus. Prior to Nurhaci's adoption of the current Manchu script, the Jurchens (ancestors of the Manchus) used the Jurchen script to write the Jurchen language (which is in effect the precursor to modern Manchu). The early Jurchen script was invented in 1120 by Wanyan Xiyin, acting on the orders of Wanyan Aguda. It was based on the Khitan script, that was inspired in turn by Chinese characters. However, because Chinese is an isolating language and the Jurchen and Khitan languages are agglutinative, the script proved to be cumbersome. The written Jurchen language died out soon after the fall of the Jin Dynasty, though its spoken form survived until the end of the sixteenth century, when Manchu became the new literary language.\nAccording to the Veritable Records (manju-i yargiyan kooli); (滿洲實錄 Mǎnzhōu Shílù), in 1599 the Manchu leader Nurhaci decided to convert the Mongolian alphabet to make it suitable for the Manchu people. He decried the fact that while illiterate Chinese and Mongolians could understand their respective languages when read aloud, that was not the case for the Manchus, whose documents were recorded by Mongolian scribes. Overriding the objections of two advisors named Erdeni and G'ag'ai, he is credited with adapting the Mongolian script to Manchu. The resulting script was known as 'tongki fuka akū hergen' (script without dots and circles).\nIn 1632, Dahai added diacritical marks to clear up a lot of the ambiguity present in the original Mongolian script; for instance, a leading 'k', 'g', and 'h' are distinguished by the placement of no diacritical mark, a dot, and a circle respectively. This revision created the Standard script, known as 'tongki fuka sindaha hergen' (script with dots and circles). As a result, the Manchu alphabet contains little ambiguity. Recently discovered manuscripts from the 1620s make clear, however, that the addition of dots and circles to Manchu script began before their supposed introduction by Dahai.\nHow to learn the Manchu script\nThere are basically two different ways one can learn the Manchu script:\n- Learn it as if it was an alphabet (as is done outside of China)\n- Learn each of the syllables separately (there are a lot of them) via the '12 characters'. Most Chinese students of the language learn Manchu this way.\nThis book will introduce both methods. It is suggested that students first learn the 24 letters for sounds native to Manchu (the 10 letters to transliterate Chinese sounds should be excluded at first) as an alphabet and once they are familiar with these 24 letters they can then work through the '12 character tables' (which are just tables of all the different syllables that can be formed by adding a final to one of the letters) so as to gain greater exposure to the script, and also understand the different rules for writing different syllables. By learning the Manchu script this way, students have the advantage of not being burdened with learning hundreds of syllables when they start learning Manchu, and by just learning the 24 letters they can therefore get a basic grasp of the script very quickly. Students can then move on to the 12 character tables and gain a deep understanding of how the script works without having to spend hours memorising each syllable (the syllables are easy to recognise once you know the 24 letters). Once students are familiar with the 24 letters native to Manchu they can then learn the 10 letters used to write Chinese sounds.\nAs mentioned in the introduction to this book, the Manchu script is only taught in lesson 12, as opposed to at the beginning of the book like in most other textbooks. The reason for this is the fact that it is much easier to decipher the script when the student has a basic grasp of the phonetic features of Manchu and a reasonable vocab.\nThe 24 letters of the Manchu script\nIf we analyse the Manchu script as if it was an alphabet (as opposed to a syllabic script) then the Manchu script has 6 letters to represent the six Manchu vowels, and 18 letters to represent Manchu consonants. As with most other Semetic scripts, each letter of the Manchu script has an independent, an initial, medial and final form.\nIf we start first with the 6 vowels (See letters 1-6 in the above chart). The characters are 1='a', 2='e', 3='i', 4='o', 5='u', 6='v'.\nJuwan juwe ujui hergen (十二个字头) - The 12 characters\nMost students of Manchu in China learn the Manchu script through the '12 characters'. The 12 characters are just the\nThe 10 letters used to transcribe Chinese loans\n- Daniels, P.T., (1996). “Aramaic Scripts for Aramaic Languages”. In Daniels & Bright (eds.) The World’s Writing Systems. New York: Oxford University Press, pp.499-513.\n- Kara, G., (1996). “Aramaic Scripts for Altaic Languages”. In Daniels & Bright (Ed.), The World’s Writing Systems. New York: Oxford University Press, pp.536-558.\n- Niu Ruji., (1997). Weiwuerzu gu wenzi yu gu wenxian daolun, [An Introduction to Uighur Scripts & Documents]. Xinjiang renmin chubanshe, [Xinjiang People’s Publishing House]."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:f9f3f422-724c-42fe-bc8f-c059866f0b2f>","<urn:uuid:23a3c73e-11d6-4170-8bc1-7958ff2b307b>"],"error":null}
{"question":"Being a philosophy student focused on Russian thinkers, I'd like to know if both Gorelov A.A. and Konon Molody wrote any published memoirs or books about their experiences?","answer":"Yes, both figures authored published works, but of very different natures. Konon Molody (under the name Gordon Lonsdale) published 'Spy: Memoirs of Gordon Lonsdale' in 1965 after returning to the Soviet Union. In this book, he discussed his experiences as a spy, though notably claimed that some of his associates were innocent. Gorelov A.A., on the other hand, authored numerous academic works including textbooks and philosophical treatises, such as 'Fundamentals of Philosophy' (2017), 'Russian Revolution and Civilizational Principles of Russia' (2018), and various works on ecology, culture, and philosophy throughout his career.","context":["Scopus Author ID: 57084632200\nGorelov Anatoly Alekseevich was born 23.09.1946 in Zarajsk of the Moscow region.\nGorelov A.A. has higher education; he has ended geological faculty of the Moscow state university of a name of M.V. Lomonosov in 1971 and postgraduate study of Institute of philosophy of the Russian Academy of Sciences in 1974.\nGorelov A.A. is the Doctor of Philosophy. In 1974 has protected the master's thesis on a theme: «Philosophical questions of modelling of biosphere». In 1988 has prepared and has protected the thesis for a doctor's degree on a theme: «the Person and the nature: ways of harmonisation of relations».\nArea of interests is philosophical questions of mutual relations between the person and the nature, development of culture of informative activity of the person.\nGorelov A.A. works at the Institute of Philosophy, since 1973; currently he is the lead researcher of the Department of Philosophical problems of cteativity.\nProfessor of Orthodox St. Tikhon Humanitarian University.\n«Philosophy», «Ecology», «Concepts of the modern natural sciences», «the Science and religion».\nTextbooks and teaching works\nGorelov A.A., Gorelova T.A. Fundamentals of Philosophy: a Tutorial. M.: KnoRus, 2017. 228 p.\nSociology. Moscow, 2016;\nPolitical science. M., 2016;\nConcepts of the modern natural sciences. Moscow, 1996-2016;\nThe History of Native Culture. Moscow, 2015.\nHistory of Russian Culture. M., 2008-2015;\nSocial Science: Textbook for secondary vocational education. Moscow, 2011-2015 (co-authored with A. T. Gorelova);\nSocial Studies: practical work for secondary vocational education. Moscow, 2011-2015 (co-authored with A. T. Gorelova).\nFundamentals of Philosophy. Moscow, 2003-2014;\nPhilosophy. Moscow, 1995-2010;\nThe Basic of Ecology. Moscow, 2013.\nEcology. M., 1998-2009;\nSocial Ecology. Moscow, 2002-2007;\nEthics. M., 2002-2007 (co-authored with A. T. Gorelova);\nCultural science. M., 2001-2007;\nHistory of World Culture. M., 2007-2011;\nHistory of World Religions. M., 2005-2008.\n«Man and Nature», «The problem of truth in scientific knowledge», «Globalization and the Future of Russia».\nAnatoly Gorelov, Stanislava Filipenok, Elena Yaroslavtseva. Creativity, Human Being, Science. Moscow, IPH RAS, 2018. - 101 p. ISBN 978-5-9540-0336-9.\nGorelov A.A Russian Revolution and the Civilizational Principles of Russia. Moscow, Telecom, 2018. 166 p. ISBN 978-5-9912-0717-1.\nGorelov A. A. Russian revolution and civilizational bases of Russia. M.: Hotline-Telecom, 2018. 166 p\n- Smirnova N.M., Gorelov A.A., Morkina J.S. Creativity, Meaning, Interpretation. M.: Russian Academy of Science, 2016. 139 p.\n- Global Neocolonialism in Action: the War of the West against Russia. M.: Letni Sad, 2016. 358 p.\n- Evolution and Meaning of Culture. Moscow: Letniy Sad, 2016. 480 p. (co-authored by T.A Gorelova)\n- Global Neocolonialism in the Activity: a War of the West against Russia and World. Moscow: Letniy Sad, 2016. 358 p.\n- Spiritual Birth and Going away by L. Tolstoy. Moscow: LENAND, 2015. 288 p.\n- Global Neocolonialism and Russian Idea. Moscow: LENAND, 2014. 256 p.\n- Truth and meaning. Moscow, 2010.\n- Globalization and the future of Russia. Moscow, 2009\n- Individuality and evolution. Moscow, 2006\n- The evolution of culture and ecology. Moscow, 2002\n- Split people and integrated personality. Moscow, 1990\n- Man – Harmony – Nature. Moscow, 1990\n- Ecology – the science – modeling. Moscow, 1986\nThe main articles (from 2008 to 2019):\n- Gorelov A.A. Relationship of creativity of culture and creativity of life. Philosophy of Creativity. 5 th Issue. Meaningful dimensions of the sociocultural spaces of creativity /Eds.: Natalia Smirnova, Irina Beskova. Moscow: IIntell, 2019.\n- Gorelov A.A., Gorelova T.A. Awareness of the possibility of self-creation in antiquity: the school of Pythagoras // Knowledge. Understanding. Skill. 2019. № 3. P. 44-54.\n- Gorelov A.A., Gorelova T.A. Self-creation as a protest potential of spiritual culture // Scientific researches of Moscow University for the Humanities. 2019. № 1. С. 148-170.\n- Gorelov A.A., Gorelova T.A. Two faces of the Russian idea in the light of the thesaurus approach [El. res] // Horizons of humanitarian knowledge. 2019. № 2. С. 21–44. URL: http://journals.mosgu.ru/ggz/article/view/1002. DOI: 10.17805/ggz.2019.2.2.\n- Gorelov A.A., Gorelova T.A. Two faces of the Russian idea // Tesaurus and problems of culture: III Academic readings for memory of V.L. Lukova: reports and materials of the all-Russian (national) scientific conference, 4 April 2019. М. : Moscow University for the Humanities, 2019. 401 p. P. 59-83. http://publications.mosgu.ru/ index.php/main/catalog/book/13\n- Gorelov A.A., Gorelova T.A. The dynamics of the ecological niche of a man // Educational technology. 2019.№ 2. P. 36-46.\n- Gorelov A.A., Gorelova T.A. Humanitarian aspects of the ecological niche of man // II Moiseev’s readings: Culture as a factor of national security: reports and materials of the all-Russian (national) scientific conference. Moscow, 26 June 2019. М. : Moscow University for the Humanities, 2019. 517 p. P. 176-186. ISBN 978-5-907194-29-8.\n- Gorelov A.A., Gorelova T.A. Contradictions of the ecological niche of man // Proceedings of the XXVIII International scientific Symposium “Unconventional ecorastenievodstvo, selection, genetics and bio-agriculture. Protection of the bio-noosphere and Cosmology. Philosophy of natural science and ecological education in the Trinity of Economics, ecology and health\". Contradictions of the ecological niche of man. 8-15 September 2019, Alushta. Simferopol: ООО «Forma», 2019. P. 47-58.\n- Gorelov A.A. the creativity and the principles of work with performances in the school of Epictetus // Knowledge. Understanding. Skill. 2018. No. 3. P. 63-78. DOI: 10.17805/zpu.2018.3.6.\n- Gorelov A.A. (In et al. with Gorelova T.) The Evolution of the cultural meaning of sacrifices // scientific works of the Moscow humanitarian University. 2018. No. 2. P. 86-99. DOI: 10.17805/trudy.2018.2.8\n- Gorelov A. A. The Concepts of N. Danilevsky, F. Dostoevsky and L. Tolstoy as spiritual alternatives to the Russian revolution // Revolution, evolution and dialogue of cultures. Reports on the 100th anniversary of the Russian revolution at the world philosophy day at the Institute of philosophy RAS on November 14 and 16, 2017. / edited by A.V. Chernyaev. Moscow : Gnosis. P. 119-133. ISBN 978-5-94244-061-9.\n- Gorelov A.A. F. M. Dostoevsky: the Russian Idea and the Russian Socialism // Knowledge. Understanding. Skill. 2017. N 1. P. 50-65.\n- Gorelov A.A. A. S. Khomyakov: Doctrine on “Sobornost” (Collegiality) and Russian Comminity // Knowledge. Understanding. Skill. 2017. N 2. P. 78-97.\n- Gorelov A.A. Civilizational Factor of the Russian Revolution // Knowledge. Understanding. Skill. 2017. N 3. P. 102-122.\n- Osvald Spengler’s Decline of the West and Possible Decline of the World // Knowledge. Understanding. Skill. 2016. № 1. P. 29-44 (co-auth. by T.A. Gorelova).\n- “Cold War” between Russia and the United States: what are we defending? // Strategic Priorities. 2015. № 3. P. 44-54.\n- N. Ya. Danilevsky’s Concept of Cultural-historical Types and the Contemporary Sociopolitical Reality // Knowledge. Understanding. Skill. 2015. № 4. P. 53-62.\n- Experience as an Evolutional Process // Knowledge. Understanding. Skill. 2015. № 1. P. 18-28 (co-authored by T.A Gorelova).\n- The Dialectics of the Concept of Immortality as the Realization of the Idea of a Common Cause of Nikolai Fyodorov // Vestnik of MCTTU. Series “Philosophical Sciences”. 2015. № 1. P. 28-38.\n- Global Neocolonialism and the Problem of Sovereignty // “Vlast” (Power). 2015. № 2. P. 19-25 (co-authored by I.A. Bronnikov).\n- Tolstoy about the Social Destination of the Creation // Philosophy of the Creation: materials of Russia Scientific Congress. 8-9 April 2015. M.: Institute of Philosophy of the Russian Academy of Sciences, 2015. P. 285-293.\n- Experience, Truth and Meaning // Experience and Meaning. M.: Institute of Philosophy of the Russian Academy of Sciences, 2014. P. 134-154 (co-authored by T.A Gorelova).\n- The Existential Foundations of the “Spiritual Birth” of Leo N. Tolstoy // Knowledge. Understanding. Skill. 2014. № 1. P. 141-152.\n- From the World Colonial System till Global Neocolonialism // Age of Globalization. Journal of Global Studies. 2014. № 2. P. 42-64.\n- Religion and modern biological view of the world // Philosophy and Culture. 2011, № 4 (co-authored with T.A. Gorelova) (0,6/1,2 p.s.).\n- Creativity and true // Creativity: epistemological analysis. М: Institute of philosophy of the Russian Academy of Sciences, 2011 (1,3 p.s.).\n- The global crisis and development of spirituality // Philosophy and Culture. 2010, № 1 (1,2 p.s.).\n- «Integral» knowledge and complete true // Bulletin ПСТГУ. A series 1: «Theology, Philosophy». 2010. Release 1 (1,0 p.s.).\n- A problem of integrity of true // True in sciences and philosophy. М: Alpha-M, 2010 (1,0 p.s.).\n- A spiritual birth and leaving in eternity // Philosophy and culture. 2010, № 5 (1,2 p.s.).\n- Globalization as an objective tendency of world development // the Century of globalization. 2009. № 1 (1,0 p.s.).\n- About meaning of the life // Philosophy and culture. 2009, № 6 (1,2 p.s.).\n- The concept of a passionarity and a problem of meaning of the life // Knowledge. Understanding. Ability. 2009. № 1, 2 (co-authored with T.A. Gorelova) (0,6/1,2 p.s.).\n- A victim as a point of a bifurcation of spiritual culture // Philosophy and culture. 2008. № 6 (co-authored with T.A. Gorelova) (0,5/1,0 p.s.).\n- Trues and disputes (historico-philosophical introduction in a true problem) // Multidimensionality of true. М: Institute of philosophy of the Russian Academy of Sciences, 2008 (1,2 p.s.).\nThe reports at international conferences (from 2008 to 2019):\n- Report on the theme \"Relationship of creativity of culture and creativity of life\" at the conference \"Philosophy of creativity\", April, 2019. Institute of philosophy RAS.\n- Report on \"Humanitarian aspects of human ecological niche\" at the all-Russian conference \"II Moses readings: Culture as a factor of national security\", Moscow, June 26, 2019. Moscow University for the Humanities.\n- Tolstoy about the Social Destination of the Creation. International Scientific Congress “Philosophy of the Creation”. M.: Institute of Philosophy of the Russian Academy of Sciences. 8-9 April 2015.\n- The Language of Tolstoy and Modernity // International Scientific Congress “Tolstoy’s Readings” M.: State Museum of L. Tolstoy, 19-20 November 2014.\n- Pro et contra: Leo Tolstoy about Patriotism. 10 International Scientific Congress: Patriotism, Civility, Culture. M.: Institute of Philosophy of the Russian Academy of Sciences. 3 October 2013.\n- «The involution of culture and modernization – are incompatible» // Culture Information Society and the problems of modernization in Russia. International Scientific Conference. Moscow, MosSU, 2011 (invited plenary lecture).\n- «Tolstoy as a philosopher» // V International Congress of Tolstoy. Moscow – Tula, 2010 (invited plenary lecture).\n- «Consumer Society and the current crisis» // International Conference «International Politics: Problems and Prospects». Drohobych, 2009 (invited plenary lecture).\n- The report «Tolstoy and religion» // 3rd International congress Of Tolstoy. Moscow, 2008 (the invited plenary report).\n- «Education in an informational society» // V International scientific conference «Higher education for the XXI century». Section 5. Higher education and development of the person. М, 2008 (the section report).\nThe editor-in-chief of the collected papers «Multidimensionality of true» (together with M.M. Novoselov).\nThe basic scientific results\nThe basic results of the work and their novelty: the definitions of self-creation and creativity of life are given. It is shown how they work in the broad context of the history of philosophy-the philosophy of Antiquity, religious systems and systems of education. (2019)\nIn the book “Russian Revolution and the Civilizational Principles of Russia” the concepts of the nineteenth century, which are part of the civilization core of Russian culture, are considered: the doctrine of the collegiality of A. S. Khomyakov, the concept cultural and historical types of N. Ya. Danilevsky, the notion of Russian idea and Russian socialism of F. M. Dostoevsky, the concept of non-resistance to evil violence of L. N. Tolstoy. Their influence on the Russian revolution is shown. (2018)\nIn the book ‘Creativity, Human Being, Science’ We consider the relationship between creativity as such and self-creativity, which is understood as a special kind of creativity related to the Creator himself, who is trying to change himself in accordance with his ideas about the ideal of man as a qualitatively new education, not former before. Identified are similar to the stages of creativity self-creativity as: 1) preparation – the attainment of peace of mind prior to the training; 2) cleansing – \"the descent of the mind\" and the attainment of correct ideas; 3) the perfection of a virtuous life; 4) verification of the obtained results. (2018)\nThe articles, published in 2018, investigated the features of collective social creativity in the period of the Russian revolution, as well as the role of ideas and cultural meaning of self-sacrifice in the individual creativity of individual thinkers. The fundamental role of social creativity is shown on the example of concepts N. Y. Danilevsky, F. M. Dostoevsky and L. N. Tolstoy in changing the world, as well as the important role of individual creativity in terms of working with ideas in the transformation of the inner world of man, since ancient times. (2018)\nIn 2010th (untill 2017) engaged in the philosophical problems of creativity: the importance of philosophy of creativity, the problem of understanding the meaning of creativity, the role of creativity in the development of man and society, the main properties and social purpose of art. It is shown that the sense of creativity is a set of processes of transformation of bodily to spiritual and spiritual to corporal. On the topic of \"Creativity and self-creativity\" on the basis of wide use of domestic and foreign historical and philosophical material from antiquity to the present day explores the properties and characteristics of self-creativity as a special kind of creativity, drawn to the subject of creativity. Shows the relationship between the creativity of culture and the transformation of the subject of creativity. In the framework of activities of the Institute of philosophy of the 100th anniversary of the Russian revolution shows how creative constructs, created by A. S. Khomyakov, N. Ya. Danilevsky, F. M. Dostoyevsky, opposed to the revolutionary violence and the disintegration of the Russian civilization.\nIn 2000th years Gorelov investigated knowledge problems, including a parity of true and sense, true and creativity, a role of concept of true in evolutionary epistemology.\nSince the early 1990s Gorelov attaches great importance to today's global problems. He believes that globalization is an objective tendency of world development that continues the general tendency of social integration of humanity. However, globalization should not be limited to any one option, and should include the use of cultural diversity made civilizations past and present. In order to take their rightful place in the globalizing world, Russia's development must comply with the current trend information, mechanization, democratization, ecologization, etc., and at the same time conform to civilization specificity that distinguishes the Russian culture as a link between the cultures of West and East to the vast space Eurasia. For the formation of a new global world is very important to overcome the ecological, socio-economic and spiritual crisis of humanity, his attainment of a higher moral level, what help can be essential spiritual and social potential of the Russian Idea.\nGorelov A.A. was engaged in the study of philosophical questions the relationship between man and nature, including the global dimension of environmental problems in 1970-80. Gorelov to trace the connection of global ecological crisis to crisis socio-economic and spiritual crisis of man. He formulated the immediate scientific, technical and socio-economic problems prevent an ecological catastrophe. To do this, in his opinion, requires the harmonization of the relationship between man and nature in three areas - transformational, cognitive, ethical and aesthetic. He developed the concept of socio-natural progress, and aggressively consumer-loving creative alternatives to the development of personality, integrity, knowledge and transformation of the world, etc.\n1. Silver medal of ENEA for the 2nd place in the contest of philosophical works for the book «Man – Harmony – Nature» in 1990.\n2. The medal «For the 850th anniversary of Moscow» in 1997.","Konon Trofimovich Molody (Russian: Ко́нон Трофи́мович Моло́дый, 17 January 1922 - 9 September 1970) was a Soviet intelligence officer, better known in the West as Gordon Arnold Lonsdale. He was an illegal resident spy during the Cold War and the mastermind of the Portland Spy Ring.\nThe real Gordon Lonsdale\nA person by the name of Gordon Arnold Lonsdale was born on 27 August 1924 in Cobalt, Ontario, Canada. His father was a miner, Emmanuel Jack Lonsdale, and his mother was Olga Elina Bousa, an immigrant from Finland. The Lonsdales were separated in 1931 and a year later, Olga took her son with her back to her native Finland. It is presumed that he died c. 1943 and that his papers were obtained by the Soviets for use by their agents. The real Gordon Lonsdale was circumcised; the imposter was not.\nMolody's early life\nKonon Molody was born in Moscow in 1922, the son of a scientist. His father died when he was a child. According to Konon's son, Trofim Molody, who authored the book about his father Мертвый сезон. Конец легенды (\"The Dead Season. End of the Legend\", 1998), the Soviet intelligence had their eyes on the young boy when the NKVD chief Genrikh Yagoda helped Konon's mother get a passport for him to go to the US in 1934 to live with an aunt in California (according to his official SVR biography, he left the USSR in 1932).\nAfter the war, in 1946, he became a student at the Law Department of the Institute of Foreign Trade, where he studied Chinese. In 1951 he was recruited to the Soviet foreign intelligence service of the KGB and trained as an \"illegal\" spy. He married and had two children.\nIn 1953, Molody went to Canada on a Soviet merchant ship, using the passport of a dead man whose late mother was a Finn married to Canadian citizen Arnold Lonsdale (this had been made possible thanks to the use of Finland's public records captured by the Soviets after the war). From Canada, \"Gordon Lonsdale\" went on to the US, where he helped the atomic spy Rudolph Abel with his communications; there, he also met Peter and Helen Kroger, two Americans, who worked for the KGB because of their communist beliefs.\nIn 1954, Konon Molody went to London, where he took courses at the London University School of Oriental and African Studies. He was an outgoing character and had numerous female friends in London and Europe. Molody went into business, selling and renting jukeboxes, bubble-gum and gambling machines to pubs, clubs and cafes. This took him to continental Europe, where he may have recruited other agents and set up dead letter boxes.\nIt was in 1959 that Molody began receiving British military secrets from Harry Houghton, who was working at the Admiralty Underwater Weapons Establishment on the Isle of Portland. His continental trips also led him to meet Morris Cohen (then using the pseudonym Peter Kroger), whom he often visited in London. He ran other spies, including Melita Norwood.\nConviction of espionage in the UK and exchange\nIn London, on 7 January 1961, Metropolitan Police Special Branch officers, led by Detective Superintendent George Gordon Smith, arrested five people, all of whom were part of the Portland Spy Ring. One of the five was Gordon Lonsdale, who was caught by officers taking secrets from a British spy Harry Houghton on Waterloo Bridge.\nTaken to Scotland Yard, Lonsdale told Smith he would not disclose any information, including his name or address. Western intelligence services, including MI5, the Central Intelligence Agency (CIA), and the Royal Canadian Mounted Police (RCMP), had to resort to extensive enquiries to learn anything about him. All they could determine was that he was Russian, had a naval background, and was not the man his papers made him out to be. By the time he and his associates came to trial at the Old Bailey on 13 March 1961, no one knew his true identity.\nThe \"Lonsdale\" who was put on trial in London in 1961 was charged with spying, along with associates Harry Houghton, Ethel Gee and Morris and Lona Cohen (who were using the aliases Peter and Helen Kroger). Still refusing to reveal his real identity, \"Gordon Lonsdale\" was sentenced to 25 years in prison in March 1961. He was taken to Winson Green Prison, Birmingham, to start his sentence. Although he was in a single cell, he fraternised with some of the Great Train Robbers.\nOn 22 April 1964, he was exchanged for Greville Wynne, a British businessman apprehended and convicted in Moscow for his contacts with Oleg Penkovsky. As part of the process, the Soviets admitted he was a spy and gave the British his real name, Konon Molody. The prisoners were swapped at the Heerstraße Checkpoint in Berlin.\nMemoir and later life\nIn 1965, a year after Molody's return to the Soviet Union, a book called Spy: Memoirs of Gordon Lonsdale was published with the approval of the Soviet authorities. He also claimed Peter and Helen Kroger, convicted as members of the Portland Ring, were innocent.\nFor Molody, life back in the Soviet Union was not a happy one. According to George Blake, he was particularly critical of the way trade and industry were handled. He was given a post of minor importance and took to drinking.\nKonon Molody died, under what was thought by some to be mysterious circumstances, during a mushroom-picking expedition in October 1970; he was 48. Retired KGB officer Leonid Kolosov, Konon's youth friend, who co-authored The Dead Season: End of the Legend, maintained that upon Konon's return from the UK, he was healthy, but shortly afterwards he began complaining that KGB doctors were giving him injections for supposed high blood pressure, whereafter Konon had headaches he never had before the injections but the doctors said he should expect to \"feel worse before he felt better\".\n- Soviet Spy Ring, by Arthur Tietjen, published by Pan Books, 1961\n- Womack, Helen (15 August 1998). \"At last, the truth emerges about Gordon Lonsdale's shadowy life\". The Independent. Retrieved 26 July 2010.\n- Молодый Конон Трофимович Molody's biography on the SVR web site.\n- Lonsdale, Gordon (1965). Spy: 20 Years Of Secret Service. London: N. Spearman. pp. 44–49. ASIN B0000CMR28. LCCN 66001151.\n- Helen Womack. Playboy, Prisoner, Salesman, Spy. - The Moscow Times, 8 August 1998, p. 7.\n- Obituary, Charles Elwell, The Telegraph, 23 January 2008\n- \"April, 22 in history – Russiapedia\". russiapedia.rt.com.\n- Кого и как обменивал Советский Союз // История вопроса Kommersant, 8 July 2010.\n- Gordon Corera, The Art of Betrayal, London, Phoenix, 2012 pp. 230\n- \"Viewpoint: Life after spying - BBC News\". BBC News. Retrieved 2016-01-13.\n- Helen Womack. Playboy, Prisoner, Salesman, Spy. - The Moscow Times, 8 August 1998, p. 8.\n- Soviet Spy Ring, by Arthur Tietjen, published by Pan Books, (1961)\n- SPY: twenty years of secret service: memoirs of Gordon Lonsdale, Hawthorn Books NY, N. Spearman, London, (1965).\n- Spy Book: The Encyclopedia of Espionage, by Norman Polmar and Thomas B. Allen, published by Greenhill Books, ISBN 1-85367-278-5 (1997)\n- The Mitrokhin Archive: The KGB in Europe and the West, by Christopher Andrew and Vasili Mitrokhin, published by Penguin Press History, ISBN 0-14-028487-7 (1999)\n- \"The Portland Spy Case\" by Ludovic Kennedy, in Great Cases of Scotland Yard by Reader's Digest, pages 306-414."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b8af168a-d4f7-4cfd-aecc-6f2b6f19df03>","<urn:uuid:cf67626a-a35e-4cef-9ce5-099ec80a4bb6>"],"error":null}
{"question":"As a health and safety consultant, I would like to understand: ¿Cuáles son las diferencias principales between employers' responsibilities under COSHH and the specific controls needed for engineered stone work in terms of worker protection?","answer":"The key differences are: Under COSHH, employers have general responsibilities like implementing control measures, providing training, maintaining equipment, and ensuring exposure limits aren't exceeded. For engineered stone work specifically, the controls are more specialized due to the very high silica content (90%) and include: eliminating uncontrolled dry cutting, using physical barriers/CNC machines, implementing local exhaust ventilation with H-class HEPA filters, water suppression systems, and requiring specific PPE like properly fit-tested respirators with appropriate filter cartridges. The engineered stone controls are particularly stringent because exposure can lead to accelerated silicosis developing in just 1-10 years.","context":["COSHH – A Guide to Employers’ and Employees’ Responsibilities\nEmployee Responsibilities under COSHH\nEmployee responsibilities within the COSHH (Control of Substances Hazardous to Health) Regulations of 2002 include:\n- Making use of control measures and facilities provided by the employer\n- Ensuring equipment is returned and stored properly\n- Reporting defects/insufficiencies in control measures\n- Wearing and storing personal protective equipment (PPE)\n- Removing PPE that could cause contamination before eating or drinking\n- Making proper use of washing, showering and bathing facilities when required\n- Maintaining a high level of personal hygiene\n- Complying with any information, instruction or training that is provided\nEmployer Responsibilities Under COSHH\nUnder COSHH regulations, employers’ responsibilities include:\n- Implementing control measures to protect workers from hazardous substances.\n- Preventing or adequately controlling exposure to hazardous substances.\n- Providing employees with suitable and sufficient information, instruction and training, and appropriate protective equipment where necessary.\n- Ensuring that control measures are maintained, kept in full working order, and in a clean condition where appropriate.\n- Drawing up plans and procedures to deal with accidents and emergencies involving hazardous substances.\n- Ensuring that any employees exposed to hazardous substances whilst at work are under suitable health surveillance.\n- Ensuring that substances do not exceed the Workplace Exposure Limit (WEL).\n- Carrying out a COSHH risk assessment.\nNaturally, workplaces with higher risks, such as catering or a hair salon, will require more action than, say, an office. But as an employer, you should be assessing what risks may be posed by hazardous substances, no matter where you work.\nThat way, you can identify if there are risks and if so take action to reduce them to a minimum.\nCOSHH Risk Assessment\nA COSHH risk assessment is essentially the same as a standard risk assessment in terms of the process, but your assessment of the workplace will focus solely on hazardous substances.\nIf you’re unfamiliar with risk assessments, here’s a breakdown of the main 5 steps:\n- Identify the hazards.\n- Decide who might be harmed and how.\n- Evaluate the risks and decide on precautions.\n- Record your findings and implement them.\n- Review your assessment and update if necessary.\nRisk assessments will also involve frequently monitoring the workplace’s processes and the level of exposure to substances.\nWorkplaces are active and constantly changing, so a one-off check won’t be sufficient in minimising the risks posed by hazardous substances. You have to remain constantly vigilant and alert to the dangers.\nRecap: What is COSHH?\nCOSHH stands for the Control of Substances Hazardous to Health Regulations (2002). It exists to ensure that both employers and employees do all they can in a workplace to minimise people’s exposure to hazardous substances and work in ways that are safe.\nThis means that all hazardous substances need to be identified and precautions need to be taken to ensure that workers know how to use and handle them safely.\nThe importance of controlling hazardous substances cannot be overstated. In 2012/13, around 35,000 workers reported that they had breathing or lung problems caused by work, and the most common type of reported skin disease was contact dermatitis.\nAnd it’s estimated that around 13,000 deaths occur each year due to occupational lung disease and cancer – fatal conditions that will have developed over a prolonged period of exposure to dusts and chemicals at work.\nAs an employee or employer, you can prevent statistics like this from increasing. If you fulfil your workplace duties, you can prevent dangerous levels of exposure and meet COSHH requirements.\nRecap: What is a Hazardous Substance?\nSimply put, a hazardous substance is any mixture or substance that is toxic, irritant, or corrosive – whether it’s a liquid, gas, vapour, fume, or dust.\nThey cause harm to the body via routes of entry:\n- By coming into contact with skin or eyes.\n- By being inhaled.\n- By being ingested through the mouth.\n- By entering the body through cuts or punctures in the skin.\nAlthough there are certain industries that will be at greater risk, hazardous substances could exist in any workplace. They are often used directly in work activities, produced by work activities, or already present in your workplace’s premises.\nExamples of hazardous substances include:\n- Chemicals, e.g. cleaning chemicals or bleach.\n- Fumes, e.g. from paint or vehicles that exhaust.\n- Gases, e.g. ammonia from refrigerators.\n- Dusts and powder, e.g. from flour.\nIt’s worth noting that even seemingly innocent substances can be harmful, and that includes natural materials like wood dust or flour.\nWhile many hazardous substances can cause immediate harm, such as a corrosive liquid being spilled onto someone’s skin, the main danger posed by hazardous substances is prolonged exposure. For example, if someone is in the presence of or uses a dangerous chemical for a long time, they could develop breathing difficulties or skin conditions.\nExamples of ill-health caused by hazardous substances includes:\n- Occupational asthma.\n- Occupational dermatitis.\n- Occupational cancers.\n- Skin irritation.\n- Infection from bacteria.\n- Injury or death as a result of exposure to toxic fumes.\nWhat to Read Next:\nSubscribe for the latest Hub updates! Tell us what you're interested in hearing about:","This safety alert highlights the serious health and safety risks of exposure to high levels of respirable crystalline silica (RCS) for those who work in the stone benchtop industry.\nSilicosis is an irreversible and progressive disease that causes fibrosis of the lungs from the inhalation of RCS. The only treatment for advanced disease is lung transplant.\nAs of February 2019, 99 confirmed cases of silicosis associated with engineered stone benchtop work had been identified in Queensland, Australia. The vast majority of these cases were in workers with no symptoms of disease.\nMany of these cases have been consistent with accelerated silicosis, a form of the disease which develops over a short period (1 to 10 years) from inhalation of very high concentrations of RCS.\nIn New Zealand the prevalence of silicosis is currently unknown. This safety alert has been issued to raise awareness of the risks to those working in the engineered stone industry.\nWhat we know\nEngineered stone benchtops have become increasingly popular for kitchens and bathrooms. They are made by mixing finely crushed rock with a polymeric resin, then moulded into slabs and heat-cured. The silica content of engineered stone is approximately 90% silica, which is much higher than natural stones.\nWorkers may be exposed to RCS while cutting, grinding, sanding and polishing stone benchtops during manufacturing and installation. Accelerated silicosis is one of three forms of silicosis that has been recently reported in workers working with engineered stone.\nAccelerated silicosis results from the inhalation of very high concentrations of silica dust. It develops in a pattern similar to that of simple silicosis, except the time from initial exposure to the onset of disease is shorter and the progression to complicated silicosis is more rapid. Specifically, nodules increase in size and merge into large lesions, leading to progressive massive fibrosis and ultimately cardio respiratory failure.\nBefore starting work using engineered stone, businesses must complete a risk assessment and review their controls. It is important to eliminate uncontrolled dry cutting, grinding or polishing of engineered stone.\nIf this is not reasonably practicable then exposures must be minimised. Options include:\n- substituting engineered stone for materials with a lower silica content\n- isolating work areas or tasks that generate dust using physical barriers or computer numerical control (CNC) machines\n- using engineering controls, such as local exhaust ventilation (LEV), water suppression (wet cutting), or on-tool dust extraction attachments. Wet sprays should be controlled by guards to prevent dust becoming airborne and wet waste must be managed. LEV system dust collectors or vacuums should be H-class HEPA filtered. Any LEV must be effective, fit for purpose, installed, set up and used correctly and maintained so that it remains effective\n- further minimisation controls include administrative controls, such as good housekeeping practice (wet wiping, using an H-class HEPA-filtered vacuum, and low-pressure water cleaning – dry wiping or sweeping is not appropriate).\nIf a risk still remains, use the appropriate personal protective equipment:\n- use a suitable respirator with a filter cartridge with the appropriate assigned protection factor; the appropriate respirator and filter cartridge combination will be informed by exposure monitoring\n- ensure the respirator is fit-tested for the worker, cleaned and maintained properly\n- wear suitable work clothing such as coveralls that are disposable or can be laundered at the workplace to avoid taking them home.\nThe current New Zealand Workplace Exposure Standard (WES) for RCS (quartz) is 0.05 mg/m3. This value was reduced from 0.1 mg/m3 in November following consultation.\nYou can engage an occupational hygienist from the New Zealand Occupational Hygiene Society(external link) (NZOHS), or from the Health and Safety Association of New Zealand (HASANZ) Register(external link) to measure RCS concentrations and to help evaluate risks to worker health.\nResponse and updates\nWe’re working with medical and health and safety professionals on a coordinated response for workers at high risk of exposure to dust from engineered stone.\nIf you're interested in receiving this information when it is available, and other accelerated silicosis updates, you can record your details in our online form.\nWe provide further guidance related to accelerated silicosis:\n- Silica dust in the workplace\n- 8 key things for workers to know: Controlling silica dust in the workplace\n- Controlling construction dust with on-tool extraction\n- Exposure Monitoring under the Health and Safety at Work (General Risk and Workplace Management) Regulations 2016\n- Health monitoring required under the Health and Safety at Work (General Risk and Workplace Management) Regulations 2016"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:e2c99559-4fd5-48cf-884f-dca2baae7e40>","<urn:uuid:f9311185-dfd7-408b-b03b-b7d83f5f5d23>"],"error":null}
{"question":"How do duration periods compare between trademark registration in the US and basic copyright protection?","answer":"Trademarks and copyrights have substantially different duration periods. In the United States, federal trademarks can potentially last forever, but must be renewed every ten years and require proof of continued use between the 5th and 6th year after registration. In contrast, copyright protection lasts for the life of the author plus 70 years. For works created jointly by multiple authors, copyright extends 70 years after the last surviving author's death, while works for hire, anonymous works, and pseudonymous works are protected for 95 years from publication or 120 years from creation, whichever is shorter.","context":["Logos, pseudonyms, and trade dress are all protected by trademark laws. However, book titles are rarely protected under trademark law because of judicial reluctance to protect titles that are used only once.\nWhat is protected under trademark law?\n(1) A trademark shall protect a sign which may be represented graphically and which is capable of distinguishing the goods or services of one undertaking from those of other undertakings. … shall not, under this Law, be considered to be trademarks.\nWhich of the following Cannot be trademarked?\nAccording to the Paris Convention, a product cannot be trademarked if it: Lacks any distinctive properties or characteristics. Consists of signs or designations that define how the product is made. Has become customary language in the country of origin.\nWhat is not protected under copyright law?\nTitles, names, short phrases, and slogans are not protected by copyright law. … To be protected by copyright, a work must contain at least a minimum amount of authorship in the form of original expression. Names, titles, and other short phrases are simply too minimal to meet these requirements.\nWhat are the examples of not protected by copyright?\nCopyright protection does not cover:\n- Idea, procedure, system method or operation, concept, principle, discovery or mere data as such, even if they are expressed, explained, illustrated or embodied in a work;\n- News of the day and other miscellaneous facts having the character of mere items of press information;\nHow long does trademark last?\nHow long does a trademark last in the US? In the United States, a federal trademark can potentially last forever, but it has to be renewed every ten years. If the mark is still being used between the 5th and the 6th year after it was registered, then the registration can be renewed.\nWhat can and Cannot be patented?\nCertain things can never be patented, regardless of how well they meet these four standards. They include the elements, theoretical plans, laws of nature, physical phenomena, and abstract ideas. … Otherwise, the USPTO will not grant the patent even if you’re trying to patent a great idea.\nCan a saying be trademarked?\nCommon words and phrases can be trademarked if the person or company seeking the trademark can demonstrate that the phrase has acquired a distinctive secondary meaning apart from its original meaning. That secondary meaning must be one that identifies the phrase with a particular good or service.\nWhat qualifies as trademark infringement?\nTrademark infringement is the unauthorized use of a trademark or service mark on or in connection with goods and/or services in a manner that is likely to cause confusion, deception, or mistake about the source of the goods and/or services.\nWhat are three types of works not protected by copyright?\nIn general, copyright does not protect individual words, short phrases, and slogans; familiar symbols or designs; or mere variations of typographic ornamentation, lettering, or coloring; mere listings of ingredients or contents.\nWhat are the 3 elements of a copyright law?\nThere are three basic requirements for copyright protection: that which is to be protected must be a work of authorship; it must be original; and it must be fixed in a tangible medium of expression.\nDoes copyright law protect ideas?\nCopyright does not protect ideas, concepts, systems, or methods of doing something. You may express your ideas in writing or drawings and claim copyright in your description, but be aware that copyright will not protect the idea itself as revealed in your written or artistic work.\nWhat has no copyright symbol?\nThere are no standard symbols that denote that something is not copyrighted, but some businesses use Creative Commons licenses or put a note next to an item indicating it is not copyrighted.\nWhat’s considered fair use?\nFair use permits a party to use a copyrighted work without the copyright owner’s permission for purposes such as criticism, comment, news reporting, teaching, scholarship, or research.","Along with blogging comes responsibility and ignorance of the law is no excuse. Copyrightable works include but are not limited to literary works such as articles, blog posts, stories, journals, or computer programs, pictures and graphics, as well as audio and video recordings. Copyrights do not need not be applied for as they are vested in the creators of intellectual property. When we create something — we own the copyright, which is our exclusive right as the creator to control who else can use our work and in what manner. *\nBerne Convention – International Copyright Agreement\nThe Berne Convention (for the Protection of Literary and Artistic Works) was established in 1886 and is an international agreement that governs copyright. The Berne Convention requires its signatories to recognize the copyright of works of authors from other signatory countries (known as members of the Berne Union) in the same way as it recognizes the copyright of its own nationals.\nIn the United States, the Library of Congress officially registers copyrights which now last for the life of the author plus 70 years.\n- In the case of a joint work prepared by two or more authors who did not do a work for hire, the term lasts for 70 years after the last surviving author’s death.\n- In the case of works for hire, and for anonymous and pseudonymous works (unless the author’s identity is revealed in Copyright Office records), the duration of copyright will be 95 years from publication or 120 years from creation, whichever is shorter.\nThanks to organizations like Creative Commons, licenses like the GNU Free Documentation License, and the public domain, there are many images, songs, movies and documents freely available for you to download and republish without fear of violating copyright.\nPublic domain definition: The public domain is generally defined as consisting of works that are either ineligible for copyright protection or with expired copyrights. Public domain refers to the total absence of copyright protection for work The public domain is a range of abstract materials commonly referred to as intellectual property which are not owned or controlled by anyone. The term indicates that these materials are therefore “public property”, and available for anyone to use for any purpose.\nOnce in the public domain, it is always in the public domain. However, any variation on any public domain work becomes the property of the person making the variation, and it receives an automatic copyright, just as do completely original works.\nWhen Copyright Protection Becomes Public Domain\nThe data below will let you know when you can safely use a piece of art or music without permission because it is now in public domain after copyright protection expiration, or how long the copyright protection will last.\n|Published before 1923 – now in public domain.\n||Published from 1923 to 1963 – When published with a copyright notice © or “Copyright [dates] by [author/owner]” – copyright protection lasts 28 years and could be renewed for an additional 67 years for a total of 95 years. If not renewed, now in public domain.||Published from 1923 to 1963 – When published with no notice – now in public domain.||Published from 1964 to 1977 – When published with notice – copyright protection lasts 28 years for first term; automatic extension of 67 years for second term for a total of 95 years.|\n|Created before 1/1/1978 but not published – copyright notice is irrelevant – copyright protection lasts for the life of author and 70 years or 12/31/2002, whichever is greater.||Created before 1/1/1978 and published between 1/1/1978 and 12/31/2002 – notice is irrelevant – copyright protection lasts the life of author and 70 years or 12/31/2047, whichever is greater.||Created 1/1/1978 or after – When work is fixed in tangible medium of expression – notice is irrelevant – copyright protection lasts for the life of author and 70 years based on the longest living author if jointly created or if work of corporate authorship, works for hire, or anonymous and pseudonymous works, the shorter of 95 years from publication, or 120 years from creation.|\n* Reblogging and WordPress.com Terms of Service: “By submitting Content to Automattic for inclusion on your Website, you grant Automattic a world-wide, royalty-free, and non-exclusive license to reproduce, modify, adapt and publish the Content solely for the purpose of displaying, distributing and promoting your blog.”\nI have listed many free sources of images on my Resources page and my list is by no means complete. Do you have favorite sources of free images, songs, movies and documents you would like to share? If so please comment so I can include your sources on my Resources page as well.\nRelated posts found in this blog:\nCopyright basics for bloggers\nHow to copyright your digital works\nContent theft: The come and get it solution\nSplog Off! Dealing with content theft\nSplogSpot: Dealing with content thieves\nCopyright: Fair Use Limitations\nWhat is copyright?"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:238df833-adfb-4ad1-9faa-7ab1f36e1464>","<urn:uuid:70d48e88-fb82-4226-a927-1bec67f79226>"],"error":null}
{"question":"Do both X-ray crystallography and Fourier transform spectroscopy use Fourier transforms in their data processing?","answer":"Yes, both techniques rely on Fourier transforms in their data processing. In X-ray crystallography, computer programs act as a lens to reconstruct molecular images from diffracted radiation by treating reflections as waves and recombining them through Fourier transforms. Similarly, in Fourier transform spectroscopy, the optical spectrum is computed from raw interferometer data by applying a Fourier transform algorithm to convert the time-domain interference signal into frequency-domain spectral information.","context":["By Gale Rhodes\nCrystallography Made Crystal transparent is designed to fulfill the necessity for an X-ray research that's among short textbook sections and entire remedies. The ebook offers non-crystallographers with an intellectually fulfilling clarification of the rules of the way protein versions are gleaned from X-ray research. the knowledge of those suggestions will foster clever use of the types, together with the popularity of the strengths and weaknesses of images or special effects. on the grounds that proteins include nearly all of the mass of macromolecules in cells and perform biologically vital projects, the publication could be of curiosity to biologists.\nProvides available descriptions of rules of x-ray crystallography, equipped on uncomplicated foundations for someone with a simple technological know-how background\nLeads the reader via transparent, thorough, unintimidating motives of the math at the back of crystallography\nExplains how one can learn crystallography papers in learn journals\nIf you utilize computer-generated versions of proteins or nucleic acids for:\nStudying molecular interactions\nDesigning ligands, inhibitors, or drugs\nEngineering new protein functions\nInterpreting chemical, kinetic, thermodynamic, or spectroscopic data\nStudying protein folding\nTeaching macromolecule structure,and a good way to learn new constitution papers intelligently; develop into a smarter consumer of macromolecular types; and need to introduce undergraduates to the real topic of x-ray crystallography, then this publication is for you\nRead or Download Crystallography Made Crystal Clear. A Guide for Users of Macromolecular Models PDF\nBest crystallography books\nReally good research starts with basics of x-ray diffraction thought utilizing Fourier transforms, then applies normal effects to numerous atomic constructions, amorphous our bodies, crystals and imperfect crystals. easy legislation of X-ray diffraction on crystals stick to as distinctive case. hugely valuable for solid-state physicists, metallographers, chemists and biologists.\nX-ray crystallography offers us with the main exact photograph we will get of atomic and molecular constructions in crystals. It presents a difficult bedrock of structural leads to chemistry and in mineralogy. In biology, the place the buildings aren't totally crystalline, it could actually nonetheless supply invaluable effects and, certainly, the effect right here has been progressive.\nCrystallography is without doubt one of the so much multidisciplinary sciences, with roots in fields as various as arithmetic, physics, chemistry, biology, fabrics technology, computation and earth and planetary technology. The structural wisdom won from crystallography has been instrumental in buying new degrees of figuring out in several clinical parts.\nDuring this thesis Johanna Bruckner stories the invention of the lyotropic counterpart of the thermotropic SmC* part, which has turn into recognized because the merely spontaneously polarized, ferroelectric fluid in nature. through polarizing optical microscopy, X-ray diffraction and electro-optic experiments she firmly establishes features of the constitution of the unconventional lyotropic liquid crystalline section and elucidates its interesting houses, between them a said polar electro-optic influence, analogous to the ferroelectric switching of its thermotropic counterpart.\nAdditional resources for Crystallography Made Crystal Clear. A Guide for Users of Macromolecular Models\nR e m e m b e r that the c o m p u t e r p r o g r a m s that m a k e this conversion An Overview of Protein Crystallography 20 are a c t i n g as a l e n s to r e c o n s t r u c t an i m a g e f r o m diffracted r a d i a t i o n . E a c h reflection is p r o d u c e d b y a b e a m of e l e c t r o m a g n e t i c r a d i a t i o n ( x - r a y s ) , so t h e c o m p u t a t i o n s e n t a i l t r e a t i n g t h e reflections as w a v e s a n d r e c o m b i n i n g t h e s e w a v e s to p r o d u c e an i m a g e of t h e m o l e c u l e s in t h e u n i t c e l l .\nB. X-ray structures are compatible with other structural evidence F u r t h e r e v i d e n c e for t h e s i m i l a r i t y of s o l u t i o n a n d c r y s t a l s t r u c t u r e s is t h e c o m p a t i b i l i t y of c r y s t a l l o g r a p h i c m o d e l s w i t h t h e r e s u l t s of c h e m i c a l s t u d ies o n p r o t e i n s . F o r i n s t a n c e , t w o r e a c t i v e g r o u p s in a p r o t e i n m i g h t b e l i n k e d b y a c r o s s - l i n k i n g r e a g e n t , d e m o n s t r a t i n g t h e i r n e a r n e s s .\n7 - 2 . 1 0 , t h e diffrac t i o n p a t t e r n s a r e t h e F o u r i e r t r a n s f o r m s of t h e c o r r e s p o n d i n g o b j e c t s o r ar r a y s of o b j e c t s . T o p u t it a n o t h e r w a y , t h e F o u r i e r t r a n s f o r m is t h e l e n s - s i m ulating operation that a c o m p u t e r performs m o l e c u l e s in t h e c r y s t a l . T h i s v i e w of p(x,y,z) t o p r o d u c e an i m a g e of as t h e F o u r i e r t r a n s f o r m of t h e s t r u c t u r e f a c t o r s i m p l i e s t h a t if w e c a n m e a s u r e t h r e e p a r a m e t e r s — a m p l i t u d e , f r e q u e n c y , a n d p h a s e — o f each reflection, t h e n w e c a n o b t a i n t h e The mathematics of crystallography: A brief description f u n c t i o n p(x,y,z), 27 g r a p h t h e f u n c t i o n , a n d \" s e e \" a f u z z y i m a g e of t h e m o l e c u l e s in t h e u n i t c e l l .","Fourier Transform Spectroscopy\nFourier transform spectroscopy is a method where one computes an optical spectrum from raw data by applying a Fourier transform algorithm. The method is applied in various techniques for spectroscopy – most often in the context of infrared spectroscopy.\nThe term time domain spectroscopy is also common, because the measured interference signal is measured in the time domain e.g. in the sense that an optical time delay is varied.\nThe operation principle of Fourier transform spectroscopy in its most common form is fairly simple to understand. The investigated electromagnetic radiation (most frequently, infrared light) is sent to an interferometer, normally in the form of a Michelson interferometer. One then measures the optical power at the output of the interferometer as a function of the arm length difference, using some photodetector. That arm length difference is usually manipulated by mechanically moving a mirror (or more conveniently a retroreflector) over some distance.\nIf the optical input to the interferometer were monochromatic, one would obtain a sinusoidal oscillation of the detected power as a function of arm length difference, and the period of that oscillation would be the optical wavelength. If the light is polychromatic, the recorded interferogram will be a superposition of contributions from the different wavelength components. Therefore, it is clear that by applying a Fourier transform to those data one can retrieve the optical spectrum – more precisely, the power spectral density as a function of optical frequency or wavelength. Information on the spectral phase is not obtained. Some corrections need to be applied to the obtained spectrum, as explained below.\nFor a mathematically founded understanding, consider that the interferogram signal, resulting from the superposition of two optical electric fields with a certain time delay can be expressed as follows:\nwhere I(τ) can be the intensity of the interference signal or alternatively a photocurrent. That signal can be decomposed into a constant and a τ-dependent part; the latter is:\nThis is essentially just the autocorrelation of the electric field. According to the Wiener–Kinchine theorem, the Fourier transform of that is the intensity spectrum of the electric field, i.e., the optical spectrum.\nThe explained operation principle can easily be adapted for absorption spectroscopy. One can record an optical spectrum with and without a specimen inserted into the beam path – before or after the interferometer – and compare the computed spectral intensities to obtain the absorption of the sample in a wide range of wavelengths. More precisely, one obtains the loss of spectral intensity caused by the sample, which may not only be caused by absorption of the specimen but also by surface reflections, for example.\nNote that there are also other, less common forms of Fourier transform spectroscopy. For example, terahertz waveforms can be recorded in the time domain with an optical sampling technique based on a photoconductive antenna (see the article on terahertz detectors). One can then apply a Fourier transform to obtain the optical spectrum of a terahertz pulse, in that case also obtaining the spectral phase.\nVarious Practical Aspects\nRequired Spatial Range and Resolution\nThe obtained spectral resolution is limited by the maximum optical path length difference. This is easy to see considering the properties of discrete Fourier transforms, or simply recognizing that the range of path length differences determines the number of oscillation cycles which can be counted. Quantitatively, the resolution in terms of spectroscopic wavenumber is the inverse of the maximum optical path length difference. Simple instruments may work with only a few centimeters of path length difference, achieving spectral resolutions of somewhat better than 1 cm−1, while high precision spectrometers work with much longer path length differences, e.g. several meters.\nOn the other hand, the maximum wavenumber is half the inverse spatial resolution of the measured path length difference. Therefore, a not particularly high spatial resolution is required for instruments working only with relatively long optical wavelengths, while UV instruments are more demanding in that respect. The spatial accuracy, however, should be much higher – see below.\nFor calculations, note that the variation of pass length difference in a Michelson interferometer is twice the amount of movement of a retroreflector.\nFor spectrometers as used in infrared spectroscopy, often uses a very broadband light source for measuring optical properties of samples in a wide wavelength region. The light source should of course have a sufficiently high spectral flux and emit continuously with stable optical properties throughout the interferometer scan. For the near infrared, incandescent lamps are suitable, but there emission is limited to wavelengths below roughly 5 μm by the transmissivity of the bulb glass. For longer wavelength regions, and therefore uses for millimeters not requiring a glass bulb – for example, Nernst glowers based on an electrically heated rod made of zirconium/yttrium ceramics. Silicon carbide rods can even be used up to about 40 μm. Also there are mercury vapor lamps.\nFor the interferometer to work properly, one requires a light beam with high enough spatial coherence. This is because different spatial components of a beam can produce different contributions to the interferogram, effectively washing out the pattern.\nIdeally, one would have a Gaussian beam from a laser source. In practice, however, one often deals with incoherent sources, where the light has to be spatially filtered, accepting some loss of optical power. However, the possible power throughput is still substantially better than for a grating monochromator as used in other forms of spectroscopy, where light needs to be fed through a narrow optical slit. This is called the Jacquinot advantage, named after Pierre Jacquinot who identified it.\nThe Beam Splitter\nThe optical components of the interferometer should of course properly work over the full spectral region of interest. The most substantial challenge arises from the beam splitter, which would ideally exhibit a 50:50 splitting ratio for all relevant wavelengths. That is not strictly required, but it should at least not lead to highly asymmetric splitting or introduce high power losses e.g. by absorption in a substrate. In infrared spectroscopy, one often uses beam splitters with calcium fluoride (CaF2) substrates for wavelengths up to 8 μm. KBr-based beam splitters with a germanium-based coating can be used up to 25 μm wavelength, but that material is hygroscopic and must therefore be carefully protected against moisture. For the far infrared, one often uses polymer films.\nCalibration of Arm Length Variations\nThe interferometer arm length difference is usually varied with a motorized drive, which can normally not be trusted to provide sufficiently accurate variations of the position. Therefore, one often simultaneously records a second interferogram, using light from a narrow-linewidth laser with sufficiently stable wavelength. One can then computationally correct the data for any deviations of the movement from a perfectly linear movement.\nNote that it is not sufficient only to have a positional accuracy which allows one to clearly resolve the oscillations of an interferogram. This is because random position errors also limit the signal-to-noise ratio of the obtained spectra. Therefore, it is essential to realize Fourier transform spectrometers with highly accurate opto-mechanics and an accurate reference interferometer. That also provides a very high wavelength accuracy – better than in dispersive instruments.\nCalibration of Spectral Power Density\nA simple Fourier transform applied to the raw data will generally not deliver a calibrated optical spectrum, mostly because the responsivity of the used photodetector and the reflectivity of the beam splitter are wavelength-dependent; further influences can come from other optical elements of the setup. Such influences do not matter in absorption spectroscopy, because one only compares spectra with and without an absorbing sample, and the obtained intensity ratios are not affected; one only requires sufficiently strong signals for all relevant wavelengths.\nWhen measuring optical spectra of sources, however, one needs to apply a calibration. It may be done, for example, by comparing with the recorded spectrum of a light source with known spectral shape. In the infrared, one often uses black body radiation for that calibration.\nIn some cases, one may even calibrate a spectrometer for obtaining absolute values of the power spectral density. This is often not easy, however, for example because of influences of the required spatial filtering of the input beam (see above).\nDiscrete Fourier transforms can quite easily and efficiently be computed, using a Fast Fourier Transform (FFT) algorithm. In the simplest form, such an algorithm works with a number of data points which is a power of 2. Even on a relatively simple microprocessor, the FFT computation usually takes much less time than the acquisition of the raw data.\nInterference-based methods of spectroscopy have been used already in the early days of optics, for example by Hippolyte Fizeau, who resolved the doublet of the yellow sodium fluorescence line in the 19th} century (→ Fizeau interferometers). However, computations of optical spectra based on Fast Fourier transform have been implemented only from the middle of the 20th century on, when computers became available; first commercial devices appeared in the 1960s.\nReducing the Sensitivity to Mechanical Noise\nFor the kind of interferometer as explained above, the sensitivity to mechanical noise (vibrations and shocks or inaccuracies of an optical delay line) is quite high. That sensitivity can be massively reduced by using a common-path interferometer based on birefringence. This can be realized, for example, with a simple optical beam path where two polarization components finally interfere at a polarizer. The optical delay between the two polarization components can be adjusted by moving a wedged birefringent crystal . Because that does not only greatly reduce the sensitivity to vibrations, but also allows very accurate scanning of the delay range, it is particularly suitable for Fourier transform spectroscopy in relatively short wavelength regions.\nApplications of Fourier Transform Spectroscopy\nThe method of Fourier transform spectroscopy is most frequently used in conjunction with infrared light – for the following reasons:\n- Particularly in the far infrared, it is difficult to realize focal plane arrays as required for conventional spectrographs, for example. It is thus preferable to use a method where only a simple photodetector is required.\n- Due to the limited sensitivity of infrared detectors (particularly at very long wavelengths), it is important to use the light efficiently. It is thus beneficial to avoid excessive power losses at the input slit of a monochromator (Jacquinot advantage, see above). Besides, one also enjoys the Fellgett advantage (named after a Peter Berners Fellgett, the pioneer of the method): if the measurement noise is dominated by detector noise (e.g. thermal electronic noise) rather than by shot noise, the achievable signal-to-noise ratio is substantially better for the Fourier transform method than for scanning the spectrum with a tunable monochromator, where only a tiny part of the optical spectrum is utilized at any time. This is particularly true in cases where a high spectral resolution is required.\n- The Fourier transform method is even somewhat simpler to implement in the infrared, because the required spatial resolution is lower than for visible and ultraviolet light.\nThe main application of the method is in devices for measuring either optical spectra of light sources or wavelength-dependent properties of materials, such as the transmissivity (e.g. reduced by absorption lines) or the reflectivity.\nThe principle of Fourier transform spectroscopy is also applied in wavemeters, although those usually deliver only the peak wavelength rather than the full optical spectrum.\nThere are also applications of the principal in technical fields outside photonics, for example in the context of nuclear magnetic resonance imaging and mass spectroscopy\nQuestions and Comments from Users\nHere you can submit questions and comments. As far as they get accepted by the author, they will appear above this paragraph together with the author’s answer. The author will decide on acceptance based on certain criteria. Essentially, the issue must be of sufficiently broad interest.\nPlease do not enter personal data here; we would otherwise delete it soon. (See also our privacy declaration.) If you wish to receive personal feedback or consultancy from the author, please contact him e.g. via e-mail.\nBy submitting the information, you give your consent to the potential publication of your inputs on our website according to our rules. (If you later retract your consent, we will delete those inputs.) As your inputs are first reviewed by the author, they may be published with some delay.\n|||P. B. Fellgett, “Theory of infra-red sensitivities and its application to investigations of stellar radiation in the near infra-red” (PhD thesis, 1949)|\n|||P. B. Fellgett, “On the ultimate sensitivity and practical performance of radiation detectors”, J. Opt. Soc. Am. 39 (11), 970 (1949), doi:10.1364/JOSA.39.000970|\n|||P. Jacquinot, “New developments in interference spectroscopy”, Rep. Prog. Phys. 23 (1), 267 (1960), doi:10.1088/0034-4885/23/1/305|\n|||L. Mertz, “Astronomical photoelectric spectrometer”, Astron. J. 71, 749 (1966)|\n|||M. F. A’Hearn, F. J. Ahern and D. M. Zipoy, “Polarization Fourier spectrometer for astronomy”, Appl. Opt. 13 (5), 1147 (1974), doi:10.1364/AO.13.001147|\n|||F. Adler et al., “Mid-infrared Fourier transform spectroscopy with a broadband frequency comb”, Opt. Express 18 (21), 21861 (2010), doi:10.1364/OE.18.021861|\n|||A. Oriana et al., “Scanning Fourier transform spectrometer in the visible range based on birefringent wedges”, J. Opt. Soc. Am. A 33 (7), 1415 (2016), doi:10.1364/JOSAA.33.001415|\n|||F. Johnston, “In search of space: Fourier-spectroscopy”, Chapter 7 of Shinn, Terry and Joerges, Bernward (Eds), Instrumentation: Between Science, State and Industry, Kluwer Academic (2000), available online|\n|||S. P. Davis, M. C. Abrams and J. W. Brault, Fourier transform spectrometry, Academic Press, ISBN-13: 978-0120425105 (2001)|\n|||F. J. J. Clarke et al., “FTIR measurements – standards and accuracy”, Vib. Spectrosc. 30 (1), 25 ( 2002)|\n|||J. Mandon et al., “Fourier transform spectroscopy with a frequency comb”, Nature Photon. 3, 99 (2009), doi:10.1038/nphoton.2008.293|\nSee also: spectroscopy, Michelson interferometers, optical coherence tomography, white light interferometers\nand other articles in the categories light detection and characterization, optical metrology, methods"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:1a016403-962f-44f5-bbe8-449818162d44>","<urn:uuid:111c826b-1820-49fa-84ea-b483250c559d>"],"error":null}
{"question":"What are the recommended treatment approaches for rotator cuff tendinopathy, and how do different imaging methods support the diagnosis?","answer":"Treatment for rotator cuff tendinopathy begins with relative rest and appropriate loading during the irritable phase, which typically lasts at least a month. Initial exercises include gentle isometric contractions, with submaximal contractions held for 30 seconds, repeated 3 times with 3-5 minutes rest between repetitions. For diagnosis and monitoring, MRI is particularly effective as it can precisely visualize soft tissues, muscles, ligaments, and tendons affected by tendinopathy. While CT scans can provide detailed bone imaging using X-rays, MRI is superior for detecting soft tissue injuries and inflammation, though it's more expensive and time-consuming than CT scans.","context":["Selecting exercises for rotator cuff-related shoulder pain – Interview with Hilkka Virtapohja\n“Creating an exercise program for a client with rotator cuff pain is more an art than a protocol. Exercise templates offer evidence- and function-based possibilities to correct the biomechanics and strengthen the muscles and tendons. Every exercise can be used as an assessment. When creating an exercise program using a template as a base, some exercises may need to be deleted or modified as not all would be suitable for every client. Some exercises may need to be modified individually to provide suitable progression for improvement of functional activity.”\nRotator cuff tendinopathy has been considered as the most common cause of shoulder pain. It refers to pain and weakness, usually experienced with shoulder elevation. Evidence suggests that tendinopathy is associated with excessive load of rotator cuff tissues. Aging is also a normal and predominant factor in the development of rotator cuff degeneration.\nWhat kind of exercises can be used at the irritable phase of rotator cuff tendon-related shoulder pain?\nAcute irritable RC tendinopathy is characterized by almost constant night pain. Relative rest and finding the correct amount of loading at work and activities is essential to calm the irritation.\nUsually it takes at least a month before the tissues calm down and before the real strengthening exercises can be started. If there is tendon or bursa swelling, gentle isometric contractions towards the painful direction may be helpful. They increase the mechanoreceptor activity and maintain the tissue strength and metabolism during recovery with respect of the irritated tissues. Exercises also guide the pressure from the localized area, from anteriorly and superiorly translated humeral head, to more centered and spacious place at glenoid fossa. Exercises may help the pressure to be spread toward the whole shoulder circle and assist the muscle activation in proximal and distal kinetic chain. Submaximal isometric contractions are suggested to be performed with 30 seconds hold (the duration can be modified), repeated 3 times with 3-5 minutes relaxation between the repetitions. Exercises are repeated from three to five times per day, if helpful.\nAfter the symptoms calm down with exercises and pain education or with help of bursal injections or medication, the gradual reloading exercises can be started.\nHow does shoulder biomechanics help to choose valuable exercises for the client?\nMovement is energy and it is most efficient when flowing freely. For balanced and smooth shoulder biomechanics, all body parts need to support that functioning. It’s important to explore, if the rotator cuff is affected by compensatory compression, torsion or translation forces due to movement restrictions or muscle weakness in other areas of the body. The body always functions as a whole and shoulder tissues can be affected e.g. by lack of body awareness or restricted spine mobility. Also often the muscle strength or length in different body areas don’t meet the demands of the task or work. The shoulder acts best like a tunnel for the movement forces, without angulations or catches. Rotator cuff muscles play an important part of maintaining the ‘tunnel’ open, by centering the humeral head to the glenoid in every arm position.\nAnatomical studies have shown that rotator cuff muscles are not really separate muscles, but overlay each other as an expansion of the joint capsule, activated and fine-tuned by the mechanoreceptors of the capsule. Also, there is no separation of rotator cuff activation and scapula muscle activation, rotator cuff exercises are scapular exercises, scapular exercises are rotator cuff exercises and can be played accordingly.\nThe exercise program usually includes different movement directions and scapular and rotator cuff muscle strengthening. The posterior rotator cuff is addressed and facilitated, because it controls the anterior translation of the humeral head during flexion.\nHow do you select exercises for rotator cuff strengthening?\nSpecific exercises may show high rotator cuff muscle EMG-activation, because there is a demand to stabilize the glenohumeral joint and prevent the translation of the humeral head. Sometimes the exercise with highest EMG-activation is not the best one, if there is tendon swelling. The biomechanics of the exercise isn’t necessarily beneficial for the client either. E.g. ‘push up plus’ exercise activates effectively serratus anterior and subscapularis, but doesn’t help a client, who is lifting or hammering above shoulder level. Shoulder extension activates the subscapularis to prevent forward translation of the humeral head, but doesn’t necessarily meet the client’s functional needs. Clinical reasoning and sensitivity are important to find the best exercises and shoulder research is a valuable tool to be used.\nClosed chain exercises increase the shoulder mechanoreceptor activity through compression and may be used to activate rotator cuff. Closed chain during the movement, wall slides or four-point kneeling or plank positions may be beneficial, if the scapular rotation or stability needs to be addressed or advanced through core stability.\nBody assessment or screening can be used to examine any deficits, which affect the shoulder function. Exercises to correct the dysfunctions related to other body parts can be added individually.\nCan you give some clinical tips on how to use an exercise template?\nYou can use the template by assessing and selecting correct exercises for the client, modifying the dosage and removing other exercises. Try the exercise with the client as if you were trying on new shoes. How does it fit, how does it feel? It doesn’t need to feel the most comfortable at first, and the use can be progressed gradually and paced for shorter periods. Some exercises may not fit at all or can be postponed. At the beginning of reloading, one exercise set per day, every three days may be enough. At the next follow-ups you can use the template again, remove the easier exercises and progress with advanced ones.\nThe load, repetitions, sets and recovery time should be progressed to meet the exercise physiology and strength training principles. For example, an eccentric shoulder external rotation exercise with a dumbbell may be repeated at the beginning five times, using two sets and repeated 3-5 times per day in order to respect the tissue recovery and to strengthen the tendons. Functional and integrated movements are also recommended to be added based on the client’s needs.\nThere is strong evidence that the psychosocial factors play a big role in shoulder, knee, neck and back pain. Adding an individual note to the template can be valuable to encourage relaxed use of the body and to address useful thought patterns and proper life style to help the recovery.\nA new template available in Physiotools\nHilkka has created a new Physiotools template Rotator Cuff Strengthening (Exercise Progression) that is available for all Physiotools users. To view the template, go to the Physiotools online Exercises page, select Templates and type the template name in the Free text search.\nHilkka Virtapohja is a Finnish physiotherapist and has studied exercise physiology, anatomy, sport coaching and sports medicine at Jyväskylä and Kuopio Universities. She has over 30 years of experience helping people to recover from shoulder pain and surgery using therapeutic exercise.\nLewis J et al. Rotator Cuff Tendinopathy: Navigating the Diagnosis- Management Conundrum. J Orthop Sports Phys Ther 2015;45(11):923-937\nThorpe AM et al. Are Psychologic Factors Associated With Shoulder Scores After Rotator Cuff Surgery? Clin Orthop Relat Res 2018;476:2062-2073","Doctors routinely request diagnostic imaging tests to monitor what is happening inside your body. The various imaging tests help the doctor make a clear diagnosis and decide on the best treatment option.\nYour doctor can identify specific medical conditions with the use of images produced by these imaging tests.\nWhat is a CT Scan?\nCT, computerized axial tomography, produces images of the body, including bone, using rotating x-rays. The patient lying on the table is rotated around by the x-ray tube. The x-ray detector is located on the patient’s opposite side. This detector picks up the beam that passes through the patient.\nThe best uses for a CT scan (also called a CAT scan) are evaluating bone damage, identifying lung and chest conditions, and finding cancer.\n- Bone structure imaging using CT is effective.\n- Some people cannot have an MRI but can have a CT scan if they have specific surgical clips, cardiac monitors, metallic fragments, or pacemakers.\n- Compared to MRI, the total testing time is less with CT.\n- For claustrophobic people, CT may be more comfortable.\nWhat is an MRI?\nA strong magnet and pulsed radio waves are used in MRI (Radio Frequency or RF). Any axis of the body is used to reconstruct the acquired data into a two-dimensional image.\nSince bone is devoid of water, it does not produce any picture data. As a result, the photos have a black area. Therefore, the imaging of soft tissue is best suited for MRI scanners.\nDoctors can use MRI technology to see soft tissues like muscles, ligaments, tendons, bones, and organs. The spinal cord and nerves can also be seen.\nMRI is also used:\n- To identify sports injuries such as strained or sprained muscles, torn anterior cruciate ligaments, and ruptured Achilles tendons\n- Discover malignant and benign tumors\n- Recognize issues with the circulatory, digestive, circulatory, and respiratory systems.\n- Check for any abnormalities in the brain, such as aneurysms and tumors\n- Identify bone and cartilage diseases\n- See how much your joints are inflamed.\nThe noninvasive diagnostic imaging process known as computed tomography (CT scan or CAT scan) creates horizontal or axial images of the body (commonly referred to as slices) using a mix of X-rays and computer technology. The CT scan is a noninvasive, pain-free, and relatively safe process that doesn’t require recovery time.\nThe noninvasive medical imaging procedure known as magnetic resonance imaging, or MRI, creates precise images of every internal bodily structure, including the organs, bones, muscles, and blood arteries. MRI scanners create images of the body by using a powerful magnet and radio waves.\nSoft tissue, bone, and blood vessel details can all be seen in great detail in CT scans. MRI scans are better and more accurate at visualizing the ligaments, soft tissue, or organs.\nSoft tissue injury, ligament damage, and herniated discs may be simpler to detect as issues after an MRI scan. Medical professionals can use a CT scan to obtain images of organs, fractured heads, or physical components.\nIn a CT scan and an MRI scan, the patient is on a bed that gently rotates across the gantry as a narrow beam of x-rays is shot into the body by the x-ray tube in case of a CT scan. For MRI, the magnetic field momentarily realigns your body’s water molecules. Then, radio waves induce these aligned atoms to emit weak signals to make cross-sectional MRI pictures.\nYou must lie still and motionless throughout the scan. If you move, the MRI or CT scan images might need to be clarified. If you are claustrophobic (afraid of enclosed places), have trouble staying still, or suffer chronic pain, your primary care doctor might prescribe you a light sedative.\nAre CT and MRI contrast the same?\nMRI and CT use the same contrast materials, except iodine, which is only present in CT.\nWhat are the differences between MRI and FMRI?\nMRI and fMRI scan both employ the same fundamental atomic physics concepts. However, MRI scans show anatomical image structure, and fMRI scans show metabolic function. As a result, MRI scan results resemble three-dimensional photographs of anatomical structures.\nWhat can an MRI show that a CT scan cannot?\nA CT scan cannot detect some disorders, which is where MRI excels. Some tumors, including liver cancers, uterine cancers, and prostate cancers, are virtually undetectable or extremely difficult to find on a CT scan. An MRI is also better at showing bone and brain metastases.\nWhich is safer, MRI or CT scan?\nA tiny dosage of ionizing radiation is used in CT scans to create the images. However, an MRI scan doesn’t operate in this manner. Instead of ionizing radiation, it creates images using strong magnets and radio waves. So, in contrast to a CT scan or x-ray, you are not exposed to radiation when you undergo an MRI scan although it does take longer.\nWhy would a doctor order a CT scan instead of an MRI?\nThe doctor might order a CT scan instead of an MRI if a patient cannot have an MRI. Due to the strong magnet inside the machine, those with metal implants, pacemakers, or other implanted devices shouldn’t undergo an MRI.\nIs MRI more expensive than CT?\nMRIs are substantially more expensive than CT scans and other imaging procedures due to the higher equipment expenses. Additionally, reading these intricate images by radiologists requires additional time.\nWhy do they inject dye for an MRI?\nDye is injected because MRI scans sometimes require it as a contrast agent. Certain tissues and blood arteries become more distinct and detailed as a result.\nCan you do CT and MRI at the same time?\nCT and MRI data are spatially and temporally registered in a perfect CT-MRI scanner. Therefore, CT and MRI scans that were collected separately could be combined to mimic a simultaneous acquisition."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:04bba987-a2cc-4d55-a79c-48b90eff341b>","<urn:uuid:6d8ca417-2ebb-431c-9184-233b2a5424fd>"],"error":null}
{"question":"What role do statistical variations play in radiation measurements, and how do environmental factors affect coral bleaching patterns?","answer":"Radiation measurements are inherently stochastic, following a Poisson distribution that can be approximated by a Gaussian distribution. The standard deviation depends on the square root of the mean count, with factors like random decay and detector interaction probability affecting measurements. Similarly, coral bleaching patterns are influenced by multiple environmental variables - while extended heat spikes are the primary cause of normal white bleaching, nutrients from fertilizer run-off can also trigger colorful bleaching events. This shows how both phenomena are affected by multiple probabilistic or environmental factors that influence their measurement and occurrence patterns.","context":["Measurement of Ionizing Radiation\nWith respect to the actual monitoring purpose the relevant sources of ionizing radiation are substances which are subject to spontaneous decay. This is the case with isotopes with instable atomic nuclei as opposed to their stable variants. Spontaneously their nuclei split into smaller nuclei (mostly unstable as well). The decay goes along with the emission of particles—Helium nuclei (alpha radiation) or electrons (beta radiation)—and/or energy quanta (gamma radiation).\nIonizing radiation can be detected by utilizing its ionizing effects. Best known, and widely applied, is the Geiger-Müller counter tube: A hermetically closed glass cylinder is filled with a noble gas (Argon or Xenon). The inner surface of the cylinder is coated with an electrically conductive layer. A thin metallic wire is mounted, isolated from the coating, in the middle axis of the cylinder. The wire and the conducting coating will be used as electrodes. An atom of the gas filling, that will be hit by a particle or quantum irradiated by radioactive materials in the ambience, will be ionized, i.e. the gas atom will be split into a positive ion and an electron. Let us assume that we apply a high voltage of e.g. 500V DC between the electrodes. Due to the electrical field between the electrodes the electrons will move towards the positive electrode while the (positive) ions will move towards the negative electrode where they will recombine with electrons. For every split and recombined atom one electron has to pass the outside circuit thus generating an elementary current impulse.\nOther detection mechanisms utilize ionizing effects in the semiconductor layers of PIN diodes, or they make use of scintillators. In a scintillator crystal high-energy quanta are converted into avalanches of photons which are captured by a photo detector (e.g. a photo diode).\nThe frequency of events which cause electrical impulses can be taken as a measure for the intensity rate of radiation emitted in the abbience of the detector. Hence the task is to count, within an appropriate time interval, the impulses generated by the detector (which explains the term “counter tube”). The choice of the time interval depends on the purpose of the measuring task since the generation of impulses is subject to a stochastic process, i.e. a case of likelihood.\nRadiation levels are stochastic quantities\nThe likelihood of the event of an eventual impulse depends on several processes. The likelihood chain starts with the random decay of an atom of the radioactive isotope. The released particle or quantum resp will pass the detector space only accidentally since it could be ‘shot’ into any spacial direction. Finally, not every particle or quantum passing the detector space will cause an interaction which results in an electrical impulse. All those probabilities are to be multiplied thus leading to a random number of events counted within a given time interval even at a constant exposure rate.\nThe distribution of counter results is described by means of a probability-density function. The Poisson distribution underlying this type of random process can be approximated by the well-known Gaussian distribution. Its maximum represents the mean value of a large number of counter results at constant exposure rate. The segments of the area below the curve represent the number of results falling into the respective segment.\nProbability-density function (source: M.W.Toews, Quelle: Wikipedia)\nA Gaussian distribution is characterized by its maximum (aka ‘expectation’) and the standard deviation σ represented, in above figure, by the (equal) width of segments. Due to the underlying Poisson distribution the standard deviation is given, in the actual case, by the square root of the expectation. Let us give a numerical example:\n- Let the mean number of events counted within an interval of 15 minutes be 100 (which approximates the conditions implemented by the TDRM sensor stations).\n- The standard deviation σ then would be 10 counts or 10% of the mean value.\n- Hence two thirds of results would lie within the interval from 90 to 110, another third would lie outside of the ±10% range.\n- A narrowing of the standard deviation range to ±1% would require an extension of the counter interval by a factor of 100 (!), thus 25 hours.\n- Then again the standard deviation range would shrink with increasing intensity of radiation. An extraordinary increase by a factor of 10 would result in a decrease of the standard deviation range to ±3%.\nThe numerical example illustrates the inevitable trade-off between sensitivity and response time associated with the measurement of stochastic phenomena.\nWe decided to set the counter interval to 15 minutes in favor of a fast response. With respect to the purpose of the monitoring network we take it for most important to indicate irregular situations without delay rather than to resolve it to the finest degree. Every minute a TDRM sensor stations delivers the result of events counted within a ‘sliding window’ of the past 15 minutes. Hence trends can already be estimated after a couple of minutes.\nTypical time diagram of measurements: variations of the radiation level are of stochastic origin rather than fluctuations of the dose rate of the ambient atmosphere","Cecilia D’Angelo, a molecular coral biology lecturer at the University of Southampton. Jörg Wiedenmann, Elena Bollati & Cecilia D’Angelo/University of Southampton, Palawan colourful bleaching image by Ryan Goehrung/University of Washington\nBleaching events used to be few and far between, but they now occur nearly every year. After the coral is exposed, it often breaks down and dies, altering the ecosystem for the diverse array of life that relies on it. Corals stand little chance of bouncing back from these events — but a new study suggests they have an unusual survival method: taking on a vibrant neon color.\nWhen bleaching events occur, extended heat spikes cause corals to turn a ghostly white, often leading to their death. Even slight increases in annual ocean temperatures can wreak havoc on this relationship, expelling the algae from the coral’s tissue and exposing its white skeleton. These corals can still undergo some of their normal functions for a short period of time as they hope their algae come back — whereas drastic changes in ocean temperature almost always lead to coral death. Reports of colorful bleaching during the most recent mass bleaching event in the Great Barrier Reef in March and April gave scientists hope that patches of the system have a chance to recover. “Bleaching is not always a death sentence for corals, the coral animal can still be alive,” said Dr. They found that colorful bleaching events occur when corals produce “what is effectively a sunscreen layer” on their surface to protect against harmful rays and create a glowing display that researchers believe encourages algae to return. The Ocean Agency describes the process as a “chilling, beautiful and heartbreaking” final cry for help as the coral attempts to grab the algae’s attention. “Our research shows colorful bleaching involves a self-regulating mechanism, a so-called optical feedback loop, which involves both partners of the symbiosis,” lead researcher Professor Jörg Wiedenmann of the University of Southampton said in a press release. Climate Change\nDying coral reefs turn vibrant neon in apparent survival effort\nScientists say climate change is turning coastal Antarctica green\nStudy: Climate change makes a Dust Bowl heat wave more likely\nAir pollution is already spiking in China with virus lockdown lifted\nIndia’s carbon emissions fall for the first time in four decades\nMore in Climate Change\nResearchers at the University of Southampton’s Coral Reef Laboratory studied 15 colorful bleaching events worldwide between 2010 and 2019 — including one in the Great Barrier Reef, the world’s largest coral reef system — and recreated those ocean temperatures in a lab. But “colorful bleaching” has the opposite effect: the dying corals gain more pigment, and glow in shades of bright pink, purple and orange. Scientists first spotted the mysterious neon coral a decade ago, but they had been unable to figure out why it occurred. As the recovering algal population starts taking up the light for their photosynthesis again, the light levels inside the coral will drop and the coral cells will lower the production of the colorful pigments to their normal level.”\nColorful bleaching Acropora corals in the Phillipines. Dying coral reefs turn vibrant neon colors in apparent last-ditch effort to survive\nBy Sophie Lewis\nMay 22, 2020 / 4:48 PM\n/ CBS News\nScientists use mini-satellites to save coral reefs\nFor years, coral reefs around the world have been devastated by mass bleaching events as the oceans continue to warm due to climate change. “If the stress event is mild enough, corals can re-establish the symbiosis with their algal partner.”\nThe internal changes that allow colorful bleaching to occur. In 2017 alone, nearly half the corals on the Great Barrier Reef died — and experts say we are running out of time to save them. “Unfortunately, recent episodes of global bleaching caused by unusually warm water have resulted in high coral mortality, leaving the world’s coral reefs struggling for survival,” D’Angelo said. Scientists emphasized that while colorful bleaching is a good sign, only a significant reduction of greenhouse gases globally — in addition to improvement in local water quality — can save coral reefs beyond this century.\n“Now that we know that nutrient levels can affect colorful bleaching too, we can more easily pinpoint cases where heat stress might have been aggravated by poor water quality,” researchers said. Ryan Goehrung, University of Washington\nCoral reefs support more species per unit area than any other marine environment, including about 4,000 species of fish, 800 species of hard corals and potentially millions of other undiscovered species, according to the National Oceanic and Atmospheric Administration. Colorful bleaching Acropora corals in New Caledonia.\nRichard Vever/The Ocean Agency/XL Catlin\nCoral animals symbiotically coexist with tiny algae, providing them with shelter, nutrients and carbon dioxide in exchange for their photosynthetic powers. Together, these actions can secure a future for coral reefs.”\nFirst published on May 22, 2020 / 4:48 PM “The resulting sunscreen layer will subsequently promote the return of the symbionts. This study, published Thursday in the journal Current Biology, suggests the corals change color as a last-ditch effort to survive. “This can be managed locally, whereas the ocean heat waves caused by climate change will need global leadership. Disruptions to coral reefs have far-reaching implications for ocean ecosystems.\nIt’s not just warming oceans that cause colorful bleaching. Researchers say changes in nutrient levels within coral reefs due to fertilizer run-off from farms also lead to bleaching events — a problem that can be fixed at the local level. Experts believe only coral that has faced mild or brief disturbances, rather than extreme mass bleaching events, can attempt to save itself using this process."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:8339801b-59a3-43d2-82ea-5fda0e1ec9ee>","<urn:uuid:369f1095-9589-4d83-9007-35418b336092>"],"error":null}
{"question":"How do the recording capabilities of Neumann's KU-80 from 1972 compare to Sennheiser's modern AMBEO Smart Headset?","answer":"The Neumann KU-80 was the first commercial head-based binaural recording system using a dummy head design, introduced at the 1972 International Radio and Television Exhibition in Berlin. In contrast, Sennheiser's AMBEO Smart Headset ($299.99) uses a simpler approach with electret omnidirectional microphones in each ear that use the listener's actual head for recording, rather than a dummy head. While both enable binaural audio recording, the AMBEO Smart Headset is specifically designed for iOS devices and includes modern features like noise cancellation and transparent hearing capabilities through a companion app.","context":["The History of Binaural Audio, Part II: The Resurgence, 1940-2000\nThe technology behind binaural audio (also known as 3d Audio) is more than a century-old. So why are most people only now beginning to hear about this superior alternative to stereo sound? Mostly because in order to experience binaural audio, you need to be wearing headphones, which used to be a lot less common than they are today. A binaural audio recording captures sound as you actually hear it by employing two microphones spaced to approximate the distance between your ears. With headphones on, binaural audio produces the incredibly immersive sensation of being in the same exact place where the recording was made.\nIn The History of Binaural Audio, Part 1, we traced the story of binaural audio from the late-19th Century, when a telephone earpiece was still required for listening to electrical audio signals, through the 1930s, which marked the arrival of stereophonic sound and the first commercial loudspeakers. Stereo eventually dictated whole new ways of making art, vastly expanding the range of sounds that humans find pleasing to the ear. Fast forward to the present day, and binaural audio is poised to make a similarly seismic cultural impact. Now, every smartphone comes with a pair of earbuds. We crave immersive experiences. And creators in every sector (VR, music, podcasts, gaming…) are tapping the limitless potential of binaural.\nHere, in the second of our three-part series, we’re throwing a spotlight on the films, albums, radio dramas, audio walks, and amusement park rides that paved the way for the binaural audio creators of today. Check back next week for Binaural Audio: The Sound of the 21st Century.\n1940s – 1960s\nIn 1940, Disney kicks off a sound revolution with Fantasia — the first commercial film released in stereo. “Fantasound” requires a multi-speaker system for playback; problem is, movie theaters aren’t equipped for that yet. “We know…that music emerging from one speaker behind the screen sounds thin, tinkly and strainy,” Walt Disney says. “We wanted to reproduce such beautiful masterpieces…so that audiences would feel as though they were standing at the podium with [conductor Leopold] Stokowski.” Fantasia gets shown at thirteen custom-renovated venues across the U.S.. Installation of the equipment proves costly, making a wide release impossible. But, over the next three decades, stereo systems for both commercial and in-home use gradually become more affordable. Stereo overtakes mono as the new standard, sidelining further experiments with binaural audio — at least for now.\nGerman company Neumann unveils the KU-80 — the first commercial head-based binaural recording system — at the 1972 International Radio and Television Exhibition in Berlin. Similar dummy heads developed by Sony, JVC, and Sennheiser soon follow, providing creators with the tool they need to invent new ways of hearing. In 1978, Lou Reed employs a dummy head designed by German sound engineer Manfred Schunk to make Street Hassle — the first commercial pop album recorded in binaural audio — then follows that up with two more binaural albums, 1978’s Live: Take No Prisoners and 1979’s The Bells. (Read our roundup of The Best Albums Recorded in Binaural Audio.)\nMeanwhile, BBC Radio 4 starts broadcasting binaural radio dramas, including a series centered on Sherlock Holmes and a 28-minute radio play that doesn’t contain a single word of dialogue — only binaurally recorded sounds — called The Revenge. Now, you might think that a radio play in which no one actually talks would make for a slow listening experience. But The Revenge hooks you from the get-go: a police siren blares and a chase ensues over land and through water. It feels like you’re the one on the run. (If you want to create a similar effect with the Verse, read How to Start a Binaural Podcast.)\nIn 1984, a non-profit audio production company in Upstate New York called the ZBS Foundation (“Zero Bull Shit”) releases a 72-minute binaural dramatization of the Stephen King novella The Mist. It takes place in a small New England town where a group of locals trapped in a supermarket have to beat back a siege of otherworldly creatures. Stereophile magazine raves: “The sense of depth and space is startling; the ambient feel of the environment combined with the sense of movement and positioning of the dialogue and highly effective sound effects makes for a memorable experience…Listen in a darkened room for maximum impact.” You can buy it on CD, download it on Audible, or: you can check out this fascinating podcast about the making of The Mist, which features excerpts:\nArtist Janet Cardiff, together with her husband George Bures Miller, starts creating her critically acclaimed binaural audio walks. Cardiff guides the listener (“Turn left here…Go through that gateway…”) through site-specific soundscapes of her own design, which simultaneously alter and deepen the listener’s perception of the physical landscape. The result is a distinct type of audio-centric virtual reality, which listeners have experienced everywhere from London’s Whitechapel Library to New York City’s Central Park. More recently, Cardiff has started making binaural video walks, including one that guides you through the old train station in Kassell, Germany.\nIn 1994, a binaural audio film called Bad Boy Bubby wins the Grand Special Jury Prize at the Venice Film Festival and four AFI Awards (Australia’s equivalent of the Oscars). Bad Boy Bubby is a beautifully twisted black comedy about a 35-year-old man who leaves his abusive mother’s house for the first time in his life after being told since birth that the air outside is poisonous. To capture the sensation of experiencing the outside world for the first time, director Rolf De Heer sews binaural microphones sinto the wig of lead actor Nicholas Hope, one above each ear.\nThat same year, in the Tomorrowland section of the Magic Kingdom, Disney uses binaural audio to terrifying effect in its newest blockbuster ride, ExtraTERRORestrial Alien Encounter. Audience members are arrayed in a ring, strapped into seats with a speaker near each ear. When a power outage casts the room in complete darkness, a maintenance worker arrives — only to be ripped to pieces by a roaring carnivorous alien that proceeds to swoop around the room, terrorizing every helplessly immobile audience member with its close, spine-chilling breath.\nSoon after, a Canadian Sound Studio called QSoundLabs puts out what has since become arguably the most popular binaural audio recording ever produced (with 25 million YouTube views and counting), “Virtual Barber Shop.” Sure, it may seem hokey next to binaural creations by Reed and Cardiff, but it’s hard to argue with “Virtual Barber Shop”‘s elegant simplicity. Your barber, Luigi, moves around your head, clipping away. When the clippers are louder in your left ear, you know he’s on your left side, and vice versa. It’s an auditory illusion that demonstrates just how dependent we are on being able to locate sounds in space. Stereo sound isn’t capable of producing that life-like sensation. Binaural audio does it perfectly.","Sennheiser, extending the AMBEO brand to the low-end of the market, has debuted its AMBEO Smart Headset, a set of binaural ear plugs that enable both the listening and recording of 3D sound from Apple iOS devices.\nBefore we go any further, let’s define that AMBEO is a marketing term Sennheiser uses to brand any and all of its products that allow 3D recording. This includes binaural, Ambiophonics, 3D mixing software and any other spacial audio technology that the company sells. Also, binaural audio and the other immersive technologies are not new — not by a long shot.\nIn fact, the history of binaural recording goes back to 1881. The first binaural unit, the Théâtrophone, was an array of carbon telephone microphones installed along the front edge of the Paris Opera House. The signal was sent to subscribers through the telephone system, and required that they wear a special head set, which had a tiny speaker for each ear.\nThen, in 1933, AT&T made headlines at the Chicago World Fair by demonstrating the first head-based binaural system. They created a mechanical man with microphones for ears, nicknamed “Oscar,” which was set up in a glass room surrounded by listeners with headphones. The public was astounded to hear what Oscar was hearing in real-time, and the exhibit was a highlight of the fair.\nAt the 1972 International Radio and Television Exhibition in Berlin, Neumann, introduced the KU-80, the first generation of dummy head microphones. It was designed to test environmental acoustics, but listeners quickly discovered the potential it had for recording realistic radio drama. Neumann’s design for the microphone improved in later generations including the KU-81, and now the KU-100.\nToday, Neumann’s KU-100 dummy head still stands at the top of the Sennheiser AMBEO line (since Sennheiser now owns Neumann). It is capable of making breath-taking recordings and has been used for over 40 years on a host of broadcast and non-broadcast recordings.\nIn 1978, Lou Reed released the first commercially produced binaural pop record, Street Hassle, a combination of live and studio recordings. Today, companies like Anderson Audio in New York have moved much further ahead with immersive technology. The company’s founders, Jim Anderson and Ulrike Schwarz, produce 3D recordings that have won more than ten Grammy awards.\nObviously, Sennheiser has a deep background in binaural audio. The sound information arriving at the left and right ears causes inter-aural time level differences. These small variations allow the brain and auditory system to calculate the direction and distance of the sound sources from the listener.\nThe AMBEO Smart Headset ($299.99) is a basic binaural set that uses an electret omnidirectional microphone in each ear to pickup sound using the real human head, rather than the dummy head with mics in its ears used by the $8,000 KU-100. The Smart Headset uses Apogee’s proprietary soft limit and mic preamp plus precisely tuned A/D and D/A conversion in an inline switch.\nThe user simply plugs the AMBEO Smart Headset into the Lightning connector of the iPhone. Each earpiece is hooked around the ears. With these, one can listen to music or videos as with any other Lightning earphones.\nWhat’s different are the microphones on the Smart Headset face outward. Using a free, downloadable iPhone app, these microphones also enable the transparent hearing and noise cancellation that allow users to blend the sounds of surroundings with music or to tune them out.\nWhen used with a video recording application, the Smart Headset adds binaural sound to the videos created with Apple’s iPhone. Users can listen back to these recordings through any pair of headphones to experience 3D sound with depth and relive the feeling of the original experience.\nThe Headset captures stereo audio, using the iPhone Camera App, FliMic Pro 48k and 96k, the Apogee MetaRecorder and iMovie. It works best to hear playback on headphones, but Sennheiser said Headset will provide stereo sound to standard loudspeakers.\nThe earpieces of the headset has a feature called situational awareness. Using the rocker switch, users can either make the outside noise fade into a personal program, which allows users to stay safe when on the street, or one can deploy active noise cancellation, blocking out the acoustic environment. Situational awareness also works when recording or making a call and can be turned off.\nThe iOS app can access advanced features and customization options. Control your input level for 3D recording, toggle between EQ presets or configure your transparent hearing settings. Customize the smart slider for an array of functions, from quickly launching your favorite audio or video recording app to toggling the interact mode to communicate with those around you.\nVoice prompts are enabled by default for an audible confirmation of any changes made via the controls on the headset. However this can be disabled from the app if desired. The app is available for free from the App Store, but is not required for normal operation of the headset.\nWe found the Sennheiser AMBEO Smart Headset works as advertised. However, it is a specialized tool only for those who want to experiment with binaural sound on an iOS device. Because of the Lightning connector, it is limited to devices users can record on. There are cheaper binaural solutions that operate more universally.\nBefore purchasing the headset, check Sennheiser’s info page here. Note the apps it will work with and what it is NOT compatible with.\nAlso our review sample was provided without an instruction manual. When we asked for one, we were told all the needed information is on the Sennheiser website. Most of the information is there, but it takes a lot of extra work to find everything. One would think for a $300 product, a manual would be included. Whether one actually exists, we don’t know.\nWith those limitations, the headset is well made, sounds good and is a good choice if you need binaural recording on an iOS device.\nYou might also like...\nIn their latest hyper-realistic VR weather warning, The Weather Channel helps viewers better understand the potential dangers created by ice storms.\nLike many professional football players themselves, CBS Sports Lead television director Mike Arnold tries to treat the Super Bowl as he would a regular season game, calling the same shots and camera angles—albeit with many more cameras at his d…\nThe Intel True View allows a production team to recreate selected clips in 3D from any vantage point in a stadium or even from a player’s perspective.\nThis year’s Super Bowl LIII telecast on CBS will be produced and broadcast into millions of living rooms by employing the usual plethora of traditional live production equipment, along with a few wiz bang additions like 4K UHD and a…\nLast Fall, “Orbital Redux” broke new ground for streaming entertainment as a live, scripted multi-episode sci-fi drama in which the audience determined the outcome of the action."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c193444c-ca0e-4bc9-9dc2-594f680aafca>","<urn:uuid:a3a1bef5-b2f5-49b2-ab0a-3c9ec0e69e45>"],"error":null}
{"question":"What are the key differences between how bacteria in human milk and dental plaque form their respective microbial communities?","answer":"Human milk and dental plaque form microbial communities in distinctly different ways. Human milk typically contains nine common operational taxonomic units (including Streptococcus, Staphylococcus, and Pseudomonas) that account for over 50% of total bacteria, with bacteria originating primarily from the maternal gut via dendritic cells. In contrast, dental plaque forms in stages, beginning with early colonizers (primarily Gram-positive bacteria like Streptococcus sanguis) adhering to the tooth's pellicle, followed by late colonizers (primarily Gram-negative anaerobes) appearing 2-4 days later. The plaque bacteria use specific ionic and hydrophobic interactions as well as lectin-like structures to bind to each other, eventually forming a complex biofilm containing 300-400 bacterial species.","context":["The human microbiome project was a major undertaking by the National Institutes of Health, with a fairly simple mission: understand the bacterial communities living in and on the human body, and the potential impact these communities may have on health. Hundreds of individuals donated everything from feces to nasal secretions. However, one key system was ignored - human milk. That’s right – the microbiome of human milk was not studied.\nProbably some of this had to do with a long standing myth that human milk was sterile. Why study something without bacteria, right? But, as we have quickly learned – human milk is far from sterile. The average baby consuming 800 mL/27 ounces of human milk will received between 100,000 and 10,000,000 million bacteria from human milk per day (Fernandez et al., 2013).\n|Figure 1: The Human Microbiome Project is not interested in milk. I fixed their image to better reflect this.|\nFortunately, research into the human milk microbiome has continued despite this oversight by the Human Microbiome Project. It appears that nine “operational taxonomic units” (generally closely related species based on DNA analysis of the bacteria) are extremely common in most mothers studied to date: Streptococcus, Corynebacteria, Bradyrhizobiaceae, Staphylococcus, Serratia, Ralstonia, Propionibacterium, Pseudomonas, and Sphingomonas. These nine groups typically account for more than 50% of total bacteria. Bififobacterium and Lactobacillus are also common, but less universal (Fernandez et al., 2013).\nThe microbiota of milk appears to be quite stable (Fernandez et al., 2013), although a few factors appear to shape the composition. First, mothers with higher BMIs (in the obese range) produce colostrum with more Lactobacillus, and mature milk with more Staphylococcus and less Bifidobacterium (Cabera-Rubio et al., 2012). Cabera-Rubio and colleagues (2012) also found that greater pregnancy weight gain predicted more Staphylococcus in the milk in a small study of 18 mothers, half obese and half of normal weight.\nBut here is the really neat part – guess what else altered the milk microbiota? Type of delivery. Mothers who had caesarian sections had a different milk microbiota than mothers who had a vaginal delivery. And the variation continued – mothers undergoing emergency caesarians after laboring had milk microbiotas closer to those of women who delivered vaginally than women with elective caesarians.\nWhere do the bacteria come from? Initially, it was thought that the milk microbiome was really just contamination from the skin microbiome. However, this is WRONG, WRONG, WRONG. While the milk microbiome does contain some of the same families of bacteria as skin, multi-site sampling of mammary skin and milk revealed that these are not the same species and/or genera. Instead, it appears that the microbiome of milk comes from several places, including the maternal gut microflora. Current evidence supports dendritic cells as the likely transfer mechanism. These cells, along with some macrophages, can open the tight junctions between cells forming the gut barrier and take in living bacteria. These cells can then maintain the live bacteria for several days in mesenteric lymph nodes scattered throughout the body (Fernandez et al., 2013). Dendritic cells are also pretty picky about what they take up – dead bacteria or latex beads will not activate immature dendritic cells for bacteria uptake, while commensal species, like Lactobacillus, show high levels of binding (Rescigno et al., 2001).\n|Figure 2: Dendritic cell (shown in blue). Image from http://www.cell.com/pictureshow/immunology|\nThis allows for oral manipulation of the milk microbiome – mothers given supplemental Lactobacillus from three strands, L. gasseri, L. fermentum, L. salivarius, showed transfer of these strands to the milk (Jimenez et al., 2008).\nThis lead to the logical question – could these strands be used to treat mastitis? Arroyo et al., (2010) randomized 352 women with mastitis to three groups – one dosed with L. fermentum, one dosed with L. salivarius, and one given standard antibiotic treatment (4 different drugs were used). Bacterial counts for milk were obtained for all mothers on Day 0 – that is before treatment started. All mothers had bacterial counts of 4.35-4.47 log10 CFU (colony forming units) – a little less than double the recommended bacterial counts for milk of 2.5 log10 CFUs. Mothers received 21 days of treatment, and milk bacterial counts were repeated on day 21. Women who received L. fermentum had mean bacterial counts of 2.61 log10 CFUs; L. salivarius had bacterial counts 2.33 log10 CFUs with clinical relief of mastitis, and all reported reductions in reported breast pain. Mothers who received antibiotics did not fare as well. Mean bacterial count for antibiotic receiving mothers was 3.28 log10 CFUs and pain scores were much higher. Three months later, only 8.8% of mothers receiving either L. fermentum or L. salivarius had experienced recurrent mastitis, while 30.1% of mothers receiving antibiotics had. All differences between antibiotic and probiotic groups were significantly different – the kind of significant difference that makes researchers do their happy dance.\nFigure 3: This is how I picture the researchers after making this discovery - just substitute a computer for the piano. Gif by PEANUTS.\nSo the milk microbiome appears to be protecting mothers – but there is also good evidence that it is protecting infants. Little is known about the salivary microbiome of infants, but based on preliminary evidence, it appears to, not surprisingly, have some overlap with the milk microbiome (Nasidze et al., 2009). The milk microbiome also appears to contribute to the microbiome of the infant GI tract, as well as the development of immune function in the infant (Fernandez et al., 2013). Infants supplemented with Lactobacillus fermentum (yes, the same as used for the treatment of mastitis) showed significant reductions in diarrheal and respiratory infections in early infancy compared to control infants (Maldonado et al., 2012). Many of the bacteria in the milk microbiome are protecting both the mother and the infant from infection, and may even be involved in the development of immune tolerance.\nMilk remains amazing – even the bacteria in milk!\nArroyo R, Martín V, Maldonado A, Jiménez E, Fernández L, Rodríguez JM. (2010) Treatment of infectious mastitis during lactation: antibiotics versus oral administration of lactobacilli isolated from breast milk. Clinical Infectious Diseases 50:1551–8.\nCabrera-Rubio R1, Collado MC, Laitinen K, Salminen S, Isolauri E, Mira A. (2012) The human milk microbiome changes over lactation and is shaped by maternal weight and mode of delivery. Am J Clin Nutr. 96(3):544-51.\nFernández L1, Langa S, Martín V, Maldonado A, Jiménez E, Martín R, Rodríguez JM. (2013) The human milk microbiota: origin and potential roles in health and disease. Pharmacol Research 69(1):1-10.\nJiménez E, Fernández L, Maldonado A, Martín R, Olivares M, Xaus J, et al. (2008) Oral administration of lactobacilli strains isolated from breast milk as an alternative for the treatment of infectious mastitis during lactation. Applied and Environment Microbiology 74:4650–5.\nMaldonado J, Ca˜nabate F, Sempere L, Vela F, Sánchez AR, Narbona E, et al. (2012) Human milk probiotic Lactobacillus fermentum CECT5716 reduces the incidence of gastrointestinal and upper respiratory tract infections in infants. Journal of Pediatric Gastroenterology and Nutrition 54:55–61.\nMartín R, Olivares M, Marín ML, Fernández L, Xaus J, Rodríguez JM. (2005) Probiotic potential of 3 lactobacilli strains isolated from breast milk. Journal of Human Lactation 21:8–17.\nNasidze I, Li J, Quinque D, Tang K, Stoneking M. (2009) Global diversity in the human salivary microbiome. Genome Research 19:636–43.\nRescigno M, Urbano M, Valzasina B, Francolín M, Rotta G, Bonasio R, et al. (2001) Dendritic cells express tight junction proteins and penetrate gut epithelial monolayers to sample bacteria. Nature Immunology 2:361–7.","FIGURE 41–1. Dental plaque biofilm. The stages of formation of the bacterial biofilm called dental plaque are shown. Early colonizers bind to the enamel pellicle and late colonizers bind to the other bacteria. (Reproduced with permission from Willey JM: Prescott, Harley, & Klein’s Microbiology, 7th edition. McGraw-Hill, 2008.)\nDental plaque is a bacterial biofilm\nPlaque forms in stages\nThe initial supragingival plaque primarily involves Gram-positive bacteria using specific ionic and hydrophobic interactions as well as lectin-like (carbohydrate binding) surface structures to adhere to the pellicle and to each other. The prototype early colonizer is Streptococcus sanguis, but other streptococci (S mutans, S mitis, S salivarius, S oralis, Sgordonii), lactobacilli, and Actinomyces species are usually present. If the early colonizers are undisturbed, the late colonizers appear in the biofilm in as little as 2 to 4 days. These are primarily Gram-negative anaerobes including anaerobic spirochetes. These include Fusobacterium, Porphyromonas, Prevotella, Veillonella, Treponema denticola, and more Actinomyces species. These bacteria use similar mechanisms to bind to the early colonizers and to each other. This sets up a highly complex biofilm in which coaggregation involves structures that the bacteria brought with them (lectins), quorum sensing, and new metabolic activity. An example of the latter is the formation of extracellular glucan polymers, which act like a cement binding the plaque biofilm together. The biofilm also fastens nutrient and growth regulatory relationships between its members and provides a shield from the outside. In all, there are thought to be 300 to 400 bacterial species present in mature dental plaque. The structure of the involved bacteria is shown in Figure 41-1 and its gross and microscopic appearance in Figure 41-2.\nFIGURE 41–2. Dental plaque. A. Disclosing tablets containing vegetable dye stain heavy plaque accumulation at the junction of the tooth and gingival. (Reproduced with permission from Willey JM: Prescott, Harley, & Klein’s Microbiology, 7th edition. McGraw-Hill, 2008.) B. Scanning electron micrograph of supragingival plaque.\nAttachment of bacteria to dental pellicle begins colonization\nEarly and late colonizers differ\nAdhesion mechanisms create biofilm\nDental plaque would coat the tooth surfaces uniformly but for its physical removal during chewing and other oral activities. Characteristically, plaque remains in the non-self-cleansing areas of the teeth such as pits and fissures, along the margins of the gingiva, and between the teeth. For this reason, the plaque-related diseases—caries, gingivitis, and periodontitis—occur most frequently and most severely at these locations. Subgingival plaque extends below the gum line to the sulcus around the tooth and periodontal pockets, which are pathologic extensions of the sulcus. This plaque has a thin adherent layer attached to the tooth surface and a nonadherent bacterial zone between that and the epithelial cells lining the sulcus. Supragingival plaque lacks such a distinct nonadherent zone. The bacterial composition of subgingival plaque is shifted toward the Gram-negative anaerobic bacteria and spirochetes. In addition to the late colonizers cited above, it may also include members of the Campylobacter, Capnocytophagia, and Eikenella genera.\nPlaque accumulates in non-self-cleansing areas\nSubgingival plaque differs in bacterial composition\nBecause the causative organisms of both dental caries and chronic periodontitis are believed to be in the dental plaque, a prime method for maintaining oral health is regular home care practices for plaque removal. Dental plaque cannot be effectively removed from the teeth solely by chemical or enzymatic means, and the use of antibiotics for prophylactic inhibition of plaque formation cannot be clinically justified, although patients undergoing long-term antibiotic treatment for other medical reasons demonstrate a lower incidence of caries and periodontal disease. Antiseptic substances that bind to tooth surfaces and inhibit plaque formation, such as the bis-biguanides, chlorhexidine, and alexidine, have been shown to be effective in reducing plaque, caries, and gingival inflammation. A commercial preparation containing 0.12% chlorhexidine can be used in controlling dental plaque and associated disease. Toothpaste and mouth rinse additives such as phenolic compounds, essential oils, triclosan, fluorides, herbal extracts, and quaternary ammonium compounds have been shown to have some plaque-reducing ability as well. The use of these substances must be accompanied by proper tooth brushing, flossing, and periodic professional cleaning to ensure effective disease prevention.\nRemoval of plaque prime element of oral hygiene\nChemicals may be used along with brushing and flossing\nDental caries are the result of progressive destruction of the mineralized tissues of the tooth. They are primarily caused by the acid products of glycolytic metabolic activity when the plaque bacteria are fed the right substrate. The basic characteristic of the carious lesion is that it progresses inward from the tooth surface, either the enamel-coated crown, or the cementum of the exposed root surface, involving the dentin and finally the pulp of the tooth (Figures 41–3 and 41–4). From there, infection can extend into the periodontal tissues at the root apex or apices.\nFIGURE 41–3. Cariogenesis. A microscopic view of pellicle and plaque formation, acidification, and destruction of tooth enamel. (Reproduced with permission from Willey JM: Prescott, Harley, & Klein’s Microbiology, 7th edition. McGraw-Hill, 2008.)\nFIGURE 41–4. Hemisected human tooth showing an advanced carious lesion on the right side of the crown and a much smaller lesion on the left side. Note the progression of the lesion through the enamel and dentin, pointing toward the pulp chamber in the center of the tooth.\nCaries produced by plaque bacteria\nMembers of biofilm produce acid\nS mutans is most cariogenic\nThe microbial basis of dental caries has been long established based on work first with Lactobacillus acidophilus and then Streptococcus mutans. Although S mutans is now regarded as the dominant organism for the initiation of caries, multiple members of the plaque biofilm participate in the evolution of the lesions. These include other streptococci (S salivarius, S sanguis, S sobrinus), lactobacilli (L acidophilus, L casei), and actinomycetes (A viscosus and A naeslundii). The acid products produced by the interaction of S mutans with multiple species in the biofilm are the underlying cause of dental caries.\nDietary monosaccharides and disaccharides such as glucose, fructose, sucrose, lactose, and maltose provide an appropriate substrate for bacterial glycolysis and acid production to cause tooth demineralization. A possible edge for S mutans is its ability to metabolize sucrose more efficiently than other oral bacteria. It also has regulatory systems which stimulate the conversion of dietary carbohydrates to acid and intracellular storage polymers. Ingested carbohydrates permeating the dental plaque are absorbed by the bacteria, and are metabolized so rapidly that organic acid products accumulate and cause the pH of the plaque to drop to levels sufficient to react with the hydroxyapatite of the enamel, demineralizing it to soluble calcium and phosphate ions. Production of acid and the decreased pH are maintained until the substrate supply is exhausted. Upon exhaustion of the immediate source S mutans is able to survive long periods of sugar starvation. Obviously, foods with high sugar content, particularly sucrose, which adhere to the teeth and have long oral clearance times are more car-iogenic than less retentive foodstuffs such as sugar-containing liquids. Once the substrate is exhausted, the plaque pH returns slowly to its more neutral pH resting level and some recovery can take place. This sets up a demineralization-remineralization cycle, which depends on carbohydrate refueling from the diet. With repeated snacking between meals, the plaque pH may never return to normal and demineralization dominates.\nDemineralization is by acid production from dietary carbohydrate\nAcid production facilitated by sticky carbohydrates\nDemineralization-remineralization related to snacking\nAn additional factor with sucrose is that it is also used in the synthesis of extracellular polyglycans such as dextrans and levans by transferase enzymes on the bacterial cell surfaces. This polyglycan production by S mutans contributes to aggregation and accumulation of the organism on the tooth surface. Extracellular polyglycan may also increase cario-genicity by serving as an extracellular storage form of substrate. Certain microorganisms synthesize extracellular polyglycan when sucrose is available but then break it down into monosaccharide units to be used for glycolysis when dietary carbohydrate is exhausted. Some oral bacteria also use dietary monosaccharides and disaccharides internally to form glycogen, which is stored intracellularly and used for glycolysis after the dietary substrate has been exhausted; thus, the period of acidogenesis is again prolonged and the carioge-nicity of the microorganism increased. These microorganisms can prolong acidogenesis beyond the oral clearance time of the substrate.\nExtracellular polyglycans from sucrose important in adherence and carbohydrate storage\nAcidogenesis prolonged by intracellular glycogen stores\nThe most common complications of dental caries are extension of the infection into the pulp chamber of the tooth (pulpitis), necrosis of the pulp, and extension of the infection through the root canals into the periapical area of the periodontal ligament. Periapical involvement may take the form of an acute inflammation (periapical abscess), a chronic nonsuppurating inflammation (periapical granuloma), or a chronic suppurating lesion that may drain into the mouth or onto the face via a sinus tract. A cyst may form within the chronic nonsuppurating lesion as a result of inflammatory stimulation of the epithelial rests normally found in the periodontal ligament. If the infectious agent is sufficiently virulent or host resistance is low, the infection may spread into the alveolar bone (osteomyelitis) or the fascial planes of the head and neck (cellulitis). Alternatively, it may ascend along the venous channels to cause septic thrombophlebitis. Because most carious lesions represent a mixed infection by the time cavities have developed, it is not surprising that most oral infections resulting from the extension of carious lesions are mixed and frequently include anaerobic organisms.\nExtension to pulp and periapical locations complicate infections\nSevere complications spread to bone or local fascia\nDental caries is the single greatest cause of tooth loss in the child and young adult. Its onset can occur very soon after the eruption of the teeth. The first carious lesions usually develop in pits or fissures on the chewing surfaces of the deciduous molars and result from the metabolic activity of the dental plaque that forms in these sites. Later in childhood, the incidence of carious lesions on smooth surfaces increases; these lesions are usually found between the teeth. The factors involved in the formation of a carious lesion are (1) a susceptible host or tooth, (2) the proper microflora on the tooth, and (3) a substrate from which the plaque bacteria can produce the organic acids that result in tooth demineralization.\nGreatest cause of tooth loss in children and young adults\nRequire microflora and suitable substrates for organic acid production\nThe newly erupted tooth is most susceptible to the carious process. It gains protection against this disease during the first year or so by a process of posteruptive maturation believed to be attributable to improvement in the quality of surface mineral on the tooth. Saliva provides protection against caries, and patients with dry mouth (xerostomia) suffer from high caries attack rates unless suitable measures are taken. In addition to the mechanical flushing and diluting action of saliva and its buffering capacity, the salivary glands also secrete several antibacterial products. Thus, saliva is known to contain lysozyme, a thiocy-anate-dependent sialoperoxidase, and immunoglobulins, principally those of the secretory IgA class. The individual importance of these antibacterial factors is unknown, but they clearly play some role in determining the ecology of the oral microflora.\nSaliva protects by mechanical flushing and multiple chemical actions\nProper levels of fluoride, either systemically or topically administered, result in dramatic decreases in the incidence of caries (50% to 60% reduction by water fluoridation, 35% to 40% reduction by topical application). In the case of systemic fluoridation, the protective effect is thought to result from the incorporation of fluoride ions in place of hydroxyl ions of the hydroxyapatite during tooth formation, producing a more perfect and acid-resistant mineral phase of tooth structure. Topical application of fluoride is believed to achieve the same result on the surface of the tooth by initial dissolution of some of the hydroxyapa-tite, followed by recrystallization of apatite, which incorporates fluoride ions into its lattice structure. Another important mode of action, namely, the inhibition of demineralization, and the promotion of remineralization of incipient carious lesions by fluoride ions in the oral fluid, has more recently been proposed as an important anticaries mechanism of fluoride, perhaps more important than the other proposed mechanisms. In any event, fluoridation represents the most effective means known for rendering the tooth more resistant to the carious process.\nFluoride produces more acid-resistant mineral phase of tooth\nPlaque-induced periodontal disease encompasses two separate disease entities: gingivitis and chronic periodontitis. These diseases are believed to be related, in that gingivitis, although a reversible condition, is thought to be an early stage leading ultimately to chronic periodontitis in the susceptible subject. The term gingivitis is used when the inflammatory condition is limited to the marginal gingiva and bone resorption around the necks of teeth has not yet begun. Gingivitis develops within 2 weeks in individuals who fail to practice effective tooth cleansing. Chronic periodontitis is used to connote the stage of chronic periodontal disease in which there is progressive loss of tooth support owing to resorption of the alveolar bone and periodontal ligament. Periodontitis can also lead to periodontal abscess when the chronic inflammatory state around the necks of the teeth becomes acute at a specific location.\nCauses destruction of supporting tissues\nBoth gingivitis and chronic periodontitis are caused by bacteria in the dental plaque that lie in close proximity to the necks of the teeth and marginal gingival tissues. Thus, subgingival plaque found within the gingival crevice or the sulcus around the necks of the teeth is thought to house the etiologic agent(s). The characteristic histopathologic picture of gingivitis is of a marked inflammatory infiltrate of polymorphonuclear leukocytes, lymphocytes, and plasma cells in the connective tissue that lies immediately adjacent to the epithelium lining the gingival crevice and attached to the tooth. Collagen is lost from the inflamed connective tissue. There does not seem to be any direct invasion of the gingival tissues by large numbers of intact bacteria, at least in the early stages of the disease.\nSubgingival plaque causes collagen loss\nAll forms of periodontitis are polymicrobial infections primarily involving anaerobic bacteria in much the same way described for other anaerobes in Chapter 29. The agents involved are derived from the predominantly Gram-negative anaerobic flora of the subgingival plaque (see previous text) led by Porphyromonas gingivalis and Treponema denticola. Just as bacteria-bacteria interactions determine the plaque, cross-feeding and growth stimulation have been observed between these two organisms when grown together. This kind of synergism between P gingivalis, T denticola, and other plaque members is felt to foster progression of gingivitis to chronic periodontitis. Some of these organisms have also been shown to produce virulence factors similar to those associated with other invasive bacterial pathogens. Treponema denticola is able to bind serum factors that interfere with complement deposition, and P gingivalis is a potent producer of extracellular proteases. The former facilitates survival in tissues and the latter injury to those tissues.\nPolymicrobial anaerobic infection from subgingival plaque\nSynergistic interaction facilitate growth\nVirulence factors cause disease\nChronic periodontitis is responsible for most tooth loss in people older than 35 to 40 years. The disease progresses slowly and results in the progressive destruction of the supporting tissues of the tooth (periodontal ligament and alveolar bone) from the margins of the gingiva toward the apices of the roots of the teeth. Progression may occur as a series of acute episodes separated by quiescent periods of indeterminate duration. More aggressive forms of periodontitis result in more rapid loss of tooth support. Aggressive types of disease called localized aggressive periodontitis occur in adolescents, and generalized aggressive periodontitis occurs in young adults. There is some evidence that the causative agents may differ in this form of periodontitis. A small capnophilic (carbon dioxide-requiring) Gram-negative rod (Actinobacillus actinomycetemcomitans) has been indicted based on studies of the flora of disease sites. A virulence factor found in those strains of A actinomy-cetemcomitans that are associated with this disease is the production of a leukotoxin by the bacteria.\nChronic periodontitis causes tooth loss\nAcute juvenile periodontitis associated with Actinobacillus\nAs the disease progresses, a point may be reached at which the alveolar bone around the necks of the teeth is resorbed; the condition is then no longer termed gingivitis, but peri-odontitis. With resorption of the bone, the attachment of the periodontal ligament is lost and the gingival sulcus deepens into a periodontal pocket. Periodontitis is not considered to be a reversible disease in that the lost alveolar bone and periodontal ligament do not regenerate with cessation of the inflammation, even though further progression may be halted. If unchecked, bone resorption progresses to loosening of the tooth, which may ultimately be exfoliated. Figure 41-5 shows a case of advanced chronic periodontitis. Occasionally, the neck of a periodontal pocket becomes constricted, the bacteria proliferate causing an acute inflammatory response in the occluded pocket, and a periodontal abscess results. This acute exacerbation requires drainage in the same way as abscesses elsewhere for the patient to obtain symptomatic relief.\nFIGURE 41–5. Periodontitis. A. Normal gingival. B. Periodontal disease, with plaque, inflammatory changes, bleeding, and shortening of the gingival between the teeth. (Reproduced with permission from Nester EW: Microbiology: A Human Perspective, 6th edition. 2009.)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:b96885a1-3feb-4198-9ac3-1b94d07427c8>","<urn:uuid:92cb4473-401f-4ba2-9a2d-1725af61f847>"],"error":null}
{"question":"How does the braking system work on a Crown RD Narrow-Aisle Rider forklift?","answer":"The Crown RD Narrow-Aisle Rider forklift uses a brake pedal that works opposite to a car brake. The operator applies the brakes by releasing the pedal rather than stepping down on it. Quick release of the pedal results in hard braking, while gradual release produces a smoother stop. The brakes are a hydraulically released, mechanically applied system. Travel can only resume by pushing the brake pedal to the floor.","context":["Standup Forklift Operator Asphyxiated by Warning Chain\nOctober 17, 2002\nNebraska FACE Investigation 02NE011\nA 45-year-old forklift operator was killed when the standup forklift he had been operating ran into a steel link warning chain, pinning him against the plexiglas mast shield. The victim had started his shift approximately 45 minutes earlier. He was responsible for moving frozen food products on pallets into and out of the ‘X’ freezer. This freezer was designated as the “withheld product Area” for product awaiting test results, etc. It is believed the victim placed a load into a bin, then backed out into the aisleway with the forks trailing, striking the steel chain. The chain had been placed there as a reminder to all operators that no Withheld Products were to be placed past that point. The chain came up over the back of the lift, catching the victim across the left side and back. The force pulled him off of the foot brake control and lift floor, pinning him chest first into the plexiglas shield. Another worker noticed that they had not seen or heard the victim for several minutes and went to investigate, finding the victim pinned on the forklift. The employee ran for help while several more employees arrived on scene and tried to move the chain. The tension was too tight, bolt cutters were obtained and the chain was cut. The victim was removed and CPR was initiated by the company’s first responders. Emergency personnel arrived within minutes and the victim was taken to a local hospital where he was pronounced dead.\nThe Nebraska Workforce Development, Department of Labor’s Investigator concluded that to help prevent future similar occurrences, employers should:\n- Ensure that warning devices themselves do not create a greater hazard.\n- Develop, implement and enforce a forklift training program for all employees that may operate such equipment.\n- Ensure that all equipment has an established maintenance program as recommended by the manufacturer.\n- Develop, implement and enforce a comprehensive safety program that includes, but is not limited to, training in all hazard recognition.\nThe goal of the Fatality Assessment and Control Evaluation (FACE) workplace investigation is to prevent future work-related deaths or injuries, by a study of the working environment, the worker, the task the worker was performing, the tools the worker was using, and the role of management in controlling how these factors interact.\nThis report is generated and distributed solely for the purpose of providing current, relevant education to employers, their employees and the community on methods to prevent occupational fatalities and injuries.\nOn March 21, 2002, at approximately 10:45 a.m., a 45-year-old forklift operator died after the standup forklift he was operating contacted a steel link chain, pinning him against the plexiglas mast shield of the forklift. The Nebraska Department of Labor was notified of the fatality the same day by the Occupational Safety and Health Administration (OSHA). The Nebraska FACE Investigator met with the investigating OSHA Compliance Officer (COSHA), company officials and employees on March 22 and 25-27 at the mishap location.\nThe victim’s employer is a multiple location frozen foods distribution company that has been in business for over 25 years. At the time of the mishap the company employed 29 employees at the incident location, including the victim. The company does have a written safety program and a full time safety consultant. All training given to employees was documented and those documents were provided for the investigation. The employer had no previous history of fatalities at this location.\nVictim: The victim was a 45-year-old male. He had been employed by this company for over 24 years in two different locations, 14 years at the incident site. His primary job was to move frozen food product on pallets into and out of drive-in freezers.\nTraining: The victim had operated different types of forklifts during his employment with this company. He was qualified to operate motorized pallet jacks/dollies, sit-down Hysters and the stand-up Crown forklifts. Documentation provided by the company showed he had been formally trained using the National Safety Council’s forklift safety course in 1996, again in 1999 and was due to receive refresher training later this year. The company also supplemented that training with their own program which included videos specific to the Crown forklifts. Visual spot checks were also conducted by supervision.\nEquipment: The forklift is a Crown model RD Narrow-Aisle Rider. This particular forklift had been in use at the company since 1995 and was the forklift the victim normally operated. At the time of the incident it had 5,618 total usage hours. There were routine maintenance and daily pre-operational check off records on file including on from the day of the accident. These did not indicate any current mechanical problems with this forklift.\nThis forklift is designed to be operated from a side stance stand-up position. The operator’s right hand controls the multi-function control handle (joystick). This handle controls the travel direction and speed, all hydraulic functions, the horn and travel direction. Travel direction is determined by forward or backward movement of the handle. Neutral is in the center position. The handle is spring loaded to return automatically to the neutral position when released. Pushing the handle forward or away from the operator will move the forklift forward, or forks first. Pulling the handle back or toward the operator reverses travel. The further the handle is moved from the center neutral position, the faster the forklift will travel. The left hand operates the steering tiller arm. The forklift steers from the rear wheels and will pivot on the load wheels. The floorboard of the forklift is spring-loaded and designed to slope back, allowing the operator to comfortably rest against the padding on the inside. When stepping onto the spring-loaded floorboard, a switch is energized which turns on the power steering. That switch also allows the operator to operate the load handling functions of the truck. The brake pedal is also located on the floorboard. It works just the opposite of the way a car brake does. The operator applies the brakes by releasing the pedal, not by stepping down on it. If you quickly release the pedal, the braking will be hard. Gradually releasing the pedal will generate a slower but smoother stop. Travel can only be resumed by pushing the brake pedal to the floor.\nShortly after the incident the forklift was driven back to the maintenance area and locked out of service. The following day, March 22, 2002 representatives from FACE, OSHA, the victim’s employer, the building owner, the forklift servicing company and affected insurance companies met to determine what type of testing/teardown should be done to the forklift. Since the braking system was determined to be the most critical component at this time, it was decided to conduct braking tests using both the accident forklift and another identical model. ASME/ANSI standard B56.1-1993 requires this type of forklift to stop when traveling at full speed (5-7 mph) within a 6-foot distance. This distance is permanently marked in solid red lines along an aisle adjacent to the maintenance area. Crown operators are required to test the brakes as part of the pre-operational checklist. The company’s forklift mechanic conducted the tests with all representatives present, first with the accident forklift, then with a like machine. He would proceed at full speed down the aisle, cross the first red line, and release the foot pedal to activate the brakes. The following were the recorded distances, all within the required specifications:\n|Incident forklift #17||70”||83”||108 ¼”||81 ½”||88”|\n|Forklift #10||65”||50”||55”||69 ½”||69 ½”|\nEven though 4 out of the 5 tests conducted were “out of range”, this was attributed to the reaction time of the operator and where he released the pedal, not bad brakes. The incident forklift was then driven back to the maintenance area and again locked out-of-service. It was then determined by all present that a physical tear down of the brakes should be accomplished either by or with a third party present.\nOn March 27th all parties again assemble, joined by an independent mechanical engineer. The brakes are a hydraulically released, mechanically applied system. Both sets of brakes were torn down by a Crown certified mechanic exposing the hubs and brake shoes. No abnormalities were found. All appeared to be in operating shape.\nWorksite: The incident occurred inside the ‘X’ freezer, a drive-in freezer at a cold storage locker facility which maintains a constant –15 Fahrenheit temperature. The floor is concrete, level and smooth. Overhead lighting is available and adequate. CFR 1910.178h standard states the minimum foot candles required for this storage area are 2 lumens/foot-candles. The OSHA Compliance Officer used a Weston Instruments Model 615 Illumination Meter to measure available lighting at the accident site. The following were the results:\nAt floor level = 16 foot candles.\nAt approximately 4’ high = 20 foot candles.\nAt approximately 6’6” high = 20 foot candles.\nChain: Chains have been used to isolate this and other company facility Withheld Product Areas since a directive was received from a government agency in 1983. Attempts to obtain a copy of this directive to see if chains were specific as the sold barrier media were not productive. The chain was moved to the current location approximately 1 ½ months ago from the rear of the aisle. It is constructed of ¼” stainless steel links, 128” in length and stretched across the aisle at a slight angle. It was wrapped and padlocked around the square shelf tubing on either side at a height of approx. 60 inches. The chain drooped in the middle to 34” above the floor level. It was placed at these heights to ensure it wasn’t a tripping hazard and to be visible to forklift operators. Attached in the middle was a metal sign approximately 12” by 10”, painted red with the words WITHHELD PRODUCT AREA in white capital letters. The key to this chain is kept in the Withheld Product Coordinator’s desk, outside of the freezer area on the loading dock. There are normally three chains across aisleways in the X freezer to essentially “box in” the Withheld Product Area: one across the entrance, one across a center aisle, and the other across the rear of the aisle where needed. At the time of the mishap, the entrance chain was down to allow access and the other two were in place. Employees interviewed stated they remember both plastic “crime scene tape” and plastic chain being used for short periods, but it was continually backed into and broke, so the stronger metal chain was used. Two employees indicated that they had become “entangled” in the metal chain on previous occasions. One received only a ripped coat, but the other had become pinned very similar to the victim. Although that operator was unable to move his right hand, he was able to work his left hand onto the joystick and slowly move the forklift back away from the chain. Neither of these two told their supervisors or other management personnel about the incidents.\nOn the morning of the accident the victim started his shift at 9:30 a.m. He went to the maintenance area and used the pre-operational check list to evaluate his forklift. He indicated on the checklist he conducted a brake test, but there were no witnesses to verify that. He proceeded to the ‘X’ freezer where he normally worked and began combining palletized frozen product in the Withheld Product Area, such as beef with other beef, chicken with other chicken, etc. After moving the product, a “location sheet” is filled out by the operator. The victim had filled his sheet out for some of his first product transfers, but had not filled it out for the task he was doing just prior to the accident. It is believed that he placed some pallets in a bin, then reversed the forklift out of the bin and into the aisle. He was traveling in reverse, with the forks trailing. Normally he would have turned towards the entrance of the freezer, but this time he turned towards the rear and the forklift made contact with the barrier chain. It is unknown how fast he was traveling, or if brakes were applied. It is believed that due to the height of the chain on the posts, the looseness/slack of the chain, the round rear shape of the forklift and the angle of the chain across the aisle, that the chain ”rode up” over the forklift as pressure was applied. It initially caught the operator across his left side, lifting him up off the floor board which would have applied the brake system. The pressure from the chain forced his shoulders and face flat against the plexiglas mast shield. An employee mentioned that they had not seen the victim for a few minutes, so his supervisor went to locate him. As he approached from the center aisle, he saw the stopped forklift, but didn’t notice the victim. Upon closer examination he noticed the victim with his arms dangling at his sides and the chain across his shoulders. He ran outside the freezer to the dock area and told other employees to summon help. Six employees ran inside the ‘X’ freezer and attempted to remove the chain, but could not budge it due to the extreme tension. They yelled at the supervisor to get bolt cutter from the maintenance area while another employee summoned local rescue personnel. The bolt cutters were brought to the scene and the chain cut. The employees stated the chain had a “sling shot” effect when cut. The two company first responders lowered the victim to the floor and covered him with their coats. They started performing CPR and continued until the rescue personnel arrived. The victim was removed from the scene and taken to a local hospital where he was pronounced dead.\nCAUSE OF DEATH:\nAccording to the death certificate, the cause of death was Mechanical Asphyxiation.\nRecommendation #1: Ensure that warning devices themselves do not create a greater hazard.\nDiscussion: The sole intent of the “warning chain” was to let forklift operators know that this area was set aside from a specific reason, not to prevent employees from entering. The “withhold products” do have bright orange stickers which differentiate them from other product. Plastic “police tape” and plastic chain link were utilized in the past, but removed because they were being constantly broke. Employees indicated that they have backed into the steel chain on numerous occasions. This should have been an indicator that other methods should be tested and the safest used. ¼” steel chain link has a minimum vertical pull breaking point of 5000 lbs. It is recommended that an elastic cord (such as bungee cord) be utilized in lieu of a chain. This would not break during incidental contact, but should allow operators to know they had entered a restricted area. (CFR 1910.176(a) & Thomas J. Glover Pocket Reference, 2nd Edition, August 1998)\nRecommendation #2: Employers should ensure that each powered truck operator is competent to operate a powered industrial truck safely.\nDiscussion: The victim had received formal training in the proper operation of the incident forklift from his current employer. He was up-to-date with all requirements. This training was given under the direct supervision of a person that had the knowledge, training and experience to train operators and evaluate their competence. This training consisted of a combination of formal instruction (e.g. lecture, discussion, interactive computer learning, video tape, written material), practical training (demonstrations performed by the trainer and practical exercises performed by the trainee) and evaluation of the operator’s performance in the workplace. This training included operating the forklift according to manufacturer’s specifications. (CFR 1910.178(l))\nRecommendation #3: Employers should establish a routing maintenance program for all equipment to meet or exceed regulatory requirements.\nDiscussion: The employer had a well established maintenance program. This machine was examined prior to being placed in service at the beginning of every shift. Any time the exam showed any condition(s) adversely affecting the safety of the forklift, it was documented and the lift removed from service. Records for the accident forklift and all other company equipment were readily available during the investigation. (CFR 1910.178(q)(7))\nRecommendation #4: Employers should provide a safe and healthful workplace, free from recognized hazards by developing, implementing and enforcing formal safety programs.\nDiscussion: The victim’s employer does have written, formal safety programs. Hazards associated with particular jobs were in writing, verbally discussed, and documented. The employer instructs each employee in the recognition, reporting procedures and avoidance of unsafe conditions and the regulations applicable to their work environment to control or eliminate any hazards or other exposure to illness or injury.\nIncident forklift specifications.\nTwo pictures taken showing incident chain in relationship to rear of forklift.\n- Thomas J. Glover Pocket Reference, 2nd Edition, August 1998\n- CROWN RR, RS and RD Training Guide for the Narrow-Aisle Rider\n- American Society of Mechanical Engineers (ASME)/American National Standards Institute (ANSI) Standard B56.1-1993\nTo contact Nebraska State FACE program personnel regarding State-based FACE reports, please use information listed on the Contact Sheet on the NIOSH FACE web site Please contact In-house FACE program personnel regarding In-house FACE reports and to gain assistance when State-FACE program personnel cannot be reached."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:b91e4be9-1a1b-4e45-b44e-086eb84a68df>"],"error":null}
{"question":"How has the evolution of web technology influenced the relationship between websites and software applications?","answer":"As technology has become more sophisticated and network infrastructure has improved, the distinction between websites and software has become increasingly blurred. Modern websites often function more like 'web applications' with functionality extending beyond typical website expectations. This evolution has required web developers and designers to expand beyond technical skills to include knowledge of information architecture, ergonomics, user research, and human psychology to create effective solutions.","context":["Web and interaction design\nWeb and interaction design is a design discipline which focuses on the interaction between human users, machines, and the systems that help them interface. It is often split into user interface (UI) design, which focuses on the arrangement and information architecture of a website or piece of software, and user experience (UX) design, which takes a more holistic approach to formulating the way a user interacts with that software. UI design and UX design necessarily have large overlaps in terms of techniques and processes.\nWhat is user interface design?\nUI design is the design of the interfaces for machines and software with the goal of maximising usability. Good user interface design allows users to easily finish the task at hand in a way which is easy and efficient. Often the best interface designs are deceptively simple, making a complex machine or process appear easy. A great example of this is the Google search engine, which performs incredibly far-reaching searches using a highly sophisticated algorithm, but presents users with a simple, self-explanatory search bar and little else to confuse them.\nThe UI design process works to balance technical functionality, usability and aesthetics in a way which makes a positive user experience. The goal is to make a system which is more than simply operational - but also enjoyable, easy to use and effective. To achieve this, UI designers utilise skills from the disciplines of graphic design, typography and information architecture to create highly usable, intuitive systems.\nWhat is user experience design?\nUser experience design is a broader discipline which consists not only of UI design, but also ergonomics, psychology and user research. This holistic field focuses its attention not just on the system, but also on the people who interact with it. The benefits of strong UX design include:\n- Removing unnecessary or confusing product features\n- Simplifying design documentation and lowering the educational requirements to utilise the software or website\n- Improving the overall usability of the system\n- Assisting the design process generally with informed user information\n- Increasing acceptance and proficiency by the users\nHow do UI design and UX design relate to the web?\nWhen the internet first went public, the technology used to design and code websites was very limiting. As technology has become more sophisticated and the network infrastructure has allowed websites to become larger and more complex, the line between a piece of software and a website has become increasingly blurred. Many websites these days would be better termed ‘web applications’, as their functionality extends beyond the typical expectations of a website.\nWhen web developers and web designers are tasked with using web technology to solve a novel or complex problem, or offer a sophisticated software functionality, they may find they lack the skills to create a usable finished product. While web developers and web designers are likely to have the technical skills to solve the problem, they may lack the knowledge of information architecture, ergonomics, user research and human psychology to create an elegant solution to the problem.\nUI and UX designers round out the whole web and interaction design team, so that websites can offer more than they ever could in the past."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:684ae091-9ee5-4379-86c7-3605ba05993e>"],"error":null}
{"question":"How were the ancient cave paintings in Paleolithic times created and what did they typically depict?","answer":"In Paleolithic times, cave paintings primarily depicted animals, including both food sources and strong creatures like rhinoceros and large Felidae (as seen in the Chauvet Cave). These paintings were created using red ochre and black pigment. The artists also drew signs like dots, and rarely included human representations, which were limited to handprints and half-human/animal figures. The technique likely involved spitting or blowing pigments onto the rock surface.","context":["- Error loading feed data.\n- Error loading feed data.\nHistory of Art, Paintings and Artists. Art History Resources\nRENAISSANCE ARTISTS, PAINTERS AND INVENTORS. RENAISSANCE ART, PAINTING, SCULPTURE, HISTORY, FAMOUS ARTISTS\nThe history of painting reaches back in time to artifacts from pre-historic humans, and spans all cultures, that represents a continuous, though disrupted, tradition from Antiquity. Across cultures, and spanning continents and millennia, the history of painting is an ongoing river of creativity, that continues into the 21st century. Until the early 20th century it relied primarily on representational, religious and classical motifs, after which time more purely abstract and conceptual approaches gained favor.\nThe oldest known paintings are at the Grotte Chauvet in France, claimed by some historians to be about 32,000 years old.\nThey are engraved and painted using red ochre and black pigment and show horses, rhinoceros, lions, buffalo, mammoth or humans often hunting. There are examples of cave paintings all over the world—in France, India, Spain, Portugal, China, Australia etc. Various conjectures have been made as to the meaning these paintings had to the people that made them. Prehistoric men may have painted animals to \"catch\" their soul or spirit in order to hunt them more easily or the paintings may represent an animistic vision and homage to surrounding nature, or they may be the result of a basic need of expression that is innate to human beings, or they could have been for the transmission of practical information.\nIn Paleolithic times, the representation of humans in cave paintings was rare. Mostly, animals were painted, not only animals that were used as food but also animals that represented strength like the rhinoceros or large Felidae, as in the Chauvet Cave. Signs like dots were sometimes drawn. Rare human representations include handprints and half-human / animal figures. The Chauvet Cave in the Ardèche Departments of France contains the most important preserved cave paintings of the Paleolithic era, painted around 31,000 BC.\nThe Altamira cave paintings in Spain were done 14,000 to 12,000 BC and show, among others, bisons. The hall of bulls in Lascaux, Dordogne, France, is one of the best known cave paintings from about 15,000 to 10,000 BC.\nIf there is meaning to the paintings, it remains unknown. The caves were not in an inhabited area, so they may have been used for seasonal rituals. The animals are accompanied by signs which suggest a possible magic use. Arrow-like symbols in Lascaux are sometimes interpreted as calendar or almanac use. But the evidence remains inconclusive. The most important work of the Mesolithic era were the marching warriors, a rock painting at Cingle de la Mola, Castellón, Spain dated to about 7,000 to 4,000 BC. The technique used was probably spitting or blowing the pigments onto the rock. The paintings are quite naturalistic, though stylized. The figures are not three-dimensional, even though they overlap\nThe earliest known Indian paintings were the rock paintings of prehistoric times, the petroglyphs as found in places like the Rock Shelters of Bhimbetka, and some of them are older than 5500 BC. Such works continued and after several millennia, in the 7th century, carved pillars of Ajanta, Maharashtra state present a fine example of Indian paintings, and the colors, mostly various shades of red and orange, were derived from minerals...\nArt history is not only a biographical endeavor.\nArt historians often root their studies in the close scrutiny of individual objects. They thus attempt to answer in historically specific ways, questions such as:\nWhat are key features of this style?\nWhat meaning did this object convey?\nHow does it function visually?\nDid the artist meet their goals well?\nWhat symbols are involved?\nand Does it function discursively?\nAn art movement is a tendency or style in art with a specific common philosophy or goal, followed by a group of artists during a restricted period of time, or, at least, with the heyday of the movement defined within usually a number of years.\nSearch for Paintings and Art\nAntiques of America\n|Antiques of America Part adventure, part history lesson, and part treasure hunt|"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:a6b092cd-137d-4675-8808-a53a1c4a9aed>"],"error":null}
{"question":"What are the accessibility features of Milan's public transportation system compared to La Scala Theater's accommodations for visitors with reduced mobility?","answer":"Milan's public transportation system features the largest underground network in Italy with most stations being accessible, though buses can be crowded making wheelchair access difficult. For La Scala Theater, specific accommodations are available, including accessible transport services and wheelchair-accessible seating positions. The theater can be reached via subway (Red Line MM1) to Duomo station, bus line 61, or trams 1 and 2, and there is dedicated parking for persons with special needs in Via Verdi.","context":["Piedmont and Lombardy accessible are among the most industrialized Italian Regions. Located in the North Western part of Italy at the foot of the Alps.\nThey are regions rich in natural beauty even if they differ from the classical iconography of the Italian environment. High mountains (the highest in Europe), large lakes, woods, villages and large cities:\nMilan & Turin:\nMilan (the economic capital of Italy), Turin (the first capital of Italy and seat of the largest Italian industry, FIAT now FCA) .\nDespite being modern cities, they retain monuments from their long past that dates back to over 2000 years ago.\nIn Turin there are archaeological remains of the Roman period, Romanesque churches, palaces of the Baroque period.\nIn Milan the oldest remains are few traces of an imperial palace of the Western Roman Empire of the third century AD metre more are the Romanesque churches, buildings of the medieval period and the last centuries.\nBoth cities have important museums and cultural institutions.\nTurin hosts the richest Egyptian Museum in the West second only to that of Cairo. Other palaces include the Palazzina di Caccia di Stupinigi and the Venaria Reale located in wooded areas around the city.\nIn addition to the aforementioned Romanesque churches, Milan has interesting museums. One of the most interesting is the “Leonardo da Vinci” Science Museum.\nIn general, both cities have good accessibility with walkable sidewalks and, especially in the central area of Turin, covered by arcades.\nThere are slides at the crossings.\nMilan has the largest underground network in Italy and most of the stations are accessible.\nTurin has only a sketch of a metropolitan network with a single line that crosses it and is, in some points connected with the railway network, thus increasing the number of points in the city that can be reached.\nAll the stations are accessible.\nBoth cities have accessible surface public transport even if the buses are crowded and therefore access with wheelchairs is difficult.\nThere are no accessible taxis, but accessible transport with driver for private services can be booked on request.\nBoth cities host important trade fairs. Milan which has 2 exhibition centers is the Italian city which hosts the largest number of trade fairs, exhibitions, fashion shows, …\nFor any service connected with participation in Fairs and Exhibitions both in Milan and Turin for exhibitors or visitors with reduced mobility, we can provide private transport (with accessible vehicles), car rental with manual controls, accessible hotels of any category, personal assistance and accompaniment carried out by professionals in the desired language.\nMilan is where, the main Italian Opera Theater and one of the first in the world, La Scala Theater is located.\nAccessibleItaly can arrange stays combined with shows in La Scala, organizing accessible transports, booking accessible hotels in a convenient position to reach the theater and book seats in positions that can be easily reached by wheelchair.\nPiedmont and Lombardy accessible to travelers by wheelchair\nLombardy has many cities rich in history and with fascinating historical centers: Bergamo, Mantua, Pavia, Cremona, … 4 large lakes: Lake Maggiore, Lake Como, Lake Iseo and Lake Garda that create a unique habitat with small villages and beautiful lake landscapes and are crossed by navigation.\nPiedmont is known for its mountains in whose ski resorts even people with disabilities can practice skiing.\nSome of the areas of Piedmont are highly reputed for food and wine tourism such as the Langhe where some of the most famous Italian wines are produced: Barolo, Barbaresco, Nebiolo and gastronomy based on rare and precious foods such as white truffles.\nTours & Stays:\n=====PIEDMONT , LOMBARDY AND TURIN, MILAN MUSEUM INFO=====\nPiedmont & Turin:\nMuseums Turino: www.museireali.beniculturali.it\nEgyptian Museum: www.museoegizio.it\nStupinigi hunting lodge: www.ordinemauriziano.it (only Italian)\nRoyal Palace of the Venaria Reale: www.lavenaria.it\nTransports Turino (Map): www.gtt.to.it\nLanghe wines and cuisine: www.langheroero.it\nMont Blanc skyway: www.montebianco.com\nNavigation Piedmont & Lombardy Lakes: www.navigazionelaghi.it (In English too)\nLombardy & Milan:\n“Leonardo da Vinci” Science Museum: www.museoscienza.org\nLa Scala Theater: www.teatroallascala.org\nTransports Milan: www.atm.it\nTransports Milan (Map): giromilano.atm.it\nAccessible Toilets: www.lonelyplanet.com","History and Architecture\nOne of the most renowned theaters in the world, La Scala Theater was commissioned by Empress Maria Theresa of Austria after the fire that destroyed in 1776 the previous royal theater of Milan, the Teatro Regio Ducale. La Scala was designed by Architect Giuseppe Piermarini, a neoclassical artist from Central Italy (he was born in Foligno, then part of the Papal States) who would be remembered especially for this project (to the point that the “the Piermarini” is sometimes used as a synonym for the Scala in Milan).\nPiermarini, as a neoclassical architect, preferred sobriety to the artifices of baroque. However, La Scala underwent several reconstructions and renovations during the centuries so that today only the general structure and the theater’s façade mirror Piermarini’s initial vision.\nThe theater takes its name from the church of Santa Maria alla Scala, which had to be destroyed to make room for the new opera house. The theater’s construction took two years. The inauguration was on August 3, 1778, with the premiere of Antonio Salieri’s Europa Riconosciuta (Europe Recognized). La Scala’s features caused an immediate sensation: its acoustics was superb, and it was evident that the “noblesse” of Milan would use the opera house as a social meeting place. During those times, in fact, theaters were used not only for art but also as gathering places and even for gambling, which was allowed only in theaters during performances.\nIn order to improve the acoustics of the theater, Piermarini used two original devices: the columns that separated the boxes were smaller than common, and a wooden vault created almost-perfect audibility from any corner of the theater. In the beginning, the Scala could host up to 3,000 people. Six levels of boxes were built. The boxes were decorated according to their owners’ (and theater funders’) preference. On top of the boxes, there are two galleries (the loggione). It is the loggione that decides the success or failure of an opera since it is the home of the most passionate theatergoers.\nToday, after the renovation following the bombardments in World War II and the renovation in 2002–2004 by Architect Mario Botta (made necessary by the new security regulations), the opera house can host 2,030 spectators: a third in the parterre, a little less than a third in the two galleries, and the remainder in the boxes.\nCultural importance of La Scala\nEven before the introduction of genial composer Giuseppe Verdi, La Scala witnessed wonderful premieres of works commissioned to Gioacchino Rossini, Gaetano Donizetti, and Vincenzo Bellini. During those times, the choice of the operas was strongly influenced by Ricordi, the official publishing house of the works represented at La Scala.\nThe premiere of Rossini’s La Gazza Ladra, famous for its incredible overture, was in 1817. Donizetti presented himself to la Scala in 1822, with Chiara and Serafina; Lucrezia Borgia, Gemma di Vergy, and others followed in the 1830s. Bellini’s work appeared in 1827, with Il Pirata, followed by La Straniera and Norma.\nGiuseppe Verdi’s first collaboration with La Scala was in 1839, with the good success of his opera Oberto, Conte di San Bonifacio. Then came Un Giorno di Regno, which proved to be a failure; apparently, Verdi was incapacitated to write good comic music because he was mourning the loss of his first wife and their children, struck by disease. Verdi’s fortunes turned on March 9, 1842, on occasion of the premiere of the Nabucco. This opera, an allegory of the captivity of the Italian nation under the Austrian rule, was immensely successful and ran 64 times during its first year. However, in spite of the success of the Nabucco and of the two following works (I Lombardi alla Prima Crociata and Giovanna d’Arco), Verdi left for 20 years the theater that had made him famous because he was reproached for producing operas that were too expensive and did not take into consideration the budget restrictions of the theater. Verdi’s operas returned to Milan only decades later. The premieres of Othello in 1887 and Falstaff in 1893 restored the incredibly fruitful collaboration between the author and the Milanese theater.\nThe Scala also witnessed the premieres of world-renowned operas in the 20th century (Giacomo Puccini’s Madame Butterfly in 1904 and Turandot in 1926). Avant-garde, electronic, and concrete music premiered in Milan as well. In 1963, Luciano Berio’s Passaggio premiered. Two decades later, there were the premieres of works of German composer Karlheinz Stockhausen, who is unanimously considered one of the most influential, if not the most influential, composers of the 20th and 21st centuries.\nThe seasons of the theater\nOriginally, the theater’s yearly activities were divided in several seasons: the Stagione di Carnevale (Carnival Season), starting on December 26 and ending on the week before Carnival; the Spring Season; the Summer Season; and the Autumn Season. The Stagione di Quaresima (Lent Season) was introduced in 1788 to allow representations even during the period from Carnival to Easter.\nOnly in 1920 was the division of the Milanese theater’s activity into seasons abolished; performances were held from November to June with little interruption. The new inauguration date of December 7 (day of the patron saint of Milan, Sant’Ambrogio) was experimented in 1940 and made official in 1951. However, in the 21st century there have been few weeks in which La Scala has not been active. The number of yearly performances in the theater now reaches an average of 280.\nHow to reach La Scala in Milano\nLa Scala is in the center of Milan, in Piazza della Scala (La Scala Square). As a city-center landmark, it is second only to the Duomo. To reach the theater, you can take the subway (Red Line or MM1) to Duomo station and walk to the theater via the Galleria Vittorio Emanuele (a Gallery that was built to link the Duomo to the theater).\nBy bus, you can take line 61 and get off at the Via Verdi–Via dell’Orso stop.\nBy tram, both lines 1 and 2 stop at Via Manzoni–Piazza della Scala. By car, it is very difficult to park in the city center close to La Scala. However, private parking can be found both in Via Agnello and Piazza Diaz. In Via Verdi, you can find a parking for persons with special needs."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:82b4223c-2b21-42d6-8773-f35120ab9ee0>","<urn:uuid:642804f0-6898-4f78-b7e0-719eff2d8d06>"],"error":null}
{"question":"As an environmental researcher, I want to know why renewable energy additions have slowed down in 2022. What are the specific numbers?","answer":"In the first half of 2022, renewable energy additions decreased compared to 2021. Wind additions fell to 5,722 MW from 7,157 MW, while solar additions dropped to 3,895 MW from 5,714 MW in the same period last year.","context":["- Renewable energy developers added 5,722 MW of wind in the first half this year, down from 7,157 MW in the same period in 2021, while solar additions fell to 3,895 MW from 5,714 MW in the first six months last year, according to the Federal Energy Regulatory Commission.\n- Looking ahead, there is 66,315 MW of “high probability” solar additions set to come online over the next three years, starting last month, FERC said in its monthly infrastructure report, released Aug. 9. There is 17,383 MW of high probability wind for the same period, according to the agency.\n- Meanwhile, the Energy Information Administration expects renewable energy, including hydropower, will account for 22% of the electricity produced in the United States this year, up from 17% in 2017. Renewable production will grow to 24% in 2023, the agency said in a forecast released Tuesday.\nThe FERC and EIA reports indicate that while renewable energy additions have slowed this year, solar and wind will make up a growing portion of U.S. power production.\nThe reports don’t reflect any near-term boost the Inflation Reduction Act, signed into law on Tuesday, will provide to renewable development.\nThe FERC data shows how the U.S. energy mix is changing. Coal-fired power plants make up 17.7% of all U.S. generating capacity, down from 23.9% five years ago. Nuclear capacity accounts for 8.2% of all capacity, down from 9.1%, and hydro capacity is down to 8% from 8.5% since mid-2017.\nWind farms make up 11.2% of U.S. generating capacity, up from 7.2% five years ago, and solar capacity has grown to 5.9% from 2.3% in that period, according to FERC. Gas-fired capacity has inched up to a 44.2% share from 43.4% in the last five years.\nFERC expects some of those trends to continue with about 22,226 MW of coal-fired generation slated to retire in the next three years, starting last month.\nOn the natural gas front, FERC said there is 21,743 MW of high probability gas-fired generation set to start operating in the next three years, with about 17,400 MW retiring in that period.\nMeanwhile, the EIA highlighted a dearth of renewable generation in three regions: the PJM Interconnection, the Southeast and the Florida Reliability Coordinating Council.\n“In each of these regions, we expect the renewable share of electricity generation to remain below half of the national average through 2023,” the EIA said. “Natural gas and nuclear sources provide most of the electricity generation in the Southeast and Florida. In PJM, the prevalent generation sources have been natural gas and coal.”\nThe Southwest Power Pool has had the most growth in the renewable share of electricity generation in the past decade, largely driven by wind generation, the EIA said. Last year, renewables, mainly wind, accounted for 40% of the power production in the SPP footprint, up from 13% in 2013, according to the agency, which expects renewable production to grow to 44% this year.\nRenewable energy production in the Electric Reliability Council of Texas increased to a 32% share this year from 10% in 2013, the agency said."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:08972ff1-1dcf-41a2-893e-6fa4c22c6ee0>"],"error":null}
{"question":"How do endocytosis and exocytosis work together in synaptic transmission?","answer":"Endocytosis and exocytosis work together in a cycle at synapses to enable neurotransmitter release and recycling. During exocytosis, vesicles containing neurotransmitters fuse with the presynaptic membrane, releasing their contents into the synaptic cleft. The membrane added during this process is then recovered through endocytosis, where portions of the membrane are internalized to form clathrin-coated vesicles. These vesicles are then recycled and refilled with neurotransmitters, ready for another round of release. This endocytic cycle is crucial for maintaining synaptic function, as demonstrated in fruit flies where disrupting endocytosis through temperature-sensitive dynamin mutations leads to temporary paralysis.","context":["A diagram of a typical central nervous system synapse. The proteins of the active zone are represented as dark brown pyramids on the upper neuron terminal\nThe active zone or synaptic active zone is a term first used by Couteaux and Pecot-Dechavassinein in 1970 to define the site of neurotransmitter release. Two neurons make contact through structures called synapses allowing them to communicate with each other. As shown in the diagram on the right, a synapse consists of the presynaptic bouton of one neuron which stores vesicles containing neurotransmitter (uppermost in the picture) and a second, postsynaptic neuron which bears receptors for the neurotransmitter (at the bottom). When an action potential reaches the presynaptic bouton, the contents of the vesicles are released into the synaptic cleft and the released neurotransmitter travels across the cleft to the postsynaptic neuron (the lower structure in the picture) and activates the receptors on the postsynaptic membrane.\nThe active zone is the region in the presynaptic bouton that mediates neurotransmitter release and is composed of the presynaptic membrane and a dense collection of proteins called the cytomatrix at the active zone (CAZ). The CAZ is seen under the electron microscope to be a dark (electron dense) area close to the membrane. Proteins within the CAZ tether synaptic vesicles to the presynaptic membrane and mediate synaptic vesicle fusion, thereby allowing neurotransmitter to be released reliably and rapidly when an action potential arrives.\nThe function of the active zone is to ensure that neurotransmitters can be reliably released in a specific location of a neuron and only released when the neuron fires an action potential. As an action potential propagates down an axon it reaches the axon terminal called the presynaptic bouton. In the presynaptic bouton, the action potential activates calcium channels (VDCCs) that cause a local influx of calcium. The increase in calcium is detected by proteins in the active zone and forces vesicles containing neurotransmitter to fuse with the membrane. This fusion of the vesicles with the membrane releases the neurotransmitters into the synaptic cleft (space between the presynaptic bouton and the postsynaptic membrane). The neurotransmitters then diffuse across the cleft and bind to ligand gated ion channels and G-protein coupled receptors on the postsynaptic membrane. The binding of neurotransmitters to the postsynaptic receptors then induces a change in the postsynaptic neuron. The process of releasing neurotransmitters and binding to the postsynaptic receptors to cause a change in the postsynaptic neuron is called neurotransmission.\nThe active zone is present in all chemical synapses examined so far and is present in all animal species. The active zones examined so far have at least two features in common, they all have protein dense material that project from the membrane and tethers synaptic vesicles close to the membrane and they have long filamentous projections originating at the membrane and terminating at vesicles slightly farther from the presynaptic membrane. The protein dense projections vary in size and shape depending on the type of synapse examined. One striking example of the dense projection is the ribbon synapse (see below) which contains a \"ribbon\" of protein dense material that is surrounded by a halo of synaptic vesicles and extends perpendicular to the presynaptic membrane and can be as long as 500 nm! The glutamate synapse contains smaller pyramid like structures that extend about 50 nm from the membrane. The neuromuscular synapse contains two rows of vesicles with a long proteinaceous band between them that is connected to regularly spaced horizontal ribs extending perpendicular to the band and parallel with the membrane. These ribs are then connected to the vesicles which are each positioned above a peg in the membrane (presumably a calcium channel). Previous research indicated that the active zone of glutamatergic neurons contained a highly regular array of pyramid shaped protein dense material and indicated that these pyramids were connected by filaments. This structure resembled a geometric lattice where vesicles were guided into holes of the lattic. This attractive model has come into question by recent experiments. Recent data shows that the glutamatergic active zone does contain the dense protein material projections but these projections were not in a regular array and contained long filaments projecting about 80 nm into the cytoplasm.\nThere are at least five major scaffold proteins that are enriched in the active zone; UNC13/Munc13, RIMs (Rab3-interacting molecule), Bassoon, Piccolo/aczonin, ELKS, and liprins-α. These scaffold proteins are thought to be the constituents of the dense pyramid like structures of the active zone and are thought to bring the synaptic vesicles into close proximity to the presynaptic membrane and the calcium channels. The protein ELKS binds to the cell adhesion protein, β-neurexin, and other proteins within the complex such as Piccolo and Bassoon. β-neurexin then binds to cell adhesion molecule, neuroligin located on the postsynaptic membrane. Neuroligin then interacts with proteins that bind to postsynaptic receptors. Protein interactions like that seen between Piccolo/ELKS/β-neurexin/neuroligin ensures that machinery that mediates vesicle fusion is in close proximity to calcium channels and that vesicle fusion is adjacent to postsynaptic receptors. This close proximity vesicle fusion and postsynaptic receptors ensures that there is little delay between the activation of the postsynaptic receptors and the release of neurotransmitters.\nNeurotransmitter release mechanism\nThe release of neurotransmitter is accomplished by the fusion of neurotransmitter vesicles to the presynaptic membrane. Although the details of this mechanism are still being studied there is a consensus on some details of the process. Synaptic vesicle fusion with the presynaptic membrane is known to require a local increase of calcium from as few as a single, closely associated calcium channels and the formation of highly stable SNARE complexes. One prevailing model of synaptic vesicle fusion is that SNARE complex formation is catalyzed by the proteins of the active zone such as Munc18, Munc13, and RIM. The formation of this complex is thought to \"prime\" the vesicle to be ready for vesicle fusion and release of neurotransmitter (see below: releasable pool). After the vesicle is primed then complexin binds to the SNARE complex this is called \"superprimed.\" The vesicles that are superprimed are within the readily releasable pool (see below) and are ready to be rapidly released. The arrival of an action potential opens voltage gated calcium channels near the SNARE/complexin complex. Calcium then binds to changes the conformation of synaptotagmin. This change in conformation of allows synaptotagmin to then dislodge complexin, bind to the SNARE complex, and bind to the target membrane. When synaptotagmin binds to both the SNARE complex and the membrane this induces a mechanical force on the membrane so that it causes the vesicle membrane and presynaptic membrane to fuse. This fusion opens a membrane pore that releases the neurotransmitter. The pore increases in size until the entire vesicle membrane is indistinguishable from the presynaptic membrane.\nSynaptic vesicle cycle\nThe presynaptic bouton has an efficiently orchestrated process to fuse vesicles to the presynaptic membrane to release neurotransmitters and regenerate neurotransmitter vesicles. This process called the synaptic vesicle cycle maintains the number of vesicles in the presynaptic bouton and allows the synaptic terminal to be an autonomous unit. The cycle begins with (1) a region of the golgi apparatus is pinched off to form the synaptic vesicle and this vesicle is transported to the synaptic terminal. At the terminal (2) the vesicle is filled with neurotransmitter. (3) The vesicle is transported to the active zone and docked in close proximity to the plasma membrane. (4) During an action potential the vesicle is fuses with the membrane, releases the neurotransmitter and allows the membrane proteins previously on the vesicle to diffuse to the peri-active zone. (5) In the peri-active zone the membrane proteins are sequestered and are endocytosed forming a clathrin coated vesicle. (6) The vesicle is then filled with neurotransmitter and is then transported back to the active zone.\nThe endocytosis mechanism is slower than the exocytosis mechanism. This means that in intense activity the vesicle in the terminal can become depleted and no longer available to be released. To help prevent the depletion of synaptic vesicles the increase in calcium during intense activity can activate calcineurin which dephosphorylate proteins involved in clathrin-mediated endocytosis.\nThe synapse contains at least two clusters of synaptic vesicles, the readily releasable pool and the reserve pool. The readily releasable pool is located within the active zone and connected directly to the presynaptic membrane while the reserve pool is clustered by cytoskeletal and is not directly connected to the active zone.\nThe releasable pool is located in the active zone and is bound directly to the presynaptic membrane. It is stabilized by proteins within the active zone and bound to the presynaptic membrane by SNARE proteins. These vesicles are ready to release by a single action potential and are replenished by vesicles from the reserve pool. The releasable pool is sometimes subdivided into the readily releasable pool and the releasable pool.\nThe reserve pool is not directly connected to the active zone. The increase in presynaptic calcium concentration activates the calcium sensitive phosphatase, calcineurin. Calcineurin dephosphorylates a protein, synapsin, that mediates the clustering of the reserve pool vesicles. Dephosphorylation of synapsin mobilize vesicles in the reserve pool and allows the vesicles to migrate to the active zone and replenish the readily releasable pool.\nThe periactive zone surrounds the active zone and is the site of endocytosis of the presynaptic terminal. In the periactive zone, scaffolding proteins such as intersectin 1 recruit proteins that mediate endocytotis such as dynamin, clathrin and endophilin. In Drosophilia the intersectin homolog, Dap160, is located in the periactive zone of the neuromuscular junction and mutant Dap160 deplete synaptic vesicles during high frequency stimulation.\nRibbon Synapse Active Zone\nThe ribbon synapse is a special type of synapse found in sensory neurons such as photoreceptor cells, retinal bipolar cells, and hair cells. Ribbon synapses contain a dense protein structure that tethers an array of vesicles perpendicular to the presynaptic membrane. In an electron micrograph it appears as a ribbon like structure perpendicular to the membrane. Unlike the 'traditional' synapse, ribbon synapses can maintain a graded release of vesicles. In other words the more depolarized a neuron the higher the rate of vesicle fusion. The Ribbon synapse active zone is separated into two regions, the archiform density and the ribbon. The archiform density is the site of vesicle fusion and the ribbon stores the releasable pool of vesicles. The ribbon structure is composed primarily of the protein RIBEYE, about 64-69% of the ribbon volume, and is tethered to the archiform density by scaffolding proteins such as Bassoon.\nProteins of the Active Zone\n|ELKS (ERCs or CAST)|\n|Docking and Priming|\n|syntaxin||Located on the synaptic membrane and binds to SNAP-25 and synaptobrevin to mediate vesicle fusion.|\n|Voltage-dependent calcium channel (VDCC)||Allows the rapid influx of calcium during an action potential.|\nMeasuring Neurotransmitter Release\nNeurotransmitter release can be measured by determining the amplitude of the postsynaptic potential after triggering an action potential in the presynaptic neuron. Measuring neurotransmitter release this way can be problematic because the effect of the postsynaptic neuron to the same amount of released neurotransmitter can change over time. Another way is to measure vesicle fusion with the presynaptic membrane directly using a patch pipette. A cell membrane can be thought of as a capacitor in that positive and negative ions are stored on both sides of the membrane. The larger the area of membrane the more ions that are necessary to hold the membrane at a certain potential. In electrophysiology this means that a current injection into the terminal will take less time to charge a membrane to a given potential before vesicle fusion than it will after vesicle fusion. The time course to charge the membrane to a potential and the resistance of the membrane is measured and with these values the capacitance of the membrane can be calculated by the equation Tau/Resistance=Capacitance. With this technique researchers can measure synaptic vesicle release directly by measuring increases in the membrane capacitance of the presynaptic terminal.\n- Chemical synapse\n- Neurotransmitter vesicle\n- Vesicle fusion\n- Paired Pulse Facilitation\n- Craig C. Garner and Kang Shen. Structure and Function of Vertebrate and Invertebrate Active Zones. Structure and Functional Organization of the Synapse. Ed: Johannes Hell and Michael Ehlers. Springer, 2008.\n- R. Grace Zhai and Hugo J. Bellen. The Architecture of the Active Zone in the Presynaptic Nerve Terminal. Physiology 19:262-270, 2004.\n- Phillips GR et al. The presynaptic particle web: ultrastructure, composition, dissolution, and reconstitution. Neuron 32: 63–77, 2001\n- Mark L. Harlow et al. The architecture of active zone material at the frog's. neuromuscular junction. NATURE, VOL 409, 25 JANUARY 2001\n- Siksou et al. Three-Dimensional Architecture of Presynaptic Terminal Cytomatrix. The Journal of Neuroscience, June 27, 2007 • 27(26):6868–6877\n- Ziv and Garner. CELLULAR AND MOLECULARMECHANISMS OF PRESYNAPTIC ASSEMBLY. VOLUME 5, MAY 2004, 385 - 399.\n- Georgiev, Danko D .; James F . Glazebrook (2007). \"Subneuronal processing of information by solitary waves and stochastic processes\". In Lyshevski, Sergey Edward. Nano and Molecular Electronics Handbook. Nano and Microengineering Series. CRC Press. pp. 17–1–17–41. ISBN 978-0-8493-8528-5.\n- Heidelberger et al.(1994) Calcium dependence of the rate of exocytosis in a synaptic terminal. Nature. Vol. 371. 513-515\n- Stanley EF Single calcium channels and acetylcholine release at a presynaptic nerve terminal. Neuron 11:1007 (1993)\n- Atasoy and Kavalali. Neurotransmitter Release Machinery: Components of the Neuronal SNARE Complex and Their Function. Structural and Functional Orgnanization of the Synapse Hell and Ehlers (eds.) 2008\n- Z. Pang and T. Sudhof. Cell biology of Ca2+-triggered exocytosisCurrent Opinion in Cell Biology. Volume 22, Issue 4, August 2010, Pages 496-505\n- C. Carr and M. Munson. Tag team action at the synapse. EMBO reports (2007) 8, 834 - 838\n- Nadja Jung and Volker Haucke. Clathrin-Mediated Endocytosis at Synapses. Volume 8, Issue 9, pages 1129–1136, September 2007\n- Cesca et al. (2010) The synapsins: Key actors of synapse function and plasticity. Progress in Neurobiology. Vol. 91. 313-348.\n- Dergai et al. Intersectin 1 forms complexes with SGIP1 and Reps1 in clathrin-coated pits. Biochemical and Biophysical Research Communications. Volume 402, Issue 2, 12 November 2010, Pages 408-413\n- Marie et al. Dap160/Intersectin Scaffolds the Periactive Zone to Achieve High-Fidelity Endocytosis and Normal Synaptic Growth. Neuron. Volume 43, Issue 2, 22 July 2004, Pages 207-219\n- George Zanazzi & Gary Matthews. The Molecular Architecture of Ribbon Presynaptic Terminals.Mol Neurobiol (2009) 39:130-148\n- Gersdorff H. and Matthews G. (1994) Dynamics of synaptic vesicle fusion and membrane retrieval in synaptic terminals. Nature. Vol 367. 735-739","Individual differences |\nMethods | Statistics | Clinical | Educational | Industrial | Professional items | World psychology |\nBiological: Behavioural genetics · Evolutionary psychology · Neuroanatomy · Neurochemistry · Neuroendocrinology · Neuroscience · Psychoneuroimmunology · Physiological Psychology · Psychopharmacology (Index, Outline)\nMost animal cells take up portions of their surface plasma membranes in a process called endocytosis. The main route of endocytosis is the coated pit which buds into a cell to form a cytoplasmic vesicle — a clathrin-coated vesicle. The membrane so internalised is processed in a series of intracellular organelles which include endosomes and lysosomes. Some of this membrane, maybe most of it, is returned to the cell surface by a process which is almost the opposite of endocytosis: this is called exocytosis. The whole cycle of endocytosis plus exocytosis is known as the endocytic cycle.\nThe endocytic cycle is crucial for the well-being of individual cells and multicellular organisms and has an important role in synapse functioning.\nImpulses between neurons are transmitted by the release of neurotransmitters at the junction between the two cells, a region called a synapse. This release is effected by exocytosis at the presynaptic terminal. A vesicle full of transmitter, acetylcholine (for example), in the presynaptic terminal fuses with its neighbouring plasma membrane and thereby releases a burst of acetylcholine into the synaptic space. The acetylcholine is rapidly degraded here, but before this happens it activates acetylcholine receptors on the postsynaptic terminal and triggers an electrical impulse in that cell. The membrane added to the presynaptic terminal is recovered by endocytosis and recycled to form fresh vesicles full of neurotransmitter, ready for another cycle of postsynaptic excitation.\nThe function of the nervous system is thus dependent on this endocytic cycle. A nice example of this dependence is found in fruit flies. A key protein required for endocytosis is dynamin: it assists in budding a coated pit into a cell to form a coated vesicle. Mutations in the dynamin gene exist in which the activity of the dynamin protein is lost at above-normal temperatures (for the fly): these are called temperature sensitive mutations. Such mutant flies have the property that, when the fly is brought from its normal 22°C to 30°C, the dynamin function is lost: however, when the flies are cooled to 22°C, it is regained. In other words, in these mutant flies, the endocytic cycle can be turned off at 30°C, and turned back on by cooling. What one observes is that, within seconds of warming to 30°C, the fruit flies become paralysed: they drop out of the air and appear near-dead. On cooling, they slowly get up, flutter their wings and fly away. The endocytic cycle has been temporarily suspended with drastic effects!\nNon-polarised and polarised endocytic cyclesEdit\na normal endocytic cycle: coated pits 'bud in' from all over the cell’s surface in a random fashion and the returned membrane is exocytosed at the cell’s surface, also at random.\nAnimal cells, such as fibroblasts, as grown in culture in the laboratory are usually stationary; they grow and divide, but rarely move about. They have\nMoving cells, such as fibroblasts, are arranged quite differently. Endocytosis by coated pits occurs, as in stationary cells, at random. But in motile cells exocytosis now occurs at the front of the cell: it is here that both LDL- and transferrin-receptors emerge from inside the cell and return to the cell surface. As the sites of endocytosis (at random on the cell surface) and exocytosis (at the front edge of the cell) are separated in space, it follows that (within the cell’s context) there is a flow of membrane from the front rearwards.\nThe consequences of this polarised endocytic cycle are profound:\n• The membrane added at the front of the cell is believed to provide the surface there for the cell to extend itself forward as it moves.\n• Evidence indicates that those molecules in the cell’s surface which act as the feet of the cell — the integrins which attach the cell to the substratum — can also be endocytosed and transported through the cell. In this way fresh adhesion sites (see cell adhesion) are provided at the cell’s front.\n• The flow of membrane from the front rearwards is not a flow of average plasma membrane proteins: the membrane internalised is a subset of plasma membrane proteins, such as LDL or transferrin receptors and the lipid bilayer in which they sit. These recycling molecules appear at the front surface of the cell and diffuse about, drifting rearwards until they are recaptured by a coated pit and transported back, through the cell, to the front. Other proteins do not participate in this cycle: therefore, they experience a slow rearward flow of the lipid bilayer in which they reside. They are thus subject to two different influences: (a) they tend to get swept backwards in this lipid flow and (b) they tend to randomise their distribution on the cell surface by Brownian motion. Calculation shows that, for non-cycling proteins, diffusion is the more important influence, so these molecules would be expected to have a near-random distribution on the cell’s surface. However, if a large object were attached to the cell surface which was unable to diffuse against the flow, it would be expected to be swept backwards by the flow towards the trailing end of the cell. Indeed, it would act as a marker for that flow. This is the reason why carbon particles attached to the dorsal surface of a moving cell, or why aggregates of surface proteins, are seen to move to the back of a cell. This process is known as cap formation.\nFurther support for this scheme comes from a study of yeast cells (S. cerevisiae) which undergo a primitive form of movement called shmooing (after Al Capp's shmoo). A protein which cycles rapidly would be expected to be most concentrated near the shmoo tip of such a cell, whereas a non-cycling protein would be expected to be swept backwards slightly. Both these distributions have been observed for a cycling and non-cycling variant of the same protein in the plasma membrane of yeast cells as they shmoo.\n• The molecular feet of a cell (usually integrins), when bound to the substratum, cannot diffuse about. Like any other macroscopic object sitting in the lipid flow, they are thus pushed backwards. However, they cannot move backwards (they are attached to the fixed substratum)) and therefore push the cell forward. This may be the force against the substratum which enables a cell to move forward.\n• Polarised endocytic cycles are believed to exist in other cellular contexts but the evidence for them at present is less clear.\n|This page uses Creative Commons Licensed content from Wikipedia (view authors).|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:f439abdd-32e8-4517-800a-f86235014da3>","<urn:uuid:df4845bd-d163-4692-b860-b0b3a08266fa>"],"error":null}
{"question":"I need to write about construction costs - combien d'argent (how much money) was spent building the Acropolis Museum versus the Sharp Centre pour le Design?","answer":"The Acropolis Museum cost $181 million to build, while the Sharp Centre for Design's construction cost was $42.5 million, making the Acropolis Museum approximately $138.5 million more expensive to construct.","context":["The new Acropolis Museum\nwill open today. It is located 300 meters from the famous ruins, and cost $181 million to build.\nGreek President Dimitrios Pandermalis wrote: The new Acropolis Museum was designed with two objectives: the first to offer the best conditions for the exhibition of its exhibits and secondly to be a Museum that welcomes and befriends its visitors. A walk through its galleries is a walk through history between the masterpieces of the Archaic and Classical periods, but also in the ancient neighborhoods of Athens. The Museum offers many opportunities for rest and recreation, as well as a visitor friendly environment for some of the most emblematic works of antiquity.\nThe monuments of the Acropolis have withstood the ravages of past centuries, both of ancient times and those of the Middle Ages. Until the 17th century, foreign travelers visiting the monuments depicted the classical buildings as being intact. This remained the case until the middle of the same century, when the Propylaia was blown up while being used as a gunpowder store. Thirty years later, the Ottoman occupiers dismantled the neighboring Temple of Athena Nike to use its materials to strengthen the fortification of the Acropolis. The most fatal year, however, for the Acropolis, was 1687, when many of the buildings architectural members were blown into the air and fell in heaps around the Hill of the Acropolis, caused by a bomb from the Venetian forces. Foreign visitors to the Acropolis would search through the rubble and take fragments of the fallen sculptures as their souvenirs. It was in the 19th century that Lord Elgin removed intact architectural sculptures from the frieze, the metopes and the pediments of the building.\nIn 1833, the Turkish garrison withdrew from the Acropolis. Immediately after the founding of the Greek State, discussions about the construction of an Acropolis Museum on the Hill of the Acropolis began. In 1863, it was decided that the Museum be constructed on a site to the southeast of the Parthenon and foundations were laid on 30 December 1865.\nThe building program for the Museum had provided that its height not surpasses the height of the stylobate of the Parthenon. With only 800 square meters of floor space, the building was rapidly shown to be inadequate to accommodate the findings from the large excavations on the Acropolis that began in 1886. A second museum was announced in 1888, the so-called Little Museum. Final changes occurred in 1946-1947 with the second Museum being demolished and the original being sizably extended.\nBy the 1970s, the Museum could not cope satisfactorily with the large numbers of visitors passing through its doors. The inadequacy of the space frequently caused problems and downgraded the sense that the exhibition of the masterpieces from the Rock sought to achieve.\nThe Acropolis Museum was firstly conceived by Constantinos Karamanlis in September 1976. He also selected the site, upon which the Museum was finally built, decades later. With his penetrating vision, C. Karamanlis defined the need and established the means for a new Museum equipped with all technical facilities for the conservation of the invaluable Greek artifacts, where eventually the Parthenon sculptures will be reunited.\nFor these reasons, architectural competitions were conducted in 1976 and 1979, but without success. In 1989, Melina Mercouri, who as Minister of Culture inextricably identified her policies with the claim for the return of the Parthenon Marbles from the British Museum, initiated an international architectural competition. The results of this competition were annulled following the discovery of a large urban settlement on the Makriyianni site dating from Archaic to Early Christian Athens. This discovery now needed to be integrated into the New Museum that was to be built on this site.\nIn the year 2000, the Organization for the Construction of the New Acropolis Museum announced an invitation to a new tender, which was realized in accord with the Directives of the European Union. It is this Tender that has come to fruition with the awarding of the design tender to Bernard Tschumi with Michael Photiadis and their associates and the completion of construction in 2007.\nToday, the new Acropolis Museum has a total area of 25,000 square meters, with exhibition space of over 14,000 square meters, ten times more than that of the old museum on the Hill of the Acropolis. The new Museum offers all the amenities expected in an international museum of the 21st century.","- Project plans\n- Project activities\n- Legislation and standards\n- Industry context\nLast edited 15 May 2018\nThe Sharp Centre for Design\nThe Sharp Centre for Design is part of the Ontario College of Art and Design (OCAD). It was created as part of a wider project to allow the college to expand, and can be found in the heart of Toronto, a kilometre north of the famous CN Tower.\nIt was designed by Alsop Architects in a joint venture with Toronto-based Robbie/Young + Wright. They were selected in 2000, and the $42.5 million construction started in 2002. It was completed in October 2004.\nThe Sharp Centre for Design is formed by a rectangular structure 26m above the ground, supported by twelve brightly-coloured steel columns, located directly over the oldest building on the Campus. The building was raised above the ground to preserve views of Grange Park through McCaul Street. The free area below the building houses a plaza, providing access to, and an extension of the park.\nIts 84m long, 31m wide, 9m high ‘table-top’ form is created by a steel box truss clad in a ‘pixelated’ black and white aluminium skin. This contains two stories of accommodation, including art studios, teaching spaces, meeting rooms, exhibition spaces and offices. It is connected to the existing building by a lift and stair core and sits on twelve 28m-long tapered steel columns supported by reinforced concrete drilled caissons.\nIn an interview in 2013, architect Will Alsop said, “Very few people say that they like it. And if they don’t like it, they really hate it, and I think that is good. Hardly anybody can remain indifferent about it. But I think it does the job in all sorts of ways. Particularly the inside of the building is received well by the students.”\nThe Sharp Centre for Design has won a number of awards, including an RIBA international award in 2004, an Excellence Award by Toronto Architecture and Urban Design in 2005, and a Canadian Consulting Engineering Award in 2005.\nThe project team included:\n- Architects: Alsop Architects.\n- Local Architects: Robbie Young+Wright Architect.\n- Engineers: Carruthers & Wallace Limited.\n- Project Manager and main contractor: PCL Constructors Canada Inc.\n- Steel Fabricator: Walters Inc.\n- Funding: OCAD, benefactors Rosalie and Isadore Sharp (after whom the building is named), and the Ontario province.\n Find out more\n Related articles on Designing Buildings Wiki\nFeatured articles and news\nBSRIA report suggest the European market will double to 415 million Euros by 2023.\nDo you understand the different types of stone and which ones you should use where?\nWhy a wellbeing strategy is vital for property managers.\nAn ECA briefing for members about the commercial implications of leaving the EU.\nA crucial moment on any project - and fraught with danger.\nThe performance gap from a Northern Ireland perspective.\nBook review: Buildings of protestant nonconformity.\nDesign and testing for health and wellbeing - free download from BRE.\nRetention in construction contracts.\nCampaign for the reform of cash retentions.\nThe key points for the construction industry and BSRIA's response."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:066b7843-eca5-4454-a619-d042d8d3cb9b>","<urn:uuid:359bc99c-07cf-478f-bef2-a796f16eb9f8>"],"error":null}
{"question":"What are the benefits of implementing AI systems in complex business operations, and what cybersecurity risks should companies be aware of when adopting AI technology?","answer":"AI systems offer several benefits in complex business operations, including bringing order to digital data, helping with searches, accelerating internal administrative processes, and simplifying collaboration and communication within organizations. AI enables companies to filter knowledge from big data and gain competitive advantages. However, companies must be aware of significant cybersecurity risks when adopting AI. These include potential false sense of security from machine learning systems that could be compromised by hackers mimicking safe code, increased workload for human analysts who need to review AI-flagged issues, and the risk of hackers using AI for attacks, as demonstrated by researchers who used AI to guess 25% of LinkedIn passwords. Companies need to ensure they can explain how their AI operates, understand vendor models, test AI security independently, and develop proper AI cybersecurity skills to manage these risks effectively.","context":["Raw materials are necessary to manufacture a product. In order to provide a service, information about the customer is required. And marketing and sales cannot take on any work without something to offer, either. There are many different factors dependent on each other. Most workflows in companies today are complicated at the very least.\nGlobalisation and digitisation complicate workflows further, but also bring new opportunities and challenges. Even small and medium-sized enterprises (SMEs) are internationally active these days: they are producing components for car manufacturers in China or expanding with branches in the USA. Profit is only one reason: they also capture markets and strengthen their competitive position. But the more departments, partners and customers there are involved, the greater the organisational effort required within the company. This makes it hard for those responsible in management to keep track of all relevant factors.\nHow does complexity occur in a company?\nA system is considered complex if it contains factors that influence each other and are partly unknown. For many SMEs that are experiencing growth, new markets are the great unknown. At the same time, their own market is highly agile. Handling and managing digital data is also increasingly gaining importance. That is why most companies today, from major car manufacturers to small start-ups, work in complex systems.\nThe four different system models:\n- Simple systems contain a small number of factors to manage. In this case, A leads directly to B. The system is predictable and stable.\n- Complicated systems comprise several known factors that influence each other. The system is comprehensible and arranged logically.\n- Complex systems contain factors that are still unknown. This makes the workflows difficult to plan. In addition, there are ever more possible ways to complete those workflows. However, this makes complex systems highly flexible.\n- In chaotic systems most factors are unknown, highly dynamic or far too similar. The system does not display any clear pattern and is therefore not plannable and almost impossible to control.\nComplexity in everyday work of SMEs\nThe market is vibrant. The structures within the company should be, too. Complexity management helps SMEs remain sustainable, agile and flexible. It covers five areas.\n- Value chain: is dependent on suppliers, partner companies, logistics, marketing, sales, etc.\n- External: includes the market, customers and competition.\n- Internal: defines the company’s own organisation, management and production, applied technology and the entire value chain.\n- Variant management: coordinates the product range. The more products and product variants there are on the market, the more complex are for example production, sales or marketing.\n- Product: includes all production factors. The aim is to produce goods or provide services as profitably as possible.\nEverything under control again: complexity management\nClassic management models coordinate workflows through clear rules and specifications: they give employees security and ensure smooth interaction with partners and customers. These systems, which are often simple or complicated, focus on standardisation and controls.\nAs the dynamic in companies today is based on many more factors, such fixed management strategies no longer apply. Management requires a flexible system where not only one path leads to the destination.\nComplexity management is an approach to coordinate processes and workflows in companies. The method connects known processes with each other in a new way and allows a change to complex systems in which uncertainties or probabilities such as upcoming complexity costs are included in planning. The aim of this approach is to reduce, control and prevent complex processes.\nAims of complexity management\nGaining control over the costs of complexity\nThe more extensive the workflows in a complex system, the greater the effort involved and, therefore, also the costs. This is additional output due to unpredicted events or unplanned projects. An anticipatory calculation includes bigger additional expenses.\nThere are three kinds of complexity costs\n- Direct: for example, production is more expensive than expected.\n- Indirect: such as unplanned, but necessary, expansion of the existing range in order to meet customer desires.\n- Product-proportional: for example, necessary developments of new offers in order to remain competitive.\nInternal and external factors generate complexity costs. Potential internal factors include a large number of new employees or necessary product developments. External factors include expansion into other markets or changes in consumer behaviour of customers to which the company has to adjust.\nManaging complexity, the team responsible attempts to compensate for accrued costs and include them in future calculations. At the same time, the team develops measures to reduce the additional effort as far as possible and to optimise the efficiency of workflows, without blundering into a complexity trap in the process.\nThe complexity trap\nAs soon as managers notice that the workflows are too complex, they tend to take a drastic approach: they reduce processes by cutting tasks and budgets in their projects or only locating problems at individual points. Instead of managing the complexity, they try to transfer their complex workflows into simpler systems. However, such measures only have an effect on individual departments, not the company as a whole.\nThese partial solutions can have additional consequences and make the long-term complexity costs even higher. Only a holistic strategy and comprehensive measures in the entire company improve workflows and sustainably control necessary complexity.\nExample measures in complexity management\n- Create transparent processes\n- Share out responsibility\n- Involve staff in new workflows at an early stage\n- Establish modern IT tools and new process models\n- improve efficiency of production\n- Reduce and simplify product diversity\n- Accelerate logistics\n- Avoid long storage times\n- Focus on service rather than covering all areas\nMore courage for complexity\nThe topic of complexity does not only have negative connotations. It can even be a good strategy for companies to consciously choose complex business models.\nFor example, some companies secure the chain of production against delays: if a partner cancels a delivery, another one fills the gap. That is why for certain components the automotive sector always relies on several suppliers.\nComplex production models also protect technical innovations against piracy. Small components in particular are often targeted by counterfeiters. Nobody fakes a car, but they do so with spare parts. A team from four Frauenhofer Institutes has developed a fluorescent dye for such cases. It is only visible at a certain light wavelength. When it is applied to products, it protects them against forgery. Many companies are giving themselves a long-term competitive advantage with elaborate manufacturing processes such as this.\nHandling big data is another example of a complex system. Every company collects digital data these days, but its processing cannot be simplified. More data means more information – about customers, competition, the market and workflows within a company. If you can make use of the knowledge, you are also ahead of the competition. Tools help you manage big data without IT barriers being created.\nIntelligent and automated IT systems are required to get that information out of the data. Artificial intelligence, AI for short, brings order to the digital data and makes it possible to filter knowledge. It helps with searches, accelerates internal administrative processes and simplifies collaboration and communication within organisations.\nSimplify complexity: agile teams and transparent management\nAgility is another contemporary management solution to dealing with complex systems. The strategy originated in the IT sector. Experts in managing complexity have adapted it to other professional branches. Many companies already put their faith in embodying everyday flexibility. They use agile methods such as scrum or kanban: this means more responsibility for individual work and smaller teams that manage themselves and can work together independent of time and place thanks to collaboration platforms.\nThese new strategies and technical assistance change the management of employees: transparency and cooperation rather than a rigid chain of command. Dynamism rather than gridlock. Alternating roles rather than fixed processes. Fewer rules, more freedom. And ultimately, also simplified workflows and manageable data.","Your company has started to use Artificial intelligence (AI), but are you effectively managing the risks involved? It’s a new growth channel with the potential to boost productivity and improve customer service. However, particular management risks need to be assessed in cybersecurity. Start by considering AI trends to put this risk in context.\nWhy Is AI an Emerging Cybersecurity Threat?\nArtificial intelligence is a booming industry right now with large corporations, researchers, and startups all scrambling to make the most of the trend. From a cybersecurity perspective, there are a few reasons to be concerned about AI. Your threat assessment models need to be updated based on the following developments.\nEarly Cybersecurity AI May Create a False Sense of Security\nMost machine learning methods currently in production require users to provide a training data set. With this data in place, the application can make better predictions. However, end-user judgment is a major factor in determining which data to include. This “supervised learning” approach is subject to compromise if hackers discover how the supervised process works. In effect, hackers could evade detection by machine learning by mimicking safe code.\nAI-based Cybersecurity Creates More Work for Humans\nFew companies are willing to trust their security to machines. As a result, machine learning in cybersecurity has the effect of creating more work. WIRED magazine summarized this capability as follows: “Machine learning’s most common role, then, is additive. It acts as a sentry, rather than a cure-all.” As AI and machine learning tools flag more and more problems for review, human analysts will need to review this data and make decisions about what to do next.\nHackers Are Starting to Use AI for Attacks\nLike any technology, AI can be used for defense or attack. Researchers at the Stevens Institute of Technology have demonstrated that fact. They used AI to guess 25% of LinkedIn passwords successfully after analyzing 43 million user profiles in 2017. In the hands of defenders, such a tool could help to educate end users on whether they’re using weak passwords. In the hands of attackers, this tool could be used to compromise security.\nThe Mistakes You Need to Know About\nAvoid the following mistakes, and you’re more likely to have success with AI in your organization.\n1. You Haven’t Thought Through the Explainability Challenge\nWhen you use AI, can you explain how it operates and makes recommendations? If not, you may be accepting (or rejecting!) recommendations without being able to assess them. This challenge can be mitigated by reverse engineering the recommendations made by AI.\n2. You Use Vendor-provided AI Without Understanding Their Models\nSome companies decide to buy or license AI from others rather than building the technology in house. As with any strategic decision, there’s a downside to this approach. You can’t trust the vendor’s suggestions that AI will be beneficial blindly. You need to ask tough questions about how the systems protect your data and what systems AI tools can access. Overcome this challenge by asking your vendors to explain their assumptions about data and machine learning.\n3. You Don’t Test AI Security Independently\nWhen you use an AI or machine learning tool, you need to entrust a significant amount of data to it. To trust the system, it must be tested from a cybersecurity perspective. For example, consider whether the system can be compromised by SQL injection or other hacking techniques. If a hacker can compromise the algorithm or data in an AI system, the quality of your company’s decision making will suffer.\n4. Your Organization Lacks AI Cybersecurity Skills\nTo carry out AI cybersecurity tests and evaluations, you need skilled staff. Unfortunately, there are relatively few cyber professionals who are competent in security and AI. Fortunately, this mistake can be overcome with a talent development program. Offer your cybersecurity professionals the opportunity to earn certificates, attend conferences, and use other resources to increase their AI knowledge.\n5. You Avoid Using AI Completely for Security Reasons\nBased on the previous mistakes, you might assume that avoiding AI and machine learning completely is a smart move. That might’ve been an option a decade ago, but AI and machine learning are now part of every tool you use at work. Attempting to minimize AI risk by ignoring this technology trend will only expose your organization to greater risk. It’s better to seek proactive solutions that leverage AI. For instance, you can use security chatbots such as Apollo to make security more convenient for your staff.\n6. You Expect too Much Transformation from AI\nGoing into an AI implementation with unreasonable expectations will cause security and productivity problems. Resist the urge to apply AI to every business problem in the organization. Such a broad implementation would be very difficult to monitor from a security point of view. Instead, take the low-risk approach: apply AI for one area at a time, such as automating routine security administration tasks, and then build from there.\n7. Holding Back Real Data from Your AI Solution\nMost developers and technologists like to reduce risk by setting up test environments. It’s a sound discipline and well worth using. However, when it comes to AI, this approach has its limits. To find out whether your AI system is truly secure, you need to feed it real data: customer information, financial data, or something else. If all this information is held back, you’ll never be able to assess the security risks or productivity benefits of embracing AI.\nAdopt AI with an Eyes Wide Open Perspective\nThere are certainly dangers and risks associated with using AI in your company. However, these risks can be monitored and managed through training, proactive management oversight, and avoiding these seven mistakes."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:fded2531-94a2-46a8-a169-d457a8e5ff6d>","<urn:uuid:75c20593-63bc-45e3-ac40-3a0820eb44e8>"],"error":null}
{"question":"As someone working in homeless services, I'd like to know how the Emergency Shelter of Northern Kentucky and Safe Haven Youth Services differ in their age requirements for providing shelter?","answer":"The Emergency Shelter of Northern Kentucky serves adults, while Safe Haven Youth Services specifically serves young people between 12 years of age and up to their 18th birthday. This reflects different specializations in addressing homelessness across age groups.","context":["Emergency shelter helps homeless in N.Ky.\n“Homeless people can be anybody,” said Melissa Collier of Covington.\nWhen problems with the company where her husband worked left them no place to go, they stayed at the Emergency Shelter of Northern Kentucky for two weeks. They recently managed to find new jobs and a new place.\n“Rachael helps people find jobs,” Collier said of Rachael Winters, the shelter’s director. “She gives you that extra push. Some people, when they’ve been homeless for so long, can get depressed.”\nRachael Winters has been working with the shelter, on Scott Street in Covington, since 2008. To help them escape homelessness, she said, some basic needs must be met.\nThose basics include safety, food, clothing, sleep, a shower, and laundry. “The next level would be a job,” she explained, adding, “some homeless people are not capable of working.”\nWinters named poverty and the lack of social support as the top reasons people become homeless.\n“They’ve lost family and friends to stay with, and they don’t have the economic means to rent an apartment or stay in a hotel,” she said.\nWilliam J. Carter lost his family in Hurricane Katrina. Having difficulty dealing with his loss, he decided to get away.\nHe was hitchhiking from Florida when he had a heart attack. When he arrived in in this area, someone took him to the shelter. “It saved my life,” he said.\nCarter has been living in his own apartment in Covington for the last six months. “I’ve got everything together, and I got my retirement started,” he said. He still comes to the shelter 10 hours a week to volunteer.\n“Rachael helped me with her Men’s Homeless Recovery Program,” said Angel Andino-Gautier Jr. “I learned how to be structured, to take a responsibility, and to make the right choices.”\nWhile in his teens, he became a ward of the state. This led to 15 years of homelessness and addiction to alcohol and drugs.\nDuring the shelter’s fundraiser called “Homeless to Hopeful,” Feb. 28 at the Notre Dame Academy in Park Hills, Gautier stood on stage and shared his story with hundreds of people.\n“The recovery program gave me the strength to once again find sobriety and achieve employment and housing,” he said in his speech. “I have not taken any cocaine or alcohol for almost four years.”\nSobriety, he said, must start with a desire. “You have to want to stop. You have to believe that you can stop using alcohol and drugs.”\nAt the age of 45, he’s even gone back to school, studying at Gateway Community and Technical College.\nThese are success stories, but there are still many people without a home. Nearly 400 adults have stayed in the shelter during this frigid winter. “That’s a 17 percent increase over last year’s numbers,” Winters pointed out.\nThe shelter has never rejected anyone. “We are supposed to accommodate 42 people,” she said. “But we’ve had an average of 75 people nightly this winter.”\nThey manage to squeeze them into the building, even using Winters’ office. It can take about $12,000 a month to run the shelter in winter. “It would be nice if every county in Northern Kentucky would consider having a shelter,” she said.\nChelsea Fite of Union, a Ryle High School junior, has been volunteering every Wednesday since last August.\n“It’s the best part of my week,” she said. “I can actually interact with the people in need and help them.”\nWinters said the volunteers sometimes get something out of working at the shelter. “Often volunteers express to me that they were helped themselves in the process and experienced healing in their own lives for a particular issue.”\nFYIThe Emergency Shelter of Northern Kentucky serves adults, and Family Promise of Northern Kentucky provides shelter to homeless families. Anyone who would like to volunteer or make donations can visit www.emergencyshelternky.org or www.familypromisenky.org.","Safe Haven Youth Services provides crisis, respite and life launch services to youth between the ages of 12 and up to their 18th birthday living in Waterloo Region.\nOur qualified and caring staff help youth develop life skills such as conflict resolution, emotion management and relationship building strategies. We support youth to continue their education and learn about health and wellness, and we connect them to a variety of community-based services to ensure ongoing support.\nBased on individual needs and eligibility, supports are also available for family reunification and for securing sustainable housing in the community. Safe Haven Youth Services strives to provide every youth with the resources and support necessary to address their immediate crisis and to build the skills required to prevent reoccurrence.\nCrisis Services offers immediate admission to our shelter on a 24 hour/7 day a week basis to youth that are experiencing a crisis and need a safe place to stay. This program helps prevent further crisis, reduces the risk of homelessness, and provides support to develop a positive resolution so youth return to their home or find an alternative safe place to live. We help youth and their families/guardians find strategies to work through their immediate crisis and connect them to community resources for ongoing support.\nEligible youth are between 12 years of age and up to their 18th birthday and may [self-refer or be referred] (link to how to access page) by family, schools, hospitals, police and health and social service agencies. Youth, aged 16 and 17 who do not have a parent/guardian to return to, may also be provided with Life Launch Services.\nRespite Services offers parents/caregivers and youth a break by providing short-term temporary care of youth who are experiencing difficulties within their home environments. Pre-planned one- or two-night stays at Safe Haven help reduce the amount of crisis occurring within the home while the youth is on the waitlist for or accessing a treatment program, or while the youth is transitioning home from a treatment or community program. Youth are provided with opportunities to learn positive life and social skills which benefit their wellbeing and relationships at school, in the community and with their family.\nEligible youth are between 12 years of age and up to their 18th birthday and may be referred by health and social service agencies, schools and the family.\nLife Launch offers shelter and supports to youth who have accessed our Crisis Service and are looking for long term housing. Staff will support youth to develop a housing plan, find suitable housing options and connect them with other resources within the community. Youth will also be supported to develop positive life and social skills to help them successfully live independently.\nYouth are eligible for this service from the age of 16 up to their 18th birthday and can access this service through our Crisis Services.\nHow To Access\nReferrals are accepted 24 hours a day by calling 519-749-1450.\nServices are supported in whole or in part by:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:e95dccef-3c39-4c1d-9bb1-00aac9fe3e4e>","<urn:uuid:4f4b9b88-c3e9-452d-b547-d58ed856d53e>"],"error":null}
{"question":"Looking to understand different medical evaluations - how do biomechanical evaluations and hearing tests differ in their results delivery and professional communication methods?","answer":"For biomechanical evaluations, patients receive a verbal summary immediately after the evaluation, followed by an electronic report within 2 weeks. Video recordings can be provided if the patient brings a flash drive, and results can be shared with healthcare providers upon request. For hearing tests, results are displayed immediately in the form of an audiogram, which is a graph showing hearing ability by plotting the softest sounds detectable at different pitches. The audiologist explains everything in detail during the appointment to ensure patient understanding.","context":["Frequently Asked Questions\nWhat is biomechanics?\nBiomechanics is the scientific discipline that applies the principles of mechanics to living organisms. With respect to the human body, biomechanics incorporates the concepts of engineering and physics in order to understand normal and pathological motion, physical performance, and mechanisms of injury.\nHow do I know if I need a biomechanical evaluation?\nAnyone who has difficulty moving or experiences pain during physical activity is a candidate for a biomechanical evaluation. For persons with pain, a biomechanical evaluation will determine if abnormal movement is contributing to your symptoms. Such an evaluation particularly is indicated when conventional forms of treatment have failed (i.e. surgery, injections, medications, chiropractic, physical therapy, etc).\nWhat does a biomechanical evaluation entail?\nDepending on the type of evaluation being performed, you likely will be asked to perform activities similar to those that cause your symptoms. Walking and running evaluations will be performed on an instrumented treadmill. For return to sport evaluations, be prepared to perform activities consistent with your sport (i.e. jumping, cutting, lateral movements, etc.). In addition to evaluating your movement, we also may evaluate your muscle strength, flexibility, joint range of motion, etc.\nWhat will a biomechanical evaluation tell me?\nUpon the conclusion of your evaluation, you will have an understanding of the potential biomechanical sources of your pain or movement dysfunction. You also will have an understanding of the causes of the observed movement impairments. Based on the information obtained during your examination, we will make treatment recommendations to remedy the deficiencies noted.\nWhat should I wear to my appointment?\nTo properly analyze your movement, we will need to see your joints, the top of your pelvis and trunk. Therefore, please avoid wearing pants or long, baggy shorts and shirts. Your shorts may need to be taped up so we can see the appropriate bony landmarks. Men may be asked to take their shirt off for the evaluation. For women, a sports bra may be recommended. Please wear the same type of shoes you would typically use when you walk or run.\nHow long does a biomechanical evaluation take?\nPlease allow 2 hours for your evaluation.\nDo I need to do anything to prepare for my evaluation?\nThere is no need to prepare for your evaluation. However, it is important to avoid vigorous activity a day or two before your examination. Please bring any pertinent medical records to your appointment, including MRI, x-rays, etc.\nHow much does a biomechanical evaluation cost?\nThe cost of your evaluation will depend on the type of testing being performed. Typical charges range from $300-$500.\nWill insurance cover the cost of the testing?\nThe Movement Performance Institute does not accept insurance for biomechanical testing. However, we will provide you with an invoice with the necessary billing codes that you can submit to your insurance carrier. Insurance plans vary considerably, so please contact you insurance company to see if the cost of your testing is reimbursable.\nWhen will I get the results of my evaluation?\nYou will receive a verbal summary of your results immediately following your evaluation. An electronic report will be provided within 2 weeks. If you would like copies of your videos, please bring a flash-drive and we will provide them to you.\nWill you communicate the results of my evaluation with my physician, physical therapist, or trainer?\nUpon your request, we will provide a copy of your report to the person(s) of your choosing. We are also available to talk directly with your health care provider to discuss your plan of care.","From audiogram to tinnitus, find all the terms commonly used for hearing loss and hearing aid technology.\nBasic Free Hearing Test\nThe very first step to better hearing is to get a free basic hearing test to determine your hearing levels. It only takes about 20 minutes. Best of all, it won’t cost you anything. If hearing loss is detected, then we will recommend further assessment with a qualified audiologist to see what you need to do next.\nDiagnostic Hearing Test\nThe diagnostic hearing test will determine the type and extent of your hearing loss and find the most suitable options for you. The results are immediate and at this appointment the audiologist will take the time to explain everything in detail, so you feel confident in the knowledge and options available to you.\nA hearing test performed by a hearing health care professional comprises a number of tests which can help to determine whether or not a client is suffering from hearing loss. The results from the hearing tests are often displayed in the form of an audiogram, which is a graph that gives a detailed description of your hearing ability by plotting the softest sounds you can hear at a range of different pitches.\nBehind-The-Ear (BTE) Hearing Aid\nThis is the most traditional model: a small, curved case that fits behind the ear with a thin, transparent tube that runs into the ear canal. Newer models are far less visible than previous generation hearing aids. The BTE hearing aid is highly versatile, providing excellent treatment for most forms of hearing loss.\nCompletely-In-The-Canal (CIC) Hearing Aid\nMoulded to fit deep within the ear, CICs are nearly invisible and tend to pick up less external (e.g. wind) noise because they’re protected by the ear itself. See a Bay Audiology clinician to determine if you’re a candidate for this style. Typically appropriate for mild to moderate hearing loss and ears of a size that can accommodate them.\nA decibel is a unit used when measuring sound. The decibel (dB) scale is based on the sounds our ears can hear in increasing intensity. A decibel level of 0 is almost complete silence. Experts agree that any noise over 85 decibels can cause hearing loss when exposed for eight hours or more.\nFeedback refers to the distracting whistling or high ringing sound that sometimes happens to hearing aid wearers because amplified sound leaks out from the speaker and gets back into the microphone forming a feedback loop. Feedback management means that a hearing aid is equipped with an advanced system to reduce occurrences of feedback.\nMulti programme hearing aid\nThis simply means that you can control the amplification settings of your hearing aid based on the sounds in your environment. The hearing aid can also store those settings so that you can easily switch between environments such as work, home, and your favourite restaurant, for example, without having to readjust the settings each time. Furthermore, some hearing aids can even learn your preferred settings and switch between programs automatically as you change environments.\nA hearing aid is a small digital device that sits in or behind the ear and aids an individual who has hearing loss. Hearing aids work by processing and amplifying sounds.\nInduction Loop Compatibility\nMany public settings, such as theatres, stadiums, and public transport stations, are equipped with induction loop systems. In these systems, microphones transmit sound to a permanently installed induction loop wire (usually located in the ceiling or under the carpet), thus generating a current and creating an electromagnetic signal. When used on a specific setting, hearing aids can pick up this signal directly and wearers can adjust the volume as desired. The effect is the same as having a sound transmitted sound directly to your ears by your hearing aid. You need to choose a hearing aid that contains an electromagnetic coil to use this feature, so be sure to tell your audiologist all about your lifestyle when discussing options.\nIn-The-Canal (ITC) Hearing Aid\nSmaller than an ITE and very discreet, the ITC fits partly into the ear canal. Due to its small size, the ITC comes with the option of remote control accessories for easy adjustment. Best for people with mild to moderate hearing loss.\nIn-The-Ear (ITE) Hearing Aid\nThis small device is worn inside the outer ear and designed to match the wearer’s skin tone. It is discreet, yet offers many features and options that smaller hearing aids can’t. Best for people with mild to moderately severe hearing loss.\nThis is an inner ear disorder that causes episodes of vertigo (spinning) along with roaring or buzzing and a blocked feeling in your ears. Your Bay Audiology audiologist can help diagnose this condition.\nThis technology helps to reduce and remove outside noises that may make hearing more difficult, such as: background talking, music, traffic and more. These things are programed to come secondary to the individual you are speaking to, or entertainment you are watching.\nHearing Aid This is a discreet, comfortable design that is easy to fit. The receiver sits inside the ear canal and the body of the hearing aid sits behind the ear similar to the BTE. This hearing aid is a great choice for those with mild to moderate hearing loss who are sensitive to sensations in their ears as its very comfortable with natural sound.\nSensorineural Hearing Loss\nThis type of hearing loss is caused by damage to the nerves in the inner ear and can be treated with hearing aids. There are many different types of hearing loss, learn more here.\nTinnitus is a condition that causes ringing or buzzing in the ears. This condition can be treated or managed with hearing aids. Learn more - Click here.\nWhen unaidable hearing loss in present in one ear, the Wireless CROS and BiCROS solution is available in some hearing aids to allow you to hear sounds from both sides."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:8540f673-fae6-45e7-898f-972acec949ae>","<urn:uuid:c7a084fc-f7f1-4383-ac73-0e770b814c81>"],"error":null}
{"question":"What is the contrast between Brazilian pão francês and authentic sourdough bread regarding their baking temperatures and times?","answer":"Brazilian pão francês is baked at 200°C to 220°C for about 20 minutes, while authentic sourdough, as practiced by experts like Ed Wood, is baked in an unpreheated oven set to 375°F (approximately 190°C) for 1 hour and 10 minutes, with the extra 10 minutes accounting for the oven to reach the desired temperature.","context":["Brazilian cuisine is a reflection of the country’s multicultural population. Depending on the region, eating habits have been shaped by indigenous, European, African or Creole influences. But different as culinary tastes may be among the 212 million inhabitants, there is surprising agreement when it comes to a particular bakery product: Brazilians love pão francês — a light-colored, yeast-raised wheat roll characterized by a tender, flaky crust and a fluffy crumb.\nThese cylindrical rolls are a central feature of meals, especially breakfast. Most Brazilians start the day with cafézino, a strong, sweet espresso, and a crispy pão francês that they spread just with butter or margarine. Those who prefer a more substantial meal fry the halved rolls in a pan with butter to make “pão na chapa.”\nFor midday lunch, the roll is served with soups, stews or salads. And in the evening, with a circle of friends in a restaurant, the Brazilians like to dip into the breadbasket and soak up the sauces from the lavish meat and fish dishes with pão francês.\nMost important sales driver\nBecause freshness has top priority in the purchase of rolls, private households and restaurant owners mostly buy what they need from a total of some 64,000 small- and medium-sized artisan bakeries that are a familiar part of the street scene in Brazil. For these businesses, pão francês is generally the main sales driver and the most important item in their product range.\nCustomers’ expectations in respect of their favorite rolls are clearly defined: the pão francês must have a light, fluffy crumb, a tender, flaky, wafer-thin crust with a pale brown color, a mild taste and a wide split. The weight is between 50 and 60 grams.\nSince the cylindrical rolls are baked close together in the oven, their ends meet as the dough rises. After cooling, these “snakes” of bread are broken apart at the joints to reveal the white, fine-textured crumb. This specific appearance is another feature consumers expect.\nQuality reference for pão francês\nMany bakeries advertise their French bread as the hallmark of their trade. Nevertheless, problems with quality and standardization often occur, sometimes because of a shortage of qualified workers.\nTo offer the industry practical help in improving its production methods, the Brazilian standardization committee ABNT (Associação Brasileira de Normas Técnicas) has developed recommendations for the production of pão francês. Since 2013, the technical standard “ABNT NBR 16170” has specified criteria for evaluating and classifying French bread. Shape, size, properties of the crust, color, crumb structure, shred, odor, flavor… every detail of a perfect pão francês is described precisely in these guidelines. Although the standard is not binding, the initiators hope it will help to ensure that “French rolls” are of uniform, defined quality in all corners of the country.\nPolysorbate as an emulsifier\nIf the rolls are to be light and fluffy inside and have a thin, crisp crust, the flour used must guarantee suitable baking performance. To reconcile quality with economy in the grinding process, the mills generally combine cheaper domestic wheat lots with higher-performance imported wheat. In the past it was the Mercosur countries Argentina, Uruguay and Paraguay that played the most important role, but countries like Russia, Ukraine and other Black Sea countries also have become established trading partners.\nTo standardize flours made up of very different wheat varieties and regions of origin, it is established practice in Brazil to treat the flour with enzymes and additives. The products used include oxidizing agents, amylases, xylanases, hemicellulases and emulsifying additives like DATEM and SSL. A peculiarity of Brazilian food law is the approval of polysorbate, a high-performance emulsifier that has a particularly strong effect on the gas retention capacity and stability of a dough. The additive azodicarbonamide (ADA) is also a much-used improver in Brazil. However, in comparison with other South American countries that permit a dosage of up to 45 ppm, the maximum quantity allowed in Brazil is lower, at 40 ppm.\nEnsuring firm doughs\nBesides a suitable flour, the making-up process for the dough is an important factor for the quality of pão francês. Even at the smallest artisan bakeries it is usual to have a special mechanical roll shaper to replace laborious manual rounding and molding.\nAfter mixing and kneading, the dough is left to ferment only for a short time before processing continues mechanically. On a feeder conveyor belt, the dough pieces are pressed automatically and pass through various roller systems with decreasing gap widths. This results in thin, portioned sheets of dough that are rolled up tightly by several deflectors.\nThe bakers only have to intervene to give the rolled dough portions a finishing touch to their appearance. The loaded proofing trays are then placed in the proofing chamber at a temperature of about 30°C and a relative humidity of 80%.\nThe “French mini-loaves” are baked for about 20 minutes at 200°C to 220°C after being scored lengthwise to create the split.\nFor many Brazilian consumers, the price and freshness of the rolls are still the most important criteria for buying. But generally speaking, the bakeries are noticing increasing expectations on the part of their customers. Fewer consumers are willing to tolerate fluctuations in appearance or taste. On the contrary, there is a demand for high and consistent product quality. Since it is sometimes only necessary to make small adjustments to the recipe or production method to enhance the appearance and sensory properties of pão francês, here is an overview of the most common mistakes made in production, with suggestions for avoiding them:\nProblem: Low bread volume.\nPossible causes: Fermentation time too short; too little yeast; unsuitable flour with too little enzymatic activity.\nSolutions: Prolong the fermentation time of the dough; use more yeast; use suitable flour; increase or adjust flour treatment (Alphamalt VC 5000, Alphamalt HCC 2).\nProblem: Inadequate shred/bloom.\nPossible causes: Fermentation time too long; dough temperature too high; flour too weak; too little enzyme activity.\nSolutions: Shorten the fermentation time; use cold water; use more steam in the oven; increase humidity in the fermentation chamber; use more oxidizing agent (OXEM, Elco P-100); add more enzyme improvers (Alphamalt EFX Mega, Alphamalt H 19480).\nProblem: Dark crust, soft crumb.\nPossible causes: Enzymatic activity too high; oven too hot; baking time too long.\nSolutions: Use wheat flour with lower enzymatic activity; add less alpha-amylase (Alphamalt VC 5000) to the flour; set the oven to a lower temperature; shorten the baking time.\nProblem: Holes in the crumb.\nPossible causes: Flour too weak; doughs too warm; fermentation chamber too warm; too much yeast; mixing and resting times too long.\nSolutions: Use stronger flour; use cold water; set a lower temperature in the fermentation chamber; use less yeast, shorten processing times; adjust flour treatment (use Mulgaprime 10, Alphamalt Gloxy TGO, Alphamalt EFX Mega).\nProblem: Not enough crispiness.\nPossible causes: Dough too firm; flour too weak; baking temperature too low; doughs too dry and bucky; too little enzymatic activity.\nSolutions: Use stronger flour; set the oven to a higher temperature; increase the amount of liquid in the recipe; adjust the enzymatic activity (Increase the flour treatment (Alphamalt GA 23750, Deltamalt FN, Mulgaprime 10).\nProblem: Pale, white crust.\nPossible causes: Fermentation chamber too dry; fermentation time too long; oven temperature too low; baking time too short; too little enzymatic activity.\nSolutions: Increase humidity in the fermentation chamber; reduce the fermentation time; set the oven to a higher temperature; prolong the baking time; add more alpha-amylase to the flour (Alphamalt VC 5000, Alphamalt GA 23750, Deltamalt FN).\nProblem: Blistered crust.\nPossible causes: Flour too weak; incorrect machine setting; fermentation time too long; doughs too soft; fermentation chamber too warm.\nSolutions: Use stronger flour; increase the pressure of the dough molding machine; shorten the fermentation time; reduce the amount of liquid in the recipe; reduce the temperature in the fermentation chamber.\nA Brazilian product with a hint of France\nNo Brazilian consumer expects a traditional French baguette when buying pão francês. The misleading name, which means “French bread,” is thought to have originated in the early 20th century, when the upper classes of Brazil took pleasure in visiting far-off destinations in Europe, especially France. Upon their return, the travelers gave their bakers rapturous descriptions of the delicious bread they had enjoyed in Paris. The typical loaves in France at that time were short and cylindrical, with a firm crumb and a golden-brown crust. They were forerunners of the baguette, which did not acquire its characteristic elongated shape until later. Following these descriptions, Brazilian bakers tried to imitate this European speciality. What emerged was a new bakery product in its own right; it was destined to become an unprecedented bestseller, known as “pão de sal” (salt bread), “cacetinho” (stick bread), “pão careca“ (bare bread) or “filão” (snake bread), according to the region.\nSven Mattutat is a product manager with Mühlenchemie. He may be contacted at firstname.lastname@example.org.","Ed and Jean Wood are two true artisans with a passion for authentic sourdough bread. After meeting in college some 40 years ago, they have spent their lives studying the science of real sourdough, baking and batching the perfect loaf, and traveling the world to uncover the hidden history of sourdough for National Geographic Society. In the process, they formed Sourdoughs International (SDI) and literally wrote the book on the ancient fermentation process and cultures that gives sourdough its unmistakable, authentic taste. Part scientists, food anthropologists, adventurers, and bakers, this is a couple that truly gives new meaning to the term good taste.\nWhat is authentic or ‘real’ sourdough bread?\n“ ‘Authentic’ sourdough bread really means only one thing: it is never made with any commercial yeast of any kind, ever! So what makes it rise? About 5,000, maybe even 10,000 years ago, bakers thought that the gods put something in their dough to make the loaves rise. They knew they had to save dough from one loaf and add it to the next batch to get it ‘started’. At Sourdoughs International and elsewhere, ‘starters’ are usually called ‘cultures’, referring to the microscopic organisms involved, which we ‘culture’. Today we know that it is the fermentation of ancient wild yeasts that make our doughs rise and we still have to add them to every batch.”\nWhy did we ever stop making bread this way?\n“What happened to our ‘staff of life’ that made sourdoughs disappear almost completely? During the so-called Industrial Revolution around the 1920s, researchers developed a method to isolate single yeast cells and grow them in pure colonies with special functions. They chose yeast with an incredibly rapid speed of fermentation to shorten the entire bread making process. With very few exceptions, the quality and flavor of commercial bread, for unknown reasons, was lost for the next 50 years. It wasn’t until almost 1970, when two researchers studying one of those exceptions, San Francisco sourdough, reported that the unique flavor of the bread was due entirely to a previously unknown strain of bacteria. They named that bacteria Lactobacillus sanfrancisco.\n“Unfortunately, that research has yet to improve the quality of most commercially produced bread. Time is money and most commercial bakers choose not to waste it. The lactobacilli require 12 to 24 hours to complete the fermentation of dough and produce that special flavor. Commercial yeasts leaven the dough so fast, it is baked long before the fermentation is complete and the flavor never develops. It isn’t easy to find authentic sourdough on store shelves. One must look very carefully at the label. It isn’t enough to read ‘leavened naturally’, and if the list of ingredients includes ‘yeast’, it’s time to move on. For ‘real’ sourdough, home bakers are finding it necessary to bake it themselves and there are lots of them doing just that!”\nWhat are the health benefits of authentic sourdough as opposed to conventional sourdough or other types of bread?\n“We call sourdough ‘The Gold Standard of Bread’ for good reason. The fermentation of flour by wild yeast and bacteria (usually lactobacilli) produces a myriad of compounds seldom or never found in breads produced by commercial yeast with no lactobacilli or other bacterial components. Sourdough quality, in turn, leads directly to a special quality of life recognized by all home bakers and everyone around them. The fermentation of authentic sourdoughs and the compounds they produce will improve your quality of life – they have improved mine immensely. The personal satisfaction of just creating the aroma that fills the house when baking sourdough is enough to uplift anyone’s spirits. The final product, the bread, is just the icing on the cake, so to speak! The enjoyment of baking is reward enough. I don’t think anyone enjoys buying tasteless commercial bread.”\nHow did you and your wife discover your love for authentic sourdough bread?\n“I met my life partner, Jean, at Oregon State College in Corvallis. She was working on a degree in Pharmacy and I was chasing one in Fish and Game Management. I was batching in an apartment with a classmate from high school and we just about lived on sourdough bread from a culture I had inherited from my grandmother. I think those sourdoughs charmed Jean, too. At least, perhaps they supplied something unique from her other dates. Two years after graduation, we married in Boston. Sourdoughs led us on all sorts of adventures capped some 40 years later when the National Geographic Society sent us to Egypt to help an archaeologist discover how the pyramid builders baked man’s first leavened bread in 4500 B. C. Our biggest discovery was the Giza culture. It leavened those breads 4600 years before we arrived and was still there. That’s another story. (National Geographic, January, 1995).”\nTell us about how you sourced these ancient sourdough cultures.\n“Along the way, we spent two years in the Middle East, where I worked as a pathologist in a Saudi Arabian hospital. That gave us an opportunity to search the surrounding desert for ancient bakeries that had never even heard of commercial yeast. Jean charmed the proprietors while I negotiated for a piece of dough. We returned to the US with 10 priceless cultures, formed Sourdoughs International (SDI) and learned how and how not to bake sourdough.”\nWhat is your advice for someone who wants to begin baking true sourdough bread?\n“We are convinced that proofing temperatures are the single most essential key for producing the different flavors of sourdoughs. To accurately regulate temperatures during the proofing cycles, we designed a simple, relatively inexpensive proofing box from a plastic cooler. This is how to do it:\nThe cooler should be large enough to go over a couple of your mixing bowls. Mine is approximately 21 x 13 x 11 ½ inches deep. A porcelain bulb holder is installed inside at the center of the cooler (the top is not used). An ordinary dimmer switch is wired into the power side of the cord to the bulb holder and that regulates the temperature. A 25 watt bulb is adequate. There is a more complete description in our current text, Revised Classic Sourdough.\nDuring winter is not the best time to find Styrofoam coolers, but large grocery stores often have them piled high on a shelf somewhere. No one really knows where the term ‘proofing’ came from or what it means but you can substitute ‘fermentation’ almost everywhere for ‘proofing’.\nI do something else that usually shocks artisan bakers and almost everyone else until they try it. I do not preheat an oven before using it for baking. When the loaves are ready, I put them into a cool oven, set the temperature control at 375oF, close the door, turn the oven on and bake for 1 hour and 10 minutes. The 10 minutes is how long it takes my oven to reach the desired temperature – yours may be different. I think I get equal or better results including ‘oven spring’ and I don’t like handling pans or other equipment at high temperatures.\nFor the novice interested in the concept and challenge of sourdoughs, I always suggest starting with our Original San Francisco culture. It will do anything you ask of it once you become acquainted. It and our current text are available on line from our web site.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:308dd7d3-f7a4-4e07-ad09-f6e9a6a06d4d>","<urn:uuid:1943af95-c050-42b9-b400-90aa479430a2>"],"error":null}
{"question":"What are the main factors affecting tactile measurements of polymer thickness?","answer":"The main factors affecting tactile measurements of polymer thickness are the probing force and the tip radius of the measuring instruments. Additionally, when measuring viscous materials with time-dependent mechanical properties, different scanning speeds can affect the level of systematic deviations in the measurements.","context":["DIN standard for the determination of the tip radius and the probing force of stylus instruments developed\nIn microsystems metallic components are increasingly being replaced by those from low-cost polymers. For the thickness measurement of polymers, there is now the DIN standard 32567 available, which describes both, optical and tactile surface measuring methods for the precise measurement of the thickness of polymer layers.\nThe standard describes methods by which both, the contact force and the tip radius of stylus instruments can be measured, a basic condition for non-destructive precision tactile profile measurements.\nFrom motion sensor to smartphones - many everyday products increasingly contain parts which are made of polymer materials. The desired operation of these components depends not only on the dimensions, often also on the mechanical properties of these materials.\nThe dimensions can be measured optically or tactilely. However, in the thickness measurement of transparent materials with optical measuring methods, but also in the stylus measurement of coatings on hard substrates systematic deviations of the measured thickness are observed. In tactile methods, the main influencing factors are the probing force and the tip radius.\nWith viscous materials whose mechanical properties are time-dependent, also different scanning speeds affect the level of systematic deviations. PTB has therefore, in cooperation with other European national metrology institutes, developed a method for correcting these systematic errors, which has been standardized in DIN 32567. In the standard, the main influencing factors are shown for tactile and optical measurements and methods for the estimation, correction and reduction of systematic errors are described.\nContact at PTB\nDr. Uwe Brand, Working Group 5.11 Hardness and Tactile Probing Methods, Telefon: 0531-592 5111, E-Mail: firstname.lastname@example.org\n• Brand, U.; Beckert, E.; Beutler, A.; Dai, G.; Stelzer, C.; Hertwig, A.; Klapetek, P.; Koglin, J.; Thelen, R. and Tutsch, R.: Comparison of optical and tactile layer thickness measurements of polymers and metals on silicon or SiO2. Meas. Sci. Technol. 22 (2011) 094021 (14pp)\n• DIN 32567 Fertigungsmittel für Mikrosysteme — Ermittlung von Materialeinflüssen auf die Messunsicherheit in der optischen und taktilen dimensionellen Messtechnik. Teile 1 – 5\n• Li, Z., Brand, U. und Ahbe, T.: Step height measurement of microscale thermoplastic polymer specimens using contact stylus profilometry. Prec. Eng. 45, 110–117 (2016)\nImke Frischmuth | Physikalisch-Technische Bundesanstalt (PTB)\nGetting closer to porous, light-responsive materials\n26.07.2017 | Kyoto University\n25.07.2017 | Vanderbilt University\nStrong light-matter coupling in these semiconducting tubes may hold the key to electrically pumped lasers\nLight-matter quasi-particles can be generated electrically in semiconducting carbon nanotubes. Material scientists and physicists from Heidelberg University...\nFraunhofer IPA has developed a proximity sensor made from silicone and carbon nanotubes (CNT) which detects objects and determines their position. The materials and printing process used mean that the sensor is extremely flexible, economical and can be used for large surfaces. Industry and research partners can use and further develop this innovation straight away.\nAt first glance, the proximity sensor appears to be nothing special: a thin, elastic layer of silicone onto which black square surfaces are printed, but these...\n3-D shape acquisition using water displacement as the shape sensor for the reconstruction of complex objects\nA global team of computer scientists and engineers have developed an innovative technique that more completely reconstructs challenging 3D objects. An ancient...\nPhysicists have developed a new technique that uses electrical voltages to control the electron spin on a chip. The newly-developed method provides protection from spin decay, meaning that the contained information can be maintained and transmitted over comparatively large distances, as has been demonstrated by a team from the University of Basel’s Department of Physics and the Swiss Nanoscience Institute. The results have been published in Physical Review X.\nFor several years, researchers have been trying to use the spin of an electron to store and transmit information. The spin of each electron is always coupled...\nWhat is the mass of a proton? Scientists from Germany and Japan successfully did an important step towards the most exact knowledge of this fundamental constant. By means of precision measurements on a single proton, they could improve the precision by a factor of three and also correct the existing value.\nTo determine the mass of a single proton still more accurate – a group of physicists led by Klaus Blaum and Sven Sturm of the Max Planck Institute for Nuclear...\n26.07.2017 | Event News\n21.07.2017 | Event News\n19.07.2017 | Event News\n26.07.2017 | Physics and Astronomy\n26.07.2017 | Life Sciences\n26.07.2017 | Earth Sciences"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:b7710ab3-e9f6-4afa-93e9-0ac85ea73894>"],"error":null}
{"question":"I am studying computer science and curious about quantum computing. How can quantum computers help improve artificial intelligence compared to regular computers?","answer":"Quantum computers are particularly well-suited for artificial intelligence applications because AI relies heavily on calculating probabilities for many possible choices during its learning process. Companies are already utilizing quantum computing for AI development - for example, Lockheed Martin is using a D-Wave quantum computer to test complex autopilot software that classical computers struggle with, while Google is applying quantum computing to develop software that can distinguish cars from landmarks. The technology is becoming so advanced that AI is now creating more AI, indicating its rapidly growing importance. Quantum computing promises to revolutionize AI applications across many industries, from automotive to medicine, with an impact that some compare to what electricity was to the twentieth century.","context":["Computers don’t exist in a vacuum. They serve to solve problems, and the type of problems they can solve are influenced by their hardware. Graphics processors are specialized for rendering images; artificial intelligence processors are made for AI; and quantum computers are designed for…what?\nWhile the power of quantum computing is impressive, it does not mean that existing software simply runs a billion times faster. Rather, quantum computers have certain types of problems which they are good at solving, and those which they aren’t. Below are some of the primary applications we should expect to see as this next generation of computers becomes commercially available.\nA primary application for quantum computing is artificial intelligence. AI is based on the principle of learning from experience, becoming more accurate as feedback is given, until the computer program appears to exhibit “intelligence.”\nThis feedback is based on calculating the probabilities for many possible choices, and so AI is an ideal candidate for quantum computation. It promises to disrupt every industry, from automotive to medicine, and it’s been said AI will be to the twenty-first century what electricity was to the twentieth.\nLockheed Martin, for example, plans to use its D-Wave quantum computer to test autopilot software that is currently too complex for classical computers, and Google is using a quantum computer to design software that can distinguish cars from landmarks. We have already reached the point where AI is creating more AI, and so its importance will rapidly escalate.\nAnother example is precision modeling of molecular interactions to find the optimum configurations for chemical reactions. Such “quantum chemistry” is so complex that only the simplest molecules can be analyzed by today’s digital computers.\nChemical reactions are quantum in nature as they form highly entangled quantum superposition states. But fully-developed quantum computers would not have any difficulty evaluating even the most complex processes.\nGoogle has already made forays in this field by simulating the energy of hydrogen molecules. The implication of this is more efficient products, from solar cells to pharmaceutical drugs, and especially fertilizer production; since fertilizer accounts for two percent of global energy usage, the consequences for energy and the environment would be profound.\nMost online security currently depends on the difficulty of factoring large numbers into primes. While this can presently be accomplished by using digital computers to search through every possible factor, the immense time required makes “cracking the code” expensive and impractical.\nQuantum computers can perform such factoring exponentially more efficiently than digital computers, meaning such security methods may eventually become obsolete. New cryptography methods are being developed, though it may take time: in August 2015 the NSA began introducing cryptography methods that would resist quantum computers, and in April 2016 the National Institute of Standards and Technology began a public evaluation process for quantum-resistant algorithms lasting four to six years.\nThere are also promising quantum encryption methods being developed using the one-way nature of quantum entanglement. City-wide networks have already been demonstrated in several countries, and Chinese scientists recently announced they successfully sent entangled photons from an orbiting “quantum” satellite to three separate base stations back on Earth.\nModern markets are some of the most complicated systems in existence. While we have developed increasingly sophisticated scientific and mathematical tools to address this, these still suffer from one major difference from other scientific fields: there’s no controlled setting in which to run experiments.\nTo solve this, investors and analysts have turned to quantum computing. One immediate advantage is that the randomness inherent to quantum computers is congruent to the stochastic nature of financial markets. Investors often wish to evaluate the distribution of outcomes under an extremely large number of scenarios generated at random.\nAnother advantage quantum offers is that financial operations such as arbitrage may require many path-dependent steps, the number of possibilities quickly outpacing the capacity of a digital computer.\nNOAA chief economist Rodney F. Weiher claims (PDF) that nearly 30 percent of the US GDP ($6 trillion) is directly or indirectly affected by weather, impacting food production, transportation, and retail trade, among others. The ability to better predict the weather would enormously benefit many fields, not to mention allow more time to take cover from disasters.\nWhile this has long been a goal of scientists, the equations governing such processes contain many, many variables, making classical simulation lengthy. As quantum researcher Seth Lloyd pointed out, “Using a classical computer to perform such analysis might take longer than it takes the actual weather to evolve!” This motivated Lloyd and colleagues at MIT to show that the equations governing the weather possess a hidden wave nature which are amenable to solution by a quantum computer.\nDirector of engineering at Google Hartmut Neven also noted that quantum computers could help us build better climate models. Such models would help us more accurately forecast future warming and better plan for its effects. The United Kingdom’s national weather service Met Office has already begun investing in such innovation.\nComing full circle, a final application of this exciting new physics might be… studying exciting new physics. Models of particle physics are often extraordinarily complex, confounding pen-and-paper solutions and requiring vast amounts of computing time for numerical simulation. This makes them ideal for quantum computation, and researchers have already been taking advantage of this.\nResearchers at the University of Innsbruck and the Institute for Quantum Optics and Quantum Information (IQOQI) recently used a programmable quantum system to perform such a simulation. Published in Nature, the team used a simple version of quantum computer in which ions performed logical operations, the basic steps in any computer calculation. This simulation showed excellent agreement compared to actual experiments of the physics described.\n“These two approaches complement one another perfectly,” says theoretical physicist Peter Zoller. “We cannot replace the experiments that are done with particle colliders. However, by developing quantum simulators, we may be able to understand these experiments better one day.”\nInvestors are now scrambling to insert themselves into the quantum computing ecosystem, and it’s not just the computer industry: banks, aerospace companies, and cybersecurity firms are among those taking advantage of the computational revolution.\nWhile quantum computing is already impacting the fields listed above, the list is by no means exhaustive, and that’s the most exciting part. As with all new technology, presently unimaginable applications will be developed as the hardware continues to evolve and create new opportunities.\nImage Credit: IBM Quantum System One in Ehningen, Germany / IBM"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:990f49c8-2130-4780-808f-892f76bf90ae>"],"error":null}
{"question":"In what ways do environmental conditions affect synthetic grass and Chinese Evergreen plants differently?","answer":"Synthetic grass maintains its appearance regardless of environmental conditions, staying green and vibrant even in dry or cold weather, while requiring no water. Chinese Evergreen plants, however, are significantly affected by environmental conditions - their water needs vary with temperature (requiring less water in cold temperatures), humidity levels (needing less water in high humidity), and light exposure (using more water with higher light levels). While synthetic grass provides consistent performance year-round, Chinese Evergreens require adjustments to their care based on seasonal changes and environmental factors.","context":["Artificial grass, also known as synthetic grass or turf, has become a popular and versatile alternative to traditional natural lawns. This innovative landscaping solution offers a range of benefits and is gaining traction in residential, commercial, and recreational settings. Let’s delve into the basics of synthetic grass, from its composition to its advantages and applications.\nComposition of Synthetic Grass\nArtificial grass is designed to closely mimic the appearance and feel of natural grass while offering enhanced durability and minimal maintenance. It is composed of several key elements:\n1. Synthetic Fibers:\nThe blades of artificial grass are made from synthetic fibers, typically polyethylene or polypropylene. These fibers are designed to be soft, resilient, and UV-resistant, ensuring they can withstand exposure to sunlight and maintain their vibrant color.\n2. Backing Material:\nThe backing of synthetic grass is usually made from a blend of materials like latex or polyurethane. This layer provides stability and reinforcement to the fibers.\nInfill materials, such as sand or rubber granules, are spread over the artificial grass to add weight, improve stability, and enhance the grass’s natural look. Infill also contributes to the surface’s shock absorption and cushioning properties.\n4. Drainage System:\nSynthetic grass is designed with a perforated backing that allows for efficient drainage. This prevents water from pooling on the surface and helps maintain a dry and usable space, even after rain.\nAdvantages of Synthetic Grass\nThe popularity of synthetic grass can be attributed to its numerous advantages, which appeal to homeowners, businesses, and sports facilities alike:\n1. Low Maintenance:\nArtificial grass requires minimal upkeep compared to natural grass. There’s no mowing, watering, or fertilizing needed, saving both time and resources.\n2. Year-Round Beauty:\nSynthetic grass maintains its lush appearance regardless of the season. It remains green and vibrant even in dry or cold weather.\n3. Water Conservation:\nOne of the most significant benefits is its water-saving quality. Synthetic grass doesn’t require regular watering, contributing to water conservation efforts.\nArtificial grass is built to withstand heavy foot traffic, making it an excellent choice for high-traffic areas like sports fields and playgrounds.\nFor individuals with grass allergies, synthetic grass provides relief as it doesn’t produce pollen or allergens.\nSynthetic grass can be installed in a variety of locations, from residential lawns and commercial spaces to sports fields and rooftops.\nApplications of Synthetic Grass\nThe versatility of synthetic grass makes it suitable for a wide range of applications:\n1. Residential Lawns:\nHomeowners can enjoy a low-maintenance, green lawn year-round without the hassle of regular upkeep.\n2. Commercial Spaces:\nBusinesses can enhance their outdoor areas with attractive landscaping that creates a welcoming atmosphere.\n3. Sports Fields:\nSynthetic grass is used in various sports fields, offering consistent playing surfaces that withstand intensive use.\nPlaygrounds benefit from the cushioning properties of synthetic grass, providing a safe and comfortable play surface.\n5. Rooftop Gardens:\nArtificial grass is used to create rooftop gardens and green spaces in urban environments.\nSynthetic grass has transformed the way we approach landscaping, offering a low-maintenance, durable, and versatile solution for various settings. Whether you’re looking to revamp your home’s lawn or improve the functionality of a sports field, synthetic grass provides a host of benefits that make it a compelling choice. With its realistic appearance and impressive range of applications, artificial grass is redefining our outdoor spaces for the better.","Chinese Evergreen or Aglaonema is a beginner-friendly houseplant that requires low maintenance. It’s a great plant to start your indoor gardening journey with. But watering the houseplants is a confusing aspect where people make the most mistakes.\nLet’s find out how much water does a Chinese Evergreen needs.\nChinese Evergreens require water once a week during the spring and summer and once in two weeks during the winters. Since this plant prefers slightly moist soil, you should not let the soil go completely dry before watering. But, you must always check the soil before watering.\nAlthough I gave you a general idea, other factors can determine or alter the water requirements of your Chinese Evergreen plant. I have covered all those information in this article, so keep reading.\nSome links in the post are affiliate links and I get a commission from purchases made through links in the post.\nChinese Evergreen Water Requirements\nThe season is the most crucial factor you need to consider while watering your Chinese Evergreen. You can’t give it the same amount of water throughout the year.\nSummer and Spring are the warmer seasons of the year, and this is when your Chinese Evergreen grows the most. This is why these two seasons are referred to as the growing season.\nDuring the growing season, the sun shines bright in the sky, and the plant absorbs the indirect rays of the sun and functions at its best. The water you give the plant is used up fast, and the soil dries fast due to the sun.\nAs soon as the soil starts drying and the top 2-3 layers dry out, the plant requires water. This takes around 7-10 days. So you need to water your Chinese Evergreen once a week or once in 10 days during the growing season.\nDon’t water the Chinese Evergreen without checking the soil’s moisture level and just by blindly following a routine.\n|Essential Plant Supplies||Check Out On Amazon|\n|Miracle-Gro Indoor Potting Mix||Buy Now|\n|Miracle-Gro Indoor Plant Food||Buy Now|\n|LED Grow Light for Indoor plants||Buy Now|\n|Kensizer Soil Tester, 3-in-1 Soil Moisture/Light/pH Meter.||Buy Now|\n|Heavy Duty Gardening Tools with Non-Slip Rubber Grip||Buy Now|\n|Govee Bluetooth Hygrometer and Thermometer||Buy Now|\n|Humidifiers for Home and houseplants||Buy Now|\n|Houseplants Self Watering System with 30-Day Digital Programmable Water Timer||Buy Now|\n|Drain Smart 9” 2-Pack Drainage Discs - Perfect for any Potted Plants||Buy Now|\nLooking for a readymade indoor plant soil mix that you can open and pour? Check out rePotme. They offer a wide range of readymade soil premixes for all your indoor plants.\nWinter is the coldest time, not ideal for most houseplants, including Chinese Evergreen. Naturally, plants become dormant during this time.\nThe low temperatures, low humidity, less intense sunlight, and shorter days make it difficult for the houseplants to grow. So, they go into a period of rest where they conserve their energy and try to survive.\nDuring this time, the activities of the plant remain less, and the soil takes time to dry out. Therefore, the plant requires less amount of water at longer intervals.\nOn average, a Chinese Evergreen will require water once in two weeks during winter but don’t water without checking the moisture.\nDuring winter, the water temperature might go down, so remember that cold water can shock the plant. Water your Chinese Evergreen with room temperature water only.\nHow do you know if Chinese Evergreen needs water?\nThere are different ways to figure whether your Chinese Evergreen needs water or not. I will discuss the ones I have used to understand if my plant requires water.\nFinger: The easiest way is to put your finger inside the soil. Make sure you stick the finger 2-3 inches deep to understand if the soil is wet. If the soil sticks to the finger, it still has water, and you must wait. But if the soil feels dry and doesn’t stick, you can water your Chinese Evergreen.\nMoisture meter: The next best thing is a moisture meter, especially for those who don’t want to get their hands dirty. This is also an efficient way of checking the soil’s moistness. You just need to insert the moisture meter into the soil, and if the reading on it is 2 or 3, you can water the plant.\nTouch the pot: If your Chinese Evergreen is sitting in a clay or terracotta pot, you can touch the pot and find out if it needs water. The pot will feel wet when you touch the sides, or the bottom of the soil is moist.\nLook at the pot: In the case of clay or terracotta pots, you can determine the water requirement even by looking at the pot. If the soil is moist, the pot will appear dark, while if the soil is dry, the pot will have a lighter shade.\nLift the pot: Another easy way of understanding the water needs of your Chinese Evergreen is by lifting the pot. If the pot feels heavy, your Chinese Evergreen will not require water, but you can water the plant if the pot feels light.\nDoes Chinese evergreen need a lot of water?\nChinese Evergreen never needs a lot of water. They do well with a moderate amount of water.\nChinese Evergreens enjoy slightly moist soil at all times. So you should water it enough to keep the soil moist and not let it go dry entirely.\nHowever, you should also not water too much that makes the soil waterlogged. The plant will not enjoy sitting in wet soil, leading to root rot.\nHow much water a Chinese Evergreen will need can depend on a few factors such as:\nLet’s understand how these factors affect the water requirements of the plant.\nTemperature: The temperature varies as per the season. When the temperatures remain high, the soil dries up faster, and hence the plant will require frequent watering.\nIn contrast, when the temperatures start going low, the soil starts taking longer to dry, requiring less water.\nHumidity: Chinese Evergreen plants love high humidity. And if it gets high humidity, it will not require a lot of water. You can use a humidifier to maintain the humidity that a Chinese Evergreen requires.\nIf the plant is not getting enough humidity, you should mist it and water it when the topsoil dries.\nLight: Plants require light to produce energy and food. If the light levels are high, the plant will remain active and perform all its daily activities, and the water will get used up fast, due to which the water requirement will be high.\nBut if the plant doesn’t get enough light, you need to water it less. Otherwise, it will get overwatered.\nSoil: Chinese Evergreen prefers a well-drained soil mix that can drain excess water fast. Such soil will dry up on time and require more water.\nBut a soil that takes time to get dry will require less water. This will, however, not be ideal for the Chinese Evergreen plants.\nA peat-based soil will perlite will be suitable for the Chinese Evergreen. You need to ensure that the potting mix retains enough water and drains the excess.\nPot: Even the pot can affect the water requirement of your Chinese Evergreen. If you have placed it in a plastic pot, it will require less water as plastic is non-porous, and it doesn’t allow the soil to dry up fast.\nBut if the plant is in a clay or terracotta pot, the water will drain out fast, and the plant will require more water.\nYou should also consider the pot size. If the pot is big, it will hold more water and take time to dry. So in a bigger pot, the Chinese Evergreen will require less water.\nThe soil will hold less water in a smaller pot that will get used up fast and require frequent watering.\nWhat happens when you overwater your Chinese Evergreen?\nChinese Evergreen plants prefer slightly moist soil, but they will not enjoy soggy soil. When you water without checking the soil’s moisture, you end up overwatering the plant.\nOverwatering is a common problem that beginners make, but one can save the plant by acting fast. If not, the condition can get severe.\nHere are some common signs of an overwatered Chinese Evergreen plant.\n- Yellow leaves: A common sign of overwatering is that the lower leaves turn yellow. You can remove these leaves by pruning them from the base, or you can wait for them to dry out and fall off.\n- Droopy leaves: Overwatering puts a lot of stress on the plant, and the roots fail to absorb nutrients from the soil and send it back to all parts. So, the leaves become weak and start drooping.\n- Weak stems: Overwatering affects not only the leaves but also the stems. Due to lack of nutrients, the stems become weak, turning soft due to standing in water.\n- Fungus growth: Overwatering can often lead to fungus or mold growth on the soil, and you must scrape the soil or change it entirely to get rid of it.\n- Pest infestation: Most pests like aphids and mealybugs will attack an overwatered Chinese Evergreen and suck the sap, weakening the plant further.\n- Root rot: Root rot is a deadly fungus disease. Overwatering keeps the roots wet and cuts the oxygen supply, due to which they start decaying. If you don’t get rid of the affected roots and repot the plant in fresh potting soil, the plant can even die.\nHow to prevent overwatering your Chinese Evergreen?\nIf you don’t want to cause the above problems to the plant, keep these tips in mind while watering.\n- Don’t follow a watering routine blindly.\n- Never water your Chinese Evergreen without checking the soil.\n- Reduce the watering frequency and amount during winter.\n- Provide enough light for the soil to dry up fast.\n- Use a well-draining soil mix that can drain the excess water out of the system.\n- The pot should have working drainage holes.\nWhat happens when you underwater your Chinese Evergreen?\nSince Chinese Evergreen prefers slightly moist soil, not giving it enough water will also cause some problems to the plant.\nAlthough treating underwatering is much easier than treating an overwatered plant, you should still try not to keep your Chinese Evergreen thirsty for too long.\nHere’s what you’ll notice on an underwatered Chinese Evergreen:\n- Brown leaves: Without enough water, the leaves start getting discolored and turn brown. These leaves don’t become healthy even after you water the plant, so you need to get rid of them.\n- Wilting leaves: When the plant doesn’t get sufficient water, the leaves start wilting to save whatever water is there in the leaves.\n- Dry soil: If you keep your Chinese Evergreen thirsty, the plant will end up with dry soil, leading to other issues.\n- Crispy foliage: The leaves lose their texture and turn dry and crisp without water. These leaves eventually fall off the plant.\n- Slow growth: Without water, the plant will not have the energy to perform its regular activities, and hence you’ll notice slow growth.\nHow to prevent underwatering your Chinese Evergreen?\nReviving an underwatered plant is not difficult but preventing it in the first place is a better idea.\n- Use a calendar to remind you that your plant needs water.\n- Check the soil and water if the topsoil is dry. Don’t wait for the soil to go bone dry.\n- If the plant is living in the ideal growing conditions, such as a warm climate with enough indirect sunlight, it will require water more often.\n- Make sure that the soil retains enough water to remain moist.\nHow do you water Chinese Evergreens?\nAvoid watering the leaves when you water your Chinese Evergreen.\nPour water into the watering can. Avoid using cold or hot water, and don’t opt for regular tap water as it contains harmful minerals such as bicarbonates, chlorine, fluorine, etc.\nNow pour water into the soil and water it thoroughly. Wait for the excess water to start draining out of the drainage holes.\nAllow the top 3 inches of the soil to dry out before watering the plant again.\nShould I mist my Chinese evergreen?\nIf your Chinese Evergreen doesn’t get enough humidity, you can mist the plant. These plants prefer high humidity, so misting them will be a good idea.\nYou can even clean the leaves with a cloth after you mist them. Avoid misting the soil and mist only the leaves.\nChinese Evergreen is a tolerant plant that can forgive you for all your mistakes if you can identify the problem on time. However, it would be better to learn the plant’s requirements instead of making a mistake.\nWater your Chinese Evergreen when the top 3 inches of the soil go dry. Don’t keep the plant thirsty for too long, as that will make the soil go completely dry. Also, avoid overwatering as soggy soil can cause root rot."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:0da457de-e9d9-4a2f-b336-b2d9aeb8f178>","<urn:uuid:d79ea216-e456-403e-82f9-a4475f4f3a7f>"],"error":null}
{"question":"I read that some street photographers use flash. What is the reason Martin Parr uses flash in his street photography?","answer":"Martin Parr uses flash for two main reasons: as an aesthetic choice to create a slightly surreal effect in his photographs, and as a technical necessity to eliminate shadows when working in bright sunlight or to complement ambient light when working indoors.","context":["Digital Camera World has partnered with Magnum Learn to publish a taster of its first online course, 'The Art of Street Photography'.\nMagnum Learn is the education portal of Magnum Photos, and the course tutors comprise Martin Parr, Peter van Agtmael, Carolyn Drake, Richard Kalvar, Mark Power, Bruce Gilden and Susan Meiselas.\n'The Art of Street Photography' consists of 10 themed lessons and costs $99. We'll be publishing a new extract every week on Magnum Learn Monday until 02 September!\nThis week, in the second extract, we're looking at how to get started in street photography, and the camera equipment you need to take photographs of people on the street.\nMiss last week's extract? Click here to read Magnum Learn Monday Part 1: How to shoot street photography like a pro\nGetting started: do your research before arriving on location\nHere we provide a short introduction to a variety of practical aspects and issues that surround street photography. From understanding local laws on photography to more complex issues of photographic representation, these should be worthy considerations when approaching or actively working within the public realm.\nWhat follows is designed to act as prompts for your own research, rather than being a comprehensive guide. Plus, we consider the mindset required to be successful as a street photographer.\nRepresentation of people and places\nPhotography brings with it significant questions surrounding representation. It is important to understand the role of the photograph and your role as the photographer; this is especially true when your primary subject is people.\nIt is worthwhile asking yourself: how am I representing this place or person? How have they, or this place, been represented in the past, and is that mode of representation problematic? How is this person’s experience different from mine?\nIt is important to recognise your own experience and be conscious of your position, particularly if it is a privileged one, when photographing the lives of others.\nUltimately, every photographer must develop their own sensibilities and principles to abide by, but drawing from the history of photography and the writings that explore these issues is always recommended.\nStart with some of these books: On Photography by Susan Sontag; Photography: A Critical Introduction, edited by Liz Wells; The Burden of Representation by John Tang; and Photography: The Key Concepts by David Bate.\nIt can also be about the presence of the street, and what that represents: humanity, society, architectural forms and more.\nThere is no one definition of street photography, no one ‘right’ way, as exemplified by the variety of approaches, represented within the history of photography.\nWorking strictly in line with tradition, categories or “pigeon holes” as course tutor Bruce Gilden calls them, can be extremely limiting, when in truth the ‘street’ can offer endless opportunities to make different kinds of photographs.\nAs alluded to by course tutor Peter van Agtmael, the beauty of the street is in the “unexpectedness of the encounters.”\nThe key is to establish a personal definition for what the street means, and to be innovative, bringing a unique vision and new ideas to challenge the genre’s traditions.\nStarting points for research\nThe history of photography is a great source of inspiration and learning. Knowing what has come before and what is being produced today can help inform your own work.\nAs course tutor Bruce Gilden says, you should want to add to what has been done before and make it yours.\nHenri Cartier-Bresson drew inspiration from many other art forms, namely painting and literature, and would spend hours in the Louvre Museum in Paris studying the work of great painters.\nLooking beyond photography can be equally inspiring and refreshing, and open you up to new approaches and new ideas.\nWhat mindset do you need to succeed?\nPhotographers need sharp senses, innate curiosity and an openness to the world when working in the street.\nStreet photography is about acting on impulse, listening to your intuition and being spontaneous. You have to expect the unexpected.\nAnd above all else, you will need drive, dedication, patience, and perseverance; to put in the hours of hard work, to put the failures behind you and to keep pushing forward.\nIdeal locations for trying street photography\nYou can discover so much about a place through the simple act of walking, getting out of your front door and engaging with the world.\nActively exploring your environment with camera in hand is the best place to start; from finding the hotspots where interactions may occur to discovering what time the light shifts to transform a specific street corner.\nThis all happens by being present, by having your feet on the ground and your mind and eye curious as to what is around you. Here are some locations to get started, or just hone your skills:\n- Street corners/intersections\n- Pedestrian crossings\n- Bustling streets/hubs: eg, Fifth Avenue in New York City or Oxford Street in London\n- End of working hours in certain locations or districts, where you can find mass exits of people\n- Keep an eye on the light in cities – at times it can become very dramatic\nHow does the law relate to street photography?\nDigital Camera says… This depends on the territory you are photographing in, as different countries have different laws.\nIt’s therefore essential to do adequate research beforehand so that you’re familiar with any restrictions that may apply. And being culturally aware is also important, as photographing people is not seen as normal practice all over the world.\nAs a general rule of thumb, there will be a distinction between taking photographs in public places and on private property. (Again, though, the definition of a public place, and what can be photographed in it, can vary from country to country – there could even be restrictions enforced in some public places.)\nPhotographing people in public places will be more straightforward than photographing people on private property, although it’s worth remembering that in most countries people in public places have a reasonable expectation of privacy, and not to be harrassed.\nTo photograph on private property, you’ll need the permission of the landlord or owner. This can also apply to public places like landmarks and attractions. If in doubt, always check with a police officer or a member of the authorities.\nWhat gear do I need for street photography?\nAs demonstrated by the course tutors, there are myriad setups for street photography, from the Leica rangefinder that is so synonymous with the medium to the slow, static, large-format view camera.\nWhen it comes to sharing their setups from the past and the present, Richard Kalvar favours a 35mm lens, as it allows him to get very close to people; while Peter van Agtmael opts for a full-frame camera with either a 28–70mm kit lens or a 28mm or 35mm lens, plus a small flashgun.\nMartin Parr also uses a 28–70mm zoom, so he doesn’t have to carry so many lenses: “It gives me more bounce in my step to keep going the full day.” Parr has also gone further, using a macro lens with a ring flash for street photography.\nIn contrast, Mark Power uses a digital medium-format technical camera, having previously used a large-format 5x4 camera for 15 years.\nPower says that it is about finding the right tools to make the work you want, and the right tools to enable you to speak with your own voice. You can make compelling work with what you have available, as long as you have something to say.\nUsing flash in street photography\nA number of the course tutors utilise different flash techniques. Bruce Gilden’s signature use of flash is seen across the course.\nHis typical equipment on the street (pictured, above) includes a handheld flashgun tethered to his camera, which he uses to isolate and illuminate his subject within the foreground.\nMartin Parr describes his frequent use of flash as both an aesthetic choice, bringing with it a slightly surreal effect to his photographs, and a technical necessity, utilising it to kill the shadows when working in bright sunlight, or to complement the ambient light when working indoors.\nAbout Magnum Learn\n‘The Art of Street Photography’ is the first course to be offered on Magnum Learn, the new online learning platform from the world’s most prestigious photo agency.\nThe course consists of 10 themed lessons comprising in-depth video and tutoring from Magnum pros, offering key advice and guidance to help improve your photography in the street… and beyond.\nSeven world-class photographers teach the course. These include Bruce Gilden, Martin Parr, Susan Meiselas, Richard Kalvar, Carolyn Drake, Peter van Agtmael and Mark Power, plus industry leaders.\nThese pros have contributed their unique insights, knowledge and experience to the lessons, and guide the pupil through the process of honing their photography skills via intimate interviews and on-location demonstrations.\nWe go on location with Martin Parr, one of the masters of photography, to find out more about his shooting style, and his tips for editing. It'll be published on 19 August, so don't miss it!\nIf you like what you've seen from ‘The Art of Street Photography’ so far, then click here to find out more about the course."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:8be33c9d-697e-4ec3-8b02-99448bbe3744>"],"error":null}
{"question":"¿Cuál es el tratamiento más efectivo para prevenir la reincidencia entre una reconstrucción del ligamento cruzado anterior y una reconstrucción del ligamento patelofemoral medial?","answer":"For ACL tears, using the patient's own tissue (autograft) rather than cadaver tissue (allograft) is most effective in preventing reinjury, particularly the gold standard patellar tendon graft which has the lowest failure rate for high-level athletes. In contrast, for patellar instability, even MPFL reconstruction surgery (considered the better surgical option) does not reliably prevent recurrence. Studies show that isolated MPFL repair cannot guarantee future knee stability, with one study finding a 52% probability of recurrent dislocations after 10 years post-surgery. Additionally, MPFL reconstruction without addressing other stabilizing structures leads to a high rate of continued problems, including a 5% recurrent dislocation rate.","context":["Knee Injury in Skiers\nUnfortunately, one of the most common injuries that occurs during skiing is a tear of the anterior cruciate ligament. The anterior cruciate ligament, or ACL, is an important ligament to prevent one’s knee from having problems with twisting, turning, or pivoting activities. One can tear their ACL during skiing by a fall, a twist of the knee when one’s foot goes to the outside of the ski path, or when one falls back on their skis while skiing. While in most activities, an anterior cruciate ligament tear happens with a noncontact mechanism, it really has not been defined in skiing as well as to whether it is more common to tear one’s ACL and then fall, or to fall and then tear one’s ACL.\nHow to Identify an ACL Tear\nOne of the most common things that happens with an ACL tear is to feel a “pop”. This pop is a bone bruise that occurs when one’s joint slips forward (subluxes) and the ends of the bone on the outside of the knee contact violently and cause a bone bruise. Bone bruises happen about 75% of the time with ACL tears and luckily do not have consequences in most circumstances.\nOne of the most common things that happens after one tears their ACL is to have a lot of bleeding. The ACL has a good blood supply and when it is torn it commonly fills the knee with blood. Other concurrent injuries that can also fill the knee with blood after an injury while skiing include a kneecap dislocation, a meniscus tear, or tibial plateau fracture.\nHow to Treat a Torn ACL\nThe initial treatment of an ACL tear starts with a good program of self-directed or supervised physical therapy. One should work on regaining knee motion, try to decrease the swelling so your quadriceps muscles do not shut down, and to make sure that the knee gets back out straight. An early diagnosis is important for an ACL tear because when one has other injuries with an ACL tear, such as a meniscus tear, it can greatly affect one’s long-term prognosis for their knee function. It is especially important to try and diagnose meniscus tears which can be repaired with an ACL tear. This is because the meniscus is an essential cushion to the knee and when one loses their meniscus tissue, there is a very high risk of developing arthritis within the first decade after an ACL tear. Therefore, an examination by an emergency room physician or a sports medicine physician would be indicated when one is concerned that they may have torn their ACL, or if their knee swells up, after a skiing injury.\nIf one participates in twisting and turning activities, or is a young athlete, then one should consider undergoing an ACL reconstruction after they tear their ACL. We have recently published that patients in their 50’s and 60’s do equally well after ACL surgery as those in their 20’s.\nHow to Perform ACL Reconstruction Surgery\nThere are many factors which go into a successful ACL reconstruction. It is important to make sure that the treating doctor is up to date on the latest literature for treating ACL tears because ACL reconstruction techniques have undergone changes over the last decade which have improved one’s outcomes.\nFirst, it is important to ensure that the ACL reconstruction graft is placed in the correct position. While this may sound elementary, the most common cause of failed ACL reconstructions is technical errors in the ACL graft placement. The most common error is to place the ACL reconstruction tunnel in the femur in the wrong position. This commonly occurs when one uses an older technique of drilling the femoral tunnel through an already drilled tibial tunnel, called the transtibial femoral tunnel reconstruction technique. The other most common technical error is to place the tunnel on the shin bone (the tibia) too far back from its normal attachment site on the tibia. This causes the graft to be straight up and down, or vertical, and does not control the rotation after surgery that a normal ACL would do. Recent studies by our group on both basic anatomy as well as electron microscopy have demonstrated that the ACL is basically adjacent to the anterior horn of the lateral meniscus and best functions when it is placed back in its normal position.\nACL Graft Choice\nThe second most common important choice for an ACL reconstruction is the graft material used. Probably the most important decision to make if one is less than 55 years of age is to use one’s own tissues for an ACL graft rather using it from a cadaver (allograft). It has been well-documented that the use of cadaver material for ACL grafts in young patients has a very high failure rate and should not be used. If one is a bit older and wishes to use cadaver material, it is important to recognize that the graft will not grow into one’s body at the same rate as using one’s own graft material and it may take up to 50% longer. Thus, one should avoid going back to full activities for at least 9-12 months after an ACL reconstruction with a cadaver graft to ensure that the graft does not tear because it is not fully healed in place.\nThe gold standard ACL reconstruction graft is the patellar tendon graft. This graft is taken with a piece of bone off one’s kneecap and the shin bone and is the most common graft used for professional and high-level athletes. Another common graft is the hamstrings graft. These grafts are used in many centers, but do have a slightly higher risk of infection, graft failure, and end up a bit looser than the patellar tendon graft. Another much less frequently used graft is the quadriceps tendon graft. However, recent studies have shown that this graft causes the most residual quadriceps weakness of all autografts, and probably be used with caution until further studies are performed on it. In any event, choosing between a patellar tendon or hamstrings graft should be made with one’s surgeon and is recommended that one choose what your surgeon feels most comfortable with performing in your particular case.\nTreating Associated Knee Injuries\nThe other important thing with an ACL reconstruction is to be sure to address all associated injuries with an ACL tear to minimize the risk of both developing arthritis down the road, as well as having a lesser chance of having an ACL reconstruction graft fail. The most common associated injury which needs to be addressed with an ACL tear is a meniscus tear. The meniscus is the cushioning cartilage that is important for joint health. In effect, it is probably more important to repair the meniscus and have it heal successfully than the ACL reconstruction itself. Therefore, one should strive to have a meniscus tear repaired at all cost when one has an ACL tear. In our most recent series of over 300 patients, we performed meniscus repairs in 55% of ACL reconstructions. The other associated injury to look for with an ACL tear is injuries to the collaterals. A medial collateral ligament, or MCL, tear is a common injury that happens with an ACL tear. When these are not complete tears, they almost always heal. However, when the knee is unstable when it is out straight, or if the MCL tears off the tibia, they often do not heal and may need to be reconstructed with your ACL reconstruction. A combined tear of the LCL, or lateral (fibular) collateral ligament, also can occur. This is a very common injury while skiing. Unfortunately, complete tears of the LCL rarely heal and one should have a concurrent ACL and LCL reconstruction when they happen in combination in all circumstances. We have published on a large series of patients with surgical outcomes with LCL reconstructions with the technique developed in my research lab and have found them to be extremely successful.\nThe other essential component of an ACL reconstruction is the postoperative rehabilitation. Even if one has a “perfect” ACL reconstruction with one’s own tissue placed in the ideal position, one has to intertwine this reconstruction with a postoperative rehabilitation program. The initial principles are to make sure that one’s knee gets out straight, that swelling is reduced, and that one works on your kneecap motion. It is important to get the knee out straight, because if one does not, the hamstrings muscles can go into spasm and prevent the knee from going out straight which may require a later surgery to address this. It is important to decrease the swelling because swelling both causes increased pain and also shuts down the quadriceps muscles. It is also important to work on kneecap motion because the kneecap can become scarred after an ACL reconstruction, both due to scarring from the original injury and scarring due to the surgery itself, and this can result in kneecap arthritis. Therefore, getting into therapy on postoperative day one is part of an essential ACL reconstruction rehabilitation program that we espouse.\nKnee Arthritis After ACL Reconstruction\nWe have published a metaanalysis recently which reported that over 50% of people develop osteoarthritis within 20 years of tearing their ACL. The most important underlying factors are to ensure that one has a meniscus repair, rather than having the meniscus taken out, with an ACL reconstruction. It is obvious that one can see a theme here, with repairing one’s tissues and addressing all associated injuries being very important to ensuring that one has a well-functioning knee that you can participate in sports for a long time.\nIn summary, an ACL tear can be a very devastating injury because one will be out of sporting activities for 9-12 months. However, being an advocate for one’s own self and choosing the correct treatment can ensure that one will be able to continue to participate in sports long into the future after an ACL reconstruction.","Surgery and non-surgical treatments for acute and chronic knee cap dislocation\nDanielle R. Steilen-Matias, MMS, PA-C\nIn this article, we will examine surgical and non-surgical options for the patient with recurrent patellar instability or kneecap dislocation.\nKneecap dislocation occurs when there is an impact injury to the knee significant enough to dislodge the kneecap. When this injury occurs there can be damage or complete disintegration of the medial patellofemoral ligament (MPFL) and medial patellotibial ligament (MPTL). When there is the complete destruction of the ligament(s), the patient will then decide if they will have a medial patellofemoral ligament and/or medial patellofemoral ligament and medial patellotibial ligament reconstruction surgery.\nWhat we will discuss in this article the various options for treatment:\n- Surgery for a completely disintegrated ligament that occurs with acute injury and dislocation\n- No surgery for a completely disintegrated ligament\n- Surgery for a medial patellofemoral ligament and medial patellotibial ligament partial tear or wear and tear damage\n- Regenerative medicine injections for medial patellofemoral ligament and medial patellotibial ligament partial tear or wear and tear damage\nAcute patella dislocation surgery\nSurgery for a completely disintegrated ligament that occurs with acute injury and dislocation.\nWhat we are going to look at first is the injury that occurs when the knee suffers a blow that knocks the kneecap out of its groove. When this injury occurs almost always the medial patellofemoral ligament is completely ruptured.\nLigaments connect bones to bones. The medial patellofemoral ligament connects the lower thigh bone to the back of the patella/kneecap. So it is easy to see when the kneecap is dislocated, this attachment snaps.\n- At this point, a decision must be made on treatment. First, we will look at the surgical treatment.\nThe first treatment decision is obviously to get the kneecap back in place and secure it.\n- Your doctor may recommend surgery to repair or reconstruct the medial patellofemoral ligament. He/she will refer to it as your MPFL.\nIf this is your first kneecap dislocation, the decision to go to surgery will come with a degree of urgency. Surgeons believe that they only get one chance to perform a repair to your original ligament. After repeated dislocations, the original ligament cannot be surgically repaired, It must be reconstructed from a tendon or ligament that the surgeons get from somewhere else in your body.\n- The points to consider here and that will be documented in research below:\n- The ligament can only be repaired in a partial tear or rupture situation\n- If the ligament suffers a total rupture/disintegration – then the surgery is not repair, but reconstruction.\n- These surgeries have been shown that they do not decrease future dislocation risk. (This is something we will discuss below, isolated repair cannot guarantee future knee stability).\nIn this October 2018 research, we see surgeons talking to surgeons in the medical journal Arthroscopy. (1) Let’s join the conversation:\nThe purpose of this research is to “clarify the discrepancy in surgical options and present evidence to treat patellar dislocation by evaluating which of the (surgical) techniques yields better improvement in stability and functional recovery for patellar dislocation.”\nIn this research, military and university researchers in South Korea shared with the international surgical community these findings:\n- Eleven clinical studies were investigated to determine the effective outcome of surgical versus non-surgical treatment\n- In patients with acute patellar dislocation, there were no significant differences in all evaluated outcomes between the conservative and surgical treatment groups.\n- For patients with recurrent patellar dislocation, MPFL reconstruction was associated with better outcome scores than compared with soft tissue realignment surgery.\nSurgical treatment of the MPFL for acute patellar dislocation is not superior to conservative non-surgical treatment in restoring knee function and clinical outcomes\n- Surgical treatment of the MPFL for acute patellar dislocation is not superior to conservative non-surgical treatment in restoring knee function and clinical outcomes\n- MPFL reconstruction is associated with more favorable clinical outcomes compared with medial soft tissue realignment surgery in patients with recurrent patellar dislocation.\n- MPFL reconstruction is the better of surgical treatment strategies, once the patient decides that they want the surgery anyway.\nThe bottom line: Surgeons are reporting to surgeons that non-surgical technique is in fact just as good as surgery.\nAcute Patella dislocation | Traditional non-surgical conservative care | Immobilization and rehab\nNow we are going to get into even more controversy surrounding the treatment of a patella dislocation. The conservative non-surgical treatment option. This is not to be confused with the non-surgical regenerative medicine option we will discuss below.\nA well-cited paper from the Department of Orthopaedics, Southern California Permanente Medical Group was published in the journal Sports Health. What the doctors were looking for was to provide guidelines, A Treatment Algorithm for Primary Patellar Dislocations, (2) which was the title of their paper. Here is what they said:\n“Surprisingly little evidence exists addressing the nonoperative treatment of the primary patellar dislocation. Contemporary treatment regimens range from immediate mobilization without a brace to cast immobilization in extension for 6 weeks.”\nHere are some of the issues they came across:\n- Immobilization in extension (your knee is fixed in a straight leg position) may give the MPFL better environment in which to heal. However, this comes at the expense of stiffness, weakness, and loss of limb and proximal control that often accompany prolonged immobilization.\n- Patient compliance can also be a factor in deciding nonoperative treatment. For these reasons, many clinicians advocate a short period of immobilization, followed by rehabilitation of the knee, with or without a patellar brace.\n- Although the management of the primary patellar dislocation remains a topic of considerable controversy, certain conclusions can be drawn. If a hemarthrosis (bleeding in the knee) is present, patients should be evaluated for osteochondral fractures (damaged to the cartilage and bone underneath).\n- Acute surgical stabilization remains controversial, with no clear long-term benefits demonstrated in the literature.\n- If nonoperative management is elected, a period of immobilization in extension up to 6 weeks will yield the lowest redislocation rate.\n- In sum, this algorithm provides an evidence-based approach that assists the clinician in the treatment of the acute first-time patellar dislocation.\nIt is clear from these two representative studies why acute patella dislocation usually becomes a situation of chronic patella dislocation\nSurgery for chronic patella dislocation – does this surgery help a 16 year old athlete? Surgery judged ineffective\nDoctors in Germany at Münster University Hospital wrote in the journal BMC Musculoskeletal Disorders June 24, 2017 that:\n- There is currently no consensus regarding the optimal surgical treatment method for patients with recurrent patellar instability. (Chronic patella dislocation).\nThe goal then of their study was to evaluate the long-term results of combined arthroscopic medial reefing (reconstructive surgery of the supporting connective tissue of the patella) and lateral release (surgery to put the patella back into its correct position.)\nThe average age of the patients at the time of surgery was 16 and comprised of adolescent athletes. The youngest patient in the study was 9. The patients were followed for about 5 – 15 years post surgery.\n- Pain continued post surgery: Residual complaints were present in 34 cases (79%).\n- Dislocation continued post surgery: Twenty-two cases had recurrent dislocation after a median interval of 30 months. The probability of recurrent dislocations amounted to 16% after 1 year and 52% after 10 years.\n- Surgery judged ineffective: The combined arthroscopic lateral release with medial reefing does not appear to be an adequate treatment for patients with chronic patellar instability in long-term follow-up.\n- Younger patients might be at a higher risk for recurrent dislocations.(3)\nWhy Do Patellofemoral Stabilization Procedures Fail?\nDoctors in the United Kingdom said it more simply in their March 2017 paper published in the Sports medicine and arthroscopy review. They asked Why do patellofemoral stabilization procedures fail?\nIn recent years, surgical interventions for patellofemoral joint instability have gained popularity, possibly revitalized by the recent advances in our understanding of patellofemoral joint instability and the introduction of a number of new surgical procedures. This rise in surgical intervention has brought about various complications.(4)\nPatellofemoral instability surgery success rates?\n- Doctors note that many treatments can make symptoms of patellofemoral instability and pain worse in some patients.\n- One paper says more than 25% of patients had significant side effects after surgical treatment of patellofemoral instability.\nPhysiotherapist Jenny Mcconnell wrote in the medical journal Manual Therapy that:\nSome cases of patellofemoral instability are difficult to manage and, in fact, some treatments can make the patient feel worse. Frequently, the patient often bounces from practitioner to practitioner, physiotherapist to surgeon, seeking some relief of symptoms. However, their underlying source of pain is not well understood, so treatment can aggravate the symptoms.\nDoctors have put a lot of emphasis on medial patellofemoral ligament (MPFL) reconstruction for the treatment of recurrent patellar dislocations/subluxations. Numerous techniques have been reported; however, there is no consensus regarding optimal reconstruction and in one paper a total of 164 complications occurred in 26% of patients. Side effects included patellar fracture, failures, and clinical instability on postoperative examination, loss of knee flexion, wound complications, and pain.(5)\nPatellofemoral instability – not addressing the whole knee leads to surgical complications\nA highly cited study in the The American journal of sports medicine from doctors at the University of Kentucky, Department of Orthopaedic Surgery and Sports Medicine also suggest that patients with medial patellofemoral ligament reconstruction without additional stabilizing treatments suffered from a high rate of continued problems including 5% who continued with recurrent dislocations.(6)\nIn the March 2016 issue of Arthroscopy, university and researchers in Rome working with the Harvard Medical School found conflicting evidence for the use of Medial Patellofemoral Ligament Reconstruction combined With Bony Procedures (bone reshaping) for Patellar Instability. Enough so that they were unable to identify an absolute indication for this type of surgery.(7)\nThis supports research from the Mayo Clinic published in the American Journal of Sports Medicine that says when you have multiple knee ligament damages – such as in degenerative wear and tear or acute injury – the medial patellofemoral ligament plays a very insignificant role in knee instability and does not even need to be addressed. (8) Of course to a doctor experienced in regenerating ligaments, all ligaments play an important role. In surgery, many times supportive tissue is discarded.\nAthletes with pain often feel there is no other choice but a surgical procedure, even a drastic one\nAthletes with pain often feel there is no other choice but surgical procedures, even drastic ones. A good example of drastic surgery is the recommendation to surgically remove the patella in order to remove the pain. This sometimes does relieve the pain, but at a significant cost to the body. The strength to extend the knee is reduced by about 30 percent, and the force exerted in the knee is increased.\nThere are a host of other risks associated with surgery. The patient must realize that with each procedure and each shaving or cutting of tissue, NSAID (non-steroidal anti-inflammatory drug) prescription, or cortisone shot, the odds of developing long-term arthritis are greatly increased. The key to keeping the knee strong is to stimulate the area to heal, not to cover up the pain with a cortisone shot or NSAID. Even worse is to eliminate the painful area by shaving or cutting. This just delays the pain for a few years until the remaining tissue becomes degenerated. The best approach for the athlete is to stimulate the area to heal.\nThe regenerative medicine option. Prolotherapy injections to pull the kneecap into place and keep it there.\nIn the research above you see that a first-time dislocation of the kneecap creates short and long-term problems of knee instability which leads to long-term problems of chronic patella subluxation or simply, chronic dislocation of the kneecap. This is also referred to as patellofemoral tracking syndrome. This is where the kneecap floats out of the groove on the femur (thigh bone) it is supposed to sit and glide up and down on.\nAs we have seen in the research above, this can equally effect an adolescent athlete as well as an older athlete.\nThe kneecap is supposed to stay in the middle of the knee, in its groove. But because of past dislocations or advanced knee instability, the kneecap wanders to the sides. When this occurs not only is there a problem of the maltracking patella, there is also a problem of accelerated wear and tear and the development of osteoarthritis. This can also lead to a problem of Patellofemoral Pain Syndrome.\nProlotherapy injections stabilize the unstable knee and regenerates damaged tissue. Please continue with this article Treatment of Patellofemoral Pain Syndrome.\nIf you have questions about patellar instability treatment options, get help and information from our Caring Medical staff\nReferences for this article\n1 Lee DY, Park YJ, Song SY, Hwang SC, Park JS, Kang DG. Which Technique Is Better for Treating Patellar Dislocation? A Systematic Review and Meta-analysis. Arthroscopy: The Journal of Arthroscopic & Related Surgery. 2018 Oct 6. [Google Scholar]\n2 Jain NP, Khan N, Fithian DC. A Treatment Algorithm for Primary Patellar Dislocations. Sports Health. 2011;3(2):170-174. doi:10.1177/1941738111399237. [Google Scholar]\n3 Schorn D, Yang-Strathoff S, Gosheger G, Vogler T, Klingebiel S, Rickert C, Andreou D, Liem D. Long-term outcomes after combined arthroscopic medial reefing and lateral release in patients with recurrent patellar instability–a retrospective analysis. BMC musculoskeletal disorders. 2017 Jun 24;18(1):277. [Google Scholar]\n4 Caplan N, Nassar I, Anand B, Kader DF. Why Do Patellofemoral Stabilization Procedures Fail? Keys to Success. Sports Med Arthrosc. 2017 Mar;25(1):e1-e7. [Google Scholar]\n5 McConnell J. Management of a difficult knee problem. Man Ther. 2012 Jun 27. [Google Scholar]\n6. Shah JN, Howard JS, Flanigan DC, Brophy RH, Carey JL, Lattermann C. A Systematic Review of Complications and Failures Associated With Medial Patellofemoral Ligament Reconstruction for Recurrent Patellar Dislocation. Am J Sports Med. 2012 Jun 7. [Google Scholar]\n7. Longo UG, Berton A, Salvatore G, Migliorini F, Ciuffreda M, Nazarian A, Denaro V. Medial Patellofemoral Ligament Reconstruction Combined With Bony Procedures for Patellar Instability: Current Indications, Outcomes, and Complications. Arthroscopy. 2016 Mar 28. pii: S0749-8063(16)00043-8. [Google Scholar]\n8. Allen BJ, Krych AJ, Engasser W, Levy BA, Stuart MJ, Collins MS, Dahm DL. Medial patellofemoral ligament tears in the setting of multiligament knee injuries rarely cause patellar instability. Am J Sports Med. 2015 Jun;43(6):1386-90. doi: 10.1177/0363546515576902. Epub 2015 Mar 25. [Google Scholar]"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:5d39c00d-20a1-440f-ba7f-ffafe55d9936>","<urn:uuid:c7240ebb-a55c-4a79-a689-40b93d1349b5>"],"error":null}
{"question":"What information does a CL-100 report contain about wood-destroying fungi, and how do subterranean termites differ from other termite species in their behavior and detection?","answer":"A CL-100 report documents evidence below the first main floor of both active wood-destroying fungi (wood moisture content ≥ 28%) and non-active wood-destroying fungi (wood moisture content < 28%). It also notes excessive moisture conditions below the first main floor (20% or above wood moisture content, standing water). Regarding subterranean termites, they differ from other species by nesting in soil rather than above ground, and they build distinctive mud tubes and bring soil to the wood where they feed. Unlike drywood termites, which nest in wood above ground and produce pellets while feeding, subterranean termites require contact with soil and construct mud tubes for travel. These termites form colonies of 60,000 to 1 million individuals, with workers constantly foraging for cellulose materials. A single colony can cover up to 260 ft linear distance and consume about 5 grams of wood daily under ideal conditions.","context":["Buying or selling a home?\nIf you’re buying a home in South Carolina, your mortgage company may request a CL100. The CL100 report is also referred to as a termite letter or South Carolina Wood Infestation Report.\nIf you’re selling a home in South Carolina, your home will more than likely need to get a CL-100 report (CL100) completed prior to the closing of your home.\nWhat is a CL-100 or Wood Infestation Report?\nA CL-100 Wood Infestation Report is a document produced in South Carolina by a licensed pest control professional, to provide information about the presence and extent of wood-destroying insects in a building. The report is used by real estate agents, homeowners, and buyers to assess the potential risks and damages to the building, as well as to determine the necessary treatment and repairs.\nIn South Carolina, the CL-100 Report is a standardized form that must be completed by a licensed pest control professional following a visual inspection of the property. The report covers the following areas:\n- Identification of the property, including the address, date of inspection, and name of the inspector.\n- Information about the structure, including the type of building, age, and construction materials.\n- Details about the inspection, including the scope of the inspection, and any areas that were inaccessible or not inspected.\n- Identification of any visible evidence of wood-destroying insects, such as holes, tunnels, or piles of sawdust.\n- A description of any conditions that may attract or harbor wood-destroying insects, such as excessive moisture, improper ventilation, or the presence of stored lumber.\n- A list of the species of wood-destroying insects that were found, if any, and an assessment of the extent and severity of infestation.\n- Recommendations for treatment and/or repairs, including any necessary follow-up inspections.\n- A signature line for the inspector and a statement indicating that the report is based on a visual inspection only, and that no guarantees are made as to the absence of wood-destroying insects.\nIt is important to note that a CL-100 Report is not a guarantee of freedom from wood-destroying insects, but rather a snapshot of the conditions at the time of the inspection. The report is intended to help property owners make informed decisions about the potential risks and expenses associated with owning a building.\nIn some cases, a CL-100 Report may indicate the presence of wood-destroying insects and the need for treatment. This may include the application of insecticides or the removal and replacement of infested wood. In other cases, the report may show no evidence of wood-destroying insects, but recommend follow-up inspections or suggest changes to the property that may help to prevent future infestations.\nThe CL-100 Wood Infestation Report is a valuable tool for property owners, real estate agents, and buyers in South Carolina, providing critical information about the presence and extent of wood-destroying insects in a building. It helps to ensure that property owners and buyers are aware of the potential risks and expenses associated with owning a building, and that they are able to make informed decisions about treatment and repairs.\nTIP: If you are buying a home, it’s best to hire your own home inspector to provide the CL100 inspection report instead of having the seller arrange for the inspection.\nWhat information is included in the Wood Infestation Report?\nThe CL-100 inspection report should include detailed information, such as:\n- If there is visible evidence of:\n- Subterranean termites\n- Drywood termites\n- Old house borers\n- Powder post beetles\n- Other wood-destroying insects.\n- If there is visible evidence of prior subterranean termite treatment.\n- If there is evidence below the first main floor(or crawl space) of the presence of\n- Active wood-destroying fungi (wood moisture content ≥ 28%)\n- Non-active wood-destroying fungi (wood moisture content < 28%).\n- If there is evidence of the presence of excessive moisture conditions below the first main floor (20% or above wood moisture content, standing water, etc.)\nIt is important to have an independent inspection completed on the home prior to your purchase. The inspection and detailed report will identify any active or previous wood-destroying insects or fungi problems.\nThe report should also make recommendations for other items such as treatments, moisture corrections and/or repairs. Areas inspected should include the crawlspace, attic, exterior and interior. Once the inspection is complete, the inspection report should be provided to you.\nAre You a Real Estate Agent?\nBarrier Pest Services provides CL100 services for the Charleston area, including North Charleston, Mt. Pleasant, Summerville, Goose Creek, Moncks Corner, Hanahan, James Island and Johns Island. All inspections are performed by a qualified, licensed South Carolina inspector.\nWe also have a licensed Residential Home Builder on staff who can provide a better understanding of the inspection results including any damages and/or recommendations.\nWe provide many inspections daily and are available to help you with your next real estate transaction.","Fact Sheet FS338\nTermites are the most destructive insects to homes. In the U.S., termite control costs alone exceeds $1.5 billion each year. Most people are not aware of termites until significant damage to the structure has occurred. It is therefore very important to understand the basic biology, prevention, and control of termites.\nAll naturally occurring termites in the northeastern U.S. belong to a group called subterranean termites (genus Reticulitermes) (Figure 1). The common name comes from their habit of nesting in the soil. Their nests are rarely found above ground. Subterranean termites build mud tubes and bring soil to the wood where they feed (Figure 2 and 3). In the southern U.S. and western U.S., drywood and dampwood termite species are also found. They nest in wood above ground. Occasionally, drywood termites may be transported along with infested furniture or timber to other parts of the country. Drywood termites do not bring soil to the wood, and they produce pellets as they feed. These termites usually do not cause damage to structures outside their natural distribution areas.\nTermites are social insects with various castes. Most of the members are white in color. Only the king, the queen, and swarmers (also called alates) are darkly pigmented. Soldiers' heads are reddish. Most people encounter termites during spring when termite swarmers (Figure 2) leave their natal nests. Termite swarmers may be confused with ant swarmers but can be distinguished by the following:\n|Points of Difference||Termite Swarmers||Ant Swarmers|\n|Size of wings||Hind wings are approximately the same size as the forewings.||Hind wings are much smaller than the forewings.|\n|Color of wings||Dusky||Clear|\n|Veins of wings||Weak||Very distinct|\n|Shape of body||The abdomen is joined snugly to the thorax, without a node or waist.||The abdomen is distinctly separated from the thorax by a very small node or thin waist.|\nEach subterranean termite colony contains reproductives (a king, a queen, and some neotenic reproductives), larvae, nymphs, workers, and soldiers (Figure 1). The neotenic reproductives are non-alates, and they produce fewer eggs than the queen. The majority of colony members are workers. They look for food, feed the other members of the colony, and maintain the nest and foraging tunnels. Soldiers have rectangular brownish heads and long mandibles. They typically account for 1–2% of the members. Nymphs develop into alates or neotenic reproductives. A colony may have hundreds or thousands of neotenic reproductives. The neotenic reproductives play an important role in the growth and expansion of colonies. Thus, killing the queen will not cause the colony to die. A mature colony has 60,000 to 1 million individuals.\nSubterranean termites naturally occur in forests and urban environments where cellulose materials (such as stumps, mulch, and dead trees) exist. Most of the houses in the U.S. contain wood materials. When the wood materials become moist or contact the soil, termites will find their way into the wood. Workers constantly forage for food. Their only food is cellulose material such as wood and paper. Under ideal conditions, a termite colony of 60,000 would consume about 5 grams, or 1/5 ounce of wood each day. A subterranean termite colony may cover 260 ft linear distance.\nA new subterranean termite colony can be formed in two ways:\n- a pair of swarmers (a male and a female alate) mate, drop wings, and build a nest;\n- budding from an existing colony.\nThe swarming adults only appear once or twice from mature colonies every year. Hundreds or thousands of swarmers leave the nest during warm sunny days between April–June. This is when most homeowners first notice termites in their homes. After a brief flight, the swarmers soon drop their wings and form pairs. They then look for a moist, sheltered place to build a nest. It takes several years for the colony to mature and produce swarmers. As the colony grows, part of it may separate from the main colony and form a new colony. Neotenic reproductives head such budded colonies.\nHow to Conduct a Proper Termite Inspection?\nIt is not difficult for a layperson to inspect a home for termites when obvious signs are present (Figure 3 and 4). Unfortunately, mud tubes are often absent, making it very difficult to determine whether termites are present. It is even more difficult for a layperson to distinguish between old termite damage and new termite damage. For these reasons, homeowners are encouraged to consult with experienced professionals for a quality inspection. The following are the general procedures for conducting a termite inspection. First, obtain inspection tools. A flashlight and a probing tool (garden trowel, screw driver, or ice pick) are a must. Knee pads, a hard hat, a mirror, collection bottles for specimens, and a moisture meter will help with an inspection. An inspection should focus in areas where there is high moisture and wood or other cellulose materials: window sills, the bottom of door frames, the crawl space, the basement, and foundations immediately above ground, etc. Look for mud tubes and damaged wood. Tap the exposed wood with a probing tool. A hollow sound may reveal damage beneath the wood surface. Old houses may have mud tubes from previous infestations. Gently break a small section of the mud tubes to determine whether they are moist and whether termites are currently present. Active mud tubes can be distinguished from old mud tubes by a darker color, moist soil, and presence of live termites. Use a garden trowel to examine the mulch near the foundation. If termites are found in the mulch, the structure should be very thoroughly inspected, since termites can occur in the immediate vicinity. It is a good idea to inspect the house at least once a year in the summer to detect termite activity.\nIt is usually much easier and cheaper to conduct preventive work than to treat an infestation. Thus, property owners should follow common sense rules to avoid expensive treatment and repair afterwards.\nFirst, avoid wood-to-soil contact or use treated wood if wood has to be used in a moist area. Ideally, any wood in a structure should be at least 6 inches above the soil. Repaint any wood that is close to the ground every few years to prevent moisture intrusion. Do not bury any wood materials (stumps, branches, wood debris) when building a home. Do not place cellulose materials (such as firewood, mulch) immediately adjacent to a house. Using rocks or rubber mulch near the house will reduce the probability of termites remaining near the house. Cut down shrubs or large trees and remove stumps near the foundation to reduce the presence of roots and plant debris.\nSecond, reduce moisture and promptly repair leaks. The gutters and down spouts should be properly installed and maintained. Seal cracks and holes on exterior walls to prevent moisture getting inside the walls. Crawl spaces should be properly ventilated.\nThird, use treated wood or steel in porches and other areas that are susceptible to termite attack.\nFourth, install stainless steel mesh or pesticide-impregnated sheeting for new construction. These technologies are currently not widely used in the U.S.\nOnce termites are found in a home, treatment is necessary to kill them. There are two types of treatments commonly used: soil treatment and baiting. Proper treatment requires special training, equipment, and materials. Homeowners should look for licensed professionals to properly eliminate termite infestations.\nA soil treatment requires digging a trench around the exterior perimeter of the house. A liquid insecticide is then applied to the trench to form a continuous barrier between the house and the soil outside of the house (Figure 5). In conjunction with the exterior treatment, the insecticide may be applied also locally inside homes in areas where termite activities are found. In areas covered by concrete or wood, small holes are drilled every 12 inches or as the label directs (Figure 6). Insecticides are then injected with a rod, and the holes are plugged or otherwise filled. Because termites nest in the soil and constantly travel back and forth between the soil and the wood, they come in contact with the treated soil. When properly treated, a colony may be eliminated within one or two months. The soil treatment can be effective for more than 5 years.\nCommonly used liquid termiticides can be grouped into non-repellent and repellent termiticides. Non-repellent termiticides include fipronil, imidacloprid, and chlorfenapyr. These active ingredients are relatively slow-acting. Termites freely re-enter treated soil. They pick up enough dose of the material as they pass through the treated areas and transfer the chemical to their nestmates, causing the death of the whole colony. Commonly used repellent termiticides include permethrin, cypermethrin, fenvalerate, and bifenthrin. They belong to a chemical group called pyrethroids. Pyrethroids are highly repellent to termites and provide a barrier around the structure. However, pyrethroid treatments do not diminish termite populations in soil. Termites in the vicinity of the treatment would continue to forage freely. Thus, non-repellent termiticides are preferred materials used by professionals for treating structures that are already infested.\nBaiting is another commonly used termite treatment method. In this method, plastic tubes are installed underground at approximately 10 ft. intervals encircling the house (Figure 7). Each tube contains wood and/or toxic bait. The bait tubes are examined regularly, from monthly to annually, depending on the type of bait tubes being used. Termites foraging around the house will eventually find the wood or bait. The bait is more palatable than the wood and contains a slow-acting material. Termite workers eat the bait and bring the material to the rest of the members of the colony. A colony will be killed over a few months once termites find the bait. Aboveground termite bait stations can be placed inside houses where termite activity is found (Figure 8). Each termite bait needs to be inspected periodically (quarterly to annually) to ensure enough bait remains, and that termites are eliminated. Commonly used active ingredients in baits are noviflumuron, hexaflumuron, and diflubenzuron.\nIn general, liquid treatment is often cheaper than bait treatment, depending upon the size of the structure to be treated. Liquid treatment provides immediate protection to the structure. Disadvantages are:\n- The procedures are somewhat destructive (such as drilling holes into concrete surfaces, digging a trench).\n- A large volume of insecticides is applied to the environment.\n- Structures with a well, spring, or cistern nearby cannot be treated with liquid insecticides due to possible water contamination issues.\nBait treatment has little environmental impact because the bait is contained inside capped plastic tubes. After termites are eliminated, the bait can be removed and no pesticides remain on the property. The equipment needed for bait treatment is simple. The treatment causes little destruction to the property. Disadvantages of this method include:\n- It may take several months or more for termites to find the stations.\n- The lag time between monitoring and baiting extends the period to achieve termite control.\n- The baits need to be inspected periodically and maintained.\n- Homeowners, their children, or pets may dislodge the bait stations through gardening, playing, or mowing activities.\nDo I Need Annual Termite Inspection Service?\nPest control companies often recommend annual inspection services after termite treatment to detect future termite activity. This is helpful to detect termite infestation early. Homes surrounded by large trees, old stumps, or beds of deep mulch are more likely to have new infestations over time. Alternatively, homeowners may use over-the-counter monitoring stations for monitoring termite activity. Strategically place the monitors in areas that favor termite survival such as in the mulch, besides a stump or wooden deck. Inspect the monitors a few times a year between April–November. Termite infestations in the northeastern U.S. almost always start from outside. Installing monitoring stations around homes is a good proactive method for detecting new termite activity.\nMention or display of a trademark, proprietary product, or firm in text or figures does not constitute an endorsement by Rutgers Cooperative Extension and does not imply approval to the exclusion of other suitable products or firms.\nPhoto credits: University of Georgia (Figure 4 right); Changlu Wang (Figures 1–3, 4 left, 5–8).\nCopyright © 2022 Rutgers, The State University of New Jersey. All rights reserved.\nFor more information: njaes.rutgers.edu.\nCooperating Agencies: Rutgers, The State University of New Jersey, U.S. Department of Agriculture, and Boards of County Commissioners. Rutgers Cooperative Extension, a unit of the Rutgers New Jersey Agricultural Experiment Station, is an equal opportunity program provider and employer."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:4ed0cdad-2034-49f5-8ce9-068f4efd6690>","<urn:uuid:0a77eddf-99e5-48a2-844b-70a663c9da84>"],"error":null}
{"question":"What will be the future impact on golf course maintenance when comparing the Aussie method of bunker raking versus Capillary Concrete technology?","answer":"Capillary Concrete technology offers a more comprehensive long-term solution for bunker maintenance through its engineered moisture control system, which moves water both up and down to regulate moisture content and prevents sand migration and waterlogged bunker bases. It provides a permanent solution that can last for decades with a freeze-thaw guarantee. The Aussie method of bunker raking, while showing current success in reducing washouts and buried lies through firm perimeter maintenance, primarily addresses immediate maintenance concerns and its labor-saving benefits vary depending on perimeter smoothing time requirements. The technology-based solution of Capillary Concrete appears to offer more sustainable long-term maintenance benefits compared to the manual raking technique.","context":["Capillary Concrete Technology\nMoisture control of the bunker sand is more than just drainage. Moisture control is the ability to rapidly drain rain events and to be able to keep moisture in the bunker sand in dry conditions.\nCapillary Concrete™, a patented and engineered polymer-based concrete, is the only building material that can move water both up and down to precisely regulate the moisture content of any adjacent material, such as sand or soil. Developed in Scandinavia, Capillary Concrete can withstand even the harshest climates with severe winter ground freezing conditions.\nCapillary Concrete combines macropores with micropores to quickly move water down with the force of gravity for drainage, and up against the force of gravity with capillary force. The material’s polymers are what create microscopic connections in the actual binding agent of the Capillary Concrete.\nCapillary Concrete testing shows how moisture moves in a dry sample (left) placed in moisture, to being saturated after 3 days (middle) and the sand sticking to the vertical face (right). The sand stays on the Capillary Concrete due to the capillary connection of moisture between the liner and the sand.\n100% Freeze-thaw guarantee\nCapillary Concrete has been designed to withstand the most severe freeze-thaw conditions for decades without any damage. Special polymers are mixed in the truck that enhance the strength and durability, and provide ample protection from the most severe winter freezing conditions, in any climate, wet or dry.\nThis freeze-thaw strength has been proven in laboratory environments multiple times, but most importantly in the field, in real-life installations for over 10 years. For instance, this 18 hole top-5 course in Oslo, Norway has had almost all green side bunkers installed over 10 years ago and has suffered zero damage to its Capillary Concrete bunkers. This location is one of the toughest golf course sites you will ever find, with some very deep ground frost on wet clay soils that move with the freeze-thaw cycles every year.\nWatch legendary Superintendent Jacques Leonard test Capillary Concrete.\nAvoiding capillary breach\nCapillary breach occurs when the pore sizes of two materials do not match, and moisture will then not be able to move against gravity. For example, capillary breach is used in a USGA green to stop irrigation from exiting into the drainage system until the profile is filled up. Water will not move from a small hole to a large hole until gravity is strong enough to pull it down.\nIn a green, capillary breach is used to store large amounts of water in the sand for the roots. In bunkers, we want to avoid a capillary breach as it only results in sand being excessively wet in the bunker base and shifting and sliding down the face of the bunker with rain events. It also results in sand having different moisture levels on faces and in the base of the bunker, and therefore different playing conditions and higher maintenance costs.\nCapillary Concrete contains large pores that freely drain water by force of gravity, but are not too large in order to avoid migration of sand particles into the drainage system.\nEliminating waterlogged bunker bases\nExcessive moisture in the base of the bunker can be an issue with modern bunker sands. Capillary Concrete is the most effective material to keep the sand in the base of the bunker from becoming overly wet because there is no capillary breach and water drains by both gravity and capillary action. This is more effective than a perched water table situation, which is what every other bunker liner product creates. Think of a USGA green situation where the sand is holding moisture on top of a gravel base. Capillary Concrete works like a paper towel placed under the sand instead, which will more effectively soak up excessive moisture from the sand through its capillary pores. In all circumstances it is always recommended to perform a Water Release Curve test on the bunker sand prior to a final decision and purchase to get a better idea of the sand’s moisture-holding percentage at the desired depth.\nPreventing migration of bunker sands\nIf you let the bunker sand dry out, which can easily happen if the liner is not capillary connected to the soil, sand will migrate down through the drainage system even if the pore sizes bridge like in a USGA greens specification. This is because dry sand can more easily penetrate and move, and dry bunker sand that gets shifted around can get worked in to the drainage system.\nMoist sand stays in place and does not migrate if the pore sizes are kept within the requirements. The key is to make sure the sand stays moist at all times, to avoid capillary breach.\nMoisture being fed to the Capillary Concrete with the drip-line system and beginning to moisturize the steep bunker face.","As some superintendents in the Southeast Region prepare to deal with falling leaves and frost delays, others are gearing up for an influx of golfers ready to enjoy golf this winter. This time of transition is a good opportunity to review several themes that have resonated throughout the region this year.\nManaging Weeds – There is no shortcut\nSuperintendents have access to more products to manage weeds on golf courses than ever before. However, different weed species are becoming problematic and resistance to herbicides is increasing. It seems that whenever a new herbicide is introduced that is particularly effective in controlling a given weed, other weeds that historically were not a problem begin to appear at higher-than-desired levels. Additionally, the increasing occurrence of herbicide-resistant weeds has refuted any notion that a superintendent can rely on an amazingly effective herbicide year after year. To help superintendents with weed management challenges, the University of Tennessee has started a Weed Diagnostics Center that offers herbicide resistance screening.\nForward Tees - Moving in the right direction\nEverybody likes a success story. Currently there is no bigger success story than installing forward tees that offer golfers with slower swing speeds a proportional challenge and enjoyable golf experience. Below are a few of the benefits associated with installing forward tees:\n- Improved golfer experience – As summed up by a longtime PGA professional, “The kindest compliment I received in 30 years was when a golfer who tried the forward tees told me he was planning to quit golf because he couldn't hit it very far any more. But he started playing from the new tees and shot scores that he hadn’t been able to shoot in years. He decided to keep playing.”\n- Potential to reduce fairway maintenance costs – Installing forward tees may allow superintendents to reduce fairway acreage at the beginning of a fairway without increasing the carry distance for players using forward tees.\n- Improved pace of play – When golfers are playing from tees that allow them to reach par-4 holes in two shots and par-5 holes in three shots they will need fewer strokes to get around the course and will be able to play faster.\n- Better for business – Any time golfer enjoyment is increased and pace of play accelerates it is good for business. Happy customers are repeat customers.\nThe “Aussie Method” of Bunker Raking – It continues to catch on\nThe “Aussie method” is a style of bunker preparation where the bottoms of bunkers are raked while perimeters are smoothed. The goal is to allow the sand along the perimeter of bunkers to become firm to reduce washouts and decrease the likelihood of buried lies. Superintendents report success in these areas when using the Aussie method. More importantly, golfers have accepted this style of bunker maintenance. The big question is whether the Aussie method saves labor hours. The answer varies based upon the amount of time spent smoothing the perimeters.\nSoutheast Region Agronomists:\nChris Hartwiger, director, USGA Course Consulting Service – email@example.com\nSteve Kammerer, regional director – firstname.lastname@example.org\nPatrick M. O’Brien, agronomist – email@example.com\nTodd Lowe, agronomist – firstname.lastname@example.org"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:6e7e1e6b-82db-4959-a0db-7485b214aa25>","<urn:uuid:f10dffb9-f03d-412e-b2fc-40999f69f3e4>"],"error":null}