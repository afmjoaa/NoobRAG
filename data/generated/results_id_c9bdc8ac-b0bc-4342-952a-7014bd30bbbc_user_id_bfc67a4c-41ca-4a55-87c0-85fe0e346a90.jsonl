{"question":"What is the projected size of the global B2C eCommerce market for apparel/fashion by 2025, and what was its value in 2020? Favor incluir ambos valores.","answer":"The global B2C eCommerce market for apparel/fashion was valued at $752.5 billion in 2020 and is forecast to reach $1.1 trillion by 2025.","context":["News briefs for the week take a look at cobot solutions for pick n’ pack fashion apparel SKUs, exosuit to manage spastic muscle conditions, homemade ingenuity in service robots, a 3D-printed cobot arm that actually “works”, and the 45-second, robot-made pizza in a moving truck.\nCobot solution: pick n’ pack fashion SKUs\nIf you are a cobot vendor and don’t yet have a solution for picking and packing apparel/fashion SKUs, then you’re missing out on a massive, global B2C eCommerce market segment, which is estimated at $752.5 billion (2020), and forecast for 2025 to be $1.1 trillion.\nApparel and fashion don’t ship on hangers. Most often apparel and fashion items are those SKUs that come to a packing area in shiny plastic packaging that’s highly reflective and difficult for vision systems to see; and therefore, difficult to recognize and sort…and, for that matter, to sort quickly and productively (like 700 SKUs an hour). The same shiny packaging might also present difficulties for a cobot’s gripper to properly pick up and sort.\nTherefore, a specially tailored solution to the apparel/fashion challenge is highly desirable. And Netherlands-based Smart Robotics (founded 2015), which is most definitely in the pick and pack cobot biz, has just announced theirs: the Smart Fashion Picker. How appropriate.\nSmart Robotics’ PR release claims that its Smart Fashion Picker “can pick a large variety of fashion items, which are placed in protective wrapping [the aforementioned [“shiny packaging”], straight from a cart or bin. Vision sensors, motion and task planning algorithms help the cobot to calculate what to pick next, as well as how to pick and place the item accurately into the next bin, and it can easily detect when the bin is empty or filled.”\nSmart Robotics says the Smart Fashion Picker is “one of the many improvements it has in the pipeline to make warehouse automation more versatile and dynamic in a highly performant logistics industry”.\n“We’re super excited to introduce this product to the wider market after successful implementation with our initial customers,” says Johan Jardenvall, CEO at Smart Robotics.\nOf course, only time will tell how effective the Smart Fashion Picker is on the job, but, for now, the vendor is presenting the desired tailored Pick N’ Pack system at the exact right time during a growth spurt in the industry.\nWhen exoskeletons become clothes: the exosuit\nThose who suffer from multiple sclerosis (MS), cerebral palsy (CP) or stroke don’t see an exoskeleton or exosuit, rather, they see a godsend!\nInvented in 2009 by Swedish chiropractor, Fredrik Lundqvist, the Exopulse Mollii Suit’s full-body exosuite, with 58 embedded electrodes, fire 20 times per second using targeted electric impulses in order to correct mobility problems. The exosuit is known officially as a drug-free, non-invasive spasticity management device with innovative, near-full-body neuro-stimulation. To Lundqvist, it’s a wearable robot.\nIn 2021, German med-tech giant Ottobock acquired Lundqvist’s Exoneural Network, buying 100% of the outstanding shares. Together with its previous acquisition of US-based SuitX (occupational exoskeletons), Ottobock is now the global leader of prosthetics, orthotics, and exoskeletons.\nLundqvist’s exosuit, still manufactured in Sweden, comes in 37 different sizes starting from 2–3 years of age and up to 5XL for men and women. Cost: $5800 or $540 rental per month.\nBetter still, a lifetime sentence of involuntary and continuous muscle spasms is 100% corrected by a wearable robot (see video).\nIn praise of homemade robots\nCOVID has prompted the spending worldwide of some $228 billion for purchases of endpoint devices, robotics, and drones…just in 2021 alone! But it doesn’t take billions of dollars, as a small town in Indonesia has shown, to create service robots that disinfect and tote food and supplies to those in lockdown. Ingenuity and localized innovation can go a long way.\nWith a robot’s head made from an old rice cooker, chest from a discarded television set, and its base from a junked toy car chassis, the residents of the Tembok Gede district of Surabaya, capital of the Indonesian province of East Java, are making due by basically dumpster-diving and improvising robotics gear.\nParticularly hard hit with COVID, especially the Delta variant (Indonesia has had 3.68 million COVID infections and more than 108,000 deaths), the villagers of Tembok Gede, under the direction of a few local engineering professors, constructed a service robot they named Delta.\nAseyanto (uses only one name), a neighborhood leader who heads the “Delta robot” project, said: “With this new Delta variant and the surging number of COVID-19 cases, I decided to turn the robot into one used for public services such as to spray disinfectant, deliver food and meet the needs of residents who are self-isolating.”\nAfter moving down the street to the home of a self-isolating villager, the robot’s speaker sends out the message “Peace be with you,” followed by “A delivery is here. Get well soon.”\nOperated by remote control, with twelve hours of battery life, Delta is now one of several robots made in the village that has won Tembok Gede a reputation for its creative use of technology.\n3D-printed robot arm that really works!\nThere are many 3D-printed robot arms available, but most are wonderful educational toys and not cut out for real work, like machine tending in a factory or piece picking in e-commerce.\nNot so for Real Robot One or RR1 from the laboratory of Pavel Surynek. Sure, it’s a prototype, but it’s a first, and may well lead to a near future of high-grade, 3D-printed robot and cobot arms becoming regular players in industrial robotics.\nThe criteria for a useful robot or cobot arms are precision, accuracy, repeatability, and strength; and the RR1 checks off all those boxes, which has been a near impossibility via any 3D-printing process, until now.\nRR1 (Real Robot One),” like most multi-axis robot and cobot arms, has stepper motors that rotate the joints. However, the RR1 is unique in that each motor has a 3D-printed planetary gearbox to increase torque and motor output resolution.\n“Those help the robotic arm lift quite a bit of weight and move with precision,” says Surynek. The RR1 has six joints, so it contains six of those gearboxes. The final degree of freedom is the end effector, which is a servo-driven gripper.”\nIn this drawing (see exploded view), the planetary gearbox consists of 3 planets—herringbone gears—and one middle gear connected to the motor axle.\nAmazingly, Surynek has pulled off the design engineering and manufacturing of this complex planetary gearbox with a 3D printer, which is a key development in making robot and cobot arms truly useful.\nRobot’s record-setting, 45-second pizza!\nFounded in 2019, by former SpaceX engineer Benson Tsai, (and 23 other former SpaceX employees), but not launching until the fall of 2022, new robot startup, LA-based Stellar Pizza, has trained its robots to knock out an oven-ready, large pizza in 45 seconds.\nThat’s record-setting speed for an autonomous machine, but still, a slowpoke compared to two-time world champion, Canadian Werner Lomker, who made three large pizzas in an impressive 47.56 seconds—equal to making each pizza in roughly 15 seconds!\nOf course, Stellar Pizza’s robots can work the dough, tomato sauce, and grated mozzarella circuit around the clock without stopping; Lomker’s productivity, on the other hand, has its limits.\nWith the global pizza industry worth $160 billion in annual sales (largest market Western Europe at $60 billion, followed by North America (U.S. and Canada) with $56 billion), there’s value in speedy, highly productive robots.\nAccording to Tsai, the main problem that Stellar Pizza solves is the rising food costs in the United States. The USDA’s inflation watch concluded that found there was more inflation for grocery and supermarket store purchases then for restaurants.\nStellar Pizza hopes to address this problem directly by selling directly to its target customer: people who want food quickly and conveniently. Stellar Pizza has raised $9 million from three VC firms.\n“The process to make them isn’t terribly different from that of pizzas made by human hands; first, a metallic arm lowers into a refrigerated box and grabs a ball of dough, depositing it onto a conveyor belt, where a disc lowers to press it into a 12-inch circle (the company makes the dough at its headquarters then loads it into the machine’s refrigerator in pre-portioned balls).\n“As the raw crust moves along on the belt, various machines dot it with tomato sauce, shake cheese onto it, add other toppings, then lift it into a 900-degree oven for baking. The pie’s journey is tracked by cameras and sensors, which can make adjustments as needed.”\nEach 12-inch, 45-second pizza, depending on topping choices, costs $7 to $10.\nBTW: All of this takes place in a moving truck!"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:51a22d33-4d97-47c2-af6c-369406117138>"],"error":null}
{"question":"What are the critical safety considerations for material auto-ignition in storage facilities, and how do HVO storage regulations address these concerns?","answer":"For material storage safety, a substance with a low auto-ignition temperature presents a greater fire hazard than one with a high auto-ignition temperature, particularly when stored in silos, bins, and fuel deposits where minimum ignition temperatures decrease rapidly. For HVO storage safety, specific regulations require that tanks over 200 liters must have a secondary containment system capable of holding 110% of the primary tank's contents. Additional safety measures are required if the tank is located within 10 meters of coastal waters or inland fresh waters, within 50 meters of a drinking water source, or where spills could reach public water sources. While HVO follows the same regulations as regular oils, it is inherently safer due to being non-toxic and biodegradable, though proper containment is still mandatory for environmental protection.","context":["You can often find it in the section 9 of safety data sheets.. Regulatory Implications of Auto-ignition Temperature Table 1: Typical minimum ignition temperatures of various dusts. Minimum Auto-ignition Temperature of Hydrocarbons  Little Pro on 2016-01-13 Views: Update:2019-11-16. Autoignition Temperatures and Flash Points for Hydrocarbons - Autoignition temperature and flash point ( °C and °F) of different types of hydrocarbons with varying carbon number up to C12 ; Biogas - Carbon Nitrogen Ratios - Carbon - Nitrogen ratios for biogas produced from various raw materials Ignition Temperatures of Materials Auto-ignition temperature - the minimum temperature required to ignite a dry material in air without a spark or flame being present: Fuel or Chemical A material with a low auto-ignition temperature is a greater fire hazard than a material with a high auto-ignition temperature. Ignition temperature definition is - the lowest temperature at which a combustible substance when heated (as in a bath of molten metal) takes fire in air and continues to burn —called also autogenous ignition temperature. ★ HOPE THIS HELPS U BUDDY (^_^) !! Minimum ignition temperature (MIT) Minimum ignition temperature of a dust cloud The minimum ignition temperature (MIT) is the lowest temperature of a hot surface that will cause a dust cloud, rather than a dust layer, to ignite and propagate flame. » BEST EXAMPLE : Candle flame & Fire. ★ While autoignition temperature is measured in a laboratory, the actual temperature of ignition in a vehicle is significantly affected by many factors. » IGNITION TEMPERATURE : It means that a substance is the lowest temperature at which it spontaneously ignites in normal atmosphere without an external source of ignition. The auto-ignition temperature or kindling point of a substance is the lowest temperature at which it will spontaneously ignite in a normal atmosphere without an external source of ignition, such as a flame or spark. Auto-ignition Temperature. For example, the minimum ignition temperature of dust clouds and layer deposites are given in the following table. minimum auto-ignition temperature is the lowest temperature at which the fuel vapours spontaneously ignite. The test follows EN ISO/IEC 80079-20-2:2016 (Explosive atmospheres Part 20-2: Material characteristics - Combustible dusts test methods). This temperature is required to supply the activation energy needed for combustion. Auto-ignition temperature is the lowest temperature at which a chemical will spontaneously ignite in a normal atmosphere without an external source of ignition, such as a flame or spark. When material is stored in silos, bins and fuel deposits, the minimal ignition temperature drops rapidly. Experiments have found, for example, that the auto-ignition temperature for newspaper is about four degrees lower than that of the filter paper used in chemistry laboratories. Hydrocarbons that have been heated can ignite if they are exposed to air. The Auto-Ignition Temperature (AIT) is the minimum temperature at which a chemical (e. g., vapor) will spontaneously ignite without an external ignition source such as a spark or pilot flame – under specific conditions – in an oxidant, which is usually air. Hot surface ignition is governed by the temperature of surfaces in the vehicle, the autoignition properties of the flammable materials, and factors that influence these parameters. Fuels and their flash points for some common substances at atmospheric pressure are indicated below: Fuel Type: Temp (Deg C) Temp (Deg F) Acetaldehyde -37.78-36: Acetone -17-78: 0: Benzene -11.11 Autoignition / Hot Surface Ignition. The figure below shows the auto-ignition temperatures of hydrocarbons at atmospheric pressure.\nApex Legends Canvas,\n80/20 Portfolio Historical Returns,\nDance Zumba Definition,\nDog Breeds That Can Walk Off-leash,\nBloem Self Watering Planter,\nBaked Potato Topping Combinations,","Tuffa Tanks are a leading storage tank manufacturer. With have over 30 years’ experience manufacturing tanks and boast a huge product range including steel and plastic tanks designed for fuels, AdBlue®, chemicals and water.\nOur guide to HVO fuel tanks has been made to educate fleet operators, those using electricity generators, oil-heated homeowners and anyone with an interest in reducing their carbon footprint. Topics discussed include the benefits of HVO, regulations, storage options and more.\nWhat is HVO?\nHydrotreated Vegetable Oil (HVO) is a sustainable, low-carbon biofuel that derives from waste products. The fossil-free fuel is currently available as a drop-in alternative to mineral diesel (DERV & gas oil), however, it will likely become commercially available as a replacement for kerosene to help decarbonise off-grid properties.\nHow is HVO made?\nHydrotreated vegetable oil is produced by treating oils with hydrogen. This involves adding hydrogen to oil’s molecules and removing esters and contaminants from the fuel. The process makes a paraffinic fuel that is almost chemically identical to mineral diesel or kerosene but with many additional advantages over the fossil fuels.\nWhat can HVO be made from?\nHVO can be made from a mixture of vegetable oils and animals fats. HVO from reputable suppliers does not contain virgin palm oil and has been certified by the ISCC (International Sustainability and Carbon Certification) as a sustainable fuel. Waste products that HVO can be made from include:\n• Animal waste:\no Fat from food industry waste\no Fish fat from fish processing waste\no Tallow – a rendered form of beef or mutton fat\n• Vegetable waste:\no Residues from vegetable oil processing\no Used cooking oil\no Technical corn oil\no Tall oil pitch palm oil\no Palm oil mill effluent (POME)\nWhat is the difference between HVO, diesel and FAME?\nMineral diesel is a fossil fuel made from refined crude oil. FAME is a first generation biofuel made from fresh organic matter. HVO is a second generation biofuel which is derived from sustainable waste products and emits up to 90% less CO2 emissions than mineral diesel. While the chemical composition of all three fuels is very similar, certain properties of HVO makes it cleaner, safer and improves the storage life. For example, HVO has higher cetane levels which means the fuel ignites\nfaster and more completely giving the engine higher performance and producing fewer emissions.\nCheck out the table below to compare the properties of HVO with diesel and FAME:\nWhat is a first generation biofuel?\nFirst generation biofuels are produced from fresh organic matter such as rapeseed and cereals. Effectively, first generation biofuels use land to grow crops that could otherwise be consumed. Additionally, first generation biodiesel is made up of fatty acid methyl esters (FAME) so is also more susceptible to oxidisation and diesel bug.\nWhat is a second generation biofuel?\nHVO is a second generation biofuel as it is produced from 100% waste products such as used cooking oils and animal fats. As HVO derives from waste products it is considered 100% sustainable and renewable. With no FAME or sulphur content HVO is a more stable than first generation biofuels which are susceptible to oxidation and diesel bug.\nWhat is a third generation biofuel?\nThere is currently greater research into and production of third-generation biofuels comprising of microalgae. If commercially viable, this could be particularly beneficial to carbon targets as microalgae can ‘absorb’ more carbon than trees. This process then creates more algae which can be converted into fuel suitable for industries including aviation.\nWhat are the benefits of HVO?\nThere are many benefits of HVO when compared to mineral diesel, kerosene or even first generation biofuels. The advantages of fueling your machinery with HVO doesn’t just stop at making a smaller carbon footprint or improving corporate social responsibility, there are tangible benefits to your business and equipment too. The advantages of HVO include:\nA reduction in greenhouse gas emissions and harmful byproducts\nWhen compared to diesel or kerosene, HVO fuel reduces CO2 emissions by almost 90%. HVO usage also reduces nitrogen oxide (NOx) emissions by up to 27% and particulate matter (PM) emissions by up to 84%. This not only reduces greenhouse gas emissions but contributes to overall cleaner air.\nHVO fuel is 100% renewable and has been certified by the International Sustainability and Carbon Certification (ISCC) as sustainable. It is also a second-generation biofuel which means that it derives entirely from waste products that would likely otherwise go to landfill. It should also be noted that the production of HVO from reputable suppliers does not contain any products which contribute to deforestation.\nPure hydrotreated vegetable oil is odourless, non-toxic and biodegradable. Therefore, leaks and spills of HVO have a considerably less detrimental impact to the environment than diesel or kerosene. The flash point of HVO is also higher than mineral diesel which decreases the risk of a fire hazard.\nLong storage life\nHVO fuel is perfect for long-term fuel storage and can be stored for up to 10 years which makes it ideal for applications where long-term storage is required such as backup generators. As the hydrogenation process removes oxygen from the fuel, there is a significantly reduced risk of degradation or oxidation. HVO does not absorb water like first generation biofuels so does not provide an environment where diesel bug can thrive. This also removes the need for regular fuel testing and maintenance programs to remove water.\nCold weather performance\nHVO has a considerably lower freeze point (-40°C) than diesel (-8°C). While this isn’t often a concern in the UK, it is one reason why HVO is so suitable for the decarbonisation of the aviation industry.\nFor most applications HVO is completely interchangeable with diesel and can completely replace diesel or be blended with diesel in any ratio. Many Original Engine Manufacturers (OEMs) including popular passenger car manufacturers, freight vehicle manufacturers and non-road vehicle manufacturers have approved the use of HVO fuel in their vehicle’s engines. If your vehicle has an OEM that has approved the fuel then there is no risk of voiding the warranty.\nUsing HVO for heating with existing oil-fired boilers will likely require only minor modifications to the boiler. Trials have shown the process is quick and relatively inexpensive at around £500.\nBetter for machinery\nHVO’s clean-burning properties significantly reduced particulate production (up to 84%) which helps to improve the engine cleanliness, prolong the lifetime of emission control systems (where fitted) and decrease the ageing of engine oils. Additionally, as HVO does not react with water or oxygen, storage of the fuel avoids sludge build-up and diesel bug so prevents filters from blocking and contaminants entering your equipment.\nWhich manufacturers have approved HVO in their vehicles?\nDue to the chemical composition of HVO being near identical to diesel, HVO works as a direct diesel replacement or blend in the vast majority of vehicles. Many Original Equipment Manufacturers (OEMs) have tested HVO in their vehicles and have approved HVO as a valid fuel. This means HVO100 can be used in the vehicle without invalidating the warranty. Here is a list of OEMs who have approved HVO in their vehicles:\nVehicle manufacturers who have approved HVO\nHeavy duty road vehicles\nWhile not all OEMs have approved HVO100 in their vehicles it should be noted that some blends of HVO meet EN590 fuel specification (the same as diesel) and can be used without requiring OEM approval.\nTo check whether your model of vehicle has been approved to use HVO100 please check your handbook or contact the manufacturer. To check whether a HVO diesel blend meets EN590 check with your fuel supplier.\nCan HVO be used as a heating oil?\nThe short answer is yes. After a year of successful trials converting domestic and commercial oil-fired boilers to burn HVO, a second phase of trials is underway to test the logistics of making it available to the mass market. Like HVO biodiesel, HVO heating oil (the same fuel but different application) reduces greenhouse gas emissions by nearly 90%. While not yet commercially available, HVO heating oil has been recognised in the UK’s Heat and Buildings Strategy (2021) as a route to decarbonise off-grid homes. Many boiler manufacturers such as Worcester Bosch and Grant have developed HVO\ncompatible boilers in anticipation of the biofuel’s use in UK heating. HVO used for heating has numerous financial and practical benefits particularly when compared to heat pumps. The cost to adapt an existing boiler to burn HVO is around £500. Comparatively, a heat pump installation can cost around £11,000 or £25,000 if retrofitting work is required to make the property’s insulation sufficient for the heat pumps low heat output.\nFor more information about the benefits, legislation and availability of HVO for heating check out our article The route to off-grid heating.\nWhat are the HVO storage regulations?\nIn the UK there are no regulations specifically for HVO biodiesel. Although HVO is safer than mineral diesel or kerosene (as a non-toxic and biodegradable fuel) it follows the same regulations as regular oils. Everyone in the UK storing oil (including diesel, biodiesel, kerosene and blends) in a container with a capacity of over 200 litres must follow oil storage regulations.\nOil storage regulations specify that oil storage tanks above a certain capacity (depending upon the industry and country) must have a secondary containment system with the capacity to store a minimum of 110% of the primary tank’s contents. This is to capture any leaks from the primary tank and prevent oil from contaminating waterways and the environment.\nAdditionally, you will require a secondary containment if the tank is sited in a location where oil spills could reach public water sources, including:\no Where oil spills could run over hard ground and reach coastal waters, inland fresh waters or a drinking water source.\no Where oil spills could run into an open drain or a loose manhole cover.\no Where the tank vent pipes cannot be seen when the tank’s being filled, for example, because the delivery tanker is parked too far away.\no Within 10 metres of coastal waters or inland fresh waters like lakes or streams.\no Within 50 metres of a drinking water source, for example, wells, boreholes or springs.\no In the inner zone of groundwater source protection zone 1\nDo I need a bunded HVO tank?\nUse the below table to identify whether you need a bunded HVO fuel tank.\nWe always recommend buying a bunded tank or constructing a secondary containment for good environmental practice. Not only is it safer for the environment, but many insurance companies will not cover you for spills and leaks if the tank does not have an integral bund or external secondary containment.\nWhat types of HVO fuel tanks are available?\nJust like our standard diesel tank, HVO fuel tanks are available in plastic and steel, as a storage only tank or with dispensing, and in capacities from 900L to 100000L. Unlike diesel, HVO is not currently available at UK fuel stations so it must be stored in private bulk tanks.\nHVO dispensing tanks\nHVO dispensing tanks are forecourt-style fuel dispensers that contain all the equipment needed to store, measure and dispense HVO or HVO and diesel blends. These self-contained dispensers are integrally fitted with all the required dispensing equipment so all that is required is a solid concrete base and an electrical connection. However, we do manufacture plastic tanks with 12/24V pumps charged directly from your vehicle, so even mains electricity isn’t necessary.\nOur HVO dispensing tanks are all bunded, compliant with UK regulations and are available in capacities from 1,350 to 100,000 litres as standard. Optional extras are also available to adapt the tank to individual site requirements. These include hose reels, higher flow rates, flowmeters and fuel\nHVO storage tank\nHVO storage tanks are a simple bunded tank available with a top or bottom outlet or with a bespoke layout if required. Our range of standard HVO fuel tanks come in capacities from 900 to 100,000 litres and are manufactured in plastic and steel. This is a cost-effective solution where integral dispensing isn’t required, for example for oil-fired heating, electricity generators or sites with existing external pumps. HVO storage tanks are ideal when used as a backup power supply with a UPS (Uninterruptible Supply System) such as the one in our case study Test, Trace & Tuffa: Providing emergency power for NHS Lighthouse Lab. This is because HVO has a longer storage lifespan than diesel (up to 10 years) and does not require regular fuel maintenance to remove water or sludge.\nCurrently around 100 homes and businesses around the UK are trialing HVO as a drop-in alternative to kerosene. We can confirm that our range of heating oil storage tanks are compatible with HVO and HVO blends.\nAt a time when HVO becomes commercially available as an alternative to kerosene for heating, our range of HVO storage tanks will be ideal for the safe and cost-effective storage of HVO heating oil.\nSteel HVO fuel tanks\nSteel HVO fuel tanks are very strong and hard-wearing with a long design lifespan of at least 30 years. These solid constructions provide more protection from fuel theft and impacts than plastic tanks. As they are hand fabricated (rather than using moulds) they are adaptable and can reach a high capacity.\nHowever, the cost of the material and additional skilled labour required makes steel storage tanks more expensive than plastic. Also, while the design lifespan is longer than plastic tanks, steel tanks do require regular maintenance to prevent rust and keep them in optimum condition. For a full comparison of steel and plastic tanks, read our article Steel Vs Plastic Tanks – Virtues and Pitfalls.\nTuffa’s steel HVO fuel tanks are available in capacities from 900 litres as storage only or from 5,000 to 100,000 litres with dispensing equipment. As standard, our steel HVO dispensing tanks are fitted with a high-security cabinet with a roller shutter door (walk-in from 20,000 litres) which houses premium ancillary equipment including a C2020 contents gauge with bund alarm, K33 mechanical flowmeter and 10 micron water and particulate filtration. Optional extras include a high-flow rate, hose reel, and a fuel management system.\nPlastic HVO fuel tanks\nOur plastic HVO fuel tanks are bunded and rotomoulded using a durable, UV stabilized and corrosion-resistant polyethylene which makes a strong and safe single unit. These are available in capacities from 1,350 to 15,000 litres as a single tank or 30,000 litres using interlinking. Our plastic HVO storage tanks are designed to be simple to install and maintain with robust and easily replaceable ancillary equipment. Plastic HVO tanks are more cost-effective than steel but have a shorter design lifespan of 20 years and are an easier target for fuel theft.\nOur plastic tanks come with a high-spec as standard including a high polymer shot weight (which creates a thicker tank wall) and premium ancillary equipment including flowmeter, 10-micron water and particulate filter and FMS gauge and bund alarm all housed within a lockable lid or cabinet. While the rotomoulding process creates a fixed tank size and shape, we do offer a variety of options to adapt the tank to the site’s individual requirements. These include a 12V/24V dispensing, a pulse meter, high flow rate pumps and fuel management.\nWhat maintenance does my HVO tank require?\nFuel tank maintenance is usually the responsibility of the homeowner in domestic properties or the business owner in commercial premises. By conducting regular inspections and maintenance you can prolong the lifespan of the tank and reduce the chances and damage caused by a leak. While visual inspections can be conducted by homeowners or untrained staff, servicing and maintenance should be conducted by a Competent Person.\nChecking your oil tank\nIt’s recommended that you check your HVO tank on a monthly basis and after any episodes of extreme weather. Some of the visible signs you should look for on the tank include:\n• The fill point arrangement for soundness and leaks\n• Any outlet valves should be checked for leaks and operation (open and close successfully)\n• The testing of contents gauge, any high level / overfill alarm and bund alarm.\n• If vents can be seen that they are clear and unblocked and free of debris.\n• A visual inspection around the tank with emphasis on the base of the tank. The inspection for plastic tanks should include any deformation of the surface of the tank such as:\no Excessive bulging\no Change in colour due to chemical attack\no Crazing or stress fractures.\n• The inspection of steel tanks should include looking for evidence of:\no Rust and heavy corrosion\no Damp patches on seams & seam fractures.\n• The bund to be visually inspected for soundness and integrity, water, spilt product, or other\nIf your HVO tank is connected to an oil-fired boiler then we recommend that you use a competent technician to maintain your tank every year – this can be done at the same time your boiler is serviced. Your technician should check the tank, bund and pipework and remove any condensation water. Upon completion, you should receive a written report on the state of the tank and any work done. This service can be performed by members of Oil Firing Technical Association (OFTEC) or the Association of Plumbing & Heating Contractors (APHC).\nOne way we have worked to reduce the costs of oil tank maintenance is by supplying all of our bottom outlet fuel tanks with an Ultra Compact Isolation Valve. As the valve provides safe isolation as the very first component it can save technicians hours on jobs such as replacing the filter valve. With basic valves usually supplied with oil tanks this would require draining and refilling the tank."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:0f30d162-db05-4030-a85c-068788b58179>","<urn:uuid:377f2fcf-678a-401a-bb0f-69e65ff812e5>"],"error":null}
{"question":"How is the BP oil spill RESTORE Act funding distributed?","answer":"The RESTORE Act funding is distributed through three pots: Pot 1 distributes 35 percent of funding equally among Gulf states, Pot 2 allocates 30 percent for ecosystem restoration projects selected by the RESTORE Council, and Pot 3 distributes 30 percent to Gulf states based on oil spill impact allocation. Louisiana is set to receive more than $930 million in total RESTORE Act funding.","context":["Louisiana’s Coastal Master Plan is the state’s blueprint for coastal restoration and protection, containing a mix of risk reduction and coastal restoration projects spread across the entire coastal zone. Updated every five years, the 2017 master plan has an estimated cost of $50 billion over 50 years. Given limited funding and resource constraints, the plan is a realistic prioritization of what can be accomplished during the 50-year timeframe.\nTo rebuild and fortify our coast in the face of increased sea level rise and storms, this funding challenge must be met head on. There are a number of sources available to help pay for coastal restoration in Louisiana, but to fully restore and protect our coast, additional funding will need to be secured and existing funding must be protected.\nNatural Resource Damage Assessment (NRDA)\nNRDA is the federally-administered damage assessment and restoration process used to determine damages caused by an oil spill and to restore natural resources. The total Deepwater Horizon NRDA settlement was $8.8 billion, and Louisiana will receive $5 billion of those funds, to be used for ecosystem restoration. $4.3 billion of these funds will be used to restore and conserve habitat, while the remaining funds will be used to replenish and protect living coastal and marine resources ($343 million); for monitoring, adaptive management and administration oversight ($258 million); to provide and enhance recreational opportunities ($60 million); and to restore water quality ($20 million).\nNFWF Gulf Environmental Benefit Fund\nThe National Fish and Wildlife Foundation (NFWF) Gulf Environmental Benefit Fund was established in 2013 following the Gulf oil disaster. The fund contains $2.544 billion, half of which ($1.272 billion) is to be spent in Louisiana on barrier island restoration and sediment diversion projects.\nFollowing the BP oil disaster, in 2012, the federal Resources and Ecosystems Sustainability, Tourist Opportunities, and Revived Economies of the Gulf Coast States (RESTORE) Act was signed into law. This law ensures that 80 percent of all Clean Water Act penalties stemming from the spill flow through the RESTORE Act and are used for Gulf Coast restoration.\nFunding flows through the RESTORE Act via three “pots”:\n- Pot 1 evenly distributes 35 percent of RESTORE funding directly to the Gulf states.\n- Pot 2 allocates 30 percent of funding to ecosystem restoration projects selected by the RESTORE Council.\n- Pot 3 distributes 30 percent of funding to the Gulf states based on an oil spill impact allocation\nLouisiana is set to receive more than $930 million in RESTORE Act funding. Louisiana has committed to use all of its Pot 1 and 3 funding on the Coastal Master Plan. Via Pot 2, Louisiana has received an additional $52.2 million to fund coastal restoration projects included in the RESTORE Council’s Funded Priorities List, which includes marsh creation, hydrologic restoration and beach nourishment projects.\nGulf of Mexico Energy Security Act (GOMESA)\nGOMESA was signed into law in 2006 and shares 37.5 percent of qualified outer continental shelf oil drilling revenue with oil-producing Gulf states. To date, more than $30 million has been distributed to the Gulf states via GOMESA Phase I. Louisiana will receive an estimated $176 million per year (approximately $8.5 billion total) in GOMESA funds during Phase II. Thirty percent of this funding will go directly to parishes, and the majority will go to the Coastal Protection and Restoration Authority (CPRA).\nCoastal Wetlands Planning Protection and Restoration Act (CWPPRA)\nCWPPRA was enacted in 1990 to provide funding for Louisiana wetlands restoration. Louisiana currently receives more than $80 million per year (approximately $4 billion total) through CWPPRA.\nAdditional sources of funding\nOther sources of funding for Louisiana coastal restoration include the Energy and Water Act (Corps Funding), Coastal Impact Assistance Program (CIAP), carbon and nutrient credits, future state funding and Louisiana’s Coastal Protection and Restoration Fund.\nCHANGING RESTORATION COSTS\nRestore the Mississippi River Delta, in conjunction with Coast Builders Coalition, released an analysis prepared by The Water Institute of the Gulf that outlines opportunities for the state of Louisiana to achieve substantial cost savings as it advances its master plan.\nThe analysis, “Changing Restoration Costs,” analyzes the opportunities that exist for the state to achieve substantial cost savings, particularly related to marsh creation projects over time."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:616444ef-a854-480f-ba28-7868fae25fca>"],"error":null}
{"question":"How do slip-stitch knitting and mosaic knitting compare in terms of their approach to working with multiple colors? What are the key similarities and differences between these techniques?","answer":"Slip-stitch knitting and mosaic knitting are very similar techniques that both achieve colorwork patterns by slipping stitches rather than working with multiple colors simultaneously. In both methods, you work with only one color at a time and slip the stitches that should be the other color. The main principles are the same - when knitting with the dark color, you slip the light stitches, and vice versa. Both techniques create floats that are carried across unworked stitches. The key difference lies in how they're typically presented and applied: slip-stitch knitting is often used for simpler effects like polka dots and can create decorative elements with floats carried in front of the work, while mosaic knitting (popularized by Barbara Walker in the late '60s) tends to focus on more geometric patterns and has specific limitations, such as only being able to slip 2-3 stitches consecutively. Both techniques work well either flat or in the round and result in a fabric that's less heavy than traditional Fair Isle knitting since there are fewer floats.","context":["With this blog in mind, I often bookmark articles I’ve found online, to share with you. This one, which I have edited slightly, comes from Interweave and was published on September 09, 2015\nWhat is a slip stitch? Pretty much just what it says: you slip a stitch from one needle to the other without working it. In knitting, there are many reasons to slip a stitch intentionally. In colorwork knitting, slipping stitches makes it easy to achieve the look of more complex colorwork techniques with little more effort than when working simple stripes. If you’re working a color stripe pattern and you slip stitches on the first round of a color change, the color from the previous round will be drawn up into the current round and it will look as if you’ve worked with two different colors on the same round. But you can do much more than imitate other colorwork techniques. You can also create effects that are unique to slip-stitch knitting.\nWhen you slip stitches without working them, the yarn must be carried from one worked stitch to the next, spanning one or more unworked stitches. The resulting yarn strand, or float, is carried either behind or in front of the slipped stitch (or stitches). If you slip a stitch with the yarn in front, the floats that are carried across the front of the work become a decorative element. (Just make sure to bring the yarn to the back of the work again when you’re ready to knit the next stitch or you’ll end up with a yarnover increase.)\nIf you’ve never tried slip-stitch colorwork, start with the simple polka dot pattern above. Before you know it, you’ll be hooked!\nTRY THIS PATTERN FOR A POLKA DOT TUBE COWL – it will make a useful scarf for this winter.\nPLEASE NOTE THAT THIS PATTERN DOES NOT TELL YOU WHAT SIZE NEEDLES TO USE, OR WHAT YARN TO USE – YOU CAN MAKE IT UP FOR YOURSELF!\nHOWEVER, TO CONTROL THE SIZING – IT WOULD BE A GOOD IDEA TO TRY KNITTING A SMALL SWATCH WITH YOUR CHOSEN NEEDLES AND YARN FIRST!\nThe polka dot pattern is a great introduction to slip-stitch knitting. Here’s what you’ll need:\n- 16″ circular needle\n- at least two colors of yarn (but use as many as you like), a main color (MC) and contrast color (CC)\n- stitch marker\nNote: Slip stitches purlwise with yarn in back.\nWith MC, and using a provisional method, cast on a multiple of 4 sts. Place marker and join in the round.\nRounds 1 and 2 With CC, *slip 2, k2; rep from * to end.\nRounds 3 and 4 With MC, knit.\nRounds 5 and 6 With CC, *k2, slip 2; rep from * to end.\nRounds 7 and 8 With MC, knit.\nRepeat Rounds 1-8 for pattern, ending with Round 6.\nBlock and join the ends of the cowl using three-needle bind-off or Kitchener stitch and MC.\nNB: You can also try this sequence without using circular needles – to make a ‘flat’ scarf. Choose the width you want the scarf, and double it – so that you can sew the two edges together. The pattern above can be adapted in any way you want, and don’t worry about the ‘technical’ names of knitting stitches – you can choose the way you like to cast on and cast off!\nStart knitting this now and you could easily finish making your cowl or scarf in time to give it to someone as a Christmas present.","you've stuck to knitting in single colors,\nor maybe just ventured into stripes, here's\nsome good news: You can achieve what appear\nto be complicated color patterns while using\nonly one yarn at a time! Many knitters suffer\nfrom fear of Fair Isle because they are loathe\nto carry two colors at once. It seems complicated\nand unwieldy. If you're ready for a little\nmore adventurous use of color and pattern,\nwhy not try mosaic knitting? Using a simple\nslip-stitch technique, you can create bands\nand borders in pattern, or even a whole sweater,\nand you won't ever have to carry a second\nColor slip-stitch knitting\nhas been around for a long time, but it was\nbrought into the spotlight for a time by Barbara\nWalker who dubbed the style \"mosaic knitting\"\nin the late '60s. Hundreds of mosaic patterns\nappear in Walker's books. More recently, Roxana\nBartlett has taken a new look at this approach\nto color patterning. If you become smitten\nwith this technique after trying it, you can\nfind more information and patterns in their\nbooks. [See \"Recommended Reading\"\nThe technique of two-color\nslip-stitch knitting is really quite simple:\nIf you can knit simple stripes and slip a\nstitch, you have all the skills you need.\nMosaic knitting simply involves slipping the\nstitches in a row that should be the \"other\"\ncolor. If you are knitting the dark color,\nyou slip the light; if you are knitting the\nlight color, you slip the dark.\nAside from the allure of\ncolor patterning using just one yarn at a\ntime, slip-stitching has other advantages\nover other multicolor techniques: Mosaic knitting\nworks well either flat or in the round which\ngives it more flexibility than intarsia or\nFair Isle techniques. It also doesn't leave\nas many floats as Fair Isle knitting, so the\nfabric is not as heavy and not as much yarn\nThere are a few limitations\nto the technique. Because stitches are slipped,\nthere's a limit to how many stitches of the\n\"wrong\" color you can work consecutively\n-- two or three, depending on who you talk\nto. This tends to give the patterns created\na geometric look, which may have reminded\nBarbara Walker of tile mosaics. Also, the\nslipped stitches make the fabric a little\nless than smooth, but blocking will usually\ntake care of this. For this reason, yarns\nwith a little more \"give\" tend to\nwork better, though the toddler sweater above\nis knit in pure cotton. If you knit very tightly,\nyou may want to go up a needle size so that\nyour stitches can be slipped two rows without\nI find slip-stitch knitting\nto be a great introduction to color patterning.\nThere are bits of \"Pay attention; read\nthe chart!\" combined with the auto-pilot\nknitting of every other row. And once you\nget the feel of a certain motif, you will\nbe able to see it in your knitting without\nREADING the chart\nUsually mosaic patterns\nare charted; most are not very large [maybe\n20 stitches wide], but there is really no\nlimit. The charts usually have a color bar\non the right side indicating which color is\nbeing worked for each row. The chart has the\nrows numbered, odd numbered rows on the right,\neven numbered rows on the left. The second\nrow worked in a color is always worked exactly\nthe same as the first: if a stitch was knitted\nin the odd row, it is worked in the even row;\nif a stitch was slipped in the odd row, it\nis slipped again.\nparticularly for simpler motifs such as the\nbrick pattern at right, directions are written\nout like standard knitting patterns.\nMC: CO a multiple of 6 sts\nRow 1: Knit.\nRow 2: Purl.\nRow 3: CC: *K5, sl1* end\nRow 4: *P5, sl1* end p5\nRepeat these 4 rows as until\ndesired length is reached.\nAfter choosing a mosaic\npattern [or patterns] you like, you can easily\nmap it on to any garment. While you want to\nrepeat the entire motif to see the whole pattern,\nyou need not necessarily have a number of\nstitches that is an exact mulitple of the\nnumber of stitches in the motif. You may cut\na motif off in the middle without making a\nmess. With any project, you will want to work\nthe selvedge stitches in the row's color.\nNever slip the selvedge stitch. This will\nkeep your pieces smooth and easy to join together.\nAfter casting on, work two\nrows in the first color [say, the lighter\ncolor], then attach the second color, leaving\nthe first hanging. With the darker yarn, the\nlight-colored stitches shown on the chart\nare slipped and the darker stitches are worked.\nAlways slip stitches as if to purl; that is,\ninsert the needle from right to left, and\ncarry the yarn on the wrong side of the work.\nAfter you have knit your second row with the\ndarker color, switch back to the light. You\nwill always work two rows in each color and\nthe two rows are the same: a stitch never\nchanges its color in the second row. Continue\nworking your way up the chart until you complete\nAfter you have finished\nwith one motif, you may repeat it again, either\nthe same way, or by trading light and dark\ncolors; switch to a second motif, or carry\non with plain knitting. A mosaic border on\ncuffs and hem will add punch to an otherwise\nplain sweater, or try working a stripe of\na larger motif across the chest of a sweater.\nAnd though I have described these techniques\nas two-color, that is certainly not the limit\nof this technique. You can keep one color\nconstant and alternate the second. Or try\none plain yarn and one variegated yarn. Using\na plain colored yarn and one space-dyed yarn\ncan yield very impressive multicolor results\n- all just one strand at a time.\nInterweave Press, 1998.\nKnitting Designs. Charles Scribner &\nEncore Editions, 1976.\nWalker, Barbara. A\nSecond Treasury of Knitting Patterns.\nCharles Scribner & Sons, 1970."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:71688a94-a1b8-488c-953c-67802e01b4db>","<urn:uuid:8bd77640-8408-4b3a-84b1-3bb9189ed9ca>"],"error":null}
{"question":"How does cognitive niche concept explain human evolution, and what role does biodiversity play in maintaining ecosystem stability? 🤔","answer":"The cognitive niche concept explains human evolution through humans' unique ability to use cause-and-effect reasoning and cooperative action to extract resources from other organisms, rather than relying on physical features. This cognitive capability allowed humans to represent prey internally and develop advanced social learning, technology use, and grammatical language. As for biodiversity's role in ecosystem stability, it functions as the foundation of resilient ecosystems where each species occupies a unique ecological niche. This diversity ensures efficient nutrient cycling, pollination, and pest control, contributing to the overall functioning and stability of ecosystems through complex interactions between organisms.","context":["In ‘Ecology in an Anthropogenic Biosphere’ Erle Ellis posits “’The First Law of the Anthropocene’: the ecological patterns, processes, and dynamics of the present day, deep past, and foreseeable future are shaped by human societies” (p. 316). The goal of the paper is to explain why this law holds—i.e. to explain what it is about this particular species, homo sapiens, that gave it “the capacity to transform an entire planet” (p. 288). The key to Ellis’ explanation—at least as I see it—is in the character of the specifically human mode of habitation, namely that human beings occupy, because they construct, a “sociocultural niche.” In this post I want to try to deepen my understanding of the niche concept, in order to grasp better what Ellis means by describing the human niche as sociocultural. I think that the Pinker article, which Ellis cites in reference to social learning (p. 298) also sheds light on how Ellis is using the idea of the niche. I offer my interpretation in hopes that readers will offer corrections of my misunderstandings.\nPinker argues that the solution to the puzzle of the evolution of the human mind lies with the theory of the “cognitive niche.” This theory, he holds, explains the evolution of three key and distinctively human traits: use of technology, cooperation among non-kin, and communication by means of a grammatical language. Ellis implicitly refers to the last of these three traits as the basis of “the unrivalled capacity of humans to transmit information by social learning” (p. 298). And he also cites the importance of non-kin cooperation (p. 298) and technology (pp. 300 ff.). Thus, things Pinker has to say about specific aspects of the cognitive niche bolster Ellis’ presentation of the corresponding aspects of the sociocultural niche.\nHowever, at a more conceptual level, what is especially helpful to me is that Pinker’s explanation of the cognitive niche idea improved my grasp of the underlying idea of the niche, thus helping me see how Ellis’ sociocultural niche is likewise an elaboration of that underlying idea.\nPinker starts with a basic notion of a niche as “the role an organism occupies in an ecosystem” (p. 8993). Focusing on animals, that role, put crudely, is to eat other organisms—which in turn will have some defenses against being eaten. But the niche idea should not be understood primarily in physical terms, as the surroundings of a given animal, containing its food. Rather an animal’s niche should be understood in functional terms: what, in its surroundings, its particular capabilities render available for it to eat. An animal’s niche, that is, is a function of its capabilities—its niche is constituted by the physical features of its surroundings that it is able to take advantage of to survive. I believe this is what Richard Lewontin means when he says “The environment of an organism is the penumbra of external conditions that are relevant to it because it has effective interactions with those aspects of the outer world” (2000, p. 49).\nA fundamental aspect of the niche idea, then, is that it is the set of resources that support a creature’s survival—given its metabolic needs and its capabilities for fulfilling them. This is a narrowly ecological conception of the niche. But there is another fundamental aspect that Pinker identifies, associated with evolution. For a niche places selection pressure on its inhabitants: individuals that are not well adapted to it will not pass their genes to succeeding generations, so that those generations will be better adapted, or not exist. And, as features of the niche change (e.g. a prey species develops better defenses), natural selection leads to the evolution of traits adapted to those changes.\nThis second aspect is the focus of “Niche Construction Theory,” which has been a recurring topic on this blog. NCT examines how animals actively create niches by modifying their surroundings to suit their metabolic needs—and observes that those modified environments serve as an “ecological inheritance” passed on to succeeding generations, which must remain adapted to them. In Pinker’s words, “An organism’s behavior alters its physical surroundings, which affects the selection pressures, in turn selecting for additional adaptations to exploit that altered environment, and so on. A classic example is the way beavers generated an aquatic niche and evolved additional adaptations to thrive in it” (p. 8995).\nHow then does the notion of the cognitive niche modify the underlying niche idea? In the underlying sense, the niche is characterized by a species’ capabilities for gaining the resources it needs from its surroundings. Let’s say a new physical trait appeared in a species—e.g. by a mutation yielding the ability to run faster, hence to capture a new type of prey. This would unlock a previously unavailable possibility for gaining survival, thereby adding a new dimension to the species’ niche. This same pattern can work with a non-physical trait, like improved cognitive abilities. As Pinker puts it, “In any ecosystem, the possibility exists for an organism to overtake other organisms’ fixed defenses by cause-and-effect reasoning and cooperative action—to deploy information and inference, rather than particular features of physics and chemistry, to extract resources from other organisms” (pp. 8993-94). Of course this is the possibility that humans exploited to gain an advantage over the species they relied on to survive; the collection of resources they gained access to in virtue of their cognitive abilities thus constitute a “cognitive niche.”\nThe cognitive niche idea conveys the notion that human evolution brings something new into the world: it adds a new non-physical dimension to the physical features of the biosphere that support metabolic survival. This is not to propose a mind-body dualism whereby cognition is metaphysically distinct from physical processes; as philosophers say, cognition “supervenes” on processes in the brain (and, as Pinker discusses, has a genetic substrate (p. 8996)). Rather, it is to say that there is a layer of cognitive representations through which human beings reach the physical resources they require. Those representations constitute an additional aspect of the human environment: people do not eat their internal representations of prey, but they do need to represent their prey internally to be able to eat them. The capacity to enter into the world of representation (a capacity that of course is tightly bound up with sociability) becomes, therefore, the key adaptation humans use to survive—and just as beavers adapted to the aquatic niche their forebears constructed, “initial increments in cooperation, communication, or know-how altered the social environment, and hence the selection pressures, for ancestral hominids,” (p. 8995) leading to distinctively human cognitive capacities. Thus, that which humans are adapted to is not simply the physical setting they inhabit, but this non-physical, cognitive world through which they inhabit it.\nPinker thus helped articulate for me what might be called the “noetic quality” of the human niche, i.e. ways the human niche cannot be understood in strictly metabolic terms, but requires an additional, mental (and social) set of categories, which nonetheless are directed at explaining human metabolic survival and evolution. Ellis’ “sociocultural niche” is perhaps a more complex concept than Pinker’s cognitive niche—but I believe it conforms to this general pattern of adding a noetic layer (or layers) to a fundamentally metabolic and Darwinian account of human life. In particular, Ellis speaks of culture in terms of information (see, e.g., his Table 1, pp. 292-293)—representations grasped, manipulated, and communicated via mental capacities. Pinker’s account helped me grasp more completely just what this means when Ellis speaks of the sociocultural system as a niche.\nBut I’ll conclude by noting a way in which Pinker points to a way Ellis might develop his theory his further. For Pinker starts and ends his paper with discussions of a puzzle posed by A.R. Wallace, the co-discoverer with Darwin of natural selection. The puzzle has to do with a mis-match between the skills needed for metabolic survival, and the intellectual accomplishments for which human beings take pride in their species. In Pinker’s words, “why do humans have the ability to pursue abstract intellectual feats such as science, mathematics, philosophy, and law, given that opportunities to exercise these talents did not exist in the foraging lifestyle in which humans evolved and would not have parlayed themselves into advantages in survival and reproduction even if they did?” (p. 8993). Wallace took those “higher” capacities than are needed for survival as evidence for Design. Pinker rejects that claim, arguing instead “that the psychological faculties that evolved to prosper in the cognitive niche can be coopted to abstract domains by processes of metaphorical abstraction and productive combination, both vividly manifested in human language” (p. 8993).\nPinker’s argument here is intricate and ingenious, but I will not summarize it here (see pp. 8996 ff.). Rather, I want to propose that Ellis might consider an analogous problem, and whether his theory contains the resources for an analogous solution. That is, we can ask whether the theory of cultural evolution Ellis offers, even if sufficient for explaining the evolution of a characteristically human way of making a living from the planet, and the ecological changes that result, succeeds at reaching “into” culture to account for cultural accomplishments that are not strictly necessary to survival. Does Ellis’ theory allow for something like Pinker’s move, whereby cultural elements that are adaptive in a narrow sense can be abstracted and combined to produce the cultural expressions we associate with the highest human achievements?","Biodiversity, the rich tapestry of life on Earth, is a critical component of our planet's health and resilience. It encompasses the variety of species, ecosystems, and genetic diversity within them. The importance of biodiversity extends far beyond its intrinsic value; it plays a crucial role in sustaining ecosystems, supporting human well-being, and maintaining the delicate balance of the planet. Understanding why biodiversity is important involves exploring its ecological, economic, and cultural dimensions.\nEcologically, biodiversity is the foundation of stable and resilient ecosystems. Ecosystems are dynamic communities where living organisms interact with each other and their environment. A diverse array of species within an ecosystem contributes to its stability and adaptive capacity. Each species, no matter how small, plays a unique role, or ecological niche, contributing to the overall functioning of the ecosystem. This interconnected web of life ensures the efficient cycling of nutrients, pollination of plants, and control of pests and diseases.\nThe concept of biodiversity is often categorized into three levels: genetic diversity, species diversity, and ecosystem diversity. Genetic diversity refers to the variety of genes within a species, enabling adaptation to environmental changes and ensuring the survival of populations. Species diversity encompasses the variety of different species in a given area, providing ecological services and enhancing the resilience of ecosystems. Ecosystem diversity involves the range of ecosystems present on Earth, from forests and wetlands to deserts and coral reefs, each with its unique set of species and interactions.\nMaintaining biodiversity is essential for ecosystem services, the benefits that ecosystems provide to humans. These services include pollination of crops by insects, purification of air and water, regulation of climate through carbon sequestration, and the provision of food, medicine, and raw materials. A loss of biodiversity can disrupt these services, compromising the ability of ecosystems to sustain life and support human societies.\nThe role of biodiversity in agriculture is particularly significant. Many crops depend on pollinators, such as bees and butterflies, for successful reproduction. A decline in pollinator diversity can lead to reduced crop yields and threaten global food security. Furthermore, diverse ecosystems contribute to pest control by fostering the presence of natural predators that keep populations of harmful pests in check, reducing the need for synthetic pesticides.\nBiodiversity also plays a crucial role in the field of medicine. Many pharmaceuticals are derived from compounds found in plants, animals, and microorganisms. The diversity of life forms offers a vast potential for discovering new medicines and treatments for various ailments. Additionally, the genetic diversity within species provides the raw material for breeding programs aimed at developing more resilient and disease-resistant crops.\nBeyond its ecological and economic significance, biodiversity holds cultural and aesthetic value. Indigenous cultures often have deep connections with their local ecosystems, relying on specific plant and animal species for food, medicine, and traditional practices. Biodiversity contributes to the cultural identity of communities, shaping their customs, rituals, and folklore.\nMoreover, the aesthetic value of biodiversity enhances human well-being. The diverse landscapes, colorful array of species, and intricate ecological interactions contribute to the beauty of nature. This aesthetic appreciation of biodiversity is not merely a luxury; it has tangible effects on mental health and emotional well-being. Green spaces, natural environments, and diverse ecosystems have been shown to reduce stress, improve mood, and enhance overall quality of life.\nDespite its critical importance, biodiversity faces unprecedented threats due to human activities. Habitat destruction, driven by factors such as deforestation, urbanization, and agriculture expansion, is a major driver of biodiversity loss. Pollution, overexploitation of natural resources, climate change, and invasive species further exacerbate the challenges faced by ecosystems and species worldwide.\nClimate change, in particular, poses a significant threat to biodiversity. Rising temperatures, altered precipitation patterns, and extreme weather events can disrupt ecosystems, leading to shifts in species distributions and potential extinctions. Species that are unable to adapt quickly enough to changing conditions may face severe population declines or extinction.\nThe loss of biodiversity is often referred to as the “sixth mass extinction,” with the current rate of species loss estimated to be significantly higher than the natural background rate. Unlike previous mass extinctions, this one is primarily driven by human activities. Recognizing the severity of the situation, conservation efforts are underway globally to mitigate the impact of human activities on biodiversity.\nConservation strategies include the establishment of protected areas, restoration of degraded ecosystems, sustainable land-use practices, and the implementation of international agreements to address transboundary issues. Conservation biology, a multidisciplinary field, focuses on understanding and mitigating the threats to biodiversity through scientific research, policy advocacy, and community engagement.\nPreserving biodiversity requires a holistic approach that considers both local and global scales. Local communities, indigenous peoples, governments, and international organizations all play vital roles in safeguarding biodiversity. Balancing the needs of human development with the imperative to protect biodiversity is a complex challenge that demands collaboration, innovation, and a commitment to sustainable practices."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:32093170-0557-4444-af15-102b1e66dcb0>","<urn:uuid:9fe34b9e-ffe8-425c-8cf6-154a227f0b49>"],"error":null}
{"question":"What similarities existed between Kurt Roger and Joseph Haydn's relationships with England?","answer":"Both composers had significant connections to England, though at different times and in different ways. Haydn made two successful trips to England in the 1790s, composing his London symphonies there and receiving an honorary doctorate from Oxford University in 1791. Roger's connection to England came as a transit point during his emigration to the United States, and his music later received notable performances by British orchestras, including the BBC Northern Orchestra under Sir Charles Groves and the BBC National Orchestra of Wales under Jac van Steen.","context":["|About this Recording\n8.572238 - ROGER, K.: Chamber Music - Clarinet Quintet / Piano Trio / Piano Sonata / Variations on an Irish Air (Gould Piano Trio, Plane, Beynon)\nKurt Roger (1895–1966)\nKurt Roger was born in Austria on 3 May 1895 to Viennese parents and studied in Vienna with Guido Adler, Karl Weigl, and in class with Arnold Schoenberg, although not following Schoenberg’s twelve-tone system. He taught at the Vienna Conservatory from 1923 to 1938 and his works were receiving high-profile performances until the Nazi Anschluss forced his emigration to the United States via London. He became an American citizen in 1945 and held teaching positions in New York and Washington DC, lecturing at several universities and giving radio talks, notably on Bruckner and Mahler. His music has received many notable performances including those by the Chicago Symphony Orchestra under Rafael Kubelik, the Rochester Philharmonic under Erich Leinsdorf, the New York Chamber Orchestra, the National Symphony Orchestra of Washington DC, the Vienna Symphony Orchestra, the BBC Northern Orchestra under Sir Charles Groves and the BBC National Orchestra of Wales under Jac van Steen.\nFrom 1948 onwards Roger was invited back to Austria on lecture tours, whose venues included the Academy of Music in Vienna and the Mozarteum in Salzburg. In 1964 he accepted a guest professorship at Queen’s University, Belfast, enabling the composer Raymond Warren to have a sabbatical. As Roger’s wife was born in Ulster, this proved to be a happy coda to his life. In 1965 the Austrian government conferred on him the Order of Merit first class in the field of art and science. He died on 4 August 1966 on a visit to Vienna and was subsequently given a grave of honour there. His scores are preserved at the Gesellschaft der Musikfreunde.\nRoger’s musical style is a testament to the fascinating and changing musical atmosphere of which he was a part. His teacher Karl Weigl, a notable Lieder composer in the tradition of Wolf and Mahler, confirmed Roger’s natural leanings towards lyricism, and from his other teacher, Schoenberg, he acquired a sense of formal construction and the ability to create complex motivic connections and dense polyphony. Whilst Roger could not be called a Modernist, and his style is indicative of Schoenberg’s more Romantic early works, he nevertheless shared the Modernist’s fascination with the past. As such, his works are often based on traditional or archaic forms enlivened by new combinations and adventurous harmony.\nMany of these characteristics are found in Roger’s Clarinet Quintet, his last work, written shortly before his death in 1966. The general atmosphere is dark, poignant and nostalgic, though the modally inflected harmonies hint at a quasi-pastoral nature. In many similarly scored pieces the wind instrument takes on a soloistic rôle, but here, as in the Brahms Clarinet Quintet, it is fully integrated into the ensemble. Moreover, the clarinet’s characteristic sonorous qualities make it especially suited to such integration; in its low register it blends with the rich colours of the viola and cello, and in the high register, matches the soaring melodies of the violins.\nIntegration is also fundamental to the motivic design, and the Clarinet Quintet demonstrates Roger’s mature mastery of thematic unity. It is a motivically dense work, though no less emotionally moving for its complexity. Within each movement the motives are combined in an ornate web of polyphony, but they also have movement-spanning connections: the martial-like theme first heard in the cello at the opening of the first movement, returns in an altered form in the second movement, though here its nature is subdued by the slow tempo and legato counter-melody. This same counter-melody, with its characteristic falling seventh ending, finds its way into the Finale, as do hints of the martial theme.\nSimilarly rich in motivic associations is Roger’s Piano Sonata, written in New York in 1943. As with many of his works, the Sonata evokes a variety of old and the new styles, using Baroque, Romantic and Modern elements in inventive combination. The first movement lives up to its anachronistic Toccata title in its display of manual dexterity, but the style is more lyrical than a traditional Toccata. The chief motivic element of the movement is the semitone interval: first heard in the listless opening theme, the rocking semitone figure gradually expands to a tone, then to staccato scales, before flowering into extended lyrical passages in an intense duet between the two hands. At a deeper level, the large scale ternary form also explores the semitonal relationship between F sharp minor and F minor. In the Interlude gently undulating chords, evocative of the first movement’s rocking semitone, are juxtaposed with a deep, ominous bass figure. This colouristic effect, reminiscent of Debussy, gradually gives way to a section in which the bass and treble become more amalgamated. The Impressionistic atmosphere returns at the end, but here the firmly rooted tonality of A flat major gives a greater sense of tangibility. A similarly portentous bass to that of the Interlude opens the Sonata’s final Phantasmagoria movement. The Phantasmagorical style, generally defined as a medley of shifting fantastical images, is evidenced in the striking contrast between the movement’s Agitato and Tranquillo sections, which are in turn linked by material based on the semitone.\nRoger was a man who felt emotions intensely; he was deeply moved by any instance of human suffering, but at the same time he lived for life’s more jovial moments. Highly varied in mood and style, the three movements of the Piano Trio, written in 1953, encompass an equally wide emotional range. The music shares an affinity with the Classical forms of the great Viennese masters but is nonetheless innovative with its free harmony and adventurous contrapuntal development.\nSuch counterpoint forms an important part of the first movement’s construction: it is used at the opening where the lively E flat major theme is presented canonically through the three parts, but it also serves as a means of developmental intensification. The counterpoint at times lends an academic air, but this is dispelled in the tender second movement where a plaintive, quasi-improvisatory melody is taken up by each instrument in turn. The melody’s wide intervals allow a particularly beautiful means of exploring the tone colours of each instrument. The final movement bursts onto the scene with a flurry of piano arpeggios moving across the instrument’s range. The main theme is a cheeky, rustic melody in which F sharps jokingly interrupt the E flat major tonality. A series of lighthearted, developmental adventures ensues, but finally E flat major establishes itself more firmly and the movement merrily romps to a close.\nWhere the Piano Trio displays Roger’s Classical Viennese roots, the Variations on an Irish Air, composed in New York in 1948, bear witness in a nostalgic sense to his visit to Ireland in 1939 where he met his future wife while awaiting passage to the United States. This ancient Air is commonly known as Down by the Salley Gardens because of W.B. Yeats’s inspired poem. The work opens with a haunting flute solo before the theme itself is introduced and explored in a set of twelve variations ranging from quiet introspection to virtuosic brilliance. The mastery of such a wide stylistic scope is noteworthy and there is immense beauty in the delicate interweaving of the instruments. Finally the ethereal voice of the solo flute returns and gradually fades away, bringing the work to a close.\nThis impressive variety in his treatment of thematic material is one of the many qualities which make Roger’s music so interesting. His works are both traditional and forward looking; reflecting an affinity with the composers he most revered and yet retaining a strong sense of originality. At a time when many composers were asserting their ‘newness’ above all else, Roger’s was a compositional voice that remained true to his own aesthetic calling.\nClose the window","31st March (Franz) Joseph Haydn is born in Rohrau on the Leitha (Lower Austria); Today there is a museum there which commemorates both Joseph Haydn and his brother, Michael.\nChoirboy (Sangerknabe) in the choir school of St. Stephen’s in Vienna; Haydn receives a musical education, as well as lessons in other subjects. As a choirboy, he takes part in performances of church music at St. Stephen’s, at other churches in Vienna and at the Viennese court.\nLargely self taught and working as a freelance artist in Vienna, Haydn composes ecclesiastical and operatic music, string trios, sonatas, dances, organ concertos, divertimenti for various arrangements and his first string quartet. In the early 1750s he lives as a tenant in a garret in Vienna’s 1st District.\nThe Italian composer Nicola Antonio Porpora resides in Vienna. Haydn later describes him as his only teacher.\nHaydn is musical director in the service of Count Ferdinand Maximilian Franz Morzin. He lives from time to time at the Count’s summer residence, the Palace of Lucavec near Pilsen. It is here that Haydn’s First Symphony is written, as well as other compositions.\n1760 26th November\nHaydn marries Maria Anna Aloisia Appolonia Keller, the daughter of a Viennese wig maker, in St. Stephen’s Cathedral. The marriage remains childless.\n1761 1st May\nHaydn enters the service of Prince Esterhazy in Vienna, at first as deputy conductor of his orchestra. Haydn principally in Eisenstadt.\nDeath of Prince Paul Anton Esterhazy, Haydn’s employer.\nPrince Nicholas Esterhazy 1, Paul Anton’s brother, becomes Haydn’s new employer. Haydn succeeds in making the Prince’s orchestra into one of the best in Europe at the time. For this he composes overtures, symphonies, concerts, suites, minuets, serenades and operas.\nDeath of Head Conductor of the orchestra, Gregor Joseph Werner. Haydn succeeds him as Head Conductor. The Prince moves into the newly-built Esterhazy Palace (Esterhaza) at the south-eastern end of Lake Neusiedl and this now becomes the most important location for Haydn’s work.\nHaydn buys a house in Eisenstadt and seldom visits Vienna.\nThe opera house in Esterhazy Palace is opened with the premiere of Haydn’s opera Lo specziale\nMaria Theresia visits Esterhazy Palace.\n1775 2nd and 4th April\nHaydn conducts the premiere of ll ritorno di Tobia in Vienna.\nHaydn sells his house in Eisenstadt.\n1785 15th January and 12th February\nHaydn visits Mozart at his Viennese apartment. Mozart’s string quartets dedicated to Haydn are played. Haydn and Mozart were friends, even if they did not meet each other very often.\n1790 28th September\nDeath of Prince Nicholas Esterhazy 1.\nPrince Paul Anton Esterhazy II succeeds Nicholas 1. He dissolves the orchestra. Haydn immediately moves to Vienna (1790), although he remains the Prince’s conductor.\nHaydn’s first trip to England. He composes his first six London symphonies. Performances of his works in London, organised by Salomon.\n1791 8th July\nHaydn is awarded an honorary doctorate of music by Oxford University.\n1793 14th August\nHaydn acquires a house in a suburb of Vienna.\nHaydn’s second trip to England, together with his copyist and valet Johann Elssler. Six further London symphonies are composed, as well as works for piano and several other pieces. Haydn makes numerous concert appearances and is feted by London society.\nPrince Nicholas Esterhazy II succeeds Paul Anton II. Haydn is commissioned to compose a mass for the nameday of the Prince’s wife Hermenegild each year. This results in the last six great masses, written between 1796 and 1802.\n1796 26th December\nHaydn’s Paukenmesse (‘Kettledrum Mass’), conducted by the composer himself, is performed for the first time at the Church of the Piarists in Vienna.\nHaydn works on his oratorio Die Schöpfung (The Creation).\n1797 12th February\nHaydn’s anthem Gott erhalte is sung for the first time at the Vienna’s Burgtheater.\nHaydn moves into his newly-extended house in Vienna.\n1798 29th-30th April\nPremiere of the Creation at Schwarzenberg Palace (now demolished).\n1799 19th March\nFirst public performance of the Creation in the Burgtheater.\n1800 20th March\nHaydn’s wife Maria Anna dies in Baden near Vienna.\n1801 24th April\nPremiere of Die Jahreszeiten (The Seasons) at Schwarzenberg Palace.\nHaydn, presented with many awards in the last years of his life, receives the great gold Salvator Medal from the City of Vienna. Haydn composes the String Quartet op. 103 (only two movements), which is regarded as his last composition.\nJohann Nepomuk Hummel succeeds Haydn as Prince Esterhazy’s concert master.\n1808 27th March\nHaydn makes his last appearance in public on the occasion of a performance of The Creation in the main hall of Vienna University.\n1809 7th February\nHaydn signs his last will and testament.\n1809 31st May\nHaydn, age 77, dies of exhaustion at his house in Vienna (Haydngasse 19 in Vienna’s 6th District) in the presence of his servants.\nArticles & Broadcasts\nDenis McCaldin’s blog for BBC Radio 3 in the bicentennial year of Haydn’s death, 2009\nStephen Moss for the Guardian Jan 2009 – ‘The great Haydn road trip‘\nBBC Proms archive of Haydn programming\nClare McCaldin on the women that Haydn met on his visits to London in the 1790s.\nThe BBC Documentary ‘Genius and Genesis’ (2011) on the development of the Symphony, presented by Simon Russell Beale\nA 1985 documentary presented by Alan Bennett on Haydn at Esterhazy\nBBC Radio 3 Building a Library (Dec 2012), Haydn’s The Creation"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:72419a1b-301c-4f77-a491-dc75717ac458>","<urn:uuid:ca9e1df6-dff3-4c8c-9af3-9c7570400fcc>"],"error":null}
{"question":"How was the Bibliobús Project executed, from design to completion?","answer":"The Bibliobús Project, funded by UNESCO, Plasmon, and The Honduran Ministry of Arts, Sports, and Culture, and the National library Juan Ramón Molina, involved several steps. The process began with proposing a design, then moved to sanding and priming the exterior of the bus. The painting phase included creating artistic elements representing world reading, emerging books, the Honduran flag, stars, and colorful formations of dreams and hopes sprouting from reading. The final step was sealing the artwork. The completed bookmobile now serves communities by providing access to books, magazines, and learning resources.","context":["Featured Awards and Public Art\nFeatured artworks in public art located in and near Broward County.\nEnlightenment, The 5th Annual Design Contest winner of The Friends of the American Latino Museum theme To Illuminate the American Story for All.\nThe inspiration behind this painting is that, \"Every one of us carries many dreams, traditions, and believes in our natural, spiritual worlds and humankind's place in it, solely based on reason.\" Adorable Monique\nNatural Selections Juried Contest, UAC Merit Award, for Nature Reveals Itself, acrylic on canvas, 24x36 Inches.\nLive art painting, Featured artist during The Shelter, Next Generation Purple Party 2019. The auction painting was created inspired by the resilience of butterflies displaying strength and beauty. During the two-hour event, the completed artwork.\nPublic Art comprised a series of works inspired by flora and fauna found in the natural world—the Board of Collier County Commissioners Board Room at the Government Center, Naples, Florida.\nFor the love of art, 65th anniversary! The theme was the 1920's, celebrating the 65th year while supporting the art programs. Ladies by the Pier was painted live during the event, using water-based latex paint on wood and measuring 84\"x96\".\nThe collective work was a contribution to the NAA.\nParticipating artist in the Third on canvas juried Plein air event and art auction.\nThe Bibliobús Project Award is an artistic-educational project funded and sponsored by the UNESCO, Plasmon, and The Honduran Ministry of Arts, Sports, and Culture, and the National library Juan Ramón Molina. The process comprised the proposed design and execution of the painted exterior that involved sanding, painting, and from primming to sealing. The elements within the artistic inspiration represent the world reading, emerging books, the Honduran flag, stars, and colorfully diverse formations of dreams and hopes sprouting from the act of reading.\nThe outcome has been successful; many children and adults alike benefit by engaging in learning experiences by the accessibility of reading books, magazines, and other learning resources within this bookmobile. The bus visits far and near communities, and everyone eagerly await its arrival.\nTierra Bendecida, was commissioned by the National NCFH Foundation to convey the traditions, beliefs, dreams, and identity of the agricultural worker population and community in the Commemorative Artwork.\nResilience, The Solitary Lollies and Blooming Spring, featured on the magazines covers of the SWFL Spotlight Magazines of Estero, Naples, and Bonita Springs.\nPaintings and illustrations grace the covers and inside content of children's fables, storytelling, and art books. The books are from local and international authors.\nNaples Illustrated Charity Registers Magazines 2020-19, 2017-16, and 2015-14 editions.\nCelebrating Friendship, on the cover of the Naples Illustrated Charity Register 2020-2019.\nResilience, on the cover of the Naples Illustrated Charity Register 2016-2017.\nThe Gift of Life, on the cover of the Naples Illustrated Charity Register 2015-2016.\nRising Notes, the 9th Annual Sunny Isles Beach Jazz Fest Poster Contest.\nRegional SWFL semi-finalist, participating with the titled painting Visions of Nature, in the Bombay Sapphire Artisan Series juried competition. Celebrated exhibit at the Lou La Vie Members Club, hosted by Chromatic Art gallery, Miami, Florida.\nThe Embracing Our Differences outdoor International art exhibit featured Celebrating Friendship, among 45 billboard size images created by local, national, and international artists, writers, and students reflecting their interpretations of the theme Enriching lives through diversity. It was an honor that the artwork was among the high ten thousand entries submitted. Sarasota, Florida.\nGrowing is Believing, inspired in the importance of education, family and embracing the natural resources add nourishment and a sense of place and responsibility with the environment. The mural's location is at the Honduran Ministry of Children's and Families of Tegucigalpa, Honduras.\nA Homage to Educators (Un Tributo al Educador) portrays the scenery within the mural as a reminiscence place beloved by locals and visitors alike. Its serene waters and magnificent view offer a peaceful atmosphere where neighboring trees range from pines, forests, and fruit trees deliver fruit to everyone. There is also a strong belief that the dense waters are sacred as it springs life, making it a perfect retreat to gain insights. Next to the artwork, a dedicated poem inspires educators and students who make a meaningful difference in the community. The poem is anonymous and brings pride to future leaders located in La Escuela Normal Mixta del Occidente, La Esperanza, Intibucá, Honduras.\nHonorable Mention received for the titled ceramic; (Metamorfosis, Hombre - Naturaleza), Metamorphosis, Humankind-Nature, In the Biennale of Visual Arts Arturo Lopez Rodezno, CAC-UNAH, Honduras.\nParticipating artist at the Pinwheels @ the Pier Event- in partnership with the Children's Advocacy Center of Collier County event.\nHand-painted using glazes on the pottery pieces for the Empty Bowls Annual Fundraiser."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:73f0f065-2cfe-4b71-9064-ee7c71f69dc8>"],"error":null}
{"question":"What are the immediate environmental threats posed by gypsy moths in Connecticut, and how can ecosystem services valuation help address such forest management challenges?","answer":"Gypsy moths pose severe environmental threats in Connecticut, particularly in the eastern region, where they have caused extensive tree mortality. In Windham County alone, nearly 600 trees died along Canterbury's roads. The moths' caterpillars defoliate trees, especially oaks, and when combined with drought conditions, this has led to tens of thousands of dead roadside trees and severe canopy loss across eastern Connecticut. As for ecosystem services valuation, it provides a framework for addressing such forest management challenges by helping evaluate management impacts and conduct cost-benefit analyses of potential policies. This approach recognizes the collective value of nature, including services like water, air, and recreational opportunities, and can help align forest management objectives with the full spectrum of services that working forests provide to society.","context":["Connecticut residents are all too familiar with the damage wrought by gypsy moths in recent years, particularly in the eastern part of the state.\nIn Windham County, for example, one of the most severely affected areas over the past few years, Canterbury first selectman Christopher Lippke puts the death of trees from gypsy moths along town’s roads at nearly 600.\nWith their voracious appetites, gypsy moth caterpillars are able to defoliate trees, particularly oaks. While healthy trees usually recuperate, the extent of the damage along with drought conditions over the past few years has impeded their recovery.\nTo help deal with the moths’ devastating impact on the environment, Lippke and many others in eastern Connecticut have engaged the resources of UConn Extension.\nUConn’s Tom Worthley an associate extension professor with a joint appointment in the Department of Natural Resources and the Environment, is at the forefront of this effort. He has met with homeowners, landowners, and community leaders throughout Connecticut’s Quiet Corner to offer advice on how to control the gypsy moth population even before an outbreak, what can be done in the midst of one, and how to mitigate damage to trees.\nDead trees are more dangerous the longer they are left to stand. Time is of the essence. — Tom Worthley\nRecently, he has spent much of his time addressing tree mortality.\n“During the spring and summer of 2018,” says Worthley, “the impact of previous years’ drought, defoliation, and secondary opportunistic pathogens became apparent as tens of thousands of roadside trees throughout eastern Connecticut and thousands of acres of oak woodlands exhibited severe mortality.”\nHe notes that both state forest lands and many private parcels in eastern Connecticut have been affected, with severe canopy loss on tens of thousands of acres, and partial canopy loss on many more, from the Rhode Island line west to the other side of the Connecticut River.\nIn Canterbury, Worthley worked with UConn students to assess the extent of the damage, recording stretches of roadway with 30 to 40 dead trees per mile. And he formed a working group of town and state officials, representatives from utility companies, and members of the forestry and arboricultural communities, to coordinate efforts to address the issues.\n“Tom got the ball rolling for us,” says Lippke, the first selectman. “His expertise and knowledge are helping us be proactive rather than reactive.”\nDead trees can be hazardous. Falling trees and branches can injure people, damage property, hinder commuters and emergency responders, and bring down power lines. Dead wood also increases the risk of wildfires. Because of this, Canterbury, like many other towns, is facing prohibitive costs, as the cost of cutting down a single tree can be hundreds of dollars.\nIn addition to local efforts, Worthley is also working at the statewide level. He has convened several meetings of stakeholders concerned about dead roadside trees as potential safety hazards, including representatives from the public utilities, the state departments of transportation and energy and environmental protection, professional arborists, tree wardens, and others. He emphasized the extent and seriousness of the problem, and encouraged communication and cooperation.\nLast month, Worthley and other forestry experts guided Rep. Joe Courtney through Pachaug Forest to survey tree damage. Recognizing the cost of clearing dead and dying trees, Courtney is exploring the possibility of using federal funds to address gypsy moth outbreaks.\n“The scale and scope of tree mortality in eastern and central Connecticut is a potential public safety hazard and a problem beyond the capacity of towns, the Department of Transportation, and utilities,” says Worthley. “Dead trees are more dangerous the longer they are left to stand. Time is of the essence.”","Exploring the financial value of ecosystem services on lands certified to SFI\nWhy This Project Matters\nEcosystem services refer to things that the natural world provides to sustain life, such as water, air and recreational opportunities. An ecosystem services perspective offers a way of looking at the collective value of nature. Placing a financial value on these services provides a tool for policymakers and conservationists to evaluate management impacts and conduct a cost-benefit analysis of potential policies. This project is exploring the financial value of ecosystem services in a California forest, owned by Fruit Growers Supply Company and certified to SFI. It will demonstrate the unique role that forests certified to SFI can play in achieving returns through climate change mitigation, watershed improvement, and habitat conservation. Demonstrating the potential for generating value from ecosystem services will help align forest management objectives with the full-spectrum of services that working forests provide society.\nHow the Project is Exploring the Financial Value of Ecosystem Services\nThe conservation group, Coalitions & Collaboratives, Inc. (COCO) and its strategic partner RenewWest, a Colorado company specializing in forest-based ecosystem services, are leading the investigation into generating conservation-focused returns through the monetization of carbon, water, and conservation markets. Results will be examined and extrapolated to show potential for other forests certified to SFI. The team will assess the potential of applying the California Compliance Offset Protocol for U.S. Forest Offset Projects for both afforestation/reforestation projects and improved forest management carbon offset projects. The project will also provide a quick and relatively inexpensive analysis of the financial values of both carbon and water for Fruit Growers Supply Company, an SFI Program Participant. The project includes elements of forest restoration, some 30,000 acres of the subject area lost forest cover in a wildfire.\nThe SFI Conservation and Community Partnerships Grant Program is supporting this project. Results from this grant are expected to increase understanding of which SFI forest management practices have the most significant impact on carbon, water, and other conservation outcomes. This data will create the opportunity to replicate the most beneficial practices, when appropriate, across certified forests. SFI will play a key role in ensuring outcomes are made available to SFI Program Participants and other forest landowners. Additionally, SFI will work closely with the project team to explore how accounting requirements relating to payments for ecosystem services models could be integrated into SFI Standards.\nHow the Project Helps Forest Managers\nDemonstrating the financial value of ecosystem services will help to align forest management objectives with the full-spectrum of services that working forests provide society. The motivations to measure conservation values are diverse: brand owners seek to understand the impact of their sourcing; conservation stakeholders can engage more effectively if they understand the values that certification can provide; and improved tracking will better equip SFI to provide sustainability related metrics and contribute meaningfully to conservation outcomes.\nThis partnership includes conservationists, researchers, and an SFI Program Participant.\n- Project lead: Coalitions & Collaboratives, Inc.\n- Sustainable Forestry Initiative\n- Colorado Springs Utilities\n- Fruit Growers Supply Company (SFI Program Participant)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:db38272b-27e9-42ab-8ebe-c1170d3cc6c6>","<urn:uuid:40b92bd3-afa7-4d61-a5ac-8d50455e60c7>"],"error":null}
{"question":"What's the difference between using white vinegar vs hydrogen peroxide for cleaning mold - which one's more effective?","answer":"Both white vinegar and hydrogen peroxide are effective for cleaning mold, but they are used differently. Undiluted white vinegar can be applied directly to mold, left for 15 minutes, then scrubbed off, rinsed and dried. For a general cleaning solution, vinegar can be diluted with water (one part vinegar to four parts water) and is particularly effective at combating mold and mildew, especially in bathrooms. Meanwhile, 3% hydrogen peroxide should be used undiluted, sprayed on the mold, left for 10-15 minutes, then scrubbed with a sponge or soft-bristled brush before rinsing and drying. Both methods may need to be repeated if mold persists.","context":["Although traditional cleaning products that you can purchase at the store are effective, they do have one major drawback: most of them are full of harmful chemicals. If you already have everyday household cleaners in your home, spend a little bit of time reading the labels on the bottles. You most likely will see a lot of warnings about only using them in well-ventilated spaces, avoiding contact with your skin and eyes, and keeping them away from children or pets.\nWhat if there was a way to get rid of these dangerous products and replace them with safe, natural products instead? As it turns out, you can. Some of the world’s most effective cleaning products are completely natural, meaning that they don’t contain any chemicals that could do major harm to your family or your pets. If you are ready to start making a change, here are the top five natural cleaning products for your home:\n1. White Vinegar\nVinegar is good for more than just making pickles or creating a delicious vinaigrette. It can also be used to clean and disinfect your home. You can use it to clean everything from your windows to your countertops and floors. It is particularly effective at combating mold and mildew, making it an excellent bathroom cleaner.\nEven though vinegar has a strong aroma, the smell quickly dissipates after it dries. Rather than using vinegar full strength, try diluting it with water. For a general purpose cleaning spray, try mixing one part vinegar to four parts water.\n2. Baking Soda\nPart of what makes baking soda such an effective cleaner is the fact that it is mildly abrasive. That means that you can use it for scrubbing dirty pots and pans, removing hard water stains, cleaning grout, and countless other challenging tasks around your home.\n3. Lemon Juice\nHigh in citric acid, lemon juice has natural antibacterial properties that make it an excellent disinfectant for your home. You can use this powerful natural cleaner on surfaces where germs and bacteria tend to accumulate. One word of caution, however: avoid using it on natural stone countertops or floors. The acidic nature of the lemon juice can sometimes etch the surface of natural stone. Always test lemon juice in an inconspicuous area before cleaning any surface to make sure that it won’t cause damage. For most surfaces, however, it is a highly effective way to kill germs.\n4. Tea Tree Oil\nThis powerful essential oil has antibacterial, antiviral, antimicrobial, and antifungal properties, making it an excellent cleaner for your home. Always dilute it with water before applying it to the surfaces of your home. It is a particularly good choice for cleaning showers, sinks, and other areas where there is a lot of moisture since it does an excellent job of killing mold. One thing that you need to know, however, is that tea tree oil can be toxic to pets in high enough concentrations. Even though it is safe for people, you may want to opt for a different natural cleaner if you have animals in your home.\n5. Olive Oil\nEven though it may sound strange, you can actually use olive oil to clean your home. It does a good job of dissolving oily residue like leftover grease on cooking pans. It also can be used to polish wood furniture, providing an inexpensive alternative to traditional furniture polish.\nThese are the top five natural cleaning products for your home. Chances are, you probably already have a lot of these products laying around. If you haven’t already, try ditching traditional cleaners in favor of these safer natural alternatives. From a health and safety standpoint, you definitely will be glad that you did.","Mold and mildew are types of fungi that grow on damp surfaces. While similar, mildew and mold have characteristics that set them apart, which dictates the most effective cleaning methods.\nMold and mildew are most common in bathrooms, basements, and areas with water damage or excessive humidity levels. Both can cause health problems, allergic reactions, and structural damage to your home.\nIf you are able to identify mold vs mildew, you can clean up the spores and stop these fungi from spreading.\nWhat Is Mold?\nMold is a type of fungi that grows in multicellular structures known as hyphae. It can take on many shapes, textures, and colors. The most toxic indoor mold is black mold, also known as Stachybotrys chartarum. It’s common on often damp surfaces such as shower ceilings.\nMold spores are always present, they just aren’t harmful during the early stages of their growth. They begin to grow when their ideal, dark and moist environment is present.\nWhat Is Mildew?\nMildew is technically a type of mold. Still, it differs from how we normally think of mold varieties. Rather than a texturized surface, mildew is flat and doesn’t grow in as many colors as mold.\nLike mold, mildew grows in moist environments. You might find it on bathroom or basement walls or growing in damp clothes that have set in a hamper for too long. Mildew poses fewer health and structural risks and is easier to clean.\nMold vs. Mildew: Here’s the Difference\nMildew Is Dry\nMildew is a dry and powdery substance that grows on surfaces. It has a texture similar to peach fuzz or mushroom spores.\nMildew Has A Light Color\nTo the naked eye, mildew spores are grey, white, or light green. Though the materials underneath mildew can be dark, if you look closely, you’ll notice that the powdery part of mildew is a light color, usually white.\nMildew Grows On Surfaces\nMildew doesn’t penetrate deep into surfaces; instead, it sits on top.\nMold Is Slimy\nWhile mildew is soft and powdery, mold is slick and slimy. It often has a raised texture (sometimes fuzzy.) If you find a substance that looks like mildew but is slick, it’s probably mold.\nMold Comes In Many Colors\nWhile mildew comes in a set amount of colors, mold comes in a rainbow of colors, including black, green, brown, pink, orange, yellow, and red.\nMold Grows Deep\nMold grows deep into surfaces, which is what enables it to cause structural damage. So if your “mildew” doesn’t” wash off easily, it’s probably mold.\nSigns Of Mold and Mildew\nThe earlier you spot mold and mildew, the less cleaning you’ll need to do. Here are the non-visual signs of mold.\nBoth mold and mildew smell damp, musty, and a little rotten. The smell is from the microbial volatile organic compounds that the mold produces.\nIf the smell is coming from your HVAC system, then there’s a good chance that there is mold growing inside of it. Test regularly to ensure that there aren’t any harmful bacteria building up.\nIf you’re experiencing breathing issues or a runny nose, mold could be the culprit. Other health effects include itchy eyes, runny nose, and respiratory problems. Check out a full list of symptoms before you self-diagnose and see a doctor if you have breathing problems.\nBubbling, peeling, and cracking indicate moisture. Where there is moisture, there is often mold.\nIf you have a leaky pipe or experienced overflowing sinks or toilets that you didn’t promptly clean up, mold may be present. Look for visual signs.\nHow To Prevent Mold Vs Mildew\nWhile getting rid of mold can be difficult, preventing mold isn’t. Take these precautions to prevent mold from growing in your home.\n- Clean up standing water – including splashed bathwater, leaks, and spills.\n- Let moisture-prone areas dry out – your laundry basket, for example, needs to air out between uses to prevent mold growth.\n- Fix leaks promptly – moisture leads to mold and mildew, and leaks can cause severe damage.\n- Run a dehumidifier – high humidity rooms (like basements) will benefit from a dehumidifier.\n- Ensure bathroom exhaust fans work – bathroom ceilings are a prime spot for mold, especially in rooms with non-working exhaust fans.\nDIY Mold and Mildew Removal\nIn most cases, you can remove mold yourself. The EPA recommends calling in mold remediation specialists when mold covers more than a 10-square-foot patch.\nTo eliminate mold, you must first kill it and then scrub it off.\nYou can kill mold and mildew with 3% hydrogen peroxide.\n- Put undiluted hydrogen peroxide on the mold (using a spray bottle is easiest)\n- Saturate and allow the peroxide to sit for 10-15 minutes\n- Scrub with a sponge or soft-bristled brush\n- Rinse and dry the wall\nIf mold or mildew persists, repeat the process.\nBleach And Water Solution\nFill a spray bottle with one part chlorine bleach to three parts water. Spray the mold or mildew, let it sit for 15 minutes, and then scrub the area. Rinse and dry the surface afterward.\nWhite Distilled Vinegar\nSpray undiluted white distilled vinegar on the mold, allow it to sit for fifteen minutes, scrub the mold off, rinse, and dry the area.\nFrequently Asked Questions (FAQ)FAQ\nHow Do I Identify Black Mold Vs Mildew?\nBlack mold is one of the most harmful types of mold. Mildew is much less harmful than most molds. The biggest difference you may notice between them is that mold will be fuzzier and darker.\nIs There A Difference In Mold Vs Mildew Remover?\nThere is a difference between mold vs mildew remover. Mold remover must be stronger while mildew can sometimes be removed with a simple cleaner or vinegar. Mold needs a special cleaner and sometimes requires a professional.\nWhat Kills Mold And Mildew Both?\nOne of the only over-the-counter substances that clean both mold and mildew is bleach. Bleach can kill almost anything you find in your home. So use it with caution and always test in small amounts first.\nDoes Mold Vs Mildew Exposure Create Different Symptoms?\nYes. Mildew symptoms are almost always mild. They are similar to mild cold symptoms. But it doesn’t usually cause any symptoms at all. Mold symptoms can be similar but are always more severe and include lung issues as well."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:0e4f198c-aa51-49b2-a604-eb78446c0161>","<urn:uuid:1868ed34-bb06-48e4-a2f1-03c0debbe24a>"],"error":null}
{"question":"What basic equipment is needed to start a small hydroponic garden, and what is the typical cost range for installation?","answer":"A basic hydroponic system requires water pipes, pumping system, growing beds or buckets, and specific materials like rockwool, gravel, sand, perlite or coco coir. The system may also include a water chiller, heater, and bubbler for oxygenation. For a small household installation, costs typically range from VNĐ6 million ($265) to VNĐ10 million ($442), which covers the complete setup including water pipes, beds, pumps, and seeds. Additional monthly expenses are minimal, around $5 for new seeds and fertilizers.","context":["Viet Nam News\nAn IT engineer in central Việt Nam is demonstrating the value of hydroculture as an effective way to build climate change resilience in the storm-prone region, Hoài Nam reports.\nAfter working with the military-run telecom group Viettel for seven years, Nguyễn Quốc Phong decided he wanted to do something else.\nThe 33-year-old man from Núi Thành District in Quảng Nam Province has since used his IT know-how and experience to study and promote hydroponic farming, launching a startup of his own.\nHydroponics is a subset of hydroculture, the method of growing plants without soil, using mineral nutrient solutions in a water solvent. Terrestrial plants may be grown with only their roots exposed to the mineral solution, or the roots may be supported by an inert medium, such as perlite or gravel. (Wikipedia)\nPhong surfed the internet for information and solutions, found suppliers in the Netherlands and Israel, imported materials and quality fertiliers and seeds.\n“It’s not new in the world, but it has just emerged in central region. Farmers have a long tradition of growing vegetables in the soil, which is low cost and easy, but also carries the risk of contamination with water and air pollution already at alarming levels. And with rapid urbanisation in the region, it is not going to get better soon.\n“Local farmers still hesitate to change to new agriculture, or hi-tech farming. They are used to growing with easy tools, but not electronically controlled devices or smart-phone guided production,” Phong said, explaining why his company, H2O Farm Viet Nam, has launched a package solution.\nPhong said the package, including recycled water mixed with fertiliser, a pumping system and pipes made with durable material, would help farmers save a lot of time working with soil, manual watering and dealing with pest problems.\n“Farmers can manage operation of the system with their smartphones to set up nutrition pumping timer for the vegetables, and they do not need to work directly with hazardous pesticides.\n“However, they have to change from their labour-intensive farming into hi-tech application production with stricter conditions like using toxin-free water and specific doses of other inputs like fertiliser and sunlight,” he said.\nPhong explained that water at all the farms have to be tested to eliminate hazardous contaminants, while seeds and fertilisers from the Netherlands and Israel would offer suitable material for hydroponic farms.\nHe said 24-hour programmed pumping system will help carry a mixture of water and nutrition to grow saplings.\n“It’s like a stream running day and night. Roots will absorb nutrition from the water running through pipes. Each sapling grows in a plastic box, and farmers can harvest easily by moving it up after 25 days,” he said, adding that a small household’s hydroponic garden could produce 40kg of greens each month.\nSafe food: A hydroponics farm in Đà Nẵng City. The hi-tech farm can supply high quality vegetables to resorts and restaurants. — VNS Photo Công Thành\nTrần Thị Anh Thư, a Đà Nẵng resident, said she installed a 6sq.m hydroponic garden on her rooftop eight months ago, growing vegetables for her family.\n“I could harvest vegetables from this garden 25 days after sowing. I do not have to buy these vegetables from the market anymore, because the garden provides plenty and we even have some for neighbours,” Thư said.\nHer family needs at least 3kg of fresh vegetables a day.\n“I invested VNĐ10 million (US$442) for installation of the hydroponic system including water pipes, beds, pumps and seeds. I just spend a little money (around $5) for buying new seeds or fertilisers a month.”\nThư said she did not spend a lot of time on the garden, and had more time now for housework and caring for children.\nThe 42-year housewife also said she had planted vegetables in soil on plastic trays or styrofoam boxes, but that required more time to water, protecting the plants from pests and rats.\nShe said the hi-tech garden was really suitable for household that wants safe vegetables.\nBùi Thị Thanh Sương of Điện Bàn District in Quảng Nam Province, said her family had grown vegetables for 20 years via traditional farming, and only recently began hydroponic farming as a trial.\n“In co-operation with the H2 Farm Việt Nam company, we have developed a small hydroponics area on our 3,000sq.m garden with total investment of VNĐ70 million ($3,000),” Sương said.\n“We saw that growers did not waste time to clear weeds, prepare soil or halt crops because of rain or floods.”\nThe 26-year-old woman said that in hydroponics, pest and insect attacks could be avoided, and strictly tested quality seeds would ensure a bumper crop. \"While it requires more investment than traditional farming, the focus is on consumers seeking high-quality produce,\" she said.\n“High and medium income customers have gradually been seeking safe food and vegetables. They are willing to pay higher prices for daily greens at selected places,” she said.\nShe said hydroponic salad greens, including lettuce costs between VNĐ60,000 ($2.6) and VNĐ90,000 ($3.9) per kilo, much higher than normally grown vegetables, which only cost between VNĐ10,000 ($0.44) and VNĐ40,000 ($1.7) per kilo.\nSương said her family supplies 70kg of vegetables every day to farms with the Việt Nam Agriculture Practice (VietGAP) certification.\nHowever, hydroponics is still limited to growing leafy vegetables, and it should be expanded to other products like beans, pumpkins, potatoes and tomatoes, Sương said.\nPhong said his company can design a household hydroponic garden of just 2 to 6sq.m at minimum costs of VNĐ6 million ($265) to VNĐ8 million ($354).\nHe reiterated that hydroponics can help prevent the alarming overuse of stimulants in agriculture.\nHe said he plans to expand hydroponics into mountainous areas in Quảng Nam, where farms are destroyed by disasters every year, and promote household gardening in Đà Nẵng and Tam Kỳ cities.\n“Household hi-tech gardening can supply safe greens to supermarkets or directly to urban communities, and its expansion would also reduce investment.” — VNS","Hydroponic Growing – An Eco-Friendly Way to Grow Plants and Vegetables\nHydroponic farming is an innovative and environmentally beneficial method of growing plants, and it's great for urban areas where farmland is scarce.\nHydroponics is the process of growing plants without using soil. In the traditional sense of growing plants, soil is usually how plants get the nutrients they need to grow, along with sun and water. It is almost impossible to grow plants in barren lands or where the soil is devoid of essential minerals. With hydroponics, nutrients are directly fed to plants by being added to the water supply, which is much more efficient than being absorbed by the plants through the soil.\nThere are so many benefits to hydroponic growing. One of the pros of hydroponics is that less land is needed to grow plants. The amount of farmable land for commercial agriculture has been steadily shrinking over the years. Much more can be grown with hydroponics in a small space, compared to traditional farming. And, because it doesn’t require soil, hydroponic growing can be used in areas with harsh climate and little fertile soil, like the desert. Less water usage is another important benefit. Hydroponic growing uses only 10% of the water that would be needed to grow plants in soil and it is recyclable. Another benefit to hydroponic growing is the elimination of herbicides and pesticides. In conventional farming, herbicides are sprayed to kill off unwanted plants (weeds), while pesticides are applied to control insects. The spraying of pesticides and herbicides is detrimental to the environment by causing runoff, which affects both soil and bodies of water, contaminating nearby ecosystems and potentially poisoning other animals as well as the target insect.\nWith hydroponics, you have full control over your garden with more optimal results. Hydroponics provides the exact needs to the plants and they typically grow faster and have higher yields than plants grown traditionally because they do not need to expand their roots to look for nutrients. And hydroponic growing doesn’t just make your growing cycle easier; it extends your growing season and is more efficient. When growing indoors, plants can be grown during any season since they are grown in their own controlled environment.\nThere are various systems available for indoor and outdoor growing. A typical outdoor/greenhouse Dutch Bucket System could contain 8 buckets (2 rows of 4) that rest on a PVC frame with an integrated drainage system. The recirculating pump in the reservoir tank feeds the optimal level of hydroponic nutrients and water to crops through drippers. The buckets have a siphon elbow drainage system to provide the correct water level for optimal root health. Sometimes a water chiller and heater are needed to maintain the temperature of the system. A bubbler can also be used to oxygenate the water supply. Net pots (cups) can be used to place the plants in, or hydroponic growing media such as rockwool, gravel, sand, perlite or coco coir.\nIf you are passionate about growing plants, or you need to grow where soil or land conditions restrict you, or if you are looking for more innovative and environmentally friendly farming practices, then hydroponic growing could be a great alternative for you.\nRDWC (Recirculating Deep Water Culture) system\nDWC (Deep Water Culture) system for sale"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:dddf944b-b78d-4676-874e-dd17164be4d5>","<urn:uuid:f620707d-1e9d-4ac7-ae28-0ea2fbf59344>"],"error":null}
{"question":"How do dark matter and dark energy differ in their effects on the universe's expansion, and what percentage of the universe does each make up? I'm really curious about these mysterious forces! 🌌","answer":"Dark matter and dark energy have opposite effects on the universe's expansion. Dark matter, which makes up about 23% of the universe, exerts a gravitational pull that would tend to slow expansion. In contrast, dark energy, comprising approximately 73% of the universe, acts against gravity and accelerates the universe's expansion, pulling the cosmos apart at ever-increasing speeds. Dark energy's effects became more prominent in recent cosmic history, as shown by observations that the universe was expanding more slowly in the distant past compared to today.","context":["The vastness of space and the puzzling nature of the cosmic objects that occupy it provides no shortage of material for astronomers to ponder.\nTo round up some of the most enduring mysteries in the field of astronomy, the journal Science enlisted help from science writers and members of the Board of Reviewing Editors to choose eight puzzling questions being asked by leading astronomers today.\nAs Robert Coontz, deputy news editor at Science, writes in his introduction to the series, the participants decided that, \"true mysteries must have staying power,\" rather than being questions that might be resolved by research in the near future. In fact, while some of the topics discussed may one day be solved through astronomical observations, others may never be solved, he added.\nIn no particular order, here are eight of the most compelling mysteries of astronomy, as presented by the journal Science:\nWhat is dark energy?\nIn the 1920s, astronomer Edwin Hubble discovered that the universe is not static, but rather is expanding. In 1998, the Hubble Space Telescope, named for the astronomer, studied distant supernovas and found that the universe was expanding more slowly a long time ago compared with the pace of its expansion today.\nThis groundbreaking discovery puzzled scientists, who long thought that the gravity of matter would gradually slow the universe's expansion, or even cause it to contract. Explanations of the universe's accelerated expansion led to the bizarre and hotly debated concept of dark energy, which is thought to be the enigmatic force that is pulling the cosmos apart at ever-increasing speeds.\nWhile dark energy is thought to make up approximately 73 percent of the universe, the force remains elusive and has yet to be directly detected.\n\"Dark energy might never reveal its nature,\" Science staff writer Adrian Cho wrote. \"Still, scientists remain optimistic that nature will cooperate and that they can determine the origins of dark energy.\"\nHow hot is dark matter?\nIn the 1960s and 1970s, astronomers hypothesized that there might be more mass in the universe than what is visible. Vera Rubin, an astronomer at the Carnegie Institution of Washington, studied the speeds of stars at various locations in galaxies. [Top 10 Strangest Things in Space]\nRubin observed that there was virtually no difference in the velocities of stars at the center of a galaxy compared to those farther out. These results seemed to go against basic Newtonian physics, which implies that stars on the outskirts of a galaxy would orbit more slowly.\nAstronomers explained this curious phenomenon with an invisible mass that became known as dark matter. Even though it cannot be seen, dark matter has mass, so researchers infer its presence based on the gravitational pull it exerts on regular matter.\nDark matter is thought to make up about 23 percent of the universe, while only 4 percent of the universe is composed of regular matter, which includes stars, planets and humans.\n\"Scientists still don't know what dark matter is, but that could soon change,\" Cho wrote. \"Within years, physicists might be able to detect particles of the stuff.\"\nBut while astronomers may soon be able to detect particles of dark matter, certain properties of the material remain unknown.\n\"In particular, studies of runty 'dwarf galaxies' might test whether dark matter is icy cold as standard theory assumes, or somewhat warmer — essentially a question of how massive particles of dark matter are,\" Cho explained.\nWhere are the missing baryons?\nIf dark energy and dark matter combine to make up roughly 95 percent of the universe, regular matter makes up about 5 percent of the cosmos. Yet, more than half of this regular matter is missing.\nThis so-called baryonic matter is composed of particles such as protons and electrons that make up most of the mass of the visible matter in the universe.\n\"As astronomers count baryons from the early universe to the present day, however, the number drops mysteriously, as if baryons were steadily vanishing through cosmic history,\" wrote Yudhijit Bhattacharjee, a staff writer at Science.\nAccording to Bhattacharjee, astrophysicist suspect the missing baryonic matter may exist between galaxies, as material that is known as warm-hot intergalactic medium, or WHIM.\nLocating the missing baryons in the universe continues to be a priority in the field of astronomy, because these observations should help researchers understand how cosmic structure and galaxies have evolved over time.\nHow do stars explode?\nWhen a massive star runs out of fuel and dies, it triggers a spectacular explosion called a supernova that can briefly shine more brightly than an entire galaxy.\nOver the years, scientists have studied supernovas and recreated them using sophisticated computer models, but how these gigantic explosions occur is an enduring astronomical puzzle. [Gallery: Supernova Explosions]\n\"In recent years, advances in supercomputing have enabled astronomers to simulate the internal conditions of stars with increasing sophistication, helping them to better understand the mechanics of stellar explosions,\" Bhattacharjee wrote. \"Yet, many details of what goes on inside a star leading up to an explosion, as well as how that explosion unfolds, remain a mystery.\"\nWhat re-ionized the universe?\nThe broadly accepted theory for the origin and evolution of the universe is the Big Bang model, which states that the cosmos began as an incredibly hot, dense point roughly 13.7 billion years ago.\nA dynamic phase in the history of the early universe, approximately 13 billion years ago, is known as the age of re-ionization. During this period, the fog of hydrogen gas in the early universe was clearing and becoming transparent to ultraviolet light for the first time.\n\"Some 400,000 years after the big bang, protons and electrons had cooled off enough for their mutual attraction to pull them together into atoms of neutral hydrogen,\" science writer Edwin Cartlidge stated. \"Suddenly photons, which previously scattered off the electrons, could travel freely through the universe.\" [Big Bang to Now in 10 Easy Steps]\nA few hundred million years later, the electrons were stripped off the atoms again.\n\"This time, however, the expansion of the universe had dispersed the protons and electrons enough so that the new energy sources kept them from recombining. The 'particle soup' was also dilute enough so that most photons could pass through it unimpeded. As a result, most of the universe's matter turned into the light-transmitting ionized plasma that it remains today.\"\nWhat's the source of the most energetic cosmic rays?\nThe source of cosmic rays has long perplexed astronomers, who have spent a century investigating the origins of these energetic particles.\nCosmic rays are charged subatomic particles — predominantly protons, electrons and charged nuclei of basic elements — that flow into our solar system from deep in outer space. As cosmic rays flow into the solar system from elsewhere in the galaxy, their paths are bent by the magnetic fields of the sun and Earth.\nThe strongest cosmic rays are extraordinarily powerful, with energies up to 100 million times greater than particles from manmade colliders. Still, the origin of these strange particles has been an enduring mystery.\n\"After a century of cosmic ray research, the most energetic visitors from space remain stubbornly enigmatic and look set on keeping their secrets for years to come,\" wrote Daniel Clery, deputy news editor at Science.\nWhy is the solar system so bizarre?\nAs astronomers and space observatories discover alien planets around other stars, researchers have been keen to understand the unique characteristics of our solar system.\nFor instance, while extremely varied, the four innermost planets have rocky outer shells and metallic cores. The four outermost planets are vastly different and each possess their own identifiable features. Scientists have studied the process of planetary formation in hopes of grasping how our solar system came to be, but the answers have not been simple.\n\"Looming over all the attempts to explain planetary diversity, however, is the chilling specter of random chance,\" wrote Richard Kerr, a staff writer at Science. \"Computer simulations show that the chaos of caroming planetesimals in our still-forming planetary system could just as easily have led to three or five terrestrial planets instead of four.\"\nBut the search for alien worlds could help scientists hoping to gain insights into the planets closer to home.\n\"Help might come from planets orbiting other stars,\" Kerr wrote. \"As exoplanet hunters get beyond stamp-collecting planets solely by orbit and mass, they will have a far larger number of planetary outcomes to consider, beyond what our local neighborhood can offer. Perhaps patterns will emerge from inchoate diversity.\"\nWhy is the sun's corona so hot?\nThe sun's ultrahot outer atmosphere is called the corona, and it is typically heated to temperatures ranging from 900,000 degrees Fahrenheit (500,000 degrees Celsius) to 10.8 million degrees F (6 million degrees C).\n\"[F]or the better part of a century, solar physicists have been mystified by the sun's ability to reheat its corona, the encircling wispy crown of light that emerges from the glare during a total solar eclipse,\" Kerr said.\nAstronomers have narrowed down the culprits to energy beneath the visible surface, and processes in the sun's magnetic field. But the detailed mechanics of coronal heating are currently unknown.\n\"Just how the magnetic field transports the energy is much debated, and how the energy gets deposited once it reaches the corona is even more mysterious,\" Kerr wrote.\nTo see Science's full report on the greatest mysteries in astronomy today, see here: http://www.sciencemag.org/site/special/astro2012/index.xhtml","THE UNSEEN UNIVERSE:\nDARK MATTER AND DARK ENERGY\nThe vast areas between the stars and galaxies appear empty and dark, but this impression is misleading. We now know that 96 percent of the universe is dominated by unknown and unseen forms of mass and energy: dark matter and dark energy. Two fundamental goals of cosmology are to determine the composition of the energy and matter in the universe. It appears that the simplest expectation, a universe made of ordinary matter (so-called \"baryons\" — the familiar neutrons and protons) is wrong. Instead, we appear to live in a universe which challenges our understanding of physics.\nModern cosmology has been built on two pillars of radiation: the residue from the big bang and the distribution and spectra of stars and galaxies. Yet it has long been known that mass, not luminosity, is the key to the structure of the universe. This is because gravity plays a central role in the formation of structure. Over cosmic time, \"over-dense\" regions become still denser. Tiny ripples of density existing 300,000 years after the big bang have grown into the complexity of mass structure — galaxies to clusters to super-clusters of galaxies — we see in today's universe some 14 billion years later. On the largest of scales, the overall expansion history of the universe is governed by its mass-energy (Einstein taught us that mass and energy are related). Since mass could not be seen directly, astronomers have until recently used the luminosity of the trace amounts of ordinary matter in existence as a proxy for total mass in cosmological studies. Yet this ordinary matter, the baryons we are made of, cannot be the chief component of most of the mass in the universe.\nA huge amount of dark matter — roughly ten times as much mass as there is in all the stars and gas and dust — controls the early evolution of structure in the universe. The dark matter is thought to be some very different kind of particle created during the hot big bang that interacts only weakly with the familiar particles of \"normal\" matter. In the earliest moments of the universe, corresponding to temperatures and energies far higher than any attainable or even imaginable on Earth, a legacy in the form of dark-matter particles was created. This legacy is detectable today in its cumulative gravitational effects on large-scale structures in the universe.\nOVER THE LAST FEW years the composition of the universe has become even more puzzling, as the observed luminosity of Type Ia supernovae at high red shift, and other observations, appear to imply an acceleration of the universe's expansion in recent times. In order to explain such an acceleration, we need dark energy with large negative pressure to generate a repulsive gravitational force. Even more puzzling is the fine-tuning of parameters which seems to be required to explain why the dark-energy density today is about the same as that of dark matter, whereas it evolves very differently with time. Moreover, this density is only a factor of ten larger than that of baryons and neutrinos. This may imply, as the Ptolemaic epicycles did, that we are lacking a sufficiently deep understanding of fundamental physics. It is possible that what we call dark matter and dark energy arise from some unknown aspect of gravity. Thus, the highest energies and the universe on the largest scales are connected. Today the worlds of particle physics and cosmology are coming together in a transformed world view: Copernicus dethroned Earth from a central position in the cosmos, and Edwin Hubble demoted our galaxy from any significant location in space. Now, even the notion that the galaxies and stars comprise most of our universe is being abandoned. Emerging is a universe largely governed by dark matter and, we are beginning to think, by an even stranger dominance of a smoothly-distributed and pervasive dark energy.\nHOW DO WE STUDY dark matter if we cannot see it? These mountains of mass will bend light rays from background galaxies like a lens and create \"cosmic mirages.\" This tool was made possible by the discovery of a population of distant galaxies so numerous on the sky that they could be used as sources for the statistical reconstruction of an image of the foreground dark matter that served as the lens. From their warping of the visible matter behind them, we now can see the dark matter clumps, map them and chart their development over cosmic time. To see the dark matter, we have to \"invert\" the cosmic mirage it produces. We have to look deep enough and wide enough into the background universe so that there are thousands of galaxies projected near every foreground lens. Exploring the full range of mass structures will require a new facility which will image billions of distant galaxies.\nHow much dark matter in our universe resides outside clusters of galaxies? If most of the mass in clusters is in a smooth distribution extending out millions of light years, perhaps most of the dark matter of the universe is distributed more broadly than clusters. To make an analogy closer to home, clusters are the Mount Everests of the universe. But most of the mass in Earth's mountains resides in the more numerous foothills. Though we are currently limited to fields of view less than a degree across, telescopes are nevertheless being used to probe this universal dark-matter distribution.\nOur new observational mass maps cover a limited range of scales. There are big and small clusters of mass, and there are what appear to be filaments of mass — enough mass to add up to about a third of the density that, in the absence of dark energy, would be required to ultimately slow the expansion of the universe to zero. Through these first probes, we have glimpsed a complex universe of mass: dilute filaments of mass coexisting with piles of dark matter centered on clusters of galaxies. This complex dark-matter structure took billions of years to grow. Its growth rate is predictable for a given model of the expanding universe. Probes of other features of the universe — from primeval deuterium to tiny fluctuations in the heat left over from the big bang to supernovae at large distances — suggest that some form of dark energy, when combined with the gravity of the dark matter, creates a flat cosmic geometry in which parallel light rays remain parallel. To determine what this dark energy is and how we can probe its physics, we look at its influence on the expansion rate of the universe. Dark energy acts against gravity, tending to accelerate the expansion.\nTo test theories of dark energy we would like to measure the way some volume which is co-moving with the Hubble flow changes with cosmic time. For a universe with a given mass density, the time history of the expansion encodes information on the amount and nature of dark energy. Measuring this change in co-moving volume by taking \"snapshots\" of mass clusters at different cosmic times would provide clues to the nature of dark energy. In turn, this would tell us something about physics at the earliest moments of our universe, setting the course for its future evolution.\nThe world of quantum gravity at a fraction of a second after the big bang, when the universe was so hot and dense that even protons and neutrons were broken up into a hot soup of quarks, connects to the world as we now see it — a vast expanding cosmos extending out 14 billion light-years. Dark energy and dark matter are relics of the first moments when unfamiliar physics of quantum gravity ruled. A route to understanding dark matter and probing the nature of dark energy is to count the number of mass clusters over the last half of the age of the universe — at a time when dark energy apparently had its greatest influence. LSST does this in several independent ways. These probes of the nature of dark energy by LSST are complimentary to those of space missions measuring the cosmic microwave background and very distant supernovae. Indeed, since we understand so little about dark energy, it is prudent to pursue all these lines of investigation.\nMASS IN THREE DIMENSIONS\nThe faintest galaxies have a range of colors, each one's color depending on its type and its distance from us. The most distant galaxies have their spectra shifted to longer, redder wavelengths by the Hubble expansion, and their light has taken up to ten billion years to travel to us. Using the colors of the galaxies, it is possible to gauge the distance to the background galaxies. Mirages also rely on distance. This is the clue that unlocks the universe of mass in three dimensions; the more distant the source, the more warped its image. If there is a foreground mass, the mirage effect on the background galaxies is stronger for more distant galaxies.\nBy measuring both the warp and the distances to the background galaxies, it is possible to reconstruct the mass map and also to place the mass at its correct distance. This enables the exploration of mass in the universe, independent of light, since only the light from the background galaxies is used. By exploring mass in the universe in three dimensions we are also exploring mass at various cosmic ages. This is because mass seen at great distance is mass seen at a much earlier time. So we can chart the evolution of dark matter structure with cosmic time.\nSurveying the numbers of cosmic mass clusters in our universe will ultimately lead to precision tests of theories of dark energy. To fully open this novel window of the three-dimensional universe of mass history, we need a new telescope and camera very unlike what we have now. We need LSST. Advances in technology have equipped us to mine the distant galaxies for data — in industrial quantity. LSST's wide-angle gravitational lens survey will generate millions of gigabytes of data and intriguing opportunities for unique understanding of the development of cosmic structure. Our challenge is twofold. These galaxies are faint, and we need to capture images of billions of them. LSST's combination of large light-collecting capability and unprecedented field of view will for the first time open this unique window on the physics of our universe. LSST will provide a wide and deep view of the universe, allowing us to conduct full 3-D mass tomography to chart not only dark matter, but the presence and influence of dark energy.\nDo we trust our current view of the universe? Combining these results with other cosmic probes will lead to multiple tests of the foundations of our model for the universe. What will our concept of the universe be when those answers are in? Perhaps the most interesting outcome will be the unexpected; a clash between different precision measurements might prove to be a hint of a grander structure, possibly in higher dimensions. LSST provides that opportunity."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:44d62114-c89a-4e20-a706-4775bcb58490>","<urn:uuid:928546be-e3fb-4584-a2c9-cde878833ca2>"],"error":null}
{"question":"What's the difference between how meeting minutes are handled in Robert's Rules versus current best practices for note-taking?","answer":"Robert's Rules of Order specifically requires that only the main motions are documented in the minutes. In contrast, current best practices are more comprehensive - they suggest either bringing a colleague to take detailed notes or recording the meeting (with attendees' permission) so the meeting leader can concentrate on running the meeting. This allows for capturing more detailed information beyond just the main motions.","context":["Copyright 2006 Deborah Torres Patel\nThorough meeting preparation alleviates anxiety. Good planning guarantees that meetings are relevant, don’t overrun and aren’t held back by uniformed, boring or disinterested attendees. Follow these 19 timeless tips to keep your meetings on track and on time.\nWhen preparing your agenda …\n1. Identify the aim of your meeting\n2. Put the most important items first\n3. Establish a clear outcome for each point\n4. Judiciously choose meeting invitees. Ask yourself, “Who should attend?” “Should attendees be present for all or just part of the meeting?”\n5. Place controversial points towards the end so the early part of the meeting can flow smoothly\n6. If you work for a large organization and not everyone knows each other there may be a need for very short introductions. Schedule time for people to quickly share, “Who I am, my role in the company and why I’m here.”\nDistribute a specific agenda at least one week before the meeting. Make sure that everyone attending has all the information they need and that presenters know exactly how much time they are allotted.\nWhen circulating the agenda, state that the meeting will start sharp and end on time. This will subtly set the tone for an efficient meeting. Obviously, it is critical that the meeting chair sticks to the timeline.\nThe meeting day…\n1. Rehearse your presentation (if applicable)\n2. Arrive early\n3. Double check equipment\n4. Serve coffee, tea, water or refreshments before a 30-60 minute meeting. Any meeting longer than 30 minutes should have drinks available throughout.\n5. If it’s an important meeting, bring a colleague with you to take notes so you can concentrate on the meeting. A discreet alternative is to record the meeting if there are no objections from attendees.\n6. Avoid giving all handouts at the beginning because people often leaf through the paperwork instead of being attentive.\nUnfortunately, well-planned meetings can be derailed by meeting participants. If you have an assertive meeting chair, s/he can easily get the meeting back on track. However, anyone can step in if they have confidence or organizational clout.\n7. An upright and open posture is commanding. You can change the volume, pitch, speed or tone of your voice to keep people’s interest and engage them by simply leaning forward.\n8. Monitoring other people’s body language can keep you on top of the meeting. Involve slouching or disinterested people by asking for their opinions.\n9. When it is your turn to present, remind others that your aim is to keep the meeting as short as possible. Your intention can motivate others to do the same.\n10. If speakers are long-winded or have a personal agenda, you can take control assuming a moderator’s role with a few well-placed interruptions like, “May we address the next item on our agenda?” or “Would it be possible for us to go over the details later? Or “Can we discuss the specifics offline?”\n11. Suggest a short toilet break to stretch if the meeting is dragging.\n12. If an argument or unresolved item prolongs a meeting, call the formal part of the meeting to an end and organize a separate meeting to address the issue.\n13. Before ending the meeting, solidify specific task ownership and action items.\nTo ensure your valuable time isn’t usurped by an endless meeting, communicate in advance that you are only available for the scheduled meeting time and politely excuse yourself if the meeting runs overtime. It is your right to leave.\nStart and end your own meetings on time and develop a reputation for short, well-organized gatherings. Your colleagues will respect you and contribute much more when they feel you value their time.","Robert’s Rules of Order is a book that was first published in 1876, by US Army Brigadier General Henry Martyn Robert that provides a suggested structure of how to run meetings effectively.\nAs the story goes, Robert had to lead a church meeting in 1863 and he felt inadequately prepared for such a responsibility. This piqued an interest for him in parliamentary procedures which were ways of running parliamentary meetings. Through his research and investigations he discovered that there was not one universally accepted way of running meetings, so he decided to publish a book on the subject that would clarify best practice.\nWhile the rules are based on parliamentary procedures, in fact they are designed for and intended to be applied to the meetings of any organization. The book Robert’s Rules of Order is currently in its eleventh edition, and is still an excellent reference point for those wanting a definitive resource of how meetings should best be run.\nRobert’s Rules of Order Simplified\nSome of the main aspects of Robert’s Rules are below. The language used in Robert’s Rules is very formal, so more down-to-earth terminology has been included. These guidelines will help people to hold more productive meetings.\nAgenda – when organizations use Robert’s Rules they usually follow a strict agenda that adds structure to their meetings. MeetingKing has an agenda that can be used to provide a good meeting structure.\nMotion – motions are used to discuss a new item of business. They are introduced on the agenda, and can also be suggested at the meeting. The word “motion” is quite old-fashioned in many organizations, but it means an idea or subject for discussion. Motions can also be used to suggest an action to be taken or a decision that should be made by the organization. Motions must be made, seconded by a different person (a person who seconds a motion is someone who supports it), debated and then voted on. If there is no second or in other words, no support for the motion, it is dropped.\nPostpone Indefinitely – this move is taken if a motion is to be not discussed further at this meeting, though it may be reintroduced again at a later meeting. The decision to postpone indefinitely must be seconded and voted on. The Parking Lot functionality in MeetingKing allows you to do this by letting you move topics there during the meeting.\nTable – this action is used to postpone discussion of an item until later in the meeting or at a later date. Again, the decision must be seconded, and voted on. This is also the same as the idea of moving a topic to the Parking Lot in MeetingKing.\nQuestion – this can be used to terminate a debate so that a motion can be voted on. As with all of the other actions, it needs to be seconded by a different person. Directly after this a vote is held and a two-thirds majority is needed for it to pass. In the case of the vote passing, the motion is then voted on directly.\nAmend – sometimes a motion needs to be changed after it has been debated a bit. Someone might suggest an amendment, and in this case, it must be seconded to be voted on. If accepted, the amendment stays. If someone suggests an amendment to a previous meeting, this is where MeetingKing can really help. That’s because MeetingKing stores all of the information of previous meetings so all of this is available at your finger tips.\nCommit – one step that can be taken with motions is to have them researched further by a separate committee and reported back on at the next meeting. This job might be assigned to an existing committee or a newly organized committee. It must be seconded and be passed by majority vote for this action to occur. At this point it is possible in MeetingKing to include these items as tasks and also add them as topics on the next agenda.\nAdjourn – someone will make a motion to end the meeting. Once again, this motion must be seconded and followed by a vote to adjourn the meeting. The agenda helps to set a time and schedule and usually the chairman will call to end the meeting. If applicable a follow-up meeting should be scheduled. In MeetingKing this can be done with one click and all relevant information, including tasks from the previous meeting(s) are added to the new agenda.\nMinutes – the minutes of the meeting will be documented throughout and distributed to the attendees—and those invited who could not attend—after the meeting is over. Robert’s Rules requires that only the main motions are documented in the minutes. One of the best things about MeetingKing is that it supports the making of meeting minutes in a simple manner. MeetingKing formats notes automatically, so there is no extra work to be done. Decisions and tasks are also included and are tracked in the system, for easy management and reliable follow-up.\nMore information on Robert’s Rules of Order\nIf you want to get an easy and good understanding on apply to use Robert’s Rules in your organization we suggest you visit: Robert’s Rules Made Simple\nThe official Robert’s Rules website has more information: http://www.robertsrules.com/\nAnother Robert’s Rules source of information is: http://www.robertsrules.org/\nThe table of rules from the 4th edition (the last one that Robert himself published) can be found at: http://www.bartleby.com/176/91.html"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:d4bf5efc-9963-4e5b-b52b-0dbf09115ceb>","<urn:uuid:18923b25-91d3-4f3e-ba1b-781dec7a6cef>"],"error":null}
{"question":"What are the key differences between pest control methods in modern food processing plants compared to traditional grain storage?","answer":"Modern food processing plants emphasize preventive measures through facility design and location selection, including site elevation, proper drainage, and physical barriers like insect and vermin barriers. In contrast, traditional grain storage relies more on reactive measures like using neem seed powder for pest prevention and fumigants such as Celphos tablets or Ethylene Dibromide after pest infestation occurs. Both approaches recognize the importance of cleanliness and proper moisture control, but modern facilities incorporate these considerations into their initial design while traditional storage often requires ongoing maintenance and intervention.","context":["From Feed to Food, Part 1\nWhat does the Federal Drug Administration (FDA) case against the now-bankrupt American Peanut Corporation have to do with dog kibble? Plenty, if a dog or cat owner contracts salmonellosis or another food-borne illness from handling tainted kibble. FDA food safety standards now apply to pet food processing, and this agency has the legal authority to press federal criminal charges against a food manufacturer or processor if unsanitary plant conditions cause human illness.\nDon’t think of the product as animal feed, or even pet food, anymore. It’s food, period.\nPet food manufacturers have watched consumers abandon well-known brands in droves after a pet food recall, and know that their flight impacts both current and future profits. Yet, in the evolution from feed to food production, many companies have not fully addressed all of the issues associated with food safety. Pet food manufacturers might consider lessons learned from human food manufacturers when it comes to designing and operating processing plants to ensure food safety.\nHere is a look at some of the strategies that pet food manufacturers can apply to improve food safety through more effective site selection, facility design, storm run-off and waste water management, work flows, waste management, and safety training.\nAvoid Sites Located in the Shadow of a Volcano\nIt is essential to select a piece of land that limits the potential contaminants. Sounds obvious, but there have been plenty of cases in which food manufacturers have been lured by cost-saving site deals, including one that was literally located in the ash plume of a volcano.\nThe right location is one that minimizes contamination risks from water and air. A good start would be a relatively flat site. Ideally, it would be elevated above the local estuaries. Elevated and flat sites minimize the cost of storm run-off management, which can be significant in many areas.\nIf a low-level site is the only option, avoid the use of open water ponds. One strategy that has proven effective in the food industry is to have the site drained efficiently, conveying water away from the building, then construct a significantly raised building pad. This will cost more initially, but will save a great deal in storm run-off management.\nSometimes the purchase of the gross acreage depends on a government entity draining the building site for the pad. In other cases, industrial park managers provide regional storm run-off management.\nChoosing the site presents cost trade-offs that must be weighed carefully, such as when the location is near a power utility station, waste treatment facility, or other manufacturing neighbors. Drawbacks presented by the surrounding structures can also invoke some creative design advantages to overcome them.\nFor example, a developer conceptualized constructing a ship-out facility for an existing food manufacturing plant utilizing the established rail yard for transporting large quantities of packaged finished goods. With the rail yard accessible to both the manufacturing plant and the new ship-out facility, goods could be trucked a short distance to the ship-out facility, sorted and marshaled into rail cars.\nDue to the physical constraints of the site, the ship-out facility’s receiving docks would face the prevailing winds. Not an ideal scenario. To complicate matters, a neighboring recycler already occupied part of the rail yard up wind. A completely exterior operation, the recycler loaded open top rail cars with “fluff” ( i.e., all the crushed non-metal components from auto recycling). Dust and debris from that operation would blow directly into the open doors of the new ship-out facility.\nTo control contamination, the ship-out facility’s receiving docks would need to be rotated to face away from the most direct means of access, as well as require an additional investment in sheltered, sealable, access-controlled doors. The cost required to control contamination from neighbors would have to be factored in to the Return On Investment (ROI) calculations to determine project feasibility and payback.\nDon’t Become the Local Watering Hole\nWater attracts and sustains wildlife and microorganisms. Pet food manufacturers should keep as much as possible off the site, and get rid of the rest pronto. The optimal storm run-off management solution depends on local building codes and hydrogeology.\nIn most cases, the solution involves a retention pond (which intermittently contains water) or detention pond (which always contains water), depending on the hydrogeology of the site and surrounding region. Regardless, locate it downwind and at least 500 feet away from the facility. Dig it shallow to better blend in with the surrounding landscape and fill it with granular material that allows water to percolate more quickly into the soil beneath.\nLandscaping at the facility site can absorb storm run-off, but should be limited to trees and shrubs that do not bear flowers or fruit or produce sap. An insect and vermin barrier should also surround the building perimeter. This is typically an uninterrupted pea-gravel band at least 3 feet wide and 3 feet deep. Landscape installers must position major plantings, such as trees, at least 30 feet from the structure and maintain branching growth to no closer than 6 feet from the structure.\nFor part two of this two-part series, please visit www.chem.info/Articles/2012/07/Plant-Operations-From-Feed-to-Food-Part-2/. For more information, please visit www.ssoe.com.","गेहूं अनाज का वैज्ञानिक भंडारण\nAgricultural commodities have to undergo a series of operations such as harvesting, threshing, winnowing, grading, bagging/packing, transportation and storage before they reach to the consumers. There are significant losses at all these stages.\nThe most economically important step in case of the cereals like wheat is the storage of the grain produce. Storage is the interim phase in the chain of transporting agricultural products from farmers to consumers. Losses during the storage constitute a significant share of food grain loss in post production operations.\nWith the production of nearly 87 million tonnes of wheat India has potential to meet the food requirement of the country. Losses in wheat production are generally noticed at various stages like threshing, transport and storage. Also the attack of rodents and birds lead to losses in grain production of wheat. Losses during storage are mainly due to attack by storage insect-pests, loss of moisture in grain, fungus infestation, rodent attack, and spillage. Since a huge amount of the wheat produce is lost during the storage of grains, various precautions need to be taken to prevent these losses.\nThe time of harvesting in wheat plays a vital role. Wheat crop should be harvested, when the crop is at field maturity and grains become hard. If wheat crop is harvested before proper maturity, it leads to low recovery of grain, higher proportion of immature seeds, poor quality seeds, with high moisture content, that are prone to diseases during storage.\nBut a delay in the harvesting causes shattering losses and exposes the wheat grains to birds, rodents and insect and pest attack. Therefore harvesting should be done in dry conditions at maturity so that the moisture content of the grains is optimum. Threshing and winnowing should be done in the fields to avoid losses. Direct sun drying and excessive drying should be avoided and the grains should be packed in sound clean gunny bags to minimize the losses.\nWheat grain is commonly contaminated with:\n1. Smut balls -\nWheat plants infected with smut fungi produce smut balls and when fully developed these are of the size of the kernels. The internal part of the seed is replaced by the smut spores and produce off-colour flour when milled thus reducing the market value. These can be removed from wheat grain, either by screening or by wind.\n2. Ergot bodies -\nErgot bodies are hard, spur like, purplish black structure which replaces the kernel on grain head. These fungal bodies contain alkaloids that may lead to poisoning on consumption.\n3. Mouse droppings -\nMouse droppings are black and of various shapes and sizes. These are commonly present in wheat that has been stored for long time.\n4. Chaff -\nChaff consists of broken wheat plant parts and the glumes (papery bracts) which enclose the kernels. These are usually easy to remove by winnowing/or screening except where the kernels are enclosed by the glumes.\n5. Insect parts -\nInsect legs, body parts etc are commonly found in wheat fields and since these are lighter in weight, can be easily removed by fanning.\nIn India, about most of farm produce is stored by farmers for their own consumption. Farmers store grain in bulk, using different types of storage structures made from locally available materials. Storage structure design and its construction also play a vital role in reducing the losses during storage. The major construction materials for storage structures in rural areas are mud, bamboo, stones, and plant materials. They are neither rodent-proof, nor secure from fungal and insect attack. Some of the major considerations in building a storage structure to reduce storage losses are:\n- The storage structure should be elevated from the ground and away from moist places in the house\n- As far as possible, the structure should be airtight, even at loading and unloading ports\n- Surrounding area of the structure should be clean to minimize insect breeding\n- Rodent-proof materials should be used for construction of rural storages\n- The structure should be plastered with an impervious clay layer to avoid termite attack, or attack by other insects.\n- Various research and development organizations in India have identified some proven, age-old structures from certain areas of the country and based on these, some improvised storage structures have also been developed and recommended for use at farmer level.\nWheat Storage Methods:\nThe storage methods vary from mud structures to modern bins. Grains can be stored indoors, outdoor or at underground level.\nIndoor storage involves grain containment in structures like Kanaja, Kothi, Sanduka and earthern pots. Kanaja is a grain storage container made out of bamboo. The base is usually round and has a wide opening at the top which is plastered with mud and cow dung mixture or covered with paddy straw or gunny bags. Sanduka, the wooden boxes with the capacity of 3-12 quintals are used for storing small quantity of grains. Kothi an indoor strorage structure is constructed with a large door for pouring grains and small outlet is made for taking out the grains. Earthen pots made locally using burnt clay are indoor storage containers for storing small quantity of grains, when arranged are arranged one above the other they are known as dokal.\nOutdoor storage of grains is done in structures made of bamboo or straw mixed with mud. Gummi is an outdoor structure that is made with bamboo strips or locally available reeds, usually circular or hexagonal in shape and plastered with mud used for storing grains. Kacheri is a traditional storage structure using paddy or wheat straw, woven as rope. Hagevu is an underground structure that is used to store grains. It is a simple pit lined with straw ropes to prevent damage from moisture.\nIt is important to note that these indigenous storage structures are not suitable for storing grains for very long periods. Regular mud plastering is required for a variety of indoor and outdoor storage containers and structures for increasing their life span and ensuring safe storage of grains.\nImproved grain storage structures\nWith several problems associated with traditional modes of grain storage some modifications have been done to offer improved grain storage structures to the farmers. For small-scale storage of grains the PAU bin, Pusa bin and Hapur tekka may be used. The PAU bin designed by Punjab Agricultural University is a galvanized metal iron structure and the capacity ranges from 1.5 to 15 quintals. Pusa bin is a storage structure is made of mud or bricks with a polythene film embedded within the walls. While the Hapur tekka is a cylindrical rubberized cloth structure supported by bamboo poles on a metal tube base, and has a small hole in the bottom through which grain can be removed.\nLarge scale grain storage is done in CAP and silos. CAP Storage (Cover and Plinth) involves the construction of brick pillars to a height of 14\" from the ground, with grooves into which wooden crates are fixed for the stacking of bags of food grains. The stacks are covered with 250 micron LDPE sheets from the top and all four sides. Food grains such as wheel, maize, gram, paddy, and sorghum are generally stored in CAP (cover and plinth) storage for 6-12 month periods. It is the most economical storage structure and is being widely used by the FCI for bagged grains. The structure can be fabricated in less than 3 weeks. It is an economical way of storage on a large scale.\nThe silos are either metal or concrete. Metal silos are cheaper than the concrete ones. In silos the grains in bulk are unloaded on the conveyor belts and, through mechanical operations, are carried to the storage structure. The storage capacity of each of these silos is around 25,000 tonnes.\nStored grain pest and their management\nThe grain produced by farmers is destroyed by pest, rats and moisture. It is important to prevent the storage losses through scientific methods and better management. Mainly khapra bettle, red flour beetle, lesser grain borer are the main pest which destroy stored grains. These insect along with by eating also affect the quality of food grains. Due to moisture & fungus also food grains because unsuitable for human consumption. Also the germination capacity of seed declines. Food grains generally and wheat specifically can be secured through following methods:-\n- Dry the grain properly under bright sun shine so that it should not contain moisture more the 10%.\n- Clean the grains before storage.\n- Use new gunny bags to fill the grains. Used bags should be disinfested by dipping them in 1% malathion solution before reusing them.\n- Bullock carts, tractor, truck or other vehicles used for the transports of grains should be cleaned before carrying the grain.\n- Properly clean the storage house, remove the cracks & fill the rat burrows with cement.\n- White wash the storage house before storing grains & spray Malathion @ of 50 E.C. 3Lt. per 100 Sq. meters.\n- Place the heap of bags 50 cm away from wall and in between the heaps give some gaps. Also there should be a gap between the roof and the bags.\n- Grain remains pest free by using Neem seed powder.\n- To kill rats zinc phosphide and warfarin should be used.\nControl after attack of pest:-\nEven after using above methods the attack of pest is observed than use fumigants.\nIt is available in markets in form of tablets as Celphos tablets. One 3.0g tablet releases 1.0g gas in moisture. This gas is lighter than air so it remains at the surface of food grains. It should be dispersed from bottom to top. 1 or 2 tablets are sufficient for 1 ton wheat grain. After applying the tablets the air tight storage house should be close for at least seven days.\nEthylene Dibromide (E.D.B.) Ampule:-\nThis fumigant is appropriate for small storage. It is available in market in 3, 6, 15 and 30 ml ampule. 3ml chemical is used for 1 Qtl of grain. The gas released by this chemical is six times heavier than air. So it is used at upper surface to get dispersed towards the bottom. The ampule is placed in grain and is broken by pressing. After this the storage house is closed. If grain is stored in heap of bags the E.D.B. ampule is used at the rate of 10 Millilitre per cubic meter and is covered by the help of polythene covers, the grain can be effectively saved from the damage of insect pests.\nBy scientific ways of storage significant amount of wheat grain can be saved. To become self-dependent in food grain and improve the economic condition of country, it is necessary to check the losses/deterioration of food grain from harvesting to consumption.\nRaj Pal Meena, Satish Kumar and C.N. Mishra\nDirectorate of Wheat Research Karnal – 132 001"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:f9ec63e8-e91a-4122-876a-37f89c76552f>","<urn:uuid:f5b30751-f3c7-4698-b721-52f125f03240>"],"error":null}
{"question":"What are the key differences between developing ML solutions in Jupyter Notebooks versus AWS distributed services, and how do calculation requirements differ between databases and applications?","answer":"In Jupyter Notebooks, developers can work with all ML processes in a single memory-space, including loading data, exploring, preprocessing, and training. However, AWS distributed services require coordination of multiple heavyweight services with distributed state management, primarily in S3 buckets. Regarding calculations, databases excel at data selection and aggregation tasks while requiring minimal resources, whereas applications are better suited for complex calculations, offering benefits like advanced data analysis capabilities, third-party library integration, better error handling, and superior debugging tools. Applications also provide better scalability through horizontal scaling, while databases are limited to scaling up.","context":["This is for newcomers to ML who want to start easy, and then step up\nEasy to large\nThis post is for newcomers to ML who have learned to work with Jupyter Notebooks and now want to know: How do I move up to a production-ready system using AWS services?\nYou’ve started lightweight and local, but and now you need to get to the distributed systems that can handle the heavy loads like:\n- Glue for transformation and metadata catalog\n- Athena for direct queries into S3 buckets\n- QuickSight for visualization of analytics\n- Sagemaker Instances for parallelized training\n- Sagemaker Endpoints for versioned, autoscaled inference servers\nStart from Notebooks\nJupyter Notebooks are a wonder for Machine Learning, an online IDE that maintains the state of local variables in memory so you can work with the kind of long-running process that is ML. Sagemaker Notebooks (Console; Docs) give you a managed environment to quickly spin these up. You can do everything there, including loading data, exploring, preprocessing, visualizing, training, evaluating, and deploying — all in a single memory-space for convenience and speed.\nDistributed AWS services\nBeyond the Notebook, it gets harder: You have to coordinate a series of heavyweight distributed services, integrating each part, making sure they authenticate to each other.\nIt’s hard to keep a clear understanding of what is happening across the ML process. First, because the state of the process is distributed (mostly in S3 buckets), and second, because of the long waits while AWS services are triggered and large amounts of data are processed.\nIn this post, I’ll explain how to make that move, gradually.\nMaking the move\nA single interactive environment is no way to run production systems. At some point, you need to move from the Notebook to that distributed system.\nI’d like to give an example of how to break each piece out as you move up the continuum towards a production system. We’ll illustrate with this open-source Notebook. In this article, you can see how to step through it in a Sagemaker Notebook instance — or even more easily, in the new Sagemaker Studio (Console; Docs). For each type of functionality, I pull out a snippet or two of code from the Notebook, then compare it to the distributed system that provides the same functionality inside AWS. In some cases, that Notebook already uses AWS, by invoking Sagemaker for training and deployment; for such cases, I explain how you would step up from simple local execution to the Sagemaker APIs, and beyond to a system orchestrated outside the Notebook.\nGetting the data into your system is the first step.\nIn a Notebook\nIn a Notebook, you just load a CSV or other file straight into your disk, for processing in memory.\n#From the example Notebook with zipfile.ZipFile('bank-additional.zip', 'r') as zip_ref: zip_ref.extractall('.')\nWith AWS Services\nWith large-scale projects, local resources in your small Notebook instance would be overwhelmed.\nInstead, take the CSV, or other data, and put it in an S3 bucket (Console; Docs) representing the origin of your data. For static data sources, put it straight into S3; for ongoing ingestion of data by streaming, use Kinesis (Console; Docs) to output into S3.\nNext is schema identification: You have a large blob of (what looks from the outside like) unstructured data, and you want to be able to query and manipulate it according to its internal structure (generally a tabular row-and-column format).\nIn a Notebook\nIn the Notebook, the schema is automatically identified from the CSV header row when it is loaded into a Dataframe.\n# From the example Notebook data = pd.read_csv(‘./bank-additional/bank-additional-full.csv’)\nWith AWS Services\nGlue Crawler inserts the schema into the Glue Data Catalog (Console; Docs). In upcoming steps (see below), Athena, Databrew, Quicksight, and other services will be able to treat these S3 objects as structured data.\nExploratory Data Analysis\nNext, you discover patterns in the data and transform it to make useful ML input.\nIn a Notebook\nEven in production scenarios, you can accomplish a lot by using a manageably-sized randomly sampled subset of data and exploring it in a Notebook.\nWith Pandas, for example, you can understand the frequency distributions.\n# From the example Notebook for column in data.select_dtypes(include=[‘object’]).columns: display(...)\nVisualization makes good use of the human brain’s visual processing centers to highlight patterns. You can visualize the data in a Notebook, creating histograms, heat maps, scatter matrices, etc. using Seaborn and other libraries.\n# From the example Notebook for column in data.select_dtypes(exclude=['object']).columns: ... hist = data[[column, 'y']].hist(by='y', bins=30) plt.show()\n# From the example Notebook\npd.plotting.scatter_matrix(data, figsize=(12, 12))\nWith AWS services\nFor visualization of data, use Quicksight (Console; Docs), directly on top of your Athena queries. These visualizations have the advantage of being easily shareable with other people, for example, business analysts, who can contribute insights.\nQuickSight created this histogram automatically when pointed at the age column\nIn a Notebook\nIn your Notebook, you use the insights from EDA to transform data into a form usable as input for your training algorithm, using APIs that work with Pandas Dataframes:\n- normalization for numerical data\n- one-hot encoding for categorical data\n# From the example Notebook # Convert categorical variables to sets of indicators # (One-hot) model_data = pd.get_dummies(data)\n- arithmetic functions like multiplication and modulus\n- dropping columns\n# From the example Notebook model_data = model_data.drop([‘duration’, ...], axis=1)\n- dropping outliers\n- string manipulation\n- and more\nYou then convert the data into a format that allows faster ML, such as Parquet or libsvm.\nWith AWS Services\nScaling up, AWS gives you several different tools for transformation:\nDataBrew (Console; Docs) is a graphical transformation builder — one of the few in my experience that works well. It reads your S3 data, as defined in the Glue Data Catalog. You choose from a selection of transformations like one-hot, normalization, and others that I listed above in the context of Notebooks.\nDataBrew generates transformations that run in the Glue framework. As a last step in the transformation, the data can be converted into the desired ML format like Parquet, Avro, CSV, or JSON.\nAlternatively, Glue Studio (Console; Docs) gives you a graphical transformation tool —though without DataBrew’s ML-oriented transformations such as one-hot — and you can write your own Python or Scala transformation code to be run in Apache Spark.\nYou may be running algorithms like XGBoost directly in the memory of your Notebook. This is good for interactive rapid development.\nSee, for example, this non-Sagemaker local invocation of XGBoost.\n# From a Notebook that does not use Sagemaker from xgboost import XGBClassifier # Non-Sagemaker package ... model = XGBClassifier(… ) ... model.fit(...)\nAs it happens, our sample Notebook is already set up to invoke the Sagemaker SDK (Docs), and if you are not doing that, it is your easy next step. The API here is the same as for other open-source libraries that run the same algorithm — you just need to import a different library.\n# Using Sagemaker (from the example Notebook) from sagemaker.estimator import Estimator # Different package ... xgb = Estimator... ... xgb.fit(...) # Same API\nThe Sagemaker SDK allows both local and remote training: You can train in local mode for fast iteration during development (you may have been doing before), but then, as you scale up, you use the SDK to trigger training on specialized Sagemaker instances (Console; Docs), including powerful features like distributed training on multiple instances.\nWith the same SDK, you can also call the algorithms with hyperparameter tuning (Docs): The same algorithm is run multiple times, each with a different set of parameters, looking for the parameters that give the same results.\nDeployment and Evaluation\nIf you are training locally, without Sagemaker, then you don’t need to deploy at all: You run inference right inside your memory space. For example, you might evaluate performance by running your inferencing (\npredict()) on a hold-out set.\n# From a Notebook that does not use Sagemaker y_pred = model.predict(X_test) # XGBClassifier model created above ... pd.crosstab(…) # Evaluate confusion matrix using Pandas\nIn Sagemaker, you’ll want to deploy to an Endpoint, a set of instances that responds to inference requests (Console), as is done in our sample Notebook.\n# Using Sagemaker (from the example Notebook) xgb_predictor = xgb.deploy(…) # Using Estimator created above\nFor scaling, you can add more inferencing instances, or configure them to autoscale, and you can call on more computing-power-as-a-service with Elastic Inference.\nWhen you redeploy a new version of the model, you compare its performance on a new endpoint, using Sagemaker’s support for A/B testing.\nInvoke inference in the endpoint with\npredict() , just as in the non-Sagemaker code. You can evaluate results by inferencing on a hold-out set, and then calculating metrics with Pandas and other in-memory APIs.\n# Using Sagemaker (from the example Notebook) xgb_predictor.predict(…) pd.crosstab(…) # Evaluate confusion matrix using Pandas\nBut that’s in a Notebook!\nOur goal was to get off the Notebook and into the AWS scalable systems, so you might be wondering why the last few stages were still in a Notebook.\nOn the one hand, these key steps did in fact invoke Sagemaker distributed APIs, rather than running in-process, so are fully scalable. In a basic workflow, where you occasionally retrain and redeploy the model by hand, they might even be workable for a manual, non-automated system.\nOn the other hand, in a typical ML production process, new versions of the model must be retrained and redeployed continually, just as you build and test your software with a Continuous Integration/Continuous Deployment pipeline. For this, you define your directed acyclic graph of ML processing steps in JSON, including, for example, data-cleaning, pre-processing, training, deployment, and evaluation APIs. (Here is a useful example for setting up a Pipeline.) You can reuse your existing code as the code in the Notebooks is just Python and can run anywhere, even in a pipeline.\nA Notebook is a powerful tool for development, but as you move into production, you will want more powerful infrastructure. This article shows you how to carve pieces out, one by one, even allowing you to continue working with the Notebook while also letting the main system run autonomously.\nI hope this post has helped you step up to large production systems.\n— — —\nP.S. This post is based on my AWS AI/ML Black Belt experience, an advanced certification that goes beyond the test-based AWS Certified ML Specialty. The Black Belt includes an advanced course and a final Capstone project which is the basis for your certification. It’s a great way to build and then show your skills in the areas mentioned above. See a more detailed post about it here.","Often, we find it difficult to decide whether a calculation should be performed in the database (RDBMS) or application code to get good performance along with convenience at the same time.\nIn this article, we'll explore the advantages and disadvantages of performing calculations in the database and application code.\nWe'll consider a few factors that can influence this decision, and we'll discuss which layer (database or application) is better suited to handle them.\n2. Calculation in the Database\n2.1. Data Selection and Aggregation\nRelational databases are highly optimized for the handling, selection, and aggregation of data. We can easily group, order, filter, and aggregate data using SQL.\nFor example, we can easily select and deselect datasets from multiple tables using LEFT and RIGHT JOIN.\nSimilarly, aggregate functions like MIN, MAX, SUM, and AVG are quite handy and faster than a Java implementation.\nAlso, we can fine-tune the performance of the disk IO by leveraging indexes while aggregating data.\n2.2. Volume of Data\nAll popular RDBMS provide unmatched performance in handling a large volume of data from tables for performing a calculation.\nHowever, we'll require a lot of resources like memory and CPU processing to process a similar volume of data in the application as compared to a database.\nAlso, to save bandwidth, it's advised to perform data-centric calculations in the database, thereby avoiding the transfer of large volumes of data over the network.\n3. Calculation in the Application\nUnlike the database, higher-level languages like Java are well equipped in dealing with complex calculations.\nFor example, we can leverage asynchronous programming, parallel execution, and multithreading in Java to solve a complex problem.\nSimilarly, the database provides minimal support for logging and debugging. However, today's higher-level languages have excellent support for such critical features, which are often handy in implementing a complex calculation.\nFor instance, we can easily add logging in a Java application by using SLF4J and use popular IDEs like Eclipse and IntelliJ IDEA for debugging. Therefore, performing a calculation in the application is a convenient option for a developer as compared to the database.\nLikewise, another argument is that we can easily unit test our calculations in the application code, which is fairly complex to perform in the database.\nUnit testing proves quite handy in keeping a check on the changes in the implementations. So, when performing a calculation in the Java application, we can use JUnit to add unit tests.\n3.2. Advanced Data Analysis and Transformation\nThe database provides limited support for advanced data analysis and transformation. However, it's simple to perform complex computations using the application code.\nFor instance, a variety of libraries like Deeplearning4J, Weka, and TensorFlow are available for advanced statistics and machine learning support.\nAnother common use-case is that we can easily objectify the data using ORM technologies like Hibernate, use APIs like Java Streams to process it, and produce results in various formats through XML or JSON parsing libraries.\nAchieving database scalability can be a daunting task as RDBMS can only scale up. However, the application code offers a more scalable solution.\nWe can easily scale out the app-servers and handle a large number of requests using a load balancer.\n4. Database vs. Application\nNow that we've seen the advantages of performing a calculation based on certain factors at each of the layers, let's summarize their differences:\n- The database is a preferred choice for data selection, aggregation, and handling large volumes\n- However, performing a calculation in the application code looks a better candidate when considering factors like complexity, advanced-data transformation, third-party integrations, and scalability\n- Also, higher-level languages provide extra benefits like logging, debugging, error handling, and unit testing capabilities\nIt's always a good idea to mix and leverage the best of both layers to solve a complex problem.\nIn other words, use the database for selection and aggregation of data, then transmit useful lean data to the application and perform complex operations over it using an efficient higher-level language.\nIn this article, we explored the pros and cons of performing calculations in the application and database.\nFirst, we discussed the advantages of performing calculations in both the database and application layers. Then, we summarized our conclusions about performing a calculation based on all the factors we discussed.\nCourse – LSD (cat=Persistence) res – Persistence (eBook) (cat=Persistence)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:307bb214-197c-4630-ac03-82909f8d4b73>","<urn:uuid:1aaf82c8-f3ee-48ea-bfab-6c4731977b1e>"],"error":null}
{"question":"I would like to know what historical sources are used to study Lincoln compared to modern presidents like Trump - are they different types? Thank you for explain.","answer":"The historical sources used to study Lincoln and modern presidents like Trump are indeed different. For Lincoln, historians rely heavily on photographs (as shown in books like 'Lincoln Through the Lens'), his own writings, personal relationships (like his friendship with Frederick Douglass), and contemporary newspaper accounts. For modern presidents like Trump, the evaluation is more direct and immediate - for example, in the 2021 C-SPAN survey, historians rated presidents on 10 specific leadership qualities including crisis leadership, economic management, and moral authority. Trump was ranked #41 in this assessment, while Lincoln ranked #1 across these same categories.","context":["Abraham Lincoln was a complex man living in difficult times. He’s been portrayed as one of our greatest presidents, a compassionate leader, a loving father, and a gifted writer. But he has also been described as slow to respond and melancholy. To historians, his life, presidency, and legacy have been a source of endless fascination.\nIn this month’s column, we feature a selection of books on Lincoln that employ a variety of perspectives through their textual structures: accounts that focus on photographs as evidence (Lincoln Through the Lens); explore personal relationships (Abraham Lincoln and Frederick Douglass); imagine a memorial news tribute (Lincoln Shot); offer a scrapbook approach (The Lincolns); and collect and comment on his writings (Lincoln In His Own Words). We provide suggestions for using these books with secondary students to meet standards in social studies and English language arts, while promoting inquiry and integration across the curriculum. Above all, by incorporating a range of titles on Lincoln, we demonstrate to students that there is no one way to approach or tell a story.\nInquiry and Integration\nTopic/Essential Question: How do various authors structure their approach to Lincoln’s life and legacy?\nGrade Span: 6-8\nDisciplinary Core Idea: Historical accounts can vary according to the questions asked and the evidence provided.\nThe College, Career, and Civic Life (C3) Framework for Social Studies State Standards:\n- D2.His.3-6-8. Use questions generated about individuals and groups to analyze why they, and the developments they shaped, are seen as historically significant.\n- D2.His.4.6-8. Analyze multiple factors that influenced the perspectives of people during different historical eras.\n- D2.His.9. 6-8. Classify the kinds of historical sources used in a secondary interpretation.\nCommon Core State Standards for Literacy in History/Social Studies\n- CCSS.RH.6-8.1 Cite specific textual evidence to support analysis of primary and secondary sources.\n- CCSS.RH.6-8.5 Describe how a text presents information (e.g., sequentially, comparatively, causally).\n- CCSS.RH.6-8.6 Identify aspects of a text that reveal an author’s point of view or purpose (e.g., loaded language, inclusion or avoidance of particular facts).\n- CCSS.RH.6-8.7 Integrate visual information (e.g., in charts, graphs, photographs, videos, or maps) with other information in print and digital texts.\nDenenberg, B. (2008). Lincoln Shot: A President’s Life Remembered. Ill. by C. Bing. New York, NY: Square Fish.\nFreedman, R. (2012). Abraham Lincoln and Frederick Douglass: The Story Behind an American Friendship. New York, NY: Clarion.\nFleming, C. (2008). The Lincolns: A Scrapbook Look at Abraham and Mary. New York, NY: Schwartz & Wade/Random House.\nMeltzer, M. (2009). Lincoln In His Own Words. Ill. by S. Alcorn. Boston, MA: Houghton.\nSandler, M. W. (2008). Lincoln Through the Lens: How Photography Revealed and Shaped an Extraordinary Life. New York, NY: Walker.\nTeaching Ideas: In the activities below, students are asked to think about what information is provided by the author and how it is presented.\n- Examining Photographs of Lincoln. In Lincoln Through the Lens, author Martin Sandler employs photography to help readers build an understanding of Lincoln’s life and times. The book consists of double-page spreads arranged chronologically, beginning with “Humble Beginnings” and ending with “A Broader Legacy.” Each spread contains the following features: title, quote, written text, photograph(s), and captions.\nAfter reading the book, focus on selected double-page spreads. Discuss how each of the elements above work together to create meaning. What information can only be found in the written text? The photograph(s)? The captions? What information is provided in both the text and the photographs? Emphasize that it’s important to integrate information from text, photographs, and captions.\n- Using some of the other titles listed above, as well as Internet sources, challenge students to create original spreads about Lincoln. Suggest that they follow the same format as Lincoln Through the Lens.\n- Examining Lincoln’s Personal Relationship with Frederick Douglass. Abraham Lincoln and Frederick Douglass had many things in common, and a genuine friendship. Both were born into poverty and both managed to educate themselves, even studying some of the same books.\nYet Lincoln and Douglass also had significantly different life experiences and worldviews. Douglass was born into slavery, and was determined to destroy it. Lincoln, while against slavery, was determined to save the Union.\nAfter reading Abraham Lincoln and Frederick Douglass: The Story Behind an American Friendship, consider first some of their differences. Fill in the chart below to show how each man’s experiences influenced his perspective.\nWhat Were Lincoln and Douglass’s Views on the Following Topics?\nTopic Lincoln Douglass\n|Why the Civil War was being foughtx||x||x|\n|How the War could be wonx||x||x|\n|Treatment of black soldiers\n- Despite these differences, Lincoln and Douglass had many similar experiences and views. What were these similarities? Which of these connections helped them form a friendship? Reread page 91 of the book for a summary of what the men had in common. Find evidence throughout the text that the author believes that the ideals and experiences these men shared were significant.\n- Examining a Lincoln Scrapbook. In the introduction to The Lincolns: A Scrapbook Look at Abraham and Mary, author Candace Fleming discusses the book’s thematic organization. The book includes a great deal of visual information—some accurate and some romanticized, and provides a mix of major events and small personal details. It is, in effect, a collection of what the author describes as a “mix of the mundane with the horrific.”\nHave students read the introduction and make a list of the book’s organizational features. Divide the class into small groups and have each use this list to describe how one of the chapters in the book presents information. Encourage the students to use specific examples to explain to the chapter’s content and arrangement.\n- Examining a Memorial Edition of a Newspaper. How can authors recount an “exemplary life?” How can they depict how a person changed over time? How can they highlight what is significant about a life? That’s the challenge presented in Lincoln Shot: A President’s Life Remembered. This book is conceived as a “Special Memorial Edition” of The National News and dated April 14,1866, exactly one year after Lincoln was shot.\nBefore reading Lincoln Shot, have students brainstorm a list of questions that a memorial edition of a newspaper would answer. What should people know about Lincoln? What is important to remember? The questions can address such topics as his early life, his family life, his political career, his accomplishments, and his assassination. It can also provide information about people who influenced Lincoln, and his legacy.\nAfter reading Lincoln Shot, ask students to discuss whether the author and illustrator answered their questions. Have students write their own memorial edition of The National News. What would they include? What do they consider historically significant?\n- Examining Lincoln’s Words. In Lincoln In His Own Words, author Milton Meltzer helps students understand Lincoln’s ideas by providing historical context. He offers information about the era in which the man wrote, including insight into some of the major issues of the day.\nUse the table below to explore the role of background information in understanding Lincoln’s writing. As students read selected chapters of the this book, ask them to note (1) important background information, (2) the gist of Lincoln’s comments, and (3) personal perspectives on his words. A sample beginning is provided.\nChapter 1: A Passion Within Me\n|Background Information That Helps You Understand Lincoln’s Words||Gist of What Lincoln Wrote||Thoughts About What Lincoln Wrote|\n|Lincoln only went to school for less than one year. In spite of this, he read many good books—history, fables, and poetry. He was concerned about communicating with clarity and simplicity.||Lincoln wrote that as a child he was irritated by talk that was too complex for him to understand.||I realize that this emphasis on simplicity and clarity of language is noticeable in his speeches and other writings throughout his political career.|\nExamine the Sources. Have students examine each author’s list of resources. What types of materials were used? How do these sources reflect the questions raised by each author? Why? Are certain materials better for answering specific questions? Why?\nIn this column we’ve highlighted five distinct accounts of Lincoln’s life. Their approaches result largely from the questions considered by each author: What do the photographs of Lincoln tell us about his life and times? How did Lincoln relate to people around him? How can Lincoln’s relationship with his wife, Mary Todd Lincoln, be characterized? How should Lincoln be remembered? What can we learn by examining his words? Depending on the questions authors hope to answer when exploring a life story, they may consult different sources, and, possibly, reach different conclusions. Their work may also spark other questions—one possible reason why there are approximately 15,000 books about Abraham Lincoln.\nThis article was featured in School Library Journal's Curriculum Connections enewsletter. Subscribe today to have more articles like this delivered every month for free.","- Nearly 100 historians and biographers rated past commanders in chief on 10 leadership qualities.\n- Abraham Lincoln was voted the best US president.\n- Donald Trump didn't make the top 25 — he's No. 41.\nHistorians agree: Abraham Lincoln was the best US president.\nFor C-SPAN's most recent Presidential Historians Survey, conducted in 2021, nearly 100 historians and biographers rated the former US presidents. The survey is released after a sitting president's term, so C-SPAN will likely include President Joe Biden in its next round of the ranking, after he leaves office.\nThe 2021 C-SPAN survey, which was released after Donald Trump left the White House, measured 10 qualities of presidential leadership: public persuasion, crisis leadership, economic management, moral authority, international relations, administrative skills, relations with Congress, vision, pursued equal justice for all, and performance within the context of his times.\nScores in each category were then averaged, and the 10 categories were given equal weighting in determining the presidents' total scores.\nGeorge Washington came in at No. 2, followed by Franklin D. Roosevelt at No. 3. George H. W. Bush ranked at No. 21, beating out his son George W. Bush who came in at No. 29. Other notable commanders in chief included John F. Kennedy at No. 8, Ronald Reagan at No. 9, and Barack Obama at No. 10.\nDonald Trump didn't make the top 25 — he ranked at No. 41. Jimmy Carter also missed the top 25, coming in at No. 26.\nHere are the top 25 presidents, according to historians surveyed by C-SPAN. The full list can be found here.\n25. Grover Cleveland (22nd and 24th president) ranked well for his public persuasion and administrative skills.\nGrover Cleveland was the only US president to serve two nonconsecutive terms in office.\n23. William Howard Taft (27th president) ranked well for his administrative skills and international relations.\nEight years after his presidency, William Howard Taft became Chief Justice of the US, and is the only person to have held positions in both offices.\n22. Andrew Jackson (seventh president) had strong public persuasion during his tenure, according to historians.\nJackson's supporters helped found the Democratic party after he lost the 1824 presidential election, despite getting the popular vote.\n21. George H. W. Bush (41st president) ranked high in how he handled international relations.\nBush conducted military operations in Panama and the Persian Gulf during his tenure.\n20. Ulysses S. Grant (18th president) ranked well for his public persuasion and international relations.\nUlysses S. Grant led the Union Armies during the American Civil War, ultimately defeating the Confederacy.\n19. Bill Clinton (42nd president) ranked high for economic management.\nFederal government spending fell during Clinton's presidency, and more jobs were created during his presidency than any other.\n18. James K. Polk (11th president) ranked highly for his crisis leadership and administrative skills.\nPolk led the US to victory in the two-year Mexican-American War.\n14. William McKinley Jr. (25th president) ranked highly for his relations with Congress.\nWith the help of Congress, McKinley passed the Dingley Act, the highest protective tariff in US history.\n13. Woodrow Wilson (28th president) ranked highly for his \"vision,\" according to historians.\nWilson led the country during World War I and was instrumental in crafting the League of Nations, a precursor the United Nations.\n12. James Monroe (fifth president) ranked highly for his handling of international relations.\nThe president lent his name to the Monroe Doctrine, which asserted Latin America was under the US's sphere of influence.\n11. Lyndon B. Johnson (36th president) ranked highly for his relations with Congress.\nJohnson passed legislation including Medicare and Medicaid programs, the Civil Rights Act of 1964, and the Voting Rights Act of 1965.\n10. Barack Obama (44th president) ranked highly for his pursuit of equal justice for all.\nObama won the Nobel Peace Prize in 2009.\n9. Ronald Reagan (40th president) ranked highly for his public persuasion.\nDomestically, Reagan is best known for cutting income taxes via two different federal laws: the Economic Recovery Tax Act of 1981 and the Tax Reform Act of 1986.\n8. John F. Kennedy (35th president) ranked highly for public persuasion.\nKennedy became the youngest man and first Catholic elected president.\n7. Thomas Jefferson (third president) ranked highly for his relations with Congress and his vision.\nJefferson vastly expanded the US borders through the Louisiana Purchase with France.\n6. Harry S. Truman (33rd president) ranked highly for his crisis leadership and his pursued equal justice for all.\nTruman took over as president when Franklin Delano Roosevelt died. He led the US through the final stages of World War II.\n4. Theodore Roosevelt (26th president) ranked highly for his public persuasion.\nMount Rushmore depicts Roosevelt's face, alongside George Washington, Thomas Jefferson, and Abraham Lincoln.\n3. Franklin D. Roosevelt (32nd president) ranked highly for his public persuasion and handling of international relations.\nFDR is the only president to have served more than two terms, dying in April 1945 at the start of his fourth term.\n1. Abraham Lincoln (16th president) ranks best for his crisis leadership, administrative skills, vision, and pursued equal justice for all.\nHistory.com calls Lincoln's Gettysburg Address \"arguably the most-quoted, most-memorized piece of oratory in American history.\"\nC-SPAN's full list can be found here."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:603cecb3-1331-48de-90c1-86cfa66f63c8>","<urn:uuid:128241ad-59c5-448e-b47d-71b15c26c849>"],"error":null}
{"question":"What are the key color differences between a ruffled holiday wreath and a chili pepper wreath?","answer":"The ruffled holiday wreath features soft muted browns and golds, with ivory fabric as the base and gold paint accents, while the chili pepper wreath showcases bright red dried arbol chile peppers that maintain their vivid color even when dried, creating a fiery sunburst appearance.","context":["We are adding the craft to your Craft Projects.\nThe project was added to your Craft Projects.\nThe great thing about this Ruffled Christmas Wreath is that you can have it on display all winter long. With soft muted browns and golds, this wreath is a great way to welcome guests to your home throughout the season. Great for vintage-inspired decor, this wreath makes a great project for crafters of all skill levels. Looking to add a pop of color? Use a bold ribbon or bow to embellish the wreath or add a simple seasonal blessing. No matter how you decorate or embellish this wreath, you're sure to love how it warms up your home this holiday season.\nPrimary Technique: General Crafts\n- 12” Styrofoam wreath\n- 8 inches of 60” wide, ruffled knit, ivory fabric -(approx. 13 yards total )length\n- 4 ft. pearl edge sheer ribbon 2” wide by Offray\n- Gold holiday stems\n- Airbrush (Testors|AZTEK airbrush system)\n- Fine line nozzle(Testors|AZTEK fine line nozzle)\n- Testors 1144TT Metallic Gold Enamel Paint (thin with Testors 8824 Enamel thinner or. if you prefer, you can use Pearl Gold Acrylic Airbrush Paint)\n- Hot Glue gun\n- Painters tape\n- Craft sticks\n- Wire cutters\n- 2 headpin wires\n- 2 pearls\n- 2 straight pins\n- Separate the ruffles by cutting the fabric into strips. Each strip should be about 1 ½” wide and consist of 1” of ruffle and ½” of a flat border. This is a knit fabric that doesn’t fray.\n- To paint the ruffles, secure both ends of the strip to a covered work surface with painters tape. Work in sections if you don’t have the room to paint an entire strip at one time. The edges of the ruffle should be taught but not so tight it curls.\n- Using the airbrush with a fine nozzle attached, create a line across the edge of the ruffle with gold paint. Use light trigger pressure, as too much pressure from the airbrush will lift the delicate fabric. Drape the ruffles over a hanger to dry.\n- Attach the end of a ruffle strip to the backside of the wreath using a glue gun. Craft sticks work well to hold the fabric in place while the glue sets. The top center of the wreath will be the point at which you start attaching ruffles.\n- Wrap the strip around the wreath, creating small tucks: one on the outer edge, one on the front flat surface, and one on the inside edge. Glue the tucks into place.\n- Bring the fabric to the back of the wreath and start your next row. The ruffle hem from the previous row should cover the flat border of the new row when it is glued in place.\n- Continue working down the wreath until you reach the bottom center. As you work the curved areas you will need to overlap the ruffles at the inner edges of the wreath more than the outer edges, because of the difference in diameter. When you’ve finished adding ruffles to one side of the wreath, move to the top center and attach the end of a ruffle strip to the backside of the wreath. Overlap the strips in the center, which will create a flat double border to be covered later. Repeat steps 5-7 above to finish covering the wreath with ruffles.\n- Add The Ribbon and Holiday Stems - Wrap a piece of pearl edged ribbon over the flat double border, bringing it to the back of the wreath. Fold the ribbon, creating a 6” loop. Slip the raw edge of the ribbon against the wreath and use straight pins to hold it in place. Dab hot glue on the pins before inserting them into the wreath.\n- Cut branches from the Holiday Stemsand glue them to the center ribbon on the right side. Add as many or as few as you like. Slip some of the stems under the ribbon and glue in place.\n- Create the bow by making four loops with one piece of ribbon. Secure the bow with wire, twisting the ends together. Leave a 2” tail of ribbon.\n- Using the point of a scissor blade poke a hole through the ribbon and in the top center of the wreath.\n- Fill the hole with hot glue and push the bow’s wire ends inside. When the glue has set, use the ribbon tail to cover the wire. Glue in place. Add more stems below the bow.\n- Cut a few gold ball stems and poke them into the wreath in clusters. Dab the stem wires with hot glue before poking them through the fabric and into the Styrofoam.\n- Finish the wreath by securing the loop you made at the back of the wreath, to the top center of the wreath. Put nail head wires through a pearl, dab the wire with glue and push it into the Styrofoam at the left edge of the ribbon. Repeat on the right side. Attaching the ribbon to the center will allow the wreath to hang properly.\nYou can find this project-and many more-in our new free eBook, 11 DIY Christmas Decorations and Ideas.\nYour Recently Viewed Projects\nImages from other crafters\nFree projects, giveaways, exclusive partner offers, and more straight to your inbox!\nReport Inappropriate Comment\nAre you sure you would like to report this comment? It will be flagged for our moderators to take action.\nThank you for taking the time to improve the content on our site.\nOur Newest Projects & Articles\n- Dr. Seuss Sneetches Costume Idea\n- Quicksilver Shawlette\n- Make DIY blank books from leftover notebook paper\n- Refashion shorts into a skirt\n- Repurposed Shutters Sign for Halloween\n- Halloween Pencil Box\n- 50+ Free Easy Crochet Patterns and Help for Beginners\n- Tea Time\n- Diamonds and Ice Mandala Coloring Page\n- Emperor's Festival Adult Coloring Page\n- 11 Crochet Shawl Patterns: Crochet Poncho Patterns, Free Easy Crochet Patterns and More\n- 12 Knitted Scarf Patterns: Fabulous Free Knitting Patterns for Beginners\n- 14 Free Crochet Patterns for Babies and Toddlers\n- 16 Free Crochet Hat Patterns, Scarves, and Gloves\n- 22 Free Crochet Patterns: Afghan Patterns, Crochet Hats, and More\n- 24 Quick and Easy Knitting Patterns\n- 7 Adult Coloring Pages\n- 8 Free Apron Sewing Patterns\n- Easy Lace Knitting Patterns\n- Quick & Easy Decoupage: 12 Fabulous Mod Podge Projects for Your Home","A wreath made of chili peppers adds spice to your home, whether it's to warm up your winter holiday décor, lend some sizzle to summer entertaining or brighten up your kitchen all year long. With the red, twisted, dried peppers resembling flames, the completed wreath takes on a sunburst shape that's quite appropriate to the fiery peppers.\nThings You'll Need\n- Dried arbol chile peppers, about 1 pound\n- 14-inch straw wreath form\n- 24-gauge florist wire\n- Raffia ribbon\nVideo of the Day\nSelect your straw wreath form, which you can find at your local craft store. The straw wreath is very solid, making it a good foundation for the chili peppers. The color of the straw is also a good backdrop for the red peppers. The one in this example is 14 inches in diameter.\nArbol chiles are the ideal peppers for making wreaths – unlike many other peppers they do not lose their color when they dry. You can find bags of dried arbol chiles in Latin and larger grocery stores. Two 8-ounce bags are enough to complete this wreath.\nCut a piece of florist wire so it is long enough to wrap around the circumference of the wreath form, with a few extra inches on both ends for tying. Select a 24-gauge wire, which is stiff enough to pierce the peppers, yet thin enough to thread easily.\nPoke the wire through each pepper stem. Choose peppers that are still whole and discard any that have cracked or are broken. You do not want the seeds of the peppers spilling out. Keep the little stem that's attached to the peppers.\nContinue to add peppers to the long piece of wire. The peppers should slide easily along the wire as you add the peppers. You will need about 150 peppers to complete the strand.\nWhen the strand is full, twist the ends of the wire together to lock the peppers in place. Position this strand of peppers on top of the wreath form – the peppers should dangle over the edge. Tie this strand to the wreath with pieces of raffia ribbon, which will disappear against the straw on the wreath. Secure it in at least eight locations along the strand.\nCut a piece of wire so that you can assemble a second loop of peppers about an inch smaller in diameter than the first one. Twist the ends of the wire to secure the peppers in place. This will be the second layer of peppers.\nPlace this second loop of peppers on top of the first layer of peppers. Attach the strand to the wreath form with raffia ribbon at several points. This second layer will press against the stems of the first layer and help prop up the peppers.\nCut a piece of wire that's the length of the inner circumference of the straw wreath form. Thread peppers on this wire to create a third, smaller strand of peppers. Twist the ends of the wire to form a loop.\nPlace the third layer of peppers on top of the first two, and secure it to the wreath form with raffia ribbon. You may have to adjust a few of the peppers that fall out of alignment, as all the peppers should be pointing outward.\nDecide which part of the wreath will be the top. On the other side, tie a wire loop for hanging.\nCreate a bow with raffia. Tie the raffia bow to the wreath."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:bfb237cf-ab1a-4f8c-b262-8f215e1516d9>","<urn:uuid:29655844-519c-4bce-a5e1-50797d8cc8ba>"],"error":null}
{"question":"How long do booklice live and reproduce, and what are the recommended methods to prevent their infestation in stored foods?","answer":"Booklice eggs take 2-4 weeks to hatch, and they can reach adulthood approximately 2 months later. Adult booklice can live for six months. To prevent infestation, it's recommended to purchase dried foods in small quantities that can be used quickly, store foods in tightly closed glass, metal, or heavy plastic containers, and keep food storage areas clean. Additionally, old unused products should be removed and discarded, and areas should be thoroughly cleaned with a vacuum cleaner, particularly in cracks and corners. However, washing areas with detergents, ammonia, or bleach will not prevent insect infestation.","context":["Temporal range: 299–0Ma Early Permian – Recent\nPsocoptera are an order of insects that are commonly known as booklice, barklice or barkflies. They first appeared in the Permian period, 295–248 million years ago. They are often regarded as the most primitive of the hemipteroids. Their name originates from the Greek word ψῶχος, psokos meaning gnawed or rubbed and πτερά, ptera meaning wings. There are more than 5,500 species in 41 families in three suborders. Many of these species have only been described in recent years.\nThey range in size from 1–10 millimeters (0.04–0.4 in) in length.\nThe species known as booklice received their common name because they are commonly found amongst old books—they feed upon the paste used in binding. The barklice are found harmlessly on trees, feeding on algae and lichen. No member of this order is currently considered endangered; in fact, in 2007, Atlantopsocus adustus, a species native to Madeira and the Canary Islands, was found to have colonized the mild Cornish coast of southwest England.\nIn the 2000s, morphological and molecular evidence has shown that the parasitic lice (Phthiraptera) evolved from within the psocopteran suborder Troctomorpha. In modern systematics, Psocoptera and Phthiraptera are therefore treated together in the order Psocodea.\nAnatomy and biology\nPsocids are small, scavenging insects with a relatively generalized body plan. They feed primarily on fungi, algae, lichen, and organic detritus. They have chewing mandibles, and the central lobe of the maxilla is modified into a slender rod. This rod is used to brace the insect while it scrapes up detritus with its mandibles. They also have a swollen forehead, large compound eyes, and three ocelli. Some species can spin silk from glands in their mouth.\nThe forewings are up to 1.5 times as long as the hindwings, and all four wings have a relatively simple venation pattern, with few cross-veins. The legs are slender and adapted for walking, rather than gripping, as in the true lice. The abdomen has nine segments, and no cerci.\nThere is often considerable variation in the appearance of individuals within the same species. Many have no wings or ocelli, and may have a different shape to the thorax. Other, more subtle, variations are also known, such as changes to the development of the setae. The significance of such changes is uncertain, but their function appears to be different from similar variations in, for example, aphids. Like aphids, however, many psocids are parthenogenic, and the presence of males may even vary between different races of the same species.\nPsocids lay their eggs in minute crevices or on foliage, although a few species are known to be viviparous. The young are born as miniature, wingless versions of the adult. These nymphs typically molt six times before reaching full adulthood. The total lifespan of a psocid is rarely more than a few months.\nBooklice range from approximately 1mm to 2mm in length (1/25\" to 1/13\"). Some species are wingless and they are easily mistaken for bedbug nymphs and vice-versa. Booklouse eggs take 2 to 4 weeks to hatch and can reach adulthood approximately 2 months later. Adult booklice can live for six months. Besides damaging books, they also sometimes infest food storage areas, where they feed on dry, starchy materials. They are scavengers and do not bite humans.\nThe Order Psocoptera is divided into three suborders.\nTrogiomorpha is the smallest suborder of the Psocoptera sensu stricto (i.e. excluding Phthiraptera), with about 340 species in 7 families, ranging from the monospecific fossil family Archaeotropidae to the speciose Lepidopsocidae (over 200 species). Trogiomorpha comprises infraorder Atropetae (extant families Lepidopsocidae, Psoquillidae and Trogiidae, and fossil families Archaeotropidae and Empheriidae) and infraorder Psocathropetae (families Psyllipsocidae and Prionoglarididae).\nTroctomorpha have antennae with 15–17 segments and two-segmented tarsi.\nTroctomorpha comprises the Infraorder Amphientometae (families Amphientomidae, Compsocidae, Electrentomidae, Musapsocidae, Protroctopsocidae and Troctopsocidae) and Infraorder Nanopsocetae (families Liposcelididae, Pachytroctidae and Sphaeropsocidae). Troctomorpha are now known to also contain the order Phthiraptera (lice), and are therefore paraphyletic, as are Psocoptera as a whole.\nSome Troctomorpha, such as Liposcelis (which are similar to lice in morphology), are often found in birds' nests, and it is possible that a similar behavior in the ancestors of lice is at the origin of the parasitism seen today.\nPsocomorpha are notable for having antennae with 13 segments. They have two- or three-segmented tarsi, this condition being constant (e.g. Psocidae) or variable (e.g. Pseudocaeciliidae) within families. Their wing venation is variable, the most common type being that found in the genus Caecilius (rounded, free areola postica, thickened, free pterostigma, r+s two-branched, m three-branched). Additional veins are found in some families and genera (Dicropsocus and Goja in Epipsocidae, many Calopsocidae, etc.)\nPsocomorpha is the largest suborder of the Psocoptera sensu stricto (i.e. excluding Phthiraptera), with about 3,600 species in 24 families, ranging from the species-poor Bryopsocidae (2 spp.) to the speciose Psocidae (about 900 spp). Psocomorpha comprises Infraorder Epipsocetae (families Cladiopsocidae, Dolabellopsocidae, Epipsocidae, Neurostigmatidae and Ptiloneuridae), Infraorder Caeciliusetae (families Amphipsocidae, Asiopsocidae, Caeciliusidae, Dasydemellidae and Stenopsocidae), Infraorder Homilopsocidea (families Archipsocidae, Bryopsocidae, Calopsocidae, Ectopsocidae, Elipsocidae, Lachesillidae, Mesopsocidae, Peripsocidae, Philotarsidae, Pseudocaeciliidae and Trichopsocidae) and Infraorder Psocetae (families Hemipsocidae, Myopsocidae, Psilopsocidae and Psocidae).\n- Christopher O'Toole (2002). Firefly Encyclopedia of Insects and Spiders. Toronto: Firefly Books. ISBN 1-55297-612-2.\n- John R. Meyer (2005-03-05). \"Psocoptera\". North Carolina State University.\n- Alfonso N. García Aldrete (2006). \"New genera of Psocoptera (Insecta), from Mexico, Belize and Ecuador (Psoquillidae, Ptiloneuridae, Lachesillidae)\". Zootaxa 1319: 1–14.\n- BBC News, \"New insect species arrives in UK\" 8 November 2007\n- Yoshizawa, K.; Johnson, K. P. (2006). \"Morphology of male genitalia in lice and their relatives and phylogenetic implications\". Systematic Entomology 31 (2): 350–361. doi:10.1111/j.1365-3113.2005.00323.x.\n- Johnson, K. P.; Yoshizawa, K.; Smith, V. S. (2004). \"Multiple origins of parasitism in lice\". Proceedings of the Royal Society of London 271 (1550): 1771–1776. doi:10.1098/rspb.2004.2798. PMC 1691793. PMID 15315891.\n- Bess, Emilie, Vince Smith, Charles Lienhard, and Kevin P. Johnson (2006) Psocodea. Parasitic Lice (=Phthiraptera), Book Lice, and Bark Lice. Version 8 October 2006 (under construction). http://tolweb.org/Psocodea/8235/2006.10.08 in The Tree of Life Web Project, http://tolweb.org/\n- Hoell, H.V., Doyen, J.T. & Purcell, A.H. (1998). Introduction to Insect Biology and Diversity, 2nd ed. Oxford University Press. pp. 404–406. ISBN 0-19-510033-6.\n- US Army Public Health Command fact sheet. http://phc.amedd.army.mil/PHC Resource Library/BookliceFSMar08WestFinal.pdf\n- C. Lienhard & C. N. Smithers (2002). \"Psocoptera (Insecta): World Catalogue and Bibliography\". Instrumenta Biodiversitatis (Muséum d'histoire naturelle, Geneva) 5.\n|Wikimedia Commons has media related to Psocoptera.|\n|Wikispecies has information related to: Psocoptera|\n- National Barkfly Recording Scheme\n- Psoco Net\n- Tree of Life: Psocodea\n- Archipsocus nomas, a webbing barklouse on the UF / IFAS Featured Creatures Web site\n- HD Video of Barklice","Insect pests of stored foods\nInsects infesting stored foods are one of the most common household insect problems. The many different kinds of insects that infest stored dried foods are often referred to as \"pantry pests.\"\nPantry pests contaminate more food than they consume, and most people find the contaminated products unfit for consumption. Pantry pests are often discovered when they leave infested foods to crawl or fly about the house. They often accumulate in pots, pans or dishes or on window sills. Fortunately, they do not bite or sting people or pets nor do they feed on or damage the house structure or contents.\nWhat do they eat?\nNearly all dried food products are susceptible to insect infestation, including cereal products (flour, cake mix, cornmeal, rice, spaghetti, crackers, and cookies); seeds such as dried beans and popcorn; nuts; chocolate; raisins and other dried fruits; spices; powdered milk; tea; and cured meats. Non-food items that may be infested include birdseed, dry pet food, ornamental seed and dried plant displays, ornamental corn, dried flowers, garden seeds, potpourri, and rodent baits.\nStored food insects are most likely to infest products that have been opened but are also capable of penetrating unopened paper, thin cardboard, and plastic, foil, or cellophane-wrapped packages. They may chew their way into packages or crawl in through folds and seams. Insects within an infested package begin multiplying and can spread to other stored foods not only in the same area but in other rooms in a home. All stages (egg, larva, pupa, and adult) may be present simultaneously in infested products.\nWhere do they come from?\nA stored food product can become infested anywhere during the process from production until it arrives in your home. However, stored food is most likely to become infested in stores or in homes. Most of the stored food insects also are pests of stored grain or other commodities and may be relatively abundant outdoors. Food products that are left undisturbed on the shelves for long periods are particularly susceptible to infestation. However, foods of any age can become infested.\nIdentification and biology of common stored product insects\nIndianmeal moths (Plodia interpunctella) are the most common moths infesting food in homes. These moths have a wingspan of 1/2 to 5/8 inch. When at rest, they fold their wings behind themselves, over their bodies. The base of the front wing is pale gray or tan and the outer two-thirds is reddish-brown with a coppery luster. The wing markings are distinctive, but may be less clear if the scales have been rubbed from the wings. Indianmeal moths may be found inside infested products or flying around homes.\nThe larvae are whitish worms with shades of yellow, pink, green, or brown and grow to 1/2 inch long.\nOnly the larvae feed in stored products, which can be any dry stored food or whole grain. Foods infested with these insects will have silk webbing present on the surface of the product. Larvae often leave the food when mature and may move long distances before stopping to spin a cocoon. It is common to find caterpillars and cocoons on ceilings and walls. Adult moths may be seen up to several weeks after the food source has been removed.\nMeal moths (Pyralis farinalis) have a wingspan of about 3/4 - 1 inch. Their forewings have a dark reddish brown band across the top and bottom of the wings with an olive or yellowish green band, outlined by wavy white lines in the center. Their abdomen is typically curved up at a 90° angle when at rest. The larvae have a black head and a whitish body with some orange at the end of the body. They often form feeding tubes made of silk and tiny pieces of food. Meal moths are found feeding on a wide variety of flour and grain products and seeds, especially when they are damp. These moths are not common in homes.\nSawtoothed grain beetles (Oryzaephilus surinamensis) are about 1/10 inch long, slender, flattened, and brownish-red to almost black in color. They are easily identified by the saw-like teeth on each side of the thorax. Larvae are cream-colored, slender, and about 1/8 inch long, although they are rarely noticed by residents. Sawtoothed grain beetles are found in many different food items, including dried fruit, cereals, nuts, dried meat, macaroni, and seeds.\nDrugstore beetles and cigarette beetles (Lasioderma serricorne and Stegobium panicum) are about 1/8 inch long, oval, and brown. The head is bent downward giving the insect a humped appearance. Both species fly and can be found around windows. Larvae are 1/8 inch long when mature, and yellowish-white with a light brown head (the larvae are not usually noticed by residents). They have a curved body covered with fine hair. Cigarette and drugstore beetles feed on a wide variety of dried plant products such as spices, macaroni and other grain based foods, dried flowers, tobacco products, and even paper products, including books.\nFlour beetles (Tribolium confusum and T. castaneum) are 3/16 inch long, reddish-brown, and elongate oval in shape. Larvae are cylindrical, whitish, or cream-colored and up to 1/4 inch long and have two small pointed spines on the tail end (the larvae are not usually noticed by residents). Two species of flour beetles may be found: red flour beetles are common in homes and the confused flour beetle is a frequent pest in flour mills. Flour beetles infest many types of dried food products, such as flour, bran, cereal products, dried fruits, nuts, and chocolate.\nWarehouse and cabinet beetles (Trogoderma spp.) are elongate oval and 1/8 to 3/16 inch long. They may be solid black or mottled with yellowish-brown markings. Larvae are long and narrow, yellowish to dark brown, hairy and generally grow to about 1/4 inch (although they may not be noticed by residents). Warehouse and cabinet beetles feed in a wide variety of food products, such as grain products, seeds, dried fruits, animal by-products skins, fur, hair, and pet food. They are also known to feed on dead insects and animal carcasses.\nGranary, rice, and maize weevils (Sitophilus spp.) are slender insects with a conspicuous snout projecting forward from the head. They are dark brown, sometimes with four orangish spots on the wing covers. They are less than 3/16 inch long. Larvae are white, legless, and looked wrinkled and are only found inside whole kernels or seeds. These weevils attack only whole grains or seeds, leaving small round exit holes in infested kernels. They rarely are found in nuts, dried fruits, macaroni, and caked or crusted milled products such as flour. (A different, larger species of weevil can be found in homes during the fall due to emergence from acorns or hickory nuts collected and stored inside).\nSpider beetles (family Ptinidae) are reddish brown, 3/16 inch beetles with long legs and a somewhat, spider-like appearance. The larvae are C-shaped and whitish; they remain in infested material and aren't normally seen. Spider beetles infest a variety of dried plant products.\nBean weevils (Acanthoscelides obtectus) are a type of seed beetle. They are a mottled light and dark brown, broadly oval, and about 1/8 inch long. They have short wing covers which exposes part of the abdomen. Unlike other weevils, bean weevils lack a conspicuous snout. The larvae are small, whitish, legless, and C-shaped. They feed inside dried beans and peas.\nPurchase dried foods in quantities small enough to be used up in a relatively short period of time. Use oldest products before newer ones, and opened packages before unopened ones.\nInspect packages or bulk products before buying. Packages should be sealed and unbroken. Also check the freshness packaging date. Look for evidence of insects, including holes in the packaging or wrapping.\nStore insect-free foods in tightly closed glass, metal, or heavy plastic containers especially if you do not use up the food very quickly (this is less important for food that is used up more quickly). You can also store susceptible foods in the refrigerator or freezer.\nKeep food storage areas clean. Do not allow crumbs or spilled food to accumulate. Remove and discard old, unused products and inspect the remainder. Thoroughly clean cracks and corners with a vacuum cleaner. Also check and clean areas where pet food and birdseed are stored as these are particularly common sources of infestations.\nWashing areas with detergents, ammonia, or bleach will not prevent insect infestation. There is no evidence that proves that placing bay leaves or sticks of spearmint gum in a cupboard will prevent or deter stored food insect pests.\nThere are several ways you may become aware of a stored product infestation. If you find small beetles in susceptible food products, that is a sure sign of a problem. It is also common to find stored product beetles on counters and in cupboards. In some cases, the beetles are attracted to light and may be found around windows.\nYou may find Indianmeal moths flying around kitchens and other rooms. As caterpillars move away from infested food to pupate, they can be found on walls and ceilings in rooms adjacent to infestations. When examining food packages, you may not only find caterpillars but silk webbing inside infested packages.\nBe careful, as not all small beetles or moths found indoors are necessarily a panty pest. If there is not a direct association with food, be sure the insects are identified correctly by an expert to determine whether they are a stored product food insect.\nWhen you know a stored product problem is present, be sure to examine all susceptible food as there could be more than one infested source. When inspecting them, look at the top surface of products with a flashlight or pour the package contents onto a cookie sheet.\nWhen you find food that is infested, just throw it away.\nUse a vacuum cleaner to thoroughly clean cabinets and shelves, especially in cracks and corners. This will pick up crawling insects and spilled or infested material that is present. Empty the vacuum cleaner or discard the vacuum cleaner bag after use to prevent reinfestation. Washing shelves with detergent, bleach, ammonia, or disinfectants will not have any effect on insect pests.\nAs a precaution against re-infestation, store susceptible foods in sealable glass, metal, or heavy plastic containers or in the freezer or refrigerator until you are convinced the infestation is gone. It is not unusual to see an occasional Indianmeal moth flying for as long as 3 weeks after the infested sources have been eliminated. However, if you continue to see Indianmeal moths after three weeks, that indicates there is an infested food source that has not been discovered yet.\nIf you have older food products and you are not sure if they are infested, you can place these products in the freezer at 0 degrees for at least 4 days or in shallow cookie sheets or pans in an oven at 130 degrees for at least 30 minutes. These temperatures will kill any eggs or insects that may be present. If insects are infesting ornaments or decorations made with plant products or seeds, place the items in a freezer for at least four days.\nInsecticide sprays are not recommended for controlling insects in stored food cupboards. Household insecticides have no effect on insects within food packages and any control of insects outside of them is temporary unless the source of the infestation is found and eliminated.\nAlso, the amount of extra work that is necessary when treating cupboards or other areas usually is prohibitive. It is necessary to first remove dishes, glasses, and food packages so they are not contaminated by pesticides. Time is then needed to allow the spray to dry before items can be returned."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2bc54c31-3775-4b71-9a2a-df417fccb457>","<urn:uuid:717e3582-0cea-438e-a2c7-f9a30abb717b>"],"error":null}
{"question":"What are the similarities between proper computer workstation setup and correct manual lifting techniques in terms of reducing physical strain?","answer":"Both computer workstation setup and manual lifting techniques focus on maintaining optimal body positioning to reduce strain. For computer workstations, this includes adjustable keyboards, screens that tilt, and proper chair height where feet can rest on the floor to maintain good circulation. Similarly, for manual lifting, the focus is on keeping the spine in its natural position, maintaining proper distance to objects, and avoiding twisting or bending. Both scenarios emphasize the importance of keeping work at an appropriate height - between mid-thigh and elbows for lifting, and at a height that prevents shoulder raising or back bending for computer work.","context":["Ergonomics mean adjusting the work to the human being. When adjusting the work to the human being, it is important to consider muscles and joints.\nHow do I use myself best?\nHuman beings are fantastic constructions and the way our minds and bodies work is unexcelled by any technological wonder.\nHuman beings are undoubtedly created to perform physical and intellectual work. By using our brains - to think about the way we tackle physical work - we give ourselves a good change of keeping healthy all our lives.\nHowever, this is what matters when talking about quality of life\n- \"a healthy mind in a healthy body\"\nWHAT DOES ERGONOMICS MEAN?\nErgonomics mean adjusting the work to the human being. When adjusting the work to the human being - the employees of the company - it is important to consider muscles and joints.\nWhere the muscles are concerned, a distinction is made between static and dynamic work.\n- With static work, the muscles are under constant strain and become tired quickly. Over longer periods, such strains often lead to disorders in both muscles and sinews.\n- With dynamic work, the muscles are alternately stretched and relaxed; the joints move the whole time. This type of work helps the circulation since the muscles receive a constant supply of fresh blood containing oxygen and nourishment\nWhen considering the joints, these should always be allowed to work in and around their “intermediate position” – their natural resting position. This helps to minimise the load on cartilage in the joints and helps to prevent osteoarthritis.\nThe shoulder joint works best when the upper part of the arm is kept vertically down. When the load is on the heavy side, the elbows work best at an angle of about 120°. For lighter loads, 90° is suitable.\nThe wrist performs best when held straight out as an extension of the forearm or bent backward with the thumbs pointing diagonally inwards/upwards.\nFinger joints work best when slightly bent.\nIt is best for the legs to alternate between standing, walking and sitting.\nThe back works best and is burdened least in standing or upright position; in this position the spine assumes its natural curvature.\nIMPORTANT WHEN ARRANGING WORKSTATIONS\nExamples of inappropriate movements when lifting and transporting loads manually:\n- Lifting too quickly or when leaning to one side\n- Starting to lift with a jerk or stopping suddenly while lifting\n- Twisting the body/neck, bending forwards or sideways. The strain increases with the extent of bending and twisting, and with the length of time the position is held\n- Lifting the arms, especially if the hands are raised above shoulder height\n- Lifting low loads; the whole body has to be lifted in addition to the load\n- Lifting from the side or with one hand; the pressure on the spine becomes about twice as much as when lifting with two hands in front of the body\nThere are different ways of making work less physically demanding, for example:\n- By ensuring that the equipment used is designed to suit the work and the user. If the lifting does not involve great power, it is often advantageous for the user to use the equipment in standing and sitting positions\n- By keeping the distance to the object being lifted as short as possible during the whole lifting operation (from the point of gripping to the point of releasing)\n- By making the working height adjustable so that a good working position is possible, i.e. the load must be kept close to the body between mid-thigh and elbows so that twisting or bending the spine is not necessary\nTransport of pallets and goods\nWhen choosing a pallet truck it is important\n- that the starting power concerning both pulling and pushing is as low as possible\n- that the handle has an ergonomically correct design\n- that only a little power is necessary for the pump function on a manual truck\nThe Panther pallet truck has an ergonomically designed handle.\nThe hoop skates on the forks are perfectly designed to ensure an easy fork insertion and withdrawal in/out of closed pallets.\nWorking at machines & assemble lines\nWhen working at machines and assembly lines, objects are taken from a pallet placed on the one side of the user, prepared and delivered to the opposite side or taken from assembly lines and put on pallets.\nWithout equipment this might imply lifting both from and to very low heights - meaning strain. A Thork Lift or a stacker could be a good solution. When choosing, it is from an ergonomic view important\n- that the user can move freely on the floor in order to avoid twisting of his back and neck\n- that the working height is correct all the time in order to avoid bending of the back - when working in too low levels - and to avoid lifting of shoulders and arms - when working in too high levels\n- that the distance to the object is as short as possible\nIf the work implies large and heavy objects, the best way to perform the work is in a standing position. Here a Thork Lift is the perfect solution.\nIf the work implies small and light objects and can be performed in a sitting position, a stacker is advantageousllets.\nTransporting and stacking\n- in restricted areas. Loading and unloading of vans.\nWithout equipment this work would mean large strain when lifting low and high in awkward positions. A stacker is here the perfect choice and from an ergonomic point of view it is important\n- that minimum power is necessary for the pump function on a manual truck, meaning a minimum strain of shoulders and arms\n- that the stacker is easy to manoeuvre, meaning a minimum strain of arms and back\n- that the handle has the correct thickness and angles according to ergonomic principles\n- that the stacker can be operated fully by the handle. Contrary to e.g. lifting/lowering by a foot pedal, meaning that the user has to stand on one leg\n- that the user has a good view over the load during the operation and at the same time has a correct working position\nHandling of drums\n- in order to fill liquids on other containers.\nIf this kind of work has to be performed manually, two persons are needed, meaning a large strain of their backs and arms.\nFrom an ergonomic point of view it is important\n- that lifting and handling of drums are not performed manually, because the strain of back and shoulders is too large\nInterthor offers a drum turner for all stackers, being able to lift, transport and turn the drum 360º and to grip from both horizontal and vertical position. It is available in both a manual and an electrical version with clamp fittings for both 200 l steel drums and for the most standard plastic drums - the drum turner will without doubt save many backs.\nFor handling horizontal drums, drum forks are offered. With drum forks different types of drums, reels and barrels can be handled.\nFilling and emptying boxes\n- of objects into/out of boxes, large plastic containers, crates etc.\nFrom an ergonomic point of view it is a claim that correct lifting methods can be used during working, no matter whether the work concerns heavy or light object. However, lifting the container, so that the top edge is in a good working height, is not enough. It is necessary that the angles allow the user to reach the objects without bending his back.\nIt is important that\n- the back can work in its \"intermediate position\"\n- the distance to the objects is as short as possible considering back and shoulders.\nThe choice could be a Logitilt or a Rotator. These products make it possible to tilt a container or box into a good working position of the back by changing the tilt degree when filling/emptying the box. In this way the user gets the least possible distance to the objects and the least possible strain of his back and shoulders.\nWith a Logitilt a load can be tilted up to 90º. The working height can be varied from 750 to 950 mm, which makes both standing and sitting work possible. The handle can be turned to the side, giving free access to the box opening.\nA Rotator can be set to turn up to 180º to each side.\nEU FOCUS ON ERGONOMICS\nSurvey of the working environment\nIn 1996 the European Foundation for the Improvement of Living and Working Conditions, Dublin, conducted a survey of the working environment. The survey involved interviews with a representative part of the working population consisting of about 1000 employees in each of the member countries of the EU, i.e. an approximate total of 15,800 persons were interviewed. 30% of the respondents named back pains as their most frequent cause for complaint. This is a problem that ought to be taken seriously.\nWork assessment according to EU Directive 89/391/EEC of 12 June 1989\nIn order to gain an overview of the working environment in the company, it is a good idea to conduct a workplace assessment. Within the EU this is a requirement, stipulated in EU Directive 89/391/EEC of 12 June 1989. A workplace assessment is conducted through co-operation between the employer, employees and safety organisation of the company and is a good starting point for laying down priorities and a plan of action for improving the working environment.\nIn particular, the purpose of a workplace assessment is to prevent or reduce physical stress at work, and to create a well-arranged and flexible workplace with a rational organisation of the work.\nThe legislation concerning working environment within the EU lays down that manual handling involving safety and health risks must be avoided. If this is not possible, effective measures must be adopted to eliminate the risks. Suitable equipment must be used when possible or appropriate, and always when the manual handling involves health risks.","What is Ergonomics?\nThe term “ergonomics” can simply be defined as the study of work. It is the science of fitting jobs to the people who work in them. Adapting the job to fit the worker can help reduce ergonomic stress and eliminate many potential ergonomic disorders (e.g., carpel tunnel syndrome, trigger finger, tendonitis). Ergonomics focuses on the work environment and items such as the design and function of workstations, controls, displays, safety devices, tools and lighting to fit the employee’s physical requirements, capabilities and limitations to ensure his/her health and well being. It may include restructuring or changing workplace conditions to reduce stressors that cause musculoskeletal disorders (MSDs).\nWhat are ergonomic hazards?\nErgonomic hazards refer to workplace conditions that pose the risk of injury to the musculoskeletal system of the worker. Examples of musculoskeletal injuries include tennis elbow (an inflammation of a tendon in the elbow) and carpal tunnel syndrome (a condition affecting the hand and wrist). Ergonomic hazards include repetitive and forceful movements, vibration, temperature extremes, and awkward postures that arise from improper work methods and improperly designed workstations, tools, and equipment.\nWho do I contact if I want an ergonomic evaluation performed on my workspace?\nContact the Environmental Health and Safety at 773.702.9999 to request an ergonomic evaluation or send an email to firstname.lastname@example.org. Please note that a request from your Physician is required for Environmental Health and Safety to conduct an ergonomic evaluation of an employee's workstation.\nI am renovating my area and purchasing new furniture, can someone review the furniture?\nA workstation outfitted with the proper furniture and equipment can lead to a more comfortable and safer work environment. Ergonomic injuries occur at workstations due to reaching, bending, awkward postures and applying pressure or force. If workstations are designed properly, most ergonomic hazards can be reduced if not eliminated.\nPrior to purchasing furniture or workstation equipment due to renovations of existing space or new construction consult Environmental Health and Safety to have the specifications reviewed and to provide guidance.\nWhat are the basic guidelines for setting up a computer workstation correctly?\nWorkstations that include video display terminals (VDTs) should be ergonomically designed for both computer and non-computer work. VDT workstations should be adjustable so users can easily change their working postures and equipped with the following:\n- Adjustable and detachable keyboards;\n- Display screens that tilt up and down;\n- Brightness and contrast controls;\n- Flexible copy-holders that reduce the distance between the screen and source material; and\n- Proper lighting and anti-glare filters should be installed to prevent glare from the VDT screen. VDTs should be placed in the workspace in such a way as to minimize or diminish glare.\nWhat are the basic criteria for an ergonomically designed chair?\nWhen an employee spends six to eight hours in the chair, the height of the chair and the work surface are critical. The human body dimension that provides a starting point for determining correct chair height is the “popliteal” height. This is the height from the floor to the crease behind the knee. The chair height is correct when the entire sole of the foot can rest on the floor or a footrest and the back of the knee is slightly higher than the seat of the chair. This allows the blood to circulate freely in the legs and feet.\nArmrests: Armrests should be large enough to support most of the lower arms but small enough so they do not interfere with chair positioning. Armrests should support your lower arms and allow your upper arms to remain close to the torso and made of soft material and have rounded edges.\nBackrests: Backrests should support the entire back including the lower region. The seat and backrest of the chair should support comfortable postures that permit frequent variations in the sitting position. The backrest angle should be adjustable but lock into place or have a tension adjustment.\nSeats: Seat pans should be height adjustable and have a user adjustment for tilt. Note: “users adjustment for tilt” is defined as any method of activation the movement of the seat pan/backrest. This can be either through the use of manual devices (e.g. levers, knobs, adjustments) or by movement of the body/body weight. Seat pans should be padded and have a rounded, “waterfall” edge, wide enough to accommodate the majority of hip sizes.\nBase: Chairs should have a strong, five-legged base and casters that are appropriate for the type of flooring at the workstation."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:13c1ff03-8036-4436-9f8f-e0d1fabda0c3>","<urn:uuid:2b799cdd-8569-4b19-92b7-19f7531c0915>"],"error":null}
{"question":"How did the Romans and Dio of Prusa each handle conflicts differently - the Romans with conquered territories versus Dio mediating between Prusa and Apameia?","answer":"While the Romans used military force and demanded tribute from conquered territories like Gaul and Spain, subjugating kings and destroying opposition, Dio took a diplomatic approach in mediating the business dispute between Prusa and Apameia. He advocated for reconciliation through negotiations and peaceful dialogue, appearing at town meetings to plead for concord and serving on official delegations to arrange terms of agreement between the two closely connected cities.","context":["1. ΕΝ ΑΘΗΝΑΙΣ ΠΕΡΙ ΦΥΓΗΣ - в Атина, за изгнанието си (13)\nὍτε φεύγειν συνέβη με φιλίας ἕνεκεν λεγομένης ἀνδρὸς οὐ πονηροῦ, τῶν δὲ τότε εὐδαιμόνων τε καὶ ἀρχόντων ἐγγύτατα ὄντος, διὰ ταῦτα δὲ καὶ ἀποθανόντος...\nIn the year A.D. 82, probably, Dio was banished by the Emperor Domitian, not only from Rome and Italy but also from his native Bithynia, on the charge of being in some way implicated in the conspiracy of one of the Emperor's relatives, Junius Rusticus, as some including Mommsen maintain, Flavius Sabinus as von Arnim with better reason believes. Each of these men was related to the Emperor, Flavius Sabinus being the husband of Julia, the daughter of Domitian's elder brother Titus, who had been Emperor before him; and each of them was executed on the charge of having conspired against him. If it is Flavius Sabinus to whom Dio refers, then since this man was executed in the year A.D. 82, we may infer that Dio's banishment began in this year, and it was intended to last his lifetime. However, with the accession of Nerva in A.D. 96 he was permitted to return.\nIn the Thirteenth Discourse Dio gives us an interesting glimpse into his thoughts and feelings at that time. Adopting the attitude of a Stoic, he resolved to endure his banishment manfully and found that it was quite endurable. Then he tells how at the urgent request of others he began to deliver moral addresses to groups of people gathered to hear him.\n2. En te patridi peri tes pros apameis homonoias - (Произнесена) в родината си, относно споразумението с апамейците (40)\nEnomizon men, o andres politai, nyn goun, ei kai me proteron axein ten hapasan hesychian...\nAs indicated by the title, the background of this Discourse is a quarrel between Prusa, the home of the speaker, and its near neighbour, Apameia. The precise nature of the quarrel remains in doubt, but it seems to have involved business relations, and possibly also property rights. The relations between the two cities were extremely intimate. Prusa used the port of Apameia, and Apameia looked to Prusa for its timber. There was constant intercourse of many kinds between the two, and citizens of the one not infrequently were citizens also of the other, sometimes even receiving a seat and vote in the Council of the second city. Dio's own connexion with Apameia was especially close. As we learn from Or. 41, not only had he himself been honoured with citizenship there, but also his father before him; his mother and her father too had been awarded citizenship in Apameia along with the grant of Roman citizenship; and, finally, it would appear that Dio's household had found a refuge in that city during his exile.\nWhatever the nature of the quarrel, it had lasted for some time prior to the date of our Discourse (A.D. 101), and it had been so bitter that Dio had feared to accept the invitation of Apameia to pay a visit joint upon his return from exile, lest by doing so he might offend the city of his birth, and for the same reason he had resisted a request to intervene in behalf of Apameia in its quarrel. He had, to be sure, urged upon his fellow citizens, as occasion offered, the desirability of reconciliation with Apameia, and negotiations to that end were actually in progress when Dio, responding with some reluctance to the summons of his fellow townsmen, appeared in town-meeting and pleaded afresh the cause of concord. It would appear that his words received a favourable hearing, for in the next Discourse in our collection, delivered at Apameia shortly afterwards, he speaks as a member of an official delegation to arrange terms of agreement.\nThis Discourse, as well as several to follow, is valuable both as shedding light upon doings in Bithynia, doings about which we get supplementary information from the correspondence of Pliny the Younger written during his term as proconsul of that province, and also as supplying biographical data regarding the speaker.\n3. Pros apameis peri homonoias - към апамейците относно споразумението (41)\nHoti men hymeis, o boule kai ton allon hoi parontes hoi metriotatoi, praos pros eme kai philikos echete...\nThis short address constitutes the sequel to Or. 40, which it must have followed closely in point of time. Dio is speaking before the Council of Apameia as a member of the official delegation from Prusa sent to conclude the reconciliation which forms the theme of both speeches. That the question was of widespread interest is shown by the presence in the audience of others than members of the Council.\nThe first half of the address is aimed at dispelling the distrust and hostility toward Dio occasioned by his seeming indifference to the Apameians in the past. This he attempts to bring about by recalling the ties which bound him and his family to that city and by explaining the delicacy of his situation as a member of both communities. The remainder of the speech is devoted to praising the blessings of concord and stressing the peculiarly intimate nature of the ties existing between the two cities.\nThe abruptness of the close might suggest that the speech is incomplete. However, such a supposition is not unavoidable. Dio has presumably achieved his immediate purpose — to restore himself to good favour at Apameia and, as a delegate from Prusa, to make his voice heard in support of concord. It is not as if he were the only delegate to be heard.\nAbout the Latin Academy in the Vatican\n5 years ago","- Prayer and Worship\n- Beliefs and Teachings\n- Issues and Action\n- Catholic Giving\n- About USCCB\nEulogy of the Romans. 1* Judas had heard of the reputation of the Romans. They were valiant fighters and acted amiably to all who took their side. They established a friendly alliance with all who applied to them. 2He was also told of their battles and the brave deeds that they performed against the Gauls,* conquering them and forcing them to pay tribute; 3and what they did in Spain to get possession of the silver and gold mines there. 4By planning and persistence they subjugated the whole region, although it was very remote from their own. They also subjugated the kings who had come against them from the far corners of the earth until they crushed them and inflicted on them severe defeat. The rest paid tribute to them every year. 5Philip* and Perseus, king of the Macedonians, and the others who opposed them in battle they overwhelmed and subjugated. 6Antiochus* the Great, king of Asia, who fought against them with a hundred and twenty elephants and with cavalry and chariots and a very great army, was defeated by them. 7They took him alive and obliged him and the kings who succeeded him to pay a heavy tribute, to give hostages and to cede 8Lycia, Mysia, and Lydia* from among their best provinces. The Romans took these from him and gave them to King Eumenes. 9* When the Greeks planned to come and destroy them, 10the Romans discovered it, and sent against the Greeks a single general who made war on them. Many were wounded and fell, and the Romans took their wives and children captive. They plundered them, took possession of their land, tore down their strongholds and reduced them to slavery even to this day. 11All the other kingdoms and islands that had ever opposed them they destroyed and enslaved; with their friends, however, and those who relied on them, they maintained friendship. 12They subjugated kings both near and far, and all who heard of their fame were afraid of them. 13Those whom they wish to help and to make kings, they make kings; and those whom they wish, they depose; and they were greatly exalted. 14Yet with all this, none of them put on a diadem or wore purple as a display of grandeur. 15But they made for themselves a senate chamber, and every day three hundred and twenty men took counsel, deliberating on all that concerned the people and their well-being. 16They entrust their government to one man* every year, to rule over their entire land, and they all obey that one, and there is no envy or jealousy among them.\nTreaty with the Romans. 17So Judas chose Eupolemus, son of John, son of Accos, and Jason, son of Eleazar, and sent them to Rome to establish friendship and alliance with them.a 18He did this to lift the yoke from Israel, for it was obvious that the kingdom of the Greeks was subjecting them to slavery. 19After making a very long journey to Rome, the envoys entered the senate chamber and spoke as follows: 20“Judas, called Maccabeus, and his brothers, with the Jewish people, have sent us to you to establish alliance and peace with you, and to be enrolled among your allies and friends.” 21The proposal pleased the Romans, 22and this is a copy of the reply they inscribed on bronze tablets and sent to Jerusalem,* to remain there with the Jews as a record of peace and alliance:b\n23“May it be well with the Romans and the Jewish nation at sea and on land forever; may sword and enemy be far from them. 24But if war is first made on Rome, or any of its allies in any of their dominions, 25the Jewish nation will fight along with them wholeheartedly, as the occasion shall demand; 26and to those who wage war they shall not give or provide grain, weapons, money, or ships, as seems best to Rome. They shall fulfill their obligations without receiving any recompense. 27In the same way, if war is made first on the Jewish nation, the Romans will fight along with them willingly, as the occasion shall demand, 28and to those who attack them there shall not be given grain, weapons, money, or ships, as seems best to Rome. They shall fulfill their obligations without deception. 29On these terms the Romans have made an agreement with the Jewish people. 30But if both parties hereafter agree to add or take away anything, they shall do as they choose, and whatever they shall add or take away shall be valid.\n31“Moreover, concerning the wrongs that King Demetrius is doing to them, we have written to him thus: ‘Why have you made your yoke heavy upon our friends and allies the Jews? 32If they petition against you again, we will enforce justice and make war on you by sea and land.’”\n* [8:1] This chapter contains the account of the embassy which Judas sent to Rome, probably before the death of Nicanor, to conclude a treaty of alliance between Rome and the Jewish nation. Without precise chronology, the pertinent data are gathered into a unified theme.\nThe image of the Roman Republic greatly impressed the smaller Eastern peoples seeking support against their overlords (vv. 1–16), because of Roman success in war (vv. 2–11) and effective aid to their allies (vv. 12–13). Numerous interventions by Rome in the politics of the Near East bear witness to its power and prestige in the second century B.C. See 1:10; 7:2; 12:3; 15:15–24; 2 Mc 11:34. With the increased Roman control of Palestine after 63 B.C., the Republic and later the Empire became heartily detested. The eulogy of Rome in this chapter is one of the reasons why 1 Maccabees was not preserved by the Palestinian Jews of the century that followed.\n* [8:2] Gauls: probably the Celts of northern Italy and southern France, subdued by the Romans in 222 B.C., and again in 200–191 B.C.; but also those in Asia Minor (the Galatians), whom the Romans defeated in 189 B.C.\n* [8:5] Philip: Philip V of Macedonia, defeated by a Graeco-Roman alliance at Cynoscephalae in 197 B.C. Perseus, his son, was defeated at Pydna in 168 B.C., and died a prisoner. With this, the kingdom of Macedonia came to an end.\n* [8:6] Antiochus: Antiochus III, greatest of the Seleucid kings. He was defeated at Magnesia in 190 B.C. By the Treaty of Apamea in 189 B.C., he was obliged to pay Rome a crushing indemnity of 15,000 talents. The weakening of Antiochene power and the growing military and economic influence of Rome may have led Antiochus IV to adopt the policy of political, religious, and cultural unification of Syria and Palestine.\n* [8:8] Lycia, Mysia, and Lydia: regions in western Asia Minor. “Lycia” and “Mysia” are restored here by conjectural emendation; the Greek text has “India, Media,” most likely through scribal error. Eumenes: Eumenes II (197–158 B.C.), king of Pergamum, an ally of Rome who benefited greatly from Antiochus’ losses.\n* [8:16] They entrust their government to one man: actually the Roman Republic had two consuls chosen yearly as joint heads of the government.\n* [8:22] The reply…on bronze tablets and sent to Jerusalem: the decree of the Senate would be inscribed on bronze and kept in the Roman Capitol, with only a copy in letter form sent to Jerusalem.\nBy accepting this message, you will be leaving the website of the\nUnited States Conference of Catholic Bishops. This link is provided\nsolely for the user's convenience. By providing this link, the United\nStates Conference of Catholic Bishops assumes no responsibility for,\nnor does it necessarily endorse, the website, its content, or"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:dda66cf0-1f2a-4d79-b7b4-9b6d250ce854>","<urn:uuid:efef13b2-93b2-4faf-8338-b8ba260149d5>"],"error":null}
{"question":"Could you compare the shelf life considerations between type 3 and type 6 solder powder? Additionally, for mil-spec products, what are the key advantages of using traditional tin-lead solder over lead-free alternatives?","answer":"Type 6 solder powder has a shorter effective shelf life than type 3 powder due to its smaller particle size, which creates more surface area per volume for flux reactions and welding to occur. Even if rated for 6 months, a type 6 powder may not provide the full shelf life of a type 3 powder under identical conditions. Regarding mil-spec products, traditional tin-lead solder is preferred because it is less costly, gentler on the board, better tested, and more reliable compared to lead-free alternatives. Military and aerospace industries have received exemptions from RoHS directives to continue using tin-lead solder for these benefits.","context":["Choosing the right solder paste is like buying a new car: the options can seem endless, and it's really a matter of preference. In the end, however, it all comes down to three considerations: (1) Lead vs. lead-free, (2) Water-wash vs. no-clean, (3) Alloy ratio.\nLead vs. Lead-Free\nDeciding whether to use lead or lead-free paste depends largely on the product's end use and target market. Consumer markets have almost completely shifted away from traditional tin-lead paste because of the RoHS directive. The U.S. military and aerospace industries, however, still require tin-lead solder, having recieved exemptions from the directive.\nThe use of lead vs. lead-free solder is still hotly debated. Yet, in the end, lead-free is a reality if you're interested in tapping into international, consumer markets. While lead-free solder may be more expensive, it would be both duplicative and costly to manufacture lead-free and tin-lead versions of the same product for markets in- and outside of Europe.\nMil-spec products use tradional tin-lead solder because, many experts agree, it's less-costly, gentler on the board, better tested and more reliable. If you do not plan to tap into consumer markets, consider using tin-lead solder.\nWater-wash vs. No-clean\nWater-wash pastes are a standard option. After the board passes through the reflow stage, flux residue is washed off, giving the board a cleaner look.\nNo-clean doesn’t require the extra step of cleaning, and is therefore quicker than water wash. However, no-clean is less common for aesthetic reasons. The flux's function in no-clean is the same as water wash paste, but it leaves residue on the board, which doesn’t make for the best looking product. While some experts contend that flux residue left by no-clean becomes inert, others say that any residue left on the board could result in negative consiquences later in the product's lifecycle.\nOverall, deciding whether to use water-wash or no-clean is a question of cosmetics.\nSelecting the Right Alloy Ratio\nDetermining the perfect solder paste alloy ratio is usually left up to an assembly shop’s manufacturing engineer. Manufacturing engineers have feel for the shop’s reflow ovens as well as which alloy ratios work best for the product. But it’s still important to understand the process by which specific alloy ratios are selected in order to fully understand your costs. Ask yourself:\nWhat strength and other required properties would work best for your assembly?\nWhat are the preferred soldering and the operating temperatures?\nWhat materials are being soldered? What is most compatible with them?\nConsider the malleability and ductilibility of the different alloys and metals.\nWhat is the operating environment of the complete assembly? Will it operate in extreme temperatures or subject to vibration or high pressure?\nPb (lead): The most common solder paste is a SnPb (tin/lead) combo. Most boards will used a 63Sn/37Pb (63% tin/37% lead) alloy ratio, although this depends on many factors, as mentioned above. Higher reliability variations contain a touch of silver, as in Sn62/Pb36/Ag2 (62% tin/36% lead/ 2% silver). There are many ratios, but these are the most common.\nPb Free: For high reliability, SAC405 (more silver than SAC305) and SnlnCe (great for thermal cycling performance) are the most common. For lower cost solder paste, Sn992, SAC105, and SAC0307 are recommended. And for low melting point, eutectic alloys such as BiSn, BiSnAg, and InSn are the most common.","Given that the flux is in direct contact with the solder powder, this allows for the flux activators to interact with the solder powder even while the solder paste sits on the shelf. Those activators can begin to \"react\" with the powder, and, given enough time, can \"clean\" the powder surface to the point where the solder particles will actually \"weld\" together. So, now instead of the paste containing free-flowing powder, it contains clumps of welded together solder particles. Those clumps often increase the viscosity and can clog stencil apertures and dispensing needles. For these reasons, the paste manufacturer will require refrigerated storage of the paste in order to realize the optimum shelf life.\nAs a rule water-washable solder pastes often include activators that are more aggressive than the activators found in no-clean and RMA type solder pastes. This is because water-washable flux residues are designed to be washed off. So, there is no concern about the flux causing corrosion over the life of the product. On the other hand, a no-clean flux generally has milder activators, because the flux residue may remain on the device indefinitely; where corrosion would be detrimental to the performance and life of the device. As a result, no-clean type solder pastes typically have a longer shelf life and are more tolerant to higher storage temperatures than water-soluble/washable solder pastes.\nA solder paste typically has a shelf life of 6 months when refrigerated. One may ask what happens if the paste has been refrigerated for 2 months, then thawed to room temperature, remains at room temperature for 12 hours and is then re-refrigerated....Will it still have a 6 month shelf life? That is a very difficult question to answer. The same situation could arise with a perishable food item that requires refrigeration, such as milk. Lets say that one buys a container of milk at the store and it has an expiration date that is 5 days away. After having it home, properly refrigerated, for 2 days, one of the kids leaves the milk on the counter for 3 hours before anybody notices it and puts it back in the refrigerator. Can one expect the milk to stay good for the remaining 3 days? What about if it is left out of the refrigerator for 1 hour? or 5 hours? You can see how difficult the questions become to answer. What is the impact if a solder paste is exposed to elevated temperatures when it is 3 days old or 3 weeks old or 3 months old or with 3 days left to expiration????? The answer is not fully known. It is impossible for the solder paste manufacturer to study every possible scenario for its impact on the shelf life of the paste.\nThe best and only sure approach is to refrigerate solder paste immediately upon receipt and only thaw when needed, in amounts that will be completely consumed. Avoid thawing and re-refrigerating pastes as much as possible, in order to take advantage of the full shelf life.\nThe particle (mesh) size of the solder powder can also impact shelf life. As the powder size decreases, the surface area per volume or mass of powder drastically increases. More powder surface area means more area for the flux to react with, and more surface area for welding to occur. Therefore, a type 3 solder paste that has a shelf life of 6 months may not provide a full 6 months of shelf life with a type 6 solder powder, all other things being equal.\nFor the most part, solder paste manufacturers are conservative in assigning shelf life. It is highly unlikely that a properly stored solder paste's performance is going to collapse 1 day after the expiration date. In fact, depending on the paste, it may still be good for months beyond the expiration date.\nHow does one know if their solder paste is still usable? This can be determined rather easily. As mentioned earlier, one artifact of a degrading paste is a rise in viscosity. So one can perform a simple printing or dispensing test to see if it still performs adequately in that regard. Another aspect that often suffers is coalescence. As the flux degrades it looses its ability to adequately remove oxides on the solder powder. In order to gauge the degradation, it is best to put a small amount of paste on a non-wettable substrate, like a piece of ceramic. Reflow the paste and see how well it coalesces. If coalescence is good, the solder paste will reflow into a ball, surrounded by a flux pool that is relatively free of uncoalesced solder particles. If the paste has significantly degraded, the paste will not coalesce well and there will be a significant amount of uncoalesced solder particles in the flux pool.\nPlease see this IPC test method for determining the coalescent properties of a solder paste."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:b28f6afd-8276-4432-a8a7-912528f69b70>","<urn:uuid:8316b041-5ec9-49f3-ac39-8d9223359a21>"],"error":null}
{"question":"What are the health risks of poor indoor air quality, and what specific workplace substances can trigger these health issues?","answer":"Poor indoor air quality can cause both immediate and long-term health effects. Immediate effects include irritation of the eyes, nose and throat, headaches, dizziness, and fatigue. Long-term health issues can include respiratory diseases, heart disease, and cancer. Poor air quality particularly affects asthma and allergy sufferers - approximately 18 million adults live with asthma, and about 7.8 percent of American adults have hay fever. In workplace settings, several substances can trigger these health issues, including: dust from wood, cotton, coal, asbestos, silica, and talc; fumes from heated metals in welding and smelting; smoke from burning organic materials; gases like formaldehyde and ammonia; vapors from solvents; and mists from paints, lacquers, hair spray, pesticides, and cleaning products.","context":["Indoor Air Quality, also known as IAQ, refers to the quality of the air within a building.\nIAQ is an important, and often overlooked, area of building improvement.\nThe effects of improved IAQ are immediate for those who work and do business in your building.\nConversely, existing health issues can be exacerbated by poor IAQ, since poor indoor air quality can damage weakened immune systems.\nSome immediate effects are irritation of the eyes, nose and throat, headaches, dizziness and fatigue.\nLong-term health issues can include respiratory diseases, heart disease and cancer.\nBreathing low-quality air can aggravate the conditions of asthma and allergy sufferers, leading to more time off of work due to illness.\nAccording to the American Academy of Allergy Asthma & Immunology, approximately 7.8 percent of American adults have hay fever, otherwise known as allergic rhinitis.\nHay fever causes cold-like symptoms, such as runny nose, itchy eyes, congestion, sneezing and sinus pressure and is caused by an allergic response to pollen, dust mites or pet dander.\nAsthma is a chronic inflammatory lung disease that occurs when the airways react to a variety of stimuli.\nApproximately 18 million adults live with asthma.\nIt can be life-threatening when not properly managed, and annually, approximately 3,000 people die from asthma-related issues.\nAsthma triggers include: Respiratory infections and colds; cigarette smoke; allergic reactions to pollen, mold, animal dander, feathers, dust, food and cockroaches; indoor and outdoor air pollutants; exercise; excitement; and stress.\nAccording to the U.S. Centers for Disease Control and Prevention (CDC), air quality can be improved by smoke-free air laws, improving ventilation and air purification in older buildings and reducing or eliminating known triggers.\nTriggers contributing to poor air quality include: Combustion sources, like oil, gas, kerosene, coal, wood and tobacco; building materials, like asbestos insulation and new carpet and paint; cleaning and maintenance products; plug-in/portable humidifiers; outdoor sources, such as radon, pesticides and outdoor air pollution; and poor ventilation.\nGetting a handle on the sources of contaminants affecting your building's IAQ is the first step.\nHere are some tips to improve your building’s indoor air quality and the health of your employees:\nSmoke outside and away from doorways.\nThe harmful chemicals in cigarettes can linger in the air, affecting other people in the building.\nTake cleaning to the next level.\nEliminating harmful mold and mildew in bathrooms and basements is essential.\nUse all-natural cleaning products whenever possible to cut down on further contaminating the building.\nRepair all standing water issues and keep your building’s humidity under 50 percent.\nAny higher and you risk creating an environment where harmful bacteria, mold and mildew can thrive.\nProperly vent all fuel-burning appliances.\nThis includes appliances like gas stoves, water heaters and fireplaces.\nDon't store chemicals, paints or solvents within the building.\nKeep them in a garage or storage area if at all possible.\nEvaluate air fresheners.\nAir-freshening devices may make your air smell nice, but they do so with a mix of chemicals that can be toxic.\nThe next level to improve IAQ is to incorporate more permanent technology to clean and purify the air.\nA licensed contractor can evaluate and assess IAQ issues and make suggestions based on your building’s specific needs.\nHere are some steps a contractor may take.\nImprove ventilation in the facility.\nThere are two ways to accomplish this: Via natural methods, like opening windows to allow for natural air flow through the building, or mechanical methods, like utilizing industrial fans to move the air.\nBy improving air flow, IAQ issues stemming from foul odors, allergens and combustion and building materials can be reduced or eliminated.\nUtilize an ozone generator, once the area is vacated.\nOzone has the ability to completely destroy the molecules found in bacteria, mold and mildew.\nThis method allows for a deep clean by permeating small cracks in the walls, therefore eliminating any issues from within the building's own structure.\nLow levels of ozone are also effective and safe.\nThe use of hydrogen peroxide and hydroxyls is the latest technology available.\nIt is extremely effective and has no adverse side effects.\nThis type of technology can be installed directly in the building’s existing HVAC system and allows for continuous cleaning of the air and surfaces, even reducing dust, allergens, mildew and bacteria by up to 99 percent.\nThis method can improve the health of everyone in the building, particularly those who suffer from allergies, asthma and other respiratory illnesses.\nIt also can cut down on the spread of the common cold and flu, since the technology cleans each surface in the environment, including bathrooms and kitchens.\nNo matter how big or small, making steps towards improving your building's indoor air quality will help everyone breathe easier.","Who is at risk for work-related lung disease?\nYou may be at risk for work-related lung disease if the air you breathe at work contains an excessive amount of dust, fumes, smoke, gases, vapors or mists. Workers who smoke are at a much greater risk of lung disease if they are exposed to substances in the workplace that can cause lung disease. Poor ventilation, closed-in working areas and heat increase the risk of disease. Outside air pollution also can increase the risk of lung disease in people who work in jobs that expose them to substances that can cause lung disease.\nWhat substances in the workplace can cause lung disease?\nMany substances found in the workplace can cause breathing problems or lung damage. Some of them include the following:\n- Dust from such things as wood, cotton, coal, asbestos, silica and talc. Dust from cereal grains, coffee, pesticides, drug or enzyme powders, metals and fiberglass can also hurt your lungs.\n- Fumes from metals that are heated and cooled quickly. This process results in fine, solid particles being carried in the air. Examples of jobs that involve exposure to fumes from metals and other substances that are heated and cooled quickly include welding, smelting, furnace work, pottery making, plastics manufacture and rubber operations.\n- Smoke from burning organic materials. Smoke can contain a variety of particles, gases and vapors, depending on what substance is being burned. Firefighters are at an increased risk.\n- Gases such as formaldehyde, ammonia, chlorine, sulfur dioxide, ozone and nitrogen oxides. These are associated with jobs where chemical reactions occur and in jobs with high heat operations, such as welding, brazing, smelting, oven drying and furnace work.\n- Vapors, which are a form of gas given off by all liquids. Vapors, such as those given off by solvents, usually irritate the nose and throat first, before they affect the lungs.\n- Mists or sprays from paints, lacquers (such as varnish), hair spray, pesticides, cleaning products, acids, oils and solvents (such as turpentine).\nWhat kinds of breathing problems can occur following exposure to such substances?\nSome substances can cause you to have upper respiratory irritation or irritation of your nose and/or throat with cold-like symptoms, such as a runny nose and scratchy throat.\nViral infections and allergies produce similar symptoms. You should become suspicious of a work-related illness if your nose and throat are often irritated and breathing problems seem to occur when you are at work. Breathing in substances at work can also increase your risk of developing bronchitis, flu-like symptoms, asthma or emphysema.\nA person who has bronchitis has a persistent cough that produces mucus or sputum and lasts at least 3 months to a year. Cigarette smoking is the most common cause of bronchitis, but workplace toxins can also play a role.\nIf you notice that you often have what seems to be the flu, your illness may be caused by something you are exposed to at work. The following are some work-related lung diseases that can make you feel as though you have the flu:\n- Allergic alveolitis (also known as \"farmer's lung\") can occur after excessive exposure to moldy hay.\n- Metal fume fever occurs from inhalation of metal vapors, such as in welding and other metallic operations.\n- Polymer fume fever can occur after breathing the fumes of polymers, such as Teflon.\nA worker who has one of these conditions develops breathing problems, cough, fever, muscle aches and general malaise (a feeling of being tired and having no energy) 4 to 6 hours after exposure to the substance. If such symptoms occur again and again when you are at work, this is a clue that your illness may be related to your work.\nIf you develop asthma for the first time as an adult, the illness could be related to something you are exposed to at work. Asthma symptoms include wheezing, chest tightness, a persistent dry cough or trouble breathing.\nEmphysema usually occurs in older people who smoke. However, people who have worked with coal, asbestos or silica dust for 20 years or more can also develop emphysema. They may have a cough, fatigue, chest tightness and difficulty breathing.\nWhat should I do if I think something in the air at work is making me sick?\nVisit your doctor if you think the air at work is making you sick. Your doctor will probably ask you to provide some of the following information:\n- When your symptoms first appeared\n- How often you have symptoms\n- The time of day that the symptoms are worse\n- Whether you feel better on some days\n- How you think your symptoms relate to work\n- What types of materials you come in contact with at work\nYou may find it helpful to keep a written record of these things to share with your doctor. Be sure to make a note of your shift or work hours, the days of the week you work and the days of the week you are off work. Try to recall previous jobs, hobbies and smoking habits -- anything that might have affected your lungs. If your doctor has sent you an occupational health history form, fill it out as completely as possible.\nYour doctor may find it helpful to know all of the ingredients listed on the containers of materials you use in the workplace. Make a list of these ingredients and write down any precautions and first-aid measures that are printed on the label.\nAsk your employer for copies of the material safety data sheets (MSDSs) at your workplace. These are information sheets about the products that you use in the workplace. All employers are required by law to complete these forms, and you have a right to see them. Bring them with you to your doctor's appointment.\nHow can I keep from having my lungs damaged by something I'm exposed to at work?\nIf you smoke, stop. This is the most important thing you can do for your overall health, regardless of risks at your workplace. Smokers have a greater risk of developing some work-related lung diseases than nonsmokers.\nUse a respirator. A respirator is a device you wear over your mouth and nose that cleans the air before it enters your body. You must be properly fitted and trained to use a respirator. Over time you should be refitted and retrained in how to use it. The respirator must be carefully cleaned after each use and it should be checked to ensure that it works properly. Use a respirator as a temporary measure until you are no longer exposed to the damaging substance.\nIf you are exposed to damaging substances at work, talk to your supervisor about the need for adequate ventilation and new procedures to reduce or eliminate your exposure. A change in ingredients, work practices or machinery can reduce hazards in the air. Ventilation systems can remove pollutants and toxins from the air to reduce exposure and prevent buildup. Local exhaust ventilation can be used to remove polluted air at the point where it is generated by a hazardous process or machine. At some jobs, people can be separated from the hazardous materials.\nWritten by familydoctor.org editorial staff"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2094a19b-70fd-4d21-8976-c4a063c20187>","<urn:uuid:e2b1daf7-dea9-4f15-b153-2a00db2732a7>"],"error":null}
{"question":"在治疗儿童OCD方面，认知行为疗法的效果如何，而家长在家里可以提供什么样的支持呢？(How effective is cognitive behavioral therapy for treating children's OCD, and what support can parents provide at home?)","answer":"Cognitive behavioral therapy (CBT) is highly effective for treating children's OCD, particularly a version known as exposure and response prevention. In head-to-head trials, CBT consistently performs better than medication, with studies showing that 70% of patients respond well when CBT is combined with medication. The therapy involves gradual exposure to anxiety-triggering situations, helping children learn to tolerate anxiety through habituation. As for parental support at home, parents can help by creating an honest and accepting environment, avoiding enabling their child's rituals (even though it may be tempting), challenging fearful thoughts through discussion, and using positive reinforcement systems like sticker charts. It's also crucial for parents to ensure consistency across all caregivers and to maintain an open dialogue with their child about their OCD symptoms.","context":["Cognitive behavioral therapy alone or combined with medication may help.\nIt is normal for many children, at various stages of development, to be concerned about symmetry and having things perfect, to insist on certain bedtime routines, or to develop superstitions and rituals like avoiding cracks in the sidewalk. But when such beliefs or behaviors become all-consuming and start interfering with school, home life, or recreational activities, the problem may be obsessive-compulsive disorder (OCD).\nOCD afflicts 1% to 2% of American adults. The disorder usually originates in childhood or adolescence, with symptoms appearing as early as age 3. Childhood OCD is more common in boys than in girls, although the adult form occurs equally in both sexes.\nObsessions are irrational thoughts, images, and impulses that are felt as unrealistic, intrusive, and unwanted. In children, obsessions may govern how toys or other personal belongings are arranged in the playroom. In teenagers, thoughts and impulses may give rise to fears of contamination or excessive concern with religious rituals, such as praying constantly. To relieve the anxiety caused by these obsessions, youths of all ages may engage in compulsive rituals such as buttoning and unbuttoning a shirt dozens of times before wearing it.\nEven young children often know that their obsessions are senseless, but they may be helpless to stop themselves. If youths try to avoid the situations that provoke the behavior, their lives may become increasingly restricted. As a result, they may become demoralized, and their development may be interrupted.\nMuch of the risk for developing OCD is genetic. Brain imaging studies show unusual activity in patients with OCD, chiefly in a circuit that runs between the cerebral cortex, the area of the brain associated with screening thoughts and sensations for significance, and the basal ganglia, a region involved in the control of body movements. As a result, patients with OCD may become overly focused on \"unimportant\" things and engage in repetitive behavior.\nInitial treatment options\nSince 2001, several randomized controlled trials and literature reviews have concluded that both cognitive behavioral therapy (CBT) and medication can help youths better manage OCD — but that the combination of both is best. In head-to-head trials, CBT is consistently more effective than medication, however, so many experienced clinicians recommend trying this form of psychotherapy before turning to medication.\nCBT. A version of CBT known as exposure and response prevention is typically used in treating OCD in patients of any age. During therapy, a clinician gradually exposes patients — either physically or mentally (through the imagination) — to the things, places, and circumstances that provoke obsessions. Eventually, if all goes well, through habituation the patient learns to tolerate the anxiety. In a sense, this detoxifies the stimuli and makes the compulsive behaviors unnecessary.\nBehavior therapy is usually conducted in weekly sessions for several months. Daily homework is also important, because the circumstances that provoke obsessions and rituals are different for each person and cannot be easily reproduced in a therapist's office. Sometimes the best place for therapy is the place where the most serious symptoms occur. Family members are usually enlisted to supervise and encourage young patients. After symptoms become less serious, later sessions may be needed to prevent relapse and to help the child cope with new obsessions and rituals if they develop.\nMedication. Although CBT remains the first recommendation for OCD treatment, it may be difficult to find a clinician experienced at working with children and adolescents — or to find an insurance plan that covers the full cost of therapy. Such real-world circumstances help explain why medication tends to be the most common treatment for youths with OCD.\nSelective serotonin reuptake inhibitors (SSRIs) such as fluoxetine (Prozac) and sertraline (Zoloft) are the drugs most often used and studied in youths with OCD. These drugs enhance the activity of the neurotransmitter serotonin by preventing its reabsorption at the nerve endings that release it.\nIt is not clear how serotonin-enhancing drugs yield helpful changes in brain function. Scientists have not confirmed any irregularity in the production, activity, or breakdown of serotonin in children or adults with OCD.\nSSRIs require two months or more to have an effect on OCD symptoms — a longer time than they usually take to relieve depression. When used for OCD, these drugs are prescribed at doses generally higher than those needed for depression. Young people may need to keep taking an SSRI for at least a year and sometimes indefinitely. When the drug is the only treatment, youths usually relapse in a few months if they stop taking it.\nAlthough SSRIs are now a mainstay of treatment for OCD, many youths obtain only partial relief from the drugs. The Pediatric OCD Treatment Study II, a randomized controlled study involving youths ages 7 to 17, examined what might help young people who continue to struggle with symptoms in spite of treatment with an SSRI. The investigators tested two interventions — traditional CBT delivered by a therapist and a briefer, more do-it-yourself version termed \"instructions in CBT.\" In this model — which is probably closer to what happens in actual clinical practice — the same person who prescribed the SSRI also spent time going over principles of CBT and provided homework for practice.\nThe study found that the briefer version of CBT wasn't much help, but that traditional CBT could indeed enhance patients' recovery. At the end of 12 weeks, only a third of patients assigned to medication combined with instruction in CBT were responding to treatment — about the same as those receiving medication alone — and in both groups, symptoms were still pronounced. In contrast, 70% of the group assigned to traditional CBT combined with medication had responded to treatment, and their symptoms had become significantly more manageable.\nOne problem with this study is that the researchers did not control for the fact that participants in the traditional CBT group spent 14 extra hours with a therapist. Even so, the evidence remains strong that CBT is the best treatment to add when a young person does not respond adequately to medications.\nFuture research may point the way to better medication options or effective shorter-duration psychotherapy. Until then, the evidence suggests that relief for youngsters with OCD will depend on increasing their access to experienced CBT therapists.\nFranklin ME, et al. \"Cognitive Behavior Therapy Augmentation of Pharmacotherapy in Pediatric Obsessive-Compulsive Disorder: The Pediatric OCD Treatment Study II (POTS II) Randomized Controlled Trial.\" Journal of the American Medical Association (Sept. 21, 2011): Vol. 306, No. 11, pp. 1224–32.\nO'Kearney RT, et al. \"Behavioural and Cognitive Behavioural Therapy for Obsessive Compulsive Disorder in Children and Adolescents,\" Cochrane Database of Systematic Reviews (Oct. 18, 2006): Doc. No. CD004856.\nRuscio AM, et al. \"The Epidemiology of Obsessive-Compulsive Disorder in the National Comorbidity Survey Replication,\" Molecular Psychiatry (Jan. 2010): Vol. 15, No. 1, pp. 53–63.\nWatson HJ, et al. \"Meta-Analysis of Randomized, Controlled Treatment Trials for Pediatric Obsessive-Compulsive Disorder,\" Journal of Child Psychology and Psychiatry and Allied Disciplines (May 2008): Vol. 49, No. 5, pp. 489–98.\nFor more references, please see www.health.harvard.edu/mentalextra.","If you’re the parent of a child with Obsessive Compulsive Disorder (OCD), you undoubtedly experience a whole mix of emotions on any given day. Your little one may have hidden his or her obsessions and compulsions from you for quite some time, leaving you feeling guilty you didn’t seek help for your child sooner. And since OCD can significantly interfere with your child’s ability to carry out basic daily functions, you may find yourself feeling extremely frustrated at times, particularly if the fears your child faces feel far-fetched and unfounded.\nWhatever emotions you are feeling, rest assured they are normal, you are not alone, and none of this is your fault. OCD is a chronic mental health disorder, and while there is no known cure, there are so many things you can do to help your child lead a normal life.\nIf you want to know how to help a child with OCD at home, we’re sharing 11 tips that are sure to inspire you.\nWhat is OCD?\nMany of us equate the term ‘OCD’ with perfectionism, and since many of us have certain things we are extremely particular about, OCD tends to be used casually in day-to-day interactions. We often hear people talk about their ‘obsessions’ or the fact that they are ‘OCD’ about certain things, like the way they fold their towels or the fact that they have to double check that their front door is locked before they go to bed each night.\nWhat few people realize, however, is that Obsessive Compulsive Disorder (OCD) is a mental health disorder that can – and often does – become so extreme that it interferes with a person’s daily functioning. OCD is chronic, and is categorized as an anxiety disorder in the DSM-IV.\nA person with OCD gets stuck in a cycle of obsessions and compulsions, which can become extremely time-consuming and prevent him/her from participating in things he/she enjoys.\nObsessions refer to unwanted/disturbing thoughts, images, and urges that cause anxiety and discomfort, such as:\n- Fear of contamination\n- Fear of harm (to self or others)\n- Need for orderliness and perfection\n- Inappropriate thoughts\n- Fear of losing control\nCompulsions refer to the repetitive/ritualistic behaviors and/or thoughts a person does to reduce his/her obsessions, including:\n- Counting, tapping, touching, etc.\n- Organizing, arranging, ordering\n*Please note that this is not an exhaustive list. If you suspect your child has OCD, it’s important to consult with a licensed medical practitioner for a proper diagnosis.\nWhat Causes OCD in a Child?\nUnfortunately, scientists still don’t know exactly what causes OCD, but evidence suggests biological and environmental factors are contributing causes. OCD appears to be hereditary, and environmental factors like losing a loved one, an illness, and abuse can all trigger or worsen the symptoms of OCD in children and adults.\nWhat Are the Signs of OCD in a Child?\nChildren are often embarrassed about their obsessions and compulsions, and often suffer in silence for quite some time before a parent or teacher begins to notice their fears and ritualistic behaviors. People with OCD often know their fears are unfounded and may seem silly to others, but their obsessions cause significant anxiety that can only be lessened by completing compulsive acts.\nThere are specific symptoms that must be met in order for a Obsessive Compulsive Disorder diagnosis to be made – and this can only be completed and confirmed by a licensed professional – but for the purposes of this post, I wanted to provide a list of signs to look out for if you suspect your child may suffer from OCD so you can get him/her the help he/she needs:\n- Inability to participate in and enjoy activities he/she is passionate about\n- Difficulty concentrating at school\n- Inability to make decisions\n- Taking longer than normal to complete simple activities, like getting dressed, brushing hair, or completing homework\n- Need for things to be perfect, and getting very upset/angry when things aren’t ‘just right’\n- Fear of germs and/or getting sick\n- Excessive hand washing and/or bathing\n- Need to count, tap, touch things repeatedly\nYou may also notice your child performing certain rituals throughout the day, or displaying a need for things to be ‘just right’ in order for him or her to feel safe and comfortable.\nHow to Help a Child with OCD at Home\nFiguring out how to help a child with OCD at home can be extremely difficult, but the tips below will definitely help and inspire you. Keep in mind these ideas are not meant to replace professional treatment, and assume you have already consulted with a trained professional and have plans in place to help your child with his or her individual needs.\nEducate yourself. If you want to know how to help a child with OCD at home, one of the very first things you need to do is to educate yourself on your child’s diagnosis. The better you understand the struggles your child faces, the easier it will be to strategize ways to help him or her at home and beyond. Putting your head in the sand, pretending your child’s challenges don’t exist, and/or blaming yourself for the diagnosis isn’t going to benefit your child in any way, shape, or form. The sooner you reach a level of acceptance and equip yourself with the information you need to help your child thrive, the better. Talk with your child’s doctors and teachers, research online, join support groups, and read as much as possible. Here are a few books to help you and your child:\n- Different: The Story of an Outside-the-Box Kid and the Mom Who Loved Him\n- What to do when your Child has Obsessive-Compulsive Disorder: Strategies and Solutions\n- Freeing Your Child from Obsessive-Compulsive Disorder\n- What to Do When Your Brain Gets Stuck: A Kid’s Guide to Overcoming OCD\n- Talking Back to OCD: The Program That Helps Kids and Teens Say “No Way”\nEducate your child. Another often overlooked way to help a child with OCD at home is to be open and honest about his or her condition. Of course, this can be tricky if your child is too young to fully understand what OCD is, but if your child struggles with obsessive thoughts and compulsive rituals, he or she is probably very aware that something isn’t quite right. Your child most likely realizes his or her peers do not share the same fears or engage in the same safety-seeking behaviors, and may find relief in knowing the reason for his or her differences.\nBe receptive to different treatment options. While there is no known cure for Obsessive Compulsive Disorder, there are treatment options that can help children (and adults) learn how to manage their symptoms, and medication can be helpful in lessening anxiety and making treatment tolerable. Discuss all options with your child’s doctor, research the pros and cons of each, and make a decision based on your child’s best interests. Remember that OCD is a chronic mental health disorder that can significantly interfere with your child’s daily functioning, and be careful not to allow any preconceived notions you may have about mental illness cloud your judgement and prevent you from helping your child.\nMake sure everyone is on the same page. Once you have a diagnosis and a treatment option in place, it’s important to ensure everyone who cares for your child is on the same page in order for treatment to be successful. For example, if your child’s therapist tells you not to given into your child’s rituals, you must make sure that all of the caregivers in your child’s life do the same for ongoing consistency.\nCreate an honest, open, and accepting environment. Make sure your home is a safe place where your child can share his or her obsessions and compulsions with you. The more honest he or she can be, the easier it will be for you and his/her therapeutic team to understand what drives his or her OCD, and what the best method of treatment is. OCD can be very lonely and isolating – not to mention frightening – for children and adults, making it more important than ever that your child has a place to be him- or herself.\nDon’t enable your child. While it can be gut-wrenching to watch your child experience feelings of intense anxiety, and very tempting to give in and provide reassurance and/or participate in his or her rituals, it’s important that you refrain from doing this. While accommodating your child’s fears may help him or her feel less anxious in the short term, it actually negatively reinforces your child’s symptoms in the long run. Explain to your child that you will not allow his or her OCD to rule your home, and be firm and consistent.\nThink before you talk. OCD can be extremely difficult and oftentimes frustrating for the entire family, and there will be times when your emotions get the best of you. It is during these times that you need to be very thoughtful and purposeful with how you react to your child. While your child’s fears – and his or her need to perform rituals to squash those fears – may seem silly, it’s important to remember that they are very real to him or her. Your child’s behaviors and emotions are not intended to upset you or ruin your day, and he or she would likely give anything not to have OCD. Take a 5-minute time out to compose yourself when you start to become frustrated, and on the days you do get upset, take the time to discuss the situation with your child after you’ve calmed down so the 2 of you can brainstorm strategies to ensure history doesn’t repeat itself.\nChallenge fearful thoughts. When your child starts to feel anxious and/or you catch him or her engaging in compulsive behavior, take the time to talk it through and challenge the thoughts and feelings he or she is having so you can change the conversation. Your child’s therapist will likely have strategies to help you do this based on your little one’s specific fears and worries, so I won’t go into too much detail here, but this article on Anxiety Canada is a great read.\nUse positive reinforcement. I talk about positive reinforcement A LOT when writing about parenting topics, and for good reason! Reinforcement is a fabulous technique to use when a child is demonstrating problematic behaviors such as compulsions and rituals, and while both positive and negative forms of reinforcement can help teach children self-control, research tends to suggest that positive reinforcement – the act of rewarding a child when he or she completes a desired behavior as a means of increasing the likelihood he or she will repeat the behavior again – is the most effective. Sticker charts are a simple, yet effective, form of positive reinforcement that can be extremely motivating for kids, and I encourage you to work with your child’s therapist to develop a good behavior chart so you know which behaviors you should be rewarding.\nRELATED: 28 Reward System Tips & Templates\nDiscuss exposure therapy with your child’s doctor. For those who aren’t familiar, Exposure Therapy (ET) is a specific type of cognitive behavioral therapy that can be very helpful to those who struggle with a specific fear or phobia. It’s usually performed over a series of 12-16 sessions, and is designed to challenge the ‘escape/avoidance’ behaviors that maintain and intensify the phobias behind a specific fear. ET is typically performed by a trained exposure therapist, but it’s worth having a conversation with your child’s therapist to determine how you can support this type of treatment at home to ensure you are complimenting rather than hindering your child’s progress.\nBe there. My last tip for those who want to know how to help a child with OCD at home is to simply be there. OCD is a very real, isolating, scary, and difficult thing for anyone to deal with, let alone a small child. It’s important for your little one to have a safe place where he or she can be open and honest, and that he or she knows you are always on his or her side.\nIf you’re looking for tips to help you figure out how to help a child with OCD at home, I hope the ideas in this post prove useful to you. Remember that acceptance is the first step towards any successful treatment plan, and never underestimate how big a role you play in your child’s recovery.\nThis post contains affiliate links.\nIf you found this collection of tips to teach you how to help a child with OCD at home helpful, please share this post on Pinterest!\nAnd if you’re looking for more parenting tips and tricks, please follow our Parenting board where we share all kinds of helpful information we find each day."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:649dd655-1d02-4fb4-aafc-a8fc12ded6b4>","<urn:uuid:09f74627-fd76-49ce-813f-0de0fc366ce5>"],"error":null}
{"question":"¿Cuáles son las principales características que definen a un valor (value) en programación?","answer":"A value has several key characteristics: 1) It is immutable by definition - once created, it cannot be changed, 2) It forms a conceptual whole where all attributes together define its complete measure or description, 3) It has no identity - it is completely defined by its attributes, 4) It has equality based on all its attributes - if all attributes match between two values, they are considered equal, and 5) It may have intrinsic ordering based on magnitude or other criteria, though ordering is not essential.","context":["A value is something intangible and immutable that measures, quantifies or describes a thing.\nIN TRANSIT (for the status of a parcel in the logistics domain). You can easily infer the meaning of those values because their syntax are widely used and known. When exploring a domain while talking with an expert you can also found a lot of those values. I use to say that a typical domain would have 3/4 of values vs 1/4 of entities.\n10 EURis an instance of an\n2 Kgis a\n54 mis a\nIN TRANSITis a value of a\nParcel Delivery State.\nA value is useful in a use-case\nCharacteristics of a Value\nImmutable by definition: after the value is created, no mutation can change it (it's not a variable, operations on value produce new value in return).\nConceptual Whole: Only together do all the attributes form the complete intended measure or description.\nE.g.: 10 €, 20 ℃, 50 ㎏. A value is completely defined by its attributes.\nNo identity: Just stating the value with all of its attributes is sufficient to grasp its meaning.\nEquality: If all the attributes of two values are equals then the two values are considered equals. It's a smell to have equality defined by only a subset of attributes.\nIntrinsic or natural ordering: Ordering may be based on magnitude, lexicographical or ordinal criteria. Be careful about imposing an ordering on unordered types. Ordering is not an essential feature of value type.\nConstraints: Value types reflect constraints.\nE.g. Amount quantity of currency is only positive (no negative amount can exist).\nA phone number has min and max digits.\nAn ISBN has 13 digits (with the last digit being a check digit).\nDon't change the value!\nReturn new value:\nE.g.: 100 m + 50 m -> 150 m,\nE.g.: 10 € + 20 € = 30 €\nCan derive new value:\nE.g.: converting speed from m/s to km/h (10 m/s -> 36 km/h)\nE.g.: converting Euros in Dollars given a conversion rate. The Rate is itself a value, composed of two currencies (from, to), an interval of validity (a value with start and end instants) and a ratio (numerator and denominator).\nCan extract details:\nE.g.: extract the country dialing-code prefix from a phone number\nCan format the value and return a string\nCan construct a value and verify its constraints: constraints MUST be enforced at the value creation, not afterwards. The value constructor can be deemed as transactional: the value constraints are respected and the creation can occurs, or they are not met and the value DOESN'T EXISTS AT ALL!\nSystem of Values\nValue Type should be built with composition of other value types. The operations, relationships and constraints of values form a system of values.\n|A positive number and the Meter unit\n|Three positive numbers between 0 and 360 (for Degree, Minutes and Seconds) if degree of arc unit is used\n|48° 51' 23.81\"\n|Distance from center, Latitude ArcDegree, Longitude ArcDegree\n|(6 371 km, 48° 51' 23.81\" North, 2 21'7.99 East\")\n|A positive number and the Second unit\n|A Distance per Duration\n|A modification of speed per second (aka. derivative)\nValue Type design tips\nConstruction should always result in a meaningful and correct value: Partial initialisation is always problematic. The value constructor must be transactional: it returns a correct value or throws an exception.\nRespect and enforce state invariant A value\nComparison and Equality is a fundamental concept of a value and an\nequalsoperation should always be provided. Total ordering may apply or not depending on the concept (E.g. an amount can be compared and sorted but not a phone number)\nValue Type Design Smells\nAnemic Value Type: no or very few behavior beyond attributes access\nRole Creep: too much responsibilities and external dependencies\nNon-meaningul constructor: constructor without useful enforcement of constraints or sufficient quality of failure\nMemory Representation and Serialization\nvalue-of: this function takes an input string and returns the in-memory representation of the value.\nto-string: this function takes an in-memory representation and returns a string. Of course\n(= v (value-of (to-string v)))and\n(= s (to-string (value-of s)))as\nto-stringare inverse functions (operations that undo each other).\nEquality and Hashcode\nhashcode are two additional useful operations.\nequals: must operates on all the attributes of the value\nhashcode: should be coherent with the value, equal value should have identical hashcode\ncompare function with two arguments (x and y), returns a negative number, zero, or a positive number when x is logically 'less than', 'equal to', or 'greater than' y.\nValue Object in an Object-Oriented Language\nTransactional Constructor: the value exists and is correct or it doesn't exist at all, there is nothing in between.\nVery strict encapsulation of the attributes: no mutation of any attributes is allowed to keep the value immutable.\nNo mutation methods: the methods exposing behavior returns a new value and never change the existing attributes.\nBrings the language of the domain into the code: Value types add the domain terminology into the codebase.\nis a Complexity Swallower: Compound value types swallow lots of computational complexity\nProgressive introduction: Value types can be added progressively into a codebase. E.g. even if some part of the code continue to use BigDecimal to represents amount, you can introduce\nis Testable: value type being immutable and with low coupling, they are very easy to unit tests.\nis Concurrency-safe: value type being immutable, the concerns of the same code being accessed simultaneously by multiple threads disappear.\nRelieve entities of complexity: especially all the surface control rules (\"does this value respects these constraints?\")\nComposes easily and improves extensibility\nValue Types is the DDD building block with the best Return-On-Investment! (ROI)."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:37054910-7359-4d19-a0e8-41b6ff047066>"],"error":null}
{"question":"How has the historical perception and preservation of both jazz and mangrove ecosystems evolved over time? Could you analyze both cultural and environmental shifts in these domains?","answer":"Jazz evolved from relying on the Great American Songbook's chord progressions to embracing modal approaches, as evidenced by 'Kind of Blue' in 1959, which has remained continuously in print as the best-selling jazz album. Similarly, mangrove conservation has shown significant improvement, with loss rates reducing by almost an order of magnitude between the late 20th and early 21st century. This positive change in mangrove preservation stems from greater public and government awareness, leading to increased investment and on-the-ground action, though challenges remain in Southeast Asia and West Africa where deforestation continues.","context":["While surfing the web the following item caught my eye….\nJimmy Cobb (January 20,1929 – May 24, 2020) drummer on Miles Davis’s ‘Kind of Blue,’ Dies at 91. He was the last surviving member of that landmark album’s sextet, he was a master of understatement, propelling his band mates with a quiet persistence.\nIt was this “blast from the past” that prompted me to revisit the album. Most Jazz fans “have the moment” imprinted on their memory of when they first heard KIND OF BLUE. For me it was during a lunch time break in a record store (remember those) in down town Sydney, Australia. In those days there were head phones or listening booths available to check out the latest releases. The radio in those days was awash with top Forty Tunes and Jazz wasn’t all that popular. Apart from late night smooth jazz radio to get one’s jazz fix you had to get it when ever and where ever you could. For me it was those lunch times listening sessions in a record store. The opening track on the album, SO WHAT, became my all time favorite Jazz composition and performance. Here is that opening track from the classic album followed by a live clip from a TV show.\nWhile the tune in both instances is the same a discerning ear can detect distinct differences between the performances. The first clip from the recording is the classic Miles Davis Sextet of Miles Davis (trumpet), John Coltrane (Tenor Sax), Julian Adderley (Alto Sax), Paul Chambers (Bass), Jimmy Cobb (Drums) and Bill Evans (piano). The contours of the solos played by John Coltrane (tenor sax) and Miles Davis (trumpet) in both clips are similar but demonstrate the variety available within jazz performances of the same material.\nThe album KIND OF BLUE was recorded recorded on March 2 and April 22, 1959, at Columbia’s 30th Street Studio in New York City. It was released on August 17 of that year by Columbia Records and in the past 60 years has never been out of print. It is regarded as the best selling jazz album of all time and because of its unique approach to composition and performance it has been deemed as one of the most influential records of all times. On this album Davis followed up on his modal experimentation on his earlier Milestones album. By basing Kind of Blue entirely on modality he departed even further from his earlier work in the hard bop jazz style.\nWhy the recording is so important in the Jazz repertoire is that it was a radical departure from the way jazz musicians normally approached performances. Throughout the early history of Jazz up until the 50s and even later, the main stay of the Jazz repertoire was what was called THE GREAT AMERICAN SONG BOOK. It was a standard repertoire filled with the songs of Cole Porter, George Gershwin etc. Generally a performance of these songs included an instrumental statement of the tune, sometimes with variations followed by individual solos by various instrumentalists. The underlying chord structures and melody line were the basis for the improvisations that applied time honored musical devices to shape individual performances. The songs may have had mundane lyrics (moon, June, love, spoon, etc) but the melodies and the harmonic structures were (are) pretty sophisticated. Jazz musicians often ditched the standard melody and made up ones of their own. Sometimes they just used the chord progressions and came up with completely different compositions. KIND OF BLUE changed that. Instead of using chord progressions for the improvisations Miles Davis came up with a Modal approach. It was no longer necessary to play in a specific key, rather the composer could dictate a series of modes to act as basic scales for the improvisation.\n“So What is one of the best examples of modal jazz music. Although improvisation takes up the majority of the piece, it does have a compelling riff that sets the piece in motion and sets up the stage harmonically for the improvisations. This riff is notable in that involves the interplay between the upright bass and the rest of the band. The antecedent phrase is played by the bass, which plays an ascending line of notes that begin with a fourth leap starting from the root note. This is followed by the “response” by the piano or rest of the band, which consists two chords that move in parallel motion downwards in answer to the bass. These chords are a whole step apart and are made up of a root, fourth, minor seventh, minor third, and fifth. The second chord-and final statement of the phrase-is an altered minor chord. This establishes the harmonic center of the piece.\nHarmonically speaking, this piece is fairly simple. It is centered around the D Dorian mode, and there are no harmonic progressions other than the modulation from D Dorian to Eb Dorian, which occurs throughout the piece. The piece follows a 32 bar AABA structure, both during the melodic line and during the solos. This translates to 16 bars in D Dorian, 8 bars in Eb Dorian, and 8 bars again in D Dorian. The piece begins with a piano and bass opening with a slower tempo than the rest of piece. After this bass and piano alone play the melodic line with the drum as accompaniment. The drums serve to get the atmosphere going with a laid back, ‘cool’ atmosphere. The other instruments join in and after one chorus, each performer takes an extended solo in the following order: trumpet, tenor sax, alto sax, and piano. After the solos, the melody line is played for a chorus. The piece ends with the melody being just played with the bass and piano (with drums for accompaniment) before fading out.\nThe harmonic simplicity of So What gives the instrumentalists a certain freedom in their improvisations not found in other forms of jazz music. The differing creative approaches are evident in each of the different solos; for example, Miles Davis’ solo can be characterized as very melodic which is mainly focused on thoughtful phrasing whereas Coltrane uses a harder and often scalar approach, playing faster and leaving less space between his phrasings. Despite this, the atmosphere throughout So What remains mostly unchanged thanks to the vamping of the rhythm section and the careful upholding of the structure of the piece. The composition and the performance is a Jazz Masterpiece. Miles Davis was famous for approaching recording sessions with no set agenda. Just a sketch of some scales or chord progressions to be played with very loose instructions to the participants about tempo, structure and what he wanted to achieve. KIND OF BLUE and SO WHAT conform to Miles’ general approach to recording. In his later electronic explorations (BITCHES BREW, etc) he even took it further using the recording studio as a compositional tool. Literally editing, cutting and pasting and shaping the final product (I find it hard to call it a performance) to his compositional needs. Miles never dwelled on his musical past and in later years when asked about the recording he tended to be dismissive of the effort and basically had the attitude “been there done that and I have moved on”.\nModal Jazz, in some ways reaches back to earlier classical and folk music ways of playing music. It did not replace the time honored Great American Song Book, rather it opened the door to different ways of composing, playing and improvisation. FREE JAZZ, a later development in jazz performance , was another way of organizing (some would say disorganizing) the music …. no prepared structure, no set key, rhythm etc. Here in 2020 it has been around for 50 years and while is still has a significant following it remains controversial.\nI have this uncalled, and I dare say sometimes unwelcome urge to educate my peer musicians in some of the finest recordings out there. I sent these clips out to friends and one response astounded me. The composition and performance was described as and interesting “song” and it kind of illustrates the difficulties modern audiences have with instrumental music. Calling SO WHAT a song is like calling Beethoven’s A minor STRING QUARTET #15 a song. We are all used to listening to “songs” but most of us have little or no educated experience with listening to instrumental compositions. As a result a large percentage of audiences have no sign posts to help them understand the music. Instrumental music is about the architecture of the piece; the use of melody, harmonic invention, rhythm and variations within all of those elements. Songs, as typified by the normal singer/song writer, and instrumental compositions in the Jazz and Classical traditions operate at two different levels and there is no way to really compare the two. Songs tend to be (not always of course) factual and concrete and generally touches our humanity with portrayals of every day circumstances and emotions. Instrumental music on the other hand tends to be more abstract and puts us in touch with music at a more mystical level.\nEach musical style or school has a specific, and often unique, way of composition and performance. For instance, Arab and Middle Eastern music is based on completely different concepts of harmony, melody and rhythmic rules to western music. To understand and appreciate that music requires a re-education in the rules of the game. Similarly, Northern and Southern Indian Classical Indian that, to some extent , came from the Arabs is different again. In fact Northern and Southern Indian traditions are sufficiently different to require another re-think when moving from one tradition to the other.\nCloser to home, Celtic Music is based on time honored airs and dance tunes with a large component of modal methods and a different feel to the music. Blue Grass had its origins in Celtic music but the feel is different. To my ears Blue Grass musicians do terrible things to Celtic tunes. At a Celtic music session in Dublin I once asked my daughter in law what she thought of the music. Her response was “it all sounds the same to me”. For most people that is the response to most, if not all, instrumental music …… “It all sounds the same to me”.\nBut with a little bit of effort it does not all have to “sound the same to me”\nPost Script. Over the years there must be thousands and thousands of words examining, defining and analyzing the album KIND OF BLUE. One book of note that I can recommend is KIND OF BLUE – THE MAKING OF THE MILES DAVIS MASTERPIECE, by Ashley Kahn, Da Capo Press Books, 2000.","More than a decade ago, a team of mangrove experts warned that mangrove forests were being lost faster than almost any other ecosystem, including coral reefs and tropical rainforests. Nonetheless, the situations are getting better now. An international team of 22 researchers led by National University Singapore (NUS) from 24 institutes including Dr Stefano Cannicci from The Swire Institute of Marine Science and School of Biological Sciences, the University of Hong Kong (HKU), found that there is now cause for optimism, with the global loss rate of mangrove forests less alarming than previously suggested.\nAfter studying various earlier presented works, the team found that globally, mangrove loss rates have reduced by almost an order of magnitude between the late 20th and early 21st century – from what was previously estimated at 1% to 3% per year, to about 0.3% to 0.6%. The drastic drop in the loss rate is attributed greatly to the successful mangrove conservation efforts. This heightens conservation optimism amongst broader projections of environmental decline.\nA commentary summarising the team’s findings was published in the scientific journal Current Biology. This international effort was the result of the Fifth International Mangrove Macrobenthos and Management conference (MMM5), the world’s largest mangrove conference which was held for the first time in Singapore last year.\n“The team deduced that the reduction in mangrove global loss rates has resulted from improved monitoring and data access, changing industrial practices, expanded management and protection, increased focus on rehabilitation, and stronger recognition of the ecosystem services provided by mangroves,” explained Associate Professor Daniel Friess from the Department of Geography of NUS.\nDr Cannicci from HKU said: “I am very proud of being one of the academics who warned about world mangrove loss a decade ago, since I think that the paper published at that time contributed to a turn the tide on mangrove degradation. There are still dangers and we still have to manage and preserve mangroves worldwide, but the perception about their importance for humankind has definitely changed. In particular, a recent survey held in Hong Kong by me showed that mangroves area in Hong Kong has increased in the last 20 years and that these ecosystems are huge reservoirs of crab, mollusca and insect diversity.”\nDr Cannicci said the data discussed in the present paper strongly support the need to implement good practices in conservations, also for Hong Kong mangroves. He remarked: “Although small and limited in size, HK mangroves harbour a magnificent diversity of plant and animal species. Eight species of trees, 53 species of crabs and 42 species of snails have been recorded in these forests, which is more than is known for the mangrove forests of the whole African continent.”\nAfter years of degradation, mangroves in Hong Kong now cover about 350 ha (Hong Kong Island has an area of about 7859 ha). Distributed across about 40 forests, they represent the largest remaining mangrove patch within the Pearl River Delta, with the largest one distributed along the coast of Deep Bay. There are also many small, but pristine patches in the North East New territories and a small one located on Hong Kong Island.\nPositive change in mangrove conservation\nMangrove forests occur along the shorelines of more than 100 countries and are incredibly important as they offer numerous critical benefits to people, including protection from coastal erosion and storm as well as cyclone damage, natural filters for pollution and sediment, carbon sequestration which helps to mitigate climate change, and provide millions of people with products such as fuelwood, construction materials and fisheries resources, since mangroves act as nursing grounds for many coastal fishes.\n“There is strong evidence that positive conservation change is occurring. Mangrove conservation has gained substantial momentum, with greater public and government awareness leading to investment and on-the-ground action. However, despite recent mangrove conservation successes, tempered optimism is necessary, as conservation gains are not evenly spread, nor guaranteed into the future,” cautioned Associate Professor Friess.\nMangroves under threat in emerging deforestation frontiers\nThe team found that mangroves continue to be threatened by aquaculture, agriculture and urban development across the world, particularly for new deforestation frontiers that are emerging throughout parts of Southeast Asia and West Africa. Southeast Asia is a traditional hot spot of mangrove deforestation as mangroves are cut down to make space for aquaculture ponds, cleared for rice paddy cultivation, and reclaimed for industrial and port development.\n“Emerging deforestation frontiers need to be addressed early. Improved environmental governance and increased public intervention can help secure positive conservation outcomes in these locations. We need to take decisive steps to improve the success and scale of mangrove rehabilitation, and increase the resilience of mangroves to sea-level rise to maintain the current progress in mangrove conservation,” commented Dr Candy Feller from the Smithsonian Environmental Research Center, one of the contributors to the study.\nMaintaining current progress in mangrove conservation\nEven mangrove rehabilitation is lauded as a method to offset historical as well as ongoing losses and can yield long-term ecosystem service provision, the team found that successful rehabilitation is still a challenge to achieve at scale. Current mangrove rehabilitation projects around the world can fail because key ecological thresholds and rehabilitation best practices are ignored – for instance, planting in low-intertidal locations that are not suitable for mangrove growth and using non-native species that can quickly become invasive, which gives rise to myriad ecological impacts on the intertidal zone.\n“Ensuring that mangrove conservation gains are not short-lived will require continued research, policy attention, and renewed efforts to improve the success of mangrove rehabilitation at a scale that will be ecologically impactful,” said Dr Erik Yando, Research Fellow at the NUS Department of Geography.\nTo ensure that the mangrove conservation gains are not short-lived, continued research, policy attention, and renewed efforts to improve the success of mangrove rehabilitation at a scale are required. The international team will continue to monitor mangrove deforestation and conduct studies to assess the benefits and values of mangroves in Southeast Asia.\n(The press release is adapted from the press release of NUS)\nImage download: www.scifac.hku.hk/press\nThe article: “Mangroves give cause for conservation optimism, for now” in Current Biology\nLink of the article: https://www.cell.com/current-biology/fulltext/S0960-9822(19)31687-2\nFor more information about Dr Stefano Cannicci’s research, please visit: https://www.imeco-lab.com/\nFor media enquiries, please contact Ms Cindy Chan, Assistant Communications Director of Faculty of Science (Tel: 3971 5286; email: email@example.com) or Dr Stefano Cannicci from The Swire Institute of Marine Science and School of Biological Sciences, The University of Hong Kong(email: firstname.lastname@example.org).\n團隊把研究結果比對之前的數據，發現20世紀末至21世紀初，全球紅樹林減少的速度減慢了大約十倍，由之前估計每年減少1%至3%，下調到0.3%至0.6%。研究結果已於學術期刊 《當代生物學》（Current Biology）發表。\n新加坡國立大學地理學系副教授 Daniel Friess解釋研究結果說：「專家小組推斷，全球紅樹林減少的速度放緩，是由於多方面的因素，包括在監察和數據搜集上有所改善，工業作業方式的轉變，林木管理及保護工作得以擴大，人們對修復保育的關注增加，以及對於紅樹林對生態系統的貢獻的認知加强等，都有積極正面的影響。」\n研究團隊成員、 Smithsonian Environmental Research Center 的Candy Feller 博士表示：「有關機構應盡快停止在紅樹林的開發行為並加強環境管制，希望可以為這些地區的保育帶來正面影響。除此之外，我們應該採取有效手段去修復紅樹林，提升它們對水位上升的防禦力。」\n有關文章: “Mangroves give cause for conservation optimism, for now” —- 《當代生物學》\n有關Stefano Cannicci 博士的研究可參閱以下網址：https://www.imeco-lab.com/"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:95fe39f0-71d4-4efc-b1e8-982092d8580b>","<urn:uuid:02e627f7-6d61-484f-b2ca-6010a0285dbd>"],"error":null}
{"question":"What role does the FAA play in regulating both model rocketry and general aviation safety?","answer":"The FAA is involved in both activities through different regulatory mechanisms. In model rocketry, special FAA permission in the form of waivers is required for high-power rocket launches, which clubs obtain to allow their members to launch at events. In general aviation, the FAA establishes regulations that pilots must follow, with the agency's rules being fundamental to aviation safety. The FAA's role isn't to restrict activities but to ensure safety - contrary to anti-authority attitudes that some might have, their rules are specifically designed to keep all participants safe in both domains.","context":["In addition to the brief Introduction to Rocketry below, this site includes information on local rocketry clubs, other web sites with additional information about rocketry, places to shop locally and online for rocketry-related products, and recent Central Florida rocketry news.\nWhat Is Rocketry?\nHobby (or sport) rocketry is the exciting, safe, and education activity of building and flying rockets. The hobby is divided into two main divisions, Model Rocketry and High-Power Rocketry.\nModel rockets are constructed of light-weight materials such as cardboard, plastic, and balsa wood.\nModel rockets are launched using small, commercially-made rocket motors (sometimes called engines). Although each motor can only be used once, the rockets are reusuable. They usually return to earth under a parachute or streamer and can be flown again by replacing the used motor with a new one.\nA model rocket motor is ignited with an electrical system called a “launch controller.” Pressing the launch button on the controller sends an electric current to an igniter installed in the rocket motor. The igniter heats up, igniting the rocket motor’s propellant and sendng the rocket skyward. Using a launch controller allows you to precisely control when the rocket launches, remain a safe distance from the rocket at launch, and be far enough away that you are able to watch the rocket in flight.\nMost model rocket motors include a delay and ejection charge. The delay allows the rocket to coast for a few seconds after the motor’s propellant has been exhausted. Then the ejection charge fires out of the top of the motor to cause the rocket’s parachute or streamer to deploy so that the rocket is returned to earth safely and without damage.\nGetting Started Flying Model Rockets\nThere are two good ways to begin.\nOne way to get started is to purchase a model rocket launch or starter set from a local store or online. An example is the Estes Tandem-X Launch Set pictured below. Like most launch sets, it includes a launch pad, launch controller, and a rocket.\nThe Tandem-X set actually includes two simple-to-build rockets. To build the rockets you’ll need some tools and supplies such as white glue, plastic cement, a ruler, a pencil, sandpaper, and a craft knife (such as an Xacto knife).\nThe motors you need to launch the rockets are not included, but you can also purchase them locally or order them from Amazon.com. You will need B4-2 motors for the larger “Amazon” rocket and A8-3 motors for the smaller “Crossfire” rocket. You’ll also need a package of recovery wadding, which is used to protect the parachute from the hot particles in the motor’s ejection charge, and four AA batteries to power the launch controller.\nAfter you’ve built the rockets you’ll need to find an open place where you can safely and legally launch the rockets. Carefully follow the directions including with the launch set as you prepare and launch the rockets.\nThe second way to get started is to build a rocket kit, purchase motors and recovery wadding for it, then bring them with you to a local rocketry club launch. You can start with a launch or starter set, but most clubs have launch pads, so you buy and build a simple kit as your first rocket. For your first rocket, you’ll want something easy to build. Look for a kit labeled “Ready-To-Fly” or “Almost Ready to Fly” if you are more interested in quickly having a rocket ready to fly than building one. Look for a kit labeled Skill Level 1 if you would like to gain some experience building a model rocket kit.\nFlying your rocket at a club launch has several advantages. You don’t have to worry about finding a large open area and getting permission to use it. You get to meet others who build and fly rockets. You can ask questions that others will be happy to answer. And, you get to watch the launch of many more rockets.\nHigh-power rockets are larger, more powerful rockets which are often constructed using stronger materials such as fiberglass and plywood.\nThese are the large rockets you may have seen on TV shows on the Discovery Channel and other cable networks. The TV shows often portray the flyers of these “large and dangerous rocket ships” as being wild and crazy. The reality is, however, that high-power rocketry has an outstanding record of safety because everyone involved is committed to following the rules to keep the hobby safe and legal.\nMost rocketeers flying high-power rockets use commercially-made motors. These motors, however, are usually reusable with just the propellant being replaced between flights.\nMany high-power rockets use electronic altimeters to deploy the rocket’s parachute. Since these rockets may fly to altitudes of several thousand feet, most use a recovery method called “dual deployment.” At apogee, the highest point in the rocket’s flight, the altimeter fires an ejection charge to deploy a small parachute called a drogue. The rocket falls quickly until the altimeter has determined that it has reached a specific altitude such as 700 feet. At that time, the altimeter sets off a second ejection charge to deploy a larger parachute which slowly lowers the rocket to the ground. Because the rocket falls faster under the smaller drogue parachute, it drifts less, reducing the distance the rocketeer has to travel to recover the rocket.\nSpecial permission from the FAA is required to launch high-power rockets. Rocketry clubs hosting high-power launches obtain a “waiver” from the FAA allowing their members to launch high-power rockets at club events. There are several rocketry clubs in Central Florida which support the flying of high-power rockets.","There is a truism in aviation that flying requires safety and there is nothing funny about safety. While stern in appearance, this is not the same as saying there is no fun in flying. If there weren't, why would we pursue flying in the first place? Ask any pilot and you get stories of flying adventures, feelings of freedom in the air, and pure \"joie de vivre\" through flight. Safety, and the procedures required for it, are part of the aviation process. Redundancies are part of the pre-flight check, general aviation safety and FAA regulations.\nAny flight, whether a short hop to an adjoining airfield, or a long jaunt cross-country, requires a pre-flight check. Too often, pilots become complacent and do a perfunctory pre-flight. This is a dangerous attitude and one that needs to be checked at the door. Make a list for your aircraft and stick to it, each and every time. The pre-flight needs to include sumping and checking the fuel, checking oil levels, feeling tightness on the belts, looking for any bends or compromises in the air frame, a full check of the landing gear, and going over the empennage and ailerons. The pitot tubes must be checked for debris and any \"remove before flight\" tags removed. This list is a basic outline, and with each different aircraft there may be more items to go over. The notion of \"familiarity breeds contempt\" changes to \"familiarity leads to laziness\" as many pilots may develop bad habits when carrying out pre-flight checks on familiar aircraft. For safety's sake do not compromise on the pre-flight check.\nPilot Attitudes Towards Safety\nThere are several attitude \"killers\" in aviation, and all of them need to be addressed, recognized and corrected if you wish to be a safe pilot who lives to fly another day. Ask any Alaska bush pilot and you hear the adage \"there are old pilots, and bold pilots, but there are no old and bold pilots.\" Having too much sense of bravado or invincibility leads to poor decisions and an over-inflated sense of skills. This is not to say you should not recognize or admit aptitude, but there is a fine line between competence and cockiness.\nThe Safety Killing Attitudes\nAmong the attitudes that contribute the most to declining safety are outlined below. Learn how to recognize these in yourself and fellow aviators so that you are able to correct them before you get into trouble:\nAnti-Authority: When an anti-authority stance is taken, \"cowboy\" attitudes envelope decision-making. Contrary to popular opinion, the FAA is not here to ruin our day, the agency's rules are there for a reason. In order to keep all of us safe, each of us is duty bound to follow rules.\nResignation: By taking this tact, you give up before you begin. Look around at most aviators and you see confidence and aptitude. However, when resignation appears, self-fulfilling prophecies swallow good piloting decisions.\nInvincibility: The idea you are not going to have any accidents or you possess some sort of magic aura that protects you from all things bad. Get over it. Bad things can and will happen to you with this attitude.\nRashness: Impulsive decisions lead to errors. Take the time to think out your moves and choices. Even in emergency situations, the difference between a one or two second breath and immediate reactions often leads to better decisions. Slow down.\nMachismo/Bravado: This is not limited to men only. This attitude has the potential to affect female pilot decisions as well. Ask yourself why are you up here flying. Is it to have bragging rights or to show off, or is it for the pure love of flying? Leave the ego at the door and just fly.\nIn Flight Safety\nWhen in the air, you need to be familiar with the many facets of safe flight. Knowing the proper altitude to fly at (are you flying east or west?), how to communicate on the radio, use a chart, and recognize lights and markers are essential (and part of any pilot's training).\nWhen in doubt, call and ask for help. There are many resources for safety while on the ground or in the air. Examples include:\nAircraft Emergency 121.5\nTalk to ATC\nKnowing how weather behaves, what the topography does to influence the weather and how the airplane reacts or is affected by weather is important for safe flight. Go through a full weather briefing before flights, using any ASIS, or weather resource available at your location. Charts have this information and with Internet empowered devices, pilots have access to weather and changing conditions at the tips of their fingers.\nPlan contingencies if mountain flying and go over the route prior to taking to the air. Foreknowledge is power and power is safety. Keep charts at the ready, or electronic devices programmed with the charts and left on the home screen.\nCome Home and Enjoy Safe Flight\nSafety is not boring or the realm of worriers. Safety is the key to fun flying. May the wind be on your tail and the sky blue.\n- FAA – FARS, 14CFR http://www.flightsimaviation.com/data/FARS/part_121-127.html\n- AOPA Safety http://www.aopa.org/asf/\n- FAA-AIM – Regulations http://www.faa.gov/regulations_policies/faa_regulations/\n- Aviation Knowledge http://aviationknowledge.wikidot.com/sop:hazardous-attitudes"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:c91b5d11-446f-432b-a9aa-fb39cbfea40b>","<urn:uuid:8b9cc92a-26e8-4590-9b6c-56de4ce0637f>"],"error":null}
{"question":"What's the origin of the Mexican word 'cacahuate' for peanut? 🤔","answer":"The Mexican word 'cacahuate' comes from the Nahuatl word 'cacahuatl' which means cocoa bean. Although peanuts originated in South America, they have been cultivated in Mexico since Prehispanic times.","context":["Home >> Salsas and Dips >>\nAncho Pepper Salsa\nAlthough the peanut is originally from South America, in Mexico it has been cultivated extensively since Prehispanic times to the present days. In fact the name cacahuate (peanut) comes from the Nahuatl word “cacahuatl\" that means cocoa bean.\nToasted, peeled, fried, with salt, with chili, with lime – the peanuts are one of the most popular snacks among the Mexicans. When they are fried and ground along with chilies and spices some extraordinary salsas are created like this one.\nThe combination of peanut with ancho pepper and other few ingredients generate this salsa that is moderately spicy and it is full of flavors and textures . Is perfect to accompany almost any preparation that has chicken. In fact you can cook some chicken breasts and when they are served bath them with this salsa and accompany the plate with white rice garnished with avocado strips. A delight!\nAbout the Recipe\nIn this recipe you are going to work a little but everything will come out fine.\nWhen the ancho peppers are bought pick the ones that are large and flexible because those are the ones that have the best flavor.\nNatural and peeled peanuts have to be used so the final flavor of the salsa will be the best.\nThe preparation will take you 20 minutes but you won’t work in the recipe all the time.\nThis recipe is for 2 cups.\nIngredients 1 cup of Peanuts (3.5 oz)2 Ancho Peppers (.7 oz)2 Tomatoes (14.1 oz)1/2 Onion (2.6 oz)2 Garlic cloves (.2 oz)\n1/2 teaspoon of Cinnamon\n1 cup of Chicken Stock (8.4 fl oz)1½ tablespoons of Olive Oil1/2 tablespoon of Salt\nCookware 1 Saucepan1 Frying Pan1 Griddle1 Blender1 Bowl1 Cutting Board1 Tongs1 Turner1 Knife\nDirections Roast the Chilies\nRemove the seeds and the veins of 2 ancho peppers.\n||If you want a more spicy salsa don’t remove the seeds nor the veins of the chilies.\nPut on a griddle over medium heat the 2 ancho peppers with the ones you worked and roast them for about 2 minutes, until all their sides are lightly roasted; move them regularly with some tongs.\nTransfer from the griddle to a bowl, with the tongs, all the ancho peppers that were roasted.\nCover the ancho peppers that were roasted with boiling water and leave them soaking for about 5 minutes, until they are soft.\nWhile the Chilies Soak\nHeat in a frying pan over medium heat 1/2 tablespoon of olive oil.\nPut in the frying pan 1 cup of peanuts and fry them for about 40 seconds, until they are lightly fried; stir regularly.\n- Transfer from the frying pan to a blender the peanuts that were fried and reserve.\nBlend and Cook\nVerify that the ancho peppers that were left soaking are soft, if not leave them soaking for a longer time.\nTransfer from the bowl to the blender, with the tongs, the ancho peppers that were soaked. Also incorporate in the blender:\nBlend very well the ingredients and reserve.\nHeat in a saucepan over medium heat 1 tablespoon of olive oil.\n- Add in the saucepan:\n- The Mixture that was blended.\n- 1/2 teaspoon of Cinnamon.\n- 1/2 tablespoon of Salt.\nMix the peanut and ancho pepper salsa and cook it for about 10 minutes, until it thickens a little and acquires a more intense red color; stir occasionally.\nSoft and Exquisite\nVariation: The peanut can also be combined with Poblano pepper – another wonderful combination of chili with peanut.\nDid you like the Peanut Salsa?\nMexican Recipes with Chicken\n☂ Chicken Burrito\n☂ Chicken Adobo\n☂ Fried Chicken Tacos\n☂ Chicken in Green Pipian\n☂ Green Enchiladas with Chicken"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:58ec2cf1-f3a4-4f63-8871-e60554847e94>"],"error":null}
{"question":"What's the main difference between LLP and regular partnership firm liability?","answer":"In a Limited Liability Partnership (LLP), the partners' liability is limited to the capital amount they agreed to introduce. In contrast, in a regular partnership firm, partners are jointly and severally liable to pay all the debts of the Partnership Firm, meaning their liability is unlimited.","context":["What is Partnership Firm?\nPartnership firm represents a business entity that is formed with a purpose of making a profit from the business. Two or more parties come together with a formal agreement (known as Partnership Deed) to own and manage the business. The risk and responsibilities are shared amongst the partners that shred the burden of an individual partner. Also, when two comes together, more capital and expertise are combined that helps to reach the business goal(s) easily.\nPartnership Act, 1932 defines the structure of a Partnership firm by providing all the necessary provisions to run the same. The Act validates both registered and unregistered partnership firms in India. However, an unregistered partnership has few shortcomings that attract partners towards Partnership Firm Registration. But, one can overcome it by registration firm anytime after it is formed.\nBenefits of Partnership Firm\nDocuments required for formation of a Partnership Firm\nHow to choose a name?\nEstablish Partnership in 3 Easy Steps\nProcess to establish Partnership Firm\nCompare Related Services\nCompare different business structures to choose the right entity type\n|Private Limited Company||One Person Company||Limited Liability Partnership||Partnership Firm||Proprietorship Firm|\n|Act||Companies Act, 2013||Companies Act, 2013||Limited Liability Partnership Act, 2008||Indian Partnership Act, 1932||No specified Act|\n|Registration under the Act is mandatory to set up business as a Private Limited Company||Registration under the Act is mandatory to set up business as One Person Company||Registration under the Act is mandatory to set up business as a Limited Liability Partnership||Both registered and unregistered partnerships are legal, but registered entity is preferred||There is no registration criteria prescribed. But, registration to establish a legal identity is recommended|\n|Number of members||2 – 200||Only 1||2 – Unlimited||2 – 50||Only 1|\n|Requires minimum 2 and not more than 200 shareholders||Only an individual,and an Indian resident can be the shareholder||No bar on maximum number of partners, but minimum 2 Designated Partners are required||It is formed with minimum 2 partners, but not exceeding 50||The proprietor is the only owner of the firm|\n|Separate Legal Entity||Yes||Yes||Yes||No||No|\n|Private Company is separate entity and can own assets in its name||OPC is separate entity and can own assets in its name||LLP is separate entity from partners and can own assets in its name||Partnership firm does not have any separate identity from its partners||Proprietor and business are the same and not different|\n|Liability of members is limited to the extent of unpaid value of shares subscribed||Liability of member is limited to the extent of unpaid value of shares subscribed||Liability of partners is limited to the capital amount agreed to introduce||Partners are jointly and severally liable to pay the debts of the Partnership Firm||Proprietor’s liability is to pay-off all the debts and obligation of a firm|\n|Statutory Audit||Mandatory||Mandatory||Dependent||Not mandatory||Not mandatory|\n|Auditor must be appointed within the 30 days of incorporation||Auditor must be appointed within the 30 days of incorporation||Applicable when turnover exceeds INR 40 Lakh or contribution exceeds INR 25 Lakh||Statutory audit not applicable. Tax audit may be applicable based on turnover||Statutory audit not applicable. Tax audit may be applicable based on turnover|\n|Shares can be transferred with the consent of other Shareholders||Shares are not transferable easily||Ownership can be changed with consent of other partners||Ownership is not transferable easily, clause of partnership deed should be referred||Firm is no different from proprietor and so ownership is not transferable|\n|Change in members or director does not affect the existence of Private Company||Change in members or director does not affect the existence of OPC.|\nThe nominee will take place of member\n|Change in Partners or Designated Partners does not affect the existence of LLP||Change in partner leads to dissolution or formation of another partnership firm||Death or insolvency of proprietor directly affects the firm|\n|Foreign Participation||Allowed||Not Allowed||Allowed||Not Allowed||Not Allowed|\n|Foreign national are allowed to invest under the Automatic Route||Member, nominee and director must be an Indian resident||Foreign nationals are allowed, subject to FDI Guidelines||Foreign nationals are not allowed to be a partner||Foreign Nationals cannot commence proprietorship business|\n|Tax rate applicable for small companies is reduced to 22%||Tax rate applicable for small companies is reduced to 22%||With tax rate of 30% on business profit, tax benefits to partners is high||With tax rate of 30% on business profit, tax benefits to partners is high||Tax rates of individual applied to Proprietorship Firm|\n|Apart from Annual filings, it has to comply with various provision laid down, but less compared to public company||Apart from Annual filing, compliances are less compared to Private Company||Annual filing and few event based filings are necessary||Separate ITR of partnership is filed, else there is no filing requirement||No compliances and no requirement to file a separate ITR|\n|Know More||Know More||Know More||Get Started||Know More|"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:1f520ef5-4335-4138-ad67-5c2b3774d0ea>"],"error":null}
{"question":"When comparing renal cell carcinoma (RCC) and urothelial carcinoma, which one represents the most common type in its respective organ? 肾细胞癌和尿路上皮癌哪个在各自器官中更常见？","answer":"Both are the predominant types in their respective organs. Renal cell carcinoma (RCC) is the most common type of kidney cancer, developing in the proximal renal tubules that make up the kidney's filtration system. Similarly, urothelial carcinoma is the most common type of bladder cancer, making up more than 90% of all bladder cancers and starts in the urothelial cells that line the inside of the bladder.","context":["Kidney & Bladder Cancer\nBeacon Hospital Hotline\nKidney & Bladder Cancer\nKidneys & bladder are the major organs of the urinary tract, along with the ureters and urethra. The kidneys main function is to filter blood to remove impurities, excess minerals, salts, and water, thus producing urine. Urine is then stored in the bladder – a hollow organ in the pelvis – before it leaves the body during urination.\nCancer in the kidney is also known as renal cancer. The most common type of kidney cancer is renal cell carcinoma (RCC) which develops in the proximal renal tubules that make up of the kidney’s filtration system. According to the National Cancer Registry Report 2007-2011, kidney cancer accounts for 1.28% cancer cases in Malaysia.\nDue to the correlation between the kidneys and bladder, both types of cancer share similar signs and symptoms, such as:\n- Blood in urine\n- Pain or burning sensation during urination\n- Feeling the need to urinate many times\n- Lower back pain on 1 side of the body\nPeople with kidney cancer may also experience these symptoms:\n- Swelling of ankles and legs\n- A mass or lump on the side or back\n- Unexplained weight loss\n- Loss of appetite\n- For men, the cluster of enlarged veins, known as varicocele, around a testicle may present if the tumour has grown larger\nThe actual cause of kidney & bladder cancer is not known. However, a few risk factors are known to be associated with the development of kidney and bladder cancer, such as:\n- High blood pressure\n- Overuse of certain medications (diuretics and analgesic pills like aspirin & ibuprofen)\n- Smoking tobacco (doubles the risk of developing cancer)\n- Genetic predisposal (family history)\n- Chronic kidney disease\n- Workplace exposure\n- Chemicals like aromatic amines, which are sometimes used in the dye industry can cause bladder cancer\n- Substances like cadmium (a type of metal), herbicides, and organic solvents, particularly trichloroethylene\nStaging of Kidney & Bladder Cancer\nStaging of cancer helps the doctor determine the treatment course and the diagram below explains the meaning behind the numbering of cancer staging.\n- Blood and urine test – would be the preliminary diagnostic procedure to check for the number of red blood cells and to check on blood in urine, bacteria or cancer cells.\n- Cystoscopy – is a procedure that allows your doctor to examine the lining of your bladder and the tube that carries urine out of your body. A cystoscope equipped with a lens is inserted into your urethra and slowly advanced into your bladder.\n- Biopsy – to obtain tumour cells for examination under a microscope.\nMedical imaging technologies such as:\n- Computed tomography (CT scan),\n- Positron Emission Tomography – Computed Tomography (PET/CT Scan)\n- Magnetic resonance imaging (MRI)\nTreatment course will be determined by the oncologist upon obtaining the results of the diagnostic. Depending on the staging of cancer, the location and size of the tumour, treatment options may vary from case to case and inclusive of:\n- Targeted Therapy","Cancerous tumours of the bladder\nA cancerous tumour of the bladder can grow into nearby tissue and destroy it. It can also spread (metastasize) to other parts of the body. Cancerous tumours are also called malignant tumours.\nBladder cancer is often divided into 3 groups based on how much it has grown into the bladder wall.\n- Non-invasive bladder cancer is only in the inner lining of the bladder (urothelium).\n- Non-muscle-invasive bladder cancer has only grown into the connective tissue layer (lamina propria).\n- Muscle-invasive bladder cancer has grown into the muscles deep within the bladder wall (muscularis propria) and sometimes into the fat that surrounds the bladder.\nUrothelial carcinoma (also called transitional cell carcinoma) is the most common type of bladder cancer and makes up more than 90% of all bladder cancers. It starts in the urothelial cells that line the inside of the bladder (the lining is called the urothelium).\nUrothelial carcinoma can be found in more than one place in the urinary tract (multifocal). So if urothelial carcinoma of the bladder is diagnosed, doctors will check other parts of the urinary tract for cancer. This includes checking the renal pelvis, ureters and urethra.\nNon-invasive urothelial carcinoma\nNon-invasive urothelial carcinomas can be described as papillary or flat (sessile) based on how they grow.\nPapillary urothelial carcinomas look like small fingers and often grow toward the centre of the bladder (called the lumen). Non-invasive papillary urothelial carcinoma can be low or high grade. Papillary urothelial neoplasm of low malignant potential (PUNLMP) is the term used to describe a tumour when there is only a small chance that it will become invasive bladder cancer.\nFlat urothelial carcinomas are flat tumours found on the lining of the bladder. They are high grade and more likely to grow deeper into the layers of the bladder wall. Non-invasive flat urothelial carcinoma is more commonly called carcinoma in situ (CIS).\nInvasive urothelial carcinoma\nInvasive urothelial carcinoma has grown beyond the inner lining into the deeper layers of the bladder wall.\nSometimes invasive urothelial carcinoma has different types of cells mixed with usual urothelial cancer cells (called divergent differentiation). When this happens, the bladder cancer usually grows and spreads quickly (it is aggressive), and it is more likely diagnosed when it’s advanced. Squamous cells, gland cells and small cells are most commonly found mixed with urothelial cancer cells.\nThere are rare subtypes of urothelial carcinoma called variants. These subtypes usually grow and spread quickly and tend to have a poorer prognosis than the usual urothelial carcinoma. Variants of urothelial carcinoma are named based on how the cancer cells look under a microscope and include:\n- giant cell\n- poorly differentiated\n- clear cell\nRare cancerous tumours of the bladder\nThe following cancerous tumours of the bladder are rare and make up less than 10% of all bladder cancers.\nSquamous cell carcinoma\nSquamous cell carcinoma of the bladder is when flat squamous cells develop in the lining of the bladder. It is often associated with long-term (chronic) irritation or inflammation of the bladder. This irritation may happen from tubes (catheters) constantly being placed in the bladder over a long period of time, urinary stones or chronic urinary tract infections (UTIs).\nSquamous cell carcinoma is usually invasive and diagnosed at a later stage. It is usually treated with surgery and sometimes chemotherapy.\nAdenocarcinoma of the bladder starts in the gland cells of the bladder. It makes up less than 2% of all bladder cancers. Adenocarcinoma can spread to the bladder from another site (called secondary adenocarcinoma of the bladder). So doctors need to know where the adenocarcinoma started to make a proper diagnosis.\nThere are many subtypes of adenocarcinoma of the bladder, including mucinous, signet-ring and clear cell.\nAdenocarcinoma of the bladder is usually treated with surgery. A radical cystectomy is done to remove the whole bladder. Adenocarcinoma tends to come back so chemotherapy is also used to treat it.\nThe urachus (also called the urachal ligament) is a connection between the belly button (navel) and bladder formed during the development of a fetus. A fine ligament remains in adults, but it has no function. Tumours can develop along the urachus and can become cancerous. Urachal cancer usually develops where the urachus joins the top of the bladder.\nUrachal cancer is usually treated with surgery to remove the bladder, urachus, nearby lymph nodes and other surrounding tissue. Chemotherapy may be given after surgery.\nSmall cell carcinoma\nSmall cell carcinoma (small cell neuroendocrine carcinoma) is a type of neuroendocrine tumour (NET) that starts in the cells of the neuroendocrine system. Neuroendocrine cells are found in almost every organ of the body. Small cell carcinoma is usually a high-grade bladder cancer that grows and spreads quickly. Some people with urothelial carcinoma may also have small cell carcinoma.\nTreatment for small cell carcinoma of the bladder usually includes chemotherapy followed by surgery to remove the whole bladder. Radiation therapy may also be used.\nFind out more about neuroendocrine tumours (NETs).\nSoft tissue sarcoma\nCancer can start in the soft tissues of the bladder, such as the muscle, blood vessels or fat. It is called soft tissue sarcoma of the bladder or bladder sarcoma. Many people initially diagnosed with soft tissue sarcoma of the bladder actually have sarcomatoid urothelial carcinoma.\nThe main risk for soft tissue sarcoma of the bladder is having had radiation therapy for a different type of cancer 20 to 30 years earlier.\nLeiomyosarcoma (in adults) and rhabdomyosarcoma (in children) are the most common soft tissue sarcomas of the bladder.\nThe pathway that urine takes from the kidneys to the urethra.\nThe urinary tract includes the renal pelvis in the kidneys, as well as the ureters, bladder and urethra.\nThe centre part of the kidney where urine collects and is funnelled into the ureter (the tube that carries urine from the kidney to the bladder).\nA description of a tumour that includes how different the cancer cells look from normal cells (differentiation), how quickly the cancer cells are growing and dividing, and how likely they are to spread.\nGrades are based on different grading systems that are used for specific cancers. Some types of cancer do not have a specific grading system.\nThe process of examining and classifying tumours based on how cancer cells look and behave under the microscope is called grading.\nThe expected outcome or course of a disease.\nThe chance of recovery or recurrence.\nWhat’s the lifetime risk of getting cancer?\nThe latest Canadian Cancer Statistics report shows about half of Canadians are expected to be diagnosed with cancer in their lifetime."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:231a346e-4608-40b3-ad97-aedd1269cf39>","<urn:uuid:32e11e25-32ad-4724-8d94-28200a5eda7f>"],"error":null}
{"question":"When did Pacific Islanders first settle different regions? List timeline in chronological order.","answer":"Here's the chronological settlement timeline of Pacific Islands: 1) Around 4000 BCE: Bismarck and Solomon Islands east of New Guinea 2) By 1300 BCE: Fiji 3) By 1000 BCE: Tonga and Samoa 4) During this period: Micronesian islands (Mariana, Caroline, and Marshall Islands) 5) First millennium CE: Hawaii, Easter Island, and New Zealand 6) Specifically for Easter Island: Recent radiocarbon dating shows settlement around 1190-1290 CE","context":["I recently attended a thought-provoking lecture by Terry Hunt titled “Rethinking Easter Island’s Mysterious Past.” Dr. Hunt, formerly of the University of Hawaii and currently dean of the Clark Honors College and professor of anthropology at University of Oregon (my alma mater), has done extensive research in the archeology and environmental history of the Pacific Islands. For a dozen years Hunt has directed archeological research on Easter Island (Rapa Nui) where his team developed the astonishing theory that the famous giant stone statues had been “walked” to their positions around the island. National Geographic Magazine featured Dr. Hunt’s research as its cover story in the July 2012 issue, and a few months later PBS aired a Nova-National Geographic TV documentary that showed his team rocking a full-scale statue replica forward on Easter Island.\nIn the course of his research, Dr. Hunt has also revised the timing of the Polynesian settlement of Rapa Nui to about 1200 CE and called into question the cause of its demise in the 17th century. The traditional theory of migratory Pacific settlement (that I have taught, as a World Civilizations instructor) is that sea-faring Austronesian people first settled the Bismarck and Solomon Islands east of New Guinea by about 4000 BCE. By about 1500 BCE they had acquired the sophisticated maritime technology to sail greater distances, as well as food crops and domesticated animals to carry with them. Sailing in large twin-hulled canoes they fanned out through Oceania, the earliest probably heading eastward, reaching the Polynesian islands of Fiji by 1300 BCE and Tonga and Samoa by 1000 BCE. Other mariners set out eastward from the Philippines to settle the small islands and atolls of Micronesia including the Mariana, Caroline, and Marshall Islands (likely rejecting tiny Wake Island, devoid of fresh water or soil). These early mariners knew to follow seabirds and to watch for floating debris to help them find land. Meanwhile the Polynesians ventured farther into the eastern Pacific, to Tahiti and the Marquesas, and finally to the most remote reaches in the first millennium CE, requiring unbroken voyages of thousands of miles to Hawaii, Easter Island, and New Zealand.\nEstimates of the Polynesian settlement of Easter Island have ranged from 300 to 900 CE. The prevailing theory is that by 1200 CE the pressures of population growth and competition on Easter Island prompted an obsession with production of massive stone sculptures. Researchers speculated that now-treeless Easter Island had once been covered with millions of palm trees which were felled in a frenzy to provide the rollers, sleds, and structures to move and raise hundreds of statues around the island. Deforestation, famine, and civil war ensued causing population crash and bringing the Rapanui to cultural and environmental collapse by the time of the first European encounter in 1722. Jared Diamond, UCLA geographer and author of Collapse: How Societies Choose to Fail or Succeed (2005), argues that “Easter Island’s isolation makes it the clearest example of a society that destroyed itself by overexploiting its own resources,” an “ecocidal” warning to us all.\nBut Terry Hunt’s research has turned that scenario on its head. First, recent application of radiocarbon dating technology suggests a more recent and rapid Polynesian colonization of Rapa Nui around 1190-1290 CE. The settlers brought chickens, rats, and taro agriculture, though the island’s sparse resources would limit population growth to a few thousand. Over the next few hundred years deforestation resulted from agricultural transition and the fact that, with no predators, Polynesian rats multiplied and found a ready food source in palm seeds. As the population grew, rival clan groups competed by carving nearly a thousand moai – some as tall as 30 feet and weighing 75 tons – representing revered ancestors. The Rapanui transported about half of the statues miles from the quarries where they were carved to their coastal destinations where they mounted them on stone platforms, facing inward to watch over their descendants.\nBut the moai were not rolled on felled palm tree over the rugged terrain. Hunt and his team studied the base angles of the statues still in quarries and the fallen, broken moai across the island and experimented with alternatives until they achieved a breakthrough: tilted upright and roped at the head to three teams, one to each side and the strongest in the rear, the statue could be rocked back and forth, pivoting on its base in forward motion. At the platform the statue’s base was leveled and it was raised to straight upright position where it could keep its eyes on the people. In the 17th and 18th centuries European mariners arrived, bringing the catastrophic diseases that decimated native populations throughout the “new world.” Chile annexed the island in 1888.\nThere are many more fascinating details to this story, but one that caught my attention is that the Rapanui placed most of their moai on stone platforms (which also served as burial sites) located near vital fresh water sources that were exposed at low tide. Easter Island has a couple of volcanic lakes, but lacks permanent fresh water flows. To sustain life on the remotest of remote Pacific islands, the Rapanui marked and protected precious water sources with their most powerful defenders: their ancestors. The extended story of Rapa Nui’s prehistory can be found in Terry Hunt’s recently published book, co-authored with Carl Lipo, The Statues that Walked: Unraveling the Mystery of Easter Island (Free Press, New York, 2011)."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:a006e185-3eb5-4f23-84db-c8226751622a>"],"error":null}
{"question":"Which has a more elliptical shape - Earth or Mars?","answer":"Earth is more elliptical than Mars in terms of its shape. Earth has a flattening of around 1/298, corresponding to a difference between major and minor semi-axes of approximately 21 km (13 miles). Mars shows less flattening overall, though it is notably 'egg-shaped' with its north and south polar radii differing by approximately 6 km (4 miles), which is considered small enough that an average polar radius is used to define its ellipsoid.","context":["In geodesy, a reference ellipsoid is a mathematically defined surface that approximates the geoid, the truer figure of the Earth, or other planetary body. Because of their relative simplicity, reference ellipsoids are used as a preferred surface on which geodetic network computations are performed and point coordinates such as latitude, longitude, and elevation are defined.\nIn 1687 Isaac Newton published the Principia in which he included a proof[not in citation given] that a rotating self-gravitating fluid body in equilibrium takes the form of an oblate ellipsoid of revolution which he termed an oblate spheroid. Current practice uses the word 'ellipsoid' alone in preference to the full term 'oblate ellipsoid of revolution' or the older term 'oblate spheroid'. In the rare instances (some asteroids and planets) where a more general ellipsoid shape is required as a model the term used is triaxial (or scalene) ellipsoid. A great many ellipsoids have been used with various sizes and centres but modern (post-GPS) ellipsoids are centred at the actual center of mass of the Earth or body being modeled.\nThe shape of an (oblate) ellipsoid (of revolution) is determined by the shape parameters of that ellipse which generates the ellipsoid when it is rotated about its minor axis. The semi-major axis of the ellipse, a, is identified as the equatorial radius of the ellipsoid: the semi-minor axis of the ellipse, b, is identified with the polar distances (from the centre). These two lengths completely specify the shape of the ellipsoid but in practice geodesy publications classify reference ellipsoids by giving the semi-major axis and the inverse flattening, 1/, The flattening, f, is simply a measure of how much the symmetry axis is compressed relative to the equatorial radius:\nFor the Earth, f is around 1/ corresponding to a difference of the major and minor semi-axes of approximately 21 km (13 miles). Some precise values are given in the table below and also in Figure of the Earth. For comparison, Earth's Moon is even less elliptical, with a flattening of less than 1/, while Jupiter is visibly oblate at about 1/ and one of Saturn's triaxial moons, Telesto, is nearly 1/ to 1/.\n|This section does not cite any sources. (October 2011) (Learn how and when to remove this template message)|\nA primary use of reference ellipsoids is to serve as a basis for a coordinate system of latitude (north/south), longitude (east/west), and elevation (height). For this purpose it is necessary to identify a zero meridian, which for Earth is usually the Prime Meridian. For other bodies a fixed surface feature is usually referenced, which for Mars is the meridian passing through the crater Airy-0. It is possible for many different coordinate systems to be defined upon the same reference ellipsoid.\nThe longitude measures the rotational angle between the zero meridian and the measured point. By convention for the Earth, Moon, and Sun it is expressed in degrees ranging from −180° to +180° For other bodies a range of 0° to 360° is used.\nThe latitude measures how close to the poles or equator a point is along a meridian, and is represented as an angle from −90° to +90°, where 0° is the equator. The common or geodetic latitude is the angle between the equatorial plane and a line that is normal to the reference ellipsoid. Depending on the flattening, it may be slightly different from the geocentric (geographic) latitude, which is the angle between the equatorial plane and a line from the center of the ellipsoid. For non-Earth bodies the terms planetographic and planetocentric are used instead.\nThe coordinates of a geodetic point are customarily stated as geodetic latitude and longitude, i.e., the direction in space of the geodetic normal containing the point, and the height h of the point over the reference ellipsoid. See geodetic system for more detail. If these coordinates, i.e., latitude ϕ, longitude λ and height h, are given, one can compute the geocentric rectangular coordinates of the point as follows:\nIn contrast, extracting φ, λ and h from the rectangular coordinates usually requires iteration. A straightforward method is given in an OSGB publication and also in web notes. More sophisticated methods are outlined in geodetic system.\nHistorical Earth ellipsoids\nCurrently the most common reference ellipsoid used, and that used in the context of the Global Positioning System, is the one defined by WGS 84.\nTraditional reference ellipsoids or geodetic datums are defined regionally and therefore non-geocentric, e.g., ED50. Modern geodetic datums are established with the aid of GPS and will therefore be geocentric, e.g., WGS 84.\nEllipsoids for other planetary bodies\nReference ellipsoids are also useful for geodetic mapping of other planetary bodies including planets, their satellites, asteroids and comet nuclei. Some well observed bodies such as the Moon and Mars now have quite precise reference ellipsoids.\nFor rigid-surface nearly-spherical bodies, which includes all the rocky planets and many moons, ellipsoids are defined in terms of the axis of rotation and the mean surface height excluding any atmosphere. Mars is actually egg shaped, where its north and south polar radii differ by approximately 6 km (4 miles), however this difference is small enough that the average polar radius is used to define its ellipsoid. The Earth's Moon is effectively spherical, having almost no bulge at its equator. Where possible a fixed observable surface feature is used when defining a reference meridian.\nFor gaseous planets like Jupiter, an effective surface for an ellipsoid is chosen as the equal-pressure boundary of one bar. Since they have no permanent observable features the choices of prime meridians are made according to mathematical rules.\nSmall moons, asteroids, and comet nuclei frequently have irregular shapes. For some of these, such as Jupiter's Io, a scalene (triaxial) ellipsoid is a better fit than the oblate spheroid. For highly irregular bodies the concept of a reference ellipsoid may have no useful value, so sometimes a spherical reference is used instead and points identified by planetocentric latitude and longitude. Even that can be problematic for non-convex bodies, such as Eros, in that latitude and longitude don't always uniquely identify a single surface location.\n- Isaac Newton:Principia Book III Proposition XIX Problem III, p. 407 in Andrew Motte translation, available on line at \n- Torge, W (2001) Geodesy (3rd edition), published by de Gruyter, ISBN 3-11-017072-8\n- Snyder, John P. (1993). Flattening the Earth: Two Thousand Years of Map Projections. University of Chicago Press. p. 82. ISBN 0-226-76747-7.\n- B. Hofmann-Wellenhof, H. Lichtenegger, J. Collins. GPS - theory and practice. Section 10.2.1. p. 282. ISBN 3-211-82839-7.\n- A guide to coordinate systems in Great Britain. This is available as a pdf document at ] Appendices B1, B2\n- Osborne, P (2008). The Mercator Projections Section 5.4\n- P. K. Seidelmann (Chair), et al. (2005), “Report Of The IAU/IAG Working Group On Cartographic Coordinates And Rotational Elements: 2003,” Celestial Mechanics and Dynamical Astronomy, 91, pp. 203–215.\n- Web address: http://astrogeology.usgs.gov/Projects/WGCCRE\n- OpenGIS Implementation Specification for Geographic information - Simple feature access - Part 1: Common architecture, Annex B.4. 2005-11-30\n- Web address: http://www.opengeospatial.org","michigander See Bro! Look over there, I told you, if we waited long enough, another one of these scooters would show up!! That one's probably better than this one I'm riding. Hop on up there and we'll see if we can find another one of those shiny thingies we've been dragging behind this slow POS!\nRegister to post comments and participate in contests.\nThis contest is fueled by the following news: Spirit, NASA's robotic rover which is not on the surface of Mars, has extended its mechanical arms for the first time. It is now taking extraordinarily close photos of the rush colored soil of Mars. Mars is 300 million miles from Earth.\nRed Planet: Mars\nIn our solar system, planet Mars is located at fourth position from the Sun and according to size it is the seventh largest planet. Planet Mars received its name from the Roman god of war, corresponding to the one called as Ares in Greek mythology. Mars is sometimes also referred as the ‘Red Planet' because of its reddish surface, which can be attributed to iron oxide.\nMars belongs to the family of terrestrial planets with an atmosphere that has very low-density. Features of the topography of Mars surface can be compared with the lunar craters and volcanoes, valleys, deserts and polar ice caps like the ones seen on Earth. Extinct volcano Mount Olympus on Mars is the highest mountain in our solar system, and Mariner Valley on the planet is the largest canyon. In addition, three articles were published in the journal Nature in 2008 that put forward the evidence that Mars has an impact crater on its northern hemisphere that is the largest known in the solar system. Its length is 10,600 km and width is 8500 km, and it is roughly four times larger than the earlier known largest impact crater, also located on Mars near its south pole. In addition to the similarity of its surface topography, rotation intervals and changing of seasons on Mars are also similar to Earth, but its climate is much colder and drier than on Earth.\nUntil the first fly-by mission to Mars ‘Mariner 4' in 1965, many researchers were of the view that its surface is water in liquid state. This opinion was based on the observations of periodic variations in light and dark areas, particularly in the polar latitudes, which looked like continents and oceans. Dark fossae on Martian surface were interpreted by some experts as irrigation canals for liquid water. Later it was discovered that it was just an optical illusion and there were no such depressions.\nBecause of low-pressure on the surface of Mars, water can not exist in liquid state there, but it is quite possible that conditions were different in the past, and therefore the existence of elementary life on the planet can not be ruled out. On July 31, 2008 NASA's Phoenix lander discovered water in the form of ice on Mars.\nCurrently (as of February 2009), there are three operational spacecraft engaged in the research of Mars: ‘Mars Odyssey', ‘Mars Express' and ‘Mars Reconnaissance Orbiter', and the number is more than on any other planet except Earth. Mars at the moment also has two exploring rovers viz. Spirit and Opportunity. Mars also has on its surface many inactive landers and rovers that have already completed their mission. Geological data, collected by all these missions, suggest that a large part of Martian surface was covered with water in the past. Studies over the past decade have also revealed geyser like activity at some places on the surface of Mars. Based on the observations from NASA's ‘Mars Global Surveyor' some parts on the southern polar cap of Mars have been gradually receding.\nMars has two natural satellites, Phobos and Deimos (in Greek they mean ‘panic/fear' and ‘terror/dread' - the names of two sons of Ares that accompanied him to battle), which are relatively small and irregular in shape. These may be the asteroids, captured by the gravitational field of Mars, and similar to Asteroid 5261 Eureka from the Trojan group.\nMars can be seen from Earth with the naked eye. Its apparent magnitude is about 2.91 m (at closest approach to Earth), and its brightness is exceeded only by Jupiter, Venus, Moon and Sun.\nMinimum distance of Mars from Earth is 55.75 million miles (when Earth comes exactly between the Sun and Mars), and the maximum is about 401 million km (when the Sun is exactly between Earth and Mars). Average distance from Mars to Sun is 228 million km (1.52 AU), its orbital period around the sun equals 687 Earth days. The orbit of Mars has quite noticeable eccentricity (0.0934), that is why its distance from the Sun varies from 206.6 to 249.2 million km. Inclination of Martian orbit is equal to 1.85 °.\nMars is almost half the size of Earth and its equatorial radius is 3396.9 km (53.2% that of the Earth). The surface area of Mars is approximately equal to the land area on Earth. Sufficiently fast rotation of the planet has led to significant flattening which is evident from the fact that polar radius of Mars is about 21 km less than the equatorial one. Mass of the planet is 6,418 × 1023 kg (11% of Earth's mass). Gravitational acceleration at its equator is equal to 3.711 m / s² (0.378 that of the Earth), orbital velocity is 3.6 km / sec and its escape velocity is 5.027 km / sec."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:dd7d6efa-01fb-4c05-ae81-faf09ed13521>","<urn:uuid:3d8b08ea-95f5-4dfc-a56e-c64bacd7860d>"],"error":null}
{"question":"What is the impact of porosity on concrete strength, and how is it affected by the water/cement ratio?","answer":"Porosity has a significant impact on concrete strength - the more porous the concrete, the weaker it will be. The porosity is primarily determined by the water/cement (w/c) ratio of the mix. In mixes where the w/c is greater than approximately 0.4, the additional water creates pore spaces that remain as voids filled with water or air when dried. As the w/c ratio increases, the porosity of the cement paste increases, which decreases the compressive strength. This was demonstrated in studies showing that concrete made with high-absorption aggregates had porosity values above 22%, though adequate compressive strength up to 30 MPa could still be achieved with proper mix design.","context":["- About this Journal ·\n- Abstracting and Indexing ·\n- Aims and Scope ·\n- Annual Issues ·\n- Article Processing Charges ·\n- Articles in Press ·\n- Author Guidelines ·\n- Bibliographic Information ·\n- Citations to this Journal ·\n- Contact Information ·\n- Editorial Board ·\n- Editorial Workflow ·\n- Free eTOC Alerts ·\n- Publication Ethics ·\n- Reviewers Acknowledgment ·\n- Submit a Manuscript ·\n- Subscription Information ·\n- Table of Contents\nAdvances in Materials Science and Engineering\nVolume 2013 (2013), Article ID 734031, 4 pages\nCarbonation Coefficients from Concrete Made with High-Absorption Limestone Aggregate\nCollege of Engineering, Universidad Autónoma de Yucatán, Avenida Industrias No Contaminantes s/n por Periférico Norte, Mérida, YUC, CP 97110, Mexico\nReceived 31 May 2013; Accepted 28 August 2013\nAcademic Editor: Amit Bandyopadhyay\nCopyright © 2013 Eric I. Moreno. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\nNormal aggregates employed in concrete have absorption levels in the range of 0.2% to 4% for coarse aggregate and 0.2 to 2% for fine aggregate. However, some aggregates have absorption levels above these values. As the porosity of concrete is related to the porosity of both the cement paste and the aggregate and the carbonation rate is a function, among other things, of the porosity of the material, there is concern about the effect of this high porosity material in achieving good quality concrete from the durability point of view. Thus, the objective of this investigation was to study the carbonation rates of concrete specimens made with high-absorption limestone aggregate. Four different water/cement ratios were used, and cylindrical concrete specimens were exposed to accelerated carbonation. High porosity values were obtained for concrete specimens beyond the expected limits for durable concrete. However, carbonation coefficients related to normal quality concrete were obtained for the lowest water/cement ratio employed suggesting that durable concrete may be obtained with this material despite the high porosity.\nConcrete is the most used material for infrastructure development. However, durability of these structures is one of the main concerns, as the expected service life is approaching more than a hundred years. Among the known durability issues for concrete, corrosion of reinforcing steel is the main problem. In concrete structures, reinforcing steel is protected due to the high alkaline environment provided by the pore solution (pH > 12.5 units). However, it may corrode if the protection is lost. The protection may be lost due to chloride attack or due to the neutralization of the concrete pore solution surrounding the reinforcing steel. This neutralization of the pore solution is also called concrete carbonation. Concrete carbonation is the result of the reaction of hydrated cement compounds with atmospheric CO2. Carbonation reduces the pH of the concrete pore solution (pH ≈ 8 units), developing uniform corrosion in the reinforcing steel. In places with tropical climate, atmospheric conditions may encourage the CO2 aggressiveness . However, the carbonation rate depends on several factors  such as the type and amount of cement, porosity of the material , and amount of pozzolanic additions .\nThe porosity of concrete is related to the porosity of both the cement paste and the aggregate and the proper compaction during casting. The porosity of the cement paste is related to the water/cement ratio and the degree of hydration. From the durability point of view, it is accepted that the porosity of the cement paste is the one that controls the porosity of concrete. According to the literature , concrete’s porosity above 15% is an indication of bad quality and below 10% is an indication of good quality.\nNormal aggregates used in concrete have absorption levels in the range of 0.2% to 4% for coarse aggregate and 0.2 to 2% for fine aggregate . However, some limestone aggregates have absorption levels above these values . And due to the high-absorption characteristics of the aggregate, concrete made with this material has porosity values above 15%, thus suggesting bad quality concrete from the durability point of view.\nOne way to prove if this material is suitable for durable concrete structures is by determining the carbonation coefficient of the material. However, as concrete carbonation proceeds at a very low rate, the use of an accelerated carbonation chamber is required.\nTherefore, the objective of this investigation was to study the carbonation rates of concrete specimens made with high-absorption limestone aggregate.\n2.1. Specimens and Materials\nFour water/cement (w/c) ratios were used (0.80, 0.70, 0.62, and 0.55). The cementitious material was Ordinary Portland Cement with no mineral additions. Crushed limestone of high absorption was used as coarse and fine aggregate (Table 1). Mixtures were designed according to the ACI specifications , and casting was made following the ASTM standards (Table 2) .\nConcrete specimens were cylindrical in shape of different size: 300 mm by 150 mm for compressive strength tests, 200 mm by 100 mm for porosity tests, and 150 mm by 75 mm for carbonation tests. After casting, the curing period consisted of 28 days of water immersion. Once cured, the specimens were allowed to condition at lab environment prior to undergoing accelerated testing. The conditioning period was 80 days. Following conditioning, the specimens were placed in a 4% CO2 environment in order to accelerate the carbonation process for ~100 days, except for mixture 0.55 that was exposed for 176 days.\n2.2. Carbonation Depth Measurements\nThe progress of the carbonation depth was determined by spraying a 1% phenolphthalein solution on top of a freshly broken concrete surface following the RILEM criteria . For each determination, two specimens were removed from the carbonation chamber per w/c. After that, an ~50 mm slice was split from each specimen using a chisel and a hammer. After applying the acid-base indicator, the measurements were performed using a digital caliper. Eight measurements were taken in the more suitable half cylinder as the cuts were irregular. Each measurement was corrected from radial measurements in a cylinder into carbonation depth measurements that would be obtained in a semi-infinite plane . Once corrected, the average was obtained for each cylinder, and the result was averaged with the result from the other cylinder, obtaining an average from each pair of cylinders. After taking the carbonation measurements, the remaining portion of the concrete cylinders was placed back in the chamber.\nIn addition, a first carbonation depth measurement was performed using the external control specimens. This test was made after the conditioning period was finished and prior to placing the specimens in the carbonation chamber in order to determine the progress of the carbonation front during the conditioning period. In turn, the first carbonation depth measurement was used to correct the carbonation coefficient during the accelerated carbonation period.\nTable 3 shows the results from the compressive strength, porosity tests, and fresh concrete slump. The compressive strength results were above the nominal values, particularly for 0.80 w/c mixture. The porosity values were high, however, in the expected range for the type of high-absorption aggregate used and with little difference among the different mixtures . Concrete slump was in the expected range of 10 cm ± 2; however, 0.55 w/c mixture had the smallest slump, and 0.62 w/c mixture had the highest.\nTable 4 shows the results from the carbonation depth measurements. As the diameter was 75 mm, the specimens were fully carbonated when the carbonation depth was 37.5 mm. Due to the fact that mixture 0.55 was a better mixture than the others, carbonation depth was not determined at 51 days but at 112 days, instead. By day 51, mixtures 0.7 and 0.8 were fully carbonated.\nFrom the data in Table 4, the average carbonation coefficients at 4% CO2 were estimated. The data is presented in Table 5. Usually, carbonation is modeled as where is the time of exposure, is the carbonation depth at time , and is the carbonation coefficient. However, when initial carbonation exists previous to accelerated carbonation, then, the carbonation coefficient at 4% CO2 may be obtained from  where is the time of accelerated exposure, is the carbonation depth at time , and is the initial carbonation depth measured after the conditioning period. The average carbonation coefficient was obtained from\nCarbonation coefficient has been modeled using the CO2 diffusion coefficient in concrete (), the CO2 concentration (), and the concentration of hydrated calcium compounds () :\nIn the event of experiments with two different external CO2 concentrations (, ) using separate specimens of the same concrete mixture, the time to reach a given carbonation depth in both specimens would be, for concentration , and, for , thus, solving for , solving for , from (4) and (7), substituting (8) in (9), solving for , and then, the average carbonation coefficients obtained in (3) were converted to atmospheric carbonation coefficient () using (11): where and are the atmospheric and CO2 concentrations, respectively.\nThe atmospheric carbonation coefficients are also presented in Table 5. According to the literature , carbonation coefficients above 6 mm/year1/2 are representative of low quality concrete and carbonation coefficients below 3 mm/year1/2 are representative of high quality concrete. The carbonation coefficients were above 6 mm/year1/2 for specimens from mixtures 0.80, 0.70, and 0.62, suggesting low quality concrete from the durability point of view but as expected when using the w/c ratios employed. However, carbonation coefficients related to normal quality concrete were obtained for the lowest w/c ratio employed, suggesting that durable concrete may be obtained with this material despite the high porosity.\nThus, despite the high porosity of the concrete mixtures, enough compressive strength was obtained and the carbonation coefficients were consistent with the compressive strength’s quality of the material.\n(i)High porosity values for concrete, above 22%, were obtained when using high-absorption aggregates.(ii)Designed compressive strength was achieved up to 30 MPa despite the use of high-absorption aggregates.(iii)Carbonation coefficients related to normal quality concrete were obtained for concrete mixtures of 0.55 water/cement ratio when using high-absorption aggregates.\nThe author would like to acknowledge the partial support from CONACyT (Grant J34433-U) and Universidad Autónoma de Yucatán in several stages of this investigation. The opinions and findings of this investigation are those of the author and are not necessarily those of the supporting organizations. The author is indebted to J. Cob, G. Domínguez, F. Duarte, and W. Castillo for performing some of the tests reported in the paper.\n- L. Véleva, P. Castro, G. Hernández-Duque, and M. Schorr, “The corrosion performance of steel and reinforced concrete in a tropical humid climate: a review,” Corrosion Reviews, vol. 16, no. 3, pp. 235–284, 1998.\n- R. F. M. Baker, “Initiation period,” in Corrosion of Steel in Concrete, p. 22, Schiessl, Chapman and Hall, London, UK, 1988.\n- E. I. Moreno, Carbonation of blended cement concretes [Ph.D. dissertation], University of South Florida, 1999.\n- E. I. Moreno and A. A. Sagüés, Carbonation-Induced Corrosion on Blended-Cement Concrete Mix Designs for Highway Structures, NACE International, Houston, Tex, USA, 1998, CORROSION/98, paper no. 636.\n- O. Trocónis-Rincón, A. Romero-Carruyo, C. Andrade, P. Helene, and I. Díaz, Manual for Inspecting, Evaluating and Diagnosing Corrosion in Reinforced Concrete Structures, CYTED, Maracaibo, Venezuela, 2000.\n- S. H. Kosmatka and W. C. Panarese, Design and Control of Concrete Mixtures, Portland Cement Association, Skokie, Ill, USA, 14th edition, 2002.\n- R. Solís-Carcaño and E. I. Moreno, “Evaluation of concrete made with crushed limestone aggregate based on ultrasonic pulse velocity,” Construction and Building Materials, vol. 22, no. 6, pp. 1225–1231, 2008.\n- Standard Practice for Selecting Proportions for Normal Heavyweight, and Mass Concrete, ACI 211.1-91, ACI International, Farmington Hills, Mich, USA, 1997.\n- Standard Practice for Making and Curing Concrete Test Specimens in the Laboratory, ASTM C 192-98, ASTM International, West Conshohocken, Pa, USA, 2002.\n- “CPC-18 measurement of hardened concrete carbonation depth,” Materials and Structures, vol. 21, pp. 453–455, 1988.","Many factors influence the rate at which the strength of concrete increases after mixing. Some of these are discussed below. First, though a couple of definitions will be useful:\nThe processes of 'setting' and 'hardening' are often confused:\nSetting is the stiffening of the concrete after it has been placed. A concrete can be 'set' in that it is no longer fluid, but it may still be very weak; you may not be able to walk on it, for example. Setting is due to early-stage calcium silicate hydrate formation and to ettringite formation. The terms 'initial set' and 'final set' are arbitrary definitions of early and later set; there are laboratory procedures for determining these using weighted needles penetrating into cement paste.\nHardening is the process of strength growth and may continue for weeks or months after the concrete has been mixed and placed. Hardening is due largely to the formation of calcium silicate hydrate as the cement continues to hydrate.\nThe rate at which concrete sets is independent of the rate at which it hardens. Rapid-hardening cement may have similar setting times to ordinary Portland cement.\nTraditionally, this is done by preparing concrete cubes or prisms, then curing them for specified times. Common curing times are 2, 7, 28 and 90 days. The curing temperature is typically 20 degrees Centigrade. After reaching the required age for testing, the cubes/prisms are crushed in a large press. The SI unit for concrete strength measurement is the Mega Pascal, although 'Newtons per square millimetre' is still widely used as the numbers are more convenient. Thus 'Fifty Newton concrete,' means concrete which has achieved 50 Newtons per square millimetre, or 50 Mega Pascals.\nWhile measurements based on concrete cubes are widely used in the construction industry, the European standard for cement manufacture, EN 197, specifies a test procedure based on mortar prisms, not concrete cubes. For example, a cement described as conforming to EN 197-1 CEM I 42.5 N would be expected to achieve at least 42.5 MPa at 28 days using the specified mortar prism test. Whether 'real concrete' made from that cement will achieve 42.5 MPa in concrete cube tests depends on a range of other factors in addition to any intrinsic properties of the cement.\nAlmost everyone interested in cement is also concerned to at least some degree with concrete strength. This ebook describes ten cement-related characteristics of concrete that can potentially cause strengths to be lower than expected. Get the ebook FREE when you sign up to CEMBYTES, our Understanding Cement Newsletter - just click on the ebook image above.\nThere are many relevant factors; some of the more important follow:\nConcrete porosity: voids in concrete can be filled with air or with water. Air voids are an obvious and easily-visible example of pores in concrete. Broadly speaking, the more porous the concrete, the weaker it will be. Probably the most important source of porosity in concrete is the ratio of water to cement in the mix, known as the 'water to cement' ratio. This parameter is so important it will be discussed separately below.\nWater/cement ratio: this is defined as the mass of water divided by the mass of cement in a mix. For example, a concrete mix containing 400 kg cement and 240 litres (=240 kg) of water will have a water/cement ratio of 240/400=0.6. The water/cement ratio may be abbreviated to 'w/c ratio' or just 'w/c'. In mixes where the w/c is greater than approximately 0.4, all the cement can, in theory, react with water to form cement hydration products. At higher w/c ratios it follows that the space occupied by the additional water above w/c=0.4 will remain as pore space filled with water, or with air if the concrete dries out.\nConsequently, as the w/c ratio increases, the porosity of the cement paste in the concrete also increases. As the porosity increases, the compressive strength of the concrete will decrease.\nSoundness of aggregate: it will be obvious that if the aggregate in concrete is weak, the concrete will also be weak. Inherently weak rocks, such as chalk, are clearly unsuitable for use as aggregate.\nAggregate-paste bond: the integrity of the bond between the paste and the aggregate is critical. If there is no bond, the aggregate effectively represents a void; as discussed above, voids are a source of weakness in concrete.\nCement-related parameters: many parameters relating to the composition of the individual cement minerals and their proportions in the cement can affect the rate of strength growth and the final strength achieved. These include:\nalite is the most reactive cement mineral that contributes\nsignificantly to concrete strength, more alite should give better early\nstrengths ('early' in this context means up to about 7 days). However,\nthis statement needs to be heavily qualified as much depends on burning\nconditions in the kiln. It is possible that lighter burning of a\nparticular clinker could result in higher early strength due the\nformation of more reactive alite, even if there is a little less of it.\nNot all alite is created equal!\nFor a particular cement, there will be what is called an 'optimum sulfate content,' or 'optimum gypsum content.' Sulfate in cement, both the clinker sulfate and added gypsum, retards the hydration of the aluminate phase. If there is insufficient sulfate, a flash set may occur; conversely, too much sulfate can cause false-setting.\nA balance is therefore required between the ability of the main clinker minerals, particularly the aluminate phase, to react with sulfates in the early stages after mixing and the ability of the cement to supply the sulfate. The optimum sulfate content will be affected by many factors, including aluminate content, aluminate crystal size, aluminate reactivity, solubilities of the different sources of sulfate, sulfate particle sizes and whether admixtures are used.\nIf this were not already complicated enough, the amount of sulfate necessary to optimize one property, strength for example, may not be the same as that required to optimize other properties such as drying shrinkage. Concrete and mortar may also have different optimum sulfate contents.\nThis fascinating area is discussed further under ' cement-related concrete strength variability .'\nIn addition to the compositional parameters considered above, physical parameters are also important, particularly cement surface area and particle size distribution.\nThe fineness to which the cement is ground will evidently affect the rate at which the cement hydrates; grinding the cement more finely will result in a faster reaction. Fineness is often expressed in terms of total particle surface area, eg: 400 square metres per kilogram. However, of as much, if not more, importance is the particle size distribution of the cement; relying simply on surface area measurements can be misleading. Some minerals, gypsum for example, can grind preferentially producing a cement with a high surface area. Such a cement may contain very finely-ground gypsum but also relatively coarse clinker particles resulting in slower hydration.\nWe have some great training and reference resources. Some are free and some are paid-for.\nThe paid-for resources are unique to Understanding Cement and proceeds from these sales contribute to the costs of operating and developing this website - see our bookstore here.\nThe free resources are available on the Cembytes Resources Page. Currently, they include:\nEbook: \"Low Concrete Strength? Ten Potential Cement-Related Causes\": this illustrated ebook is a checklist of some of the main causes of cement-related low strength in concrete or mortar.\nCement glossary: glossary of over 100 cement-related definitions and chemical formulae.\nScreensaver/desktop images: six microscope images of clinker and concrete that you can use as desktop/tablet or screensaver images, or anything else you want.\nSubscribers to Cembytes, our free Understanding Cement Newsletter, can download these free resources from the Cembytes Resources Page.\nIf you are not already a subscriber, just sign up using the box below and you can be downloading the ebook, glossary and photos within a minute or two. Make sure you bookmark the resources page so you can get back to it later - there are no links to it from the website navigation as it is for Cembytes subscribers only!\nIf you are already a subscriber to Cembytes, you can access the Cembytes Resources Page directly if you have bookmarked it. However, if you have forgotten how to reach the Resources page, you can easily get back to it. Just enter your email in the signup box below and if you are on the subscriber list you will see a link to the Resources Page.\n(Note: we currently have two Cembytes subscriber lists, and old one and a new one, and we are gradually transferring to the new one. Depending when you subscribed, you will be on one list or the other. If you are on the old list, the signup box won't recognize you because it is only connected to the new list, but that's fine. Just confirm your subscription and you will be on the new list. After you confirm you will receive an email with a link to the Resources Page.)\nArticles like this one can provide a lot of useful material. However, reading an article or two is not really the best way to get a clear picture of a complex material like cement. To get a more complete and integrated understanding of cement and concrete, do have a look at the Understanding Cement book or ebook.\nThis easy-to-read and concise book contains much more detail on concrete chemistry and deleterious processes in concrete compared with the website.\nFor example, it has about two-and-a-half times as much on ASR, one-and-a-half times as much on sulfate attack and nearly three times as much on carbonation. It has sections on alkali-carbonate reaction, frost (freeze-thaw) damage, steel corrosion, leaching and efflorescence on masonry. It also has about four-and-a-half times as much on cement hydration (comparisons based on word count).\nCheck the Article Directory for more articles on this or related topics"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:1e023b14-139c-407c-be74-5a47ded43bc6>","<urn:uuid:e16efa14-c072-44f6-a5e9-94c38a531120>"],"error":null}
{"question":"My home is over 30 years old and I'm worried about building codes - what's the best way to make sure I'm covered if something happens? 🏠","answer":"For homes 30 or more years old, you should add 'ordinance or law coverage' as an endorsement to your insurance policy. This will cover the additional costs of complying with current building codes when rebuilding after an insured loss. This is particularly important because some communities require complete demolition and rebuilding of structures damaged over a certain percentage (often 50%), and standard homeowners policies don't cover demolition costs.","context":["The worst time to find out that you have no insurance, or inadequate insurance, is after a loss. If any of the following situations apply to you, a standard insurance program might not be enough.\n1 Problem: Your home is 30 or more years old. Insurance will cover the cost of bringing a damaged or destroyed building back to its previous state. But will you have enough to make sure it complies with current building codes? Some communities require an owner to demolish and completely rebuild a building that has been damaged over a certain percent (often 50 percent). The standard homeowners policy does not cover demolition costs.\nSolution: Add “ordinance or law coverage.” This policy endorsement (addition) will cover your additional costs of complying with ordinances or laws when rebuilding after an insured loss.\n2 Problem: You have a historic or highly customized home. If your homeowners policy provides “actual cash value” coverage, it will pay a maximum of the replacement value of your lost or damaged property, less depreciation. That probably won’t come anywhere close to the cost of replacing the special craftsmanship and materials that go into many historic homes. Even an ordinary “replacement cost value” policy probably won’t provide enough protection.\nSolution: Buy a homeowners policy that pays losses on a restoration cost or guaranteed value basis. A restoration cost policy will pay to restore your home’s features, while a guaranteed value policy will pay up to a maximum amount that you select.\n3 Problem: Your property is subject to a homeowners association. Since you share ownership of common areas with others, you’ll want to check that the association has appropriate insurance coverage. You’ll also want to make sure it has enough reserves to pay for repairing or replacing common elements near the end of their useful life. Associations that lack reserves for necessary repairs can assess members to pay the difference.\nSolution: Buy a loss assessment endorsement for your policy. This will add coverage for assessments when common elements owned by the homeowners association need repairs.\n4 Problem: You live in an area where costs are increasing rapidly. Unless you’re vigilant about updating your coverage, your policy might not pay enough to repair or replace your home after a loss. This problem could become worse if a disaster affects many properties in your area. Labor and material shortages would push up the cost of repairs.\nSolution: You can add an inflation guard endorsement to your homeowners policy. This will automatically increase your coverage limits every year by a specified percentage.\n5 Problem: You occasionally rent out your home. Short-term rentals can bring in cash—they also increase your risk exposures.\nSolution: Minimize your risk by requiring tenants to pay a deposit and/or provide proof of their own coverage. Airbnb provides landlords some protection—it will reimburse hosts for up to $1 million of damage to eligible property. However, this is not insurance and is not regulated by the state insurance department. If you plan to rent your home through Airbnb or a similar service, please call us to discuss your insurance needs.\n6 Problem: You have business equipment or property at your home. The typical homeowners insurance policy limits coverage for “business personal property” to $2,500, which might not be enough.\nSolution: Add an increased limits business property endorsement to cover business personal property at home and off premises. If your business property consists of computers or electronics, you might need a computer or electronic equipment endorsement for your homeowners policy. Without this endorsement, the typical homeowners policy limits coverage for computers and electronics to $2,500; one high-end computer and accessories could easily cost more than that to replace.\n7 Problem: You have valuable trees and shrubs. Landscaping can add as much as 20 percent to a home’s value. Is yours protected?\nSolution: Know your coverage. Most homeowners policies will cover a tree or shrub damaged or destroyed by a “covered peril.” These typically include fire, lightning, explosion, aircraft, vehicles not owned by the resident, theft, vandalism and malicious mischief. Most policies limit coverage for any one tree or shrub to $500 apiece, to a maximum of 5 percent of the amount of the coverage on your home’s structure.\n8 Problem: You have people who are not related to you living in your home. Your policy might not cover them.\nThe homeowners policy covers the “named insured,” or person whose name is on the policy. (If your house is jointly owned, both owners should be named on the policy.) The policy also insures people who are related to the named insured and other people under age 21 who are in the care of any insured. The policy might not cover other people, particularly for liability, even if you consider them household members.\nSolution: Contact us to discuss your situation. You can ask your insurer to cover another individual as a named insured or insured.\n9 Problem: You own high-value collectibles, jewelry, antiques or firearms. Homeowners policies usually put a lower sublimit on coverage for these high-worth items. If your policy limits coverage to $2,500, your valuables might be underinsured.\nSolution: Add an endorsement to your homeowners policy or buy a separate “floater” insurance policy. We can help you decide which is best for your needs.\n10 Problem: You have remodeled, added to or upgraded your home. Do you have enough coverage?\nSolution: Check your insurance coverage at least annually, and after any significant remodel or upgrade. Update your limits as needed.\nIf you have any questions on your coverage, or if you’d like to schedule a policy review, please contact us."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:6a792a98-d8e2-4db6-928e-1a04e558ebc9>"],"error":null}
{"question":"How do the missions of Army Special Forces Green Berets and Air Force Special Tactics Officers differ in terms of their primary focus?","answer":"Army Special Forces Green Berets focus primarily on special reconnaissance, unconventional warfare, direct action, foreign internal defense, and counter-terrorism, working with indigenous forces as 'force multipliers' and helping stabilize conflicted areas. In contrast, Air Force Special Tactics Officers primarily focus on managing airfield operations, combat search and rescue, guiding airstrikes, providing fire support using air assets, and tactical weather operations for special operations missions.","context":["19th Special Forces Company C jumps into new leadership\nDecember 24, 2009\nBy L.A. Shively\nFORT SAM HOUSTON, Texas -- While most people might spend a weekend mowing their lawns, the Green Berets of Charlie Company, 5th Battalion, 19th Special Forces Group (Airborne); a reserve National Guard unit at Camp Bullis, had other plans.\nThat meant giving up a Saturday to don parachutes, jump out of helicopters and bid farewell to their company commander in their own unique way.\nMaj. Timothy Ochsner relinquished command of Company C to Maj. Theodore Unbehagen during a change of command ceremony Dec. 19 and every available member of the unit participated in the celebration.\n\"We're jumping to honor our company commander who's moving up to another job,\" said Sgt. 1st Class Ted Heckerman, who has 800 jumps under his belt during a 32-year career.\nPlus, jumping out of aircraft is part of the job. \"We're required to do this as part of our proficiency for Special Forces,\" Heckerman said.\n\"To be part of this elite unit means to be airborne,\" said Lt. Col. Ken Chavez, 5th Battalion, 19th Special Forces Group commander, as he and Heckerman checked and double-checked each other's gear while they prepared to board the CH-47D Chinook that carried several \"sticks\" or groups of paratroopers to jump altitude.\nKnown as Green Berets, the mission of the U.S. Army Special Operations Forces includes special reconnaissance, unconventional warfare, direct action, foreign internal defense and counter-terrorism. To accomplish the mission, Charlie Company is organized into 12-man Operational Detachments Alpha or \"A-teams\" specializing in \"high altitude-low opening\" or military freefall insertions, combat diving, maritime and urban operations.\nMembers cross train because ODAs are small and flexible, and can operate for an indefinite period of time in remote locations with little or no outside support.\n\"One person is responsible but everyone is capable,\" said Sgt. 1st Class John McIntosh, adding that micromanagement is not an option. Each trusts the other to know and carry out his job while also confident his team will back him up.\nBeyond kicking in doors, ODAs work one-on-one with local authorities and individuals to help stabilize a conflicted area and its people. Language and cultural skills are paramount in these duties that often include search and rescue, peacekeeping, security, humanitarian assistance and counter-drug operations.\n\"We're force multipliers. Our bread and butter is to work with the indigenous forces,\" said Sgt. 1st Class Iverson, who explained his cadre aligns with civil affairs groups and builds or repairs schools, infrastructure and tackles health issues in communities. In fact, health is often the highest priority for a village or district, Iverson said.\nAt a 1,500 feet altitude aboard the Chinook, the first of three waves of SF hooked up their static lines and made ready to leave the helicopter. Hand gestures counted down the time before the jump at four minutes, one minute, and 30 second intervals. A final green light signaled their exit.\nAt 12,500 feet, where breath crystallized and oxygen levels were thin, the HALO team made their jump. On the ground, friends and Family watched as the helicopter hovered, chutes popped open and men landed in the drop zone.\nDuring the change of command ceremony, Ochsner passed the company guidon to Unbehagen and the group celebrated the growth of Charlie Company, 5th Battalion, 19th Special Forces Group (Airborne) from just seven members to 100 percent strength in less than two years.\n\"This company is poised and prepared to go anywhere,\" Ochsner said during his closing remarks.\n\"We will continue to bring the best of Special Forces when asked to win hearts and minds,\" said Unbehagen as he took command.\n\"We have a very tight organization and all these men are very proud of what they do. They're three-time volunteers,\" Chavez said. \"They volunteered for the Army, they volunteered for the Airborne and they volunteered for Special Forces.\n\"We're here to protect the nation and the state of Texas.\"\nTheir lawns will be mowed next weekend.","|U.S. Air Force Special Tactics Officer|\n|Branch||United States Air Force|\n|Part of||Air Force Special Operations Command (AFSOC)|\n|Motto(s)||First There... That Others May Live|\nA United States Air Force Special Tactics Officer (AFSC 13DXB) is a United States Air Force Special Operations Command (AFSOC) officer who manages the training and equipping of U.S. Air Force ground special operations. Special Tactics Officers deploy as team leaders or mission commanders in combat, seizing and controlling airstrips, combat search and rescue, guiding airstrikes and fire support using air assets for special operations and tactical weather observations and forecasting. Special Tactics Officers are not pararescuemen or Combat Controllers, but they lead the Special Tactics Squadrons and Groups and thoroughly understand how to conduct, manage, and provide these special operations missions to both conventional and joint special operations missions needed within Joint Special Operations Command (JSOC) and United States Special Operations Command (SOCOM).\nSpecial Tactics Officers lead U.S. Air Force Pararescuemen, U.S. Air Force Combat Controllers and U.S. Air Force Special Operations Weather Technicians. They do not have their own training course, instead they go through the Combat Control training while Combat Rescue Officers attend Pararescue training. Many attain qualifications as Joint terminal attack controllers. Their 35-week initial training and unique mission skills earn them the right to wear the scarlet beret akin to Combat Controllers. From that point they attend a 12-15 month advanced skill training course.\n- Combat Control Selection Course, Lackland Air Force Base, Texas (2 weeks)\nThis selection course focuses on sports physiology, nutrition, basic exercises, combat control history and fundamentals.\n- Combat Control Operator Course, Keesler Air Force Base, Mississippi (15.5 weeks)\nThis course teaches aircraft recognition and performance, air navigation aids, weather, airport traffic control, flight assistance service, communication procedures, conventional approach control, radar procedures and air traffic rules. All air traffic controllers in the Air Force attend this course.\n- Air Force Basic Survival School, Fairchild Air Force Base, Washington (3 weeks)\nThis course teaches techniques for survival in remote areas. Instruction includes principles, procedures, equipment and techniques that enable individuals to survive, regardless of climatic conditions or unfriendly environments, and return home.\nTrainees learn the basic parachuting skills required to infiltrate an objective area by static line airdrop.\n- Combat Control School, Pope Air Force Base, North Carolina (13 weeks)\nThis course provides final Combat Controller qualifications. Training includes physical training, small unit tactics, land navigation, communications, assault zones, demolitions, fire support and field operations including parachuting. Graduates of the course are awarded the 3-skill level (Apprentice), scarlet beret and CCT flash.\n- Special Tactics Advanced Skills Training, Hurlburt Field, Florida (12 to 15 months)\nAdvanced Skills Training is a program for newly assigned Air Force Special Tactics operators. AST produces mission-ready operators for the Air Force, Joint Special Operations Command and United States Special Operations Command. The AST schedule is broken down into four phases: water, ground, employment and full mission profile. The course tests the trainee's personal limits through demanding mental and physical training. Special Tactics Officers also attend the following schools during AST:\n- Army Military Free Fall Parachutist School, Fort Bragg, North Carolina, and Yuma Proving Ground, Arizona (5 weeks)\nThis course instructs free fall parachuting procedures. The course provides wind tunnel training, in-air instruction focusing on student stability, aerial maneuvers, air sense, parachute opening procedures and parachute canopy control.\n- Air Force Combat Diver School, Navy Diving and Salvage Training Center, Naval Support Activity Panama City, Florida (6 weeks)\nTrainees become combat divers, learning to use scuba and closed circuit diving equipment to covertly infiltrate denied areas. The course provides training to depths of 130 feet, stressing development of maximum underwater mobility under various operating conditions.\nNotable Special Tactics Officer\nIn 2010 Captain Barry Crawford, a Special Tactics Officer, was awarded the Air Force Cross for heroism during a 14 hour battle near Laghman Province, Afghanistan, on May 4, 2010. Captain Crawford was the JTAC attached to an Army Special Forces team along with Afghan commandos. The team conducted a night time helicopter assault into a village to gather intelligence. After sunrise they came under attack from enemy combatants and several Afghan soldiers were wounded. After calling for a MEDEVAC Captain Crawford exposed himself to enemy gunfire by running out into the open in order to guide the helicopter to the landing zone. While he was guiding the helicopter in one of his radio antenna was shot off of his back. After the helicopter took off Captain Crawford and the Special Forces team managed to exfiltrate the area with zero American casualties. In the ten hour battle Captain Crawford coordinated Close air support, calling in over 40 airstrikes from 33 different aircraft. The AH-64 helicopters and F-15E fighters called in eliminated enemy positions and combatants utilizing strafing runs along with 500 - 2,000-pound bombs and Hellfire missile strikes. The special operations team suffered two Afghan commando casualties, but more than 80 insurgents were killed during the engagement, including three high-ranking enemy commanders.\n- \"AFSOC Special Tactics\". http://www.afsoc.af.mil/specialtactics/. Retrieved January 14, 2013.\n- \"COMBAT CONTROLLERS AND SPECIAL TACTICS OFFICERS\". 1/9/2009. http://www.nationalmuseum.af.mil/factsheets/factsheet.asp?id=13579. Retrieved January 14, 2013.\n- \"Special Tactics Officer\". 30 Apr 2003. http://www.specialtactics.com/cctofficer.shtml. Retrieved January 14, 2013.\n- \"Special Tactics\". http://www.afsoc.af.mil/specialtactics/. Retrieved January 14, 2013.\n- \"Air Force Special Tactics Officer: Overview\". http://www.military.com/special-operations/air-force-special-tactics-officer.html. Retrieved January 14, 2013.\n- Air Force Capt. Kristen D. Duncan (April 12, 2012). \"Special Tactics Officer Receives Air Force Cross\". http://www.defense.gov/news/newsarticle.aspx?id=67915. Retrieved January 14, 2013.\n- \"Special tactics officer to get Air Force Cross\". Apr 6, 2012. http://www.airforcetimes.com/news/2012/04/air-force-cross-special-tactics-airman-crawford-041612/. Retrieved January 14, 2013.\n- Capt. Kristen D. Duncan (April 12, 2012). \"'Battle of survival:' Special tactics officer awarded Air Force Cross\". http://www2.afsoc.af.mil/news/story.asp?id=123297783. Retrieved January 14, 2013.\n|This page uses Creative Commons Licensed content from Wikipedia (view authors).|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:917cc9f1-c2aa-41b3-bb17-829713f4aa1c>","<urn:uuid:dffb9265-30f5-439f-b624-80a09bc99ab7>"],"error":null}
{"question":"What limitations should individuals be aware of when considering short-term health insurance coverage, and how does it compare to traditional disability benefits for mental health conditions?","answer":"Short-term health insurance has several significant limitations: it may not cover pre-existing conditions, preventive care, or essential health benefits required under the Affordable Care Act (ACA). These plans don't guarantee renewal, and policyholders might need to reapply and pass medical underwriting to continue coverage. They also don't qualify for federal premium subsidies. Regarding mental health conditions, short-term disability coverage provides specific benefits including income replacement, continued health insurance coverage, and mental health care services. However, coverage for mental health conditions requires proof from a doctor or medical professional that the condition is severe enough to prevent work, and some policies may require additional documentation such as therapy records or medication lists. The exact coverage and requirements vary by insurer and state regulations.","context":["Short-Term Health Insurance for Small Business Owners: A Comprehensive Guide\nSmall business owners face unique challenges when providing healthcare coverage for themselves and their employees. While larger companies often have the resources to offer comprehensive health insurance plans, small business owners may need help. This is where short-term health insurance can be a valuable option. This comprehensive guide will explore the world of short-term health insurance for small business owners. We’ll discuss short-term health insurance, its benefits, and limitations, how to choose the right plan, and provide insights into the regulatory landscape.\nSection 1: What is Short-Term Health Insurance?\nShort-term health insurance, also known as temporary health insurance or STM, is a type of health coverage designed to provide individuals and families with temporary medical benefits. These plans are typically purchased for short durations, ranging from one month to a year, and are often used as a bridge to cover gaps in healthcare coverage. Small business owners may opt for short-term health insurance between jobs, waiting for their group health insurance plan to start, or simply looking for a cost-effective healthcare solution.\nSection 2: Benefits of Short-Term Health Insurance\nShort-term health insurance offers several benefits for small business owners:\n- Affordability: Short-term plans are generally more affordable than traditional health insurance, making them an attractive option for budget-conscious small business owners.\n- Flexibility: These plans allow for flexibility in choosing coverage periods, making aligning with your business’s specific needs easier.\n- Customization: Small business owners can tailor short-term plans to include the specific benefits they need, such as doctor visits, prescription drug coverage, or maternity care.\nSection 3: Limitations on Short-Term Health Insurance\nWhile short-term health insurance can benefit small business owners, it’s essential to be aware of its limitations.\n- Limited Coverage: Short-term plans may not cover pre-existing conditions, preventive care, or essential health benefits required under the Affordable Care Act (ACA).\n- No Guaranteed Renewal: These plans do not guarantee renewal, and policyholders may need to reapply and pass medical underwriting to continue coverage.\n- No Federal Subsidies: Short-term plans do not qualify for federal premium subsidies, potentially making them more expensive for some individuals.\n- State Regulations Vary: Short-term health insurance regulations vary by state, affecting plan availability and terms.\nSection 4: How to Choose the Right Short-Term Health Insurance Plan\nSelecting the right short-term health insurance plan requires careful consideration.\n- Assess Your Needs: Evaluate your healthcare needs, considering age, health status, and expected medical expenses.\n- Compare Plans: Obtain quotes from multiple insurance providers, comparing coverage options, deductibles, and premiums.\n- Check Provider Networks: Ensure that your preferred healthcare providers accept the short-term insurance plan you are considering.\nSection 5: The Regulatory Landscape\nThe regulatory environment for short-term health insurance varies from state to state and may change over time. While the federal government has relaxed short-term plan regulations, some states have implemented stricter rules to protect consumers. Small business owners must stay informed about local regulations that may impact their insurance choices.\nShort-term health insurance can be a valuable option for small business owners seeking affordable and flexible healthcare coverage. However, it’s crucial to understand the benefits and limitations of these plans and carefully assess your specific needs before making a decision. Doing so can ensure that you and your employees have access to the healthcare coverage that best suits your circumstances. Stay informed about the regulatory landscape in your state, and remember that short-term health insurance is not a long-term substitute for comprehensive coverage.","Mental health issues can range from mild anxiety to severe depression. These conditions can be debilitating and can cause a person to miss work. That is why it is important to understand the benefits of short-term disability insurance when it comes to mental health issues.\nKnowing your rights and entitlements when it comes to short-term disability for mental health can help you get the treatment and support you need during this challenging time.\nLet’s dive into the details about short-term disability so you can make an informed decision about whether it’s right for you.\nWhat Is Short-Term Disability For Mental Health?\nShort-term disability for mental health is a form of insurance that covers the costs of medical treatment related to your mental health condition.\nIt can also provide financial support if you are unable to work due to your mental health condition. Depending on the policy, this may include both physical and psychological treatments such as counselling or medication.\nHow Does Mental Health Fit Into Short-Term Disability Coverage?\nMental health issues such as depression, anxiety, bipolar disorder, PTSD and more can qualify for short term disability for mental health coverage in some cases – although it’s important to note that each policy has its own set of rules and guidelines when it comes to mental health conditions.\nSome policies may require proof from a doctor or other medical professional that the condition is severe enough to prevent work; others may require additional documentation such as therapy records or medication lists.\nTalk with your insurer about what types of evidence they require before filing a claim related to mental health conditions. Lets talk more here for medical bill grants, how you can apply for it.\nWhat Does Short-Term Disability Cover?\nShort-term disability insurance typically covers up to around three months of income replacement in the event of an injury or illness.\nThe exact coverage varies depending on the policy, but it typically covers some or all of the following :-\n- Doctors’ visits and hospital stays.\n- Medication costs related to your illness or injury.\n- Lost wages, including any bonus payments you may have received had you not been disabled.\n- Expenses related to rehabilitation, such as physical therapy or occupational therapy.\n- Expenses related to medical equipment, such as crutches and wheelchairs.\n- Any other medical expenses related to your illness or injury that are not covered by another source, such as health insurance.\nAdditionally, some short term disability for mental health policies also provides additional coverage for mental health issues such as depression and anxiety. This can be especially beneficial for those dealing with mental health issues that prevent them from working.\nWho Is Eligible For Short-Term Disability?\nIf you’re considering applying for short-term disability benefits, it’s important to know who qualifies and what the eligibility requirements are.\nShort-term disability insurance is designed to provide income replacement when a person is temporarily disabled due to an illness or injury and cannot work. Let’s take a look at who is eligible for this type of coverage.\nThe eligibility requirements for short term disability for mental health vary by state and insurer, but generally speaking, anyone between the ages of 18 and 65 can apply for coverage.\nIn addition, applicants must be able to show that they have been employed in their current job for at least six months prior to applying.\nPeople with pre-existing conditions may be denied coverage, as well as those with certain dangerous hobbies, such as rock climbing or skydiving.\nDocumentation Needed For Application\nOnce you determine that you meet your employer’s eligibility requirements, it will be important to gather the necessary documentation in order to complete your application.\nYour employer should provide you with an application form as well as a list of documents required in order to submit your application.\nYou will likely need medical records from your doctor or psychologist verifying your diagnosis and any treatments or therapies prescribed; proof of income; and information about any other insurance policies you may have such as private health insurance or short term disability for mental health insurance.\nAll of these documents should be submitted along with your completed application form in order for it to be processed properly and quickly.\nWhen Can I Start Receiving Benefits?\nTypically you must wait a certain period of time before receiving benefits—usually it’s between 14 and 30 days after your disability begins.\nYour insurer will usually pay benefits after that waiting period has ended; however, this could vary depending on your policy and circumstances.\nBe sure to check with your insurer if you have any questions about your specific policy.\nFrequently Asked Questions\nWhat are the benefits of short term disability for mental health?\nSome of the benefits may include :-\n- Income replacement so that you can continue to pay your bills\n- Continued health insurance coverage.\n- Vacation and sick time payouts.\n- Mental health care services.\nTalk to your human resources department or insurance company to learn more about the specific benefits offered through your policy.\nDoes stress qualify short-term disability?\nIt depends. If a person’s stress is caused by an illness or injury covered under their short-term disability insurance policy, then the stress may qualify for benefits.\nHowever, the policy must provide a clear definition of what constitutes an illness or injury covered by the policy and how it must be documented by a medical professional.\nHow long does short term disability for mental health last?\nThe duration of short term disability for mental health can vary depending on the insurance provider and plan. Generally, most plans offer coverage for up to 6 months.\nCan I appeal if I am not eligible for short term disability for mental health?\nYes, you can appeal if you are not eligible for short term disability for mental health. Each state has different laws and regulations regarding the appeals process, so you should consult with a disability attorney to learn more about how to appeal your decision.\nCan you go on short-term disability for anxiety?\nThe answer to this question depends on your situation, as short-term disability policies vary. Generally speaking, if you qualify for short-term disability, it is possible to receive benefits due to anxiety or other mental health conditions.\nIf you’re living with a mental health condition, understanding the benefits of short term disability for mental health can make all the difference when dealing with its impact on employment.\nThis type of insurance provides financial security during periods of illness or injury caused by mental health issues so that you can focus on getting better without having to worry about how you’ll make ends meet during that time.\nBe sure to research what options are available in your area so that if needed, you can access these life-saving protections when they are needed most."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:f3a3c940-b073-41cb-86fc-e472e466e1f9>","<urn:uuid:b8cde684-b616-4d67-942c-06f389b523a3>"],"error":null}
{"question":"Hey history buffs! I was wondering: which lasted longer - the Old London Bridge built in 1209 or Australia's London Bridge before it collapsed? Can anyone compare their lifespans? 🤔","answer":"The Old London Bridge in London lasted significantly longer. It stood for about 553 years, from its completion in 1209 until 1762. In contrast, Australia's London Bridge, which was located in Port Campbell National Park, Victoria, stood until January 15th, 1990, when its inner arch collapsed. While the exact construction date of Australia's London Bridge isn't specified, it existed at least since 1959 when documented in photographs, making its known lifespan considerably shorter than its London counterpart.","context":["Some of the world’s most beautiful natural rock formations are also some of the most fragile, a quality that has much to do with their beauty. Though they may be thousands, even millions of years old, all of these unique formations will some day succumb to the forces of wind, weathering and gravity – sometimes before our very eyes. Alas, for these 10 fallen natural rock formations, there is no cosmic Clapper.\nEphemeral Arch, Nevada, USA\nThe most recent prominent natural rock formation to return to its roots, as it were, Ephemeral Arch lived up to its name on or around May 26th, 2010. The arch used to be a popular attraction for tourists visiting the Natural Arch Trail in Nevada’s Valley of Fire State Park.\n(image via: Zehrer-Online)\nThough Ephemeral Arch itself was rather small (6 feet tall and 5 feet across), its location atop a 40 ft high rock formation provided onlookers with prime views silhouetted against clear desert skies. Ephemeral Arch was nicknamed “The Dragon”, as some noted a resemblance to a mother dragon feeding her offspring.\nEl Dedo de Dios (God’s Finger), Canary Islands, Spain\nEl Dedo de Dios, or “God’s Finger” translated from Spanish, can be found on the coast of Grand Canaria near the town of Agaete… at least, most of it can still be found there. The formation, which resembled a hand reaching heavenward, took on its unique appearance over a period of 200,000 to 300,000 years.\nEl Dedo de Dios and the surrounding rocks were formed from basaltic lava laid down approximately 14 million years ago. When Tropical Storm Delta hit the Canary Islands in November of 2005, gusting winds snapped off the upper portion of the formation, toppling it into the Atlantic Ocean. Spanish authorities briefly considered reconstructing the finger-shaped spire but decided reluctantly to let nature take its course.\nWall Arch, Utah, USA\nWall Arch, located in Utah’s Arches National Park, was formed over untold thousands of years by the action of, mainly, wind-blown desert sand. It seems somehow unfair that this magnificent arch was only documented in 1948: a mere 60 years later, on the night of October 4th, 2008, the central span of Wall Arch collapsed into rock shards and dust.\n(image via: Lobsang Studio)\nArches National Park contains around 2,000 natural sandstone arches of all shapes and sizes, but Wall Arch was one of the grandest. The 71-ft wide opening beneath the arch and its 33.5-ft height ranked it 12th in size… so much for that, huh?\nOld Man of the Mountain, New Hampshire, USA\nLook at the reverse of New Hampshire’s 2000 State Quarter and you’ll see the Old Man of the Mountain, as much a part of New Hampshire’s iconography as the state motto, “Live Free or Die”.\nFirst noted in 1805 by surveyors working in the Franconia Notch region, this somewhat spooky face was immortalized by Daniel Webster, who wrote: “Men hang out their signs indicative of their respective trades; shoe makers hang out a gigantic shoe; jewelers a monster watch, and the dentist hangs out a gold tooth; but up in the mountains of New Hampshire, God Almighty has hung out a sign to show that there He makes men.” Above is the Old Man in his prime; just below is a post-collapse closeup showing the steel cables and hooks that finally yielded to nature’s will.\n(image via: Wikipedia)\nThis composite image shows the Old Man of the Mountain both before and after most of the distinctive granite ledge crashed to the valley floor below on May 3rd, 2003. The collapse is thought to be the inevitable result of innumerable freeze-thaw cycles occurring since the final retreat of Ice Age glaciers nearly 10,000 years ago. Man-made supports and annual maintenance done since cracks began to appear in the 1920s only forestalled the inevitable.\nTroll Woman, Iceland\n(image via: Not About Books)\nLegends of trolls both male and female abound in Iceland, a country subject to immense natural forces far beyond the control of mankind. One such folk tale describes the fate of trolls and bewitched humans who are caught out in the open by the rays of the morning sun: they are turned to stone where they stand (or sit). The seaside rock shown above, according to a local folktale, was once a woman who waited in vain for her fisherman husband to return, eventually turning to stone. The “Troll Woman”, perhaps tired of her long and fruitless wait, finally tumbled into the sea sometime before May of 2006.\nJumpoff Joe, Newport, Oregon, USA\nJumpoff Joe was a famous sea stack that featured a keyhole arch, located at Nye Beach near Newport, Oregon. Composed of relatively soft concretionary sandstone, Jumpoff Joe has been extensively documented photographically for over 100 years as it evolved from a natural pier to a solitary sea stack to finally, isolated rock outcrops that barely rise above the ocean waves.\n(image via: Postcard Paradise)\nThe above postcard illustration dates from around the turn of the 20th century. It may seem amazing how quickly such a large natural structure can be reduced to nearly nothing, but it just goes to show the unmatched power of nature and the Earth’s inexorable geologic forces at work.\nLondon Bridge, Australia\n“London Bridge is falling down…” and so it did, on January 15th, 1990, leaving a single-arched sea stack now known as London Arch. In either its twin-arched Bridge form or as a stand-alone sea stack Arch, this massive rock formation proves that when it comes to memorable landmarks one CAN have it both ways.\n(image via: Pizzodisevo)\nThe above photo of London Bridge shows it in all its glory. The formation certainly looks both solid and ageless in this image, taken in December of 1959, but appearances can be deceptive. Located in Australia’s Port Campbell National Park in Victoria and easily accessible along the Great Ocean Road, it’s a wonder the inner arch’s collapse didn’t take several oblivious sightseers with it. It was a near thing, however: a couple of tourists left stranded on the suddenly isolated sea stack had to be rescued by helicopter.\nThe Twelve Apostles, Australia\nJust down the Great Ocean Road from London Arch, you’ll find The Twelve Apostles; a spectacular grouping of nine (yes, nine) limestone sea stacks that began eroding away from their parent cliffs between 10 and 20 million years ago.\n(image via: Wikipedia)\nBad enough that the original Twelve Apostles numbered only nine, on July 3rd, 2005 a 50 meter (164-ft) high sea stack suddenly collapsed like a house of cards, leaving only eight Apostles to stand guard against the relentless waves of the Southern Ocean. The images above show the grouping in dramatic “before & after” fashion, with the “after” photo taken several minutes following the collapse.\nChimney Rock, Nebraska, USA\n(image via: Visit USA)\nChimney Rock National Historic Site, located near Bayard, Nebraska, is an example of a rock formation undergoing a slow-motion collapse… from OUR point of view, that is. The spire is topped with hard sandstone that has protected underlying layers of clay and volcanic ash to some degree.\nThe spire was noticeably taller in pre-photographic times, however, and among the weathering agents slowly but surely reducing its height are lightning strikes. Want to see Chimney Rock? Check your pockets… if you have a 2006 Nebraska State Quarter in your change, you’ll see Chimney Rock and a passing wagon train engraved on the reverse.\n(image via: Sierra College)\nThe distinctive natural spire of Chimney Rock towers 325 feet above the rolling plains of the North Platte River Valley and it served as a true landmark for travelers trekking the California, Mormon and Oregon trails. It was said that once you spied Chimney Rock, your long journey across the Great Plains had ended and a shorter, more arduous one across the Rocky Mountains would soon begin. The drawing above, penned by Joseph Goldsborough Bruff back in the days of the 49-ers and the California Gold Rush, looks markedly taller than in recent photos.\nThe Eye of the Needle, Montana, USA\nNot every natural rock formation meets a natural end, and that’s the case of The Eye of the Needle, a graceful 11-ft high white sandstone arch located near the upper headwaters of the Missouri River near Missoula, Montana.\n(image via: California Hick)\nThe graceful arch had stood for many thousands of years and for all intents and purposes could have stood for several thousand more if not for the misguided antics of drunken vandals (NOT the people in the images above) who toppled the Eye of The Needle over the Memorial Day weekend in 1997. The perpetrators have never been found and suggestions that the arch be repaired have been dismissed, with many stating that would give out the wrong message: that unique natural monuments are easy to repair.\nThough it’s unfortunate when familiar landmarks fall to bits, at times their passing brings about new monuments that themselves may persist for centuries or more. A prime example is Lange Anna in Helgoland, Germany. This once majestic arch collapsed back in 1868. What remains is a 47 meter (154 feet) tall sea stack that stands in solitary grandeur, looking out across the tempestuous North Sea that carries within it the seeds of its future demise.\nWe live within a snapshot, a blink of an eye on the geologic scale. Rock formations have been laid down, built up and eroded away time and time again over our planet’s 4.5 billion year lifetime – and even the Earth suffers from fallen arches. While we can’t resurrect the ones that have fallen, it’s certain that given enough time, new ones will arise.","Remains of the Old London Bridge\nA bridge has spanned the Thames between the City of London and Southwark on the site of a natural causeway since the original Roman crossing was built in AD50. Since the conquest it has evolved over time and existed in countless different forms, one of the most famous being the medieval ‘Old London Bridge’ which was finished under the reign of King John in 1209 and survived until 1762. Though only 8m wide it was 270m long and by the Tudor era hosted a haphazardly placed row of more than 200 shops. Drawings depict comical ratios between the bridge foundations and seven-storey high buildings overhanging the river and encroaching on the street – making crossings last up to an hour!\nAt the end of the 18th century it was over 600 years old and congestion was becoming so serious the mayor ruled that all traffic from Southwark should keep to the west side and traffic from the City should keep to the opposite side; it is said this is the origin of driving on the left in Britain. The precariously stacked buildings of the old bridge have long since been demolished, yet a few fragments of the stonework remain scattered across the city. One of the best surviving examples is the old pedestrian entrance which now forms an arched pathway under the St Magnus the Marytr’s church tower.\nA water pump was first installed here upon an old well head in the sixteenth century and then in the eighteenth century replaced by the stone water pump seen today. Despite standing overlooked at a bustling junction in the financial district, Aldgate Pump remains a hidden statue to the areas gruesome past. Today the pump is no longer functional but in 1876, before being linked to the mains water supply, it was served by an underground stream. At first the people of Aldgate, unaware of the oncoming horror story, said the water contained healthy minerals but soon began to complain of an unusual taste which was quickly linked to the many recent deaths in the area. An investigation made the shocking discovery that, as the water was being channelled past the new cemeteries in the city; calcium had leached out from human bones and into the water! Yet by the 1920s the water quality had been turned around to the extent Whittard’s tea merchants even filled their kettles at the pump – declaring it the best water for tea tasting. Today the pump also resembles less grisly events such as marking the official point at which the East End starts and where mileages east of London are calculated from.\nAround 2000 years ago there was a 4km defensive wall built by the Romans around the settlement of Londinium. William I’s march on London 1000 years later found the enclosed City of London to be harder to conquer so eventually special privileges were granted to encourage peace with the city. For example, it is said the City of Westminster was expanded in an attempt to draw power and wealth west from the City of London. Today the City of London still holds special rules and customs – one is that the Queen has to ask permission before entering as the people ‘still retain their ancient privilege of being able to bar the Sovereign from entering their streets.’ The ancient wall was only maintained until the 18th century but around fourteen sections have survived and now sit in a disjointed arc above the Thames.\nThe oldest church in the City of London was founded by the Abbey of Barking in 675AD. It became known as ‘All Hallows Barking’ and then in 300 years ‘All Hallows-by-the-Tower’ due to its proximity to the Tower of London. In future decades it would survive explosions, fires and the Blitz but still continue its job as a church. In the 1500s it was popular with monarchs, such as Edward IV, who made it a royal chantry meaning executions at the Tower were sent for temporary burial at All Hallows and in 1797 John Quincy Adams, sixth president of the USA, was married at All Hallows. Over the years it has undergone many reconstructions to become a mix of architectural styles and despite its dramatic history still sits on Roman foundations against the surrounding walls of 21st century glass.\nABOUT THE AUTHOR"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:1786d9ae-d0b5-49cc-9601-1f9b1e1ab5ce>","<urn:uuid:90db3033-f85b-482e-b206-e99f76e37e9e>"],"error":null}
{"question":"Which organization started earlier - HPRG or Dr. Hacker's leadership at NCCDPHP?","answer":"The Healthy Places Research Group (HPRG) started much earlier, being established in 2003, while Dr. Karen Hacker assumed her position as Director of CDC's National Center for Chronic Disease Prevention and Health Promotion (NCCDPHP) in August 2019, representing a 16-year difference in their starting dates.","context":["Creating Healthy Places to Improve Public Health\nSep 15, 2017 | Atlanta, GA\nNearly 15 years ago, Dr. Catherine Ross and a group of like-minded individuals came together to address the rising obesity epidemic and rates of chronic disease in the United States.\nShe believed that “to find solutions, we need all perspectives in the room.”\nTo bring these perspectives together, Ross worked with individuals from Georgia Tech, the Centers for Disease Control and Prevention (CDC), Emory University, Georgia State University, local government leaders and health departments to create the Healthy Places Research Group (HPRG) in 2003.\nThey set out to address questions such as: How is public health influenced by the built environment? What role do policy makers, planners, architects, engineers and designers play in addressing these challenges?\nRoss, director of the Center for Quality Growth and Regional Development (CQGRD), and colleagues sought to answer these and other questions through both applied research and by building multi-disciplinary partnerships.\nThe group is guided by the concept that \"Healthy Places\" can refer to buildings, neighborhoods, and even entire metropolitan areas. HPRG works to explore how to develop, design, and build places that promote good health, support community values, and restore vitality to communities.\nThe overall goal of HPRG is to provide a forum to share research and build relationships, keeping dialogue open for opportunities to work together. Participants in HPRG have formed teams which have submitted successful research proposals.\nThe early supporters of HPRG were alarmed by the increasing rates of overweight and obesity in the United States, and felt that all individuals who could change this trajectory should strive to do so.\nThese initial discussions were motivated by the increasing body of evidence that planners, architects, engineers, policymakers, designers, and others who are not in the traditional fields of public health or medicine can still positively influence public health.\nOne early steering committee member was Andrew Dannenberg, an M.D. with a master’s in public health, who is currently an affiliate professor in Environment and Occupational Health Sciences as well as Urban Design and Planning at the University of Washington.\nHe states that, “The built environment influences public health in many ways - for example through rates of physical activity, air pollution, injuries, mental health, social capital and environmental justice. Multiple disciplines come together to create the built environment. … Design and policy interventions can positively affect obesity and chronic disease rates. Public health can't fix these problems alone.”\nHe still attends meetings if they occur when he is in Atlanta\nHPRG meets monthly during the academic year at the CQGRD at Georgia Tech.\nAt each meeting, speakers present their work on a common theme which explores how places can be built to promote good health. Some examples of previous topics have included:\n- Creating Green Infrastructure\n- The Relationship Between Health and Equity\n- How Healthy and Sustainable Places Drive Economic Development\n- Creating Opportunities for Physical Activity: Developing the PATH400 Trail\n- Healthy Food Access: Working with the Community\nThe contributions of the HPRG program were recognized in 2004 and 2005 by the local pedestrian advocacy group Pedestrians Educating Drivers on Safety (PEDs), for work that encourages active lifestyles and other healthy choices, such as walking.\nThe goal of PEDS is “to make communities in Georgia safe, inviting and accessible to everyone who walks.”\nSally Flocks, the Executive Director and Founder of PEDs, describes what she sees as the value of the group:\n“HPRG provides an opportunity for people in a variety of fields who are interested in connecting the built environment and public health, to come together to share their work through presentations and discussion.”\nFlocks has been a longtime supporter of HPRG and continues to attend HPRG meetings.\nParticipation is open to anyone interested in the characteristics and advancement of healthy places within the built environment. HPRG meetings typically include participants from the School of City and Regional Planning, CQGRD, Emory, the Georgia Health Policy Center at Georgia State University, the CDC, local, state and federal employees, and individuals from the private sector. It is also open to the public.\nThe first meeting of the school year in September was canceled in the wake of Hurricane Irma and will be rescheduled.\nThe October meeting will be from 7:30 – 9:00 a.m. on Tuesday, October 24, at CQGRD, 760 Spring St., Suite 213, Atlanta, Ga. 30308.\nFor More Information Contact\nDigital Communications Specialist","Karen Hacker, MD, MPH\nDr. Karen Hacker, MD, MPH, is the Director of CDC’s National Center for Chronic Disease Prevention and Health Promotion (NCCDPHP), a position she assumed in August 2019. NCCDPHP has an annual budget of about $1.2 billion and more than 1,000 staff members dedicated to preventing chronic diseases and promoting health across the life span.\nDr. Hacker leads an executive team that sets the strategic direction for the center’s portfolio, which focuses on:\n- Surveillance and epidemiology to move data into action.\n- Policy and environmental improvements to support health and healthy behaviors.\n- Health care system collaboration to strengthen delivery of preventive services.\n- Links between community and clinical services to improve self-management of chronic conditions and enhance quality of life.\nFrom 2013 to 2019, Dr. Hacker served as Director of the Allegheny County Health Department in Pennsylvania, where she was responsible for 1.2 million residents in 130 municipalities, including Pittsburgh. Under her leadership, the Department achieved national public health accreditation in 2017. Dr. Hacker also launched the Live Well Allegheny initiative, aimed at reducing cigarette smoking, obesity, and physical inactivity.\nPreviously, Dr. Hacker was the Senior Medical Director for Public and Community Health at the Cambridge Health Alliance in Massachusetts. Between 2002 and 2013, she held a variety of leadership roles at the Cambridge Public Health Department and the Institute for Community Health (both part of the Cambridge Health Alliance).\nDr. Hacker served as Interim Chief Public Health Officer (2006 to 2007) and Medical Director for the Cambridge Public Health Department (2004 to 2013) and as Executive Director of the Institute for Community Health (2002 to 2013). She also spent several years working for the Boston Public Health Commission, with a focus on adolescent health, serving as Division Director for Child and Adolescent Health (1999 to 2002), Director of Adolescent and School Services (1992 to 1999), and Director of Adolescent Services (1989 to 1992).\nAs an expert in community-based participatory research (CBPR), Dr. Hacker served as the Director of the CBPR program of the Harvard Clinical and Translational Science Award Initiative (Harvard Catalyst). She wrote Community-Based Participatory Action Research, a widely used academic text, and taught a course on the topic at the Harvard T.H. Chan School of Public Health. As the Executive Director of the Institute for Community Health, she designed, led, and published on numerous community participatory health projects. She has published 67 peer-reviewed articles on a wide variety of topics, including adolescent health and school-based health centers, obesity, substance use, and health policy.\nDr. Hacker received her BA from Yale University, her MD from Northwestern University School of Medicine, and her MPH with Honors from Boston University School of Public Health. She completed her internship and residency training in primary care internal medicine at Boston City Hospital, followed by an adolescent medicine fellowship at Children’s Hospital in Los Angeles. She is board-certified in internal medicine and has served as an Associate Professor at Harvard Medical School (2010 to 2015) and at Harvard School of Public Health (2012 to 2015). She is currently a Visiting Professor in the Graduate School of Public Health and Clinical Professor in the School of Medicine at the University of Pittsburgh."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:b42b38c1-2858-49e1-8b64-8fcdbcdf9ba2>","<urn:uuid:ca138fe0-9e60-45db-8744-f5ee35161ce1>"],"error":null}
{"question":"以色列和约旦有什么地理分界呢？想去玩的时候提前了解一下！","answer":"Israel's eastern geographical border is defined by the Jordan Valley Rift. The country's other borders include the Mediterranean to the West, Lebanon mountains to the North, and Eilat Bay marking the southern tip. Israel is situated in the Middle East, on the narrow region that connects Africa and Asia.","context":["The traveler to Israel walks through history: from windswept crusader castles to ports where seamen, pilgrims and famous travelers spent some time and then moved on; from desert landscapes that were home to traveling tribes, half-forgotten armies and merchants in camel caravans, to sheikhs’ tombs with whitened domes, silent monasteries and ancient synagogues decorated with colorful mosaics. The State of Israel was created in the Land of Israel which was promised to the People of Israel according to Jewish tradition. It was where Jesus, the Christian Messiah, was born and the place where Mohammed, the Moslem Prophet, ascended to heaven. The meeting place of three continents and two seas, the country is a skein of cultures, customs and traditions, a country that was home to many people, cultures and changing religions. On the crossroads of ancient routes of commerce, the land also saw waves of conquering armies: the Canaanites, Hebrews, Babylonians, Persians, Greeks, Romans, Arabs, Crusaders, Ottoman Turks and the British, made this much-desired small country into a battlefield where they strove for eminence, built fortifications, castles and royal palaces.\nIsrael has become known worldwide as the “Silicon Wadi”, having more hi-tech startups than any country other than the USA; dozens of companies traded on the NASDAQ; research and development facilities of tech giants such as Intel, Microsoft, IBM, Google, HP, Philips, Cisco, Oracle, SAP and CA; and industry leaders such as Teva (pharmaceuticals), Amdocs (software), Comverse (telecom)and Checkpoint (data security). Israeli companies routinely win international contests in the start up industry; the venture capital industry is vibrant; and the per capita patent rate is the highest in the world. During the conference, you will be able to have a taste of Israel’s tech industry through the conference exhibition.\nIsrael is located in the Middle East, on the narrow region connecting Africa and Asia.\nThe country's geographical borders are the Mediterranean to the West, the Jordan Valley Rift to the East, Lebanon mountains to the North and Eilat Bay marking the country's southern tip. Although small in territory, Israel's landscape and climate are varied. There are snow-capped mountains in the north alongside dry wildernesses in the south, and desolate areas alongside modern lively cities. Israel’s ethnic and religious mosaic is rich and fascinating, and it has numerous cultural institutions and entertainment centers. Thanks to its rich history and sanctity for the three monotheistic religions, it has many ancient and holy sites. Israel’s dense population does not interfere with a wealth of plants and wildlife, as well as many historical and natural attractions\nIsrael offers travelers:\n- A wealth of touring opportunities, including pilgrimage journeys, resort holidays, cultural tours, water sports adventures and desert safaris.\n- Luxury hotels, glamorous spas and unique restaurants. In addition to world- class Israeli hotels, almost every international hotel chain is represented in Israel.\n- A central location, where Europe, Asia and Africa meet.\n- Numerous connecting flights to and from all major world centers.\n- Countless reasons to bring a loved one and stay a few more days.\nBefore, during and after the conference, participants and accompanying guests can enjoy a diverse choice of sightseeing tours and excursions specially tailored to highlight the magic and spectacle of Israel."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:92d0fed5-b3d7-4492-ab34-b31b923f9b10>"],"error":null}
{"question":"Can someone explain how to verify the authenticity of movie memorabilia from both official stores and auctions? Looking for specific authentication tips! 🔍","answer":"To verify movie memorabilia authenticity, several key factors must be checked: 1) Origin and traceability - examine the item's source and ownership trail with documented evidence. 2) Identity and association - verify if the item can be easily linked to a particular aspect of the person's career. 3) Trust - ensure items come from recognized sources. At It's a Wrap store, authenticity is verified through production tags and code legends posted around the shop that link items to specific movies or TV shows. For auction items, experts recommend being extremely cautious as forgeries are common - even authentication services can be fooled, as FBI agent Timothy Fitzsimmons notes that forgers sometimes manage to get fake signatures authenticated while real ones are marked as forgeries. The most important factor is tracking an item's history back to its ultimate source.","context":["Los Angeles is the land of crazy luxury shopping, picture-perfect beaches, tasty food (do not miss out on the best tacos in LA!), and an overall glitzy vibe with an infusion of laid-back relaxation. Visitors flock to LA for its sunny glamour, and nothing may be more glamorous than a thorough exploration of all things Hollywood! Yes, visitors can get the Hollywood experience by taking a tour of Universal Studios or visiting the Chateau Marmont (the best place for celeb sightings, by the way!). However, this LA-area thrift shop offers more than just a brush with Hollywood; visitors can actually buy a piece of it.\nLocated in Burbank, California, It’s a Wrap thrift store sells movie and television costumes from sets located right around the corner at all the major film lots. From millennial favorites like “The Vampire Diaries” to classic long-running sitcoms, it’s the only store open to the public where regular old fans can buy things from their favorite Hollywood productions. Ready to peruse the racks? Read on to find out how!\nWhat To Know About Buying Hollywood Movie Items At This Unique Thrift Store In Los Angeles\nHollywood sign in the hills of Hollywood\nFounded in 1981, this woman family-owned business has been selling Hollywood’s costumes since 1981. It’s a Wrap is extremely unique; they boast that they are “the only store in the world” where the public can directly purchase movie and television costumes. No gatekeeping auction houses, no crazy online markups, just some good old-fashioned thrifting across two stories of bursting racks and shelves of clothing, accessories, and props!\nEach of the top neighborhoods in Los Angeles and every LA-adjacent town has its unique flare, and Burbank’s vibe is in harmony with its Hollywood links. It’s a Wrap’s Burbank location makes it central to the surrounding Hollywood studios and a prime spot for sourcing and selling Hollywood merchandise. It’s also close-by other oddities and memorabilia shops filled with Hollywood souvenirs (and a fantastically stocked Goodwill)!\n- Address: 3315 W. Magnolia Blvd. Burbank, CA 91505\nPro-tip: get there early for street parking; otherwise, parking gets complicated!\nIt’s A Wrap Hours\nIt’s a Wrap is extremely accessible to the public and has really great hours of operation. Can’t make it to the store? They sell a small selection of their pieces remotely, but it’s best to email them for more information. Here are their store’s operating hours:\n- Monday & Tuesday, Saturday & Sunday: 10:30 am- 6:00 pm\n- Wednesday-Friday: 10:30 am-8:00 pm\nNo complete travel guide to Los Angeles and SoCal would be complete without this special store on the itinerary, especially if someone’s a shopaholic and big-screen fan. Even before shoppers walk through the doors, they have the opportunity to shop the racks outside, which feature the store’s best deals. This is more of the traditional “thrift” experience where shoppers may have to sift through racks and racks of discounted products to find the diamond in the ruff. But if willing to hunt, dedicated thrifters may be well rewarded. After examining the deals outside, shoppers have plenty to work through inside the store.\nSurrounded by legendary pieces of Hollywood history all over the walls of the store, shoppers will first encounter a large smattering of women’s clothing. Visitors will find everything from vintage pieces to wedding dresses to contemporary outfits with all the hallmarks of being a Hollywood costume – the interior tags often feature the character’s name in sharpie or shoppers may find a production tag intact that indicates the actor or actress who wore the piece!\nTo figure out where the costumes came from, shoppers need to check the store’s tag. Each tag has a code written on it, and code legends are posted all around the shop. The code corresponds to a movie or television show on the legend, so folks can decipher exactly what piece of La La Land glamour they’re buying. Further, into the store, there is a smaller men’s section, but don’t be discouraged by the well-curated offering. Rather, it makes sifting through the men’s racks easier, and the gems quicker to find!\nWhile it may seem like the concept of thrift shopping and buying a piece of Hollywood memorabilia may be at odds with one another price-wise, that couldn’t be further from the truth. It’s a Wrap prices its pieces with the economical shopper in mind and often runs deals to liquidate stock faster to bring in more inventory. It’s possible to pick up some genuine designer pieces as well, so shoppers can potentially hit gold twice – find a designer piece for a discounted price that was worn in a movie!\nJust a word of warning, however. Shoppers may find pieces that look worn, distressed, or in some cases, straight-up destroyed…remember these are movie and television show costumes, so some of these pieces may be altered to look like a victim of a vampire bite was wearing it! Otherwise, there are so many things (in pretty pristine condition) for shoppers to get their hands on – happy (Hollywood) shopping!","Elvis Autographs : Real or Fake: Authenticating Elvis Presley Autographs\nBy: Elvis Australia\nMay 20, 2010 - 11:10:39 AM\nElvis is difficult because there are so many variations in the way his name was signed.\nToday it is the source of the biggest rip-offs in the Elvis world.\nSo how do you tell real from fake?\nIt could be said that you can't, not 100%. Usually if it looks real, it comes down to the trust placed in the person who it was signed for and an authentication letter from them. Obviously someone famous and well connected to Elvis has a much higher probability of truth, the 'genuine article'. They can provide an authentication letter, but this is only credible if provided directly to you by the trusted person signing the letter!\nThe problem is that is it 'looks real' it means very little. Over the years Elvis' secretaries and staff became very adapt at 'signing Elvis' autograph' and this is one big reason also why it is so difficult to say what is real.\nSo an authentic document like a drivers licence, contract, employee record (as below) give the buyer the knowledge that it is a genuine signature. Whereas one on a blank piece of paper or anything on a publicly available document, like a Las Vegas Dinner Menu or scarf has no link to authenticate it.\nAbove, two authentic Elvis autographs.\nAbove left picture, one of the oldest known Elvis Presley autographs/signatures.\nA woman in Tupelo that had been employed at the school in the 1950s came across the English Fairy Tales book. The astute Milam Junior High librarian noticed that one of the names on the checkout card - which every student had to sign when borrowing a book - was that of Elvis Presley!\nWritten next to the name was the notation 7-C, signifying Elvis' 7th grade homeroom class.\nA further search of the library unearthed another card with the singing star's signature. Obtaining permission from the school, the excited woman took the two cards home and put them in her scrapbook.\nThere are many that claim they can authentic an Elvis autograph, for a fee, it is our opinion most of these a pure fraud and the rest can only authenticate the logical as mentioned above mention official documents etc.\nSo our advise, be wary.\nThis article is intended to help the many that ask us for help.\nAuthenticating Elvis Presley Autographs\n1 Origin & Traceability - The Who, What, When, and How?\nThe Items' source, it's ownership trail, and is this information supported by documented evidence?\n2 Identity & Association - Is the item intrinsically well known or easy to associate with a particular facet of the person's life, career or of some other particular significance?\n3 Trust - Does the item come from a recognized and respected source?\nDo you trust the seller? ... What are there credentials?\n4 Description - Is the item description accurate in documenting the artifacts origin and evidencing traceability. Do the people, places and dates all stack up? Is the story of how the item was originally obtained and subsequently transferred between owners plausible? Is there any corroborating evidence?\n5 Condition - Elvis has been gone for 30+ years. Hence all original items can be expected to show some signs of age however well protected. Is the condition consistent with similar artifacts of the period?\n6 Comparison with known exemplars - How does the item compare with known authenticated documents of the same type e.g. personal checks, contracts, letters, 'in-person' signed autographs from the same contemporary time period.\nProxy Signatures aka 'Secretarial':\nSome of the hardest signatures to authenticate are those that have been faked deliberately freehand in a style designed to duplicate the original form of the autograph. Long before autopens had been in wide use, many vintage Hollywood stars were known to employ secretaries who were authorised to sign photographs and fan mail replies on their behalf. Frank Sinatra and Walt Disney are two examples where it can be sometimes difficult to establish provenance, especially in Mr. Sinatra's case recognising the wide variation of his signature across a long and illustrious career. Elvis' office at Graceland are known to have signed letters, and thank you note replies back to fans etc. These items are referred to as 'Secretarial' autographs to help satisfy the enormous number of requests in reply to fan mail. Numerous examples abound of Hollywood dignitaries of the period using a similar method. Experts report in the case of JFK that\n'... you can tell if it was by secretary one, two or three'.\nAbove, Elvis Presley's Signature and note to President Richard M. Nixon.\nYou will find is the use of 'stock template' autographs provided by Elvis to use pre-printed on various items of merchandise and in particular album covers, photo albums and the like. These are fairly easy to spot and for the most part come from one or other of the following samples provided by the Graceland archives:\nElvis autographs used as sample to be reproduced on merchandise.\nMost celebrity autographs are fake!: Fake Celebrity Autographs Outnumber RealET Entertainment - AP By Colleen Long, Associated Press Writer NEW YORK\nBefore you plunk down hundreds for a signed copy of a dusty Beatles album or a golf ball whacked by Tiger Woods, take a good look at it. Chances are, it's fake.\nMost celebrity autographs are incredibly difficult to authenticate, experts say, and if a deal to purchase something autographed by your favorite star seems too good to be true, it probably is.\nOnly six percent of all autographed Beatles memorabilia is authentic, according to PSA/DNA Authentication Services, a California-based organization that examines collectibles.\nOnly 24 percent of Marilyn Monroe and Elvis Presley signatures PSA/DNA has examined were genuine, and only 33 percent of more than 10,000 Woods and Michael Jordan autographs they scrutinized were real.\nCelebrities are aware of the problem, but there's not much they can do. Woods won't sign golf balls, because they are such easy targets for forgeries. He says he has seen hundreds of false autographs. 'I feel sorry for the person who buys it and thinks it's authentic', Woods told The Associated Press.\n'It's a reflection not only on the person that sold it — and they don't care, they get their money. I don't want to leave that impression, because I didn't do it.' PSA/DNA has been tracking fakes for about two years, and started tracking celebrity autographs last year, president Joe Orlando said. The company's experts analyze handwriting and materials, and preside over signings to authenticate signatures. Items are outfitted with a hologram which tracks the life of the signed poster, jersey or album cover. So far, the company has authenticated more than eight million items, but that doesn't scratch the surface of the millions of forged items out there, Orlando said. 'People's autographs change over time, and you must have a keen eye as a collector', he said. 'Experts must know how they change and when they change.\nEven then, signatures are different all the time. Forgers capitalize on that. It's very tricky'.\nMemorabilia can also be pricey, but it's worth it to collectors. A Babe Ruth signed ball went for $115,000 and one of his bats fetched $1.3 million at auction. Generally cards, jerseys, albums and posters can be worth a few hundred dollars. Woods, Jordan and many celebrities use services like Upper Deck or PSA/DNA to verify that their merchandise is real. 'It's curtailed it a little bit', Woods said. 'Now we have a way to make sure my signature is authentic with the hologram. The thing is, the average person who wants a quick hit just goes on online auction. 'OK, I get can that for $100,' and boom, they got it', even though it might not be real.\nBut FBI agent Timothy Fitzsimmons said it's dangerous to rely too much on authenticators.\n'The forgers sometimes go to great lengths to get items authenticated', he said. 'Sometimes, forged signatures were even identified as real ones, and the real ones as forgeries'.\nFitzsimmons says it's most important to find out the item's history, all the way back to its ultimate source. Based in San Diego, he has been working on prosecuting forgers for the past eight years. He said more than 60 suspects have been arrested around the country for selling forged merchandise, mostly on Internet auction houses. 'Marlon Brando, Tom Cruise , these are very famous people and their signatures are worth a lot because they don't sign too often', he said. 'Fakes just breed on the Internet'.\nThey must know what they're spending their money on', said Chris Donlay, spokesman for the site. Donlay said there are links to organizations that help figure out if an item is real. Estee Portnoy, Jordan's business manager, said the basketball legend has had a contract with Upper Deck for about 14 years, and he does signings every month. 'I can't tell you the number of people who call our office, especially parents, to see if we can verify if something they bought is authentic', Portnoy said. 'About 99 percent of the time, we can't'.\nTupelo's Own Elvis Presley DVD + 16 page booklet.\nNever before have we seen an Elvis Presley concert from the 1950's with sound. Until Now! The DVD Contains recently discovered unreleased film of Elvis performing 6 songs, including Heartbreak Hotel and Don't Be Cruel, live in Tupelo Mississippi 1956. Included we see a live performance of the elusive Long Tall Sally seen here for the first time ever.\nThis is an excellent release no fan should be without it.\nThe 'parade' footage is good to see as it puts you in the right context with color and b&w footage. The interviews of Elvis' Parents are well worth hearing too. The afternoon show footage is wonderful and electrifying : Here is Elvis in his prime rocking and rolling in front of 11.000 people. Highly recommended."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:31921b07-f07f-44b1-b831-caf9df543f28>","<urn:uuid:870192b6-bfea-40d0-ad83-2a27cd0e96b4>"],"error":null}
{"question":"Could you explain basic steps for analyzing customer data in Excel?","answer":"The basic steps for analyzing customer data in Excel are: 1) Identify questions you want to answer, 2) Add categorization columns to group numeric data into bands, 3) Perform basic analysis using COUNTIF, SUMIF, and AVERAGEIF functions for single-field segmentation, 4) Conduct cross-reference analysis using COUNTIFS, SUMIFS, and AVERAGEIFS for multiple criteria, 5) Perform trend analysis over time, 6) Check assumptions about the data, and 7) Review insights with operational staff.","context":["One of the most useful aspects of Microsoft Excel is its ability to quickly slice and dice customer data from live systems to identify important trends and behaviors which can inform strategy. In this article I will share with you a simple 7-step plan based on a fully worked example of Customer Data Analysis using Excel.\nThe example I will use is a retail banking customer dataset however the same principles and techniques apply equally well to any live customer dataset for any business sector.\nTo master this approach you simply need to understand the power of 3 excel functions – IF, COUNTIF (single criteria) and COUNTIFS (multiple criteria) along with their cousins SUMIF and AVERAGEIF.\nFor a copy of the worked example excel spreadsheet containing all the formula please email me.\nMy example spreadsheet, which would be extracted from your live customer system, has one row per customer containing just the following fields:\n- Customer Reference\n- Interest Rate\n- Date A/C Opened\nThe objective of the analysis is to understand this data to see if we can spot any useful business insights which could inform our strategy on customer acquisition and retention.\nStep 1 – Identify some questions you want answers to\nBefore you start any data analysis it’s always a good idea to identify a number of questions you want answers to. At a minimum you need to answer these. However during the analysis the results will also suggest other questions you can answer which you won’t have anticipated!\nStep 2 – Add the required Categorisation Columns\nThe problem with numeric fields such as balances or interest rates is that all we can really do with them in their raw state is is to total them or average them. To go further we need to group them into bands and categories.\nIn the example I have created a new column (F) in the spreadsheet called “Balance Band” to group customers by balance by simply using the excel IF function as follows:\n=IF(D2>1000000, “1M+”, ( IF(D2>500000, “500K-1M”,(IF(D2>250000, “250-500K”,(IF(D2>100000, “100-250K”,(IF(D2>50000, “50-100K”,(IF(D2>10000, “10-50K”, “1-10K”)))))))))))\nA similar problem arises with dates so I have created a new column (G) in the spreadsheet “Account Age” (in months) using the excel DATEDIF function.\nStep 3 – Perform the Basic Analysis\nNow we are ready to begin the basic analysis of the data – first by simply segmenting the data by a single field such as Region or Balance Band. I do this using the excel COUNTIF, SUMIF and AVERAGEIF functions. It’s important to do this first and to check that the totals (by rows and values) are correct before we move on to the move advanced analysis. It’s a simple job then to graph the data using the excellent excel chart facilities.\nFor example, we can count the number of customers in North Region in Cell B3 with the formula ‘COUNTIF(LIVE!$B$2:$B$101, $A3)‘ which uses the COUNTIF function in excel.\nIn our example the basic analysis (FIGURE 1 below) indicates that South Region accounts for 17% of customers by number but 30% by value and has an average customer balance 2-3 times greater than the other regions. It looks like South is unique in the fact that it is dealing with a small number of very valuable customers?\nFIGURE 1 – Basic Analysis\nStep 4 – Perform the Cross-Reference Analysis\nIt’s unlikely the basic analysis will tell us very much we do not already know but we might be surprised. However when we start to do the cross reference analysis new insights will emerge.\nTo achieve this we need to use an extended version of the COUNTIF, SUMIF and AVERAGEIF functions imaginatively named COUNTIFS, SUMIFS and AVERAGEIFS which allow us to supply multiple criteria. The simpler versions of the function are single criteria.\nIn our example the cross-reference analysis (FIGURE 2 below) shows that 56% of the total balances sit with just 7% the customers whose balances are greater above £250K but we are paying them less interest than the lower balance customers. Why is this? Does it make sense? Are we creating a problem? Again we repeat this kind of cross-reference analysis for all the fields in the data.\nFIGURE 2 – Cross-Reference Analysis\nStep 5 – Perform the Trend Analysis over Time\nWe now extend our analysis to include the time dimension to see how things change over time. For example, are the numbers and values of accounts opened growing over time or decreasing or static. How are the Regions doing against each other over time. Are a disproportionate amount of high-balance customers closing their accounts in the last 3 months. You get the idea.\nFor example we can calculate the average balance for each month for each region in Cell F473 with the formula ‘SUMIFS(LIVE!$D$2:$D$101, LIVE!$G$2:$G$101,$A47, LIVE!$B$2:$B$101,F$46)/1000′ which uses the SUMIFS function in excel to sum across multiple criteria, in this case using “age of account” and “customer region”.\nIn our example the Trend Analysis (FIGURE 3 below) shows that at an overall level the business is recovering well from a major problem. However when you look at this regionally it is clear that South Region’s business has in fact totally collapsed but this has been compensated for by strong growth in East Region.\nFIGURE 3 – Trend Analysis\nStep 6 – Check your assumptions about the data\nIt’s important never to do data analysis in a vacuum. First you need to check what assumptions you have made about the underlying data and confirm these with a person (e.g. in IT) who understands in detail the behaviour of the data in the live system which has been provided to you. For example, do closed accounts stay on the system forever or are they archived after 12 months? The answer to this will affect your results on account closure trends.\nStep 7 – Review the Data Insights with Operational Staff\nAll you really have so far is a set of potential insights some of which may have real value to the business and others may be trivial, wrong or can be explained in other ways. You need to share these insights and the data you based them on with staff who are intimate with the operational details of the processes and you need to be open to have your conclusions challenged, destroyed, revised, improved or confirmed.\nFinally …Some Performance Tips\nIn my experience you can easily handle large data sets (up to 100,000 records) using these techniques. Once you go above 10,000 rows you should set formula calculation to “manual” rather than automatic to maximise spreadsheet performance. Above 50,000 rows you can make a copy of your spreadsheet with all your categorisation calculations converted to values (COPY, PASTE SPECIAL (VALUES)).\nAbove 100,000 records, if you need to, you can either create multiple spreadsheets with a consolidation sheet or else you can use the excel RAND function to extract a random subset (say 25%) of the data to analyse."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:d34b2905-a3c1-470d-b5fd-54117fb65c61>"],"error":null}
{"question":"How does lysine catabolism affect the accumulation of lysine in transgenic plant seeds?","answer":"The lysine content in mature seeds of transgenic plants is determined by the balance between increased accumulation (due to high activity of feedback-insensitive DHDPS) and the rate of lysine catabolism. Enhanced lysine catabolism, shown by higher activities of lysine-ketoglutarate reductase (LKR), can prevent lysine accumulation in seeds. The effect varies among plant species - for example, in oilseed rape and soybean, lysine accumulation was significant despite some catabolism, while in maize, lysine accumulation was only detected in the embryo.","context":["InIran 2 I\n78 bp 927 bp cDNA of dhdps-2\nFigure 2 Structure of the dhdps-1 and dhdps-2 genes and cDNAs. prom, promoter; bp, base pair.\nchromosome 2 for dhdps-2 and chromosome 3 for dhdps-1. That dhdps-2 encodes a functional protein was shown by the growth on a nonsupple-mented medium of an E. coli DHDPS-deficient strain transformed with the dhdps-2 apoprotein coding sequence. Activity tests performed in the presence of increasing concentrations of lysine proved that the DHDPS-2 enzyme is also strongly feedback inhibited, with a 50% loss of activity at 30 /xM lysine.\nVia promoter-GUS fusion, expression of the dhdps-2 gene was observed during the whole developmental cycle and appears to be quite similar to that observed for the dhdps-1 gene, although the dhdps-2 gene is in general a little more expressed. DHDPS-2 activity was strongly detected in the vasculature of stems and leaves, in carpels, and in developing seeds (33).\nD. Improving Lysine and Threonine Accumulation via Transfer of Bacterial Genes\nThe availability of bacterial and plant genes encoding feedback-insensitive enzymes allowed redirecting the expression of these genes in plants and in particular in storage organs. Initial experiments made use of constructs monitored by the strong cauliflower mosaic virus (CAMV) promoter harboring a bacterial dapA gene that encodes a DHDPS partially or fully feedback insensitive and mutated alleles of the E. coli lysC gene encoding a feedback-insensitive AK (see Ref. 16 for review). Transgenic lines have been obtained in tobacco (34-36), potato (37), barley (38), Arabidopsis (39), and oilseed rape and soybean (40).\nAs previously mentioned for Nicotiana sylvestris mutants, expression in tobacco of the bacterial AK led to accumulation of free threonine not only in vegetative tissues and in tubers in the case of potato but also in reproductive organs and seeds. In the case of the bacterial DHDPS, the lysine overproduction, although present in vegetative tissues, was not expressed in mature seeds. Constitutive overexpression of the bacterial DHDPS gene was also accompanied by phenotypical alteration of the transgenic plants, as observed with the selected N. sylvestris mutants, at least when a high lysine level was reached (36). Targeting the expression of these bacterial genes to sink organs such as seeds was then proposed as a way to alleviate such deleterious effects. Expression of the chimeric AK and DHDPS gene under the control of the /3-phaseolin promoter obtained from the bean gene coding for this seed protein was thus evaluated. Transgenic tobacco seeds showed an important increase in free threonine but no significant change in the amount of lysine in mature seeds, although the activity of bacterial feedback-insensitive DHDPS was clearly higher than the DHDPS activity measured in nontransformed plants.\nThis lack of lysine accumulation in tobacco seeds was then ascribed to enhanced lysine catabolism as shown by higher activities of the major lysine catabolism enzyme lysine-ketoglutarate reductase (LKR) (41). We have to underline that the lysine content present in mature seeds of transgenic plants appears to be the result both of increased accumulation due to high activity of a feedback-insensitive DHDPS and of the rate of lysine catabolism. According to the balance between enhanced biosynthesis and induced catabolic degradation, the accumulation of lysine in seeds may vary among plant species (Table 1). This is best shown by the results obtained by Falco et al. (40) and Mazur et al. (42) when similar bacterial ak and dhdps genes were transformed into oilseed rape and soybean. Alone or combined with AK, expression of the bacterial DHDPS resulted in a dramatic increase of free lysine (10-100 times the level of nontransformed plants) with a significant effect on the total lysine content, which increased more than twice. In these last cases, accumulation of catabolic products such as saccharopine and a-aminoadipic acid was eventually observed but only at a minor level. However, in maize, when the bacterial AK and DHDPS were expressed with an endosperm or an embryo-specific promoter, lysine accumulation was detected only in the embryo but was sufficient to raise the overall lysine concentration in seed by 50 to 100% (42). Bacterial enzymes can thus be expressed with success in vegetative and sink organs such as seeds leading to deep modifications in lysine metabolism.\nWas this article helpful?"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:5646746a-5a33-4ee5-94b0-c7275c3c58db>"],"error":null}
{"question":"How do water quality impacts compare between hydraulic fracturing operations and industrial effluent discharge?","answer":"Hydraulic fracturing's impact on water quality occurs through multiple mechanisms including spills of fracturing fluids (median 420 gallons per spill), underground migration of contaminants through failed well casings, and inadequate wastewater treatment, though documented cases of contamination are relatively few compared to the number of fractured wells. Industrial effluent discharge, particularly from pharmaceutical, textile and dye industries, consistently shows high levels of contamination, with parameters like conductivity, total suspended solids, dissolved solids, turbidity, chemical oxygen demand and biochemical oxygen demand exceeding WHO permissible limits. These effluents also contain heavy metals like cadmium above permitted levels, making their impact on water quality more apparent and measurable compared to fracturing operations.","context":["“Because of the significant data gaps and uncertainties in the available data, it was not possible to fully characterize the severity of impacts, nor was it possible to calculate or estimate the national frequency of impacts on drinking water resources from activities in the hydraulic fracturing water cycle.” However, a nice addition to the final report was the “Synthesis” section. This sections written for state and local regulators identifies the factors that can be managed, changed, or used to help reduce current vulnerabilities of drinking water resources to activities in the hydraulic fracturing water cycle. Since the overall rate that fracking does impact our water resources is low, this section is excellent suggestions for reducing the risk even further through regulation and monitoring.\nThe final report examines all the potential vulnerabilities in the water lifecycle that could impact the 3,900 potential drinking water sources for the 8.6 million people living within one mile of a hydraulically fractured well. EPA’s assessment relies on existing scientific literature and data. Literature evaluated included articles published in science and engineering journals, federal and state government reports, non-governmental organization (NGO) reports, and industry publications. Data was gathered from databases maintained by federal and state government agencies, other publicly-available data and information, and data, including confidential and non-confidential business information, submitted by industry to the EPA.\nEPA concluded that there are above and below ground ways that hydraulic fracturing can potentially impact drinking water resources. These mechanisms include water withdrawals in times of drought, or in areas with, limited water availability; spills of hydraulic fracturing fluids and produced water; fracking directly into underground drinking water resources; below ground migration of liquids and gases from inadequately cased or cemented wells; and inadequate treatment and discharge of wastewater.\nEPA did not find evidence that there has been widespread impacts on drinking water resources; but did find specific instances where one or more failures in design, well completion and fluid storage led to contamination of drinking water wells. The number of identified cases, however, was small compared to the number of hydraulically fractured wells reported by EPA. This low level of documented contamination might be due to insufficient pre- and post-fracturing data on the quality of drinking water resources; the lack of long-term systematic studies of areas and lack of careful monitoring of groundwater resources and the presence of other sources of contamination precluding a definitive link between hydraulic fracturing activities and an impact when using fracking to redevelop and extend the life of existing oil and gas wells.\nThe assessment follows the water used for hydraulic fracturing from water acquisition, chemical mixing at the well pad site, well injection of fracking fluids, the collection of hydraulic fracturing wastewater (including flowback and produced water), and wastewater treatment and disposal. Though cumulatively, hydraulic fracturing used on average 44 billion gal of water a year in 2011 and 2012, this represented less than 1% of total annual water used in the United States. However, in areas of drought the need for a median of 1.5 million gallons to frack a well could impact water supplies.\nThe median amount of water used to frack a well is determined by well length, formation geology and the fracking fluid formulation. The overall amount of water used in fracking is a fraction of the daily water use in the United States.\nEPA found that surface spills were by far the most common incident reported, though they are also the most easily observed. The reported volume of fracturing fluids or chemicals spilled ranged from 5 gallons to more than 19,000 gallons, with a median volume of 420 gallons per spill. Spill causes included equipment failure, human error, failure of container integrity, and other causes (e.g., weather and vandalism). The most common sited cause was equipment failure. The frequency of on-site spills from hydraulic fracturing could be estimated for only two states. If the estimates are representative, the number of spills nationally could range from approximately 100 to 3,700 spills annually, assuming 25,000 to 30,000 new wells are fractured per year.\nEPA identified a list of 1,076 chemicals used in hydraulic fracturing fluids over multiple wells and years. These chemicals include acids, alcohols, aromatic hydrocarbons, bases, hydrocarbon mixtures, polysaccharides, and surfactants. According to the EPA’s analysis of disclosures, the number of unique chemicals per well ranged from 4 to 28, with a median of 14 unique chemicals per well. In addition, EPA reports an estimated 9,100 gallons of chemicals are mixed with the median 1.5 million gallons of water per well. Given that the number of chemicals per well ranges from 4 to 28, the estimated volume of chemicals injected per well may range from approximately 2,600 to 18,000 gallons.\nEPA found that groundwater can be impacted by fracking fluids or methane gas if the casing or cement on a well are inadequately designed or constructed, or fail. A study done in the Williston Basin in North Dakota suggests that the risk of groundwater contamination from leaks inside the well decreases by a factor of approximately one thousand when the well casing extends below the bottom of the drinking water aquifer.\nEPA suggests that fracking of older wells to restore production can contribute to casing degradation and failure, which can be accelerated by exposure to corrosive chemicals, such as hydrogen sulfide, carbonic acid, and brines. No data was provided on this risk. The study found that one of the best protections for groundwater is the physical separation between the gas production zone and groundwater resources. Many hydraulic fracturing operations target deep formations such as the Marcellus Shale or the Haynesville Shale, where the vertical distance between the base of drinking water resources and the top of the shale formation may be a mile or greater.\nHowever, not all hydraulic fracturing is performed in zones that are deep below drinking water resources, but should be. The EPA’s survey of oil and gas production wells hydraulically fractured in 2009 and 2010 estimated that 20% of wells had less than 2,000 feet of vertical separation between the point of shallowest hydraulic fracturing and the base of the protected groundwater. There are also places in the subsurface where oil and gas resources and drinking water resources co-exist in the same formation. When hydraulic fracturing occurs within these formations the process injects of fracturing fluids into formations that may currently serve, or in the future could serve, as a source of drinking water for public or private use and should not be allowed.\nWater, of variable quality, is a byproduct of oil and gas production. After hydraulic fracturing, the injection pressure is released and water flows back from the well. Initially this water is similar to the hydraulic fracturing fluid, but as time goes on the composition is affected by the characteristics of the formation and possible reactions between the formation and the fracturing fluid. EPA calls all this water produced water. The final area that EPA looked at was management of the produced water. Hydraulic fracturing generates large volumes of produced water that require management. In 2007, approximately one million active oil and gas wells in the United States generated 2.4 billion gallons a day of wastewater. It is unknown what portion of this total volume is produced by hydraulically fractured wells, but really, after the flowback period there is little difference.\nAs is pointed out in the report wastewater management and disposal could impact drinking water resources. Inadequate treatment of wastewater could result in discharge of contaminated water to rivers and streams. Though, in recent years a larger proportion of fracking produced water is reused. In addition, spills can occur during transportation of wastewater away from the well head or spills and leaks from wastewater storage pits. Also, migration of contaminants from inappropriate use of land application to dispose of of wastewater, and other inappropriate methods of wastewater treatment.\nThe take away is there is still the need for more research to be able to fully model and understand fracking. In addition there is a need for more extensive baseline studies prior to drilling and long term monitoring to even know if water (or human health) has been impacted by fracking. Predrilling data needs to include measurements of groundwater and surface-water quality and quantity. There have been virtually no comprehensive studies on the impact of fracking on human health while state regulators and laws in some instances allow fracking virtually in people’s backyards and without adequate vertical separation from groundwater supplies. Fracking needs to be well understood and the risks managed to make sure that is a boon to mankind and only is used in appropriate geology and low risk locations. Many of the sources of contamination identified by EPA could be prevented or reduced with more care, training and thought. Overall, hydraulic fracturing for oil and gas is a practice that continues to evolve and our methods and controls should continue to evolve with it.","Background. Discharged effluents from industry have been responsible for the deterioration of the aquatic environment in many parts of the world, especially in developing countries. Increasing industrialization and urbanization have resulted in the discharge of large amounts of waste into the environment, resulting in high pollution loads. Utilization of microbes such as fungi and bacteria have been used for pollution degradation.\nObjectives. The aim of this research was to utilize microbial agents such as fungi and bacteria to reduce pollutant loads such as heavy metals in effluent samples.\nMethods. Three types of effluent (pharmaceutical, textile effluent, and dye) were obtained from Surulere in Lagos Metropolitan Area, Nigeria. Heavy metals analysis was carried out using a flame atomic adsorption spectrophotometer according to standard methods. Samples were cultured for microbes and identified. Bacteria samples were inoculated on nutrient agar and incubated at 37°C for 24 hours. Fungi counts were carried out using potato dextrose agar and incubated at 28°C for 3–5 days. The isolated organisms were identified based on their morphological and biochemical characteristics. Then 100 mL of the effluents was dispensed into 250 mL flasks, and the pH of the medium was adjusted to 7.2 by the addition of either sodium hydroxide or hydrogen chloride and autoclaved at 121°C for 15 minutes. The autoclaved flask was inoculated with 1 mL of bacteria and fungi for 21 days and pH was recorded properly every 48 hours.\nResults. The results of the physicochemical parameters indicated that conductivity, total suspended solids, total dissolved solids, turbidity, chemical oxygen demand and biochemical oxygen demand for all the three industrial effluents were higher than the World Health Organization (WHO) permissible limits. Heavy metal analysis results show that the effluents had high values for cadmium, above the WHO limit of 0.003 mg/L. Concentrations of zinc ranged from 0.136–1.690 mg/L, and nickel ranged between 0.004–0.037mg/L for the three effluents, within the WHO limit. The identified bacteria were Bacillus subtilis, Klebsiella pneumonia, Salmonella typhi and Bacillus cereus and isolated fungi were Aspergillus flavus and Penicillium chrysogenum. All the physicochemical parameters and heavy metal concentrations were reduced after the biodegradation study in the effluents.\nConclusions. The responses observed in the various microbes indicated that the use of microbes for the reduction of environmental pollutants has an advantage over the use of other methods because it is environmentally friendly, low cost, and no new chemicals are introduced into the environment. This method should be encouraged for pollution reduction to bring about ecosystem sustainability advocated for Ghana.\nMicroorganisms have been utilized in environmental remediation for decades. Bioremediation is defined as the use of biological agents such as bacteria, fungi, or green plants (phytoremediation) to remove or neutralize hazardous substances in polluted soil or water. 1,2 Increasing human population has led to an increase in industrial activities. One of the main sources of pollution worldwide is the textile industry and its dye-containing wastewaters. About 25% of textile dyes are lost during the dyeing process, and 2–20% are discharged as aqueous effluents in different environmental components. The discharge of dye-containing effluents into the water environment is undesirable because of its colour, direct release and its breakdown products are toxic, carcinogenic or mutagenic to life forms due to carcinogens such as benzidine, naphthalene and other aromatic compounds.3 The textile industry generates a high volume of waste water with the potential for water pollution. Among the many chemicals in textile waste water, dyes are major pollutants.4\nEnvironmental problems such as appearance of colour in discharges from various industries, combined with the increasing cost of water for the industrial sector, have made the treatment and reuse of effluent increasingly attractive to the industry.5\nThe textile industry is one of the oldest industries in India with over 1000 factories. Due to the volume and composition of its effluent, textile wastewater is considered to be the most polluting among all of the industrial sectors.6,7 Wastewater from a typical textile plant is characterized by high values of biochemical oxygen demand (BOD), chemical oxygen demand (COD), colour and pH.8,9 It is a complex and highly variable mixture of many polluting substances ranging from inorganic compounds and elements to polymers and organic products.10 The incomplete use of dye and washing operations result in textile wastewater retaining a considerable amount of dye.11\nPharmaceutical wastewater is a complex mixture of different organic and inorganic compounds, including residues of active pharmaceutical substances, solvents, and toxic and bio-recalcitrant chemicals that inhibit microbial activity of the activated sludge process and present a great challenge for the proper treatment and downstream processing of wastewater.12 In the pharmaceutical industry, wastewater is mainly generated through equipment washing activities. Although the wastewater discharged is small in volume, it is highly polluted due to the presence of substantial amounts of organic pollutants. Levels of wastewater pollution vary from industry to industry, depending on the type of process and the size of the industry.13 Typically, pharmaceutical wastewater is characterized by a high COD concentration and some pharmaceutical wastewaters have COD levels reaching as high as 80.000 mg/L. Pharmaceutical companies are one of the major contributors of hazardous and toxic effluents. Ireland alone generates about 43 tons BOD in its pharmaceutical industry.14 The recycling of treated wastewater has been recommended due to the high levels of contamination stemming from dyeing and finishing processes (i.e. dyes and their breakdown products, pigments, dye, intermediate, auxiliary chemicals and heavy metals).15–18\nThe aim of this research was to utilize microbial agents such as fungi and bacteria to reduce pollutant loads such as zinc, cadmium, and nickel in effluent samples.\nEffluents were collected in Surulere in Lagos Metropolitan Area, Nigeria from different industries from their main sites. Surulere is a commercial area where many manufacturing industries are located. The coordinates of the sample locations are presented in Table 1. Lagos Metropolitan Area is a megacity and contains 70% of the industries of Nigeria.\nEffluent samples were collected at the discharge pipe at about 7:30 am with three replicates (A1, A2, A3, etc.) from pharmaceutical, textile and dye industries. The samples were collected in sterile sample bottles and transported immediately to the laboratory and stored at around 4°C. Then, 250 mL samples were collected and put in sterile reagent bottles (500 mL capacity). The samples were subjected to immediate physicochemical analysis on site. These samples served as the source for the isolation of micro-organisms.\nPhysicochemical and Heavy Metal Analysis of Effluents\nAll samples were analyzed for heavy metals (zinc, cadmium, and nickel) and physicochemical parameters according to internationally accepted procedures and standard methods.19,20 The analyzed parameters included temperature, chemical oxygen demand, dissolved oxygen, biochemical oxygen demand, turbidity, odour, colour, total suspended solids, pH, conductivity, and total dissolved solids. In addition, pH, temperature, and dissolved oxygen were determined on site using appropriate meters (pH meter hanna HI9813, TDS-3 HM digital for temperature, and DO analyser model JPSJ-605). The concentrations of heavy metals were determined using an atomic absorption spectrophotometer.\nTotal Bacterial Count\nThe collected samples were analysed for the presence of microorganisms. First, 1 mL of each effluent sample was transferred into 9 mL of sterile saline solution in a test tube and shaken vigorously. The solution was serially diluted and 10−3 dilution was taken and plated using the pour plate technique on Petri dishes. The bacteria were inoculated on nutrient agar and incubated at 37°C for 24 hours. This was carried out using procedures which have been previously reported.21\nTotal Fungal Count\nFungal counts were conducted using potato dextrose agar with 10% tartaric acid using the spread plate method. This was carried out according to previously reported methods.22 Microbial count of the effluents samples were reported as colony forming units per gram (cfu/g).\nCharacterisation and Identification of Organisms\nThe identification of bacteria was based on biochemical characterizations including citrase, urease, catalase, indole, raffinose, xylose, galactose, starch hydrolyses, and oxidase reaction. The macroscopic colonial appearances of fungal growth in plates were observed and recorded. The macroscopic examinations were based on colony texture, size, pigmentation, time of growth, color on the reverse side of the plate and colony margin.23 A drop of lactophenol cotton blue was placed on a grease free, scratch-free glass slide.24 A small portion of the fungal growth was picked with a wire loop and teased out using a mounting needle. The preparation was covered with a cover slip.25 The slide was observed under 10× and 40× objective lenses. Observed characteristics were recorded and compared with the established identification keys as previously described.26\nBiodegradation of Effluents\nMineral salt medium prepared with the following composition was used for the studies: disodium phosphate (1.065 g) ammonium chloride (0.25 g), magnesium sulfate heptahydrate (0.10 g), monopotassium phosphate (0.65 g), and added to 500 mL of the effluents. Then, 100 mL of the effluents was dispensed into 250 mL flasks, the pH of the medium was adjusted to 7.2 by addition of either sodium hydroxide or hydrogen chloride, and autoclaved at 121°C for 15 minutes. The autoclaved flask was inoculated with 1 mL of bacteria inoculum of the microorganism and the flask was incubated for 21 days. The pH was recorded every 48 hours.\nMineral salt medium prepared with the following composition was used for the studies: disodium phosphate (1.065 g), ammonium chloride (0.25 g), magnesium sulfate heptahydrate (0.10 g), monopotassium phosphate (0.65 g), and added to 500 mL of the effluents. Then, 100 mL of the effluents was dispensed into 250 mL flasks, the pH of the medium was adjusted to 5.6 by addition of either sodium hydroxide or hydrogen chloride, and chloramphenicol was added and autoclaved at 121°C for 15 minutes. The fungi plate was emulsified with 10 mL of sterilized distilled water, and then 1 mL of the fungal inoculum was inoculated into each autoclaved flask. The flasks were kept in the mechanical shaker and incubated at room temperature for 21 days. The pH was recorded at 3-day intervals.\nPhysicochemical Parameters of Effluents\nThe results of the physicochemical parameters show that conductivity, total suspended solids (TSS), total dissolved solids (TDS), turbidity, COD and BOD for all the three industrial effluents were higher than the World Health Organization (WHO) permissible limits for water quality.27 The effluent from the local dye industry had the highest values for pH (12.02), conductivity (24500 m scm−1), DO (10 mg/L), TSS (7100 mg/L), TDS (12.500 mg/L), COD (290 mg/L), and BOD (150 mg/L), compared to effluents from the other two industries.\nThe pH of pharmaceutical and textile effluents were within the WHO permissible limits.27 Physical observation revealed the colour of pharmaceutical effluent, textile effluent and local dye effluent to be yellowish, black and reddish-brown, respectively. Odour was observed to be choky for pharmaceutical effluent and pungent smell was observed for textile and local dye effluents. Heavy metal analysis results show that the effluents had high values for cadmium, above the WHO limit of 0.003 mg/L. The concentration of zinc ranged from 0.136–1.690 mg/L, and the concentration of nickel ranged from 0.004–0.037 mg/L for the three effluents, all within the WHO limit.\nBiochemical Identification of Bacterial Isolates\nThe results of the biochemical test are presented in Table 3. The identified bacteria included Bacillus subtilis, Klebsiella pneumonia, Salmonella typhi and Bacillus cereus. The isolates were Gram-positive rod for all samples from pharmaceutical and local dye effluent, and Gram positive and Gram negative rod for samples from textile effluent. Isolates were all negative for urease for samples from pharmaceutical effluent and negative for hydrogen sulfide gas production and indole for all samples from the three effluents.\nMorphological Characteristics and Identities of Isolated Fungi Associated with Biodegradation\nThe isolated fungi were Aspergillus flavus and Penicillium chrysogenum. The colony morphology and microscopic characterization are presented in Table 4.\nThe microorganisms used for biodegradation include Bacillus subtilis for sample A (pharmaceutical effluent), Salmonella typhi for sample B (textile effluent) and Bacillus cereus for sample C (Dye effluent). There was a reduction in pH of the three effluents after 21 days. The pH of the pharmaceutical effluent reduced from 7.20 – 6.70, pH of the textile effluent reduced from 7.20-6.94, and pH of the dye effluent reduced from 7.20 – 7.00 after 21 days.\nResults of the physicochemical analysis and heavy metal concentrations of the three effluents before and after biodegradation are presented in Table 5. All the physicochemical parameters and heavy metal concentrations were reduced after the biodegradation study (Table 5). Reductions of 18.41%, 9.0%, and 32.00% were recorded for conductivity in the pharmaceutical, textile and dye effluent, respectively. There was a 55.94%, 32% and 62% reduction in dissolved oxygen in the pharmaceutical, textile and dye effluent, respectively. A reduction of 13.75%, 11%, and 29% was recorded for turbidity in pharmaceutical, textile and dye effluent respectively. There was a reduction of 38%, 82.5% and 26% for total suspended solids in the pharmaceutical, textile and dye effluent, respectively. Total dissolved solids value showed a 62%, 28% and 33% reduction in pharmaceutical, textile, and dye effluent, respectively. A reduction of 51%, 43.05%, and 78% was recorded for COD in the pharmaceutical, textile and dye effluent, respectively. A reduction of 46%, 35%, and 77% was recorded for BOD in the pharmaceutical, textile and dye effluent, respectively. There was a reduction of 63%, 48%, 53% for cadmium, 50%, 56%, 58% for nickel, and 41%, 20%, 36% for zinc in pharmaceutical, textile and dye effluents, respectively.\nThe microorganism used for biodegradation was Penicillium chrysogenum, since it is common to all three effluents. There was a reduction in the pH of the three effluents after 21 days. The pH of the pharmaceutical effluent reduced from 5.6-5.33, the pH of the textile effluent reduced from 5.60-4.92, and the pH of the dye effluent reduced from 5.60-5.26 after 21 days.\nIn this study, the results of the physicochemical analysis of the three effluents showed levels of almost all the analyzed parameters to be higher than WHO permissible limits.27,28 This is in agreement with a report by Lokhande et al., which stated the physicochemical parameters of effluent was greatly increased in the paint, pharmaceutical and dye industry effluent compared with permissible limits.30 Heavy metal analysis in the three samples revealed the presence of cadmium, nickel and zinc, which may be because that these metals form part of the chemical constituents of various mixtures used as by-products in these industries. Similar findings were also reported by Anyakora et al., who stated that the concentration of different metals in the effluent varied significantly, giving credence to the idea that these metals are part of the manufacturing process.31 Cadmium was detected in the highest concentration in the sample of pharmaceutical effluent. The concentration of nickel was also revealed to be the highest in the textile effluent, followed by local dye effluent, whereas zinc was observed to be highest in local dye effluent. This was in line with the findings of Ogunleye et al., which reported an increase in the heavy metal concentration in some analyzed industrial effluents, beyond the permissible limits.32 In addition, Obasi et al., similarly reported the presence of zinc and nickel in pharmaceutical waste water from Lagos Metropolitan Area.33 These metals are known to be readily soluble in water, which makes them available for aquatic life to take up. These metals can also be taken up into the fat tissues of aquatic organisms and become magnified along the food chain, which may lead to detrimental health issues if the there is no proper check on industrial discharges. This was reported by Haman and Bottcher, who stated that long-term exposure to cadmium can cause serious damage to the liver, kidney, bone and blood.34\nThe microbial analysis of the samples revealed the presence of four bacteria: Bacillus cereus, Salmonella typhi, Kleb pneumoniae, Bacillus subtilis and two fungi: Aspergillus flavus, and Penicillium sp. B. subtilis. Aspergillus flavus and Penicillium chrysogenum were isolated in pharmaceutical effluent, Aspergillus flavus, Penicillium chrysogenum, Bacillus cereus, Salmonella typhi and Kleb pneumoniae were observed in the textile effluent, while local dye effluent contained Penicillium chrysogenum, Bacillus subtilis and Bacillus cereus. The pharmaceutical effluent was biodegraded using Bacillus subtilis and Penicillium chrysogenum, and a reduction in value of physicochemical parameters and heavy metals concentration was observed. Biodegradation of the textile effluent by Salmonella typhi and Penicillium chrysogenum showed a reduction in the initially recorded values of physicochemical parameters and heavy metals. This shows that the bacteria and fungi used were capable of breaking down and utilizing these pollutants with low or no impact on the various components of the aquatic environment. These results are in agreement with those of Joutey et al., who reported that microbes that inhabit the soil and groundwater utilize some pollutant chemicals for food and when they completely digest the chemicals and change them into water and harmless gases.35 The reduction of the physicochemical values by the isolated microbes may be due to consumption of inorganic and organic matter by microbes for food, a conclusion supported by the work of Elizabeth et al. and Noorjahan and Jamuna.36,37\nMost industrial effluents contain hazardous chemicals that may have direct or indirect impacts on aquatic biota by bioaccumulation along the food chain and which may later become biomagnified. Many heavy metals that are found in these effluents have been shown by previous studies to be toxic to both aquatic fauna and flora and therefore stricter regulation of these industries is needed.38 This present study confirmed the capability of different microbes (Bacillus subtili, Salmonella typhi and Bacillus cereus) to break down the pollutants in three effluents, pharmaceutical, textile and local dye, to a less toxic form. These microbes should be enhanced in their natural ecosystem in other to be able to degrade more of these pollutants. This method should be embraced because of its advantage over other methods; it is environmentally friendly, lower cost, equally effective, and able to bring about a cleaner and more sustainable ecosystem.\nCompeting Interests. The authors declare no competing financial interests."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:5c124740-a868-4922-932b-23edd06b72a6>","<urn:uuid:640c071d-def6-448d-8cc0-e56a8436dc81>"],"error":null}
{"question":"¿Cuáles son las ventajas principales de exponer cultivos celulares en la interfaz aire-líquido para estudiar contaminantes del aire?","answer":"The air-liquid interface exposure method offers several key advantages: 1) It eliminates the need for time-consuming particle collection on filters, 2) Gases and particles can be sampled either simultaneously or separately, 3) There's no need to resuspend particles in biological medium, which could cause chemical and physical changes, 4) Particles deposit directly on the cell surface without solvents that could cause biological effects, similar to how it occurs in the human lung, 5) The method provides high reproducibility of deposition efficiency and biological measurements through controlled conditions.","context":["The current debates on driving bans in European cities show not only how important air quality is to the public but also reveal the lack of available methods to directly assess the adverse effects of air pollutants on human health. Exposing human lung cells to aerosols at the air-liquid interface, in a similar way to how this occurs in the open, promises significant advances and is increasingly being used by leading researchers worldwide.\nToday's legal thresholds for air pollution are partly based on epidemiological studies that correlate mortality and morbidity with measurements from public monitoring networks. The association found can demonstrate the impact of air pollutants on human health but cannot prove their immediate effects without additional toxicological data.\nIt is better to measure the biological effects of air pollutants directly as dose-response relationships. Therefore, additional animal experiments and in vitro cell culture assays are used to directly assess the impact of air pollutants on biological systems. OECD animal testing guidelines to measure the acute lung toxicity of chemicals do exist, but for ethical and financial reasons such tests are used only in isolated cases for air pollutants.\nMethods for assessing the toxicity of air pollutants using human cells have not yet been standardized. In most such laboratory experiments, collected particles are applied as a suspension to human lung cells. Not only are the conditions of these cell experiments non-physiological, the results obtained are not suitable to assess the effects of real-life, complex mixtures of aerosols resulting from incineration. The estimation of the applied dose is another difficult issue.\nAn automated system for reproducible exposure of human cells\nBelow we will present the techniques we developed for simultaneously measuring the biological effects and the particle dose of aerosols using an exposure system for human lung cells. This system enables both reproducible sampling and conditioning of aerosols as well as the exposure of cell cultures under conditions that are similar to those in the human lung. Large scale measurement campaigns have demonstrated the suitability of this system for complex combustion aerosols. The system is now being manufactured by the company Vitrocell Systems GmbH (Waldkirch, Germany) and is distributed worldwide (Fig. 1). Its basic principles were described by Paur et. al (2011) .\nFig. 1 (A) Exposure setting in an exposure chamber: the airborne particles are brought to the surface of the human lung cells through the aerosol inlet. (B) The automated exposure system to expose cells to aerosols (Photo: Vitrocell Systems GmbH)\nIn the exposure system, the cells are cultured in paralleled exposure modules on membranes supplied with nutrient medium from the base. The aerosol, which is classified through a size-selective inlet, is conditioned to a temperature of 37 °C and 85 % relative humidity and subsequently passed directly over the cell surface, where it settles. Control measurements in the parallel chambers allow replicates and simultaneous control measurements with clean air and filtered aerosol to be carried out and the deposited particle dose to be determined. The exposed cell samples are analyzed further using standardized bioassays to quantify biological endpoints that shed light on the mechanisms and effects of the aerosols. The dose-response relationship allows the toxicity, which can manifest itself in inflammation or damage to the DNA, to be assessed .\nExposing cell cultures at the air-liquid interface has several advantages. Direct sampling from exhaust fume flows eliminates the need for the time-consuming and error-prone particle collection on filters. Gases and particles can be sampled either simultaneously or separately. There is also no need to resuspend the particles in a biological medium, which could lead to chemical and physical changes in the particles and make it difficult to determine the actual applied dose. And lastly, like in the human lung, the particles are deposited directly on the cell surface without the possibility of an added solvent causing biological effects. Controlling the temperature, the humidity and the volume flow ensures high reproducibility of the deposition efficiency and therefore of the subsequent biological measurements as well. Recording all relevant physical parameters makes it easy to prove that the test procedure has been performed correctly or to identify any sources of error. The exposure system’s high level of automation guarantees compliance with the preset test protocol and thus the reproducibility of serial measurements.\nUnder standard conditions, only about 1.5 % of the particle concentration is deposited on the cells, which prevents overdosing. The applied dose can be determined by an integrated quartz crystal microbalance (QCM) if the surface concentration is at least 20 ng/cm2. At lower concentrations, image evaluation can be applied to microscopy images of exposed surfaces to estimate the dose where aerosol densities are known. To increase the deposition efficiency, an electric field was integrated into the exposure module. Depending on the charge state of the aerosol, this allows the dose that deposits to be increased up to tenfold.\nApplications for the cell exposure technique and outlook\nFig. 2 State of the art: Examined aerosols (top left column) and cell cultures (top right column) as well as bioassays from cell cultures and endpoints to determine the effects\nEspecially in recent years, research institutes and companies have increasingly been using the technique. Measurements of dose-response relationships were performed using the aerosols and cell cultures shown in Figure 2. Lacroix et al. (2018)  recently gave a review of the state of the art. When investigating complex combustion aerosols, the biological effects of gaseous and particulate pollutants can be determined simultaneously.\nCurrent research is aimed at extending exposure duration to 48 hours and increasing the deposited dose in order to use the procedure at the lower concentrations of the air in our environments. There is also a need to validate the method using animal experiments to reduce the use of laboratory animals in mandatory testing.\nCategory: Toxicology | Aerosols\n Paur, H.-R., Cassee, F.R., Teeguarden, J. et al. (2011) In-vitro cell exposure studies for the assessment of nanoparticle toxicity in the lung – A dialog between aerosol science and biology, J Aerosol Sci 42, 668-692, DOI: 10.1016/j.jaerosci.2011.06.005\n Mülhopt, S., Dilger, M., Diabate, S. et al. (2016) Toxicity testing of combustion aerosols at the air-liquid interface with a selfcontained and easy-to-use exposure system, J Aerosol Sci 96, 38-55, DOI: 10.1016/j.jaerosci.2016.02.005\n Lacroix, G. et al. (2018) Air-Liquid Interface In Vitro Models for Respiratory Toxicology Research: Consensus Workshop and Recommendations, Appl In Vitro Toxicol, 4, 1-16, DOI: 10.1089/aivt.2017.0034\nHeader image: iStock.com | Kyryl Gorlov, gruizza, StudioM1; Composing: LUMITOS\nDate of publication:"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:8a35dcff-933d-402a-9934-ad51870f8670>"],"error":null}
{"question":"Can someone explain how Weber's bureaucratic theory relates to the management vs. leadership distinction in terms of organizational efficiency and employee satisfaction? Really need to understand this for work! 🙏","answer":"Weber's bureaucratic theory aligns more closely with management functions than leadership qualities. While Weber argued that bureaucracy was the most efficient administrative form with its hierarchical structure and written procedures, this contrasts with the leadership approach which focuses on energizing organizations and creating transformative relationships. According to management experts, managers handle complex systems to run efficiently (similar to Weber's model), while leaders create systems that enable growth and evolution. However, studies have shown that Weber's bureaucratic approach can make workers inflexible and unable to respond to changing circumstances, potentially reducing both efficiency and satisfaction. This differs from effective leadership, which emphasizes motivation, communication, and vision-setting to enhance both organizational performance and employee engagement.","context":["Is there a difference between a manager and a leader? Although they share similarities, there are clear differences between the two. According to Michael Maccoby (2000), a world-renowned expert in leadership, “Management is a function that must be exercised in any business, leadership is a relationship between leader and led that can energize an organization.”\nBoth roles are crucial in an organisation, however, all too often, the two terms are used interchangeably. It is important to understand the similarities and differences between the two.\nKris Parfitt (2009), Operations Manager at Lenati, points out several comparable characteristics between good managers and good leaders, all of which aim to encourage commitment, action and personal growth: taking responsibility, active listening, consideration and courtesy, open communication, demonstrating intellectual and emotional aptitude, good judgment, open mindedness, and a focus on immediate, intermediate and long-term achievement, as well as others.\nWhile there are many obvious similarities between the two roles, the differences outweigh the similarities. Effective leaders and managers often have different personality styles. Leaders are frequently described as brilliant, courageous, charismatic, risk takers, aloof, imaginative and especially motivating. Their ideas of success typically focus on achievements. Managers, on the other hand, are often described as rational, problem-solvers, goal oriented, persistent, analytical and intelligent. Managers’ ideas of success generally center on results.\nAccording to Kotter (2012), the table below describes the typical approaches to tasks for managers and leaders.\n|Planning and budgeting||Creating vision and strategy|\n|Organising and staffing||Communicating and setting direction|\n|Controlling and problem solving||Motivating action|\n|Taking complex systems of people and technology and making them run efficiently and effectively, hour after hour, day after day||\nCreating systems that managers can manage and transforming them when needed to allow for growth, evolution, opportunities, and hazard avoidance\n“A management model is the choices made by a company’s top executives regarding how they define objectives, motivate effort, coordinate activities and allocate resources; in other words, how they define the work of management,” (Birkinshaw and Goddard 2009). When describing management techniques, three models stand out: Douglas McGregor’s Theory X and Theory Y, Max Weber’s Principles of Bureaucracy and Henri Fayol’s Principles of Management.\nMcGregor, a social psychologist, thought that there were two types of employees; those who dislike work, are unambitious and avoid responsibility (Theory X) and those who are relatively self-motivated and use their intellect and creativity in working toward organisational objectives without external pressure and who readily accept responsibility (Theory Y). Supervising Theory X employees requires an “authoritarian management style” which uses directives and punishment threats as motivation. Supervising Theory Y employees entails a “participative management style” which offers recognition and rewards as motivation. I believe today’s management should have the flexibility to use an appropriate combination of both depending on the personality of the employee. McGregor’s theory is too black and white and most employees fall somewhere on the continuum between these two extremes (Manktelow 2014).\nWeber, a founding figure of modern sociology, introduced his Theory of Bureaucracy in the late 1800’s, advocating that organisations should manage employees in an impersonal and hierarchical way, with strict rules, regulations and division of labour. The following video describes his theory in greater detail:\nWhile there are advantages to Weber’s theory such as rapid decision making and working efficiently, I think the disadvantages outweigh them. In this type of organisation, individuals who are self-motivated and enjoy using their intellect and creativity (the majority of people, according to McGregor) would be demoralised and dissatisfied. Under these conditions, innovation and teamwork is discouraged and the organisation would be less able to adapt to changing business conditions.\nFayol, often referred to as the father of management, first introduced his 14 Principles of Management in 1916. They explain how managers should organise and relate to their employees in a productive manner. The 14 principles are listed and described in the table below (Manktelow 2014).\nEven though current managers find Fayol’s principles common sense, they paved the way for successful management in the early 1900’s. His theory serves as the foundation for many future management theorists. It is still considered the most comprehensive and is offered as a reference to assist managers today.\nBecause no two people are alike, effective managers must be able to employ a variety of management styles depending on the personality of the individual being managed. It is important for managers to understand what motivates each employee and implement the style that maximises their productivity.\nOf these three managerial theories, and my research on others, I would want to be managed using a combination of Fayol’s principles and McGregor’s “participative management style” because I’m self-motivated and enjoy applying my ingenuity and creativity. Since I desire a limited amount of direction, I find a democratic leadership style more motivating than an autocratic or laissez-faire one.\nI leave you with this video of quotes from some of history’s greatest leaders. I hope you find it as inspirational as I do!\nBirkinshaw, J. and Goddard, J. (2009) What is Your Management Model? [online] available from <http://sloanreview.mit.edu/article/what-is-your-management-model/> [25 May 2014]\nKotter, J. (2012) Change Leadership [online] available from <http://www.kotterinternational.com/our-principles/change-leadership> [26 2014]\nMaccoby, M. (2000) Understanding the Difference Between Management and Leadership [online] available from <http://www.maccoby.com/Articles/UtDBMaL.shtml> [26 May 2014]\nManktelow, J. (2014) Henri Fayol’s Principles of Management [online] available from <http://www.mindtools.com/pages/article/henri-fayol.htm> [25 May 2014]\nManktelow, J. (2014) Theory X and Theory Y: Understanding Team Member Motivation [online] available from <http://www.mindtools.com/pages/article/newLDR_74.htm> [26 May 2014]\nParfitt, K. (2009) Are Leadership and Management the Same Thing? [online] available from <http://careerrocketeer.com/2009/08/are-leadership-and-management-the-same-thing.html> [26 May 2014]\nSentenn Capital Investments (2009) Great Quotes from Great Leaders [online] available from <https://www.youtube.com/watch?v=ZCE_9hoRWlk> [25 May 2014]\nVonteese, D. (2013) Bureaucracy Max Weber’s Theory of Impersonal Management Free Principles of Management [online] available from <https://www.youtube.com/watch?v=fiKa9BHS5c0&list=PLpamLSgDwJ3egdH5VFZ6PiRc4zDpPc_Wt> [25 May 2014]","||Bureaucracy originally meant and still means â€˜rule by officialsâ€™: the implication being that it is a possible, fully fledged system of government, just like democracy or aristocracy. However, bureaucracy has also required more neutral meanings, both as a synonym for public administration, and as a form of organization characterized by a hierarchy of offices, impersonality in its recruitment of staff and its procedures, continuity in form and files, and the primacy of functional expertise.\nMuch political controversy still surrounds the first and original meaning of bureaucracy. Marxists and radical left-wing critics argue that bureaucracy subverts democracy, and prescribe participatory and egalitarian political systems to prevent this possibility. Their critics in turn maintain that such solutions are worse than the problems of bureaucracy. The German sociologist Max Weber (1864 - 1920) gloomily predicted the bureaucratization of the world, maintaining that all non-bureaucratic organizational forms would be displaced because they would prove less efficient. Weber and his precursor, the French political theorist Alexis de Tocqueville, thought that modern mass societies would be both the cause and consequence of bureaucratization. They saw strong liberal representative governmental institutions as the only effective means to prevent over-centralized states from crushing their civil societies.\nIt was Weber who produced what has become the classic model of bureaucracy. Bureaucratic organizations did exist in a limited form in traditional societies, but Weber believed the expansion of bureaucracy to be an inevitable feature of modern societies. The spread of bureaucracy in the modern world exemplified the process of rationalization. Weber suggested that a bureaucracy was the most efficient administrative form for the rational pursuit of organizational goals.\nWeber constructed an â€˜ideal typeâ€™ of bureaucracyâ€”that is an abstract description which exaggerates certain features of real cases to underline their essential characteristics. Bureaucracies, as outlined by Weber, have the following fundamental features. (1) There exists a precise hierarchy of authority which can be depicted as a pyramid, with a chain of command stretching from the top to the bottom. (2) The conduct of the office holders at all levels is governed by written rules of procedure. (3) Officials are full-time, salaried and recruited on the basis of formal qualifications. Each job within the hierarchy has a fixed salary attached to it. It is expected that individuals will make a career within the organization. (4) The tasks of the officials within the organization are separate from their life outside. (5) In bureaucracies officials do not own the material resourcesâ€”offices, desks, machineryâ€”with which they work.\nStudies have shown, however, that bureaucracies may not work in exactly the ways Weber described. The written rules can make workers inflexible and unable to respond to changing circumstances. It has also been suggested that informal practices developed by the workers themselves may be more efficient than adherence to written standards of procedure.\nEconomists, especially economic liberals in the public choice school, define bureaucratic public administration as hostile to the free market and enterprises. They claim that bureaucracies maximize their budgets or their staff, and crowd out, over-regulate and stifle the private sector: that is, they see bureaucracies as incipient forms of state socialism. They prescribe privatization, contracting-out and â€˜market-testingâ€™ as solutions to bureaucratization. Defenders of bureaucratic public administration maintain by contrast that it provides effective and accountable government and is the best known means of preventing public corruption. DA BO\\'L\nSee also accountability; authority; career; division of labour; Ã©lite theory; ideal type; occupation; organization; pluralism; profession; rationalization.Further reading David Beetham, Bureaucracy; , Patrick Dunleavy, Democracy, Bureaucracy and Public Choice; , Stephen Robbins, Organization Theory."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:0b807c69-9037-40a8-be7d-64ac7f0a2f79>","<urn:uuid:43f4e3fd-675d-49fd-ae84-07a1fb5d2e88>"],"error":null}
{"question":"How do the testing processes differ between new aircraft types and GOES-18 satellite? What kind of validation procedures do they undergo before becoming fully operational?","answer":"New aircraft types undergo extensive testing to ensure performance and safety margins, requiring certification from governing agencies like the FAA before entering operation. They typically conduct maiden flights with experienced test pilots and chase planes to verify altitude, airspeed, and airworthiness. In comparison, GOES-18 undergoes post-launch testing and calibration of its instruments and systems, including validation of its Advanced Baseline Imager (ABI). The satellite's testing includes performance verification of specific features like Band 7 imagery and the cooling system, with NOAA working alongside the satellite data user community to address any issues before it becomes the operational GOES-West satellite in early 2023.","context":["While NOAA’s GOES-18 undergoes post-launch testing and calibration of its instruments and systems, the new satellite is in position to help forecasters during the height of the 2022 Pacific hurricane season.\nGOES-18, which launched on Mar. 1, 2022, was initially delivered to 89.5 degrees west longitude over the Central U.S. and began post-launch testing and calibration of its instruments and systems in that location. Usually, GOES satellites complete post-launch testing in that location, but GOES-18’s early successes allowed NOAA to move it to 137.0 degrees west early. In this location, its Advanced Baseline Imager (ABI) data will be available to forecasters during the “warm” periods that degrade some GOES-17 (GOES-West) imagery during the height of hurricane season.\nNOAA implemented a unique solution to mitigate the loss of warm period GOES-17 imagery by delivering GOES-18 ABI data to GOES-West data users. Starting on Aug. 1, 2022, GOES-West data users will receive GOES-18 ABI data for operational use. This will be accomplished through a data “interleave” that will deliver GOES-18 ABI data with GOES-17 lightning mapper and space weather data through the GOES Rebroadcast and Product Distribution and Access data delivery systems. This period of data interleave will end on Sept. 6. A second period of data interleave will take place from Oct. 13 through Nov. 14 of this year.\nNOAA is working with the satellite data user community to prepare them for these interleave periods and make them aware of an imagery artifact discovered with one of the ABI channels. During post-launch testing of the GOES-18 ABI, scientists noticed vertical bars, similar in pattern to a barcode, in some of the imagery from one of the instrument’s channels that has not improved with on-orbit instrument calibration activities to date.\nThis artifact appears only in Band 7, one of the channels that measure infrared imagery. While visible to the eye, the barcode effect is minor. It is more apparent at night and when Band 7 is combined with data from other ABI channels in some multispectral imagery.\nNOAA expects minimal impact on operational forecasting applications. The majority of Band 7 imagery is not affected. Band 7 is used to identify fog and low clouds at night, identify fire hot spots, detect volcanic ash, estimate sea surface temperatures, and estimate low-level atmospheric winds.\nThis GOES-18 fire temperature imagery with GeoColor overlay showing wildfires in Alaska on June 30, 2022, does not have the barcode artifact. This type of imagery is used to visualize both a fire’s hotspots and smoke plumes.\nThe GOES-R Program, including the instrument development team, continues to troubleshoot and develop corrective steps for the system. NOAA and NASA are certain it is not related to the performance of the loop heat pipes or cooling system issue that affects the GOES-17 ABI. The GOES-18 ABI cooling system is performing well. The ABI was redesigned for GOES-18 to reduce the likelihood of future cooling system anomalies. The new design uses a more simple hardware configuration that eliminates the filters susceptible to debris.\nNOAA plans to make GOES-18 the operational GOES-West satellite in early 2023. GOES-17 will become an on-orbit spare.","The maiden flight, also known as first flight, of an aircraft is the first occasion on which an aircraft leaves the ground under its own power. The same term is also used for the first launch of rockets.\nThe maiden flight of a new aircraft type is always a historic occasion for the type and can be quite emotional for those involved. In the early days of aviation it could be dangerous, because the exact handling characteristics of the aircraft were generally unknown. The maiden flight of a new type is almost invariably flown by a highly experienced test pilot. Maiden flights are usually accompanied by a chase plane, to verify items like altitude, airspeed, and general airworthiness.\nA maiden flight is only one stage in the development of an aircraft type. Unless the type is a pure research aircraft (such as the X-15), the aircraft must be tested extensively to ensure that it delivers the desired performance with an acceptable margin of safety. In the case of civilian aircraft, a new type must be certified by a governing agency (such as the Federal Aviation Administration in the United States) before it can enter operation.\nNotable maiden flights (aircraft)Edit\nAn incomplete list of maiden flights of notable aircraft types, organized by date, follows.\n- June, 1875 – Thomas Moy's Aerial Steamer, London, England (pilotless, tethered)\n- October 9, 1890 – Clément Ader – took off from Gretz-Armainvilliers, Ouest of Paris, France.\n- August 14, 1901 – Gustave Whitehead From Leutershausen, Bavaria.\n- May 15, 1902 – Lyman Gilmore – took off from Grass Valley, California.\n- March 31, 1903 – Richard Pearse – took off from Waitohi Flat, Temuka, South Island, New Zealand.\n- December 17, 1903 – Wright brothers Wright Flyer – first heavier-than-air powered aircraft. Took off four miles south of Kitty Hawk, North Carolina.\n- March 18, 1906 – Traian Vuia, a Romanian inventor and engineer, who flew 11 meters in his self-named monoplane at Montesson near Paris, France.\n- October 23, 1906 – Alberto Santos-Dumont 14-bis flew the first pre-announced public flight in the World of a heavier-than-air flying machine that could take off by its own means in Bagatelle Park, Paris, France.\n- July 4, 1908 - Glenn Curtiss flew the first pre-announced public flight in the United States of America of a heavier-than-air flying machine. He flew 5,080 feet, to win the Scientific American Trophy and its $2,500 purse (equivalent to $70,000 in 2018).\n- December 22, 1916 - Sopwith Camel - this iconic biplane first took off from Brooklands, Weybridge, Surrey.\n- July 28, 1935 – Boeing B-17 Flying Fortress – World War II American heavy bomber.\n- December 17, 1935 – Douglas DC-3 – propeller-driven passenger and cargo aircraft of which more than 10,000 were produced.\n- December 29, 1939 – Consolidated B-24 – World War II American heavy bomber.\n- November 2, 1947 – Hughes H-4 Hercules – only flight of this oversized flying boat.\n- July 27, 1949 – de Havilland Comet – first jet airliner.\n- August 23, 1954 – Lockheed C-130 Hercules – military transport plane.\n- May 27, 1955 – Sud Aviation Caravelle – first jet airliner with engines mounted in the tail.\n- March 25, 1958 - Avro Canada CF-105 Arrow - Canadian supersonic fighter interceptor. First non-experimental aircraft designed and equipped with a fly-by-wire flight control system.\n- April 25, 1962 – Lockheed A-12 – supersonic reconnaissance aircraft.\n- June 29, 1962 – Vickers VC10 – first airliner with 4 engines mounted in the tail.\n- April 9, 1967 – Boeing 737 – short-to-medium-range airliner.\n- October 4, 1968 – Tupolev Tu-154 – Soviet/Russian airliner, still in operation.\n- December 31, 1968 – Tupolev Tu-144 – Soviet supersonic airliner.\n- February 9, 1969 – Boeing 747 – first widebody airliner.\n- March 2, 1969 – Anglo-French Concorde – supersonic airliner.\n- September 19, 1969 – Mil Mi-24 – Russian/Soviet-made helicopter used by many countries to this day.\n- October 28, 1972 – Airbus A300 – first Airbus aircraft, short- to medium-range wide-body jet airliner.\n- February 22, 1987 – Airbus A320 airliner – first civilian aircraft to have an all-digital fly-by-wire system.\n- December 21, 1988 – Antonov An-225 Mriya – jet with the longest fuselage and wingspan and overall heaviest aircraft.\n- June 12, 1994 – Boeing 777 – long-range airliner with the most powerful jet engines ever made.\n- April 27, 2005 – Airbus A380 – double-decker jet airliner, currently largest capacity in the world, took off from Toulouse–Blagnac Airport.\n- December 11, 2009 – Airbus A400M – military cargo plane, Airbus' first propeller plane.\n- December 15, 2009 – Boeing 787 Dreamliner – first major widebody airliner to use non-metal composite materials for most of its construction.\n- November 11, 2015 - Mitsubishi Regional Jet - Japanese twin-engine regional jet, the first designed and built in Japan, took off from Mitsubishi Heavy Industries, Tokyo.\n- May 5, 2017 - Comac C919 - Chinese commercial aircraft.\nNotable maiden flights (rockets)Edit\n- October 3, 1942 - V-2 Rocket made its first successful test flight. The nose cone crossed the Karman line, widely considered the end of Earth's atmosphere, making it the first human-made object to reach space.\n- August 3, 1953 - PGM-11 Redstone, designed by Wernher von Braun, was the US's first large ballistic missile. Launched from Cape Canaveral Air Force Station Launch Complex 4, it flew for 80 seconds until an engine failure caused it to crash into the sea.\n- October 4, 1957 - Sputnik, first orbital rocket.\n- December 22, 1960 - Vostok-K, first human-rated rocket (first manned flight April 12, 1961).\n- November 9, 1967 - Saturn V, most powerful rocket launched so far, was used to launch humans to the Moon.\n- April 12, 1981 - Space Shuttle, first partially reusable launch system, largest payload at the time of its maiden flight.\n- December 21, 2004 - Delta IV Heavy, largest payload at the time of its maiden flight.\n- February 6, 2018 - Falcon Heavy, largest payload at the time of its maiden flight, partially reusable. \n|Wikimedia Commons has media related to Maiden flights.|\n- Gary Bradshaw. \"Thomas Moy's Aerial Steamer, 1874. lifted six inches (15 centimeters) off the ground\". U.S. Centennial of Flight. Retrieved February 18, 2016.\n- Harwood, William (February 6, 2018). \"SpaceX Falcon Heavy launch puts on spectacular show in maiden flight\". CBS News. Retrieved February 6, 2018."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:af60a3da-ee33-472d-8449-80444f492d71>","<urn:uuid:a327cbfc-a6cc-4904-9016-2d011928b4f7>"],"error":null}
{"question":"How did the process of bronze artifact preservation and digitization differ between ancient and modern times?","answer":"In ancient times, bronze artifacts were preserved mainly through burial in graves, as seen in both Greek and Etruscan contexts where many bronze objects were found in grave sites. In modern times, preservation efforts have evolved to include digital preservation, as exemplified by the Nantucket Historical Association's collaboration with the Boston Public Library to digitize their collection of 620 historical artifacts. This modern preservation process involves evaluation of collections, review of metadata practices, and transportation of materials to digital labs for scanning. Through programs like Digital Commonwealth, these digitization services are provided free to member organizations, making previously inaccessible historical materials available online.","context":["Last month I had the pleasure of traveling to the Nantucket Historical Association (NHA) in Nantucket, Massachusetts for an archival site visit with the good folks from the Boston Public Library’s (BPL) digital services department and the Norman B. Leventhal Map Center. We were there to meet with NHA staff and survey the 120 year-old island historical association’s collection of historical maps, which number in the order of 620 and have yet to be digitized for online access, largely because NHA doesn’t have the scanning equipment needed to capture large-format digital images. BPL’s digital services department works in partnership with Digital Commonwealth, a non-profit collaborative organization and DPLA Service Hub, to evaluate, digitize, describe, and host unique artifacts located in Massachusetts’ cultural organizations. Absent Digital Commonwealth’s assistance, many of these vital organizations would be unable to make their materials available online, due in part to local funding and staffing constraints.\nHow does this work exactly? Digital Commonwealth members can apply for free digitization services from BPL. While these services were originally supported by a grant from the Massachusetts Board of Library Commissioners, itself funded by the Library Services and Technology Act, they are now supported by the state of Massachusetts as part of its Library for the Commonwealth program. These digitization services generally include an evaluation of the collections to be digitized, a review of the organization’s metadata practices and archival management, and the establishment of a gameplan for transporting the materials to and from the BPL’s digital lab, where the actual digitization takes place. Throughout the process BPL staff stays in close contact with the organization, providing advice on how to best prepare their materials for upload into the Digital Commonwealth system, which in turn feeds into DPLA.\nBy virtue of this statewide work, Digital Commonwealth maintains scores of relationships with libraries, archives, historical societies, museums, and other cultural organizations across the state, including the Nantucket Historical Association, making the often-daunting and sometimes unfeasible task of digitizing and describing archival items a realizable goal through one-on-one meetings, site visits, and other consultations. Moreover, what makes this even more powerful is that it’s happening not just in Massachusetts but in myriad states across the country, many of whom we’re proud to say serve as DPLA Service Hubs.\nThe gallery below, detailing my day as a happy interloper, hopefully conveys the general mechanics of a Digital Commonwealth site visit and the essential services they provide. If you happen to have any questions about Digital Commonwealth and their statewide digitization program, the Norman B. Leventhal Map Center, or the Nantucket Historical Society, please don’t hesitate to be in touch.\n The Norman B. Leventhal Map Center is physically located in the BPL’s Central Library in downtown Boston but it’s technically a separate entity with its own Board of Directors and staff.\n For a comprehensive summary of NHA’s history, visit http://www.nha.org/history/hn/HNpride.html.\n The Library Services and Technology Act, or LSTA for short, is a federal program administered by the Institute of Museum and Library Services (IMLS) that provides essential funding to libraries across the US.\n Digital Commonwealth recently launched a brand-new beta interface for these collections, available here.\n Hence the service in service hub!\nAll written content on this blog is made available under a Creative Commons Attribution 4.0 International License. All images found on this blog are available under the specific license(s) attributed to them, unless otherwise noted.","\"Lost\" and Found: Greek, Etruscan, and Roman Bronzes in the Bryn Mawr College Collection\nCurated by Jessica Miller '04\nSpring 2004 through Fall 2004\nRhys Carpenter Library for Art, Archaeology, and Cities\nKaiser Reading Room and Fong Reading Room\nMycenaean Age 1400-1150 BC\nSub-Mycenaean Period 1150-900 BC\nGeometric Period 900-700 BC\nArchaic Period 700-479 BC\nClassical Period 479-323 BC\nHellenistic Period 323- 31 BC\nEtruscan Culture 1000-200 BC\nVillanovan Culture 900-700 BC\nRoman Republic 200-27 BC\nAugustan Age 27 BC- AD 14\nRoman Empire 27 BC- AD 337\nLost Wax Casting\nMost solid bronze objects in this collection have been created using the lost wax, or cire perdue method of casting. Through this process artists could create very complex shapes that would be impossible to form from bending or hammering alone. The process begins with a wax model of the object to be made. Rods of wax, called sprues or gates, are then attached to the model. These channels allow the molten bronze to be poured into the totality of the object. The next step, called investment, involves immersing the wax model and coating it several times in a liquid ceramic material. This material becomes a rock hard shell that is then fired in a kiln to melt the wax model inside, creating a hollow space for the bronze to fill.\nBronze in the Greek World\nThen the bronze is then heated to around 1700 degrees F and poured into the mold. After cooling, the shell is broken away and the bronze is chased (cleaned up).\nBronze statuary in the Greek world began with a technique called sphyrelaton, in which a wooden body is covered with hammered bronze, exemplified in such figures as those found at the sanctuary of Apollo at Dreros, Crete. The first evidence of hollow casting seems to come from the island of Samos, starting in the 7th century BC. Much of the archaeological evidence for the working of bronze in Greece comes from workshop sites such as that of Pheidias at Olympia. Further information can be gained from the scenes on vase paintings, such as those found on the Foundry painter's cup and the Berlin oinochoe.\nIn this collection, examples of both utilitarian and votive objects are found. Objects for everyday use include fibulae, a strigil, and toilette items, such as the bronze mirror (M-49). The fibulae, which constitute the most numerous object group on display, can be related to the modern safety pin, used to gather and secure the fabric of homespun garments. The strigil, pictured left, was an implement commonly used by athletes to cleanse the body of dirt and oil.\nPerhaps the most striking object, however, is the bronze mirror with the face of a female in profile on its cover. This mirror type is rare in Greece in that it functioned like a modern compact with a hinged casing that could be opened to expose the mirror surface of polished bronze. Votive objects, such as the Geometric period figurines, were left as dedications at temples and were often mass-produced at the temple site for purchase. It must be kept in mind that most Greek bronzes come from grave contexts.\nBronze in the Etruscan World\nLike the majority of Etruscan art and handicraft, the bronzes dating from the Etruscan time period and region were initially greatly influenced by the art of the Near East. An even greater influence was the art of the Greek world due to the proliferation of Greek colonies in the Italian peninsula. Regardless, the Italic people were able to retain much of their native style. In fact, the Greeks often praised the Etruscan craftsmen for their ability to create invigorated domestic bronze objects. These pieces, such as open style horse bits, elaborate fibulae, and beautiful mirrors, are notable for their exquisite use of incised decoration. In this exhibit, we find examples of such utilitarian items from both the Etruscan and related Villanovan periods. One very interesting piece from the Villanovan period on display is the bronze razor, which like many similar examples shows signs of wear from use.\n| Votive objects are also well known from the Etruscan world, such as the bronze tripod incense stand in this exhibit. Bronze votive statuettes were less common in the Etruscan world in comparison to that of the Greeks. This is possibly due to the Etruscan preference for terracotta when creating the human form, though examples of Etruscan bronze figures do exist. The Etruscans were better known, however, for their incredible work with armature and horse trappings, which may themselves have been purely ceremonial due to the fact that relatively few show signs of damage or wear and most come from grave contexts.\n| Roman Bronzes\nMost scholars agree that the bronzes of the Roman world have direct relationships to the Etruscan bronze tradition. Some have even gone so far to suggest that the Roman bronzesmiths may have been descendants of the Etruscan artists of the late 5th and 4th centuries. This idea exemplifies the greatest difficulty in discussing Roman bronze work; identifying what is Roman. This is a two fold problem, which first involves distinguishing when the Roman-style first emerges and how it is differentiated from earlier Italic bronze work. Secondly, there are issues involving objects dated to the height of the Roman Empire that were created in far reaching lands and by people of differing ethnicities, but were still in the style and subject area of the Roman tradition. Often, archaeologists and art historians attempt to resolve this problem by identifying distinctively Roman subjects, such as the Lares, men sacrificing dressed in togas, priestesses, gladiators, and portraits of the Roman Emperors.\nFor Further Information on Bronze Work in the\nAncient Mediterranean World:\n* Winifred Lamb, Greek and Roman Bronzes (New York, 1929).\n* Aspects of Ancient Greek Art. An Exhibition Organized by the Allentown Art Museum with the Cooperation of Gloria Ferrari Pinney and Brunilde Sismondo Ridgway. Allentown Art Museum, Sept 16-Dec 30, 1979.\n* Gisela Richter, Greek, Etruscan, and Roman Bronzes (New York, 1915).\nMiss Rebecca Lessem '03 for all her help in getting this project started.\nCarol Campbell and Tamara Johnston for their years of advice, guidance, and patience.\nMaps Created by:\nChris Neely, Stu Shell and Stefan Iacob\nLost Wax illustrations courtesy of\nMary Ann Dabritz\nDonors to the Ella Riegel Memorial Study Collection,\nwho made this exhibition possible:\nMiss Frances Browne, AB 1909 and\nMiss Norvelle Browne, Class of 1911.\nAnne Stainton Dane, AB 1961,\nIn honor of Machteld J. Mellink.\nMrs. Lincoln Dryden\n(Clarissa Compton Dryden, AB 1932, MA 1935).\nAgnes Kirsopp Lake Michels, AB 1930, MA 1931, Ph.D. 1934\nFrom the Lily Ross Taylor (Ph.D. 1912) Collection.\nHobson Pittman Collection, by Bequest 1972.\nDr. and Mrs. Henry S. Robinson\n(Henry S. and Rebecca Wood Robinson, AB 1945, MA 1950).\nMrs. John Jay Whitehead.\nAnonymous donors, various.\nThe Ella Riegel Memorial Study Collection\nBryn Mawr College 2004"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:5590891c-82b6-49b1-8ac0-392aa32f6880>","<urn:uuid:8cd8c8f2-e827-40ed-801b-a9f1589c669e>"],"error":null}
{"question":"¿Cómo se comparan las visiones de Singer y Bookchin sobre el valor de la vida? How do their perspectives on human vs non-human life differ? 🤔","answer":"Singer and Bookchin present contrasting views on the value of human and non-human life. Bookchin criticizes deep ecology's anti-human tendencies and argues that ecological problems stem from social relations of domination, not from humanity itself. He emphasizes that it's capitalism and hierarchical systems, not humans per se, that cause environmental destruction. In contrast, Singer argues that humans have no inherent right to better treatment than animals, and that rationality and ability to suffer should be the key criteria for moral consideration. Singer even suggests that some animals (like adult chimpanzees) may have more personhood than human infants, though this view doesn't account for human potential. The key difference is that Bookchin focuses on social and systemic causes of ecological problems, while Singer focuses on individual characteristics when determining moral value.","context":["Anarchist FAQ/What is Anarchism?/3.3\nA.3.3 What kinds of green anarchism are there?\nAn emphasis on anarchist ideas as a solution to the ecological crisis is a common thread in most forms of anarchism today. The trend goes back to the late nineteenth century and the works of Peter Kropotkin and Elisee Reclus. The latter, for example, argued that a \"secret harmony exists between the earth and the people whom it nourishes, and when imprudent societies let themselves violate this harmony, they always end up regretting it.\" Similarly, no contemporary ecologist would disagree with his comments that the \"truly civilised man [and women] understands that his [or her] nature is bound up with the interest of all and with that of nature. He [or she] repairs the damage caused by his predecessors and works to improve his domain.\" [quoted by George Woodcock, \"Introduction\", Marie Fleming, The Geography of Freedom, p. 15]\nWith regards Kropotkin, he argued that an anarchist society would be based on a confederation of communities that would integrate manual and brain work as well as decentralising and integrating industry and agriculture (see his classic work Fields, Factories, and Workshops). This idea of an economy in which \"small is beautiful\" (to use the title of E.F. Schumacher's Green classic) was proposed nearly 70 years before it was taken up by what was to become the green movement. In addition, in Mutual Aid Kropotkin documented how co-operation within species and between them and their environment is usually of more benefit to them than competition. Kropotkin's work, combined with that of William Morris, the Reclus brothers (both of whom, like Kropotkin, were world-renowned geographers), and many others laid the foundations for the current anarchist interest in ecological issues.\nHowever, while there are many themes of an ecological nature within classical anarchism, it is only relatively recently that the similarities between ecological thought and anarchism has come to the fore (essentially from the publication of Murray Bookchin's classic essay \"Ecology and Revolutionary Thought\" in 1965). Indeed, it would be no exaggeration to state that it is the ideas and work of Murray Bookchin that has placed ecology and ecological issues at the heart of anarchism and anarchist ideals and analysis into many aspects of the green movement.\nBefore discussing the types of green anarchism (also called eco-anarchism) it would be worthwhile to explain exactly what anarchism and ecology have in common. To quote Murray Bookchin, \"both the ecologist and the anarchist place a strong emphasis on spontaneity\" and \"to both the ecologist and the anarchist, an ever-increasing unity is achieved by growing differentiation. An expanding whole is created by the diversification and enrichment of its parts.\" Moreover, \"[j]ust as the ecologist seeks to expand the range of an eco-system and promote free interplay between species, so the anarchist seeks to expand the range of social experiments and remove all fetters to its development.\" [Post-Scarcity Anarchism, p. 36]\nThus the anarchist concern with free development, decentralisation, diversity and spontaneity is reflected in ecological ideas and concerns. Hierarchy, centralisation, the state and concentrations of wealth reduce diversity and the free development of individuals and their communities by their very nature, and so weakens the social eco-system as well as the actual eco-systems human societies are part of. As Bookchin argues, \"the reconstructive message of ecology. . . [is that] we must conserve and promote variety\" but within modern capitalist society \"[a]ll that is spontaneous, creative and individuated is circumscribed by the standardised, the regulated and the massified.\" [Op. Cit., p. 35 and p. 26] So, in many ways, anarchism can be considered the application of ecological ideas to society, as anarchism aims to empower individuals and communities, decentralise political, social and economic power so ensuring that individuals and social life develops freely and so becomes increasingly diverse in nature. It is for this reason Brian Morris argues that \"the only political tradition that complements and, as it were, integrally connects with ecology -- in a genuine and authentic way -- is that of anarchism.\" [Ecology and Anarchism, p. 132]\nSo what kinds of green anarchism are there? While almost all forms of modern anarchism consider themselves to have an ecological dimension, the specifically eco-anarchist thread within anarchism has two main focal points, Social Ecology and \"primitivist\". In addition, some anarchists are influenced by Deep Ecology, although not many. Undoubtedly Social Ecology is the most influential and numerous current. Social Ecology is associated with the ideas and works of Murray Bookchin, who has been writing on ecological matters since the 1950's and, from the 1960s, has combined these issues with revolutionary social anarchism. His works include Post-Scarcity Anarchism, Toward an Ecological Society, The Ecology of Freedom and a host of others.\nSocial Ecology locates the roots of the ecological crisis firmly in relations of domination between people. The domination of nature is seen as a product of domination within society, but this domination only reaches crisis proportions under capitalism. In the words of Murray Bookchin:\n\"The notion that man must dominate nature emerges directly from the domination of man by man. . . But it was not until organic community relations. . . dissolved into market relationships that the planet itself was reduced to a resource for exploitation. This centuries-long tendency finds its most exacerbating development in modern capitalism. Owing to its inherently competitive nature, bourgeois society not only pits humans against each other, it also pits the mass of humanity against the natural world. Just as men are converted into commodities, so every aspect of nature is converted into a commodity, a resource to be manufactured and merchandised wantonly . . . The plundering of the human spirit by the market place is paralleled by the plundering of the earth by capital.\" [Op. Cit., pp. 24-5]\n\"Only insofar,\" Bookchin stresses, \"as the ecology consciously cultivates an anti-hierarchical and a non-domineering sensibility, structure, and strategy for social change can it retain its very identity as the voice for a new balance between humanity and nature and its goal for a truly ecological society.\" Social ecologists contrast this to what Bookchin labels \"environmentalism\" for while social ecology \"seeks to eliminate the concept of the domination of nature by humanity by eliminating domination of human by human, environmentalism reflects an 'instrumentalist' or technical sensibility in which nature is viewed merely as a passive habit, an agglomeration of external objects and forces, that must be made more 'serviceable' for human use, irrespective of what these uses may be. Environmentalism . . . does not bring into question the underlying notions of the present society, notably that man must dominate nature. On the contrary, it seeks to facilitate that domination by developing techniques for diminishing the hazards caused by domination.\" [Murray Bookchin, Towards an Ecological Society, p. 77]\nSocial ecology offers the vision of a society in harmony with nature, one which \"involves a fundamental reversal of all the trends that mark the historic development of capitalist technology and bourgeois society -- the minute specialisation of machines and labour, the concentration of resources and people in gigantic industrial enterprises and urban entities, the stratification and bureaucratisation of nature and human beings.\" Such an ecotopia \"establish entirely new eco-communities that are artistically moulded to the eco-systems in which they are located.\" Echoing Kropotkin, Bookchin argues that \"[s]uch an eco-community . . . would heal the split between town and country, between mind and body by fusing intellectual with physical work, industry with agricultural in a rotation or diversification of vocational tasks.\" This society would be based on the use of appropriate and green technology, a \"new kind of technology -- or eco-technology -- one composed of flexible, versatile machinery whose productive applications would emphasise durability and quality, not built in obsolescence, and insensate quantitative output of shoddy goods, and a rapid circulation of expendable commodities . . . Such an eco-technology would use the inexhaustible energy capacities of nature -- the sun and wind, the tides and waterways, the temperature differentials of the earth and the abundance of hydrogen around us as fuels -- to provide the eco-community with non-polluting materials or wastes that could be recycled.\" [Bookchin, Op. Cit., pp. 68-9]\nHowever, this is not all. As Bookchin stresses an ecological society \"is more than a society that tries to check the mounting disequilibrium that exists between humanity and the natural world. Reduced to simple technical or political issues, this anaemic view of such a society's function degrades the issues raised by an ecological critique and leads them to purely technical and instrumental approaches to ecological problems. Social ecology is, first of all, a sensibility that includes not only a critique of hierarchy and domination but a reconstructive outlook . . . guided by an ethics that emphasises variety without structuring differences into a hierarchical order . . . the precepts for such an ethics . . . [are] participation and differentiation.\" [The Modern Crisis, pp. 24-5]\nTherefore social ecologists consider it essential to attack hierarchy and capitalism, not civilisation as such as the root cause of ecological problems. This is one of the key areas in which they disagree with \"Primitivist\" Anarchist ideas, who tend to be far more critical of all aspects of modern life, with some going so far as calling for \"the end of civilisation\" including, apparently, all forms of technology and large scale organisation. We discuss these ideas in section A.3.9.\nWe must note here that other anarchists, while generally agreeing with its analysis and suggestions, are deeply critical of Social Ecology's support for running candidates in municipal elections. While Social Ecologists see this as a means of creating popular self-managing assemblies and creating a counter power to the state, few anarchists agree. Rather they see it as inherently reformist as well as being hopelessly naive about the possibilities of using elections to bring about social change (see section J.5.14 for a fuller discussion of this). Instead they propose direct action as the means to forward anarchist and ecological ideas, rejecting electioneering as a dead-end which ends up watering down radical ideas and corrupting the people involved (see section J.2 -- What is Direct Action?).\nLastly, there is \"deep ecology,\" which, because of its bio-centric nature, many anarchists reject as anti-human. There are few anarchists who think that people, as people, are the cause of the ecological crisis, which many deep ecologists seem to suggest. Murray Bookchin, for example, has been particularly outspoken in his criticism of deep ecology and the anti-human ideas that are often associated with it (see Which Way for the Ecology Movement?, for example). David Watson has also argued against Deep Ecology (see his How Deep is Deep Ecology? written under the name George Bradford). Most anarchists would argue that it is not people but the current system which is the problem, and that only people can change it. In the words of Murray Bookchin:\n\"[Deep Ecology's problems] stem from an authoritarian streak in a crude biologism that uses 'natural law' to conceal an ever-diminishing sense of humanity and papers over a profound ignorance of social reality by ignoring the fact it is capitalism we are talking about, not an abstraction called 'Humanity' and 'Society.'\" [The Philosophy of Social Ecology, p. 160]\nThus, as Morris stresses, \"by focusing entirely on the category of 'humanity' the Deep Ecologists ignore or completely obscure the social origins of ecological problems, or alternatively, biologise what are essentially social problems.\" To submerge ecological critique and analysis into a simplistic protest against the human race ignores the real causes and dynamics of ecological destruction and, therefore, ensures an end to this destruction cannot be found. Simply put, it is hardly \"people\" who are to blame when the vast majority have no real say in the decisions that affect their lives, communities, industries and eco-systems. Rather, it is an economic and social system that places profits and power above people and planet. By focusing on \"Humanity\" (and so failing to distinguish between rich and poor, men and women, whites and people of colour, exploiters and exploited, oppressors and oppressed) the system we live under is effectively ignored, and so are the institutional causes of ecological problems. This can be \"both reactionary and authoritarian in its implications, and substitutes a naive understanding of 'nature' for a critical study of real social issues and concerns.\" [Morris, Op. Cit., p. 135]\nFaced with a constant anarchist critique of certain of their spokes-persons ideas, many Deep Ecologists have turned away from the anti-human ideas associated with their movement. Deep ecology, particularly the organisation Earth First! (EF!), has changed considerably over time, and EF! now has a close working relationship with the Industrial Workers of the World (IWW), a syndicalist union. While deep ecology is not a thread of eco-anarchism, it shares many ideas and is becoming more accepted by anarchists as EF! rejects its few misanthropic ideas and starts to see that hierarchy, not the human race, is the problem (for a discussion between Murray Bookchin and leading Earth Firster! Dave Foreman see the book Defending the Earth).","Peter Singer is an Australian who is now professor of bio-ethics at Princeton in the US. His parents were Jewish and three of his relatives died in the Holocaust. He repudiates all religion and refused to have a bar mitzvah. Singer is passionately committed to the view that ethics must be about how life is lived: \"There would be something incoherent about living a life where the conclusions you came to in ethics did not make any difference to your life. It would make it an academic exercise. The whole point about doing ethics is to think about the way to live. My life has a kind of harmony between my ideas and the way I live. It would be highly discordant if that was not the case.\"\nSinger: some animals have equal value to humans\nSinger is a preference utilitarian. They argue that the consequences to be promoted are those which satisfy the wishes or preferences of the maximum numbers of beings who have preferences. In other words, the more people get what they want, the better, from a moral point of view, the world is. The more people's desires are frustrated, the worse the world is. It is only morally right to frustrate the preferences of others if by so doing we enable more beings to satisfy their preferences. Actions should not be judged on their simple pain-and-pleasure outcomes, but on how they affect the interests, the preferences, of all beings involved. Singer asks an important additional question - \"What sort of beings should we include in the sum of interests?\" Singer argues that this question is not addressed by Christians - they assume that humans are more valuable than animals. Singer rejects this assumption.\nWhy should humans be valued more than animals? What is the intellectual basis for experimenting on animals rather than a person in a persistent vegetative state? Singer argues that humans have no inherent right to better treatment than animals - instead their ability to suffer and their rationality need to be evaluated. A dolphin or a chimpanzee may be more rational and be able to suffer more than a newborn baby. Beings that have rationality or self-consciousness are more important than mere sentient beings. If you had to choose to save a child or a dog, you should save the higher \"person\" - the child. For Singer, not all persons are humans, and some humans are definitely not persons. An adult chimpanzee can exhibit more self-consciousness, more personhood, than a newborn human infant. If the choice was between saving a newborn baby who had no family and a mature chimpanzee and could only save one of them, the chimp should be saved. \"Killing them [babies], therefore, cannot be equated with killing normal human beings, or any other self-conscious beings. No infant - disabled or not - has as strong a claim to life as beings capable of seeing themselves as distinct entities existing over time\" (Practical Ethics). Singer has proposed a post-natal 28-day qualification period during which infants - non-persons at that stage – could be killed.\nSinger puts forward arguments that, while rational, go against fundamental human intuitions. Perhaps most significantly, however, he does not take potential into account. A baby has the potential to become an adult human being and destroying this potential may be an evil act. On this basis, the value of beings should be measured by their potential - and a disabled baby may still have more potential than a dolphin.\nThe great attraction of Benthamite utilitarianism is that it seems to appeal to common sense in that most people think that happiness is the main aim in life and, in addition, it is often held to be measurable in financial terms.\nHappiness and fulfilment\nFor the great Greek philosophers, what it means to live a fulfilled human life cannot be measured, and what matters does not depend on funding or \"measurable outcomes\". Also, what many people want may not be what is in their best interests in terms of human fulfilment. GE Moore said \"you cannot get an ought from an is\" – the fact that many men enjoy watching football, having too much to drink on a Saturday night and looking at pornography does not mean that this is what they should desire – philosophy holds there is more to life than this.\nThe search for personal happiness may take people away from the importance of a search for meaning and understanding what a fulfilled human life should be life. The growing tide of utilitarianism, however, threatens to sweep aside ideals like justice, goodness, truth or, indeed, the distinctiveness and importance of each individual, so its influence and importance is likely to increase. Mill's approach is far more sophisticated than Bentham's because it recognises that fulfilling human potential is essential for ethics and that happiness is not something personally chosen but is directly linked to the common human nature that we all share.\nRecently there has been a coming together of utilitarianism with virtue ethics through the work of Robert Merrihew Adams. Since 1976 Adams advocated \"motive utilitarianism\", i.e a utilitarianism which starts with the individual, demanding that they cultivate the character traits and skills which are likely to yield the greatest happiness for the greatest number. This utilitiarianism does not just focus on acts and consequences, but also considers the character and motivations, in this it shares a great deal with virtue ethics, a subject which Adams has also written on in the last couple of years.\nClick here to read about Catholic social teaching."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9a1fc3e7-ff5d-4706-bb91-0d8fb4d897c8>","<urn:uuid:1c9a909c-819f-4d02-820e-d3010871d7c0>"],"error":null}
{"question":"¿How do Fiji and Mauritius compare in terms of their historical connection to sugar cane industry and economic development? / ¿Cómo se comparan Fiji y Mauritius en términos de su conexión histórica con la industria de la caña de azúcar y el desarrollo económico?","answer":"Both Fiji and Mauritius have strong historical ties to the sugar cane industry, but with different trajectories. In Fiji, indentured laborers from India transformed the economy from traditional agrarian to a blend of agrarian and milling, with sugar becoming the key sector - representing 65% of total merchandise exports until 1970. In Mauritius, sugar cane was introduced by the Dutch East India Company in 1598, with subsequent French and British colonial powers using East African and Indian slaves to work the cane fields. However, while Fiji remained heavily dependent on sugar, Mauritius has diversified significantly, particularly since 2000, becoming a financial services hub specializing in private equity and hedge funds, with one of Africa's highest GDP per capita at $15,600 in 2012.","context":["By Dr Neelesh Gounder in Suva\nWhen a ship docked in Mauritius on November 2, 1834, carrying 36 Indian indentured labourers, the British called it the “great experiment”.\nAlmost 45 years later, when the Leonidas docked in Fiji, 479 girmitiya were to become the first group of this experiment in Fiji.\nMost of the girmitiya could not read or understand the agreement they signed. Surely then they would not have known that this agreement would be their new lifestyle. More importantly, they were oblivious that the decision to sign on the agreement would leave a permanent legacy for their descendants.\nFor the girmitiya the longest journey of their lives was symbolic of their unforgiving struggle that lay ahead; at least in their lifetime.\nIn 87 voyages that were made to Fiji between 1879 and 1916, some 60,600 indentured labourers arrived.\nLast week – May 14, 2016 – spanned 137 years since the first arrival of indentured labourers.\nThis year is worth remembering the girmitiya for another reason. The indenture system was discontinued in November, 1916, 100 years ago. For Fiji, this happened four years before the system was abolished worldwide.\nHow do we remember and pay tribute to the girmitiya — our forefathers? We can begin by celebrating the achievements and legacy of the girmitiya.\nBeyond doubt, the indentured labourers during indenture but more importantly after indenture as ‘freed’ men and women transformed Fiji’s social and economic landscape.\nThe indentured labourers became a source of cheap labour for the European planters who were struggling after the failure of the cotton industry. Capital investment from Colonial Sugar Refining Company of Australia in milling was probable on the back of the availability of this human capital.\nThis combination transformed the Fijian economy from a traditional agrarian to a blend of agrarian and milling. Soon sugar industry established itself as the key sector of the economy, with the indentured labourers essential to its success.\nSugar production in 1916 was 122,000 tonnes with 99 per cent of that volume exported. This initial involvement in foreign trade shaped Fiji’s first step towards the process of integration into the world economy.\nOn the whole, the sugar industry was largely not only responsible for the success of the individual planters but in changing the landscape of the Fijian macro-economy.\nSugar held its stronghold among the export commodities by comprising 65 per cent of total merchandise exports until 1970. The industry was basically responsible for the rise and development of Western division.\nApart from the economy, the lives of the grimitiya continue to influence generations after them. The girmitiya are well recognised for their work ethics and determination to break away from the norm to improve their lot. The descendants seem to have followed on the heels of girmitiyas.\nMajority of indentured labourers chose to stay in Fiji after the end of their indenture and after indenture itself was abolished. Despite fresh from the bonds of girmit and the benighted work on the plantation, the girmitiya were swift to move on to entrepreneurship such as shopkeepers, hawkers and transport providers.\nA new existence and determination to be oneself unleashed the power of personal initiative and entrepreneurship that soon challenged the existing commercial enterprises at that time.\nThere were also free migrants from India during the indenture period. By 1925, the free migrants mostly owned businesses such as tailoring, jewelry businesses, laundry and bootmakers.\nThe girmitiya and their descendants, on the other hand, owned majority of construction companies, transport/taxi businesses and services such as auto servicing and photography.\nThis structure and arrangement turned the first stage of the economy into a more merchant oriented and expanding the scope of labour for wages. This entrepreneurial energy helped prepare the ground for economic success of Fiji. It also set the foundation for industrial development (for example, domestic manufacturing sector) which has a robust association to trade and investment.\nDespite the common purpose towards improving their lives, descendants of the girmitiya were obliquely divided, mostly along religious lines but also along language and cultural grounds. Out of these fractures emerged socio-cultural and religious organisations promoting their distinct agendas.\nThese organisations were, however, necessary to build social cohesion, identity, ethnic binding towards achieving common problems. Organisations such Then India Sanmarga Sangam (TISI), Sanatan Dharm Pratinidhi Sabha and Arya Samaj are important examples.\nOne of the outcomes of these organisations is the provision of education by building schools. This is also true of other organisations that do not have any particular links to the girmitiya. The spirit of community partnership has had a huge impact on the development of education policy and contribution to national development.\nWhile it may sound regular to start a school these days, the girmitiya had to carve out schools amid tough policies of the colonial government. Resources were meagre but the community spirit overruled this and other challenges.\nThe economic returns to education would have been the key motivating factor as well. Educated and literate people earned more than farmers. These organisations ensured that spread of education did not remain limited to the elite as in other countries during British colonial rule.\nThe legacy does not stop with the economy and education; there are other far reaching spillovers beyond these two important dimensions. Our present is inextricably intertwined with the past. Others are free to argue about the bittersweet legacy that still influences many facets of our life today.\nDr Neelesh Gounder is an economics lecturer at the University of the South Pacific. This article was first published by The Fiji Times and is republished here with the author’s permission.","Once dependent almost entirely on sugar cane and tourism, this tiny island nation is fast becoming the entrepôt of choice for private equity and hedge funds pursuing investment opportunities in the emerging markets of East Africa and India.\nIdeally situated off the coast of Madagascar in the southwestern Indian Ocean, Mauritius is a politically stable, economically vibrant and racially harmonious potpourri of 1.3 million Hindu, Muslim, Christian and Chinese citizens. It was discovered by Arab mariners in the 9th century but not settled until 1598 by the Dutch East India Company which introduced sugar cane to the island. The Dutch colony lasted only 20 years and was succeeded first by the French (in 1715) and then by the British (in 1810) who brought East African and Indian slaves to work the cane fields and run the sugar mills. Slavery was abolished in 1835 and Mauritius achieved political independence in 1968. To this day, however, the French and British remain the island’s industrial elite and Mauritius’ legal system is based on both English common law (for property and contract rights) and the Code Napoléon (for the penal system). Mauritians also commonly speak both English and French (along with Hindi, Creole and other Indian and African mother tongues).\nSince 2000, Mauritius has dedicated itself to becoming the Singapore of the Southern hemisphere, with a specialization in financial services. It has passed legislation prohibiting discrimination between local and foreign investment (Investment Promotion Act) and has entered into Investment Promotion and Protection Agreements with India and 19 (soon-to-be 25) sub-Saharan African nations guaranteeing against nationalization and permitting the free repatriation of capital and profits.\nBy far the most magnetic element in the island’s attraction of over 800 private investment funds to date, however, are the Double Taxation (Avoidance) Agreements it has negotiated with India and now more than 36 other countries, including 19 in sub-Saharan Africa. These treaties enable foreign fund operators to avoid income taxation altogether in the investee economies and, in Mauritius, to limit the taxation of their interest and dividend income to a maximum of 3% and to completely eliminate taxation of their capital gains.\nThanks to its favorable regulatory climate, Mauritius currently accounts for over 40% of foreign direct investment in India and an increasing proportion of the FDI flowing into Africa (which totalled $50 billion last year). Descendants of the Indian and African slaves who initially populated the island now constitute a financially sophisticated talent pool of lawyers, accountants and fund administrators determined to mold Mauritius into the financial hub of the Southern hemisphere. Its GDP per capita (almost $15,600 in 2012) is one of the highest in Africa and its literacy rate is almost 90%.\nSurrounded by pristine waters and blessed with gorgeous beaches and the second cleanest air in the world (behind Estonia), Mauritius has for years been renowned as a top tourist destination, boasting a long list of ultra-luxurious resort hotels and celebrity vacationers. Any private fund manager with an investment interest in India or sub-Saharan Africa would thus be well-served by combining a little Mauritian R&R with a little financial R&D."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:4d521a8b-36d9-46c8-a270-5274f3563759>","<urn:uuid:10baabe3-72e6-42e4-af85-142060aa58bd>"],"error":null}
{"question":"How do VMware's virtualization solutions and NetApp's storage technologies differ in their approach to performance management for critical applications?","answer":"VMware's virtualization solutions like vSphere focus primarily on resource allocation and utilization optimization, but can make it harder to deliver consistent performance to critical applications due to resource contention and dynamic sharing. In contrast, NetApp's approach includes performance-focused solutions like NetApp Balance that provide cross-domain visibility and mapping from guest VMs across hosts and SANs into array architectures, helping to identify performance impacts. The two companies are now working together to enhance performance through new certifications and support for accessing vVols via NVMe-oF, offering improved block storage flash performance and more granular VM storage management.","context":["Is Virtualization Stalled On Performance?\nVirtualization and cloud architectures are driving great efficiency and agility gains across wide swaths of the data center, but they can also make it harder to deliver consistent performance to critical applications. Let's look at some solutions.\n- By Mike Matchett\nOne of the hardest challenges for an IT provider today is to guarantee a specific level of \"response-time\" performance to applications. Performance is absolutely mission-critical for many business applications, which has often led to expensively over-provisioned and dedicated infrastructures. Unfortunately broad technology evolutions like virtualization and cloud architectures that are driving great efficiency and agility gains across wide swaths of the data center can actually make it harder to deliver consistent performance to critical applications.\nFor example, solutions like VMware vSphere and Microsoft Hyper-V have been a godsend to overflowing data centers full of under-utilized servers by enabling such high levels of consolidation that it has saved companies, empowered new paradigms (i.e. cloud), and positively impacted our environment. Yet large virtualization projects tend to stall when it comes time to host performance-sensitive applications. Currently, the dynamic infrastructures of these x86 server virtualization technologies don't provide a simple way for applications to allocate a \"real performance level\" in the same manner as they easily allocate a given capacity of virtual resource (e.g. CPU, disk space). In addition, virtualization solutions can introduce extra challenges by hiding resource contention, sharing resources dynamically, and optimizing for greatest utilization.\nThe good news is that additional performance management can help IT virtualize applications that require guaranteed performance, identify when performance gets out of whack, and rapidly uncover where contention and bottlenecks might be hiding. We have identified a few solutions that help address these performance challenges head-on. But first, in order to really understand why these solutions are necessary let's explore why performance in today's IT environment is such a challenge.\nPerformance Is a Result, Not a Resource\nVirtualization is primarily designed to efficiently share expensive IT resources. It strives to extract maximum utilization out of the physical infrastructure. And since virtualized resources can be managed and optimized independently of other IT domains, virtualization represents a dynamically flexible way to allocate and balance resources across applications. Aiming for these kinds of efficiency goals at first seems to provide the biggest ROI in terms of the financial investment made in capital assets, but virtualization can introduce significant performance-related challenges including:\n- An inability to simply dial-in performance in the same manner as one might allocate a quantity of resource. Performance isn't a simple resource to allocate, rather it is the result of a complex function of non-linear interactions between contentious clients -- and virtualization adds complications with virtual configurations, allocation policies, host scheduling and dynamic load balancing.\n- Not knowing what performance actually is. Performance can't be fully measured or managed through simple utilization metrics. Unfortunately, what many IT performance management solutions analyze are natively measured resource utilization \"percentages.\"\n- Degraded performance from an intentionally busy system. A system's performance naturally degrades as internal utilization increases. Significantly, performance deteriorates faster if utilization is pushed past an inherent, but hard to discern, inflection point. Simply aiming to maximize utilization can destroy performance.\n- Cross-domain blindness in not knowing who is really sharing or contending for which actual resources. Ultimately, ensuring good client performance requires visibility that spans the whole IT infrastructure from end to end, apps to servers to storage. Virtualization can make it hard to discern the causes and impacts of contention both within a shared pool of infrastructure and hidden beneath and between virtualized domains.\nIT architectures that blindly optimize for maximum utilization, and subsequently make it hard to figure out who is really doing what, are unsuitable for hosting performance-sensitive applications -- which is to say, most mission-critical applications. But the situation is not hopeless. There are solutions in the marketplace today that aim to help IT with performance-focused management in dynamic IT infrastructures.\nManaging For Desired Performance\nFrom the point of view of an application, the performance it gets from the infrastructure is the sum of performance from all of the components -- CPU, memory, network, and storage. IT Performance management is an inherently cross-domain problem. However, many management solutions approach performance from a 'silo' or element perspective. While some performance-impacting issues do derive from hardware faults or operational errors, performance degradation can also stem from non-failure causes including hidden utilization bottlenecks, unintentional sharing and contention, and even dynamic \"thrashing\". Many of these performance issues are only identifiable when looking across multiple domains holistically.\nCross-domain visibility and analysis solutions include a variety of APM tools including VMware's vFabric APM and vCenter Infrastructure Navigator that map applications, transactions or even code back to virtualized resources, virtualization storage performance solutions like NetApp's Balance that map from within guest vm's across hosts and SANs into array architectures, deep infrastructure performance solutions like Virtual Instruments that can map vm's down into SAN protocol level communications, and service optimization solutions like TeamQuest Surveyor that map application processes across vms and hosts to array disks. While none of these solutions serves all IT needs, they each provide a valuable perspective. It's likely that one or two will align more or less with particular IT staff responsibilities, but all of these solutions enhance cross-domain communication and problem-solving.\nMany IT organizations are stuck in a reactive break/fix mindset when it comes to performance management. A good approach to maturing performance management practices is to calculate key performance indicators (KPI) to be used in managing the environment proactively with an eye towards optimization. For example, VMware's vCenter Operations Manager produces KPI's for health, risk, and efficiency. By deliberating setting goals to reduce or optimize such KPIs, IT organizations can even track their own relative performance.\nVirtualized organizations only really become performance-savvy when the relationship between infrastructure utilizations and delivered performance is well understood. One way to deeply understand performance is with sophisticated analytical modeling that mathematically relates resource utilizations to desired performance. The better the \"model,\" the better IT can actively and accurately make configuration, deployment, and operational decisions to deliver targeted performance-based service levels. Key to assuring performance is modeling that goes beyond simply determining if an application's capacity requirements can \"fit\" to predicting that the resulting \"response time\" performance will be acceptable.\nAt one time BMC had the most well-known, if not difficult to master, performance-based server modeling. In the last few years, key elements have been folded into the broader capacity-focused Proactive management suite. With twenty-two years of experience, TeamQuest supports expert predictive performance modeling with their Predictor solution, while the younger NetApp Balance internally leverages cross-domain modeling to automatically analyze performance, but doesn't provide interactive \"what if?\" capabilities.\nBoth NetApp and TeamQuest leverage their analytical modeling engines to produce uniquely valuable infrastructure performance KPIs. These KPIs are useful to IT in proving both ongoing service quality and improvements gained with infrastructure upgrades to business-side clients.\nCertainly performance can be improved with a system that automatically balances its resources based on current application needs. However, just because a system does a good job of balancing the capacity it has does not mean it has enough resources or know where to assign them to meet performance-based goals. And perhaps the biggest key to guaranteeing performance is the ability to plan infrastructure in advance of application growth or change. Solutions supporting \"what-if?\" modeling and scenario planning for virtualized environments help ensure that the right resources are deployed at the right time and place going forward.\nDelivering Promised Performance\nFor the many reasons described, it's hard to host performance-sensitive applications in capacity-optimizing virtualized environments. And I've heard rumblings about excessive costs from over-provisioning in the supposedly cheap and elastic cloud -- perhaps reprising the poor way IT often guaranteed performance in dedicated infrastructures.\nIn both cases I think performance-based capacity planning for performance has been prematurely dropped from many IT organizations in favor of simpler solutions that focus on capacity management for efficiency. Not only does this hamper IT's ability to deliver on specific performance goals, but the difference is also one of attitude -- managing for internal benefit versus managing to deliver high quality services for clients.\nThe good news is that additional performance management can help IT revitalize virtualization initiatives and host critical applications that require guaranteed performance. Good solutions pierce through virtualization layers when necessary to identify causes of contention and other performance impacting issues. They help measure and report on delivered performance across IT domains including servers and storage. And key to helping IT deliver on specific performance goals and virtualize important applications is that they can predict the future.\nMike Matchett is a senior analyst and consultant with IT analyst firm Taneja Group.","NetApp and VMware on Monday outlined how they’ve been working together to improve the performance and availability of their customers’ virtualized and containerized workloads.\nSpecifically, the two said they are adding depth and breadth to their existing partnership with co-engineered multi-cloud certification, improved Kubernetes application development and deployment, and enhanced access to vVols using NVMe-oF along with a new integration to use NFS 4.1 with virtual machines and ONTAP storage.\nA statement from VMware CEO Raghu Raghuram reads: “At this stage it’s clear: multi-cloud is the model we’re going to rely on for many years to come. It is the de facto operating model for the digital era, giving customers the freedom required to build, deploy, and manage applications in the way that best suits their business requirements.”\nA twin statement from NetApp CEO George Kurian said: “By delivering powerful new solutions that help companies optimize their virtual data centers, modernize their applications, and provide cost-efficient, enterprise-class data management services to VMware Cloud, we can meet customers anywhere they are on their cloud journey.”\nThe two are said to be working together to certify and support VMware Cloud and NetApp Cloud Services on the world’s largest three public cloud providers. The pair said this will help customers “seamlessly migrate, extend, or protect” enterprise workloads and files right into the cloud from their on-premises equipment with “reduced cost and risk.”\nNetApp and VMware have also, we’re told, created new certification and support for accessing vVols via NVMe-oF with “enhanced block storage flash performance and more granular VM storage management over multiple types of network transports.”\nThis includes a “new integration to enhance the availability and security of running virtualized workloads across NFS 4.1 environments.” This version of NFS provides better security, performance, and interoperability compared to version 3, we’re told.\nFinally, customers using VMware Tanzu and ONTAP-based storage arrays to manage Kubernetes environments can “now simplify and accelerate new modern application development and deployment alongside traditional virtualized workloads using enterprise-scale, high-performance and protected solutions that are jointly validated and supported.”\nThis is good stuff, giving the soon-to-be-Broadcom-owned VMware great access to NetApp’s ONTAP customers as they tick four boxes: multi-cloud, NVMe-oF, NFS 4.1, and containerization. For NetApp, as its strategists look at a less-Dell-influenced VMware, this gives it a set of good messages to pump into the VMware customer base.\nSAP provided a third-party angle on this. Ozren Kopajtic, VP Global Cloud Services at the European software giant, said: “NetApp and VMware deliver the scalable, high-performance foundation of SAP’s global public cloud platform, supporting nearly 30 petabytes of information across tens of thousands of virtual volumes and virtual machines, and running one of the largest global deployments of VMware and NetApp technologies.”\nIt occurs to Blocks & Files that NetApp has a terrific CloudOps business. Could that be used with VMware compute? Specifically, customers can use Spot to get the right (lower cost) NetApp storage instances with its cloud cost brokerage capability. Could you also use Spot to get the right compute instances for VMware and lower your compute costs that way?\nVMware’s Narayan Bharadwaj, veep of cloud solutions, said: “That’s a thought. No, not yet. I think, these are points of discussion in our in our collaboration stream as we continue our joint engineering approach.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:d7af3196-de8d-4e68-99f6-86b313602898>","<urn:uuid:3c102431-2c17-40aa-b09c-f51963837426>"],"error":null}
{"question":"Hey there! Can anyone walk me through how to set up a payment on behalf of (POBO) structure? Looking to streamline our treasury ops! 🏦","answer":"To set up a POBO structure, you need to follow these key steps: 1) Start with a payments factory that manages centralized standard payments processes. 2) Establish a single legal entity to handle third-party debt obligations for the group. 3) Exchange external bank accounts owned by group entities for in-house bank accounts per country/currency. 4) Ensure all centralized payments and reconciliation processes are standardized and controlled by one process owner. Critical considerations include having proper IT systems in place and evaluating tax implications such as withholding tax rules, transfer pricing, and VAT requirements. You'll also need to work closely with a bank that understands your corporate structure and operational goals.","context":["Many treasury departments already benefit from payment and receivables on behalf of (POBO and ROBO) structures. However, many others are still striving to reach this level of sophistication. Here, we go back to basics to find out how these structures work and how treasury teams can adopt them.\nPayments and collections are bread and butter processes for treasury departments around the world. Yet for many treasury teams, these processes still remain burdensome, costly and inefficient.\nThere is good reason for this: payments and collections are riddled with complexity, especially when the organisation is operating in multiple jurisdictions with many bank accounts and currencies. But there are solutions that, in some parts of the world, have enabled corporates to centralise their payments and (sometimes) collections processes to drive efficiency and cost savings.\nOn behalf of structures (OBO) are not a new concept, especially to those organisations that have centralised their treasury operations in an effort to drive efficiency. But it is a space that is evolving and a structure that many corporates could benefit from.\nIn focus: OBO\nA payment on behalf of (POBO) or receivables on behalf of (ROBO) – sometimes also known as collections on behalf of (COBO) – structure is a single point of payment or collection, which is set up as a separate entity and administered from a commercially convenient location for the benefit of other entities within a group of companies.\nInstead of each entity owning and operating its own external bank accounts, a single legal entity sits at the heart of the company, often supported by an in-house bank (IHB) that manages collections and all payments on behalf of participating entities. The structure enables treasurers to consolidate bank accounts and transactions across the group by using a standard payments process and streamlined bank account structure.\n“OBO structures are typically introduced within many corporates with the objective of cutting costs by significantly reducing the number of bank accounts they operate,” explains Cédric Derras, Global Head of Cash Management at UniCredit. “Forward thinking organisations will also use this as an opportunity to better manage their liquidity as money will be concentrated in one physical account without the need to use pooling structures and without the costs and haste of making intercompany transactions.”\nAs a result of these benefits, OBO structures have crept up the corporate to do list, especially since the financial crisis. Derras notes that regulatory developments have also facilitated the proliferation of OBO, especially in Europe.\n“SEPA really triggered this trend,” he says. “It gave corporates the opportunity to standardise and rationalise their multiple bank accounts and payments formats, potentially down to just one for the entire SEPA zone.” Increased automation of payments processes has become a very real prospect under SEPA too, especially with the homogenisation of electronic direct debits and credit transfers facilitated by the across-the-board adoption of ISO 20022/XML payments standards.\nIn Asia Pacific (APAC) there has also been movement in the direction of POBO/ROBO, with numerous international banks helping their most sophisticated clients adopt the structure – although this is not without its challenges.\nThe journey to OBO\nAs companies develop, collections and payments commonly start with local execution by subsidiaries. From here, regional centres of liquidity may evolve to include shared service centres (SSCs). An IHB banking structure may extend the centralisation programme, replacing most or all external bank accounts for subsidiaries with one in-house operation.\nWith the technical and operational structure of an IHB in place, the next logical move is to establish a regional payment factory. With a change in the internal bank account structure, a POBO operation may be deployed so that the company can make payments using a single bank account per currency or country for all participating group entities.\n“POBO/ROBO are tools that can be deployed in structuring treasury flows which can bring about tremendous benefits for treasurers,” says Shirish Wadivkar, Global Head of Payments and Receivables at Standard Chartered. “POBO is really the penultimate step on the journey – one account for payments in one currency is a very neat structure. ROBO is the last leg, although very few corporates, even in Europe, are there yet. Such initiatives typically require a deep understanding of the client’s corporate structure and various local regulations.”\n“Whilst there is a common path to putting in place an OBO structure, there is no one route. “How these structures look and how they are used is very much dependent on how the group is organised in the first place, its business model and level of treasury sophistication,” says Jeffrey Ngui, Head of Regional Sales Asia, Cash Management at BNP Paribas. “Simply putting an OBO structure in place without considering how it might impact all areas of the business is unwise, as the change can be drastic and it will shock the organisation.”\nThe problem with POBO\nDespite delivering numerous benefits, POBO is still not the complete solution that corporates are seeking – even in Europe, notes Francisco de Barros, EMEA Regional Treasurer at AbbVie.\nFollowing the spin-off of AbbVie from Abbot Laboratories in 2013, the company inherited over 500 bank accounts spread across over 25 banking partners. Payments were typically managed at a local level by affiliates. This was clearly inefficient on many levels and the treasury set a goal: to create a simple and streamlined best in class treasury organisation that allowed our affiliates to focus on their core competencies.” The team set about doing this by building an IHB that included local accounts for each of the major currencies used by the company.\nCore to AbbVie’s plans was a POBO structure. The treasury had many positive conversations about the solution and its benefits with its banking partners and peers. However, when implementing the solution, de Barros admits that he was surprised by some of the limitations that exist around POBO, even within the Single European Payments Area (SEPA). Even more surprising was that these were not discovered until the structure was being implemented. “Most of the banks, which are big advocates of the solution, or papers on the topic, often don’t talk about these limitations or how to address them,” comments de Barros.\nWithin the SEPA region, the main problem arose from the fact that some institutions in a number of countries, including Italy, Portugal and Spain, do not allow or recognise payments made on behalf of by a sister company.\nThese are mainly payments related to taxes, regulatory agencies or payroll related items, says de Barros. “Also, despite the SEPA mandate, we still see some of the local institutions mandating older legacy formats or payments being made from accounts in-country, meaning that these cannot be included in our POBO structure.”\nOutside of SEPA, the hurdles around OBO structures are more well known, but de Barros found that these were still not well communicated by advocates of the solution. “A good example is China where under certain structures cross-border POBO is allowed, but domestic POBO is still regulated,” he explains. “Also, the standardisation around bank coding and payment formats needs to be improved and standardised further for this to be truly effective for corporate treasury. In fact, across many of the emerging markets, the regulatory environment is one of the key impediments for POBO.”\nTaking the solution forward\nThe result of these issues is a solution that doesn’t deliver on all its promises. And for de Barros it may take some time for these to be resolved because there is “little to no open discussion with the different authorities to find a path forward”.\n“By now most treasury organisations are aware of POBO and its advantages. We now need to move towards highlighting the issues so the discussions around the topic can start,” says de Barros. “At this point, however, I have not seen any discussions around the issue, nor workgroups to attempt to address it.” He also calls for more standardisation in payment formats and regulation so that the needs of corporate treasuries can be better met.\nSetting it up\nTherefore, before undertaking any OBO project a number of elements should be in place to ensure success. Having a centralised treasury and all participating entities on the same IT systems is nice to have, but not essential. What is critical, though, is to ensure that all the centralised payments and reconciliations processes are standardised and controlled by one process owner.\nThe starting point of a POBO operation will usually be a payments factory, which manages a centralised standard payments process for participant entities. With the technical infrastructure and processes in place, a single legal entity will be established to pay the third-party debt obligations of another legal entity in the group. The process requires the exchange of external bank accounts owned by the group entities for IHB accounts (owned by the payment factory) per country and/or currency.\nROBO has a similar structure, requiring entities to substitute external accounts for IHB accounts. In this set-up, a central collections factory initiates a claim on behalf of a group entity for payment from a third party. The third party will make the payment into the relevant central account for the currency country.\nCorporates must also consider the tax implications of their OBO structures. Some areas to consider include assessing whether withholding tax and thin capitalisation rules are applicable for a proposed OBO operation. There may also be issues around transfer pricing and Controlled Foreign Companies legislation. In addition, VAT and stamp duty may impact the feasibility of an operation or the entities it covers.\n“To put either a POBO or ROBO structure in place, the corporate needs to work closely with a bank who understands the structure of the corporation and how it operates,” notes Derras. “There are so many nuances involved – especially when doing this in multiple countries – that it is vital that the banks understand relationships between the holding companies and subsidiaries, their legal structures, and what the group is trying to achieve overall.”\nWhen talking about OBO, it is hard to avoid a discussion about virtual accounts, a solution that all banks are currently keen on discussing. And there is good reason: they facilitate OBO structures.\nAlthough virtual account solutions vary slightly from bank to bank, with some nuanced functionality, they largely operate in the same way. Essentially each account is a ‘subsidiary’ or sub-account of the client’s own physical account with the bank; they cannot exist outside of that immediate relationship, hence they are virtual. The key to a virtual account is thus the virtual account number/identifier.\n“We have virtual IBANs in every country that we have a branch in,” says Dick Oskam, Global Head of Sales for Transaction Services at ING. “These enable our clients to route their payments from their main bank account through these virtual IBANs. This means that the payment is still centralised but the routing mechanism makes it look like it is a domestic payment from a domestic account, so the supplier can understand better who is paying them.”\nVirtual accounts can also help on the collections side. “ROBO is more challenging for corporates because of the reconciliation issues that exist,” says Oskam. “We have therefore deployed our virtual account solution to enable our clients’ customers to pay to local IBANs, which then route the payment directly to the corporates header account. This includes information about who has paid and for what.”\nEven if a company has a highly sophisticated IT structure, uses a centralised treasury function with a highly streamlined bank account structure and is leveraging virtual accounts, it doesn’t mean that OBO will definitely be possible. This is especially true in APAC where the regulatory and taxation landscape prohibits OBO structures in many of the region’s markets as it translates to inter-company borrowing. Further if the OBO structure is envisaged to be cross-border then it ends creating cross-border capital flows too. “If it is hard to set up an OBO structure in Europe, it is more challenging in Asia,” says Standard Chartered’s Wadivkar.\n“If you look across the markets in Asia there are only a few that are liberal in nature,” adds Shi Wei Ong, Global Head, Cash Liquidity Management Products at Standard Chartered. “So whilst you can do OBO to some degree in Singapore, Hong Kong, Australia and Japan, it is nearly impossible to get the full benefit by setting up a regional programme.”\nThe regulatory challenges differ from one country to another. Some have capital controls, for example. Others come with onerous conditions, such as per-transaction reporting, that add another layer of complexity to an already complex structure. “We support numerous regional treasury centres in Asia and very few are in the POBO/ROBO space,” says Ong. “It is the holy grail in Asia.”\nBNP Paribas’s Ngui agrees that true POBO/ROBO is hard to achieve in Asia. The bank has clients which operate hybrid structures whereby true OBO structures are deployed in the countries where this is possible. “In the more restricted markets where POBO is not allowed, ‘payments in the name of ‘can be applied,” he says. “This is quite similar to POBO and sees them operate an account belonging to the entity in the country from a centralised location. This doesn’t offer the full benefits of POBO but it does help, to an extent, and there are savings in terms of resource, IT deployment and economies of scale because it is still a shared service centre style model.”\nMaking the change\nOBO clearly has many benefits if done correctly. But the big issue for corporates is the sheer amount of work required to get there. And although this is a solution that can benefit many corporates, it may just be one step too far for some.\nUltimately though, the decision to head in this direction should be made as part of a broader structural overhaul of the company. The project will touch all corners of the business and all impacted departments must be intimately involved from the start if it is to deliver the desired results. But if the work is put in, the reward may be a truly best in class treasury department."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:19e7de0e-a62e-4440-b3a9-f4adc4b7ec6c>"],"error":null}
{"question":"¿Cuál es la principal diferencia entre la bronquitis y los problemas dentales caninos en cuanto a los síntomas de dolor? 🤔","answer":"The main difference in pain symptoms is that dental problems in dogs primarily manifest through localized pain while eating (showing changes in eating habits, dropping food, or chewing on one side), while bronchitis pain is mainly felt in the chest area, causing chest tightness and discomfort. Dogs with dental problems may paw at their mouth and show reluctance to eat, while bronchitis patients experience chest pain along with coughing and wheezing symptoms.","context":["Canine Dental Problems\nCanine dental problems include such periodontal disease as gingivitis and periodontitis. Dogs with dental problems\nommonly show symptoms such as bad breath, excessive drooling, loss of appetite, and more. Read on and learn more about the various symptoms, possible causes,\nand treatment of dog dental problems.\nDog dental problems are common canine health problems. In fact, a lot of the most serious health problems in dogs are related to their gums and teeth.\nAs dog parents, we need to take good care of our dog's teeth by regular brushing. We should also check our dog's mouth regularly for early signs of dental problems.\nOur dogs will also give out tell-tale signs when there is something wrong with their mouth or teeth.\nSome common signs and symptoms of canine dental problems include:\nSigns and Symptoms of Canine Dental Problems\n[Top of Page]\n- A change in eating habit: Dogs usually eat heartily. However, if they have pain in their mouth, they eat slowly and sometimes drop food that is\ntoo large or difficult to chew. They may also chew on only one side of the mouth.\n- Appetite loss: If the pain in the mouth is too severe, the dog will stop\n- Excessive drooling; Drooling is a sign of pain. If your dog suddenly drools\na lot, be sure to check inside his mouth for possible dental problems.\n- Other Signs of Pain: You will also notice other signs indicating that the dog is in pain, such as pawing at the mouth,\nlethargy, and reluctance to play or move.\n- Bad breath: The most common causes of bad breath in dogs are\nperiodontal disease such as gingivitis and periodontitis. If your dog has bad breath and is drooling excessively, there is a high chance that he is suffering from\nsome form of dental problems.\n- Redness: A dog with periodontal disease has gums that have turned red. That is the result of inflammation of the gums - there is an\nincrease in blood supply and an influx of white blood cells to fight infection.\n- Bleeding: Dogs with dental problems bleed easily (sometimes even when eating) because inflammation can weaken the walls of the blood vessels.\n- Swelling: Canine dental problems can cause swellings which can occur in the mouth itself - check the gums around the edges of the teeth and higher\nup and deep under the lips. Swellings can also occur on the face just below the eye. Sometimes these can be linked to tooth root abscesses.\nCommon Canine Dental Problems\nCommon canine dental problems include tartar build-up, and more seriously, periodontal disease. Canine periodontal disease starts out as gingivitis, which is an inflammation\nof the gums. If left untreated, gingivitis can develop into periodontitis, which is an inflammation of the deeper structures supporting the teeth.\n[Top of Page]\n- Tartar Build-up\nTartar (dental calculus) is composed of calcium salts, food particles, bacteria, and other organic material. It is yellow-brown and, at its early stage, it is soft and is\ncalled \"plaque\". Once plaque hardens, it becomes tartar. Tartar can develop on the surface of all teeth, but is most commonly found on the cheek side of the upper premolars\nTartar build-up occurs to some extent on the teeth of all dogs over two years of age. Small dogs and certain breeds (e.g. Poodles) tend to be more prone to tartar\nIf tartar is not removed from the teeth, gingivitis will result.\nCanine gingivitis is caused by tartar build-up along the gum line, producing points where the gum is forced away from the teeth. As a result, small \"pockets\" are formed\nbetween the gums and the teeth trapping food and bacteria. Over time, this causes inflammation and infection of the gums.\nOne sign of gingivitis is bad breath. If you look at the dog's gums, you will find them swollen and red, and they bleed easily even if you just press on them gently. Sometimes,\npus will also ooze from the gum line when pressed.\nIf your dog has gingivitis, it is imperative that you take him to the vet immediately to prevent it from developing into periodontitis, which is more serious and irreversible.\nYour dog may be put under general anesthesia so as to get the tartar and plaque removed. After that, regular oral care at home should be done to prevent further tartar build-up.\nAs gingivitis progresses some of the damage becomes irreversible.\nPeriodontitis is a continuation of gingivitis. The gum infection progressively attacks the supporting structures to the tooth, such as the fibres of the periodontal\nligament, the actual bone supporting the tooth, and the bone socket. As a result, the affected tooth or teeth will begin to loosen and eventually decay.\nPeriodontitis is extremely painful, causing the dog to stop eating or drop food from his mouth. Excessive drooling is common. A root abscess resulting from\nperiodontitis can rupture into the nasal cavity, causing swelling below the eye, and a pus-like\nnasal discharge from one nostril.\nA dog with periodontitis should have his teeth professionally cleaned (as in gingivitis). If the disease is in its advanced stage, some or all of the teeth need to be\nextracted. A portion of the infected gum may need to be removed. Antibiotics are given for the infection for one to three weeks.\nAftercare at home is also necessary for the gums to heal. It usually involves rinsing the mouth with 0.2 percent chlorhexidine solution once or twice daily. A plastic\nneedle-less syringe can be used to squirt the solution directly onto the teeth and gums. This is followed by massaging the gums in a gentle circular motion. A soft gauze\npad wrapped around the finger can do the job. While the gums and teeth are healing, feed soft food to the dog, such as canned food mixed with water.\nImportance of Canine Dental Care\nCanine dental problems can not only cause a lot of pain to our dogs, dental treatment is usually very expensive for dog parents!\nMore importantly, dog dental problems do not limit to the damage of the dog's teeth and gums. Dental problems in dogs can also have adverse systemic (whole body) effects,\ncausing problems to major organs, such as the lungs, heart, and kidneys.\nIn view of the above, it is extremely important for us dog parents to do all we can to give the best\ndental care to our dogs.","Symptoms And Causes Of Bronchitis: What Are the Signs and Symptoms of Bronchitis?\nAfter you already have the flu or a cold acute bronchitis due to an illness generally develops. The main symptom of acute bronchitis is a constant cough, that might last 10 to 20 days. Other symptoms of acute bronchitis include wheezing (a whistling or squeaky sound when you breathe), low fever, and chest tightness or pain. If your acute bronchitis is serious, in addition you may have shortness of breath, particularly with physical action. The signs or symptoms of chronic bronchitis include coughing, wheezing, and chest discomfort.\nChronic Bronchitis Symptoms, Treatment and Contagious\nBronchitis is considered chronic when a cough with mucus continues for at least two years in a row, and at least three months, for most days of the month. Bronchitis occurs when the trachea (windpipe) and the big and small bronchi (airways) within the lungs become inflamed due to disease or annoyance from other causes. Chronic bronchitis and emphysema are types of a condition defined by progressive lung disorder termed chronic obstructive pulmonary disease (COPD).\nBronchitis and asthma are two inflammatory airway illnesses. The illness is called asthmatic bronchitis when and acute bronchitis happen together. Common asthmatic bronchitis triggers include: The symptoms of asthmatic bronchitis are a blend of the symptoms of asthma and bronchitis. You may experience some or all the following symptoms: You might wonder, is asthmatic bronchitis contagious? Yet, persistent asthmatic bronchitis commonly is just not contagious.\nBronchitis is a Familiar Infection Causing Irritation and Inflammation\nIf you suffer from chronic bronchitis, you might be vulnerable to developing more serious lung disorders as well as heart problems and infections, so you should be tracked by a physician. Acute bronchitis is generally due to lung infections, 90% of which are viral in origin. Repeated attacks of acute bronchitis, which weaken and irritate bronchial airways can lead to chronic bronchitis.\nBoth adults and children can get acute bronchitis. Most healthy individuals who get acute bronchitis get better without any issues. After having an upper respiratory tract illness like the flu or a cold frequently someone gets acute bronchitis a day or two. Breathing in things that irritate the bronchial tubes, including smoke can also causes acute bronchitis. The most common symptom of acute bronchitis is a cough that generally is not wet and hacking at first.\nBronchitis (Acute) Symptoms, Treatment, Causes\nWhat's, and what are the causes of acute bronchitis? Acute bronchitis is inflammation of the bronchial tubes, and acute bronchitis is suggested by a cough lasting 5 or more days as a cause. Chronic bronchitis may be developed by people who have persistent acute bronchitis. The most common causes of acute bronchitis are viruses. Bacterial causes of the disorder contain: Other irritants (for instance, tobacco smoking, chemicals, etc.) may irritate the bronchi and cause acute bronchitis.\nCoughing Up Green Mucus Contrary to what many think, mucus secretion is important for the body. This sticky secretion lubricates our respiratory organs and protects their membranes against infectious bacteria, fungi, and other environmental pollutants. An average human...\nYou can Find Two Types of Bronchitis: Acute (Short-Term) and Chronic (Long-Term)\nWhile smokers and people over 45 years of age are most likely to develop chronic bronchitis, babies, young kids, and the elderly have a heightened risk of developing acute bronchitis. Smoking is the most common cause of chronic bronchitis and may also result in acute bronchitis. Treatment for chronic bronchitis includes bronchodilators, anti-inflammatory drugs, and chest physical therapy for loosening mucus in the lungs. Seek prompt medical care if you're being treated for moderate although bronchitis symptoms recur or are persistent.\nBronchitis Symptoms, Treatments & Causes Merck Manuals\nInfectious bronchitis typically begins with the symptoms of a common cold: runny nose, sore throat, fatigue, and chilliness. When bronchitis is intense, fever may be slightly higher at 101 to 102 F (38 to 39 C) and may last for 3 to 5 days, but higher fevers are unusual unless bronchitis is brought on by influenza. Airway hyperreactivity, which is a short term narrowing of the airways with restriction or damage of the quantity of air flowing into and out of the lungs, is common in acute bronchitis. The damage of airflow may be activated by common exposures, such as inhaling light irritants (for instance, cologne, strong scents, or exhaust fumes) or cold atmosphere. Older individuals may have unusual bronchits symptoms, like confusion or accelerated respiration, rather than temperature and cough.\nThe study - led by Cardiff University in the UK - shows for the very first time the calcium-sensing receptor (CaSR) plays a key part in causing the airway disorder. Daniela Riccardi, principal investigator and a professor in Cardiff's School of Biosciences, describes their findings as \"unbelievably exciting,\" because for the first time they've linked airway inflammation - that may be activated for example by cigarette smoke and car fumes - with airway twitchiness. She adds: \"Our paper shows how these triggers release substances that activate CaSR in airway tissue and drive asthma symptoms like airway twitchiness, inflammation, and narrowing.\nWhat Does Chronic Bronchitis Sound Like RECORDING (Wheezing symptoms emphysema Need Help Acute Cough\nAudio Recording of how chronic Bronchitis cough sounds like while laying down. The difference between bronchitis & pneumonia is that bronchitis causes an ...\nProf. Riccardi reasons: The researchers believe their findings about the purpose of CaSR in airway tissue could have significant consequences for other respiratory conditions such as chronic obstructive pulmonary disease (COPD), chronic bronchitis. The researchers, from Washington University School of Medicine in St. Louis, consider their findings will lead to treatments for a variety of diseases including asthma, COPD, cystic fibrosis and even certain cancers."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:592a00d8-a914-4ca0-988f-0a04171bdadc>","<urn:uuid:70ac49db-3c3e-4094-a20d-edd719169897>"],"error":null}