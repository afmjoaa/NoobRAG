{"question":"What's main difference between angel investors and venture capital investors in their funding approach? Need quick comparison!","answer":"Angel investors are wealthy individuals who invest their own personal funds in startups, while venture capital investors manage other investors' money through a fund. Angels typically invest in startups and small businesses looking for high returns while also often providing mentorship as former entrepreneurs themselves. VCs focus exclusively on high-growth early stage businesses, take larger stakes, and sometimes require board seats to manage their fund's investments.","context":["In the previous article in this series, I encouraged you to scrutinise your business goals and to decide if sourcing external funding will help you achieve those goals, without putting undue pressure on your business. We looked at funding for a short-term business boost – like buying inventory ahead of a peak season, hiring staff or launching a marketing campaign – and for long-term goals, like expanding into new markets or investing in an asset. I also played devil’s advocate and asked if there were other, more cost-effective ways to achieve these goals other than selling shares in your company or repaying a loan plus interest.\nBut if you’ve explored all options and are confident that money is the only thing standing between you and success, then deciding on the type of funding that best suits your business goals and financial circumstances is the next pivotal task.\nEquity vs Debt funding\nBusiness funding can be split into equity and debt funding. With equity funding, you raise money from investors in exchange for a portion of ownership in your business aren’t bound by hard repayment term. With debt funding, you are borrowing money without giving up ownership, but have to pay back the loan, with interest, within a specified time frame. The final option is to receive funding that you do not have to repay. This comes in the form of grants or competitions.\nLet’s take a look at the nuances of the different small business funding options available.\nAngel and Venture Capital (Professional Investors)\nAngel and Venture Capital (VC) investors offer you funding in exchange for a stake in your business. Both Angel investors and VCs fall under the “professional” investor category since they are more experienced and dedicated investors than friends and family. Both categories of investors expect high growth in their investment. They also bring significantly more capital to your business, but to do that; they would conduct a thorough assessment of all aspects of your business (a due diligence) before deciding to invest.\nAngel investors are typically wealthy individuals who invest their own money in start-ups or small businesses, in exchange for a share in the business. They are motivated by the potential to make high returns on their money (In the form of sharing profits or the sale of their shares at a later stage). But they are often former entrepreneurs, who want to support entrepreneurship in general and do so by investing and mentoring other entrepreneurs.\nVenture capital investors manage other investors’ money through a fund. The fund is tasked to make investments in a number of high growth, early stage businesses within a specified timeframe. VCs seek investments that can deliver massive returns and are less risk averse than traditional funds. However, to do that, they often take on a considerable stake in the business and in some cases a seat on the board.\n- Opportunity to raise larger amounts of money compared to alternative options\n- No need for collateral\n- Many investors are serial entrepreneurs and can offer useful business advice, guidance, and knowledge\n- No monetary repayments\n- Flexible business arrangements (in most cases)\n- Because investors expect shares in exchange for funding, you lose some control of your business\n- The fundraising process is time-consuming and often grueling.\n- Not a good fit if your business is not set up to be high-growth\n- High competition among businesses competing for funding\n- Some VCs come with complex investment structures, which requires some experience on the legal and financial side\nFriends and family\nFriends and family are often the first sources of funding you go to, especially when you are starting out. You can choose either an equity or a debt finance model. You can either agree to pay them back (hopefully with interest) or give them equity in your business. To help you decide, ask yourself if the person can give provide guidance or assist your business beyond funding. If so, giving them equity may be the right call. If you do not want the person directly involved in your business, then it’s advisable to opt for a loan. Just make sure you’re confident you will be able to repay them.\n- Quick access to money\n- Relatively low-risk funding option\n- Gain the support of those closest to you\n- Friends and family are less likely to turn down your request for finance if they have the money\n- Repayment terms will be more flexible and low-interest, or interest-free\n- If you run into difficulties with your business, this could impact your personal relationships\n- You will feel responsible if your friends or family member loses money. This concern may make you more risk averse than you should be.\nQuick tip: Even though you are working with someone you know, put a contract in place.\nDEBT FUNDING OPTIONS\nBank loans (Term loans and Working capital loans)\nWhen borrowing money from a bank, you generally have two options: a term loan, or a working capital loan. Term loans are best for long-term expenses like expanding into new markets or purchasing an asset. Working capital loans are ideal for short-term financial commitments, like hiring seasonal staff or buying stock ahead of a busy season.\nTerm loans are repaid in regular payments – generally monthly – over a set period – anything from three to 10 years. Term loans are subject to interest repayments, which can be substantial and are affected by economic fluctuations.\nWorking capital loans take the form of overdrafts, short-term loans (three to six months) or debtors funding, to fund the day-to-day running of the business and to cover immediate and short-term financial commitments.\n- No need to sell shares in your business, so you maintain full control\n- Among the most secure funding options\n- Working capital loans are relatively straightforward products and are typically better options to alternative lending\n- Working capital loans do not require collateral\n- Bank loans are subject to interest rate repayments, which means you end up paying a lot more for the money you borrowed – especially if interest rates increase during the repayment term\n- You may need to attach an asset to secure the loan\n- It can be a long process and involves a lot of paperwork before you actually see any money in the bank\n- If you default on payments, the bank could attach your assets\n- Bank loans can be difficult to secure for start-ups that don’t have the necessary credit history or personal sureties and assets\nGovernment loans are issued by the Department of Trade and Industry and organisations like the Small Enterprise Development Agency. Unlike grants, government loans do need to be paid back.\nThere are a number of funding agencies in South Africa that support different business needs. The Land Bank, for example, is a good option if you need to invest in an asset for a farming business, and the Small Enterprise Finance Agency might be able to help you expand into other markets.\n- Lower interest rates than bank loans\nLonger or more flexible repayment terms\n- Very strict qualification criteria\n- The application and approval process can be long and frustrating\nAlternative lenders are digital credit providers other than banks that offer financing to small businesses that do not qualify for a bank loan. These technology-driven businesses don’t have the infrastructure and operating costs on the scale that banks do, which allows them to offer loans at comparatively low-interest rates and with flexible repayment terms. Many alternative lenders promise fast turnaround times thanks to data-driven decision-making and transparent fees. This makes them an attractive funding option for small businesses that need to access finance quickly.\nRetail Capital has an interesting operating model. The loan granted to you depends on your average monthly card turnover. If you’re using a solution like Yoco, you can quickly calculate your average turnover in the business dashboard. You repay the loan in daily micro-payments, calculated as a percentage of your earnings for the day. The percentage is agreed upon upfront so there are no unwelcome surprises at the end of the day and you can better manage your cash flow.\nLulalend offers six- and 12-month repayment options that include a monthly cost. Finance is granted based on the health of your business and your personal credit score.\n- Access to finance is quick, with some lenders promising a turnaround of a few hours to a few days\n- Little to no paperwork, as everything is done online\n- Maintain full control of your business\n- No assets required\n- Transparent fees and flexible repayment terms\n- Less stringent qualification criteria\n- Simplified borrowing experience\n- Potentially high or confusing interest rates\n- Shorter terms and smaller loan amounts, depending on how much you qualify for\n- Some alternative lenders may not be fully regulatedlo\nUNIQUE FUNDING OPTIONS\nSecuring funding that you do not have to repay can be a fantastic boost for your business. You could enter a small business competition, request donations if you’re an NPO or apply for Grants. Alternatively, you could look into crowd funding. This is where you can decide what you would like to offer sponsors in return for their contributions.\nYou can finance your project or business venture by raising small amounts of money from lots of people who believe in your idea. Crowdfunding sites like Kickstarter, or South African platforms, like Thundafund and Jumpstarter, are ideal channels for connecting entrepreneurs and investors. The idea behind crowdfunding is that ‘every little bit helps’ and it works best for projects that require relatively small amounts of capital.\nIt’s important to remember that crowd funding does not mean free money and donors will expect something in return – free products or shares in your business, for example.\n- Raise money quickly. If there’s demand for your product in the market, people are more likely to donate – especially if it earns them a discount on that product in return.\n- Market your business at the same time. Crowdfunding campaigns are most successful when you leverage your social networks to reach more potential donors. You also have an opportunity to test market appetite for your product or service and get feedback on improvements that you can make before releasing it to the wider market.\n- It’s free. Well, nearly. It’s free to set up a crowdfunding campaign, and there are no penalties if you don’t meet your goal in time. But if you do, a commission will be payable to the platform, which is generally in the region of 5-10%.\n- Crowd funding can be very time consuming and resource intensive. For example, distributing your product to your backers after the campaign can take up a lot of time depending on the size of the base.\n- If you’re offering shares in your business in exchange for money – which is less common in crowd funding – you could end up being beholden to thousands of inexperienced shareholders when it comes to making important business decisions.\nThere’s a risk that someone could copy your idea.\nThe South African government offers funding grants to small businesses, but these come with strict guidelines and qualification criteria. Grants are linked to government’s expressed goals and economic growth strategies, which could include – but are not limited to – the advancement of black economic empowerment, job creation, and economic development.\nSay, for example, you want to expand your black-owned business into other markets. You could qualify for the Black Business Supplier Development Programme grant. To qualify, you must be a CIPC registered company or CC, must be 50.1% black-owned, must have valid tax clearance from SARS, and your turnover needs to meet a certain threshold.\nThis is one of many different types of government grants designed for start-up and growing businesses.\n- Government grants do not need to be repaid and do not accrue interest\n- You maintain full control of your business as you don’t need to give away equity\n- Paperwork intensive\n- Government may specify how you can spend the funds\n- The application and approval process can take a long time to complete\nWhichever funding option you choose, it’s likely that you’ll have to produce proof of your revenue and operating expenses, especially when applying for funding with banks or investors.\nSecuring the funding\nOnce you know which funding option you want to go, you need to start preparing. Most of the funding options will require you to","WHAT IS AN ANGEL INVESTOR?\nAn angel investor is an individual who makes direct investments of personal funds into a venture, typically early-stage businesses. This is in contrast to venture capitalists who typically raise and invest third party funds. Because the capital is being invested at an early, risky stage in a business venture, angel investors must be capable of taking a loss of the entire investment; as a result most angel investors are relatively high-net-worth individuals.\nWHAT IS AN ANGEL GROUP?\nAn angel group is a formal or informal assembly of active angel investors who consider co-investing in businesses. Key characteristics of an angel group are: organization and control of the group by one or more member angels (who manage the group's processes and procedures), and collaboration by member angels in the investment process.\nWHEN AND WHERE DOES branch venture group MEET?\nMeetings are limited to angel members and invited guests only. Branch Venture Group meets once per month in Boston, MA.\nIS MEMBERSHIP IN branch venture group RESTRICTED?\nYes. Please see BECOME A MEMBER for a description of membership requirements above.\nWHAT IS branch venture group's POLICY ON GUESTS?\nGuests must be sponsored by an existing member and are welcome to attend one meeting given advanced notice to the Partners. Guests’ behavior must conform to Branch Venture Group’s Code of Conduct. Before investing in an opportunity presented at a group meeting, guests must be approved as members and agree to the terms of the Membership Agreement.\nMUST I MAKE A MINIMUM INVESTMENT EACH YEAR TO REMAIN A MEMBER?\nMembers are encouraged to be active investors, but since each investor’s threshold and interests differ, there is no strict investing requirement. In order to keep Branch Venture Group’s membership as active and vibrant as possible, the Partners reserve the right to refuse renewal for members who are neither investing nor making active contributions to the community.\nIS THERE A MINIMUM OR MAXIMUM AMOUNT OF MONEY I MAY INVEST?\nNo. Individual deal terms may impose per-angel minimums or per-angel maximums on a deal-by-deal basis. Typical deal minimums will range from $10,000 -$25,000.\nHOW ARE INVESTMENT DECISIONS MADE?\nBranch Venture Group members will pool their expertise, experience, and observations and work together on due diligence to review deals of interest, but in the end, each member will make his or her own personal investment decisions.\nDOES Branch venture group HAVE ITS OWN \"SIDE-CAR\" FUND?\nNo. Branch Venture Group encourages active rather than passive involvement by its angel members and does not provide a passive conduit for investing. However, we have the opportunity to syndicate deals and with other funds.\nHOW IS branch venture group FUNDED?\nThe administration of Branch Venture Group is funded by its members’ dues. Members of Branch Venture Group invest directly on their own behalf.\nDOES branch venture group MAKE MONEY OFF MY INVESTMENTS?\nNo. Branch Venture Group does not charge any management fee or receive carry on your investments, nor does Branch Venture Group receive (nor would it ever consider receiving) any finder or referral fees from the companies in which you invest. You keep all the returns you generate. Branch Venture Group's role is to provide its membership with the best opportunities in which to invest.\nHOW WILL PRESENTING COMPANIES BE SELECTED?\nA screening committee consisting of the Partners and interested members reviews plan submissions and meets to discuss and select the opportunities likely to be of greatest interest to the group. Any member is welcome to apply to join the screening committee.\nHOW SHOULD COMPANIES APPLY FOR FUNDING?\nGo to the Entrepreneurs section of this site, review our investment criteria, preferred stages and investment process and then visit our Gust profile here.\nARE THEIR GEOGRAPHICAL RESTRICTIONS ON WHERE branch venture group WILL INVEST?\nYes. Branch Venture Group focuses on companies located in Boston or within a few hour’s drive of the Boston area but we will consider opportunities from other regions in the United States.\nHOW IS DUE DILIGENCE CONDUCTED?\nMembers conduct due diligence themselves on a voluntary basis. A lead diligence coordinator is generally chosen, an agreed-upon diligence checklist is utilized (which is never less thorough than the minimum guidelines set by the Angel Capital Association), groupware and collaborative tools are utilized wherever possible to enhance efficiency for members and entrepreneurs, and diligence reports are shared openly within the member group (and with syndication partners from other groups, where appropriate). Diligence reports are not shared outside of the group or with the entrepreneur.\nHOW IS branch venture group RUN?\nThe Partners administer the group and welcome the input of Branch Venture Group members on how the group is administered. Branch Venture Group works in service of members and consults with members on matters relating to long-range issues and fundamental policy questions.\nHOW DO I LEARN MORE ABOUT ANGEL INVESTING?\nBranch Venture Group organizes programming for new members as an intensive introduction to angel investing. Branch Venture Group has speakers on relevant industry topics as part of the monthly meetings. Members also learn from other experienced angel investor group members, and the Partners can direct interested members to additional materials as needed."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:bb16cad6-f7bf-4688-90c6-9bd28de5427b>","<urn:uuid:6ccfd985-b4a6-43ba-8ed7-970eec71b6a4>"],"error":null}
{"question":"I'm planning to sail through French Polynesia and need to know - what specific wind conditions should I watch out for in Society Islands, and how do these winds impact reef pass navigation?","answer":"In the Society Islands, the main wind to watch for is the mara'amu, a south-eastern wind that blows between July and September, reaching speeds of 25-30 knots (40-60 kph). This creates short and choppy seas, particularly in inter-island channels. Additionally, strong westerly winds can occur between December and February. These winds significantly impact reef pass navigation, especially when they oppose strong currents, creating confused standing waves. This is particularly dangerous in passes exposed to the south during mara'amu conditions. Even in passes that are typically navigable, wind conditions can combine with swell to create hazardous situations, as winds can seriously disturb passes when moving opposite to strong currents.","context":["East-west trade winds make for generally easy navigating between the Society Islands and the Marquesas Islands, while a little more caution is required in the Tuamotu Islands, the Gambier Islands and the Austral Islands. One of the Society Islands’ strong distinctive features is their location at an amphidromic point, or a tidal system where the tidal range is almost zero. Only the solar tide functions, but it is weak and occurs at the same time every day. The navigation beacons in the lagoons and at the entry of passes through coral reefs in the Society Islands are very efficient. These islands offer many mooring places and nautical facilities for a variety of boats and yachts. A wind known in Tahitian as the mara’amu blows between July and September, coming from the south-east and capable of reaching speeds of Force 6-7 (25-30 knots/40-60 kph/25-38 mph).\nThis causes short and choppy seas, particularly in the channels between islands. Between December and February, a wind from the west can cause strong wind blows.\nCoral reef passes\nWhile these passes are wide and passable in all weather conditions in the Society Islands, with the exception of Maupiti in the leeward Islands, it is recommended that they be navigated when conditions are slack in the Tuamotu Islands, where there may be strong currents. In fact, these passes are not very deep and are exposed to strong swells, making them dangerous to navigate, particularly passes exposed to the south when there is a mara’amu, or strong southerly wind.\nThe majority of the Society Islands have deep and sheltered bays. The exterior coral reefs are often bordered with vast expanses of white sand on the lagoon side, are not very deep, and are scattered with coral formations which make heavenly places to drop anchor.\nRenting a sailboat with or without a crew (for the more experienced sailors) remains one of the best ways to discover The Islands of Tahiti.The almost unlimited number of moorings and the navigational conditions make it a yachter’s paradise.\nIn each island, we suggest you to heed advice in order to select the best place to drop anchor, without disturbing local activities such as fishing in the lagoon, cultured pearl farming…\nPapeete Port Authority\nThe port of Papeete is the only international commercial port in French Polynesia. It is equipped with port facilities that can accommodate commercial and cruise ships as well as pleasure crafts and luxury yachts. Extensive renovations are under way in order to ensure comfortable and secure harbor installations for foreign sailors and the local population alike.\nIn Tahiti, Moorea, Raiatea (the main center for nautical activities) and Bora Bora, several marinas and nautical bases are available for pleasure boaters.\nTypes of Yacht Experiences\n- Private Sailing Charter\nCrewed sailing catamarans or monohulls sailing multiple islands with flexible itineraries.\n- Private Motor Yacht Charter\nCrewed motorized yachts sailing multiple islands with flexible itineraries.\n- Sailing Cruise Cabins\nPrivate cabins aboard a sailing catamarans or motorized yacht with fixed itinerary, on an all-incluse package to multiple islands.\n- Bareboat sailing Charter\nCaptain of your own catamaran or monohull.\nChecklist for the Perfect Sailing Experience:\n- Trade winds are predictable and weak to moderate most of the year.\n- Inter-island sailing is short and voyages can include multiple islands and atolls.\n- Virtually every island and atoll has an 80°F (27°C) neon-blue lagoon.\n- Lagoons are calm and protected with many anchorages.\n- Passes are wide, have weaker currents, and feature beacon systems.\n- Supplies are easily found at island markets, marinas, shops and food stands of fisherman and farmers.\n- Safety is a part of the islands’ ocean culture with a permanent VHF maritime radio channel, daily meteorological reports, emergency services and medical evacuations.\n- Choices among many expert charter companies.\nFor more information, please visit our page Cruises & Sailing.","Many sailors crossing the Pacific choose not to stop in many of the Tuamotos atolls due to the challenge of navigating the reef passes — the currents can be extremely dangerous, flowing up to 10 knots, occasionally creating standing waves to 2-3 feet.\nPredicting when it is safe to navigate a reef pass can be tricky. Sailors often talk about “timing the slack tide” in Tuamotos passes, but this is somewhat misleading. What is really important is the overall current in the pass– which is caused by a combination of tide, swell and wind.\nThe tide has the most obvious effect on the current. Luckily the tidal variation in French Polynesia is only about 1-2ft, but the atolls are so large that a formidable amount of water still needs to move in and out of the lagoon twice a day.\nSlack tide is normally the calmest time in a reef pass – assuming there is no wind and swell. The wind can seriously disturb the pass when it is moving in the opposite direction to a strong current, resulting in confused standing waves. So we avoid transiting the pass when wind and current are in opposition.\nAn even more significant hazard for the pass, however, are the swell conditions. If there’s a moderate to large swell (2m+) there is a huge amount of water that is poured into the lagoon. Satellite images and nautical charts make it seem like the atolls are a complete ring of land, but the majority of the ring is actually submerged reef with a peppering of motus ( little islands of land in the barrier reef).\nWaves can come from groundswell (far away storms, with large periods) or windswell (localized strong winds with short periods) such as the ‘Maraamu’ South-East trade winds that blow 25knots+. The extra water these swells push into the lagoon must escape through the reef pass. This creates an additional outgoing current of 1-6 knots.\nWhen this swell-driven outgoing current combines with an outflowing tide, a river of water can flow towards the ocean at an incredible 8-10 knots! Even a big ship might have a hard time during such conditions.\nThe best time to enter a pass during such a swell event is usually during the peak of an incoming tide, so that it neutralizes the outgoing current. Then, the outgoing current may be a reasonable 1-2 knots. Therefore, in the case of swell events, slack current is NOT slack tide.\nNote: swell can also create large waves that break next to narrow reef passes making them difficult to impossible – like Maupiti in the western Society Islands. This is an entirely different problem, but still relevant for navigation!\nNaturally, it’s best to travel across reef passes when the swell and wind is moderate, and tide is slack. But we don’t always have that luxury. Being able to factor for the effects of swell and wind is critical for the safe navigation of reef passes in less-than-ideal conditions.\nPHOTO: shows the Tiputa pass in Rangiroa. It is a 40 mile wide atoll with a huge amount of water moving in and out of the pass, generating large standing waves which can be seen on satellite images. This famously attracts dolphins, which divers come to swim with. We entered Tiputa pass after waiting an hour for the tide to shift, and still the outgoing current was nearly 6 knots. However we were blessed with an amazing moment when a huge dolphin jumped directly in front of Aldebaran as we were barely moving forward with the engine at full throttle!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:073b6ce9-7c2b-4a12-9b05-a7bae1d75316>","<urn:uuid:f03ad46a-96b1-4be4-81b7-3ad0e0266080>"],"error":null}
{"question":"What is the role of EMI software in manufacturing, and how does it relate to the challenges utilities face with electric vehicle adoption?","answer":"EMI (Enterprise Manufacturing Intelligence) software plays a critical role in manufacturing by providing a near-real-time picture of performance and allowing all team members to access role-based views of the same information. It helps decision-makers across the organization react to changing conditions and makes metrics available to users from different data sources. Similarly, utilities face challenges with EV adoption that require sophisticated software solutions - they need to manage complex billing with multiple stakeholders, handle various customer types and charging locations, and implement dynamic pricing strategies. Both manufacturing EMI and utility systems aim to bring together different players and data sources to improve operational efficiency and decision-making.","context":["No one discipline dominates the automation landscape any longer. The road to building this team is not at the end, and there have been many bumps along the way. The economic downturn currently buffeting the industry is prodding more companies to look at the need to bring the team together. Many roles comprise the team, including operators, maintenance technicians, engineers, managers, plant management, division management and enterprise management—all playing their important roles.\nAll the players must make correct decisions in real time in order for manufacturing to succeed. To do this, they must have the same information base provided in the way that helps each role make the decision required. It’s much like a perfect transition play in basketball where one player grabs the rebound and sees the outlet player. That player surveys the defense and decides to dribble or pass. The ball gets under the basket, and then that player makes the last pass to the open shooter for a three-point shot. Each player has a role and knows where to go and what decisions to make to help the team win.\nA software application often called enterprise manufacturing intelligence, or EMI, has evolved to provide a near-real-time picture of how manufacturing is performing. Giving all the players a role-based view of the same information, not surprisingly, is, in the words of Doug Lawson, vice president of Incuity Software, a Rockwell Automation company, “no longer a nice-to-have application, but is now a must-have in the minds of our customers.” Or, as Greg Gorbach, vice president of collaborative manufacturing for analyst firm ARC Advisory Group Inc., in Dedham, Mass., put it, “EMI started with the pioneers and visionaries. It has since moved into the mainstream. A few years ago, there was much distrust about technology among executives. But in today’s economy and with growing understanding of technologies such as the Web, portals and integration, it’s more accepted.”\nSimon Jacobson, research director at analyst firm AMR Research Inc., in Boston, says, “It’s about how the whole facility operates. Intelligence looks back at plant characteristics and makes them visible. For example, if a valve died, it means one thing to the operator who must get the process continuing or back up. It means something else to executives such as supply chain managers who need to plan if the batch will be late or ruined. People are using intelligence tools today to help achieve multi-plant efficiencies.”\nAccording to Maryanne Steidinger, MES/EMI marketing program manager at Wonderware, a Lake Forest, Calif., software supplier, many companies already have manufacturing execution systems (MES) and historians, but accessing all the information these applications contain may not be easy. Therefore, they need a manufacturing intelligence application to sit on top and extract that information. Adds Christian-Marc Pouyez, Wonderware product marketing manager, “The key area is around metrics and performance. EMI gives the ability to measure performance, building from different data sources, and make the metrics available to users.”\n“Decision makers are scattered around an organization, but all need to react to changing conditions,” adds Incuity’s Lawson, “so the ability to create actionable information from disparate data dynamically for decision support is key. Executives use it to make internal and external supply chain decisions, such as where to manufacture a sudden large order or how to juggle operations if a major order is cancelled. On the other end of the spectrum, there is a plant I know where they determined three metrics for production-line performance and put large monitors up at strategic locations to display real-time performance vs. goal for everyone to see.”\nEMI brings the entire automation team together to focus on the important goal of making manufacturing efficient, effective and profitable.\nGary Mintchell, email@example.com, is Automation World’s Editor in Chief.","Utilities in the age of electric vehicles\nUtilities in the age of electric vehicles\nUtilities in the age of electric vehicles\nThe impact of vehicles on the environment has driven regulatory mandates to adopt a more sustainable way of commuting. As a result, electric vehicles (EVs), and the necessary infrastructure to operate them, has changed the automobile and utility industries over the past decade.\nElectric vehicles are powered by a charged battery pack and can be separated into two categories:\n- Battery Electric Vehicles (BEVs): These EVs are purely electric with lithium ion batteries suitable for short to medium distances.\n- Plug-In Hybrid Electric Vehicles (PHEVs): Electric vehicles with an internal combustion engine (ICE) with support from a small electric motor.\nWhy Electric Vehicles?\nThe 2015 Paris Agreement has challenged countries to reduce their carbon emissions to “net zero” over the coming years. This international treaty has prompted governments around the world to phase out gas and diesel powered vehicles, shifting instead to EVs:\nSales of electric vehicles have grown steadily over the last decade. The following chart from the International Energy Agency shows China leading market share at 47%. Twenty other countries have reached a market share of above 1%: emissions. As a segment, the automobile industry can tout sustainability and the environmental benefits of emerging technologies to entice consumers to buy EVs.\nAccording to a study by IRENA (International Renewable Energy Agency) on EVs:\n- Electric passenger cars will reach 200 million by 2030\n- Electric two-wheeled and three-wheeled vehicles could outnumber four-wheeled vehicles, with as many as 900 million on roads by 2030\n- Electric buses and light-duty vehicles could surpass 10 million by 2030\nFactors Contributing to EV Adoption\n1. Consumer Interest:\nEco-friendly consumers who want to decrease their carbon footprint prefer to buy EVs. Transportation around the globe is one of the biggest contributors of carbon emissions. As a segment, the automobile industry can tout sustainability and the environmental benefits of emerging technologies to entice consumers to buy EVs. A 2019 international electric vehicle consumer survey (of 7,600 consumers in seven regions) shows consumer interest in electric vehicles is high. 50% of consumers say they’re interested in owning an EV and 28% say they’ll purchase one as their next vehicle.\nConsumer benefits to owning electric vehicles:\n- Reduced operating costs, lower charging prices and simpler maintenance\n- Quieter driving experience\n- Exemption in Clean Air Zones – areas that charge fees to vehicles that pollute the environment\n- Government subsidies that make EVs cheaper than ICE vehicles\n- Preferential parking permits in dense urban areas\nEV technology has vastly improved. Range limitations and charging times have been addressed, alleviating concerns and increasing purchase momentum. Consider these three, top selling EV models in the world in 2020:\n3. NetZero Target:\nIn support of the 2015 Paris Agreement, utility and automobile companies are working to achieve net-zero emissions. To do this, they’re offering customers low-carbon products such as renewable electricity and electric vehicles. By taking advantage of these offerings, individual consumers can reduce their carbon footprint. Businesses can reduce their overall cost of fleet ownership, and organisations can reduce fuel costs, reap tax benefits and take advantage of government incentives.\nChallenges for Utilities:\nEVs help combat climate change. However, barriers to adoption exist:\nCharging Pricing: An increase in the number of electric vehicles can lead to disorganised charging. This makes peak shaving difficult, creates incremental costs for generators, increases transmission and distribution pressures and reduces grid reliability and security. It also degrades power quality and increases the harmonics of the grid. Ultimately, an unreasonable pricing structure can lead to its failure. A dynamic pricing strategy can help utilities overcome the challenges of EVs and can reduce the burden of power on a grid.\nComplex Billing: EVs also present billing challenges for utilities:\n- Number of Stakeholders: Charging hosts, charging point operators, eMobility service providers, roaming network providers, etc. are all involved in the billing process. These stakeholders have to manage multiple plans – pre-paid, postpaid, ad hoc, group plans, etc.\n- Customer Type and Charging Location: Plans offered will vary based on customer type such as individual, fleet, business, public and private. They’ll also vary based on location, including home, office, fleet charging center, parking lot, multi-tenant unit, municipal location and more.\n- Price Per Charge: The customer can be charged based on charge point, price per kWh or by minute/ hour (flat fee). The charging session may include ancillary fees such as a connection fee or a waiting fee for staying connected after reaching a full charge.\nCharging Infrastructure: The mechanics of charging pose challenges to utility companies:\n- Network: Availability remains limited\n- Technology: Fast-charge still in its initial stage and widely unavailable in the network\n- Customer Experience: Unpredictable charging experience negatively affects customer opinion\nService and Maintenance: Electric vehicles require specialised mechanics who are still difficult to find. According to a study done by UK’s Institute of the Motor Industry (IMI), 97% of today’s mechanics aren’t qualified to work on electric vehicles. Of the 3% of mechanics who do qualify, many work directly for EV dealerships, limiting service options for general EV buyers.\nHigher Upfront Investment: Higher manufacturing costs vs. the cost to make a combustion engine vehicle make EVs more expensive to buy. This sticker shock feeds consumer doubt about the long-term economic benefits of an electric vehicle. Government subsidisations help alleviate that doubt, but total consumer buy-in will take time.\nThe Utility Opportunity\nAs more people switch to electric cars, the impact of EV charging loads on generation, transmission and distribution networks translates into more energy and more revenue opportunities for utility companies.\n- Charging Infrastructure: Utilities can play a vital role in modulating charging rates and shifting charging times to provide grid services that support supply and demand. Consider these energy giants already investing in charging infrastructure:\n- Shell recently announced the rollout of 500,000 electric charging stations over the next four years.1\n- Ecotricity a “Big Six” UK energy supplier, partners with Moto, RoadChef and Welcome Break to offer 45-minute fast-charge stations. They call the network “The Electric Highway.”2\n- In the UK, companies like Centrica are building out their EV charging capabilities by acquiring smaller independents. Centrica invested in Driivz, a software company that manages EV fleets and charging networks, to create Centrica Electric Vehicle Services (CEVS) 3\n- New EV Tariffs: Consumer tariff structures (e.g. time-of-use tariffs) reward consumers who slow-charge during off-peak hours. These tariffs, which reduce consumer bills and prevent overloads on the grid, help influence EV drivers to shift their charging behavior. By partnering with EV manufacturers, utility companies can create custom electricity tariffs that can be bundled into the purchase of an electric vehicles. Energy suppliers in the US, UK and other European countries have already begun offering EV energy tariffs.\n- Improved Customer Experience: Careful planning, phased execution and synergy with non-utility businesses can help electric utilities facilitate a smooth transition to EV adoption. With the right customer relationship management (CRM) platform in place, utilities can offer consumers “charging ecosystems” – chargers, charging plans, etc. - for their vehicle. This positive consumer experience, combined with the financial upside of aligning with non-utility companies, translates into increased revenue for the utility company.\n- Vehicle-to-Grid (V2G): While EV tariffs can prevent overloads by shifting charging behavior, they also present challenges. If too many EV drivers charge during off-peak times, it can spike load levels and lead to grid congestion. To counter this, vehicle-to-grid (V2G) enables energy to be pushed back to the power grid from the battery of an electric vehicle. This helps balance the variations in energy production and consumption. Furthermore, V2G can support the integration of renewable energy resources into the grid.\n- AI Driven EV Marketing: With electric vehicle purchases on the rise, utilities must position themselves at the forefront of energy innovation to ensure brand credibility. AI-driven marketing helps identify crucial digital touchpoints for targeted messaging.\n- Data Advantages: Utilities can use data analytics and data science tools to develop services, applications and hyper-personalised product offerings. They can also leverage the data to expand into non-core markets. This allows for:\n- Joint offerings with automotive companies\n- After sales services in conjunction with car dealerships\n- Installation of charging stations at locations where customers park electric vehicles for more than an hour\nGlobally the uptake of electric vehicles is leading to the transportation and electricity sectors becoming increasingly connected.\nEven though barriers to EV adoption exist they are phasing out due to technological advancements. Innovation is the key in identifying opportunities to minimise costs and reducing pain points.\nFor utilities the biggest challenges is to ensure grid reliability and resilience. Providing a successful infrastructure for EV adoption will require coordination among various parties - Vehicle & charging manufactures, Electricity service providers, Distribution network operators, and Regulatory authorities\nIn the long run - Utilities that invest in electric vehicle infrastructure and technology will be well-equipped to offer solutions that benefit future customers.\n2Ecotricity and Nissan install UK electric-car-charging network | Guardian sustainable business | The Guardian\nEXL Utilities Academy"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:31db4bba-3b06-4a2a-b6ba-4aa4acabf0dc>","<urn:uuid:dc7f0a0b-46cd-416c-b618-7a830371a8fd>"],"error":null}
{"question":"Which injury causes more severe mobility limitations: a complete Achilles tendon rupture or severe shin splints?","answer":"A complete Achilles tendon rupture causes more severe mobility limitations. An Achilles tendon rupture typically requires surgery, followed by wearing a plaster or walking boot for at least six weeks, with total rehabilitation taking six months or more. In contrast, shin splints, while painful, can be treated with more conservative measures like ice, stretching, and anti-inflammatory medications, and don't typically require such extensive treatment or recovery time.","context":["Achilles Tendonitis is a term that commonly refers to an inflammation of the Achilles tendon or its covering. It is an overuse injury that is common especially to joggers and jumpers, due to the repetitive action and so may occur in other activities that requires the same repetitive action.\nMost experts now use the term Achilles tendinopathy to include both inflammation and micro-tears. But many doctors may still use the term tendonitis out of habit.\nWhat Causes Tendon Pain?\nTendons are the tough fibres that connect muscle to bone. Most tendon injuries occur near joints, such as the shoulder, elbow, knee, and ankle. A tendon injury may seem to happen suddenly, but usually it is the result of many tiny tears to the tendon that have happened over time.\nHealth professionals may use different terms to describe a tendon injury. You may hear:\nTendonitis (or Tendinitis): This actually means \"inflammation of the tendon,\" but inflammation is rarely the cause of your tendon pain.\nTendinosis: This refers to tiny tears in the tissue in and around the tendon caused by overuse.\nWhat Causes Achilles Tendonitis?\nAchilles tendonitis is an overuse injury that is common especially to joggers and jumpers, due to the repetitive action and so may occur in other activities that requires the same repetitive action.\nMost tendon injuries are the result of gradual wear and tear to the tendon from overuse or ageing. Anyone can have a tendon injury, but people who make the same motions over and over in their jobs, sports, or daily activities are more likely to damage a tendon.\nA tendon injury can happen suddenly or little by little. You are more likely to have a sudden injury if the tendon has been weakened over time.\nCommon Causes of Achilles Tendonitis include:\n- Over-training or unaccustomed use – “too much too soon”\n- Sudden change in training surface – e.g. grass to bitumen\n- Flat (over-pronated) feet\n- High foot arch with tight Achilles tendon\n- Tight hamstring (back of thigh) and calf muscles\n- Toe walking (or constantly wearing high heels)\n- Poorly supportive footwear\n- Hill running.\n- Poor eccentric strength\nWhat are the Symptoms of Achilles Tendonitis?\nAchilles tendonitis may be felt as a burning pain at the beginning of activity, which gets less during activity and then worsens following activity. The tendon may feel stiff first thing in the morning or at the beginning of exercise.\n- Achilles tendonitis usually causes pain, stiffness, and loss of strength in the affected area.\n- The pain may get worse when you use your Achilles tendon.\n- You may have more pain and stiffness during the night or when you get up in the morning.\n- The area may be tender, red, warm, or swollen if there is inflammation.\n- You may notice a crunchy sound or feeling when you use the tendon.\nHow is Achilles Tendonitis Diagnosed?\nYour physiotherapist or sports doctor can usually confirm the diagnosis of Achilles tendonitis in the clinic. They will base their diagnosis on your history, symptom behaviour and clinical tests.\nAchilles tendons will often have a painful and prominent lump within the tendon.\nFurther investigations include US scan or MRI. X-rays are of little use in the diagnosis.\nWhat are the Aims of Achilles Tendonitis Treatment?\nAchilles tendonitis is one of the most common problems that we see at PhysioWorks and it is unfortunately an injury that often recurs if you return to sport too quickly – especially if a thorough rehabilitation program is not completed.\nYour calf muscle is a large powerful group of muscles that can produce sufficient force to run, jump and hop. Your achilles tendon attaches your calf muscle to your heel bone. It is a tendon or non-contractile soft tissue structure, which does have a different level of blood supply and function, which does alter the rehabilitation from a calf tear.\nResearchers have concluded that there are essentially 7 stages that need to be covered to effectively rehabilitate these injuries and prevent recurrence.\nPhase 1 - Early Injury Protection: Pain Reduction & Anti-inflammatory Phase\nAs with most soft tissue injuries the initial treatment is RICE - Rest, Ice, Compression and Elevation.\nIn the early phase you’ll be unable to walk without a limp, so your Achilles tendon needs some active rest from weight-bearing loads. You may need to be non or partial-weight-bearing, utilise crutches, a wedged achilles walking boot or heel wedges to temporarily relieve some of the pressure on the Achilles tendon. Your physiotherapist will advise you on what they feel is best for you.\nIce is a simple and effective modality to reduce your pain and swelling. Please apply for 20-30 minutes each 2 to 4 hours during the initial phase or when you notice that your injury is warm or hot. Anti-inflammatory medication (if tolerated) and natural substances eg arnica may help reduce your pain and swelling. However, it is best to avoid anti-inflammatory drugs during the initial 48 to 72 hours when they may encourage additional bleeding. Most people can tolerate paracetamol as a pain reducing medication.\nAs you improve a kinesio style supportive taping will help to both support the injured soft tissue.\nPhase 2: Regain Full Range of Motion\nIf you protect your injured Achilles tendon appropriately the torn tendon fibre will successfully reattach. Mature scar formation takes at least six weeks. During this time period you should be aiming to optimally remould your scar tissue to prevent a poorly formed scar that will re-tear in the future.\nIt is important to lengthen and orientate your healing scar tissue via massage, muscle stretches, neurodynamic mobilisations and eccentric exercises. Signs that your have full soft tissue extensibility includes being able to walk without a limp and able to perform Achilles tendon stretches with a similar end of range stretch feeling.\nPhase 3: Restore Eccentric Muscle Strength\nCalf muscles work in two directions. They push you up (concentric) and control you down (eccentric). Most Achilles injuries occur during the controlled lengthening (eccentric) phase. Your physiotherapist will guide you on an eccentric calf strengthening program when your injury healing allows.\nPhase 4: Restore Concentric Muscle Strength\nCalf strength and power should be gradually progressed from non-weight bear to partial and then full weight bear and resistance loaded exercises. You may also require strengthening for other leg, gluteal and lower core muscles depending on your assessment findings. Your physiotherapist will guide you.\nPhase 5: Normalise Foot Biomechanics\nAchilles tendon injuries can occur from poor foot biomechanics eg flat foot. In order to prevent a recurrence, your foot will be assessed. In some instances you may require a foot orthotic (shoe insert) or you may be a candidate for the Active Foot Posture Stabilisation program.\nPhase 6: Restore High Speed, Power, Proprioception & Agility\nMost Achilles tendon injuries occur during high speed activities, which place enormous forces on your body (contractile and non-contractile). In order to prevent a recurrence as you return to sport, your physiotherapist will guide you with exercises to address these important components of rehabilitation to both prevent a recurrence and improve your sporting performance.\nDepending on what your sport or lifestyle entails, a speed, agility, proprioception and power program will be customised to prepares you for light sport-specific training.\nPhase 7: Return to Sport\nDepending on the demands of your chosen sport, you will require specific sport-specific exercises and a progressed training regime to enable a safe and injury-free return to your chosen sport.\nYour PhysioWorks physiotherapist will discuss your goals, time frames and training schedules with you to optimise you for a complete return to sport. The perfect outcome will have you performing at full speed, power, agility and function with the added knowledge that a through rehabilitation program has minimised your chance of future injury.\nWhat Results Should You Expect?\nThere is no specific time frame for when to progress from each stage to the next. Your Achilles tendonitis rehabilitation status will be determined by many factors during your physiotherapist’s clinical assessment.\nYou’ll find that in most cases, your physiotherapist will seamlessly progress between the rehabilitation phases as your clinical assessment and function improves.\nIt is also important to note that each progression must be carefully monitored as attempting to progress too soon to the next level can lead to re-injury and frustration.\nThe severity of your tendon injury, your compliance with treatment and the workload that you need to return to will ultimately determine how long your injury takes to successfully rehabilitate.\nCan Your Achilles Tendon Rupture?\nThe worst case scenario is a total rupture of your Achilles tendon. Treatment in this case usually requires surgery, plaster or a walking boot for at least six weeks. Most of these injuries take six months or more to adequately rehabilitate.\nThe best advice is to seek early advice to do all you can to avoid this nasty rupture happening in the first place.\nTreatment in this case usually requires surgery, plaster or a walking boot for at least six weeks. Most of these injuries take six months or more to adequately rehabilitate.\nThe best advice is to seek early advice from your physiotherapist to do all you can to avoid this nasty rupture happening in the first place.","Top 10: Common Sports Injuries\nShin splints refer to pain on the inner side of the shinbone caused by inflammation of the muscles that surround it. They often affect people who aren't used to exercising; they can be caused by increasing the intensity of your workout too fast, wearing worn-out shoes or by jumping or running on hard ground.\nPrevention and treatment: Wearing good shoes, cross training, stretching, and not increasing workout intensity too quickly are the best preventive measures. As for treatment, ice, stretching and anti-inflammatory medications are your best bets.\nLower back pain\nAlthough lower-back pain is much less common among athletes than among sedentary and overweight people, it can affect runners, cyclists, golfers, tennis, and baseball players. While there are many types of lower-back pain — bulging discs, back spasms, and pain reaching down the leg from the lower back, known as sciatica — the most common reason for sports-related back pain is simply improper stretching. In the case of runners, having even the slightest discrepancy in leg length can cause back pain.\nPrevention and treatment: Although some lower-back injuries cannot be prevented, warming up properly before exercising will greatly reduce your risk of injury. While bulging discs and sciatica require fast medical treatment, you can treat a simple muscle pull or back spasm yourself with RICE, anti-inflammatory medication and stretching. Runners with a difference in leg length can get orthotic lifts from a podiatrist to correct the problem.\nNot warming up properly, fatigue, lack of flexibility, and weakness can cause all types of athletes to pull a muscle. The most commonly pulled muscles are hamstrings (especially in sports involving running, such as jogging, basketball and soccer) and calves (particularly in older tennis players). The hamstrings are the muscles behind your thighs; pulling them is painful and can even cause bruising. While these are the most common, you can pull many different muscles depending on the sport you are performing.\nPrevention and treatment: The best way to prevent pulling a muscle is to stretch properly before and after exercising, and avoid working out when you are fatigued and weak. As with most injuries, RICE and anti-inflammatory drugs are helpful, as well as gentle stretches. When the injury has begun to heal, you can begin exercising again, but stop every so often during your workout to stretch until you are completely healed.\nTennis or golf elbow\nElbow injuries account for 7% of all sports injuries. Tennis elbow consists of tendon degeneration in the elbow due to repeated backhand strokes in tennis. It causes pain on the outside of the elbow. Golf elbow, on the other hand, usually affects the inside of the elbow, although it can sometimes attack the outside. The pain experienced is a result of an inflammation of the epicondyle, the area on the inside of the elbow where the forearm-flexing muscles attach to the upper arm.\nPrevention and treatment: The best way to prevent these ailments is to perform forearm-strengthening exercises, such as wrist curls, reverse wrist curls and squeezing a soft rubber ball. Also, improving your swing technique and wearing an elbow brace can be very helpful. Treatment can be as simple as RICE and anti-inflammatory medications, but in some cases physiotherapy and a prolonged break from the sport may be necessary.\nRELATED: 5 Effective Arm Workouts\nWatch out for your ankles, shoulders and the most vulnerable part of your body..."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:b94d5d77-68cf-4ae5-bf6e-84537cfce74d>","<urn:uuid:8a469e93-a039-4a79-b12c-f7ac30304207>"],"error":null}
{"question":"I'm researching deadly infectious diseases. Could someone explain the relationship between malaria's protein mechanisms (like heat shock proteins) and its ability to cause widespread mortality in Africa?","answer":"Heat shock proteins 70 (Hsp70) and heat shock proteins 40 (Hsp40) are involved in protein folding, stabilization, and membrane translocation in malaria parasites. These proteins, particularly PfHsp70-x and its co-chaperones, play a crucial role in trafficking and folding of exported proteins including malaria virulence factors. This molecular mechanism contributes to malaria's severity, which makes it one of the deadliest diseases in Africa, causing a million cases of disease annually and being one of the leading causes of death in the region.","context":["may be the protozoan parasite in charge of one of the most virulent type of individual malaria . . Heat shock proteins 70 (Hsp70) chaperone and its own co-chaperone heat surprise proteins 40 (Hsp40) get excited about facilitating proteins folding stabilization degradation and translocation across membranes [12 13 Hsp70s bind to brief hydrophobic parts of unfolded substrate protein within an ATP-controlled way that is controlled by Hsp40 co-chaperones . Hsp40s are seen as a the current presence of the J area necessary for the 17-AAG (KOS953) excitement from the ATPase activity of Hsp70s . Based on their domains the Hsp40s have already been categorized into types I-IV [16 17 with types I and II with the capacity of binding to unfolded substrate protein for concentrating on to partner Hsp70s [18 19 You can find six Hsp70s (PfHsp70s) five which are parasite-resident with PfHsp70-1 getting one of the most well characterized [20-28]. PfHsp70-x 17-AAG (KOS953) may be the just Hsp70 within the parasitophorous vacuole (PV) as well as the contaminated erythrocyte cytosol [29 30 The web host cell cytosol includes residual individual Hsp70  therefore it is luring to take a position that PfHsp70-x may raise the chaperone power of ANGPT4 the compartment to assist proper folding from the huge exportome. PfHsp70-x provides been proven to co-localize with two exported type II Hsp40s PFE0055c and PFA0660w in buildings known as J-dots in the contaminated erythrocyte cytosol. Furthermore the J-dots affiliate with erythrocytes membrane proteins 1 (PfEMP1) the main malaria virulence aspect [29 32 It’s been suggested that PfHsp70-x/PFE0055c/PFA0660w play a significant function in the trafficking and folding of exported protein including malaria pathogenesis elements . Small-molecule inhibitor research  and modelling  have already been conducted in PfHsp70-x homology. Nevertheless the biochemical information on its relationship with exported plasmodial Hsp40s (PfHsp40s) stay to become elucidated. Attempts to secure a practical appearance vector The optimized coding series for appearance of PFA0660w in was synthesized and given by GenScript(R) (USA). The PFA0660w coding area was inserted in to the pQE30 appearance vector (Qiagen Germany) to make a plasmid encoding the (His)6-PFA0660w (PFA0660w) proteins. Heterologous appearance and purification of PFA0660w PFA0660w was over-expressed and purified using the M15[pREP4] stress (Qiagen Germany). Proteins appearance was induced by addition of 0.4 mM IPTG (isopropyl-β-D-thiogalactopyranoside). Bacterias cells expressing PFA0660w had been gathered by centrifugation as well as the cell pellet resuspended in lysis buffer (LB: 10 mM Tris-HCl 17-AAG (KOS953) pH 8.5 300 mM NaCl 50 mM imidazole 1 mM PMSF 1 mg/ml lysozyme) permitted to are a symbol of 20 min at room temperature and frozen at -80°C overnight. Pursuing thawing and sonication at 4°C the insoluble pellet was cleaned 3 x in clean buffer (WB: 50 mM Tris-HCl pH 8.5 200 mM NaCl 10 mM EDTA 1 Triton X-100 1 mM PMSF) and twice in twin distilled water as previously referred to [40 41 The pellet was retrieved after every wash by centrifugation (10000 × at 4°C for 10 min). The pellet was after that resuspended in solubilising buffer (SB: 100 mM Tris-HCl pH 8.5 300 mM NaCl 8 M urea 50 mM imidazole 5 mM DTT 0.1 mM EDTA 1 mM PMSF) and clarified by centrifugation at 16000 × for 30 min at 4°C. To make sure correct refolding the solubilised proteins was diluted to your final focus of 250 μg/ml with refolding buffer (RB: 100 mM Tris-HCl pH 8.5 300 mM NaCl 50 mM imidazole 10 glycerol 5 sucrose 1 mM DTT 0.1 mM EDTA 0.1% PEG 2000 1 mM PMSF) supplemented with 2 M urea and incubated with gentle stirring at 4°C for 2 h. The supernatant was filtered through 0.45 μm filters and loaded onto a 5 ml HisTrap HP column (GE Healthcare UK). The column was cleaned with five column amounts of RB accompanied by five column amounts of RB without PEG 2000. Proteins was eluted with three column amounts of elution buffer (EB: 100 mM Tris-HCl pH 8.5 300 mM NaCl 0.5 M imidazole 10 glycerol 5 sucrose 1 mM DTT 0.1 mM EDTA 1 mM PMSF). Mouse monoclonal anti-His antibody (1:5000; GE Health care UK) mouse monoclonal anti-DnaK antibody (1:5000; Sigma-Aldrich Germany) and anti-mouse HRP-conjugated supplementary antibody (1:10000; GE Health care UK) were 17-AAG (KOS953) utilized to confirm the current presence of the target proteins and eliminate the current 17-AAG (KOS953) presence of contaminating DnaK (Hsp70). The.","Malaria comprehensive overview covers symptoms, causes, treatment, prevention of this infectious disease. Unicef is committed to doing all it can to achieve the sustainable development goals it causes severe attacks of diarrhoea that malaria malaria is a serious disease caused by a parasite carried by certain types of mosquitoes. Africa check has published an updated factsheet on the leading causes of death in africa following the lower respiratory tract infections, tuberculosis, diarrhoeal disease and malaria drawn from the who's global health estimates summary tables for cause of death in 2000. Malaria is a mosquito-borne infectious disease affecting humans and other animals caused by parasitic protozoans (a group of single-celled microorganisms) belonging to the plasmodium type malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches in. Malaria is a serious disease that causes high fever and chills what causes malaria malaria - topic overview articles on malaria malaria malaria topic overview cause symptoms these types can be deadly how is malaria diagnosed.\nHiv (human immunodeficiency virus) is a virus that attacks the immune system, the body's natural defense system without a strong immune system, the body has trouble fighting off disease both the virus and the infection it causes are called hiv white blood cells are an important part of the immune. 145 protists, fungi, and human disease lesson objectives malaria disease caused by plasmodium protozoa and transmitted by mosquitoes in tropical and subtropical but they can be deadly both are important causes of disease and death in other living things including humans protists. Protista classification the kingdom protista (in the five kingdom system) unique characteristics: it causes vaginitis by infecting the vagina and urethral species is plasmodium sp which causes malaria the host species of this disease is the anopheles. Deadly disease essay examples 3 pages an overview of the characteristics and causes of malaria, a deadly disease 476 words 1 page an overview of the causes and characteristics of cancer, a deadly disease 1,603 words 4 pages.\nParasite study paves way for therapies to tackle deadly infections date: october 10, 2017 source: university of edinburgh summary: new understanding of a parasite that causes a million cases of disease each year could point towards effective drug treatments. Mosquitoes are considered one of the most dangerous creatures on the planet because of their ability to spread deadly diseases the us centers for disease control report that the insects kill more than one million people a year just through the transmission of malaria. Malaria is a mosquito-borne disease that can cause infected people to become very sick with high fever modified experimental vaccine protects monkeys from deadly malaria , may 22, 2017 world malaria day 2017, april 25 overview of r01 process understand due dates. Two diseases - malaria and hiv/aids - have the highest impact on health in africa and this gives some background to what causes them the disease is caught when people go into water where the larvae of the worms have been released by freshwater snails. Malaria is a life-threatening disease caused by parasites that are transmitted to people through the bites of infected female mosquitoes it does reduce the risk that malaria infection will cause severe disease for this reason, most malaria deaths in africa occur in young children.\nCivil war medicine: an overview of medicine doctors had yet to develop bacteriology and were generally ignorant of the causes of disease generally the civil war soldier also faced outbreaks of measles, small pox, malaria, pneumonia, or camp itch. Malaria victims: how environmentalist ban on ddt caused 50 million deaths: this is a story of triumph and tragedy the triumph occurred in the middle part of the 20th century, when the larger part of mankind finally succeeded in overcoming the ravages of malaria, the deadly infectious disease. Malaria is a deadly disease - malaria is a deadly disease malaria is a disease and deadly is most common in africa region and it can cause deadly form of disease current status of malaria vaccinology one must first take an overview of the whole of the whole disease. Outbreak alerts from space and vegetation set the stage for a population surge in disease-carrying pests the single-celled plasmodium parasite that actually causes malaria operates too slowly to go through its infection cycle before the host mosquito dies.\nJoseph derisi gives an overview of malaria, the disease, and biology of the disease causing parasite plasmodium falciparum.\nIs sickle cell anemia lethal follow 6 answers 6 sickle cell anemia is a really cool disease because people who have only one gene for it have what is called sickle cell trait but not for the paracite that causes malaria. Malaria: an infectious disease caused by protozoan parasites from the plasmodium family that can be transmitted by the bite of the anopheles mosquito or by a contaminated needle or transfusion falciparum malaria is the most deadly type the symptoms of malaria include causes of a heart. Epidemiology of infectious disease: general principles characteristics of the organism usually are not known the cause of lyme disease, table 2-1 clinical classification of infections classification infection diarrheal diseases secretory invasive respiratory diseases upper respiratory. Summary overview: cerebral malaria types of cerebral malaria symptoms causes of cerebral malaria cerebral malaria as a complication caused by other conditions disease topics related to cerebral malaria research the causes of these diseases that are similar to. There is no malaria virus that causes the disease plasmodium falciparum, plasmodium ovale, plasmodium vivax, and plasmodium malariae malaria virus: a summary instead of a malaria virus, the cause of malaria is an infection with a parasite.\nMalaria is a disease caused by a parasite transmitted by mosquitoes malaria is a major cause of death worldwide the most deadly type occurs in africa south of the sahara desert. This work is licensed under a creative commons attribution-noncommercial-sharealike license cuneiform tablets mention deadly malaria-like malarial anemia are major causes of mortality."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:5c1e31ac-3ee4-47ac-9401-64f6fd51d144>","<urn:uuid:b6e6a215-a0e4-4145-a0f5-37a599279b24>"],"error":null}
{"question":"How do display rules for emotions compare to the 'as-if' body loop concept - are they both ways that humans can simulate or modify emotional responses?","answer":"While both concepts relate to modifying emotional responses, they work differently. The 'as-if' body loop allows hypothetical body states to be mentally represented and considered internally. In contrast, display rules are cultural conventions that govern how emotions should be outwardly expressed in public, like how Japanese participants showed more positive emotions than Americans when watching disturbing films in the presence of others, despite having similar private reactions. Display rules are often followed unconsciously and vary across cultures, while the 'as-if' loop is an internal simulation mechanism.","context":["As an aside, he proposes at least four levels of self, from least complex to most complex:\n1. Neural Self (or proto-self) - a short term collection of neural patterns of activity which represent the current state of the organismIn Damasio's view, one which I share, emotions are body states that then are interpreted by the brain to assign a label based on memory and previous learning. The classic example of this was demonstrated years before Damasio wrote his book. In the study, (Schachter & Singer, 1962) researchers gave epinephrine to subjects (telling them it was a vitamin), half of which were told what to expect and half were told nothing or given false information. All subjects were left with a confederate who either acted euphoric or angry. Those told what to expect attributed their arrousal to the injection. Those who were given no information or false information labeled their own experience in line with the behavior of the confederate, not having any other information on which to base their feelings. The researches suggest that emotion is based on arousal + cognition, on the assumption that most emotions share similar body-states.\n2. Core Self - a second-order entity which maps the state of the proto-self in rather the same way the proto-self maps the current state of the body: whenever an encounter with an object impinges on the proto-self, the change is registered by activity in the core self\n3. Autobiographical Self - draws on permanent (though modifiable) memories instead of just the immediate experiences which power the core self. At this point, there is a real, though still pre-linguistic, sense of self. Damasio thinks chimpanzees and probably dogs enjoy this level of consciousness\n4. Reflective Self - greater use of longer-term memory, delivers the kind of foresighted, reflective consciousness which we typically associate with human beings\nIn general, then, the autobiographical self, or narrative self, creates a story to explain body states based on either environmental cues or previous experience.\nTo further complicate matters, Damasio also posits an \"as-if\" body loop, which allows hypothetical states of the body to be represented and considered - that seems to imply an 'as if' self.\nHere is the rather complicated way Damasio describes the attribution of emotion:\nIt may sound strange, at first, that feelings of emotion - which are steeped in the representations of body states, only come to be known after other representations of body state have been integrated to give rise to a proto-self. And it sounds strange, for certain, that the means to know a feeling is another feeling. The situation becomes understandable, however, when we realize that the proto-self, feelings of emotion, and the feeling of knowing feelings emerged at different points of evolution and to this day emerge at different stages of individual development. Proto-self precedes basic feeling and both precede the feeling of knowing that constitutes core consciousness. (The Feeling of What Happens, pg. 280-281)One more quote that supports the study above:\nThe collection of neural patterns which constitute the substrate of a feeling arise in two classes of biological changes: changes related to body state and changes related to cognitive state. (pg. 281)Emotion = body state (arousal) + cognition.\nAll of which is to set up this cool lecture from Damasio - The Katz Lecture: Emotion, Feeling, and Social Behavior: The Brain Perspective (2003).\nUniversity of Iowa\nEmotion, Feeling, and Social Behavior: The Brain Perspective“Neither anguish nor the elation that love or art can bring about are devalued by understanding some of the myriad biological processes that make them what they are… Our sense of wonder should increase before the intricate mechanisms that make such magic possible.”\nAntonio Damasio’s trilogy, Descartes’ Error: Emotion, Reason, and the Human Brain (1994), The Feeling of What Happens (1999) and Looking for Spinoza (2003), inspired the theme for the 2003 UW Summer Arts Festival, Spheres. Delving into activity in the anterior portion of the cerebral hemi\"spheres,\" Damasio’s research “… brings us closer to understanding the delicate interaction between affect, consciousness, and memory - the processes that both keep us alive and make life worth living” (Harcourt Books).","The Many Facets of Emotion\nWe experience emotions such as happiness, fear, sadness, pride, and anger when we consider our situation (either real or imagined) to be relevant to our active personal goals (Scherer, Schorr, & Johnstone, 2001). Some goals that make a situation meaning-ful are of long-term concern, such as wanting to be liked. Other goals may be more fleeting, such as hoping to get the last slice of cake, or rooting for the underdog in a football match.\nWhatever the goal may be, once we’ve evaluated a situation as being personally rele-vant, three types of changes are evident that, taken together, characterize emotion. These changes affect our behavior (how we act), our subjective experience (how we feel), and our physiology (how various systems in the body are functioning) (Mauss, Levenson, McCarter, Wilhelm, & Gross, 2005). We can identify similar changes in the states we call moods, but psychologists distinguish emotions from moods in several ways. For one, emotions typically have a clear object or target (e.g., we are happy about something, or mad at someone); moods do not. Emotions are also usually briefer than moods, lasting seconds or minutes rather than hours or days.\nSome of our bodily responses to emotion are quite general, such as a broad pattern of approaching with interest in response to emotionally positive stimuli, or a general withdrawal in response to emotionally negative stimuli. Perhaps the most prominent behaviors associated with emotion, however, are our facial behaviors—our smiles, frowns, laughs, gapes, grimaces, and snarls.\nCharles Darwin hypothesized that our facial expressions of emotion are actually vestiges of our ancestors’ basic adaptive patterns (1872b). He argued, for example, that our “anger” face, often expressed by lowered brows, widened eyes, and open mouth with exposed teeth, reflects the facial movements our ancestors would have made when biting an opponent. Similarly, our “disgust” face, often manifested as a wrinkled nose and protruded lower lip and tongue, reflects the way our ancestors responded to foul odors or spit out foods. (For elaborations, see Ekman, 1980, 1984; Izard, 1977; Tomkins, 1963.)\nIn support of this position, Darwin noted that our facial expressions resemble many of the displays made by monkeys and apes. Darwin also believed that the expressions would be identical among humans worldwide, even “those who have associated but little with Europeans”. This point, too, can be confirmed—for example, in observations of children born blind, who nonetheless express emotions using the typical, recog-nizable set of facial expressions despite the fact that they could not have learned these expressions through imitation (see, for example, Eibl-Eibesfeldt, 1970; Galati, Scherer, & Ricci-Bitti, 1997; Goodenough, 1932).\nA different test of this universality claim involves comparisons between cultures (Russell, 1994; Tracy & Robins, 2008), but only a tiny number of studies have used the participants most crucial for this test: members of relatively isolated non-Western cul-tures (Ekman, 1973; Ekman & Oster, 1979; Fridlund, Ekman, & Oster, 1983; Izard, 1971). Why is this group crucial? If research participants, no matter where they live, have been exposed to Western movies or television, their responses might indicate only the impact of these media and thus provide no proof of the universality claim. Therefore, we need participants who have not seen reruns of Western soap operas, or Hollywood movies, or a slew of Western advertising.\nIn one of the few studies of this critical group, American actors were photographed showing expressions that conveyed emotions such as happiness, sadness, anger, and fear. These photographs were then shown to members of various modern literate cultures (Swedes, Japanese, Kenyans) and to members of an isolated nonliterate New Guinea tribe. All participants who saw the photos were asked to\npick the emotion label that\nmatched each photograph. In other cases, the procedure was reversed. For example, the New Guinea tribesmen were photographed portraying the facial expressions that they considered appropriate to various situations, such as happiness at the return of a friend, grief at the death of a child, and anger at the start of a fight (Figure 12.29). American college students then looked at the photographs and judged which situation the tribesmen in each photo had been asked to convey (Ekman & Friesen, 1975).\nIn these studies, all the participants, including those in relatively isolated cultures, did reasonably well. They were able to supply the appropriate emotion label for the pho-tographs, or to describe a situation that might have elicited the expression shown in the photograph. But they were more successful at recognizing some expressions than at recognizing others. We highlighted the biological roots of smiling, and, in fact, these were, in this study, generally matched with “happy” terms and situations, with remarkable levels of consistency (Ekman, 1994; Izard, 1994; see also Russell, 1994). Other emotions, such as disgust, were less well recognized, but still identified at levels well above chance, suggesting that the meaning of emotional expressions tran-scends cultural and geographic boundaries.\nLet us note, though, that even though the perception of emotions may be similar in all cultures, the display of emotions is surely not. A widely cited example comes from research in which American and Japanese participants were presented with harrowing surgical films (Figure 12.30). Participants first watched the films privately (i.e., with no one in the room with them), but their facial expressions were recorded by a hidden camera. The facial reactions of Americans and Japanese were virtually identical. But when the participants then watched one of the films again while being interviewed by an experimenter, the results were quite different. In this context, the Japanese showed more positive emotion than the Americans showed (Ekman, 1972; Friesen, 1972). Thus, when in public, participants’ facial expressions were governed by the display rules set by their culture—deeply ingrained conventions, often obeyed without awareness, that govern the facial expressions considered appropriate in particular contexts (Ekman &Friesen, 1969; Ekman, Friesen, & O’Sullivan, 1988).\nOf course, display rules are not limited to a person’s reactions to a gruesome film. Other studies have extended the analysis of display rules in contexts as diverse as par- ticipating in sports (H. S. Friedman & Miller-Herringer, 1991) and receiving presents one does not like (P. M. Cole, 1985). Research has also explored the way in which indi- viduals differ in their knowledge of display rules (Matsumoto, Yoo, & Nakagawa, 2008). These differences include variation not only from one person to the next, but also between the genders. For example, women in Western cultures are more likely to express their emotions than men are, particularly emotions such as sadness (Brody & Hall, 2000; Kring & Gordon, 1998).\nAlong with changes in our behavior, emotion also involves changes in how we feel. Indeed, emotional experience has long been the essence of poetry, literature, and other forms of artistic expression that are all replete with expressions of undying love, mortal hatred, and unquenchable sadness. How can we study these fleeting and complex feelings hidden inside the mind (Barrett, Mesquita, Ochsner, & Gross, 2007)? Here, as elsewhere, scientists begin by seeking a proper classification scheme, and one proposal has focused on defining specific categories of emotions (see, for example, R. S. Lazarus, 1991). One problem with this approach, though, lies in defining exactly what the categories are. Common language gives few clues. There are over 550 emotion words in English (Averill, 1975), and many more in other languages that cannot be translated readily into English. However, as Phillip Shaver and his colleagues have shown, people typically use emotion words in ways that reveal a relatively small num- ber of “clusters,” which are defined by words with similar meanings (Shaver, Schwartz, Kirson, & O’Connor, 1987). As in Figure 12.31, one cluster involves words associated with love, another involves words associated with joy, and other clusters describe anger, sadness, and fear. An alternative approach describes emotions in terms of dimensions rather than categories: “more this” or “less that” rather than “this type” versus “that type.”There are various ways in which we might define these dimensions, but one relies simply on how pleasant or unpleasant the emotion feels, and then how activated the person feels when in the midst of the emotion (Barrett, 1998; Larsen & Diener, 1992; Russell, 1980, 1983); these two axes can be used to create a circle within which all the various inter- mixtures of the dimensions can be described, as in Figure 12.32.\nEither of these categorization schemes can help us figure out how emotions relate to one another—which are similar, which are sharply distinct. But neither scheme really tells us what the emotions really feel like, and so neither scheme answers questions about individual or cul- tural differences in emotional experience. Does your happiness feel the same as mine? When someone in Paris feels triste, is that person’s feeling the same as the feeling of someone in London who feels sad, or someone in Germany who feels traurig?\nFor that matter, how should we think about cultures that have markedly different terms for describing their emotions? The people who live on the Pacific Island of Ifalik lack a word for “surprise,” and the Tahitians lack a word for “sadness.” Likewise, other cultures have words that describe common emotions for which we have no special\nterms. The Ifaluk sometimes feel an emotion they call fago, which involves a complex mixture of compassion, love, and sadness experi-enced in relationships in which one person is dependent on the other (Lutz, 1986, 1988). And the Japanese report a common emotion called amae, which is a desire to be dependent and cared for (Doi, 1973; Morsbach & Tyler, 1986). The German language reserves the word Schadenfreude for the special pleasure derived from another’s misfortune. Do people in these cultures experience emotions that we do not (Figure 12.33)? Or are emotional experiences common across cultures, despite the variations in cultures’ labels for emotional expe-riences? On these difficult questions, the jury is still out.\nWhen we respond emotionally, it is often a whole body affair, and the bodily reactions associated with different emotions certainly feel different from one another (Levenson, 1994). That is, not only do the emotions differ in how they feel inside our “head,” but they also seem to differ in how they feel in the rest of the body. The sick stomach and wrinkled nose of disgust, for example, feel decidedly different from the squared shoul-ders and puffed chest of pride. And anger’s hot head and coiled muscles seem opposite fear’s cold feet and faint heart.\nFrom a common-sense perspective, it seems that emotions arise when we encounter a significant stimulus, and this encounter leads to bodily changes that dif-fer by emotion (Figure 12.34A). Interestingly, this sequence of events was turned on its head by one of the first emotion theories in the field, namely, William James’s the-ory that different emotions provoke different patterns of physiological response (James, 1884). According to the James-Lange theory of emotion (Carl Lange was a European contemporary of James’s who offered a similar account), the reason emo-tions feel different from one another subjectively is that we sense the different phys-iological patterns produced by each emotion. Specifically, this view holds that emotion begins when we perceive a situation of an appropriate sort—we see the bear or hear the insult. But our perception of these events is, as James put it, “purely cog-nitive in form, pale, colorless, destitute of emotional warmth”. What turns this perception into genuine emotion is our awareness of the bodily changes produced by the arousing stimuli. These changes might consist of skeletal movements (running) or visceral reactions (pounding heartbeat), but only when we detect the biological changes do we move from cold appraisal to emotional feeling, from mere assessment to genuine affect (Figure 12.34B). Moreover, the claim is that the specific character of the biological changes is crucial—so that we feel fear because we are experiencing the pattern of bodily changes associated with fear; we feel hap-piness because of its pattern of changes in the body, and so on.\nSubsequent theories, however, made quite different predictions about the degree of physiological patterning we should expect in emotion. For example, Walter Cannon, whom we met earlier as a pioneer in the study of the “fight or flight” response, believed that our physiological responses are quite general (W. B. Cannon, 1927). According to the Cannon-Bard theory of emotion (Philip Bard was a contem-porary of Cannon’s who espoused a similar view), it’s not easy to distinguish the bodily changes associated with different emotions, so that the bodily changes associated with anger are actually rather similar to the changes associated with happy excitement (Figure 12.34C).\nCannon’s view gained support from early studies in which participants received injections of epinephrine, which triggered broad sympathetic activation with all its consequences—nervousness, palpitations, flushing, tremors, and sweaty palms. These biological effects are similar to those that accompany fear and rage, and so, according to the James-Lange theory, people detecting these effects in their bodies should experience these emotions. But that was not the case. Some of the participants who received the injections simply reported the physical symptoms. Others said they felt “as if ” they were angry or afraid, a kind of “cold emotion,” not the real thing (Landis & Hunt, 1932; Marañon, 1924). Apparently, the visceral reactions induced by the stimulant were by themselves not sufficient to produce emotional experience.\nEven so, there is an obvious challenge to the Cannon-Bard theory. If different emo-tions produce comparable physiological responses, then why do we have the subjective impression that our bodies are doing quite different things in different emotional states? This question was addressed by the Schachter-Singer theory of emotion (Figure 12.34D). According to this theory, behavior and physiology are (as James proposed) cru-cial for emotional experience. James was wrong, though, in claiming that the mere per-ception of these bodily changes is sufficient to produce emotional experience. That is because, in addition, emotion depends on a person’s judgments about why her body and physiology have changed (Schachter & Singer, 1962).\nIn a classic study supporting this theory, participants were injected with a drug that they believed was a vitamin supplement but really was the stimulant epinephrine. After the drug was administered, participants sat in the waiting room for what they thought was to be a test of their vision. In the waiting room with them was a confederate of the experimenter (someone who appeared to be another research participant but was actu-ally part of the research team). In one condition the confederate acted irritable, made angry remarks, and eventually stormed out of the room. In another condition he acted exuberant, throwing paper planes out the window and playing basketball with balled-up paper. Of course, his behavior was all part of the experiment; the vision test that the participants were expecting never took place (Schachter & Singer, 1962). Participants exposed to the euphoric confederate reported feeling happy, and, to a lesser degree, par-ticipants exposed to the angry confederate reported that they felt angry. Although this study has come under criticism (G. D. Marshall & Zimbardo, 1979; Mezzacappa, Katkin, & Palmer, 1999; Reisenzein, 1983), it remains influential because it is a reminder that bodily arousal only partially determines the emotion that is experienced.\nOver the past 50 years, researchers have tried to clarify how the body responds dur-ing emotional experiences. One of the most interesting conclusions from this research is that our perceptions of bodily differences among the emotions may in some cases be illusions, compelling experiences that are not well grounded in reality (Cacioppo, Berntson, & Klein, 1992). It seems, therefore, that the various emotions are surprisingly similar if we examine the body’s response “from the neck down.”\nEven so, the emotions are distinguishable biologically—in the pattern of brainactivation associated with each emotion. Evidence on this point comes from studiesin the field of affective neuroscience (R . J. Davidson & Sutton, 1995; Panksepp, 1991, 1998), whose proponents argue that emotions arise not in one, but in multiple neu-ral circuits. Some brain regions are activated in virtually all emotions (Murphy, Nimmo-Smith, & Lawrence, 2003; Phan, Wager, Taylor, & Liberzon, 2002)—for example, the medial prefrontal cortex. One likely possibility is that this section of the brain plays a general role in attention and meaning analysis related to emotion. Other brain regions, however, seem to be related to specific emotions. For example, fear is often associated with activation of the amygdala, and sadness is often associ-ated with activation of the cingulate cortex just below the corpus callosum (although activation in these brain regions is not specific to these emotions; see Barrett & Wager, 2006). Many researchers are convinced that brain data like these will eventu-ally allow us to determine the extent to which different emotions have different phys-iological profiles."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:dcd9ab21-cb26-4b39-984f-7fdf3804982e>","<urn:uuid:fe48ec8b-45a6-4a23-b664-2ef512a462e1>"],"error":null}
{"question":"What are the historical examples of small-sized stamps, and what are the essential maintenance tips for preserving such delicate collectibles? ✨","answer":"Historical examples of small-sized stamps include the 1863 Colombian Bolivar 1-peso red Coat of Arms stamp (measuring under ½-inch per side), the 1857 Brunswick 1-gutegroschen stamp that could be divided into four smaller stamps, and Great Britain's 1870 ½-penny rose Queen Victoria stamp. To preserve such delicate collectibles, several maintenance practices are essential: keeping items away from humidity, rotating collections seasonally to avoid constant exposure to light, using proper dusting techniques with microfiber cloth or soft brushes, minimizing handling (using gloves when necessary), and utilizing quality, chemical-free packing products for storage.","context":["By Rick Miller\nI have friends who love boating. They actually have two boats and spend summer weekends cleaning them, keeping them in repair, and sometimes taking them out for a spin in a nearby lake. With fall approaching, they will be pulling them out of the water and putting them into dry dock.\nBoats are great big things. You can't just park a boat on a book shelf and pull it out on a cold winter night.\nIn contrast, we stamp collectors have it made. If we want a boat — or a dozen boats — we can easily take out a stamp album and find a fleet of them.\nCompact size is one of stamps' most appealing features, but if you have been collecting for a while you realize that a stamp collection seems to grow exponentially. In my case, my collection is about 30 percent greater than the available shelf space allows. Stamps are little, but they go into albums that are big and bulky.\nAdd catalogs, books, magazines, newspapers, newsletters, show programs, stock books and first-day-of-issue ceremony programs. Soon you have stacks of philatelic flotsam all over the place, taking up almost as much room as two boats. Or maybe more.\nIt has occurred to me that it might be possible to save shelf space by collecting only little stamps. After all, small-size stamps take up less than half the space of larger commemoratives or souvenir sheets, don't they?\nSo what stamps will be in this collection? Great Britain's Penny Black, issued in 1840, established the benchmark size for a regular-issue stamp that still holds true today. Any stamp smaller than that is littler than normal. The littlest stamp I could find in my own collection was issued by the Colombian department of Bolivar in 1863.\nThis imperforate 1-peso red Coat of Arms stamp (Scott 3) shown in Figure 1 measures just under a ½-inch on each side, including the margins. It is shown much larger here, but a magnifier is a must if you want to appreciate the detail of the coat of arms on this tiny stamp.\nThe German state of Brunswick issued an odd stamp in 1857. The 1-gutegroschen black on brown paper Crown stamp (Scott 12) shown in Figure 2 comprises four ¼gg stamps, each of which could be cut from the stamp and used as a 3-pfennig stamp. As a unit, the 1gg stamp does not qualify as little, but the individual ¼gg stamps are about 12 millimeters square.\nSpain issued a small-size ¼-centimo green Mural Crown stamp (Scott 190) in 1873 that bears a remarkable resemblance to the earlier stamp from Brunswick. A block of four of the stamps is shown in Figure 3.\nGreat Britain came up with a novel idea in 1870 when it issued a ½-penny rose Queen Victoria stamp (Scott 58, shown enlarged in Figure 4) that was much smaller than the 1d stamp. This stamp is a great choice for collectors who want to begin a specialized collection. They are plentiful and still relatively inexpensive in used condition.\nThe stamps were printed in sheets of 480, in 20 rows of 24. Each stamp in the sheet has letters in the corners that today aid collectors in determining the position the stamp came from in the sheet. The stamps from the first row all have the letter \"A\" in the lower left corner, and a letter in the lower right corner that ranges from \"A\" through \"X.\"\nTherefore, the stamp illustrated in Figure 4, which has the letters \"G\" and \"S\" in the lower left and right corners is from the seventh row in the sheet and is the 19th stamp in that row.\nThe letters in the upper corners are in reverse order of the bottom letters, so always look at the bottom letters to determine placement.\nSome collectors like to reconstruct an entire sheet with nice postally used examples of this ½-penny stamp. Others will choose to take the project even further and try to reconstruct a complete sheet from each of the 20 plates. The plate number from which the stamp was printed is inscribed on the stamp within the inner framework that flanks the denomination on both sides of the stamp. This type of project is a lifelong pursuit that will result in many volumes of stamp albums crowding the shelves in your stamp room.\nTwo Australian colonies, South Australia and Victoria, issued vertical half-sized ½d stamps, following Britain's example for size but not format. These stamps are even gentler on the stamp budget and not nearly so complex as the British examples. A South Australian ½d chocolate-brown Queen Victoria stamp (Scott 76) is shown in Figure 5.\nRussia, the world's largest country, has had a penchant for small stamps for much of its history. Figure 6 shows an 1883 Imperial Russian 3-kopek carmine Imperial Eagle and Post Horns stamp (Scott 33) at left, and a 2006 Russian Federation 5.60-ruble Coat of Arms stamp (6962) at right.\nTo conserve paper during World War II, South Africa and South West Africa issued small stamps known as \"bantams.\" The designs were the same as the previous stamp issue but were shrunk to half the size. The stamps were issued in pairs or strips of three that were perforated around the sides, but rouletted between.\nThe 4d and 1-shilling stamps have bilingual inscriptions (English and Afrikaans). The other stamps in the set are in bilingual pairs. A pair of South African 6d orange Welder bantams (Scott 96) is shown in Figure 7.\nThe United States has also issued little stamps. The small 13¢ Indian Head Penny stamp (Scott 1734) shown enlarged in Figure 8 was issued in 1978.\nCollecting little stamps provides big fun, but doing so may not actually save much shelf space. Even if it did, I'm sure we could all figure out how to fill the recovered space with more stamps and covers.","Collectible items come in different sizes, shapes, and values. Some of the popular ones include stamps, coins, action figures, comic books, and authentic signed memorabilia.\nEach collection has its unique story, and every collector has a distinct purpose behind their hobby. The reasons may vary from enjoyment, business, historical value, emotional investment, childhood memorabilia, recognition, and the thrill of the hunt. But no matter the reason, collecting is healthy, ordinary human activity.\nProper storage is critical when it comes to collecting items. But it can cause serious problems once the collection piles up. We end up neglecting most of our collectibles, causing them to accumulate dirt, dust, mold. If left unprotected, most collectibles lose their value if they get scratched or damaged over time.\nTo keep your collectibles safe and protected, we’ll discuss the proper ways of storing and maintaining them.\nKeep them out of humidity\nIt doesn’t matter whether you’re collecting movies, comic books, toys, or vinyl records. They come at a costly price, so you surely want to preserve them as possible.\nHumidity is one of the greatest enemies of every passionate collector. If you’re storing your collection in a place with high levels of humidity, then you better transfer them someplace else before it gets worse.\nThere are plenty of storage options available to protect the quality and value of your items. In fact, some businesses offer professional storage services that ensure greater security, better climate control, and proper handling to prevent any form of damage.\nBut if you prefer to store your collectibles in your home, consider buying silica gel packets. These little packets act as desiccant dehumidifiers that naturally absorb moisture.\nRearrange your collection\nCollectors with a large inventory of collectibles prefer to use their prized possessions as decorations to show to their family, friends, or guests.\nWhichever place you want to show off your collection, make sure to rotate them seasonally. This means putting some of your collectibles on display while the others are in safe storage. Placing them together in one area exposes them to dust, sunlight, and other damaging elements.\nWhen rotating items, make sure to use quality mobile storage to keep them protected while on display. It’s also worth noting that putting your collectibles in areas with constant access to light will do more harm than good. Ideally, place your collectibles in darker areas to protect items from chemical reactions that cause items to dry out or fade over time.\nObserve proper dusting\nWhether you want to store or display your collectibles, it’s essential to practice proper dusting regularly. Believe it or not, dirt and dust have their way of reaching the tiniest gaps. So regardless of how you store them, you have to dust them regularly!\nOnce you establish a regular cleaning routine, you can ensure that dirt and dust won’t easily tarnish or build up on your collectibles.\nTo effectively remove dirt buildup, use products and tools that won’t affect the quality of your collections. Microfiber cloth is known for its ability to trap and pick up dirt particles. For items with delicate surfaces, use a feather duster or any tool with soft brushes.\nKeep your hands off\nEver noticed how museum staff use gloves when handling historical items? Gloves serve as the protective barrier to prevent perspiration and dirt from your hands from sticking on the items.\nWhile it’s nice to pull out your collectibles from their boxes and admire them from time to time, overhandling causes long-term damage. Any particle can ultimately destroy the value of your precious collectibles. So, a good pair of gloves is always a must-have to handle your items without worry.\nUse quality packing products\nKeeping your collections in safe storage is never enough. To retain their quality and value, you have to pack them individually using reliable packing products that are chemical-free.\nAvoid using tissue paper whether as a cleaning tool or as a protective layer. Tissue papers often contain acid, causing damage to any textile or fabric, such as furniture, dolls, and clothes.\nInvest time in discovering the appropriate packing product fit for your collection. This will ensure your collection will retain its value over time. Sealed containers make the best packing products to prevent dust, moisture, and pests from damaging your collectibles.\nOur collections are among our most prized possessions, so it’s important to store them properly to keep them well-preserved and valuable regardless of their age. Remember that collectibles involve plenty of time, money, and effort. Learning to store them properly can turn your innocent hobby into a lifelong investment."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:c88cf63e-ff7f-4c95-81af-aefa8847b686>","<urn:uuid:daa49a7f-7f18-44ae-ac2e-e21c7b138d0e>"],"error":null}
{"question":"How do garlic and yogurt function as condiments in Mediterranean cooking, and what are their respective health impacts?","answer":"In Mediterranean cooking, yogurt is used as a condiment where small dollops are used as toppings or mixed into sauces and drizzled over dishes like kebabs or falafel, with whole-milk yogurt being preferred for its richer flavor. Garlic, meanwhile, serves as a crucial ingredient that provides both flavor and health benefits - it can reduce blood pressure, help unclog arteries, lower bad cholesterol (LDL), and acts as a natural blood thinner that can reduce blood clots. Studies have shown that garlic can reduce cholesterol levels by 12 percent after just four weeks of consumption.","context":["While you’ll find familiar ingredients like chicken and beef scattered throughout the Mediterranean, you’ll also find a lot of less familiar ingredients: grains like freekeh and farro, beans like cranberry and fava, meats like oxtail and quail, and seafood like monkfish and squid. To have success with Mediterranean cooking, you’ll have to stock your pantry with ingredients native to the Mediterranean (especially flavor building ingredients like sumac and pomegranate molasses, which the average American home cook might not have on hand). Below, you’ll find information about core pantry ingredients crucial to the Mediterranean diet.\nCanned and Dried Beans\nLegumes are a major source of protein in the Mediterranean diet. They are eaten simply on their own and also paired with diverse other ingredients to add heft and texture. For salads, quicker-cooking soups, and sautés, canned beans work just as well as or even better than dried; they hold their shape nicely and don’t require soaking or extended cooking. Use dried beans in recipes where the cooking of the beans builds body and flavor in the dish. Brining dried beans helps them to hold their shape during cooking and results in fewer blown-out beans.\nRice and Grains\nRice and grains are a vital part of many Mediterranean dishes. There’s also a best way to cook each grain. Grains like farro are best when cooked in a large amount of water like pasta, but finer-grained bulgur needs only to be rehydrated in water. At least half of the grains you eat should be whole grains (those that retain their original kernel); we like barley, bulgur, farro, freekeh, and wheat berries in the test kitchen.\nPasta and Couscous\nPasta takes center stage in many Mediterranean dishes but is often used in ways unfamiliar to most American home cooks. Try to use hearty whole-wheat pasta, which has an earthy flavor that pairs well with robust ingredients like pancetta and escarole. In North Africa, couscous, which is made of the same type of wheat as pasta, is popular both as a vehicle to sop up saucy dishes and stews and as a small plate in its own right. Eastern Mediterranean cuisines often use pearl couscous; its grains are larger than those of regular couscous and are toasted rather than dried, which gives them a nutty flavor.\nFresh and Dried Herbs\nThe brightly flavored dishes of the Mediterranean region rely on an abundance of fresh herbs, especially mint, oregano, dill, and basil, so it is helpful to keep your fridge or garden well stocked with these essentials. In the test kitchen, we use fresh herbs as a bright garnish for a large number of dishes, but they also act as main flavor components in some recipes too.\nMost fresh herbs are fairly perishable, but if washed and stored correctly they will keep for a week or longer. We recommend gently rinsing and drying herbs (a salad spinner works perfectly) and then loosely rolling them in a few sheets of paper towel. Seal the roll of herbs in a zipper-lock bag and store it in the crisper drawer of your refrigerator.\nWe also like to keep a variety of dried herbs on hand, which are great for use in long-cooked dishes like stews and braises. Although we usually use only hardy herbs such as rosemary and thyme in dried form, mint is another important dried herb in Mediterranean cuisines, valued for its earthy yet bright flavor.\nDried herbs lose their potency six to 12 months after opening, so it’s important to replace them frequently. You can test dried herbs for freshness by rubbing a small amount between your fingers; if the herbs don’t release a bright aroma, it’s time to buy a new jar. You can quickly dry hearty herbs (thyme, rosemary, oregano, and bay leaf ) in the microwave. Lay a single layer of herbs on a paper towel–lined plate and cover with a second paper towel. Microwave on high for 1 to 3 minutes, checking occasionally, until the herb appears dehydrated. Cool at room temperature and then crumble or store whole in a plastic bag.\nOlives and Olive Oil\nIf there is one ingredient that is the emblem of Mediterranean cooking, it is olives. Of course they are best known for their use in olive oil. There are countless varieties of eating olives, from French niçoise to Greek kalamata. We recommend buying olives from the refrigerated section of your supermarket, since jarred shelf-stable ones tend to be saltier. If you have time, we recommend that you buy unpitted olives and pit them yourself, as they will be less mushy than prepitted ones. (Pitting an olive is easy: place the olive on a cutting board and hold the flat edge of a knife over it. Press the blade firmly with your hand to loosen the olive meat from the pit, then remove the pit with your fingers.)\nBuy Now on Amazon: Extra-Virgin Olive Oil\nSpices, Spice Blends, and Pastes\nThe flavor combinations vary, but spices are often what make one Mediterranean cuisine distinct from another. Many of the spices used in Mediterranean cooking are probably already in your pantry, like cinnamon, paprika (both sweet and smoked), and saffron. There are also a number of spices that may be unfamiliar to you, such as sumac, which adds a floral, tangy flavor to recipes, and Aleppo pepper, which lends subtle heat to dishes.\nSpice blends and pastes also add potent flavor: There’s North African ras el hanout, which can include some 25 spices, seeds, dried flowers, berries, and nuts; za’atar, a popular eastern Mediterranean blend that consists of wild thyme, sumac, sesame seeds, and salt and is used as both a seasoning and a condiment; and harissa, a North African chili paste made with hot and/or mild chiles, garlic, and oil and often cumin, coriander, and caraway. You can buy these and other spice blends, but if you can’t find them you can make your own at home.\nStoring your spices out of the light and heat will extend their shelf life. Check for freshness by observing the aroma and color of your spices. When buying spices, brand makes a difference. Some of our favorites are Penzey’s Extra Fancy Vietnamese Cassia Cinnamon, The Spice House Hungarian Sweet Paprika, Simply Organic Smoked Paprika, and Morton & Bassett Saffron Threads.\nCheeses, Cured Meats, and Nuts\nA little cheese (feta or goat cheese), cured meat (pancetta or chorizo), or nuts (almonds or pine nuts) goes a long way in adding rich flavor to Mediterranean dishes. For example, Paniscia, a regional Italian risotto, is flavored with bits of salami, and many pastas and vegetable salads rely on a little nutty Parmesan to boost savory flavor. Nuts are common additions to salads and are also an essential element of the North African seasoning blend dukkah.\n(For long-term storage in the refrigerator, we find that cheeses are best wrapped in parchment paper and then aluminum foil. The paper allows the cheese to breathe, and the foil keeps out off-flavors and prevents the cheese from drying out. We recommend storing nuts in the freezer to prevent their natural oils from turning rancid. We often toast nuts before using them to bring out more depth of flavor.)\nYogurt: You may not think of yogurt as a condiment, but it’s used that way in much of the Mediterranean, where a small dollop is used as a topping or it’s mixed into sauces and drizzled over kebabs or falafel. We usually prefer the richer flavor of whole-milk yogurt (our favorite is Brown Cow Cream Top), but some recipes will work with low-fat yogurt as well. Our favorite Greek yogurt is Fage Total Classic Greek Yogurt. (Buy Now on Amazon | Fage Total Classic Greek Yogurt)\nTahini: Made from ground sesame seeds, tahini is used much like yogurt: It can be used as a topping on its own or as a base for sauces. It’s also an essential component of dips like hummus and baba ganoush. Our favorite brand is Joyva Sesame Tahini. (Buy Now on Amazon | Joyva Sesame Tahini)\nPomegranate molasses: A reduction of pomegranate juice, pomegranate molasses is thick and syrupy with a unique, sweet-sour flavor. You can buy it or make your own. Pomegranate molasses adds complex tanginess to many Mediterranean dishes. (Buy Now on Amazon | Pomegranate Molasses)\nPreserved lemons: Preserved lemons, a specialty of North Africa, add an intense lemon flavor to many dishes. They are easy to make, and they keep for six months in the fridge.\nDukkah: This Egyptian condiment is a blend of nuts, seeds, and spices that can be sprinkled into olive oil as a dip for bread or can be used to add texture and flavor to certain Mediterranean dishes. Note: the ingredients vary depending on the brand. (Buy Now on Amazon | Dukkah)\nFor more information on the Mediterranean Diet, read the following posts:\n- What is the Mediterranean Diet?\n- 8 Easy Ways to Eat the Mediterranean Way\n- Everything You Need to Know About Olive Oil\n- The Mediterranean Diet Food Pyramid [INFOGRAPHIC]\nWhat's your favorite Mediterranean cuisine? Let us know in the comments!\nLearn more about this heart-healthy diet The Complete Mediterranean Cookbook\nBring the Mediterranean—from Italy and Greece, to Morocco and Egypt, to Turkey and Lebanon—into your kitchen with 500+ fresh, flavorful recipes. This comprehensive cookbook translates the famously health Mediterranean diet for home cooks with a wide range of creative recipes.","Warnings about foods that harm your heart abound: fried potatoes, greasy burgers, cream-rich pasta… But here’s a look at four fabulous foods that do your heart a world of good. The great thing is, they are all easily available and not expensive. Chances are, you are already enjoying them in your daily diet. If yes, rejoice, because they come with loads of heart-healthy benefits. If not, put them on your menu.\nHazelnuts: While most tree nuts are known to be beneficial for health, hazelnuts in particular have been found to heal the heart. They have a high concentration of oleic acid, an omega-9 fatty acid that can reduce your blood pressure, prevent strokes, and save you from heart attacks. A study published in the European Journal of Clinical Nutrition shows that eating 30 grams of whole, ground, or powdered hazelnuts per day for four weekscan help raise your good cholesterol (HDL), lower the bad cholesterol (LDL) and thus bring down your total cholesterol, which damages the arteries and contributes to plaque build-up.\nGarlic: Numerous studies have shown that garlic can reduce blood pressure and help unclog the arteries. According to scientists at Weill Medical College of Cornell University in New York, the compounds in garlic may play a significant role in maintaining a healthy cardiovascular system. Garlic lowers bad cholesterol (LDL) and brings down blood pressure. A study published in the Journal of the Royal College of Physicians found that after just four weeks there was a 12 percent reduction in cholesterol levels in the research groups that had taken garlic. It is also a natural blood thinner, so it can reduce the incidence of blood clots that can lead to stroke and thrombosis.\nFlax seeds: Flax seeds are one of the richest sources of the plant-based omega-3 fatty acid, alpha-linolenic acid (ALA), which is known to counter cardiovascular disease. According to a study published in the Canadian Journal of Cardiology, the best way to get the heart-healthy benefits of flax seeds is to grind or crush them, then mix them or stir them into your food. This makes the nutrients more bioavailable to the body. Start with a daily teaspoon of flaxseeds and work your way up to two tablespoons, if you are not used to them.\nKidney beans: These delicious beans are a rich source of omega-3s, which reduce triglycerides, stabilize your heartbeat, make platelets “less sticky” and lower blood pressure. Omega-3s also boost levels of good cholesterol (HDL) and help clear your arteries. In fact, the entire legume family—comprising beans, peas and lentils—is heart-friendly. A large scale national study, published in the Archives of Internal Medicine says including up tofour servings of legumes in your diet every week can cut your risk of heart disease by up to 22 per cent. If beans give you flatulence, don’t avoid them—simply cook them with herbs and spices. This helps reduce their gas-causing properties. The best spices and herbs to cook beans with are ginger, turmeric, cumin, basil, and fennel seeds."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:a940f148-9ce0-4e2d-9fcf-e292e36b62a4>","<urn:uuid:6352e13a-6975-4182-a720-b3d85c41215e>"],"error":null}
{"question":"How many stocks should be included in a well-diversified portfolio versus using passive index funds?","answer":"A well-diversified individual stock portfolio should be limited to about 15 stocks, as experts believe 90% of diversification benefits can be achieved with this number. Going beyond this number can make portfolio tracking impractical and returns may only match market averages of around 12% per annum. At that point, it would be better to invest passively in index-based mutual funds or ETFs, which automatically provide broad market exposure with lower management requirements.","context":["Knowledge about how to build a stock portfolio is perhaps as essential as the stock analysis itself. Though the former looks more daunting to me.\nA portfolio that might look apt for me might not work for someone else. But there are few basic rules for building a stock portfolio. One can apply these rules and build a customized portfolio for themselves.\nIn this article, I’ll write about those basic rules. These are rules which I’ve grabbed through reading and application to build my stock portfolio.\nGood Vs. Bad Stocks\nOut of all the stocks that one picks for his/her portfolio, only about 60% will perform as intended. So if some shares are not doing well, it is acceptable.\nBut what differentiates between good investors and others is the proportion of money parked in good (60%) versus other stocks (40%). Achievers tend to weigh heavy on the 60% side.\nSo even while adding stocks to the portfolio, good investors tend to segregate them into two categories, 60%-Group versus 40%-Group. Based on the available opportunity, more money should flow into stocks in 60%-Group, compared to the 40%-group.\nWhat are Good Stocks?\nWhat makes a stock good? The following three factors together can make stocks great:\n- Wide Moat: One of the first determinants of a profitable portfolio is the inclusion of wide-moat stocks. These are stocks of great companies. What makes these companies great is a combination of factors like a quality product, market share, management quality, brand value, etc.\n- The margin of Safety: If we buy a stock at a discounted price below its intrinsic value, we are maintaining a margin of safety. Generally, good stocks trade at overvalued price levels. So buying a good stock at a discount is like catching a plane before its take-off.\n- Hold For Long Term: The return generated by a stock portfolio is also dependent on the holding time. Stocks of great companies tend to yield higher profits over time. Why does this happen? It happens because of the power of compounding.\nA combination of the above three factors can make a stock portfolio incredible. A wide-moat stock, bought at an undervalued price and held for a long-term will yield higher returns.\nWhat if Good Stocks Are Not Available?\nBuild Cash Reserves. If pro-investors are not investing, they are piling up cash to make them available when investment opportunity comes.\nThere will be instances where one may not find opportunities to buy stocks. There can be two cases:\n- The investor is not able to identify a wide-moat company. People who do not know how to analyze stocks may never locate a good share. It is a limitation. Such people can learn the process of fundamental analysis of stocks. If they do not have time, they can use a shortcut – my stock analysis worksheet.\n- The Stocks are available at overvalued price levels. First, buy only undervalued stocks. Second, gather Stocks by maintaining a margin of safety. These two sub-rules make stock purchases a RARE activity even for experts.\nSo it means, most of the time, experts sit idle and do nothing. Why? Because the opportunity to buy good stocks is rare. So what these people do in their idle time?\nWarren Buffett says he mostly spends time reading annual reports of companies. It is his way to identify wide-moat stocks.\nThere is another thing that happens automatically during idle time (periods of non-investment). What is it? Accumulation of cash. I allow my spare cash to accumulate using recurring deposits.\nDiversifiable and Non-Diversifiable Risk\nInvesting money in one company is not advisable. Similarly, keeping shares of a lot of companies in one’s stock portfolio must also be avoided.\nWe must spread our money in multiple stocks (companies). It will minimize the risk of loss.\nBut there is a limit to risk minimization. Broadly speaking, there are two types of risks in stock investing:\n- Diversifiable Risk: This is the risk of loss associated with the company itself. If the company makes a loss, its stock price will also suffer. Such a risk can be minimized by including multiple stocks in our portfolio. It is called investment diversification.\n- Non-Diversifiable Risk: This is the risk of loss caused due to reasons not attributable to a company. Examples of it can be a bad economy, sector downturn, etc. Such reasons affect all business coming under its ambit. For example, a Yr-2008-09 like global economic meltdown will affect all companies across the globe.\nWhat does it mean? Out of the total risk associated with stock investing, we can manage only the diversifiable risk.\nSo, how many stocks shall be included in the portfolio to get a good diversification? The more, the better? Not really.\nMaximum Number of Stocks in A Stock Portfolio\nIt is possible to manage only the diversifiable risk. Even 1,000 number stocks in a portfolio will not reduce the non-diversifiable risk. So we must not waste our efforts trying to eliminate the non-diversifiable risk.\nSo, how many stocks to include to reduce the diversifiable risk? The limit should be about 15 stocks. Why?\nTo understand this, we must see it from two perspective\n- Potential Returns, and\n- Portfolio Management Rules.\nWhy limit the stock to 15 numbers max? The more stocks in a portfolio, the better it’s diversified. It is true. But in this case, the returns generated by the portfolio will only match the market’s average (like 12% per annum). Similar returns are possible by investing passively in an index-based mutual fund or an Exchange Traded Fund (ETF). But if one wants to earn returns more than the market’s average, only a concentrated portfolio will help.\nExperts believe 90% diversification benefits can be achieved through 15 stocks only. Moreover, owning too many shares in the portfolio induces a practical problem. How? Too many stocks mean less time for individual companies. When we hold stock in a portfolio, we must read their related news, quarterly performances, and annual reports. If there are many stocks, it means we can track our holdings only superficially.\nIn this case, a better alternative will be to own stock indirectly through mutual funds.\nIf one wants to build a stock portfolio for self, then investing within one’s circle of competence is a must. Owning too many stocks is a sign of stock ownership beyond this circle. It is very risky.\nWeight of Individual Stocks in a Portfolio\nLimiting a portfolio to about 15 number stocks is one step. But equally important is to know how much weight must be put on the individual stocks. From what I’ve read about portfolios, I’m not sure if there is a rule. So I’ll speak from what I’ve learned from practice.\nIdeally, we must be overweight in stocks that have more chances of doing well in the future. How to do it?\nWhen I analyze my stocks, I do several things at a time. The first I do is get a first impression of the company. I use my stock analysis worksheet. It estimates the intrinsic value, gives it an overall rating and also generates its 10-Yr snapshot. It gives me a general feel about the stock’s strengths and weaknesses.\nOnce I’m impressed by the stock, I start reading all news feeds about the company for at least 6-7 days. Once I’ve saturated myself with it, I download its latest annual report and read at least the first 30 pages.\nAfter doing this exercise, I try to build an impression about the company & its stock. I try to categorize them into the 60%-Group or 40%-Group. If the stock fall’s in the 60%-Group, I start buying its stocks. Sometimes, I start with as low as Rs.5,000 per trade. But I make sure that a stock’s weight does not go beyond 5% of the total size of the portfolio. For stocks falling under 40%-Group, I make sure that their percentage does not cross 1% till my impression change about them.\nBuy Less, Sell Even Lesser\nYou have already seen the definition of good stocks.\n“These are wide-moat stocks, bought at discounted price levels, and held for a very long term.”\nIn this case, the realm of the long term can also be 15-20 years. So you can see, the holding time is VERY long. When holding time is so long, people do not seem to sell their holdings. As Warren Buffett says, buy stocks to hold them forever.\nMoreover, wide-moat stocks are rarely available at discounted price levels. Hence, for the investors, the buying opportunity is also less. During these times of inactivity, make sure to build your cash reserves.\nWide-moat companies are harder to buy. So, once we have them in our portfolio, we must not think to book profits too soon. Why? Because we may not get another opportunity to buy such wide-moat stocks, at those price levels again.\nBuilding A Circle of Competence\nAll great long-term investors have their preferences and biases towards a few sectors and companies. For sure, they like these companies, and that is why they keep buying and holding their stocks.\nFor people like us, more important is to know what makes great investors like a company. Generally speaking, great investors will only buy stocks of companies with a wide moat. We already know this right?\nBut it is not easy to identify a wide-moat company. There is no magic formula that will tell us that a company has a wide moat. We can only estimate their Moat based on assumptions.\nBut big investors like Warren Buffett follow a better strategy. They build a ‘circle of competence’ about a sector/company. It is nothing but a process of developing a deeper understanding. How to do it? By reading more about the company. Contents like news, annual reports, quarterly reports, and analysis of experts.\nWhile I was in a job, I have mainly worked in Steel and Power sectors. Hence, compared to other industries, my visualization of the Steel and Power sector is better. Hence, if I try to build my circle of competence in these two sectors, it will be better.\nOne can also expand their circle of competence to other sectors by doing the same type of exercise.\nWhy build the circle of competence? Because bigger will be our circle of competence, we will be able to locate more wide-moat stocks.\nWhy Only Stocks? Can We Include Mutual Funds in The Portfolio?\nTaking support of mutual funds can be a good strategy. Generally, an individual’s circle of competence is limited to only a few sectors. But it is only natural to get tempted to buy stocks from sectors beyond our circle of competence. How to do it? We can invest in such stocks through sector mutual funds.\nSuppose we are in India, and we want to buy stocks of companies in the USA. In this case, we can take the help of mutual funds to buy those stocks.\nBuilding a stock portfolio that complements the strategy described above will generate returns higher than the market average.\nRemember to build a circle of competence wide enough to maintain 15 stocks in the portfolio. Make sure to buy them by maintaining a margin of safety.\nIf one wants to diversify beyond one’s “circle of competence,” including mutual funds will be a good idea.","There has been a lot of debate over whether active funds or passive funds are \"better,\" but the answer is that they can serve different purposes. Investors care most about which type performs better, meaning which has higher returns, but risk is another important consideration.\nIn short, actively managed funds have investment managers or management teams that pick stocks or securities that they expect to outperform their benchmark or otherwise add value to the overall portfolio. Passively managed funds, on the other hand, track an index and don't have managers making investment decisions. Each of those approaches comes with pros and cons that investors should know about.\nBelow I'll discuss some of the most important differences between active and passive funds. Note: Throughout this article I broadly refer to \"funds,\" but I don't make a distinction between mutual funds and exchange-traded funds (ETFs). There are both passive and active mutual funds and ETFs, though most ETFs are passive, while mutual funds are more often actively managed.\nHow passive funds work\nTo understand passive funds, you first need to understand a couple of important points about index investing.\nThere are a little under 4,000 exchange-traded stocks. In the investing world, an index has two main purposes: Measuring the overall stock market's performance and serving as a benchmark for individual investors' portfolio performance. Indexes are like complex averages that reflect how the overall market is doing; theoretically, the average of their performance should mirror the health of the stock market as a whole.\nThere are several popular U.S. market indexes that measure different collections of stocks:\n- S&P 500: Includes 500 of the largest stocks (by market capitalizations) listed on the New York Stock Exchange (NYSE) and the NASDAQ.\n- Nasdaq Composite: A market cap-weighted index including 3,300 common equities (without going into too much detail, the index includes securities other than common stocks) listed on the NASDAQ stock exchange.\n- Wilshire 5000: Represents the total stock market. It is weighted by market cap and contains over 3,000 securities of all market caps.\n- Russell 2000: Composed of 2,000 of the smallest stocks (by market cap) in the Russell 3000 index.\nThe above are all stock indexes, though there are also a wide array of bond indexes.\nThese indexes are used to gauge the overall performance of the market -- or of a particular slice of the market. For example, if the S&P 500 gains 0.8% in a month, while the S&P Small Cap 600 gains 1.7%, you might hear that small-cap stocks outperformed large-cap stocks by 0.9%.\nUnderstanding what indexes are is a big part of understanding what passive funds are and what they do. This is because passively managed funds are often defined by the index they track. For example, a fund that tracks the S&P 500 buys and sells stocks as they are added to and dropped from the S&P 500. Passive funds also buy and sell stocks to maintain the same weighting as the index, which is based on market capitalization in the case of the S&P 500. Some index funds won't hold all of the index's securities but only a subset that should mimic the index's performance.\nBecause there's less managerial involvement, passive funds generally have lower expense ratios than actively managed funds. Expense ratios can be a significant differentiator between passive funds that track the same index, because the funds will, in theory, have the same returns.\nNow to the important part -- returns. Passive funds, at best, will match the performance of the indexes they track. In fact, after fees are taken into account, index funds tend to slightly underperform their benchmarks, whose returns are measured without fees.\nProponents of passive investing say the benefits include lower fees, additional transparency, and the potential for better tax efficiency. Passive funds are more transparent because an index's underlying holdings are easily found online. They're sometimes considered more tax efficient because they may buy and sell securities less frequently than their actively managed counterparts -- although this is not always the case.\nHow active funds work\nWhile active funds can outperform their benchmark indexes, most of them historically have not, though of course some actively managed funds outperform from time to time, or even over the long term. Proponents of active funds say the benefits include greater flexibility, the ability to hedge, and the ability to better manage taxes by having more control over when securities are sold. Active funds have greater flexibility because their managers are not limited by following the index; they can buy and sell whatever securities they think fit the fund's strategy. Fund managers can also hedge their bets by using short positions, futures, or options, whereas a passive fund typically can't. This added flexibility can lead to more risk, as investment decisions are in the hands of human managers who face fewer restrictions.\nThe Morningstar Active/Passive Barometer is a semiannual report that measures the performance of U.S. active funds against passive peers in their respective Morningstar categories. The report showed that active funds outperformed passive funds in several categories during 2017, but the June report said this trend changed: \"Just 36% of active managers categorized in one of the nine segments of the U.S. Morningstar Style Box both survived and outperformed their average passive peer over the 12 months through June 2018. In 2017, 43% of active managers achieved this feat.\"\nThe report makes it clear that actively managed funds \"have left much to be desired\" over the longer term.\nThere may be room for both active and passive funds in your portfolio\nSometimes passive funds do better than active funds and vice versa. Most of the arguments for and against either are generalities; there are no absolutes.\nActively managed funds can be costlier if trades are made more often, and frequent trading can also have tax implications. High turnover may also be a consideration for passively managed funds, however: If the index has a lot of turnover, the fund will as well.\nActively managed funds may be best for investors who want to make specific bets on the market or who believe in specific managers. Actively managed funds can offer investors a chance to outperform the overall market. Passively managed funds may be best for investors who want to \"set it and forget it,\" as they can provide consistent low-cost market exposure. But again, these guidelines aren't absolute.\nActive and passive funds are very different beasts, but there may be a place for both in an investor's portfolio."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:a2463a97-7eeb-43f1-83a3-b69aad64d380>","<urn:uuid:805c2a95-ce06-4094-b4c4-f4a087c3bb3b>"],"error":null}
{"question":"How do connotations and linguistic relativity influence our understanding of words differently?","answer":"Connotations and linguistic relativity influence our understanding in distinct but related ways. Connotations represent meanings acquired by words over time through shared use, often more powerful than their formal dictionary definitions. For example, 'home' has emotional connotations of family and comfort beyond its basic meaning of a dwelling. Linguistic relativity, on the other hand, suggests that the very structure of our language shapes our thoughts and perceptions more fundamentally. While not as extreme as once believed, research shows language can influence how we process things like color perception, gender associations, and spatial orientation. Both concepts show how language affects meaning beyond literal definitions, but operate at different levels - connotations through cultural usage and linguistic relativity through grammatical structure.","context":["Whether and how our languages shape our thoughts, perceptions and worldviews is a perennially vexed subject. (For starters, what do we mean by shape and thoughts?) Known as linguistic relativity or the Whorfian or Sapir-Whorf hypothesis, the nature and extent of this influence have proved difficult to establish. Traditionally, some philosophers made grandiose claims about it, but the currency of such claims plummeted in the 20th century.\nLinguist Guy Deutscher, in a NYT Magazine article titled ‘Does Your Language Shape How You Think?’ says that with ‘Science and Linguistics‘ (PDF), Benjamin Whorf ‘seduced a whole generation into believing that our mother tongue restricts what we are able to think’. I don’t think Whorf deserves so much responsibility, or blame, for whole-generational seduction,* but here’s a pertinent excerpt from his influential essay:\nFormulation of ideas is not an independent process, strictly rational in the old sense, but is part of a particular grammar, and differs, from slightly to greatly, between different grammars. We dissect nature along lines laid down by our native languages. The categories and types that we isolate from the world of phenomena we do not find there because they stare every observer in the face; on the contrary, the world is presented in a kaleidoscopic flux of impressions which has to be organized by our minds — and this means largely by the linguistic systems in our minds.\nDespite linguistic relativity’s fall from academic favour, it persists – thrives, even – in the popular imagination. In his new book Through the Language Glass: Why the World Looks Different in Other Languages, Deutscher looks at how valid it really is and what conclusions may be drawn about it. Sifting through a weight of data and theories, he describes several ways in which a weak form of linguistic relativity seems to obtain – colour perception, gender, and spatial orientation – and makes the case that language can influence our thoughts and thought patterns not radically, but more significantly than is sometimes acknowledged.\nAfter a prologue summarising the issues, the book begins with colour. Deutscher delivers a long and fascinating account of how William Gladstone’s Homeric scholarship brought attention to the peculiar treatment of colour in the Iliad and Odyssey: colour descriptions were strangely lacking, and where they occurred they seemed wayward (‘the wine-dark sea’). Gladstone’s explanation was equally strange: ‘nothing less than universal color blindness among the ancient Greeks’. Quoting Gladstone:\nColours were for Homer not facts but images: his words describing them are figurative words, borrowed from natural objects. There was no fixed terminology of colour; and it lay with the genius of each true poet to choose a vocabulary for himself.\nGladstone suggested that maybe colours, as abstract phenomena distinct from objects, began striking people only after the development of artificial colours such as in paints and dyes. The sky and sea are blue to us, but is this because we’re so familiar with the colour in more tangible forms?\nDeutscher tells the story of Lazarus Geiger, a philologist who picked up where Gladstone left off by investigating the description of colour in old texts in other languages. He found similar deficiencies throughout this etymological terrain, and also that colour terms when they emerge do so ‘according to a definite succession’ and ‘in the same order everywhere’. Geiger died young, in 1870, his prescient ideas overlooked.\nIn 1969, Berlin and Kay’s Basic Color Terms reiterated Geiger’s sequence of colour terms, more or less. It’s still being explored and debated: is colour perception a human universal, or culturally imposed? Deutscher believes ‘culture enjoys freedom within constraints’, and that ‘nature’s hold loosens considerably in the realms of abstraction’. Different languages divide the colour spectrum subtly differently, and TTLG supplies a helpful narrative with enough detail to enlighten without becoming laborious.\nAt the end of part 1, Deutscher skewers what he calls ‘the dogma of equal complexity’: the idea that all languages are equally complex. His corrective, or reminder, is worth repeating: ‘there is no a priori reason why different languages should all mysteriously converge on even roughly the same degree of complexity’.\nThen we’re back to Edward Sapir and the ‘linguistically determined thought world’ described by his student, Whorf. Deutscher conveys the ‘heady atmosphere of discovery’ that allowed a ‘daring idea about the power of language’ to shoot to prominence. He tells how the idea later crashed precipitously ‘when it transpired that Sapir and especially his student Whorf had attributed far-fetched cognitive consequences to what were in fact mere differences in grammatical organization’:**\nThe excitement about the – largely factual – strangeness of expression in American Indian languages was somehow taken as sufficient to deduce the – largely fictional – differences in their speakers’ perceptions and thoughts.\nPop-Whorfianism often takes the form ‘no word for X‘; a language’s lack of a given term is supposed to imply its speakers’ lack of the concept. Deutscher rails against the ‘toxic fallacy’ that our language limits what we are able to understand. Languages are not fully directly translatable, but they are generally sufficiently so.\nDeutscher quotes the anthropologist Franz Boas, who in 1938 wrote that grammar ‘determines those aspects of each experience that must be expressed’, and that these aspects vary between languages. Years later, Roman Jakobson repeated the essential point: languages ‘differ essentially in what they must convey and not in what they may convey’. More recently, Barbara Schmiedtová said it’s ‘not about whether you can say anything in any language. The interesting thing is what you prefer to say […] when you are under pressure, which is the normal situation when you produce language.’\nFor spatial orientation, Deutscher describes the Australian aboriginal language Guugu Yimithirr, which instead of right, left, behind and in front of uses cardinal points: the dog was east of the tree. Native speakers automatically perceive object relations this way, even in dreams and memories, and with no painstaking cognitive navigation. It is a kind of automatic mental compass.\nThis mode of perception and description is far from unique. Other ‘geographic coordinate systems’ around the world take their cues from local landmarks like seas, rivers, or mountains, as distinct from the ‘body-framed mapping’ we likely employ. Several are summarised in this presentation (PDF). Deutscher recounts how Stephen Levinson, who studied Guugu Yimithirr, asked its speakers for advice on how he might improve his sense of direction, and they suggested:\nthe differences in brightness of the sides of trunks of particular trees, the orientation of termite mounds, wind directions in particular seasons, the flights or bats and migrating birds, and alignment of sand dunes in the coastal area.\nIntroducing the subject of grammatical gender, Deutscher reprints Heinrich Heine’s poem Ein Fichtenbaum Steht Einsam along with two translations, one of which (Emma Lazarus’s) takes into account the fact that the German original marks one tree as male and one as female. This grammatical feature of German seems central to Heine’s metaphor, yet many translations sidestep it, referring to both trees as it.\nThere has been some research into how gender systems might affect our ideas about things. Is it significant (beyond grammatical curiosity) that forks in Spanish are masculine and spoons are feminine, and that this leads some people to associate certain stereotypical properties with them? Deutscher contends there is ‘little doubt that the idiosyncrasies of a gender system exert a significant influence on speakers’ thoughts’.\nIt depends on what you consider significant, I suppose. In some respects his claim is fair: sexist pronoun use (and related terminology) reflects and contributes to sexist culture. So I was surprised to see man used generically in TTLG (‘man landed on the moon’). And this passage gave me pause, but not in the way the author presumably intended:\nBut if you native speakers of English are tempted to feel sorry for those of us who are shackled by the heavy load of an irrational gender system, then think again. I would never want to change places with you. My mind may be weighed down by an arbitrary and illogical set of associations, but my world has so much to it that you entirely miss out on, because the landscape of my language is so much more fertile than your arid desert of “it’s.”\nIt seems an oddly triumphalist way to make a point. Sometimes, too, the book sets up its arguments heavy-handedly, underwhelming the reader. For the most part, though, Deutscher writes skilfully, making light work of knotty and subtle concepts. Human brains are ‘innately equipped with powerful pattern-recognition algorithms, which sort similar objects into groups’; TTLG shows how we do them a disservice by ignoring the facts, wherever they lead us.\nLike Deutscher’s The Unfolding of Language, which examined how languages function and change, Through the Language Glass is an engrossing and entertaining book. It offers an illuminating and accessible account of linguistic relativity, and a colourful tour of many surprising linguistic features. Drawing judiciously on overlooked history and modern research, it updates us on complex and oft-misunderstood issues that affect us all – however subtly – every day.\nDisclaimer: I’m not a linguist. Corrections and clarifications are welcome.\n* ‘Though Whorf’s view of relativism is only one out of a great many, and though it has no privileged status from a scientific point of view, it does have a privileged historical status.’ – George Lakoff, Women, Fire, and Dangerous Things\n** Arika Okrent in her book In the Land of Invented Languages describes the idea’s fluctuating acceptability thus: ‘the Whorfian hypothesis endured a long half century of being proven, disproven, defended, demolished, revived, mocked, and revived again.’","Key language ideas\nThese key language ideas will help you to understand theory of knowledge, and produce a powerful TOK essay and presentation. You should try to the terms as much as possible, and ideally link them to key TOK thinkers.\nThe connotation of a word is the meaning that has been acquired by a word over time, via individual and shared use. It is different, therefore, from the ‘denotation’ of a word, which its more formal meaning, that may be defined in a dictionary.\nThe connotation of a word is often more powerful than its denotation, meaning that when we use a word with one intention, we end up actually expressing a different idea. It also means that two words with a similar denotation can mean totally different things because of their connotations.\nFor example, the two words ‘house’ and ‘home’ have similar denotations – dwellings in which people live. However, because of the way we have used the word ‘home’, it has a lot more connotations, involving family, warmth, safety, and comfort.\nA constructed language is a language that has been deliberately designed and constructed, rather than one that has evolved over time. Examples include the language ‘Esperanto’ which was created in order to facilitate communication between different language-speaking cultures, and the code used to create websites (such as this one!).\nThe denotation of a word is its literal meaning, and the one you will find defined in a dictionary. See our discussion above for the difference between ‘denotation’ and ‘connotation’.\nA euphemism is a word or phrase used in an attempt to lessen the negative aspects of an event, phenomenon, or state of being. Euphemisms are used skilfully by many people who are good at persuading others, such as those who work in the advertising industry, sales, or politics. Examples abound – we no longer buy ‘second-hand’ goods, we buy ‘pre-loved’ items (note the use of the connotation of ‘loved’); military leaders do not talk about civilian (ie innocent) deaths caused by their bombing campaigns, they talk about ‘collateral damage’; rather than say a person has died, we might say that person ‘passed away’, or is ‘no longer with us’.\nLinguistic relativity (or the Sapir-Whorf hypothesis)\nLinguistic relativity is the concept that our ideas, beliefs, and perspectives are shaped by the language that we speak. This was based on studies made of different cultures by (amongst others) Edward Sapir and Benjamin Whorf, that linked different thought processes and world views with different languages. Although the idea influenced many works of literature (such as Orwell’s 1984), it was largely replaced by the Universal Grammar theory of Chomsky.\nMetaphors are used widely in language, and are responsible for conveying a huge proportion of language’s meaning. A metaphor uses a recognisable and approachable word or phrase to compare to a real situation or condition in order to give it a different meaning. One of the most commonly quoted examples is the way William Shakespeare compares human existence to a stage and set of actors, in his play As You Like It:\nAll the world’s a stage,\nAnd all the men and women merely players;\nThey have their exits and their entrances\nBy comparing life in this way, Shakespeare gives us the impression that life is temporary, beyond our control, and possibly absurd or tragic (or both), thus providing us with a different insight into its true nature.\nA natural language is one that has been allowed to develop independently of any design or intention, as most of the languages that we speak have done. It is the opposite of a ‘constructed’ language, as mentioned above.\nOnomatopoeia is an interesting language phenomenon, and illustrate how diverse the way in which language develops can be. Onomatopoeic words are formed from the sounds associated with what the word denotes, for example, the bleat of a sheep, the honk of a car horn, the sizzle of a saucepan, or the clap of a hand. These words may even end up as the name of that which makes the noise, such as a ‘Cuckoo’.\nOnomatopoeia also indicates that different cultures seem to hear things differently – just look at the words for the noises made by different farm animals (particularly the cockerel!) across different cultures.\nUniversal grammar theory\nThe Universal grammar (UG) theory is a theory proposed by Noam Chomsky, who argued that the human brain is ‘hardwired’ in order to understand grammar, and that we have an instinctive ability to learn the rules of different languages without having to be taught them explicitly. Although Chomsky’s theory has been accepted for many years, it is beginning to be questioned by cognitive scientists and linguists, who point to increasing evidence that children learning to speak languages do not do so via grammatical thinking."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:a24df8c5-a92c-4ac7-85fd-c468f3d4042d>","<urn:uuid:f41f639c-d4bd-470d-ad85-74e1ab6f8f3a>"],"error":null}
{"question":"What are the key differences in blend composition between The Prisoner's Cuttings and Ridge's Geyserville wines from 2016?","answer":"The Prisoner's Cuttings is a blend dominated by 80% Cabernet Sauvignon, with the remaining 20% consisting of Petite Sirah, Syrah, and Zinfandel. Ridge's Geyserville, on the other hand, is primarily made up of 73% Zinfandel, followed by 17% Carignane, 7% Petite Sirah, and 3% Alicante Bouschet.","context":["Deep purple color; black cherry, blueberry, leather on the nose; black cherry, blackcurrant, plum, earthiness, oak spice on the palate.\nDry; behaved tannin and oak—both more elegant than my notes from previous vintage. Full body. Blend of 80% cabernet sauvignon, 20% petite sirah, syrah, zinfandel. The cab delivers richness and depth, the petite sirah and syrah structure and urgency, and the dash of zin spice and smoothness.\nIntriguing how The Prisoner manages its oak program. This spent 18 months in French barrels, 40% new, but this is not an oak monster. Nicely integrated. Balancing acidity (3.8 pH) makes for soft, engaging, easy drinker; 15.1 ABV—slightly less than previous vintage. Good move.\nThe “Cuttings” name comes from the traditional practice of planting vines using cuttings from quality vines and sites, in this case from favorite hillside vineyards of Dave Phinney, who started Orin Swift Cellars in 1998. Phinney grew his next brand, The Prisoner, from 385 cases to 85,000 cases in 10 vintages, an emphatic wine success story.\nPhinney sold The Prisoner brand to Huneeus Vinters in 2010, and Huneeus sold to Constellation Brands in 2016, this vintage. Bill Newlands of Constellation said at the time of the acquisition: “More than ever, consumers are seeking high quality, distinctive wines, and the portfolio we are acquiring from The Prisoner Wine Company delivers.”\nPhinney also sold his Orin Swift Cellars brand to E.&J. Gallo in 2016. Since The Prisoner was a separate brand from Orin Swift, Phinney has now sold his brands to the largest wine company in the world—Constellation (The Prisoner) sells more than 67 million cases (804 million bottles) of wine each year—and Orin Swift to the largest family-owned wine company in the world—Gallo. A pretty nice trick.\nConstellation paid $285 million for The Prisoner Wine Company, and that was only the brand. The wine world cleaves into two camps: wineries that make a consistent style of wine (The Prisoner Wine Company is an example) sourcing from multiple growers, and wineries that strive to express a particular plot of land and the terroir elements that go into that equation. Wine drinkers can sit back and enjoy sipping while debating the existential nuances of each approach.\nChrissy Wittmann is the winemaker at The Prisoner. After earning a bachelor’s degree in Ecology and Systematic Biology at California Polytechnic State University in San Luis Obispo, the future winemaker found her love for wine while analyzing soil and waste water samples in a lab. She returned to Cal Poly and earned a master’s degree in Agriculture. Waste water vs. wine, not a hard call.\nWittmann joined Scheid Vineyards in 2005 as assistant winemaker; two years later she joined the prestigious Wild Horse Winery & Vineyards in Paso Robles. Over the next nine years, Chrissy rose to director of winemaking, while mastering the art of multi-vineyard sourcing. In 2016, Chrissy and her family moved to Napa Valley and joined The Prisoner Wine Company, where her expertise in vineyard partnerships is in play every day. Chrissy says, “The Prisoner Wine Company’s wines have true personalities that start in the vineyards and which are encompassed in the blending of the wine. I’m thrilled to be working with so many vineyards of quality, to tap into my inner wine geek every day.”\nThe Prisoner Wine Company Cuttings Cabernet Sauvignon 2016 is lush, fruit forward, full and lavish in the mouth. This is quintessential big Cali cab. Some will adore this style and its jammy bigness. Others will prefer less flamboyant flavors, alcohol, and oak. Pair with well-marbled steaks; lamb; venison; veal; burgers; meat loaf. $42-55","73% Zinfandel, 17% Carignane, 7% Petite Sirah, 3% Alicante Bouschet\n96 Points – James Suckling, JamesSuckling.com\n95 Points – Antonio Galloni, Vinous Media\n95 Points – William Kelley, The Wine Advocate\n95 Points – Jeb Dunnuck, JebDunnuck.com\nSaturated ruby in color. Plum and black cherry fruit aromas, mint, and gravelly earth. Ripe bramble fruit on entry, balanced acidity, and elegant tannins. Long finish. EB (1/18)\nFollowing four years of drought, winter storms finally delivered more rainfall. The vines responded by setting a full crop. Harvest was completed in the month of September. Once assembled, the wine spent twelve months in barrel integrating the fruit and tannins. This exotic wine shows the great complexity of a field blend. Appealing now, it will reach full maturity within the next fifteen years.\nRidge has made the Geyserville as a single-site zinfandel every year since 1966. The grapes are grown in three adjoining vineyards on a defined stretch of gravelly soil approximately one-and-a-quarter miles long and a half-mile wide.\nRainfall: 31.6 inches (below normal)\nBloom: Mid May\nWeather: A wet December and January helped ease the drought; a very wet March emphatically ended it. Cool weather in April reduced yields a bit. Several heat spikes in late June and late July helped ripen the below normal size crop.\nHarvest Dates: 6 – 30 September\nGrapes: Average Brix 24.9˚\nFermentation: Full Crush, 100% floating cap. Natural primary and natural secondary (malolactic) fermentations; daily pump-overs; pressed at 9 days.\nBarrels: 100% air-dried american oak barrels (20% new, 6% one, 9% two, and 65% three and four years old.)\nAging: Twelve months in barrel\nSustainably farmed, hand-harvested estate-grown grapes; destemmed and crushed; fermented on the native yeasts, followed by full malolactic on the naturally-occurring bacteria; oak from barrel aging; minimum effective sulfur for this wine (35 ppm at crush; 180 ppm over the course of aging); pad filtered at bottling. In keeping with our philosophy of minimal intervention, this is the sum of our actions.\nJamesSuckling.com (Nov 2018): 96 Points “This is impressive for the sense of presence and depth it delivers on the nose and palate with subtly spiced dark plums and cherries, as well as cocoa powder and a superbly composed structure that delivers a seamless, long and fresh finish. A blend of 73 per cent zinfandel, 17 per cent carignane, seven per cent petite sirah and three per cent alicante bouschet. Drink or hold. ” – James Suckling\nVinous Media (Aug 2018): 95 Points\n“The 2016 Zinfandel Geyserville is a powerful, tightly wound wine. Beams of supporting tannin and acidity give the 2016 its distinctly virile, imposing personality. Black cherry, graphite, lavender, and spice are all laced throughout. Deceptively medium in body, the 2016 packs a serious punch. It also needs at least a few years in bottle to settle down, as the tannins are quite imposing, There is a purity to the 2016 that is absolutely striking.” – Antonio Galloni\nJebDunnuck.com (Jun 2018): 95 Points. “The 2016 Geyserville is the usual Zinfandel-dominated blend and includes 17% Carignan, 7% Petite Sirah, and 3% Alicante Bouschet. It offers a gorgeous bouquet of ripe red and black cherries, caramelized plums, toasted spices, and exotic flowers. This gives way to a seamless, medium to full-bodied, elegant Geyserville that has plenty of intensity, sweet tannin, and perfect balance. This beauty just glides over the palate and is one of the more approachable vintages of this cuvée I can remember. Nevertheless, it will keep for at least a decade. It’s a great wine.” – Jeb Dunnuck\nThe Wine Advocate (May 2018): 95 Points. “The 2016 Geyserville Proprietary Red Wine is a real success this year, bursting from the glass with an exuberant bouquet of sweet cherries, blackberries and plums, with subtle hints of the licorice and spice to come with age, and a subtle framing of sweet new oak. On the palate, it’s medium to full-bodied, supple and layered, with a lovely core of ripe, succulent fruit and beautifully fine-grained structuring tannins. The finish is long and pure. There are 23 different grape varieties co-planted in the Geyserville vineyard, but in 2016, Zinfandel dominates at 73%, followed by 17% Carignane, 7% Petite Sirah and 3% Alicante Bouschet.” – William Kelley\nVinous Media (Aug 2017): 91-94 Points. “The 2016 Zinfandel Geyserville is bright, lifted and decidedly medium in body. The flavors are distinctly red toned and the acids are bright. Sweet red cherry, red plum, mint and floral notes add nuance, but it is the wine’s overall energy that really makes an impression. Even at this early stage, the 2016 Geyserville is a knock-out.” —Antonio Galloni\nCalifornia Grapevine (Vol. 44, May 2018): “Medium-dark purplish ruby; attractive, deep, intense, spicy, raspberry and black cherry fruit aroma with hints of cinnamon, dill, cedar, and black pepper; full body; forward, ripe, concentrated, mouth-filling, red and dark fruit flavors with undertones of American oak, and currently tending to be a bit hard on the finish; full tannin; lingering aftertaste. Deserves two to four more years of bottle aging. Very highly recommended.” (Group Score: 16.4, 2/1/0; My Score: 17 [91/100], first place)\nAverage Rating: 91.4\nNo. of Tasting Notes: 44\nView this wine on CellarTracker"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:02369671-a783-41d8-bc43-8c71b2ca6668>","<urn:uuid:2911bd4e-38d6-4b4a-98dc-e3ff75484d41>"],"error":null}
{"question":"How does the historical preservation approach differ between the Pawnee Agency and Boarding School Historic District and the Sod House Museum?","answer":"The Pawnee Agency and Boarding School Historic District, listed in 2000, is preserved as a defined area within the Pawnee Tribal Reserve, roughly bounded by Morris Road, Harrison Street, and Agency Road. In contrast, the Sod House Museum takes a more focused preservation approach by protecting a single structure - the original sod house - under a protective cover while maintaining it in its original location. The museum also includes additional facilities like exhibits, a root cellar, and a separate building for displaying horse-drawn equipment and farm implements to interpret pioneer lifestyle from 1893 to 1920.","context":["National Register of Historic Places listings in Pawnee County, Oklahoma\nThis is a list of the National Register of Historic Places listings in Pawnee County, Oklahoma.\nThis is intended to be a complete list of the properties and districts on the National Register of Historic Places in Pawnee County, Oklahoma, United States. The locations of National Register properties and districts for which the latitude and longitude coordinates are included below, may be seen in a map.\nThere are 12 properties and districts listed on the National Register in the county.\n|||Name on the Register||Image||Date listed||Location||City or town||Description|\n|1||Arkansas Valley National Bank||November 17, 1978\n|547 6th St.\n|2||Blackburn Methodist Church||September 28, 1984\n|D St. and 4th Ave.\n|3||Blue Hawk Peak Ranch||October 10, 1975\n|West of Pawnee on U.S. Route 64\n||Pawnee||2014 boundary increase, refnum 14000428|\n|4||Corliss Steam Engine||May 7, 1979\n|Pawnee County Fairgrounds\n|5||First State Bank of Maramec||June 5, 2007\n|Junction of 2nd Ave. and Hickory St.\n|6||Mullendore Mansion||June 22, 1984\n|910 N. Phillips St.\n|7||Pawnee Agency and Boarding School Historic District||December 28, 2000\n|Pawnee Tribal Reserve, east of Pawnee, roughly bounded by Morris Rd., following Harrison St. and Agency Rd.\n|8||Pawnee Armory||May 20, 1994\n|Junction of 1st and Cleveland Sts.\n|9||Pawnee County Courthouse||August 23, 1984\n|500 Harrison St.\n|10||Pawnee Indian Agency||April 11, 1973\n|Eastern edge of Pawnee\n|11||Pawnee Municipal Swimming Pool and Bathhouse||September 2, 2003\n|1.1 miles north and 0.35 miles east of the junction of U.S. Route 64 and State Highway 18\n|12||Ralston Opera House||July 28, 1987\n|501-503 Main St.\n|Wikimedia Commons has media related to National Register of Historic Places in Pawnee County, Oklahoma.|\n- List of National Historic Landmarks in Oklahoma\n- National Register of Historic Places listings in Oklahoma\n- The latitude and longitude information provided in this table was derived originally from the National Register Information System, which has been found to be fairly accurate for about 99% of listings. For about 1% of NRIS original coordinates, experience has shown that one or both coordinates are typos or otherwise extremely far off; some corrections may have been made. A more subtle problem causes many locations to be off by up to 150 yards, depending on location in the country: most NRIS coordinates were derived from tracing out latitude and longitudes off of USGS topographical quadrant maps created under the North American Datum of 1927, which differs from the current, highly accurate WGS84 GPS system used by most on-line maps. Chicago is about right, but NRIS longitudes in Washington are higher by about 4.5 seconds, and are lower by about 2.0 seconds in Maine. Latitudes differ by about 1.0 second in Florida. Some locations in this table may have been corrected to current GPS standards.\n- \"National Register of Historic Places: Weekly List Actions\". National Park Service, United States Department of the Interior. Retrieved on March 9, 2018.\n- Numbers represent an ordering by significant words. Various colorings, defined here, differentiate National Historic Landmarks and historic districts from other NRHP buildings, structures, sites or objects.\n- National Park Service (2009-03-13). \"National Register Information System\". National Register of Historic Places. National Park Service.\n- The eight-digit number below each date is the number assigned to each location in the National Register Information System database, which can be viewed by clicking the number.","Sod House Museum\n4628 State Highway 8\nAline, OK 73716\nDirector: Renee Trindle\nTuesday through Saturday\n9 a.m. to 5 p.m.\nClosed Sunday, Monday, and state holidays.\nDue to staffing limitations we recommend calling prior to planning a visit; illness and other unexpected events occasionally result in an unplanned closing.\n(up to 6 people)\n|Group Rate (10+)||$5/person|\nChildren (under 6),\nVeterans and Active Military (with ID)\nUse of drones over Oklahoma Historical Society property is not permitted without written approval of the facility director.\nSod House Museum\nAbout the Museum\nSod House Museum seeks to preserve Oklahoma’s only remaining sod house and interprets the early-day lifestyles of a pioneer, from the establishment of the Cherokee Outlet in 1893 to 1920. The museum encloses the original sod house which is the key exhibit. Visitors can enjoy the experience of walking through the “soddy” and explore exhibits, artifacts, photographs, and a root cellar. The artifacts and exhibits portray the daily activities of the pioneers. Museum grounds include an additional building displaying horse-drawn equipment and period farm implements.\nThe museum offers exhibits, tours, educational programs, and events.\n- June 11, 9 a.m.–11 a.m.\n- July 9, 9 a.m.–11 a.m.\n- August 13, 9 a.m.–11 a.m.\n- September 10, 9 a.m.–11 a.m.\n- October 8, 9 a.m.–11 a.m.\n- November 12, 9 a.m.–11 a.m.\n- December 10, 9 a.m.–11 a.m.\nMarshal McCully and the Sod House\nAt one time, thousands of sod houses dotted the plains of North America. This two-room soddy, built by Marshal McCully in 1894, is the only remaining sod house Oklahoma that was built by a homesteader. McCully took part in the largest of Oklahoma’s land runs when the Cherokee Outlet opened for settlement at noon on September 16, 1893. McCully first lived in a one-room dugout, hollowed out of a ravine bank. He built the two-room sod house in August 1894 using blocks of the thick buffalo grass blanketing Oklahoma’s prairies.\nMcCully hitched his team to an fourteen-inch sod plow and split the grass into long rows. Using a flat shovel, he chopped the rows into eighteen-inch sections. He then laid the sod blocks like bricks to form the walls. To make the roof, McCully split poles from the few trees growing in the area and laid them across the top of the walls for rafters. Twelve inches of sod laid on the rafters completed the roof. Unlike many sod houses, McCully plastered the interior walls with alkali clay.\nMarshal McCully received the homestead certificate for the land on April 15, 1902, and the patent on December 31, 1903. The McCully family lived in the sod house from 1894 until 1909 when they built a large two-story frame house. They continued to use the soddy for storage until 1963. On December 31, 1963, exactly sixty years after McCully received his patent, the Oklahoma Historical Society acquired the sod house.\nAlthough the soddy remains in its original location, a cover now protects it from the elements. Visitors can walk through the furnished sod house and imagine what life was like for Oklahoma’s early settlers.\nFarming in Early-Day Oklahoma\nA steel beam rod-breaking plow is on display in the museum. This plowed a twelve-inch wide strip of sod four inches thick. The sod strips were then cut into sod blocks. It took one half-acre of sod to build the two-room sod house, a total of ninety-six tons of sod. It has been estimated that in the United States and Canada there were some one million sod buildings in use from 1903–13.\nOklahoma farmers produced a wide variety of crops including corn, cotton, winter wheat, oats, milo, maize, potatoes, sweet potatoes, peanuts, broomcorn, cowpeas, alfalfa, wild hay, and others. They also produced and sold poultry, eggs, cheese, butter, and garden/orchard products. The main crops by acreage and value, however, were corn, cotton, and winter wheat. An acre was the amount of land tillable by one man behind one ox in one day. Traditional acres were long and narrow due to the difficulty in turning the plow. One acre measures forty rods long and three rods wide.\nIn the 1890s it would have taken Marshal McCully forty to fifty hours of labor to produce one hundred bushels (five acres) of wheat with a gang plow, seeder, harrow, binder, thresher, wagons, and horses. By 1987, it took only three hours of labor to produce one hundred bushels (three acres) of wheat with a tractor, thirty-five-foot sweep disk, thirty-foot drill, twenty-five-foot self-propelled combine, and trucks. Farming has come a long way."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:0b5d8b3f-672a-40ad-8201-430b01a90e48>","<urn:uuid:ebba6dff-fdac-4fb3-91af-7bc7cce63477>"],"error":null}
{"question":"What are the main warning signs of having too little cortisol in the body? Need a quick checklist!","answer":"The main warning signs of having too little cortisol include: fatigue, dizziness, weight loss, muscle weakness, mood changes, and darkening of regions of the skin. This condition can be caused by problems in either the pituitary gland or adrenal gland (known as Addison's disease). This is a potentially life-threatening condition that requires urgent assessment by an endocrinologist (a specialist hormone doctor).","context":["Cortisol is a steroid hormone that regulates a wide range of vital processes throughout the body, including metabolism and the immune response. It also has a very important role in helping the body respond to stress.\nWhat is cortisol?\nCortisol is a steroid hormone, one of the glucocorticoids, made in the cortex of the adrenal glands and then released into the blood, which transports it all round the body. Almost every cell contains receptors for cortisol and so cortisol can have lots of different actions depending on which sort of cells it is acting upon. These effects include controlling the body’s blood sugar levels and thus regulating metabolism, acting as an anti-inflammatory, influencing memory formation, controlling salt and water balance, influencing blood pressure and helping development of the foetus.\nHow is cortisol controlled?\nBlood levels of cortisol vary throughout the day, but generally are higher in the morning when we wake up, and then fall throughout the day. This is called a diurnal rhythm. In people that work at night, this pattern is reversed, so the timing of cortisol release is clearly linked to daily activity patterns. In response to stress, extra cortisol is released to help the body to respond appropriately.\nThe secretion of cortisol is mainly controlled by three inter-communicating regions of the body; the hypothalamus in the brain, the pituitary gland and the adrenal gland. When cortisol levels in the blood are low, a group of cells in a region of the brain called the hypothalamus releases corticotrophin-releasing hormone, which causes the pituitary gland to secrete another hormone, adrenocorticotropic hormone, into the bloodstream. High levels of adrenocorticotropic hormone are detected in the adrenal glands and stimulate the secretion of cortisol, causing blood levels of cortisol to rise. As the cortisol levels rise, they start to block the release of corticotrophin-releasing hormone from the hypothalamus and adrenocorticotropic hormone from the pituitary. As a result, the adrenocorticotropic hormone levels start to drop, which then leads to a drop in cortisol levels. This is called a negative feedback loop.\nWhat happens if I have too much cortisol?\nToo much cortisol over a prolonged period of time can lead to a condition called Cushing's syndrome. This can be caused by a wide range of factors, such as a tumour that produces adrenocorticotropic hormone (and therefore increases cortisol secretion), or taking certain types of drugs. The symptoms include:\nIn addition, there has been a long-standing association between raised or impaired regulation of cortisol levels and a number of psychiatric conditions such as anxiety and depression. However, the significance of this is not yet clearly understood.\nWhat happens if I have too little cortisol?\nToo little cortisol may be due to a problem in the pituitary gland or the adrenal gland (Addison's disease). The onset of symptoms is often very gradual. Symptoms may include fatigue, dizziness, weight loss, muscle weakness, mood changes and the darkening of regions of the skin. Without treatment, this is a potentially life-threatening condition.\nUrgent assessment by a specialist hormone doctor called an endocrinologist is required when a diagnosis of Cushing's syndrome or Addison's disease is suspected.\nOne of the most common requests we get asked each week is for options for low carb meals.\nPart of our answer is that the only meal which is important to be 'strictly' low carb is evening meals and snacks... If you haven't got dinner organised for tonight yet, here's a fantastic chicken option. Enjoy!\nMediterranean chicken with cauliflower rice\nComing up at the start of June is the True Grit obstacle challenge. This event has multiple distance options however we’re putting together a team to run in the 5km.\nEach week on Wednesday night at 6:45, all members are welcome to join us in an epic training opportunity to have you ready for the True Grit Challenge.\nThe picture above is Gwenael who was the only ‘lucky’ member to do the first of the training sessions and check out what he completed... future training routines will be devised by Steve Loulou. With his serious experience in obstacle course racing and personal training experience we can’t wait to see each weeks training plans unfold.\nRegister via the 6X app for the free training sessions.\nRegister for the True Grit event\nLets hear it for Alana.\nEarlier this month Alana finished the 8 week challenge to lose an amazing 10.6kg. Alana joined the challenge with her two sisters, mum and boyfriend.\nAt the end of this month Alana has just managed to top the ladder on the 6XPulse heart rate system racking up 3857 points in 33 classes during April.\nWith a gung-ho attitude and relentless smile when in the studio, we love your enthusiasm and intensity Alana, keep up the massive efforts.\nHaving the most points for the month, Alana has earned herself a Free PT session - Congratulations again!\nWhy do people train? Initially, it's about achieving a goal that is personal to an individual.\nWhy do people continue training? They see results and enjoy what they are doing.\nWe believe it is all about mixing business with pleasure. Although results is what we focus on. We still have fun a long the way. As we all know; It's the journey that tells the story.\nSo why not have the experience of a life time when training rather than making it seem like a chore. So rather than carrying on with what you're doing when you achieve a little milestone, go out and celebrate it. Go out with some mates, go do an adventure race, go to a sorts game, have a cheeky pizza or whatever it may be. Reward yourself and don't think twice.\nWhat will happen from doing this? It makes it easy to be consistent, creates a fun lifestyle, increased positivity and most importantly; memories that can be shared over a life time.\nLet the fun begin.\nBig congrats to this 'brave' crew for attending the latest Secret 6X.\nOn a very stormy night yesterday we travelled into the city to attend the latest exhibition by Body Worlds Vital to experience the human body like never before.\nWhat a great experience!\nFor details and registration for the next Secret 6X, we will be hosting another later this year, don't miss out SECRET 6X\nAnd that's how fast four weeks goes... Hold on tight, the next four goes even quicker!\nLook forward to some events coming up in the next month to have you firing on all cylinders for the finish line of the challenge\nThe leaders in overall weight loss so far are\nFire up team, this is going to be four weeks to remember!\nWe've gained momentum in the challenge this week with members noticing changes in their fitness, strength, endurance and overall ability within the gym.\nIt's amazing just how quickly changes happen when training consistently and in particular one-on-one with your personal trainer. This crew, and many more, are pumping through over 5 training sessions each week which would leave some wondering about the chance of burnout, fatigue and injury however speak to any of these guys and you'll come to realise the balance of cardio, weights, stretching and education has them all fired up to train even more.\nWith the increased fitness we're looking forward to seeing the results continue to speed up in this next week and second half of the challenge, we're really excited to see everyone's half way check-in scans.\nSee you in the gym!\nWeek 2 of the challenge is complete and the results are mixed this week.\nSome lost weight, others gained weights, some lost less than last week... others like this great man lost MORE than the first week!\nShout out to Alex Renwick for owning his challenge this week. After week one, Alex was please with his result of a couple kilos however on writing it on the progress board and witnessing what others achieved in comparison, Alex decided he could do better, and that he did, losing a cool 3.1kgs in the last 7 days.\nStrict meal plans and daily PT has Alex on track and powering towards his goal of 24kg in this 8 weeks.\nProud of you mate!\nSee you in the gym\nOn Saturday, we saw this crazy crew tackle the Sand Dune Challenge and they absolutely smashed it!\nSome met at the studio and carpooled, others knew the way ‘from last time’.\nThe challenge this time, 8 laps of 3 down-hills and 3 up-hills ... and it was tough! Just Steve and Rach came close to finishing the challenge this time with everyone putting in a hug effort with some very sore quads.\nCoffees and a swim finished the Challenge morning before 9:30 and we were home again.\nCongrats to everyone that took part, you all did great!\nSee you in the gym\nThe trainers at 6X are an inspiring, energetic and caring team of fitness professionals, passionate and qualified to help 6X members achieve success in health and fitness"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:61ed55e6-6ee1-4cdf-8d92-a7fd8da8615a>"],"error":null}
{"question":"Could you explain the key differences between SOC for Cybersecurity examination and fraud prevention policies in terms of their approach to security controls and risk management?","answer":"SOC for Cybersecurity and fraud prevention policies differ in several key aspects of their security controls and risk management approaches. SOC for Cybersecurity focuses on enterprise-wide cyber risk management, using two main criteria: descriptive criteria (which provides a narrative of the current risk management program) and control criteria (which serves as an ideal baseline for comparison). It includes standards like NIST Framework and ISO 27001/27002, and results in a formal examination report. In contrast, fraud prevention policies concentrate on physical and financial asset protection through specific operational controls like segregation of duties, pre-screening of employees, regular asset inventory, and strict cash handling procedures. While SOC for Cybersecurity aims to provide verified proof of secure cyber operations, fraud prevention policies focus on preventing theft and misappropriation of parish assets through internal controls and clear procedural guidelines.","context":["Fraud Prevention Policies and Procedures\nIn times of economic distress, acts of fraud are on the rise. Enacting fraud prevention policies and procedures is one way to help prevent these incidents from occurring at your parish.\nWhat is fraud?\nFraud is theft or misappropriation of parish assets for another's gain. Examples include:\n- Mishandling cash or assets\n- Forgery or alteration of bank documents, checks, or financial reports\n- Any other dishonest act involving funds, furniture, fixtures, equipment or supplies\nWhy is it necessary to have a fraud prevention policy?\nIn addition to the financial ramifications, acts of fraud can be damaging to an organization’s reputation. For example, if a parish employee is caught stealing from the offertory, parish leadership will need to work to regain the trust of parishioners who may be reluctant to continue their contributions. Taking proactive steps to prevent fraud is the best defense.\nA policy that clearly defines the steps that management, staff and volunteers should take if fraud is suspected helps those responsible for the stewardship of these assets to fulfill their duties. A formal fraud policy also shows staff and volunteers that the organization is serious about fraud and will prosecute individuals who are caught. Finally, implementing and monitoring these internal controls will keep the parish in compliance with anti-fraud recommendations established by the Archdiocese.\nDevelopment of Fraud Prevention Policy and Procedures\nThe pastor, in collaboration with his business manager and finance council, should develop the relevant policies and procedures, as well as a plan to communicate them to current staff members and volunteers. These parish leaders should also ensure that employees and volunteers abide by the policy and procedures by maintaining regular supervision.\nStaff members and volunteers need to know what to do in the event fraud is suspected, and should be assured that they can communicate their concerns in a confidential manner. The USCCB states, “A strong preventive and detective measure against fraud in an organization is the ability of employees and other constituents to anonymously report suspected wrongdoing without the threat of retaliation.”\nEffective communication of the policy is another key element to its success. In order to ensure understanding, the parish’s fraud prevention policy should be in writing and should be reviewed with each new staff member and volunteer who assists with the stewardship of parish assets.\nImplementation of Fraud Prevention Policy and Procedures\nDuring the Hiring Process\nAll potential employees and volunteers should be pre-screened for:\n- Identity verification\n- Criminal record checks\n- Reference checks\n- Verification of qualifications\nEach parish should have an organization chart with clearly defined roles and reporting lines. In addition, employee job descriptions should include:\n- Authorization levels where necessary\n- Acknowledgement of employee’s duty to report financial discrepancies or suspected fraud and the duty not to disclose any private financial information to a third party\nSegregation of Duties\nIt is important to maintain a system of checks and balances to help prevent fraud. Embezzlement most often occurs when trusted employees have access to both assets and financial records. A fundamental tenet of internal accounting controls is to keep the financial record keeping duties separate from those individuals who have access to assets, particularly cash. For example, bank account statements should be reconciled in a timely manner by someone other than the person charged with making deposits. In addition, the parish administrator should be not be charged with entering transactions into the accounting system and conducting audits of the parish finances; someone other than the parish administrator should perform these tasks.\nWhile only the pastor can authorize checks, limits should be established relative to the commitment of the parish to any expenditure, contract, or petty cash. Written procedures should be kept for all financial areas including money collection, money counting, banking, checks, credit cards, contractor payments, reconciliation, and audits.\nMonetary collections, such as the weekly offertory, should be counted and physically secured immediately. Cash counting and banking should be signed off by at least two people and supervised by a senior parish member at all times and this money should be deposited in a bank account ideally within 48 hours. Finally, the amount of cash to be kept on-hand should be defined in advance and the actual count should be documented.\nWhile many counting rooms or safe areas are located in the rectory, steps can be taken to minimize access to the area. The keys and/or combination to the parish safe need to be restricted to a small number of documented parish managers.\nInventory of Assets\nThe parish should maintain an asset register and/or inventory sheet listing all physical and financial assets owned by the parish. An inventory of physical assets should be conducted on a regular basis, at least annually.\nThe parish should prepare annual budget forecasts for regular operations. Budgets and actual financial performance should be compared for discrepancies.\nReview of Financial Reports and Budgets\nParish Finance Council meetings should be conducted on a regular basis to discuss financial reports and budgets. Minutes of these meetings should be prepared and kept on file.\nProtection of Sensitive Data\nIdentity theft and other forms of data breach are a serious concern for all organizations. All staff should comply with the Commonwealth's recently enacted privacy law: . This regulation establishes minimum standards to be met in connection with the safeguarding of personal information contained in both paper and electronic records.\nThe following recommendations should help ensure the protection of data.\n- Files containing financial and personnel information should be stored in lockable, fire-rated cabinets in a secure storage room.\n- The key register should be properly maintained and kept up-to-date. In particular, it should be updated whenever personnel changes occur.\n- Computers should be password-protected with complex passwords that are at least eight characters long and should include letters, numbers and special characters. Computers should be set to automatically lock if unused for more than 10 minutes.\n- Computers should be protected with firewalls and regularly updated antivirus software.\n- When disposing of paper documents that contain sensitive information, documents should be shredded first.\nData protection procedures should be regularly reviewed and amended as necessary.\nRegular Review of Policy and Procedures\nCompliance reviews should be conducted for the following control areas:\n- Account balances and accounting records\n- Parish and personnel practices\n- Duty segregation practices\n- Data and data protection practices\nIf controls are deemed ineffective or outdated, revised controls must be established and implemented.\nFraud is a serious concern. It is vital that parish leadership and staff be vigilant in adhering to internal controls to help prevent fraud. If you have any questions, please contact the Director of Finance at 617-746-5878.","Listen to: \"A Closer Look at SOC for Cybersecurity for a Better Understanding\"\nWith the number and type of cybersecurity threats rapidly increasing over the past decade or so—the threats are showing no signs of slowing in the near or far future, by the way—you’ve most likely heard of SOC for Cybersecurity; especially if you outsource one or more aspects of your business to a specialized service organization. In case you haven’t heard about this framework, we are here to bring you up to speed.\nWhat Is SOC for Cybersecurity Examination and Do You Need it for Your Business?\nWith anticipated security threats like crime-as-a-service (CaaS) and the inherent security risks to the IoT and the supply chain, it seems like every business—in every industry and at every size—is going to have to tighten up cybersecurity.\nSOC for Cybersecurity may be the solution everyone has been scrambling to find to combat the potential risks on the horizon.\nAny business that works with service organizations, which specialize in services that include cloud storage and software-as-a-service (SaaS), must ensure that each service organization maintains a safe and secure cyber environment.\nWith the increasing need for organizations to demonstrate proper management of cybersecurity threats with effective processes and controls in place to detect, respond to, mitigate and recover from breaches and other security events, the American Institute of Certified Public Accountants (AICPA) designed a cybersecurity risk management reporting framework.\nThe resulting framework is a key component of the new System and Organization Controls (SOC) for Cybersecurity engagement.\nSide note to curious IT professionals: You might find yourself wondering, “Didn’t SOC once stand for “Service Organization Controls?” If that is the case, you are correct. SOC for Cybersecurity is the cyber controls set and described by each organization’s enterprise-wide cyber risk management program.\nGiven the known and anticipated risks of doing business via the Internet, we believe it is important that you adopt the SOC for Cybersecurity framework for your business. It is a great way to monitor and manage your internal controls, as well as keeping your management and board members, investors, analysts, clients and prospective clients, business partners and industry regulators abreast of the health of your system on a regular basis.\nWhy Did the AICPA Design SOC for Cybersecurity?\nThe AICPA took a look at the cyber environment and realized that it could do something to help all businesses manage the risks. The body designed SOC for Cybersecurity to work as a reporting mechanism for any organization, rather than solely for service organizations that provide services to client organizations, also known as user entities. All other SOC reporting options designed by the AICPA—SOC 1, 2 and 3 examinations—are only intended for service organizations. With this new reporting option, everyone can work with a consistent reporting mechanism for assurance regarding its cybersecurity controls.\nWhat Do You Need to Know About the SOC for Cybersecurity Examination?\nThe SOC for Cybersecurity goes hand-in-hand with the framework. It offers guidelines on the best ways for you to document your own cybersecurity risk management program. It also provides a number of controls and objectives that you may use to stay on track for the best possible cybersecurity.\nAdditionally, the examination presents standards for public accounting firms. These standards allow the firms to report on the cybersecurity programs while also giving clear guidance to CPAs to provide cybersecurity assurance to clients.\nWhat Are the Primary Components of SOC for Cybersecurity Examination?\nThe AICPA designed SOC for Cybersecurity using two basic and important criteria:\n- Descriptive Criteria. Descriptive criteria provide a narrative description of the company’s current risk management program and approach. This key step gives a baseline measurement of the effectiveness of the current controls within the program.\n- Control Criteria. Control criteria are the ideal baseline against each company is able to compare their own baseline measurement within their descriptive criteria to determine where they stand, such as how near to or far from the mark they are.\nUser entities can choose from a few different pre-existing control criteria, also known as a family of standards, including:\n- Trusted Services for Security, Availability and Confidentiality\n- NIST Critical Infrastructure Cybersecurity Framework\n- ISO 27001/27002\nThe SOC for Cybersecurity examination report will ultimately include:\n- Management’s description of their organization’s cybersecurity risk program\n- Management’s assertion about their cybersecurity risk program\n- Practitioner’s report\nSide note: Keep in mind that there is a difference between SOC for Cybersecurity and risk assessments. The main difference between the two is that a risk assessment measures an organization’s exposure against a specific collection of threats through an evaluation. SOC for Cybersecurity offers an independent opinion regarding an entity’s complete risk management program methodology and practices, which also includes its own risk assessment process.\nA Few Key Benefits of the SOC for Cybersecurity Examination and Reporting\nThere are several benefits associated with adopting the most appealing SOC for Cybersecurity framework for your business and submitting to SOC for Cybersecurity examination and reporting.\n- You provide verified proof that your organization is dedicated to operating within the most secure possible cyber environment.\n- You can stay a step ahead of the competition by taking this extra step that provides assurance to your customers, business associates and any other interested or invested parties.\n- Catch potential issues before they become full-blown data breaches.\nThe Importance of Your Auditor’s Role in SOC for Cybersecurity\nScheduling a SOC for Cybersecurity audit is an important part of implementing your framework, making sure it is running properly and obtaining verified proof that you are doing everything possible to run a safe computing environment in the context of the cyber realm.\nOur team at I.S. Partners, LLC. has championed this new step made by the AICPA to help our clients stay ahead of the curve when it comes to online operations. However, it is understandable that you may feel overwhelmed at the prospect of undergoing one more adoption and implementation of more controls. We can help you sort through it all to make the process easier."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:7644d01c-db45-4c0d-b536-078092b59b93>","<urn:uuid:42e942fe-83b5-4f7f-9271-d141bcfc04b8>"],"error":null}
{"question":"Could you compare the historical significance of DNA fingerprinting in criminal cases, as exemplified by Colin Pitchfork's case, with Jack's story of DNA genealogy databases - what do these cases reveal about the evolution of DNA technology in solving crimes?","answer":"Both cases demonstrate major milestones in DNA technology's role in justice, but with different applications. Colin Pitchfork was the first criminal caught using DNA fingerprinting in 1987, after raping and murdering two girls. His case established DNA as a groundbreaking forensic tool. In contrast, Jack's case shows the modern evolution of DNA technology through commercial databases like 23andMe and AncestryDNA, which enabled familial DNA searches to solve a decades-old mystery about his true identity. While Pitchfork's case demonstrated DNA's power in directly matching a suspect to a crime, Jack's story reveals how DNA databases now allow broader familial searches to uncover historical truths, though this raises new ethical questions about genetic privacy and database access that weren't relevant in the early days of DNA forensics.","context":["“How do you like the name Jack?” the woman on the phone asked.\nOn April 26, 1964, a nurse came into the hospital room of Dora Fronczak, who had just given birth to her young son, Paul. She told Mrs. Fronczak that it was time to take the baby to the nursery (at that time newborns did not stay in the room with the moms), took the baby, and left. A few hours later, another nurse came into the room to take young Paul to the nursery. It was then that everyone realized a mother’s worst fear: Her infant had been stolen.\nAuthorities were able to determine how the woman left the hospital and that she got into a cab, but they were never able to find the woman. However in 1965, a small toddler-aged boy was found, abandoned outside a store in New Jersey. Blood tests were not inconsistent with him being Paul Fronczak (DNA testing was not available), and there were no other missing children cases in the area that were matches. The little boy was sent to Chicago as Paul Fronczak and the case was closed.\nHowever, as an adult Paul Fronczak, began to suspect that the couple who raised him were not his biological parents, and in 2012 Paul underwent DNA analysis to test his suspicions. The results showed that indeed, he was not the biological son of Dora and Chester Fronczak. His next step was to enlist the help of a genetic genealogist to assist him in finding his true biological parents and his identity.\nBy conducting “familial searches” using commercially available DNA databases like 23andMe and AncestryDNA and many resources, the genealogist’s group found a match to his DNA on the east coast. Further ground work, discovered that this family was indeed Paul’s…now Jack.\nThe knowledge of Jack’s true identity, didn’t bring with it a joyous union of the adoptive family who had raised and loved Jack (as Paul) with the biological family who had pined for him over the years as many might imagine.\nJack’s biological parents and one of his siblings had passed away, and much of his story with them. The story that remains is one of mystery and intrigue. He has a twin sister, and he and his twin sister went mysteriously “missing” around age two. His biological parents told the father’s side of the family that the twins were with the mother’s family, and not to ask questions. Similarly, they told the mother’s side of the family that the twins were with the father’s family and also not to ask questions. The disappearance was never reported. Family albums that had pictures of the twins were altered to remove any evidence that the children ever existed. Jack, does not know what became of his twin sister, Jill.\nJack knows his biological origins, but he doesn’t know why he was abandoned or what happened to his twin sister. The Fronczaks still do not know what happened to Paul Joseph Fronczak, stolen from his mother’s arms that day in 1964. A stolen baby case now cold again. A new mystery and another missing person. At the center of it all lies DNA.\nWhen Answers Yield Questions\nThis story illustrates the power of asking questions to find the truth. It’s what scientists do. Day in. Day out. This story also illustrates what happens when you ask questions to find the truth: Truths can be confusing, sometimes unpleasant and often lead to more questions. Many of us who have pursued training in the sciences relish the messy answers and puzzles. However, many people are not so comfortable with answers that muddy the picture. We are taught in school that questions have answers. We take tests and get things right or wrong. So, it is not surprising that much of the general public enters into DNA testing with an expectation of definitive answers that are clear and actionable.\nJack’s story illustrates the power of DNA familial searches when combined with resources, determination and skill. It is also brings to mind many ethical questions about the accumulation of genetic information in commercial DNA biobanks like those being built by 23andMe and AncestryDNA. Ethical concerns and discussions of guideline development, protections of rights, etc. have swirled around official databases such as CODIS for years. We have even written about how familial searching works and its promise and ethical dilemmas in the criminal forensics environment. However, little discussion has happened around commercial DNA databases and biobanks.\nSuch discussion would include questions about access, protections and informed consent. Who has access to the information in these biobanks for research? How are the individual identities associated with the genomes protected? Can the systems be hacked from outside? What are the guidelines for gaining informed consent when someone mails swab in for testing? How does the business ensure that the sample was collected voluntarily? What obligation do they have to do so? Can a sample be collected by someone with guardianship or power of attorney?\nAlthough these biobanks have noble goals of connecting people with their past or helping people better understand their health risks, there is little protection governing the misuse of the information in these databases. For instance, could insurance companies use these genetic databases to identify populations based on geography or ethnicity that have higher proportions of genetic alleles that are associated with chronic diseases? Then could such information be used to deny people from these populations coverage for these diseases, even though the associations that are known are not cause-effect, may be heavily dependent on environmental influences and the totality of an individual’s genetic background?\nThe technology and information gathering have progressed exponentially, while the understanding of or even the willingness to discuss the risks involved in maintaining such information databases and biobanks have proceeded at a snail’s pace.\nJack’s story is powerful, and like any powerful story, it spurs discussion and imagination. If you are interested in this story and others like it, consider attending the International Symposium on Human Identification, ISHI28 in Seattle, WA, USA this year.\nFurther Reading on the Fronczak Case\nBreakthrough in Paul Fronczak mistaken identity kidnap case, Las Vegas TV report says http://chicago.suntimes.com/news/breakthrough-in-paul-fronczak-mistaken-identity-kidnap-case-las-vegas-tv-report-says/ [accessed 4/20/2017]\nPaul Joseph Fronczak The Charley Project website http://www.charleyproject.org/cases/f/fronczak_paul.html [accessed 4/20/2017]\nVideo news report about the case https://www.youtube.com/watch?v=C8gzjGhyJqY [accessed 4/20/2017]","Top 10 Significant Firsts in Law\nThe law influences all of us in our daily lives. We have become so used to it that we seldom think about it. There was once a time that the laws and trappings of law that we are now familiar with did not exist. This is a list of the top 10 significant firsts in legal history.\n10. First Patent Laws\nThere is evidence suggesting that something like patents were used among some ancient Greek cities. The creator of a new recipe was granted an exclusive right to make the food for one year, and a similar practice existed in some Roman cities. Patents in the modern sense originated in Italy in 1474. At that time the Republic of Venice issued a decree by which new and inventive devices, once they had been put into practice, had to be communicated to the Republic in order to obtain the right to prevent others from using them. England followed with the Statute of Monopolies in 1623 under King James I, which declared that patents could only be granted for “projects of new invention.” [Wikipedia]\n9. First Copyright Laws\nCopyright was not invented until after the advent of the printing press and with wider public literacy. As a legal concept, its origins in Britain were from a reaction to printers’ monopolies at the beginning of the eighteenth century. In Britain the King was concerned by the unregulated copying of books and used the royal prerogative to pass the Licensing Act of 1662. Which established a register of licensed books and required a copy to be deposited with the Stationers Company, essentially continuing the licensing of material that had long been in effect. The Statute of Anne was the first real copyright act, and gave the author rights for a fixed period, after which the copyright expired. Internationally, the Berne Convention in 1887 set out the scope of copyright protection, and is still in force to this day. We have come a long way from the original intention of Copyright to the modern RIAA style. [Wikipedia]\n8. First Law of Universal Suffrage\nIn 1893, New Zealand became the first major self-governing country in the world to give women the vote. This was the first time in western history that women had as much power as men did to change the laws of the land through their political representation. Women’s suffrage was granted after about two decades of campaigning by women such as Kate Sheppard and Mary Ann Müller and organisations such as the New Zealand branch of the Women’s Christian Temperance Union. They felt that female voting would increase the morality of politics. A special mention should be made of Lydia Taft, who, in 1756, was the first woman in America to legally vote (though this was a singular case, and universal suffrage did not occur in the United States until the 19th Amendment was ratified in 1920.)\n7. First Jury\nThe concept of a modern jury trial stems back at least to Magna Carta, which gave English nobles and freemen the right to be tried by a panel of their peers. Rather than by summary judgment of the king or other official who often had the utter power to impose his own arbitrary judgment. On the other hand, some criminal defendants today may prefer a bench trial if they believe that a jury would be over-influenced by emotional testimony. The concept can also be traced to Normandy before 1066, when a jury of nobles was established to decide land disputes. In this manner, the Duke, being the largest landowner, could not act as a judge in his own case. Many ancient cultures had similar concepts; notably ancient Judea whose panel of judges called the Sanhedrin served a similar purpose. The Athenians by 500 BCE had also invented the jury court, with votes by secret ballot. These courts were eventually granted the power to annul unconstitutional laws, thus introducing judicial review. [Wikipedia]\n6. First Use of Habeas Corpus\nBlackstone (an English Jurist) cites the first recorded usage of habeas corpus ad subjiciendum in 1305, during the reign of King Edward I. however; other writs were issued with the same effect as early as the reign of Henry II in the 12th century. Blackstone explained the basis of the writ, saying:\nThe King is at all times entitled to have an account, why the liberty of any of his subjects is restrained, wherever that restraint may be inflicted.\nThe procedure for the issuing of writs of habeas corpus was first codified by the Habeas Corpus Act 1679, following judicial rulings, which had restricted the effectiveness of the writ. A previous act had been passed in 1640 to overturn a ruling that the command of the King was a sufficient answer to a petition of habeas corpus. In modern times, Habeas Corpus has been suspended in cases of war. [Wikipedia]\n5. First Lawyers\nThe earliest people who could be described as “lawyers” were probably the orators of ancient Athens. However, Athenian orators faced serious structural obstacles. First, there was a rule that individuals were supposed to plead their own cases, which was soon bypassed by the increasing tendency of individuals to ask a “friend” for assistance, and secondly, orators were not allowed a fee for their service. A law enacted in 204 BC barred Roman advocates from taking fees, but the law was widely ignored. Emperor Claudius, who legalized advocacy as a profession and allowed the Roman advocates to become the first lawyers who could practice openly, abolished the ban on fees. Very early on, unlike Athens, Rome developed a class of specialists who were learned in the law, known as jurisconsults (iuris consulti). Jurisconsults were wealthy amateurs who dabbled in law as an intellectual hobby; they did not make their primary living from it. By the start of the Byzantine Empire, the legal profession had become well established, heavily regulated, and highly stratified. In the words of Fritz Schulz, “by the fourth century things had changed in the Eastern Empire: advocates now were really lawyers.” [Wikipedia]\n4. First Criminal Caught Through DNA\nColin Pitchfork was the first criminal caught based on DNA fingerprinting evidence. Pitchfork raped and murdered two girls in Narborough, Leicestershire, on November 21, 1983, and on July 31, 1986. He was arrested on September 19, 1987, confessed, and was sentenced to life in prison on January 23, 1988. On November 21, 1983, 15-year-old Lynda Mann left her home to visit a friend’s house. She did not return. The next morning, Lynda was found raped and strangled on a deserted footpath known locally as the Black Pad. Using forensic science techniques available at the time, a semen sample taken from her body was found to belong to a person with type A blood and an enzyme profile that matched only 10 percent of males. With no other leads or evidence, the case was left open. Three years later another body was found. The modus operandi matched that of the first attack, and semen samples revealed the same blood type. Leicestershire police and the FSS then undertook a project where 5,000 local men were asked to volunteer blood or saliva samples. This took six months, and no matches were found. Later, a man named Ian Kelly was heard bragging that he had given a sample while masquerading as his friend, Colin Pitchfork. Pitchfork, a local baker, was arrested at his house in the neighboring village of Littlethorpe and a sample was found to match that of the killer. He was sentenced to life in prison. [Wikipedia]\n3. First Use of Fingerprinting\nThere is no clear date at which fingerprinting was first used. However, significant modern dates documenting the use of fingerprints for positive identification are as follows:\n14th century Persia: various official government papers had fingerprints (impressions), and one government official, a doctor, observed that no two fingerprints were exactly alike.\n1823: Jan Evangelista Purkyn?, a professor of anatomy at the University of Breslau, published his thesis discussing 9 fingerprint patterns, but he did not mention the use of fingerprints to identify persons.\n1880: Dr Henry Faulds published his first paper on the subject in the scientific journal Nature in 1880. Returning to the UK in 1886, he offered the concept to the Metropolitan Police in London but it was dismissed.\n1892: Sir Francis Galton published a detailed statistical model of fingerprint analysis and identification and encouraged its use in forensic science in his book Finger Prints.\n1892: Juan Vucetich, an Argentine police officer who had been studying Galton pattern types for a year, made the first criminal fingerprint identification. He successfully proved Francisca Rojas guilty of murder after showing that the bloody fingerprint found at the crime scene was hers, and could only be hers. [Wikipedia]\n2. First Police\nIn Western culture, French legal scholars and practitioners developed the contemporary concept of a police paid by the government in the 17th and early 18th centuries, notably with Nicolas Delamare’s Traité de la Police (“Treatise on the Police”), first published in 1705. The German Polizeiwissenschaft (Science of Police) was also an important theoretical formulation of police. In 1667, the government of King Louis XIV created the first police force in the modern sense in the largest city in Europe. The royal edict, registered by the Parliament of Paris on March 15, 1667 created the office of lieutenant général de police (“lieutenant general of police”), who was to be the head of the new Paris police force. That defined the task of the police as “ensuring the peace and quiet of the public and of private individuals, purging the city of what may cause disturbances, procuring abundance, and having each and everyone live according to their station and their duties”. Gabriel Nicolas de la Reynie, who had 44 commissaires de police (police commissioners) under his authority, first held this office. [Wikipedia]\n1. First Codified Laws\nThe history of law is closely connected to the development of civilizations. Ancient Egyptian law, dating as far back as 3000 BC, had a civil code that was probably broken into twelve books. It was based on the concept of Ma’at, characterized by tradition, rhetorical speech, social equality and impartiality. Around 1760 BC under King Hammurabi, ancient Babylonian law was codified and put in stone for the public to see in the marketplace; this became known as the Codex Hammurabi. However like Egyptian law, which is pieced together by historians from records of litigation, few sources remain and much has been lost over time. The influence of these earlier laws on later civilizations was small. The Old Testament is probably the oldest body of law still relevant for modern legal systems, dating back to 1280 BC. [Wikipedia]\nThis article is licensed under the GFDL. It uses material from the Wikipedia articles cited above."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:6fee88e3-94f4-4f2d-bd12-222091eced99>","<urn:uuid:08e4d6b4-0802-4510-94c6-507fa82686c7>"],"error":null}
{"question":"Hey, mining expert here - what's the specific advantage of using HDG wire mesh panels over bare steel wire in blast zones?","answer":"Hot-dip galvanized (HDG) wire mesh panels have been found to withstand blasting better than bare steel wire within affected blast zones, resulting in fewer panels needing replacement after each blast. Additionally, the zinc and iron alloy layers formed during the HDG process are up to 60% harder than bare steel wire, providing extra durability.","context":["Demix Quarry Modernization\nLaval, QC Canada | 2017\nOnly a miner knows what it’s like to be chasing a vein while thousands of feet underground, blasting through hundreds of tons of solid rock each day in pursuit o precious metals. This is not a job for the meek or claustrophobic or those who need to see a little daylight or breathe fresh outdoor air in a 12-hour shift. Rather it takes strong will to hang your tag and make the journey deep underground facing the challenges that could be waiting in the darkness.\nMining is a tight-knit family of men and women that rely on each other to ensure “everyone goes home every day.” The durability of hot-dip galvanized wire mesh plays a small, but vital role in making this possible.\nMining is a tight knit family of men and women that rely on each other to ensure that “everyone goes home every day.” The Stillwater Mine, along with the nearby East Boulder Mine located outside Nye, Montana are owned and operated by Sibanye Stillwater and are the lone palladium and platinum producing mines in North America. These two mining operations extract, processes and refine palladium, platinum, gold and associated metals.\nNo industry understands the importance of safety and training more than mining. Thousands of tons of heavy wire mesh panels are secured to the walls and ceilings throughout every tunnel in the mine to assist in providing the necessary support to the exposed rock of the mine excavations, thus creating a safe work environment. This painstaking process includes drilling 5’ deep holes into the solid rock and then driving rock bolts through bearing plates affixed to the wire and into the drilled holes, securing the steel mesh panel in place.\nAfter each blast, new safety wire is secured in place to protect the blasting team before the subsequent blast. Wherever electrolytes (moisture) are present, the resulting corrosion and consequential weakening or potential failure of the wire mesh is of grave concern. This corrosion concern is resolved by utilizing hot dip galvanized (HDG) wire along with galvanized rock bolts and bearing plates, providing decades of protection in this highly corrosive environment.\nThe hot-dip galvanized wire provides an added benefit as the initial zinc and iron alloy layers that form in the HDG process are significantly harder (up to 60%) than the bare steel wire, thus providing additional durability. A recent discovery by the mine has determined the galvanized wire is able to withstand the blasting better than the bare steel wire within the affected blast zone. This results in fewer panels needing to be replaced after each blast. The incomparable advantage of long-lasting protection from corrosion and the added benefit of superior strength for this critical ground support component far outweigh the cost of hot-dip galvanizing.\nIt’s safe to say that miners will keep chasing that vein and hot-dip galvanizing will fill a small but vital role in making sure that everyone goes home!\nNye, MT United States\nCoating Durability, Corrosion Performance, Ease of Specifying, Initial Cost, Life-Cycle Cost, Prior HDG Experience, Sustainability, Turnaround Time\nWire Mesh Panels, Rock bolts & Bearing Plates\nF & H Mine Supply\nAZZ Galvanizing - Denver\nAZZ Galvanizing - Arizona\nThank you! Your vote has been accepted."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:5ff94e67-0a5e-4294-861f-4b46b3dbc1ec>"],"error":null}
{"question":"How do the information collection methods differ between PEST analysis and Competitive Analysis? Por favor explain the key differences.","answer":"PEST analysis and Competitive Analysis use distinct approaches to information collection. PEST analysis involves examining broad macro forces through seven lenses (STEEPLE variation): Social, Technological, Economic, Environmental, Political, Legal, and Ethical factors. It requires looking at various aspects like demographics, cultural trends, technology adoption, economic situations, regulations, and environmental concerns. In contrast, Competitive Analysis follows a more structured procedure focused specifically on competitors, involving five main components: price, quality, customer service, financial stability, and location. The process includes defining the problem, analyzing the situation, obtaining specific data, interpreting the data, generating solutions, and designing an action plan. While PEST takes a broader macro-environmental view, Competitive Analysis is more focused on direct market competitors and specific business aspects.","context":["The longer, broader view. Take a look at SWOT’s neglected sibling.\nPEST* analysis is the somewhat neglected sibling of SWOT analysis, which is a shame because it is an excellent tool for analyzing various macro forces that critically impact a businesses. While SWOT is good for targeting near-term priorities, PEST looks longer, broader view. As such, PEST is a great approach for identifying new ideas, opportunities and interesting topics for discussion.\nIn a sales and consulting environment, PEST can be applied to identify outlying topics that broaden a customer or client’s thinking and change their perception of the person they are dealing with. The person becomes more of a business consultant, more than a product or service provider.\nThe letters of PEST represent Political, Economic, Social and Technological. Over the years, marketers and business consultants have refined and re-shaped PEST to resulting in multiple variants such as DESTEP, PESTEL and STEEPLE. Different derivatives of the model may be more suited to different industries (hospitality versus software say) and scenarios (new business business unit or new product).\nHere we look in more depth at our preferred variation: STEEPLE.\n- Population demographics\n- Cultural trends\n- Artificial intelligence\n- Research & development\n- Climate change\n- Weather seasonality\n- Interest rates\n- Consumer protection\n- Labor laws\n- Safety standards\n- Wage equality\n- Safe working\n- Social responsibility\n- Community relations\nIt takes just a few minutes to scroll through each of the 7 lenses above, and come up with a question for each. Applying STEEPLE to a hotel business for example might result in the following questions:\nHow will increased concern over health and cleanliness impact our cleaning costs and demand for shared leisure facilities?\nHow can we better leverage mobile technology in our hotel cars to improve the customer experience?\nWith a fall in overseas visitors, what should we be doing to attract more domestic customers?\nWhat could we do to reduce the amount of water we use and become more water independent?\nHow would an increase of 1% in sales tax / VAT impact our customers and our business?\nHow do we ensure that our sub-contractors comply with labor laws?\nWhat are our policies and processes for ensuring wage equality between sexes?\nNote that in the above example it could be argued that the question under Political relating to taxation could be argued to be an Economic question. It doesn’t matter! The key is to use the framework to broaden your thinking and come up with interesting questions and topics for discussion.\nTaking these questions to your clients and customers helps them think about topics that they may not have considered and positions you as someone who a) cares about the future of their business and b) thinks broadly and commercially – traits that are highly valued in business.\nSee below for an expanded list of considerations under each heading:\n- Demographics – Population size and growth rate. Birth rates. Death rates. Life expectancy. Age distribution.\n- Wealth distribution. Social classes. Income disparity.\n- Marriage and divorce rates. Family size.\n- Immigration and emigration rates.\n- Attitudes to immigrants and minorities.\n- Education levels. Crime levels.\n- Lifestyle trends and leisure interests\n- Consumer attitudes and opinions\n- Media views\n- Fashion and role models\n- Ethnic/religious factors\n- Cultural norms and values\n- Mobile phone penetration\n- Drug proliferation\n- Economic situation and trends\n- Employment level and disposable income\n- Skill level / productivity of workforce\n- Level of government intervention in the economy\n- Growth rate, exchange rates, interest and inflation rates\n- Savings and credit levels\n- International trade/monetary issues\n- Overseas economies and trends\n- Specific industry factors\n- Market routes and distribution trends\n- Efficiency of financial markets\n- Infrastructure quality\n- Business cycle stage (e.g. prosperity, recession, recovery)\n- Political stability\n- Government stability/instability\n- Corruption level\n- Press freedom and independence\n- Government involvement in business\n- Competition regulation\n- Level of government subsidies\n- Bilateral relationships and favored trading partners\n- Government policies\n- Government term and change\n- Funding, grants and initiatives\n- Home market lobbying / pressure groups\n- International pressure groups\n- International memberships\n- Pricing regulations\n- Wars and conflict\n- Brand reputation\n- Transparency and accountability\n- Racial and sexual equality\n- Gender tolerance and equality\n- Money laundering\n- Impact on communities\n- Minimum wage\n- Artificial intelligence\n- Data security and privacy\n- Automation / worker productivity\n- Research funding\n- Replacement technology\n- Technology capacity and life-cycle\n- Innovation levels\n- Technology incentives and government support\n- Internet and communication infrastructure\n- Copyright, patent and licensing\n- Technology adoption rate\n- Weather, climate and climate change\n- Seasonality/weather issues\n- Natural disasters, mitigation measures and disaster recovery plans\n- Energy and water security\n- Air, water, noise and light pollution\n- Waste levels and disposal\n- Recycling efforts\n- Energy mix\n- Water use and recapture\n- Community attitudes\n- Worker participation and community engagement\n- Support for renewable energy\n- Pressures from NGO’s\n- Legal and regulatory regime\n- Legal framework for contract enforcement\n- Intellectual property protection\n- Litigation environment\n- Legal administrative complexity and timescales\n- Trade regulations and tariffs\n- Foreign exchange restrictions\n- Regulatory bodies and processes\n- Laws on:\n- Consumer protection\n- Copyright and patent\n- Health and safety\n- Data protection\n- Waste and recycling\n- Anti-money laundering\n* The origins of PEST are generally traced back to Francis Aguilar, a professor at Harvard Business School. He outlined the less easily remembered ETPS approach in his 1967 book “Scanning the Business Environment”. The fours lenses are: Economic; Technical; Political and Social.","Presentation on theme: \"3.05 Employee Marketing-information to develop a marketing plan\"— Presentation transcript:\n1 3.05 Employee Marketing-information to develop a marketing plan Conduct Market AnalysisConduct SWOT AnalysisConduct Competitive Analysis\n2 Market AnalysisWhat Is It? An evaluation of the market for a company's goods and services. For example, a company might be interested in the characteristics of consumers who are buying the firm's products, or a comparison of its products with those offered by competitors.Why do we need It? Before you can describe your marketing and sales strategies, you need to figure out what market you serve and what need you fulfill.\n3 Components of a Market Analysis Target Segment DescriptionTarget Segment’s Needs (Why do customers need the product/service?)Distribution channels (Where do customers need the product/service?)Buying Habits (What factors influence the target market to buy?)Communication channels (How will you communicate with the customers?)\n4 Advantages of Using a Spreadsheet for Market Analysis Keep your market numbers organized.It helps you track the basic numbers of potential customers by segment, with columns to estimate growth rates and the projected future numbers.\n5 Steps for Conducting Marketing Analysis 1) What do you want to know?2) Draft your questions and hypotheses for how people will answer.3) Find the right group of people for your market research.4) Determine the best method to ask your questions.5) Analyze your findings.\n6 CONDUCT SWOT Analysis for use in the Marketing Planning Process SWOT analysis is a strategic planning technique that analyzes a company’s internal STRENGTHS and WEAKNESSES, and studies OPPORTUNITIES and THREATS in the external sales environment.Who to involve in a SWOT Analysis?For those who want to improve the competitiveness of a company, region or country.People directly involved in various hierarchical levels of decision making in an organization or business, or a wider sample of actors if the SWOT analysis concerns a whole region or nation. Representatives from a variety of stakeholders groups should be involved, as they would bring in the analysis their own particular perspectives. At least one expert in SWOT analysis should take part or moderate the process.\n7 WHEN SHOULD A SWOT ANALYSIS BE CONDUCTED? SWOT analysis is done as part of the overall corporate planning process in which financial and operational goals are set for the upcoming year and strategies are created to accomplish these goals.When a company wants to get a picture of how the company should position itself against competitors.When seeking out new opportunities and forecasting longer term opportunitiesTo help companies be better prepared for whatever it will encounter in the external environment.\n8 BENEFITS OF A SWOT ANALYSIS The main advantages of conducting a SWOT analysis:Little or no costAnyone who understands your business can perform a SWOT analysisWhen you don't have much time to address a complex situationAnother advantage of a SWOT analysis is that it concentrates on the most important factors affecting your business. Using a SWOT, you can:understand your business betteraddress weaknessesdeter threatscapitalize on opportunitiestake advantage of your strengthsdevelop business goals and strategies for achieving them.\n9 FACTORS CONSIDERED IN A SWOT ANALYSIS Internal FactorsChanges in spending on other parts of the company’s marketing and promotional mixChanges in what is happening in different territories or marketsChanges in sales management practicesChanges in number of salespeople supervised by one personExternal FactorsIntensity of competitionTotal market potentialConcentration of potential high-volume buyersGeographic distribution of customers\n10 PROCEDURES FOR CONDUCTING A SWOT ANALYSIS Step 1 – Information collection – In the here and now…List all strengths that exist now. Then in turn, list all weaknesses that exist now. Be realistic but avoid modesty!You can conduct one-on-one interviews. Or get a group together to brainstorm. A bit of both is frequently best.You’ll first want to prepare questions that relate to the specific company or product that you are analyzing. You’ll find some questions and issues below to get you going.When facilitating a SWOT – search for insight through intelligent questioning and probing\n11 PROCEDURES FOR CONDUCTING A SWOT ANALYSIS (cont’d) Step 2 – What might be…List all opportunities that exist in the future. Opportunities are potential future strengths. Then in turn, list all threats that exist in the future. Threats are potential future weaknesses.Step 3 – Plan of action…Review your SWOT matrix with a view to creating an action plan to address each of the four areas.\n12 Benefits of Competitive Analysis Identify Your Own Competitive Strengths - You'll discover your company's competitive advantage -- the reason customers do business with you instead of your competition. Determine Areas of Weakness and Stimulate Innovation - Analyzing competitors' offerings may spur ideas for innovative improvements to your product offerings.Understand Pricing Better - Are you charging 50% below market for your products and services? Are you charging double what the competition is charging? The only way to find out is determine what competitors are charging. With competitor pricing in hand, you can make smarter decisions with respect to your own pricing.\n13 Benefits of Competitive Analysis (cont’d) Find Untapped Opportunities - You might find that there are some categories of customers whose needs are not being met. For example, if you plan to prepare and deliver gourmet meals, you may discover that a particular part of town is not currently being served. If you can satisfy unmet needs, you'll develop a market \"niche.\"Learn Industry Best Practices - By observing the actions of your competitors, you might learn more about your market. For example, does a successful competitor offer reduced prices during a particular season? If so, what might that tell you about your market's spending habits?Steer Clear of Overly Competitive Markets - If you find that your market is saturated with capable competitors, you can avoid the costly mistake of starting a business without adequate demand. You can then redirect your efforts toward something that will pay off instead. (For example, your research may tell you that there's an ample number of thriving gourmet meal services in your targeted market area already.)\n14 Components of a Competitive Analysis PriceQualityCustomer ServiceFinancial StabilityLocation\n15 Procedures for Conducting a Competitive Analysis Defining the problemAnalysis of the situationObtaining data that is specific to the problemAnalysis and interpreting the dataFostering ideas and problem solvingDesigning a plan"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:6c386d4a-e7eb-47b7-8ae0-d928d821973d>","<urn:uuid:84cf435b-96c4-42d7-a4d2-72770021efba>"],"error":null}
{"question":"How does infant sleep pattern relate to solid food introduction, and what is the proper timeline for starting complementary foods?","answer":"The belief that early solid food introduction helps babies sleep through the night is a myth. Sleep pattern changes are a natural progression as infants age and often coincidentally align with early solid introduction. Babies typically experience growth spurts at 3-4 months, 6-7 months, and 9-10 months, which may cause night wakings regardless of solid food consumption. Regarding timeline, while some pediatricians suggest starting at 4 months, the American Academy of Pediatrics, CDC, and WHO recommend exclusively offering breast milk or formula until 6 months of age. After 6 months, start with fortified baby cereals and pureed foods, gradually increasing from one teaspoon to one tablespoon twice daily. By 8-10 months, babies can progress to more varied foods including pureed meats, proteins, and appropriate finger foods.","context":["How Will You Know When Your Baby is Ready to Eat Solid Foods?\nHow do you know if your baby is ready for solid foods? Your baby may be 3 months old or 4 months old when you start to feel she may need \"something more\" than formula or breastmilk. Maybe she is beginning to awaken more often at night or eat more often than \"usual\" and you wonder if introducing solid foods may be what she needs.\nA Growth Spurt May be Confused with a Readiness for Solid Foods\nPlease keep in mind that a growth spurt will occur between 3-4 months of age. Your baby may begin to wake more frequently at night for a feeding and/or may begin to eat non-stop (cluster feed) as she once did as a newborn. This growth spurt often accounts for the increased hunger in your baby and it should not be taken as a sign that your baby needs solid foods added to her diet!\nTry offering your baby more frequent nursing sessions and/or bottle feedings instead of solids; you will find that within a week or two, your baby is oftentimes over the growth spurt and back to feeding \"as usual\".\nHere are a few \"signs\" that may indicate your baby is ready for Solid Foods:\nLoss of tongue-thrust reflex - This allows baby to drink and swallow liquids with ease; with the tongue-thrust reflex still present, baby may simply drink in liquidy purees or push the food back out. According to Dr. JimSears, in the first four months the tongue thrust reflex protects the infant against choking. When any unusual substance is placed on the tongue, it automatically protrudes outward rather than back. Between four and six months this reflex gradually diminishes, giving the glob of cereal a fighting chance of making it from the tongue to the tummy\nAbility to let you know she is full from a \"meal\" with signs such as turning away from the bottle or breast. This is important so that baby is able to self-regulate the amount of food being eaten. This helps stop baby from accidentally overeating as parents may continue to feed baby thinking that she is still hungry.\nAbility to sit up and hold head up unassisted\nInterest in your food (we tend to disagree with this one as when a baby reaches the age of 4-6 months, he is interested in putting everything in his mouth!)\nDoubling of birth weight\nFrequently waking in the middle of the night when a solid sleeping pattern had been established. This may not be the best indicator that your baby is ready for solids! Please keep in mind that a growth spurt will occur between 3-4 months of age, 6-7 months of age and also 9-10 months of age. Baby may also be waking due to an illness or teething.\nI know Many Baby's Who Started Solids \"Early\" - Why Shouldn't My Baby?\nMany parents say that their own pediatricians or their friends' pediatricians have said that it's fine to start solids (typically cereal) at 4 months of age. It is still common for pediatricians to just say \"start solid foods when your baby is 4 months old\" because this has been the norm for many years. You will find that the vast majority of pediatricians are not experts in pediatric nutrition and many are not aware of their own governing body's guidelines!\nThe American Academy of Pediatrics (AAP) acknowledges that there are no \"strict\" age guidelines on introducing solid foods to your baby. However, the AAP, along with the CDC and the WHO do recommend that you offer nothing but breast milk and/or formula until you baby is at least 6 months old.\nThis ensures optimal nutritional exposure and may stave off food allergies amongst other issues. Further studies have shown that an infant's gastrointestinal tract has not or may not have matured enough to properly digest/utilize solid foods until around 6-8 months old! (See page end for reference links)\nStudies show that babies are highly individual in developing a readiness for solid foods. One baby might seem to be ready for solids at 4 months, while another shows no signs of readiness until around 6 or 7 months. Just because your friend's baby may have began eating solid foods at 4 months of age does not mean that your baby should. Don't be pushed into starting solids and don't feel like you are a \"bad Mommy\" if you feel your baby is ready prior to 6 months of age!\n**Please keep in mind that \"outward\" signs of being ready for solids do not mean that your baby's inner digestive system is mature and ready! You should thoroughly discuss starting your baby on solid foods with your baby's pediatrician.\nIf your pediatrician insists that you start your 4 month old infant on solids, ask him or her to explain the benefits of starting solids early. You might be surprised to hear the answer is not based on nutritional science!\nAnd remember, you never HAVE to begin introducing complementary foods simply because your pediatrician has suggested that you do so! Only when you have thoroughly discussed the pros and cons of introducing solid foods with your pediatrician will you be able to have a better grasp of just when you should begin offering baby solid foods.\nWon't My Baby Sleep Through the Night If We Start Solids?\nSome parents believe that if they start solids \"early\" then their infants will sleep through the night sooner. As your baby grows, his sleeping patterns as well as eating patterns change continually.\nAround the time a few parents begin to offer solids early is just about the time that an infant may be sleeping for longer periods at a time. This is a natural progression as an infant ages and it oftentimes coincides with the addition of early solids. This coincidence perpetuates the dangerous myth that early offerings of solid foods will help an infant sleep \"through the night\".\nTo further this explanation, let us recall that between 6-8 months old, baby is often back to waking at night for a feeding. By this time baby should be eating solids and it appears that those solids are no longer helping baby sleep through the night. In reality, baby is hitting another growth spurt and may wake again during the night for more feedings regardless of eating solids! This really is \"normal\" and your baby may wake again during the night for more feedings regardless of eating solids!\nThe best advice when considering starting solid foods for your baby, \"Watch the Baby - Not the Calendar!\" This is true for both breastfed and formula fed infants. Follow your baby's hunger cues and you'll never go wrong!\nMy Parent's Insist That My Baby Needs \"Real Food\"\nSome parents may be tempted to give in to relatives, grandmothers and sometimes even their own mothers, who say \"Give that baby some real food, she's starving!\" or \"Nursing that baby isn't enough to, he needs some real food\".\nRemember that \"real food\" is breastmilk and/or formula and these contain all the important nutrients that an infant needs to develop properly! Breast milk in particular, and/or formula, will be enough to sustain your baby's nutritional needs for up to age 1 year old! In fact, introducing solids too early may displace the important nutrition your baby needs to receive from breast milk and/or formula!","Solid And Finger Foods For Babies\nAs you have probably learned by now, having a baby is a great adventure that is packed with surprises and milestones all along the way.\nYour baby reaching the solid and finger foods milestone is another exciting time that also comes with a bit of apprehension. Of course you want to do everything right, but how and when should you start?\nRead Next: How To Introduce Your Baby To New Flavours\nWhen Should I Start?\nBefore your baby is six months old you should never feed any type of solid food. The digestive tract is still developing during this time so solids can wreak havoc.\nIf you think your baby might be ready for solids, look out for:\n- Baby is holding her head up well\n- She can sit in a highchair\n- Baby has started to make chewing motions\n- The birth weight has at least doubled\n- Baby has started to show some interest in food\n- She can close her mouth around a spoon\n- Baby can move the tongue front to back, but does not push food out of the mouth\n- She can move food to the back of the mouth\n- Your baby still seems hungry after feeds\nRead Next: Everything You Need To Know About Weaning Your Baby\nHow Should I Start?\nYou should start off by feeding your baby the same amount of milk feeds as usual, while introducing fortified baby cereals, and various types of highly pureed foods. Some good options include apples, peaches, sweet potatoes, bananas, pears, and squash. Offer just a small amount in the beginning, roughly about one teaspoon. You may want to mix a teaspoon of baby cereal with about four or five teaspoons of breast milk, so that it is very runny and easy to swallow.\nOver time, you can gradually increase the amount of solid food to about one tablespoon, or one tablespoon of cereal thinned with milk, offered twice a day. You can also gradually thicken the cereal’s consistency over time by adding less milk.\nIf you find your baby does not want to try the cereal or food being offered, do not get upset. Simply hold off for a few days and then try the item again. Sometimes the baby may just not be ready and may need a bit more time to adjust.\nRead Next: A Solid Start: The Three Stages Of Weaning\nBy the age of eight to ten months, your baby will be ready for a more varied diet, which can include finger foods. You can introduce pureed meats, small amounts of proteins such as tofu, mashed beans, and eggs. Finger foods can include small pieces of soft ripe fruit, such as banana, lightly toasted pieces of bread cut up into bite sized pieces, pasta that is well cooked, cereals, and teething biscuits. Regardless of the types of foods you offer your baby, be sure to introduce one new food at a time. You should wait a minimum of three days before introducing another new food to ensure there are no allergic reactions.\nRemember, introducing solid and finger foods should be an exciting, fun time in both your life and your baby’s life. Do not rush the process, and never force your baby to eat anything he or she does not want to eat.\nWhat are your tips to introducing your baby to solid and finger foods? Share them in the comments below."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:f8ad2af7-3bf9-4ec6-af8e-3a2c4b62ac23>","<urn:uuid:483002d6-17bd-4475-9b5e-d1bafc598d15>"],"error":null}
{"question":"Why did Jeroboam fear that the kingdom would revert to the House of David? Really curious about the political dynamics!","answer":"Jeroboam feared that if people continued going to the Temple of Yahweh in Jerusalem to offer sacrifices, their hearts would turn back to their lord, Rehoboam king of Judah, and they would put him to death.","context":["26 Jeroboam thought to himself, 'As things are, the kingdom will revert to the House of David.\n27 If this people continues to go up to the Temple of Yahweh in Jerusalem to offer sacrifices, the people's heart will turn back again to their lord, Rehoboam king of Judah, and they will put me to death.'\n28 So the king thought this over and then made two golden calves; he said to the people, 'You have been going up to Jerusalem long enough. Here is your God, Israel, who brought you out of Egypt!'\n29 He set one up at Bethel,\n30 and the people went in procession in front of the other one all the way to Dan. In Israel this gave rise to sin, for the people went to Bethel to worship the one, and all the way to Dan to worship the other.\n31 He set up shrines on the high places and appointed priests from ordinary families, who were not of levitical descent.\n32 Jeroboam also instituted a feast in the eighth month, on the fifteenth of the month, like the feast kept in Judah, when he offered sacrifices on the altar. This he did at Bethel, offering sacrifices to the calves which he had made and, at Bethel, installing the priests of the high places which he had set up.\n33 Jeroboam did not give up his wicked ways after this incident, but went on appointing priests for the high places from the common people. He consecrated as priests of the high places any who wished to be.\n34 Such conduct made the House of Jeroboam a sinful House, and caused its ruin and extinction from the face of the earth.\n6 Like our ancestors, we have sinned, we have acted wickedly, guiltily;\n19 At Horeb they made a calf, bowed low before cast metal;\n20 they exchanged their glory for the image of a grass-eating bull.\n21 They forgot the God who was saving them, who had done great deeds in Egypt,\n22 such wonders in the land of Ham, such awesome deeds at the Sea of Reeds.\n1 And now once again a great crowd had gathered, and they had nothing to eat. So he called his disciples to him and said to them,\n2 'I feel sorry for all these people; they have been with me for three days now and have nothing to eat.\n3 If I send them off home hungry they will collapse on the way; some have come a great distance.'\n4 His disciples replied, 'Where could anyone get these people enough bread to eat in a deserted place?'\n5 He asked them, 'How many loaves have you?' And they said to him, 'Seven.'\n6 Then he instructed the crowd to sit down on the ground, and he took the seven loaves, and after giving thanks he broke them and began handing them to his disciples to distribute; and they distributed them among the crowd.\n7 They had a few small fishes as well, and over these he said a blessing and ordered them to be distributed too.\n8 They ate as much as they wanted, and they collected seven basketfuls of the scraps left over.\n9 Now there had been about four thousand people. He sent them away\n10 and at once, getting into the boat with his disciples, went to the region of Dalmanutha.\nReading 1, Sirach 48:1-4, 9-11: 1 Then the prophet Elijah arose like a fire, his word ... Responsorial Psalm, Psalms 80:2-3, 15-16, 18-19: 2 over Ephraim, Benjamin and Manasseh; ... Gospel, Matthew 17:10-13: 10 And the disciples put this question to him, 'Why then do the ... continue readingMore Daily Readings\nThe New Jerusalem Bible (NJB) is a Catholic translation of the Bible published in 1985. The New Jerusalem Bible (NJB) has become the most widely used Roman Catholic Bible outside of the United States. It has the imprimatur of Cardinal George Basil Hume.\nLike its predecessor, the Jerusalem Bible, the New Jerusalem Bible (NJB) version is translated \"directly from the Hebrew, Greek or Aramaic.\" The 1973 French translation, the Bible de Jerusalem, is followed only \"where the text admits to more than one interpretation.\" Introductions and notes, with some modifications, are taken from the Bible de Jerusalem.\nSource: The Very Reverend Dom (Joseph) Henry Wansbrough, OSB, MA (Oxon), STL (Fribourg), LSS (Rome), a monk of Ampleforth Abbey and a biblical scholar. He was General Editor of the New Jerusalem Bible. \"New Jerusalem Bible, Regular Edition\", pg. v."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:b988cbbb-afa9-4717-bfa7-4957aa30e8c4>"],"error":null}
{"question":"What was the original purpose of the Merrie Melodies cartoon series when it was introduced in 1931?","answer":"Merrie Melodies cartoons were initially intended to be music videos of their day, serving as animated commercials for the Warner Bros-owned sheet-music library.","context":["\"What's up, Doc?\"Looney Tunes and Merrie Melodies were two series of theatrical cartoon shorts running from 1930 to 1969. Warner Bros. initially distributed the cartoons for independent producer Leon Schlesinger before buying the studio in 1944 and moving it in-house in 1955.Originally, as the names indicate, these cartoons were meant to riff off the sweet, sentimental musical shorts then in vogue: for instance, Disney's Silly Symphonies. That basing cartoons around popular public-domain songs — or, even better, ones the studio already owned — was a fast and relatively cheap way of producing them didn't hurt any, either.The first set, Looney Tunes, was introduced with 1930's \"Sinkin' in the Bathtub\" featuring minstrel-like mascot Bosko, the Talk-Ink Kid, and for its first decade relied more heavily on recurring characters and thus lower budgets. Merrie Melodies, introduced in 1931's \"Lady, Play Your Mandolin!\" featuring the (suspiciously Mickey Mouse-esque) character \"Foxy\", were initially intended as the music videos of their day, basically animated commercials for the Warners-owned sheet-music library.When Looney Tunes switched to color in 1942, and the Merrie Melodies line ditched the music around the same time in favor of its own rising star — one Bugs Bunny — differences between the two were limited to their distinctive theme songs, until 1964 (when both series wound up using the same theme music as a result of using a modernized, and slightly bizarre, opening/closing sequence).Over the course of their tenures at 'Termite Terrace', as the WB animation studio was informally known, the legendary directors Chuck Jones, Tex Avery, Bob Clampett, Friz Freleng, Frank Tashlin, Art Davis, and Robert McKimson — assisted by talented animators such as Ken Harris, Emery Hawkins, Abe Levitow, Bill Melendez, Virgil Ross, and Rod Scribner; brilliant writers like Warren Foster, Michael Maltese, and Tedd Pierce; ace musical arrangers Carl Stalling and Milt Franklyn, sound effects whiz Treg Brown; and of course the incomparable Mel Blanc — created and refined a large and diverse cast of characters, the most famous of which include (listed in chronological order of introduction):\nLooney Tunes Main Cast\n...along with dozens of lesser known and one-shot characters. Quite nearly all of these were voiced by Mel Blanc, the Man of a Thousand Voices; in fact, that was used as a gag in at least one short. Other WB voice artists included Stan Freberg, June Foray, Daws Butler, Bea Benaderet, Billy Bletcher, Arthur Q. Bryan (the voice of Elmer Fudd), and Robert C. Bruce (who narrated most of the \"travelogue\" and \"newsreel\" shorts).For more detailed information on the recurring cast, refer to the franchise's character sheet.The cartoons starring this pantheon originated many of the classic Animation Tropes, co-opting or perfecting most of the rest. Being primarily character-driven comedy, with the various stars working and reworking their shticks solo or in combination, their comedic style is firmly rooted in vaudeville, early Broadway, and silent-film slapstick — an ancestry they cheerfully acknowledged: as in Robert McKimson's 1950 short \"What's Up Doc?\", an Animated Actors look at Bugs's rise to stardom by way of Elmer Fudd's vaudeville act.The freewheeling house style was also heavily influenced by, well, the house movies. Answering accusations of excessive violence from parental action groups in later years, Jones noted that these shorts were originally intended to ride with such sweet, wholesome family fare as Little Caesar and The Public Enemy. \"We didn't make them for kids,\" he explained. \"We made them for ourselves.\"Helping the anarchistic spirit along were a succession of humourless bosses that more or less invited open rebellion. Founder Schlesinger won unwitting immortality as the inspiration for Daffy Duck's trademark lisp: \"You're dethpicable!\". The Warner Bros. themselves really didn't know or care what was going on in their animation unit, leaving hands-on oversight to bean counter Eddie Selzer. Recounting the genesis of the classic \"Bully for Bugs\", Jones recalled the day Selzer showed up at his door as he and writer Mike Maltese were hashing out story ideas, and bellowed: \"I don't want any pictures about bullfights! Bullfights aren't funny!\" Then Selzer marched off, leaving his dumbfounded staff staring at each other. \"Well,\" Maltese said, \"Eddie's never been right yet...\"Warners ceased production of the classic series in 1963 and outsourced new cartoons to other entities in something of a Dork Age until 1969; a Revival of new production of the classic cartoons occurred during the '90s. Moving to television in 1960 with the original incarnation of the The Bugs Bunny Show, the Warners' shorts took a level in ubiquity. Various repackagings became staples of the American Saturday morning schedule for the next forty years, reintroducing themselves through the generations, until they had permanently entered the collective consciousness.\"Looney Tunes\", the generic term by which all Warners animation is now known and sold, is a brand name more than anything nowadays, but is most heavily associated with the \"classic\" theatrical shorts. The Tunes have been the mascots of the Six Flags theme parks for years.The merchandising for Looney Tunes products ceased production when AOL ended its merger with Time Warner in order to save money (it did the complete opposite), and Cartoon Network hasn't been kind to the Tunes until November 2009, when they began running the classic shorts again.It is impossible to discuss the impact of animation on any culture in the world without mentioning these characters and their famous shorts. They have a global influence equaled only by the Classic Disney Shorts. Not only by dint of their quality and originality, but by the scope of their exposure, Looney Tunes have influenced every corner of the animated world. In the 1940's in particular, nearly everybody copied their antics—even Disney tried their hands at Warners-esque comedy from time to time!For a complete filmography of the original cartoons, visit this page. For a taste of the best shorts the series has to offer, refer to The 50 Greatest Cartoons list, as well as The 100 Greatest Looney Tunes list. For the 2011 animated sitcom that premiered on Cartoon Network, go here. For the video game starring Bugs Bunny and Taz in a time-traveling adventure, go here.\n- Porky Pig — \"I Haven't Got a Hat\", 1935, Freleng. The Everyman and Straight Man to the rest of the cast, known for his ridiculously thick stutter. Also a Deadpan Snarker, usually when paired with Chuck Jones' pompous Daffy or a Butt-Monkey when paired with the crazy, hyperactive screwball Daffy. Either way, he does not like being paired up with Daffy.\n- Daffy Duck — \"Porky's Duck Hunt\": 1937, Avery. Was originally a screwball/Cloud Cuckoo Lander, later Flanderized into a jerkass with an enormous ego. In this incarnation, he's used either to parody action-adventure heroes, or paired up and serving as a foil for Bugs in an Odd Couple scenario. Later also joined Sylvester on the hunt for Speedy Gonzales. First named in the short \"Daffy Duck and Egghead\".\n- Granny — \"Little Red Walking Hood\" 1937, Avery. A kind, elderly woman most remembered as Tweety's owner, and who packed a hidden amount of badass-ery when inflicting pain on Sylvester when he tried to catch Tweety.\n- Elmer Fudd — \"Elmer's Candid Camera\", 1940, Jones. One of only three humans in the regular cast, the others being Yosemite Sam and Tweety's owner Granny. The Butt-Monkey, often Too Dumb to Live. An avid hunter, thus Jones' favourite adversary for both Bugs & Daffy, reaching a peak in the iconic Rabbit Season trilogy. Less popular with the other directors — particularly Freleng — who found him too wimpy. To compensate, the other directors often made Elmer crafty in their pictures; see \"Quack Shot\" by Robert McKimson, where he's one step ahead of Daffy the entire cartoon, and \"Hare Brush\" by Friz Freleng, where it's debatable that he faked being insane in order to both avoid the IRS and get revenge on Bugs Bunny. Surprisingly, Elmer didn't appear as frequently as most people think, only encountering Bugs in over 30 pictures out of Bugs' 168 short lineup.\n- Bugs Bunny — \"A Wild Hare\", 1940, various, notably Avery. A famous, snide, Brookyln/Bronx-accented Karmic Trickster and cultural icon. For decades, always considered the \"main character\" and \"star\" of the core cast.\n- Tweety Bird — \"A Tale of Two Kitties\", 1942, Clampett. \"I tawt I taw a puddy tat!\" In Bob Clampett's hands, Tweety was a pink, sadistic trickster who used his wits to get rid of cats. Under Friz Freleng, Tweety became yellow (the Hays Office balked because the pink made him look naked), found a recurring adversary in Sylvester, and often depended on an umbrella-wielding Granny or an angry bulldog to get rid of the \"bad old puddy tat\". Time has seen modern generations often mistake Tweety for a female (this doesn't happen in Spanish-speaking countries, as its local name, \"Piolín\", is unequivocally male).\n- Pepe Le Pew — \"Odor-Able Kitty\", 1945, Jones. A Funny Foreigner and Handsome Lech, completely oblivious to his body odor problem... and thus to why all the pretty 'young ladiee skonks' keep running from him in disgust. Of course, the fact that they're nearly all actually cats, unaware that they've had white stripes painted on their backs, doesn't help either. Can at times be a Depraved Bisexual: Pepé has gone after a male cat who was painted up as a skunk in his first cartoon, a white-striped Sylvester at the end of 1954's \"Dog Pounded\", and accidentally made out with a man on a Tunnel of Love ride in 1951's \"Scent-imental Romeo.\" Based in part on characters made famous by actor Charles Boyer.\n- Sylvester the Cat — \"Life With Feathers\", 1945, Freleng. A cat with a speech impediment who usually tries to eat Tweety or Speedy Gonzales, with little success, making him a mild version of the Villain Protagonist. One of the most versatile of the ensemble, prone to neuroses and usually the star of the comic melodramas. In Robert McKimson's hands, slobby Sylvester has a hyper-articulate son named Sylvester, Jr., whom Dad tries to impress by chasing what turns out to be a baby kangaroo; when he retreats gibbering at the \"giant mouse!\" Junior is mortified. Also known for a trio of spooky cartoons in which he is Porky Pig's pet, where, despite being The Voiceless for these shorts, Sylvester attempts to convey to his master that their lives are in danger (twice from murderous mice, once from a curious alien); unfortunately, Porky is Captain Oblivious for most of this, believing Sylvester to be cowardly and paranoid, and only in the first short of the trio does he realize the truth.\n- Yosemite Sam — \"Hare Trigger\", 1945, Freleng. A brash little outlaw with handlebar mustachios and a severe temper problem, introduced as 'a more worthy adversary' for Bugs than the meek Elmer. Said to be a caricature of his short, brash, redheaded creator. Introduced as a Wild West bandit, he eventually became the stock blowhard villain character: Civil War general, Viking, pirate, Black Knight (no Python references please), politician, Arab sheik, etc. Oddly enough, he wears his bandit mask no matter what role he plays. Said to have been inspired by Chuck Jones' great-uncle, a short, redheaded retired Texas Ranger.\n- Foghorn Leghorn — \"Walky Talky Hawky\", 1946, McKimson. A loud, obnoxious rooster with a Southern accent, based on Kenny Delmar's 'Senator Claghorn' radio character. Considers himself the life of the party; demonstrates by tricking little Henery Hawk out of capturing him, abusing the barnyard dog by whomping his ass with a wooden board and painting his tongue green, or babysitting a genius chick named Egghead, Jr. in order to cozy up to his widow hen mother.\n- Marvin The Martian — \"Haredevil Hare\", 1948, Jones. An Ineffectual Sympathetic Villain who wants to see an Earth-Shattering Kaboom, and is the Trope Namer thereof. Invariably foiled by Bugs. Like the Tasmanian Devil, he only appeared in a handful of shorts from the original shorts, but became popular enough to be featured in nearly every adaptation thereafter. His universe was expanded in the 2000s animated show Duck Dodgers. A CGI film starring Mike Myers as Marvin was planned in 2008 and ultimately shelved.\n- Wile E. Coyote and the Road Runner — \"Fast and Furry-ous\", 1949, Jones. A speedy bird and the coyote who uses a variety of backfiring Acme Company traps and mail-order gadgets to try to catch him — 'try' being the operative word. The coyote was named in his first face-off against Bugs (Operation Rabbit), where he became \"Wile E. Coyote, Super Genius\". The Road Runner remains mute (aside from his iconic \"beep beep!\") to this day. Incidentally, Time Warner Cable for a long time used them as the mascot for their \"Road Runner\" internet service; no longer the case since the company was spun off as independent from Time Warner in 2009.\n- Speedy Gonzales — \"Cat-Tails for Two\", 1953, McKimson. Another Funny Foreigner and good-natured Trickster who moves at Super Speed to help his poor Mexican mouse friends get cheese from \"el gringo pussygato\" (usually Sylvester). Has a lethargic cousin named (inevitably) \"Slowpoke Rodriguez\" who uses a gun to incapacitate cats instead. For obvious reasons, the Speedy shorts — particularly the late 1960s ones with Daffy as his antagonist — tend not to be received well by animation fans and historians. Ironically, despite being blacklisted for a while in the U.S. for stereotyping, he's the most popular Looney Tunes character in Mexico.\n- The Tasmanian Devil — \"Devil May Hare\", 1954, McKimson. The destructive, hurricane-spinning, Extreme Omnivore who talks in Hulk Speak when he talks at all. Though he only appeared in five Golden Age-era cartoons, he is nowadays considered as popular as Bugs Bunny and Daffy Duck, having been nicknamed Taz and often appearing in merchandise, comic book stories, and even his own TV spinoff (Taz-Mania).\n- Witch Hazel — \"Bewitched Bunny\", 1954. A parody of the Wicked Witch trope who was always in a delightfully flighty mood, and who was interested in cooking, fashion, and gossip. Usually acted as a Big Bad for Bugs and/or Daffy when trying to cook them into her soup cauldron.\n- Michigan J. Frog — \"One Froggy Evening\": 1955, Jones. A frog from The Gay '90s is discovered by a man in modern times. Unfortunately, the frog acts as his Not-So-Imaginary Friend. Listed here as an honorable mention, as he only ever appeared in two cartoons (one a direct sequel to the other) which he didn't share with any other iconic characters, and was never really iconic himself until he became the mascot for the WB Network in the 90's.\nFor tropes about Looney Tunes in comics, go here. See also Noteworthy Looney Tunes Staff for info on the many people who contributed to this franchise.\nTrope Namer For:\n- Acme Products (indirectly) - the Coyote's quest to catch the Roadrunner with gadgets inevitably purchased from the Acme Corporation\n- And Call Him \"George\"! - the Abominable Snowman\n- Duck Season, Rabbit Season - Bugs and Daffy, respectively.\n- Earth-Shattering Kaboom - Marvin's snit fit in \"Hare-Way to the Stars\" when Bugs foils his attempt to clear the Earth out of his view of Venus: \"Where's the Kaboom? There was supposed to be an Earth-shattering kaboom!\"\n- Elmuh Fudd Syndwome\n- Had the Silly Thing in Reverse\n- Hair-Raising Hare - a Chuck Jones-directed Bugs cartoon from 1946 (which marked the debut of Gossamer)\n- Mexicans Love Speedy Gonzales\n- One Buwwet Weft (formerly)\n- Porky Pig Pronunciation\n- Pronoun Trouble (possibly; the phrase turns up in \"Rabbit Seasoning\", but refers to a series of variations on Duck Season, Rabbit Season and has nothing to do with gender)\n- Road Runner vs. Coyote\n- Seen-It-All Suicide - one of many reasons why the Looney Tunes get edited on TV\n- That's All, Folks! - but often subverted, see below\n- Where's the Kaboom? - \"There was supposed to be an Earth-Shattering Kaboom!\"\n- Wrong Turn at Albuquerque\nLooney Tunes Tropes (Troperifficus Merriemelodieus):\n- Tropes A to C\n- Tropes D to F\n- Tropes G to I\n- Tropes J to L\n- Tropes M to O\n- Tropes P to R\n- Tropes S to U\n- Tropes V to Z\nTh-th-the-th-th-the-th-th-that's all, folks!"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:70a9ffb0-0216-49dd-8a75-fa489889ecbd>"],"error":null}
{"question":"Hey economics folks! Can you explain how market economies work and what causes economic cycles at the macro level? 💫","answer":"In market economies, individuals and businesses exchange goods freely based on supply and demand, with producers and consumers determining what is produced and sold. Prices rise when consumer demand increases, leading to increased production to satisfy demand. At the macroeconomic level, economies operate in cycles. As positive influences promote prosperity, increased demand may trigger higher prices, which can suppress the economy as households restrict spending. When supply begins to outweigh demand, prices may drop, leading to renewed prosperity until the next shift in economic supply and demand.","context":["Breaking Down Economies\nAll activity that relates to the trade, production, and consumption of services and goods is encompassed by an economy within a specified area. Through a combination of hierarchical or collective decisions and market, transactions are how these decisions are made. This process has everyone from corporations, families, individuals, entities, corporations, and governments participating. The laws, geography, culture, history, and several other factors are how the economy of specific countries or regions is governed and the actions and choices of those participating are how it evolves. This is why there are no two economies that are exactly the same.\nAccording to supply and demand, individuals, as well as businesses, are able to exchange goods freely using the market due to market-based economies. What is produced and sold is determined through producers and consumers in the United States due to it being mostly a market economy. Consumers will decide and buy what they will buy and how much they will pay for an item, while producers will choose what is produced and sold.\nThe laws for supply and demand use these decisions above in order to total production and determine what prices will be. The price of items rises when the consumer demand increases for a specific good, as consumers are more willing to pay more, the price will rise as well. This will lead to production increasing in order to satisfy consumer demand as producers are driven by the profits they make. This typically results in a market economy balancing itself out naturally. When the prices for a sector of the industry rise from demand, the labor and money needed to fill that demand will shift into places as needed.\nCentral planning and some government intervention prevent there being any pure market economies. The United States can even be considered a mixed economy due to the slight differences. In order to fill in the gaps left from a market economy, social security benefits, regulations, and public education are provided by the government in order to create balance. This results in the term market economy referring to an economy that is more market-oriented in general.\nCommand-based economies, on the other hand, rely on a central political agent, which will control the distribution of goods and the price of the goods. Imbalances are common in this system as supply and demand are not able to naturally play out due to this system being centrally planned.\nEconomics is known as the study of economies as well as the factors that affect economies. There are two major aspects that the discipline of economics can be broken down to, which are macroeconomics and microeconomics.\nThe entire economy, focusing on the larger issues and decisions is what macroeconomics studies. The study of factors that affect the economy as a whole such as the inflation on the economy and the effects of rising prices. The GDP or gross domestic product, which is a representation of an economy’s total amount of services and goods produced, is also a part that macroeconomics studies. Finally, macroeconomics covers the changes in the national income and changes in unemployment. Basically, you could say that macroeconomics is studying the way that aggregate economy behaves.\nAs for microeconomics, this focuses on the firm’s and individual’s behavior in order to determine why they make the economic decisions that they do and how these decisions will affect the larger economic system. How individuals cooperate and coordinate with one another along with the reason that various goods have different values are also a part of microeconomic studies. The focus of microeconomics is often on economic tendencies, for example how changes in production are impacted by individual actions and choices.\nConcept of Economy History\nThe word economy comes from the Greek language and it means “household management”. Originally the study of economics began in ancient Greece by philosophers, most notably Aristotle, but the modern study of economics did not start until the 18th century in Europe, particularly in France and Scotland.\nAdam Smith, an economist and Scottish philosopher wrote a famous book on economics in 1776 titled The Wealth of Nations. He as well as his contemporaries believed that economies developed into credit-based economies from the prehistoric bartering systems.\nIn the 19th century, both the growth of trade internationally and technology led to the creation of stronger ties for countries. This process is what accelerated into World War 2 as well as the Great Depression. Economies have seen a renewed globalization in the late 20th and the early 21st centuries.","What Is a Macroeconomic Factor?\nA macroeconomic factor is an influential fiscal, natural, or geopolitical event that broadly affects a regional or national economy. Macroeconomic factors tend to impact wide swaths of populations, rather than just a few select individuals. Examples of macroeconomic factors include economic outputs, unemployment rates, and inflation. These indicators of economic performance are closely monitored by governments, businesses and consumers alike.\nAn Academic Look at Macroeconomic Factors\nThe relationships between various macroeconomic factors are extensively studied in the field of macroeconomics. While macroeconomics concerns the broad economy as a whole, microeconomics narrows its realm of study to individual agents, such as consumers and businesses, and their respective economic behaviors and decision-making patterns. A macroeconomic factor may include anything that influences the direction of a particular large-scale market. For example, fiscal policy and various regulations can impact state and national economies, while potentially triggering broader international implications.\nNegative Macroeconomic Factors\nNegative macroeconomic factors include events that may jeopardize national or international economies. Fears of political instability caused by a nation’s involvement in a civil or international war, are likely to heighten economic turbulence, due to the reallocation of resources, or damage to property, assets, and livelihoods. Unanticipated catastrophic events, such as the 2008 United States economic crisis, subsequently created a far-reaching ripple effect, resulting in tighter capital preservation requirements for banking institutions on a global scale. Other negative macroeconomic factors include natural disasters, such as earthquakes, tornadoes, flooding, and brushfires.\nNeutral Macroeconomic Factors\nCertain economic shifts are neither positive nor negative. Rather, the precise implications are determined by the intent of the action, such as trade regulation across state or national borders. The nature of the action in question, such as enacting or rescinding a trade embargo, will trigger myriad effects, depending on the economy being influenced.\nPositive Macroeconomic Factors\nPositive macroeconomic factors include events that subsequently foster prosperity and economic growth, within a single nation or a group of nations. For example, a decrease in fuel prices within the United States might drive consumers to purchase more retail goods and services. Moreover, as the demand for goods and services increases, national and international suppliers of those items will invariably enjoy increased revenues from the heightened consumer activity. In turn, increased profits may drive up stock prices.\nMacroeconomic Factor Cycle\nEconomies are often cyclic at the macroeconomic level. As positive influences promote prosperity, increased demand may trigger higher prices, which may, in turn, suppress the economy, as households become more restrictive of their spending. As supply begins to outweigh demand, prices may again dip, leading to further prosperity, until the next shift in economic supply and demand.\n- A macroeconomic factor is an influential fiscal, natural, or geopolitical event that broadly affects a regional or national economy.\n- The relationships between various macroeconomic factors are extensively studied in the field of macroeconomics.\n- Examples of macroeconomic factors include economic outputs, unemployment rates, and inflation.\nReal World Example\nDiseases can also be defined as macroeconomic factors. Case in point: after the 2014 Ebola virus struck West Africa, the World Bank Group’s Macroeconomics and Fiscal Policy Global Practice (MFM) stepped in to help support local governments in combating the virus."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:fa24faa9-46fd-44aa-9900-7853b9a33231>","<urn:uuid:1a64dea6-1afe-4e00-b21d-48e0a8d67e2b>"],"error":null}
{"question":"Which camera type provides better autofocus capabilities: DSLR or mirrorless cameras?","answer":"Mirrorless cameras generally provide superior autofocus capabilities. They include Hybrid Auto Focus systems that combine fast phase-detection AF (PDAF) with precise contrast-detect AF (CAF), allowing for quick focus on fast-moving subjects and better tracking than most DSLRs. While DSLRs traditionally used both phase and contrast detection autofocus for tracking fast subjects, they don't perform well in live view mode since the phase detection doesn't work when the mirror is up. Modern mirrorless cameras with hybrid autofocus systems, like the Sony a6300 with 425-phase detection AF, can capture images quicker than traditional models.","context":["In 2004, when the world first saw a mirrorless camera, industry experts and professional photographers wrote it off completely. But the first truly mirrorless camera was made by Epson, a technology company that isn’t exactly known for its cameras.\nPeople start taking mirrorless cameras seriously from around 2008-2010 when a number of camera makers, like Sony Leica, Olympus, Fujifilm, Pentax, and Panasonic.\nBy 2008, mirrorless cameras were consider as a lifestyle product given their beautiful design and look.\nThe picture quality they boast was way better than any point-and-shoot camera could offer but not as good as DSLRs, even your basic, APS-C sensor DSLRs.\nToday DSLRs, once the mainstay in photography as well as filmmaking, is going the way of the dinosaur.\nYou know things are bad for the camera format when major manufacturers like Nikon and Canon are considering switching their production capacities completely to mirrorless cameras.\nWhat is the Differences between Mirrorless and DSLRs cameras?\nDSLRs or Digital Single Lens Reflex cameras have a pentaprism glass mechanism, which projects the image on a viewfinder.\nThe reason why we need this mechanism is to see what kind of framing would one get when one would be taking photos.\nLight enters through the lens, which is reflect off a mirror in the camera body that bounces light into the viewfinder.\nWhen you click the shutter, the mirror flips down and exposes the digital sensor, which absorbs the light and captures the image.\nIn a mirrorless camera, this glass pentaprism is absent. Instead, we use a digital viewfinder, which is basically a video feed of what the lens is seeing, which is shown to users usually at 60fps.\nBecause the design is more simple and more streamline in Mirrorless cameras, they are considerably smaller and more portable.\nThe smaller size of mirrorless cameras has also allow manufacturers like Nikon to create much wider lens mounts, which allow them to go for apertures like f1.2 or f0.95.\nWhat makes Mirrorless cameras better?\nMirrorless cameras are proving to be better than DSLRs or mirror cameras in a number of ways.\nThe more compact and lightweight bodies of mirrorless cameras mean they are very easy to carry and work with.\nThe autofocus systems on mirrorless cameras are far superior to DSLRs, especially when we consider autofocus tracking in videos.\nAlso, mirrorless cameras include Hybrid Auto Focus systems that combine the advantages of fast, decisive on-sensor phase-detection AF (PDAF) and the precision of contrast-detect AF (CAF), which allows users to select very precise focusing points.\nThis is also why mirrorless cameras are able to focus quickly on very fast-moving subjects and track focus better than most DSLRs.\nAnother advantage that mirrorless cameras have over DSLRs is the digital viewfinders that they use.\nWhen using viewfinders in DLSRs, you don’t actually get to see how your image or video is going to get expose, whether you should tweak your ISO, exposure compensation, or aperture opening.\nWhen using the viewfinder in mirrorless cameras, you get to see how your image will be expose in real-time and can make adjustments accordingly.\nFor professional photographers, this actually helps them save a lot of time in post-production.\nWhat makes DSLR cameras better?\nDSLRs with good glass pentaprism optical viewfinders, provide a brilliant real feel viewing image that no mirrorless Electronic View Finders can quite match.\nEVFs can be jarring, especially for people who have shot on DSLRs all their lives.\nBecause DSLRs are bigger in size, they are very ergonomic to use and can be operate even when you’re thick, heavy gloves.\nAlso, their bigger size allows manufacturers to use bigger batteries, which last much longer than most mirrorless cameras.\nAs far as built quality goes, it is really difficult to beat a DSLR, especially the ones that have a magnesium alloy body.\nSome photographers and filmmakers feel that a heavier camera, like a full-frame DSLR, also provides a more stable shooting platform than some of the smaller, lighter mirrorless cameras.\nThere is the range of lenses available for DSLRs.\nBecause mirrorless cameras are fairly new, the range of lenses on any platform, be it Sony, Nikon or Canon is very limited.\nDSLRs have a wide variety of lenses to choose from.\nFor an amateur photographer or a hobbyist, this may not be a concern, but for a professional filmmaker or photographer who invests almost thrice of what their camera body costs on their lenses, this is a big deal.\nYes, there are adapters that let you use your older lenses meant for DSLRs with new mirrorless camera bodies, but they invariably downgrade the quality of your image, ever so slightly.\nYou lose out on a lot of features and qualities of the lens when you’re using an adapter.\nWhy are Camera Manufacturers like Nikon and Canon moving Away from DSLRs?\nThe most of this is because of social media, YouTube and what most content creators want.\nMost camera buyers right now are either amateur filmmakers and content creators who want lightweight, easy-to-use cameras.\nFor someone who needs to upload videos or photos almost every day, they need the convenience of a point shoot and the quality that a flagship DSLR provides.\nMirrorless cameras have able to provide just that and hence have chosen as the way to go forward.\nFor example. When film SLRs and DSLRs were at their peak, Nikon and Canon together had over 80% of the camera market share, even among professionals.\nThere were the likes of Leicas, Olympus and Pentax, but they were very minor players, at least in terms of numbers.\nAfter that Sony with their first mirrorless cameras first in the APS-C format and then as full-frames, and went to turn the market on its head.\nIndependent filmmakers rave about how easy to use these cameras were, and how they were offering much better quality, ease of use, and feature sets, compare to the best DSLRs that either Canon or Nikon had to offer.\nFrom 2014 onwards, Sony cameras outsold Nikon and Canon combine.\nThe two major company had to respond, and therefore, in 2018, both these players start selling mirrorless cameras.\nIn last 4 years since they launch their mirrorless cameras, Canon and Nikon have able to claw back some of the market shares they lost.\nAs of today, the three major players in the mirrorless market, Sony, Canon and Nikon, have roughly the same numbers in terms of units of cameras sold.\nSony may have a slight advantage over Nikon and Canon at times, but the disparity is nowhere near what it was during the mid-2010s.","Both the mirrorless and DSLR cameras are great for photography, but each has its advantages and downsides that you ought to consider before buying one for your hobby or profession.\nIn this post, we have reviewed and compared the top features of each model to help you purchase the right one for your photography or cinematography needs.\nMirrorless Vs. DSLR Cameras\n1. Size and weight\n• DSLRs are bulky and heavy due to their mirror and prism\n• The mirrorless camera is slim and light.\nMirrorless cameras’ slim size is their primary selling point. Unlike their bulky counterparts, the mirrorless cameras are an ideal option for highly mobile photography hobbyists or professionals who are looking for a light and compact gadget that they can conveniently carry around or fit into a camera bag.\nHowever, it is imperative to note that some mirrorless cameras, more so those with an APS- C sized sensors can feel bulky because of their massive lenses and other additional features like large grips.\nOn that account, if you want to invest in a compact sized mirrorless camera, we recommend that you shop around for one with retractable or power zoom lenses. Some of the great options that you would consider are the Panasonic and Olympus models. In most cases, the mirrorless cameras from the duo giant manufacturers use small and light sensors.\nDespite the fact that DSLR cameras are relatively bulky, you can still find a portable sized DSLR. In fact, some DSLR camera manufacturers have resorted to reducing the size and weight of their cameras to cater for the needs of photographers who would love to purchase a DSLR, but not a bulky one.\n• Some DSLR cameras like Nikon, Canon, Sony, and Pentax accept a variety of lenses for professional photography and cinematography.\n• Most mirrorless cameras restrict you to select lenses. However, mirrorless cameras from manufacturers like Fujifilm, Panasonic, and Olympus may accept a variety of lenses.\nDespite their bulky nature, DSLR camera would be a sound investment for any photographer or hobbyist who is looking for a camera that he or she can customize according to their unique photography needs.\nThis is because a DSLR accept an extensive range of lenses from the manufacture as well as third part manufacturers. The multiple choices let you pick those that suit your photography needs as well as budget.\nOn the other hand, you can consider investing in the mirrorless camera if you are comfortable with the limited choices accepted by your camera. Despite the restricted choice, some mirrorless brands like Olympus and Panasonic use the Micro Four third lenses which are powerful enough to deliver similar photography results as those you will find in a DSLR.\n• DSLRs’ optical view provides a lag- free viewing.\n• Mirrorless cameras let you view a digital presentation of your scene before you capture it. However, its preview might be grainy or jerky in low light settings.\nAs a norm, DSLR cameras use an optical viewfinder instead of an electrical viewfinder used in their mirrorless counterparts. In the opinion of professional photographers who have used both viewfinders, the optical viewfinder used in DSLRs gives you an exact preview of what your camera will capture.\nOn the other hand, mirrorless cameras have an electronic viewfinder that gives you a preview that is closer to what the final image captured will look like. The primary downside of an electrical viewfinder is that its preview might be dull or jerky when capturing fast-moving subjects or when shooting in scenes with low light intensity.\nSome modern mirrorless cameras come with an upgraded electronic viewfinder that can exceed the performance of an optical viewfinder. The improved viewfinders have a digital feature that lets you regulate the brightness and contrast to favor your photography demands.\nSo, what camera has the best viewfinder? Both cameras have an excellent viewfinder that can deliver consistent results in most photography settings.\n• DSLR use both the phase and contrast detection autofocus, which make it an ideal option for tracking fast subjects.\n• Mirrorless cameras use either the contrast detection autofocus or a hybrid autofocus made up of the phase detection and contrast detection sensors.\nTraditionally, most photographers preferred DSLR cameras to mirrorless cameras due to their phase and contrast detection autofocus that measured the convergence of two beams of light quickly. They disregarded the mirrorless cameras because they utilized the contrast detection autofocus that is slower especially in dim light.\nToday’s mirrorless cameras utilize a hybrid autofocus with both the phase detection and contrast detection autofocus. On that account, the modern mirrorless cameras will let you capture images quicker than the traditional models would.\nSome of the mirrorless cameras with the hybrid autofocus are Sony a6300, which comes equipped with a 425-phase detection AF, and Nikon D3400, which utilizes an 11 phase detection autofocus. Other mirrorless cameras that use the hybrid AF systems are Olympus OM- DE- M1 and Fujifilm X- T2.\nIn regards to the autofocus, we can say that both camera types offer a reasonable performance. However, DSLRs may not be an ideal option for shooting in live view mode since the camera’s phase detection does not work when the mirror is up.\n5. Continuous shooting\n• DSLRs have a slower shooting speed than mirrorless cameras.\n• Mirrorless cameras have a high shooting speed.\nIf you intend to capture action shots, then you will want to invest in a camera that comes with an excellent continuous shooting speed. In this case, we would recommend the mirrorless camera because of the mirrorless design that lets you capture image after image in quick succession.\nTo add to the bargain, some mirrorless cameras utilize either a mechanical or an electronic shutter that lets you capture a spurt of images in the shortest time possible. One of the mirrorless cameras with an outstanding shooting speed is Olympus OM- DE- M1 mark II with 60 frames per second.\nBesides the mirrorless cameras, there are also some DSLRs that come with a reliable shooting speed for continuous shooting. In fact, high-end models like Canon’s EOS- 1D X can compete well with most mirrorless cameras.\n6. Video Quality\n• DSLRs offer a reasonable video quality since they support a wide range of lenses.\n• Mirrorless cameras have modern features like the 4K video capability and a live view AF that lets you record excellent quality videos. Some offer better videos than that of a DSLR.\nInitially, most video shooting enthusiasts preferred DSLRs since they could accept a broad range of lenses, a feature that let a camera’s user to tune his or her camera according to his or her unique photography needs.\nCurrently, mirrorless cameras have gained the ground due to their innovative features like the 4K functionality and the live view autofocus. These two features let you record videos with four times the resolution of the standard HD footage. Only a few high- end DSLR cameras provide the 4K video functionality.\nTherefore, if you are looking for a camera that is ideal for shooting cinematic like videos, then it will be a wise decision to invest in a mirrorless camera. Some of the mirrorless cameras that are known to provide an outstanding video quality are Sony’s Alpha A7S II and Panasonic’s Lumix GH5.\n7. Operational Features\n• DSLRs are like pacesetters\n• Most mirrorless cameras have similar functional features as those found on DSLRs. Some mirroless cameras, however, have improved features.\nDSLRs and mirrorless cameras are almost similar in regards to the operational features and controls. Ideally, both types have pretty identical manual controls that are not hard to use, as long as you have some basic experience in photography.\n8. Image Quality\n• Most DSLRs use the latest APS- C or full- frame sensors to provide excellent quality images\n• Mirrorless cameras have similar sensors although some traditional models have smaller sensors.\nWith regards to image quality, it would be prudent to say that both types of cameras offer high-quality images since they utilize the standard full frame sensors. However, you should be wary of some traditional mirrorless cameras that offer a low-quality image due to their smaller sensors.\nSome of the mirrorless cameras with a small size are those manufactured by Panasonic and Olympus using the Micro Four Thirds format instead of the standard full size. These mirrorless cameras offer a reduced quality image, but their slim size is great for photographers who want a camera that they can carry around conveniently.\nAs such, both the DSLR and mirrorless cameras are great for taking photos as long as they utilize similar sensors and image processors.\n9. Battery Life\n• DSLR cameras have an excellent battery life.\n• You would need spare batteries for a mirrorless camera\nDSLR cameras come with a powerful battery that can let you take several shots before it drains. On average, a fully charged DSLR battery can take 600- 800 shots while some more powerful models like Nikon D7200 will help you take up to 1100 shots. You can also extend your DSLR’s battery life by opting not to use its LCD screen.\nDifferent from the DSLRs, the mirrorless cameras have a weak battery that will only last for an average of 300 – 400 shots. Therefore, you will need to purchase spare batteries if you intend to take more than the average shots.\n• You can get a cheap DSLR with quality features\n• Cheap Mirrorless cameras do not have viewfinders\nAs a rule, the market offers you a variety of DSLRs and mirrorless cameras ranging from the most affordable, reasonably priced, to the wildly expensive. If you want an affordable but feature-rich camera, we advise that you go for a DSLR model.\nAn excellent example of an affordable but feature rich DSLR is Nikon’s D3300 that gives 24 megapixels, utilizes an APS- C sensor, and has an optical viewfinder. Its fully charged battery can take up to 700 shots.\nYou can hardly find a mirrorless camera with such features at an affordable price. However, you can always shop around or bargain for a fair price.\n• DSLRs are bulky, but they offer reliable image and video quality. Moreover, DSLRs are reasonably valued, which makes them a great deal for low budget photographers.\n• Mirrorless cameras are compact, have innovative features, and they offer high-quality images and videos. However, they are quite pricey.\nAs is evident from the review above, both DSLR and mirrorless cameras are excellent for photography and cinematography. Therefore, the choice of what makes the best investment depends on the photographer’s unique photography needs."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:061069af-dd87-4497-b2d6-e72ff4a128bb>","<urn:uuid:74d5d4f6-0f28-443e-9d47-99cebc939d66>"],"error":null}
{"question":"What are the religious views on marriage compatibility in Islam, and how do cultural barriers affect healthcare access for Muslims?","answer":"In Islam, religion should be the main compatibility factor for marriage, not clan or tribal affiliation. The Prophet taught there is no superiority of Arab over non-Arab or white over black except through piety, and he himself arranged marriages across tribal and ethnic lines. Regarding healthcare access, Muslim patients' religious beliefs significantly influence their healthcare practices and medical decisions. They view God as the controller of health, which affects how they seek care. Healthcare providers need to be sensitive to religious practices like Ramadan fasting and gender preferences for medical examinations to build trust and encourage Muslims to seek and comply with medical treatments.","context":["Don’t restrict yourself in marrying only from your clan\nAn evil trend among some who claim to be from Banu Hashim (the Prophet’s family) is that they do not get married outside their clan, nor do they allow anyone else to get married into their clan.\nThey say there is no compatibility between them and other people. This is a great error; ignorance; oppression against women; and a legislation that Allah and His Messenger (peace be upon him) have not prescribed.\nInstead, Allah said: “O mankind! We have created you from a male and female, and have made you into nations and tribes; that you may know one another, Indeed the most noblest of you with Allah is the one who has the most taqwaa (piety, fear, and obedience of Allah).” (Qur’an, 49:13)\n“Indeed the Believers are but brothers.” (Qur’an, 49:10) “The Believers – men and women – are allies and protectors, one to another.” (Qur’an, 9:71)\nAllah’s Messenger (peace be upon him) said: “Indeed there is no excellence for an Arab over a non-Arab, nor for a non-Arab over on Arab, nor for a white person over a black one, nor for a black person over a white one, except through taqwa (piety). People are from Adam, and Adam was from dust.” Ahmad (5/411)\nThe Prophet (peace be upon him) also said: “Indeed my Awliya (friends and allies) are not the tribe of so and so. Rather my friends and allies are the pious – wherever they may be.” (Al-Bukhari, 10/351 and Muslim, no. 215)\nThe Prophet (peace be upon him) said: “If there comes to you a person whose Religion and character are pleasing to you, then marry him (i.e. give the girl in marriage to him). If you do not do this, there will be Fitnah (trial and discord) and greet fasad (corruption) upon the earth.” (Al-Tirmidhi, no.1085)\nThe Prophet (peace be upon him) married Zainab Bint Jahsh of the Quraish (i.e. the Prophet’s clan) to Zaid Bin Haarithah, his freed slave. He married Fatimah Bint Qays from the Quraish clan, to Usamah, the son of Zaid. Bilal Bin Rabah, the Ethiopian, married the sister of Abdul Rahman Bin Awf of the Quraish.\nThe claim that marrying outside the clan or tribe is forbidden or detested is false. The religion should be the main compatibility factor.\nThe Prophet (peace be upon him) distanced from Abu Talib and Abu Lahab (his uncles) because they were not Muslims but drew near Salman the Persian, Suhaib the Roman, and Bilal the Ethiopian because they possessed Iman (faith) and piety. Whoever adopts this false and ignorant practice of barring Hashimi women from marrying from outside their clan or tribe will only achieve faulty results – such as corruption of the people or adversely affecting the birthrates.\nO Muslims! Fear Allah with regard to yourselves and the daughters, sisters, and other women whom Allah has been placed under your charge and authority. Realize what is good and what brings happiness to the society.\nRemember that you will all be questioned and held to account about your actions, as Allah said:\n“By your Lord! We shall call them all to account for all that they used to do.” (Qur’an, 15:92)\n– By Sheikh Abdul Aziz Bin BazMajmoo Fatawa (3/100-103)","Cross-Cultural Communication “Communication in health care is a complex issue. Language and cultural barriers complicate the situation. Language is the framework in which the world view of a culture is molded, and it describes the boundaries and perspectives of a cultural system. A language barrier disarms a communicant's ability to assess meanings, intent, emotions, and reactions and creates a state of dependency on the individual who holds the keys to the entire process” (Putsch, 1985, para. 1 ). It is common for Patients in minority populations to receive a lower quality of care.\nMuch of this is contributed to cultural communication barriers. Part of the solution maybe to incorporate the six principles of cross-cultural communication in order to communicate effectively. Differences in worldviews, values, and communication styles can all contribute to misunderstandings. We must also take into consideration that most breakdowns in communication are often attributed to cultural differences. This may lead a person to use caution when speaking to someone that does not share their cultural beliefs.\nThis includes non-verbal as well as verbal communication. Cross cultural communication also requires an understanding of a groups “do’s and taboos” and is respectful of them. This may include removing your shoes before entering ones home or understanding cultural meal etiquette. If you frequently communicate with a certain cultural group or race of people, learning about their variations in communication style will increase your understanding of that group. This is particular important when it comes to health care.\nI found interest in the cultural differences of Muslim Americans (part of Middle Eastern culture). When considering the healthcare needs of American Muslim patients, require open minded views from health care providers when it comes to religious practice, rituals, and traditions. Religious values and beliefs are important to this community. They are a major influence in their health care practices, expectations of health care and medical decision making. Muslims see God as the dictator and controller of health.\nThey believe that God decides who develops certain types of cancer, who survives the ordeal and who succumbs to the disease. Their belief that a particular illness is a disease of fate greatly influences how they seek healthcare, if at all. This is because some feel they are destined to suffer while others put all of their faith in prayer. This is why it is crucial for health care providers to be sensitive to the religious beliefs of Muslim Americans. Making an effort to accommodate Muslim patients can be crucial to their health.\nIt will increase the trust they have for the health care community. This will encourage them to seek health care, as well as be compliant to medical treatments. Certain things to consider are customs such fasting during Ramadan and their adherence to dietary restrictions. It is also important to be sensitive to the needs of females in this community. It is not acceptable for them to be examined by a male doctor. Given them a choice when it comes to gender will encourage them to seek needed health care.\nProper communication skills are key to improving the health care needs of many. This includes disease awareness, along with the prevention and spread of illness. References Padela, A. , Gunter, K. , & Killawi, A. (2011, June). Meeting the Healthcare Needs of American Muslims. I. S. P. U. , (), . Retrieved from http://www. ispu. org Putsch, R. W. (1985, December). The Special Case of Interpreters in Health Care. The Journal of the American Medical Association, 254(23), . Retrieved from"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:d42cd215-b989-4076-b87b-defcfe54dfd4>","<urn:uuid:64f11015-d64e-4ec7-b724-d650b0d17d1f>"],"error":null}
{"question":"What protection mechanisms does warp sync have against long-range attacks in Polkadot?","answer":"To mitigate long-range attacks in Polkadot, warp sync's starting block should be at max 28 days old, keeping it within the slashing period for misbehaving nodes. While theoretically warp sync can start from the Genesis Block, this is not advised in practice to prevent scenarios where past validators could collude to finalize alternative blocks.","context":["7. Light Clients\n7.1. Requirements for Light Clients\nWe list requirements of a Light Client categorized along the the three dimensions of Functionality, Efficiency, and Security.\nUpdate state (Section 2.4.) to reflect the latest view of the blockchain via synchronization with full nodes.\n(Optional) Verify validity of runtime transitions (Section 2.6.).\nMake queries for data at the latest block height or across a range of blocks.\nAppend extrinsics (Section 2.3.) to the blockchain via full nodes.\nEfficient bootstrapping and syncing: initializations and update functions of the state have tractable computation and communication complexity and grows at most linearly with the chain size. Generally, the complexity is proportional to the GRANDPA validator set change.\nQuerying operations happen by requesting athe key-value pair from a full node.\nFurther, verifying the validity of responses by the full node is logarithmic in the size of the state.\nSecure bootstrapping and Synchronizing: Probability that an adversarial full node convincing a light client of a forged blockchain state is negligible.\nSecure querying: Probability that an adversary convinces light client to accept a forged account state os negligible.\nAssure that the submitted extrinsics are appended in a successor block or inform the user incase of failure.\nPolkadot Specific Requirements:\nThe client MUST be able to connect to a relay chain using chain state.\nThe client MUST be able to retrieve checkpoint state from a trusted source to speed up initialization.\nThe client MUST be able to subscribe/unsubscribe to/from any polkadot-spec-conformant relay chain (Polkadot, Westend, Kusama)\nThe client MUST be able to subscribe/unsubscribe to/from parachains that do not use custom protocols or cryptography methods other than those that Polkadot, Westend and Kusama use.\nThe client MUST support the following RPC methods:\nThe client MUST support the @substrate/connect connection extension protocol:\n7.2. Warp Sync for Light Clients\nWarp sync (Section 4.8.4.) only downloads the block headers where authority set changes occurred, so-called fragments (Definition 41), and by verifying the GRANDPA justifications (Definition 78). This protocol allows nodes to arrive at the desired state much faster than fast sync. Warp sync is primarily designed for Light Clients. Although, warp sync could be used by full nodes, the sync process may lack information to cater to complete functionality set of full nodes.\nFor light clients, it is too expensive to download the state (approx. 550MB) to respond to queries. Rather, the queries are submitted to the Full node and only the response of the full node is validated using the hash of the state root. Requests for warp sync are performed using the\n/dot/sync/warp Request-Response substream, the corresponding network messages are detailed in Section 4.7..\nLight clients base their trust in provided snapshots and the ability to slash grandpa votes for equivocation for the period they are syncing via warp sync. Full nodes and above in contrast verify each block indvidually.\nIn theory, the\nwarp sync process takes the Genesis Block as input and outputs the hash of the state trie root at the latest finalized block. This root hash acts as proof to further validate the responses to queries by the full node. The\nwarp sync works by starting from a trusted specified block (for e.g. from a snapshot) and verifying the block headers only at the authority set changes.\nEventually, the light client verifies the finality of the block returned by a full node to ensure that the block is indeed the latest finalized block. This entails two things:\nCheck the authenticity of GRANDPA Justifications messages from Genesis to the last finalized block.\nCheck the timestamp of the last finalized block to ensure that no other blocks might have been finalized at a later timestamp.\nLong-Range Attack Vulnerabilities: Warp syncing is particularly vulnerable to what is called long-range attacks. The authorities allowed to finalize blocks can generate multiple proofs of finality for multiple different blocks of the same height, hence, they can finalize more than one chain at a time. It is possible for two-thirds of the validators that were active at a certain past block N to collude and decide to finalize a different block N', even when N has been finalized for the first time several weeks or months in the past. When a client then warp syncs, it can be tricked to consider this alternative block N' as the finalized one. However, in practice, to mitigate Long-Range Attacks, the starting point of the warp syncing is not too far in the past. How far exactly depends on the logic of the runtime of the chain. For example, in Polkadot, the starting block for the sync should be at max 28 days old, to be within the purview of the slashing period for misbehaving nodes. Hence, even though in theory warp sync can start from Genesis Block, it is not advised to implement the same in practice.\nWe outline the warp sync process, abstracting out details of verifying the finality and how the full node to sync with is selected.\nAlgorithm 23. Warp Sync Light Clients\nAbstraction of Warp Sync and verification of latest block’s finality.\n: Determines the full node that the light client syncs with.\n: Returns the header of latest finalized block and a list of Grandpa Justifications by the full node.\n: Verification algorithm which checks the authenticity of the header only at the end of an era where the authority set changes iteratively until reaching the latest era.\n: Verifies the finalty of the latest block using the Grandpa Justifications messages.\nThe warp syncing process is closely coupled with the state querying procedure used the light client. We outline the process of querying the state by a light client and validating the response.\nAlgorithm 24. Querying State Light Clients\nQuerying State Algorithm.\n: Returns the response to the query requested from the Full Node for the query at block height .\n: Predicate that checks the validity of response and associated merkle proof by matching it against the Commit Root Hash obtained as a result of warp sync.\n7.3. Runtime Environment for Light Clients\nTechnically, though a runtime execution environment is not necessary to build a light client, most clients require interacting with the Runtime and the state of the blockchain for integrity checks at the minimum. One can imagine an application scenarios like an on-chain light client which only listens to the latest state without ever adding extrinsics. Current implementations of Light Nodes (for e.g. Smoldot) uses the wasmtime as its runtime environment to drastically simplify the code. The performance of wasmtime is satisfying enough to not require a native runtime. The details of runtime API that the environment needs to suppport can be found in (Appendix C).\n7.4. Light Client Messages\nLight clients are applications that fetch the required data that they need from a Polkadot node with an associated proof to validate the data. This makes it possible to interact with the Polkadot network without requiring to run a full node or having to trust the remote peers. The light client messages make this functionality possible.\nAll light client messages are protobuf encoded and are sent over the\nA message with all possible request messages. All message are sent as part of this message.\n|The request type|\nrequest can be one of the following fields:\n|1||A remote call request (Definition 96)|\n|2||A remote read request (Definition 98)|\n|4||A remote read child request (Definition 100)|\nA message with all possible response messages. All message are sent as part of this message.\n|The response type|\nresponse can be one of the following fields:\n|1||A remote call response (Definition 97)|\n|2||A remote read response (Definition 99)|\n7.4.3. Remote Call Messages\nExecute a call to a contract at the given block.\nDefinition 96. Remote Call Request\nRemote call request.\n|2||Block at which to perform call|\nDefinition 97. Remote Call Response\nRemote call response.\n|2||An Option type (Definition 190) containing the call proof or None if proof generation failed.|\n7.4.4. Remote Read Messages\nRead a storage value at the given block.\nDefinition 98. Remote Read Request\nRemote read request.\n|2||Block at which to perform call|\nDefinition 99. Remote Read Response\nRemote read response.\n|2||An Option type (Definition 190) containing the read proof or None if proof generation failed.|\n7.4.5. Remote Read Child Messages\nRead a child storage value at the given block.\nDefinition 100. Remote Read Child Request\nRemote read child request.\n|2||Block at which to perform call|\n|3||Child storage key, this is relative to the child type storage location|\nThe response is the same as for the Remote Read Request message, respectively Definition 99.\n7.5. Storage for Light Clients\nThe light client requires a persistent storage for saving the state of the blockchain. In addition it requires efficient Serialization/De-serialization methods to transform SCALE (Section A.2.2.) encoded network traffic for storing and reading from the persistent storage."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:27f2ff8c-1c83-4902-be15-2c0c03f102cd>"],"error":null}
{"question":"Is Cryptosporidium parvum exclusively transmitted through water contamination, and what preventive measures can be taken during food preparation to avoid foodborne illnesses?","answer":"No, while Cryptosporidium parvum is most commonly known for waterborne transmission, it has also been identified as an emerging food contaminant, with documented outbreaks in improperly pasteurized milk, chicken salad, and uncooked green onions. To prevent foodborne illnesses, follow the 4-step method of Clean, Separate, Cook, and Chill. This includes washing hands and utensils with hot water for 20 seconds, using separate cutting boards for different food groups, cooking foods to safe minimum temperatures, and properly refrigerating foods.","context":["Database of Food Borne Human Pathogens\nBacteria, Virus, Fungi, Protozoa, Helminths\nProtozoan Food Pathogen\nProtozoa are unicellular, motile, eukaryotic and heterotrophic microorganisms. They are bound to moist or aquatic habitat. They establish a symbiont relationship with organisms like bacteria, fungi and mammals or they are parasitic to them.\nPathogenic protozoa are commonly transmitted through food in developing countries. The vast majority of protozoa do not harm us. But there are a few that cause disease. The main protozoa of concern are Toxoplasma, Crptospordium and Giardia in developed countries. In developing countries the main problem is with Cyclospora, Entamoeba and Sarcocytis. Like virus, they do not multiply on food. Protozoan parasite can be ingested in form of cysts through food.\nSome species of protozoa are part of the normal microbial flora of animals, and live in the guts of insects and mammals, helping to break down complex food particles into simpler molecules.Cryptosporidium parvum and Giardia lamblia are parasitic protozoa which live in the guts of animals.\nGiardia lamblia is a common parasite that can cause villous atrophy, a similar kind of wasting of the gut's villi that is seen in Celiac Disease. Villous atrophy reduces gut absorption of several minerals, nutrients and fat soluble vitamins.\nEntamoeba histoltica is another common parasite. This nasty parasite can infect adjacent organs such as the liver where it can form cysts, abscesses and can even form pus.\nC. parvum possesses numerous surface glycoproteins thought to play a role in pathogenesis.\nCyclospora causes a syndrome of prolonged, intermittent diarrhea associated with profound fatigue and anorexia and it infects the upper intestine.\nCryptosporidium is a microscopic and apicomplexan parasite.C. parvum is an obligate intracellular parasite, that infects both humans and livestock transmitted via highly durable oocysts in feces.Development inculde both a cyclic asexual reproduction and the production of gametes giving rise to further oocysts, which are either excreted or reinfect the host.C. Parvum oocysts-forming apicomplexan protozoa, which complete their life cycle both in humans and animals, through zoonotic and anthroponotic transmission, causing cryptosporidiosis.\nCryptosporidium paravum is most commonly known for waterborne transmissionmore but recently identified as an emerging food contaminant. Several documented food-associated outbreaks have been reported in improperly pasteurized milk , chicken salad in , uncooked green onions.\nEntamoeba histolytica is a anaerobic and Zoonotic parasitic protozoan it causes Amebiasis .The highest prevalence of amebiasis is in developing countries. Get tramsmitted by feces and food and water supplies, are inadequate that infects predominantly humans and other primates.Cysts are viable in water, soils and on foods.\nGiardia intestinalis is a protozoan flagellate (Diplomonadida) that can live in the intestines of animals and people, it causes giardiasis. It is found in every region throughout the world and has become recognized as one of the most common causes of waterborne (and occasionally food-borne) illness.\nToxoplasma gondii are obligate intracellular Parasites that belong to the family Sarcocystidae.T. gondii is ubiquitous and isfound around the world and considered to be a leading cause of death attributed to foodborne illness. Oftenly found in hot, humid climates and\nlower altitudes and occurs in a wide range of intermediate hosts that include warm blooded vertebrates, such as birds, carnivores,\nrodents, pigs, primates, and humans.\nCyclospora cayetanensis is a unicelluar, coccidian, microscopic parasite which causes cyclosporiasis, an intestinal disease in humans and small to be seen without a microscope. Its full name is Cyclospora cayetanensisa, and it has a life cycle that involves both sexual and asexual reproduction.Cyclosporiasis occurs in many countries, but it seems to be most common in tropical and subtropical regions. In areas where cyclosporiasis has been studied, the risk for infection is seasonal. However, no consistent pattern has been identified regarding the time of year or the environmental conditions, such as temperature or rainfall.","Learn these quick and easy tips for food safety to keep the whole family safe when preparing foods at home!\nThe Food and Drug Administration defines foodborne illness as a sickness that occurs when people eat or drink harmful microorganisms (such as bacteria, parasites, viruses) or chemical contaminants found in foods or drinking water. According to the Centers for Disease Control and Prevention, over 76 million people in the United States suffer from a foodborne illness each year, with the most susceptible being young children, older adults, pregnant women, and individuals with a compromised immune system.\nYou can help reduce the risk of contracting foodborne illness at home by taking a few simple precautions. Below is a list of helpful definitions, identifiers, and practical tips to make your kitchen safe and food friendly.\nWhat are symptoms of foodborne illness?\nIt depends on which type of foodborne illness you’ve contracted, but common symptoms include (but are not limited to) nausea, vomiting, diarrhea, abdominal cramps, aches, dehydration, and fever.\nHow quickly will symptoms appear?\nAgain, it depends on the type of foodborne illness, but symptoms can appear in as little as 1 hour or up to a week or more. The best thing to do is to contact your health care provider if you suspect the onset of foodborne illness.\nWhat causes foodborne illness?\nImproper food handling:\n- Raw food that has been contaminated via processing, storage, transportation, or preparation can be a carrier.\nIncorrect cooking/reheating time:\nMany times, the outer appearance of food doesn’t change, even when bacteria is multiplying within! The key to stopping growth is to fully cook or reheat the food. Refrigeration and freezing will slow, but not prevent, the growth of harmful bacteria.\nInappropriate holding times:\nAt home, it’s crucial to keep food out of the “danger zone” where bacteria multiply most rapidly. Bacteria grow best at temperatures above 40 ° F and below 140° F, so to keep you and your family safe, be sure to keep your refrigerator set to 40 ° F or lower and cook foods to 140° F or higher.\nWhat are the preventative measures you can take to prevent foodborne illness at home?\nRemember the 4 step method- Clean, Separate, Cook, Chill.\n- Don’t wash produce until right before it’s to be used.\n- Wash everything that the raw food item comes into contact with\n(hands, utensils, cutting boards, etc.) with hot water for at least 20 seconds.\n- Post-prep, sanitize counters and utensils with a bleach solution.\n- Keep food groups separate during prep- don’t place ready to eat food next to the raw ingredients.\n- Use a different colored cutting board and utensils for each food group.\n- Example: Red for meats, green for veggies, and white for fish\n- Invest in and use a food thermometer to make sure internal temperatures are out of the danger zone. These are the minimum temperatures that your meats should be cooked at:\n145°F– Roasts, steaks, chops of beef, veal, and lamb\n160°F– Pork, ground veal, ground beef\n165°F– Ground poultry\n180°F– Whole poultry\n- Also, remember to throw away foods that have set out at room temperature for over two hours.\n- Freeze or refrigerate leftovers immediately after consuming.\n- Check the temperature of your refrigerator and freezer regularly.\n- The refrigerator should be no warmer than 40° F, and the freezer should register 0° F."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:4fe408b1-d200-4a39-b95c-9186748d99c4>","<urn:uuid:85118753-454c-4cdf-aa35-88d338f226e2>"],"error":null}
{"question":"How do open-cell pavers and French drain systems compare in terms of water management effectiveness?","answer":"Open-cell pavers and French drain systems manage water differently. Open-cell pavers are a permeable surface solution that allows water to pass directly through the driveway to the ground below, while also enabling grass to grow in the gaps for a dramatic effect. French drains, on the other hand, work by collecting water in a perforated pipe surrounded by gravel, which then channels the water away to a designated area. The French drain system requires a minimum 1% gradient slope to work efficiently and can direct water to various endpoints such as rain gutters, gravel pits, or soakaway systems. Both solutions are effective, but French drains may require more maintenance and can be trickier to install due to the need to avoid underground utilities.","context":["A concrete driveway with a discrete trench drain inserted just before the gates.\nWhether you’re about to build a new driveway yourself or are contracting a professional to do the job, it pays to consider the drainage system for your driveway in order to avoid future problems.\nHaving good a driveway drainage system is especially important if your home sits below the street level and your driveway therefore slopes down from the street towards your home or garage.\nClearly, if this is the case, without correct drainage, you run the risk of water pooling at the bottom of the driveway and potentially entering your home and causing damage.\nFixing driveway drainage problems after installing a new driveway can be costly, so it’s best to consider how to drain water away from the driveway whilst in the planning phase of your project.\nSo let’s have a look at the different drainage systems for driveways, so that you can understand how they work and which one will best suit your home.\nThere are several different options for driveway drainage:\nIn an ideal world your house or garage will be sited on land that is slightly raised from the street level. In this way you can ensure that your driveway will slope downward from your property to the street level, creating a natural path for rain water to slow away from your home.\nWhilst it’s true that having a home that is located higher than the street level will cause rainwater to natural run down towards the street, you might want to add a rainwater run off either side of the driveway, with metal driveway drainage grates sited every so often to ensure the water runs off correctly and into the main drainage system of your property. This run off is like a shallow channel that goes the length of the driveway and can be made using cobble pavers, bricks or cement.\nThis long asphalt and cobblestone driveway has drainage channels built into the design.\nIf your property is only slightly raised from the street level and has lawn or landscaping either side of it, then it may be worth consider creating driveway swales.\nA swale is created when the land either side of the driveway is slightly graded, making a shallow dip. This dip of the swale gives water a place to go, where it can then be absorbed into the ground below.\nDrainage swales are usually covered with lawn, but can also be disguised with appropriate landscaping such rockery work and water loving plants.\nGrass swales have been built into the landscape either side of this rustic gravel driveway.\nOkay, so let’s take a look at the most common and effective driveway drainage solutions:\nThis is one of two driveway drainage solutions where your home is sited lower than the street level, that is to say where the driveway slopes down from the street towards your home.\nA small trench is dug along the entire width of the driveway – usually where the driveway meets the garage. Inserted into this trench is a metal U-shaped channel which is then connected to pipes that run water out to underground or street level drains, or potentially to a landscaped pond.\nThe driveway channel drain works by collecting water flowing down the driveway and channeling it elsewhere. The channel drain is usually completed with a metal driveway drainage grate, which keeps debris such as leaves and plants form blocking the drain, and also allows your car to easily drive over it.\nThere are different styles of metal drainage grates now available to personalize the look – some can be quite ornate, others are very slick and functional. In very wet climates or for very long driveways you might need more than one channel drain at different points long the driveway.\nAn example of a channel drain at the end of a downward sloping paver driveway.\nThis is the second possible option for a driveway that slopes downwards. Often confused with a channel drain, a French drain essentially works the same way but has a different construction.\nWith a French drain a large trench is typically dug along the length of a driveway, a perforated pipe is then placed in the trench and then covered with gravel or pebbles. The French drain perforated pipe needs to be sited low enough into the ground to intercept groundwater, and be laid at a slight angle heading downwards away from your home.\nThe French drain works by allowing water above to trickle through the gravel layer, into the perforated pipe, and then channeled towards the street. To complete a French drain, it can be covered with topsoil and lawn, or you can leave the gravel or pebbles visible and make that part of the look of your landscaping.\nFrench drains covered in pebbles have been installed either side of this concrete driveway.\nAlso referred to as porous driveways or sustainable driveway drainage systems, this is a catch-all terms for a number of options that will allow water to simply pass through the driveway to the ground below. Personally, if you’re starting your project from scratch, then I think choosing a permeable surface is one the best drainage ideas for your driveway as you can find some really attractive options. Options include:\nOpen-cell pavers allow natural drainage and grass to grow in the gaps, creating a dramatic effect.\nA channel or trench drain is the best option for a concrete driveway, particularly along its width where it meets your home. In an ideal world the channel or trench would be factored into the initial pouring stage of the concrete, but – with the right tools and effort – you can cut a trench into your concrete driveway after it has set. You may also consider adding French drains or swales either side of a concrete driveway to combat water running off the side edges.\nAgain, channel drains are the best solution for draining driveways that are made with impervious materials like asphalt. French drains of swales can also be installed along the sides.\nYou may not need any extra drainage if you have a gravel driveway, as water should naturally work its way through the gravel to the ground below. But if you have a very steep gravel driveway then French drains are an ideal gravel driveway drainage solution, as once covered in gravel you will hardly know the drain is even there.\nThe most important thing when it comes to drainage around your home is to feel confident that you know what you are doing – we have see first hand DIY drainage projects that have gone horribly wrong, resulting in flooding that has damaged property and required costly repairs.\nThis article should be considered just a starter in your research as you explore the right drainage solution for your driveway; there are technical aspects to all of the above drainage solutions that need to be considered in the light of your home’s exact location and the budget you have to spend.\nQualified and experienced driveway contractors will be able to give you detailed advice about which driveway drainage solution is the best for your particular project.\nWhile the basic principles are as outlined above, there’s no reason why you can’t also have fun when designing your driveway drainage solution, especially if you have a great budget and can choose something a little bit out of the ordinary.\nHere’s some more driveway drainage inspiration…\nSimply cutting curved gaps into concrete where grass can grow, is a clever, attractive and cost-effective drainage solution.\nUsing large scale pavers with gaps for drainage, creates a bold geometric pattern that is both pretty and practical.\nA similar effect here with large rectangular concrete pavers with gaps filled with fine gravel.\nPervious cobblestones style pavers allow water to drain through and give this driveway a contemporary design edge.\nToday trench drain grates come in lots of different styles, materials and colors, like this one with a very attractive leaf pattern made from iron.\nSwales finished with pretty rocks and pebbles have been landscaped into either side of this concrete driveway.\nThis impressive gravel driveway is built up from the lower paddocks either side, creating a natural slop for drainage.\nGravel driveways are great for natural drainage, plus if there is a French drain in place you’d never know!\nFrench drains covered with large pebbles make up part of the landscaping either side of this brick driveway.\nFrench drains and stepping stone pavers make a natural pathway on one side of this asphalt and concrete drivewa","Land drain, French drain, weeping tile, rock drain, filter drain? You might have seen all these phrases floating around on the internet if you’re looking to fix a drainage problem in your home. But what do they all mean?\nEssentially, they are all the same thing. However, French drain more typically refers to the process of removing the surface water by installing a trench and backfilling with gravel. Before there was such a thing as a land drain pipe, this method was called a French drain - without a pipe. Nowadays people still call this method a French drain even though it will most likely incorporate a perforated land drain pipe.\nWhere can you use a French drain?\nMost typically, a French drain is used in and around your home where excess surface water is a problem.\n- This could be in your lawn, artificial or real grass, or back yard area.\n- A basement or garage where damp is a recurring problem.\n- Or it can be used outside the home in outdoor open spaces such as playground areas and golf courses.\nOn a larger scale, for field drainage in agriculture to give the land a greater potential to be farmed more productively, increasing the growing season and crop yeild.\nWhy use a French drain?\nIf you have any drainage problems on your property, installing a French drain is the easiest and most cost-effective method to free your land of the excess water. The land drain pipe allows the water to easily be collected through the perforations and flow through the pipe distributing the water to a more suitable place where flooding won’t be a problem.\nWhere to end a French drain?\nAfter you have planned out your French drain installation, the next problem you may face is knowing where to end the drain, and where the water can go. The drain must end at the lowest point for the water to be able to flow at a downwards slope and work efficiently. The slope needs to be no less than 1% gradient.\nIdeally if installing in your home or garden you want the end of the drain to be relatively close to the start to save costs, but long enough to actively remove the water to a drier area away from the problem area.\nThere are a number of suitable end points for your French drain system:\n- The water can be directed into an existing rain gutter.\n- Into a gravel pit or dry well filled with small gravel stones.\n- You can install a soakaway system such as a RAINBOX® crate which will hold the water and then slowly release back into the soil avoiding flooding.\nBefore discharging from a French Drain system, get approval from your local planning authority.\nDisadvantages of using a French drain system\n- Installation can be tricky. If you are DIY-ing there are things to look out for when digging your trench such as gas lines and underground utilities. You would need to be aware of where they are before starting to remove the ground.\n- Obviously as the land drain pipe is underground it can sometimes be difficult to maintain and clean, using a geotextile material around the pipe and trench should protect the land drain from mud and debris."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:18cfc2c8-2fce-4137-9f8b-2dbc9a1ee5bd>","<urn:uuid:6ce18c19-a20d-4340-9fa1-17a6d187614f>"],"error":null}
{"question":"Who is the author of the bilingual children's book 'Just Ask!' and what is her professional role?","answer":"Just Ask! was written by Sonia Sotomayor, who is a Latina Supreme Court Justice. The book is a bestselling children's book that celebrates differences and varying abilities among people.","context":["13 Kids Books That Celebrate Latinx Stories\nEach year, we observe National Hispanic Heritage Month from September 15 to October 15 by celebrating the history, the culture, and the myriad contributions of those from Spain, Mexico, Puerto Rico, Argentina, and more. This year, include your child in the festivities by adding some of these books that celebrate Latinx stories into your storytime rotation! From board books for babies to award-winning picture books for big kids, there’s no doubt that you’ll find so many libros that your niños will adore!\nWritten and illustrated by Paloma Valdivia\nIf you love the classic children’s book Runaway Bunny, you’ll for-sure adore this beautiful bilingual tale of a mama and her tot imagining themselves as a variety of animals and showing how precious their bond is. This picture book’s unique illustrations are charming and unforgettable, while the words are sweet and poetic, making Nosotros Means Us a just-right read for little ones up to 5 years old—and a fantastic way to start building up your bub’s bilingual vocabulary.\nWritten by Leonarda Carranza, illustrated by Rafael Mayani\nThis poignant, empowering, and important children’s book is a must-have addition to any child’s home library. Author Leonarda Carranza skillfully addresses racism and microaggressions through the experiences of a young child and her granny (abuelita) who are running errands. The intergenerational pair shows bravery, grace, and empowerment—while teaching readers that treating others poorly because of their differences is wrong. There’s little wonder why Abuelita and Me has snagged numerous awards, including being named one of the best books of 2022 by Kirkus and the International Latino Book Awards.\nWritten by Sonia Sotomayor, illustrated by Rafael López\nWhat do you get when you cross a Latina Supreme Court Justice with an award-winning Latino artist? The answer: Justice Sonia Sotomayor’s bestselling children’s book Just Ask! This Amazon Teachers’ Pick celebrates how the differences and varying abilities among all of us should be viewed as special powers, not detriments. Through fresh and spunky illustrations, readers witness children working together to build a community garden, each asking poignant questions along the way.\nWritten by Matt de la Peña, illustrated by Loren Long\nGet the tissues ready! This bestselling and award-winning picture book masterfully hugs your heart, stirring up all the love inside with its poetic prose and gorgeous illustrations. Here, readers are given magnificent glimpses of the many ways we experience love throughout childhood and beyond. (Love is the sound of the first voices we ever hear; it is the color of the night sky over a happy home; it is the echo of summer laughter.) This tender tale is sure to be a new classic. (You can also pick up the Spanish language version Amor.)\nWritten by Patty Rodriguez & Ariana Stein, illustrated by Citlali Reyes\nPart of the Lil’ Libros bilingual board book series, Counting With Frida introduces little ones to the iconic Mexican painter Frida Kahlo…all while teaching them how to count to 10 in English and Spanish. Simple and sweet, each of the lively illustrations lean on Kahlo’s famous style and known works. Bonus: The length of this board book is perfect for a baby’s limited attention span! (Find more number books for babies and toddlers.)\nWritten and illustrated by Carlos Aponte\nArmed with an old photo and a full heart, a young boy named Carlitos sets on a journey through the heart of Old San Juan to find his papi. Across the Bay is not your typical wrapped-up-in-a-bow children’s book. Instead, it gracefully and honestly illustrates the true meaning of family and home, which is based on author/illustrator Carlos Aponte’s own childhood in Puerto Rico. This enchanting picture book is perfect for the 3- to 7-year-old set.\nWritten and illustrated by Melisa Fernández Nitscheoung\nInvite your child into the world of Argentinian folk singer extraordinaire Mercedes Sosa, who turned her artistry into activism with songs that spoke to what it truly means to battle injustices in Latin America. Her music was so powerful, in fact, that she was forced into self-exile when her home country was taken over by a military dictatorship. But that didn’t stop Sosa from dedicating her life to being the voice of the voiceless. This bright and breathtaking book is designed to inspire and empower young readers (age 4 and up) across the globe…including yours!\nWritten by Yamile Saied Méndez, illustrated by Jaime Kim\nIt’s easy to see why Where Are You From? has garnered so many prestigious book awards! Not only are the illustrations captivating and heart-warming, the story is a lyrical powerhouse. Here, a little girl who’s always fielding the title’s question, but doesn’t have a simple answer, learns all about self-acceptance with the help of her beloved abuelo (grandfather). This is a wonderful story for children 4 and up to enjoy, but especially for those who’ve ever felt they didn’t belong. The Spanish-language edition, De Dónde Eres?, is also available.\nWritten by Carmen Agra Deedy, illustrated by Eugene Yelchin\nDon’t let this playful story about a sing-songy bird fool you! The Rooster Who Would Not Be Quiet! is a fantastic picture book that skillfully teaches children about human rights and the consequences of oppression. Geared toward kindergarteners through second graders, this Amazon Teachers’ Pick is set in a happy, but noisy village of La Paz where the mayor outlaws loud singing in the street…then at home…then everywhere! But Rooster just won’t stop and when the mayor threatens the town’s favorite bird, a singing revolution begins!\nWritten and illustrated by Andrea Cáceres\nWhen Aurora arrived in America from Venezuela, she learned to speak English…but her beloved pup, Nena, did not. That means, Nena doesn’t understand sit, or wait, or treat, but she definitely knows siéntate, espera, and postre! This picture book is a sweet vehicle to explore code switching, learning a second language, and having to translate for others. My Dog Just Speaks Spanish is also a fun tool to help young readers practice reading—and learn English or Spanish terms!\nWritten by Alyssa Reynoso-Morris, illustrated by Mariyah Rahman\nAbuela says, “plátanos are love.” I thought they were food. But Abuela says they feed us in more ways than one. This is more than just a delicious story about cooking plátanos (plantains), it’s about tradition, community, and most importantly intergenerational love between a doting abuela and her granddaughters. Written for children 4 and up, this joyous and lyrical picture book is sprinkled with Spanish words and heaps of warmth.\nWritten by David Bowles, illustrated by, Erika Meza\nAvailable in English and Spanish, this multi-award-winning picture book tells the heartfelt tale of a father and son living on the U.S./Mexico border, and their weekend tradition of going to their favorite places on The Other Side. The pair visit a cherished restaurant, devour sweet treats…and drop off much-needed supplies to friends who are seeking asylum. The luminous watercolors and gentle words weave weighty realities into the everyday…and come together perfectly for children aged 4 to 8.\nWritten by Naibe Reynoso, illustrated by Jone Leal\nSpecially designed for the youngest “readers,” this is the bilingual board book version of the original, Be Bold, Be Brave: 11 Latinas Who Made U.S. History. Within the pages, your little one will be introduced to notable Latinas such as Supreme Court Justice Sonia Sotomayor, astronaut Ellen Ochoa, famed singer Selena, Oscar-winner Rita Moreno, and seven more. This is the ideal book to start introducing your child to the countless Latinas who’ve made a difference in the world!\nMore Children’s Books\n- The Best Spanish-English Bilingual Books for Babies\n- Books That Teach Little Kids About Race, Justice, and Equality\n- Kids’ Books That Celebrate Amazing Women in History\n- Children’s Books That Teach Empathy and Kindness\nHave questions about a Happiest Baby product? Our consultants would be happy to help! Submit your questions here.\nDisclaimer: The information on our site is NOT medical advice for any specific person or condition. It is only meant as general information. If you have any medical questions and concerns about your child or yourself, please contact your health provider."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:c8051784-f4a6-40c0-b1de-9c77315cc7a7>"],"error":null}
{"question":"What are the key specifications to consider when selecting photo editing software, and how do these requirements align with high-speed imaging camera selection?","answer":"For photo editing software, key specifications include resolution capabilities, file format support, and pricing model (subscription vs one-time fee). Programs range from free options like GIMP to premium solutions like Adobe Photoshop, with features varying based on intended use. For high-speed imaging cameras, critical specifications include frame rate (60 fps to over one million fps), spatial resolution (which affects measurement accuracy), bit depth (8-12 bits for most applications), and light sensitivity (minimum 1200 ISO/ASA for monochrome). Both types of systems require careful consideration of resolution - in photo editing for image quality, and in high-speed cameras for accurate measurement and detail capture.","context":["- It’s easy to create professional-looking photos with today’s photo editors.\n- Free editors, such as GIMP, have similar features to Photoshop.\n- Android and iOS apps let you edit photos on the go.\nPhoto editing apps have come a long way since the early days of Photoshop. Today’s apps are easier to use, more powerful and more affordable. If you want to make use of Adobe’s fancy filters and plugins, you still need to get a Creative Cloud subscription. However, many plugins are cross-compatible with other programs, and you can replicate a lot of Photoshop’s best looks with other, more affordable programs. Read on for a list of some of the best free and premium photo editing programs that home users and professional photographers can utilize to create stunning images quickly and easily.\nThere’s No Need to Pay a Fortune for Photo Editing Software\nOne common misconception is that if you want to do professional photo editing, you need to spend a lot of money. This may have been the case in the past, but today you can pick up free or affordable graphic design and photo editing apps that offer professional features. Sites such as AlternativeTo can help you find open-source alternatives to the most popular big-name software packages.\nTop 10 Photo Editing Programs\nThe best photo editing program depends on your current skill level, the hardware you’re running it on and your photo editing goals. Premium tools, such as Photoshop and Lightroom, are powerful, but if all you want to do is spruce up some images for social media, you can get by with PiZap or Canva. These tools run well even on older hardware and give you access to some impressive photo editing features in their free tiers.\n1. Adobe Photoshop\nBest for: Graphic Designers\nPrice: Monthly Fee (from INR 1,655.60)\nPhotoshop is probably the best-known image editing program, and it’s been an industry leader for a long time. Today, you can purchase a monthly subscription to the Adobe Creative Cloud to get access to Photoshop and other tools in the Adobe suite. Photoshop is a powerful program that’s ideal for editing layered images and doing detailed graphic design work. However, it has a much steeper learning curve than its current rivals.\nBest for: Those looking for a free alternative to Photoshop\nGIMP is a free, open-source alternative to Photoshop. It has a large library of plugins that replicate many of the most popular Photoshop features, and skilled users can achieve impressive results with it. As with Photoshop, however, GIMP is a complex program, and it can be confusing for newcomers. There’s a large community of users, however, so it’s relatively easy to find help, either in the form of forums or tutorial videos that explain the program’s quirks.\n3. Adobe Lightroom\nBest for: Managing large photo libraries\nPrice: Monthly Fee (from INR 797.68)\nLightroom is Adobe’s dedicated photo editing solution. Because it’s designed purely for photos, it has a more streamlined interface than Photoshop. Lightroom makes it easy to touch up, crop and edit photos. It includes a useful tool for managing media libraries and has some quick and easy features for color correction, blurring, sharpening and other frequently used effects. Adobe offers an affordable Lightroom + Photoshop subscription package, too.\n4. Skylum Luminar AI\nBest for: Versatile photo editor and organizer\nPrice: Tiered pricing\nSkylum Luminar AI is an easy-to-use, affordable package that sits somewhere between Lightroom and Photoshop in terms of features. It offers a wide choice of preset workspaces for editing various types of photos. One thing that sets Luminar AI apart from many of the other premium packages on this list is the pricing. While the program costs money, it’s a one-time fee for ownership rather than a monthly subscription. If you expect to do a lot of heavy-duty photo editing, this may be an attractive way of buying your software.\nBest for: Social media enthusiasts\nPrice: Free tier available\nCanva is available as an app for Android and iOS plus as a browser-based tool. It’s an incredibly easy-to-use package for graphic design and basic photo editing. It’s aimed at people who want to make eye-catching banners or fun social media posts, offering drag-and-drop options for adding stickers, captions and frames to images. Canva doesn’t replace dedicated desktop apps for serious photo editing, but if you just want to put a sepia effect on a fancy dress photo, it’s more than up to the job.\nBest for: Plugin support and familiar Windows interface\nPaint.NET was originally designed to be an upgraded version of Microsoft Paint, but it’s grown to become so much more than that. Paint.NET now offers plugin support and sophisticated photo editing options. It’s definitely up to the task of casual photo editing. The interface is a little dated, and the program doesn’t have a Linux or MacOS version. However, it remains popular with Windows users thanks to its familiar interface and its generally reliable and stable performance.\n7. DxO Photolab\nBest for: RAW support and flexible export features\nPrice: Approximately INR 20,000\nDxO Photolab is a relatively new entry into the list of alternative photo editors. It features RAW support and allows you to save files in a wide variety of formats. DxO Photolab has a lot of options for colour correction, filters and customisable profiles to save you time and help you get consistent results. This package is available for both Mac and Windows users. A native Linux version isn’t available at this time, but the Windows version likely runs well enough under WINE.\n8. Corel PaintShop Pro\nBest for: Touch-ready for tablet PC use\nPrice: Tiered pricing available\nHistorically, Corel packages were considered a “cheap alternative” to professional graphic design packages, but that’s no longer the case. PaintShop Pro now offers a great user experience with its touch-friendly interface and easy-to-use options for effects and filters. The package even offers interactive tutorials to help would-be photo editors come to grips with some of the more powerful features. A 30-day trial and regular discounts are available.\n9. Adobe Photoshop Express Editor\nBest for: Photo editing on the go\nPrice: Free tier available\nThe Photoshop Express Editor is a browser-based version of Photoshop Express. Mobile app versions of the tool are also available. This free editor boasts an impressive list of filters and tools. However, it’s not suited for professional photo editing because you’re limited to saving your photos in JPG format. If you’re just hoping to touch up a photo to post on a blog or website, however, it’s a great solution.\nBest for: Browser-based editing on the fly\nPrice: Free tier available\nPiZap is a free, online photo editor designed to let you edit photos and share them on social media with just a few clicks. It features built-in integration with Google Drive, Dropbox and several social media sites, so you can upload a photo, make changes, then save or share it all from within your browser. The free version is surprisingly flexible. High-quality file exports, however, require you to pay for the premium package.","High-Speed Video: Selecting a Slow-Motion Imaging SystemThe demand for high-speed imaging has increased for a broad range of applications, from automotive crash testing to animal behavior to product component performance.\nAndrew Bridges, Photron USA, Inc.There is a growing market for imaging systems that provides an immediate, slow-motion view of a process that allows one to see events that happen too quickly for the human eye to perceive or comprehend.\nThe process of selecting a system that will suit a particular need or application can be difficult because of the wide range of available systems. This article will serve as a guide for evaluating the performance parameters and specifications of a particular system. Whether solving a costly production-line jam, watching a dummy’s head hit a steering wheel in a 50-mph head-on crash, capturing a shark attack on a Cape fur seal (Figure 1), checking the sabot separation from a tank-killing shell or simply trying to adjust your golf swing, the following will provide an overview. This discussion will include information about several cameras and systems and what questions to consider before purchasing a slow-motion imaging system.\nFigure 1. Shark attacking a Cape fur seal. Footage captured at 1000 fps (1024 × 1024 pixels) with Photron’s ultima APX camera. Courtesy of BBC-TV’s Planet Earth, “Pole to Pole” Episode.\nHigh-speed video cameras operate across a wide range of frame rates — from 60 frames per second (fps) to over one million fps. All high-speed video cameras operate at full resolution up to a certain speed, and then reduce the resolution, or window, to achieve higher speeds. It’s important to establish what frame rate you require to capture the event that you are viewing in slow motion. When recording a cyclical production, such as labeling or packaging that takes place at x number of times per second, it generally requires a minimum of three images per cycle to view and understand the phenomenon. If a box folding process on a production line occurs at 6000 units per minute, the procedure obviously will equal 100 boxes folded per second. With the above rule, it will be necessary to record the box folding process at a minimum of 300 fps to capture the process for easy viewing and comprehension.\nFigure 2. High-speed image sequence of Atlas missile launch.\nIf the event is not cyclical, such as a missile launch (Figure 2) or a vehicle impact test, then careful planning is required to capture the action at the most significant moment. It is important to determine what temporal detail must be measurable in the finished image sequence or output video.\nIn an automotive crash test (recorded at 1000 fps, per federal mandate), most of the action occurs within 0.01 s or 10 ms. In recording a missile launch, the speed of the action can be even higher. If a projectile is traveling at 500 m/s (the Sidewinder missile easily exceeds this), and there is a 100-m field of view (FOV), it will pass through the image window in 0.2 s or 200 ms.\nHowever, if you need to capture 100 frames within this 100-m FOV, you will need a camera that can take an image every 2 ms, which equates to 500 fps. If the FOV is reduced to 10 m while all other criteria remain the same, it will require 10 times (5000 fps) the speed to capture the same 100 frames. Frame rate comes down to how many images you want to see of the event, regardless of whether it is per cycle or the whole event.\nAnother area to consider when evaluating a high-speed digital video system is record duration or record time. This is often confused with how the camera is triggered, which will be discussed later. The real question is: How long does the process last or how much (in seconds) of the event need be recorded? High-speed videos use on board digital random access memory (RAM) to save the images. There are ways to push the record duration such as reducing the speed or resolution, but in essence you have to determine how long you need to record. The latest systems also enable users to push the recording time by reducing the bit-depth of the pixels recorded (more on this topic later). Reducing the bit depth from 12 to 8 bits will produce a fifty percent increase in recording capacity.\nIf the event is occurring intermittently, then the question is “How long a record time do I need?” A more relevant question would be “How do I trigger the camera so that I capture video every time the problem occurs?” Digital high-speed cameras can remain in record mode almost indefinitely as they cycle the data through their memory buffer on a first-in/first-out (FIFO) basis. This is a vast improvement over older film cameras that took time to get up to speed (a digital camera is instantly locked to any crystal stabilized speed you select) and then could only maintain that speed for a few seconds before running out of film. When the digital buffer is full, the first image recorded is automatically overwritten. The system continues to overwrite data until it receives a trigger signal such as an optical or audio trigger, switch closure, or a digital TTL trigger such as an alarm or keyboard keystroke.\nDepending upon how the system has been configured by the operator, it can save all the images recorded before the trigger signal was received, save everything after the signal came in, or a variable percentage of pre- or post-trigger images. Advanced systems can automatically download some or all of the saved images to a networked hard drive before automatically rearming to await the next trigger signal.\nResolution, or more correctly spatial resolution, must be considered when seeking the ideal system for your specific needs. The best example of why resolution matters is detailed in this real-life scenario. One customer needed to be able to measure within 1/10 in. in an 8.5 ft field of view. Since 8.5 × 12 = 102, the camera would have 102 in. to cover. In order to measure to 1/10 in., it would require 10 times this number, or 1020 pixels.\nFigure 3. Fastcam SA-X - megapixel resolution to 12,500 fps.\nDevelopments in motion tracking algorithms enable motion analysis software to track very accurately to about one-tenth of a pixel. However, it is still recommended that whenever possible, you have the full quota of pixels needed to discern what you are viewing. To achieve the desired framing rate (camera speed), you may be forced to sacrifice some of the resolution. The highest resolution that Photron’s Fastcam SA-X can achieve is 12,500 fps at full mega pixel resolution (Figure 3). When selecting a camera, it is important to determine what the pixel resolution is at the speed you require, since all high-speed video cameras reduce the resolution to achieve higher speeds.\nThe other form of resolution to consider is called bit depth, sometimes referred to as dynamic range. Bit depth refers to how many shades of gray the sensor uses to transition from pure white to pure black. Older systems used 8 bits, which means they utilized 256 steps to transition from white to black. Systems now offer either 10 bits (1024 steps), 12 bits (4096), or even 14 bits (16,384) which are essential for certain advanced applications. For the most part, 8 to 12 bits are more than enough, especially given that Windows is an 8-bit operating system and many times the sensor only produces eight to 10 usable bits, the remainder are lost in noise. In order to fully appreciate those additional two to six bits, you would need to invest in specialized and expensive hardware and displays. The additional bits are useful in mega pixel systems such as the Fastcam SA5 and SA-X and high definition (HD) cameras like the SA2, SA6 and BC2 systems because they offer the ability to select which 8 of the12 bits recorded you display. This can be a very effective means of extracting the maximum detail from shadows or other underexposed areas, or providing an additional means of prolonging the record time.\nAll high-speed systems should be available in both monochrome and color. They all use the same basic monochrome sensor, but the color versions have a color filter attached, which does sacrifice some light sensitivity, even when microlenses are utilized to maximize the amount of photons falling on the light-gathering part of the sensors’ pixel. Most systems adopt a color matrix known as a Bayer pattern to produce acceptable looking colors, from what is in reality a black-and-white sensor. This simulated color requires three bits to each of the monochrome pixels, which is why color sensors have three times the number of bits; 24 vs. 8 or 30 vs. 10, etc. If you do not have a critical need for color images, it is best to stick with monochrome systems, as they tend to be less expensive as well as more sensitive while providing comparable image quality.\nShuttering and light sensitivity\nIt is possible to record some high-speed images of a mousetrap closing (Figure 4) at 1000 fps with no additional shuttering, so the effective shutter or exposure time is 0.001 s, but upon closer examination it will be apparent that the trap jaws are quite blurred. One might assume more frames per second are needed. However, there are already sufficient images, but the images are blurred. The solution would be to increase the shutter speed.\nFigure 4. Image sequence of a mousetrap.\nShutter speed is often confused with the framing rate, but they are distinctly different. A 35-mm film camera has shutters ranging from seconds to thousandths of a second, but it still takes only one to three pictures per second at most. Similarly, if a high-speed camera is recording at 1000 fps, ideally it is gathering light (exposing the sensor) for 0.001 s. With digital gating electronics, the actual time the sensor is exposed to light can be reduced to microseconds or possibly less. With the mousetrap example, if we keep the record rate at 1000 fps but push the shutter from the reciprocal of the frame rate (0.001 s or 1 ms) to 100 μs, it will reduce the blur by one-tenth.\nDo not discount blur; it can be a very important consideration when working with high-speed events, especially projectiles where it can be used to accurately calculate the speed a projectile is moving if the framing rate and shutter exposure time are known. Photron recently won a contract to replace the aging film cameras at a major military test range. The contract required our high-speed digital cameras to be fitted with an automatic exposure control. The shutter needed to auto-adjust to compensate for changing lighting conditions, such as the sun appearing or disappearing from behind cloud cover. One important requirement was that the shutter speed would not be adjusted above a maximum predefined value that had been calculated to ensure the object of interest remained blur-free. In this case, it was better to be underexposed than to have any blur.\nIn the life sciences arena, light sensitivity is a major concern as high-intensity lighting has a tendency to generate a great deal of heat — not always a good thing when recording animals or insects. Because new CMOS sensors are even more sensitive than their CCD counterparts, they remove the problems of image blooming, (also known as whiteouts, tearing or smearing), where an illuminated hot spot results in a larger vertical streak throughout the image.\nQuantifying the sensor’s light sensitivity is an inexact science when using the more familiar ASA/ISO measurement units used to rate 35 mm film, though more and more companies are adopting ISO Ssat method 12232. Make sure any camera’s sensitivity is provided according to a recognized ISO or similar standard on not on one the manufacturer has made up! If the subject is light- or heat-sensitive or the production environment is problematic (e.g. some production lines use light sensors as safety guards and additional lighting can accidentally trigger them), it is best to avoid using anything less than 1200 ISO/ASA for a monochrome camera, or one-third that for color as it is important to note that color sensors are usually one-half to one-third as sensitive as their black-and-white counterparts.\nBecause of these varying and potentially confusing factors, the best advice for finding the right camera system for the right job is to invite the camera vendors in and insist on a live demonstration. It is easy to demonstrate a system in the conference room, under ideal and controlled conditions, but your purchase should be based on a real-life demonstration with actual conditions in the exact environment in which you’ll need the system to perform.\nConsidering the end use\nThe next consideration is the end use. While troubleshooting a gasket manufacturing line, an instant slow-motion review of the high-speed video recorded may be all that is needed. If it is necessary to save a portion of the image sequence for later review and/or analysis, you will need to determine some fundamental issues, such as how to get the images out of the camera’s RAM and into the real world.\nThe key is to look at your PC and determine what communication protocols it supports. Or you may be able to use the camera’s standard HD-SDI orRS-170 video outputs. Most PCs, including laptops, will include the means to connect to external devices via one or two Gigabit Ethernet ports. This should be the first choice unless you have more complex requirements, such as operating a camera located several miles away, which is often the case in military tests that involve explosives and/or projectiles.\nFigure 5. Photron’s PCI-1024, a PC-based, megapixel, high-speed imager.\nIt is best to stay away from protocols requiring specialized hardware unless your requirements demand them. It is also important to be able to download the images directly into a recognized and immediately usable format (AVI, JPEG, TIFF, etc.) onto a PC of your choice. Some systems require time-consuming post-mission file conversion while others can quickly download to a modified controller, but then take forever to download into the real world.\nWhat type of physical package does your application require? There is a wide variety of systems available, from inexpensive, low-resolution plastic units for almost disposable usage on the production line, to huge systems specifically built for long record times to cover a missile’s launch or re-entry into the Earth’s atmosphere. Some PCI systems use lower cost CCD or supersensitive megapixel CMOS sensors that are made for use in personal computers or laptops. More complex systems require housing that is engineered to operate reliably onboard crash vehicles or near missile impacts. All of these systems have strengths, weaknesses and differences that may influence your decision when considering your particular imaging requirements.\nFor systems that require use or control via a computer, it is essential to become familiar with the software that is supplied with the camera. The software should be easy to use and intuitive, without requiring a master’s degree in computer science. Some manufacturers, including Photron, will supply their systems with a software developer’s kit, LabVIEW, and MATLAB wrappers to enable advanced users to develop their own interface or integrate camera control into an existing interface.\nRequest a live demo\nAs with any relatively new technology, there is a lot of seemingly conflicting information. The main question is: What works best for your needs? The answer is in finding a comfortable fit with your requirements and following this single, important rule: Require the systems manufacturer to demonstrate the camera with a real-life, real-time demonstration, within the actual environment in which the high-speed imaging system will be used. A live, in situ demo will bring out the best and the worst of the high-speed camera systems you review. With that, you’ll have the information you need to make the best choice."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:b4bb1893-79eb-4273-8c57-3f603caeb2de>","<urn:uuid:2e69f0f5-13ba-49cb-b76a-abd354970e02>"],"error":null}
{"question":"How did Elvis respond when he was accused of making racist comments about black people?","answer":"When a quote circulated about black people only being good for shining his shoes and buying his music, Elvis did an interview with black-owned Jet magazine to refute the story.","context":["My mother loved Elvis Presley. Growing up, Elvis was always playing in the house or on long car rides, along with The Beatles and a variety of other huge pop acts of the '50s and '60s. I wasn’t a music historian (I’m still not, as far as I know) — I didn’t know who copied from whom or what the lives of any of these people were like. I just knew that the music was great.\nThat music is the primary focus of the new documentary Elvis Presley: The Searchers. It follows the evolution of Elvis’s sound from mixing up gospel, blues and country in the '50s through to the movie musicals and Vegas crooning in the '60s. And while the question of whether Elvis was a cultural appropriator of black music or merely its champion/messenger may have been settled in the minds of many, the doco goes to great lengths to show us that neither are really true.\n“He was a light for all of us. We all owe him for going first into battle,” Tom Petty says. “He had no road map, and he forged a path of what to do and what not to do. We shouldn’t make the mistake of writing off a great artist by all the clatter that came later. We should dwell in what he did that was so beautiful and everlasting. Which was that great, great music.”\nOf course, Elvis’s legacy isn’t just as a creator of great music. He’s “The King of Rock and Roll”. He single-handedly took popular culture into a new and exciting direction, leading John Lennon to famously say, “Before Elvis, there was nothing.”\nBut there was actually quite a bit before Elvis.\nAnd while the issue of whether he explicitly “stole” anything may not be as fiery a topic, the lingering resentment over a white performer becoming famous and profiting by adopting a black style is more than understandable. And it’s an important part of how we appreciate Elvis’s significance.\n\"He's singing the same thing I'm singing now. And he knows it,” the heavily influential blues guitarist Big Bill Broonzy, who ended up working as a janitor, said of Elvis. “'Cause really, the melody and the tune and the way we used to call it 'rocking the blues' years ago when I was a kid... that's what he's doing now. Rock and roll is a steal from the old, original blues.\"\nIn the early '50s, black performers like Little Richard, Joe Turner, Ike Turner, Ray Charles, Ruth Brown and Fats Domino were already making rock and roll — some white performers were too. 1951’s “Rocket 88” by Ike Turner is now more widely considered the first rock and roll song than “That’s All Right”, which is a cover of an Arthur “Big Boy” Crudup song.\nBut this was in pre-Civil Rights America, when black artists were marginalised as creators of what was called “race music” and largely ignored by the white mainstream audience, who were more interested in balladeers like Tony Martin and Johnnie Ray. At least, they pretended to be. There’s evidence that white audiences listened to black music privately, even though it wasn’t socially acceptable to do so publicly.\nWhite artists like Pat Boone and Bill Haley frequently sanitised and released the songs of black artists, who were often paid very little if at all by record companies. These covers often climbed much higher up the charts than the original versions, which were considered too dirty or sexual. Indeed, Elvis himself was subjected to a racist backlash over his “blackness”. Crudup ended up working on a farm. And Ike Turner said that he was only paid $20 for “Rocket 88”. “It was easier for them [to succeed], because they were white…” he said.\nBoone certainly didn’t feel bad about it.\n“Here’s the bottom line: there were lots of rhythm and blues artists, and they were doing well in their genre and they were famous and they had the charts and everything,” he said. “[But] the only ones anybody knows today are the ones that were covered by The Beatles, by Elvis, by me and by many artists.”\nIn fact, if you want to demonise a white rock star, you should probably start with Boone, whose covers of songs like Fats Domino’s “Ain’t That a Shame” are musical hate crimes. At the time, audiences preferred Boone’s version, but that doesn’t make them right. His cover is bland and terrible. And his versions of Little Richard’s “Tutti Frutti” and “Long Tall Sally” are even more abominable. (Little Richard, who initially resented Boone for his smooth, white-friendly, more popular version of his classic hit, eventually made peace with it.)\nBut Elvis was never an appropriator. He loved the music he heard on Beale Street in Memphis and in black churches. He believed in it and bought into it. He studied it in a way that a lot of white people at the time would not have. And you can hear that passion in his voice and see it in the way he moved.\nPaul McCartney described The Beatles as “plagiarists extraordinaire” for the way they took rock and roll and blues songs and made them their own. Elvis was no different. He, like all great artists, took from other originals to create his own originality.\nAnd even if there were already people making rock music, the way he blended blues, country and gospel with a passionate, soulful voice was truly original.\nAt the time, there was an acknowledgement of the idea that the success of Elvis and others was thanks to the fact that they weren’t black. But James Brown called Elvis “his brother” and Little Richard, who had a lot to be angry about given how his songs were stripped of their soul by white artists for white audiences, was similarly complimentary.\n“I thank God for Elvis Presley,” he said. “I thank the Lord for sending Elvis to open that door so I could walk down the road, you understand?”\nEventually, the backlash to Elvis came in full force, most notably in Public Enemy’s protest anthem “Fight the Power”.\nElvis was a hero to most,\nBut he never meant s*** to me…\nStraight up racist the sucker was simple and plain…\nIt was a stirring sentiment that made people think differently about how the music they loved was created and that talent alone has never been a gauranteer of fame and financial success. If it had, would Fats Domino have been more famous? Could Chuck Berry have been The King?\nAs for the racism accusation, it looks like that was overblown. When a quote about black people only being good for shining his shoes and buying his music was reported, Elvis did an interview with black-owned Jet magazine to refute the story.\nAnd Elvis seemed to only have glowing praise for his contemporaries, giving credit where credit was due.\n\"The coloured folks been singing it and playing it just like I'm doing now, man, for more years than I know,\" Elvis said. \"I got it from them. Down in Tupelo, Mississippi, I used to hear old Arthur Crudup bang his box the way I do now, and I said if I ever got to the place where I could feel all old Arthur felt, I'd be a music man like nobody ever saw.\"\nYears later, Chuck D said his lyrics weren’t a personal attack on Elvis as a racist. He was attacking Elvis (and John Wayne, who he doesn’t feel bad at all about attacking) as a symbol.\nIndeed, even if he adored the black musicians that inspired and came before him, Elvis has come to represent the cultural crime committed against them. He became a target for people who were (rightly) tired of the “parade of white heroes” they were taught to idolise. He didn’t invent the racist system that held him above his black contemporaries, but he certainly benefitted from it.\n\"To me, Elvis represented somebody who — because our country was not ready then to embrace the black artist and make them No. 1 — became No. 1 because of his rendition of what some black people sounded like,” trumpeter Wynton Marsalis said. “What made it distasteful is that we had people who could do it better than him, but who couldn't be accepted at that time because of the colour of their skin.\"\nFor me, it doesn’t take away from Elvis’s legacy to acknowledge the greatness of these artists.\nIn fact, that’s probably what Elvis would have wanted…\n“A lot of people seem to think I started this business,” he said. “But rock ‘n’ roll was here a long time before I came along. Nobody can sing that kind of music like coloured people. Let’s face it: I can’t sing like Fats Domino can. I know that.”\nSo here are just some of the songs by artists that inspired Elvis Presley — artists he admired and, in some way, even helped by becoming as big as he did:\nBig Mama Thornton — “Hound Dog”\nMaybe it’s just because of the ubiquity of Elvis’s version, but I prefer Thornton’s bluesy growl in “Hound Dog”. It’s a lowdown dirty banger.\nLloyd Price — “Lawdy Miss Clawdy”\nPrice’s version is a little smooth for my taste and Elvis has his mojo working for this one.\nChuck Berry — “Memphis, Tennessee”\nLike Elvis, Berry has been considered the king of rock and roll and was a big inspiration for Elvis, who sang a few of his hits, including “Memphis, Tennessee”, “Johnny B Goode” and “Maybelline”. I’m going with Berry’s version of this one — just a little bit cooler.\nLaVerne Baker — “Tweedle Dee”\nBaker’s version has something that Elvis’s doesn’t.\nJunior Parker — “Mystery Train”\nElvis’s version is smoother, quicker and a little more fun.\nArthur Gunter — “Baby Let’s Play House”\nGunter’s version swings and it’s very good. But I prefer Elvis’s. It’s iconic.\nRoy Brown — “Good Rockin’ Tonight”\nSmiley Lewis — “One Night (of Sin)\nArthur “Big Boy” Crudup — “So Glad You’re Mine”\nRay Charles — “What’d I Say”\nElvis’s version is from the movie Viva Las Vegas, which has him playing at some sort of whites-only dance party. It’s uncomfortable and certainly not as good as Ray Charles’s version.\nFats Domino — “Ain’t That a Shame”\nFats was a big deal to Elvis. So much so that when he was around, Elvis didn’t like to be called “The King”.\nBig Joe Turner — “Shake, Rattle and Roll”\nRoy Hamilton — “Unchained Melody”\nUltimately, Elvis was chosen by the public to be the one to bring rock and roll to the people and make it the cultural juggernaut that it is. He was an integrator, bringing black and white music together at a time when race mixing was a crime.\nAs Bruce Springsteen says in The Searchers, “Elvis and Elvis’ music pointed to black culture and said, ‘This is something that’s filled with the force of life. If you want to be a complete and fulfilled person, if you want to be an American, this is something you need to pay attention to.’”\nFollow the author on Twitter.\nWatch the first episode of Elvis Presley: The Searcher at SBS On Demand:\nPart 2 airs Sunday 3rd June at 8:30pm."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:304ffa74-69e1-45bd-b03a-5384ab149b06>"],"error":null}
{"question":"Hi, I'm researching online privacy. What's the difference between first-party and third-party cookies, and how does their data sharing impact consumer tracking?","answer":"First-party cookies are generated and managed directly by the website manager of the site being visited, while third-party cookies are generated by external parties. This distinction is important for consumer tracking because third-party cookies enable broader data sharing practices - companies can sell anonymized data to other parties, such as telecom companies selling geolocation data or credit card companies sharing data with advertising companies. First-party data holders may also share their data with other parties for various purposes including transaction completion, surveys, fraud prevention, and marketing. This extensive data sharing network increases the ability to track consumers across different platforms and creates comprehensive user profiles, contributing to the significant increase in online tracking capabilities.","context":["WHAT ARE COOKIES?\nCookies are small text files generated by the websites you visit, and which preserve your session's data for the purpose of facilitating their subsequent use within the site. These text data allow the website to preserve your information within web pages and to also analyze the way you interact with the website. Cookies are safe - they can only preserve information provided by the browser and relative either to browser access or as part of the requested web page. They cannot be turned into codes and cannot be used to access your computer. If a website encrypts information inside cookies, only the website itself is able to read such information.\nThere are two main categories:\n- technical cookies\n- profiling cookies\nTechnical cookies are generally needed for the correct functioning of the website and to allow navigation within the site; without them you may not be able to correctly visualize pages or use some of the services. For example, a technical cookie is indispensable to keep the user connected during the entirety of the website visit, or to memorize language settings, visualization settings and so on.\nTechnical cookies can be then divided into:\n- surfing cookies guarantee normal navigation and use (allowing, for example, online purchases or user authentication to access reserved areas);\n- analytical cookies are similar to technical cookies only when used directly by the site manager to collect information, in aggregated format, on the number of users and on the way they visit the site itself;\n- functional cookies have the purpose of improving service and allow the user to navigate the website based upon a series of selected criteria (for example, language preference or products selected for purchase).\nProfiling cookies collect user profiles and are used to send online advertising based on user-selected preferences while such user was navigating the site.\nCookies can also be classified as:\n- session cookies, which are deleted immediately upon browser closure;\n- persistent cookies, which differ from session cookies by being saved by the browser for a set period of time. They are used, for example, to recognize the device connecting to the site, therefore making user authentication easier;\n- first-party cookies are cookies generated and managed directly by the website manager of the site the user is surfing;\n- third-party cookies are generated and managed by parties other than the website manager of the site the user is surfing.\nWHICH COOKIES DO WE USE?\nWe make use of technical cookies in order to guarantee the correct functioning of our website. We also use third-party cookies by Google Analytics.\nGoogle Analytics is a web analytics service offered by Google Inc., which sends out cookies to your device. Data collected do not identify the user, but obtain statistical information on site navigation: pages visited, time spent surfing the site, browser used, and geographical data.\nGoogle Analytics was employed by this website to maintain the user's IP address anonymous.\nTo disable analytical cookies and to disallow Google Analytics from collecting navigational data, please download the following additional browser component:\nFor additional information:\nWe use the following third-party technical cookies:\n- Google Fonts: Google Fonts is a Google Inc.-managed service used to visualize character styles and allowing our website to integrate such information within our pages.\nFor additional information:\n- Widget Google Maps: Google Maps is a Google Inc.-managed mapping service allowing this website to integrate such contents within our web pages.\n- Google Docs Viewer: Google Docs Viewer is a Google Inc.-managed service allowing this website to view online documents and multimedia files without leaving the browser and accessing the local file system.\nCookies management: blocking and deleting\nIt is possible to visit www.aboutcookies.org for information on how to manage /delete cookies based on the type of browser used. Procedures are different for each browser. You must find the instructions under your browser's Help menu. If you use more than one computer, check that each browser is set in a way that suites your needs. Through your browser you can also view cookies found on your computer, delete some or all of them.\nEach browser requires different procedures to disable cookies; to deny cookies you may use the following instructions:\nMicrosoft Internet Explorer\nClick the 'Tool' icon in the upper right corner and select 'Internet Options'. Select 'Privacy' in the pop-up window. Set your cookie preferences. Otherwise, to set your cookie preferences use the following link:\nClick the 'Tool' icon in the upper right corner and select 'Settings'. Now select 'Show Advanced Options' (\"Under the hood'\") and change the 'Privacy' settings. Set your cookie preferences. Otherwise, to set your cookie preferences use the following link:\nFrom the drop-down menu in the upper left corner select 'Options'. Select 'Privacy' in the pop-up window. Set your cookie preferences. Otherwise, to set your cookie preferences use the following link:\nhttp://support.mozilla.org/en-US/kb/Enabling and disabling cookies\nFrom the drop-down set-up menu in the upper right corner select 'Preferences'. Select 'Security’. Set your cookie preferences. Otherwise, to set your cookie preferences use the following link:\nIf you are not using any of the above mentioned browsers, select 'cookies' in the appropriate section of the guide to find out where your cookie folder is located.\nThird-party websites that can be accessed through this site are not covered by the current information guide. Padua University, owner of www.upstore.it, declines any responsibility regarding these. Cookie categories used and type of treatment of personal information by these companies are regulated in conformity to the information given by such companies.","A rise in the number of devices has contributed to increasing amounts of generated data.\nTable of Contents\nWhat is Personal Data?\nPersonal data is information about a person that refers to an individual.\nSome types of data may be directly personal:\n- Email address\n- Home address\n- Phone number\n- Work history\nPersonal indirect Data is a description of someone which still enables identification. Personal Data may define race/ethnic origin, religious beliefs, political views, sexual orientation, and health conditions, and may demand more strict regulatory requirements. In some situations, consumers provide personal information for free. In others – data is found by giving consumer’s search history, location data, or buyer’s profile. Artificial intelligence and machine learning will drive inferred data.\nData on its own may not stand out as personal data, but when combined with other data it may be private if it identifies information about an individual. The ability to de-anonymise data has increased due to numerous facts. Marketers have become too liberal in the collection of consumer data. It describes radioactive data like customer data, e.g., personally identifiable, which could violate a business agreement if lost. There are about 15 data types, 6 of them are considered radioactive and 7 are considered toxic.\nWhat are the Purposes of Data Collection?\nThere are eight purposes of data collection.\nOne reason for collecting user data is providing the ability to deliver more targeted advertising, based on consumer’s features and demographics. Programmatic advertising became a result of buyer’s tracking. The rise of cookie leaking/syncing enhances the ability to target. According to a report, 5% digital marketers of 83% consider that people-based campaigns perform better than cookie-based.\nPersonalise and improve products/services\nUser’s Data is utilised by site hosts to improve website performance, as well as to personalise content and products, offering to individual consumers.\nAnalysis of data is used to detect fraud and to identify suspicious activity. Both fraud and the incidence of “false positives,’’ i.e., the rejection of a valid transaction can be a significant problem for financial institutions and merchants. Allowing a fraudulent transaction can require the merchant financially, but it can cost the financial institution as well, depending on the nature of an operation. Machine learning improves fraud detection as well as reduces the incidence of false positives.\nBusiness Efficiencies and Processes\nBy retaining customer data, an online retailer can pre-fill forms, e.g., delivery and card details. Data can also be used to decide what products to stock by location.\nThe remaining four reasons relate to a combination of data selling, leakage, sharing, and discrimination.\nSeveral companies sell anonymised data to other parties, e.g., telecom companies may sell data for geolocation tracking, and credit card companies sell these data to advertising companies. First-party data may also give their data to other parties to complete transactions, conduct surveys, and prevent fraud/for security, or marketing.\nHow are consumers tracked online?\nThere are many data trails that we leave as consumers, online and offline. They rise as the use of technology becomes more established in everyday life. Technological progress has resulted in a significant increase in online tracking.\nCookies are the most spreading way of tracking/collecting data which usually contain a string of text as a “unique identifier.” Storing relevant details about a user’s interaction with a site and preferences in this way helps facilitate a more user-friendly experience. When a member returns to a particular website, cookies allow the website to recognise a user’s web browser and remember information about him.\nThere are different types of cookies:\n- Session cookies: Enable websites to link the actions of a user during the browser session\n- Persistent cookies: Allow to check user information and settings/preferences\n- First- and third-party cookies: Refer to the website or domain placing the cookie\nIt is usually the persistent, third-party and tracking cookies which are of most concern from a privacy perspective. These cookies are hard to delete and they have expiration dates that can extend with the website.\nFlash cookies have also become popular. This information can be kept on a computer which is designed to save data, e.g., scores on games. It is difficult to delete them in the same way as other cookies, which means some companies use them to reload other cookies back onto a computer.\nWe can also distinguish ‘Super Cookie’ and ‘Zombie Cookie.’ A super cookie is designed to be permanently stored on a user’s computer and is harder to find out and remove than regular cookies. A Zombie Cookie is projected to return to life after being deleted.\nThere are some newly developed forms of tracking:\nFingerprinting involves collecting unique identifying patterns of information to define a specific device or application. It often consists of gathering unique identifying patterns of information\n- Web Beacons / Pixel Tags are small objects embedded into a webpage or email which are not visible to the user. When a page with one of these objects loads, it will make a call to the server for the object, and this allows the company to know that someone has loaded the page\n- Mobile Tracking\nA new age of connectivity has influenced on such openings.\nThere are several ways of consumer’s mobile tracking:\n- Advertising ID: Apps downloaded on a phone show advertising based on advertising IDs. These include Apple’s Advertising Identifier (IDFA), Android’s Advertising ID and Facebook App User IDs\n- Wi-Fi: When a phone is linked up to a Wi-Fi network, sensors can use the phones media access control (MAC) address to track movement, e.g., around a store\n- Carrier: The mobile carriers may provide de-identified data to third parties for advertising and other purposes\n- GPS: Geolocation tracking (via GPS satellites), e.g., if you pass a particular restaurant on a regular basis, the restaurant could use the information to offer a coupon\n- iBeacons/Antennas: Small wireless devices that use radio signals to communicate with mobiles/tablets\n- Facial Recognition uses biometric software which identifies individuals in a digital image\n- Cross-Device Tracking pulls together disparate datasets to create a picture of consumer behavior/usage as device proliferation has increased\nData protection is not only a dynamic space in Europe. Technological trends are driving changes on a global scale. The implication has been growth, and rising heterogeneity, in the data protection landscape.\nThe United States has taken a more principle and sector-specific based approach than the European Union. U.S. regulators have been willing to work collaboratively with industry operators, encouraging self-regulation. Legislation has been used to address specific risks.\nFair Practice Principles\nModern Data Protection in the United States remains based on traditions stemming from ‘fair practice’ principles first enunciated in 1973.\nCalifornia Has Been the Pioneer at a State Level\nSome states have led the way in privacy regulation, pushing ahead of the Federal Government.\nIn some cases, state provisions can converge to a considerable degree. For example, a notification requirement exists in 47 U.S. States as well as the District of Columbia and Puerto Rico. Despite this, there are significant divergences in areas where states have specific concentrations regarding industry exposure, such as New York for financial services.\nIndustry-Specific Approaches; From Self-Regulation to Legislation\nThere is sector-specific legislation that includes requirements around Data Protection. The framework applies to all businesses that use consumer data.\nThe framework has three key components:\n- Privacy by Design\n- Simplified Consumer Choice\nAsia-Pacific Economic Cooperation (APEC) Privacy Approach Economically Driven\nAPEC’s aims in the area of data protection are exclusively economic. The framework builds public confidence in the safety and security of data flows and to realise the potential of electronic commerce. It is a contrast to Europe where the critical aims of data protection revolve around the protection of fundamental rights and freedoms.\nWhat are the Main Issues for Corporates and Investors in Data Protection?\n- Although the opportunities from data are significant, a lot of speakers have underplayed and underestimated the challenges associated with ensuring ePrivacy and adequately protecting personal data.\n- The implementation of the GDPR in May 2018 represents one of the most excellent events in ePrivacy/data protection regulatory history. The regulation is a game changer regarding not only its scope and ambition but also the significant penalties for non-compliance.\n- The purpose of the developing data protection regulation in Europe is the transferring power to consumers, and in the end, the increase of transparency and trust in how companies use consumer’s data. Best case, consumers become more trusting. The outcome being that data quality improves, and data as an asset increases in value.\n- The very same companies that the GDPR was potentially designed to constrain will probably end up being least affected. Of course, there will be increased regulatory costs associated with compliance, but those larger companies that already have consumer trust and the necessary resources to remain compliant with much stricter and more complex EU rules will likely be less disrupted than smaller enterprises. For all companies, it will probably require a cultural change.\n- The asymmetric approach toward regulation between different regions could lead to a similar level of asymmetry regarding access to investment and 21st century consumer services – artificial intelligence, machine learning, and the Internet of Things.\nIt is in the interests of companies to provide comprehensive data privacy assurances. The World Economic Forum estimates that capturing the share of data privacy-conscious consumers should put about $330 billion at stake in 2015-25. The number of buyers moving to companies with strong data privacy measures is estimated to grow from 5% in 2015 to 25% in 2025.\n- Personal data is any information relating to an identified or a living person\n- The purposes of data collection are diverse\n- Cookies are the most spreading way of tracking/collecting data\n- There are five key issues for Corporates and Investors in Data Protection"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:e48f8822-66ec-4b62-967c-f193c58f9509>","<urn:uuid:412b27f0-b14c-48dc-95fc-1e76dc3de4eb>"],"error":null}
{"question":"What are the key vulnerabilities to extreme weather events in Italy versus Spain, particularly regarding heat waves and their impacts?","answer":"Both countries face significant but distinct vulnerabilities to extreme weather events. In Italy, extreme event risk has increased by 9% in the last twenty years, with impacts particularly affecting vulnerable populations like children, elderly, and disabled people through increased mortality from heat-related conditions such as heart diseases, strokes, and respiratory problems. In Spain, heat waves have become increasingly severe, with projections indicating that by 2070, Mediterranean regions will experience up to 16 days annually with temperatures above 40.6°C - a threshold where vital human organs cannot function normally. These extreme temperatures in Spain also severely impact agriculture, as evidenced by the 2009 heat wave that damaged pepper plantations.","context":["It could be worth up to 8% of GDP per capita, exacerbate the differences between north and south, between society’s rich and poor, as well as affect a number of Italy’s strategic sectors: climate change is a risk accelerator for many aspects of both the economy and society. The report, “Analisi del rischio. I cambiamenti climatici in Italia – Risk Analysis. Climate Change in Italy” has been published. Realized by the CMCC Foundation, Euro-Mediterranean Center on Climate Change, it is the first integrated analysis of climate risk in Italy. A document that bases itself on climate predictions for the coming years whilst focusing on specific sectors so as to provide information on what to expect from the future. It is a valuable support tool for concrete resilience and sustainable development strategies.\nRisks associated with climate change affect all Italian regions and their economic sectors. Despite contrasts, with different areas being affected in different ways, there are no regions that can be considered immune from climate risks, which have already increased in recent years, in particular when considering extreme events.\nThe analysis carried out by the CMCC Foundation starts from climate scenarios that, through an advanced use of high-resolution climate models applied to the study of the Italian context, provide information on Italy’s future climate. This information is then applied to risk analysis for a number of sectors of the Italian socio-economic system. What emerges is a framework where, in the coming decades, risk grows in many areas with significant economic and financial costs for the country. Furthermore, the impacts will affect disadvantaged members of society more severely and also involves all sectors, not least of which infrastructure, agriculture and tourism.\nRisk, scientific knowledge and response strategies\n“This report contains the most up to date and advanced knowledge of the impacts and integrated risk analysis of climate change in Italy”, explains Donatella Spano, member of the CMCC Foundation and professor at the University of Sassari, who coordinated the thirty authors whose five chapters make up the research study. “Analysis of risks and their effects on environmental, natural, social and economic capital allow us to take response options identified by scientific research into account and develop integrated and sustainable management plans for the Italy, enhancing specific features, peculiarities and competences of the different territorial contexts”, continues Spano. “This knowledge is the result of innovative research, networking between the universities that contribute to the CMCC Foundation’s work, and international collaborations. It is also the product of top-level computing infrastructure at a global level. Putting all these aspects together in a multidisciplinary research perspective is a scientific community endeavour, the results of which are at the service of society and produce knowledge that benefits the entire country.”\n“The challenge of risk connected to climate change – concludes Donatella Spano – starts from scientific knowledge and integrates adaptation and the solutions needed to face risks, in all phases of the decision-making processes including public policies, investment programs and the planning of public expenditure, so as to guarantee sustainable development at all territorial and governance levels.”\nThe report addresses the issues summarized below and is accompanied by a series of key messages, infographics and an executive summary that facilitates the reading and use of the report (available at this link).\nItaly’s expected climate. The different climate models used concur in evaluating an increase in temperature of up to 2°C in the period 2021-2050 (compared to the period 1981-2010). In the worst-case scenario, the temperature increase may reach 5°C. Summer precipitation decreases in the central and southern regions, whereas intense precipitation events increase. In all scenarios, the number of hot days and periods without rain increases. The consequences of climate change on the marine and coastal environment will have an impact on coastal “ecosystem goods and services” that sustain socioeconomic systems through the provision of food and climate regulation services. (See the infographic).\nAggregate risk for Italy. Adaptation capacity and resilience are themes of concern for the entire Italian region, from north to south. Although the northern regions are richer and more developed, they are not exempt from climate change impacts, nor are they more prepared to face them. As for extreme events, the probability of risk has increased by 9% in the last twenty years in Italy.\nEconomic costs, tools and financial resources. The costs of climate change impacts in Italy increase exponentially as temperatures rise in the different scenarios, with values ranging between 0.5% and 8% of GDP by the end of the century. Climate change increases economic inequality between regions. All sectors of the Italian economy are negatively affected by climate change. However, the greatest losses concern the country’s networks and infrastructure, as well as the agricultural and the tourism sector, in both summer and winter. Climate change will require sizeable investments and represents an opportunity for sustainable development, which is recognized by the European Green Deal as the only possible development model for the future. Today is the best moment for new ways of doing business and for new ways of sustainable land management to become part of the know-how of companies and of public, local and national bodies. (See the infographic)\nCities and urban environment. Children, the elderly, the disabled and the most vulnerable members of society will be those who suffer the most from the increase in average and extreme temperatures, the greater frequency (and duration) of heat waves and intense precipitation events. Indeed, we expect an increase in mortality due to ischemic heart diseases, strokes, nephropathy and metabolic disorders due to thermal stress, and an increase in respiratory diseases due to the link between phenomena related to temperature rise in the urban environment (heat islands) and concentrations of ozone (O3) and fine dust (PM10). (See the infographic)\nGeo-hydrological risk. From the combined analysis of anthropogenic factors and climatic scenarios, the worsening of an already very complex situation is expected. Rising temperatures and an increase in localized precipitation phenomena play an important role in exacerbating risks. For the former, melting snow, ice and permafrost indicates that the areas most affected by variations in magnitude and seasonality of instability phenomena are the Alpine and Apennine regions. For the latter, intense precipitations contribute to a further increase in the hydraulic risk for small basins and the risk associated with superficial landslides in areas with higher permeability soils. (See the infographic)\nWater resources. Most of the impacts of climate change on water resources envisage a reduction in the quantity of both surface water and groundwater in almost all semi-arid regions, with a consequent increase in risks for Italy’s sustainable development. The expected changes in climate (drought, extreme events and changes in the rainfall regime, reduction in the flow rate), entail risks for water quality and availability. The most relevant risks for water availability are linked to high competition between sectors (civil, agricultural, industrial, environmental, energy production), which worsens in the hot season when resources are scarce and demand increases (for example for agricultural needs and tourism). (See infographic)\nAgriculture. Agricultural systems may suffer from increased production variability, with a tendency towards yield reduction for many cultivated species, accompanied by a probable decrease in the qualitative characteristics of the produce. However, impacts are vary significantly depending on the geographical area and specific crops in question. Negative impacts are also expected for the livestock sector, with both direct and indirect impacts on farmed animals and consequent repercussions on the quality and quantity of production. (See infographic)\nWildfires. Rising temperatures, reduction in average annual precipitation, and greater frequency of extreme weather events such as heat waves and drought will interact with the effects of abandonment of cultivated areas, pastures and areas that used to be managed forests, a strong exodus towards cities and coastal areas, and increasingly efficient monitoring, prevention and active control activities. Climate change is expected to further exacerbate specific components of fire risk, resulting in impacts on vulnerable people, assets and ecosystems in the most vulnerable areas. Increases in the danger of wildfires, shift in altitudes that are considered vulnerable areas, lengthening of the fire season and an increase in extremely dangerous days are expected. This may translate into an increase in burnt areas, with a consequent increase in greenhouse gas emissions and particulate matter, with impacts on human health and the carbon cycle. (See infographic)\nRisk Analysis. Climate Change in Italy\nSpano D., Mereu V., Bacciu V., Marras S., Trabucco A., Adinolfi M., Barbato G., Bosello F., Breil M., Coppini G., Essenfelder A., Galluccio G., Lovato T., Marzi S., Masina S., Mercogliano P., Mysiak J., Noce S., Pal J., Reder A., Rianna G., Rizzo A., Santini M., Sini E., Staccione A., Villani V., Zavatarelli M., 2020. “Risk Analysis. Climate Change in Italy”.\nAll material available at the following link: https://www.cmcc.it/it/analisi-del-rischio-i-cambiamenti-climatici-in-italia","Climate Change Issue in Spain: Temperature Incensement\nNowadays mankind is facing huge global ecological problems, most of which are the results of people’s activity. One of such problems is climate change occurring worldwide. A variety of countries suffer from damages caused by climate change, and the main focus of this research paper is to examine these issues in Spain. Over the past 100 years, average global temperature in Europe increased by 1.2C, the 1990s were the warmest decade in the last 150 years. It has been predicted that the average temperature in this region will increase by 1.4-5.8C during 1990-2100. The largest increase is expected in Eastern and Southern Europe. In such a context, Spain is a relevant example of a country that faces and copes with major climate changes. It also has to be mentioned that since 1980 half of the glaciers in Spain have melted. This paper describes major climate changes in Spain, specifies the country’s major climate change – the one in temperature, as well as defines and explains environmental, social, and economic aspects of this issue.\nSpain is a country located in southwestern Europe and, partly, in Africa. Spain is one of the warmest countries in Southern Europe. The average number of sunny days equals 260 to 285. The average annual temperature on the Mediterranean coast is 20C. Usually, in winter, the temperature decreases below 0C only in the central and northern parts of the country. In summer, the temperature rises up to 40C and higher (from the central part to the southern coast). On the northern coast, the temperature is not so high – only about 25C.\nThe climate of Spain is considered one of its most important natural resources. The country occupies the first place in Europe based on the number of sunny days per year. Spain is almost completely located in the subtropical climate zone and its natural conditions are similar to other Mediterranean countries, but differ in respect of being close to the Iberian Peninsula. The proximity to Africa and the influence of steep terrain and extensive adjacent waters of the Atlantic Ocean and the Mediterranean Sea play crucial role in the country’s climate formation.\nSpain will reach the temperature of North Africa by 2050 if climate continues changing at the current rate. The report, prepared by Cambio Climático and measuring climate changes since 1950 with a forecast up to 2050, warns that some of the most popular and successful agricultural crops of Spain may be at risk if nothing is done to prevent change of the climate in the country. Andalusian olives, lemons and even famous Valencian vineyards of La Rioja may be at risk if the temperature increases similar to Morocco’s. In Europe, the rate of warming is higher than on other continents, and the hottest decade for European countries was in the years 2002-2011.\nIn Spain, the situation is more serious than in other countries as its temperature has risen by an average of 0.5C, whereas in other countries – 0.2C per decade since the beginning of the 20th century. In addition to the decline in rainfall since 1950, it becomes clear that Spain has become much warmer and drier.\n“The new report shows that climate change is the reality of Spain”, says Jonathan Gomez Cantero, who worked on the report. ‘People need to be informed and know the reality of this phenomenon. If the temperature in Southern Europe increases only by two degrees, it will be equal to the temperature in North Africa’.\nHeat waves are periods of abnormally high temperatures causing a range of negative consequences from destruction of crops to increased mortality. In Spain, heat waves are becoming more and more common. One of them, in July 2009, severely damaged the planting pepper plantation in Miguel Campa. In the years 1961-1990, heat waves lasted for an average of two days a year, while in the years 2021-2050, according to experts, the period will become 13 days long. According to the same forecasts, the number of days with temperatures above 40.6C (the value of possible drastic violation of thermoregulation when vital human organs cannot function normally) in the Mediterranean countries will increase to 16 a year by 2070.\nClimate change in Spain has a negative impact on the economy, because the main part of the state budget’s profit consists of winemaking. The consequences of rising temperatures include overmaturity of grapes, drying, increase in acidity and vulnerability to pests and diseases. High levels of carbon dioxide accelerate the process of photosynthesis, which leads to a change in the process of rising and maturation of wine grapes. In addition, due to the high temperatures, the harvesting season might shift from early October to September. These changes may adversely affect the taste and vitality of grapes. More hot weather will cause a reduction in the acidity of grapes and, accordingly, a significant change in the taste of wine.\nSpain’s tourism industry could be at risk as well. The reason of this is behind climate change that causes increasingly hot and dry summers. This could potentially lead to higher rates of drought, forest fires and loss of some wild animals. In addition, the temperature which is too hot makes visiting the Mediterranean countries at the height of season less comfortable. These factors have the potential to make Spain less attractive to tourists, while the countries located to the north, with more moderate summers, can get, as a result, growth in tourist revenues. It is estimated that the Southern Mediterranean countries may lose tourism revenues amounting to 0.45% of GDP annually. For Spain, this may mean the loss of up to €5.6 billion annually.\nThe Spanish government approved a list of measures to combat climate change. The list consists of eighty points and is particularly aimed at reducing emissions of greenhouse gases. The Spanish are going to cut the 2012 emissions of greenhouse gases by 37% to meet the Kyoto Protocol requirements. The government decided to encourage purchase of vehicles that are less polluting. At the same time, the ‘dirty’ vehicles purchase tax was significantly increased. The owners of vehicles in which the amount of carbon dioxide is less than 120 grams per kilometer will not pay the tax, while the drivers whose number is 120-160 grams will have to pay 4.75% of the vehicle price, 160-200 grams – 9.75%. About 56% of cars in Spain are classified as highly pollutant. The government decided to spend 2.5 billion Euros on the development of renewable energy sources, the ban on weekend pipes in 2012 and gradual transition to cars on biofuels.\nThus, the climate change issue in Spain is very serious. It affects not only people’s lives, but the country’s economy as well. If Spanish government does not implement measures against the climate change issues, the future of the country will be sorrowful. But, as long as the government is aware of this problem and actively fights climate change by introduction of sets of measures, the situation can be successfully changed."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:7f520ec0-4d9c-4016-9716-ee8abd2233fc>","<urn:uuid:92ff7d38-a344-42cf-8c00-28b91d351e83>"],"error":null}
{"question":"As a philosophy student studying vagueness concept, me want compare how Sartre y Russell differ en their approach to linguistic precision. Could you explain principales diferencias?","answer":"Sartre and Russell had contrasting approaches to linguistic precision. Russell was deeply troubled by semantic vagueness and sought to create a logically ideal language to escape natural language's problematic vagueness. In contrast, Sartre embraced a more fluid approach to language, as evidenced by his shift from using the term 'consciousness' to 'le vecu' (lived experience) - which he described as 'neither the preconscious, nor the unconscious, nor consciousness, but the terrain in which the individual is perpetually overflowed by himself.' This fundamental difference reflects their broader philosophical perspectives - Russell's analytical approach seeking precision versus Sartre's existentialist philosophy emphasizing subjective experience and the complexity of human consciousness.","context":["thinker born in 1905, engagement philosopher\n, existentialist, writer, Nobel Prize winner (he declined it).\nHis existentialist philosophy, proposes no god, no ethic\n, no moral, and was meant to be a cleaning of the old secular\nvalues, where god is replaced by some ethical statements. It completely denied the existence of some kind of rules on to behave. The solution was the subject being conscious of his position towards the world, his question was made to be: \"What would happen if all acted this way\nThe decision of the subject in good faith\n, and freedom, was the real act of man\nHis literature, not meant to be a completion of his philosophical work, but a parallel creation, nevertheless contained a lot of his philosophical symbols and the characters were situated inside the existential anguish that Sartre described as the consequence of freedom.\nHere are some of Sartre's more memorable and delectable citations or extracts:\n”I have replaced my earlier notion of consciousness (although I still use the word a lot), with what I call le vecu - ‘lived experience’. I will try to describe in a moment what I mean with this term, which is neither the preconscious, nor the unconscious, nor consciousness, but the terrain in which the individual is perpetually overflowed by himself and his riches and consciousness plays the trick of determining itself by forgetfulness.” (1969)\n“A simple formula would be to say that life taught me la force des choses – the power of circumstances.” (1969)\n“But the most striking feature of the man, it seems to me, was the metaphysical anguish which he endured so openly and modestly. Not a single day passed without him being tempted to kill himself. But this suspended death gave him a kind of charming and destructing irony - his native intelligence, which was above all the art of finding and establishing in his daily life, and even in his perception, a lethal duet to which he submitted all the objects of this world.” (Mallarme: the poetry of suicide)\ngarcin: Wait a minute, there's a snag somewhere; something disagreeable. Why, now, should it be disagreeable? ...Ah, I see; it's life without a break.\nvalet: What are you talking about?\ngarcin: Your eyelids. We move ours up and down. Blinking, we call it. It’s like a small black shutter that clicks down and makes a break. Everything goes black; one's eyes are moistened. You can't imagine how restful, refreshing, it is. Four thousand little rests per hour. Four thousand little respites--just think!...So that's the idea. I'm to live without eyelids.\nhuis clos: Wait a minute, there's a snag somewhere; something disagreeable. Why,now,should it be disagreeable? ...Ah,I see; it's life without a break.(Huis Clos)\nBiography of the man:\n1905 - born 21 June\n1906 - 21 September his father dies, due to a lung disease. Sartre goes to Paris with his mother, to live with his grandparents\n1914 - First World War\n1917 - His mother marries Joseph Mancy\n1917 - Enters the Lyceum of La Rochelle\n1918 - First World War ends\n- Starts teaching in Lyceum\n1938 - Publishes La Nausee\n1939 - Second World War starts. He enters the army as a meteorologist, June 21\n1940 - Taken prisoner by the German force.\n1941 - March: regains freedom. April 2nd, returns to Paris after a year spent out of the city\n1943 - June 3rd Les Mouches\n1943 - June: Nothingness and being\n1944 - Huis Clos\n1944 - Paris is liberated\n1945 - Travels to USA\n1954 - Travels to USSR\n1958 - The Freud Scenario\n1960 - Critique de la raison dialectique\n1960 - Travel to Cuba\n1960 - Camus dies\n1967 - Travel to Israel\n1964 - Wins Nobel prize and rejects it\n1970 - Becomes director of la cause du people\n1972 - Sartre par lui meme by contat and astruc\n1978 - Publishes Pouvoir et Liberte in Temps Modernes\n1980 - April 15, dies in Broussais hospital","- Modernist Fiction and Vagueness: Philosophy, Form, and Language by Megan Quigley\nIn 1922, the Cambridge philosopher Bertrand Russell gave a lecture entitled “On Vagueness.” In Megan Quigley’s excellent new book, Modernist Fiction and Vagueness, Russell’s lecture, while not as decisive as the other literary and philosophical texts of that annus mirabilis, is importantly incisive. Russell, she argues, highlights a significant philosophical issue—the problem of vagueness—in the intellectual history of modernism. But in Quigley’s book, Russell is just one of many writers and thinkers taking up the problem of vagueness in the first decades of the twentieth century. Others include Russell’s former students and (former) friends, Virginia Woolf, Ludwig Wittgenstein, and T.S. Eliot. Quigley connects James Joyce to this group via, unexpectedly and usefully, C.K. Ogden. She also identifies attention to vagueness in the work of writers associated with pragmatism, including C.S. Peirce and William and Henry James. Indeed, one of Quigley’s central and valuable aims is showing how in both Cambridges—one the birthplace of analytic philosophy and the other the birthplace of pragmatism—literary writers and philosophers shared a common concern about vagueness and what it reveals about the relationship of language to truth.\nIn philosophy, this concern manifests as attention to the vagueness of language, a problem that goes back at least as far as the sorites paradox of Classical Greece, which demonstrates the fuzziness of semantic concepts, such as a “heap” of sand (if you add grains of sand one by one, when does it become a “heap”?). Russell, Ogden, Peirce, and the early Wittgenstein were troubled by semantic vagueness; Russell’s logically ideal language, Ogden’s basic English, Peirce’s emphasis on clarity, and the propositions that Wittgenstein lays out in the Tractatus are attempts to escape natural language’s problematic vagueness. James and the later Wittgenstein similarly acknowledge the vagueness of language but do not find it problematic or troubling, in part because their priorities are (or are now) in representing thought and language as they are used in practical situations in ordinary life.\nQuigley shows how modernist novels anticipate this second curve of the linguistic turn. Henry James’s novels grow, in his own estimation, richer in meaning as they grow more vague: as his style becomes more imprecise, the characters become more diffuse, and the plots thinner. Virginia Woolf similarly increasingly embraces vagueness in her novels; her “sense of community” depends on people being connected “in some vague way,” even though vagueness, by endorsing subjectivity, risks solipsism (quoted on 99). The relationship between vagueness and community is picked up by the next chapter on James Joyce. Here, vagueness is tied to sex; the teenage Stephen Dedalus is infected with it after he kisses a prostitute. The infection spreads: Quigley traces vagueness ramifying throughout Joyce’s oeuvre and argues that Joyce’s transformation from an early idealist belief in logical language to his late diffuse and deliquescent prose parallels Wittgenstein’s evolving beliefs about language’s necessary vagueness. The final chapter, on T.S. Eliot’s criticism, finds this same arc: Eliot’s early criticism invokes and attempts to mimic scientific precision, while his later writings acknowledge and even value vagueness.\nThe great strength of Quigley’s work is her ambitious and rigorous interdisciplinarity. She reconstructs a conversation about what she calls the “crepuscular” with clarity and precision. The philosophers she discusses are among the most important, and among the most difficult, of the twentieth century. The same is true of the literary writers. And in discussing both philosophy and literature, Quigley is able to combine breadth—she discusses large oeuvres over the course of long careers—with the depth of an expert close reader. The philosophic and literary figures in [End Page 472] this book have long been canonical and so long been the subjects of critical industries; Quigley provides not only new ways to read them, but also, in her thorough bibliographic work, a resource for literary scholars. This is a book that is both dense with..."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:b68fc14c-efe1-4683-b157-be304a4cc827>","<urn:uuid:53f7f731-7ab8-4510-857e-1d49dc961f64>"],"error":null}
{"question":"What documentation techniques and tools were used for preserving ancient architectural details in both the Regia terracotta analysis and the Taharqa Gate recording?","answer":"In documenting the Regia terracottas, careful attention was given to describing and interpreting each piece through traditional analysis methods, while the Taharqa Gate documentation employed both traditional and digital techniques. For the Regia, researchers focused on visual and historical context analysis of the fragments. For the Taharqa Gate, the process began with penciling on photo enlargements at 1:4 scale using silver nitrate negatives from the 1930s, which preserved more reliable detail than the current structure. The documentation then evolved to use digital tools, particularly the Wacom Intuos Pro tablet, which offered features like the Pro Pen 2 with 8,192 pressure levels, Bluetooth connectivity, and multi-touch capability. The process included scanning drawings at 1200 dpi, preparing Photoshop files, and incorporating historical photographs as layers for reference, allowing for precise digital documentation of the deteriorating monument.","context":["Susan B. Downey, Architectural Terracottas from the Regia. Memoirs of the American Academy in Rome, Vol. 40. Ann Arbor: University of Michigan, 1995. Pp. 109. $42.50. ISBN 0-472-10571-X.\nReviewed by Nancy A. Winter, American School of Classical Studies, firstname.lastname@example.org.\nThis long-awaited presentation of the terracotta roof fragments from the American Academy in Rome excavations in the Regia is a most welcome contribution to our knowledge of the early architecture of Rome. The Regia, as the house of Rome's highest religious official, the Pontifex Maximus, was one of the most important structures in the Roman Forum. At least two successive roofs decorated this building: the earlier, belonging to the third building phase, is dated ca. 570 BC and the later, of the Fourth Regia, to the third quarter of the 6th century BC (ca. 530 BC by implication that the latest pottery in the destruction debris of the Third Regia dates 540-530 BC).\nThe roofs are indeed pitifully fragmentary, but Downey has done a masterful job of recreating their original state -- without ever falling prey to over-speculation. Careful attention is given to describing and interpreting each piece, and placing it in its wider context, both visually and historically. She exhibits good control of the complex literature covering a wide range of topics from iconography to socio-political history, and provides a very logical assessment of the wide-ranging theoretical interpretations which have previously been expressed about some of the early architectural terracottas from Rome, Etruria and Latium. Far from providing merely a catalogue of the pieces from the American Academy excavations, she effectively summarizes our current understanding of the 6th century BC roofs of Rome and the sources that influenced them.\nThe Third Regia consisted of two rooms facing inward from North and Southwest into a walled courtyard; a roofed portico may have connected the two. The south building carried the best-documented roof. Based on findspots, Downey assigns the two different types of revetment plaques, one with walking felines and a Minotaur, and the other with walking felines and a bird, to the wooden slopes and horizontal architrave, respectively; both scenes have examples moving to left and to right, as is regularly the case for decoration of pediment slopes, and display two different styles of crowning mouldings, which necessitates their different placement of the roof. An alternate suggestion is that there was a decorated back pediment, looking out onto the Forum, in addition to that facing the courtyard, and that only these two pedimental slopes were so decorated. Above the revetment plaques ran a raking sima with tall cavetto profile, the inner curve of which carries tall, convex stribils, above a flat fascia with a painted guilloche. The end of the ridge tile may have carried a disc acroterion with a painted tongue pattern on the outer edge and possibly a central Gorgoneion. A life-sized terracotta foot in a boot probably represents an acroterion from the ridge.\nDowney suggests that the north building carried antefixes with a relief Gorgoneion, and possibly the same type of raking sima as the south building. A fragmentary antefix with female head could also belong to this roof or to the portico between the two buildings; the latter association would conform more to the practice at Poggio Civitate (Murlo), where the Gorgoneia occur alone on the eaves, and Acquarossa, where female heads occur alone on a portico.\nRegia IV has scantier evidence for the roof, but nonetheless highly significant and controversial. The three fragmentary antefixes decorated with female heads are of a type well-documented at Caere, and of typical red, Caeretan clay. These were apparently combined with a lateral sima with feline water spout, provided with openings at each side for the insertion of the antefixes and their tile backers. The sima type is known from Velletri and other places in Rome, where it is associated with female head antefixes of a more stylistically advanced type than the Regia antefixes; the clay of the Regia sima conforms to the Velletri clays and differs from the Caeretan antefixes. Yet the archaeological context places both with the Fourth Regia, as Downey stresses.\nThe wider picture provided by Downey through this material shows that Rome probably had a local workshop producing its own architectural terracottas in the second quarter of the 6th century BC, but by the third quarter, Rome belongs to a Central Italian unit which is characterized by standardized forms.\nA cautionary word must be said about the drawings, as they often differ from the accompanying photographs and text. Figure 6 places the nail hole in the wrong position; Figure 10 omits the feline leg to the left; Figure 25 lacks the red semicircles on the tips of the white strigils; etc. A very useful addition would have been the inclusion of a plan indicating findspots, also for associated fragments from the Temple of Divus Julius.\nNonetheless, the contribution of this work far exceeds the impression given by the unassuming nature of the fragments, and shows that mighty works from small beginnings are indeed often made.","Taharqa Meets Wacom — A Case Study for Taharqa Gate and A Wacom Intuos Pro review\nWritten by Dominique Navarro, artist at the Epigraphic Survey of the Oriental Institute, University of Chicago\nThe North side of Taharqa's gate in the morning light (Photo by Dominique Navarro)\nDocumenting Taharqa Gate\nWithin the Medinet Habu Temple complex in Thebes, just north of the Small Temple and near the Sacred Lake, is an unassuming monument standing alone among the remaining rubble of an ancient mudbrick wall. Referred to as “Taharqa Gate,” the structure would have served as a gateway entrance to the Small Temple during the reign of the ancient Egyptian Twenty-fifth Dynasty pharaoh Taharqa (690 BC to 664 BC), and credited to him despite all cartouches (names of the king) being thoroughly destroyed by vigilant ancient hackers.\nGround plan of the Small Temple at Medinet Habu in the Ptolemaic period\nReconstruction illustration appearing in OIP 41. (The Excavation of Medinet Habu, Volume 2: The Temples of the Eighteenth Dynasty by Uvo Hölscher, The University of Chicago Press, 1939.)\nAlong with such rigorous hacking, the sandstone monument has suffered thousands of years of deterioration from wind, sand, water (humidity, moisture, and occasional rain), ancient religious graffiti, pilgrim or fertility gouges (scrapings of the walls for good luck), and the devastating disintegration from soluble salt in a high water table rapidly destroying the integrity of its bottom-most blocks, and leaving the entire monument vulnerable and contorting, the western wall especially askew in a precarious tilt under the weight of its heavy lintel. Modern climate change and tourism has taken its toll, as with all the monuments.\nThus, Taharqa Gate’s raised reliefs and hieroglyphs are grievously corroded and fractured, and at first glimpse, retain little of the artistry and craftsmanship of the skilled hands which carved it. It is desperate for documentation and conservation.\nPhoto by Dominique Navarro\nOn behalf of the Epigraphic Survey, beginning in 2017, I began the epigraphic drawings to document the entire Taharqa Gate. By 2018, I had completed the penciling of both sides of the monument — identified as MHE 85 (North) and MHE 86 (South) — using the Chicago House method, penciling at the wall on photo enlargements at a scale of 1:4. Each side of the Gate was broken up into 6 different drawing panels, thus creating 12 separate drawings for the entire monument to be rejoined later. The enlargements were printed from silver nitrate negatives of the gate photographed in the 1930’s. The earlier the photograph, the less damage to the gate, and indeed, the old photographs retain more reliable detail than the structure itself does today.\nDetail from Taharqa Gate MHE 86 (south-east lower section); 1930’s silver nitrate photograph next to a contemporary photograph (2017, by DN), next to the digitally inked drawing (by Dominique Navarro)\nDue to the extent of the damage to the monument, and the preservation of detail in the historic photographs, my penciling and recording of detail relied heavily on the older photographs, including the angles of blocks and architectural lines. Whereas blocks have corroded and shifted, and perpendicular angles have been altered in the present structure, photographs from the silver nitrate negatives retain the true stature of the monument.\nTracing paper over the photograph was also extensively utilized throughout the penciling stage. At times, figures and hieroglyphs were so drastically deteriorated on all sides of the structure, that it seemed that lines and shapes were impossible to distinguish. Yet, the gate uses the repetition of hieroglyphic scenes and phrases in such a way, and with such excellent craftsmanship (shape and dimensions are nearly identical or similar) that one can easily trace a figure or hieroglyph, flop it (reverse it), move it to the opposite side or scene, and find traces of a similar figure or hieroglyph almost mirrored in the midst of deep damage. In this way, going back and forth from the east side to the west side of the gate, and the north side to the south side, I could establish accurate details throughout, which were otherwise challenging to distinguish by naked eye alone.\n1930’s silver nitrate photograph of Taharqa Gate MHE 86 lintel, south-eastern side, showing the extent of the damage.\nTaharqa Gate MHE 86 lintel drawing (without damage, plaster, or graffiti detail): western portion of the lintel in blue line and flopped, with the eastern portion in black line, to show the mirroring effect and how figures and hieroglyphs are carved in nearly similar shapes and dimensions.\nDuring the summer of 2018, I was able to complete traditional inking of MHE 85, the north side of the gate, using Rapidograph pens and ink directly on the penciled photograph enlargements. (In a later stage, the photograph is “bleached” leaving just the drawing on the paper.)\nThen, in early 2019, I was officially trained by the Epigraphic Survey in digital drawing, and it was determined that I would transition the Taharqa Gate epigraphic drawings into a hybrid: using both traditional and digital methods for the final drawings. MHE 85 inked drawings and MHE 86 penciled drawings were all scanned at 1200 dpi and prepared as Photoshop tif files. I also scanned all the photographic negatives and brought them in as layers on each of the Photoshop files, requiring me to resize and fit the photos to the line drawings.\nMHE 86 was digitally inked in the summer of 2019. The entire monument is now fully documented but is currently being digitally corrected and altered to attain cohesion throughout all the drawings. The next steps will be the Egyptologists’ epigraphic checks for accuracy, further corrections, the director’s check, and final publication.\nWacom Cintiq 22 HD Tablet versus the Wacom Intuos Pro Tablet\nMy digital training was performed using a Wacom Cintiq 22 HD Tablet and the standard tablet used at the Epigraphic Survey. Personally, I have been using a Wacom tablet since 2012, yet even I had to adjust to the Wacom Cintiq; I was not used to drawing directly on a screen, had never utilized hot keys, and had never produced precision line drawing artwork digitally like this before. But I am patient when it comes to adapting to new technology, and I committed myself to 3 months of diligent training on the Cintiq 22 HD until I was producing digital artwork that would meet the high standards of the Epigraphic Survey.\nHowever, with the end of our winter work season in Luxor and the arrival of summer, I returned to my home and no longer had access to the Wacom Cintiq 22 HD, and needed to come up with a solution to complete my summer work digitally. Due to some personal unforeseen circumstances, I suddenly had precarious, transitional, and limited workspace, as well as power issues. Therefore, I could not simply borrow a Wacom Cintiq from another colleague, or have the Epigraphic Survey purchase me a new one. I considered options such as Astropad on my iPad (Apple Sidecar was not yet available). Instead, I returned to a familiar old friend: the Wacom Intuos Pro Tablet.\nPhoto by Wacom\nWithout a display-screen surface to draw directly on, the Wacom Intuos Pro tablet requires that one relies completely on eye-hand coordination, drawing on the tablet while looking at the monitor, which in my case was my MacBook Pro screen.\nUsing the Wacom Intuos Pro since 2012 has thoroughly developed my eye-hand coordination so that it has become second-nature. Curious at the challenges for a beginner, I asked a colleague who has never tried it before to test the tablet for her immediate reaction. She found the eye-hand coordination less difficult to manage than she had assumed, but where she became disoriented was learning to use the Wacom pen as it relates to the tablet: the hovering mouse cursor on the screen when the pen is held above the tablet, the physical distance from pen-tip-to-tablet (about 1/4 inch to a few millimeters above), and how it all relates to actually drawing a line on the screen. These are all minor discomforts that lessen with greater use and familiarity. She found drawing a straight line or creating a smooth curve challenging, but I explained that for every difficulty she encountered, there are personalized settings, tips, tricks, and shortcuts that can accomplish the desired results effectively, using a combination of customizable features in both Wacom and Photoshop. As with anything, it is a matter of familiarizing yourself with the tools you are using.\nLike the Cintiq, the Wacom Intuos Pro tablet has numerous customizable features for its Express Keys, Touch Ring, and Radial Menu, and I used the same settings recommended by the digitalEPIGRAPHY website manual.\nBut what makes the Wacom Intuos Pro my preferred tablet for uncompromised quality?\nWhether you are using the Wacom Intuos Pro or the Wacom Cintiq, there are certain standard features you are guaranteed. Perhaps the best is the patented Wacom Pro Pen 2, which Wacom is known for. It is a highly responsive pen with little noticeable lag, excellent pressure responsiveness, and gives precision control to your drawing. It has tilt recognition and 8,192 levels of pressure. It is battery free, light and slim, featuring a nib and eraser, and a two-button side switch, all which can be personalized with settings and shortcuts.\nAside from the budget-friendly, reasonable price (from $250 to $500 depending on size: small, medium, or large; I prefer medium), Wacom Intuos Pro has unique particulars that Cintiq simply lacks:\nPortability: My medium size Wacom Intuos Pro is 338x219x8mm (13.2x8.5x0.3 inches), and weights 700 g (1.54 lb), making it easy to transport and flexible to work with in different office, studio, or workplace situations.\nPower: Unlike the Cintiq which requires its own AC 12-volt power source and multiple cables, the Wacom Intuos Pro can be powered directly by the computer using one USB cable, or no cable at all using Bluetooth connectivity. This was essential for me, as I have often found myself with power issues that make it challenging enough just to power my computer.\nCords of Wacom Cintiq 22 HD versus Wacom Intuos Pro Medium with Bluetooth connectivity (Photo by Dominique Navarro)\nScreen Options: It is crucial to have an excellent screen image when drawing with the Wacom Intuos Pro tablet, and I completely rely on my MacBook Pro’s vivid retina display screen with IPS technology, 2560 x 1600 pixels, 500 nits brightness, wide color (P3), and true tone technology. However, one has the choice to connect a larger monitor, use dual displays, or find other solutions suitable to an individual’s needs. Personally, I prefer my MacBook Pro screen to the Cintiq display screen which I never really got used to, despite its IPS UHD high brightness panel and 1920 x 1080 pixel resolution.\nPhysicality: Spending hours doing artwork, for weeks and months at a time, can be physically taxing on the body. I found working on the Cintiq 22 HD painful: leaning over a large screen, staring down with my head heavily tilted and my neck strained, my arm moving in large gestures across the wide surface. With the Wacom Intuos Pro, I have the freedom to find a number of positions to work. For months, I worked at a standing desk with my MacBook Pro elevated on a stand so that my head could look straight ahead with no pressure on my neck. The tablet was on the table surface at the right height to let my arm and shoulders remain slack. The smaller tablet surface also means I’m not doing wide gestures and movements across the tablet.\nMulti-Touch Capability: Wacom Intuos Pro has an excellent multi-touch feature which allows you to quickly zoom, pan, rotate, and navigate the screen quickly and easily. Some Cintiq models also offer the multi-touch feature, but these are the priciest Wacoms on the market. I can live without this feature, but I prefer to have it. I find it essential to keeping a good pace to my work, moving faster and more efficiently when navigating around, and saving the slow, careful moments for drawing.\nDominique Navarro developing an inked line drawing for the Taharqa Gate using the Wacom Intuos Pro (Photo by Krisztián Vértes)\nOther features: Built-in Bluetooth connectivity, compatibility with Mac and Windows, Texture Sheet options, Pro Pen nib options and pen varieties, and the option to switch orientation for left or right-handed users.\nPractice recommendation: use your Wacom Intuos Pro as an alternative to using a mouse for all your computer use. The more time you spend with the pen in your hand on the tablet, the more natural and faster you become with your eye-hand coordination. Eventually, you won’t want to do anything without your Intuos Pro, even for simple computer tasks.\nTaharqa In Hindsight\nIn conclusion to the long learning curve of this documentation project —recording Taharqa Gate’s quickly deteriorating structure and carvings for posterity — it may have been more efficient to start these drawings as digital files from the beginning. Had I to do it all over again, I would have taken all photographs of Taharqa Gate, historical and contemporary, scanned them and created layers in a Photoshop file for each drawing, so that throughout the drawing process each photograph could easily be referred to for details.\nOn site, I would have used Procreate on my iPad Pro using my iPen to produce the penciling layer. Penciling for me consists of a lot of preliminary sketching to find the right shape and proportions, before committing to a final pencil line that I will ultimately ink. Unfortunately, when penciling on the photo enlargement, there is a huge limitation to “sketching,” and the more one pencils and erases, the more the surface of the photograph is compromised and deteriorated, making a more vulnerable surface when you do finally apply the ink. Whereas, sketching on the iPad Pro in Procreate is a risk-free technique, and I have a freedom to sketch without fear or constraints, utilizing layers to turn on and off the photographic reference background. This ultimately leads to a final pencil line that I can feel fully confident in. Taharqa Gate is full of vague and obliterated details; I used tracing paper on the photo enlargements to find obscured details in the damage, whereas the same techniques could have easily be done digitally as a separate layer.\nAll my traditionally penciled drawings were scanned and prepared as Photoshop files, as well as the photographic negatives which were brought in as layers and resized to fit the line drawings. But my digital penciling would have been much simpler to bring into Photoshop and prepare for inking, saving me many hours without all the readjusting.\nFinding the optimal technique for the most precise facsimile recording of ancient Egypt’s temple and tomb artwork and hieroglyphic records was vitally important to James Henry Breasted—founder of the Epigraphic Survey—even a century ago, and is part of his legacy. Today’s digital technology is rapidly and perpetually improving, and it is our duty as epigraphic artists to constantly adapt and strive for the best methods to create the most accurate documentation we are capable of.\nRegardless of what I would have done differently, Tahraqa Gate is being preserved by the best documentation achievable. It has a long way to go before final publication; it must first go through intensive research and epigraphy checks by multiple Egyptologists, endure corrections until the final drawings are as accurate as possible, and meet the high standards of the Epigraphic Survey. I feel confident that Taharqa Gate is getting the attention and documentation it deserves, and I am so honored to be part of its fascinating history.\nWHAT TO READ NEXT\nIn 2006, Dr. Ray Johnson, the director of the Epigraphic Survey in Luxor, contributed to the Amenmesse Project/KV 10⎼KV 63 by lending me and my artistic skills to draw the coffins found by Dr. Otto Schaden † and his team in the Valley of the Kings."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:2feb2ea2-a854-43ba-9fea-9dcd8351d3ab>","<urn:uuid:de5d42ef-b86b-45fa-bbc9-ffe413e820e7>"],"error":null}
{"question":"As someone who designs audio interfaces, I'm curious - what are the main differences between passive noise isolation and modern auditory virtual reality (AVR) systems in terms of their approach to managing sound environments?","answer":"Passive noise isolation and AVR systems represent fundamentally different approaches to managing sound environments. Passive noise isolation simply uses physical layers and padding to block out sound, similar to wearing earmuffs, and can only partially reduce incoming sounds. In contrast, AVR systems are sophisticated interactive environments that use real-time signal processing, sound-field modeling, and binaural impulse responses to create complete virtual acoustic spaces. AVR systems consider head movements, incorporate individual head-related transfer functions (HRTFs), and can simulate complex room acoustics with sound reflections and object interactions. Additionally, AVR systems are typically knowledge-based, allow for user interaction, and can be integrated with other sensory modalities like visual and tactile feedback to create comprehensive virtual environments.","context":["(Abstract of the 8th Richard C. Heyser Memorial Lecture, Amsterdam, March 2003)\nRuhr-Universität Bochum, D-44780 Bochum, Germany\nInstrumental analysis and synthesis of auditory scenes are a major current issue in Communication Acoustics, particularly coding of scenes and generation of scenes from code. As an introduction to these topics we start with the consideration of a class of systems which are typical for audio engineering, namely, audio-transmission systems. These systems have the following essential components: one or more microphones, means to transmit, store and process audio signals, and loudspeakers or headphones at the play-back end. With authentic transmission in mind, so-called binaural systems are a good choice, i. e. systems which use artificial heads as a front end (Fig. 1).\nFig. 1 Schematic of an audio-transmission system\nAs Fig. 2 shows, such systems can be separated into two parts, the left panel representing the analysis side and the right one the synthesis side. As to the analysis side, this can be interpreted as a system where the listener of Fig. 1 has been replaced by a signal-processing component which, in a way, mimics the perceptive and mental capabilities of a listener depending on the specific purpose of the computational analysis. In the synthesis system, auditory scenes are generated from input information which, in a complete transmission system, would originate from the analysis part - for example, information gained by computer algorithms based on perceptive and mental processes. In communication acoustics the analysis side is often termed \"computational auditory scene analysis (CASA)\", while the synthesis side is called \"auditory virtual-reality generation (AVR)\".\nFig. 2 Schematic of systems for computational analysis (left)\nand computational synthesis of auditory scenes (right)\nComputational Auditory Scene Analysis (CASA)\nThere is pronounced technological demand for auditory scene analysis. Important application areas are: systems for the identification and localization of sound sources - especially in acoustically adverse environments, such as multi-source, noisy or reverberant situations - e.g., for acoustically-based surveillance and/or navigation. Further, systems to separate and decolor concurrent sound sources (so-called cocktail-party processors) that are, for instance, needed as front-ends for hearing aids or robust speech recognizers. Also, for the modeling of auditory recognition and assessment tasks, it is often advisable or even indispensable to start with a scene analysis, for example, in systems for analysis in architectural acoustics or in systems for quality assessment of speech and product sounds. Also, in this context, so-called content filters are worth mentioning. These filters gain in relevance with respect to the tasks of automatic archiving and retrieving of audio-visual program material. There, they are used to analyze and code the contents of this material (see the MPEG7 coding as proposed by ISO/IEC).\nAs far as CASA systems make use of human auditory signal processing as a prototype, their structure follows, as a rule, a schematic as given in Fig 3. The systems are binaural, i. e. have two front ports which take the signals from the left and right ear of a human or a dummy head as an input. Relevant signal-processing stages are as follows: After a moderate band-pass filtering, which simulates the middle ear, the two ear signals are fed into a cochlea model. Here, two things are done: The signals are decomposed into ear-adequate spectral components (so-called critical bands) and than converted into signals which represent the neural activity (spike-density function) as generated by the inner ear. The two cochlea-output signals are then sent to a binaural module which analyzes the interaural arrival time and level differences, i. e. differences between the left- and right-ear signals. This information is later needed to identify individual sound-sources and their lateral positions in space - amongst other features. From this process a 4-dimensional pattern (time, frequency, intensity, lateral position) results, which is called binaural activity pattern. Of course, it has to considered too that we can also hear with one ear only (additional monaural modules).\nFig. 3 Architecture for an analysis system for auditory scenes.\nA number of tasks within CASA can be performed based on this, so far strictly bottom-up, processing and its resulting binaural activity pattern, e. g., localization and tracking of multiple sound sources in not-too-reverberant scenarios. Also decoloration and separation of concurrent sound sources succeeds quite well under these conditions sometimes even better than humans can do.\nUnfortunately, these algorithms decrease rapidly in performance with reflected sound being added in quantities which are typical for common architectural spaces. It seems that a strictly bottom-up process cannot deal well with these situations amongst others. For this reason more recent approaches provide modules on top of the binaural activity pattern which work on a top-down basis, i. e. hypothesis-driven, rather than on a bottom-up, signal-driven basis. In this way, it becomes possible to include knowledge-based processing into the structure. Model architectures of such a kind had already proven to be successful in automatic speech recognition.\nA possible architecture for the complete system is depicted in the upper part of Fig 3. The binaural activity pattern is input to a grouping and segmentation process which produces an usually error infected symbolic representation of it. The symbolic representation is then put on a black-board module, where it can be inspected by different knowledge based expert modules. The expert modules, then, generate hypotheses with the aim of arriving at plausible interpretations of the activity patterns with the aim of producing a meaningful identification and analysis of the auditory scene. The individual hypotheses are evaluated step by step, eventually modified, and finally accepted or rejected. Each expert module acts on the basis of its specific knowledge. This knowledge can be represented in the form of explicit rules or data bases. Typical knowledge domains involved are, e. g., knowledge of the current position of the sound-recording head, prior knowledge on the scene, cross-modal information (tactile, visual, etc.), knowledge about the sound source and the sound signals radiated by it. Non-auditory sensual information (e.g., visual, tactile, proprioceptive) may also be considered. Once a plausible parametric representation of the auditory scene has been obtained in this way, any further processing and utilization depends on the specific task involved.\nAuditory Virtual Reality (AVR)\nInstrumental synthesis of auditory scenes is currently of even higher relevance than their instrumental analysis particularly where the listeners can interact with the synthesized scenes. In the following, a number of possible applications are listed as examples, based on projects which the Institute of Communication Acoustics at Bochum has been involved in: auditory displays for pilots of civil aircraft, AVR for the acoustic design and evaluation of space for musical and oral performances, for individual, interactive movie sound, and for teleconferencing. Further, there are virtual sound studios and listening rooms, musical practicing rooms, and systems to generate artificial sound effects,\n- especially so-called spatializers - and the auditory representation in simulators of all kinds of vehicles (e.g., aircraft, passenger cars, trucks, train, motorcycles).\nFig. 4 Schematic of a virtual-reality generator with\nauditory, tactile and visual representation\nFurther applications are: AVR for archiving cultural heritage, for training (e.g. police and fire-fighter training), for rehabilitation purposes (motoric training) and as an interface to the web (internet kiosk). Last but not least, AVR is a preferred tool for research purposes (e. g. psychophysics, behavioral studies).\nIn the following, the architecture of an auditory virtual-reality generator is schematically depicted (Fig. 4). To clarify that AVR is usually a component of multi-modal VR generators, i. e. an embedded system, the figure shows an auditory/tactile/visual generator. The example shows a system where the acoustic signals are presented via headphones. Loudspeaker presentation would be possible, too. Multi-channel loudspeaker reproduction systems as used in consumer electronics (movie theatres, home theatres, TV, CD, DVD, radio) can indeed be seen as a first step towards virtual reality - although they usually lack an important feature of VR, namely, interactivity. The sample system in Fig. 4 contains as its core, a world model. This system component, among other things, contains descriptions of all objects which are to exist in the VR. In a layer inside the world model, rules are listed which regulate the interaction of the objects with respect to the specific applications intended. Then, a central-control layer collects the reactions of the subjects which use the VR system interactively and prompts the system to execute appropriate responses. In other words, the world model is a part of the system which is essentially knowledge based, i.e. contains explicit knowledge in the form of data-banks and rules.\nIn the system shown, head, hand, and finger positions of the subject are continuously monitored. The head positions are of relevance, as the signals, being presented via the headphone, have to be adapted constantly for the subject to perceive a spatial perspective which stays spatially still when the head is moving about. By moving hands and fingers the subjects can influence the virtual reality. Those system components that generate the signals which are finally presented to the subjects via actors (headphones for the auditory modality) are called \"renderers\". The most important component of the auditory renderer is the sound-field model. This is a module which creates a set of binaural impulse responses based on the geometric data of the virtual space, plus the absorption characteristics of all walls and geometrical objects in the space, plus the directional characteristics of both sound source and receiver. The characteristics of the receiver are given by the subjects head-related transfer functions (HRTFs). These HRTFs must be measured individually on the subjects to achieve best possible performance. The binaural impulse responses contain all information on the auditory environment, are then convolved with electronically or pre-recorded signals such as speech or music. These signals should be acoustically dry, i.e. not contain a-priori room information. The product of the convolution process is then fed into the headphones.\nIn many applications of virtual reality it is aimed at exposing the subjects to a virtual situation such that they feel perceptively present in it. This is especially important whenever scenarios are to be created in which the subjects are supposed to act intuitively as they would do in a respective real environment. Human/system interfaces which base on the principle of virtual reality have the potential of simplifying human-system interaction considerably. Think of tele-operation, design or dialog systems in this context also of computer games. The efforts involved in creating perceptual presence is task specific and dependent on the particular user requirements. For example, for vehicle simulators the perceptual requirements are far less stringent than for virtual control rooms for sound engineers. Generally, the virtual environment must appear sufficiently plausible to the listener to provide presence. As soon as interaction is at stake and this is the rule with generators for plausible virtual-reality real-time signal processing becomes indispensable. The system reaction must happen within a perceptually-plausible time span (for the auditory representation within roughly 50 ms). Further, the refresh rate for the generated scene must be so frequent that the perceptual scenario neither jolts nor flickers. To this end the refresh rate has to be above 30 times per second for moderately moving objects. For objects moving fast, Doppler shifts may have to be taken into consideration and modeled. To develop a reasonable fast generator for plausible VR, the developer, needs detailed knowledge on human sensory perception, since it has to be decided at every instant which attributes of the signals are perceptually relevant and, thus, have to be presented accurately and instantly. Less relevant attributes can be calculated later or even be omitted.\nModern speech technology offers components which can be integrated into virtual reality. Examples are: systems for instrumental speech synthesis and recognition. By utilization of these, human-system interaction can be performed via voice signals, thus incorporating human-machine speech dialogs into the systems. In this context, it is worthwhile mentioning that the perception of ones own voice in virtual realities is an important issue. Through careful analysis and simulation of the sound propagation through the air and through the skull, this task could recently be mastered. Since virtual worlds are artificial, namely, generated by computers, they rest on parametric representations of scenes. Then the parameters which represent a scenario, can be transmitted across time and space with telecommunication technologies. There exist description languages already which allow virtual worlds to be defined and specified in a formal, parametric way. The representation includes semantic (content) aspects. MPEG7 coding, as mentioned above, plays a role in this regard.\nWith the use of parametric coding it becomes possible that users, which actually reside in different locations, displace themselves perceptually into a common virtual room, where they may confer together (tele-conferencing) or even jointly exercise a mechanical task (tele-operation). Further, one may enter a virtual environment to inspect it or objects in it (e.g., virtual museum, virtual tourism). As entrance to virtual spaces can be provided via the internet, manifold applications can be imagined. VRs can further be superimposed on real realities (augmented reality) to assist navigation or provide other on-line support. Further, virtual realities are a very useful tool for scientific research. This is mainly due to the fact that they allow for flexible and economic presentation of complex experimental scenarios. Scenarios can be modified and changes can be performed without any physical effort. Research in areas like psychophysics, psychology, usability, product-sound design and assessment is about to take advantage of this possibility.\nCommunication Acoustics deals with those aspects of acoustics which relate to the information, communication and control technologies. Modern systems in these areas frequently contain embedded components which deal with the analysis and synthesis of auditory scenes - a genuine field of activity for Communications Acoustics. As Communication Acoustics, among other things, deals with both the acoustic and the auditory domains it is truly interdisciplinary.\nWith computational auditory scene analysis (CASA) taken as one example, it was shown that a major research aim in the field is the development of algorithms which analyze real scenarios in order to extract a parametric representation. Some human capabilities in analysis and recognition can already be mimicked or even surpassed. To achieve this, in addition to audio-signal processing, symbolic processing and content processing are needed. Especially in modern speech technology this line of thinking can clearly be observed. As to the synthesis of auditory scenarios (AVR), it becomes evident that the virtual-realty generators become more and more multi-modal and knowledge based. In a close alliance with the auditory modality, tactile (incl. vibration), visual and proprioceptive information is presented. The synthesis is parameter controlled and interactive in a majority of cases. Also in this context, speech technology has taken a leading role (e.g., in the form of spoken-dialog systems).\nIt is obvious that advanced systems in information, communication and control technologies become increasingly knowledge-based and multi-modal., i.e. the systems contain explicit knowledge, consider contents and interact on a cognitive level. This development has now reached Communication Acoustics as well: \"Audio Goes Cognitive\". The good news about this is, that Communication Acoustics thus faces an extremely interesting challenge, as cognition has to do with the mental capabilities of human beings - and humans have always been the most interesting object of human research. The bad new is that, as a rule, audio engineers and acousticians are not yet sufficiently educated to meet this new challenge.\n(Note: A full version of this lecture is being prepared and will form a chapter of a book on Communication Acoustics to be edited by this author and to be published in 2004 by Springer Verlag, Heidelberg, New York.)","Active noise cancellation is a technology that has existed for a while now, but until recently it was pretty expensive to implement. However, we're now seeing many more headphones being launched with this technology, and prices are coming down as well. And while the name of the technology itself is the biggest and most obvious indication of what it is, many people are often left wondering what exactly the term means. Today, we're going to try and explain it, so you can decide if active noise cancellation is for you.\nAs we've already said, active noise cancellation is just that — headphones that mute the noise around no. However, no pair of headphones exists that can entirely shut out all noise, but many are able to reduce a significant amount of background noise and make things ‘quieter', so to speak. Here we will explore exactly how noise cancellation works, and whether it will suit you.\nSimply put, active noise cancellation is a technology that cancels out certain elements of ambient sound. The key word here is active — the technology does not negate noise by simply attempting to block it. Instead, it listens to the sound using microphones on the headset, and produces a sound wave of its own that effectively cancels out the existing sound. With this, the sound is actively and deliberately reduced.\nActive noise cancellation was first used in practice back in the 1950s, based on patents by Lawrence Fogel, who is regarded as the inventor of the technology. Initially invented and used for aviation, the technology helped in making it quieter for pilots of airplanes and helicopters. The tech is also used in some cars such as the new Ford Endeavour available in India, to make the cabin quieter. However, the most common implementation of the technology today is on consumer-centric headphones.\nOn headphones, active noise cancellation uses one or more microphones to listen to the environment of the wearer, and capture the most steady sound frequency — this could be the hum of an airplane engine, car, machinery or even air conditioning, among other things. The system then generates a ‘reverse' frequency — it emits a sound of the same amplitude but with an inverted phase to the original sound. This combines with the first sound to form a new wave in a process called ‘interference', which effectively cancels out the sound going into your ears.\nAs a result, you hear less of the primary ‘noise', and in some cases with good active noise cancellation, you may hear nothing at all. It works best with sounds that are steady, since the microphones can pick up and reverse these frequencies. Irregular sounds such as voices, honking or anything that isn't regular won't get cancelled by the system, and you'll still be able to hear it. Therefore, active noise cancellation isn't the same as sound-proofing; it simply reduces the noise in certain environments.\nApart from the microphone and components used to pin-point and generate the new sound, active noise cancellation also requires a battery to power the system. This means that any headset with active noise cancellation will have to be large enough to include these components. Since wireless headphones also require batteries for powered use and usually come with microphones for hands-free calling, it makes sense to combine these two.\nIndeed, the best active noise cancelling headphones today are also wireless and enable voice calling with a paired smartphone. However, this increases the size of the components, and makes any headset with the technology a bit bigger and bulkier.\nFurthermore, active noise cancellation works best when combined with good passive noise isolation. While you do get earphones with active noise cancellation, the reduced level of isolation makes this form of headset the least suited for the technology. But what is the difference between active and passive noise isolation?\nWhile active noise cancellation uses an active method to reduce sound, another way to do it is passively. Passive noise isolation uses layers and padding to block out sound, and is the simplest way to reduce noise. On headphones, the concept works similarly to simply wearing earmuffs or earplugs. However, this can only block out some sound, while loud sounds will still creep through to your ears. Yet, it's the simplest and least complicated way to do things — a well-padded pair of over-ear headphones will completely cover your ears and significantly reduce outside noise.\nIt's worth noting here that active noise cancellation works best when combined with passive noise isolation, although the two methods are different in their approaches. Where noise isolation reduces the amount of sound coming in, active noise cancellation captures the droning sounds and filters them out as well. The result is a listening experience that is significantly quieter than leaving your ears open.\nNoise isolation is best on over-ear headphones, but can be effective even with on-ear headphones and earphones as well. Eartips and foam padding can be used to improve isolation with earphones.\nThe biggest and most obvious advantage is the silence — active noise cancellation reduces sound and makes it easier to concentrate, sleep, or just unwind after a long day. However, when used while listening to music or watching videos on a screen, it can also offer a perceived improvement in sound quality, and allow vocals or dialogue to be heard more clearly even at relatively lower volumes.\nThe biggest advantages can be heard in certain scenarios, such as on airplanes. The loud, droning hum of an airplane is often completely filtered out by a good pair of noise cancellation headphones, making it easier to sleep in the otherwise noisy, bright, and unpleasant environment of an aircraft cabin. You can also use it on commutes, in office, or anywhere that is a bit noisy.\nWhile the advantages are fairly obvious even to the layperson, the disadvantages are less straightforward. The biggest of which is an actual reduction in sound quality, which may not be perceived and may even be mistaken for better sound because of the reduction in ambient sound interference. However, the additional sound wave can interfere with the regular audio being played on the headphones, and in fact have a detrimental effect on sound quality. The sound being played isn't entirely pure as a result of the addition of the reverse wave.\nOther disadvantages include the higher cost of the system, because of the need for additional components. Headphones with active noise cancellation are often more expensive than equivalent wireless headphones, and this price premium often puts active noise cancellation out of the reach of many buyers. Furthermore, the system requires power to run, and you'll have to keep your noise cancelling headphones charged to use the feature. All of these components make the headphones larger and bulkier as well.\nDespite the terminology, active noise cancelling headphones can't cut out all sound, and any sound that isn't steady will remain unaffected. Voices, the use of tools, traffic sounds and just about anything that can't be classified as a hum will still go through to your ears. And finally, even on good active noise cancelling headphones, switching on the system results in a sort of ‘vacuum' effect. While this is normal for the technology, the level of silence can be unsettling and uncomfortable for many. However, over time, users tend to get used to this vacuum effect.\nThe usefulness of active noise cancelling headphones largely depends on what you intend to do with your headphones, and where you intend to use them. If you do most of your listening indoors or in relatively quiet environments, noise cancelling headphones won't make much of a difference to you. Even if you listen to audio in noisy environments, noise cancellation is only truly effective in certain situations.\nFrequent flyers and commuters are likely to be benefited the most by active noise cancellation headphones. The technology works most effectively with the hum of an airplane engine or the hiss of air on an open commuter train or bus. Furthermore, these headphones can help you concentrate or sleep in these environments, but the inability to cut off all sound means that you won't see any significant difference over what good passive noise isolation provides."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:3f246f1f-12da-4ca7-8e35-f47499d02f58>","<urn:uuid:d524c90a-4271-4240-805a-adc487f5875e>"],"error":null}
{"question":"What are the common vocal health challenges in extreme metal vocals, and how do medical professionals diagnose them?","answer":"In extreme metal vocals, common challenges include vocal fold stress from intense singing and potential reflux issues. For death metal vocalists, extensive singing parts require significant force and air control. Medical professionals like laryngologists examine multiple factors to diagnose vocal problems - they look at diet, sleep patterns, medications, and work environment. One specific condition they check for is reflux (LPR or GERD), where stomach acids move up to the vocal folds. This condition is common in vocal performers and can occur without pain while causing inconsistencies in voice quality and range.","context":["Tampa, Florida's Obituary were instrumental in helping to define the death metal movement of the late 1980s, and the band has been at it ever since, never compromising on their style or execution, releasing a string of brutally heavy records since their 1989 debut, Slowly We Rot. The band's first three albums are hailed as classics in the genre, which they will revisit for their latest tour, consisting of fan-chosen tracks from their earliest records, including 1992's The End Complete, which has sold over 500,000 copies. For the latest edition of Tell Me About That Album, we chatted with singer John Tardy about the record, the band's upcoming tour and how he takes care of that death metal growl of his. The band play Studio Seven on Sept. 23rd.\nHow have the records held up for you? Is it an enjoyable process to go back and listen to them again or is it painful? Especially the first two records, we were so young and we had no idea what we were doing. It's kind of like having some friends of yours going into your parents' house where there's old pictures of you up on the wall with braces and a strange haircut. Sometimes when I listen to it it feels a little like that. But at the same time, Slowly We Rot is what it is. You've kind of got to live with it. But it'll be nice to go back and pick some of these songs that we haven't played for a while.\nThe End Complete is your best-selling record and also the most divisive amongst fans. Why do you think that is? When we were asked to do Slowly We Rot, we just kind of recorded some songs [we had], and that even kind of rolled over into Cause of Death. It really wasn't until The End Complete that we wrote a complete album. Our third trip to the studio was a lot more relaxing. We weren't in shock and awe about everything. Our first album we had no idea what we were doing. Our second album we had all these ideas and things we wanted to do, but didn't know how to get comfortable and go after them. The End Complete was actually a good-sounding record and one that we actually sat down and wrote as one whole album.\nOn The End Complete, some critics complained that the song structures were more traditional and the vocals were actually understandable for the first time. Were those conscious decisions? That was natural progression. On Slowly We Rot, there isn't really a full set of lyrics. There are plenty of times where I was just making up something and growling. I don't really know why I did that to begin with. Keep in mind, we were in high school when we were writing some of this stuff, so it was a little weird for us too. By The End Complete there was more of a thought process about the lyrics and what I was saying. Clearer pronunciation is just kind of the way it goes.\nYou guys have managed to stay true to one style of metal over the years, never straying too far from your core sound, which some people appreciate and others might not. We're all stuck in our ways with the music that we listen to. I still like throwing on the old [Celtic] Frost and the old Slayer albums and Venom. That's just what I enjoy listening to, and I think when we approach our music it's just the same way. Donald, Trevor and I have been writing together for a long time and we're all similar in our ideas and we don't really change that much. Like you said, it's six of one, half a dozen of the other. Some kids don't think it's enough different and some kids like it to sound the same but we like to think we go after each album with the thought of being Obituary first.\nWould you say your taste in music hasn't evolved over the years? Pretty much not. I still listen to my old Lynyrd Skynyrd albums and stuff like that.\nDo you have a favorite song on The End Complete? \"Back to One\" is one of the ones that has a lot of that fast singing. I grew up on Slayer and Tom Araya is the master of being able to ramble on endlessly in a song. That's kind of a cool song. \"Killing Time\" is one that we've played for a long time.\nWhat makes one death metal song harder to sing than another? The amount of singing in a song. \"Don't Care\" takes a lot of air and a lot of force to do that, especially live.\nDo you do anything to take care of your voice by doing any vocal exercises or anything? A lot of kids will ask, \"How do you do that?\" And I tell them to treat it like any other muscle in your body. Start off slow, build up strength slowly and rest in between. In my case, it can be pretty easy to get into some hard partying every night, but I've got to lay back on some of that and get into a quiet place.\nCan you talk about putting together the cover for The End Complete? That was the first time we used Andreas Marschall to do artwork for us. When he did that album cover we were like, \"That is pretty cool!\" Nowadays it kind of stinks because people just download the record and get a little thumbnail, but back when we had album covers coming out, these things were pretty cool. That artwork has so much detail in it and then he came up with the idea of the creature for the T that sort of comes and goes from our albums also. Since then he did the Anthology cover, he did the Frozen in Time cover.\nHave you ever hurt your neck from headbanging? No.","When you have problems with your voice, sometimes folk wisdom and the advice of friends will not do. It may be time to become a detective, looking more deeply into vocal health than you ever have before.\nVoiceCouncil Magazine has asked leading voice scientists, doctors, therapists and teachers to help us discover the causes behind one’s voice getting trashed—and to show the way forward to vocal health.\nYou aren’t hitting those high notes like you used to. Something is “off” with the tone of your voice. Perhaps there’s some pain. But you are in the middle of a busy schedule. Your manager has expectations—so does the audience—so do you. It’s damn the torpedoes, full steam ahead. But what is the cost to your voice?\nUnderstanding some basic facts about vocal health and vocal function should be required for anyone relying upon their voice to make a living. VoiceCouncil Magazine will explore some central issues below and we invite you to go to our forums to keep the discussion alive as we continue exploring the health of your voice.\nWhat’s Going On Down There?\nIn Voice Science 101 students learn that vocal folds (a.k.a. the vocal cords) are muscles in the larynx that are responsible for producing sound. You can feel them at work: just hold your fingers on your throat and make a “zzzz” sound; those vibrations you feel are the vocal folds hitting against each other.\nVocal folds slap together at high speeds and with forces which vary according one’s volume and tone. Voice problems often have something to do with these folds; there is swelling due to stress or illness and a subsequent loss of vocal abilities. A singer wants vocal folds that are not swollen and able to produce sounds naturally, without undue stress. Healthy vocal folds are perhaps the singer’s greatest ally.\nBut vocal folds are not the whole story. Dr. John Rubin, a leading ear, nose and throat (ENT) surgeon and clinician, observes that it takes a whole body to produce a sound. Understanding how vocal folds work is just one part of a much larger picture that involves emotional, physical, social and nutritional dimensions: “When people come to me with vocal problems I need to look at which parts of the body have gone wrong. This is why I spend a lot of time with my patients; we look at diet, patterns of sleep, the medication they are on and the environment in which the singer works and plays—the number of things than could be contributing to voice problems is almost endless. Sometimes what is going on with vocal folds is not the central issue at all.”\nWhile there can be many causes behind one’s voice getting trashed there are only 2 main enemies of vocal health.\nVocal Enemy #1: Stress\nThere are many ways to put stress on one’s voice: by talking too loudly, using a technique or style that doesn’t “fit”, and by singing in environments that force the voice to push more than it can handle. Dr. Ronald Scherer, well-known voice scientist and educator, says: “The main point is that whatever is making sound in the throat should be created specifically, but with healthy technique—the more training one has, the more one should be able to control the variety, color, loudness, and degree of clarity of one’s voice, without undo fatigue; that is performance vocal health”.\nDr. Rubin underscores the fact that vocal stress can happen without people even thinking about it. “A singer may be causing chronic stress to their voice by competing against a lot of background noise; singing in the car or getting into extended conversations in noisy pubs or clubs, for example”.\nNoted vocal coach Melissa Cross says that the biggest cause of vocal stress is the vocalist thinking that they have to have someone else’s voice: “A singer can sometimes have someone else’s voice in their head—it’s like driving from the passenger seat. A good vocal teacher is someone who can identify natural sensations that are essential to voice production and translate them into a language of the imagination so that it can be used in performance without thinking”.\nVocal Enemy #2: Disease\n“Disease” can be perceived as a frightening term, conjuring up images of an army of microorganisms attacking the throat – forces against which the singer is powerless. However, “disease” simply means that a part of the body is not functioning well. The causes of disease can certainly include microorganisms but they can just as easily be related to nutritional, environmental and toxic factors, all of which can be treated. In the case of singers, there are common ailments and proven paths back to health.\nA magazine article cannot pretend to be the place where a singer can diagnose an illness. Laryngologists (or ENT doctors who specialize in the larynx) can locate problems and prescribe cures. Whether it’s the common cold or some form of “itis”, it may be essential that singers see their doctor so that the cause can be isolated and the path to healing begun.\nDr. Scherer notes that one disease that shows up often with singers is reflux (laryngopharyngeal reflux, LPR, also known as gastroesophageal reflux disease, or GERD): “Reflux is where the gastric juices from the stomach move up the esophagus and onto the vocal folds. It is quite common in vocal performers and always suspected as part of a person’s vocal problem.”\nSometimes, Scherer notes, reflux has no consistent symptoms. That is, with disease we expect to feel pain, but reflux can occur without pain, yet cause inconsistencies in voice quality and vocal range.\nFor people with reflux, Dr. Scherer says there is very good news: “Both medicine and lifestyle change can often allow a person to manage this problem well. Eventually, lifestyle choices may be all that is required to maintain vocal health. The problem is definitely one for which a person should see a laryngologist and other health professionals to receive the most appropriate care”.\nLifestyle Over Medicine\nVoice scientists, doctors, therapists and teachers are unanimous: while there are effective medicines that can be used to treat stress and disease, there is much a vocalist can accomplish through diet, rest and new habits.\nControlling cigarette, alcohol and caffeine consumption will have a direct effect on vocal health. “Think of the process of smoking” observes Dr. Rubin. “You are taking heated materials down into the larynx—this causes irritation. Nicotine is a drug that causes drying in the larynx, not to mention the relationship between smoking and cancer”.\nThe vocalist would do well to see alcohol more as a voice irritant than as a balm for nervousness: alcohol can have a drying effect on the throat and excess consumption can be a factor in reflux.\nExcessive caffeine has a diuretic effect, depleting the body of needed water—water that is needed, in part, to keep the larynx lubricated, so that the quality of singing voice can be maintained. People get tired and so drink a cup of coffee or have a coke to stay awake –unaware that this puts stress on their voice.\nEven the beloved latte has drawbacks for the singer, says Dr. Marcus Coneys. The diuretic effect remains the same as this is caused by the caffeine and is not mitigated by the milk. In addition to this, the milk tends to stimulate the formation of mucus on the larynx, producing a “phlegmy” sounding voice. Singers should ensure that they drink a cup of water after having any drink containing caffeine.\nIn the case of reflux, one can reduce the intake of foods and drink that are acidic: eat less tomato based products, hot spicy foods and drink less orange juice. Not eating 3 hours before sleep and having one’s head slightly elevated keeps acidic fluids where they should be kept—in the stomach.\nEating healthy foods, drinking adequate amounts of water (6 cups a day), getting adequate rest and not being harsh with your voice is not a moralistic message—professionals say that it’s essential to support a lifestyle that depends on the voice.\nIs Devotion to Health Realistic?\nUnderstandably, there can be a fear that in addressing medical issues the singer will fall down an endless pit of medical appointments, lost time and rigorous self-analysis leading to compulsions about diet and environmental noise. The last thing any singer wants to be is so obsessed about health that important opportunities are lost, demanding schedules are held up and performances are cancelled.\nYet, the advice that comes from professionals is straightforward: simple lifestyle choices combined with learning how to use the voice more naturally in performance is achievable. Voice and speech therapist Dr. Ruth Epstein maintains that any treatment has to be sensitive to the demands on the singer’s life. “A doctor or therapist needs to build trust with the singer—this is not accomplished by demanding immediate changes that interrupts essential work, causing even greater stress for the performing artist.”\n“Sometimes we practice some ‘SOS management’, acute treatments that keep a singer on track for the short term. However, we want the singer, over time, to adopt a lifestyle that will support their vocation—that makes economic as well as medical sense”.*\nThe Team Behind the Singer\nThe professionals whose voices you heard in this article see themselves as a part of a anti-voice-trashing support network for the singer. Laryngologists (or ENT docs) take the time needed to diagnose the problem; they work closely with speech therapists who examine behavior changes that can be developed to increase vocal health. Voice teachers extend the work of both doctors and therapists by helping the singer to find more natural ways to use their voice.\nSometimes the “back-up” a singer needs is not musical. Even a determined soloist may need to be in harmony with this team.\n* The first occupational voice symposium focusing on protecting the voice in the workplace, will take place in London on the 25-26 March 2009. For further details go to:www.royalfree.nhs.uk/slt-ent\n† VoiceCouncil Magazine thanks the outstanding team of professionals who supported this article:\nMelissa Cross is considered worldwide as an expert on rock vocal technique. Her well-known clientele attest to the fact that the traditional basics of vocal technique can be applied to even the most unorthodox of musical genres. Her unique method of vocal training has culminated in the critically acclaimed release of two vocal instructional DVDs: “The Zen of Screaming” and “Zen of Screaming 2”, available at http://www.melissacross.com/\nRuth Epstein PhD is Head of Speech & Language Therapy Services and Consultant Speech and Language Therapist (ENT) at the Royal National Throat, Nose & Ear Hospital, London. She is also the Director of the MSc programme in Voice Pathology at the Ear Institute, University College London.\nJohn S. Rubin, MD, FACS, FRCS is Consultant Ear Nose and Throat Surgeon at the Royal National Throat, Nose and Ear Hospital Division of The Royal Free NHS Trust. He is also the Lead Clinician of the Voice Disorders Unit as well as its Clinical Director.\nRonald C. Scherer, PhD. is a voice scientist and educator in the Department of Communication Disorders, Bowling Green State University, Bowling Green, Ohio. He teaches courses on voice disorders and voice and speech science. His research interests include the physiology and mechanics of basic, abnormal, and performance voice production, and the methodologies involved in such research. For more about Dr. Scherer’s work see:http://www.bgsu.edu/departments/cdis/page36449.html\nThe editor also thanks Marcus C. D. Coneys, MD for checking on the accuracy of many aspects of this article. Dr. Coneys is an anesthesiologist and pain clinician in Red Deer, Alberta.\n© 2008 Gregory A. Barker, PhD."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:8eca2710-dd2f-4b0d-99ff-61443bdaf08c>","<urn:uuid:e2aa8908-8191-47b9-a37b-0bb1351585e0>"],"error":null}
{"question":"Which safety monitoring is required for older patients taking either Adderall or Anzemet?","answer":"For Adderall, patients with conditions like bipolar disorder, depression, hypertension, liver and kidney diseases require constant supervision. For Anzemet, older patients specifically require ECG monitoring due to their particular risk for prolongation of the PR, QRS, and QT intervals. Doctors may also suggest ECG monitoring for Anzemet patients with congestive heart failure, very slow heart rate, or liver impairment.","context":["Adderall is a psychostimulant, a drug from the class of phenethylamines. In some countries, it is used in the treatment of attention deficit and hyperactivity and narcolepsy. Also, it is illegally used as a cognitive enhancer, both an aphrodisiac and for achieving a euphoric effect. The drug is a mixture of two amphetamine salts, stereoisomers and inactive components. Salts are the active ingredients, distributed as follows: 75% – Dextroamphetamine (right-rotary isomer) and 25% – Levoamphetamine (left-handed isomer).\nThis remedy works by increasing the activity of neurotransmitters norepinephrine and dopamine in the brain cells. Adderall duplicates many chemical and pharmacological reactions of endogenous neurotransmitters, especially phenethylamine and methylphenethylamine, the latter isomer of amphetamine, which is produced in the human body.\nAdderall is generally well tolerated and effective in treating the symptoms of ADHD. The most common side effects are cardiovascular, such as increased heart rate, and psychological, such as euphoria or anxiety. Large doses of the drug are likely to impair cognitive function and cause rapid muscle breakdown. Drug addiction is a serious risk of abuse of this substance, but it occurs very rarely in the case of therapeutic use. Very high doses can lead to a psychotic state (for example, delirium and paranoia), which is rarely seen with a therapeutic dosage even with prolonged use.\nStudies using magnetic resonance imaging discover that long-term drug treatment leads to a decrease in abnormalities in the brain structure and functions in patients with ADHD, improves the functioning of certain brain regions, such as the caudate nucleus in the basal ganglia.\nCurrent models of ADHD suggest that it happens because of functional impairment in some mediator systems, including a decrease in dopamine neurotransmission in the mesocorticolimbic region and norepinephrine in the synapses in the blue spot and the prefrontal cortex.\nAdderall is available in immediate-release tablets or sustained-release capsules.\nTherapeutic doses of the drug increase productivity in solving both complex and repetitive tasks. Higher dosages disrupt short-term memory and cognitive processes. It increases endurance and reaction time, primarily due to inhibition of dopamine reuptake.\n- Drug abuse;\n- Heart disease;\n- Neurotic diseases;\n- Glaucoma (increased intraocular pressure);\n- Hyperthyroidism (excessive production of thyroid hormone);\nThese tablets are prescribed under constant supervision if the patient has bipolar disorder, depression, hypertension, liver and kidney diseases, psychosis, Raynaud’s syndrome, convulsions.\n- Physical: hypertension or hypotension, Raynaud’s syndrome (decrease in blood flow to the extremities), tachycardia, erectile dysfunction, abdominal pain, loss of appetite, nausea, dry mouth, difficulty urinating, reduction or acceleration of the motility of the digestive tract.\n- Psychological: Mood changes, insomnia, anxiety, irritability, obsessive states. Psychoses occur very rarely when using the drug as prescribed.\n- Pathological over-activation of the mesolimbic pathway that connects the areas of the ventral lid plays a central role in the formation of drug addiction.\n- Drug addiction is a serious risk when Adderall is used in non-medical purposes but is unlikely to result from medical use in the dosages prescribed. Tolerance is formed quickly with Adderall abuse.\n- Persons receiving long-term treatment with Adderall and other similar drugs with sudden discontinuation of the intake report a limited withdrawal syndrome, which disappears within 24 hours after their last dose.\n- Symptoms of withdrawal include anxiety, depressed mood, fatigue, increased appetite, lack of motivation, insomnia, or drowsiness.\nNowadays, online pharmacies offer Adderall without a prescription. But remember, only doctor may prescribe appropriate dosage.\nTags: attention deficit hyperactivity disorder, human health","IMPORTANT SAFETY INFORMATION\nDownload Full Prescribing Information\nAnzemet® (dolasetron mesylate) Tablets\nWHAT ARE ANZEMET TABLETS USED FOR?\nAnzemet® (dolasetron mesylate) Tablets are used to prevent nausea and vomiting associated with chemotherapy.\nWHEN SHOULD I NOT TAKE THE DRUG?\nDO NOT USE Anzemet Tablets if you have ever had a bad reaction to any form of Anzemet (tablets or injection) in the past.\nWHAT WARNINGS SHOULD I KNOW ABOUT ANZEMET TABLETS?\nQT, PR, and QRS Interval Prolongation\nThe QT, PR, and QRS Intervals reflect separate measures of time in the heart’s electrical cycle.\nQT Interval: A lengthened QT Interval can be associated with rapid heartbeat and can increase your risk for sudden death. Anzemet Tablets can increase the QT Interval in a dose-dependent fashion. If you are prescribed Anzemet Tablets and have congestive heart failure, very slow heart rate, liver impairment, or if you are older, your doctor may suggest ECG monitoring.\nPR and QRS Interval: Lengthened PR and QRS Intervals are associated with heart block, heart attack, irregular heart beat, and serious slow heart rate in adults and children. Anzemet Tablets can increase these conditions in a dose-dependent fashion and can result in death. If you have any of these conditions, or if you are taking other drugs that impact the PR or QRS Intervals, your doctor will require ECG monitoring.\nSerotonin syndrome occurs when there is excess serotonin in the body. This can have serious, potentially life-threatening and sometime fatal consequences. Anzemet Tablets can cause excess serotonin levels in the body. If you are taking Anzemet Tablets, watch for the following symptoms:\n- Change in your mental status\n- Changes in basic body functions such as blood pressure, heart rate, sweating and digestion\n- Lack of coordination, or overactive reflexes\nIf you are taking Anzemet Tablets and any other serotogenic drugs such as certain antidepressants, then you may be at a higher risk for developing serotonin syndrome. If any of these symptoms occur, stop taking Anzemet and seek emergency treatment.\nSafety and effectiveness in pediatric patients (2 years and older) is based on studies in adults. Safety and effectiveness in pediatric patients under 2 years of age have not been established.\nAnzemet Tablets can be used with children old enough to swallow tablets. For children that do not meet the weight requirements for taking Anzemet 100 mg Tablets or children unable to swallow, Anzemet Injection solution may be mixed into apple or apple-grape juice for oral dosing. (See Anzemet Injection).\nOlder patients are at particular risk for prolongation of the PR, QRS, and QT interval. Caution should be exercised and ECG monitoring should be performed when using Anzemet Tablets in older patients.\nWHAT SHOULD I TELL MY HEALTHCARE PROVIDER?\nIf you are prescribed Anzemet Tablets and have congestive heart failure, very slow heart rate, QT syndrome, liver impairment, have hypokalemia (potassium deficiency in bloodstream) or hypo-magnesemia (magnesium deficiency in bloodstream), take diuretics, anti-arrhythmic drugs or other drugs which lead to QT prolongation, or take high doses of anthracycline, your doctor will monitor you closely.\nYou should tell your doctor if you are pregnant, thinking of becoming pregnant, or nursing.\nAnzemet Tablets have been shown to cause liver cancer in mice at 3,6, and 12 times the recommended doses.\nAnzemet Tablets do not have an effect on fertility and reproduction in rats at up to 9 times the recommended dose.\nHowever there have been no studies in pregnant women, so your doctor will only prescribe Anzemet Tablets if clearly needed.\nIt is not known whether Anzemet Tablets pass through to human milk. If you are nursing or considering nursing, tell your doctor.\nWHAT OTHER MEDICATIONS MIGHT INTERACT WITH ANZEMET TABLETS?\nVery few drugs interact with Anzemet Tablets. However, if you are going to take Anzemet Tablets with any other drugs, make sure you tell your doctor. Your doctor may monitor you if you are taking Anzemet Tablets with certain chemotherapy drugs, drugs that may cause serotonin syndrome, or drugs that affect your QT Interval and/or cause low blood potassium or magnesium.\nWHAT ARE THE SIDE EFFECTS OF ANZEMET TABLETS?\nGet immediate medical help if you notice any of the following side effects:\n- Change in your heart rate\n- Irregular heartbeat, weak pulse, slow breathing\n- Swelling in your hands or feet\n- Headache with chest pain and severe dizziness, fainting, fast or slow pounding heartbeats\n- Urinating less than usual or not at all\n- Agitation, hallucinations, delirium, and coma\n- Muscle stiffness, muscle spasms, overactive reflexes, lack of coordination\n- Seizures, with or without nausea, vomiting, diarrhea\nLess serious side effects may include:\n- Mild headache\n- Tired feeling, mild dizziness\n- Diarrhea, constipation, upset stomach, loss of appetite\n- Chills, shivering, numbness or tingly feeling\n- Fever, sweating\n- Joint or muscle pain\nThe most common side effects reported in patients taking Anzemet Tablets were headache, fatigue, diarrhea, abnormally slow heartbeat (bradycardia), dizziness, pain, abnormally rapid heartbeat (tachycardia), indigestion, and chills/shivering.\nTHIS IS NOT A COMPLETE LIST OF SIDE EFFECTS AND OTHERS MAY OCCUR. TELL YOUR DOCTOR ABOUT ALL MEDICINES YOU USE. THIS INCLUDES PRESCRIPTION, OVER-THE-COUNTER, VITAMIN AND HERBAL PRODUCTS. DO NOT START A NEW MEDICATION WITHOUT TELLING YOUR DOCTOR. YOU MAY REPORT SIDE EFFECTS TO VALIDUS PHARMACEUTICALS LLC AT 1-866-982-5438 (1-866-9VALIDUS).\nThere is no known specific antidote for dolasetron mesylate, and patients with suspected overdose should be managed with supportive therapy. If overdose is suspected, seek emergency medical assistance or call the Poison Help Line at 1-800-222-1222."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:8fd51951-0a49-4d87-894d-bcfcfce8d0da>","<urn:uuid:becca328-c7b7-4bc3-99dd-fc1d6296c12e>"],"error":null}
{"question":"What security measures are required for data transmission under both HIPAA and PCI DSS frameworks?","answer":"Both frameworks require encryption for data transmission over public networks. HIPAA mandates security measures to protect electronic protected health information (ePHI) during transmission, while PCI DSS specifically requires strong cryptography keys when transmitting cardholder data over open and public networks. Both standards emphasize the implementation of policies and procedures to prevent unauthorized access, including risk analysis, system activity monitoring, and proper authentication methods such as multi-factor authentication to protect sensitive data.","context":["HIPAA Compliance Best Practices\nQuestions and Answers to Improve Security and Avoid Penalties\nBy Bill Becker\nEven after 14 years, public and private sector organizations are still routinely found out of compliance with the Health Insurance Portability and Accountability Act (HIPAA). Security management processes are among the weakest links in HIPAA compliance. In this article, we’ll look at some of the basics that covered entities and their business partners need to follow to ensure that they are not hit with financial or other penalties.\nFor the uninitiated, HIPAA regulates the use and disclosure of certain information held by health plans, health insurers, and medical service providers that engage in many types of transactions.\nEnforcement of HIPAA Privacy and Security Rules falls to the Department of Health and Human Services’ Office for Civil Rights (OCR). Enforcement of compliance began in 2005, with OCR becoming responsible for Security Rule enforcement four years later. Since April 2003, over 150,000 HIPAA Privacy Rule complaints have been investigated by OCR. 98% (or 147,826) of the complaints have been resolved.\n3 Steps To HIPAA Compliance\nPlease see HIPAA Journal\n- Step 1 : Download Checklist.\n- Step 2 : Review Your Business.\n- Step 3 : Get Compliant!\nThe HIPAA Journal compliance checklist provides the top priorities for your organization to become fully HIPAA compliant.\nOCR enforces HIPAA Rules by applying “corrective measures,” including ether settlement or a civil cash penalty.\nOnly 47 cases have resulted in a settlement, although the total monetary penalty is still an eye-opening $67,210,982.00. Most compliance issues, OCR reports, stem from improper use or disclosure of electronic protected health information (ePHI); poor health information safeguards; inadequate patient access to their ePHI; and the absence of administrative safeguard for such information.\nIn other words, there is a fundamental failure in developing and maintaining appropriate security management processes. Which is ironic because one of the very first stipulations in HIPAA § 164.308 (a)(1) calls for organizations to implement policies and procedures to prevent, detect, contain, and correct security violations.\nThere are several required specifications to implement these management safeguards. These include the following:\nRisk analysis – Accurate and thorough assessment of the potential risks and vulnerabilities to the confidentiality, integrity, and availability of electronic protected health information held by the covered entity (or its business associate/s).\nRisk management – Security measures to reduce risks and vulnerabilities to a “reasonable and appropriate level.”\nSanction policy – Workforce members who do not comply with the security policies and procedures must be sanctioned according to a standard policy applied to violations.\nInformation system activity review – Procedures to review records of information system activity, including audit logs, access reports, and security incident tracking reports.\nBefore any of that, however, organizations must use best practices to get their arms around the protected information under their control, and to apply some common sense thinking to managing access to that information.\nLet’s look at some of these best practices.\nIdentify relevant information systems – It seems obvious, but here’s where many organizations fail. You have to be able to identify all information systems that house ePHI. Moreover, you have to be able to analyze business functions and verify the ownership and control of those information systems.\nAsk yourself the following questions:\n- Does the hardware and software in your information systems include removable media and remote access devices?\n- Have you identified the types of information you manage?\n- Have you identified and evaluated the sensitivity of each type of information?\nConduct a risk assessment – You have to have an accurate and thorough assessment of the potential risks and vulnerabilities to the confidentiality, integrity, and availability of ePHI.\nTo ensure accuracy and thoroughness, ask yourself the following questions:\n- Is the facility located in a region prone to any natural disasters?\n- Have you assigned responsibility to check all hardware?\n- Have you analyzed current safeguards and identifiable risks?\n- Have you considered all processes involving ePHI — including creating, receiving, maintaining, and transmitting protected information?\nAcquire IT systems and services – After identifying your systems and exposure to risk, you may find that you’ll need additional hardware, software or services to adequately protect information such as:\n- Multi-Factor Authentication\n- Data-at-Rest Encryption\n- Data-in-Transit Encryption\n- Cryptographic Key Management\nWhen planning for new systems or services, ask yourself the following questions:\n- Will new security controls work with the existing IT architecture?\n- Have you conducted a cost-benefit analysis to make sure the investment is reasonable when measured against potential security risks?\nCreate and deploy policies and procedures – This is the crux of any working set of management processes. You have to have policies that clearly establish roles and responsibilities and assign ultimate responsibility for the implementation of each control to particular individuals or offices. Does your formal system security and contingency plan stand up to that kind of scrutiny?\nIn both the public and private sectors, hospitals, clinics, and other health care providers that manage private health information today must adhere to strict policies for ensuring that data is secure at all times. The best practices presented here can help ensure that data isn’t stolen or compromised, and that your organization doesn’t face steep fines for being out of compliance.\nBill Becker is Technical Director of SafeNet Assured Technologies. He can be reached at [email protected]","IDC Names Securiti a Worldwide Leader in Data PrivacyView\nPayment Card Industry Data Security Standard Compliance, better known as PCI DSS Compliance, a term which at first glance may seem daunting, is a critical framework that ensures the security of sensitive financial data. In a landscape rife with cyber threats and data breaches, understanding PCI DSS Compliance is paramount for organizations that handle credit card transactions.\nPayment Card Industry Data Security Standard, commonly known as PCI DSS, is a set of security standards and guidelines designed to ensure the secure processing, storage, and transfer of payment card data, including debit and credit card data. The two key objectives of PCI DSS compliance are protecting sensitive cardholder data and minimizing the likelihood of financial fraud and data breaches.\nInstead of being a one-size-fits-all strategy, PCI DSS Compliance is a flexible framework that organizations can customize to meet their unique requirements. PCI DSS is developed and upheld by the Payment Card Industry Security Standards Council (PCI SSC) and applies to all entities that handle credit card data, ranging from small online retailers to multinational organizations.\nPCI DSS certification refers to complying with several specific requirements and standards designed to ensure the secure handling of payment card data. The current version of the PCI DSS is PCI DSS v4.0.\nCybercriminals actively target digital transactions containing credit card data, and PCI DSS attempts to reduce such evolving security risks. These threats include:\nThreat: Unauthorized access to cardholder data while it's being processed, stored, or transferred.\nPCI DSS Requirement: When transmitting cardholder data over open, public networks, ensure data is encrypted.\nThreat: Malicious software with the ability to steal cardholder data and jeopardize system security.\nPCI DSS Requirement: Utilize antivirus software and update it regularly.\nThreat: Attempts made fraudulently to assume the identity of reliable organizations to obtain sensitive data.\nPCI DSS Requirement: Establish policies in place to protect against phishing attempts, such as training and awareness campaigns for employees.\nThreat: Inadequate authentication methods that may lead to unauthorized access.\nPCI DSS Requirement: Use strong authentication methods, such as multi-factor authentication, to protect system access.\nThreat: Weaknesses in network security that can be exploited to gain unauthorized access.\nPCI DSS Requirement: Regularly monitor and test networks and implement security measures like firewalls to protect cardholder data.\nThreat: Inability to promptly detect and respond to security incidents.\nPCI DSS Requirement: Implement robust logging and monitoring systems to track and alert security events.\nTokenization can be used to reduce the risk involved with maintaining actual card data by substituting non-sensitive tokens for sensitive cardholder data.\nFrom the point of interaction, such as a card swipe, until it reaches the payment processor, use P2PE to encrypt card data.\nConduct security audits and risk assessments to identify vulnerabilities and ensure PCI DSS compliance.\nThe PCI DSS security standards are implemented by an alliance of major credit card organizations, including Visa, Mastercard, American Express, JCB, and Discover, to ensure that every organization that accepts credit cards does so in a secure environment. Your organization may be classified into one of four PCI categories based on the annual volume of card transactions you process:\nPCI Level 1: Businesses processing over 6 million transactions per year\nPCI Level 2: Businesses processing 1 million to 6 million transactions per year\nPCI Level 3: Businesses processing 20,000 to 1 million transactions per year\nPCI Level 4: Businesses processing less than 20,000 transactions per year\nOrganizations must go through several phases and activities in the PCI DSS assessment process to ensure PCI DSS compliance. The following essential components are usually included in the process:\nEach system and procedure that handles, transmits, stores, or processes sensitive authentication data (SAD) and cardholder data (CHD) must be identified and recorded. Additionally, the cardholder data environment (CDE) must be determined to accurately assess the scope of PCI DSS compliance.\nGet familiar with the PCI DSS standard's 12 core requirements and accompanying sub-requirements, and recognize the specific security controls and practices that are essential to meet each requirement.\nSAQ types differ according to the processing, storing, and sending of cardholder data methods. Identify the relevant Self-Assessment Questionnaire (SAQ) or, in the case of Level 1 merchants, submit a Report on Compliance (ROC) evaluation.\nExamine your organization’s security controls with PCI DSS requirements to identify any vulnerabilities in the current security framework.\nTo comply with PCI DSS requirements, patch identified gaps by implementing security controls and measures. Develop and implement a remediation strategy to resolve vulnerabilities and enhance security.\nCreate security policies and procedures that align with PCI DSS requirements, document them, and ensure employees receive adequate training.\nEstablish a system for routine security testing and vulnerability assessments, as well as methods for continuous monitoring to identify and resolve security events swiftly.\nConduct the SAQ or ROC assessment in compliance with the specified parameters and relevant laws.\nProvide the acquiring bank and card brands with the completed SAQ or ROC and any necessary supporting documentation. Assure prompt submission and adherence to reporting requirements.\nEstablish and execute plans to identify and resolve any non-compliance concerns and collaborate with the QSA or Internal Security Assessor (ISA) to validate remediation efforts.\nMaintain accurate records of all security assessments, processes, policies, and remediation activities. These records would also come in handy for audit purposes and demonstrating ongoing compliance.\nEstablish procedures for continuous security control testing, monitoring, and evaluation. Additionally, to handle new threats and landscape changes, evaluate and update security measures on a regular basis.\nKeep track of compliance status by communicating with acquiring banks and payment card companies and respond to the credit card companies' requirements for more details or actions.\nReevaluate and validate compliance with PCI DSS requirements to renew the compliance status.\nInstall and maintain network security controls by employing strong firewalls, intrusion detection systems, and encryption methods to prevent data breaches and cyberattacks.\nApply secure configurations to all system components by changing default passwords, eliminating unnecessary software, functionalities, and accounts, and deactivating or uninstalling unnecessary services to reduce the possibility of compromising the system.\nProtect stored account data using encryption, truncation, masking, and hashing. Employ risk-reduction strategies such as avoiding holding account information unless absolutely essential, truncating cardholder data when the entire PAN is not required, and refraining from providing unprotected PANs via end-user messaging platforms like email and instant messaging.\nProtect cardholder data using strong cryptography keys during transmission over open and public networks. This increases the likelihood of data secrecy, integrity, and non-repudiation. Any transmissions of cardholder data through a network that stores, processes, or transmits cardholder data are immediately subject to PCI DSS. Such networks must be evaluated and assessed to comply with the applicable PCI DSS regulations.\nTo protect all systems and networks from malicious software, malicious software or firmware must be identified and eliminated. Examples of malicious software include viruses, worms, Trojans, spyware, ransomware, keyloggers, rootkits, malicious code, scripts, and links.\nDevelop and maintain secure systems and software to prevent security vulnerabilities that can be exploited to gain privileged access to systems. Organizations must routinely update their software components via the necessary software patches to ensure no software intrusion.\nRestrict access to system components and cardholder data by business need-to-know to ensure that only authorized individuals gain access to data. These requirements apply to user accounts and access for employees, contractors, consultants, internal and external vendors, and other third parties.\nTwo fundamental principles of identifying and authenticating users are to establish the identity of an individual or process on a computer system and prove or verify the user associated with the identity is who the user claims to be.\nThe element used to prove or verify the identity is known as the authentication factor. Authentication factors include something you know, such as a password or passphrase; something you have, such as a token device or smart card; or something you are, such as a biometric element.\nRestrict physical access to systems that store, process, or transmit cardholder data since it enables individuals to access and/or remove systems or hardcopies containing cardholder data.\nLog and monitor all access to system components and cardholder data to prevent, identify, or mitigate the effects of a data compromise. Logs are present on every system component and in the Cardholder Data Environment (CDE), enabling full monitoring, notification, and analysis if something goes wrong. Without system activity logs, it is difficult, if not impossible, to identify the cause of a compromise.\nTo ensure that security policies continue to take into account the ever-evolving environment, system components, processes, and customized and custom software should all undergo regular testing.\nThe overall information security policy of the organization establishes the tone for the entire organization and specifies what is expected of the employees. Every employee should understand the sensitivity of cardholder data and the need for protection.\nOrganizations face several obstacles in their quest for PCI DSS compliance, including:\nTo overcome these challenges, organizations must actively engage in strategic planning, demonstrate PCI DSS compliance commitment by implementing required security measures, and utilize PCI DSS compliance to negotiate the complex landscape of credit card security regulations.\nOrganizations that violate PCI DSS requirements may face dire repercussions, including:\nThere could be serious penalties from card companies and regulatory agencies. The fine amount depends on how serious the infraction was and how many cardholder data records were stolen.\nOrganizations that do not comply may be subject to higher transaction costs and extra scrutiny by payment processors and acquiring banks. This could result in processing payments with higher operational costs.\nA non-compliance-related data breach may damage customer trust. Consumers may become less confident in the organization’s ability to protect their personal data, which could harm its reputation and result in lost revenue.\nNon-compliance may initiate legal action, such as a lawsuit from impacted consumers, regulatory authorities, and payment card brands. Settling such lawsuits can drain an organization’s financial resources and dent its reputation.\nThe merchant accounts of organizations that do not comply may be closed by acquiring banks and payment processors. Revenue streams may be impacted if this interferes with the company's capacity to accept credit card payments.\nRegulatory agencies or payment card brands may require certain security remediation procedures. Organizations that don't comply may have to spend additional financial resources on security enhancements.\nIf there is a data breach, the organization can be held liable for the expenses associated with investigating and remediating the incident. This entails conducting legal investigations, informing those impacted, and implementing remedial action plans.\nRegulatory agencies and credit card companies may beef up their monitoring and auditing of non-compliant organizations. This might strain internal resources and interfere with regular corporate operations.\nOrganizations that do not comply with PCI DSS compliance risk missing out on opportunities to work with other organizations that prioritize security since compliance is frequently a requirement for collaboration.\nOrganizations may find it difficult to obtain cybersecurity insurance or may see higher premiums as a result of greater perceived risk if they do not comply with PCI DSS.\nFor non-compliant organizations, regulatory agencies and payment card companies may mandate more frequent and stringent compliance assessments, imposing a continuous cost on the firm.\nThe costs and efficiency of an organization can be negatively impacted by remediation activities and the repercussions from non-compliance, which include legal complexities and customer resentment towards the organization.\nPCI DSS compliance significantly benefits organizations that process credit card transactions. These include:\nOrganizations can implement strong security measures designed to reduce vulnerabilities to safeguard sensitive payment card data, reducing data breaches, fraud, and other security risks.\nIn the event of a data breach, non-compliance with PCI DSS may result in fines and penalties. Businesses can prevent these financial implications by achieving compliance.\nPCI DSS compliance may be required by law in certain regions. Being PCI DSS compliant ensures that the company is in good legal standing, preventing unforeseen legal complexities.\nOptimizing data security practices is frequently necessary to ensure PCI DSS compliance, ultimately resulting in more cost-effective and effective operations.\nMajor card networks like Visa and MasterCard require PCI DSS compliance to process payments, enabling companies to process payments swiftly without interruptions.\nPCI DSS compliance demonstrates a commitment to security, which protects and enhances an organization’s reputation and improves customer trust.\nCompliance with PCI DSS gives your company a competitive edge over competitors who might not be compliant.\nIndustry leaders embrace and mandate merchant compliance as failure to comply with the PCI DSS can lead to security lapses and the loss of sensitive credit card data, which can result in severe penalties and other legal consequences.\nIt is imperative for organizations to determine what type and level of encryption exists in their systems today. With a lack of visibility, it is essential for organizations to embrace automation and gain a holistic view of their data.\nSecuriti Data Command Center can scan, classify and keep encryption types in a data graph that is continuously updated and auditable. That visibility generates a prioritized work list for risk inventories and remediation plans.\nRequest a demo now to learn how Securiti can help improve compliance with PCI DSS v4.0."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:18e3134e-4c6c-496a-b8c4-b639465bd7ff>","<urn:uuid:a9acb6e1-37c4-4bad-b932-4578a90edb0e>"],"error":null}
{"question":"What preparations should be made before flooding, and what makes storm surge particularly destructive?","answer":"Before flooding, key preparations include signing up for alerts, keeping vehicles fueled, storing drinking water and non-perishable food, maintaining first aid supplies, and keeping NOAA Weather Radio and flashlights operational. It's also important to know local shelter locations and evacuation routes. Regarding storm surge's destructive nature, it is typically the most devastating aspect of hurricanes, capable of causing extensive damage even in lower-category storms. For example, during Hurricane Sandy, storm surge caused flooding up to nine feet above ground in parts of New York and New Jersey, leading to billions in damage, despite being only a Category 1 hurricane.","context":["Flooding occurs when water accumulates faster then it can escape. There are many reasons this happens including naturally occurring depressions, inadequate or ineffective drainage and excessive rainfall.\nBecause of the large number of lakes, rivers, and streams in the County flooding can become a big problem very quickly. Your ability to prepare for this depends on your understanding of where the water is likely to come from and how it is likely to affect you.\nAsk yourself these questions;\n- Do you live on or near a lake, river, or stream?\n- Do you live in a low area that has the potential to collect water from the surrounding area?\n- Do you know of any drainage problems in your area?\n- Has there been any past flooding in your area?\nSOME potential flooding sources and areas that have flooded in the past:\n- W. Cummings near Lakeshore Way\nBefore the Flood\n- Sign-Up for Alert Polk to receive alerts on your phone or via email.\n- Keep your automobile fueled; after a flood, gas stations may not be able to operate pumps for several days.\n- Store drinking water in clean bathtubs and in various containers.\n- Keep a stock of food that requires little cooking and no refrigeration.\n- Keep first aid supplies on hand.\n- Keep a NOAA Weather Radio, emergency cooking equipment, and flashlights in working order.\n- Know the location of the Polk County public shelters near you.\n- Become familiar with the Evacuation Routes in Polk County.\n- A Hurricane Kit can serve for both situations.\nDuring the Flood\n- Get out of areas subject to flooding. This includes dips, low spots, streams, rivers, etc.\n- Avoid already flooded and high velocity flow areas.\n- Do not attempt to cross flowing streams. If you come upon a flowing stream where water is ankle deep, STOP! Turn around and go another way.\n- If driving, be aware that the road bed may not be intact under flood waters. Turn around and go another way. NEVER drive through flooded roadways!\n- Be especially cautious at night when it is harder to recognize flood dangers.\nAfter the Flood\n- If fresh food has come in contact with flood waters, throw it out.\n- Boil drinking water before using.\n- Wells should be pumped out and the water tested for purity before drinking. If in doubt, call your local public health authority.\n- Seek necessary medical care at the nearest hospital.\n- Do not visit disaster areas. Your presence might hamper rescue and other emergency operations.\n- Stay away from power lines and electrical wires\n- Turn off your electricity and gas when you return home\n- Electrical equipment should be checked and dried before being returned to service.\n- Use flashlights, not lanterns, torches or matches, to examine buildings.\n- Report broken utility lines to appropriate authorities.\n- Look before you step and watch for animals, especially snakes\nGetting Updated Information\nFor up to date information on severe weather (including flooding) please tune to local media stations including: Bay News Nine (Cable Channel 9), WONN 1230AM, and WPCV 97.5 FMThe National Weather Service continually monitors local weather conditions. If major flooding is anticipated within the County, NOAA will broadcast notices on the NOAA Weather Radio. These notices will also be broadcast through television and standard radio stations. They are intended to help residents prepare for the possibility of severe flooding in the neighborhood.\nWarning times for these events may be as long as five days for hurricanes, down to one to two hours notice for flash floods and urban advisories. The key is to keep aware of these notices when watching, listening to or reading the various media sources.","Storm surge, the massive amount of water that builds up and comes ashore during a hurricane, is often the deadliest and most destructive threat from these devastating storms.\nStorm surge has accounted for about half of all the deaths in hurricanes since 1970, according to the National Hurricane Center. It caused most of the 1,800 deaths in Hurricane Katrina in 2005.\nStorm surge is characterized by water being pushed toward the shore by the force of the winds moving around the storm, the National Oceanic and Atmospheric Administration said.\nStorm surge watches and warnings are now separated from hurricane alerts because hurricane-force winds and storm surges don't always occur at the same place or the same time, said Rick Knabb, former director of the Hurricane Center and now the hurricane expert at the Weather Channel.\nHurricane Ian eyes Florida: Live updates as Hurricane Ian build strength, targets Florida\nHurricane categories, explained: What is the Saffir-Simpson hurricane wind speed scale? Explaining hurricane categories.\nPreparing for hurricane-force winds is different than for storm surge, he said. For storm surge, people should evacuate; for wind, they can stay in place in a strong structure as long as it's away from flood-prone areas, Knabb said.\nWhat is a storm surge warning?\nA \"storm surge watch\" is issued when flooding is possible, while a \"warning\" is issued when flooding is expected. Every coastal city along the Gulf or East Coast of the U.S. is at risk of storm surge, the center said.\nIn addition to being the deadliest threat, storm surge is also typically the most destructive part of a hurricane. In Hurricane Sandy in 2012, storm surge-induced flooding measured as high as nine feet above ground in parts of New York and New Jersey, leading to billions of dollars in damage.\nRapid intensification: Experts say to beware of more rapidly intensifying storms\nWhat is a tornado? Everything you need to know about these violent storms\nThe damage occurred even though Sandy spun ashore as the equivalent of only a Category 1 hurricane, with winds of about 80 mph, and was downgraded below hurricane status shortly thereafter.\nImpact of climate change\nClimate change could worsen storm surge, one recent study suggests. The frequency of extreme storm surges is projected to increase by as much as 10 times in coming decades because of rising temperatures, a 2013 study found. Global warming has already doubled the chance of storms like Katrina, according to the study, which was led by climate scientist Aslak Grinsted of the University of Copenhagen in Denmark.\nWhat is a flash flood watch or warning?Here's what to know about this deadly weather hazard\nStorm surge flooding does not include floods caused by the heavy rain from a hurricane, such as what happened in 2017 in the Houston area with Hurricane Harvey and in North Carolina in 2016 during Hurricane Matthew.\nIt also has nothing to do with tsunamis, large ocean waves generated by offshore earthquakes that are not related to weather.\nThis article originally appeared on USA TODAY: What is storm surge? Explaining hurricane's deadly, destructive threat"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:9487112c-b8e0-4d58-9564-eaddd3e7d20f>","<urn:uuid:6b1cfe5b-475b-4df0-b780-dea15694bd50>"],"error":null}
{"question":"How do the duration requirements differ between completing the Wine Business Management Certificate and becoming a Master Sommelier at the introductory level?","answer":"The Wine Business Management Certificate's first level (Foundation) requires 16 hours over 4 weeks, while the Master Sommelier Introductory Course is completed in just two intensive days. Both programs provide foundational knowledge, but they differ significantly in time commitment and structure.","context":["Course Series Information\nThe Wine Business Management Certificate is made up of three independent levels. Each level builds upon the next and is designed to challenge participants while creating a superior understanding of wine business operations.\n(16 hours over 4 weeks)\nFoundation: Introduction to Wine Business\nThis four week course provides an overview of wine business. Students will develop an appreciation of the realities of the wine business as a business and come to understand the steps required for getting from the vineyard and into the glass of the consumer.\n- Understand the components of a wine business and the viewpoints of the various stakeholders in the areas of:\n- Grape growing, including its costs, values, quality considerations, supply and demand, risks and opportunities\n- Production, winemaking, maturation and packaging\n- Distribution, a complex and highly regulated process\n- Domestic sales, selling wine in retail stores (off-premise), restaurants and bars (on premise), online and direct to consumer (through tasting rooms, events and wine clubs)\n(24 hours over 8 weeks)\nPrerequisites: Successful completion of Level 1\nIntermediate: A Survey of the Key Drivers in a Wine Business\nBuilding on the concepts developed in the Introduction to Wine Business, this eight week course expands the project-based learning focused on the business of making and selling wine. The class surveys the key drivers of a wine business and the necessary decisions when a wine business goes to market, providing a deeper understanding of the entire value chain.\nThe subject of marketing, distributing and selling expands to include both a primary brand as well as a range of products, including options for negociant brands.\n- Understand the business issues associated with viticulture, production, marketing, distribution and points of sale.\n- Evaluate alternative approaches associated with those issues.\n- Calculate costs across the component areas.\n- Evaluate the contribution of the component areas to the value of a wine product.\n- Expand the understanding of distribution and regulatory issues to include the international wine market.\n- Assess the internal environment, identifying the strengths and weaknesses in a wine business.\n- Assess the external environment, identifying the opportunities and threats to a wine business.\n- Evaluate a SWOT analysis and draft a plan for going to market with a new wine business product.\n- Plan for and consider future wine markets and alternative channels of wine commerce.\n(48 hours over 16 weeks, plus optional certification exam)\nPrerequisites: Successful completion of Levels 1 and 2\nParticipants will choose two of the following three electives:\nGlobal Wine E-commerce (24 hours over 8 weeks)\nThis level provides an in-depth study of electronic commerce aspects from a wine industry perspective. Participants will explore opportunities and challenges associated with electronic commerce (e-commerce/e-business), and review the impact of e-commerce with meeting strategic objectives of an organization in the wine industry. This level includes an overview of e-business issues as they pertain to direct-to-consumer and government oversight/compliance, wine club management, and winery management software.\nAt the conclusion of this level, participants will be able to:\n- Explain how electronic commerce affects the strategic intent and operational functioning of a wine industry organization\n- Describe the procedures for developing and managing an electronic commerce site, as it furthers the strategic, marketing, financial, or operational plans of an organization\n- Make recommendations in design of web-site that implement best practices in electronic commerce\n- Identify and explain the various security, legal, and privacy issues and understand how they may impact a wine industry organization's e-business strategy\n- Evaluate feasibility of and recognize benefits and limitations of various winery management software systems as they relate to direct-to-consumer issues\n- Effectively communicate with customers through the available digital channels\n- Evaluate and optimize the customers' mobile experience\nProduction and Quality in the Wine Supply Chain (24 hours over 8 weeks)\nThis level provides an in-depth review of the business issues associated with wine production and operations. This level emphasizes the basic concepts of operations, purchasing, logistics, and supply chain management as they apply to the wine industry. More specific topics include value analysis, total quality management, make/buy decisions, negotiation, and supplier development.\nAt the conclusion of this level, the participant will be able to:\n- Explain the key business issues in the production and operation of a winery\n- Identify supply chain management considerations for effective operations\n- Describe the planning process from wine forecasting and product development through distribution\n- Calculate total cost analysis and its role in supply procurement\n- Assess the appropriateness of the capital equipment necessary to operate a winery\n- Identify how quality is achieved in the production of wine\nThis level provides an introduction to wine marketing terminology and concepts, including the steps in brand creation, packaging decisions, integrated communication strategy, navigating the three tier distribution channels, and direct to consumer sales. Topics of tasting room management, wine tourism, importing, and exporting wine are also presented. The culminating project is the creation of a marketing and sales plan for the launch of a new wine business.\nAt the conclusion of this level participants will be able to:\n- Describe important wine consumer segments\n- Create a wine brand to appeal to a chosen target market\n- Understand the marketing implications of package design\n- Coordinate promotional elements of advertising, public relations, and special events\n- Understand the complexities of wine distribution at the distributor and retail levels\n- Create a distribution and sales strategy utilizing distributors and/or direct to consumer methods\n- Describe key aspects of tasting room management\n- Integrate wine marketing into a broader wine tourism context\n- Describe the steps to importing and exporting wine\n- Create a draft marketing and sales plan\nFor more information or to register contact:\nWine Business Institute","Introductory Sommelier Course & Exam\nThe Master Sommelier Introductory Sommelier Course & Examination is the first of four required steps to become a Master Sommelier. The program is given over a two day period with candidates receiving intensive review, instruction and training by a team of Master Sommeliers on wines and spirits knowledge, proper wine service, and blind tasting. The program ends on the second day with the Introductory Sommelier Examination, a multiple choice theory exam consisting of 70 questions. The intent of the Introductory Sommelier Course is to provide wine and hospitality professionals with a thorough review to the world of wines and spirits at the highest professional standards.\nIntroductory Sommelier Course & Examination $525\nThe Introductory Sommelier Course & Examination is open to all beverage and hospitality professionals interested in pursuing the highest standards of wine service and product knowledge in a fine dining room setting. Candidates come from restaurant, wholesale and retail backgrounds. The Introductory Sommelier Course & Examination is also a prerequisite for the Certified Sommelier Examinations, which includes a written theory exam, a blind tasting exam and a practical wine service exam. Candidates have three years after passing the Introductory Sommelier Examination to sit the Certified Examination or the candidate must re-take the Introductory Course & Examination at the reduced rate of $325.\nA candidate’s success in the Introductory Sommelier Course & Examination will depend on a combination of career experience and level of preparation. The Introductory program is two-days of a very fast-paced intensive review of the world's wine producing regions, elements of wine service, and several tasting exercises where candidates will learn the Court of Master Sommeliers Deductive Tasting Method. At the end of the second (and final) day of the course, a multiple-choice exam of seventy questions is given. A student must achieve a minimum score of 60% to pass. It is highly recommended that students acquire a general guide to wine to reference and reinforce the topics included on the Introductory Course syllabus. Download the Introductory Course Syllabus and Recommended Study Resources at www.mastersommeliers.org/resources.\nCandidates who successfully complete the Introductory Sommelier Examination have achieved the following:\n- Have received intensive instruction over two days from a team of Master Sommeliers.\n- Have received an overview of all the major wine growing regions of the world, as well as instruction in spirits, beer, saké and proper wine service.\n- Have passed a written examination based on the material covered throughout the 2-day course.\n- Are capable of discussing, buying and serving a comprehensive range of alcoholic beverages with confidence and skill.\n- Have learned the Court of Master Sommeliers Deductive Tasting Method of blind tasting from some of the best wine tasters in the world—a tasting method that is an integral part of the Certified Sommelier Exam, the Advanced Course and the Master Sommelier Diploma. The acquired tasting skills will enable them to confidently recognize both wine quality and flaws.\n- Have received adequate direction to successfully study and prepare for the Certified Sommelier Exam, the prerequisite for the Advanced Course and Master Sommelier Exam itself.\nThe Introductory Course is two-days of a very fast-paced intensive review of the world's wine producing regions, elements of wine service, and several tasting exercises. At the end of the second day, from approx. 3:45 to 4:30, a multiple-choice exam of seventy questions is given followed by the results and sparkling wine reception at 5:00.\nA student’s success in the Introductory Course & Examination will depend on a combination of career experience and level of preparation. A minimum of three years in the wine/service industry is strongly recommended. It is expected that students walk in the door with strong knowledge of all major wine producing countries and regions of the world and basic wine service. In preparation, it's recommended that students acquire a general guide to wine to reference and reinforce the topics included on the Introductory Course syllabus. Download the Introductory Course Syllabus and Recommended Study Resources at www.mastersommeliers.org/resources. Upon enrollment, students receive access to download the Introductory workbook which follows the PowerPoint presentation.\nThe Court of Master Sommelier programs are specifically designed for those in the beverage hospitality industry and not the wine consumer. For the Introductory Course & Examination and the Certified Examination we recommend that you have a minimum of three years in the beverage service industry, but it is not required. The Advanced Course & Examination and the Master Sommelier Diploma Examination have a mandatory three year minimum experience in the beverage service industry before applying.\nIt is expected that students walk in the door with strong knowledge of all major wine producing countries and regions of the world and basic wine service. A minimum of three years in the wine/service industry is recommended, but not required.\nIt is without exception that candidates must be 21 years of age.\nWe do not mail the workbook. Upon enrollment, students can download the workbook. At the course check-in, students will receive a spiral-bound hard copy of the workbook to use in class and take home.\nWe do not offer practice exams. The Guild of Sommeliers, a national community for sommeliers looking to forward their knowledge and skills in the beverage service arena, provides invaluable study guides and review questions within the “Learn” section of their website, www.guildsomm.com, that students in our programs find very beneficial. The Court of Master Sommeliers supplies every first time Introductory Course & Examination registrant with a complimentary 1-year subscription to the Guild of Sommeliers.\nAttire for the Introductory is smart or upscale casual; jackets and ties are not required. No tank tops, shorts or jeans, please. We advise against wearing perfume, cologne, scented lotions, or similar products as this can disrupt you or others while tasting wines.\nIt is the policy of the Court of Master Sommeliers that we do not provide the list of wines poured at any of our programs. The wines that are selected are wines that represent a region or a characteristic well. It is the representation we want you to understand. For example, you should understand that wine “x” represented a region because of “y” and “z” and that wine “a” produced more oak than wine “b” due to a specific reason – all cause and effect. By not providing the list we encourage you to engage in the cause and effect and learn varietal profiles yourself.\nWe allow up to 20 students on the waitlist. The amount of students that get in from the waitlist depends on cancellations. Typically, if students cancel it is at least 31 days prior to the program. If a seat becomes available, notification is sent via email.\nCan I be enrolled in a program and on a waitlist?\nYes, you can be enrolled in a program and be on as many waitlists as you would like. Should a seat open up from the waitlist, and you accept the seat, we can transfer your payment over.\nIs there a fee if I cancel from the waitlist or decline a seat when offered to me? No.\nThe following policy applies to the Introductory Course & Examination, Certified Examination, and the Deductive Tasting Method Workshop.\n- Cancellation notification must be submitted in writing and emailed to email@example.com.\n- 45 days or more: $50.00\n- 31-44 days: 50% of the registration fee.\n- 30 days or less: 100% of the registration fee (no refund or transfer).\n- Cancellation due to an illness or medical emergency require a doctor’s note within 14 days of the program for refund consideration.\n- Cancellation due to personal reasons, work conflicts, etc will not receive a refund.\n- CMS is not responsible for cancellations that were emailed but never received.\nApplies to the Introductory Course & Examination only. The Court of Master Sommeliers offers a discounted fee of $325 to retake the Introductory Course & Examination based upon the following criteria: You must retake within a calendar year of your prior Introductory Course & Examination. You must wait 90 days before retaking. To receive this rate, please email firstname.lastname@example.org with the non-waitlisted program listed on our schedule that you would like to take.\nIt is without exception that all candidates must attend both days and pass the examination of the Introductory Course & Examination prior to taking the Certified Examination.\nThe time the CMS allows between passing the Introductory Sommelier Course & Exam and taking the Certified Sommelier Exam is 3 years; 5 years with a recommendation from a Master Sommelier who is directly mentoring you. After the elapsed time students are required to retake the Introductory Course & Exam in order to apply for the Certified Exam.\nWe recommend candidates only take these two programs back to back if they are in the service industry and proficient with their tasting and service skills. The Certified Examination has a pass rate of 64% and it is lower for many who take the Certified Examination directly after taking the Introductory Course & Examination. We introduce the tasting technique and the style of service that is examined at our Certified Examination in the Introductory Course, and taking them back to back does not give a candidate a lot of time to practice these items. If you feel that you are prepared for both then you can register for both.\nOur programs have been approved by the CA Department of Veteran Affairs. Please visit http://www.benefits.va.gov/gibill/licensing_certification.asp.\nNo. Attendees are responsible for their own transportation and accommodation arrangements.\nNo electronics are allowed in the examination rooms. Attendees are NOT permitted to record the lectures."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:b972c006-40f1-46b1-8b7f-a88de489448f>","<urn:uuid:a15f8f33-4751-4b23-a298-6dd4a99114fb>"],"error":null}
{"question":"As a researcher studying cancer treatments, I'm curious how GT19715 and Takeda's cell therapy programs differ in their approaches to treating blood cancers?","answer":"GT19715 and Takeda's cell therapy programs take different therapeutic approaches to blood cancers. GT19715 works by degrading the c-Myc protein, which is dysregulated in 70% of cancers, and has shown promising results in treating therapy-resistant AML and venetoclax-resistant cases. In contrast, Takeda's cell therapy programs use genetically modified immune cells to target cancer cells, including CD19-targeted CAR-NK cells (TAK-007) for non-Hodgkin's lymphoma and CLL, and next-generation CAR-T cells (TAK-940) for relapsed/refractory B-cell cancers.","context":["EQS Newswire / 10/11/2022 / 10:09 UTC+8\nKintor Pharma Announces Oral Presentation at ASH 2022 Highlighting the Data from Novel Dual C-Myc-GSPT1 Degrader GT19715\nSuzhou, November 10, 2022 – Kintor Pharmaceutical Limited (“Kintor Pharma”, HKEX: 9939), a clinical-stage biotechnology company developing innovative small molecules and biological therapeutics, announced that the results from the novel dual c-Myc/GSPT1 degrader GT19715 have been selected for oral presentation at the annual meeting of the American Society of Hematology 2022 (ASH 2022), and the abstract is available on ASH’s official website.\nThe ASH annual meeting is the largest conference worldwide of hematology, bringing together basic, translational and clinical researchers in hematology. ASH 2022 will be held from Saturday, December 10, to Tuesday, December 13, 2022, in New Orleans, Louisiana.\nThe oncoprotein c-Myc is dysregulated in 70% of all human cancers, mostly with amplified expression levels and enhanced activities to promote progression of cancers and leukemias. Rearrangements of MYC are best known in the translocation in Burkitt’s lymphomas, however, increased expression levels are also observed in acute myeloid leukemias (AML), including those with TP53 mutations or venetoclax (ven) resistance, both of which have devastating clinical outcomes despite recent advances in AML therapies.\nThe senior author Michael Andreeff, M.D., Ph.D., professor of Leukemia at The University of Texas MD Anderson Cancer Center, together with his team and Kintor Pharma, evaluated preclinical activities of the first-in-class c-Myc protein degrader GT19715 in therapy resistant AML models.\nUtilizing single cell mass cytometry, researchers found increased c-Myc protein levels in immature CD34+ AML cells compared to normal bone marrow CD34+ cells. GT19715 demonstrated promising anti-leukemia efficacy in patient-derived AML cells with TP53 mutations, especially in immature CD34+ fractions; and treatment of GT19715 in mice eradicated lymphoma and leukemia cells in Daudi Burkitt’s lymphoma and Venetoclax-resistant AML models. Presentation will include updates and results will be presented by the first author Yuki Nishida, M.D., Ph.D. on Dec 10, 2022.\n“We are really thrilled to present the data of encouraging preclinical activities of the first degrader of MYC proteins especially in MYC-driven leukemias and lymphomas at ASH this year. MYC governs various oncogenic programs for cancer initiation and progression and has long been one of the “undruggable” targets in cancer therapy despite numerous attempts to target MYC or its pathways. Actually, we reported MYC inhibition by p53 reactivation at the ASH last year, but now we have the first compound that successfully degrades MYC itself, demonstrating very promising early activity, which should also be applicable to other MYC-driven malignancies.” Dr. Andreeff commented.\nDr. Youzhi Tong, founder, Chairman, and Chief Executive Officer of Kintor Pharma, commented, “We are very excited that the results of the novel dual c-Myc/GSPT1 degrader GT19715 were selected for oral presentation at the ASH annual meeting, which indicates the international hematology community’s recognition of GT19715’s therapeutic potential. We will accelerate the progress of GT19715 into the clinical stage and hope to bring more innovative treatment options for patients with unmet needs.”\nThe abstract of GT19715 presented at the ASH 2022 is as follows:\nC-MYC Targeting by Degradation: Novel Dual c-Myc/GSPT1 Degrader GT19715 Induces TP53-independent Cell Death in Acute Myeloid Leukemia and Lymphomas\nFormat: Oral Presentation\nSession: 604. Molecular Pharmacology and Drug Resistance: Myeloid Neoplasms: Targeted Protein Degradation and Emerging Small Molecule Inhibitors in Myeloid Neoplasms\nTime: Saturday, December 10, 2022: 2:00 PM\nAbout Kintor Pharma\nKintor Pharma is developing and commercializing a robust pipeline of innovative small molecule and biological therapeutics for androgen-receptor-related disease areas with unmet medical needs, including COVID-19, prostate cancer, breast cancer, liver cancer, alopecia and acne. For more information, please visit www.kintor.com.cn.\n10/11/2022 Dissemination of a Financial Press Release, transmitted by EQS News.\nMedia archive at www.todayir.com","Takeda Opens New R&D Cell Therapy Manufacturing Facility to Support Expansion of Next-Generation Clinical Programs\nTakeda Pharmaceutical Company Limited (TSE:4502/NYSE:TAK) (“Takeda”) today announced the expansion of its cell therapy manufacturing capabilities with the opening of a new 24,000 square-foot R&D cell therapy manufacturing facility at its R&D headquarters in Boston, Massachusetts. The facility provides end-to-end research and development capabilities and will accelerate Takeda’s efforts to develop next-generation cell therapies, initially focused on oncology with potential to expand into other therapeutic areas.\nThis press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20200915005224/en/\n“We are collaborating with some of the best scientists and innovators around the world establishing a highly differentiated immuno-oncology pipeline leapfrogging into new modalities and mechanisms with curative potential,” said Chris Arendt, Ph.D., Head of Takeda’s Oncology Therapeutic Area Unit. “With three oncology cell therapy programs in the clinic and two more targeted to enter the clinic in fiscal year 2021, we are working with urgency and purpose for patients. This new facility helps us rapidly scale our manufacturing capabilities so we can simultaneously advance multiple highly differentiated cell therapy programs.”\nOncology cell therapy is a type of immunotherapy that uses genetically modified immune cells to find and kill cancer cells. Because cell therapies are engineered from living cells, they need to be manufactured in a highly regulated environment to maintain cleanliness, consistency and contamination control. Each oncology cell therapy platform has unique process requirements for how they are formulated, manufactured, transported and ultimately administered to patients. Next-generation cell therapy is one of the multiple investigational platforms that Takeda is researching in oncology as part of its focus on redirected immunity. Takeda’s pipeline of diverse immuno-oncology programs harnesses innate immunity, including through innovative cell therapies, immune engager platforms, innate immuno-modulation, novel-scaffold immune check point platforms and oncolytic viruses.\nA Purpose-Built Facility to Rapidly Advance Cell Therapy Research & Development\nThe R&D cell therapy manufacturing facility will produce cell therapies for clinical evaluation from discovery through pivotal Phase 2b trials. The current Good Manufacturing Practices (cGMP) facility is designed to meet all U.S., E.U. and Japanese regulatory requirements for cell therapy manufacturing to support Takeda clinical trials around the world. It will be instrumental in building Takeda’s cell therapy capabilities and capacity to advance multiple next-generation oncology cell therapy platforms and programs with world-class collaborators including Nobel Laureate Shinya Yamanaka, M.D., Ph.D., Kyoto University (induced pluripotent stem cells), Adrian Hayday, Ph.D., Gamma Delta Therapeutics (gamma delta T-cells), Koji Tamada, M.D., Ph.D., Noile-Immune Biotech (armored CAR-Ts), Michel Sadelain, M.D., Ph.D., Memorial Sloan Kettering Cancer Center (next-generation CARs), and Katy Rezvani, M.D., Ph.D., The University of Texas MD Anderson Cancer Center (CAR-NK).\nTakeda and MD Anderson are developing a potential best-in-class allogeneic cell therapy product (TAK-007), a Phase 1/2 CD19-targeted chimeric antigen receptor-directed natural killer (CAR-NK) cell therapy with potential for off-the-shelf use being studied in patients with relapsed or refractory non-Hodgkin’s lymphoma (NHL) and chronic lymphocytic leukemia (CLL). Two additional Phase 1 studies of Takeda cell therapy programs were also recently initiated: 19(T2)28z1xx CAR T cells (TAK-940), a next-generation CAR-T signaling domain developed in partnership with Memorial Sloan Kettering Cancer Center (MSK) to treat relapsed/refractory B-cell cancers, and a cytokine and chemokine armored CAR-T (TAK-102) developed in partnership with Noile-Immune Biotech to treat GPC3-expressing previously treated solid tumors. Dr. Sadelain and MSK have intellectual property rights and associated interests related to the content of this release by virtue of licensing agreements between MSK and Takeda.\nHarnessing the Power of Takeda’s Cell Therapy Translational Engine\nProactive and deep collaboration between research and development and commercial manufacturing is critical to developing and delivering next-generation cell therapies. Takeda’s Cell Therapy Translational Engine (CTTE) connects clinical translational science, product design, development, and manufacturing through each phase of research, development and commercialization. It provides bioengineering, chemistry, manufacturing and control (CMC), data management, analytical and clinical and translational capabilities in a single footprint to overcome many of the manufacturing challenges experienced in cell therapy development.\n“The proximity and structure of our cell therapy teams allow us to quickly apply what we learn across a diverse portfolio of next-generation cell therapies including CAR NKs, armored CAR-Ts and gamma delta T cells, among others,” said Stefan Wildt, Ph.D., Head of Pharmaceutical Sciences and Translational Engine, Cell Therapies at Takeda. “Insights gained in manufacturing and clinical development can be quickly shared across our global research, manufacturing and quality teams, a critical ability in our effort to deliver potentially transformative treatments to patients as fast as we can.”\nAbout Takeda Pharmaceutical Company Limited\nTakeda Pharmaceutical Company Limited (TSE:4502/NYSE:TAK) is a global, values-based, R&D-driven biopharmaceutical leader headquartered in Japan, committed to bringing Better Health and a Brighter Future to patients by translating science into highly-innovative medicines. Takeda focuses its R&D efforts on four therapeutic areas: Oncology, Rare Diseases, Neuroscience, and Gastroenterology (GI). We also make targeted R&D investments in Plasma-Derived Therapies and Vaccines. We are focusing on developing highly innovative medicines that contribute to making a difference in people's lives by advancing the frontier of new treatment options and leveraging our enhanced collaborative R&D engine and capabilities to create a robust, modality-diverse pipeline. Our employees are committed to improving quality of life for patients and to working with our partners in health care in approximately 80 countries.\nFor more information, visit https://www.takeda.com.\nFor the purposes of this notice, “press release” means this document, any oral presentation, any question and answer session and any written or oral material discussed or distributed by Takeda Pharmaceutical Company Limited (“Takeda”) regarding this release. This press release (including any oral briefing and any question-and-answer in connection with it) is not intended to, and does not constitute, represent or form part of any offer, invitation or solicitation of any offer to purchase, otherwise acquire, subscribe for, exchange, sell or otherwise dispose of, any securities or the solicitation of any vote or approval in any jurisdiction. No shares or other securities are being offered to the public by means of this press release. No offering of securities shall be made in the United States except pursuant to registration under the U.S. Securities Act of 1933, as amended, or an exemption therefrom. This press release is being given (together with any further information which may be provided to the recipient) on the condition that it is for use by the recipient for information purposes only (and not for the evaluation of any investment, acquisition, disposal or any other transaction). Any failure to comply with these restrictions may constitute a violation of applicable securities laws.\nThe companies in which Takeda directly and indirectly owns investments are separate entities. In this press release, “Takeda” is sometimes used for convenience where references are made to Takeda and its subsidiaries in general. Likewise, the words “we”, “us” and “our” are also used to refer to subsidiaries in general or to those who work for them. These expressions are also used where no useful purpose is served by identifying the particular company or companies.\nThis press release and any materials distributed in connection with this press release may contain forward-looking statements, beliefs or opinions regarding Takeda’s future business, future position and results of operations, including estimates, forecasts, targets and plans for Takeda. Without limitation, forward-looking statements often include words such as “targets”, “plans”, “believes”, “hopes”, “continues”, “expects”, “aims”, “intends”, “ensures”, “will”, “may”, “should”, “would”, “could” “anticipates”, “estimates”, “projects” or similar expressions or the negative thereof. These forward-looking statements are based on assumptions about many important factors, including the following, which could cause actual results to differ materially from those expressed or implied by the forward-looking statements: the economic circumstances surrounding Takeda’s global business, including general economic conditions in Japan and the United States; competitive pressures and developments; changes to applicable laws and regulations; the success of or failure of product development programs; decisions of regulatory authorities and the timing thereof; fluctuations in interest and currency exchange rates; claims or concerns regarding the safety or efficacy of marketed products or product candidates; the impact of health crises, like the novel coronavirus pandemic, on Takeda and its customers and suppliers, including foreign governments in countries in which Takeda operates, or on other facets of its business; the timing and impact of post-merger integration efforts with acquired companies; the ability to divest assets that are not core to Takeda’s operations and the timing of any such divestment(s); and other factors identified in Takeda’s most recent Annual Report on Form 20-F and Takeda’s other reports filed with the U.S. Securities and Exchange Commission, available on Takeda’s website at: https://www.takeda.com/investors/reports/sec-filings/ or at www.sec.gov. Takeda does not undertake to update any of the forward-looking statements contained in this press release or any other forward-looking statements it may make, except as required by law or stock exchange rule. Past performance is not an indicator of future results and the results or statements of Takeda in this press release may not be indicative of, and are not an estimate, forecast, guarantee or projection of Takeda’s future results.\n+81 (0) 3-3278-2095\nMedia outside Japan\n+1 (617) 374-7726\nAbout Business Wire\nSubscribe to releases from Business Wire\nSubscribe to all the latest releases from Business Wire by registering your e-mail address below. You can unsubscribe at any time.\nLatest releases from Business Wire\nESMO 2020: Cabometyx ® (cabozantinib) in Combination With Opdivo ® (nivolumab) Demonstrates Significant Survival Benefits in Patients With Advanced Renal Cell Carcinoma in Pivotal Phase III CheckMate -9ER Trial19.9.2020 18:30:00 CEST | Press release\nRegulatory News: Ipsen (Euronext: IPN; ADR: IPSEY) today announced the first presentation of results from the pivotal Phase III CheckMate -9ER trial, in which Cabometyx® (cabozantinib) in combination with Bristol Myers Squibb’s Opdivo® (nivolumab) demonstrated significant improvements across all efficacy endpoints, including overall survival (OS), in previously untreated advanced renal cell carcinoma (RCC).1 Cabometyx® in combination with Opdivo® reduced the risk of death by 40% versus sunitinib (HR: 0.60 [98.89% Confidence Interval [CI]: 0.40–0.89]; p= 0.0010; median OS not reached in either arm). In patients receiving Cabometyx® in combination with Opdivo®, median progression-free survival (PFS), the trial’s primary endpoint, was doubled compared to those receiving sunitinib alone: 16.6 months versus 8.3 months respectively (Hazard Ratio [HR]: 0.51 [95% CI 0.41–0.64], p < 0.0001). In addition, Cabometyx® in combination with Opdivo® demonstrated a superior objective response rate, wit\nTakeda Presents New Data Highlighting Scientific Advancements in Lung Cancer at ESMO Virtual Congress18.9.2020 12:55:00 CEST | Press release\nTakeda Pharmaceutical Company Limited (TSE:4502/NYSE:TAK) (“Takeda”) today announced that the company is presenting data from its lung cancer portfolio at the virtual European Society for Medical Oncology (ESMO) conference. Notably, insights from sub-analyses of the Phase 3 ALTA 1L study reinforce both the compelling evidence of intracranial efficacy with ALUNBRIG® (brigatinib) as a first-line treatment for patients with anaplastic lymphoma kinase-positive (ALK+) non-small cell lung cancer (NSCLC) as well as associated quality of life (QoL) data. Takeda is also featuring updated 10-month follow-up results from the Phase 1/2 trial of mobocertinib (TAK-788), demonstrating mobocertinib achieved a duration of response (DoR) of more than one year in the trial’s study population of patients with epidermal growth factor receptor (EGFR) Exon20 insertion+ metastatic NSCLC (mNSCLC). “We’re pleased to present our ongoing research in lung cancer at this year’s virtual ESMO congress, including new\nIncyte Announces Encouraging Results From Phase 2 Trial of Retifanlimab (INCMGA0012) in Patients With Previously Treated, Advanced Squamous Cell Carcinoma of the Anal Canal18.9.2020 12:00:00 CEST | Press release\nIncyte (Nasdaq:INCY) today announced results from its Phase 2 POD1UM-202 trial evaluating retifanlimab, a PD-1 inhibitor, in previously treated patients with advanced squamous cell carcinoma of the anal canal (SCAC) who have progressed following standard platinum-based chemotherapy. The trial enrolled 94 patients, including those with well-controlled human immunodeficiency virus (HIV) infection (10%). Retifanlimab monotherapy resulted in a confirmed objective response rate (ORR) of 14% as determined by independent central review (ICR) using RECIST v1.1. Responses were observed regardless of PD-L1 status, presence of liver metastases, age or HIV+ status. Retifanlimab was generally well-tolerated with a safety profile as expected of a PD-1 inhibitor and no loss of HIV infection control. Key findings from POD1UM-202: N=94 ORR* (95% CI) 13.8% (7.6-22.5) Best OR*, n 1 CR 12 PR 33 SD DCR 48.9% DOR, median (95% CI), months 9.5 (5.6-NE) PFS, median (95% CI), months 2.3 (1.9-3.6) OS, median (95\nSigfox and Cube Infrastructure Managers Announce Major Partnership in IoT Infrastructure18.9.2020 09:30:00 CEST | Press release\nSigfox, the global 0G network1 and cloud provider for industrial data, is proud to announce a new strategic alliance with Cube Infrastructure Managers (Cube), through the sale of its German 0G network to Cube. Sigfox has grown its 0G IoT services by rolling out 0G networks across 72 countries and regions, which was largely achieved with partners called Sigfox Operators. These operators are the owners of the 0G networks, which they operate as exclusive connectivity providers of Sigfox IoT services, offering worldwide connectivity to customers. The sale of the German network to Cube will allow Sigfox to finance its continued innovation efforts in data value extraction and improvements in cloud algorithms to reduce energy consumption and allow the implementation of even more cost-effective devices and sensors. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20200918005116/en/ Cube Infrastructure Managers, the European infrastructu\nSensorion successfully raises approximately €31 (US$ 36.5) million in an oversubscribed private placement to US and European investors18.9.2020 08:00:00 CEST | Press release\nRegulatory News: Not for release, publication or distribution, directly or indirectly, in or into the United States, Canada, Australia or Japan. This press release is not intended as an offer and is for informational purpose only Sensorion (Paris:ALSEN) (FR0012596468 – ALSEN – the “Company”) a pioneering clinical-stage biotechnology company which specializes in the development of novel therapies to restore, treat and prevent within the field of hearing loss disorders announces today the success of its previously announced capital increase. The Company has placed 18,236,000 new ordinary shares with a nominal value of €0.10 each (the “New Shares”), for total gross proceeds of approximately € 31 million by means of an accelerated bookbuild offering to the benefit of categories of persons (the “Reserved Offering”). The issue price of the New Shares is €1.70 per share, representing a 3.5% discount to the weighted average share price on the day preceding the date on which the issuance price\nESMO 2020: Phase II CLARINET FORTE Results Show Increasing Dose Frequencies of Somatuline ® Autogel ® (lanreotide) Allows Patients with NETs to Delay Treatment Escalation by up to 8.3 Months18.9.2020 07:00:00 CEST | Press release\nRegulatory News: Ipsen (Euronext: IPN; ADR: IPSEY) today announced the release of first efficacy and safety data from the CLARINET FORTE study, with the abstract to be presented as a mini-oral presentation at the 2020 European Society for Medical Oncology (ESMO) Congress, taking place virtually from 19-21 September 2020. The prospective single-arm, open-label, exploratory, international Phase II study investigated the efficacy and safety of increasing the dose frequency of Somatuline® Autogel®(lanreotide) in patients with pancreatic or midgut NETs with centrally-assessed progression within the last two years while on a standard lanreotide regimen for ≥24 weeks. An extension of progression-free survival (PFS) rates and encouraging disease-control rates (DCR) were recorded in both tumor types, with no new safety signals. “These results support a clinically meaningful benefit to a population of patients with high unmet medical need by potentially delaying escalation to more toxic treatmen\nIn our pressroom you can read all our latest releases, find our press contacts, images, documents and other relevant information about us.Visit our pressroom"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:9193309b-1009-462f-81c2-c8b0f576fae7>","<urn:uuid:1fe68642-ace0-4adc-a2dd-bfcfc4bad8ea>"],"error":null}
{"question":"How do Chinese and Persian New Year celebrations compare in terms of house preparation rituals?","answer":"Both cultures have specific house preparation rituals for their New Year celebrations. In Persian culture, Spring cleaning is performed before Nowruz (Persian New Year), along with purchasing new clothes and placing the Haft Seen. In Chinese culture, families clean their houses on New Year's Eve by sweeping floors, washing clothes, cleaning spiders' webs, and dredging ditches. However, while Persians continue cleaning through the celebration, Chinese people avoid cleaning on New Year's Day itself as they believe it could sweep away good fortune. Chinese families also decorate their houses with Spring Couplets, Door-God pictures, New Year's paintings, paper-cuts, and lanterns to drive away evil spirits and bring good luck.","context":["My intention with this post is to share an aspect of my Persian culture in order to inspire you to seek out further knowledge either by asking me directly or using good old Google. I'm tremendously proud of my Persian heritage, please don't hesitate to reach out and ask questions!\nNowruz (The Persian New Year) is a celebration that survived the Muslim conquest in 650 CE of the area known today as Iran.\nIt is celebrated at the vernal (Spring) equinox (the exact moment the sun crosses the celestial equator equalizing night and day). In our year 2018 (Year 1397 in Iran) the Spring equinox occurred at 11:15am CST. This is also known as the first day of Spring.\nHowever, Persian New Year celebrations begin before the first day of Spring. Spring cleaning, purchasing new clothes, placing the Haft Seen, and Chaharshanbe soori (The Persian Festival of Fire) are some examples.\nCartoons of Chaharshanbe soori seem to be slim on the pickings. I need to go into illustration. Here is a picture of kids around a fire until my big illustrative break.\nChaharshanbe soori (Persian Festival of Fire) is celebrated on the evening of the last Wednesday before Nowruz. It involves celebrators jumping over a bonfire (symbol of purification) while singing: \"Sorkhi-ye to az man, Zardi-ye man az to\" meaning \"Your fiery red color to me, and my sickly yellow paleness to you.\" This is an ancient purification ritual symbolizing the jumper giving their sickness, paleness and problems to the fire and the fire in return giving you its energy, warmth and vigor.\nSophreh Haft Seen (Table of Seven S's)\nOne of my favorite elements of Nowruz is the setting of the Haft Seen (pictured above). A Haft Seen is a deeply symbolic table placement. Seven items symbolic of renewal and Spring are placed on a decorative tablecloth. Seven represents a lucky number. Also on the Haft Seen table are non-S items that have representations and meaning behind them.\nSome examples of the Seven S's are:\n1. Sabzeh (greens/sprouts): symbolic for rebirth. This is often placed at the center of the Haft Seen and seen as the most important element.\n2. Seeb (apple): symbolizing beauty and health\n3. Serkeh (vinegar): symbolizing age and patience\n4. Sekeh (coins, preferably gold): symbolizing wealth and prosperity\n5. Sonbol (hyacinth flower): symbolizing the coming of Spring\n6. Shirin (sweets): symbolizing spreading the sweetness for the year to come\n7. Sir (Garlic): symbolizing good health\nNon-S items on the table have cultural or historical significance. These items \"complete\" the Haft Seen table and can be seen placed around the Seven S's.\nExamples of these are:\n1. Sham (Candles): Technically an S, but I always see this in addition to the Seven S's. One for each child representing enlightenment and happiness.\n2. Ayyeneh (Mirror): For self reflection and introspection\n3. Tokhmeh Morgh (Decorated Eggs): One for each family member to represent fertility\n4. Mahi (A bowl with a goldfish): Representing life and the sign Pisces which the sun is leaving.\n5. A crystal bowl of water with an orange in it: Representing the earth floating in space\n6. Rose water: Placed on the table to represent its magical cleansing powers\n7. A Holy Book or Book of Poetry: We are non-denominational Christian so this book would be the Bible in our family.\nYou'll also notice a Haft Seen is symmetrical. Symmetry is incredibly important as it symbolizes harmony. I find the Haft Seen to be my favorite element of Nowruz for its elegance, intricate designs and deep symbolic nature. I believe this to be the heart of Nowruz with the making of the Sophreh Haft Seen surrounded in a beautiful poetry.\nNext year on the first day of Spring, what will your Haft Seen look like?\nEide Shoma Mobarak! (Happy New Year)","Top Chinese New Year's Eve Traditions\nChinese New Year's eve is the big day for ringing out the old and ringing in the new. With the development of the economy, some traditional customs have gradually disappeared, some new ways of celebration appeared. Here are the top 8 traditions of Chinese New Year's eve.\n1. Having a big dinner with Family\nChinese New Year's eve is the reunion day for every Chinese family. With a 7-day public holiday, all Chinese people would try their best to go back home no matter how far they live.\nAll family members will gather together, enjoying a grand family reunion dinner, chatting, singing, and laughing to celebrate this special day.\n2. Cleaning Houses\nChinese families will clean their houses: sweeping the floor, washing clothes, cleaning spiders' webs, and dredging ditches on Chinese New Year's Eve.\nIt is traditionally believed that dust represents \"old\" things, so cleaning houses means doing away with the \"old\" and preparing for the \"new\"; with the intention of sweeping all the rotten luck from the previous year out the door.\nInterestingly, one of the taboos of Chinese New Year is avoiding cleaning houses on Chinese New Year Day. People believe that if they do sweep or dust on New Year's Day, their good fortune will be swept away.\n3. Decorating House\nApart from cleaning houses, Chinese people will also decorate their houses on Chinese New Year's eve.\nPeople believe that auspicious decorations such as Spring Couplets, Door-God pictures, New Year's paintings, paper-cuts, and lanterns can drive away evil spirits and bring good luck.\n4. Worshiping Ancestors\nWorshiping ancestors is an old custom dating back thousands of years. People believe that the deceased family members have a continued existence in heaven, their spirits will look after the family and have the ability to influence the fortune of the living. So people offer sacrifices to ancestors at traditional festivals and hope the deceased ancestors would bless the whole family.\nOn Chinese New Year's eve, people will offer various dishes, fruits, and burn incense to worship ancestors.\n5. Watching CCTV New Year's Gala\nCCTV's New Year Gala, also known as the Spring Festival Gala, is called Chunwan ( chūn wǎn) in Chinese. The TV show is broadcast annually on Chinese New Year's eve at 8:00 PM.\nSitting together and watching the Spring Festival Gala has become a popular way to celebrate Chinese New Year's eve since the 1980s.\nThe show includes many different kinds of performances such as singing, dancing, comic dialogue, sketch comedy, magic, and acrobatics.\n6. Setting off Fireworks\nAncient Chinese people believed that fire could dispel bad luck, sparks could bring good luck, loud noise could scare away evil spirits and smoke made Yang energy (a kind of positive life-energy) rise. Fireworks produce such effects as fire, sparks, sound, and smoke when they are set off. It naturally combines with people's good wishes and becomes an ideal product for celebrations.\nIn ancient times, people would set off fireworks\nat 12:00 PM on Chinese New Year's Eve to celebrate the coming of the new year. Nowadays, many cities across China have imposed bans or restrictions on the use of fireworks for the sake of safety and environmental protection.\n7. Giving Red Envelopes\nThe red envelope or red packet is used as a monetary gift from the older generation to the younger generation during festivals or special occasions in China. Through giving red envelopes, parents send their good wishes to the children.\nCurrently, giving digital red envelopes via Wechat or Alipay app becomes a trend among young people.\n8. Staying up Late\nStaying up late or all night on Chinese New Year's Eve. After the grand reunion dinner, all family members will sit together, chatting, playing cards or mahjong, watch the gala to welcome the arrival of the New Year.\nPeople believe that staying up late can delay the aging process of the elder family member and prolong their life. The longer children stay awake, the longer their parents will live. You will find many families will keep their light on all night on Chinese New Year's eve.\nTop Traditional Dishes to Eat on New Year's Eve\nThe family reunion dinner held on the evening of Chinese New Year's eve is one of the major events of the Spring Festival. In ancient times, it was the biggest meal of the whole year. People could eat a lot of food that they wouldn't normally eat.\nAlthough there are many choices for food in modern times, traditional Chinese New Year's Eve dinner is also a very important meal for every Chinese family. Every dish has a different meaning.\n1. Fish: Always Have More Than You Wish For\nThe Chinese word for \"fish (鱼 yú)\" has the same pronunciation as the word for \"surplus (余yú)\". Chinese people always like to have a surplus at the end of the year, people think that saving things helps to make more in the next year. Eating fish stands for \"always have more than you wish for\".\nWhat's interesting is that people will not finish all the fish as leaving some on the table signifies having everything in abundance for the coming year.\n2. Chicken: Good Luck!\nThe Chinese word \"Chicken (鸡 jī)\" sounds like the word \"吉jí\", which means good luck and auspice in Chinese culture. In southern China, people will serve a whole chicken on the dinner table to show happiness and reunion.\n3. Nian Gao: Wish You a Prosperous Year\nChinese New Year Cake in South China\nNian gao (年糕 nián gāo), also known as \"rice cake\" or \"Chinese New Year cake\", is a traditional food made from glutinous rice flour. People believed nian gao carries auspicious meaning.\nFor old people, nian gao expresses the wish for longevity. For young people, it expresses the wish for\npromotion and high income. For kids, it expresses the wish to\n4. Dumplings: Change from the Old and to the New\nDumplings, in Chinese, sound like the Chinese expressions of changing from the old to the new.\nThe shape of a Chinese dumpling is similar to a gold ingot, people believe that eating dumplings on Chinese New Year's Eve means \"ushering in wealth and prosperity\".\nWhat's interesting is that people who eat the dumplings occasionally stuffed with special materials like a coin, a candy, a peanut, or a red date, will have the best luck in the new year.\nClick Chinese New Year Food to find out more lucky food eaten during Chinese New Year."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:e1928b1b-1a67-40d2-8a35-e001ce4ec4de>","<urn:uuid:4b0fac7f-102a-48ee-b41e-e95518ebc889>"],"error":null}
{"question":"As a linguist studying language evolution theories, I'm curious about the contrasting views between the Integration Hypothesis and Jackendoff's approach to language development. How do their perspectives on the combination of linguistic elements differ?","answer":"The Integration Hypothesis and Jackendoff's approach differ in their views on how linguistic elements combine. The Integration Hypothesis proposes that human language arose from integrating two pre-adapted systems: the Lexical system (with isolated symbols that don't combine) and the Expressive system (with finite state patterns). It predicts that lexical elements cannot combine directly - they must be mediated by expressive elements (L-E-L structure). In contrast, Jackendoff argues for a multi-generative approach where syntax, semantics, and phonology are all generative components connected via interface rules, rejecting a syntax-centered view. He contends that neither syntax alone should determine semantics, nor vice-versa, but they interface to produce proper phonological output.","context":["Thanks for taking up our paper in your blog (Nóbrega and Miyagawa, 2015, Frontiers in Psychology). We are glad that you appreciate our arguments against the gradualist approach to language evolution. There are two things that don't come out in your blog that we want to note.\nFirst, our arguments against the gradualist view are predicted by the Integration Hypothesis, which Miyagawa proposed with colleagues in earlier Frontiers articles (Miyagawa et al. 2013, 2014). The gradualists such as Progovac and Jackendoff claim that compounds such as doghouse and daredevil are living fossils of an earlier stage in language, which they call protolanguage. The reason is that the two \"words\" are combined without structure, due to the fact that these compounds (i) have varied semantic interpretations (NN compounds), and (ii) are unproductive and not recursive (VN compounds). We argued that if one looks beyond these few examples, we find plenty of similar compounds that are fully productive and recursive, such as those in Romance and Bantu. These productive forms show that the members that make up the compound are not bare roots, but are \"words\" in the sense that they are associated with grammatical features of category and sometimes even case.\nThis is precisely what the Integration Hypothesis (IH) predicts. IH proposes that the structure found in modern language arose from the integration of two pre-adapted systems. One is the Lexical system, found in monkeys, for example. The defining characteristic of the L-system is that it is composed of isolated symbols, verbal or gestural, that have some reference in the real world. The symbols do not combine. The other is the Expressive system found in birdsong. The E-system is a series of well-defined, finite state song patterns, each song without specific meaning. For instance, the nightingale may sing up to 200 different songs to express a limited range of intentions such as the desire to mate. The E-system is akin to human language grammatical features. These are the two major systems found in nature that underlie communication. IH proposes that these two systems integrated uniquely in humans to give rise to human language.\nBased on the nature of these two systems, IH predicts that the members of the L-system do not combine directly, since that is a defining characteristic of the L-system. E must mediate any such combination. This is why the IH predicts that there can't be compounds of the form L-L, but instead, IH predicts L-E-L. Such an assumption bears a close relation to how human language roots are ontologically defined, as feature-less syntactic objects. Once roots are feature-less they are invisible to the generative system, thus there is no motivation a priori to assume that syntax merges two bare roots, that is, two syntactically invisible objects.\nThe second point is that the L-system is related to such verbal behavior as the alarm calls of Vervet monkeys. We focus on the fact that these calls are isolated symbols, each with reference to something in the real world (thus, they are closer to concepts rather than to full-blown propositions). You question the correlation by noting that while the elements in a monkey's alarm calls appear purely to be referential, words in human language are more complex, a point also Chomsky makes. We also accept this difference, but separate from this, roots and alarm calls share the property, if we are right, that they are isolated elements that do not directly combine. This is the property we key in on in drawing a correlation between roots and alarm calls as belonging to the L-system. In addition to the referential aspect of alarm calls, there is another important question to solve: what paved the way to the emergence of the open-vocabulary stored in our long-term memory, since alarm calls are very restricted? Perhaps what you’ve mentioned as “something ‘special’ about lexicalization”, that is, the effect that Merge had on the pre-existing L-system, may have played a role in the characterization of human language roots, allowing the proliferation of a great number of roots in modern language. Nevertheless, we will only get a satisfactory answer to this question when we have a better understanding of the nature of human language roots.\nFinally, you might be interested to know that Nature just put up a program on primate communication and human language on Nature Podcast in which Chomsky and Miyagawa are the linguists interviewed.\nAlso, BBC will be broadcasting a Radio 4 program on May 11 (GBST) about evolution of language that will in part take up the Integration Hypothesis (or so, Miyagawa was told).\nMiyagawa, S., Berwick, R. C., and Okanoya, K. (2013). The emergence of hierarchical structure in human language. Front. Psychol. 4:71. doi: 10.3389/fpsyg.2013.00071\nMiyagawa, S., Ojima, S., Berwick, R. C., and Okanoya, K. (2014). The integration hypothesis of human language evolution and the nature of contemporary languages. Front. Psychol. 5:564. doi: 10.3389/fpsyg.2014.00564\nApril 29, 2015","|Born||January 23, 1945|\n|Fields||Generative grammar, Cognitive science, Music cognition|\n|Alma mater||MIT, Swarthmore|\n|Doctoral advisor||Noam Chomsky|\n|Notable students||Neil Cohn|\n|Notable awards||Fellow of the AAAS\nJean Nicod Prize\nRumelhart Prize (2014)\nRay Jackendoff (born January 23, 1945) is an American linguist. He is professor of philosophy, Seth Merrin Chair in the Humanities and, with Daniel Dennett, Co-director of the Center for Cognitive Studies at Tufts University. He has always straddled the boundary between generative linguistics and cognitive linguistics, committed as he is both to the existence of an innate Universal Grammar (an important thesis of generative linguistics) and to giving an account of language that meshes well with the current understanding of the human mind and cognition (the main purpose of cognitive linguistics).\nJackendoff's research deals with the semantics of natural language, its bearing on the formal structure of cognition and its lexical and syntactic expression. He has also done extensive research on the relationship between conscious awareness and the computational theory of mind, on syntactic theory, and, with Fred Lerdahl, on musical cognition, culminating in their Generative theory of tonal music. His theory of conceptual semantics developed into a comprehensive theory on the foundations of language, which indeed is the title of a recent monograph (2002): Foundations of Language. Brain, Meaning, Grammar, Evolution. Much earlier, in his 1983 Semantics and Cognition, he was one of the first linguists to integrate the vision faculty into his account of meaning and human language.\nJackendoff studied under the famed linguists Noam Chomsky and Morris Halle at the Massachusetts Institute of Technology, where he received his PhD in linguistics in 1969. Both Chomsky and Halle are now Institute Professors emeriti at MIT.\nBefore moving to Tufts in 2005, Jackendoff was professor of linguistics and Chair of the Linguistics Program at Brandeis University from 1971 to 2005. During the 2009 spring semester, he was an external professor at the Santa Fe Institute.\nInterfaces and generative grammar\nJackendoff argues against a syntax-centered view of generative grammar (called syntactocentrism by him), at variance with earlier models such as Standard Theory (1968); Extended Standard Theory (1972); Revised Extended Standard Theory (1975); Government and binding theory (1981); Minimalist program (1993), in which syntax is the sole generative component in the language. Jackendoff takes syntax, semantics and phonology all to be generative, connected amongst each other via interface components. Thus, the task of his theory is to formalize the proper interface rules.\nWhile rejecting mainstream generative grammar due to its syntactocentrism, the cognitive semantics school has offered an insight that Jackendoff would sympathize with, namely, that meaning is a separate combinatorial system not entirely dependent upon syntax. Unlike many of the cognitive semantics approaches, he contends that neither syntax alone should determine semantics, nor vice-versa. Syntax need only interface with semantics to the degree necessary to produce properly ordered phonological output (see Jackendoff 1996, 2002; Culicover & Jackendoff 2005).\nContribution to musical cognition\nJackendoff, together with Fred Lerdahl, has been interested in the human capacity for music and its relationship to the human capacity for language. In particular, music has structure as well as grammar (a means by which sounds are combined into structures). When a listener hears music in an idiom he or she is familiar with, the music is not merely heard as a stream of sounds; rather, the listener constructs an unconscious understanding of the music and is able to understand pieces of music never heard previously. Jackendoff is interested in what cognitive structures or \"mental representations\" this understanding consists of in the listener's mind, how a listener comes to acquire the musical grammar necessary to understand a particular musical idiom, what innate resources in the human mind make this acquisition possible and, finally, what parts of the human music capacity are governed by general cognitive functions and what parts result from specialized functions geared specifically for music (Jackendoff & Lerdahl, 1983; Lerdahl, 2001). Similar questions have also been raised regarding human language, although there are differences. For instance, it is more likely that humans evolved a specialized language module than having evolved one for music, since even the specialized aspects of music comprehension are tied to more general cognitive functions \n- Jackendoff, Ray (1972). Semantic Interpretation in Generative Grammar. Cambridge, MA: MIT Press. p. 400. ISBN 0-262-10013-4.\n- Jackendoff, Ray (1977). X-Bar Syntax: A Study of Phrase Structure. Cambridge, MA: MIT Press. p. 248. ISBN 0-262-10018-5.\n- Jackendoff, Ray (1983). Semantics and Cognition. Cambridge, MA: MIT Press. p. 283. ISBN 0-262-10027-4.\n- Lerdahl, Fred & Ray Jackendoff (1983). A Generative Theory of Tonal Music. Cambridge, MA: MIT Press. p. 369. ISBN 0-262-12094-1.\n- Jackendoff, Ray (1987). Consciousness and the Computational Mind. Cambridge, MA: MIT Press. p. 356. ISBN 0-262-10037-1.\n- Jackendoff, Ray (1990). Semantic Structures. Cambridge, MA: MIT Press. p. 322. ISBN 0-262-10043-6.\n- Jackendoff, Ray (1992). Languages of the Mind: Essays on Mental Representation. Cambridge, MA: MIT Press. p. 200. ISBN 0-262-10047-9.\n- Jackendoff, Ray (1993). Patterns in the Mind: Language and Human Nature. New York, NY: Harvester Wheatsheaf. p. 243. ISBN 0-7450-0962-X.\n- Jackendoff, Ray (1997). The Architecture of the Language Faculty. Cambridge, MA: MIT Press. p. 262. ISBN 0-262-10059-2.\n- Jackendoff, Ray (2002). Foundations of Language: Brain, Meaning, Grammar, Evolution. Oxford: Oxford University Press. p. 477. ISBN 0-19-827012-7.\n- Culicover, Peter W. & Ray Jackendoff (2005). Simpler syntax. Oxford: Oxford University Press. p. 589. ISBN 0-19-927108-9.\n- Jackendoff, Ray (2007). Language, Consciousness, Culture: Essays on Mental Structure (Jean Nicod Lectures). Cambridge, MA: MIT Press. p. 403. ISBN 0-262-10119-X.\n- Jackendoff, Ray (2010). Meaning and the Lexicon: The Parallel Architecture 1975–2010. Oxford: Oxford University Press. p. 504. ISBN 0-19-956888-X.\n- Website at Tufts University\n- Center for Cognitive Studies at Tufts University\n- Ray Jackendoff, Conceptual Semantics, Harvard University, 13 November 2007 (video)\n- Semantics and Cognition, in Shalom Lappin (1996), \"The Handbook of Contemporary Semantic Theory\", 539-559. Oxford: Blackwell.\n- Possible stages in the evolution of the language capacity, Trends in Cognitive Sciences, Vol. 3, No. 7 (July 1999)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:2271030e-5faf-4c4a-9949-ca32e0b3561b>","<urn:uuid:ac84727d-cf3a-45a1-b1db-3f720eafd1b1>"],"error":null}
{"question":"What's the key difference between periodic crystal patterns and quasicrystals in terms of their structural symmetry?","answer":"While both periodic crystals and quasicrystals can tile a plane completely, they differ fundamentally in their symmetry properties. Periodic crystals have translational symmetry, meaning you can move a section of the pattern and find other sections that exactly match through simple translation or rotation. In contrast, quasicrystals lack this periodicity - you cannot find any section that will repeat perfectly no matter how far you move it, even though they exhibit long-range order. This makes quasicrystals unique in that they can fully tile a plane while never creating a regularly repeating pattern.","context":["Icosahedral quasicrystals (IQCs) are materials that exhibit long-range order but lack periodicity in any direction. Although IQCs were the first reported quasicrystals1, they have been experimentally observed only in metallic alloys2, not in other materials. By contrast, quasicrystals with other symmetries (particularly dodecagonal) have now been found in several soft-matter systems3,4,5. Here we introduce a class of IQCs built from model patchy colloids that could be realized experimentally using DNA origami particles. Our rational design strategy leads to systems that robustly assemble in simulations into a target IQC through directional bonding. This is illustrated for both body-centred and primitive IQCs, with the simplest systems involving just two particle types. The key design feature is the geometry of the interparticle interactions favouring the propagation of an icosahedral network of bonds, despite this leading to many particles not being fully bonded. As well as furnishing model systems in which to explore the fundamental physics of IQCs, our approach provides a potential route towards functional quasicrystalline materials.\nThis is a preview of subscription content\nSubscribe to Journal\nGet full journal access for 1 year\nonly $3.90 per issue\nAll prices are NET prices.\nVAT will be added later in the checkout.\nTax calculation will be finalised during checkout.\nRent or Buy article\nGet time limited or full article access on ReadCube.\nAll prices are NET prices.\nShechtman, D., Blech, I., Gratias, D. & Cahn, J. W. Metallic phase with long-range orientational order and no translational symmetry. Phys. Rev. Lett. 53, 1951–1953 (1984).\nTakakura, H., Gómez, C. P., Yamamoto, A., Boissieu, M. D. & Tsai, A. P. Atomic structure of the binary icosahedral Yb–Cd quasicrystal. Nat. Mater. 6, 58–63 (2007).\nZeng, X. et al. Supramolecular dendritic quasicrystals. Nature 428, 157–160 (2004).\nHaji-Akbari, A. et al. Disordered, quasicrystalline and crystalline phases of densely packed tetrahedra. Nature 462, 773–777 (2009).\nDotera, T. Quasicrystals in soft matter. Isr. J. Chem. 51, 1197–1205 (2011).\nDotera, T., Oshiro, T. & Ziherl, P. Mosaic two-length scale quasicrystals. Nature 506, 208–211 (2014).\nSavitz, S., Babadi, M. & Lifshitz, R. Multiple-scale structures: from Faraday waves to soft-matter quasicrystals. IUCrJ 5, 247–268 (2018).\nSubramanian, P., Archer, A. J., Knobloch, E. & Rucklidge, A. M. Three-dimensional icosahedral phase field quasicrystal. Phys. Rev. Lett. 117, 075501 (2016).\nEngel, M., Damasceno, P. F., Phillips, C. L. & Glotzer, S. C. Computational self-assembly of a one-component icosahedral quasicrystal. Nat. Mater. 14, 109–116 (2015).\nDamasceno, P. F., Glotzer, S. C. & Engel, M. Non-close-packed three-dimensional quasicrystals. J. Phys. Condens. Matter 29, 234005 (2017).\nReinhardt, A., Schreck, J. S., Romano, F. & Doye, J. P. K. Self-assembly of two-dimensional binary quasicrystals: a possible route to a DNA quasicrystal. J. Phys. Condens. Matter 29, 014006 (2017).\nSadoc, J.-F. & Mosseri, R. Geometric Frustration (Cambridge Univ. Press, 1999).\nDoye, J. P. K. & Wales, D. J. Structural consequences of the range of the interatomic potential: a menagerie of clusters. J. Chem. Soc. Faraday Trans. 93, 4233–4243 (1997).\nDmitrienko, V. & Kléman, M. Tetrahedral structures with icosahedral order and their relation to quasi-crystals. Crystallogr. Rep. 46, 527–533 (2001).\nLiu, L., Li, Z., Li, Y. & Mao, C. Rational design and self-assembly of two-dimensional dodecagonal quasicrystals. J. Am. Chem. Soc. 141, 4248–4251 (2019).\nTracey, D. F., Noya, E. G. & Doye, J. P. K. Programming patchy particles to form complex ordered structures. J. Chem. Phys. 151, 224506 (2019).\nJanot, C. Quasicrystals: A Primer (Oxford Univ. Press, 2002).\nCademartiri, L. & Bishop, K. J. M. Programmable self-assembly. Nat. Mater. 14, 2–9 (2015).\nLiu, W., Halverson, J., Tian, Y., Tkachenko, A. V. & Gang, O. Self-organized architectures from assorted DNA-framed nanoparticles. Nat. Chem. 8, 867–873 (2016).\nZhang, Z., Keys, A. S., Chen, T. & Glotzer, S. C. Self-assembly of patchy particles into diamond structures through molecular mimicry. Langmuir 21, 11547–11551 (2005).\nVega, C., Sanz, E., Abascal, J. L. F. & Noya, E. G. Determination of phase diagrams via computer simulation: methodology and applications to water, electrolytes and proteins. J. Phys. Condens. Matter 20, 153101 (2008).\nGlotzer, S. C. & Solomon, M. J. Anisotropy of building blocks and their assembly into complex structures. Nat. Mater. 6, 557–562 (2007).\nLi, W. et al. Colloidal molecules and patchy particles: complementary concepts, synthesis and self-assembly. Chem. Soc. Rev. 49, 1955–1976 (2020).\nXiong, Y. et al. Three-dimensional patterning of nanoparticles by molecular stamping. ACS Nano 14, 6823–6833 (2020).\nVeneziano, R. et al. Designer nanoscale DNA assemblies programmed from the top down. Science 352, 1534 (2016).\nTian, Y. et al. Ordered three-dimensional nanomaterials using DNA-prescribed and valence-controlled material voxels. Nat. Mater. 19, 789–796 (2020).\nMa, N. et al. Directional assembly of nanoparticles by DNA shapes: towards designed architectures and functionality. Top. Curr. Chem. 378, 36 (2020).\nMan, W., Megens, M., Steinhardt, P. J. & Chaikin, P. M. Experimental measurement of the photonic properties of icosahedral quasicrystals. Nature 436, 993–996 (2005).\nSteurer, W. Boron-based quasicrystals with sevenfold symmetry. Phil. Mag. 87, 2707–2712 (2007).\nSnodin, B. E. K. et al. Introducing improved structural properties and salt dependence into a coarse-grained model of DNA. J. Chem. Phys. 142, 234901 (2015).\nWilber, A. W., Doye, J. P. K., Louis, A. A. & Lewis, A. C. F. Monodisperse self-assembly in a model with protein-like interactions. J. Chem. Phys. 131, 175102 (2009).\nBen Zion, M. Y. et al. Self-assembled three-dimensional chiral colloidal architecture. Science 358, 633–636 (2017).\nAnderson, J. A., Jankowski, E., Grubb, T. L., Engel, M. & Glotzer, S. C. Massively parallel Monte Carlo for many-particle simulations on GPUs. J. Comput. Phys. 254, 27–38 (2013).\nRapaport, D. C. The Art of Molecular Dynamics Simulation (Cambridge Univ. Press, 2004).\nEngel, M., Umezaki, M., Trebin, H.-R. & Odagaki, T. Dynamics of particle flips in two-dimensional quasicrystals. Phys. Rev. B 82, 134206 (2010).\nRovigatti, L., Šulc, P., Reguly, I. Z. & Romano, F. A comparison between parallelization approaches in molecular dynamics simulations on GPUs. J. Comput. Chem. 36, 1–8 (2015).\nBai, X.-C., Martin, T. G., Scheres, S. H. W. & Dietz, H. Cryo-EM structure of a 3D DNA-origami object. Proc. Natl Acad. Sci. USA 109, 20012–20017 (2012).\nSnodin, B. E. K., Schreck, J. S., Romano, F., Louis, A. A. & Doye, J. P. K. Coarse-grained modelling of the structural properties of DNA origami. Nucleic Acids Res. 47, 1585–1597 (2019).\nJun, H. et al. Automated sequence design of 3D polyhedral wireframe DNA origami with honeycomb edges. ACS Nano 13, 2083–2093 (2019).\nDouglas, S. M. et al. Rapid prototyping of 3D DNA-origami shapes with caDNAno. Nucleic Acids Res. 37, 5001–5006 (2009).\nSuma, A. et al. TacoxDNA: a user-friendly web server for simulations of complex DNA structures, from single strands to origami. J. Comput. Chem. 40, 2586–2595 (2019).\nDietz, H., Douglas, S. M. & Shih, W. M. Folding DNA into twisted and curved nanoscale shapes. Science 325, 725–730 (2009).\nWe are grateful for financial support from the Agencia Estatal de Investigación and the Fondo Europeo de Desarrollo Regional (FEDER) under grant numbers FIS2015-72946-EXP(AEI) and FIS2017-89361-C3-2-P(AEI/FEDER,UE) (to E.G.N.) and the Croucher Foundation (to C.K.W.). We acknowledge the use of the University of Oxford Advanced Research Computing (ARC) facility and the Cambridge Service for Data Driven Discovery (CSD3).\nThe authors declare no competing interests.\nPeer review information Nature thanks Oleg Gang, Ron Lifshitz and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nExtended data figures and tables\nProjections of the BCI 5P (left) and 2P (right) IQCs along the five-fold axis, three-fold axis and two-fold axis (top to bottom). On the three-fold axis, self-similar triangular motifs follow a 1:τ:τ2:τ3 scaling that is a characteristic of IQCs.\nComparison of the diffraction patterns of the ideal body-centred IQC and its 3/2 approximant, with those of the assembled IQCs using the BCI 5P, 3P, 2P and 2P-mod models. All four assembled systems have diffraction patterns with clear five-fold symmetry confirming their IQC character. By contrast, the diffraction pattern of the approximant when viewed along the pseudo-five-fold axis shows clear deviations from fivefold symmetry. The diffraction patterns of the BCI 5P and 3P models most closely resemble those of the ideal IQC; the indexing of all peaks with a BCI indexing scheme is detailed in Supplementary Information section 4E. The diffraction patterns for the BCI 2P and 2P-mod models exhibit additional features, particularly along the two-fold and five-fold axes; these include both diffuse scattering and extra weak peaks that cannot be indexed in a BCI indexing scheme (see Supplementary Information).\na, Primitive IQC model with a dodecahedral occupation domain viewed along the five-, three- and two-fold rotational axes (left to right). In the top row, only red and those blue particles that are bonded to red particles are shown; these views clearly show that not all the clusters are perfect. In the bottom row, all particles are shown. The yellow particles form an additional shell around the icosahedral cluster depicted in Fig. 3a and all other matrix particles are coloured pink. b, Views of the 8/5 rational approximant along the psuedo-five-, three- and two-fold rotational axes (left to right).\nProjections of the primitive icosahedral 3P WT (left) and 2P WT (right) models along the five-fold axis, three-fold axis and two-fold axis (top to bottom). On the three-fold axis, self-similar triangular motifs follow a 1:τ:τ2:τ3 scaling that is a characteristic of IQCs.\nComparison of the diffraction patterns of the ideal primitive IQC and its 8/5 approximant with those of the IQCs obtained from simulations. All four assembled systems have diffraction patterns with clear five-fold symmetry confirming their IQC character. By contrast, the diffraction pattern of the approximant when viewed along the pseudo-five-fold axis shows clear deviations from five-fold symmetry. Despite the assembled systems being simpler than the ideal IQC in terms of the number of environments, the diffraction patterns are very similar to that for the ideal IQC with the patterns along the two-fold axis showing the most differences. The primitive icosahedral 2P diffraction patterns could be satisfactorily indexed with a primitive icosahedral indexing scheme, but the better-resolved primitive icosahedral 3P patterns have additional weak peaks that require a face-centred icosahedral indexing scheme with a hypercubic lattice parameter that is double that of the primitive icosahedral indexing scheme, indicating some superstructural ordering (see Supplementary Information section 4E). The diffraction patterns for the systems with and without torsions show no clear differences, confirming their structural similarity.\nThe properties of the body-centred IQCs obtained with the 5P, 3P and 2P models are compared with those of the ideal body-centred IQC and its 3/2 rational approximant. a, PDFs considering only those particles within a sphere of radius 15σLJ centred at the centre of mass of the solid cluster. There is good coincidence between the broadened (due to thermal motion) peaks for the assembled IQCs and the distances present in the ideal IQC consistent with their similar structure. The PDFs for the BCI 5P and 3P systems are essentially the same, with the differences from the BCI 2P system still being relatively small. b, Radial density. The bulk densities of the ideal IQC, the BCI 5P and 3P systems and the 3/2 approximant are very similar, but the bulk density for the BCI 2P system is lower, as expected, due to the absence of the red particles. The radial density of the assembled systems decreases from its bulk value to zero over 10σLJ–15σLJ, reflecting the irregular nature of the surface of the assemblies. c, Mole fraction of each particle type in the different structures. The incorporation of the yellow and purple particles into the BCI 5P assembly is much lower than for the ideal IQC. The fraction of red and blue particles in the BCI 5P and 3P assemblies is also closer to that for the approximant than the ideal IQC. The BCI 2P assemblies have a substantially increased fraction of green particles. d–i, Probability of the number of dangling bonds for each particle type (d–h) and for all particle types (i). The assembled IQCs show a greater number of dangling bonds for the blue matrix particles than the ideal IQC, whereas the ideal IQC has a greater number of dangling bonds for the red and green particles that form the triacontahedral clusters.\nThe properties of the primitive IQCs obtained with the 3P and 2P models with and without torsions are compared with those of the ideal primitive IQC. a, PDF considering only those particles within a sphere of radius 15σLJ centred at the centre of mass of the solid cluster. The peaks in the PDFs are consistent with the distances in the ideal IQC, even though the assembled systems do not have particles corresponding to many of the environments in the ideal IQC. The 2P system does not have the peak at ~0.8σLJ that is due to the red–red bonds. b, Radial density. The bulk densities of the 3P systems are a bit lower than the ideal IQC with the 2P systems having even lower densities due to the absence of red particles. c, Mole fraction of each particle type in the different structures. Note that in the ideal structure the mole fractions do not add to 1, as there are additional local environments that are not included in this plot. Particularly noticeable is the much smaller number of red particles in the ideal IQC. d–g, Probability of the number of dangling bonds for each particle type (d–f) and for all particle types (g). The blue particles in the 3P systems have a particularly low probability of having no missing bonds, because when forming part of the inter-cluster matrix, they do not use their two patches that can only bond to the red particles that form the dodecahedral clusters; such particles contribute to the relatively large number of blue particles with two dangling bonds.\na–d, Contributions to the BOOD for the BCI 5P (a), BCI 2P (b), primitive icosahedral 3P WT (c) and primitive icosahedral 2P WT (d) quasicrystals from the different patch–patch bonds types in these systems. For the BCI systems, the green–green and purple–purple bonds are directed along the two-fold axes, the blue–green and blue–purple along the three-fold axes, the blue–red, red–green, blue–yellow and yellow–purple along the five-fold axes and the blue–blue can be directed along five-fold (bonds formed through patches 1 to 3) and three-fold (bonds formed by patch 4) axes. For the primitive icosahedral systems, the red–red bonds are directed along the two-fold axes, and the green–blue and red–blue along the three-fold axes. Thus, for the primitive icosahedral 2P WT system the total BOOD only exhibits spots directed along the three-fold axes, whereas for the primitive icosahedral 3P WT system there are also spots along the two-fold axes. Contributions to the BOODs calculated using a distance cutoff exhibit additional spots but always directed along the rotational axes of the Ih point group (Supplementary Fig. 7). e, f, The relationship between the symmetry of the patchy particles and the Ih point group. All particles can be oriented so that their patch vectors point exclusively along the rotational axes of Ih. This is illustrated for the patchy particles in the BCI 5P (e) and primitive icosahedral 3P (f) models by depicting the direction of the patch vectors Pi by circles (with the colours representing (one of) the particles with which they interact) on the surface of an icosahedron. The projections chosen are along the highest rotational axis of the particles. Edges on the back faces of the icosahedra are dashed and cyan. Similarly, for patch vectors on the back faces, the colour shade is lighter and ringed in cyan rather than black. The BOODs in a–d are fully consistent with the particles being oriented in this way with respect to the global icosahedral order of the IQCs. We also note that the preferred torsional angles of each patch–patch interaction are those that ensure the propagation of this global orientational order.\nVan Hove autocorrelation functions for the simulated body-centred IQCs, evaluated after one million Monte Carlo cycles and considering only those particles within a radius of 20σLJ, which corresponds to the interior of the cluster where the radial density is constant (Extended Data Fig. 6b). The majority of the particle mobility is associated with the blue matrix particles, whereas the red and green particles that form the inner shell of the icosahedral clusters have very limited mobility. The yellow particles in the 5P system are the most mobile but are only present at very low mole fraction.\nVan Hove autocorrelation function for the simulated primitive IQCs, evaluated after one million Monte Carlo cycles and considering only those particles within a radius of 20σLJ, which corresponds to the interior of the cluster where the radial density is constant (Extended Data Fig. 7b). Although the overall pattern shows clear five-fold symmetry, the peaks merge into each other more than for the BCI systems. This is partly because the size of the allowed hops is shorter. The green particles are the most mobile, and the red particles, which form the inner shell of the icosahedral clusters, are least mobile. It is also noticeable that without torsions, the 3P system has greater mobility, and that the 2P patterns are less well defined, with the motion of the blue particles close to isotropic.\nAbout this article\nCite this article\nNoya, E.G., Wong, C.K., Llombart, P. et al. How to design an icosahedral quasicrystal through directional bonding. Nature 596, 367–371 (2021). https://doi.org/10.1038/s41586-021-03700-2","Infinite patterns hidden in plain sight in medieval Islamic architecture.\nPatterns of repeating geometric shapes are everywhere. Take a quick look around and see if there’s any you can spot - maybe the regular pattern of square ceiling tiles in your classroom, decorative hexagons tiling your kitchen floor, or even a checkerboard pattern decorating your clothes.\nWhen regular geometric shapes fit together and cover a surface, mathematicians call that a tessellation or a tiling. These tilings are everywhere - from the human scale, like the hexagons inside a beehive, all the way down to the atomic scale, like the lattice of atoms that make up solid materials.\nPeriodic crystal tilings include things like the honeycomb lattice you find inside a beehive or the regular stacks of atoms that make up solid materials\nThe Art of Tiling\nHumans have been using tilings as decoration for thousands of years. Ancient cultures from India to Mesopotamia to the Mediterranean used tiles to create beautiful, complex designs that nonetheless have a simple, repeating pattern to them. When geometric patterns are made from a set number of basic shapes that repeat over and over, they create what’s called a periodic tiling. Periodic tilings have a base unit cell that is moved around and rotated to fill in the space of the plane.\nGeometric patterns made from tiles are especially important in Islamic architecture, which avoids using images of people in decoration. Instead, many structures built in the ancient Islamic world were decorated with elaborate geometric designs of repeating squares, circles, stars, or other regular polygons. These decorative patterns are complicated, but they are also highly symmetrical. If you take a section of the pattern, for example, you can translate it (move it up and down or left and right) or rotate it (spin it around) and find other sections that exactly match.\nThis is an example of a periodic tile pattern. What are the most basic shapes you need to make this pattern? That’s called the unit cell of the crystal. By moving and rotating the unit cell, you can create the repeating pattern of the crystal.\nHere are some examples of geometric patterns from an Islamic palace in Spain called the Alhambra. Can you find any repeating elements? What are the base shapes that make up the pattern?\nQuasicrystals: An Impossible Pattern?\nHumans have been making repeating patterns out of shapes for millennia. But what about a pattern that doesn’t repeat? Is there any way to take some number of shapes and fit them together without a regular, periodic pattern, even if you continue laying down the tiles forever? For a long time, mathematicians thought that this was impossible. It wasn’t until the 1970s and 1980s that modern scientists started seeing evidence for these weird kinds of crystal patterns, called quasiperiodic crystals, or quasicrystals for short. But there’s evidence that humans might have figured out how to create these mysterious, infinite, non-repeating patterns hundreds and hundreds of years ago. This evidence was hidden in plain sight in the geometric tiles of an ancient Islamic building!\nBut wait, what even is a quasicrystal? For something to be a crystal, it has to fully tile the plane, which means that when you lay down the pieces of your crystal edge-to-edge, it can continue out to infinity with no gaps in between. Quasicrystals can tile the plane, but they aren’t periodic the way that normal crystals are. This means that they don’t have the same translational symmetries that we talked about earlier. In a quasicrystal, you can’t draw a box around a section, move that section over, and continue the pattern. There’s always a mismatch, even if you have a really huge box. You can look out to infinity to try to find a box big enough to repeat the pattern, but you’ll never find it!\nIt doesn’t seem like this kind of pattern should be able to exist, but there are actually ways to make a quasicrystal with just a few simple rules. One example is with something called Penrose tiles. With just two simple shapes, called a “kite” and a “dart”, along with some simple rules about how you can connect them, you can create your own quasicrystal pattern (if you’re curious, check out our activities below to learn more about kites and darts and see what you can make).\nCan you find a section that matches just by sliding that section left and right? What about by spinning the section around?\nEven though the Penrose pattern above wasn’t developed until 1974, we’ve recently found evidence of these kinds of patterns in medieval architecture across Western and Central Asia. Researchers, including physicists and architects, noticed that some ancient buildings, built around 1200 CE, had patterns on them that looked different than the ordered geometric tilings that are typical of the decoration style. On closer inspection, we realized that these ancient tiles were actually patterned the same way as quasicrystals! In some regions, the patterns were almost perfect Penrose tilings, built more than 500 years before Roger Penrose was even born. The few mistakes that do exist in the pattern could easily be fixed by flipping one of the tiles. So scientists think that those flaws are just simple mistakes made by the person arranging the tiles, not mathematical mistakes made by the designers.\nThis pattern is found in the Darb-i Imam shrine in Iran, and it was made in 1453. The pattern, recreated from the base shapes (each colored differently), doesn’t have translational symmetry and never repeats, even if you continue it forever.\nWhile these patterns don’t go on forever (or else we’d need an infinite number of tiles and an infinitely long wall!), you could continue the patterns out to infinity using just a few tiles decorated with connected lines, just like the design on the Penrose tiles. These tiles are called girih tiles. They allowed ancient tile artists to make increasingly complicated patterns just by alternating between the tiles they have, like laying down pieces of a puzzle. This process of tessellation is much easier than drawing all the lines with a straightedge and a compass. The final product of zigzagging lines and overlapping geometric shapes is almost dizzying to look at, but there are patterns you can pick up if you look long enough!\nThe quasicrystal patterns in the Darb-i Imam shrine look very complicated, but are made from just a few repeating tiles.\nHidden in Plain Sight\nFor centuries, mathematicians and scientists thought that there could be no such thing as a regular pattern that doesn’t repeat. But once Western mathematicians theorized that quasicrystals could exist, evidence of them started turning up everywhere. In rare minerals, in some aluminum alloys, and even in fused sand melted from a nuclear explosion, scientists found nonperiodic crystal structures. The closer that scientists looked, the more examples of quasicrystals they found. But it turns out that humans had already figured out how to make these once-impossible patterns, 500 years ago in Central Asia. We’re just now starting to appreciate the complexity of these patterns in medieval Islamic buildings. What other great discovery could be hidden in plain sight like these ancient quasicrystals? Who knows what you might find if you start looking closely!\nPhotos courtesy of Wikimedia Commons, \"Decagonal and Quasi-Crystalline Tilings in Medieval Islamic Architecture\" by P.J. Lu and P.J. Steinhardt, and AlhambraDeGranada.org\nWritten by Caroline Martin\nEdited by Madelyn Leembruggen\nIllustrations by Taylor Contreras\nSources and additional readings:\nAncient Islamic architects created perfect quasicrystals by Physics World\nIslamic Artisans Constructed Exotic Nonrepeating Pattern 500 Years Before Mathematicians by Scientific American\nIn Medieval Architecture, Signs of Advanced Math by the New York Times\nQuasicrystalline Medieval Islamic Architectural Tilings by Peter Lu\nMake your own tessellations and explore math in ancient cultures!\nDefine (15 minutes): Identify lines of symmetry. Why are lines of symmetry important for creating periodic tilings?\nPlay (60-90 minutes): Create a periodic tessellation inspired by M.C. Escher's designs.\nInteract (30-45 minutes): Assemble a digital Penrose tiling using kites and darts.\nExplore (30-60 minutes): Gather examples of geometry from cultures across the globe."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:90775cb7-4842-4300-b92e-62444e2a77e3>","<urn:uuid:477b9a00-debc-4419-9d34-fbcdec12c06b>"],"error":null}
{"question":"How does the Loop platform work to address plastic waste, and what market factors are driving the need for such solutions?","answer":"Loop is a circular e-commerce platform that allows consumers to purchase products in durable, refillable packaging. Consumers shop for products on the Loop website, receive items in a specialized shipping tote, and once finished, place empty packages in Loop totes for pickup. Loop then cleans the packaging using custom technologies and refills products as needed. The platform also collects used products like diapers, razors, and brush parts for recycling or reuse. This solution has emerged in response to market factors including growing environmental concerns about plastic waste, increasing regulatory measures against single-use plastics, and shifting consumer preferences toward sustainable packaging. The fast-paced lifestyle and e-commerce boom have traditionally driven demand for single-use packaging, but growing awareness of environmental impacts has created a need for circular economy initiatives and sustainable alternatives.","context":["The Procter & Gamble Company today announced the introduction of reusable, refillable packaging on some of its most popular products as part of a new effort that aims to change the world’s reliance on single-use packaging and disposable waste. Additionally, new “collect and recycle” circular solutions that help eliminate waste were also introduced as part of a partnership with Loop™, a circular e-commerce platform developed by international recycling leader TerraCycle. Many of P&G’s largest global brands, including Pantene, Tide, Cascade, and Oral-B will participate in this innovative platform later this year.\nLoop is a first of its kind global packaging and shopping circular solution which aims to improve the environmental performance and convenience standards compared to current e-commerce solutions through packaging that is collected, cleaned, refilled, and reused. Loop also offers the option to collect used products from consumers’ doorsteps for further recycling or reuse. P&G was the first consumer products company to join Loop.\nAnnounced at the World Economic Forum in Davos, Switzerland, this partnership further advances P&G’s Ambition 2030 sustainability goals and the company’s ongoing commitment to turning sustainable intentions into positive everyday actions.\nP&G scientists and engineers have developed innovative manufacturing, packaging, and distribution solutions that it says will delight consumers and make sustainable living easy for participating consumers. Eleven P&G brands will be available in Loop in one of three formats. The Loop system will be validated and optimized through real in-market learning experiments beginning in mid-2019 in New York and Paris.\nHousehold favorites such as Pantene, Tide, Cascade, and Crest have designed new durable and refillable packaging, featuring luxurious containers with new features and functionalities.\n- Pantene is introducing a unique bottle made with lightweight, durable aluminum for its shampoo and conditioner.\n- Tide is participating in Loop with its Tide purclean plant-based laundry detergent in a new durable bottle made from stainless steel with a simple twist-cap and easy-pour spout.\n- Cascade, continually looking for ways to make the dish cleaning experience better and environmentally friendly, has developed a new ultra-durable packaging for Cascade ActionPacs, which enable consumers to skip the prewash.\n- Crest is introducing new Crest Platinum mouthwash, a unique formula that delivers fresh breath and stain prevention in a sustainable, refillable glass bottle.\n- Ariel and Febreze are participating with durable, refillable packaging that is also available in stores, testing a new direct-to-consumer refill-and-reuse model.\nRecycling products refills\n- Oral-B will test circular solutions for both its electric rechargeable and manual toothbrushes. Oral-B CLIC, a new design for manual toothbrushes features a durable handle equipped with a unique mechanism that allows consumers to only exchange the brush head. The Loop platform will recycle used brush heads for both manual and electrical brushes.\n- Gillette and Venus will provide premium travel packs as durable packaging that the consumer will keep in addition to the handle. The used parts and blades from these grooming products will be collected from consumers for further recycling by TerraCycle.\nRecycling used hygienic products\nPampers and Always will test the collection of used hygiene products from consumer homes for further recycling using ground-breaking, proprietary technology developed by Fater, a P&G and Angelini Group Joint Venture. The technology turns used absorbent hygiene products into secondary raw materials for higher-value applications.\n“We are building on more than 180 years of innovation and world-class consumer insight to enable responsible consumption at scale,” says Virginie Helias, P&G’s Vice President and Chief Sustainability Officer. “We’re proud to partner with TerraCycle as the first CPG company to be part of this transformative program, which is just one of the many ways we are delivering on our Ambition 2030 goals to accelerate sustainable innovation and drive circular solutions.\n“The time to act is now. We are passionate about harnessing the power of our global reach and the strength of our trusted global brands to scale-up more sustainable solutions. Transformative partnerships are key to achieve this mission as no one can succeed alone.\nSays TerraCycle CEO Tom Szaky, “We are happy to partner with P&G and other global brands, retailers, infrastructure companies, and the World Economic Forum to create a new way to more responsibly consume products. Loop aims to not just eliminate the idea of packaging waste but greatly improve the product experience and the convenience in how we shop. Through Loop, consumers will be able to responsibly consume products in specially-designed durable, reusable, or fully recyclable packaging. Through the power and reach of trusted brands such as those of P&G we will be able to change consumers’ habits and achieve the scale required for the model to achieve its objectives.”\nHow Loop works\n- Shop: Consumers will go to the Loop website or Loop partner retailer’s websites and shop for trusted brands now redesigned to be packaging waste-free.\n- Receive: Consumers receive their durable products in Loop’s exclusively designed state-of-the-art shipping tote that eliminates the need for single-use shipping materials like cardboard boxes.\n- Enjoy: Consumers experience elegance and convenience all while eliminating the idea of throw-away packaging waste.\n- Pick-up: There is no need to clean and dispose of the package; as consumers finish their products, they place the empty package into one of their Loop Totes. Loop will pick-up directly from their home.\n- Clean: Loop’s team of scientists has developed custom cleaning technologies so that each product may be safely reused.\n- Refill, recycle, or reuse: Loop promptly replenishes products as needed and returns the refilled shipping totes to the consumer. If there is recoverable used product such as diapers, pads, razors, or brush parts, they will be recovered to be reused or recycled.\nRead more about Loop here.","Single-use plastic packaging refers to the use of plastic materials for packaging products that are intended to be used once and then discarded. This type of packaging is widely used across various industries, including food and beverages, healthcare, personal care, and household products. Single-use plastic packaging offers convenience and affordability, but it has also raised concerns due to its negative environmental impact.\nSingle-use plastic packaging is any form of packaging made from plastic materials that are designed for one-time use. These packaging materials include plastic bottles, bags, containers, wraps, and other disposable items. They are commonly used for packaging products that require protection, preservation, or containment. Single-use plastic packaging is typically lightweight, cost-effective, and offers various functionalities such as sealing, flexibility, and transparency.\n|UNIT||Value (USD Million/Billion)|\nThe single-use plastic packaging market has witnessed significant growth in recent years, driven by factors such as urbanization, changing lifestyles, and the rising demand for convenience products. However, concerns over plastic waste management and environmental pollution have led to increased scrutiny and regulations on single-use plastics. This has prompted the industry to explore sustainable alternatives and adopt more environmentally friendly practices.\nKey Market Insights\n- Growing Demand for Convenience: The fast-paced lifestyle and increasing urbanization have fueled the demand for convenience products, driving the growth of single-use plastic packaging. Consumers prefer products that are easy to use, portable, and require minimal effort for disposal.\n- Environmental Concerns: The environmental impact of single-use plastic packaging, particularly plastic waste and pollution, has become a significant concern. Governments, organizations, and consumers are increasingly focusing on reducing plastic waste and promoting sustainable alternatives.\n- Regulatory Measures: Many countries have implemented regulations and policies to curb the use of single-use plastics. Bans on certain plastic products and the imposition of plastic taxes or levies have influenced market dynamics and encouraged the adoption of eco-friendly packaging solutions.\n- Shift Towards Sustainable Packaging: The industry is witnessing a shift towards sustainable packaging alternatives, such as biodegradable plastics, compostable materials, and recyclable packaging. Manufacturers are investing in research and development to develop innovative and eco-friendly packaging solutions.\n- Changing Consumer Lifestyles: The busy and on-the-go lifestyles of consumers have increased the demand for convenience products, which are often packaged in single-use plastic materials.\n- E-commerce Boom: The rapid growth of e-commerce has led to a surge in demand for packaging materials that can protect products during transportation. Single-use plastic packaging offers durability, flexibility, and cost-effectiveness for e-commerce packaging needs.\n- Increased Food and Beverage Consumption: The rising population and changing dietary habits have resulted in increased consumption of packaged food and beverages. Single-use plastic packaging provides a convenient and hygienic solution for food and beverage products.\n- Cost-Effective Packaging: Single-use plastic packaging is often more affordable compared to other packaging materials, making it an attractive choice for manufacturers, especially in price-sensitive markets.\n- Environmental Concerns and Regulations: Growing concerns about plastic waste and pollution have led to stricter regulations on single-use plastics. This has created challenges for manufacturers and increased the demand for sustainable packaging alternatives.\n- Shifting Consumer Preferences: As consumers become more aware of the environmental impact of single-use plastics, their preferences are shifting towards eco-friendly packaging options. This has created a need for manufacturers to adapt and invest in sustainable packaging solutions.\n- Recycling and Waste Management Issues: The recycling infrastructure for plastic packaging is often inadequate, leading to low recycling rates. Improving recycling capabilities and waste management systems is crucial to address the environmental impact of single-use plastic packaging.\n- Alternative Packaging Materials: The development and adoption of alternative packaging materials, such as paper-based packaging, bioplastics, and compostable materials, pose a challenge to the growth of single-use plastic packaging.\n- Sustainable Packaging Solutions: The demand for sustainable packaging alternatives presents a significant opportunity for manufacturers to develop and offer eco-friendly options. Investing in research and development to create innovative and sustainable packaging materials can help companies capitalize on this growing market segment.\n- Circular Economy Initiatives: The concept of a circular economy, where materials are reused, recycled, or composted, provides opportunities for the single-use plastic packaging market. Embracing circular economy principles can help reduce plastic waste and create a more sustainable packaging ecosystem.\n- Collaboration and Partnerships: Collaboration among stakeholders, including packaging manufacturers, brand owners, retailers, and waste management organizations, can foster the development of effective recycling and waste management solutions. Partnerships can also drive innovation and the adoption of sustainable packaging practices.\n- Consumer Education and Awareness: Educating consumers about the environmental impact of single-use plastics and promoting the benefits of sustainable packaging can create opportunities for companies to meet the evolving consumer preferences and capture a larger market share.\nThe single-use plastic packaging market is influenced by various dynamic factors. These include changing consumer preferences, regulatory landscape, advancements in packaging technology, and the emergence of sustainable packaging solutions. The market is characterized by intense competition, with players striving to differentiate themselves by offering innovative and sustainable packaging options. Collaborations and partnerships across the value chain are becoming increasingly important to address the challenges associated with plastic waste management and create a more sustainable packaging ecosystem.\nThe single-use plastic packaging market exhibits regional variations in terms of consumption patterns, regulations, and market dynamics. Developed regions such as North America and Europe have been at the forefront of implementing regulations to reduce single-use plastics. These regions also witness a higher consumer awareness and demand for sustainable packaging alternatives. In contrast, developing regions, particularly in Asia-Pacific and Latin America, are experiencing rapid industrialization and urbanization, driving the demand for single-use plastic packaging. However, these regions are also witnessing a growing emphasis on environmental sustainability, leading to increased efforts to reduce plastic waste and promote sustainable packaging practices.\nThe single-use plastic packaging market is highly competitive, with numerous players operating on both global and regional levels. Key players in the market include packaging manufacturers, material suppliers, and brand owners. These companies are actively investing in research and development to develop sustainable packaging solutions and gain a competitive edge. Partnerships, acquisitions, and collaborations are common strategies adopted by market players to expand their product portfolio, enhance their market presence, and address the challenges associated with plastic waste management.\nThe single-use plastic packaging market can be segmented based on various factors, including packaging type, end-use industry, and geography.\n- By Packaging Type:\n- Bags and Pouches\n- Wraps and Films\n- By End-Use Industry:\n- Food and Beverages\n- Healthcare and Pharmaceuticals\n- Personal Care and Cosmetics\n- Household Products\n- By Geography:\n- North America\n- Latin America\n- Middle East and Africa\n- Bottles: Single-use plastic bottles are extensively used for packaging beverages, including water, soft drinks, juices, and alcoholic beverages. The demand for lightweight and convenient packaging options has contributed to the growth of single-use plastic bottles in the market.\n- Bags and Pouches: Single-use plastic bags and pouches are widely used for packaging various products, such as snacks, groceries, and personal care items. The convenience and affordability offered by these packaging options have led to their widespread adoption.\n- Containers: Single-use plastic containers are commonly used for packaging food products, including ready-to-eat meals, sauces, and condiments. The durability and ability to preserve the freshness of the products have made plastic containers a preferred choice for manufacturers and consumers.\n- Wraps and Films: Single-use plastic wraps and films are used for packaging purposes, such as wrapping fresh produce, meat, and dairy products. These packaging materials provide protection, extend shelf life, and enhance the visual appeal of the products.\nKey Benefits for Industry Participants and Stakeholders\n- Cost-Effectiveness: Single-use plastic packaging is often more cost-effective compared to other packaging materials. It provides a cost advantage for manufacturers, particularly in price-sensitive markets.\n- Convenience and Functionality: Single-use plastic packaging offers convenience and various functionalities, such as sealing, flexibility, and transparency. These features enhance the usability and attractiveness of the packaged products.\n- Product Protection and Preservation: Plastic packaging provides effective protection against contamination, moisture, and physical damage. It helps preserve the freshness, quality, and shelf life of the packaged products.\n- Branding and Marketing Opportunities: Plastic packaging offers ample space for branding, labeling, and product information. Manufacturers can leverage this opportunity to enhance their brand visibility and communicate with consumers effectively.\n- Product Differentiation: The versatility of plastic packaging allows manufacturers to create unique shapes, sizes, and designs, enabling product differentiation in the market. This can help companies stand out and attract consumer attention.\n- Cost-effective packaging solution\n- Versatility and functionality\n- Excellent product protection\n- Environmental impact and concerns\n- Recycling and waste management challenges\n- Shifting consumer preferences\n- Growing demand for sustainable packaging\n- Circular economy initiatives\n- Collaboration and partnerships\n- Regulatory restrictions on single-use plastics\n- Competition from alternative packaging materials\n- Negative public perception and consumer backlash\nMarket Key Trends\n- Adoption of Sustainable Alternatives: The market is witnessing a growing trend towards the adoption of sustainable packaging alternatives. Manufacturers are exploring biodegradable plastics, compostable materials, and recyclable packaging to address the environmental concerns associated with single-use plastics.\n- Innovation in Packaging Technology: Advancements in packaging technology are driving innovation in the single-use plastic packaging market. Manufacturers are developing new materials, designs, and production techniques to enhance the functionality and sustainability of plastic packaging.\n- Circular Economy Initiatives: The concept of a circular economy is gaining traction in the packaging industry. Companies are exploring closed-loop systems, recycling initiatives, and partnerships to ensure the proper disposal and recycling of single-use plastic packaging.\n- Consumer Awareness and Education: Increasing consumer awareness about the environmental impact of single-use plastics is influencing purchasing decisions. Consumers are demanding more sustainable packaging options and actively supporting brands that prioritize eco-friendly practices.\nThe COVID-19 pandemic has had a significant impact on the single-use plastic packaging market. The increased demand for essential products, such as food, beverages, and personal care items, led to a surge in the use of single-use plastic packaging. The need for hygienic and convenient packaging options during the pandemic further drove the demand for single-use plastics. However, the pandemic also highlighted the importance of sustainable packaging and the need to address plastic waste management. The crisis has accelerated efforts to develop and adopt eco-friendly packaging alternatives, such as compostable materials and biodegradable plastics, to reduce the environmental impact of single-use plastic packaging.\nKey Industry Developments\n- Ban on Single-Use Plastics: Several countries and cities have implemented bans on certain single-use plastic products, such as plastic bags and straws. These bans aim to reduce plastic waste and promote the use of reusable and sustainable packaging alternatives.\n- Plastic Taxes and Levies: Governments have introduced taxes and levies on single-use plastics to discourage their usage and promote environmentally friendly alternatives. These fiscal measures encourage manufacturers to adopt sustainable packaging practices and invest in eco-friendly materials.\n- Collaboration for Recycling Infrastructure: Various stakeholders, including packaging manufacturers, waste management companies, and governments, are collaborating to improve recycling infrastructure. Investments are being made to enhance recycling capabilities and promote the circular economy.\n- Product Innovation: Manufacturers are focusing on developing innovative packaging materials and designs to reduce the environmental impact of single-use plastics. This includes the use of bio-based plastics, compostable materials, and packaging designs that minimize material usage.\n- Embrace Sustainability: Companies should prioritize sustainability and invest in research and development to develop eco-friendly packaging solutions. This will not only address environmental concerns but also cater to the changing consumer preferences and regulatory requirements.\n- Collaborate Across the Value Chain: Collaboration among packaging manufacturers, brand owners, retailers, and waste management organizations is crucial to develop effective recycling and waste management systems. Partnerships can also drive innovation and help create a more sustainable packaging ecosystem.\n- Educate Consumers: It is important to educate consumers about the environmental impact of single-use plastics and the benefits of sustainable packaging. Communication campaigns and labeling initiatives can help raise awareness and influence consumer behavior towards more eco-friendly choices.\n- Adapt to Regulatory Changes: Companies should closely monitor and adapt to changing regulations on single-use plastics. This includes complying with bans, implementing recycling programs, and exploring alternative packaging materials to ensure business continuity.\nThe single-use plastic packaging market is undergoing a significant transformation. The increasing focus on sustainability and the drive towards a circular economy will shape the future of the industry. The market is expected to witness a shift towards more sustainable packaging alternatives, such as bio-based plastics, compostable materials, and reusable packaging solutions. Collaboration and innovation will play a crucial role in addressing plastic waste management challenges and meeting the evolving consumer demands. The adoption of sustainable packaging practices will not only contribute to environmental conservation but also provide long-term business opportunities for industry participants.\nThe single-use plastic packaging market is at a turning point, with growing concerns over plastic waste and environmental pollution. While single-use plastics offer convenience and affordability, the negative impact on the environment has led to increased scrutiny and regulations. The industry is responding by exploring sustainable alternatives, investing in research and development, and collaborating across the value chain. The future of the market lies in adopting eco-friendly practices, embracing the circular economy, and educating consumers about the benefits of sustainable packaging. By prioritizing sustainability and innovation, industry participants can navigate the changing landscape and contribute to a more environmentally conscious and responsible packaging industry."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:5f578697-5b18-40ed-b818-be5ab9c2b3f0>","<urn:uuid:0b1c65f9-0336-4525-8f92-768273a00e87>"],"error":null}
{"question":"How do the high-resolution capabilities of Reference Recordings' HDCD format compare to the Rega DAC-R's maximum supported bit depth and sampling rate?","answer":"Reference Recordings uses HDCD encoding at 176.4 kHz/24-bits for their masters, while the Rega DAC-R supports up to 192 kHz sampling rates with 24-bit resolution across all its inputs (USB, optical, and coaxial). Both systems are capable of high-resolution audio, with the DAC-R actually supporting slightly higher sampling rates than Reference Recordings' standard format.","context":["25th Anniversary Special Feature!\nThank you for the opportunity to write about our label for Enjoy the Music.com. First, hearty congratulations on your 25th anniversary, a real milestone! For so many years, Enjoy the Music.com has presented well-written, impartial information and reviews which are so valuable to the audiophile and music community. Thank you!\nReference Recordings was founded in 1976. Producer John Tamblyn (\"Tam\") Henderson combined his love and knowledge of music with his quest for \"the sound of real musicians making music in real space,\" molding Reference Recordings into one of the most innovative and respected independent labels in the music business. His productions for RR are widely praised for their dedication to high quality sound in the service of great music. In 1978, Tam Henderson began working with engineer Keith O. Johnson, who was already an audio legend in the recording industry. The 100-plus recordings made by their team are considered by many to be the finest-sounding classical and jazz discs ever made.\nIn addition, during these years Mr. Johnson designed and patented many innovative products in the professional and consumer audio fields. The original \"RR Sound\" comes from his singular methods and equipment, almost all hand-built or extensively modified by him. Microphone techniques range from purist to complex, depending on the musical forces and the performing space involved. Johnson's investigation of electronic behavior and acoustic perception led to his development (with digital engineer Michael Pflaumer) of the revolutionary High Definition Compatible Digital encoding process, originally produced and marketed by Pacific Microsonics and later acquired by Microsoft.\nReference Recordings released the first commercial CDs with the process. Currently all Keith Johnson masters are recorded at 176.4 kHz/24-bits, still using the never-surpassed Pacific Microsonics Model Two HDCD encoders. HDCD brings a high level of accuracy and musicality to digital recordings, whether made into compact discs, SACDs, or released as high-resolution digital files.\nAfter 36 years as President and chief executive at Reference Recordings, Tam Henderson stepped down from day to day operations in 2012. He continues consulting on important decisions and directs operations of the label's LP division, Reference Mastercuts.\nThis is where I come in to the story. I (Marcia Martin) became Tam's business partner and RR's Vice-President in 1980, so this year is my 40th anniversary with RR. Happily, in 1990 Keith Johnson and I were married. I took over as Executive Director and Managing Director of day to day operations in 2012. I continue to be RR's business, legal and finance manager, but now am responsible for all artistic decisions as well. I am proud to have strategically positioned RR as one of the few American labels recording and releasing productions of top-flight American orchestras, with a special emphasis on audiophile sound and formats. I believe strongly in a team approach to business, and am fortunate to have assembled and worked with an outstanding group of audio and music industry professionals at RR for many years. We all love what we do in the service of great music and recordings. I am pleased to introduce our team to your readers here.\nKeith O. Johnson, GRAMMY winner and 13-time nominee, continues as Technical Director, engineering and mastering many of RR's critically acclaimed, award winning classical, jazz and blues releases. Vital to RR's technical work is second-generation RR team member, four-time GRAMMY- nominated engineer Sean Martin: digitally capturing our recording sessions, editing releases and keeping RR in the forefront of digital advancements.\nReference Recordings works with Victor Ledin and Marina A. Ledin of Encore Consultants L.L.C., in a strong collaboration to assist in signing classical artists and with repertoire decisions. RR benefits from their extensive music industry experience, including fresh ideas on production and label management. GRAMMY-winning producers, the Ledins have produced and edited many of Reference Recordings' recent releases, and will continue in that capacity for the future.\nRR veteran Janice Mancuso, Sales and Marketing Director for the label, works for us from her beloved Portland, Oregon. Ms. Mancuso handles media relations for Reference Recordings. For our audiophile base, she promotes and markets our series of \"Prof. Johnson\" recordings and releases, as well as the outstanding licensed recordings on our FRESH! series. She emphasizes high end formats, both physical and digital. In addition, as A & R Director for RR's non-classical releases, she works on signing and licensing, producing recordings and broadening our offerings of Blues, Roots and World music.\nJoAnn Nunes, now married to engineer Sean Martin, has handled logistics and assisted at RR's recording and editing sessions for several years. She has recently become our VP of Administration. I am very grateful for her work! In addition, Ms. Nunes and long-time RR veteran Bill Roarty share graphic design duties for RR, creating beautiful album graphics which make us proud.\nAudio industry and e-commerce professional Ric Mancuso (Triad Speakers, Spica, and Monster) is part of the RR team, working on special sales programs and industrial relations.\nLast but not least, Rosebrook Media's founder David Weuste manages our social sites and digital media as well as the Reference Recordings website from his Texas office. Up-and-coming team member Marcus Johnson assists David Weuste with website sales and ad campaigns, and assists us in the RR office with new release submissions, customer service and invoicing. Full disclosure- engineer Sean Martin is my first son, and Marcus Johnson is the son of Keith Johnson and myself. RR is a family business and proud of it.\nReference Recordings records and manufactures Hybrid SACDs, Reference Mastercuts LPs, compact discs, and our own format HRx discs: 176.4 kHz/24-bit music on DVD audio discs. Our recordings are also offered digitally through our own site and multiple sites worldwide, both streaming and as downloads including high resolution PCM downloads and stereo and multi-channel DSD downloads.\nOur current artist roster includes The Kansas City Symphony, The Pittsburgh Symphony Orchestra, The Utah Symphony Orchestra, True Concord Voices and Orchestra, The San Francisco Ballet Orchestra, pianists Joel Fan and Nadia Shpachenko, The Dallas Wind Symphony, blues singer/songwriters Doug MacLeod and Fiona Boyes, organists Jan Kraybill and Mary Preston, and more. Legacy artists available on RR titles perennially in print include: The Minnesota Orchestra, jazz pianist Dick Hyman, Eileen Farrell, Mike Garson, the original Canadian ensemble Tafelmusik, Red Norvo, The Kronos Quartet, The Chicago Pro Musica, Frederick Fennell, Ruggiero Ricci, Robert Farnon, Jose Serebrier, The Turtle Creek Chorale, Flora Purim, Airto Moreira, and many others.\nReference Recordings has recorded and licensed multi-channel high resolution masters for many years. We have a special interest in Immersive Sound, and we have released many in the SACD format. We are now offering digital files in multi-channel as well. Our first Surround Sound SACD release won a GRAMMY award for Best Surround Sound Recording in 2010.\nAn increasingly important part of our label, our FRESH! series showcases outstanding artists and engineering teams from beyond the original RR team. We are extremely pleased to release high quality, hybrid multi-channel SACDs on this series with some of the best American orchestras and other groups. Most are recorded and mastered in DSD by the team at Soundmirror, whose outstanding orchestral, solo, opera, and chamber recordings have received over 120 GRAMMY nominations and awards.\nEspecially in these extraordinary and difficult times, music is so important to uplift our spirits and give solace. Reference Recordings is honored to offer the joy of music to our customers. And, crucially, we are trying our utmost to support our artists, many of whom are struggling now. They all are using very creative means to present their performances and music to audiences without the ability to perform live. We salute them and truly hope that we all come through this crisis stronger and better than ever.\nAgain, thanks for this opportunity to present Reference Recordings to your readers.\nPS: Below is a YouTube video we made of one of the last recording sessions with Tam Henderson producing. This was a session in 2011 with the Dallas Winds. It shows how hands-on Keith is when mixing on the fly!\nVoice: (650) 355-1845","Developed to be simple to set up and use, the Rega DAC-R is designed to optimise performance from any two channel PCM digital audio source.\nWith the PC now widely accepted as a credible medium for storing and streaming music,the use of high quality lossless files such as WAV, FLAC and ALAC offer performance through the DAC-R equal to, and in some cases better than, Red Book CD. Great care has been taken to remove noise generated by the PC and other input sources. (During development this was identified as a major drawback with many DAC’s on the market today).\nThe DAC-R is housed in a custom aluminium and steel case and boasts a pair of Wolfson DAC IC’s, three user selectable digital filters and one isolated asynchronous USB, two isolated Co-axial inputs & two Toslink SPDIF inputs. The DAC-R has been designed and engineered to integrate perfectly into any system and achieve the highest performance in its class. We hope you enjoy this Rega product for any years to come. The optical and coaxial input stage comprises a Wolfson digital receiver with a high stability, low jitter clock driving the receiver PLL. The receiver and PLL have their own dedicated power supplies.\nThe DAC stage comprises a pair of parallel-connected Wolfson WM8742 DAC’s, that are driven via a buffer stage, which ensures the integrity of the data being fed to the DAC IC’s similar to the arrangement used in the Isis reference CDP. The USB input stage is comprised of a bit perfect XMOS USB Audio 2.0 with asynchronous clocking. The USB input stage feeds the Optical and Coaxial input stage via an isolating transformer giving total isolation from the host computer.\nThe output amplifier employs a discrete differential, multiple feedback filter and output amplifier, with a high cut-off frequency for use with higher sample rates. We decided not to use a sample rate converter and process the data at the incoming sample rate which keeps the signal processing to a minimum. Jitter was minimised by synchronously locking the digital data with our receiver PLL (removing any jitter from the input signal).\nAll the capacitors associated with the analogue signal path are audio grade bypassed with MMK polyester capacitors, and low impedance conductive polymer capacitors are used for DAC decoupling. The power supply utilizes a toroidal transformer, fast rectifier diodes and again audio grade capacitors. There is a power supply for the control micro controller, separate from the digital & analogue audio stages. Special attention being paid to the inter IC control signals ensures the control data noise is kept to a minimum.\nDAC 2x Wolfson WM8742\nFrequency Response (100KΩ load) Filter 1 selected\nLow data rate 44.1/48kHz = 10Hz -0.05dB to (44.1K) 20.02kHz (48K) 21.7kHz -0.03dB\nMedium data rate 88.2/96kHz = 10Hz -0.05dB to (88.1K) 28.7kHz (96K) 31.2kHz -3dB\nHigh data rate 176.4/192kHz = 10Hz -0.05dB to (176.4K) 44.1kHz (192K) 47.7kHz -3dB\nMaximum output level = 2.175V into 100KΩ load\nBit resolution (all inputs) 16 to 24bit\nSupported data rates = 32, 44.1, 48, 88.2, 96, 176.4, 192kHz\nTotal Harmonic Distortion all inputs (24bit 96kHz) = 0.006% @ 1kHz\nSignal to Noise Ratio -105dB (relative to maximum output level with a 100Hz to 22kHz bandwidth)\nUSB Asynchronous Isolated (24bit 44.1/48/88.2/96/176.4/192kHz)\nInput 1 Optical/Toslink (24bit 32/44.1/48/88.2/96/176.4/192kHz)\nInput 2 Optical /Toslink (24bit 32/44.1/48/88.2/96/176.4/192kHz)\nInput 3 Isolated 75Ω Co-axial (24bit 32/44.1/48/88.2/96/176.4/192kHz)\nInput 4 Isolated 75Ω Co-axial (24bit32/44.1/48/88.2/96/176.4/192kHz)\nDigital SPDIF outputs (via receiver & PLL)\nIsolated 75Ω Co-axial\nDimensions W 215mm x D 320mm x H 80mm\nRemote NEC system (6E91)\nWeight 4.0 Kg\nWhat can I plug into my DAC?\nDAC's will usually have a number of inputs. Coaxial and Toslink Optical are the most common, while USB is still also very common.\nThe Optical and Coaxial connections can have a CD player, network streamer, television, dvd/blu-ray disc player, game console, and some computers.\nThe USB input can handle pretty much any kind of computer, or a phone with the appropriate adapter (OTG for android, Camera Connect Kit for iOS devices).\nWhat is a DAC?\nEvery digital device with an analog output has a Digital to Analog Converter of some kind. In the case of phones, televisions, and computers, they are usually stuck on as an afterthought, or just to do the job of creating analog sound to be amplified and played through speakers.\nHaving a standalone DAC is a worthwhile investment in any modern Hi-Fi system, essentially all the circuitry involved in creating that signal you can hear from the 1's and 0's is housed in its own box, with usually a much better design than what you would find in your television or computer.\nA DAC can be useful to connect digital devices to an analog amplifier if the amplifier doesn't have digital inputs. Or if you are after a cleaner, more refined sound.\nThings to listen for in higher quality DAC's are separation of sounds (how well you can hear a single voice or instrument in the mix of all the others), \"textures\" of sounds (raspyness of a bow being drawn across the strings of a double bass), or the clarity of echo's and quiet noises among loud instruments."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:5545aac2-d672-497e-ae84-b2ecacd809d7>","<urn:uuid:711204e7-173d-4c48-b758-1c722eb98711>"],"error":null}
{"question":"Hey wine lovers! I'm wondering - does Pinot Bianco from Alto Adige Terlano share any similar growing conditions with the DO wines from Catalonia? 🍷","answer":"While both regions produce wines, they have distinct growing conditions. Alto Adige Terlano's vineyards are located at 250-900 meters above sea level on red porphyry soil, with Mediterranean-like conditions featuring warm days and cool nights. The region benefits from Alpine protection against Atlantic winds and has only about one-third of the typical precipitation for southern Alpine foothills. In contrast, Catalonia's twelve DO areas have varying conditions - for instance, Conca de Barberà stands between 350-600m above sea level, while Penedès is situated between the pre-coastal mountain range and Mediterranean coast. Each Catalan DO has unique geography and climate creating distinctive wines.","context":["Twelve Designation of Origin (DO) areas guarantee the geographical origins and quality white, red, rosé and sparkling of the wines made in Catalonia from specific grape varieties and with a great deal of hard work and dedication from the producers and growers. The geography, climate and varieties of each region create different wines. Many of them come from sustainable, organic and biodynamic wineries with one common denominator: excellence.\nDesignation of Origin Alella\nThis ancient Designation of Origin isn’t far from Barcelona. It covers a small area and is the only one in Catalonia to produce 100% organic wines. The Designation of Origin Alella mainly comprises small wineries producing top-quality wines. The Pansa Blanca is the most exotic and highly prized variety.\nDesignation of Origin Penedès\nIn a prime location between the pre-coastal mountain range and the Mediterranean coast, the Penedès reflects the wealth and variety of its soils with distinctive characterful wines which have the Xarel·lo grape as their trademark.\nDesignation of Origin Pla del Bages\nIts location, between mountain ranges and geological formations, makes the wines from this DO in central Catalonia fruity and fresh. Their own distinctive personality is the result of the native grape variety Picapoll, and this is the only place in Spain where it is grown.\nDesignation of Origin Catalunya\nA wine-growing area covering 50,000 hectares across more than 300 towns and villages in Catalonia. It produces modern, innovative wines from 35 authorised grape varieties. This diversity brings us unusual, surprising white, rosé and red coupages that invite us to keep discovering the ever-evolving range of wines.\nDesignation of Origin Cava\nCava is the Catalan sparkling wine par excellence and has its own DO. It is produced throughout the entire wine-growing region of Barcelona and Catalonia, with the former – which has 63 cava-making towns – being the biggest producer in Spain. Sant Sadurní d’Anoia is the cava capital and records dating from 1872 belonging to the Codorniu winery attest to the fact that cava was first produced here.\nDesignation of Origin Conca de Barberà\nThe towns and villages in this Designation of Origin in the north of Tarragona province stand between 350 and 600 m above sea level, and produce fresh, light, highly aromatic wines. Trepat, a light, fruity black grape, is the local variety and a festival is held in its honour in the village of Barberà de la Conca.\nDesignation of Origin Costers del Segre\nDespite the fact that the Designation of Origin Costers del Segre is located inland, in the west of Catalonia, away from the normal commercial channels, this hasn’t prevented it from being a pioneer in introducing non-native grape varieties and adopting Californian wine-producing techniques. Their wines are usually multi-varietal and, as a result, the reds are balanced and well structured and the whites fresh and fruity.\nDesignation of Origin Empordà\nIn the north of Catalonia, close to the French border, the counties of the Alt Empordà and Baix Empordà produce the wines from this DO. They produce a wide variety of wines: the reds are full bodied and highly aromatic and the whites, which are often made from native varieties, fresh and delicate. The sweet wine, Garnatxa de l’Empordà, made with the native grape variety of the same name, is particularly noteworthy.\nDesignation of Origin Montsant\nThe native grape varieties Garnatxa and Carinyena are the hallmarks of this Designation of Origin. Reds are the most common and are characterised by their intense and well-balanced aromas and great complexity.\nDesignation of Origin Priorat\nThis small, compact, well-defined mountainous region, with slaty soil, stands in the heart of the counties of Tarragona. It is a Qualified Designation of Origin, which links the flavour and style of the wine to the specific location where the grapes are grown, like the French concept of terroir. All the traditional red wines from Priorat are made from Garnatxa Negra or Garnatxa and Carinyena and are extremely smooth, balanced and subtle.\nDesignation of Origin Tarragona\nWines from Tarragona were already prestigious at the time of the Roman Empire. Its vineyards are planted along the coast and further inland, rising gently from sea level, near Tarragona, or are located on small, flat areas of land by the river Ebro in the sub-area of Ribera d’Ebre. The different grape varieties grown produce full-bodied and aromatic reds and smooth, well-balanced and fruity whites.\nDesignation of Origin Terra Alta\nSituated between the river Ebro and the lands of Matarranya in Aragon, the Designation of Origin Terra Alta grows the Macabeu, Parellada, Samsó and Garnatxa grape varieties, the latter being the most widely used in its wines. The result: wines with their own strong character and personality.","Pinot Bianco 2002\n\"Cantina Terlano has an unusual offering in the form of its Rarities, special editions of mature white wines that have been left to age on the lees in steel pressure tanks for at least ten years. The 2002 Rarity is a Pinot Bianco with a youthful freshness that belies its maturity. That makes it perfect for a long period of aging in the bottle. Terlano has the terroir to produce great white wines, as its Rarities so convincingly demonstrate.\"\n- Doc denomination: Alto Adige Terlano\n- Variety: 100% Pinot Bianco\n- History of the variety: first vintage 1979\n- Year: 2002\n- Bottles produced: 3,340\n- Yield: 42 hl/ha\n- Quality line: The rarities\nManual harvest and selection of the grapes; gentle whole cluster pressing and clarification of the must by natural sedimentation; slow fermentation at a controlled temperature in stainless steel tanks with partial malolactic fermentation (50%) and aging on the lees in big wooden barrels for 12 months; further aging on the lees in steel pressure tanks without filtering or fining for at least ten years.\n- Country: Alto Adige Terlano DOC\n- Provenance: Alto Adige\n- Altitude: 250 - 900 m a. s. l.\n- Slope: 5 - 70 %\n- Orientation: South - Southwest\n- Color: intensive light straw yellow with delicate greenish reflections\n- Smell: Terlano’s 2000 rarity wine has an impressive freshness and a wealth of aromas, with new components revealed at every tasting, including herbal notes of camomile, lemon balm and lovage together with a hint of dried kaki and apricot. The multifaceted bouquet also displays aromas of bread crust and yeast bun paired with flint.\n- Taste: The wine is smooth and creamy on the palate, with a strong acid backbone that leaves a both youthful and delicate impression and strikes a fine balance with the mineral components. The finish is elegant and silky, but also enormously deep and firm.\nThe ideal meditation wine.\nExcellent with poached lobster and sautéed mussels, also in combination with taglierini with butter and white truffles, and with beef carpaccio with fresh Alba truffles or a veal sirloin steak.\n2002 will also be remembered as a warm year, even though the January was very cold and the heat of summer did not last very long. Thanks to above-average temperatures in the transitional months, however, the wines have an attractive tension and complexity. During the whole of the winter, Terlano was not once carpeted in snow. The combination of soils without a protective snow cover, the lack of precipitation since the end of the previous autumn and the long period of cold weather made it a difficult start to the year in the vineyards. It was not until February that a normal volume of rain was delivered. That also heralded a relatively positive pattern of spring weather. The vegetation emerged from its hibernation in the middle of March and, thanks to the warm temperatures, developed very quickly. Not even a late frost at the end of the month or the unsettled – but always warm – weather in April were a problem for the grapevines. The pronounced lack of moisture during spring was compensated by a large number of rainy days in May – as well as continually above-average temperatures – so that even the lower layers of soil received an adequate water supply. Summer arrived unexpectedly early, but the hot weather did not last very long and, after just thirty days, it seemed to be over by the end of June. Neither the July nor August were very summery, with several days of rain but only a limited amount of precipitation. The autumn began with some very warm and sunny days, but the temperatures dropped steeply toward the end of September, returning to the normal range for the time of year. Following a short period of rain, the October was a very good month for the harvest.\nThe vineyards are located at between 250 and 900 meters above sea-level on a bed of striking red porphyry, an igneous rock with large mineral inclusions known as quartz porphyry in geological terminology. This terroir is home to salty wines with a fine tension to intrigue the palate plus outstanding longevity. The south-facing slopes receive maximum sunshine. Under these almost Mediterranean conditions, a wide range of grape varieties flourish, while in Terlano itself various Mediterranean plants like olive, pomegranate, cypress and almond trees are to be found. The warm days and cool nights of the ripening period are the key to a high sugar content, intensive aromatics and the typical Alpine freshness of the wines.\nIn addition to “Alto Adige DOC” as the geographic designation of origin for Alto Adige, the wines are additionally labeled “Terlano” in recognition of the specific climatic and geological character of the terroir. The term “Terlaner classico” is used for those grape varieties that grow in the traditional wine-growing area between Andriano, Nalles and Terlano.\nThe high peaks of the main Alpine chain protect South Tyrol from the Atlantic winds and cold northerlies, while the region benefits from the Mediterranean climate from the south. That explains the pronounced differences between day- and night-time temperatures, which are the key to full maturity and elegant wines.\nTo the south, a number of mountain massifs like the Adamello also have a protective function. As a result, annual precipitation is only about one third of the average for the southern Alpine foothills, and the number of hours of sunshine is higher. The climatic conditions are not unlike those to be found in wine-growing areas like the Swiss Canton Valais.\nWhen the sun rises behind the mountains east of Terlano on one of the year’s 300 sunny days, it is already high in the sky as the wine-growing area has a westerly to south westerly exposure. The lower atmospheric density permits more direct solar irradiation with less diffuse sunlight. That increases the difference between the slopes on the sunny and shady sides of the valley.\nMicroclimate in Terlano\nContinental climate (Cfa Köppen-Geiger)\nAnnual sunshine hours: ø 2135\nMaximum temperatures: 38,2 °C\nAverage temperatures: 12,9 °C\nMinimum temperatures: -10,7°C\nAnnual percipitation: ø 558 mm\nAverage global radiation: 150,1 W/m²\n- North foehn: cool and dry down-slope wind\n- Ora: valley wind system from the south, bringing in air from the Po Valley\n- falstaff 2014: 94 points\n- James Suckling 2014: 92 points\n- Wein-Plus 2014: 94 points\n- Gambero Rosso - Vini d'Italia 2015: two glasses\n- Vitae - La guida vini AIS 2015: Tastevin and 4 vines\n- Bibenda/Duemilavini 2015: 5 grapes\n- Jancis Robinson 2014: 17+ points\n- Guida essenziale ai Vini d’italia – Daniele Cernilli 2015: 95 points\n- Alcohol content: 13.5 % vol\n- Residual sugar: 2.9 g/l\n- Total acidity: 5.5 g/l\n- Storage advice: Cool storage at constant temperatures, high level of humidity, good ventilation and as little light as possible\n- Cellar temperature: 10 - 15 °C\n- Minimum maturity: 8 years\n- Serving temperature: 12 - 14 °C\nGlass for an evolved white wine"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:2ca4fca2-0210-4f10-85d7-2e08681ea2f4>","<urn:uuid:226c5e03-6069-4e56-80b2-76c300c98c30>"],"error":null}
{"question":"Hey science nerds! 🔬 Can someone explain the key difference between a food web and a food chain? I'm writing a paper and want to make sure I get this right!","answer":"The key difference is that food chains are linear, typically involving a single species from each food rung, while food webs combine all food chains in an ecosystem to show how matter flows within the set of trophic relationships. Food webs are more complex, more abundant, and more difficult to graph since they show the sum of all trophic chains in an ecosystem.","context":["We explain what a food or food web is, differences with a food web chain and its characteristics in terrestrial or aquatic environments.\nWhat is a food web (trophic network)?\nIt is called the food web or food cycle to the natural interconnection of all food chains belonging to an ecological community . It is usually represented visually, as a network or also a pyramid.\nRemember that these food chains linearly describe the way in which matter and energy pass from one living being to another within a specific habitat . In other words, the sum of all the trophic chains of an ecosystem will result in its food web.\nTrophic relationships between various life forms are understood based on a primary and fundamental distinction between organisms:\n- Autotrophic organisms . They are able to synthesize their nutrients from inorganic matter\n- Heterotrophic organisms . They are incapable of such synthesis and therefore are obliged to consume the organic matter of other living beings, whether autotrophic or heterotrophic.\nEach of these categories composes a trophic level, in which all living beings can be classified. However, heterotrophic organisms or consumers are subdivided into different groups in turn, depending on what strategies they put in place to consume the organic matter of other living things and what kind of living things they usually feed on.\nThat is to say that among the heterotrophs are:\n- Herbivores or primary consumers . They feed on plants and other autotrophic beings.\n- Carnivores or secondary consumers . They feed on herbivores.\n- Tertiary predators or consumers . They feed on both primary and secondary.\n- Decomposing organisms : They are also heterotrophic, but they feed on decomposing organic matter, that is, dead.\nAll this classification is contemplated in the trophic networks, an ecological perspective that the English zoologist Charles Elton inaugurated with his text Animal Ecology (1927), the first attempt to organize living beings into functional groups according to their way of nourishing.\nThen the contributions in the matter of Raymond Lindeman (1942) were added, insisting on the vital role of decomposers in the ecological circuit. All this vital to the understanding that we currently have in the way in which matter and energy are transmitted along the trophic networks of an ecosystem.\nThe difference between trophic networks and trophic chains is subtle: the sum of the trophic chains of an ecosystem will result in a trophic network . Trophic chains are linear, generally involving a single species from each food rung.\nThe networks instead try to combine them all to establish a map of how matter flows within the set of trophic relationships of a given place. That is why networks are more complex, more abundant and more difficult to graph and conceive .\nExamples of food web chains\n- In the sea, phytoplankton (plant) serve as food for malacostraceous crustaceans (krill), which are eaten by (very) small fish. These, in turn, are preyed upon by larger fish such as sardines, which serve as food for predators such as barracuda. These, when dying, are decomposed by scavengers such as crabs and other crustaceans.\n- The rabbits eat plants and herbs, but are predated by pumas, foxes and other carnivores quadrupeds medium sized. When they die, the latter serve as food for carrion birds such as gallinazos (zamuros).\n- The plants are parasitized by caterpillars, which serve as food for various small birds, in turn preyed upon by hunting birds such as the eagle or the hawk, whose bodies will be decomposed by bacteria and fungi when they die.\n- The insects like locusts eat leaves of plants, insectivorous toads eat them snakes and toads. And finally, these snakes may be eaten by larger ones.\n- The marine zooplankton serves as food for the whales, which capture them with their long bales, and these are preyed upon by man.\n- The decomposing flesh of dead animals serves as food for the larvae of flies, which as they grow and become imagos will be preyed upon by spiders, in turn victims of other larger spiders, which serve as food for raccoons and coatis. finally preyed upon by carnivorous hunting snakes such as the rattlesnake.\n- The grass nourishes the sheep, favorite victims of jaguars and pumas, who when they die are decomposed into humus by bacteria and fungi , thus nourishing the grass again.\n- The bark of the trees serves as food for certain types of fungi, which are in turn food for small rodents (such as squirrels), which are in turn preyed upon by birds of prey (such as owls).\n- The marine phytoplankton is the food of bivalves such as mussels, which are preyed upon by crabs and these in turn by seagulls.\n- The beetles dung eat the feces of higher animals, but they are preyed upon by lizards, their food instead of mammals such as coyotes.\n- Many insects such as bees subsist on floral nectar, and are preyed upon by spiders that in turn feed small birds, victims of wild cats such as the wild cat.\n- The zooplankton marine feeds the small molluscs such as squid, fish preyed mainly by medium, in time to feed seals and marine mammals, which can in turn be hunted by orca whales.\n- Decomposing organic matter feeds bacteria, which do the same with protozoa (such as free-living amoebae) and with certain nematodes (worms), which in turn provide sustenance for larger nematodes.\n- The butterflies feed on nectar floral or fruit, and are food for predatory insects like the praying mantis. But this also serves as food for the bats, who are finally preyed upon by the possums.\n- The weed supports large herbivores such as the zebra, which in turn is preyed upon by the crocodile.\n- The earthworm ground feed on organic matter rotting in the earth itself, and are eaten in turn for small birds, also a victim of hunters felines like the cat that die back land organic matter to new feeding worms.\n- The corn serves as food for the chickens, whose eggs are eaten by the weasels, and these in turn by the hunting snakes.\n- Some aquatic spiders feed on hunting the larvae of other insects, during their submerged stage, and at the same time serve as prey for some river fish, which are preyed upon by the kingfisher bird or by storks.\n- In the sea, plankton serve as food for small fish, and these for larger fish, which in turn are preyed upon by larger fish. The proverb says that there is always a bigger fish in the ocean.\n- Certain parasitic insects on the fur of mammals (such as ticks) are the food of symbiotic birds that obtain their food by cleaning these mammals. These birds are in turn preyed upon by birds of prey such as the condor.\nAquatic Food Web\nIn aquatic ecosystems , trophic networks are fully adapted to life inside, below and on the surface of the water . This applies to large bodies of water such as oceans , lakes and other water deposits .\nAquatic food chains usually start in algae and certain types of photosynthetic microorganisms that float on the surface, called phytoplankton , and that play the role of autotrophic producers.\nThey are fed by primary consumers, usually other microorganisms ( zooplankton ) or tiny crustaceans , when not small fish, sponges or other simple life forms .\nThe next link involves larger fish, jellyfish and other very first predators. The third link of consumers already shows fish of good size, and even some final predators.\nThese chains must incorporate actors that feed on the sea, but do not live in it , such as seabirds (such as pelicans) capable of fishing from schools on the surface.\nAlso involved in trophic networks are marine mammals (seals, walruses, whales) that usually act as final predators (except in the case of the seal, a favorite prey for the orca whale and certain sharks). In lakes, rivers or certain islands, amphibians and reptiles also participate , as active predators according to their size (such as crocodiles).\nSimilarly, the decomposers of the sea are legion . Scavenger crustaceans, tiny fish and various types of microorganisms are responsible for the organic matter left over from the hunts, which in turn constitutes a rain of food for the deepest and darkest regions of the sea.\nFood Land Network\nIn terrestrial ecosystems , trophic networks are even more vast than marine ones, since they involve a gigantic variety of autotrophic organisms (plants).\nAs a consequence, there is a wide diversity of primary consumers : from insects that feed on sap or nectar, through birds that devour fruits and ruminant herbivores of different volumes, to symbiotic and decomposing fungi, leaf-eating insects and a huge number of others.\nLikewise, such a variety of herbivores supports an equally diversified number of secondary consumers , including especially small rodents, some primates and arthropods such as the spider.\nThey also depend on tertiary consumers, larger and carnivorous appetite, such as big cat hunters, bears, lizards, birds of prey, superior primates and, of course, the human being .\nThe most common decomposers are bacteria and other microorganisms, as well as fungi, scavenger insects or larvae of various types.\nFood Pyramids And Their Levels\nThe functional groups listed here (producers, primary, secondary and tertiary consumers, decomposers) that make up all the chains and trophic networks, can be organized visually based on the criteria of abundance of each group .\nThat is, the farther away from the producing organisms , the less abundant life tends to be, given that the energy and nutritional requirements tend to be higher, as they have larger species. In this way, food chains and networks can be illustrated in the form of a pyramid: the trophic pyramid.\nThe pyramid will be sectioned in levels, each corresponding to a trophic link , having decomposers at the base, and together with them producers, forming the base of the pyramid: abundant and primary, do not depend on any link, but hold to those above.\nOn the producers will be the primary or herbivorous consumers, and on them the secondary and tertiary consumers, with as many levels as necessary, as we tend to species of greater size, greater appetite, but at the same time less abundance, something represented in the narrowing of the pyramid towards its tip.\nThus, for example, the final predators, located at the very tip of the pyramid, will have nothing above, but will depend nutritionally on all lower levels. However, it is important to remember that they also serve as food for decomposers.\nDesert Food Web\nThe desert is an intense ecosystem, of life adapted to resist the brutal daily temperatures and the terrible drought, which is a challenge given that there is a scarce vegetation in these places, designed to resist a long time without water or to capture it from the air, and therefore a very low biodiversity rate .\nHowever, in the desert it is possible to find all the trophic levels of a pyramid : the producers, among which will be the xerophytic plants, such as cacti, never too numerous, unlike other ecosystems.\nOn the other hand, decomposers are much more abundant compared to the other levels : insects, scavengers and microorganisms, since in the desert the intense conditions cause nothing to be wasted.\nBased on these decomposers, rather than the plants, the rest of the trophic network is sustained . It contains small primary consumers, mostly insects and some small rodents.\nThey eat hunting arthropods (such as scorpions), poisonous snakes or some small birds. And finally there is a third link of consumers consisting of birds of prey , snakes of good size or some canids such as coyote, depending on the location and type of desert."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:86f84ae2-be4a-4b9c-8704-d02e7cf359e8>"],"error":null}
{"question":"Looking at recent asteroid research - what's the connection between an asteroid's size and its behavior during impacts?","answer":"Recent research has revealed a counterintuitive relationship between asteroid size and impact behavior. While scientists previously believed that larger asteroids were easier to destroy because they have more weak points, new research from Johns Hopkins University has shown the opposite. Larger asteroids are actually harder to destroy and require much more energy than previously thought. This is demonstrated by both theoretical models and specific cases - for example, asteroid Duende, at just 30 meters in diameter, would have produced an airburst of 530 kilotons if it had impacted Earth, while larger asteroids of 70 meters could produce impacts of 8.5 megatons, showing how destructive force increases significantly with size.","context":["Destroying an asteroid flying toward Earth will be more difficult than previously thought\nUsing a new computer modeling method and recent findings on rock fracture processes, researchers at Johns Hopkins University have demonstrated that destroying an asteroid on a collision course with Earth will be more difficult than previously thought.\nThe threat from an incoming asteroid is a popular theme of Hollywood productions. In the movies, brave heroes spring into action and fly to meet the space rock to destroy it and save the Earth from annihilation. Such a scenario can be seen, for example, in the movie Armageddon. But as researchers at Johns Hopkins University (JHU) are finding, destroying an asteroid may be more difficult than previously thought.\nScientists in earlier calculations estimating the power needed to successfully smash a large asteroid also used computer models. However, the new model presented by the JHU researchers took into account a variable thatórą earlier models overlooked. This refers to the rate at which cracks propagate through the asteroid after it hits it.\nThe findings, whichóre will be published on March 15 in the periodical „Icarus”, may pomóc in developing effective defense strategies against asteroid impactsów. They can róalso pomóc to better understand the formation processes of the Solar System.\n– Until now, we believed that the larger the object, the easier it is to destroy, because larger objects have more weak punków. However, our findings show that asteroids are harder than we thought and require much more energy to destroy them – said Charles El Mir, head of theówny author of the publication.\nJHU scientists began by creating a computer model thatóry simulated the effects of an asteroid with a diameter of one kilometer hitting another with a diameter of 25 kilometersów. The impact was simulated at a speed of five kilometersów per second. The same scenario was tested in earlier computer models. They took into account many factorsów: mass, temperature or fragility of the material, and the result indicated that in such a collision the larger asteroid would be completely destroyed.\nBy taking a closer look at the minute changes in the asteroid’s structure, scientists have developed a more accurate picture of the post-impact situation. In the new model, El Mir and his coóDerek Richardson and K. T. Ramesh took into account more detailedół smaller-scale processes, whichóre occur during asteroid collisions.\nPrevious models did not adequately account for the limited velocity of cracks in asteroids. The new model, called the Tonge-Ramesh model, was able to give researchers a more accurate picture of what might happen during such a collision. The results showed that a larger asteroid, instead of being completely destroyed as previous models indicated, would only be partially destroyed.\nThe new model suggests that gravity can pomóc asteroids to stay in one piece even after a strong impact and that much more energy is needed to completely destroy a space rock.\nThe simulation was divided into two phases: a fragmentation phase on a shortótion on a short time scale, and a phase of reaccumulation under the influence of gravity occurring on a longer time scale. The first phase considered processes thatóre start immediately after the asteroid’s impact, processes that occur in fractions of a second. The second long-term phase takes into account the effect of gravity on the rock fragments formed during the impact, as well as the gravitational re-accumulation occurring for hours after the impact. Below, the two phases shown on the krótic video films.\nAs the simulation indicated, the first phase created millions of cracks in the asteroid. This phase examined the individualólne fractures and predicted ogólne patterns of their propagation. The new model showed that the asteroid was only partially destroyed, contrary to what was previously thought. Instead, the damaged asteroid nucleus exerted a strong gravitational influence on the fragments formed in the second phase of the simulation.\nAccording to the results, the final impact was not just a pile of debrisów – a collection of small fragmentsóin loosely coupled gravitational. The impacted asteroid was not completely destroyed, indicating that much more energy is needed to remove the asteroid threat.\n– It may sound like science fiction, but many of the studies address asteroid impacts. For example, if there is an asteroid that is on a collision course with Earth, would it be better to smash it into small pieces or change its flight trajectories? And if the response is the second option, how much energy is needed to change its trajectory without destroying it? It’s someóre of the aspects we are consideringów,” explained El Mir.\n– Quite often we are hit by small asteroids, such as in Chelyabinsk a few years ago. It’s only a matter of time before our academic musings become the answer to a serious threat. We need to know exactly what we should do when that time comes, and such research is crucial for decision-making – emphasized Ramesh.","|Discovered by||OAM Observatory, La Sagra (J75)\n|Discovery date||February 23, 2012|\n|MPC designation||2012 DA14|\n|Post 2013-Feb-15: Aten\nPre-2013: Apollo NEO\n|Aphelion||0.9917 AU (Q)|\n|Perihelion||0.8289 AU (q)|\n|0.9103 AU (a)|\n|Dimensions||~45 meters (148 ft)\n20 m × 40 m (66 ft × 131 ft) (elongated)\ngeometric mean = 18 m\n|Albedo||0.44 ± 0.20|\n|7.2 (2013 peak)|\n25.29 HV; 24.5 HR \n24.4 (2012 estimate)\n367943 Duende, also known by its provisional designation 2012 DA14, is a near-Earth asteroid with an estimated diameter of 30 meters (98 ft). Before radar imaging, its estimated diameter was 45–50 meters. During its 15 February 2013 close passage, Duende passed 27,700 km (17,200 mi), or 4.3 Earth radii, from Earth's surface. This is a record close approach for a known object of this size. About 16 hours before the closest approach of Duende, an asteroid entered Earth's atmosphere above Russia, which was, however, unrelated to it because it had a completely different orbit.\nDiscovery and past risk assessments\nDuende was discovered on February 23, 2012, by the Observatorio Astronómico de La Sagra, Granada Province in Spain (J75), operated remotely by amateur astronomers in Mallorca, seven days after passing 0.0174 AU (2,600,000 km; 1,620,000 mi) from Earth. It was named after the duende, fairy- or goblin-like mythological creatures from Iberian, Latin American and Filipino folklore.\nBased on the still relatively imprecise orbit deduced from the short arc of the 2012 observations, there was a cumulative 0.033% risk estimate (1 in 3,030) of Duende impacting Earth sometime between 2026 and 2069. It was already clear that Duende would pass no closer to Earth's surface than 3.2 Earth radii during its 2013 passage. Eliminating an entry on the Sentry Risk Table is a negative prediction; a prediction of where it will not be.\nOn January 9, 2013, Duende was observed again by Las Campanas Observatory and the observation arc increased from 79 days to 321 days. On February 15, 2013 at 19:25 Universal Time, Duende passed 0.0002276 AU (34,050 km; 21,160 mi) from the center of Earth, with an uncertainty region of about 0.0000001 AU (15 km; 9.3 mi). It passed 27,743 kilometers (17,239 mi) above Earth's surface, closer than satellites in geosynchronous orbit. It briefly peaked at an apparent magnitude of roughly 7.2, a factor of a few fainter than would have been visible to the naked eye. The best observation location for the closest approach was Indonesia. Eastern Europe, Asia, and Australia also were well situated to observe Duende during its closest approach. Duende was not expected to pass any closer than 1950 km to any satellites. Goldstone Observatory observed Duende with radar from February 16 to February 20. Radar observations have shown it to be an elongated asteroid with dimensions of 20 by 40 meters (66 by 131 feet). This gives Duende a geometric mean (spherical) diameter equivalent to 28 meters (92 ft).\nDuring the close approach an observational campaign involving 5 different telescopes in 4 different observatories was carried on in order to get information on the physical properties of this NEO. Visible and near-Infrared photometry, and visible spectroscopy were obtained at Gran Telescopio Canarias, Telescopio Nazionale Galileo and Calar Alto Observatory and put together. The classification using the M4AST online tool says this is an L-type asteroid those peculiar asteroids are characterized by a strongly reddish spectrum shortward of 0.8 μm, and a featureless flat spectrum longward of this, with little or no concave-up curvature related to a 1 μm silicon absorption band. Time-series photometry was also obtained in the Observatorio de La Hita (I95) and Observatorio de Sierra Nevada during two consecutive nights (15–16 February 2013). All of this data were co-phased to build a light curve of the object. This light-curve is double-peak and presents large variations in magnitude, implying a very elongated object, which is compatible with radar observations. The amplitude of the light-curve yields an axial ratio that assuming a long axis of 40 m, as can be inferred from the radar images by Goldstone, results in an equivalent diameter of 18 m, much smaller than the estimations before the close-approach.\nThe rotational period was precisely determined from the light curve obtaining a value of 8.95 ± 0.08 h. This value is confirmed with an analysis of all the photometry of this objects reported to the Minor Planet Center. Using data pre and post close approach the authors find that the object suffered a spin-up during the event that decreased the rotational period from 9.8 ± 0.1 h down to 8.8 ± 0.1, which is compatible with the more accurate value estimated from the light-curve.\nThe close approach to Earth reduced the orbital period of Duende from 368 days to 317 days, and perturbed it from the Apollo class to the Aten class of near-Earth asteroids. Its next close approach to Earth will be on 15 February 2046 when it will pass about 0.0148 AU (2,210,000 km; 1,380,000 mi) from Earth. Based on 7 radar observations, the next close approach to Earth similar to the 2013 passage will be on 16 February 2123 when Duende will pass no closer than 0.0002 AU (30,000 km; 19,000 mi) from the center of Earth. For the 2123 passage, the nominal pass will be 0.003 AU (450,000 km; 280,000 mi) from the center of the Moon and then 0.005 AU (750,000 km; 460,000 mi) from the center of Earth.\nDuring closest approach to Earth in 2013 the orbital period of Duende was reduced from 366 days to 317 days. Its aphelion was reduced from 1.110 to 0.9917 AU, leaving it almost entirely inside Earth's orbit.\n|Longitude ascending node\n|Argument of perihelion\nRisk assessments calculated before the 2013 passage were based on a diameter of 45 meters and a mass of 130,000 metric tons. It was estimated that, if it were ever to impact Earth, it would enter the atmosphere at a speed of 12.7 km/s, would have a kinetic energy equivalent to 2.4 megatons of TNT, and would produce an air burst with the equivalent of 2.1 megatons of TNT at an altitude of roughly 10.1 kilometers (33,000 ft). The Tunguska event has been estimated at 3–20 megatons. Asteroids of approximately 50 meters in diameter are expected to impact Earth once every 1200 years or so. Asteroids larger than 35 meters across can pose a threat to a town or city. As a result of radar observations it is now known that Duende is only about 30 meters in diameter.\n- The uncertainty region of Duende during planetary encounters is now well determined through 2123.\n- Duende was removed from the Sentry Risk Table on 16 February 2013.\n- It is estimated that there are more than a million near-Earth asteroids smaller than 100 meters.\n|Diameter||Kinetic energy at atmospheric entry||Airburst energy||Airburst altitude||Average frequency|\n|30 m (98 ft)||708 kt||530 kt||16.1 km (53,000 ft)||185 years|\n|50 m (160 ft)||3.3 Mt||2.9 Mt||8.5 km (28,000 ft)||764 years|\n|70 m (230 ft)||9 Mt||8.5 Mt||3.4 km (11,000 ft)||1900 years|\n|85 m (279 ft)||16.1 Mt||15.6 Mt||0.435 km (1,430 ft)||3300 years|\nFor kinetic energy at atmospheric entry, 3.3 Mt is equivalent to DF-4, 9 Mt is equivalent to Ivy Mike and 15.6 Mt is equivalent to Castle Bravo. For airburst energy, 530 kt is equivalent to W88 and 2.9 Mt is equivalent to R-12 Dvina.\n- \"MPEC 2012-D51 : 2012 DA14\". IAU Minor Planet Center. 2012-02-24. Retrieved 2012-03-05. (K12D14A)\n- Paul Chodas and Don Yeomans (February 1, 2013). \"Asteroid 2012 DA14 To Pass Very Close to the Earth on February 15, 2013\". NASA/JPL Near-Earth Object Program Office. Retrieved 2013-02-01.\n- \"JPL Close-Approach Data: (2012 DA14)\" (2013-02-19 last obs (arc=362 days (Radar=7 obs); Uncertainty=0)). Retrieved 2013-02-19.\n- \"WayBack Machine archive from 25 Aug 2012\". Wayback Machine. 2012-08-25. Retrieved 2013-01-10.\n- Dr. Lance A. M. Benner (2013-01-13). \"2012 DA14 Goldstone Radar Observations Planning\". NASA/JPL Asteroid Radar Research. Retrieved 2013-01-15.\n- \"L. Johnson 2012 DA14 Update: radar images showing elongated object ~20x40m\". Minor Planet Center.\n- de Leon, J.; Ortiz, J. L.; Pinilla-Alonso, N.; Cabrera-Lavers, A.; Pinilla-Alonso, N.; Cabrera-Lavers, A.; Alvarez-Candal, A.; Morales, N.; Duffard, R.; Santos-Sanz, P.; Licandro, J.; Pérez-Romero, A.; Lorenzi, V.; Cikota, S.; et al. (2013). \"Visible and near-infrared observations of asteroid 2012 DA14 during its closest approach of February 15th, 2013\". Astronomy and Astrophysics 555: L2–L6. arXiv:1303.0554. Bibcode:2013A&A...555L...2D. doi:10.1051/0004-6361/201321373.\n- Bruce L. Gary (2013-02-18). \"Asteroid \"2012 DA14\" Rotation Light Curve\". Retrieved 2013-02-20.\n- \"2012 DA14 Ephemerides for 15 February 2013\". NEODyS (Near Earth Objects – Dynamic Site). Retrieved 2013-01-10.\n- Don Yeomans and Paul Chodas (March 1, 2013). \"Additional Details on the Large Fireball Event over Russia on Feb. 15, 2013\". NASA/JPL Near-Earth Object Program Office. Retrieved 2013-03-02.\n- \"Russia Meteor Not Linked to Asteroid Flyby\". NASA. Retrieved 15 February 2013.\n- \"Russian Asteroid Strike\". ESA.int. Retrieved 16 February 2013.\n- \"Earth remains safe for now—but what about next asteroid?\". tri-cityherald. Retrieved 2 March 2015.\n- JPL SBDB\n- Paul Chodas, Jon Giorgini & Don Yeomans (March 6, 2012). \"Near-Earth Asteroid 2012 DA14 to Miss Earth on February 15, 2013\". NASA/JPL Near-Earth Object Program Office. Retrieved 2012-03-06.\n- \"2012 DA14 Orbit\" (2013 01 09 (arc=321 days)). Minor Planet Center. Retrieved 2013-01-11.\n- \"Closest approaches of 2012 DA14 to known satellites – no encounter is closer than ~2000 km\". Jonathan's Space Report No. 674. 2013-02-10. Retrieved 2013-02-11.\n- SPACE DAILY\n- Phil Plait (2013-02-19). \"An Asteroid's Parting Shot\". Bad Astronomy blog. Retrieved 2013-02-19.\n- Horizons output. \"Horizon Online Ephemeris System\". Retrieved 2013-01-10. (\"Ephemeris Type: Elements\" PR value)\n- Robert Marcus, H. Jay Melosh, and Gareth Collins (2010). \"Earth Impact Effects Program\". Imperial College London / Purdue University. Retrieved 2013-02-09. (solution using 45 meters, 2600 kg/m3, 12.7 km/s, 45 degrees)\n- \"Sandia supercomputers offer new explanation of Tunguska disaster\". Sandia National Laboratories. 2007-12-17. Retrieved 2007-12-22.\n- \"Record Setting Asteroid Flyby\". NASA Science. Jan 28, 2013. Retrieved 2013-01-29.\n- Will Ferguson (January 22, 2013). \"Asteroid Hunter Gives an Update on the Threat of Near-Earth Objects\". Scientific American. Retrieved 2013-01-23.\n- \"Date/Time Removed\". NASA/JPL Near-Earth Object Program Office. Retrieved 2013-02-16.\n- \"WISE Revises Numbers of Asteroids Near Earth\". NASA/JPL. September 29, 2011. Retrieved 2012-03-06.\n|Wikimedia Commons has media related to 2012 DA14.|\n- Video: Fly by, Feb 15, 2013\n- Asteroid 2012 DA14 to Safely Pass Earth (JPLnews video Feb 4, 2013)\n- Guide to Asteroid 2012 DA14 Super Close Approach (Bruce Betts 2013/02/04 includes video)\n- Physical characteristics of 2012 DA14, nasa.gov\n- 2012 DA14 Earth Impact Risk Summary, nasa.gov\n- Real-time video of asteroid 2012 DA14 passing Earth (Clay Center Observatory, beginning 6 p.m. EST on Feb. 15th)\n- No, asteroid 2012 DA14 will not hit us next year, Bad Astronomy blog (Phil Plait, March 4, 2012)\n- Cool animation showing asteroid DA 14′s near miss next year, Bad Astronomy blog (Phil Plait, March 8, 2012)\n- 2012 DA14 sparks asteroid fever (Astro Bob, March 6, 2012)\n- Near-miss asteroid will return next year (ESA – 15 March 2012)\n- Discovery of 2012 DA14 (Jaime Nomen for OAM team – La Sagra Sky Survey), includes animated discovery images\n- 2012 DA14 orbit calculation with a 3 day observation arc (mpml : February 26, 2012)\n- Meteoroid hazard to satellites comes overwhelmingly from the marbles, not the mountains (mpml : March 26, 2012)\n- Table of next close approaches (Sormano Astronomical Observatory)\n- SAEL – Small Asteroid Encounter List (Sormano Astronomical Observatory)\n|Large NEO Earth close approach\n(inside the orbit of the Moon)\n15 February 2013\n(153814) 2001 WN5"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:4b1b8efe-d7d1-4f82-be67-397988802a2f>","<urn:uuid:7b193237-4b64-46bd-a2e6-518fac7eaf23>"],"error":null}
{"question":"I work in corporate finance and want to understand valuation approaches. How do fair value accounting methods determine asset values, and what considerations apply when classifying financial instruments between equity and debt?","answer":"Fair value accounting determines asset values through three levels: Level 1 uses quoted prices in active markets, Level 2 uses observable information for similar items, and Level 3 uses unobservable inputs when markets are non-existent. The valuation can use market, cost, or income approaches depending on the asset type. For classifying financial instruments, the key distinction is that debt requires the issuer to deliver cash or financial assets to the holder, while equity represents a residual interest in the entity's assets after deducting liabilities. The classification follows substance over form principle, with specific factors determining debt classification including redemption at the holder's option, limited instrument life, or non-discretionary dividends. Equity classification applies to instruments like ordinary shares where payments are at the issuer's discretion, or instruments convertible into a fixed number of ordinary shares.","context":["The approach of fair value accounting is the measurement of assets and liabilities at their current market value. The fair value is the amount of the asset that can be sold or liability paid for a fair value both for the buyer and the seller.\nThe best way to determine an asset’s fair value is through an exchange listing of security. Fair value accounting is also known as “mark-to-market,” one of the most commonly recognized standards of valuation when it is sold or assets are acquired.\nWhat this article covers:\n- How to Account for Fair value?\n- How do you value assets with the fair value accounting method?\n- Advantages of Fair Value Accounting\n- Fair value vs. historical cost accounting\nHow to Account for Fair value?\nFair value accounting uses current market values as the basis for recognizing certain assets and liabilities. Fair value is the estimated price at which an asset can be sold or a liability settled in an orderly transaction to a third party under current market conditions. This definition includes the following concepts:\n- Current market conditions: The derivation of fair value should be based on market conditions on the measurement date, rather than a transaction that occurred at some earlier date.\n- Intent: The intention of the holder of an asset or liability to continue to hold it is irrelevant to the measurement of fair value. Such intent might otherwise alter the measured fair value. For example, if the intent is to immediately sell an asset, this could be inferred to trigger a rushed sale, which may result in a lower sale price.\n- Orderly transaction: Fair value is to be derived based on an orderly transaction, which infers a transaction where there is no undue pressure to sell, as may be the case in a corporate liquidation.\n- Third party: Fair value is to be derived based on a presumed sale to an entity that is not a corporate insider or related in any way to the seller. Otherwise, a related-party transaction might skew the price paid.\nHow do you value assets with the fair value accounting method?\nAccording to IFRS 13 Fair Value Measurement, there are three levels of data that you can use to determine the value of an asset or liability. These are as follows:\nLevel 1 – The quoted price of identical items in an active market (a market where liabilities and assets are transacted frequently and at high volumes, giving ongoing pricing information).\nLevel 2 – Observable information for similar items in active or inactive markets, rather than quoted prices. For example, real estate in similar locations.\nLevel 3 – Unobservable inputs, only used when markets are non-existent or illiquid. Examples include your company’s own data, such as an internally generated financial forecast.\nRemember that these levels are only used to select your inputs to different valuation techniques, not to estimate the fair value of the assets themselves. There are a broad range of different valuation techniques that you may wish to make use of to make the actual valuation, including the market approach, the cost approach, or the income approach. These valuation techniques vary wildly, so the best technique to use for your company’s assets depends on the type of asset you hold.\nAdvantages of Fair Value Accounting\nFair value accounting measures the actual or estimated value of an asset. It is one of the most commonly used financial accounting methods because of its advantages, which include:\n1. Accuracy of valuation\nWith fair value accounting, valuations are more accurate, such that the valuations can follow when prices go up or down.\n2. True measure of income\nWith fair value accounting, it is the total asset value that reflects the actual income of a company. It doesn’t rely on a report of profits and losses but instead just looks at actual value.\n3. Adaptable to different types of assets\nSuch a method is able to make valuations across all types of assets, which is better than using historical cost value which may change through time.\n4. Helps businesses survive\nFair value accounting helps businesses survive during a financially difficult time because it allows asset reduction (or the act of declaring that the value of an asset that is included in a sale was overestimated).\nFair value vs. historical cost accounting\nThe differences between fair value accounting and historical cost accounting are stark. Essentially, historical cost accounting values assets and liabilities at the initial price they were exchanged for. In other words, it provides you with the cost of the asset. However, fair value accounting values assets at the prevailing market price. This means that it provides you with the expected return that an asset would fetch if you wanted to sell it.\nThere are a few other distinctions that are worth noting. Whereas fair value can be used to compare assets from different entities, the historical cost cannot (as different methods may have been adopted for depreciation). It’s also worth remembering that the fair value calculation is much more complex than historical cost and requires various assumptions. So, when it comes to fair value vs. historical cost accounting, both accounting methods have virtues, but to assess the current value of an asset, fair value accounting is a more appropriate option.\nIf you are looking for more helpful resources and guidance, then check out our resource hub. Companies, statement, statement, market price, risk, cash, loan.\n- WHAT IS ACCOUNT PAYABLE?\n- What is Liability in Accounting?\n- Cash vs. Accrual Accounting: What’s the Difference?\n- What Is an Accounting Journal? Definition of Journal in Accounting\n- What Is Management Accounting?\n- What Is A Ledger In Accounting?\n- What Is A Journal Entry In Accounting?\n- What Are the Generally Accepted Accounting Principles?\n- What Is Financial Accounting?\n- Why Is Accounting Important?\n- What is Equity in Accounting?\n- Specific Identification Accounting 101","The difference between debt and equity in an entity’s statement of financial position is not easy to distinguish for preparers of financial statements. Many financial instruments have both features, with the result that this can lead to inconsistency of reporting.\nThe International Accounting Standards Board (IASB) agreed with respondents from its public consultation on its agenda (December 2012 report) that it needs greater clarity in its definitions of assets and liabilities for debt instruments. This should therefore help eliminate some uncertainty when accounting for assets and financial liabilities or non-financial liabilities. The respondents felt that defining the nature of liabilities would advance the IASB’s thinking on distinguishing between financial instruments that should be classified as equity and those instruments that should be classified as liabilities.\nThe objective of IAS 32, Presentation, is to establish principles for presenting financial instruments as liabilities or equity and for offsetting financial assets and liabilities. The classification of a financial instrument by the issuer as either debt or equity can have a significant impact on the entity’s gearing ratio, reported earnings, and debt covenants. Equity classification can avoid such an impact, but may be perceived negatively if it is seen as diluting existing equity interests. The distinction between debt and equity is also relevant where an entity issues financial instruments to raise funds to settle a business combination using cash or as part consideration in a business combination. Understanding the nature of the classification rules and potential effects is critical for management and must be borne in mind when evaluating alternative financing options. Liability classification normally results in any payments being treated as interest and charged to earnings, which may affect the entity’s ability to pay dividends on its equity shares.\nThe key feature of debt is that the issuer is obliged to deliver either cash or another financial asset to the holder. The contractual obligation may arise from a requirement to repay principal or interest or dividends. Such a contractual obligation may be established explicitly or indirectly, but through the terms of the agreement. For example, a bond that requires the issuer to make interest payments and redeem the bond for cash is classified as debt. In contrast, equity is any contract that evidences a residual interest in the entity’s assets after deducting all of its liabilities. A financial instrument is an equity instrument only if the instrument includes no contractual obligation to deliver cash or another financial asset to another entity and if the instrument will or may be settled in the issuer’s own equity instruments.\nFor instance, ordinary shares, where all the payments are at the discretion of the issuer, are classified as equity of the issuer. The classification is not quite as simple as it seems. For example, preference shares required to be converted into a fixed number of ordinary shares on a fixed date or on the occurrence of an event certain to occur should be classified as equity.\nA contract is not an equity instrument solely because it may result in the receipt or delivery of the entity’s own equity instruments. The classification of this type of contract is dependent on whether there is variability in either the number of equity shares delivered or variability in the amount of cash or financial assets received. A contract that will be settled by the entity receiving or delivering a fixed number of its own equity instruments in exchange for a fixed amount of cash or another financial asset is an equity instrument. This has been called the ‘fixed for fixed’ requirement. However, if there is any variability in the amount of cash or own equity instruments that will be delivered or received, then such a contract is a financial asset or liability as applicable.\nFor example, where a contract requires the entity to deliver as many of the entity’s own equity instruments as are equal in value to a certain amount, the holder of the contract would be indifferent whether it received cash or shares to the value of that amount. Thus, this contract would be treated as debt.\nOther factors, which may result in an instrument being classified as debt, are:\n- is redemption at the option of the instrument holder\n- is there a limited life to the instrument\n- is redemption triggered by a future uncertain event that is beyond the control of both the holder and issuer of the instrument, and\n- are dividends non-discretionary.\nSimilarly, other factors, which may result in the instrument being classified as equity, are whether the shares are non-redeemable, whether there is no liquidation date or where the dividends are discretionary.\nThe classification of the financial instrument as either a liability or as equity is based on the principle of substance over form. Two exceptions from this principle are certain puttable instruments meeting specific criteria and certain obligations arising on liquidation. Some instruments have been structured with the intention of achieving particular tax, accounting or regulatory outcomes with the effect that their substance can be difficult to evaluate.\nThe entity must make the decision as to the classification of the instrument at the time that the instrument is initially recognised. The classification is not subsequently changed based on changed circumstances. For example, this means that a redeemable preference share where the holder can request redemption is accounted for as debt even though legally it may be a share of the issuer.\nIn determining whether a mandatorily redeemable preference share is a financial liability or an equity instrument, it is necessary to examine the particular contractual rights attached to the instrument’s principal and return elements. The critical feature that distinguishes a liability from an equity instrument is the fact that the issuer does not have an unconditional right to avoid delivering cash or another financial asset to settle a contractual obligation. Such a contractual obligation could be established explicitly or indirectly. However, the obligation must be established through the terms and conditions of the financial instrument. Economic necessity does not result in a financial liability being classified as a liability. Also a restriction on the ability of an entity to satisfy a contractual obligation, such as the company not having sufficient distributable profits or reserves, does not negate the entity’s contractual obligation.\nSome instruments are structured to contain elements of both a liability and equity in a single instrument. Such instruments – for example, bonds that are convertible into a fixed number of equity shares and carry interest – are accounted for as separate liability and equity components. Split accounting is used to measure the liability and the equity components upon initial recognition of the instrument. This method allocates the fair value of the consideration for the compound instrument into its liability and equity components. The fair value of the consideration in respect of the liability component is measured at the fair value of a similar liability that does not have any associated equity conversion option. The equity component is assigned the residual amount.\nIAS 32 requires an entity to offset a financial asset and financial liability in the statement of financial position only when the entity currently has a legally enforceable right of set-off and intends either to settle the asset and liability on a net basis or to realise the asset and settle the liability simultaneously. An amendment to IAS 32 has clarified that the right of set-off must not be contingent on a future event and must be immediately available. It also must be legally enforceable for all the parties in the normal course of business, as well as in the event of default, insolvency or bankruptcy. Netting agreements where the legal right of offset is only enforceable on the occurrence of some future event, such as default of a party, do not meet the offsetting requirements.\nRights issues can still be classified as equity when the price is denominated in a currency other than the entity’s functional currency. The price of the right is denominated in currencies other than the issuer’s functional currency when the entity is listed in more than one jurisdiction or is required to do so by law or regulation. A fixed price in a non-functional currency would normally fail the fixed number of shares for a fixed amount of cash requirement in IAS 32 to be treated as an equity instrument. As a result, it is treated as an exception in IAS 32.\nTwo measurement categories exist for financial liabilities: fair value through profit or loss (FVTPL) and amortised cost. Financial liabilities held for trading are measured at FVTPL, and all other financial liabilities are measured at amortised cost unless the fair value option is applied.\nThe IASB and US Financial Accounting Standards Board have been working on a project to replace IAS 32 and converge IFRS and US GAAP for a number of years. The ‘Financial instruments with characteristics of equity’ project (‘FICE’) resulted in a discussion paper in 2008, but has been put on hold.\nGraham Holt is an examiner for ACCA, and associate dean and head of the accounting, finance and economics department at Manchester Metropolitan University Business School"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:af34a8ee-f41f-4fd6-8dbd-5729db497f1c>","<urn:uuid:7987545f-cdc3-4202-a1ea-912319c5d9a5>"],"error":null}
{"question":"Can someone explain the key differences between personality assessments in psychometric batteries vs SHL personality tests? Really need to understand the technical distinctions here!","answer":"Personality assessments in psychometric batteries evaluate how a person deals with their environment, including their preferences, opinions, coping mechanisms, and ways of interacting with others. They also indicate potentially counter-productive behaviors based on specific roles. In contrast, SHL personality tests specifically assess 32 personality characteristics through a format where candidates must choose statements that best and worst align with their personality. Unlike standard right/wrong tests, SHL personality assessments focus on identifying characteristic traits to ensure job capability, with candidates selecting from multiple statements that could describe 'correct' behavior.","context":["Psychometric assessments are conducted through the use of a combination of instruments, generally referred to as an assessment battery. The most important factors that need to be considered for recruitment and development purposes include:\nPersonality provides information regarding a person’s way of dealing with the environment and includes preferences, opinions and coping. It is an indication of how the person relates to others, what styles of interaction and inclinations the person will use when interacting with others or with their environment in general. The personality profile may also be indicative of preferences and behaviour that could be considered as counter-productive depending on the particular role the individual finds him / herself in\nCognitive ability provides an indication of the person’s intellectual and problem solving abilities. Independent of personality, the cognitive ability gives an indication of how well a person can solve problems and deal with external influences, in order to achieve certain goals. Cognitive ability is an integral part of job functioning, providing the foundation on which the person’s job skills will be built\nAssessment Centres provide valuable information for management development andgive candidates opportunities to demonstrate behaviours and skills that are manifestly job related. The results of these evaluations are more readily accepted by candidates and by the individuals. Hence, the aim of assessment centre is to define critical success factors for specific positions and their systematic evaluation. The assessment centre approach involves the utilisation and combination of a variety of assessment tools to predict job success and developmental areas of candidates. Included are the following exercises: In-basket, Case Studies, Role play, Team briefing and counselling session. We offer the assessment centre as a more holistic approach in which it will provide an accurate predictor of behaviour knowledge and potential that is related to a variety of competencies. For example the behaviours related to task structuring, leadership, communications and interpersonal style can be observed in a team based simulation. Trained assessors observe behaviours and conduct assessments in this live setting and generate ratings based on standardised scoring templates\nTechnical Ability tests were developed specifically for the selection and allocation of Apprentice and Learnership candidates. They are suitable for educational levels below and above Matriculation. All evaluations include a practice phase, which ensures that the candidate understood the instructions and knows what to expect in the actual tests. The value addition is to predict future successful performance of a candidate in a technical work environment\nPsychomotor performance refers to a person’s ability to perceive information from the environment, to process the information (mentally or cognitively) and to act correctly after processing the information by using one’s limbs. These skills are required for all tasks where a person’s limbs are used, e.g. operating equipment, driving a vehicle or working on a continuous production line. The Vienna/ Dover Test System originated in Austria in 1978 and have been used in South-Africa since the early 1980s. Research done indicates that drivers and operators world-wide use the same basic universal skills to operate vehicles and equipment. It has proven a valuable tool for identifying ability to perform driving/equipment operating jobs as well as profiling specific weaknesses and trainability.\nA range of our instruments is displayed below:\nThe results of psychometric assessments are provided by means of an integrated report that addresses the specific purpose of the assessment and includes specific recommendations to aid decisions. Candidate data and integrated reports are managed in accordance with client specifications as well as guidelines set by the Health Professions Act and Health Professions Council of South Africa.\nJob Competence Profiling\nThe determinant for assessment is often the establishment of a competency profile for the position(s) being considered. This is done through a consultative process and the application of a questionnaire regarding the requirements of the environment and position. Human Capital Management has therefore developed the ability to conduct job competency profiling within the client environment based on specific job families and related categories, depending on the client organisation complexity and industry, for instance:\nTechnical – Those job families that have Technical/Engineering-specific qualification and experience at their core, whether that is development, management or maintenance. Technical tasks can be performed as an internal or external service and can be related to product development or service delivery\nService Delivery – Those job families where direct contact and service to the client forms the core. The majority of tasks could be provided on the client site and individuals may not be bound to the client offices\nManagement – Individuals functioning in the middle to senior level of the organisation where longer term decisions are made and where clear guidelines may be vague. Managers are required to establish or improve organisational processes and performance\nSupport – Those positions that relate to the crucial processes that need to be implemented in order to ultimately provide a service to clients.\nHCM understands that clients may already have or could be assisted to develop:\n- Job Description (what must be done in this function/role/task/job?)\n- Competency profile (what does the environment or position require the incumbent to be/know/do to perform well in this position)\n- Performance Contracts (What would the position require the incumbent to agree to do?)\n- Measurement criteria (How to measure whether the incumbent performed well in this position)\nWe believe that an individual can only perform optimally if the competency profile reflects the complete functioning within the requirements of the position.\nThe figure below provides a schematic description of the process followed to design a competency profile. This interactive process requires the input from our specialists along with management participation. We follow a consultative process with subject matter experts and individuals to whom the profile applies.","What is an SHL Test?\nSHL is a leading brand of psychometric tests which is employed by a wide range of companies as a part of the recruitment process, particularly in graduate recruitment campaigns where a certain level of competence is required to progress through the selection process. They are widely used for their efficiency and cost-effectiveness in testing for a minimum level of technical ability within a specific role. The various types of SHL assessment are intended to measure and assess an individual’s level of ability in a given competency relevant to the position for which the test is being administered.\nHow Do SHL Tests Work?\nThe SHL assessment process usually consists of one or two stages.\nIn the latter instance, known as a Verify test, a candidate first electronically completes an unsupervised SHL test, and then a short supervised exam is carried out for the purpose of verifying results.\nIn the former instance, known as a Management and Graduate Item Bank (MGIB) test, the entire examination is supervised in an assessment/testing center, either online or on paper.\nThe test itself comprises of a number of questions to be answered within a challenging time limit. This is intended to maximize pressure as the purpose of the test is to gauge a candidate’s true potential. While the questions tend to be displayed one at a time, one can move back and forth through questions as desired and answer in any order they see fit.\nSHL tests are a means of estimating one’s maximum ability level. The application of difficult questions within a challenging time limit effectively gauges the potential of a candidate and then compares it to the average level of a ‘norm’ group, which is usually made up of individuals from a similar demographic in factors such as age, nationality and education level.\nThe candidate’s ability is calculated against the norm group as a benchmark, and then a predefined cut-off point is applied to check that results meet the minimum ability requirements for the job role/department function\nWhat Kind of SHL Tests Are There?\nThere is a wide range of SHL tests available to employers for the assessment of candidates at a variety of levels. The test administered is chosen to match the job level and field in which the candidate is applying. Some jobs may require the undertaking of an SHL verbal reasoning test, while others might involve an SHL numerical reasoning test; it is entirely contingent upon the nature of the job. It is also quite common for employers to administer a combination of tests in order to assess multiple attributes.\nThe available range of SHL tests are formulated for the assessment of four main categories:Aptitude, Behavior, Personality and Situational Judgment:\nAptitude: Also known as reasoning or cognitive ability tests. These include but are not limited to SHL numerical tests, verbal reasoning tests and inductive reasoning tests. These tests are intended to measure a candidate’s understanding and comprehension of a variety of challenging stimuli. These tests are, essentially, an assessment of one’s logical capacity in a variety of applications.\nBehavioral: Frequently utilized in the hiring process, these tests are used to gain an insight into a candidate’s professional attributes and how well they fit the core values of the company, as well as how well one would handle the responsibilities of the position being applied for and unexpected situations which might arise in said position.\nPersonality: SHL personality tests assess 32 personality characteristics. In this form of test, a number of statements are provided, and the candidate is required to choose which statements align best and which align worst with their own personality. In this instance, there is no binary right or wrong response; multiple statements may appear to describe the ‘correct’ behavior. This is intended to pinpoint the characteristic traits displayed by the candidate and ensure that they are capable of handling the job.\nSituational Judgment: These tests are used as a metric of a candidate’s cognitive and behavioral abilities in applied scenarios; realistic situations relevant to the job role are presented hypothetically so that the candidate responds spontaneously based on intuition. While one would, of course, take the time to consider decisions more carefully before execution, situational judgment tests are a powerful tool for assessing the manner in which a candidate would handle challenges which might arise in the workplace. It is highly recommended that one familiarizes oneself with the types of situations that might be presented in this assessment in order to be able to consistently select the most appropriate answer.\nHow Can I Prepare For an SHL Test?\nSHL Tests are intended as a metric of your maximum level of performance. As the questions are difficult and the time limit is challenging, you need to be able to work quickly and accurately.\nThe key to this is that you put yourself in a position where you understand the format of the test, the manner of questions to expect and the conditions under which the test will be administered. By familiarising yourself with these core elements of the test you will be in an optimal position for success in these tests. The key to success is practice. If you need to take an SHL test, it is recommended that you take a number of SHL practice tests so that you may be more confident and comfortable in approaching the assessment.\nWhile SHL tests cover a wide range of assessments, the best means of preparation is invariably practice. If you are required to take an SHL test for a prospective job, make sure that you take the time to familiarize yourself with the format of the test so that you may be appropriately prepared for the conditions under which the test is administered and maximize your performance."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:e36f5681-0154-4a6a-b696-47d2c119a408>","<urn:uuid:dcacfa8f-4979-46da-b117-6c4caaf4a2ea>"],"error":null}