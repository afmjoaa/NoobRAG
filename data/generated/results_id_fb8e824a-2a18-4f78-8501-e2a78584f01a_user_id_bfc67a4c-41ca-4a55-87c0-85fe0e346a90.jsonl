{"question":"Are both The Telepathy Project and Spyros Syrmos' works internationally recognized? Tell me about their global presence","answer":"Yes, both have received international recognition. Spyros Syrmos' work has been performed in London (The Blank Canvas at King's Head Theatre) and the US (film 'Bright' at Louisville's International Festival of Film). The Telepathy Project (Sean Peoples and Veronica Kent) has presented their work more extensively internationally, with projects shown in multiple countries including USA, Germany, Spain, France, and India, through various exhibitions, festivals, and collaborative projects.","context":["Spyros Syrmos is an award – winning Composer based in Athens. His work include orchestral and chamber music, opera, film music, ballet, sound art installations and traditional music arrangements from south Italy.\nSpyros was born in Athens.\nHe studied guitar with Kostas Grigoreas at the National Conservatory of Athens and with John Mills and Eleftheria Kotzia at the Royal Welsh College of Music and Drama, where he gained the PGdip in performance.\nHe also had Lute lessons with Nikos Panagiotidis in Athens and Mathew Nisbet in Wales as a second study.\nHe continued studies in composition with Ioannis Metallinos in Athens and Peter Reynolds at the contemporary music departement of the Royal Welsh College of Music and Drama in Cardiff, where he gained the PGdip in Composition.\nOperatic Novices make their mark\nOPERA – “The Blank Canvas”\n“He won the “Flourish Opera competition 2013”.\nPremiered at the King’s Head Theatre in London in 31st August 2014.\nFILM MUSIC – “Bright”\nIn 2021 he completed the score for the film “Bright” directed by Samuel R. Gibson and produced by Electradome Pictures in the US.\nLondon Shorts Film Festival Nominee “Best Short Drama“\nBright will release November, 11th 2021 exclusively at Louisville’s International Festival of Film\n“Evocative and unsettling” The Guardian\nTRADITIONAL MUSIC FROM SOUTH ITALY\nAfter many years of studying the music of South Italy, under the motivation and supervision of his close friend Vangelis Haramis (Sociologist/musician/Universtita di Salerno) he has arranged a large number of traditional songs and music, giving them a “nowadays” perspective as well as he has composed several pieces inspired by this music. The group ANTIDOTUM TARANTULAE of which Spyros was a principal member, had performed often at festivals and music theaters in Greece.\nReviews – Interviews – Links\nSpyros Syrmos: Opera, the meeting point of all the arts\nRead Full Review ” operatic novices make their mark “\nΣπύρος Σύρμος, ο Ελληνας που έγραψε νέα όπερα για το Λονδίνο\nΣυνέντευξη στη Βασιλική Χρυσοστομίδου\nLIST OF WORKS\n3 Inventions (for piano solo)\nA Brawny Tree (for reed quintet)\nFast Forward (for string quartet)\nHabanera (for clarinet quartet)\nHabanera (for piano trio)\nTres La Lune… (for piano, violoncello and flute)\nThe Adventure of an Exhibit (for double bass, bassoon and violoncello)\nK.P.I.S.N. (for two violins and violoncello)\nMilonga para un Amante Uruguayo (for piano and flute)\nMid Winter’s Summerdream (for violonello and piano)\nHonoris Causa (for violoncello and piano)\nPersistent Galiard (for violoncello and harpsichord)\nSoneto de separacao (for voice and piano) Poetry by Vinicius de Moraes\nΒάο, Γάο, Δάο (for voice and piano) Poetry by Napoleon Lapathiotis\nTrilce (for voice and guitar) Poetry by Cesar Vallejo\nNight Flight – In the Time Zone\nThe Road from Colonus\nThe Blank Canvas (libretto by Fay Wrixon)\nBright (directed by Samuel R. Gibson)\nBallet Camouflage (for violoncello solo and electronics)\nWaving Trees (electroacoustic)\nBeyond Everything (electroacoustic)\n3 Διηγήματα (3 short stories, text by Vania Syrmou)\nAntidotum Tarantulae – Sed Nove (traditional music from south Italy)\n“Every so often you uncover a gem like this\nwhich may actually play a part in securing opera’ s future”\nTO CONTACT A&R REPRESENTATIVE\nGözde Sezgin (A&R Representative)\nNo 24 Sariyer st.\nIstanbul – Turkey\nPlease fill in the form below :\nTO CONTACT LEGAL AGENT\nMargarita Giannopoulou (Legal agent)\n6 Phidiou st.\nAthens – Greece\nPlease fill in the form below :\nTo contact Spyros\nPlease fill in the form below:","Australian based artists Sean Peoples and Veronica Kent are engaged in a critical practice based on the possibilities of alternate forms of communication. Telepathy and dreams serve as extended metaphors and working method through which they explore alternate ways of being, communicating and collaborating, and act as the premise for the setting up of encounters that test and provoke such relationships. Their projects have been presented in private and public galleries and festivals in Australia, USA, Germany, Spain, France and India. These projects have often been realised collaboratively both between each other and with other artist collectives such as A Constructed World and with the general public, students and exhibition audiences taking forms such as; musicals, dream recitals, remote international drawing projects, epic sleepovers, telepathically curated exhibitions, collaborative paintings and institutional interventions.\nMarch 6, 1986. Born Melbourne, Australia\n2007 Diploma of Education. University of Melbourne\n2004-2006 Bachelor of Fine Art/Painting, Victorian College of the Arts.\nSean’s website seanpeoples.com\nJuly 7, 1970. Born Sydney, Australia.\n2012 PhD, University of Melbourne.\n2004- 2007 Bachelor of Fine Arts with First Class Honours. University of Melbourne\nVeronica’s website veronicakent.com\nAWARDS, GRANTS and RESIDENCIES\n2016 Australia Council, Arts Project Grant\n2014-2016 & 2012-2014 Gertrude Contemporary Studio Artist\n2014 New Initiatives, the Faculty of VCA & MCM and Department of Premier and Cabinet\n2014 Artbank/Gertrude Contemporary New Work commission\n2014 Australia Council, Experimental Art grant (as part of The Unconscious Collective)\n2013 Last Ship Residency, Mumbai India\n2011 Australia Council for the Arts, Barcelona Residency\n2010 Bundanon Trust Residency\n2008 Arts Victoria Presentation Grant\n2008 City of Melbourne ‘Young Artists’ Grant\n2007 Janet Holmes a’ Court Grant\nZzzzz: Sleep, Somnambulism, Madness, MAMA Murray Art Museum, Albury\nZzzzz: Sleep, Somnambulism, Madness, Gertrude Contemporary/Melbourne International Festival.\nThe Telepathy Project, Melbourne Art Book Fair, NGV, Melbourne\nA Meeting Of Waters, Shepparton Art Museum, Victoria.\nAngels and Dogs, Room 301 for Spring 1883.\nTom & Eva, From the Collection: Chapter One, Warrnambool Art Gallery + Gertrude Contemporary, Victoria, Australia.\nPlein Air Dreaming, Artbank/Gertrude Contemporary New Work commission, Australia.\nPrimavera 2014 Museum of Contemporary Art, Sydney, Australia.\nMotel Dreaming Dark Mofo, Hobart, Australia.\nDream Recital White Nights, National Gallery of Victoria (Fed Square), Australia.\nReading Solaris to the Great Moorool, Tarrawarra Biennale (Whisper in my Mask) TarraWarra Museum of Art, Healesville, Australia.\nSinging Solaris to the Great Moorool. Melbourne International Festival, Australia.\nStar Pavilion, MPavilion, Melbourne, Australia.\nFramework 1: extrasensory bonds insitu, Berlin, Germany\nDreaming The Collection Melbourne Now, National Gallery of Victoria, Australia.\nDream Studio Gertrude Contemporary, Vic, Australia\nDream Paintings Studio 12, Gertrude Contemporary, Vic, Australia\nThe Telepathy Project, Remote Space, Barcelona, Spain.\nDreaming The Arabian Sea (en)counters festival, Mumbai, India\nThe Telepathy Project Margaret Lawrence Gallery, Melbourne Australia.\nTelepathy and Love, Australia Council for the Arts Studio and Apartment, Barcelona, Spain\nSpeech Objects Musée de l’Objet, Blois, France\nTelepathy and Love, West Space, Melbourne Australia.\nSpeech and What Archive,Y3K Gallery, Melbourne Australia\nOnce More With Feeling, VCA Margaret Laurence Gallery,curated by Meredith Turnbull, Vic, Australia.\nTransformation, Moving Galleries/Creative Spaces, Melbourne, Aus.\nThe Telepathy Project, for the 2008 Next Wave Festival. Forum Theatre, Vic.\nElectromagnetic Imaginary: Redux, Spectrum Project Space, WA, Aus\nYou Are Probably My Favourite Person In The Whole World, curated by Hooray For Humans, The Academy, Long Beach California, USA.\nProvisional Investigations,curated by Melanie Irwin, George Paton Gallery, Vic. Aus.\nElectromagnetic Imaginary, AREA Contemporary Art Space, Vic, Aus.\nTelepathy Lecture, Federation Hall, VCA , Vic. Aus.\nWOOD : The Telepathy Project Group Show, IIBL , Melbourne, Australia\nAll Night Telepathy “My Reality” CUBE37, curated by Amanda Slacksmith, Frankston Arts Centre, Vic. Aus.\nEVENTS WITH A CONSTRUCTED WORLD\n2012The Speakeasy Medicine Show Ian Potter Museum, Melbourne\n2010 Fragments In A Constructed World Plausible Artworlds, Basekamp, Philadelphia, USA.\n2010 The Speakeasy Medicine Show, Belleville Biennale, Paris, France.\n2010 Speech and What Archive,Y3K Gallery, Melbourne Australia, replayed for Explaining Contemporary Art to Live Eels #6, Double Blind,Villa Arson for A Constructed World. Nice, France.\n2008 Explaining Contemporary Art To Live Eels,in collaboration with ‘Superflou’ as part of the A Constructed World exhibition ‘Saisons Increase’, CAPC Museum of Contemporary Art, Bordeaux, France.\n2007 Explaining Contemporary Art To Live Eels,(in collaboration with A Constructed World) Increase Your Uncertainty, Australian Centre for Contemporary Art, Melbourne, Australia\nJeff Khan, ‘Big, Intimate and Fearless’, REALTIME 84, 2008\nFiona Scott-Norman, ‘Do You See What I See’, THE AGE, May 16, 2008\nAnneke Jaspers (ed.),‘The Telepathy Project’, RUNWAY #11, 2008\nLeon Goh, ‘The Telepathy Project’ Text Camp Reader, 2008\nTony Reck, ‘Exotic Intimacies’, REALTIME 86, 2008\nMeredith Turnbul, The Telepathy Project, SPEECH, 2008\nPenny Modra, ‘In the frame’, THE AGE, 2008\nDylan Rainforth/Ann Marsh/Meredith Turnbull, ‘Once More With Feeling’,ExCat. 2009\nMelissa Loughnan, ‘Skeptics V’s Mystics’ RUNWAY #14, 2009\nLauren Bliss,’A Love Story’ DISCIPLINE #3, 2013\nAndrew Stephens,The AGE,The Telepathy Project: to sleep perchance to dream, to share the dreams that come February 16, 2014\nJohn Neylon, THE MELBOURNE REVIEW, In The Belly Of The Beast February 2014\nAndrew Stephens,The AGE, Artists tap into hidden histories, http://www.theage.com.au/entertainment/artists-tap-into-hidden-histories-20140805-1004za.html 2014\nGina Fairley, Arts Hub, What goes around comes around at Primavera 2014 http://visual.artshub.com.au/news-article/news/visual-arts/what-goes-around-comes-around-at-primavera-245968\n*All works made collaboratively by Veronica Kent and Sean Peoples"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:40f32cae-a449-4bb7-965d-f6405cf7c555>","<urn:uuid:8ef6a544-bbf5-455c-8828-71527e325891>"],"error":null}
{"question":"How do the initial symptoms and diagnostic challenges compare between Kawasaki disease and Takayasu arteritis?","answer":"Kawasaki disease begins with a high fever up to 104 degrees F that lasts five or more days and is resistant to usual doses of acetaminophen or ibuprofen. Its diagnosis can be challenging as there is no single diagnostic test, and symptoms can resemble common conditions like strep throat and influenza. Takayasu arteritis, on the other hand, is characterized by an early inflammatory phase with symptoms like fever, arthralgias, and weight loss, followed by a late occlusive or pulseless phase. Its diagnosis is based on clinical findings like decreased or absent pulses, unequal limb blood pressure, and bruits, though modern imaging techniques can detect activity before vascular compromise develops.","context":["Kumbhar P*, Honavar P**, Samant PY***\n(* Junior Resident, ** Assistant professor, *** Additional Professor, Department of Obstetrics and Gynecology, Seth G S Medical College and KEM hospital, Mumbai, India.)\nTakayasu arteritis (TA) affects aorta and its branches, sometimes coronary and pulmonary arteries. TA is a chronic process progressive in nature with underlying inflammation of affected arteries. We report two cases with this multivessel arteritis, one of them with a valvular involvement; and their management in pregnancy.\nTA is a large vessel vasculitis, primarily involving aorta and branches in the head, neck, upper and lower limbs. As the disease is more common in women of child-bearing age, adverse pregnancy outcome is a common phenomenon. Hence such pregnancies need multidisciplinary management. Here are two cases with aortoarteritis with successful antepartum and intrapartum management.\nA thirty year old primigravida came for antenatal registration in late first trimester. She was a known case of aortoarteritis, which was diagnosed 15 years ago when she had acute left sided chest pain and was detected to have high blood pressure of 160/100 mm of Hg. Computerized tomographic aortogram at that time showed left subclavian and renal artery stenosis, and left coronary artery disease. (Figure 1) She was advised coronary artery bypass grafting, but opted for medical management, and was discharged on prednisolone, atenolol, prazosin and aspirin. She took the medications for one month and stopped. Since her symptoms never recurred, she never followed up with the physicians.\nFigure 1. CT Angiogram with arrows showing areas of arterial narrowing.\nOn examination, her vital parameters were normal except blood pressure of 140/90 mm Hg in the left upper arm and 120/80 mmHg in the right. All pulsations were well felt. In sitting positon, left sided neck pulsations were prominent. Her obstetric examination revealed uterine size corresponding to the period of gestation. Hemoglobin was 11 gm % and urine albumin was absent. Her aCLA IgG/IgM, anti- β 2 glycoprotein antibodies, ANA and anti-ds DNA were all negative. C Reactive Protein, Erythrocyte Sedimentation Rate and C3/C4 levels were all normal.\nEchocardiography showed ejection fraction of 60% with trivial mitral regurgitation. Subclavian artery doppler showed narrowing of left subclavian artery, but normal flow in all four limbs. She was then started on tablet prednisone 30 mg once a day with tapering doses, tablet labetalol 100 mg twice a day and nifedipine 20 mg once a day in consultation with cardiologist and rheumatologist. Blood pressure in left limb remained in the range of 150 to 140 / 90 mm Hg and in right limb 130 to 120/ 80 mm Hg. Her antenatal course was uneventful. At term, weekly non-stress tests were done and were reactive. Anesthetic review was done for epidural analgesia. In view of her stable condition, vaginal delivery was planned after counseling of the patient and her relatives. However, at term she went into spontaneous labor but cesarean section had to be done for brow presentation under epidural and spinal anesthesia. Postoperative course was uneventful, and anti-hypertensives were continued post-delivery. She was discharged on the fifth day, and she chose barrier method of contraception. She was asked to follow up with cardiologists and rheumatologists regularly.\nA twenty four year old primigravida, a diagnosed case of aortoarteritis, came for antenatal registration at 10 weeks of gestation. She was on regular follow up with nephrologist after being diagnosed with renal hypertension a year before. She had no history of chest pain, limb claudication or syncopal attacks. Systolic blood pressure difference in right and left arm was 20 mmHg. Systolic murmur was heard, and echocardiography showed rheumatic heart disease with moderate mitral regurgitation, mild tricuspid regurgitation, mild pulmonary hypertension and good biventricular function. Her hemoglobin was 12 gm % and urine albumin was absent. Liver and renal functions were normal with creatinine of 1.1 mg/ dl, 24 hr urine protein 267 mg, C3 and C4 were 95.6 and 26.7 respectively. Her aCLA, beta 2 gycoprotein, ANA and ds DNA were negative. However, lupus anticoagulant was weakly positive. Angiography done in non-pregnant state had shown infraceliac abdominal aortic narrowing upto infrarenal aorta, 5 cm in extent. Left renal artery was narrow with multiple tortuous collaterals. Rest of the aorta (aortic root, arch of aorta, thoracic aorta and infrarenal aorta) was normal.\nShe was started on tab metaprolol 50 mg once a day, tablet aspirin 75 mg once a day and tablet labetalol 100 mg thrice a day. Later tablet alpha methyl dopa 250 mg thrice a day was added as the diastolic BP of both upper limbs rose to 110 mm Hg. Renal doppler study showed parvus tardus waveform in segmental and hilar renal arteries confirming bilateral renal artery stenosis.\nHer TSH was 15.86 U/ ml, for which tablet levothyroxine 75 mcg per day was started. Her antepartum course was uneventful. Obstetric sonography showed normal fetal growth with normal uterine artery doppler flow. Weekly non-stress tests were done and were reactive. She went into spontaneous labor at 39 weeks. Cesarean section was performed for breech presentation under epidural anesthesia. She delivered a female child of 2.67 kg with Apgar score of 9/10. Her postnatal period was uneventful. She was discharged on the sixth day with oral medications. She also chose barrier method of contraception and was asked to follow up with nephrologists regularly.\nTakayasu’s arteritis is an early onset granulomatous aortoarteritis with aortic inflammation also known as 'pulseless disease'. It leads to proximal occlusion and/ or aneurysms of aorta, carotid, subclavian, pulmonary, iliac, and renal arteries. Here, one patient had subclavian artery involvement and the other patient had renal artery involvement. Women in their second and third decades of life are predominantly affected. It is more common in the southeast Asian and Indian subcontinent regions than in the western population. The disease is characterized by an early inflammatory phase, in which there may be fever, arthralgias, weight loss, and a late occlusive or a pulseless phase. Its etiology remains primarily idiopathic. However, both our patients presented at a time when the diagnosis had been achieved prior.\nThe first patient in this report was labeled as type V TA and the second as type IV TA as per the angiographic classification of Takayasu Arteritis. As proposed by Ishikawa et al, the disease can be classified into groups on basis of complications such as hypertension, retinopathy, aneurysms, and aortic insufficiency.[1,2] According to this classification, the first patient belonged to group II a and the 2nd to group III. Pregnancy may not affect disease progression but TA affects pregnancy in several ways like preterm labor, preeclampsia, intrauterine fetal growth restriction/ demise, and abruption. In a study by Gatto et al, 51.7% fetal growth restriction was reported. Other complications of TA are retinopathy, secondary hypertension, aortic regurgitation, and aneurysms. The disease process also causes reduction in elasticity and narrows arteries, impairs aortic and carotid baroreceptors function. Though diagnosis is based on clinical findings like decreased or absent pulses, unequal limb blood pressure and bruits, magnetic resonance angiography and fluoro-deoxy-glucose positron emission tomography can detect TA activity before the development of vascular compromise.\nIn our second patient, TA was associated with rheumatic heart disease (RHD). Association of RHD and TA is rare. Though aortic valve involvement is more common, mitral and tricuspid valves may be involved in upto 20% without thickening. In a large autopsy study, there were very few cases on mitral involvement. Aortic valve involvement was more common. Management of TA involves a multidisciplinary approach aimed at mitigation of inflammation, treatment of complications, correction of stenotic lesions or even angioplasty when required.\nPreconceptional counseling includes dose adjustment or stopping cytotoxic drugs, and starting folic acid. Ideal time of pregnancy is during remission. Both our patients were in remission during conception. Early registration in tertiary care centre, regular antenatal visits with blood pressure monitoring, renal and cardiac monitoring and preeclampsia screening are vital in these patients. In our two cases, renal and cardiac involvement were present, and regular follow up with the specialists helped.\nFetal surveillance is as crucial as tests for maternal well being. Our patients were monitored for fetal well being regularly using NST and obstetric sonography, which were normal at all stages. Though the preferred mode of delivery is vaginal, in stages IIb and III, cesarean section is preferred to avoid cardiac decompensation due to increased blood volume and blood pressure. Neuraxial blockade is preferred for analgesia in labor. Both our patients underwent cesarean section, for obstetric indications.\nPregnancy with TA presents a challenge. Successful pregnancy outcome is possible with meticulous interdisciplinary management.\n- Ishikawa K, Maetani S. Long-term outcome for 120 Japanese patients with Takayasu's disease: clinical and statistical analyses of related prognostic factors. Circulation. 1994; 90(4): 1855–60.\n- Johnston S, Lock R, Gompels MM. Takasayu arteritis: a review. J Clin Pathol. 2002; 55(7): 481–6.\n- Gatto M, Iaccarino L, Canova M, Zen M, Nalotto L, Ramonda R, et al. Pregnancy and vasculitis: a systematic review of the literature. Autoimmun Rev. 2012 May;11(6-7): A447-59.\n- Leal P, Silveira F, Sadatsune E, Clivatti J, Yamashita A. Takayasus’s Arteritis in Pregnancy. Case Report and Literature Review. Revista Brasileira de Anestesiologia. 2011; 61(4): 479-85.\n- Andrews J, Al-Nahhas A, Pennell DJ, Hossain MS, Davies KA, Haskard DO. Non-invasive imaging in the diagnosis and management of Takayasu’s arteritis. Ann Rheum Dis 2004; 63(8): 995–1000.\n- Gormezano N, Dos Santos M, Okuda E, Catani, L Sacchetti S. Association between rheumatic fever and Takayasu’s arteritis. Revista Brasileira de Reumatologia. 2016; 56(2): 178-80\n- Kinare SG. Cardiac lesions in non-specific aorto-arteritis. An autopsy study. Indian Heart J. 1994; 46(2): 65-9.\n- Marwah S, Rajput M, Mohindra R, Gaikwad HS, Sharma M, Topden SR. Takayasu’s Arteritis in Pregnancy: A Rare Case Report from a Tertiary Care Infirmary in India. Case Reports in Obstetrics and Gynecology. 2017:2403451. doi: 10.1155/2017/2403451.\n- Papandony M, Brady S, Aw T. Vasculitis or fibromuscular dysplasia? Medical Journal of Australia, 2015; 202(2):100-1.\n- Henderson K, Fludder P. Epidural anaesthesia for caesarean section in a patient with severe takayasu’s disease. Br J Anaesth, 1999; 83:956-9.\nKumbhar P, Honavar P, Samant PY. Aortoarteritis In Pregnancy: Two Cases. JPGO 2018. Volume 5 No.9. Available from: http://www.jpgo.org/2018/09/aortoarteritis-in-pregnancy-two-cases.html","See what questions\na doctor would ask.\nKawasaki disease is a rare disease that occurs in children and leads to vasculitis, in which there is an inflammation of the blood vessels of the body. This blood vessel inflammation can result in many manifestations in different organs and body systems and can lead to serious complications, such as aneurysms and heart attack.\nKawasaki disease is also known as mucocutaneous lymph node syndrome, lymph node syndrome, and Kawasaki syndrome. Kawasaki disease has no known cause, but it is suspected to be an autoimmune disease, in which the body's immune system mistakes healthy cells of the body as dangerous invaders and attacks them.\nKawasaki disease is most common in Japan. In the U.S., Kawasaki disease is the second most common cause of heart disease in children. Kawasaki disease most frequently occurs in children under five and in boys.\nKawasaki disease begins with a high fever, up to 104 degrees F, that lasts five or more days. Unlike fevers due to many other causes, the fever associated with Kawasaki disease does not come down with usual doses of acetaminophen (Tylenol) or ibuprofen (Advil). In addition to vasculitis, Kawasaki disease can result in a variety of symptoms many parts of the body. The most common sites of symptoms include the mucus membranes and lymph nodes. For more information on symptoms, refer to symptoms of Kawasaki disease.\nMaking a diagnosis of Kawasaki disease begins with taking a thorough medical history, including symptoms and fever history, and completing a physical examination. There is no single test that can diagnose Kawasaki disease. A history of a high and persistent fever combined with other classic symptoms is generally used to make a diagnosis.\nA variety of tests may be done to rule out other diseases that can cause fever. These can include a blood culture, urinanalysis and urine culture and sensitivity, and throat culture and sensitivity. Other tests that may be done to evaluate any potential complications of Kawasaki disease, such as heart disease, include a chest X-ray, C-reactive protein, echocardiogram and electrocardiogram.\nIt is possible that a diagnosis of Kawasaki disease can be missed or delayed because there is no specific diagnostic test for it, and because some symptoms, such as fever, vomiting, diarrhea and cough can resemble symptoms of other more common diseases, such as strep throat and influenza. For more information on misdiagnosis, refer to misdiagnosis of Kawasaki disease.\nIt is vital that children with Kawasaki disease get treated as soon as possible to prevent the development of serious, life-threatening complications, such as aneurysms and heart attack. Even with early recognition and treatment, about one quarter of children with Kawasaki disease develop complications with the coronary arteries. Treatment of Kawasaki disease includes hospitalization, close monitoring, medications and ongoing medical care and screenings for heart disease. For more information on treatment, refer to treatment of Kawasaki disease....more »\nA diagnosis of Kawasaki disease may be overlooked, delayed or missed in the U.S. because it is rarer in the U.S. than in Japan. Some symptoms may also initially be attributed to more common conditions. Symptoms, such as fever, nausea, vomiting, cough and joint pain are similar to symptoms of more common conditions, such as influenza, upper respiratory infection, strep throat, or ...more misdiagnosis »\nThe following medical conditions are some of the possible\ncauses of Kawasaki disease.\nThere are likely to be other possible causes, so ask your doctor\nabout your symptoms.\n» Review Causes of Kawasaki disease: Causes\nEarly recognition and treatment of Kawasaki disease is critical to minimizing the chances of develop serious, even life-threatening, complications, such as aneurysms and heart attack. Even with early recognition and treatment, about one quarter of children with Kawasaki disease develop complications with the coronary arteries. However, death only occurs in about 1% of children with ...Kawasaki disease Treatments\nSome of the possible treatments listed in sources for treatment of Kawasaki disease may include:\nReview further information on Kawasaki disease Treatments.\nResearch the causes of related medical symptoms such as:\nSystemic disease primarily of infants and young children, characterized by skin rash, swelling of hands and feet, enlarged cervical lymph nodes, \"strawberry tongue\", dry and cracked lips, high fevers, and coronary artery disease.\n- (Source - Diseases Database)\nAn acute disease of young children characterized by a rash and swollen lymph nodes and fever; of unknown cause\n- (Source - WordNet 2.1)\nKawasaki disease is listed as a \"rare disease\" by the Office of\nRare Diseases (ORD) of the National Institutes of Health\n(NIH). This means that Kawasaki disease, or a subtype of Kawasaki disease,\naffects less than 200,000 people in the US population.\n- (Source - National Institute of Health)\nThe list below shows some of the causes of Kawasaki disease mentioned in various sources:\nThis information refers to the general prevalence and incidence of these diseases, not to how likely they are to be the actual cause of Kawasaki disease. Of the 5 causes of Kawasaki disease that we have listed, we have the following prevalence/incidence information:\nThe following list of conditions have 'Kawasaki disease' or similar listed as a symptom in our database. This computer-generated list may be inaccurate or incomplete. Always seek prompt professional medical advice about the cause of any symptom.\nSelect from the following alphabetical view of conditions which include a symptom of Kawasaki disease or choose View All.\nAsk or answer a question about symptoms or diseases at one of our free interactive user forums.\nMedical story forums: If you have a medical story then we want to hear it.\nDoctor-patient articles related to symptoms and diagnosis:\nThese general medical articles may be of interest:\nSearch Specialists by State and City"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:57d9daf6-cfbf-486e-b577-fee8ce31eef2>","<urn:uuid:b7ac7858-92fc-47e8-bc40-fd84f5bfed57>"],"error":null}
{"question":"What quality of piano samples are used in the Kawai MP10 versus the Yamaha YDP S54, and which famous acoustic pianos were they sampled from? 🎹","answer":"The Kawai MP10 features samples from the Kawai EX concert grand piano, which is hand-built at the Shigeru Kawai Piano Research Laboratory in Ryuyo, Japan. Each of the 88 keys was individually recorded at different dynamics from pianissimo to fortissimo to ensure faithful reproduction. The Yamaha YDP S54 uses samples from Yamaha's flagship CFX concert grand piano, which was developed over 19 years and gained recognition after being used by the winner of the International Chopin Competition in 2010. The CFX samples are known for their scintillating highs and powerful bass response.","context":["Evolution, not revolution: that's the concept behind the new MP10 Professional Stage Piano. Kawai took its predecessor's great interface and compact form, updated the keyboard action and sound technol... Click To Read More About This Product\nEvolution, not revolution: that's the concept behind the new MP10 Professional Stage Piano. Kawai took its predecessor's great interface and compact form, updated the keyboard action and sound technology, then added a host of modern new features to sweeten the deal. The result is a flexible stage piano that's ideal for gigging, with a solid, reliable action and an impressive selection of sounds.\nThe MP10 features brand new acoustic and electric pianos that underscore a host of tonal improvements throughout the entire sound list. In addition to various sampled organ sounds, the MP10 now includes a tonewheel organ simulator, allowing you to dial-in your favorite drawbar positions, then switch on the new amp simulation and rotary speaker effects for that authentic vintage organ growl.\nWhether you're in the studio or out on the road, the MP10 is more than capable of meeting the master keyboard needs of a professional stage performer. Split the entire keyboard into four independent zones, then assign each one to play internal sounds, trigger external MIDI devices, or control both simultaneously. Adjust the levels and parameters for all four zones using realtime knobs and faders. Then, once you've found the perfect mix for each song, store the entire panel setup to one of the 256 internal memories (or a USB stick) and you're ready for the gig.\nSo often stage pianos are let down by poorly designed keyboard actions prone to break downs. Not the MP10. Its Responsive Hammer action delivers a stunningly realistic piano playing experience, but it's also built to last. Structural reinforcements make RH the strongest, most rigid plastic action Kawai has ever developed, ensuring keys don't wobble around from side to side or crack under pressure when you're laying down the big chords. And, its Ivory Touch surface featured on this action as well, you can be confident that your fingers will remain firmly on the keys throughout even the most blistering runs.\nMP acoustic piano sounds: the Kawai EX concert grand piano\nThe Kawai EX concert grand piano is widely regarded by many professional pianists and piano technicians as one of the finest instrument of its class. Hand-built by Kawai's distinguished Master Piano Artisans at the Shigeru Kawai Piano Research Laboratory in Ryuyo, Japan, each EX instrument undergoes a lengthy, meticulous process of regulation and refinement within a specially designed anechoic chamber. The completed EX piano then receives a final series of rigorous quality inspections before eventually taking its place in some of the world's most prestigious musical institutions and concert halls, and now your home.\nThe beautiful sound of the EX grand piano is at the heart of the new MP10, with all eighty-eight keys of this world-class instrument painstakingly recorded, analyzed, and reproduced as high-fidelity digital waveforms.\nRecording each key individually in this way”as opposed to stretching the same tone over several different notes”ensures that the rich, harmonic character of the original EX grand piano is preserved, and guarantees that the sound heard when playing one of the MP10's keys is a faithful representation of the acoustic source.\nWhen playing the MP10, the amount of pressure applied to the keyboard affects both the volume of the sound produced and the unique tonal character of each note. In order to gather a realistic acoustic portrait of the EX grand piano, not only is each key recorded individually, but also captured at various different strengths ranging from gentle pianissimo to thunderous fortissimo. The new MP10 offers a highly detailed acoustic portrait, with smooth tonal transitions across the keyboard and throughout the dynamic range.\nMP keyboard actions: inspired by eight decades of acoustic piano excellence\nEmbracing many of the long-established principles associated with an acoustic piano, the MP10's RM3 Grand keyboard action combines realistic materials, motion, and mechanism to recreate the exceptional touch of the world's finest concert grand pianos.\nAll 88 black and white keys are crafted entirely from long pieces of wood which pivots smoothly on a central balance pin to replicate the familiar \"seesaw\" motion of an acoustic piano action. When pressed, the back of each key triggers a grade-weighted hammer to play the note while additional counter-weights are also placed at the front of the bass keys, helping to lighten their heavier touch during pianissimo passages.\nIt is equipped with a finely textured Ivory Touch surface to absorb finger moisture and assist the playing control, while let-off simulation recreates the subtle 'notch' sensation felt when softly playing the keys of a grand piano.\nNew EPs, Tonewheel organs, and Vintage amplifier effects\nComplementing its impressive variety of acoustic piano sounds, the MP10 also features a selection of brand new electric pianos. Classic tines, reeds, and clavs”each one lovingly sampled from original vintage instruments to ensure their distinctive characteristics (and occasional imperfections) are richly preserved.\nThe tonal character of an amplifier or speaker cabinet is an important component of any vintage electronic instrument. The MP10 therefore features an amp simulator function that helps to bring a feeling of analog warmth to the raw electric piano sound. It also offers a selection of 6 different amp/speaker cabinet types ranging from the crisp-clean Tweed Deluxe and classic Jazz Combo, to the crunchier growl of a British Blues or UK Class A. It allows direct access to the amplifier's drive and volume levels, in addition to dedicated Lo, Mid, and Hi tone controls. Simply select your favorite EP sound, crank up the amp drive, and dig-into vintage keyboard playing heaven.\nRealtime parameter adjustments, direct panel control\nBesides adjusting volumes for the individual MIDI and sound sections of the MP10, the intuitive panel layouts and solid, smooth faders featured on it provide direct control over mixing internal and external sound sources.\nFurther realtime adjustments are made possible using the instrument's four control knobs, with the MP10 allowing a variety of parameters to be freely assigned for each section. When playing acoustic piano sounds, for example, it may be desirable to have direct control over Virtual Technician parameters such as voicing and string resonance.\nThen, when switching over to the electric pianos, you will undoubtedly appreciate having immediate access to the simulated amplifier's tone controls”momentarily boosting your mid-range in order to cut through the mix for a solo.\nIt's also possible to adjust the parameters of external devices by assigning MIDI control change messages to each knob. Thanks to the M10's thorough MIDI implementation and comprehensive panel controls, the MP10 is ideally suited to handle the role of master keyboard in a studio environment with assignable MMC buttons, allowing effortless integration with DAW software and hardware.\nUSB Connectivity, MP3/WAV Record and Playback\nThe MP10 is equipped with USB ports that not only allow it to be connected to a Mac or PC for MIDI use, but also to load and save data directly to USB memory devices. This 'USB to Device' functionality allows customized sounds, SETUP memories, and recorder songs stored in internal memory to be saved to USB for future gigs, or even exchanged with other MP owners online.\nUSB memory devices can also be used to play back MP3 or WAV audio files, and SMF song data, allowing you to play along with professional backing tracks, or simply learn the chords or melody for a new piece.\nIt is even possible to record performances directly as MP3/WAV audio or SMF song files for emailing to band members, casual listening away from the keyboard, or further editing using an audio workstation. Furthermore, the MP10 also supports recording from external audio sources such as amped guitars or vocals using the instrument's convenient LINE IN jacks. Adjust the input level from a dedicated panel fader, select the preferred audio format, then press record”the MP takes care of the rest. Produce crystal-clear digital recordings of all your performances, without ever having to leave the keyboard.\nOur product catalog varies by country due to manufacturer restrictions. If you change the Ship-To country, some or all of the items in your cart may not ship to the new destination.","View Full Description\nMusic That You feel\nTravel from your bedroom to the concert hall. The YDP S54 now includes samples from Yamaha's flagship piano - the Yamaha CFX concert grand. The CFX grand piano represents 19 years of research and development to combine traditional craftsmanship with modern day engineering. This harmonious combination enabled the engineers at Yamaha to examine every element of the piano, discovering how every detail can impact the sound.\nIn 2010, the CFX was the piano of choice by the winner of the prestigious International Chopin Competition. Since then, the CFX has gained worth recognition as one of the greatest concert grands of all time. Now you can experience this, all in the comfort of your own home. Boasting scintillating highs and a powerful bass, these CFX samples will transform your practice into a recital.\nGraded Hammer 3 Piano Action\nAs well as the faithful samples, the YDP S54 features fully-weighted keys, reproducing the touch of an acoustic piano for natural playability. The Graded Hammer 3 (GH3) 88-key piano action recreates the natural feel when touching the keys, reacting accordingly to velocity changes and providing that 'heavy feel' in the lower register. The piano action also features a three-sensor configuration which analyses and interprets the behaviour of the keyboard, providing a natural response and feel. This lets you hear every nuance of the sound, providing authentic expressive control with the synthetic ebony and ivory key tops only adding to its natural feel.\nWho said pianos had to be traditional? The Smart Pianist app offers a range of features that places you in control. The app enables you to select instrument voices, adjust various settings, record your performance, and much more. You will find yourself making music faster with this app as it gives you the freedom to this on your personal smart device.\nWant to play along with your favourite songs? Well, now you can! Smart Pianist scans the audio songs loaded in your music library, analyses the chord progressions, and then displays them for you on the screen of your smart device. It's that simple. Play along as the chords scroll by, all the while hearing the song play back through the S54's speaker system. Loading MIDI songs into the app will also allow you to follow along with digital sheet music while you play.\nStereophonic Headphone Optimizer\nA great feature of digital pianos is the ability to practice in silence using headphones. The YDP S54 has catered to this feature with the Stereophonic Optimizer. The Stereophonic Optimizer works by adjusting the spacing of the sound and the separation from the instrument, resulting in a natural and spacious surround sound quality, immersing you within the pianos natural tones. This helps to perceive the sound of the piano as coming from the body of the instrument rather than the headphones, providing a more realistic and natural playing experience.\n3 Month Flowkey Premium Offer\nYamaha and the piano tutorial app Flowkey have collaborated for a limited time offer to give you 3 months of premium learning. Simply download the Flowkey app on to your smartphone or tablet, register your instrument, and begin your subscription. The user friendly interface allows you to learn each song in both a sheet music format as well as a play-along video. Thanks to the adjustable difficulty settings, Flowkey can be used by complete novices and developed musicians alike. Premium Flowkey members have access to over 500 songs covering major hits from pop, classical, rock and more. This intuitive app can even track your playing and offer instant feedback to help improve your skills. Any Yamaha digital piano purchased between 15th July 2016 and 31st March 2021 is now eligible for three months of premium Flowkey access.\nPlease visit Flowkey's website to find out more about this exciting offer.\n- Width: 1353 mm\n- Height: 792 mm\n- With key cover open: 976 mm\n- Depth: 309 mm\n- With key cover open: 317 mm\n- With anti-fall brackets attached: 404 mm\n- Weight: 40 kg\n- Keyboard: Graded hammer (GH3) keyboard with synthetic ebony and ivory keytops\n- Number of Keys: 88\n- Touch Sensitivity: Hard/Medium/Soft/Fixed\n- Pedals: Damper/Sostenuto/Soft\n- Key Cover: Folding key cover with music rest\n- Finish: White\nFunctions and Voices\n- Tone Generator: Yamaha CFX\n- Polyphony (Max): 192\n- Number of Voices: 10\n- Effects: Reverb (four types), Intelligent Acoustic Control (IAC), Stereophonic Optimizer, Damper Resonance\n- Preset Songs: 10 Demo Songs, 50 Piano Preset Songs\n- User Recording: 1 user song with 2 tracks\n- Data Capacity: 100 KB per song (approx. 11,000 notes)\n- Compatible Data Format (Playback): SMF (format 0, format 1)\n- Compatible Data Format (Recording): SMF (format 0)\n- Metronome: Yes\n- Tempo Range: 5 - 280 bpm\n- Transpose: -6 to 0, 0 to +6\n- Tuning: 414.8 - 440.0 - 446.8 Hz\n- Internal Memory: Total maximum size approx. 900 KB (User song: One song approx. 100 KB, Loading song data from a computer: Up to 10 Songs)\n- Connectivity: Stereo headphone jack x 2; USB-to-host\n- Amplifiers: 20 W x 2\n- Speakers: 12cm x 2\n- Acoustic Optimizer: Yes\n- Power Supply: PA-300 or an equivalent recommended by Yamaha\n- Auto Power Off: Yes\n- Power Consumption: 13 W (When using PA-300C AC adapter)\n- Included Accessories: PA-300C AC adapter; song book (50 Greats for Piano)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9eb142b4-5260-4a99-b5b0-72a25c8f11a7>","<urn:uuid:02a9422f-12b8-4ceb-881d-6539b909b959>"],"error":null}
{"question":"How does inattentional blindness affect both radiologists and drivers in dangerous ways?","answer":"Both radiologists and drivers can miss critical things due to inattentional blindness. In a study, 83% of radiologists failed to notice a gorilla image 48 times larger than the nodules they were searching for in lung scans, even when looking directly at it. Similarly, drivers often cause accidents with motorcyclists because while they technically see the motorcycles, their focus on looking for cars and trucks makes them effectively blind to the cyclists, leading to crashes when pulling out in front of them.","context":["Researchers at Brigham and Women’s Hospital (BWH) in Boston have found that having a singularity of focus can alter perceptions and create inattentional blindness (IB). In the new study, published on July 19, 2013 in Psychological Science, radiologists looking for cancer nodules were oblivious to a huge gorilla that floated across their computer screen.\nHave you ever had the experience of being so in the ‘zone’ that your laser-like focus turned into a type of tunnel vision? The ability to direct your focus and block out distractions in the periphery is key to achieving any difficult task with precision, but can also backfire.\nThe researchers asked 24 radiologists to perform a familiar lung nodule detection task. The radiologists examined five scans; each scan contained an average of 10 nodules. A gorilla that was 48 times larger than the average nodule, was inserted in the final scan. The researchers found that 83 percent of radiologists did not report seeing the gorilla.\nAs a writer, I often miss typos because my mind is so focused on the content and communicating ideas that my eyes glide over glaring errors. This is why having a second pair of eyes proofread writing whenever possible is imperative. The researchers at BWH tracked the eye-movements of the radiologists and found that the majority of those who missed the gorilla looked directly at it.\nWhen focusing on a difficult task the researchers at BWH found that even people with a high level of expertise are vulnerable to IB. Trafton Drew, PhD, post-doctoral researcher at BWH and lead author on this study said, \"When engaged in a demanding task, attention can act like a set of blinders, making it possible for stimuli to pass, undetected, right in front of our eyes. We found that even experts are vulnerable to this phenomenon.”\nBird’s Eye View vs. Worm’s Eye View\nAs a triathlete, I learned how to flip between taking a bird’s eye view and a worm’s eye view throughout every competition. For example, if I was strategizing how to get to the front of the pack on the bike I would zoom in on very specific competitors and focus on advancing past them one-by-one like a Pac-Man. I would become so laser-focused and immersed in the moment of achieving the task at hand that the rest of the world completely dissolved. However, in order to succeed over the long haul of an Ironman race I would have to snap out of a state of laser-like focus — zoom out — and take inventory from a bird’s eye perspective. Flipping between a bird's and worm's eye view was key to maintaining a mindset of peak performance all the way to the finish line.\nCan you find the gorilla?\nRegardless of what demanding task you are tackling, the best way to avoid the backlash of inattentional blindness is to consciously remind yourself to zoom out from the worm’s eye view periodically and take a bird’s eye view. Having myopic vision and singularity of focus can cause you to miss the big picture and makes you vulnerable to making mistakes.\n\"The radiologists missed the gorillas not because they could not see them, but because the way their brains had framed what they were doing. They were looking for cancer nodules, not gorillas,\" explained Jeremy Wolfe, senior psychologist and director of the Visual Attention Laboratory at BWH. \"This study helps illustrate that what we become focused on becomes the center of our world, and it shapes what we can and cannot see.\"\nConclusion: Mindfulness Training Improves Attention\nWe all have the power to direct our focus and guide our thoughts. Through mindfulness training you can flex your mind’s ability to hone in on details but to also zoom out and look at the world around you through a more panoramic lens. The ability to flip your perspective and see things from a different point of view also applies to the explanatory style you choose to take about life.\nYou can decide what you want to see in the world around you and if the glass is half full or half empty. Without being a Polyanna, you can decide to focus your attention on the positive things in the world around you—in doing so the negativity will literally cease to exist in your perception of reality. Athletics is a great way to train your mind to look on the bright side and find silver linings. Dwelling on negativity sabotages your odds of winning any competition in life and sport.\nThe researchers at BWH conclude that even expert searchers typically only see what they are looking for. Without mindfulness training it is easy to remain oblivious to glaringly obvious things that are directly in front of your eyes. The researchers at BWH hope that their findings will lead more expert searchers to recognize the important role of attention in determining what the searcher will see and what he or she may miss.","Test your ability to focus.\nAs you watch this one-minute video of people passing a ball back and forth, count the number of times the people in white pass the ball.\nThe video was created by psychologists Daniel Simons and Christopher Chabris.\nThey developed it to demonstrate what happens when we focus on a specific task – a phenomenon known as inattentional blindness. When people are looking for something in particular, they are likely to be blind to important and obvious things. Some refer to it as selective attention bias.\nAfter watching the video, study subjects were asked if they had noticed anything unusual.\nHalf reported seeing nothing out of the ordinary. Go back and watch it again, this time not counting passes, but observing the entire scene.\nAlthough obvious, it was beside the point.\nWhen we look for something in particular, we don’t notice the fully-visible but unexpected. People miss the gorilla because it has nothing to do with white shirts and passing balls. The human brain subconsciously filters out visual images that are irrelevant to the task we are pursuing, even though the images are right in front of us.\nResearchers presented radiologists with a CT scan of a pair of lungs and asked them to click on anything strange.\nThe study, published in the journal Psychological Science, showed they all found the ten abnormalities in the lungs. It also showed that three out of four of them did not notice the image of a gorilla the researchers had inserted in the scan, even though they scrolled past it an average of four times each.\nIf you’re betting the gorilla image was tiny and hard to see, you lose, because the gorilla image was 48 times larger than the average nodule that the radiologists were looking for.\nEye-tracking software showed the radiologists who did not report seeing the gorilla had in fact looked directly at it.\nThe radiologists missed the gorillas not because they could not see them, but because of the way their brains had framed what they were doing. They were looking for cancer nodules, not gorillas. Framing is an important step in problem-solving because even slight changes in how we look at things can lead to vastly different processes and solutions. “What we become focused on becomes the center of our world, and it shapes what we can and cannot see,” said Jeremy Wolfe, director of the Visual Attention Laboratory at Brigham and Women’s Hospital. Trafton Drew, the lead researcher, said “attention can act like a set of blinders, making it possible for stimuli to pass undetected right before our eyes.”\nWhat does this have to do with research?\nStudy sponsors usually have desired outcomes and pet theories they want confirmed. My colleagues and I have observed on many occasions that when they watch focus groups, most sponsors pay attention only to the things they are looking for. Studies show that when sponsors and researchers observe the same discussions, sponsors see only the evidence that supports what they already believe. Because they are trained to watch and listen to everything, researchers see the bigger picture, which almost always includes evidence pro and con.\nSome car-motorcycle crashes are caused by inattentional bias.\nMany oncoming motorcyclists are injured in accidents when drivers pull out directly in front of them. After the wreck, those who caused the accident said they never saw the motorcycle. Most actually did see it, but because they were looking for cars and trucks, they ignored the cyclists, pulled out right in front of them, and caused the accident.\nInattentional blindness is a cousin to change blindness.\nThis occurs when we fail to notice the difference between a previous state and the current state. Manipulating this phenomenon is a part of the toupée seller’s toolbox. Bald men who have made the decision to wear a wig are advised to first grow a mustache or beard and keep it long enough for people around them to get used to it. Then they are instructed to shave it off and appear bare-faced the first day they wear their new wig. The idea is that people will notice your beard is gone and not think about how you have miraculously sprouted a full head of hair. To no one’s surprise, the success of this tactic is related to the quality of the hairpiece and the flamboyance of the facial hair.\nTo read more articles like this, click here."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:339c0d54-bd9a-4406-bdc5-c1f562f88c41>","<urn:uuid:a5cd2533-5643-4412-adad-78205db2258f>"],"error":null}
{"question":"What local food products can you typically find in Arizona grocery stores?","answer":"In Arizona grocery stores, you can typically find local products such as lettuce and leafy greens from Yuma (particularly in winter), eggs, milk, dairy products, and tortillas. However, items like chicken, pork, fish, coffee, apples, and bananas come from out of state.","context":["The Mystery of the One Percent\nBy Julie Murphree, Arizona Farm Bureau Communication and Organization Director: Recently, a random statistic was thrown out that claimed Arizona imports 99% of the food that goes to our neighborhood groceries, restaurants and homes. The 1 percent claim of what’s home-grown then served and sold in the state is a powerful statistic to use against our current Arizona food system, if it were true.\nWith a quick query to our cadre of researchers and scientists at the University of Arizona, our land-grant university, the mystery of the 1% was debunked in a matter of days.\nAccording to Ashley Kerna, an Economic Impact Analyst at the University of Arizona's Cooperative Extension and Department of Agricultural and Resource Economics, approximately 30% of Arizona household spending on raw agricultural food products comes from Arizona farm and ranches.\nApproximately 30% of Arizona household spending on raw agricultural food products comes from Arizona farm and ranches; an estimated 18% of Arizona household spending on processed food products comes from Arizona processing facilities.\nEven for processed foods, an estimated 18% of Arizona household spending on processed food products comes from Arizona processing facilities. While these processing facilities may not necessarily be using Arizona ingredients, there are\nThese figures are a far cry from the 1% claimed by some random statistic.\nSo, how did the U of A team get to this? The Input-Output model Kerna and Frisvold use, IMPLAN, has data on trade flows between more than 5,400 industries and to households of different income categories. Trade flows describe the movement of goods and services between Arizona and the outside world, generating what is known as Regional Purchase Coefficients (RPCs) for all commodities. An RPC is the proportion of local demand that is supplied by local producers. The IMPLAN trade flow data is based on national statistics from the Department of Commerce (Bureau of Economic Analysis) and Census Bureau. The U of A team used the IMPLAN trade flow data to calculate RPCs for both on-farm products and for processed foods.\nThese RPCs are estimated based on household demands, not industry demands. So the RPCs presented here are estimates of the proportion of local household demand for agricultural commodities and processed food products that are supplied locally (i.e., by the region to itself). In this case \"locally\" means within Arizona.\nRPCs are higher for individual items that Arizona specializes in such as vegetables and melons, beef, fluid\nIt seems to me we’ve lost a sense of maximizing\nDo we really want other states saying, “Why are we buying all this lettuce from Arizona, that’s a bad thing?” If you go to a Phoenix grocery store, the lettuce, and leafy greens in the winter are going to come from Yuma; the eggs and milk, and much of the other dairy products will come from Arizona; tortillas too.\nThe chicken, pork, and fish will come from out of state, as will coffee, apples, bananas,\nAs our U of A team correctly points out, many items coming from outside the state are processed foods. Arizona’s food manufacturing industries are not as large as other states that specialize in corn and wheat. The food processing sector has been growing though. Milk and dairy product\nAnd back to Arizona cotton: virtually all our cotton is exported, but much of it returns to the United States as clothing and household goods. Edible commodities are marketed in\nAnd the origins of the one percent? It’s a mystery.\nEditor’s Note: The Economic Impact Analyst (EIA) team formed over four years ago within the Cooperative Extension System (CES) in an effort to address the many issues and needs the agriculture industry has to quantitatively document the impacts of agriculture in Arizona and our CES programs. Collectively, the EIA group is providing to the CES the capacity to address questions and issues just like what we queried them about regarding the percentage of Arizona agriculture going to Arizona families. The EIA team does so in a technically correct manner, based on appropriate economic methods, providing valuable and accurate information in a timely manner."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:716393fa-8760-48ff-b7cf-649f909857ed>"],"error":null}
{"question":"Which rare book had more copies survive - the Bay Psalm Book or Shakespeare's First Folio?","answer":"Shakespeare's First Folio had more surviving copies. Of the First Folio, around 1,000 copies were printed and 228 still exist according to the most recent census. In contrast, the Bay Psalm Book is much rarer, with only 11 copies known to exist.","context":["More words have been written about this book than there are words in the book. It had a courageous start as The Whole Booke of Psalms Faithfully Translated into English Metre. Over time, while its moniker shortened to The Bay Psalm Book, its reputation and stature increased. Within the rarified world of the exceptionally important it is for early American books what “the Babe” is for baseball aficionados: mythical, mystical. This is the first book printed in British North America.\nHere is how the Library of Congress describes it. “Known as The Bay Psalm Book, but really titled The Whole Booke of Psalmes Faithfully Translated into English Metre, it represents what was most sacred to the Puritans--a faithful translation of God's Word, to be sung in worship by the entire congregation. Other Protestant denominations relied on selected paraphrases of the Scripture, but the Puritans believed this could compromise their salvation. The same faith that compelled them to leave England and strike out for the New World prompted them to commit this text to print before all others.”\nThe most recent copy at auction came up in 1947 and was the subject of dispute. Dr. A. S. W. Rosenbach, the exceptional book dealer with a commitment from Yale for $105,000, in the heat of battle at Park-Bernet went all the way to $151,000 to buy it, then had to raise additional money to pay for it when Yale balked. That copy stands today as one of the great gems in the Beinecke Collection and as a reminder that the greatest booksellers know their books well and the future even better. Rosenbach, in exceeding Yale’s instructions, made the right decision but it would take a year or so for the field to recognize what he instinctively knew: the very best must be acquired when it is possible. Who handles the bids in the upcoming sale will be equally important. People with the money to buy greatly outnumber the rarified community with the courage to do so. History says the buyer will treasure the prize but there will be questions and doubters in the near term. The Bay Psalm is not for the faint hearted. Nor should it be.\nThe most recent exchange of this book took place in 1966 when the Van Sinderen family presented their copy to the Library of Congress. The presenter, the late Jean Van Sinderen Henry, was bookseller Bill Reese’s godmother.\nThe next copy comes up on November 26th at Sothebys in New York. It comes from the vaults of the Old South Church in Boston for whom it is both diadem of their exceptional history and talisman for the broadening ministries they envision possible with the proceeds. It can be fairly said that who buys this book improves the world.\nIn preparation the book has been on tour, being exhibited at many of the principal libraries and collecting associations in the United States, in total at a dozen locations. The tour is unprecedented but the book itself is so steeped in history no special arrangements and special events seem excessive. Like Halley’s comet we can predict another copy will someday come to market but it will be our grandchildren who will hear of it. For this and the next generation this is the only chance we get.\nFor the collecting fields the Bay Psalm has already served collectors well by raising awareness. Some new players are probably already pawing the ground but it is very difficult to buy the best right out of the gate. Many though will be interested.\nAfter an extensive tour of the United States, this copy will present itself for sale at Sotheby’s on November 26th. Its last stop before its exhibition at Sotheby’s headquarters is at the Grolier Club, itself a royal setting for this royal book. It has been a busy year.\nPhiladelphia, Rosenbach Museum & Library\nChicago, Newberry Library\nSt. Louis, Mercantile Library\nCleveland, Cleveland Public Library\nHouston, Museum of Fine Arts (Bayou Bend)\nSan Francisco, the Bentley Reserve\nDallas, Dallas Public Library\nThe Ronald Reagan Presidential Library\nLos Angeles, Doheny Memorial Library, University of Southern California\nNew York City, Grolier Club\nNew York City, Sothebys, 1334 York Ave\nFor the serious and seriously wealthy collector of American books and history, the upcoming sale of the Old South Church’s copy of the first printed book in British North America, the 11th copy known, the Bay Psalm is a once in a lifetime opportunity. This is not a pedestrian rarity. It is important and unique and examples almost never appear, the other 10 the prized possessions of important institutions. When sometime in the future another copy comes to market it too will invariably sell, setting records as it does, then to settle into the vault of a great library or museum or possibly a great collector. That will be then. For this era the moment is now.\nLet’s hope that this generation’s Rosenbach is in the room and authorized to bid to the sun. When all is over the winner will achieve lasting recognition, the under bidder the lasting gratitude of the Church. In a hundred years the winning bidder’s name will have been woven into this volume’s provenance. The acquirer will have pursued history, captured it, and in time become part of it.\nDavid Redden, Worldwide Chairman of Books and Manuscripts at Sotheby’s, describes the Bay Psalm as “the most famous book in America that no one has ever heard of.” He has done a lot to change that. On November 26th we’ll learn how much.\nOnce in a Lifetime - An AE Story published in January 2013\nLink to the Old South Church's electronic copy of the Booke of Psalmes\nThe full text of the Parke-Bernet Sale in 1947\nThe full text of lot 847 in the Brinley Sale in 1879\nThe full description in the Crowinshield Sale in 1859\nThe full online catalogue at Sothebys.","|Cover artist||Martin Droeshout|\n|Language||Early Modern English|\n|Genre(s)||English Renaissance theatre|\n|Publisher||Edward Blount and William and Isaac Jaggard|\n|Publication date||Late 1623|\nPrinted in folio format and containing 36 plays (see list of Shakespeare's plays), it was prepared by Shakespeare's colleagues John Heminges and Henry Condell. It was dedicated to the \"incomparable pair of brethren\" William Herbert, 3rd Earl of Pembroke and his brother Philip Herbert, Earl of Montgomery (later 4th Earl of Pembroke).\nAlthough eighteen of Shakespeare's plays had been published in quarto prior to 1623, the First Folio is the only reliable text for about twenty of the plays, and a valuable source text even for many of those previously published. The Folio includes all of the plays generally accepted to be Shakespeare's, with the exception of Pericles, Prince of Tyre and The Two Noble Kinsmen, and the two \"lost plays\", Cardenio and Love's Labour's Won.\nThe contents of the First Folio were compiled by Heminges and Condell; the members of the Stationers Company who published the book were the booksellers Edward Blount and the father/son team of William and Isaac Jaggard. The Jaggards were printers as well as booksellers, an unusual but not unprecedented combination. William Jaggard has seemed an odd choice by the King's Men, since he had published the questionable collection The Passionate Pilgrim as Shakespeare's, and in 1619 had printed new editions of ten Shakespearean quartos to which he did not have clear rights, some with false dates and title pages (the False Folio affair). The paper industry in England was then in its infancy and the quantity of quality rag paper for the book was imported from France. It is thought that the typesettingand printing of the First Folio was such a large job that the King's Men simply needed the capacities of the Jaggards' shop. At any rate, William Jaggard was old, infirm, and blind by 1623, and died a month before the book went on sale; most of the work in the project must have been done by his son Isaac.\nThe First Folio's publishing syndicate also included two stationers who owned the rights to some of the individual plays that had been previously printed: William Aspley (Much Ado About Nothing and Henry IV, Part 2) and John Smethwick (Love's Labour's Lost, Romeo and Juliet, and Hamlet). Smethwick had been a business partner of another Jaggard, William's brother John.\nThe actual printing of the Folio was probably done between February 1621 and early November 1623. The first impression had a publication date of 1623; the Bodleian Library, in Oxford, received its copy in early 1624 (which it subsequently sold for £24 as a superseded edition when the Third Folio became available in 1664).\nThe thirty-six plays of the First Folio occur in the order given below; plays that had never been published before 1623 are marked with a *. Each play is followed by the type of source used, as determined by bibliographical research.\n[Some definitions are needed. The term \"foul papers\" refers to Shakespeare's working drafts of a play; when completed, a transcript or \"fair copy\" of the foul papers would be prepared, by the author or by a scribe. Such a manuscript would have to be heavily annotated with accurate and detailed stage directions and all the other data needed for performance, and then could serve as a \"prompt-book,\" to be used by the prompter to guide a performance of the play. Any of these manuscripts, in any combination, could be used as a source for a printed text. On rare occasions a printed text might be annotated for use as a prompt-book; this may have been the case with A Midsummer Night's Dream.]\n- 1 The Tempest * — the play was set into type from a manuscript prepared by Ralph Crane, a professional scrivener employed by the King's Men. Crane produced a high-quality result, with formal act/scene divisions, frequent use of parentheses and hyphenated forms, and other identifiable features.\n- 2 The Two Gentlemen of Verona * — another transcript by Ralph Crane.\n- 3 The Merry Wives of Windsor — another transcript by Ralph Crane.\n- 4 Measure for Measure * — probably another Ralph Crane transcript.\n- 5 The Comedy of Errors * — probably typeset from Shakespeare's \"foul papers,\" lightly annotated.\n- 6 Much Ado About Nothing — typeset from a copy of the quarto, lightly annotated.\n- 7 Love's Labour's Lost — typeset from a corrected copy of Q1.\n- 8 A Midsummer Night's Dream — typeset from a copy of Q2, well-annotated, possibly used as a prompt-book.\n- 9 The Merchant of Venice — typeset from a lightly edited and corrected copy of Q1.\n- 10 As You Like It * — from a quality manuscript, lightly annotated by a prompter.\n- 11 The Taming of the Shrew * — typeset from Shakespeare's \"foul papers,\" somewhat annotated, perhaps as preparation for use as a prompt-book.\n- 12 All's Well That Ends Well * — probably from Shakespeare's \"foul papers\" or a manuscript of them.\n- 13 Twelfth Night * — typeset either from a prompt-book or a transcript of one.\n- 14 The Winter's Tale * — another transcript by Ralph Crane.\n- 15 King John * — uncertain: a prompt-book, or \"foul papers.\"\n- 16 Richard II — typeset from Q3 and Q5, corrected against a prompt-book.\n- 17 Henry IV, Part 1 — typeset from an edited copy of Q5.\n- 18 Henry IV, Part 2 — uncertain: some combination of manuscript and quarto text.\n- 19 Henry V — typeset from Shakespeare's \"foul papers.\"\n- 20 Henry VI, Part 1 * — likely from an annotated transcript of the author's manuscript.\n- 21 Henry VI, Part 2 — probably a Shakespearean manuscript used as a prompt-book.\n- 22 Henry VI, Part 3 — like 2H6, probably a Shakespearean prompt-book.\n- 23 Richard III — a difficult case: probably typeset partially from Q3, and partially from Q6 corrected against a manuscript (maybe \"foul papers\").\n- 24 Henry VIII * — typeset from a fair copy of the authors' manuscript.\n- 25 Troilus and Cressida — probably typeset from the quarto, corrected with Shakespeare's \"foul papers.\"\n- 26 Coriolanus * — set from a high-quality authorial transcript.\n- 27 Titus Andronicus — typeset from a copy of Q3 that might have served as a prompt-book.\n- 28 Romeo and Juliet — in essence a reprint of Q3.\n- 29 Timon of Athens * — set from Shakespeare's foul papers or a transcript of them.\n- 30 Julius Caesar * — set from a prompt-book, or a transcript of a prompt-book.\n- 31 Macbeth * — probably set from a prompt-book.\n- 32 Hamlet — one of the most difficult problems in the First Folio: probably typeset from some combination of Q2 and manuscript sources.\n- 33 King Lear — a difficult problem: probably set mainly from Q1 but with reference to Q2, and corrected against a prompt-book.\n- 34 Othello — another difficult problem: probably typeset from Q1, corrected with a quality manuscript.\n- 35 Anthony and Cleopatra * — possibly \"foul papers\" or a transcript of them.\n- 36 Cymbeline * — possibly another Ralph Crane transcript, or else the official prompt-book.\nTroilus and Cressida was originally intended to follow Romeo and Juliet, but the typesetting was stopped, probably due to a conflict over the rights to the play; it was later inserted as the first of the Tragedies, when the rights question was resolved. It does not appear in the table of contents.\nAs far as modern scholarship has been able to determine, the First Folio texts were set into type by five compositors, with different spelling habits, peculiarities, and levels of competence. Researchers have labelled them A through E, A being the most accurate, and E an apprentice who had significant difficulties in dealing with manuscript copy. Their shares in typesetting the pages of the Folio break down like this:\nCompositor \"E\" was most likely one John Leason, whose apprenticeship contract dated only from November 4, 1622. One of the other four might have been a John Shakespeare, of Warwickshire, who apprenticed with Jaggard in 1610-17. (\"Shakespeare\" was a common name in Warwickshire in that era; John was no known relation to the playwright.)\nThe First Folio and variants Edit\nW. W. Greg has argued that Edward Knight, the \"book-keeper\" or \"book-holder\" (prompter) of the King's Men, did the actual proofreading of the manuscript sources for the First Folio. Knight is known to have been responsible for maintaining and annotating the company's scripts, and making sure that the cuts and changes ordered by the Master of the Revels were complied with.\nSome pages of the First Folio — 134 out of the total of 900 — were proofread and corrected while the job of printing the book was ongoing. As a result, the Folio differs from modern books in that individual copies vary considerably in their typographical errors. There were about 500 corrections made to the Folio in this way. These corrections by the typesetters, however, consisted only of simple typos, clear mistakes in their own work; the evidence suggests that they almost never referred back to their manuscript sources, let alone tried to resolve any problems in those sources. The well-known cruxes in the First Folio texts were beyond the typesetters' capacity to correct.\nThe Folio was typeset and bound in \"sixes\" — 3 sheets of paper, taken together, were folded into a booklet-like quire or gathering of 6 leaves, 12 pages. Once printed, the \"sixes\" were assembled and bound together to make the book. The sheets were printed in 2-page forms, meaning that pages 1 and 12 of the first quire were printed simultaneously on one side of one sheet of paper (which became the \"outer\" side); then pages 2 and 11 were printed on the other side of the same sheet (the \"inner\" side). The same was done with pages 3 and 10, and 4 and 9, on the second sheet, and pages 5 and 8, and 6 and 7, on the third. Then the first quire could be assembled with its pages in the correct order. The next quire was printed by the same method: pages 13 and 24 on one side of one sheet, etc. This meant that the text being printed had to be \"cast off\" — the compositors had to plan before-hand how much text would fit onto each page. If the compositors were setting type from manuscripts (perhaps messy, revised and corrected manuscripts), their calculations would frequently be off by greater or lesser amounts, resulting in the need to expand or compress. A line of verse could be printed as two; or verse could be printed as prose to save space, or lines and passages could even be omitted (a disturbing prospect for those who prize Shakespeare's works).\nPerforming Shakespeare using the First Folio Edit\nSome Shakespeare directors, and theatre companies producing Shakespeare, believe that modern editions of Shakespeare's plays, which are heavily edited and changed to be more readable, remove possible actor cues in the Folio, such as capitalization, different punctuation and even the changing or removal of whole words. Among the theater companies that have based their production approach upon use of the First Folio was the Riverside Shakespeare Company, which, in the early 1980s, began a studied approach to their stage productions relying upon the First Folio as their textual guide. In the 1990s the First Folio, soon was reissued in a popular, paper back format more accessible to the general public.\nToday, many theatre companies and festivals producing the works of Shakespeare use the First Folio as the basis for their theatrical productions and training programs, including London's Original Shakespeare Company, a theatre company which works exclusively from cue scripts drawn from the First Folio.\nHowever, the First Folio does not contain every word of the plays. For instance, small passages of Hamlet are omitted — among them Horatio's line \"A mote it is to trouble the mind's eye\", and his subsequent speech beginning with \"In the most high and palmy state of Rome, / A little ere the mightiest Julius fell...\" Also missing is Hamlet's encounter with the Norwegian captain from Fortinbras's army in Act IV, Scene IV, along with perhaps the most important cut, the soliloquy \"How all occasions do inform against me\".\nHoldings, sales, and valuationsEdit\nIt is believed that around 1,000 copies of the First Folio were printed. The most recent census (1995–2000) records 228 still in existence. The British Library holds the following copies:- 1st impression (1623) - 2 copies; 2nd impression (1632) - 5 copies; 3rd impression (1663) - 1 copy, total 8 copies. The Folger Shakespeare Library in Washington, D.C. holds the world's largest collection with 82 copies. Another collection (12 copies) is held at Meisei University in Tokyo, Japan, including the Meisei Copy (coded MR 774), said to be unique because of annotations by its reader.\nOn 13 July 2006, a complete copy of the First Folio owned by Dr Williams's Library was auctioned at Sotheby's auction house. The book, which was in its original 17th century binding, sold for £2.5 million hammer price, less than Sotheby's top estimate of £3.5 million. This copy is one of only about 40 remaining complete copies (most of the existing copies are incomplete); only one other copy of the book remains in private ownership.\nOn 11 July 2008 it was reported that a copy stolen from Durham University, England in 1998 had been recovered after being submitted for valuation at Folger Shakespeare Library in Washington, D.C., in the United States. The folio's value was estimated at up to £15 million. Although the book, once the property of John Cosins the Bishop of Durham, was returned to the library, it has been mutilated and is missing its cover and title page. The folio was returned to public display on 19 June 2010 after its 12 year absence. Raymond Scott was jailed for eight years for handling stolen goods (he was acquitted of the theft of the copy). A July 2010 BBC programme about the affair, Stealing Shakespeare, portrayed Scott as a fantasist and petty thief.\n- Greg, W. W. The Shakespeare First Folio: Its Bibliographical and Textual History. London, Oxford University Press, 1955.\n- Hinman, Charlton. The Printing and Proof-Reading of the First Folio. Oxford, the Clarendon Press, 1963.\n- Pollard, Alfred W. The Foundations of Shakespeare's Text. London, Oxford University Press, 1923.\n- Walker, Alice. Textual Problems of the First Folio. Cambridge, Cambridge University Press, 1953.\n- Willoughby, Edwin Eliott. The Printing of the First Folio of Shakespeare. Oxford, Oxford University Press, 1932.\n- ↑ More generally, the term \"first folio\" is employed in other appropriate contexts, as in connection with the first folio collection of Ben Jonson's works (1616), or the first folio collection of the plays in the Beaumont and Fletcher canon (1647).\n- ↑ Shakespeare's First Folio, British Library, Undated. Retrieved: 16 April 2011.\n- ↑ \"Fame, Fortune, & Theft: The Shakespeare First Folio\", exhibit at the Folger Shakespeare Library, June 3 - September 3, 2011, curated by Anthony J. West and Owen Williams, with Melissa Cook; accessed 3 July 2011.\n- ↑ Hinman, pp. 363-5.\n- ↑ Robert M. Smith (July 1939). \"Why a First Folio Shakespeare Remained in England\". The Review of English Studies 15 (59): 257–264.\n- ↑ G. Blakemore Evans, textual editor, The Riverside Shakespeare, Boston, Houghton Mifflin, 1974.\n- ↑ F. E. Halliday, A Shakespeare Companion 1564–1964, Baltimore, Penguin, 1964; p. 113.\n- ↑ Halliday, p. 390.\n- ↑ Halliday, p. 319.\n- ↑ The First Folio of Shakespeare, Introduction by Doug Mostin, Applause Books, 1995, p. vii.\n- ↑ Patrick Tucker, Secrets of Acting Shakespeare: The Original Approach, (Routledge, 2002).\n- ↑ \"Bard's first folio fetches £2.8m\". BBC. 2006-07-13. http://news.bbc.co.uk/1/hi/entertainment/5175044.stm. Retrieved 2007-02-11.\n- ↑ About Meisei Copy from the site of the university\n- ↑ \"WILLIAM SHAKESPEARE’S FIRST FOLIO SELLS FOR $6,166,000 AT CHRISTIE’S NEW YORK, ESTABLISHING A WORLD AUCTION RECORD FOR ANY 17TH CENTURY BOOK\". christies. 2001-10-08. http://www.christies.com/presscenter/pressread.asp?pressId=198&PDF=FALSE. Retrieved 2008-04-01.\n- ↑ Antiques Trade Gazette, 22 July 2006.\n- ↑ \"Man bailed over Shakespeare theft\". BBC News. 2008-07-11. http://news.bbc.co.uk/1/hi/england/wear/7501546.stm. Retrieved 2010-05-02.\n- ↑ MacKnight, Hugh (9). \"Raymond Scott guilty of handling stolen folio of Shakespeare's plays\". guardian.co.uk: The Guardian. http://www.independent.co.uk/arts-entertainment/books/news/stolen-shakespeare-folio-is-given-its-day-in-court-2004770.html. Retrieved 31 July 2010.\n- ↑ MacKnight, Hugh (2010-06-19). \"Stolen Shakespeare folio is given its day in court\". London: The Independent. http://www.independent.co.uk/arts-entertainment/books/news/stolen-shakespeare-folio-is-given-its-day-in-court-2004770.html. Retrieved 31 July 2010.\n- ↑ \"County Durham man jailed over Shakespeare folio\". BBC News. 2010-08-02. http://www.bbc.co.uk/news/uk-england-wear-10838480. Retrieved 2010-08-02.\n- ↑ \"Stealing Shakespeare, BBC One\". theartsweb.com: the arts web. 2010-07-30. http://www.theartsdesk.com/index.php?option=com_k2&view=item&id=1933:stealing-shakespeare-bbc1&Itemid=31. Retrieved 2 August 2010.\n- First Folio - Shakespeare Digital Collection\n- First Folio - plain vanilla text from Project Gutenberg\n- Landmarks in Printing: Shakespeare's First Folio - British Library\n- William Shakespeare in Quarto - British Library\n- The Internet Shakespeare Editions maintains a collection of full colour facsimilies of the folios and quartos. Full text transcriptions are also available.\n- Jonathan Bate: The Case for the Folio\n|This page uses Creative Commons Licensed content from Wikipedia. (view article). (view authors).|\n| This page uses content from Wikinfo . The original article was at Wikinfo:First Folio.|\nThe list of authors can be seen in the (view authors). page history. The text of this Wikinfo article is available under the GNU Free Documentation License and the Creative Commons Attribution-Share Alike 3.0 license."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:de6b2946-f95f-4aa4-93eb-923260eb86e9>","<urn:uuid:ff58ea9a-89a5-49e2-9e94-78958bd7ad7a>"],"error":null}
{"question":"How do the Orion XT6 and XT8 Dobsonian telescopes compare in terms of performance capabilities, and what technical factors determine their light-gathering abilities?","answer":"The Orion XT6 and XT8 Dobsonian telescopes differ primarily in their light-gathering capabilities. The 8-inch f/6 telescope has greater light-gathering ability than the 6-inch f/8, following the principle that larger apertures collect more light. However, the 6-inch scope has some practical advantages - it requires less collimation and is more forgiving of poor collimation due to its f/8 ratio. It also has less weight and size to manage. The light-gathering ability of these telescopes is directly proportional to their primary mirror area, meaning the 8-inch telescope can capture significantly more light than the 6-inch, allowing it to see fainter objects. Both telescopes are considered good options out of the box, though they will need eventual add-ons and updates.","context":["Full list of affordable, beginner telescope recommendations can be found here: http://eyesonthesky.com/Blog/tabid/80/EntryId/242/The-best-affordable-beginner-telescope-options.aspx\nFirst, I would strongly encourage you to learn about telescope types, accessories like eyepieces and barlow lenses, plus how to calculate magnification, understanding telescopic and apparent field of view, and the types plus how to align a finderscope. So that equatorial mounts are not confusing, you can also learn how to align and how to use an equatorial mount will help make those kinds of mounts more easily understood as well. You’ll be better informed in about an hour, and (almost) all are closed-captioned so you can read what’s said while watching them (for instance, if you’re at work… not that anyone does that ever, LOL). ALL OF THAT CAN BE FOUND HERE: http://eyesonthesky.com/Blog/tabid/80/EntryId/179/The-Ultimate-Beginners-Guide-to-Telescopes-and-Amateur-Astronomy.aspx\n300mm focal length Dobsonian-type reflectors – three versions:\nA, Orion Funscope – will need a table/tripod, and very soon require a 2.5x or 3x barlow lens, review here.\nB. Skywatcher Heritage 76 is a similar version available in Europe.\nC. Celestron has TWO versions of their similarly-made Firstscope, one of which does not include finderscope and is not recommended; the other is the COSMOS Firstscope that DOES come with a red dot finder and two 3-element eyepieces that are better than the 2-element ones included with the lesser Firstscope.\nAstronomers Without Borders OnesSky telescope (Skywatcher Heritage 130P is identical scope) – Gary Seronik of Sky and Telescope says “I can’t imagine a beginner not being thrilled with the views it provides. It gets my vote for the best bang-for-the-buck beginner’s scope currently available.” Still needs a barlow and a table/tripod on which to place it. Also the focuser is helical, not the typical rack and pinion type. May require some work to avoid stray light entering focuser/eyepiece. Overall good value.\nOrion Starblast 4.5 – Needs barlow lens and table/tripod. Adequate included optics, but will require eventual upgrades of either barlow lens and/or shorter focal length eyepieces. Coma around edges is apparent, but does provide wide field views.\nHIGHLY RECOMMENDED: Orion XT4.5 Dobsonian – Another good option for a first telescope, but not motor driven. Optics will produce good images with little to no coma, and are marginally diffraction limited. Includes usefully sized (and not stopped down!) 6×26 correct image finderscope and TWO good quality Plossl-type eyepieces. Excellent value, though small and requires a crate or box to be at adequate height for most teens and adults even with a chair.\nHIGHLY RECOMMENDED: Orion XT6 or XT8 – Many people will say that the best 6″ f/8 telescope is an 8″ f/6. But don’t neglect the weight, size, and less collimation required for a 6″ scope. An f/8 is more forgiving of poor collimation. But an 8″ scope has more light gathering capability. Both of these are good options. Both will need eventual add on’s and updates, but are good telescopes right out of the box.\nHIGHLY RECOMMENDED: Apertura 8 – Another worthwhile option for a Dobsonian telescope; some people prefer these due to the focuser / eyepiece options and believe they are better than Orion’s offerings.\nTO SUM UP\nKeep in mind that ALL of these telescopes will likely need either some additional accessories, perhaps a bit of work on a tripod, or some other improvements. Remember, retailers are trying to hit prices points so you’ll even CONSIDER them. If they cut a corner, you may need to rebuild that corner, sooner or later. But these are all acceptable options for a telescope you can buy, right now, without having to ask if they are a “good” telescope.\nTL;DR – Buy an 6″ f/8 Dobsonian telescope. If you have other questions or don’t have that kind of budget, go back to the beginning and read the whole post.\nSee what’s up in the night sky every week with “Eyes on the Sky” videos, astronomy made easy.","If you’re an avid stargazer, you’ve probably wondered if bigger telescopes really are better. The short answer is yes, they are. The size of a telescope’s aperture, or the diameter of its main lens or mirror, determines how much light it can gather. The more light a telescope can collect, the fainter the objects you can see, and the more detail you can observe.\nWhen it comes to telescopes, bigger is better. Larger telescopes have a higher resolving power, which means they can see finer details and more clearly distinguish between objects that are close together. They also have a higher light-gathering power, which allows them to see fainter objects and gather more data in a shorter amount of time. This is especially important when observing distant objects, such as galaxies and nebulae, which emit very little light.\nOf course, bigger telescopes also come with some drawbacks. They are more expensive, heavier, and harder to transport than smaller telescopes. They also require more maintenance and calibration to ensure they are functioning properly. However, for serious astronomers and researchers, the benefits of a larger telescope far outweigh the costs and challenges.\nLight Gathering Power\nWhen it comes to telescopes, bigger is usually better. One of the main reasons for this is light gathering power. In this section, we will explore the relationship between telescope size and light gathering power.\nThe Relationship Between Telescope Size and Light Gathering Power\nThe amount of light a telescope can gather depends on the size of its primary mirror or lens. A larger mirror or lens can capture more light, which means that fainter objects can be seen. This is because the light-gathering ability of a telescope is proportional to the area of its primary mirror or lens.\nTo illustrate this, let’s consider two telescopes: one with a 2-inch diameter mirror and another with a 4-inch diameter mirror. The 4-inch telescope has four times the light-gathering area of the 2-inch telescope. This means that the 4-inch telescope can capture four times as much light as the 2-inch telescope, making fainter objects visible.\nIt’s important to note that light gathering power is not the same as magnification. A telescope with a larger primary mirror or lens can gather more light, but it doesn’t necessarily provide higher magnification. Magnification is determined by the eyepiece used with the telescope.\nIn summary, a telescope’s light gathering power is directly proportional to the size of its primary mirror or lens. A larger telescope can capture more light, allowing us to see fainter objects in the night sky.\nWhen it comes to telescopes, resolution is a critical factor that determines how much detail you can see in an image. In simple terms, resolution refers to the ability of a telescope to distinguish between two closely spaced objects. The higher the resolution, the clearer and more detailed the image will be.\nHow Bigger Telescopes Improve Resolution\nOne of the key advantages of larger telescopes is their ability to gather more light, which improves their resolution. When a telescope has a larger aperture, it can collect more light from an object, allowing it to see fainter details that would be invisible to smaller telescopes.\nAnother way that larger telescopes improve resolution is by reducing the effects of atmospheric turbulence. When light passes through the Earth’s atmosphere, it can become distorted, causing images to appear blurry. This effect is known as atmospheric seeing. Larger telescopes are less affected by atmospheric turbulence because they have a larger collecting area, which allows them to capture more light and reduce the impact of atmospheric distortion.\nFinally, larger telescopes can produce sharper images because they have a higher angular resolution. Angular resolution refers to the smallest angle that can be resolved by a telescope. The larger the telescope’s aperture, the smaller the angle it can resolve, resulting in sharper images with more detail.\n|Telescope Size||Angular Resolution|\n|10 cm||2.5 arcseconds|\n|20 cm||1.25 arcseconds|\n|40 cm||0.625 arcseconds|\nAs you can see from the table above, larger telescopes have a significantly higher angular resolution than smaller ones. This means that they can resolve finer details in an image and produce sharper, clearer pictures.\nOverall, the larger the telescope’s aperture, the better its resolution and the more detail you can see in an image. If you want to see the faintest and most detailed objects in the night sky, a larger telescope is the way to go.\nWhen it comes to telescopes, sensitivity is a crucial factor. Sensitivity refers to the minimum signal that a telescope can distinguish above the random background noise. In other words, it is a measure of how well a telescope can detect faint objects.\nThe Importance of Sensitivity in Astronomy\nWithout sensitivity, astronomers would not be able to detect many of the celestial objects that exist in the universe. This is especially true for objects that are very far away or emit very little light, such as distant galaxies, brown dwarfs, and exoplanets.\nFurthermore, sensitivity is important for studying the properties of these objects, such as their composition, temperature, and motion. By detecting faint signals, astronomers can gather more data and make more accurate measurements.\nHow Bigger Telescopes Improve Sensitivity\nOne of the main advantages of larger telescopes is that they are more sensitive than smaller ones. This is because a larger primary mirror or lens can gather more light, which increases the signal-to-noise ratio.\nFor example, the Australia Telescope National Facility states that “all other things being equal, a telescope of larger primary mirror or lens is more sensitive than one with a smaller primary.” This means that a larger telescope can detect fainter objects than a smaller one.\nIn addition, larger telescopes can also have better spatial resolution, which allows astronomers to distinguish between objects that are closer together. This is important for studying complex systems such as star clusters, galaxies, and planetary systems.\nOverall, sensitivity is a critical factor in astronomy, and larger telescopes have a clear advantage when it comes to detecting and studying faint objects.\nCost and Practicality\nWhile bigger telescopes offer many advantages, there are also significant costs and practical limitations to building and operating them.\nThe High Cost of Building and Operating Bigger Telescopes\nBuilding and operating a large telescope is an expensive undertaking. The cost of the mirror alone can run into the hundreds of millions of dollars, and that’s before you factor in the cost of the telescope’s support systems, including the dome, the mount, the instrument package, and the control systems.\nFurthermore, the cost of operating a large telescope can be just as high as the cost of building it. The telescope must be staffed around the clock, and the cost of maintaining and upgrading the telescope’s hardware and software can also be significant.\nDespite these high costs, many astronomers believe that the benefits of large telescopes outweigh the costs. Large telescopes can help us answer some of the biggest questions in astronomy, and they can also inspire the next generation of scientists and engineers.\nThe Practical Limitations of Building and Operating Bigger Telescopes\nBuilding and operating a large telescope also comes with practical limitations. For example, the telescope must be located in a place with clear skies and minimal light pollution. This often means building the telescope on a remote mountaintop or in a desert.\nFurthermore, the telescope must be designed to withstand the harsh conditions of its location. This can include high winds, extreme temperatures, and even earthquakes. Building a telescope that can withstand these conditions can be a major engineering challenge.\nAnother practical limitation of large telescopes is their size. A large telescope can be difficult to transport and assemble, and it may require a large team of engineers and technicians to operate it.\nDespite these practical limitations, many astronomers believe that the benefits of large telescopes outweigh the challenges. Large telescopes can help us answer some of the biggest questions in astronomy, and they can also inspire the next generation of scientists and engineers."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:181c7818-4587-402e-b688-a315ce76ac55>","<urn:uuid:6d3e2989-5452-42a1-bae8-d1b309d04aa8>"],"error":null}
{"question":"How do immune checkpoint inhibitors work in cancer treatment, and what are the main challenges in their clinical application?","answer":"Immune checkpoint inhibitors work by blocking molecules like PD-1, PD-L1, and CTLA-4 that cancer cells use to escape immune recognition. These antibodies prevent tumor cells from binding to immune cells, thereby exposing cancer cells to immune system attack. For example, anti-PD-1 antibodies like pembrolizumab prevent cancer cells' PD-L1 from binding to T cells' PD-1, while CTLA-4 antibodies like ipilimumab block CTLA-4 binding to B7 to remove immunosuppression. However, there are significant challenges in their application. The treatment currently only works on specific cancer types like melanoma, lung, renal cell, and bladder cancers. Even within these cancer types, not all tumors respond to the treatment. Additionally, researchers face difficulties in maximizing efficacy while minimizing toxicity, and in expanding the treatment to more cancer types.","context":["Hopfner Lab - Research\nTherapeutic Protein Engineering\nMonoclonal antibodies have become a powerful tool in cancer immunotherapy and many antibody-based approaches and novel classes of engineered antibody constructs have been proven successful to eradicate cancer cells. Although our immune system has great potential to eliminate tumors, cancer progression is often accompanied by profound immune suppression that interferes with an effective anti-tumor response and tumor elimination. Thus, the recent development of monoclonal antibodies targeting immune regulatory checkpoints, which are often utilized by cancer cells to escape immune recognition and gain immune resistance, is a highly promising strategy for treating different types of cancers. The development of monoclonal antibodies targeting immune regulatory checkpoint molecules such as PD-1 (programmed cell death-1), its ligand PD-L1 (programmed death-ligand 1) or the “don’t eat me” receptor CD47 has reinvigorated the field of immunotherapy and convincingly demonstrated their immense potential in cancer therapy and clinical efficacy.\nFigure: Schematic overview of mechanism of action. Monoclonal antibodies or derivatives thereof induce target cell killing by activating immune cells through a local immune checkpoint blockade and/or immune cell activating signals.\nOur research is driven by the vision to use our expertise in protein science to improve antibody-based cancer immunotherapy and to optimize the molecules for the medical needs of cancer patients. The main goal of our projects is to develop therapeutic molecules that provide cancer patients a long-term possibility for progression free survival with a low risk of side effects during treatment. To this end, we engineer therapeutic antibodies and derivatives thereof to target a specific tumor antigen with a simultaneous activation or inhibition of immune checkpoints. By this approach, we locally restrict the immune response to the tumor cells and consequently reduce the likelihood of systemic side effects or immune-related adverse events (irAEs).\nAdditionally, our novel multi-specific antibody formats further allow for the engagement and activation of effector cells of the innate and/or adaptive immune system. Finally, we analyze our molecules on patient samples and in in vivo models in cooperation with the group of Marion Subklewe and other clinical collaborators to better judge on their potential to treat hematological and solid malignancies.\nSIRPα-antibody fusion proteins stimulate phagocytosis and promote elimination of acute myeloid leukemia cells.\nPonce LP, Fenn NC, Moritz N, Krupka C, Kozik JH, Lauber K, Subklewe M, Hopfner KP.\nOncotarget. 2017 Feb 14;8(7):11284-11301. PubMed\nCD19-specific triplebody SPM-1 engages NK and γδ T cells for rapid and efficient lysis of malignant B-lymphoid cells.\nSchiller CB, Braciak TA, Fenn NC, Seidel UJ, Roskopf CC, Wildenhain S, Honegger A, Schubert IA, Schele A, Lämmermann K, Fey GH, Jacob U, Lang P, Hopfner KP, Oduncu FS.\nOncotarget. 2016 Dec 13;7(50): 83392-83408. PubMed\nDual-targeting triplebody 33-3-19 mediates selective lysis of biphenotypic CD19+ CD33+ leukemia cells.\nRoskopf CC, Braciak TA, Fenn NC, Kobold S, Fey GH, Hopfner KP, Oduncu FS.\nOncotarget. 2016 Apr 19;7(16):22579-89. PubMed\nEpitope characterization and crystal structure of GA101 provide insights into the molecular basis for type I/II distinction of CD20 antibodies.\nNiederfellner G, Lammens A, Mundigl O, Georges GJ, Schaefer W, Schwaiger M, Franke A, Wiechmann K, Jenewein S, Slootstra JW, Timmerman P, Brännström A, Lindstrom F, Mössner E, Umana P, Hopfner KP, Klein C.\nBlood. 2011 Jul 14;118(2):358-67. PubMed","In 2013, Science ranked cancer immunotherapy as \"the most significant breakthrough of the year\".Cancer treatment has ushered in changes, from the previous focus on the tumor cells themselves, to now open new therapies for the immune system, cancer treatment ideas have changed dramatically.\nIn 2018, the Nobel Prize in Physiology or Medicine was awarded to two researchers, respectively: Dr. Tasuku Honjo, Kyoto University, Japan, and Dr. James Allison, Anderson Cancer Center.\nRecently, the top international medical journal JAMA reviewed the achievements and perplexities we have achieved in cancer immunotherapy in the past few years, and looked forward to possible future research directions.With author M.J.In Friedrich's words, immunotherapy has entered 2.Time 0.\n1, immunotherapy 2.Time 0: Success & Difficulty\nThe awards of Dr. Tasuku Honjo and Dr. James Allison of the Anderson Cancer Center at Kyoto University in Japan stem from the discovery of two immune checkpoint pathways that inhibit CD8 + T cells and prevent them from destroying cancer cells.\nIn the mid-1990s, Allison demonstrated that blocking CTLA-4 protein (cytotoxic T-lymphocyte protein 4) can shrink tumors in mice.Around the same time, Honjo discovered a second immune checkpoint pathway, the PD-1 (programmed cell death protein) pathway.Activated T cells contain PD-1 protein on their surface.Cancer cells bind to the PD-1 protein with their own surface ligands, resulting in inhibition of T cell activity.\nThese immune checkpoint pathways were originally designed to prevent T cells from attacking healthy tissues, but were exploited by tumor cells to evade the encirclement of T cells.These studies prompted scientists to begin the development of immune checkpoint inhibitors.\nExisting immune checkpoint inhibitors include anti-PD-1 monoclonal antibody, anti-PD-L1 monoclonal antibody, and anti-CTLA-4 monoclonal antibody.\nRepresentative anti-PD-1 mAbs include pembrolizumab and nivolumab.Both bind to the PD-1 protein on T cells and prevent it from binding to the ligand protein PD-L1 (programmed cell death ligand) on the surface of cancer cells, thereby exposing the cancer cells to the view of the immune system.Other checkpoint inhibitors, such as atezolizumab, achieve the same effect by targeting PD-L1, a ligand on the surface of cancer cells.CTLA-4 antibodies, including ipilimumab, work in a manner similar to anti-PD-1/PD-L1.Activation of T cells depends on dual activation of the first signal (antigen/antibody complex formation) and the second signal (B7-mediated activation signal), whereas binding of CTLA4 to B7 inhibits T cell activation.Ipilimumab, a targeted immunotherapy drug, can just block the binding of CT-LA4 to B7, remove immunosuppression and mobilize specific anti-tumor immune responses.These immune checkpoint inhibitors, as well as several others, are now approved by the US FDA.\nDespite the great potential of these immune checkpoint inhibitors, it is currently only possible to act on certain specific types of cancer, such as melanoma, lung, renal cell, and bladder cancers, as well as cancers with microsatellite instability with high mutational load.Even in these tumor types, not all tumors respond to checkpoint inhibitors.\nTherefore, there are two major difficulties facing researchers: 1) how to use it to maximize the efficacy of immune checkpoint inhibitors and minimize toxicity; 2) how to expand the use of immune checkpoint inhibitors to a wider range of cancer types.\n2, Biomarkers used to predict efficacy: CD8 + T cell infiltration and tumor mutation burden\nThere are currently nearly 2,000 clinical trials of immune checkpoint inhibitors for single and combination therapies.\nTo enhance the efficacy of immunotherapy, many trials have combined immune checkpoint inhibitors of CTLA-4 and PD-1/PD-L1.Although this combination therapy is usually more effective than monotherapy, the risk of adverse events should not be underestimated.There is an urgent need for reliable biomarkers in the medical community to screen out patients suitable for single and combination therapies.\nDr. Padmanee Sharma of the Anderson Cancer Center and her husband Allison worked on developing and improving checkpoint inhibitors for many years.She is conducting a new study to assess whether CD8 + T cell infiltration in tumors can be used as a biomarker to predict the efficacy of immune checkpoint inhibitors.\nThe premise of this study is that \"hot\" tumors with high CD8 + T cell infiltration will respond to a PD-1 inhibitor such as nivolumab, whereas \"cold\" tumors with low levels of CD8 + T cells may require additional help with CTLA-4 inhibitors to allow T cells to enter the tumor.Based on this premise, Dr. Sharma divided the volunteers into two groups using nivolumab monotherapy or nivolumab and ipilimumab combination therapy, and the grouping principle depended on whether the CD8 + T cell infiltration was high or low in the volunteers' tumors.\nAll types of patients with advanced metastatic cancer are involved in clinical trials.Sharma and colleagues hypothesized that a proportion of all solid tumors (even those considered \"cold\" like pancreas and prostate cancer) may have the properties of high CD8 + T cells, which may make them responsive to checkpoint blockade.\nAnother biomarker currently evaluated by the FDA is tumor mutation burden (TMB).Dr. Matthew Hellmann, who commemorates the Sloan Kettering Cancer Center, said that every mutation in the tumor has the potential to produce \"new antigens\" that can act as \"warning lights\" for the immune system to alert it to cancer cell invasion.\nA few years ago, Hellmann and colleagues demonstrated that treatment with pembrolizumab works well in NSCLC patients with higher TMB.Hellmann said, \"Since then, a positive correlation between high TMB and immune checkpoint inhibitor response has been found in most cancer types studied.A prime example of this situation can be found in patients with genetic disease caused by a deficiency of mismatch repair genes, which can also lead to the development of tumors with high microsatellite instability.In 2017, pembrolizumab received FDA approval for high microsatellite instability tumors, regardless of where the cancer originated.This is the first FDA approval based on a biomarker rather than a single tumor indication.\nHowever, no biomarker is perfect.For example, four different immunohistochemical tests for detecting high PD-L1 tumor expression have received FDA approval for certain cancer indications (including non-small cell lung cancer), but PD-L1 expression does not always accurately predict which patients will or will not respond to immune checkpoint inhibitors.\nHellmann states, \"Taken together, various biomarkers may predict efficacy from different perspectives.\"\n3, gut microbiota as biomarkers and modulators\nAccording to several recent studies by Houston, Chicago and French researchers, the diversity and composition of the gut microbiome is another factor influencing the response to immune checkpoint inhibitors.Researchers are now beginning to investigate the use of gut microbes as biomarkers and modulators of the checkpoint blockade response.\nDr. Jennifer Wargo at Anderson Cancer Center will collaborate with the Parker Institute and Seres Therapeutics to conduct phase I clinical trials to explore both possibilities.The study will randomize patients with metastatic melanoma to use 1 of 3 therapies before starting anti-PD-1 therapy: oral tablets containing fecal microbiota from anti-PD-1 responders; oral tablets containing a specific mixture of microorganisms (mimicking the microbial components found in anti-PD-1 responders) or placebo.\nIn another phase II trial, Dr. Hassane Zarour of the University of Pittsburgh is investigating whether fecal microbiota transplantation in long-term anti-PD-1 responders can improve the efficacy of PD-1 inhibitors in patients with resistant PD-1 melanoma.\nSimilar to the Wargo research team, the aim of the study by Zarour and coworkers was also to determine which specific microbiota could modulate the response to immune checkpoint inhibitors.\n4, breaking the \"ceiling\" that limits efficacy\nDr. James Gulley, Director, Division of Urogenital Malignancies, NCI, Bethesda, said, \"Responding to immune checkpoint inhibitors requires T cells to enter the tumor microenvironment.If T cells are no longer in the tumor microenvironment, they must be placed in the tumor microenvironment anyway.\"Many tumors escape T cell infiltration, and scientists are exploring ways to solve this problem.A molecular marker that prevents T cells from entering the tumor microenvironment is transforming growth factor beta (TGF-β), a cytokine that has many functions, including immunosuppression.To overcome this obstacle, Gulley has been working on M7824.M7824 is an enhanced version of PD-L1 that can antagonize both TGF-β and PD-L1, and both systems have a synergistic effect at the same time.\nGulley and his team have started phase I and II trials of M7824 for patients with metastatic castration-resistant prostate cancer, a cold tumor.This novel experimental design accelerates the evaluation of four different experimental reagents targeting five different immune targets.Gulley said, \"If we study each drug separately in a subsequent phase II study, it will take a long time.\"\nIn this trial, the Gulley R & D team set up three study groups, the enrollment of the three study groups will be sequentially advanced, and after the safety of the drug combination used in the previous study group is confirmed, a new immunotherapy formulation will be subsequently added by one of the study groups.All patients were treated with M7824.\n\"This is an adaptive trial design, and we keep advancing, adding another drug until we get a clear clinical signal,\" Gulley said.\nAnother innovative approach is to use CD40 antibody therapy to prime the immune system and sensitize tumors to immune checkpoint inhibitors.Dr. Robert Vonderheide, director of the Alberramson Cancer Center at the University of Pennsylvania, said that his laboratory data show that CD40 can be used as a \"converter\" to convert cold tumors that do not produce enough immune responses into hot tumors that can produce immune responses.The CD40 antibody is the agonist that turns this switch on.In addition, there are experimental data from other scientists to support this.\nVonderheide is collaborating with the Parker Institute in Phase Ib and Phase II clinical trials to investigate whether anti-CD40 antibodies can improve immune checkpoint blockade in patients with pancreatic cancer.Pancreatic cancer is the least \"cold\" type of immunotherapy.These patients will be randomly assigned to receive anti-CD40 antibody, nivolumab or anti-CD40 antibody and nivolumab combination.In addition, during this process, all patients who have not previously received treatment for metastatic disease will also undergo standard chemotherapy.\nIn addition, clinical trials testing the combination of anti-CD40 antibodies with nivolumab for metastatic melanoma and lung cancer patients with ineffective checkpoint blockade are also in progress.Scientists hope that this approach will work even in the most difficult cancers.\nVonderheide, \"We used these methods to try to break the\" glass ceiling that can be achieved by immune checkpoint inhibitors.\"\n5, Immune Checkpoint Inhibitors for Early Tumors\nResearchers are also studying the effect of using immune checkpoint inhibitors in early cancers. Scientists believe that early tumors may not have developed a good defense against immune attacks, so immune checkpoint inhibitors may respond better to early tumors.\nDr. Suzanne Topalian of the Bloomberg-Kimmel Institute for Cancer Immunotherapy, Johns Hopkins University School of Medicine, noted that last year nivolumab had received FDA approval for the adjuvant treatment of lymph node metastases after surgical resection in melanoma patients.\nTopalian and some other scientists are also actively exploring the use of immune checkpoint inhibitors for the neoadjuvant treatment of various types of cancer.The immune checkpoint inhibitor moved from surgery to pre-surgery.Topalian's group recently published the results of a pilot study that reported for the first time data on the preoperative use of nivolumab in patients with untreated early stage NSCLC.\n6, safety deserves attention\nIn addition, immune-related adverse events (irAEs) are also the focus of current research by scientists.Immune-related adverse events may occur in any organ, but most commonly in the skin, gastrointestinal tract, endocrine glands, and liver.\nMost irAEs can be mitigated by dose modification, interruption of treatment, or use of steroids according to clinical practice guidelines.However, rare and irreversible conditions such as type 1 diabetes may still occur.Dr. Jeffrey Bluestone, professor at the University of California, San Francisco and CEO of the Parker Institute, said that the Parker Institute is developing a strategic plan to try to solve some of the problems surrounding irAE.\nBluestone also stated that the purpose of this strategic plan is to gain a deeper understanding of the biological, biochemical, and genetic basis behind these adverse events, thereby identifying which patients are at greatest risk of adverse events and trying to avoid them.In the process, a deeper understanding of the causes of type 1 diabetes in the absence of these therapies is also possible.\nAlthough various barriers limit the efficacy and safety of checkpoint inhibitors, Bluestone highlights that these drugs have revolutionized cancer therapy.Above all, it's not over, it's just beginning.\nOriginal: Immunotherapy 2.0: Improve the response to Checkpoint Inhibitors\nAuthor: M.J. Friedrich"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:e49a9a29-4728-4934-91ce-f10273918dc5>","<urn:uuid:3b26cf19-2f1b-4b86-8713-7f2c1b771d08>"],"error":null}
{"question":"I've been reading about different research methods. What's the main difference between translation theory studies and psychological experiments in terms of methodology?","answer":"Translation theory studies primarily rely on historical analysis and theoretical examination of texts across centuries, as evidenced by collections of translation writings from the fifth century BCE to the nineteenth century. In contrast, psychological experiments use controlled procedures with specific treatment conditions, requiring random assignment of participants and careful measurement of variables to test hypotheses. While translation theory focuses on interpreting and analyzing historical perspectives, psychological experiments emphasize measurable, observable behavior and systematic manipulation of variables.","context":["The Bloomsbury spouse to Cognitive Linguistics is a accomplished and obtainable reference source to analyze in modern cognitive linguistics. Written through top figures within the box, the amount offers readers with an authoritative assessment of equipment and present learn themes and destiny directions.\nThe quantity covers all of the most crucial matters, suggestions, pursuits and techniques within the box. It devotes area to taking a look in particular on the significant figures and their contributions. it's a entire source for postgraduate scholars and researchers operating inside cognitive linguistics, psycholinguistics and people extra usually in language and cognition.\nBy Hans Arens\nThis quantity includes a fragment from Aristotle’s Peri Hermeneias [16a1–17a7], with a translation into English and a observation. This fragment is essential to the certainty of Aristotle’s puzzling over language. it really is through (translations of) commentaries on Aristotle’s textual content via students among 500 and 1750, exhibiting how his textual content was once perceived through the years. The commentaries are by means of Ammonius, Boethius, Abelaerd, Albertus Magnus, Thomas Acquinas, Martinus de Dacia, Johannes a S. Thoma, and James Harris. every one statement is in flip commented upon by way of the compiler of this quantity.\nArticles collected within the quantity specialize in conventional and modern debates in the philosophy of language, and at the interfaces among linguistics, philosophy, and good judgment. the subjects of person contributions conceal such diversified matters as analytic debts of the a priori and implicit definitions, medieval and modern theories of fallacy, game-theoretical semantics, modal video games in average language and literary semantics, possible-world theories and paradoxes concerning based propositions, extensions to Dynamic Syntax, semantics of right names, judgement-dependence, tacit wisdom and linguistic realizing, ontology in semantics, implicit wisdom and idea of which means, and lots of extra. The multitude of themes indicates that the convergence of linguistic, philosophical, formal, and cognitive techniques opens new examine views inside modern philosophy of language and linguistics. the quantity comprises contributions via (among different authors): Luis Fernandez Moreno (Madrid), Chris Fox (Essex), Ruth Kempson (London), Alexander Miller (Birmingham), Arthur Sullivan (Newfoundland), Mieszko Talasiewicz (Warsaw).\"\nBy James McGlasson\nBy Cambridge School Classics Project\nThe best Latin path around the globe publication I starts within the urban of Pompeii presently ahead of the eruption of Vesuvius. publication I is complete color all through, with a transparent format of reports and language notes. that includes a word list for fast reference and comprehension questions, the publication additionally encompasses a complete rationalization of language issues and grammar perform exercises.\nBy Douglas Robinson\nDouglas Robinson deals the main finished number of translation thought readings on hand up to now, from the Histories of Herodotus within the mid-fifth century earlier than our period to the top of the 19th century. the result's a startling panoply of wondering translation around the centuries, overlaying such issues because the most sensible form of translator, difficulties of translating sacred texts, translation and language educating, translation as rhetoric, translation and empire, and translation and gender.\nThis pioneering anthology includes 124 texts through ninety authors, nine of them ladies. 16 texts by way of four authors look the following for the 1st time in English translation; 17 texts by means of nine authors seem in thoroughly new translations. each access is supplied with a bibliographical headnote and footnotes.\nIntended for lecture room use in background of Translation concept, heritage of Rhetoric or background of Western inspiration classes, this anthology also will turn out worthwhile to students of translation and people drawn to the highbrow heritage of the West.\nThis paintings combines interdisciplinary wisdom and adventure from learn fields of psychology, linguistics, audio-processing, desktop studying, and machine technological know-how. The paintings systematically explores a unique study subject dedicated to computerized modeling of character expression from speech. For this objective, it introduces a singular character overview questionnaire and provides the result of large labeling classes to annotate the speech info with character exams. It presents estimates of the large five character characteristics, i.e. openness, conscientiousness, extroversion, agreeableness, and\nneuroticism. in keeping with a database outfitted at the questionnaire, the publication provides types to distinguish diverse character varieties or sessions from speech automatically.\nReleased less than the pseudonym, Clive Hamilton, Spirits in Bondage was once C. S. Lewis' first ebook. published in 1919. many of the poems seem to have been written among 1915 and 1918, a interval in which Lewis used to be a pupil less than W. T. Kirkpatrick, an army trainee at Oxford, and a soldier serving within the trenches of worldwide conflict I. Their outlook varies from Romantic expressions of affection for the sweetness and straightforwardness of nature to cynical statements in regards to the presence of evil during this international. In a September 12, 1918 letter to his good friend Arthur Greeves, Lewis stated that his e-book was once, \"mainly strung round the concept that i discussed to you before--that nature is absolutely diabolical & malevolent and that God, if he exists, is open air of and towards the cosmic arrangements.\" In his cynical poems, Lewis is facing an identical questions about evil in nature that Alfred Lord Tennyson explored from a place of stricken religion in \"In Memoriam A. H.\" (Stanzas 54f). In a letter written might be to reassure his father, Lewis claimed, \"You be aware of who the God I blaspheme is and that it isn't the God that you just or I worship, or the other Christian.\" no matter what Lewis believed at the moment, the perspective in lots of of those poems is sort of varied from the angle he expressed in his many Christian books from the Thirties on. makes an attempt in videos and on degree performs to painting Lewis as a sheltered professor who knew little approximately soreness until eventually the loss of life of his spouse past due in lifestyles, need to deal not just with the various tragedies he skilled from a boy on, but in addition with the hectic concerns he confronted in lots of of those early poems.\nBy J. H. Allen, J. B. Greenough\nA venerable source for greater than a century, Allen and Greenough's New Latin Grammar continues to be looked via scholars and academics because the best Latin reference grammar to be had. Concise, entire, and good geared up, it really is unmatched extensive and readability, putting a wealth of recommendation on utilization, vocabulary, diction, composition, and syntax inside effortless succeed in of Latin students in any respect levels.\nThis sourcebook's three-part therapy begins with phrases and types, overlaying components of speech, declensions, and conjugations. the second one half, syntax, explores instances, moods, and tenses. The concluding part deals info on archaic usages, Latin verse, and prose composition, between different matters. large appendixes characteristic a thesaurus of phrases and indexes. scholars of heritage, faith, and literature will locate lasting worth during this modestly priced variation of a vintage advisor to Latin.\nBy Noam Chomsky\nThis publication is an exceptional contribution to the philosophical examine of language and brain, by means of some of the most influential thinkers of our time. In a sequence of penetrating essays, Chomsky cuts in the course of the confusion and prejudice that has contaminated the research of language and brain, bringing new recommendations to standard philosophical puzzles and clean views on problems with common curiosity, starting from the mind-body challenge to the unification of technology. utilizing a number innovative and deceptively easy linguistic analyses, Chomsky defends the view that wisdom of language is inner to the human brain. He argues right examine of language needs to take care of this psychological build. in accordance with Chomsky, as a result, human language is a \"biological item\" and will be analyzed utilizing the technique of the sciences. His examples and analyses come jointly during this publication to provide a different and compelling point of view on language and the brain.","Conducting research to describe, explain, predict, and control behavior. These goals form the basis of the psychological enterprise.\nThe scientific study of the behavior of individuals and their mental processes.\npeople begin life with a blank slate. psychological theory asserting that knowledge comes only or primarily via the flexibility of sensory experience. John Locke and Aristotle.\npioneer of the behaviorist perspective of psychology\na scientific approach that limits the study of psychology to measurable or observable behavior.\ntheory asserting that human behavior and thinking are largely determined by irrational, unconscious drives.\npsychologists who developed the psychodynamic perspective. His model was the first to recognize that human nature is not always rational and that actions may be driven by motives that are not in conscious awareness.\npsychological perspective primarily concerned with observable behavior that can be objectively recorded and with the relationships of observable behavior to environmental stimuli.\nperspective that stresses human thought and the processes of knowing, such as attending, thinking, remembering, expecting, solving problems, fantasizing, and consciousness.\nmodel in which behavior is explained in terms of past experiences and motivational forces; actions stem from inherited instincts, biological drives, and attempts to resolve conflicts between personal needs and social requirements.\nalso known as human development, is the scientific study of systematic psychological changes, emotional changes, and perception changes that occur in human beings over the course of their life span.\n34% of psychology PhD's are applied in this subfield; a branch of psychology that studies, assesses, and treats people with psychological disorders\na subdiscipline of psychology exploring internal mental processes. It is the study of how people perceive, remember, think, speak, and solve problems.\n28% of psychologists work within this setting; devoted to counseling children in elementary and secondary schools who have academic or emotional problems\na branch of psychology that assists people with problems in living (often related to school, work, or marriage) and in achieving greater well-being\nThe application of psychological concepts and methods to optimizing human behavior in workplaces\nstructuralism; in 1879 founded first psychology laboratory in world at University of Leipzig; introspection, basic units of experience\nbehaviorism; pioneer in operant conditioning; behavior is based on an organism's reinforcement history; worked with pigeons\nthe perspective on mind and behavior that focuses on the examination of their functions in an organism's interactions with the environment.\ncognitive psychology; disagreed with Skinner about language acquisition, stated there is an infinite # of sentences in a language, humans have an inborn native ability to develop language\nstudy of the structure of the mind and behavior; the view that all human mental experience can be understood as a combination of simply elements or events.\na gestalt psychologist who argued against dividing human thought and behavior into discrete structures\nthe school of psychology that emphasizes the tendency to organize perceptions into meaningful wholes\n1902-1987; Field: humanistic; Contributions: founded person-centered therapy, theory that emphasizes the unique quality of humans especially their freedom and potential for personal growth, unconditional positive regard, fully functioning person\nthe generation effect\nSelf-produced information (generated) is better remembered than information that is not self-produced.\nExamples in research: fill in the missing letter for the pair of words or read the pair of words aloud. Doctor - N rse\nApplied Examples (studying): Making note cards, creating sample test questions.\nnight, sleeping helps memory retention\nthe best time of the day to study and why\nThe biological process through which memories are transformed from a transient and fragile status to a more permanent and robust state; according to most researchers, occurs over the course of several hours.\nCombination of both, mainly testing for best memory retention\nreading/studying vs. testing the material?\nA controlled procedure in which at least two different treatment conditions are applied to subjects whose behaviors are then measured and compared to test a hypothesis about the effects of the treatments on behavior.\na tentative and testable explanation of the relationship between two or more events or variables; often stated as a prediction that a certain outcome will result from specific conditions.\nan organized set of concepts that explains a phenomenon.\na subset of a population selected as participants in an experiment.\nvariable that the researcher manipulates with the expectation of having an impact on values of the other variable.\na variable that the researcher measures to assess the impact of the variation in the other variable.\na definition of a variable or condition in terms of the specific operation or procedure used to determine its presence.\ngroup exposed to the treatment or experiences a manipulation of the independent variable.\ngroup in an experiment not exposed to a treatment or does not experience a manipulation in the independent variable.\nchange in behavior in the absence of an experimental manipulation; an inert substance given to the control group in an experiment.\nAn experimental procedure in which both the research participant and the research staff are ignorant about whether the participants have received the treatment or the placebo.\nrandom assignment of experimental units to treatments or of treatments to trials.\nCase study method\nAn in-depth examination of an individual that often involves compiling and analyzing information from a variety of sources such as observing, testing, and interviewing the person/people who knew the individual.\na research method that involves gathering information from people through the use of surveys or questionnaires.\nNaturalistic observation method\nmethod of research based on careful, unobtrusive observation of behavior in natural settings.\na research project designed to discover the degree to which two variables are related to each other.\nEx post facto method\nan experiment where the researcher examines the effect of a naturally occurring treatment after it has occurred.\nin an experiment, a variable, other than the independent variable, that could influence the dependent variable.\ncell in the nervous system specialized to receive, process, and/or transmit information to other cells.\none of the branched fibers of neurons that receive incoming signals.\nthe extended fiber of a neuron through which nerve impulses travel from the soma to the terminal buttons.\nnerve impulse activated in a neuron that travels down the axon and causes neurotransmitters to be released into a synapse.\nthe polarization of cellular fluid within a neuron, which provides the capability to produce an action potential.\nthe process during the action potential when sodium is rushing into the cell causing the interior to become more positive.\nthe level of stimulation required to trigger a neural impulse.\nthe period of rest during which a new nerve impulse cannot be activated in a segment of an axon.\ninsulating material that surrounds axons and increases the speed of neural transmission.\nchemical messengers that traverse the synaptic gaps between neurons. When released by the sending neuron, they travel across the synapse and bind to receptor sites on the receiving neuron, thereby influencing whether that neuron will generate a neural impulse.\nprinciple that the action potential in a neuron does not vary in strength; the neuron either fires at full strength or it does not fire at all.\n\"morphine within\"--natural, opiatelike neurotransmitters linked to pain control and to pleasure.\nSmall knobs at the end of axons that secrete chemicals called neurotransmitters.\nInformation entering a neuron that signal it not to fire or to fire.\nthe junction between the axon tip of the sending neuron and the dendrite or cell body of the receiving neuron.\nCentral Nervous System (CNS)\nthe body's primary information processing system; includes the brain and spinal cord.\nPeripheral Nervous System (PNS)\npart of the nervous system composed of the spinal cord and cranial nerves that connect the body's sensory receptors to the CNS and the CNS to the muscles and glands.\nAutonomic Nervous System\nsubdivision of the PNS that controls the body's involuntary motor responses by connecting the sensory receptors to the CNS and the CNS to the smooth muscle, cardiac muscle, and glands.\nSomatic Nervous System\nsubdivision of the PNS that connects the CNS to the skeletal muscles and the skin.\nthe division of the autonomic nervous system that arouses the body, mobilizing its energy in stressful situations.\nsubdivision of the autonomic nervous system that monitors the routine operation of the body's internal functions and conserves and restores body energy.\nbrain structure that regulates the body's basic life processes.\nregion of the brain that regulates higher cognitive and emotional functions.\nregion of the brain located above the lateral fissure and in front of the central sulcus; involved in motor control and cognitive activities.\nregion of the brain behind the frontal lobe and above the lateral fissure; contains somatosensory cortex; processes information about temperature, touch, body position, and pain.\nrearmost region of the brain; contains primary visual cortex.\nregion of the brain found below the lateral fissure; contains the auditory cortex.\nregion of the cerebral cortex that controls the action of the body's voluntary muscles.\nregion of the parietal lobes that processes sensory input from various body areas.\nparts of the cerebral cortex in which many high-level brain processes occur.\ncontrols language expression-an aread of the frontal, usually in the left hemisphere, that directs the muscle movements involved in speech.\ncontrols language reception-a brain area involved in language comprehension and expression;usually in the left temporal lobe.\nregion of the brain attached to the brain stem that controls motor coordination, posture, and balance as well as the ability to learn control of body movements.\nthe part of the limbic system that is involved in the acquisition of explicit memory.\nthe region of the brain that regulates emotional behavior, basic motivational urges, and memory, as well as major physiological functions.\nregion of the brain stem that alerts the cerebral cortex to incoming sensory signals and is responsible for maintaining consciousness and awakening from sleep.\nbrain structure that regulates motivated behavior (eating and drinking), and homeostasis.\nbrain structure that relays sensory impulses to the cerebral cortex.\npart of the limbic system that controls emotion, aggression, and the formation of emotional memory.\nthe mass of nerve fibers connecting the two hemispheres of the cerebrum.\na condition in which the two hemispheres of the brain are isolated by cutting the connecting fibers (mainly those of the corpus callosum) between them.\ncollection of glands that secrete hormones into the blood which regulate growth, development, and homeostasis.\none of the chemical messengers, manufactured and secreted by the endocrine glands, that regulates metabolism and influence body growth, mood, and sexual characteristics.\nthe endocrine system's most influential gland. Under the influence of the hypothalamus, the pituitary regulates growth and controls other endocrine glands.\nbelief that certain ideas or personal characteristics are innate or inborn; hard-wired. Emmanuel Kant was proponent.\nfather was a wealthy business man. Studies medicine at Harvard, suffered from depression, and had an interest in the nature of free will. Wrote the Principles of Psychology.\ntendency for a measure within an experimental design to produce the same results from the same things.\ntendency for a measure within an experimental design to accurately capture what's intended.\ncomponent of the nervous system which sends signals toward the CNS.\ncomponent of the nervous system which sends signals to others of its kind as well as motor neurons.\ncomponent of the nervous system, sends information from the CNS to the muscles and glands.\ncells which hold together and facilitate neural transmissions. Guide, protect, and prevent harm to neurons.\nneurotransmitter which in excess can contribute to schizophrenia but lack can cause Parkinson's disease.\nneurotransmitter; deficiency of it can cause depression.\nLeft/Right hemispheres of the brain\ntechnique for producing temporary inactivation of brain areas using repeated pulses of magnetic stimulation.\na recording of the electrical activity of the brain.\ntechnique that uses narrow beams of X-rays passed through the brain to assemble images.\nbrain image produced by device that obtains detailed pictures of activity by recording the radioactivity emitted by cells during different cognitive/behavioral processes.\nMRI and fMRI\nbrain imaging using magnetic fields and radio waves; improved technique which detects magnetic changes in the flow of blood to the brain.\nprocesses that organize information in the sensory image and interpret it as having been produced by properties of objects or events in the external, 3-D world.\nthe process by which stimulation of a sensory receptor gives rise to neural impulses that result in an experience, or awareness, of conditions inside or outside the body.\nthe process that puts sensory information together to give the perception of a coherent scene over the whole visual field.\nperceiving objects as unchanging (having consistent lightness, color, shape, and size) even as illumination and retinal images change.\na Gestalt principle of organization holding that there is an innate tendency to perceive incomplete objects as complete and to close or fill gaps and to perceive asymmetric stimuli as symmetric.\na type of apparent movement based on the rapid succession of still images, as in motion pictures.\nthe focusing of conscious awareness on a particular stimulus.\na state of focused awareness on a subset of the available perceptual information.\na mental predisposition to perceive one thing and not another; temporary readiness to perceive or react to stimulus in a particular way.\ninformation processing guided by higher-level mental processes, as when we construct perceptions drawing on our experience and expectations.\nanalysis that begins with the sensory receptors and works up to the brain's integration of sensory information.\n-The spontaneous redirection of attention. -Attention can be captured by changes in movement, abrupt onsets, visual color, auditory pitch, etc.\na determinant of why people select some parts of sensory input for further processing; it reflects the choices made as a function of one's own goals.\nOptical Illusion; sensory stimuli that can be perceived in different ways.\ninappropriate interpretations of physical reality. Often occur as a result of the brain's using otherwise adaptive organizing principles.\nLaw of proximity\na Gestalt principle of organization holding that (other things being equal) objects or events that are near to one another (in space or time) are perceived as belonging together as a unit.\nLaw of similarity\nThe Gestalt principle that we tend to group similar objects together in our perceptions.\nLaw of good continuation\nFrom Gestalt Psychology, it is the tendency for elements appearing to follow in the same direction (such as a straight line or a simple curve) to be grouped together.\nLaw of common fate\na Gestalt principle of organization holding that aspects of perceptual field that move or function in a similar manner will be perceived as a unit.\nan illusion of movement created when two or more adjacent lights blink on and off in quick succession.\nThe apparent movement of a stationary pinpoint of light displayed in a darkened room.\ndelay in reaction time when color of words on a test and their meaning differ.\nstates of consciousness that require little attention and do not interfere with other ongoing activities.\nWhen information processing involves conscious, alert awareness and mental effort focused on achieving a particular goal.\nThe theory that we act to reduce the discomfort we feel when two of our thoughts are inconsistent. For example, when our awareness of our attitudes and our actions clash, we can reduce the feeling by changing our attitudes."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:07e292ef-ff08-43c1-bc8b-3f96ff90b7ad>","<urn:uuid:f37525f4-44b0-4b0a-92cc-a18ab9a39103>"],"error":null}
{"question":"¿Qué es más valioso para hacer joyas: el cuarzo o el corindón? ¡Necesito saberlo!","answer":"Corundum is more valuable than quartz for jewelry. Corundum gemstones include rubies and sapphires, which are among the most highly valued gemstones, while quartz gemstones like amethysts and citrines, although beautiful, are less valuable. Additionally, corundum has a higher hardness rating (9 on Mohs scale) compared to quartz (7 on Mohs scale), making it more durable for jewelry use.","context":["Gems and precious stones are generally beautiful minerals that can be cut and polished to create jewelry items. Some gems are not classified as minerals, and these include lapis-lazuli, amber, and jet. The values of gems and precious stones are determined by their luster, clarity, and rarity. Gemstones that are highly valued include diamonds, rubies, sapphires, and emeralds.\nGemstones Information and Gem Pictures\nAlmost all the gems and precious stones that have been found were discovered beneath the surface of the Earth. Different types of gemstones originate from different formation processes. Many gemstones are formed by the reaction between underground minerals and water. A solution will be created when minerals dissolve in water, and this solution will become a gemstone when it solidifies. When minerals that are rich in silica are dissolved in water, opals, amethysts, and agates will be formed, and minerals that are rich in copper will create turquoise, malachite, and azure. Other geological processes that lead to the creation of gemstones are crystallization in magma, and the formation of metamorphic rocks and olivine. Diamonds are formed by extreme heat and pressure in the mantle of the Earth, and they are brought to the surface by volcanic eruptions.\nFormation of Gems\nThere are many types of gems and precious stones, and they are divided into several major categories. Diamond is the hardest natural substance that has been found on Earth, and it is the most highly valued of all gemstones. The second hardest type of gemstone is the corundum, and gems that fall into this category are rubies and sapphires. Chrysoberyls, such as alexandrite and cat’s eye, are also very hard gemstones, and they are admired for their abilities to change color when exposed to different lighting. Onyx, citrines, amethysts, and carnelians are quartz gemstones, and they come in many different colors. Beryls share certain similarities with quartz stones and chrysoberyls, and they are transparent when they are in their simplest form. Aquamarines and emeralds are colored variants of beryls. Other major types of gemstones are jade, feldspar, and organic gemstones, which include amber and pearl.\nTypes of Gemstones\nGemstone dealers and enthusiasts have to understand certain gem-related terms to communicate intelligently about gemstones. These terms will help them provide better information about the characteristics, conditions, and values of gemstones. Some of the basic terms include “carat”, which is the weight of a gemstone; “density”, which measures the ratio of a gemstone’s weight to its volume; “hardness”, which refers to a gemstone’s resistance to scratches and abrasion; “luster”, which is a gemstone’s ability to reflect light; and “inclusions”, which refers to substances that are found inside a gemstone, such as liquids, bubbles, or other minerals.\nGems and precious stones have been regarded as precious objects since thousands of years ago. While some cultures in the past used them for the creation of jewelry items, others used them as healing objects. Even up to today, many people believe that gems and precious stones have metaphysical and healing powers.\nGems and Precious Stones Resources:\nHistory of Gemstones\nInternational Gem Society\nGemological Institute of America\nGuide to Gems & Precious Stones","Moh’s Scale of Hardness\nThere are two methods to measure the hardness of materials: scratch hardness and static load indentation hardness. Scratch hardness, also known as Mohs hardness, is a relative hardness and is rather rough.\nIt uses ten natural minerals as standards. The hardness order does not represent the absolute size of a particular mineral’s hardness, but indicates that a mineral of higher hardness order can scratch a mineral of lower order. The hardness of other minerals is determined by comparison with these standard minerals.\nThe unit of Mohs hardness is kilogram-force per square centimeter (kgf/cm²), denoted as [Pa]. It’s a standard for expressing a mineral’s hardness, first proposed in 1824 by German mineralogist Frederich Mohs. The hardness is represented by the depth of the scratch made on the surface of the tested mineral using the scratch method with a pyramid-shaped diamond needle.\nThe hardness scale is as follows: talc 1 (softest), gypsum 2, calcite 3, fluorite 4, apatite 5, orthoclase (also known as feldspar or periclase) 6, quartz 7, topaz 8, corundum 9, diamond 10 (hardest). Mohs hardness is also used to express the hardness of other solid materials.\nFor a more specific method: one would scratch the mineral to be tested against the standard hardness on the Mohs hardness scale to determine the hardness of the tested mineral.\nFor example, if a mineral can scratch calcite and be scratched by fluorite, then the hardness of that mineral is between 3 and 4. Alternatively, one can use a fingernail (hardness 2-2.5), a coin (hardness 3.5), or a small knife (hardness 5.5) to scratch the mineral in order to broadly determine its hardness.\n|Representative Mineral Names||Common Uses||Hardness Scale|\n|Talc, Graphite||Talc is the softest known mineral, commonly used in the form of talc powder.||1|\n|Skin, Natural Arsenic||1.5|\n|Nails, Amber, Ivory||2.5|\n|Gold, Silver, Aluminum||2.5~3|\n|Calcite, Copper, Pearls||Calcite can be used as carving material and industrial raw material.||3|\n|Fluorite (also known as Fluorspar)||Carving, Metallurgy, Building Materials||4|\n|Phosphorite||Phosphorus is an important component of biological cells; it is used as raw material in feed, fertilizer, and chemical production.||5|\n|Glass, Stainless Steel||5.5|\n|Orthoclase, Tanzanite, Pure Titanium||6|\n|Teeth (outer layer of crown)||The main component is hydroxyapatite.||6~7|\n|Soft Jade – Xinjiang Hetian Jade||6~6.5|\n|Pyrite||It is used as raw material for the production of sulfuric acid; gold refining; and can also be used in medicinal purposes.||6.5|\n|Hard Jade – Burmese Jadeite and Jade||6.5~7|\n|Quartz Glass, Amethyst||7|\n|Electric Stone, Zircon||7.5|\n|Quartz||According to the old hardness scale, quartz is rated as 7.||8|\n|Topaz, Chromium, Tungsten Steel||On the old hardness scale, topaz is rated as 8.||9|\n|Moissanite||Synthetic gems are 2.5 times brighter than diamonds and cost 1/10th of the price.||9.5|\n|Corundum||Corundum is rated as 9 on the old hardness scale. Natural gems such as rubies and sapphires are now considered types of corundum, as is the hardness of synthetic sapphire crystals.||12|\n|Diamond||Diamonds are rated as 10 on the old hardness scale, making them the hardest natural gem on earth.||15|\nWhat is Mohs Hardness?\nMohs Hardness is a standard that indicates the hardness of minerals, first proposed in 1824 by German mineralogist Friedrich Mohs. This standard is established by using a pyramid-shaped diamond drill to scratch the surface of a mineral, with the depth of the scratch indicating the hardness.\nThe hardness of a mineral refers to its ability to resist certain external mechanical forces such as scratching, indentation, or grinding. In mineralogy, the hardness often referred to is Mohs hardness, which is the scratch hardness compared to the Mohs hardness scale.\nThe Mohs hardness scale is based on ten minerals of different hardness, divided into ten levels from low to high: 1. Talc; 2. Gypsum; 3. Calcite; 4. Fluorite; 5. Apatite; 6. Orthoclase; 7. Quartz; 8. Topaz; 9. Corundum; 10. Diamond.\nIn use, standard minerals are scratched against minerals of unknown hardness. If the mineral can be scratched by apatite but not by fluorite, its hardness is determined to be between 4 and 5.\nThis method was established and named by German mineralogy professor Friedrich Mohs (1773-1839). However, accurate measurement of mineral hardness still requires a microhardness tester or hardness tester. Mineral hardness is also one of the physical properties of minerals. Minerals with high hardness have been widely used in industrial technology.\nDiamonds, corundum, and other minerals are not only used in industry, but also become precious gemstones. As gemstones, they usually have a high hardness.\nFor example, the hardness of opal is 5.5-6.5, quartz is 6.5-7, sphalerite is 7.5-8. Tsavorite is 8.5, and the hardness of sapphires and rubies is 9, second only to diamonds. People choose high-hardness minerals as gemstones, probably because they are wear-resistant, symbolizing their timeless value!\nAccording to needs, people have also developed a gem hardness scale to identify the mineral hardness of gemstones, from the softest to the hardest minerals: talc, gypsum, calcite, fluorite, apatite, zircon, corundum, silicon carbide, boron carbide, diamond, etc.\nWhen there is no standard hardness mineral, the simplest way to measure hardness is with a fingernail or a small knife. The hardness of a fingernail is 2.5, a copper coin is 3, and glass and a small knife are both 5. Those above 6 are almost all gemstone-like minerals."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:d98cfcd0-7ab9-43fd-ade0-3eab7d6dfd6b>","<urn:uuid:6f5dff6d-ddef-43d0-937c-cbff7f05f328>"],"error":null}
{"question":"What are the key health benefits of playing rugby, and what are the specific risks of shoulder injuries that athletes need to be aware of?","answer":"Rugby offers numerous health benefits including cardiovascular fitness, endurance, and strength in both upper and lower body. It also provides team skills and social interaction benefits. However, athletes need to be particularly careful about shoulder injuries, as the shoulder has the greatest range of motion of any joint in the body. The rotator cuff, consisting of four muscles (infraspinatus, supraspinatus, subscapularis, and teres minor), can be damaged through traumatic injuries or wear and tear. Common shoulder injuries include rotator cuff tears, which can be either acute (from sudden injury) or chronic (from repetitive use), as well as tendinitis and bursitis. These conditions are particularly common in athletes over 40 and can cause shoulder pain, weakness, and difficulty performing overhead movements.","context":["The Health Benefits of Rugby\nThe health benefits of rugby are plenty of and varied, and also at the most basic level, rugby is like a method of increasing the time spent doing exercise.\nRugby is a popular sport that needs strength, endurance and fitness. Around australia, both Rugby League and Rugby Union codes are played. Rugby is greatly a contact sport and involves two teams whose players push, tackle, throw, kick and go to get the ball behind the opposition’s try line. Points may also be scored by kicking the ball with the goal posts for any conversion, penalty\nkick or drop goal.\nIdeal for all ages and abilities\nRugby could be played both socially and competitively. People of every age group and abilities can get involved with rugby through clinics and modified rules games.\nAlthough rugby is a message sport, the practice of tackling is usually only brought to appropriate age groups. Children and younger players are brought to rugby gradually, through more modified and non-contact versions from the sport.\nBenefits of Playing Rugby\nWhile playing rugby generally requires agility and strength, particular positions require specific innovative skills such as jumping and precision kicking.\nRugby involves sprinting, tackling, pushing and kicking. Health advantages include:\n- Cardiovascular fitness and endurance\n- Strength in lower and upper body\n- Ball-handling and kicking skills.\nRugby also brings other benefits, for example:\n- Team skills\n- Social interaction\n- Starting out\nYou can play in a local club or look for a clinic for beginners. Regardless of what shape, size or age you’re, you’ll find a level of rugby that’s best for you.\nRugby is quite complex as well as you to learn many individual and team skills. The Australian Rugby Union believes that younger Rugby Union players ought to be introduced to the game through its Junior Player Pathway Program, which is made to introduce kids towards the skills and tactical concepts of rugby inside a safe and fun manner.\nFor kids the recommendation is a total of a minimum of 60 minutes of at least moderately intense exercise each day. At least twice per week this should include activities that leave high physical stresses around the bone and improve muscle strength and adaptability.\nThe Department of Health has additionally found that:\nPhysical activity includes a range of benefits during childhood, including healthy development and growth, maintenance of energy balance, psychological well-being and social interaction.\n- Exercise is a major factor in reducing the probability of coronary heart disease in men and ladies.\n- Inactive, unfit individuals have almost double the chance of dying from heart disease than more active, fit people.\n- Half an hour of at least moderately intense exercise at least five days every week significantly reduces the chance of cardiovascular disease.\nThere is a low risk of stroke by as much as three times for physically active older people compared with their sedentary counterparts\nGroup recreational sports and activities will probably have positive social and mood benefits.\nAs tackling is the primary defensive tactic, rugby players could be prone to injuries, including:\n- Shoulder joint sprains\n- Hamstring strains\n- Knee ligament injuries\n- Ankle sprains.\nYou are able to reduce your risk of injury when playing rugby by:\n- Starting to warm up and cooling down\n- Using protective gear\n- Developing your skills\n- Using correct technique, particularly during connection with other players\n- Enforcing and following game rules.\nPlaying fairly in rugby is not just important to help prevent injuries, but additionally to ensure you and your team mates benefit from the game. You can aid in increasing fair play in rugby by:\n- Not setting up with foul play, for example dangerous tackles\n- Being respectful for your team mates, the opposition and also the referees.\nThings to remember\n- In Australia, the codes of Rugby League and Rugby Union are played.\n- Rugby is greatly a contact sport and involves two teams whose players push, tackle, throw, kick and go to get the ball behind the aim posts.\n- There are many health advantages you can gain from playing rugby.\n- You are able to play at a local club or look for a clinic for beginners.\n- Communication skills","Preventing Shoulder Cuff Injuries in Athletes << Back to Blog The shoulder has the greatest range of motion of any joint in the body. It allows the arm to move away from the body’s midline, towards the body’s midline, forward, and backwards. It can also move the arm in a full circle, as well as rotate it towards or away from the midline. The articulation of the shoulder is so unique, in fact, that the hip is the only other joint in the human body that is also classified as a spheroidal joint, which is commonly known as a “ball and socket” joint. While the rotator cuff is highly complex, it is also relatively weak. Similar to other structures of the body, it can be damaged by traumatic injuries, and is also extremely susceptible to wear and tear. Athletes who excessively use their shoulder—such as baseball and tennis players—are at the greatest risk of a rotator cuff injury. But, as any of these athletes would tell you, repetitive use of the shoulder is an unavoidable part of the game. So what can athletes do to prevent rotator cuff injuries? Like all other sports-related injuries, prevention entails education, physical conditioning, and practice. The Anatomy of the Rotator Cuff The shoulder consists of the humerus (upper arm bone), the scapula (the shoulder blade), and the clavicle (the collar bone). The joint is formed from the spherical head of the humerus, which acts as a “ball”, and the glenoid cavity, which forms the “socket”. The rotator cuff is a group of four muscles that come together at the head of the humerus. These muscles not only help stabilize the shoulder but they also keep the arm in its socket. In addition, each muscle also serves a specific, unique function related to movement. Infraspinatus: Rotates the arm laterally (away from the midline) at the shoulder joint. Supraspinatus: Helps the deltoid muscle abduct (raise) the arm at the shoulder joint. Subscapularis: Rotates the arm medially (towards the midline) at the shoulder joint. Teres minor: Rotates the arm laterally and weakly adducts (lowers) the arm at the shoulder joint. Common Rotator Cuff Injuries Rotator Cuff Tears Rotator cuff tears are a common orthopaedic condition. When a rotator cuff muscle tears, it partially or completely detaches from the head of the humerus, causing shoulder pain and instability. Tears are most likely to occur in the supraspinatus muscle and tendon. Because these injuries result from a variety of causes and range in severity, they can be categorized in a number of ways. Acute Tears An acute tear results from a sudden injury, such as blunt force to the shoulder or a fall on an outstretched arm; however, a rotator cuff with pre-existing degeneration can also be torn by simple, everyday activities such as gardening or putting away the dishes. Chronic Tears A chronic tear, also called a degenerative tear, results from repetitive use of the shoulder cuff muscles over time. Athletes are at especially high risk for this type of overuse injury. Degradation, like the kind that is associated with chronic tears, occurs naturally with age as well. As such, people over 40 are at increased risk for this type of injury. Some common causes include: Reduced blood flow – Blood oxygen levels naturally decrease as people become older. Since oxygen is essential to all parts of the body, reduced oxygen both increases the risk of a rotator cuff tear and makes healing more difficult. Bone spurs – Bone spurs are little growths that occur on the bone as a result of too much calcium. These spurs rub on the rotator cuff, causing pain and degradation. Bone spurs can also lead to shoulder impingement, a condition where the arms cannot go above shoulder level without severe pain. Signs of a rotator cuff tear include: A sudden tearing sensation Immediate weakness in your arm A cracking sensation when moving your arm Pain during certain movements Symptoms of a rotator cuff tear include: Shoulder pain that gradually worsens over time and persists throughout the night Shoulder weakness that gradually worsens over time Difficulty performing overhead movements A rotator cuff tear is described as either a partial or full thickness tear based on its severity. Partial Rotator Cuff Tears This form of injury is also referred to as partial thickness rotator cuff tear because the tear only extends part way through the tendon and often only involves one of the four rotator cuff muscles. It is more common than a complete tear and more common in younger individuals. And while seemingly less severe than a complete tear, there is no correlation between the size of a tear and the amount of pain it causes. In fact, in some instances, a partial tear can be more painful than a full-thickness tear. Full-Thickness Rotator Cuff Tears A full-thickness rotator cuff tear occurs when one or more muscles and tendons completely separate from the humerus. Large tears can cause significant shoulder weakness. For example, an individual may have to support their injured arm with their other arm when lifting or moving an object. Large tears can also cause a loss of shoulder mobility; however, this is not always the case. Tendinitis and Bursitis Tendinitis causes tendons to become swollen and painful, often due to over use and inflammation. Similarly, bursitis occurs when bursae—small fluid-filled sacs between bones, tendons, and muscles—become inflamed. These conditions can occur alone or simultaneously and are more likely to occur in people age 40 or older. Tendonitis, in particular, is often a precursor to a tear. When tendons become inflamed, they scrape against the bones in the shoulder joint. This frays the tendons and consequently makes them more susceptible to tears. Treatments for Rotator Cuff Injuries First Aid RICE (Rest, Ice, Compression, and Elevation) should be applied as soon as possible after an injury occurs. Rest provides time for the injury to heal while ice and compression reduce the amount the amount of swelling. In the case of rotator cuffs, however, individuals may forgo elevating the arm if it is difficult or uncomfortable. Professional Treatment The majority of rotator cuff injuries can be treated with a combination of rest and physical therapy. Physical therapy helps restore shoulder flexibility and strength as well as reduce pain. If a patient does not achieve relief with conservative treatments, a physician may recommend regenerative medicine, which is centered on the body’s own ability to itself. Regenerative medicine aids the body in creating new, functional tissue to replace old and damaged tissue. Forms of regenerative medicine that have been shown to be effective for rotator cuff injuries include: Regenexx PRP (platelet rich plasma) therapy Stem cell therapy Surgery is also an effective option; however, it is often a method of last resort. Candidates for surgery have symptoms that limit normal, everyday function and do not improve with conservative treatments. A talk with an orthopaedic specialist can determine if surgery is the right option for you. How to Prevent Rotator Cuff Injuries In general, the best way to prevent rotator cuff injuries is to strengthen the rotator cuff. Keep in mind that even simple, daily exercise can go far in strengthening muscles and tendons. The “doorway stretch” is one such exercise that can be performed at home. To perform the exercise: Stand in an open doorway and spread your arms out to your side. Grip the sides of the doorway at shoulder height. While maintaining your grip and keeping a straight back, lean forward until you feel a light stretch in the front of your shoulder. Slowly return to the original position. Repeat 10 times. As always, speak with a physical therapist before starting an exercise program. In addition to exercise, athletes should focus on proper form. Improper form puts increased stress on the rotator cuff and increases an athlete’s risk of an injury. Frequent breaks can help an athlete avoid fatigue that consequently leads to improper form. When to See an Orthopaedic Specialist While the majority of rotator cuff injuries heal with rest, it’s a slow process. Moreover, because many everyday activities require the use of the shoulder, patients can potentially worsen their condition by not seeking early treatment. For all of these reasons, it’s important to talk with an orthopaedic specialist at the first sign of an injury. An orthopaedist will ensure that you’re on the best path to recovery. Dr. Robert Rolf is a board certified orthopaedic surgeon at Beacon Orthopaedics and Sports Medicine who provides extensive expertise in rotator cuff tears as well as other conditions related to the shoulder or elbow. Patients can meet with Dr. Rolf at Beacon’s Batesville, Lawrenceburg, or Northern Kentucky location as well as Beacon West in Harrison, Ohio. Dr. Rolf also offers informational Shoulder Talks at Beacon West. For a list of upcoming talks, visit Dr. Rolf’s page."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:1a2fc6a3-dc50-4cea-8233-b6615ca93865>","<urn:uuid:afbf3a0f-4dc5-446d-97f9-26f1c8aaf1a7>"],"error":null}
{"question":"Which occurred first chronologically: the hybridization event that led to jawed vertebrates or the appearance of the first dinosaurs?","answer":"The hybridization event that led to jawed vertebrates occurred first, taking place over 450 million years ago when two different species of fish mated and produced a hybrid species with doubled chromosomes. This significantly predates the first dinosaurs, which began to appear during the Triassic Period, around 243 to 231 million years ago, having evolved from reptiles called Archosaurs.","context":["To look at how life evolved, scientists usually turn to the fossil record, but this record is often incomplete. Researchers from the Okinawa Institute of Science and Technology Graduate University (OIST), alongside an international team of collaborators, have used another tool - the chromosomes of living animals - to uncover clues about our past. The study, published in Nature Ecology and Evolution, reveals early events in the evolution of vertebrates, including how jawed vertebrates arose through hybridization between two species of primitive fish.\n\"It's remarkable that although these events occurred almost half a billion years ago, we can figure them out by looking at DNA today,\" said Professor Daniel Rokhsar, who leads OIST's Molecular Genetics Unit.\nReading the stories in our genes\nChromosomes are tiny structures that carry an organism's genetic material. They normally come in paired sets, with one set inherited from each parent. While humans have 23 pairs, this number varies across species.\nThe study found that, even over hundreds of millions of years, chromosomes can be surprisingly stable. Although mutations and rearrangements have occurred, the chromosomes of modern animals have striking similarities to each other.\n\"We can use these similarities to trace our evolution and infer biology from the distant past,\" said Professor Rokhsar, \"If a group of genes is carried together on the same chromosomes in two very different animals - say, snails and sea stars - then these genes were also likely together on the same chromosome in their last common ancestor.\"\nTwo former OIST postdoctoral scholars, Professor Oleg Simakov, now at the University of Vienna, and Dr. Ferdinand Marlétaz, now at University College, London, led the study that compared the chromosomes of amphioxus, a small marine invertebrate, to those of other animals, including mollusks, mammals, birds, frogs, fish, and lampreys.\nAfter accounting for a handful of rearrangements, they concluded that the chromosomes of amphioxus resemble those of long-extinct early vertebrate ancestors and confirmed the existence of 17 ancient chromosomal units. The researchers then traced the evolution of these ancient chromosomes in living vertebrates.\n\"Reconstructing the ancestral chromosomes was the key that allowed us to unlock several puzzles of early vertebrate evolution,\" said Professor Rokhsar.\nDuplicating and disappearing\nThe puzzles center on a phenomenon known as 'genome duplication.' In the 1970s, geneticist Susumu Ohno suggested that vertebrate genomes were doubled, perhaps repeatedly, relative to their invertebrate ancestors. Genomic studies have confirmed and refined this suggestion, but how many doublings there were, and how and when they occurred, are still debated.\nPart of the challenge is that duplicated genomes change rapidly, and these changes can obscure the duplication itself. Although a doubled genome starts with redundant copies of every gene, most of these extra copies will be inactivated by mutation and eventually lost; the doubled chromosomes themselves may also become scrambled.\nUsing the 17 ancestral chromosome pairs as an ancient anchor, the researchers concluded that there were two separate instances of genome doubling.\nThe first duplication is shared by all living vertebrates - both the jawed vertebrates, including humans, birds, fish, and frogs, as well as the jawless lampreys and their relatives. The researchers inferred that this most ancient duplication occurred about five hundred million years ago, around the same time the earliest vertebrate fossils appear.\nThe second duplication is shared only by jawed vertebrates. The researchers found that, unlike the first event, gene loss after the second doubling occurred unevenly across the two sets of chromosomal copies - a surprising but informative feature.\n\"This kind of uneven gene loss is the hallmark of a genome duplication that follows the hybridization of two species,\" said Professor Rokhsar.\nUsually, the hybrid offspring of two different species are infertile, in part because the chromosomes from the two parents aren't properly coordinated. But very occasionally, in some fish, frogs, and plants, the hybrid genome becomes doubled to restore chromosomal pairing. The resulting offspring have twice as many chromosomes as their mismatched parents - and are often more vigorous. The new study unexpectedly found that such hybrid-doubling occurred in our ancient ancestors.\n\"Over 450 million years ago, two different species of fish mated and, in the process, spawned a new hybrid species with twice as many chromosomes,\" said Professor Rokhsar, \"And this new species would become the ancestor of all living jawed animals - including us!\"\nThe research team also included Dr. Jia-Xing Yue from Université Côte d'Azur, Dr. Brendan O'Connell and Professor Richard E. Green from the University of California, Santa Cruz, Mr. Jerry Jenkins and Mr. Jeremy Schmutz from the HudsonAlpha Institute for Biotechnology, Dr. Alexander Brandt from the University of California, Berkeley, Mr. Robert Calef, and Dr. Nicholas H. Putnam from Dovetail Genomics, Dr. Che-Huang Tung, Dr. Tzu-Kai Huang and Professor Jr-Kai Yu from Academia Sinica, and Professor Nori Satoh from OIST's Marine Genomics Unit.\nNature Ecology & Evolution","When were dinosaurs alive? On this page you’ll learn about the dinosaur periods of the Mesozoic Era – the ‘Age of Reptiles’.\nWe’ll look at each of the three ‘dinosaur periods’ – the Triassic, Jurassic and Cretaceous – and find out what life on Earth was like in those times.\n- You can find out more about dinosaurs at our main Dinosaur Facts page.\nWhen Were Dinosaurs Alive?\nDinosaurs were alive in the Triassic, Jurassic and Cretaceous periods. Together, these three periods make up the Mesozoic Era. Due to the dominance of the dinosaurs and other reptiles during the Mesozoic Era, it has become known as the ‘Age of Reptiles’.\nMesozoic Era (252.17 to 66 million years ago)\n- Triassic Period (252.17 to 201.3 million years ago): Dinosaurs begin to appear, having evolved from reptiles called Archosaurs.\n- Jurassic Period (201.3 – 145 million years ago): Dinosaurs become the dominant land vertebrates.\n- Cretaceous Period (145 – 66 million years ago): Dinosaurs continue to thrive and diversify.\n- Cretaceous–Paleogene Extinction Event (around 66 million years ago): The dinosaurs are wiped out; only avian dinosaurs (birds) survive.\nFurther down the page you’ll find information on each of the three periods of the Mesozoic Era, including examples of the dinosaurs that existed during each period.\nMesozoic Era: ‘Age of Reptiles’\nThe Mesozoic Era lasted for around 186 million years. It began around 252.17 million years ago, and ended around 66 million years ago. Dinosaurs started to appear around 243 to 231 million years ago, during the Triassic Period.\nDinosaurs are among the most successful groups of animals ever to have lived. They walked the earth for around 170 million years. (Bear in mind that humans have only existed for 2 million years!)\nHowever, the dinosaurs’ successful reign came to an abrupt end.\nDinosaur Periods: Extinction\nThe Mesozoic Era ended with the ‘Cretaceous–Paleogene Extinction Event’, which wiped out the dinosaurs and many other species.\n(An extinction event is a sudden and wide-ranging decrease in the amount of life on Earth.)\nDivisions Of The Mesozoic Era\nAs we’ve seen, the Mesozoic Era is subdivided into three periods: the Triassic, Jurassic and Cretaceous periods. These are ‘geological periods’ of time: each corresponds to a particular layer of rock.\nThe scientific body responsible for measuring the geological timescale is the International Commission on Stratigraphy.\nIn the geological timescale, each period is further divided into ‘epochs’. The Triassic and Jurassic periods are divided into three epochs, the Cretaceous into two.\nMesozoic Era (252.17 to 66 million years ago)\n- Triassic Period (252.17 to 201.3 million years ago)\n- Early Triassic Epoch (252.17 – 247.2 million years ago)\n- Middle Triassic Epoch (247.2 – 237 million years ago)\n- Late Triassic Epoch (237 – 201.3 million years ago)\n- Jurassic Period (201.3 – 145 million years ago)\n- Early Jurassic Epoch (201.3 to 174.1 million years ago)\n- Middle Jurassic Epoch (174.1 to 163.5 million years ago)\n- Late Jurassic Epoch (163.5 to 145 million years ago)\n- Cretaceous Period (145 – 66 million years ago)\n- Early Cretaceous Epoch (145 to 100.5 million years ago)\n- Late Cretaceous Epoch (100.5 to 66 million years ago)\nThe Age of the Dinosaurs\nLet’s find out what the world was like during the three ‘dinosaur periods’ of the Mesozoic Era …\n1: Triassic Period (252.17 to 201.3 million years ago)\nThe Eoraptor is one of the first dinosaurs. It appeared during the Triassic Period.\nThe Triassic Period followed the worst extinction event that the world has ever experienced. (Yes, it was even worse than the later extinction event that would wipe out the dinsoaurs.)\nThe Permian-Triassic Extinction Event, or the ‘Great Dying’ as it is otherwise known, caused between 90% and 96% of all species on Earth to became extinct.\nAt the beginning of the Triassic Period, life on Earth struggled to rebuild itself. At first, the mammal-like reptiles, successful before the extinction event, started to dominate once again.\nMammal-like animals such as the Lystrosaurus – a strange, dog-sized animal with tusks – thrived in the Early Triassic. Mammals themselves would appear towards the end of the Triassic Period.\nHowever, it was another group of animals that would win the race to supremacy: the Archosaurs. Through the course of the Triassic Period, these reptiles became the dominant land vertebrates (animals with backbones).\nAs the Triassic Period progressed, the archosaurs split into two main branches. One of these was the Pseudosuchia, which includes the Crocodilians, and similar animals. The other branch was the Avemetatarsalia, which itself split into Pterosaurs and Dinosaurs.\nThe first dinosaurs were, small, walked on two legs, and ate meat.\nIn the Triassic period, all of the world’s land was joined together in one vast supercontinent called ‘Pangaea’, which means ‘whole Earth’. The climate was hot and dry. The interior of Pangaea was a vast desert.\nPlants such as conifers and ginkgoes flourished. There was no grass, and there were no flowering plants.\nWhen Were Dinosaurs Alive: Triassic Period Dinosaurs\nOne of the earliest dinosaurs was the Eoraptor, a small predator that lived around 228 million years ago (see picture, above). Found in north-west Argentina, it walked on two legs and stood knee-high to a human.\nEoraptor looked like a ‘typical’ dinosaur, with a long tail, long neck and short arms.\nSaurischia and Ornithischia\nEoraptor was a saurischian, one of the two main types of dinosaur. All of the meat-eating dinosuars were saurichians, as were many of the large, four-legged plant eaters.\nThe other main type of dinosaur was ornithischia. The Ornithischians were all herbivores (plant-eaters)\nOne of the earliest known Ornithischians is the Pisanosaurus. This was a small dinosaur that walked on two legs, and outwardly looked much like early bipedal (2-legged) saurichians.\nWhen Were Dinosaurs Alive: Triassic Period Dinosaurs\n2: Jurassic Period (201.3 – 145 million years ago)\nThe Jurassic Period began with another extinction event – the Triassic-Jurassic Extinction Event. While not quite as devastating as the earlier Permian-Triassic Extinction Event, it still led to a substantial decline in both sea and land species.\nThe extinction set the scene for dinosaurs – who were already flourishing by the end of the Triassic – to completely dominate the land.\nPangaea began to break apart in the Jurassic Period, forming two landmasses, Laurasia in the north, and Gondwana in the south. As the continents separated, the climate became wetter and more humid.\nConifer forests covered much of the land, especially near the poles. Ginkgoes, palms, tree-ferns and horse-tails were also present. The first birds appeared. Insects and amphibians continued to develop. Mammals evolved fur coats and started giving birth to live young.\nHuge sauropods such as Brachiosaurus walked the land, feeding on the flourishing vegetation. Large predators, such as Allosaurus also made an appearance.\nWhen Were Dinosaurs Alive: Jurassic Period Dinosaurs\nExamples of Early Jurassic Dinosaurs\nExamples of Mid Jurassic Dinosaurs\nExamples of Late Jurassic Dinosaurs\n3. Cretaceous Period (145 – 66 million years ago)\nDuring the Cretaceous Period, the continents continued to separate. Gondwana broke up to form South America and Africa, and Antarctica and Australia also drifted away. Laurasia began to split into North America, Europe and Asia. The world – as we know it today – began to take shape.\nThe climate became wetter, and the newly formed oceans offered new habitats.\nFlowering plants appeared in Laurasia at the beginning of the Cretaceous Period. By the end of the Cretaceous they were the Earth’s dominant plant type. Oak, maple and beech trees were all present. Insects evolved alongside the new plants.\nWhen Were Dinosaurs Alive: Cretaceous Period Dinosaurs\nAlthough some types of dinosaur, such as the Stegosaurs, died out during the Cretaceous Period, ever-more varied types evolved to take their places.\nDuck-billed dinosaurs such as Iguanodon, armoured dinosaurs such as Polacanthus, and the Ceratopsids, including Triceratops, all thrived during the Cretaceous Period.\nCretaceous–Paleogene Extinction Event\nMany of these dinosaurs existed right up to the end of the Cretaceous Period. However, 66 million years ago, the Earth was rocked by a catastrophic event that wiped out over 75% of all living species.\nThis was the Cretaceous–Paleogene Extinction Event, and it brought the whole Mesozoic Era to a close. At the dawn of the Cenozoic Era, the only dinosaurs still alive were the birds.\nAll of the non-avian dinosaurs had become extinct.\nScientists believe that the Cretaceous–Paleogene Extinction Event (which is also known as the K–Pg extinction event, or Cretaceous–Tertiary (K–T) extinction event) was caused by a large asteroid striking the Earth.\nThe explosion would have thrown up a dust cloud big enough to hide the sun for months, or even years. Plants would have been unable to grow in the darkness, and many of the Earth’s animals – including the dinosaurs – would have been unable to find enough food to survive.\nThe age of the dinosaurs had come to an end.\nExamples of Early Cretaceous Dinosaurs\nExamples of Late Cretaceous Dinosaurs\nDinosaur Periods: When Were Dinosaurs Alive – Conclusion\nThe Mesozoic Era is subdivided into three periods: the Triassic, Jurassic and Cretaceous periods.\nDinosaurs first appeared towards the end of the Triassic Period. Following the Triassic-Jurassic extinction event, the dinosaurs quickly became the dominant land animals on Earth. They would remain so throughout the Jurassic and Cretaceous Periods.\nThe dinosaurs became increasingly diverse throughout the Cretaceous Period, and thrived right up to the Cretaceous–Paleogene Extinction Event.\nThis extinction event, believed to have been caused by a large asteroid striking the Earth, wiped out all of the (non-avian) dinosaurs. Only avian dinosaurs – or birds – survived. (Many scientists today believe that birds are actually dinosaurs, meaning that birds are the only dinosaurs to have survived the extinction event).\nWe hope that you’ve enjoyed reading about the three ‘dinosaur periods’ of the Mesozoic Era.\nOther Relevant Pages on Active Wild\nYou can find more dinosaur information on our main Dinosaur Facts page, or check out these pages for in-depth dino facts:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:72fbac54-a2e3-4af2-9240-1271a774f726>","<urn:uuid:9a6e8fa5-cab4-43f4-9eb6-c7dda4ff9d92>"],"error":null}
{"question":"How do maintenance requirements compare between agricultural well drainage systems and residential septic systems?","answer":"Agricultural well drainage systems and residential septic systems have different maintenance focuses. Well drainage systems require careful design consideration of depth, capacity, discharge, and spacing, with maintenance centered on ensuring proper water table and soil salinity control. The wells must be placed in permeable soil layers and operated at safe discharge rates to prevent groundwater depletion. Septic systems, on the other hand, require regular pumping and cleaning to manage the accumulation of scum and sludge layers, as these layers can reduce the quantity of effluent and increase contamination with suspended solids. Additionally, septic system maintenance includes protecting the drain field area from structures, trees, and vehicle traffic to prevent costly damage.","context":["- Well drainage\nWell drainage means drainage of agricultural lands by wells. Agricultural land is drained by pumped wells (vertical drainage) to improve the soils by controlling water table levels and soil salinity.\nSubsurface (groundwater) drainage for water table and soil salinity in agricultural land can be done by horizontal and vertical drainage systems.\nHorizontal drainage systems are drainage systems using open ditches (trenches) or buried pipe drains.\nVertical drainage systems are drainage systems using pumped wells, either open dug wells or tube wells.\nBoth systems serve the same purposes, namely water table control and soil salinity control .\nBoth systems can facilitate the reuse of drainage water (e.g. for irrigation), but wells offer more flexibility.\nReuse is only feasible if the quality of the groundwater is acceptable and the salinity is low.\nAlthough one well may be sufficient to solve groundwater and soil salinity problems in a few hectares, one usually needs a number of wells, because the problems may be widely spread.\nThe wells may be arranged in a triangular, square or rectangular pattern.\nThe design of the well field concerns depth, capacity, discharge, and spacing of the wells.\n- The discharge is found from a water balance.\n- The depth is selected in accordance to aquifer properties. The well filter must be placed in a permeable soil layer.\n- The spacing can be calculated with a well spacing equation using discharge, aquifer properties, well depth and optimal depth of the water table.\nThe determination of the optimum depth of the water table is the realm of drainage research .\nFlow to wells\nThe basic, steady state, equation for flow to fully penetrating wells (i.e. wells reaching the impermeable base) in a regularly spaced well field in a uniform unconfined (preactic) aquifer with an hydraulic conductivity that is isotropic is  :\n- Q = 2π K (Db - Dm) (Dw - Dm) / ln (Ri/Rw)\nwhere Q = safe well discharge - i.e. the steady state discharge at which no overdraught or groundwater depletion occurs - (m3/day), K = uniform hydraulic conductivity of the soil (m/day), D = depth below soil surface, Db = depth of the bottom of the well equal to the depth of the impermeable base (m), Dm = depth of the watertable midway between the wells (m), Dw is the depth of the water level inside the well (m), Ri = radius of influence of the well (m), Rw=radius of the well (m), ln = natural logarithm, and π = the number pi.\nThe radius of influence of the wells depends on the pattern of the well field, which may be triangular, square, or rectangular. It can be found as:\n- Ri = sqrt (At/πN)\nwhere At = total surface area of the well field (m2), N = number of wells in the well field, and sqrt=square root.\nThe safe well discharge (Q) can also be found from:\n- Q = q At / N Fw\nwhere q is the safe yield or drainable surplus of the aquifer (m/day) and Fw is the operation intensity of the wells (hours/24 per day). Thus the basic equation can also be written as:\n- Dw - Dm = q At ln (Ri/Rw) / 2π K (Db - Dm) N Fw\nWith a well spacing equation one can calculate various design alternatives to arrive at the most attractive or economical solution for watertable control in agricultural land.\nThe basic flow equation cannot be used for determining the well spacing in a partially penetrating well-field in a non-uniform and anisotropic aquifer, but one needs a numerical solution of more complicated equations.\nThe costs of the most attractive solution can be compared with the costs of a horizontal drainage system - for which the drain spacing can be calculated with a drainage equation - serving the same purpose, to decide which system deserves preference.\nThe well design proper is described in \nAn illustration of the parameters involved is shown in the figure. The hydraulic conductivity can be found from an aquifer test.\nThe numerical computer program WellDrain  for well spacing calculations takes into account fully and partially penetrating wells, layered aquifers, anisotropy (different vertical and horizontal hydraulic conductivity or permeability) and entrance resistance.\nWith a groundwater model that includes the possibility to introduce wells, one can study the impact of a well drainage system on the hydrology of the project area. There are also models that give the opportunity to evaluate the water quality.\nSahysMod  is such a polygonal groundwater model permitting to assess the use of well water for irrigation, the effects on soil salinity and on depth of the water table.\nAgricultural water management IrrigationSurface irrigation · Tidal irrigation · Irrigation of alluvial fans · Irrigation statistics · Irrigation management · Irrigation environmental impacts Subsurface drainageTile drainage · Drainage equation · Drainage system (agriculture) · Watertable control · Drainage research · Drainage by wells Surface water/runoff GroundwaterGroundwater flow · Groundwater energy balance · Groundwater model · Hydraulic conductivity · Watertable Problem soils Agro-hydro-salinity groupHydrology (agriculture) · Soil salinity control · Leaching model (soil) · SaltMod integrated model · SahysMod polygonal model: Saltmod coupled to a groundwater model Related topicsSand dam ·\n- ^ a b c Boehmer, W.K., and J.Boonstra, 1994, Tubewell Drainage Systems, Chapter 22 in: H.P.Ritzema (ed.), Drainage Principles and Applications, Publ. 16, International Institute for Land Reclamation and Improvement (ILRI),Wageningen, The Netherlands. pp. 931-964, ISBN 90 70754 3 39 . On line : \n- ^ ILRI, 1999, Drainage and Hydrology/Salinity: Water and salt balances, 29 pp. Lecture notes of the International Course on Land Drainage (ICLD), International Institute for Land Reclamation and Improvement (ILRI), Wageningen, The Netherlands. On line : \n- ^ a b ILRI, 2000, Subsurface drainage by (tube)wells: Well spacing equations for fully an partially penetrating wells in uniform or layered aquifers with or without anisotropy and entrance resistance, 9 pp. Principles used in the \"WellDrain\" model. International Institute for Land Reclamation and Improvement (ILRI), Wageningen, The Netherlands \nDownload \"WellDrain\" software from :  , or from : \n- ^ SahysMod, Spatial Agro-Hydro-Salinity Model: Description of Principles, User Manual, and Case Studies. SahysMod working group of the International Institute for Land Reclamation and Improvement, Wageningen, the Netherlands. On line:  .\nDownload the model from :  , or from : \n- Salinity Control and Reclamation Program (SCARP) using wells in the Indus valley of Pakistan.\n- Website on waterlogging and land reclamation by horizontal and vertical drainage systems : \n- Hydraulic engineering\n- Land management\n- Land reclamation\n- Water and the environment\nWikimedia Foundation. 2010.","Important Septic Tank Pumping Information Homeowners Need to Know\nRoutine maintenance is essential to your septic system. A well maintained system will provide a safe and lawful means of waste disposal for your home’s plumbing system. Proper maintenance will also ensure a healthy and operational system for many years. The more informed you are as a septic system owner and the more familiar you are with the function of your septic system, the better you will be able to care for it.\nThe most common system in our area is a gravity fed conventional septic system. This system consists of three main parts:\nYour Septic Tank\nThis is where all liquid waste travels when it leaves your home via your plumbing system, which is the piping routed throughout your home’s walls and crawl space. These pipes tie into your main sewer line, which leads directly into your septic tank.\nSeptic tanks come in numerous sizes. The size of yours will depend upon the size of your home, i.e. the number of bedrooms in your home. Law requires proper tank sizes, check the VA state regulations if you have questions.\nAs the waste in your tank accumulates, it will begin to separate into three layers. The first or top layer is aptly named the scum layer or can also be known as the cake. This thick hardened layer consists of human waste, toilet tissue and other solid materials disposed of from your home. This scum, or cake floats on the middle layer, which is simply waste water known as the effluent. As the bacteria in your septic tank feed off the organic matter in the scum layer, the residual settles to the bottom of the tank, this creating the last layer, known as sludge.\nOver time the scum layer grows thicker and the sludge layer deepens. As this occurs the quantity of effluent lessons and becomes more contaminated with suspended solids. This is why septic tank pumping & cleaning on a regular and consistent basis is so important. If you are unsure of how long it has been since your last service, or whether you are do for service, please contact us for a septic tank inspection so we can inform you of your tank’s condition.\nYour Distribution Box\nWhen the effluent leaves your septic tank, it travels via gravity through a pipe and into your distribution box. This box is buried downhill from your septic tank. Your distribution box has several holes in it, where your drain field pipes are attached. The box evenly distributes the effluent to these pipes.\nYour Drain Field\nYour drain field is a maze of underground, perforated pipes, which are lying on gravel. The effluent travels through these pipes; escaping through the perforations and seeping down through the gravel, into the soil and eventually back into the water table. Your drain field is an intricate part of your septic system. Please note you must keep your drain field area free of structures, trees, shrubs, swimming pools etc. Also remember not to park on or drive over your drain field area, as this can cause irreversible damage, resulting in costly septic system repairs."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:cfaef3d7-bf8b-47a5-ab14-1b69c4a28f86>","<urn:uuid:c16e1c55-a9ab-41bf-904b-e8321de2cd7c>"],"error":null}
{"question":"What architectural elements connect the Paphos Odeon Amphitheatre to early Greek theatrical traditions?","answer":"The Paphos Odeon Amphitheatre reflects the Greek tradition of outdoor theatre construction, as it was built as an ancient Roman outdoor theatre. Like the earliest Greek theatre architecture that developed from threshing circles at the base of hillsides, there is another ancient theatre in Paphos that was specifically built into a hillside. While the Odeon is still used today for performances, the hillside theatre, though less well preserved, provides insights into the evolution of ancient theatre design.","context":["The ancient city of Paphos, Cyprus is filled with so many remnants of its rich history, that the entire city has been declared a World Heritage Site by UNESCO. Many of these sites are located around the harbour and Old Paphos, where you can also enjoy staying in one of the numerous villas in Paphos. In this article you will be able to read about ten of the top visitor attractions of Paphos, Cyprus.\n1. Petra tou Romiou, or the Rock of Digenis – As an area with great hikes, one of the first places you might want to visit is the rock from where it is believed Aphrodite was born. This rock is located not far from the famed Paphos Harbour, making a solitary figure just north of Fabrica Hill.\n2. Paphos Castle – In the harbour, it was originally built as a fort during the Byzantine Empire, but has been destroyed and rebuilt many times over by the various conquerors of the island. Today, you can walk along the quay for terrific views and enjoy fresh seafood at one of its many taverns or restaurants. You’ll need to cross a bridge to get to it, which is usually open from 9am – 5pm.\nThe photo above shows Paphos Castle.\nImage Source: http://en.wikipedia.org/wiki/File:Pafos_castle.jpg\n3. Roman Mosaics – These beautiful mosaics have been found in a number of structures and a ticket costing roughly three Euros will admit you to see them. The oldest mosaics, in the House of Dionysus and House of Theseus, date from the second century A.D. Just a couple hundred years younger are the Houses of Aion, Orpheus, and Four Seasons. All the mosaics depict different myths in intricate levels of detail.\n4. Tomb of the Kings – A necropolis with a network of tombs and caves that pre-date Jesus. This site is particularly interesting as an example of Egyptian influence in the area. The ticket price is around two Euros.\n5. Saint Solomoni Church – An underground church with 12th century frescoes and catacombs. It’s also believed hanging a personal offering from the tree just above it will cure diseases.\n6. Odeon Amphitheatre (Pano Odeon Paphos) – An ancient Roman outdoor theatre which is still used for musical and theatrical performances still today. There is another ancient theatre in Paphos, just diagonal from the harbour that was built into a hillside. Not as well preserved as the Odeon, it offers some interesting architectural insights into the evolution of ancient theatre design.\nThe photo above shows a view of the Odeon Amphitheatre in Paphos, Cyprus.\nImage Source: http://en.wikipedia.org/wiki/File:PanoOdeonPaphos.jpg\n7. Basilica of Panagia Chrysopolitissa – This is the largest basilica excavated on the island and dates from the fourth century. While originally a seven aisle basilica, much of it was destroyed by Arab invaders in the seventh century. After that it has been rebuilt and fortified against further destruction.\n8. Paphos Archaeological Museum – Home too many antiquities, from the Neolithic Age to the 1700s, including a marble bust of Aphrodite. The museum is open Tuesday through Saturday and costs around two Euros.\n9. Byzantine Museum of Paphos – This museum focuses entirely on the Byzantine period. It contains many Byzantine icons from the twelfth to nineteenth centuries. The collection has a range of liturgical items, which are also detailed examples of Byzantine art. The museum is open Monday through Saturday and costs around two Euros.\n10. Kato Akourdaleia Folk Art Museum – Includes items from ancient and traditional lives in Paphos, from clothes to farming equipment to rug making, and even old photographs. This museum is open Monday through Friday and costs a little more than one Euro.\nThese sites don’t even begin to explain the beautiful beaches and hiking around Paphos as well. If you stay in one of the Hotels/villas in Cyprus in the Paphos area, you’ll have full access to all these attractions on your doorstep. There are plenty of activities to keep you entertained in Paphos, Cyprus where legend says Aphrodite first appeared.","- Размер: 1.3 Mегабайта\n- Количество слайдов: 27\nОписание презентации HISTORY OFOF THEATRE по слайдам\nHISTORY OFOF THEATR\nA passion play performed annually at Abydos from about 2500 BC to about\n. . . dealt with the death and resurrection of the god Osiris\nDespite the advanced civilization that developed in ancient Egypt, theatrical activity never progressed beyond ritual, pageantry, burial ceremonies, and commemorations of dead pharaohs.\nGreek philosopher Aristotle (4 th century BC) claimed that theatre began with hymns to god Dionysus presented at an annual festival.\nAccording to tradition Dionysus died each winter and was reborn each spring\nGreece’s earliest theatre architecture took its form from the threshing circle — a round, flat circle at the base of a hillside that was used for separating wheat from the chaf\nBy the 5 th century BC, when the classical period began, 2 performance areas were cradled within the curve of a hillside: One where a chorus performed, usually portraying ordinary citizens And the other where the main actors performed\nOne speaking actor portrayed mythical and historical characters, at first in an empty space and later in front of a rectangular building that formed a neutral background\nThis scene building could represent diferent places as needed: a palace a temple a house or a cave for example.\nInitially audiences stood or were seated on the ground, later, wooden or stone benches on the hillside formed an auditorium. The open-air theaters of ancient Greece, which held some 20, 000 people, became the prototypes for amphitheaters, Roman coliseums, and modern sports arenas\nThe most celebrated theater of classical Athens, theater of Dionysus, was located on the slope of a hill below the Acropolis\nThe four Greek playwrights whose work has survived, wrote for annual dramatic festivals held there:\nTheir plays expanded and interpreted the characters and stories of legend and history\nDuring the 5 th century BC, the features of Athens‘s annual dramatic festival became fixed: three groups of players—each consisting of a chorus, musicians, and two (later three) actors—competed in acting four sets of plays. .\nEach set contained three tragedies and a satyr play, a burlesque of Greek myth that served as comic relief Costumes were richly decorated, masks elaborate, and physical action restrained. In the 4 th century BC, theaters throughout the Greek world grew more elaborate.\nThe first drama was performed outdoors at annual games dedicated to the gods, and Roman theatre maintained a circus-like atmosphere Works by only two Roman writers survived till today:\nEarly Roman stages were temporary narrow platforms of wood approximately 30 m (100 ft) long. The stage house was decorated with columns, statues, niches, and porticoes, and covered with a roof.\nThe platform served as a street, where the dramatic action occurred, and openings in the back wall served as doorways into fictional houses that bordered the street.\nThe first stone theater in Rome, in imitation of Greek theatres, was built in the 1 st century BC In the permanent stone theatres, the stage house and the auditorium formed a single architectural unit, and the orchestra was a half circle between the stage and auditorium.\nA distinguishing feature of Roman theater was a curtain at the front of the stage that dropped into a slot at the beginning of a performance and was raised at the end.\nRoman actors wore thin sandals, garments of the time, and masks that were useful for playing multiple roles\nBy the 1 st century AD, these spectacles had become increasingly bloodthirsty. The last recorded performance in Rome occurred in 533 AD.\nThe ruins of many Roman theatres erected in Europe, Asia and Africa may still be seen today. Theatre reemerged in religious festivals of medieval Europe."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:540773ef-e41e-4ff4-abdc-8315c8a64117>","<urn:uuid:c3ccf671-3466-4d5a-af7b-16d58100fe0c>"],"error":null}
{"question":"What are the main differences between ready mix plants and central mix plants in concrete production?","answer":"Ready mix plants combine all materials and ingredients needed for concrete except water, with water being added onsite during discharge from the ready-mix truck. In contrast, central mix plants combine all ingredients, including water, at a central location before transporting the mixed product to the construction site.","context":["Creating a modern, durable, and cost effective building involves careful planning and healthy partnering with the related services in the construction business. For example, a batch plant in Dallas, TX will supply ready mixed concrete. Other services are those offered by concrete pumping companies and the concrete contractors. Effective partnership with these players make for fewer headaches during your construction project, especially for the big jobs.\nSo, What’s a Concrete Batch Plant?\nA batch or batching plant is simply an equipment that is used to combine different ingredients to make concrete for use in construction projects. Some of the ingredients include water, sand, aggregate, potash, fly ash, and cement. In situations where you are handling large construction jobs, this would be an essential machine to use.\nEssentially, there are two types of batching plants and they include ready mix plants and central mix plants. Ready mix plants can combine all the materials and ingredients needed to form concrete except for water. Using a ready-mix truck, the mixture is discharged onsite where water is added during the discharge.\nWhen it comes to central mix plants, all the ingredients are combined including water at a central location. The mixed product is then transported to the construction site.\nCommon Ready Mixed Concrete Types Used by Batching Plants\nThere are broadly two options for a batch plant in Dallas, TX to prepare the ready mixed concrete. These are either dry or wet mix concrete. The dry mix is a flexible alternative where the concrete is put in the truck and water added using a charging chute. The mixing is activated during transportation to the job site. If the site is far from the batching plant, using this option prevents your concrete quality from degrading while on transit.\nWet mix concrete is mixing all the ingredients at a single point in a central location of the plant and either using agitators to prevent the concrete from setting when transporting it to the job site or carrying the ready mixed concrete on an open-bodied dump truck. Most plants use an adaptation of the two techniques called shrink mixing.\nModernized plants also employ computer aided controls to ensure the concrete mixed performs to the highest standards possible. For example, digital scales are used for precision and accuracy in the proportion of water versus other aggregate materials mixed.\nThe alternative to ready mixed concrete is mixing the concrete on the job site, which is time-consuming, requires more labor, and increases the budget for your project. Additionally, concrete mixed in a central plant has experienced and skilled manpower working on the batches to ensure the quality of the concrete that you use.\nAlthough the ready mixed concrete will still be mixed on the site to get the proper slump for placement, a batching plant in Dallas, TX must ensure that the primary raw materials are well mixed to proportions fitting the specific needs of your construction. In addition, their pumping service partners reduce your fuel costs by bringing the concrete to your job site."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:d4f243b6-b783-40f4-9273-c72ba9d83540>"],"error":null}
{"question":"I need to understand basic home insurance. Can you please explain what named perils coverage includes?","answer":"Named perils coverage, also known as basic coverage, only covers specific perils that are stated in your policy. These typically include fire damage, lightning strikes, explosion, smoke damage, theft, vandalism, and wind damage. For example, if someone spray-paints graffiti on your garage door, you would be covered because vandalism is an insured peril. However, natural deterioration like rotting wood fences or aging shingle roofing would not be covered as these are uninsured perils.","context":["Dear valued HUB SmartCoverage client: The ongoing COVID-19 crisis is causing extremely high traffic volume and wait times may be longer than usual. We recommend that you register for online self serve to submit your request. Be sure of one thing, we’ll take care of you!\nDid you know that there are two types of home insurance coverage?\nComprehensive, also known as all perils or all-risk, covers property damage or loss unless they are specifically listed as being excluded. That’s why it’s important to read the fine print in your policy.\nThe other type is named perils, also called basic, that covers only those perils that are specifically stated.\nLet’s take a closer look at both.\nComprehensive insurance tends to be more expensive since it’s the most comprehensive kind of property coverage available. In addition, it may even be mandatory for your mortgage conditions.\nThis type of policy typically covers things like:\nEven though it’s called comprehensive, there are still some kinds of coverage it doesn’t include. Earthquake coverage, for example, would likely be something named as an exclusion. It’s another reason to know exactly what is excluded from your policy.\nAn important side note is that comprehensive only covers property, not liability issues. Liability insurance covers legal matters in relation to your property such as if you are sued and bodily injury and property damage.\nWhile not exhaustive, here are some of the exclusions commonly found in comprehensive policies:\nIn many cases, when something is named as an exclusion, you may be able to pay more for the coverage. That’s called a rider. If overland water damage is not covered in your policy but you want it, for example, you’ll have to pay more for it. Consider it an upgrade to your policy. That said, not all exclusions can be added as a rider.\nComprehensive insurance is sometimes required by your mortgage provider. If you don’t have a mortgage or they don’t require comprehensive coverage, you – along with your broker –can decide what type of coverage is best.\nYour decision will likely be based on your location and financial situation. Ask yourself the following questions:\nYou’ll also be selecting a deductible for your policy, so if you’re worried about the price of a comprehensive policy you could consider choosing a higher deductible.\nKeep in mind that property claims have increased more than 11 per cent in Canada since 2004 and made up 36.8% of all claims in 2018. Climate change has been pointed to as one of the main causes of this, with insured damage due to severe weather costing Canadians $1.3 billion in 2019.\nAt the end of the day, the three main things you need to remember about comprehensive insurance claims is that they must be:\nIf you want to save money and carry the financial risk of some losses yourself, you can consider named perils coverage, also known as basic. In this case, only those perils that are specifically stated in your policy are covered. These are likely to include:\nA good way to think about named perils coverage can be illustrated in this example: The gradual deterioration of shingle roofing or the rotting of a wood fence is natural and expected, so it is an uninsured peril. But if someone spray-paints graffiti on your garage door, and the graffiti must be removed and the door cleaned, the damage is due to an insured peril – vandalism. In the latter case, you would be covered.\nWhen it’s time to figure out which policy is best for you, talk to your broker. And always read the fine print in your policy."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:a30f1db9-dbbc-41f4-95e6-f47378ce47f6>"],"error":null}
{"question":"What caused the decline of African kingdoms, and what environmental factors threaten agriculture in these former empire regions today?","answer":"Most West African kingdoms slowly came to an end, with some being taken over by European countries. In these same regions today, multiple environmental challenges threaten agriculture, including population growth, climate changes, pollution, and agricultural water waste contributing to fresh water shortages. The land faces severe degradation issues, including nutrient depletion, loss of biodiversity, erosion by water and wind, reduced vegetative cover, drought, compaction, increased soil temperatures, reduced organic matter, and salinization. These problems are particularly concerning as it's estimated that by 2050, the global population will reach 9 billion, putting additional pressure on these historically significant regions.","context":["1. The Aksumite Empire. Also known as the Kingdom of Aksum (or Axum), this ancient society is the oldest of the African kingdoms on this list and is spread across what is today Ethiopia and Eritrea in an area where evidence of farming dates back 10,000 years.\nWas there an African empire?\nThe Songhai Empire\nFor sheer size, few states in African history can compare to the Songhai Empire. Formed in the 15th century from some of the former regions of the Mali Empire, this West African kingdom was larger than Western Europe and comprised parts of a dozen modern day nations.\nWhat were the African empires?\nThe leading civilizations of this African rebirth were the Axum Empire, the Kingdom of Ghana, the Mali Empire, the Songhai Empire, the Ethiopian Empire, the Mossi Kingdoms and the Benin Empire.\nWhat were the 3 major African empires?\nIn this collection, we examine the big three of the Ghana Empire, Mali Empire, and Songhai Empire as well as the lucrative trade connections they made with West and North Africa.\nWhich became the largest empire in Africa?\nUnder the rule of Sonni Ali, the Songhai surpassed the Malian Empire in area, wealth, and power, absorbing vast areas of the Mali Empire and reaching its greatest extent. Following Ali’s reign, Askia the Great strengthened the Songhai Empire and made it the largest empire in West Africa’s history.\nWhy Africa has no history?\nAccording to this imperial historiography, Africa had no history and therefore the Africans were a people without history. They propagated the image of Africa as a ‘dark continent’. … It was argued at the time that Africa had no history because history begins with writing and thus with the arrival of the Europeans.\nWhat happened to African empires?\nWhat happened to the African kingdoms? Most West African kingdoms slowly came to an end. Then new African kingdoms grew up to take their place. However, some kingdoms were taken over by European countries.\nIs Africa the first civilization?\nAfrica’s first great civilization emerged in ancient Egypt in c. 3400 BC. Carthage was founded by Phoenicians in the 9th century BC. Ancient civilization, based around the River Nile in Egypt, which emerged 5,000 years ago and reached its peak in the 16th century BC.\n|UC Merced Library||Closed|\n|GIS Lab (SpARC – SSM 209)||–|\nWho Found Africa?\nPortuguese explorer Prince Henry, known as the Navigator, was the first European to methodically explore Africa and the oceanic route to the Indies.\nWhat is the biggest kingdom in Africa?\nWhat is the largest kingdom in Africa? The largest and most powerful empire was the Songhai Empire. It is believed to be the largest state in African history. The empire existed between 1000 CE and 1591 CE and came to an end as a result of the Moroccan musketry.\nHow old is Africa?\nThe oldest formed about 3.4 billion years ago, the second some 3 to 2.9 billion years ago, and the third some 2.7 to 2.6 billion years ago. Some of the oldest traces of life are preserved as unicellular algae in Precambrian cherts of the Barberton greenstone belt in the Transvaal region of South Africa.\nHow many kingdoms are in Africa today?\nOut of the fifty-four countries in Africa today, only three monarchies have maintained monarchical significance and remain as head of state or government. They rule with fiat and possess unmatchable wealth in Morocco, Lesotho and Swaziland.","Fresh water may soon become a costly commodity. The fundamental social problem of feeding society is growing larger due to a rising scarcity of water and an ongoing depletion of agricultural land. Population growth, climate changes, pollution, and agricultural water waste contribute to growing fresh water shortages around the world. Depletion of soil nutrients through poor farming techniques, floods, poor irrigation, and winds have seriously damaged agricultural land.\nApproximately 40% of the world’s agricultural ground is unsuitable for farming. The role of agricultural business desperately needs to align with the evolving ethos of a rapidly growing society. Can hydroponic farming provide a sustainable solution to environmental problems caused by traditional farming methods? What practical applications does hydroponics have in densely populated urban areas? Farmers describe soil degradation as thinning and unproductive land that leads to low yielding crops.\nLand degradation includes nutrient depletion, loss of biodiversity, climate change, erosion by water, erosion by wind, reduced vegetative cover, pollution, drought, compaction by animals or machinery, sedimentation, increased soil temperatures, reduced organic matter, and salinization (Stockings, 2000 pg 5). According to the United Nation’s food and agriculture program, 854 million people do not have sufficient food for an active and healthy life (Sample, 2007). The population has increased by nearly 2 billion in the last 20 years and food production increased by 50%.\nIt is estimated that by 2050 the population will reach 9 billion. (Sample, 2007). State and federal officials have drafted accords in attempts to rectify water shortages. UN officials have gathered to create and execute a plan of action to improve conservation of soil and restoration of degraded land, but such plans are merely band-aids on a broken limb. The ultimate form of soil conservation is eliminating the use of soil in agriculture completely. Hydroponic agriculture offers a permanent solution to this rapidly growing problem of water shortages, pollution, and land degradation.\nDevelopment of hydroponic systems took place from 1925 through 1935. Experimentation with soil-less nutrient solutions and advancement in agricultural plastic by Professor Emery Myers Emmert at the University of Kentucky sparked an interest in hydroponic food production. Efforts were primarily aimed at large scale commercial food growth, but Hydroponic food systems were eventually abandoned due to high construction and operation costs (University of Arizona). Modern hydroponic systems are relatively inexpensive to build and offer several effective methods of food production.\nThe most popular commercial agriculture hydroponic system is the Nutrient Film Technique. NFT consist of a plastic pipe or gutter, a water reservoir, and a water pump. A thin film of nutrient infused water constantly flows through the tubing. Plant roots are suspended over the piping with only the roots touching the stream of nutrient water. The pipe or gutter is slightly elevated at the far end to allow water to drain back into the water reservoir. The NFT system delivers high levels of oxygen to the roots promoting vigorous plant growth.\nNFT systems are ideal for leafy greens such as lettuce, cabbage and basil but are effective for a multitude of fruits, herbs, and vegetables. A minimal operating cost makes this system ideal for commercial applications. [pic] [pic] A popular system for larger plants is the ebb and flow or flood and drain system. This table system consists of a plastic tray, water pump, timer, reservoir, and tubing. The plants lay on the top table separate from the nutrient reservoir. The pump is programmed to turn on in 15 minute increments with variations specific to the type of plant and stage of development.\nWhen activated, the pump will fill the table at top with the nutrient infused water from the reservoir below. Once it is turned off the water will drain back into the reservoir below. [pic] One of the simplest hydroponic methods is the deep water culture system. Often used in commercial applications, this system can be built with materials found at a local hardware store or discount retail market. The deep water culture system can be built with a non-transparent storage tote, an aquarium air pump, and an air stone commonly used in fish tanks.\nPlant roots are permanently submerged in nutrient rich water. This system does not require a water pump or timer. Urbanization and technological developments extended the distance produce traveled to reach consumers. Heilbroner explains the changed by describing that “industrial technology has literally refashioned the human environment, bringing with it all the gains-and all the terrible problems-of city life on a mass scale” (Heilbroner, Milberg pg 83 ). Hydroponics is the tool to address the basic social problem of feeding society by making produce readily accessible and grown in urbanized areas.\nAlthough traditional agriculture has served its purpose for many years, the ethos of rapidly growing demands sustainable and healthy forms of food production. Hydroponic farming in conjunction with greenhouses can help solve land degradation and water pollution problems through out the world. Lufa Farms, a Canadian farm based in Monteral, engineered a series of greenhouses on top of a 31,000-square foot office building. Greenhouse growing allows the Canadian farm to supply fresh produce all year round, even in 45 degree temperatures.\nIt is estimated that Lufa Farms can deliver more than 1,000 baskets of produce per week, three times more than land-based competitors. ( Business and the Environment). Greenhouse hydroponic farming not only solves the problem of land degradation and a water shortage due to traditional farming practices but it also alleviates the pressure of fossil fuel consumption and vehicle pollution. Because sustainable farming systems can be established in city rooftops and barges, transporting fruits and vegetables from remote farms is no longer necessary.\nProduce grown in the city have shorter distances to travel reducing transportation expenses and fuel consumption. Consumers can purchase a fresher product at their local market. According to Columbia University professor of public health and microbiology Dickson D. Despommier, a 30-story, one square block farm could produce the same amount of produce as a 2,400 outdoor soil based farm ( Business and the Environment ). The Science Barge, a greenhouse and hydroponic system on top of a barge based in Yonkers, New York, is a fully functioning urban farm prototype for a fully sustainable food production sytem.\nThe Science Barge produces tomatoes, cucumbers, and lettuce without carbon emissions, chemical pesticides, or waste water runoff (New York Sun Works, 2010-2011). The barge uses solar and wind power to sustain greenhouses on top of the barge. Purified river water and rain water is used to irrigate the crops. Science Barge grown produce use seven times less land and four times less water than traditionally grown crops. (New York Sun Works, 2010-2011). The economic advantage of hydroponic farming can be identified through a cost-benefit analysis.\nConsidering that in 2000, 41% of all freshwater used in the United States was for Agricultural purposes (Agricultural Resources and Environmental Indicators, 2006 Edition); reduction in water cost is one of the main economic advantages of hydroponic farming. BrightFarms is a company that has taken this emerging technology and developed a business model of “Better food, better prices, better environment. ” BrightFarms designs, finances, builds, and operates hydroponic greenhouse farms on supermarket rooftops.\nBrightFarms pledge is “to deliver produce at equal or higher service levels than the retailer currently requires of its other suppliers. There is no cost to the retailer to build the BrightFarm, only an obligation to purchase the output. ” The company intends to increase profits by reducing shrink due to longer produce shelf life. They have calculated that this practice will produce higher gross margins for the retailer. The company implements a long-term price fix contract to protect the retailer from unstable prices, rising cost costs and inconstant supply. BrightFarms 2012). BrightFarms feels that “with the elimination of shipping, and the drastic reduction of fuel consumption, carbon emissions and water use, BrightFarms enables grocery retailers to change their produce supply chain in a way that improves the planet and their bottom line. ” (BrightFarms 2012). According to Brian H. Kurbjeweit, professor of Contemporary Business at the University of Redlands, economic progress is an interconnected system comprised of science, economics, law, and ethics all influenced by the ethos of a society.\nThe ethos of modern society has expressed an urgent need for sustainable business practices. Science has engineered the tools to effectively create a new form of responsible farming. Materials to build hydroponic systems are readily available at a relatively low cost. Through social media, instructional videos and written instructions to build and operate hydroponic systems are readily accessible world wide through urban farming forums free of cost on the Internet. There is a basic economic need to feed and sustain life.\nThe basic economic problem creates a market for those willing to invest in large-scale urban farming businesses. Responsible agriculture is an ethical guideline that all farmers in global and local economies should follow to ensure the basic economic problem is addressed. Continued land degradation, water pollution, and pesticide smothered produce combined with a rapidly growing world population will lead to further human suffering. It is an ethical duty of world leaders to implement sustainable forms of agriculture to nurture its citizens. Law, the missing factor is yet to be addressed.\nIt may be the ethos of future generations that persuades governments to execute laws to speed up the implementation of sustainable farming techniques or it may be severe ethical violations by non-complying agricultural corporations that spark the creation of laws to effectively protect citizens from illness caused by pesticides and water pollution. It may simply be an epidemic interest in well being or as author Malcom Gladwell describes, a tipping point may be reached where local co-ops solve the basic economic problem, one small community at a time, using sustainable forms of agriculture.\nStockings, M., Niamh, M. (2000). Land Degradation. Guidelines for Field Assesment, 5, 59-67.\nHeilbriner, R., & Milberg W. (2008) The Making of Economic Society (12th Ed.). Saddle River, NJ: Pearson Education, Inc.\nSample, I (2007). Global food crisis looms as climate change and population growth strip fertile Land. The Guardian.\nRetrieved from http://www.guardian.co.uk/environment/2007/aug/31/climatechange.food. Buying Local Takes on New Meaning. (2011). Business & the Environment, 22(9), 1-4. University of Arizona, Growing Tomatoes Hydroponically Retrieved from http://ag.arizona.edu/hydroponictomatoes/overview New York Sun Works, Center For Sustainable Engineering. (2011). Retrieved from http://nysunworks.org/thesciencebarge\nBrightFarms. (2012) Better Food, Fresher Food. Retrieved from http://brightfarms.com/about/"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c80d3fa4-5e6d-492f-8d65-525b36efd183>","<urn:uuid:f48335fe-9324-48ac-9ffa-30fc843b9511>"],"error":null}
{"question":"What are the types of gas meters used in the industry, and what are the key inspection requirements for storage tanks that contain petroleum products?","answer":"There are three primary types of gas meters: 1) Diaphragm meters, which use four chambers with a membrane and rotating piston to measure gas flow, 2) Rotary displacement meters, which use opposing rotary impellors to form chambers and count turns, and 3) Turbine meters, which measure flow rates using a gear system and turbine in an extrusion housing. As for storage tank inspection requirements, routine in-service external inspection must be done monthly, external ultrasonic thickness inspection must occur at least every five years when corrosion rates are unknown, and internal inspection intervals must not exceed 10 years for initial inspection and 20 years for subsequent inspections unless special conditions are met. All inspections must be conducted by an API 653 Certified Tanks Inspector and documented in detailed written reports.","context":["Is Your Gas Meter Squeaking? (Possible Causes & Fixes)\nYou finally locate that strange squeaking noise. The noise, a high-pitched squeaking sound, is coming from your gas meter. What causes a gas meter to squeak and what should I do about it?\nMore than likely, the cause of the squeaking noise is the internal parts of the gas meter rubbing against each other. The sound is not dangerous, as the gas measurement system is encased in a metal housing. Check if your gas meter is working well first, and then notify the gas utility company.\nAll gas meters have internal moving parts that measure the amount of gas that flows into your gas system. Like any moving machine, parts wear out or get out of adjustment. Other problems sometimes happen with gas meters, but they are rare. A bit of understanding about gas meters can help any fears that the gas meter may start leaking.\nDo you suspect other problems with a gas appliance? Check out this article about gas-fired furnaces.\nHow Does a Gas Meter Work?\nThere are three primary types of gas meters used in the industry. Two of these varieties of natural gas meter uses known volume methods of measurement. The third type uses the actual flow rate of the gas to measure the amount delivered through the meter. All three types have moving parts that can make noises.\nDiaphragm Meters – Round and Round the System Goes\nThe diaphragm gas meter has four chambers. A membrane, the diaphragm, covers the chamber. A rotating piston moves from chamber to chamber, allowing gas to stretch the diaphragm and fill the chamber with gas.\nThe piston then moves to the next chamber, and the gas-filled chamber goes to your home gas system. By counting the number of chambers filled and released during a billing cycle, the utility can calculate how much gas you use.\nRotary Displacement Meters – Counting Turns\nRotary displacement meters use the same principle of filling and counting a known space. In a rotary displacement meter, two opposing rotary impellors form the chamber. Gas enters the chambers created as the impellers rotate and the meter counts the number of turns on the impeller.\nTurbine Meters – Go with the Flow\nTurbine systems also measure flow rates but in a slightly different way. The turbine gas meter consists of a gear system, a turbine in an extrusion housing, and a sealed gas meter body. When gas flows through the extrusion housing the turbine, the turbine begins to turn.\nThe gear body counts the number of turns the turbine makes. The gas company uses this number to calculate how much gas has flowed through the meter.\nGas companies use turbine meters in a system that experiences fluctuations in the flow rates or pressures in the gas system. The turbine can accurately measure gas flow despite fluctuations in the system, making turbine meters a bit more accurate than other types.\nVideo: Fixing Your Squeaky Gas Meter\nThat Squeaking Your Hear Maybe Normal\nEvery style of gas meter has moving parts. Most of these moving parts are plastic designed for use with natural gas. However, natural gas may contain traces of other gases that can wear or damage the plastic parts. In time, the plastic parts will also wear out.\nThe squeaking, clicking, or moaning, you hear coming from your gas meter is not unusual. These noises can be annoying and irritating, especially if the gas meter is inside your home. A call to your gas utility will have a technician on the way. The technician will either fix the meter or replace the noisy one with a new meter.\nIf the Meter Makes Noise, is it Dangerous?\nAbsolutely not. Gas meters are safe. The entire gas measurement system sits inside of a sturdy metal housing. The housing has seals to prevent any leakage. Even if the measuring system fails, the housing will contain the gas.\nIs there anything I can do to Stop the Squeaking?\nAgain, the answer is no. Your gas meter is the property of the gas company. You should never attempt to fix, repair, or remove a gas meter. Tampering with a gas meter, even in good faith, can be a criminal act in some states. Your best bet is to call your gas utility and let them handle the problem.\nHow Do I Know if the Gas Meter is Working Right?\nMost states regulate gas meters and require inspection and certification regularly. However, if you suspect that your gas meter is not correctly calculating the amount of gas you are using, you should check on the meter readings. Reading your gas meter is not hard and can be done in a few simple steps.\nStep 1 – Locate the Meter and the Dials\nFind your gas meter and look on the front. You will see a row of dials with the number 0 through 9 printed on the dial. The number of dials may vary between manufacturers of the gas meter, but the process is the same.\nStep 2 – Work from Left to Right\nAlways read the dials from left to right. If the hand on one of the dials is between two numbers, the proper reading is the lower number. Write down the number you get after reading the dials from left to right\nStep 3 – Calculating the Amount of Gas You Use\nYou need two readings to calculate the amount of gas that the meter has recorded. A starting reading gives you a baseline number. A second reading, taken several days apart, gives you your final reading. Subtracting the baseline reading from the final reading gives you the total amount of gas passed through the meter.\nWhat The Number Means\nMost gas utility companies measure gas usage in cubic feet. In areas of the country that get extremely cold, the measurement may be in thousands of cubic feet. To determine what units your utility company measures, a phone call to the customer service department will be all it takes.\nYou will also need to know the rates that the gas utility company charges for each unit of measurement. With the current rate, you can calculate approximately what your gas bill is for the period you measured.\nBut What Does that Tell Me?\nIf you are doing your meter reading, you should know about how much gas you have used during the period you measured. If the only natural gas appliances in your home are your heating system and a water heater, a high gas meter reading in the summer months could be a clue to a problem.\nBe Safe – Work with your Gas Utility if You Suspect a Problem\nNatural gas has proven to be an extremely safe product. Gas companies go to great lengths to keep their systems operating at peak efficiency and safety. If you suspect a problem with your gas meter, contact your gas utility and report the suspected problem.\nIf you think your gas meter is leaking or you can see damage to the meter itself, there are some things you should do immediately.\n- If there is no apparent leak, call the gas utility company promptly.\n- If the gas meter has visible damage or you can hear escaping gas, immediately evacuate the area around the leak. Call 911 and report the gas leak.\n- If the leak is in your home, get everyone out of the structure. It may also be a good idea to evacuate your neighbors if you live in an apartment of joined housing.\nUncontrolled natural gas can quickly become an explosion and fire hazard. Never try to diagnosis a gas leak yourself. The act of flipping a light switch may be all it takes to create an explosion and fireball.\nAnnoying, But Not Really Dangerous\nA gas meter making noise may be annoying but doesn’t present a danger unless the meter is damaged somehow. The gas utility company owns the meter. You should notify the gas utility promptly of any problem with your gas meter or service.\nWe hope that this article has answered your question about your squeaking gas meter. Understanding how meters work and how to keep track of gas usage by reading your meter can be handy tools.\nDennis is a retired firefighter with an extensive background in construction, home improvement, and remodeling. He worked in the trades part-time while serving as an active firefighter. On his retirement, he started a remodeling and home repair business, which he ran for several years.\nMore by Dennis Howard","What you should know about your petroleum storage tanks\nAre your storage tanks fit for service or are they an impending disaster? The purpose of this article is to advise you on the inspection of your storage tanks and how to minimize risks that they may pose to you, the environment and other stakeholders.\nAboveground Storage Tanks (AST) are used for storage of crude oil and its derivatives such as gasoline, kerosene, diesel and fuel oil. The material of construction is carbon steel, with tank sizes ranging from 3m to 60 m in diameter. They are constructed to BS 2654, API 650 or its predecessor API 12C. There are approximately 300 storage tanks in Kenya. The main components of storage tanks are the shell, bottom, and the roof. Other components include nozzles, roof trusses, electrical shunts, stair ladders, roof seals, among others.\nOwing to the impurities present in hydrocarbon such as salts and sulphur, tanks are prone to damage mechanisms such as corrosion, which if not adequately mitigated, leads to failure of the tank.Corrosion in tanks occurs at rates and patterns unique to every component and product stored. Different products have different corrosivity and corrosion patterns, thus they dictate different life spans for the tanks. The difference in product corrosivity results from the difference in the percentage composition and nature of impurities (corrosive media) embedded in each product.\nAs a result, some products may be more aggressive on bottom plates while others may attack the roof or the shell more. For instance, kerosene tanks corrode heavily on the bottom plates but negligibly on the roof plates whereas diesel tanks corrode heavily of the roof plates internally. While the shell plates for kerosene and diesel tanks may suffer mild corrosion, gasoline tanks on the other hand may suffer heavy uneven corrosion on the shell plates.\nConsequence of Failure\nUnmitigated corrosion and lack of sufficient inspection of storage tanks eventually results in failure by product leakage. Loss of containment is an undesired eventuality with unpleasant consequences such as;\n? Environmental pollution from spillage of hazardous product\n? Product loss\n? Environment decontamination/ clean-up cost\n? Negative publicity\n? Possible hazards e.g. fires, fatalities\n? Legal implications\n? Loss of asset, costly repairs/ replacement\n? Loss of operation time due to unplanned outage\nCase studies from around the world show that the major contributor to failure of storage tanks is failure to carry out sufficient, timely inspection. For instance in 1919, USA, 21 people died and 12 million litres of product lost after a storage tank exploded. This was attributed to inadequate inspection and hefty fines were incurred in legal suits by the owner. Several other failures have occurred in the recent past, including in Kenya resulting in various consequences listed above.\nA common malpractice among tank owners has been carrying out in-service external inspection that is assumed sufficient for evaluating the fitness of the tank for continued service. Similarly, during out-of-service inspection, inspection is carried out without sweep blasting of bottom plates, in which case, visual inspection results are compromised.Inspection, Repair, Reconstruction, and Alteration of AST is governed by API 653, which gives guidance on inspection methodology, frequency, evaluation for suitability for service (acceptance and rejection criteria for components), repair methods, welding guidelines, and QA/QC measures during and after repairs.\nInspection and tank evaluation must be done by an API 653 Certified Tanks Inspector.\nEvaluation of Suitability for Service\nWhen tank inspection results show that a change has occurred from the original physical condition of the tank, an evaluation must be done to determine its fitness for continued use or change of service. In addition, when making decisions regarding repairs, alterations, dismantling, relocating, or reconstructing an existing tank, this evaluation must be done.\nPlates corroded to an average thickness of less than 2.3 mm in any 100 m2 area or plates with through hole(s) must be repaired or replaced.\nFlaws, deterioration, and corrosion greater than the original corrosion allowance that might adversely affect the performance or structural integrity of the shell must be evaluated and fitness for service determined. The tank shell must be evaluated for strength, stability, remaining life and the maximum filling height allowable after corrosion.\nThe integrity of tank bottoms must be determined to prevent perforation and subsequent leakage of hydrocarbon. The bottom may fail from;\n? Internal corrosion (product side) – pitting or uneven corrosion\n? External (Soil side) corrosion\n? Uneven settlement resulting in high localized stresses in bottom plates/ welds\nProduct side corrosion may be minimized or delayed by use of internal lining protection whereas soil side corrosion may be mitigated by installation of cathodic protection system on bottom plates. Consequences associated with bottom leakage may be minimized by installation of leak detection systems and leak prevention barriers which prevent escape of released material and/ or contain/ channel it for leak detection.\nAPI 653 stipulates that the minimum bottom plate thickness at the time of next inspection must be 2.54 mm for sketch and rectangular plates and not less than 5mm for annular plates. This implies that the thickness at current inspection must be such that after incorporating corrosion rate over the interval to next inspection, the minimum thickness at next inspection must be as stated above. Therefore, evaluation of tank bottom integrity must be done such that all sections that do not meet API 653 requirements are repaired or replaced.\nWhen determining the inspection intervals for AST, several factors must be considered e.g.;\n? Nature of product stored\n? Corrosion allowance and corrosion rate\n? Corrosion prevention systems and leak detection systems in place\n? Condition at previous inspection and repairs done\n? Location of tanks and level of risk associated with failure\nService history of a tank or that of others in similar service can help in determining the inspection intervals.\nRoutine in-service external inspection must be done monthly by the owner/ operator personnel. This aims to observe for evidence of leak, shell distortion, signs of settlement, corrosion, condition of the foundation, paint coating insulation and grounding systems.\nExternal Ultrasonic Thickness Inspection must be done at intervals not exceeding five years when the corrosion rates are unknown.\nInternal inspection is required to ensure that the bottom is not severely corroded and leaking. It helps gather data necessary for minimum bottom and shell thickness assessment, as well as identify and evaluate bottom settlement.\nThe interval from initial service to the initial internal inspection must not exceed 10 years unless special conditions outlined in API 653 are met. The interval between subsequent inspections is determined in accordance with the established corrosion rates. Unless a Risk Based Inspection assessment is performed as described in API 653, the interval must not exceed 20 years.\nA written report must be prepared by the Tank Inspector after every external or internal inspection. This report must include;\n? Date of inspection\n? Type of inspection (internal or external)\n? Scope of inspection including mention of areas that were not inspected and reasons.\n? Description of the tank (number, size, capacity, year of construction, material of construction, service history, roof and bottom design)\n? List of components inspected and their condition\n? Inspection methods and tests used e.g. visual, MFL, UT and the results\n? Corrosion rate of bottom and shell\n? Settlement survey measurements and analysis\n? Recommendations for repair, replacement, monitoring, calculated inspection interval\n? Name, company, API 653 certification number and signature of the Authorized Inspector responsible for the inspection\n? Drawings, photos, NDE reports and other pertinent information appended\nTank Repair and Alteration\nAll repair work must be authorized by the Authorized Inspector (AI) or an engineer experienced in storage tank design before commencement of work by a repair organisation. All proposed design, work execution, materials, welding procedures, examination, and testing methods must also be approved. The AI must designate inspection hold points required during the repair or alteration sequence and minimum documentation that must be submitted upon job completion. During repair and alteration, various minimum requirements of API 653 must be met.\nWelding and Testing\nWelding procedure specifications, welders and welding operators must be qualified in accordance with ASME IX. Preheat and PWHT requirements must be observed where required.\nExamination and testing of repaired components shall be performed in accordance with sec 12 of API 653. Among techniques that are commonly used for testing are Visual Inspection, Magnetic Particle Inspection, Dye Penetrant Testing, Diesel Test, Vacuum Box Test, Radiography, hydrostatic test.\nStorage tanks are a critical asset to the energy sector. Proper maintenance and care is vital to ensure continued availability in service and prevent costly failures. Timely inspection by an authorized inspector is important to evaluate the corrosion rates, remaining life, strength, stability, fitness for service and maximum filling height."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9d51ff5c-b129-414e-b1d6-7a3c0824ec89>","<urn:uuid:f352adbe-3d47-4791-ae06-cc66b6ce115b>"],"error":null}
{"question":"What are the key sources of vitamin D intake, and how do athletes' vitamin requirements differ from the general population?","answer":"Vitamin D can be obtained through three main sources: sunlight exposure (15 minutes daily without sunscreen), dietary sources (egg yolks and fish, particularly salmon), and supplements. However, dietary sources alone are typically insufficient - one egg yolk provides only 20 units, and even fortified foods like milk and cereals provide just 100 units per serving. Regarding athletes, while there are no set recommended amounts specifically for them versus the general population, athletes likely have higher vitamin and mineral requirements due to their physical training demands. However, these increased needs are usually met through their higher caloric intake, as athletes typically consume more food overall.","context":["Vitamins and minerals are essential nutrients that must be consumed since our bodies cannot make them. Vitamins and minerals help you to stay healthy by playing a wide variety of roles in the body from immune system function, to helping our hearts beat.\nThe amount of each vitamin and mineral we need each day is variable depending on sex, age and stage of life. There are no set recommended amounts of vitamins and minerals for athletes versus the general population. However, athletes likely do have higher requirements because of the physical demands of training. For instance, athletes often have higher iron needs due to the losses that can occur during exercise and higher antioxidant needs to help with damaged muscles.\nWhile vitamin and mineral needs are likely higher in athletes, so are calorie requirements, which means athletes should be eating more food. This increased food consumption should equate to increased vitamin and mineral intake. For instance, an athlete may have a couple more slices of bread at breakfast or an extra snack before bed, which will provide a higher daily intake of vitamins and minerals.\nWhile nutrition fact labels are certainly useful, they can be a little misleading when providing information on vitamin and mineral content of food. Nutrition fact labels only provide information on 4 vitamins and minerals: vitamin A, vitamin C, calcium and iron. For each of these vitamins and minerals, the % daily value (DV) will be listed. However, the measurement that the % DV is based on may be more or less than what you actually require. Below highlights what intake a label is based on, versus what is recommended based on sex and age.\n> 70 years\n|Vitamin A||1000 RE||900 RE||900 RE||700 RE||1000 RE|\n|Vitamin C||60 mg||90 mg||90 mg||75 mg||75 mg|\n|Calcium||1100 mg||1000 mg||1200 mg||1000 mg||1200 mg|\n|Iron||14 mg||8 mg||8 mg||18 mg||8 mg|\nAs you can see, you may need more or less than what is used to determine the %DV. For instance, if something has 20% DV for iron, this is actually 35% of the daily requirements for males and females over 50 years and 16% DV for females 19-50 years of age. This doesn’t mean that the %DV is useless. Rather, the % DV for vitamins and minerals should be used as a guide. If something is greater than 15% DV then it’s considered a lot. If it’s less than 5% DV then it’s a little.\nIf you are eating a wide variety of foods from each food group and adequate calories, then you’re likely eating adequate amounts of vitamins and minerals to meet your requirements. On the other hand, athletes who consume too few calories or do not eat certain foods or food groups such as meat, dairy products and gluten are at risk for vitamin or mineral deficiencies. However, before you starting popping supplements, I would suggest meeting with a Registered Dietitian for a more in-depth analysis of your current diet. More is not always better and in some cases, can be harmful for health.\nTo get this free weekly newsletter straight to your inbox, click here.","Since the beginning of the coronavirus disease 2019 (COVID-19) pandemic, people have turned to vitamins and other supplements to boost their immune systems in hopes of fighting off COVID-19. Vitamin D, in particular, has received attention as a potential safeguard against infection.\nIn recent studies that examined the effect of vitamin D on COVID-19, one found that people who had a vitamin D deficiency were more likely to test positive for the virus that causes COVID-19 than those who had normal levels of vitamin D. Another study found high rates of vitamin D deficiency in people with COVID-19 who experienced acute respiratory failure.\nHowever, at this point, there is not enough data to recommend the use of vitamin D to prevent or treat the virus, according to the National Institutes of Health.\n“Researchers did some small observational studies that showed an inverse association between vitamin D levels and COVID-19, meaning the lower the vitamin D levels, the higher the COVID-19 cases,” says UNC Health family medicine physician Sarah Ruff, MD. “But common risk factors for both COVID-19 and vitamin D deficiency are age and obesity.”\nIn other words, vitamin D deficiency is more common in people who are older and people who have a body mass index of 30 or higher (obesity), and these factors also increase the risk of severe COVID-19 symptoms.\n“There’s no clear evidence that vitamin D supplementation would decrease your risk or the severity of COVID-19,” Dr. Ruff says. “That being said, regardless of COVID-19, everybody should make sure that they’re getting enough vitamin D.”\nThat’s because vitamin D supports the immune system by fighting off harmful bacteria and viruses in addition to helping with bone health. Vitamin D also:\n- Helps regulate how much calcium is in your bloodstream, which helps with heart function, among other roles\n- Corrects mineral imbalances to keep your kidneys healthy\n- Improves muscle function\n- Keeps nerves and nerve receptors healthy to encourage good brain-body communication\n- Reduces inflammation\nDr. Ruff says you should try to get 800 to 1,000 units of vitamin D per day. So how do you do that? She offers three tips for boosting vitamin D:\n1. Spend some time in the sun every day.\nThe most natural way to get vitamin D is with sunlight. Brief exposure to the sun without sunscreen—15 minutes daily—is the most efficient way to spur the production of vitamin D in the body.\n“Vitamin D is actually produced in our bodies,” Dr. Ruff says. “The sun’s energy turns a chemical in your skin into vitamin D, and that goes into the liver and then the kidneys, where it becomes active and your body can actually use it.”\nAlthough sun exposure increases your risk of skin cancer, Dr. Ruff says the recommended time per day needed to get adequate vitamin D is probably not enough to cause skin cancer.\n“Now that we’re all very worried about skin cancer, we don’t get the sun like we did in the olden days, and in order to get the vitamin D activation from the sun, you have to have bare arms, bare face and no sunscreen for 15 minutes a day,” Dr. Ruff says.\n2. Eat lots of eggs and fish.\nIf spending time in the sun is not an option, you can try to get vitamin D through your diet, though it’s nearly impossible to get an adequate amount through diet alone. Egg yolks and some types of fish contain vitamin D. Salmon is the best choice, Dr. Ruff says.\n“If you just wanted to get it naturally from fish and egg yolks, you have to eat a ton,” Dr. Ruff says. “One egg yolk is only 20 units. To get 400 units from fish, you’d have to have a 5-ounce salmon, 7 ounces of halibut, 30 ounces of cod or two 8-ounce cans of tuna every day.” That’s obviously not feasible.\nAlthough there are other ways to get vitamin D through our food, such as fortification in milk and cereals, each serving is typically about 100 units. So even if you get 100 to 400 units a day from the foods that you eat, that’s not enough.\n3. Take a vitamin D supplement.\n“Most patients should take a vitamin D supplement, especially in the winter,” Dr. Ruff says.\nDr. Ruff recommends vitamin D3, because that’s the kind the human body naturally makes. Vitamin D3 is animal-derived, whereas vitamin D2 is from plants. Both can help increase vitamin D levels in your body.\nAlso, don’t take more than the recommended amount.\n“It is a fat-soluble vitamin, which means if you take too much of it, you can’t urinate it out like a water-soluble vitamin, so it can build up in your fat in your body if you have too much,” Dr. Ruff says. “And it can cause toxicity (meaning it can make you very sick).”\nThink you may have a vitamin D deficiency? Talk to your doctor or find one near you.\nPhoto credit: ©Tashi-Delek – gettyimages.com"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:68ae4d53-af4d-463a-b2f8-a009ae7f078a>","<urn:uuid:d003bb49-7f86-48b9-8463-efd9f0bc89ab>"],"error":null}
{"question":"How does social identity bias affect both racial and gender discrimination in professional settings?","answer":"Social identity bias affects both racial and gender discrimination in professional settings through unconscious mental processes. In racial contexts, studies show that people naturally divide others into 'ingroups' and 'outgroups,' though this bias can be reduced through shared team identities or institutional monitoring. For gender discrimination, implicit bias leads to systematic preference for men in hiring and promotions - for example, studies found identical resumes with male names were rated as more competent than those with female names, and even a small 1% bias in early career promotions can result in men holding 65% of leadership positions over time. Both types of discrimination operate through subconscious categorization rather than explicit prejudice.","context":["]’s seminal work on the role of identity in affecting economic choices, an increasing number of economics papers have incorporated insights of social identity theory, going in the direction of building what was called “an economics of identity” [2\n]. Starting from Tajfel’s research and his investigation on the cognitive aspects of prejudice in the 1960s and 1970s, in the “Power of Us”, Van Bavel and Packer [3\n] recall the key definitions and concepts of social identity as the product of a person’s group membership(s) (“the collective self”) that determines her sense of who she is and shapes group and intergroup processes. As posited by [4\n], the groups (e.g., social class, family, nation, ethnic enclaves, etc.) which people belong to give them a sense of social identity and are an important source of self-image. The cognitive process of social categorization divides the world into “them” (the “outgroup”) and “us” (the “ingroup”) and tends to exaggerate the differences between groups and the similarities of things in the same group. The starting point of the book is the central hypothesis of social identity theory, according to which group members of an ingroup will seek to find negative aspects of an outgroup, thus enhancing their self-image. In a related vein, the social distance between an individual and other individuals or groups of individuals belonging to the outgroup is “the degree of identification” (“social tie”) between them [5\n]. When social distance decreases, the “other” is no longer some unknown individual from some anonymous crowd but becomes an “identifiable victim” [7\n] and the importance of her well-being increases.\nThe fundamental contribution of Van Bavel and Packer is the idea that the social self is highly flexible and “chameleonic” in this respect. Sharing a common identity might be enough to change attitudes towards other people, even in contexts where racial bias would be expected to emerge. The authors discuss their own research on mixed-race teams, where participants were assigned randomly to two mixed-race teams made up of people (half were Black and half were White) who initially did not know each other. Subjects were asked to memorize the faces of their team peers and to identify them on a screen while their brains were scanned. The analysis of their brain scans showed a greater activity of the amygdalae when they saw a teammate, regardless of race. A simple and fast manipulation of which identity had become central (the team identity instead of the racial identity) succeeded in activating a region of the brain that responds to stimuli that are highly relevant to people. Interestingly, the authors claim that our brains are not wired for racism, “but (if anything) are wired for social identity” (p. 128). In this respect, consider minimal group paradigm experiments (see [8\n] and follow-up studies). An important reason why, also in minimal group paradigm experiments, people express a preference for ingroup members is because a certain social identity is made more salient than another, and this opens the road to the possibility of shifting group boundaries to form new identities.\nThe perceived distance between a subject and the ingroup, but also between the subject and the outgroup, can be manipulated in several ways. This is well known in the experimental economics literature (see, e.g., [9\n]). However, Van Bavel and Packer interestingly highlight how the social distance between the subject and the outgroup might be reduced by shifting weight on a dimension on which all (both ingroup and outgroup) members are identical. The social distance between the subject and the outgroup might be reduced by shifting weight on a dimension on which all (both ingroup and outgroup) members are identical. In this respect, the authors discuss Mark Levine and coauthors’ experiment [14\n] on Manchester United fans and show the effectiveness of activating a larger and more inclusive identity through a “cross-categorization” procedure, referring to a larger super-ordered category which encompasses both supporters (the Manchester United ones and those of their rivals) in the same ingroup of “football fans”. How much we offer help to other people (like Levine and coauthors’ subjects) depends on whether we see them as sharing a part of our identity. The philosopher Peter Singer refers to this idea as a “moral circle”, the boundaries of which determine who is worthy of our concern and who is not.\nThe other important insight that can be food for thought for economists is about the role of institutions. The more people trust the social institutions, including the government, the legal system, or the police, the more comfortable they are in their interaction with the members of other groups. Van Bavel and Packer recall the findings of another of their experiments with a mixed-race group of White and Black participants. Subjects were informed that, later on in the study, they would play a series of games with other participants; from a set of photos on their screen, they learnt that other participants could be White or Black. Participants played individually. Both White and Black subjects completed an implicit racial bias test; treated subjects were made aware that an independent, impartial observer would monitor participants’ behavior. The results show that the presence of a monitoring presence reduced the level of racial bias between White and Black participants significantly. Having an institutional-type structure which promotes trust between people has been shown to reduce bias even in the absence of a team and of a shared identity, as demonstrated in this experiment. Becoming part of a team is something that we seek to overcome the fear of dealing with people and situations we do not know. Promoting trust, for instance, in the form of a fair enforcer, might reduce our need for being part of a team.\nThis book challenges the idea that social identity and ingroup bias are ineradicable. For economists, it also succeeds in explaining when and why “leveraging the collective mind” [15\n] can be an effective way to affect behavior, sometimes even more effective than providing monetary incentives, thus enlarging the set of tools that economists are familiar with and advocating for a deeper comprehension of how social identity can be built but also dismantled.","On April 29, 2019, I moderated a panel discussion for the State Bar of Wisconsin’s Diversity Counsel Program titled “Closing the Gender Leadership Gap.” The following statistics were shared at the program. According to a study by the American Bar Association, “A Current Glance at Women in the Law,” half of the students graduating from law school with a J.D. are women. Yet, only 22.7% of law firm partners are women, 22% of state court judges are women, and 26.4% of Fortune 500 general counsel positions are held by women. A significant barrier for women in the workplace is implicit bias. After serving on this panel, I was curious to explore how the concept of implicit bias might contribute to the gender leadership gap in the legal profession.\nImplicit bias is the term that describes how the subconscious mind categorizes people. The concept was first developed by psychologists Mahzarin Banaji and Anthony Greenwald in the 1990s. Through the use of implicit association tests (“IAT”) Banaji and Greenwald evaluated the time it took for a participant to categorize concepts such as family or career with gender. The quicker the applicant could categorize concepts, the stronger the implicit association. The most frightening aspect of implicit bias is that a person may be consciously opposed to gender discrimination but may unknowingly discriminate against women due to an implicit bias that exists only in the subconscious mind.\nStudies suggest that implicit bias may play a role in explaining why men are systematically preferred for positions over women. For example, a Yale study demonstrated a statistically significant preference for men in the field of science. The study involved sending a fictional resume to 100 faculty members at top universities. The only difference was that 50 fictional students were named John, while the other 50 fictional students were named Jennifer. Even though the candidates had identical experience and qualifications, faculty members were more likely to find John competent and were more likely view him as a suitable candidate for lab positions. Furthermore, faculty members were more likely to mentor John and would offer him a higher salary than Jennifer. Another, study by Cecilia Rouse and Claudia Goldin demonstrated woman’s chances of advancing through the preliminary rounds of orchestra auditions increased 50 percent when a screen blinded the judges from knowing her gender.\nThese and other studies suggest that a wide array of professions, including the legal profession, are systematically discriminating against women in the hiring process. It is unlikely that the universities and orchestras explicitly prefer male candidates to their female counterparts. The more plausible explanation is that those in the position of selecting candidates have unknowingly allowed implicit bias to affect hiring decisions.\nSimilarly, male preference in promotion decisions can result in devastating ripple effects over time. A study using computer simulation to generate the real world effect of gender bias suggests that the consequence of even a 1% bias favoring promotion of men at early career stages could result in men holding 65% of leadership positions downstream. The same study demonstrated that a 5% bias favoring the promotion of men at early career stages resulted in men holding 71% of leadership positions downstream. Thus, “by taking into account the relative scarcity of very senior-level positions in organization as well as the weight accorded early career performance, a little bias hurt women a lot.” As I enter my ninth year of practice, I am grateful for my past and current mentors, who recognize my competence, challenge me, and afford me great responsibility. Yet, as I progress through my career, the frequency with which I encounter women litigators at my level of practice diminishes.\nAs a 2016 ABA study, “You Can’t Change What You Can’t See” explained, women and people of color face challenges in the workplace not experienced by white men. For example, most people commonly associate lawyers with white men. Lawyers who don’t fit into that category report having to set forth more evidence of competence to be accepted. Women of color often face the greatest challenge in this and other arenas.\nAnother particularly difficult challenge women face is learning to strike a balance between being assertive and being viewed as overly aggressive or shrill. As the 2016 ABA study concluded:\nboth women and people of color have been invited into today’s legal workplaces, but the kinds of behaviors white men exhibit in order to get ahead are less likely to be accepted from other groups. Instead, women and people of color are more likely than white men to report that they are expected to be ‘worker bees’ who keep their heads down but do not seek the limelight.\nFor litigators, such as myself, this task can become daunting when dealing with a difficult witness during trial. Jury decisions are often influenced based upon their general like or dislike of counsel. For women, it’s a thin line between strong advocacy and being unlikable.\nThe effects of implicit bias will not be mitigated overnight. The first step is to recognize its existence. When selecting this topic, I decided to take an IAT on gender. The test asked me to group concepts such as salary or wedding with male or female names such as John or Emily. I took the test three times, and three times the test revealed that I have a strong association of men with the concept of career and women with concept of family. As a career woman, a feminist, and a progressive, I was shocked by the results. While controversy exists over the efficacy of IATs, if Banaji and Greenwald are right, then I am implicitly gender biased. In their book, “Blindspot: Hidden Biases of Good People,” Banaji and Greenwald speak at length about the tension people feel when their implicit biases do not reflect their consciously held beliefs. Most people, such as I, are disturbed to learn about their hidden bias against groups to which they belong. In fact, participants often question the validity of the tests when confronted with undesirable results.\nUnfortunately, it is difficult to retrain our brains and reverse the effect of implicit bias. Bypassing bias is sometimes possible through blinding, such as erecting a screen between candidates and judges in the orchestra studies. However, the ability to manipulate circumstances in this manner is not always possible. The most practical solution is to remain objective when interacting with evaluating others. When determining whether to hire a candidate, consider concrete strengths and weaknesses and stay away from gut decisions. Consider structuring performance evaluations on objective versus subjective standards. Remain mindful of who you choose to mentor, who you choose to give meaningful projects to, and why you have made those choices. Most importantly, consistently question yourself. Did I make that judgment on merit, or is there something else going on? If we all ask these questions, we can take a step toward mitigating implicit bias."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:474561af-473f-42ea-bbe9-39112610cc22>","<urn:uuid:964c4738-c9f7-4826-b6ec-ae55d913ed86>"],"error":null}
{"question":"How do peacebuilding and competitive conflict style differ in their approach to handling conflicts?","answer":"Peacebuilding and competitive conflict style represent opposing approaches to handling conflicts. Peacebuilding focuses on creating sustainable peace through reconciliation, institution building, and addressing root causes of conflict, while emphasizing interdependence and partnership. It aims to transform relationships and create spaces for peaceful interaction. In contrast, the competitive conflict style maximizes individual assertiveness and minimizes empathy, with individuals seeking domination over others. Competitive individuals often use power tactics like arguments, insults, or accusations to force others to accept their views through intimidation, which goes against peacebuilding's principles of limiting violence and promoting peaceful conflict resolution.","context":["Selected Definitions of Peacebuilding\nThe following list is not exhaustive and only provides some of the most prominent academic and institutional definitions of peacebuilding.\nConflict Information Consortium, University of Colorado\nPeacebuilding is a process that facilitates the establishment of durable peace and tries to prevent the recurrence of violence by addressing root causes and effects of conflict through reconciliation, institution building, and political as well as economic transformation. This consists of a set of physical, social, and structural initiatives that are often an integral part of postconflict reconstruction and rehabilitation.\nPeacebuilding is the process of creating self-supporting structures that “remove causes of wars and offer alternatives to war in situations where wars might occur.” Conflict resolution mechanisms “should be built into the structure and be present there as a reservoir for the system itself to draw upon, just as a healthy body has the ability to generate its own antibodies and does not need ad hoc administration of medicine.”\nGlobal Partnership for the Prevention of Armed Conflict\nPeacebuilding involves addressing social and political sources of conflict as well as reconciliation.\nJoan B. Kroc Institute for Peace & Justice, University of San Diego\nStrategic Peacebuilding Principles:\n- Peacebuilding is complex and has multiple actors.\n- Peacebuilding requires values, goals, commitment to human rights and needs.\n- Peacebuilding goes beyond conflict transformation.\n- Peacebuilding cannot ignore structural forms of injustice and violence.\n- Peacebuilding is founded on an ethic of interdependence, partnership, and limiting violence.\n- Peacebuilding depends on relational skills.\n- Peacebuilding analysis is complex; underlying cultures, histories, root causes, and immediate stressors are essential.\n- Peacebuilding creates spaces where people interact in new ways, expanding experience and honing new means of communication.\n- Peacebuilding heals trauma, promotes justice and transforms relationships.\n- Peacebuilding requires capacity and relationship building at multiple levels.\nLederach, John Paul\n[Peacebuilding] is understood as a comprehensive concept that encompasses, generates, and sustains the full array of processes, approaches, and stages needed to transform conflict toward more sustainable, peaceful relationships. The term thus involves a wide range of activities that both precede and follow formal peace accords. Metaphorically, peace is seen not merely as a stage in time or a condition. It is a dynamic social construct.\nOrganization for Economic Cooperation and Development\n[Peacebuilding] includes activities designed to prevent conflict through addressing structural and proximate causes of violence, promoting sustainable peace, delegitimizing violence as a dispute resolution strategy, building capacity within society to peacefully manage disputes, and reducing vulnerability to triggers that may spark violence.\nSchool of Conflict Analysis and Resolution at George Mason University\nPeacebuilding is a term used within the international development community to describe the processes and activities involved in resolving violent conflict and establishing a sustainable peace. It is an overarching concept that includes conflict transformation, restorative justice, trauma healing, reconciliation, development, and leadership, underlain by spirituality and religion. It is similar in meaning to conflict resolution but highlights the difficult reality that the end of a conflict does not automatically lead to peaceful, stable social or economic development. A number of national and international organizations describe their activities in conflict zones as peacebuilding.\nUnited Nations Development Program\nPeacebuilding involves a range of measures targeted to reduce the risk of lapsing or relapsing into conflict by strengthening national capacities at all levels for conflict management, and laying the foundations for sustainable peace and development. Peacebuilding strategies must be coherent and tailored to the specific needs of the country concerned, based on national ownership, and should comprise a carefully prioritized, sequenced, and therefore relatively narrow set of activities aimed at achieving the above objectives. This office works specifically with peacebuilding in the context of conflict prevention.\nUnited Nations: Peacebuilding Support Office\nPeacebuilding is rather the continuum of strategy, processes and activities aimed at sustaining peace over the long-term with a clear focus on reducing chances for the relapse into conflict…. [It] is useful to see peacebuilding as a broader policy framework that strengthens the synergy among the related efforts of conflict prevention, peacemaking, peacekeeping, recovery and development, as part of a collective and sustained effort to build lasting peace. This office works specifically with peacebuilding in the context of postconflict reconstruction.\nUnited States Institute of Peace\nOriginally conceived in the context of postconflict recovery efforts to promote reconciliation and reconstruction, the term peacebuilding has more recently taken on a broader meaning. It may include providing humanitarian relief, protecting human rights, ensuring security, establishing nonviolent modes of resolving conflicts, fostering reconciliation, providing trauma healing services, repatriating refugees and resettling internally displaced persons, supporting broad-based education, and aiding in economic reconstruction. As such, it also includes conflict prevention in the sense of preventing the recurrence of violence, as well as conflict management and postconflict recovery. In a larger sense, peacebuilding involves a transformation toward more manageable, peaceful relationships and governance structures—the long-term process of addressing root causes and effects, reconciling differences, normalizing relations, and building institutions that can manage conflict without resort to violence. The US Government does not have a publicly available definition of peacebuilding, other than the definition provided by USIP.","Meaning Of Conflict Resolution\nConflict resolution is a conceptualize soft skill because of the methods and processes involved in facilitating the peaceful ending of conflict and retribution. Dedicated group members always try their best to solve group conflicts. They do so by actively communicating, sharing information about their conflicting motives to the remainder of the group (e.g., intentions and reasons for holding certain beliefs), and engaging in collective negotiation. Dimensions of resolution typically parallel the size of conflict within the way the conflict is processed.\n1. Cognitive Resolution\nCognitive resolution is the way in which disputants understand analyze and examine the conflict, with beliefs, perspectives, understandings, and attitudes.\n2. Emotional Resolution\nEmotional resolution is within the way disputants feel a couple of conflicts, the emotional energy.\n3. Behavioral Resolution\nBehavioral resolution is reflective of how the disputants act, their behavior.\nUltimately a large range of methods and procedures for addressing conflict exist, including negotiation, mediation, mediation-arbitration, diplomacy, and artistic peacebuilding.\nThe term conflict resolution can be used interchangeably with the term ‘dispute resolution’, where arbitration and litigation processes are critically involved. The concept of conflict resolution is often thought to encompass the employment of nonviolent measures by conflicted parties in a trial to push the effective resolution.\nTheories And Model Of Conflict Resolution\nThere is a plethora of various theories and models that link to the concept of conflict resolution.\n1. Conflict Resolution Curve\nThere are many samples of conflict resolution in history, and there has been a debate about the ways to conflict resolution whether it should be forced or peaceful. Conflict resolution by peaceful means usually appears to be a higher option. The conflict resolution curve derived from an analytical model that provides a calm solution by motivating and appreciating conflicting entities. Forced resolution of conflict might invoke another conflict within the future.\nThe conflict resolution curve (CRC) separates conflict styles into two separate domains: the domain of competing entities and the domain of accommodating entities. There’s a kind of agreement between targets and aggressors on this curve. Their judgments of badness compared to goodness of every other are analogous on CRC. So, the arrival of all conflicting entities to some negotiable points on CRC is vital before peacebuilding. CRC doesn’t exist (i.e., singular) essentially if the prospect of aggression if the aggressor is definite. Under such circumstances, it would cause apocalypse with mutual destruction.\nThe curve explains why nonviolent struggles ultimately toppled repressive regimes from power and sometimes forced leaders to alter the character of governance. Also, this technique has been applied to capture the conflict styles on the Korean Peninsula and the dynamics of negotiation processes.\n2. Dual Concern Model\nAccording to the dual concern model, group members balance their concern for satisfying personal needs and interests with their concern for satisfying the wants and interests of others in numerous ways. The intersection of those two dimensions ultimately leads individuals to exhibit different types of conflict resolution. The twin model identifies five with number four being the target to finish the cycle and illuminate the difficulty at hand. Conflict resolution styles or strategies that individuals may use depend upon their dispositions toward pro-self or pro-social goals.\n3. Avoidance Conflict Style\nCharacterized by joking, changing or avoiding the subject, or maybe denying that an issue exists, a strong dislike for following the foundations the conflict avoidance style is employed when a person has withdrawn in addressing the opposite party, when one is uncomfortable with conflict, or because of cultural contexts. During the conflict, these avoiders adopt a “wait and see” attitude, often allowing conflict to end on its own with none personal involvement. By neglecting to deal with high-conflict situations, avoiders risk allowing problems to fester or spin of control.\n4. Yielding Conflict Style\nIn contrast, yielding, “obliging”, smoothing or suppression conflict styles are enhance by a high level of concern for others and an occasional level of concern for oneself. This pro-social passive approach comes forth when individuals derive personal satisfaction and happiness from meeting the requirements of others and have a general concern for maintaining stable, positive social relationships. When faced with conflict, a person with a yielding conflict style tends to harmonize into others’ demands out of respect for the social relationship.\n5. Competitive Conflict Style\nThe competitive, “fighting” or pressurizing conflict way or style increases to maximum individual assertiveness (i.e., concern for self) and minimizes empathy (i.e., concern for others). Groups consisting of competitive members usually enjoy seeking domination over others. Fighters tend to force others to easily accept their personal views by employing competitive power tactics (arguments, insults, accusations, or maybe violence) that foster intimidation.\n6. Conciliation Conflict Style\nThe conciliation conflict style is typical of people who possess an intermediate level of concern for both personal and others’ outcomes. Compromisers value business and, in doing so, anticipate agreed give-and-take interactions. This conflict style is often consider as an extension of both “yielding” and “cooperative” strategies.\n7. Cooperation Conflict Style\nCharacterized by an energetic concern for both pro-social and pro-self behaviour, the cooperation, integration, confrontation, or problem-solving conflict style is usually used when a private has elevated interests within their own outcomes moreover as in the outcomes of others. During the conflict, cooperators collaborate with others in an attempt to seek out an amicable solution that satisfies all parties involved in the conflict. Individuals using this sort of conflict style tend to be both highly assertive and highly empathetic. By seeing conflict as an ingenious opportunity, collaborators willingly invest time and resources into finding a “win-win” solution. In line with the literature on conflict resolution, a cooperative conflict resolution style is usually recommended specifically others. This resolution could also be achieved by lowering the aggressor’s guard while raising the ego.\n8. Relational Dialectics Theory (RDT)\nRDT, introduced by Leslie Baxter and Barbara Matgomery (1988), explores the ways within which people in relationships use verbal communication to manage conflict and contradiction as critical psychology. This idea focuses on maintaining a relationship even through contradictions that arise and the way relationships are managed through coordinated talk. RDT assumes that relationships are composed of opposing tendencies, are constantly changing, and tensions arise from intimate relationships.\nThe concept is that the contrary has the characteristics of its opposite. People can seek to be during a relationship but still need their space.\nThe totality comes when the opposites unite. Thus, the link is balanced with contradictions and only then it reaches totality\nComprehended through various social processes. These processes simultaneously continue within a relationship in an exceedingly recurring manner.\nProcess Of Conflict Resolution\nThe resolution of conflicts in the workplace typically involves some of the following processes:-\n- Recognition by the parties involved that an issue exists.\n- Mutual agreement to handle the problem and find some resolution.\n- An effort to grasp the angle and concerns of the opposing individual or group.\n- Identifying changes in attitude, behavior, and approaches to figure by each side that may lessen negative feelings.\n- Recognizing triggers to episodes of conflict.\n- Interventions by third parties like Human Resources representatives or higher-level managers to mediate.\n- A willingness by one or both parties to compromise.\n- Agreement on a thought to deal with differences.\n- Monitoring the impact of any agreements for change.\n- Disciplining or terminating employees who resist efforts to defuse conflicts\nConflict Resolution In The Workplace\nAccording to the Cambridge dictionary, an awfully basic definition of CONFLICT is full of life disagreement between people with opposing opinions or principles. Conflicts like disagreements may occur at any moment, being a standard a part of human interactions.\nThe sort of conflict and its severity may vary both in content and degree of seriousness; however, it’s impossible to completely avoid it. Actually, conflict in itself isn’t necessarily a negative thing. When handled constructively it can help people to face up for themselves to evolve and find out how to figure together to realize a mutually satisfactory solution. But if the conflict is handled poorly it can cause anger, hurt, divisiveness, and more serious problems.\nConflict can be avoided?\nIf it’s impossible to completely avoid conflict because it was said, the probabilities to experience it are usually higher particularly in complex social contexts within which important diversities are at stake. Especially due to this reason, speaking about conflict resolution becomes fundamental in ethnically diverse and multicultural work environments, within which not only “regular” work disagreements may occur but within which also different languages, worldviews, lifestyles, and ultimately value differences may diverge.\nConflict Resolution is a process by which two or more parties engaged during a disagreement and reach an agreement solving it. It involves a series of stages, involved actors, models, and approaches that will rely upon the sort of confrontation at stake and therefore the surrounded social and cultural context. However, there are some general actions and private skills that will be very useful when facing a conflict to unravel (independently of its nature)\nExample: an open-minded orientation able to analyze the various point of view and perspectives involved, in addition to the capability to empathize, carefully listen, and clearly communicate with all the parts involved.\nSources of Conflict in Workplace\nSources of conflict are also numerous, counting on the actual situation and also the specific context, but a number of the foremost common include:\nPersonal differences like values, ethics, personalities, age, education, gender, socioeconomic status, cultural background, temperament, health, religion, dogmas, etc. Thus, any social category that serves different people may become an object of conflict when it does negatively diverge with people that don’t share clashes of ideas, choices, or actions.\nConflict occurs when people don’t share common goals or common ways to achieve a specific objective (e.g. different work styles). Conflict occurs also when there’s direct or indirect competition between people or when someone may feel excluded from a specific activity or by some people within the corporate. Lack of communication or poor communication is a significant reason to start out a conflict, to misunderstand a specific situation, and to form potentially explosive interactions.\nFrequently Asked Questions-\nAns. a) Avoiding: Someone who uses a strategy of “avoiding” mostly tries to ignore or sidestep the conflict, hoping it will resolve itself or dissipate.\nAns. Having good conflict resolution skills help employees to reduce the friction that may damage their working relationships. By resolving conflicts in a very more professional and respectful manner, they’re enabled to collaborate better with co-workers, building stronger work relationships. Reduced disruptions.\nAns. a) Communication: Employees must understand how to speak to every other so as to grasp the source of conflict, additionally as alternate points of view but communicating well involves over talking and getting your point across to others.\nc) Stress Management\nd) Emotional Agility\nAns. There are two forms of conflict in literature:\nInternal conflict is within the character’s mind. Internal conflict is described as a struggle between opposing forces of desire or emotions within someone.\nExternal may be a conflict between a personality and an out of doors forces.\nAns. a) Poor communication: Communication relies on clear and complete messages being sent moreover as being received.\nc) Lack of designing\nd) Poor staff selection\ne) Frustration or stress"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:7a473c9a-2a9f-45a1-8356-9071b7f65836>","<urn:uuid:f46a958b-caa2-4889-b83f-86b15542b574>"],"error":null}
{"question":"How do the architectural placements of Toledo Cathedral's Emperor's Organ and SsAM's Katherine Esterly Organ differ in terms of their impact on their respective buildings' designs?? 🤔","answer":"The Emperor's Organ in Toledo Cathedral is located in the south arm of the transept on the inner side of the Lions Gate, sharing its plateresque style and functioning like a stone altarpiece. Meanwhile, the Katherine Esterly Organ at SsAM required adding a new chamber in the front of the church behind the choir on the pulpit side, which created architectural balance with matching pipe facades on both sides of the room. Additionally, SsAM's organ includes pipework around the central round window in the rear gallery, while maintaining a similar layout to the original design.","context":["Toledo Cathedral has a dozen or so organs, if I don’t leave any. They are the following: the General, the Echevarría, Verdalonga, the Mozarabic Chapel, the two of the Chapel of the New Kings, the Chapel of the Virgin of the Alcázar, the Chapel of the Virgin of the Tabernacle, the Chapel of St. Peter and the Organ of the Emperor. An article by Francisco Javier Martín Fernández.\nInterior of the Lions Gate, Organ of the Emperor and Gothic rose window. Toledo Cathedral. Photograph corresponding to an old postcard from 1960 made by Luis Arribas and belonging to the Municipal Archive of Toledo.\nThis organ, which is the one we are dealing with today, is undoubtedly the most spectacular of all, as well as the most important. It is located in the south arm of the transept, on the inner side of the Lions Gate, with which it shares not only a wall but also a plateresque style, like a stone and imposing altarpiece.\nJust to the right, as seen from inside the Cathedral, is the enormous painting of San Cristobal (popularly known as ” San Cristobalón” ), decorating a wall behind which is the musical archive of the Cathedral. Precisely between San Cristóbal and the interior of the Puerta de los Leones there is a beautiful little door that leads to the Emperor’s Organ.\nThe interior of the Lions Gate is designed in plateresque style, although there are still clearly Gothic elements in its lower body. It was the work of Alonso de Covarrubias, who made a double door divided by a mullion and crowned with a historic tympanum, whose motif is the tree of Jesse. Both the resurrected one that appears in the mullion and the tympanum are previous works to the rest, having been created in Gothic style by Hanequín de Bruselas in 1460.\nSure you’re also interested: 21 monuments and must-see places to visit in Toledo (Updated 2019)\nThe leaves of the doors (and their boards) were carved in walnut wood by Miguel Copin (son of Diego Copín), with the collaboration of Diego Velasco and Anas (1541).\nIn the niche on the left there is a small mausoleum in which lies Canon Alonso de Rojas, who died in 1577, while the niche on the right remains empty.\nAbove the door is a second body with a large medallion representing the Coronation of the Virgin, the work of Gregorio de Borgoña, framed between the statues of David and Solomon, the work of Miguel Copín.\nIn the upper body we finally find the Emperor’s Organ, which is the oldest of the organs existing today in the Cathedral and was built by the master Gonzalo Hernandez de Cordoba (although it was finished by the Toledo Juan Gaytan) between the years 1543 and 1549.\nA beautiful gothic rose window completes the ensemble.\nThe Emperor’s Organ was repaired in later centuries, and several registers were added to it, although, except for those modifications, the organ as we know it today hardly differs from that created in the time of Charles V.\nAlberto Merklin described the organ in detail in his book ” Organology” written in 1924. According to Merklin, it consists of ” two 45-key manual keyboards, with short octave, and 13 double keys on the bottom bracket (natural tones with all sharps).\nThis idea of double keys is an invaluable resource, as pressing one half of the key only sounds the tongue registers of the bottom bracket that are removed. When you step on the other part of the key, the flutes sound, and of course, when you step on both keys, the feet sound tongue and flutes. This was at that time a transcendental innovation for organic music, being able to emphasize or not that ” cantus firmus” without opening each time the registers. The ” Piano automatic pedal” is what in the modern organ replaces this invention of the double keys in the bottom bracket” .\nI’m sure you’re also interested in: Godulfo (The Chasuble of a Saint)\nLeaves, mullion and tympanum on the inside of the Lions Gate of Toledo Cathedral. Photograph of Casiano Alguacil belonging to the photographic collection of this author property of the Municipal Archive of Toledo.\nMerklin deepens in his book in the detailed description of the organ, which he considers very original, in the following way:\n” The registers, as in all the old Spanish organs, are split, i.e. treble and bass, right and left. (For ease of reference we quote the complete records):\nBottom keyboard: one flute of 26 (16 feet), one flatuado of 13 (8 feet), one violon of 26 (16 feet) and octave (4 feet).\nExterior tongue of the keyboard below (added in the 18th century): royal trumpet (8 feet), field clarinet (4 feet), clear clarinet (4 feet), shiny clarinet (2 and 4 feet), and magna trumpet (16-foot treble).\nKeyboard above: flute of 13, flute of 26, violon of 13, octave, covered octave, transverse flute (tiples), fortnight (2 feet), dozen and fortnight (2 2/3 and 2 feet), nasardos of 8 per point, nasardos of 5 per point, full of 8 per point and corneta magna.\nTop keyboard trumpet: royal trumpet, trumpet magna (tiples), bassoon and clarinet (4 and 8 feet) and violet (bass).\nCons: sound over the first half of the bottom bracket key: 52 (32 feet) cons, 26 (26) cons on the façade, 13 (13) cons and octave (4) cons. They sound on the second half of the bottom bracket: cons in fortnight (2 feet), cons in 22ª (1 foot), cons in bombardas (16 feet), cons in clarines and clarines in fortnight (2 feet).\nThis organ carried neither couplings nor expressive box nor tremolo.”\n<font color=#38B0DE>-=www.catedralesgoticas.es=- Proudly Presents\n” Evolution of the Spanish organ: XVI-XVIII centuries” , by Louis Jambou. 1988.\n” Organology,” by Alberto Merklin. 1924.\n<font color=#38B0DE>-=http://www.jdiezarnal.com/catedraldetoledo.html=- Proudly Presents\n(Note: none of the images illustrating this article belong to the collection of Francisco Javier Martín Fernández)\nPublished by permission of its author: Francisco Javier Martín Fernández (see original post)","INSTALLATION OF THE\nKATHERINE ESTERLY ORGAN\nThe installation of the Katherine Esterly Organ began on Monday, March 7, 2016.\nWHY A NEW ORGAN?\nAs many have experienced, the music at the Episcopal Church of Saints Andrew and Matthew, during the services and at special events, is well known to lift the spirits and nourish the souls of those in attendance. However, the organ reached the end of its useful life cycle several years ago; in its place currently is a temporary electronic organ.\nAfter a lengthy in-depth process of research and evaluation, the Organ Committee at SsAM developed a plan for rebuilding and improving this 60-year-old organ. Quimby Pipe Organs, Inc., specializing in organ designs closest to the unique musical needs of parishes, was selected. They were also chosen because they best understood SsAM’s philosophy as well as our desire to incorporate pipes from the three former parishes that have consolidated into one… St. Andrew’s, St. Matthew’s and the Cathedral Church of Saint John’s.\nThis renovation has improved the organ’s sound and will maintain the instrument for years to come. An enhanced organ honors the excellence of our choir and looks forward to the blending of young voices of the Cathedral Choir School of Delaware.\nBecause of SsAM’s location in downtown Wilmington and its commitment to serving as a spiritual, cultural and social force in the community, the new organ will have an impact on music and culture far beyond the context of worship. Being particularly well designed as a bold concert instrument, it will also support the many groups that perform regularly at SsAM as well as attract additional musical groups.\nThe new organ has been named after Dr. Katherine (Kitty) L. Esterly, a revered and honored pediatrician, a beloved and respected member of our congregation and of our state. She has been a major proponent of the new organ, understanding its importance to the congregation and the community. The organ was dedicated on February 19, 2017.\nWAS THIS A RENOVATION OR A NEW ORGAN?\nThe SsAM organ project is a fairly radical renovation project that utilizes some pipe work from the organs of old St. Matthews and from old St. Andrews, which were both built by the M. P. Moller Co. in the mid-1940’s. Essentially, a new organ is being built that meets all design objectives while incorporating historic elements of the two instruments: approximately 8 ranks of pipes from the St. Andrews organ and 4 ranks from the St. Matthews organ.\nThere are a number of challenges in building an ideal organ for SsAM. Aside from the issue of finding the right design philosophy that can produce an instrument suitable for the wide range of musical styles suggested by the SsAM liturgies, the limited space requires that this be done with a limited number of pipes. With the current organ at 34 ranks, a new instrument housed in the same confined space would have difficulty providing the tonal and musical options that our parish needs. Although many congregations are now effectively solving this issue with digital instruments or by adding digital divisions, the Organ Task Force, as well as the SsAM Vestry unanimously agreed that our preference is for a custom built pipe organ. Another challenge was to find a builder who could embrace our unique musical customs and create a quality instrument that suits our space, musical requirements and budget limitations.\nUNIQUE DESIGN CONCEPT\nConsidering the space limitations as well as challenges with regard to design strategy, some creative solutions were needed to fill such a tall order. Many excellent designs were submitted by some of the leading organ builders, but Quimby Pipe Organs, Inc. of Warrensburg, Missouri was chosen as the builder. Michael Quimby and his outstanding staff in collaboration with SsAM’s music director David Christopher have developed what we believe to be the perfect instrument for the church. With the addition of a new organ chamber, increasing the size of the new instrument to 45 stops and using the most modern technology, David and Michael have devised a visionary instrument that we believe will meet all of the parish’s musical and liturgical needs. The design strategy has at it’s core a classic instrument with well developed choruses of sound. However, the addition of a few orchestral stops as well as placing the three main divisions under expression (Great, Swell and Choir) will expand the musical options beyond the scope of most traditional mid-size organs. The new instrument with be particularly strong at supporting congregational singing, accompanying a wide range of choral repertoire, facilitating inspiring improvisations within the context of liturgy, providing continuo for early music, and serving as a dynamic and inspiring recital instrument. In other words, the new SsAM organ will “step out of the box” much the same way that our parish in known for stepping out of the box spiritually, socially and liturgically.\nClick here to see a stop list for the new instrument. TONAL SPECIFICATIONS\nClick here to learn more about QUIMBY PIPE ORGANS, INC.\nIMPORTANT SYMBOLISM OF THIS PROJECT\nPerhaps the most important aspect of the new organ is that it serves as a symbol and critical milestone in the evolution of the parish. By taking the strongest elements from the organs of the two historic parishes of SsAM (St. Matthews and St. Andrews) and weaving them into a new and visionary entity, we give credibility and a sense of permanency to the successful union of these two churches. The project is enhanced with the addition of pipes from the Cathedral Church of St. John’s. Every time the new instrument is used in worship or in concert, our hope is that it will serve as a reminder that God’s will can be realized against all odds. In our case, this means defying all of the statistics that surround the viability of creating a parish such as SsAM; one that is radically welcoming, yet intentionally diverse. It also means finding a new way to stay faithful to the principles behind traditional values, but in a way that allows the Holy Spirit to help us evolve into a community that is alive and relevant to the world today.\nThe most significant change to the architecture of the sanctuary was the addition of a new chamber in the front of the church directly behind the where the choir sits on the pulpit side of the nave. As a result, the front of the church has a stronger sense of balance with matching pipe facades on both side of the room. According to SsAM’s architect, Lee Sparks (Design Collaborative, Inc.), the addition of the second organ chamber is not only an inevitable improvement because of its beauty coupled with functionality, but it completes the architectural balance of the room in a way that stays true to the spirit of original (historic) architectural concept. In the rear gallery of the church, attractive pipe work around the central round window replaces the former facade, but the basic layout (of the antiphonal organ) is very similar to the original design."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:1b01650e-8077-4fea-90b1-db86a995fc6e>","<urn:uuid:5c54a424-59d7-4267-91fc-7c84ff839e58>"],"error":null}
{"question":"I'm curious about different cooking vessels that use wood-based fuel sources. Could you explain the difference between how La Chamba Brassier and Shokunin Kamado Grill utilize wood materials in their cooking processes?","answer":"The La Chamba Brassier and Shokunin Kamado Grill use wood-based materials differently. The Brassier traditionally uses hot ambers from a fire placed inside it to cook or keep food warm, requiring careful ventilation and safety precautions due to the burning embers. The Shokunin Kamado Grill, on the other hand, uses hardwood lump charcoal as its base fuel, with wood chunks or small wood splits added on top of the charcoal to boost heat and add subtle wood flavor to the food. In the Kamado grill, the fire is positioned below the food, with the wood smoke contributing to the flavor, unlike traditional wood-fired ovens where the fire is adjacent to the food.","context":["Black Clay, La Chamba Brassier\nAbout the piece\nHow they are Made\nIf you want to cook or serve this over open fire, no better way than to do it on our Black Clay, La Chamba Brassier. It certainly can make an impressive presentation on any buffet table. The burner fits most Black Clay, La Chamba Pots we sell.\n|How we measure|\nAbout Black Clay, La Chamba, Cookware\nBlack Clay, La Chamba Cookware, is well-known and used throughout Colombia in restaurants and homes for preparing and serving traditional dishes such as Ajiaco. Its origins can be traced back at least 700 years to vases and pitchers found in pre-Columbian archaeological sites. It is still made in the traditional manner, by families in a small village on the banks of the Magdalena River in Central Colombia.\nAs with most handmade products by artisans around the world, the pieces may have slight imperfections in the form or finish of the materials. Sometimes sizes of same model pieces may also vary slightly, however, these imperfection do not compromise the aesthetics or functionality of the pieces and are considered normal and to be expected. Likewise, although lids are made individually to match each pot, they do not always fit perfectly as the two parts may shrink differently during the firing process.\nYou may observe with certain use that your pieces may change in color, especially on the bottom where there is direct contact with heat. This is normal and does not affect the pieces in any way.\nThe Black Clay, La Chamba Brassier is very easy to use and take care of.\nIn Colombia, the most common way to use the Brassier is to place hot ambers from a fire inside of the Brassier to cook or keep its content warm.\nBe careful when using hot coals! It is not recommended to use indoors as the burning ambers (especially charcoal) can be dangerous. When using the Brassier with hot ambers, make sure you are in a well ventilated area; do not use coals in enclosed areas. If outdoors, avoid placing the Brassier where the wind can spread the ashes to flammable materials. Do not put a hot Brassier directly on top of or around any highly-inflammable surfaces. For your safety, use protective mitts when handling the Brassier.\nTo keep the contents of a pot warm or for service in a buffet table, the most common use is to place canned heat such as Sterno strategically placed inside the Brassier. A 2.6 oz can has the right dimensions for the job (2 1/2\" diameter). Of course, candles also work fine, especially if you want to have an authentic and unique presentation, perfect for any occasion.\nYou should wash and dry the Brassier before first use to remove any dust which could have gathered during shipping.\nAfter using, allow the Brassier to cool before washing as rapid swings in temperature may crack the clay. Simple remove any ash if using ambers and wash the inside before storage. If using candles of canned heat, simply remove the candles or can, and clean out any fuel or wax which was deposited inside the Brassier.\nIn a small village on the banks of the Magdalena River in central Colombia, live entire families of talented artisans that make Black Clay, La Chamba Pottery. Like their ancestors have done for 700 years. The process of making Black Clay, La Chamba Pottery has remained virtually unchanged since before the Spanish conquest. Still today, there are no mechanical wheels and each piece is made entirely by hand. It is a laborious process of molding, polishing, and firing.\nThe entire community is dedicated to the production of these ceramics and it is how the community sustains itself. Rather than one artisan completing an entire piece, families or individuals specialize in certain steps of the process, which make the creation of each piece an entire community affair. After each step is completed, the pieces are then transported by burros, motor bikes or even on top of their heads to other families or individuals who complete other steps and so on until the pieces are completely done and packaged.\nThree different types of clays from local mines are used in the making of these ceramics. They are dissolved in water and strained to remove little stones and other impurities. The interior of the ceramics are made with a mixture of a smooth, grey clay that gives the piece its strength and body, and a sandy grey clay that makes it easier to mold. The exterior is then “glazed” with a watery red clay which gives the piece a smooth surface, and allows for the polishing.\nThe process begins by flattening a ball of clay and forming it around a mold specifically made for each model. This “shell” is then smoothed and trimmed using tools fashioned from bamboo and plastic pieces. The pieces are then left to air dry, receiving a light polishing during the drying to make sure they have a smooth surface and free of sand. The pieces are then transported to other artisans that specialize in adding rims and/or handles. The handing off progression continues with other artisans which specialize in the production of the lids, which are custom made for each piece. Even the knobs on the lids are made by artisans who specialize in only adding knobs to lids.\nAfter each step the pieces are laid out to dry in the scorching La Chamba sun. Sometimes the process is delayed due to rains which prevent the drying of the pieces. When it begins to rain, the entire community is mobilized to quickly cover drying pieces that were outside in various production stages. The pieces are then lightly polished with a soft sponge to ensure a smooth exterior, and they are then taken to artisans that glaze each piece with the watery red clay. The glaze is not let to dry completely to allow for the next step of polishing, which is probably the most laborious part of the process, the polishing.\nThe hand polish, a process known as burnishing, is done by rubbing the entire piece with semiprecious stones until it has a uniform brightness on the surface. Depending on the size of a piece, the polishing can take up to half an hour to an hour. Originally, this polishing was done with smooth river rocks from the bottom of the Magdalena River, but is now done with agate stones brought from Brazil which are stronger. With use, the stones grind down even creating grooves from corners and edges.\nAfter the pieces are polished and fully dry, they are taken to be fired in conically shaped, wood burning ovens, although some artisans now use custom made gas kilns which make the process somewhat easier and faster.\nTo start the firing process, the pieces are placed inside metal drums, with smaller objects first going inside larger earthenware pieces. These metal drums are then placed in wood burning ovens, usually late in the day so as to minimize the heat to the workers. The firing process takes approximately three hours, with the furnace reaching approximately 750°C. This high heat, together with the glazing and burnishing process, result in utilitarian pieces strong enough to withstand rough treatments and make them ovenproof.\nAfter the firing comes probably the most interesting part of the entire process and is what gives the pieces their unique black color. The metal drums are pulled out of the ovens with the help of long sticks.\nProcessed vegetable materials in the form of saw dust, rice husks, or other materials are then added to the metal drums with the white hot clay pieces still inside. The materials immediately catch fire and begin to smoke. The drums are then covered to extinguish the flames but keep the material smoking vigorously for quite some time. The smoke penetrates the pieces giving each piece the black color that so characterizes products from La Chamba.\nThe entire village and artisans are extremely proud of their products, as we are bringing them to you.","Making Pizzas in the Shokunin Kamado Grill\nThe Shokunin Kamado Grill is incredibly versatile. Use it to smoke, roast, bake and grill with a flavorful charcoal and wood fire. One of our favorite things to cook in the Shokunin is pizza. Because of the amazing efficiency of the Shokunin’s insulated design, it is easy to maintain a 750°F baking temperature for hours at a time. At this temperature, you can make a perfect pizza in just three minutes.\nThis simple guide to baking pizzas in Kalamazoo’s kamado grill will walk you through the tools, steps and techniques needed for a successful artisan pizza night at home.\nThis two-piece baking deck creates an air gap for more gentle heat.\nTHE TOOLS YOU’LL NEED\nThe most important tool you’ll need is a hollow-core baking deck or pizza stone. The cordierite two-piece deck from PizzaCraft is ideal for balancing the heat and preventing the bottom of the pizza from cooking too quickly. Make sure your pizza stone is no more than 16 inches in diameter.\nIf your pizza stone is solid, without an air gap, you can use an aluminum pizza screen between the pizza and the stone to slow down the cooking.\nA perforated pizza peel, like the one included in Kalamazoo’s Pizza Master’s Essentials Kit (PMEK), is very helpful in transferring the assembled pizza into the grill. A wooden pizza peel, or even a thin cutting board can also be used. The PMEK also includes a pair of dough boxes that are helpful when resting the dough prior to shaping the crust.\nOnce the pizza has cooked, you can lift it out of the grill with a large grill turner or spatula, or you can use a pizza peel.\nPizza cooking with a pizza screen on top of the stone.\nA chimney starter gets the charcoal started quickly with no chemicals.\nSETTING UP THE GRILL\nConfigure your grill with the fire grate placed in the highest of the three positions. Because of the tapered interior shape of the Shokunin, you’ll want to slide each part of the two-piece adjustable fire grate to the outside (far left and far right) of the grill. Pile a little hardwood lump charcoal in each of the four corners of the grill.\nOpen all four vents (two supply vents in the base of the grill and two exhaust vents in the lid) to the wide-open positions.\nStarting the Fire\nFill the chimney starter with more lump charcoal. We typically use a paper grocery sack to start the chimney, but you can also use paraffin fire starters. To use paper, loosely wad it up and put it inside the bottom of the chimney starter. Place the chimney starter in the center of the fire grate, then use the plumber’s torch to light the paper on fire. Leave the grill lid open and the cooking grates flipped up out of the way while the charcoal starts, about 15 minutes.\nOnce the fire has travelled all the way up the chimney and the coals at the top are burning, it is time to distribute them in the grill. Use long, protective gloves to pour the burning coals out of the chimney starter and onto the unlit charcoal that was positioned earlier. You want to keep all of the charcoal positioned toward the corners of the fire grate so that the heat envelopes the pizza and reflects down from the inner curvature of the lid.\nOnce the charcoal is distributed, you can add wood chunks or small wood splits on top of the charcoal. The wood will boost the heat and add subtle wood flavor to the pizza. (Unlike a wood-fired pizza oven, where the fire is next to the pizza and the smoke travels out the chimney, the fire in the Shokunin is below the pizza so the flavor is noticeable.)\nNow you can lower the cooking grates into position and place the pizza stone in the center of the cooking grate.\nClose the lid and preheat the grill to 750°F, about an hour of preheating time.\nCOOKING THE PIZZA\nAssemble your pizza on the countertop, then transfer to the pizza stone using a pizza peel (see tips that follow). Close the lid and start a three-minute timer. The pizza will cook evenly from all sides and does not need to be rotated in the Shokunin. With a hollow-core pizza stone, the pizza will cook untouched for three minutes.\nWith a solid stone, check the pizza after one minute. If the crust has “set” and the pizza is easily moved, gently lift it off the stone and slide a pizza screen into position. Close the lid and continue cooking for the remaining two minutes.\nRemove the pizza when done and let rest for a minute before slicing.\nPIZZA MAKING TIPS\nBecause we are cooking pizzas quickly at a high temperature, use a pizza dough made in whole or in part with Tipo ‘00 flour. This is the flour used in traditional Neapolitan pizzas. Our Artisan Fire Pizza Dough recipe is perfect for cooking in the Shokunin.\nPizza dough needs to relax before stretching, and ideally needs to come up to room temperature. We recommend keeping dough balls in the dough box at room temperature for three to four hours before making pizza. A seven to ten ounce dough ball size is good for working with a 14-inch diameter pizza stone.\nWhen forming the pizza, be gentle with the dough. You want to avoid “bruising” it and creating areas that can’t form air pockets inside as the crust bakes. We form our pizzas by hand rather than using a rolling pin. The dough can be formed to a flat disc. There is no need to form it with a raised crust around the outside. The heat of the grill will do that for you.\nThe first two minutes of this pizza making video includes tips for stretching the dough and getting the assembled pizza onto the pizza peel.\nOur Favorite Pizza Recipes\nThese recipes are all written for the Artisan Fire Pizza Oven, but are easily cooked in the Shokunin following the directions above. Browse the full collection of pizza recipes in the recipes section of our website. Below are some of our favorites:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9beb27cf-f00d-499c-96ad-b6c07bba5b3d>","<urn:uuid:f3fbe984-562f-471b-8a09-c5cfc016b6f0>"],"error":null}
{"question":"What are the key characteristics of ocean floor deposits at Site U1394, and how do they compare to modern marine mineral resources being extracted globally?","answer":"At Site U1394, located offshore Montserrat, the ocean floor deposits consist of four main lithologies: hemipelagic mud containing calcareous biogenic fragments, turbiditic sand and mud with volcaniclastic and bioclastic particles, mafic volcaniclastics, and tephra deposits. These deposits date from the Pleistocene period. In contrast, current global marine mineral extraction focuses primarily on different resources. Present-day ocean mineral extraction is limited to salt (at 3% concentration in seawater), magnesium (at 1,000 parts per million), placer deposits of gold, tin, titanium, and diamonds from near-shore sediments, and fresh water. While Site U1394's deposits represent complex volcanic and sedimentary processes, modern commercial marine mining targets more accessible and economically viable resources.","context":["Site U1393 |\nSite U1395 |\nSite U1396 |\nSite U1397 |\nSite U1398 |\nIODP Expedition 340:\nLesser Antilles Volcanism and Landslides\nSite U1394 Summary\nPDF file is available for download.\nBackground and objectives\nIntegrated Ocean Drilling Program (IODP) Site U1394 (CARI-03C, 16°38.43'N; 62°2.29'W, 1115 m below seafloor [mbsl]) is located offshore the island of Montserrat (about 24 km from point Shoerock, the SE tip of Montserrat).\nThe site survey data obtained for Site U1394 revealed the presence of\ndebris avalanche deposits (deposit 2 and underlying deposit 8). The site survey\nseismic data indicate that Site U1394 might penetrate through different units\nof debris avalanche deposit 2 (Lebas et al., 2011, Watt et el., 2012). Our plan\nwas to drill 244 m through volcanic and biogenic sediments with intercalated\nchaotic debris avalanche deposit 2.\nThe objective of Site U1394 is to characterize the processes occurring\nduring debris avalanche emplacement and associated erosional processes. The\nsediments overlying the avalanche deposit will allow us to date this collapse\nevent using δ18O chronostratigraphy.\nWe will be able to determine whether the avalanche occurred as a single event,\nor in a series of closely spaced, separate events.\nBased on a detailed lithological, sedimentological, and textural fabric\nanalysis of the cored debris avalanche material, we will be able to test the\nhypothesis that the debris avalanche incorporated sediments eroded from the sea\nfloor during its emplacement. We hope to reach the bedded units below the\nchaotic debris avalanche unit and thus have access to the textural and\nstructural characteristics of the base of the debris avalanche (Komorowski et\nal., 1991; Glicken, 1991, 1996).\nExisting numerical models of debris avalanche emplacement currently do\nnot sufficiently considerate the interaction of the avalanche with the\nsubstratum and its influence on the mobility of the flow. We will be\nspecifically looking for structural evidence of the development of shear zones\nfrom the base of the avalanche into the overlying plug and fluidization\nstructures (clastic dykes) described in terrestrial avalanche deposits (e.g.\nGlicken, 1996; Voight et al., 2002; Gee et al., 1999; Clavero et al., 2002,\nShea et al., 2008).\nThe vessel arrived at Site\nU1394 (CARI-03C) after a 5.3 nautical mile transit in DP mode from Site U1393\n(CARI-02C). Site U1394 consisted of 2 holes. The original plan called for 2\nholes to a depth of ~244 mbsf. Hole U1394A was advanced 235 m into the\nformation, whereas Hole U1394B was shortened to ~180 mbsf due to challenging coring\nconditions, after ensuring that the lower 60 m at this site were scientifically\nless interesting. Hole U1394B was successfully logged with both the FMS-sonic\nand triple combo logging strings, but an attempt to perform a VSP experiment\nwas canceled when all efforts to deploy the VSI tools through the BHA failed. A\ntotal of 48 cores were retrieved at Site U1394 with an average recovery rate of\n23% (57 m of material) in Hole U1394A and 78% (162 m of material) in Hole\nlithologies cored at Site U1394 are (1) hemipelagic mud, (2) turbiditic sand\nand mud, (3) mafic volcaniclastics, and (4) tephra. (1) The hemipelagic mud\nmainly consists of fine-grained calcareous biogenic fragments and siliciclastic\nsediment. It is often pale yellowish gray to dark gray in color, and varies\nfrom fine silt to fine mud in grain size, being moderately to poorly sorted. (2)\nThe turbiditic sand and mud is characterized by normal graded, massive,\nwell-sorted, mud (for thin deposits with a few centimeters in thickness) to\nvery coarse sand (for thick deposits with meters in thickness) consisting of\nvolcaniclastic and bioclastic particles. Volcaniclasts include fragments of\nfresh andesitic lava and pumice, altered lava, and crystals (feldspar,\namphibole, etc.). Bioclasts include fragments of carbonate materials such as\ncorals and shells. The ratio of volcaniclastic to bioclastic components varies.\n(3) The mafic volcaniclastics consist of basaltic turbidites. (4) The tephra\ndeposits retrieved from this site are composed of fining upward units of\nnormally graded (from pebble to fine sand) basaltic particles and basaltic\nscoria. The fining upward units vary in thickness from 6 to 13 to 30 cm.\nof the main lithologies by X-ray diffraction shows that the pelagic sediment\nintervals predominantly contain calcite, high-Mg calcite, plus or minus\naragonite, together with minor volcanic phases (mostly plagioclase with lesser\northopyroxene and hornblende), and amorphous clay minerals. Samples from the volcaniclastic\nhorizons contain dominantly plagioclase and lesser amounts of orthopyroxene and\nhornblende, and minor sedimentary carbonate. One of the investigated tephra\nlayers contains abundant smectite in addition to the volcanic minerals and\nbackground pelagic phases noted above. Analysis of the CaCO3 content\nof the cored material shows that the CaCO3 content is much higher in\nthe largely pelagic sections (average approximately 60 wt.%) than in the\nlargely volcanic turbidite units, however the presence of 3.5 to 7.5 wt.% CaCO3 in the latter indicates a significant biogenic component and is consistent with the optical inspection of the core.\nOrganic carbon concentrations in the pelagic sections are similar to\nthose expected in this area (average approximately 1 wt.%), and are much lower\nin the turbiditic sections (<0.2 wt.%).\nBased on abrupt\nor gradational changes in the abundance of these lithologies and distinctive\nmarker horizons (tephras and turbidites), 6 main lithostratigraphic units,\ntermed Unit A, B, C, D, E, and F, have been defined. Unit A is further divided\ninto seven subunits (A-1 to A-7). Unit A (~7 m in thickness) basically consists of a series of volcaniclastic turbidites\nwith varying proportions of bioclastic particles separated from each other by\nvariably thick hemipelagic sediments consisting of mud to rarely silty mud. The\nturbidites mainly fine upward from a sandy base into a muddy top. A basaltic\ntephra layer resides at the base of Unit A. Unit B is ~1.5 m thick. The upper\npart of Unit B consists of a fairly thick interval of mainly relatively coarse\ngrained, stacked and amalgamated turbidites ranging in composition from mainly\nbioclastic to volcaniclastic. The turbidites are massive such that planar or\nripple cross-lamination is always absent, suggesting rapid deposition that\nprevented bed-load reworking into laminae. Below this the turbidites are flay\nlying, relatively thin and sometimes mildly deformed. The thick turbidite\ninterval is separated from the thin layer turbidites by an interval of very\ncontorted hemipelagic silty mud with fine laminae. The lower part of Unit B has\nnot been recovered. Unit C (~24 m thick) contains alternating sequences of hemipelagic\nsediment and volcaniclastic turbidites. Individual intervals of the hemipelagic\nsediment can be up to 130 cm long. Unit D (~18 m thick in Hole U1394A, 36 m\nthick in Hole U1394B) is dominated by massive coarse-grained turbiditic\nsandstones composed of variable amounts of volcaniclastic and bioclastic\nmaterial. The turbidites are either graded or ungraded. Chaotically distributed\nclasts (andesitic and biogenic) up to several centimeters in length in some\nungraded sections may suggest en-masse emplacement by debris flow. Unit E contains\nsignificant amounts of hemipelagic mud alternating again with turbidites\nconsisting either mainly of volcaniclastic or bioclastic material or a mixture\nof it. The upper part of Unit E contains relatively thick and coarse-grained\nturbidites; turbidite abundance and thickness is lower in the middle part of\nUnit E. Turbidite thickness seems to increase again near the base of the unit.\nDistributed throughout Unit E there are massive brown ash layers, some of which\nare 20 cm thick, and layers of pumice clasts (up to 5 cm in diameter). Unit F has been only retrieved in core catcher samples at the base of Hole U1394A and\nconsists of coarse-grained andesitic clasts.\ninvestigation of the volcaniclastic and fall out deposits based on thin section\nmicroscopy shows that the recovered clasts of andesitic lava and the pumice\nfragments consist of plagioclase (70%), amphibole (25%), FeTi oxides (4%), and\northopyroxene (1%) phenocrysts. In the case of the andesites,\nthe phenocrysts are contained in a microcrystalline matrix of plagioclase,\noxides, and pyroxene microlites. The groundmass of the investigated pumice\nparticles is significantly different. It consists of glassy material exhibiting\nconsiderable flow lineation. No flow lineation was observed in the lava clasts.\nThe groundmass to phenocrysts ratio is approximately 60:40 and 70:30 for\nandesite and pumice, respectively. The\nmicroscopic investigation of the tephra layers revealed that the uppermost\ntephra layer is basaltic in composition, containing fragments of plagioclase,\nolivine, clinopyroxene, and oxides in descending abundance. The deeper ones are\nof basaltic andesitic or andesitic composition, containing fragments of\nplagioclase, amphibole, orthopyroxene, and FeTi oxides.\nresults of the detailed study of the nannofossil and\nmicrofossil content of the sediments described above are consistent with their\nintensely reworked nature. Calcareous nannofossils and planktic and benthic\nforaminifera of varying abundances and varying levels of preservation have been\nobserved. Large volcanic clasts, coral fragments, abundant reef-dwelling\nbenthic foraminifera as well as highly fragmented foraminifera, are characteristically observed in the majority of the core catcher samples. Biostratigraphic\ndatums derived from both calcareous nannofossils and planktic foraminifera show\nthat Site U1394 contains many levels of reworked sediment from the Early\nPleistocene amidst a background of Late Pleistocene sedimentation.\nobserved nannofossils generally show an extreme poor preservation and their\nabundance is low. The species observed throughout the entire cored sediments\nare Gephyrocapsa oceanica, Gephyrocapsa caribbeanica, Pseudoemiliania lacunosa, Calcidiscus leptoporus, Helicosphaera kamptneri as well as Helicosphaera inversa. Helicosphaera\ninversa is a very important species due to its short range through the\nEarly and Late Pleistocene (Late CN13 to CN14), placing the cored materials\nwithin the Pleistocene. Pleistocence age of the sediments is also indicated by\nthe occurrence of Helicosphaera sellii (CN10 to CN13) in a number of samples. Pseudoemiliania lacunosa, a species indicative of Early Pleistocene sediments, was also abundant in several samples, with particularly large specimens (over 7 µm) in\nsample U1394A-24XCC. Globigerinoides ruber (white) and Globigerinoides\nsacculifer dominate the planktic foraminifera observed at Site U1394. Other\nabundant species include Globigerina falconensis, Globigerinita glutinata, Globigerinoides elongatus, Globorotalia tumida and Neogloboquadrina dutertrei (dextral).\nThe species present in all samples are indicative of warm sub-tropical waters. Three datum species were commonly encountered, Globorotalia\nflexuosa (0.07-0.40 Ma), Globigerinella calida (base of occurrence at 0.22 Ma), and Globorotalia tosaensis (top of occurrence at 0.61 Ma), placing the investigated sediments, in accordance with the nannofossil observations, within the Pleistocene. The presence of the benthic foraminiferal species Osangularia, Globocassidulina, Cibicides,\nand Laticarinina spp. suggests a bathyal paleodepth for site U1394.\nthe ages obtained based on the nanno- and microfossil community observed in the\ncore catcher samples of this site are similar to the ones obtained by measuring\nthe natural remnant magnetization of the cored sediments. Expected inclination\nfor the site is ~31° assuming a geocentric axial dipole (GAD). The data plot close to or steeper than GAD. Inclination steepening is probably related to\nbioturbation. All of the data show normal polarity indicating that the sediment\nhigher than ~210 mbsf at this site was deposited within the Brunhes Chron. However, due to the lack of a continuous record this is only a tentative result.\nphysical properties of the material retrieved at Site U1394 can generally be\ncorrelated to the lithological variations, including composition, grain size\nand lithification observed. Bioclastic and volcaniclastic turbidites as well as\nthick tephra layers can be discerned from background sedimentation (carbonate\nooze) by most of the physical proprieties. In total, at least 12 turbiditic\nunits, including 3 very thick ones, can be identified from the continuous\nphysical property logs. The hemipelagic muds overall show little variations and\nhave a low magnetic susceptibility, low P-wave velocity and relatively high natural\ngamma radiation. In contrast, the physical properties of the turbidites are\nrelatively heterogeneous, with overall high values of magnetic susceptibility\nand P-wave velocity but low natural gamma radiation and density. Single bioclastic\nand volcaniclastic turbiditic units throughout the entire holes can be traced\nby there monotonically up-hole decreasing values in magnetic susceptibility, P-wave\nvelocity, and density, mimicking basically their grading in grain size. A sharp\ndrop in each of these values occurs at the boundaries of the turbiditic units.\nWhere sufficiently thick, ash layers give positive peaks in magnetic susceptibility.\nBetween 10 and 15 mbsf the magnetic susceptibility, density, and natural gamma\nradiation data systematically differs from the \"A\" to the \"B\" hole. This is\nconsistent with the differences in the lithologies retrieved from both holes. Hole U1394A is dominated by dark, dense turbidites with andesitic composition at\nthis interval, whereas lighter-colored pumice-rich deposits were cored in Hole U1394B. The thermal conductivity data obtained at this site range from 0.564 W/mK (at 111.2 m) to 1.07 W/mK (at 197.4 m); the mean is 0.94 W/mK. These values are similar to expected values for high porosity sediments. Measured porosity of\nhemipelagic samples ranges from 48 to 66%. Turbidites display porosity values between\n42 and 60%. For hemipelagic samples, there is a poorly constrained trend with\ndepth ranging from about 61% at the mud line to about 54% at 200 mbsf. Bulk density of the hemipelagic sediments ranges from 1.52 to 1.85 g/cm3, whereas bulk density in the turbiditic sediments ranges from 1.65 to 2.08 g/cm3. Generally, the turbidites display a good, negative linear correlation between porosity and bulk density with depth; no similar correlation is found for hemipelagic\nsediments. Grain density values of the hemipelagic sediments exhibit a narrow\nrange between 2.63 and 2.79 g/cm3, whereas the grain density values\nobtained from the turbidite samples show a slightly larger range between 2.6\nand 2.9 g/cm3. Turbidites consisting of a mixture of bioclastic and\nvolcaniclastic material have grain densities ≤ 2.79 g/cm3.\nsitu measurements of physical properties obtained by the downhole logging\noperations are in general agreement with the physical property data obtained\nfrom the cores. The natural gamma radiation data do not exhibit a clear\ndownhole trend over the measured interval, but generally the fine scale\nvariations in total gamma ray have a higher frequency above 110 mbsf than below\nthis depth. This change appears to be coinciding with a change from stacked\nturbidites into a series of alternating turbidites and hemipelagic sediments.\nThe sonic velocities generally increase downhole ranging from ~1650 to 1900\nm/s. There are distinctive local peaks in compressional velocity (Vp) that\ncoincide with increased resistivity and magnetic susceptibility, corresponding\nmost likely to similar scaled turbiditic units identified in the cores.\naddition to the studies done on the retrieved sediments pore water samples\nobtained from the hemipelagic sediments have also been analyzed. Pore water\nsamples from the turbiditic units have not been obtained, since it is not\npossible to collect meaningful pore water data from such permeable material. Pore water samples were largely taken from intervals dominated by pelagic carbonate. Alkalinity values increased from 3.3 mM in the\nuppermost section to a consistent value of 11.5 mM at roughly the middle of the\nhole, before decreasing to 7.5 mM at the base of the hole. The pH values\nremained relatively constant at 7.4 to 7.5 throughout the hole. Of the major\ncations, calcium decreases from values close to bottom water in the uppermost\nsample to a minimum at roughly the middle of the hole before increasing towards\nthe bottom, whereas magnesium shows a monotonic decrease in concentration with\ndepth, apart from the deepest sample. Neither sodium nor potassium\nconcentrations show clear trends with depth. Sulfate concentrations show a\nsimilar pattern to those of calcium. Chloride concentrations fluctuate within\nthe normal range (540-580 mM) expected for pore waters obtained from squeezing\ncarbonate sediments. Overall,\nthe pore water data are consistent with diagenesis of carbonate-rich sediments\nwith organic carbon concentrations that are typical of an open marine setting.\nThe slight change in pore water concentrations in the deepest sediments reflect\nnon-steady state diagenetic conditions that may be due to water advecting\nthrough the relatively permeable volcanic-rich turbidite that lies at the base\nof the hole.\nClavero, J., Sparks, R. S. J., Huppert, H. E., and Dade, W. B.\n(2002) Geological constraints on the emplacement mechanism of the Parinacota\ndebris avalanche, northern Chile. Bulletin of Volcanology 64, 3-20.\nGee, M.J.R., Masson, D.G., Watts, A.B., and Allen, P.A. (1999) The\nSaharan debris flow: an insight into the mechanics of long runout debris flows,\nSedimentology 46, 317-335.\n(1991) Sedimentary architecture of large volcanic-debris avalanches, in Fisher\nR.V., and Smith G.A., eds., Sedimentation in volcanic settings: SEPM (Society\nfor Sedimentary Geology) Special Publication, 45, 99-106.\nH.X. (1996) Rockslide-debris avalanche of May 18, 1980, Mount St. Helens\nVolcano, Washington. US Geol. Survey. Open File Rept. US Geol Survey, Washington, DC, 98pp.\nKomorowski, J-C, Glicken, H, and Sheridan, M.F. (1991) Secondary electron imagery of micro cracks\nand hackly fracture surfaces in sand-size clasts from the 1980 Mount St. Helens\ndebris-avalanche deposit: Implications for particle-particle interactions.\nGeology 19, 261-264.\nLebas, E., Le Friant, A., Boudon, G., Watt, S., Talling, P., Feuillet, N., Deplus, C., Berndt, C., and Vardy, M. (2011) Multiple widespread landslides during the long-term evolution of a volcanic island: insights from high-resolution seismic data, Montserrat, Lesser Antilles. Geochemistry Geophysics Geosystems 12, 1, doi: 10:1029/2010GC003451.\nShea, T., van Wyk de Vries, B.,\nPilato, M. (2008) Emplacement mechanisms of contrasting debris avalanches at Volca Mombacho (Nicaragua), provided by structural and facies analysis. Bulletin of Volcanology, 70. 899-921.\nVoight, B., Komorowski, J.-C., Norton, G. E., Belousov, A. B., Belousova,\nM., Boudon, G. et al. (2002) The 26 December (Boxing Day) sector collapse and\ndebris avalanche at Soufriere Hills Volcano, Montserrat. In Druitt, T.H. and Kokelaar, B.P. (eds.), The eruption of Soufriere Hills Volcano,\nMontserrat, from 1995 to 1999.Geological Society London Memoir 21, 363-407.\nWatt S.F.L., Talling, P.J., Vardy, M.E., Heller, V., Hühnerbach, V., Urlaub, M., Sarkar, S., Masson, D.G.,\nHenstock, T.J., Minshull, T.A., Paulatto, M., Le Friant, A., Lebas, E., Berndt,\nC., Crutchley, G.J., Karstens, J., Stinton, A.J., Maeno, F. (2012) Contributions\nof volcanic-fank and seafloor-sediment failure offshore Montserrat and their\nimplications for tsunami generation. Earth and Planetary Science Letters 319,","Mineral Resources from the Ocean\nOceans cover 70 percent of Earth's surface, host a vast variety of geological processes responsible for the formation and concentration of mineral resources, and are the ultimate repository of many materials eroded or dissolved from the land surface. Hence, oceans contain vast quantities of materials that presently serve as major resources for humans. Today, direct extraction of resources is limited to salt; magnesium; placer gold, tin, titanium, and diamonds; and fresh water.\nAncient ocean deposits of sediments and evaporites now located on land were originally deposited under marine conditions. These deposits are being exploited on a very large scale and in preference to modern marine resources because of the easier accessibility and lower cost of terrestrial\nPrincipal Mineral Resources\nResources presently extracted from the sea or areas that were formerly in the sea range from common construction materials to high-tech metals to water itself. Chemical analyses have demonstrated that sea water contains about 3.5 percent dissolved solids, with more than sixty chemical elements identified. The limitations on extraction of the dissolved elements as well as the extraction of solid mineral resources are nearly always economic, but may also be affected by geographic location (ownership and transport distance) and hampered by technological constraints (depth of ocean basins).\nThe principal mineral resources presently being extracted and likely to be extracted in the near future are briefly considered here.\nSalt, or sodium chloride, occurs in sea water at a concentration of about 3 percent and hence constitutes more than 80 percent of the dissolved chemical elements in sea water. The quantity available in all the oceans is so enormous that it could supply all human needs for hundreds, perhaps thousands, of years. Although salt is extracted directly from the oceans in many countries by evaporating the water and leaving the residual salts, most of the nearly 200 million metric tons of salt produced annually is mined from large beds of salt. These beds, now deeply buried, were left when waters from ancient oceans evaporated in shallow seas or marginal basins, leaving residual thick beds of salt; the beds were subsequently covered and protected from solution and destruction.\nLike the sodium and chlorine of salt, potassium occurs in vast quantities in sea water, but its average concentration of about 1,300 parts per million (or 0.13 percent) is generally too low to permit direct economic extraction. Potassium salts, however, occur in many thick evaporite sequences along with common salt and is mined from these beds at rates of tens of millions of metric tons per year. The potassium salts were deposited when sea water had been evaporated down to about one-twentieth of its original volume.\nMagnesium, dissolved in sea water at a concentration of about 1,000 parts per million, is the only metal directly extracted from sea water. Presently, approximately 60 percent of the magnesium metal and many of the magnesium salts produced in the United States are extracted from sea water electrolytically. The remaining portion of the magnesium metal and salts is extracted from ancient ocean deposits where the salts precipitated during evaporation or formed during diagenesis . The principal minerals mined for this purpose are magnesite (MgCO 3 ) and dolomite (CaMg[CO 3 ] 2 ).\nSand and Gravel.\nThe ocean basins constitute the ultimate depositional site of sediments eroded from the land, and beaches represent the largest residual deposits of sand. Although beaches and near-shore sediments are locally extracted for use in construction, they are generally considered too valuable as recreational areas to permit removal for construction purposes. Nevertheless, older beach sand deposits are abundant on the continents, especially the coastal plains, where they are extensively mined for construction materials, glass manufacture, and preparation of silicon metal. Gravel deposits generally are more heterogeneous but occur in the same manner, and are processed extensively for building materials.\nLimestone and Gypsum.\nLimestones (rocks composed of calcium carbonate) are forming extensively in the tropical to semitropical oceans of the world today as the result of precipitation by biological organisms ranging from mollusks to corals and plants. There is little exploitation of the modern limestones as they are forming in the oceans. However, the continents and tropical islands contain vast sequences of limestones that are extensively mined; these limestones commonly are interspersed with dolomites that formed through diagenetic alteration of limestone. Much of the limestone is used directly in cut or crushed form, but much is also calcined (cooked) to be converted into cement used for construction purposes. Gypsum (calcium sulfate hydrate) forms during evaporation of sea water and thus may occur with evaporite salts and/or with limestones. The gypsum deposits are mined and generally converted into plaster of paris and used for construction.\nThe deep ocean floor contains extremely large quantities of nodules ranging from centimeters to decimeters in diameter (that is, from less than an inch to several inches). Although commonly called manganese nodules, they generally contain more iron than manganese, but do constitute the largest known resource of manganese.\nDespite the abundance and the wealth of metals contained in manganese nodules (iron, manganese, copper, cobalt, and nickel), no economic way has yet been developed to harvest these resources from the deep ocean floor. Consequently, these rich deposits remain as potential resources for the future. Terrestrial deposits of manganese are still relied on to meet human needs.\nComplex organic and inorganic processes constantly precipitate phosphate-rich crusts and granules in shallow marine environments. These are the analogs (comparative equivalents) of the onshore deposits being mined in several parts of the world, and represent future potential reserves if land-based deposits become exhausted.\nMetal Deposits Associated with Volcanism and Seafloor Vents.\nSubmarine investigations of oceanic rift zones have revealed that rich deposits of zinc and copper, with associated lead, silver, and gold, are forming at the sites of hot hydrothermal emanations commonly called black smokers. These metal-rich deposits, ranging from chimneyto pancake-like, form where deeply circulating sea water has dissolved metals from the underlying rocks and issue out onto the cold seafloor along major fractures. The deposits forming today are not being mined because of their remote locations, but many analogous ancient deposits are being mined throughout the world.\nPlacer Gold, Tin, Titanium, and Diamonds.\nPlacer deposits are accumulations of resistant and insoluble minerals that have been eroded from their original locations of formation and deposited along river courses or at the ocean margins. The most important of these deposits contain gold, tin, titanium, and diamonds.\nToday, much of the world's tin and many of the gem diamonds are recovered by dredging near-shore ocean sediments for minerals that were carried into the sea by rivers. Gold has been recovered in the past from such deposits, most notably in Nome, Alaska. Large quantities of placer titanium minerals occur in beach and near-shore sediments, but mining today is confined generally to the beaches or onshore deposits because of the higher costs and environmental constraints of marine mining.\nThe world's oceans, with a total volume of more than 500 million cubic kilometers, hold more than 97 percent of all the water on Earth. However, the 3.5-percent salt content of this water makes it unusable for most human needs.\nThe extraction of fresh water from ocean water has been carried out for many years, but provides only a very small portion of the water used, and remains quite expensive relative to land-based water resources. Technological advances, especially in reverse osmosis , continue to increase the efficiency of fresh-water extraction. However, geographic limitations and dependency on world energy costs pose major barriers to large-scale extraction.\nSEE ALSO ; M INERAL R ESOURCES FROM F RESH W ATER ; .\nJames R. Craig\nCraig, James R., David J. Vaughtan, and Brian J. Skinner. Resources of the Earth: Origin, Use, Environmental Impact, 3rd ed. Upper Saddle River, NJ: Prentice Hall, 2001.\nLahman, H. S., and J. B. Lassiter III. The Evolution and Utilization of Marine Mineral Resources. Books for Business, 2002.\nUSGS Minerals Information: Mineral Commodity Summaries. U.S. Geological Survey. <http://minerals.usgs.gov/minerals/pubs/mcs/> .\nUSGS Minerals Information: Minerals Yearbook. U.S. Geological Survey. <http://minerals.usgs.gov/minerals/pubs/myb.html/> ."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:aeff830d-63e1-4d76-bae2-c167b71f1c71>","<urn:uuid:6c7beb72-7a7b-4bd9-ab7e-c5c996fdfaa0>"],"error":null}
{"question":"When did the Union Pacific Coal Company close its mines in Hanna, Wyoming and what was the main reason for the closure?","answer":"The Union Pacific Coal Company closed its Hanna mines in February 1954. The main reason for the closure was that diesel was replacing coal as the railroad's primary fuel.","context":["Since fur-trade times, Wyoming has straddled the main travel route across North America. The Oregon and Overland trails were followed by the Union Pacific Railroad, which was followed by the Lincoln Highway and today’s Interstate 80. The routes are full of stories.\nA Brief History of Hanna, Wyoming\n“Coal is King,” proclaims a yearly celebration in Hanna, Wyo. The sentiment is echoed by the dragline on the horizon to the north. The Arch Mineral Corporation’s Page model 742 was notable in its day and worked from 1977 to 2005 uncovering coal and reclaiming pits. Of the dozen or so goliath machines once working the strip mines of the Hanna Basin, it is the sole, now silent, survivor.\nDuring the winter of 1889, workmen began opening Union Pacific mines at Chimney Springs, soon renamed Hanna in honor of Mark A. Hanna. A year earlier, the Cleveland, Ohio-based coal and shipping magnate had convinced Union Pacific powers that the railroad’s future lay in the retention and development of its coal fields.\nProspects in the area were good; seams were between 15 and 30 feet thick. The company officials hastily brought the No. 1 Mine in production. From its inception in 1890 until the closing of its Hanna mines in 1954, the Union Pacific Coal Company opened six mines, some mere prospect holes, others long-lived collieries.\nA company town from the start\nHanna was an exemplary, if primitive, company town, with rows of identical houses punctuated by privies and coalhouses in the alleys, and nearby company structures dedicated to commerce and social interaction.\nMiners and their families were defined by workplace. One Town by No. 1 Mine was a compact area occupying a treeless flood plain. As mines were added, so were accompanying villages Two Town and Three Town. While ethnic and racial groups might concentrate within areas, the towns were essentially integrated. Exceptions were Jap Town, a settlement near No. 2 Mine, where space allowed expansive gardens. Elmo, a Finnish village incorporated in 1917 was located a mile to the east of Hanna, beyond the controlling reach of the company. In Elmo, saloons and other types of free enterprise flourished.\n“Mine No. One is a pathetic story of grief for the company, its employees and their families.” In that order History of Union Pacific Coal Mines described the impact of the tragedies of June 30, 1903, and March 28, 1908. With 228 fathers, brothers, sons, uncles and friends lost, Hanna became a town of widows. Decades later, Wyoming magazine’s Editor Mike Leon pondered what was unique about Hanna: “Its atmosphere is different … They have united over barriers that divide men elsewhere, have drawn together in a shared, prideful tradition … and in a common sorrow.”\nThe United Mine Workers of America\nThe “shared, prideful tradition” was sustained in part by entities outside the company’s domain; this included the miners’ union. The late Muriel Kitching, Hanna historian, related in her memoir that a United Mine Workers organizer disembarked from a Union Pacific train outside town limits in 1906 and hiked the quarter mile to the top of Spion Kop Hill (named by British miners after an important hill and battle in the Boer War) where he enrolled willing miners.\nIn 1907, faced with a general strike, a dearth of replacement laborers and a rising market, the coal company formally recognized the United Mine Workers of America union in all its camps. The newly formed Hanna Local 2335 of the UMWA, with its membership including blacks and Japanese, became part of the national union’s District 22. An early union victory in the same year—company acceptance of the eight-hour day—was celebrated annually in Hanna on April 1, a holiday second only to Labor Day. Carol Boam, who was born in 1939, recalled years later that many Hanna families honored a unique trinity: Christ, longtime UMWA President John L. Lewis and President Franklin D. Roosevelt.\nThe Finn Hall, churches and schools\nIn September 1908, The Finnish Temperance Society of Hanna, Wyo., filed for incorporation under state statutes. Often allied in their efforts, the society and the UMWA shared the generous space of the Finn Hall, which had been constructed in old Carbon, the Union Pacific’s first coal camp about ten miles to the south.\nThe entire structure was hauled, intact and manually, to the new company town. The sturdy Finns may have literally carried the building or, perhaps, rolled it on logs. Stories differ. Gert Salo Milliken, daughter of a Finnish miner, commented,” Everyone was always welcome at the Finn Hall.”\nChurches and schools were also outside the company domain. The school board of independent, practical miners created one of the best systems in the state. Hanna High School was first accredited by the North Central Association in 1942. Sports programs were offered to both boys and girls. A uniformed marching band and musical programs entertained the town from the early years and even after the 1954 mine closing. From the 1920s until the boom of the late 1970s, the only newspaper in town was written and printed on a lead-type press by high school students.\nAnd there were, north and south, large stretches of accessible land, where as Episcopal Priest Harv Wilbur noted years later, “One season followed another. When we arrived it was fishing season—label that trout … Early September brought antelope season, followed by deer. And for those lucky few who won the drawing, the moose and elk. Ducks and geese came in there somewhere. … It was as much for camaraderie as for wild game.”\nIn the 1920s, Union Pacific Coal Company President Eugene McAuliffe and social worker Jessie McDiarmid, introduced long-lasting innovations throughout the company system. Hanna had its own section in the new employees’ magazine. Shared were births and obituaries, photographs especially of children and small triumphs with headlines like “Hanna Girl Wins County Spelling Contest.”\nMcAuliffe’s innovative UPCC Old Timers Association honored miners and management with twenty or more years of service at an annual, festive reunion. There were company bands, garden contests and presentations of safety awards. Community councils served as advisory groups to improve town life. Company-sponsored scout troops offered camping experiences and first aid contests. Hanna’s senior girl scouts first aid team won national recognition in 1926.\nHanna’s World War I Monument contains 114 names of area men who went to the Great War. The Role Of Honor for World War II lists 142 men and women. Five Scarpelli brothers served, as did two Leino sisters. Brothers Bernard and William Lucas perished in the war. Immediately after the attack on Pearl Harbor, the UPCC removed all Japanese from the mine. A few days later, they returned to work; the war effort required the entire community.\nRemaining miners worked long hours. Women picked boney—pieces of rock—from moving conveyers of coal at the tipples, where coal was being separated into various sizes before shipment. Girls boxed cookies and wrote letters. Children saved newspapers and aluminum foil and helped gather a mountain of scrap metal, all as part of the national recycling effort.\nThe switch to strip mines\nIn February of 1954 Union Pacific Coal Company’s Hanna mines closed. Diesel was replacing coal as the railroad’s primary fuel. Ruth Boam recalled, “What fools we were. When the first diesel locomotives came through town, there we were by the tracks and cheering our heads off. We never thought for a moment what it meant for us.” Ruth’s father had worked in the mines for fifty years; four miners from her family had been victims of the 1908 No. 1 Mine Disaster. Now, her husband was out of work.\nCatherine Fenimore, wife of a Hanna businessman, remembered, “Many tears were shed as most people had been born and raised here. … A big number moved away—house and all.” Afterward in Hanna, there was limited work in other mines, in the timber industry, on ranches, and with the railroad. U.S. Highway 30, then a major route, brought travelers through, supporting a few small businesses.\nMeanwhile, the switch from underground to surface mining had begun in 1937, when Monolith Portland Midwest Company opened the first strip mine in the Hanna field. Road builders Peter Kiewit and Sons of Omaha, in 1959, began operations for Rosebud, a surface mine that was to endure for forty years.\nEnergy Development Company initiated strip operations in 1970 and later added underground mines. By 1979, Arch Mineral was the field’s largest employer with 600 workers. Carbon County Coal’s underground pit was dedicated in 1980.\nThe miners and their families plus the people hired for the required support jobs gave Hanna the state’s prize for population growth for the decade: The numbers rose from 460 in 1970 to 2,294 in 1980, an increase of 398.7 percent.\nApril 1979 brought publication of volume one, number one of the Hanna Herald. For the next five years its young, energetic staff documented the resurrection of the town. The surviving coal company houses took on new life and value. The long vacant Finn Hall, however, while being converted to an apartment house, burned to the ground in 1982.\nMost new homes, generally mobile or modular, were placed in “old town” or in expanding hillside developments. Withered institutions revived. Businesses flourished. A miniature mall offered rental spaces in puce-and-ochre metal structures. The UPCC store became The Miner, an eatery and nightspot.\nThe Catholic congregation moved into a massive new structure containing chapel, priest’s apartment, and parish hall; twenty-four clergymen attended the dedication. On April 6, 1983, work began on a 31,000 sq. ft. recreation center. The 1984 class of the consolidated Hanna-Elk Mountain High School numbered a record 51. They emerged from a newly constructed building, which accommodated 300 students. In Herald profiles, 30 of the seniors anticipated going to college or trade school, but not one intended to enter mining.\nThen, the Herald revealed that the Hanna Basin in 1982 had provided only 4.6 percent of Wyoming’s coal production, down dramatically from 46 percent in 1978. Contracts were being lured away by cheaper coal produced in northeastern Wyoming’s Powder River Basin and elsewhere. An anticipated Consolidated Edison facility, with plans to perhaps employ 1,000 miners, failed to appear.\nThe editorial in the final issue of the Hanna Herald, published Jan. 23, 1985, was entitled “The Boom Was Great But….”\nHanna’s population declined: 1076 in 1990; 873 in 2000; and 841 in 2010. Jobs evaporated as mine reclamation was completed. The Union Pacific consolidated section crews and the completion of Interstate 80 in 1970 eliminated most local highway business. Wind-power development brought limited employment.\nIn 2013, Hanna is a bedroom community, where housing is cheap, but the commute may be long. Rawlins, the closest town of any size, is 43 miles west. Most businesses, notably gas stations and grocery stores, have closed.\nThings may change. As of late 2013, Medicine Bow Fuel and Power’s gasification plant in the Carbon Basin to the southeast remained a proposal. The Wyoming School Facilities Commission had broken ground on a new elementary school, projected to cost $7 million. Inexpensive properties and the great outdoors continued to draw retirees and sportsmen. Perhaps Hanna is, as locals have long said, “the town that wouldn’t die.”\n- Boam, Carol, Ph.D. Interview by Nancy Anderson. Tape recording, Oct. 1980. Hanna Basin Museum.\n- Boam, Ruth. Interview by Nancy Anderson. Tape recording, Oct. 1980. Hanna Basin Museum.\n- Fenimore, Catherine. A Small Corner of My Heart: Reminiscing on Coming to Wyoming. Hanna, Wyo.: the Author, 1989.\n- Hanna Herald, 1979-1985.\n- Kiewit Corporation. Kieways. July/August, 2001.\n- Kitching, Muriel. Interview by Nancy Anderson. Tape recording, Oct. 25, 1980. Kitching Collection, Hanna Basin Museum.\n- Leon, Mike. “Hanna’s Old Timers.” Wyoming. June/July 1957, 4-10.\n- Milliken, Gert Salo. Interview by Nancy Anderson. Tape recording, November 2008. Hanna Basin Museum.\n- Penman, Ralph. Interview by Nancy Anderson. Tape recording, November 2005 Hanna Basin Museum.\n- Union Pacific Coal Company Employees’ Magazine, 1923-1945.\n- Wilbur, Harv. Hanna Field: Story of a Fledging Episcopal Priest and his Six Wyoming Missions. Denver, Colo.: the Author, 1995.\n- Young, Noah. Letter concerning No. 1 Mine Disaster of 1908 written at the request of Governor B.B. Brooks. Brooks Papers. Wyoming State Archives.\n- Aiken, Ellen Schoening. “The United Mine Workers of America Moves West: Race, Working Class Formation, and the Discourse on Cultural Diversity in the Union Pacific Coal Towns of Southern Wyoming, 1870-1930.” Ph.D. diss., University of Colorado, 2002.\n- Klein, Maury. Union Pacific: The Birth of a Railroad 1862-1893. Garden City, New York: Doubleday & Co., 1987.\n- Long, Priscilla. Where the Sun Never Shines: A History of America’ s Bloody Coal Industry. New York: Paragon House, 1991.\n- McAuliffe, Eugene, et al. History of Union Pacific Coal Mines. Omaha, Neb.: Colonial Press, 1940.\nFor Further Reading\n- Leathers, Bob. “The Story of Hanna, Carbon Wyoming.” Accessed Dec. 11, 2013 at http://www.hannabasinmuseum.com/hanna-story.html.\n- The photo of the miners’ monument is by Elfino, from Panoramio. Used with thanks. The September 2012 photo of the Arch Mineral Corp. dragline is by Tom Rea.\n- The sketch of Two Town in 1951 by Harv Wilbur, from his book Hanna Field, and the rest of the photos are all from the collections of the Hanna Basin Museum. Used with permission and thanks. The sign over the heads of the crew of underground miners in the color photo reads, “You are Expected Home Tonight. Please Be Careful.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:876af1ec-85ed-4737-9686-a5375d7d0460>"],"error":null}
{"question":"How many cards are dealt to each player in Pai Gow Poker?","answer":"In Pai Gow Poker, each player is dealt seven cards, which must be arranged into two hands - one five-card hand and one two-card hand.","context":["Pai gow poker is played among up to seven players. One of the players is designated as the player-dealer. (indicated by a dealer button). Each player is dealt seven cards, which are arranged into two hands. One hand consists of five cards; the other hand consists of two cards. After players have arranged their hands so the five-card hand ranks higher than the two-card hand, the house dealer reveals and arranges the player-dealer’s hand.\nHands then compete, one at a time, as in a blackjack game, against the player-dealer hand. The player-dealer hand competes against player hands clockwise beginning with the hand in an order determined by the shaking of dice.\nDouble-hand uses poker rankings to compare the two player hands with the two hands of the player-dealer. The player wins if both hands beat those of the player-dealer; the player loses if both hands are beat by those of the player-dealer. If either hand ties that of the player-dealer, this is considered a win for the dealer. The player pushes (neither wins nor loses) if one hand wins and the other hand loses or ties.\nBay 101 provides house dealers for all games but does not participate in the actual play of the game and has no interest in the outcome of play. No player ever plays against or makes a wager against Bay 101.\nIn pai gow poker (also known as double-hand poker), the deck consists of 53 cards, 52 arranged into four suits, spades, hearts, diamonds, and clubs, plus one joker. Unlike bridge and other games, no suit is “worth” more than any other. Each suit consists of 13 cards, ranked, from highest to lowest: ace, king, queen, jack, 10, 9, 8, 7, 6, 5, 4, 3, 2.\nAn ace can also rank as the lowest card, but only in a 5-high straight. That is, A-K-Q-J-10 is the highest straight, while in this hand, the straight 5-4-3-2-A, the ace is the lowest card.\n|From highest to lowest, with examples:|\n|five aces||A♥ A♠ A♦ A♣ joker|\n|royal flush||A♥ K♥ Q♥ J♥ 10♥|\n|straight flush||J♠ 10♠ 9♠ 8♠ 7♠|\n|four of a kind||K♠ 7♥ 7♠ 7♦ 7♣|\n|full house||9♠ 9♥ 9♦ J♦ J♥|\n|Flush||Q♠ 10♠ 7♠ 4♠ 2♠|\n|straight||10♠ 9♦ 8♦7♠ 6♥|\n|three of a kind||7♠ 7♦ 7♣ Q♥ J♥|\n|two pair||A♠ A♥ J♦ J♣ 5♠|\n|one pair||2♠ 2♦ J♥ 5♣ 3♠|\n|no pair||K♥ J♦ 8♠ 6♥ 3♣|\nIn five-card hands, the joker can be used to complete any straight or flush. In any other five-card hand, the joker ranks as an ace. For example, in this hand, the flush A♥ joker 7♥ 4♥ 2♥ , the joker represents the K♥ (In any flush that does not contain an ace, the joker ranks as the ace of that suit.) In this hand, the straight 10♠ 9♦ 8♦ joker 6♣, the joker represents a 7. Other examples: A♠ A♥ joker is three aces, while Q♥ Q♠ J♦♦J♣ joker is two pair with an ace kicker (side card).\nTwo-card hands fall into two categories, pair and no pair. The pairs rank from highest, A-A, to lowest, 2-2. The no-pair hands rank from highest, A-K, to lowest, 3-2.\nIn two-card hands, the joker is just another ace. For example, the hand A♥-joker is a pair of aces and joker-4♣ is the same as any other A-4 combination.\nThe player-dealer puts up a bank against which the remaining players make wagers.\nEach player may take the bank twice, then the button rotates clockwise. A player, in turn, may decline or accept the player-dealer position.\nThe house dealer deals seven hands (piles) directly in front of the chip tray, each consisting of seven cards. The player-dealer selects one of the seven piles as the first hand to be delivered. The house dealer places an action button on the selected hand.\nThe position the hands are dealt to is randomly decided by the roll of three dice. The player-dealer position counts as either 1, 8, or 15. For example, if the dice total 12, the first hand would be dealt to the fourth position clockwise from the player-dealer’s position (12 - 8 = 4). All hands go directly to the seat position, except for the hand of the player-dealer, which goes directly in front of the house dealer. After distributing the seven hands, the house dealer picks up any hands dealt to an empty seat; those cards are taken out of play.\nPlayers arrange their hands, then place the hands in front of them, with the two-card hand (front hand) in front of the five-card hand (back hand). This is called setting the hand [see figure 3 ]. The back hand must rank higher than the front hand. A player can ask the house dealer to set the hand for him. If so, the hand is set house way, that is, in a predetermined manner [see figure 4 ]. After all player hands have been set, the player-dealer reveals and sets his hand. Hands then compete, one at a time, as in a blackjack game, against the player-dealer hand. Play begins with the action button and proceeds clockwise.\nThe order in which hands compete against the player-dealer is important, because if the player-dealer loses his stake, not all player hands may get to compete. (The commonly used term for this is the bank does not cover.) Also, a hand may not get complete action. For example, the player-dealer puts up a bank of $200. The action button wagers $100 and the next five players wager $50 each. The player-dealer loses the first three hands, paying the action button $100 and the next two players $50 each. The bank is now empty, and the remaining three players get no action on their $50 wagers. It doesn’t matter whether their hands would win or lose against the player-dealer. A player may never win or lose more than the total of his wager.\nAfter a player-dealer has had two opportunities to put up a bank, the next player clockwise has the option of putting up a bank. No player is ever obligated to put up a bank.\nThe house dealer controls the shuffling and dealing of the cards, orderliness of the game, and the house collections for each hand.\nMultiple players can wager on the same hand. When a player bets on a hand and is not seated at that location at the table it is called backline betting. Backline bettors usually stand behind the seated player on whose hand they’re betting but they can also be seated at another seat location. Whoever wagers the most on a hand determines how to set the hand.\nTo win, both player hands must rank higher than the player-dealer's two hands.\nTo lose, both player hands must rank lower than or tie the player-dealer’s two hands. (If either hand ties that of the player-dealer, this is considered a win for the dealer. For example, both player and player-dealer have ace-king as their front hand. The player-dealer wins.)\nTo push (neither win nor lose, that is, for no money to change hands) one of the player's two hands wins and one loses to or ties the player-dealer's hand. The second illustration shows an example of a tie push."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9d22c511-ee43-495c-b161-168e71d626e4>"],"error":null}
{"question":"How do the Civil Air Patrol's rescue missions compare to the UAV Challenge's simulated rescue scenarios?","answer":"The Civil Air Patrol performs real-world inland search and rescue missions, conducting about 90% of continental U.S. operations and saving an average of 78 lives annually. In contrast, the UAV Challenge features simulated rescue scenarios where unmanned aerial vehicles must navigate obstacles like bad weather, magpies, and commercial aircraft to reach a dummy called Outback Joe in various emergency situations. While both involve aerial rescue operations, the UAV Challenge is a testing ground for developing rescue robot technology, while Civil Air Patrol conducts actual life-saving missions with human pilots.","context":["How one fair dinkum ‘dummy’ has transformed the future of flying rescue robots\n“Outback Joe is one helluva dummy!” says Australian Centre for Robotic Vision Chief Investigator Jonathan Roberts; one of the brains behind ‘the dummy’ in the world’s biggest airborne robotics challenge set to touch down in rural Queensland next week (24-28 September).\nAptly entitled the UAV Challenge, organised by QUT with the support of the Australian Centre for Robotic Vision and co-organiser CSIRO’s Data61 , the visionary event has drawn an eye-boggling array of agile, low-cost lifesavers aka unmanned aerial vehicles (UAVs) over the past 12 years.\nFlying robots of all shapes and sizes, weighing-in anywhere from 3kg (the weight of a brick) to around 20kg (think three bowling balls or checked-in luggage) and resembling everything from mini planes to UFO-like contraptions. All tasked with the white-knuckle challenge of navigating remote terrain, bad weather and an onslaught of obstacles (terrestrial and airborne), to aid a person in need of emergency medical assistance.\nBehind it all, as Professor Roberts explains, lies one fair dinkum dummy, Outback Joe. Literally; face-down in a Dalby paddock, 200km west of Brisbane. The same rural town that produced Hollywood star Margot Robbie, who regularly visits family and friends.\nWhile Outback Joe is yet to scale the same dizzy heights as Robbie, who last year made TIME magazine’s ‘100 Most Influential’ list, the firefighting dummy and quintessential ‘quiet achiever’ has done more to advance the future of emergency UAVs on the frontline than can easily be quantified.\n“He’s certainly no ordinary Joe,” says Professor Roberts of the dummy that has inspired advances in UAV design, software and communications systems. Not least being enhancements to the functionality and codebase of open source autopilot software, ArduPilot, now embraced by major players of the ilk of Microsoft, Boeing, 3DR, jDrones, Precision Hawk, AgEagle and Kespry.\n“In fact, Outback Joe has become something of a celebrity thanks to his starring role in the UAV Challenge. Fortunately, much like the community of down-to-earth farmers, he doesn’t let it go to his head.\n“I know he’d welcome a co-star if Margot Robbie happens to be in town.”\nOutback Joe, of course, is no stranger to the rigours of acting. Over the years, he’s stepped into character as a bushwalker lost in Woop Woop; a tradie whose ute conks out in the back of beyond; and a seriously ill farmer needing to send an urgent blood sample to his doctor.\nIn the UAV Challenge, Outback Joe’s perilous life-or-death situation is further amplified by the addition of simulated flash flooding. In the air, meanwhile, UAVs also have to contend with potential air strike from magpies during swooping season.\n“A few of the UAVs have been clipped by maggies during the competition,” says Dennis Frousheger, senior engineer at Data61.\n“Others come to grief in trees or crash and burn due to technical glitches. It doesn’t help that we also simulate obstacles, like commercial aircraft and extreme weather, for the UAVs to avoid.\n“So, winning team members, in our book, are nothing short of super heroes! Actually, the biennial Medical Express challenge has not yet been completed. One team – Canberra UAV – came extremely close in 2016 and they’re coming back to give it another crack.\n“However, we’ve upped the ante. To win the maximum prize of $75,000, unmanned aircraft need to complete the mission without pilot intervention while avoiding all the simulated obstacles we throw at them. It’s going to be a cliffhanger!\n“But, above all, the UAV Challenge is a lot of fun, bringing together a tight-knit community of hobbyists and experts from all corners of the globe.”\nOn 24 September, Dalby will welcome the largest pool of UAV Challenge competitors to date, numbering 150 and surpassing 2016 levels, when hotel rooms sold out. Hailing from all corners of the globe, the competitors will contest:\nOf note, a record line-up of 12 qualifying teams have made the cut to contest the open Medical Express challenge, hailing from Australia, Poland, Thailand, the Netherlands, Canada, India and China. The Airborne Delivery Challenge has similarly scored 12 teams of excited high school students coming from the United States, Korea and Australia.\nTeam Dhaksha, from Madras Institute of Technology, has already set tongues wagging ahead of the Medical Express challenge. Indian actor, Ajith Kumar, a leading star in Tamil Cinema – who, like George Clooney, is known for his trademark charm and salt & pepper hair, but unlike Clooney happens to also be a licensed pilot, capable of flying a fighter jet – steps up as the team’s ‘Test Pilot and UAV System Advisor’ (pictured, Centre, with Team Dhaksha, below). Plus, check out the team’s video entry as part of qualification for the 2018 UAV Challenge.\nDid you know? Outback Joe is not alone when it comes to run-ins with disaster. Born and raised in Dalby, Doug Browne, whose farm the UAV Challenge takes place on, could arguably give Outback Joe a run for his money. Indeed, the 72-year-old, was sucked under flood waters and snared on a barbed wire fence during the 2010-11 Queensland Floods, needing to be airlifted out by a neighbour with a helicopter. As Doug says: “I was pretty lucky to survive that one. My legs were pretty cut up. Another time, I was knocked out cold when working on a stationary road train. I was in the wrong place at the wrong time when the tarp fell off and a piece of metal hit me on the head!” Not surprisingly, Doug is the first to applaud the life-saving search and rescue mission behind the UAV Challenge. “At the moment, it’s Outback Joe who needs rescuing, but you never know, it might one day be me…”\nThe 2018 UAV Challenge is sponsored by: Queensland Government; Insitu Pacific and Boeing Research & Technology – Australia; Northrop Grumman; Lockheed Martin Australia; Defence Science and Technology Group (part of the Australian Government Department of Defence); and MathWorks. Members of the public are welcome to join the UAV Challenge as spectators.\nFor more information, including competition schedule click here.\nCommunications Specialist, Australian Centre for Robotic Vision\nP: +61 7 3138 4265 | M: +61 416 377 444 | E: firstname.lastname@example.org\nAustralian Centre for Robotic Vision\n2 George Street Brisbane, 4001\n+61 7 3138 7549","Sayville, NY - June 15, 2017 - Civil Air Patrol 1st Lt. Albert “Al” C. Cerullo Jr., of Syosset, has been awarded the Wright Brothers “Master Pilot” Award for his 50 year record of safe flying with no aviation mishaps by the Federal Aviation Administration. According to the FAA, “The Wright Brothers Master Pilot Award is the most prestigious award the FAA issues to pilots certified under Title 14 of the Code of Federal Regulations (14 CFR) part 61. This award is named after the Wright Brothers, the first US pilots, to recognize individuals who have exhibited professionalism, skill, and aviation expertise for at least 50 years while piloting aircraft as ‘Master Pilots’.”\nComplementing his love of flying, Al has been a member of the Civil Air Patrol since 2009 and currently serves as the aerospace education officer at Civil Air Patrol’s Bethpage-based Lt. Quentin Roosevelt Cadet Squadron - its commander, C.A.P. 1st Lt. Mark Del Orfano is pictured at right with Al. At different times, Al served as testing officer, cadet activities officer and deputy squadron commander.\nWhile serving as a helicopter pilot with U.S. Army’s 176th Assault Helicopter Company during the Vietnam War, Al flew 1,605 combat hours in 241 flying days from 1967 to 1968. During that year of combat flying as a disciplined 23-year-old Chief Warrant Officer Two, Al had earned many decorations and recognitions, including the Distinguish Flying Cross, the Air Medal with multiple citations, and the Purple Heart. After returning from overseas, he was able to achieve his instrument instructor rating and accumulated over 35 years of flight experience with over 25,000 hours.\nAs of late, Al currently runs Hover-Views Unlimited, an aerial cinematography service, catering mainly to the New York metropolitan area. According to HoverViews.com, the company website, “his numerous credits include breathtaking cinematography in film, television and commercials.” Additionally, Al is a member of the Screen Actors Guild, a founding member of the Motion Picture Pilots Association and is a recipient of the 2009 Society of Camera Operators Lifetime Achievement Award.\nCivil Air Patrol, the longtime all-volunteer U.S. Air Force auxiliary, is the newest member of the Air Force’s Total Force, which consists of regular Air Force, Air National Guard and Air Force Reserve, along with Air Force retired military and civilian employees.\nC.A.P., in its Total Force role, operates a fleet of 550 aircraft and performs about 90 percent of continental U.S. inland search and rescue missions as tasked by the Air Force Rescue Coordination Center and is credited by the AFRCC with saving an average of 78 lives annually. C.A.P.’s 56,000 members nationwide also perform homeland security, disaster relief and drug interdiction missions at the request of federal, state and local agencies. Its members additionally play a leading role in aerospace education and serve as mentors to more than 24,000 young people currently participating in the C.A.P. cadet program.\nPerforming missions for America for the past 75 years, C.A.P. received the Congressional Gold Medal in 2014 in honor of the heroic efforts of its World War II veterans. C.A.P. also participates in Wreaths Across America, an initiative to remember, honor and teach about the sacrifices of U.S. military veterans. Visit www.capvolunteernow.com for more information.\nLong Island Group Civil Air Patrol is headquartered on the grounds of Long Island MacArthur Airport. For more information, please visit the Group’s website at http://lig.nywg.cap.gov."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:7c1849b9-4b32-49a8-9590-be4a55d133ed>","<urn:uuid:8e5ef80c-e15b-4398-bac2-bd991d0f03a1>"],"error":null}
{"question":"What's the difference between genetic testing for hereditary cancer and diagnostic tests for liver problems?","answer":"Genetic testing for hereditary cancer involves analyzing blood or cheek cell samples to check for specific gene mutations, and is typically preceded by genetic counseling to assess family history and determine testing appropriateness. In contrast, liver diagnostic procedures focus on identifying current conditions through monitoring symptoms like chronic fatigue and digestive difficulties, and measuring specific markers such as elevated levels of hepatic enzymes, bilirubin, and decreased levels of albumin and fibrinogen.","context":["Each year, millions of people in the United States suffer from gallstone-related pain. However, it may surprise many of these individuals to discover they are, in fact, suffering from the presence of liver stones. Liver stones are the less publicized cousins of the gallstone, as the two are essentially the same. Ultimately, location defines the difference between the two. To help reduce the risk of suffering from this painful condition, everyone should understand how the liver functions and what can be done to support its health.\nThe Location and Purpose of the Liver\nThe liver is the largest gland in the body and it’s located at the beginning of the small intestine. The gallbladder, for comparison, is located behind the liver, near its base. The main purpose of the liver is to produce bile, which is an essential secretion for breaking down digested lipids into smaller particles so fats and fat-soluble vitamins can be absorbed by the small intestine.\nThe liver forms bile from water, electrolytes, and other molecules including cholesterol, bilirubin, bile acids, and phospholipids. Adults produce between 400 and 800 ml of bile each day and it’s stored in the gallbladder until needed.  Besides manufacturing bile, the liver works as a filter to detoxify and purge harmful chemicals (such as alcohol) from the body.\nWhat are Liver Stones?\nLiver stones are commonly formed whenever an excess of cholesterol exists in the body. When excess dietary cholesterol is present, the remainder often crystallizes into small, pebble-shaped masses. Recently, researchers at the University of Bonn in Germany discovered that heredity may also be a contributor to the condition. The scientists believe they have isolated a gene within the body that greatly increases a person’s chances of forming liver and/or gallstones. \n4 Common Diseases of the Liver\nDue to the liver’s role as the body’s detoxifying system, much of what we eat and drink can greatly affect its health. Here are some of the more common diseases of the liver:\nDamage to the liver causes scarring which further prevents the liver from functioning the way it should. Common causes of cirrhosis include alcoholism and hepatitis. About 5% of cirrhosis patients develop liver cancer and end up receiving liver transplants due to excessive scarring.\n2. Fatty Liver\n“Fatty liver” is a common condition in people who drink too much alcohol. The liver becomes enlarged by the accumulation of fat cells. Usually, the only symptom presented is a slight discomfort within the abdomen. The condition normally improves if the person in question ceases drinking alcohol.\nAlcoholic hepatitis (liver inflammation) is also caused by the intake of too much alcohol and is often a precursor to cirrhosis and eventual liver failure. If detected early enough, damage to the liver may be somewhat reversed. Viral Hepatitis, on the other hand, attacks the liver directly. Actually five different types of viral Hepatitis exist:\n- Hepatitis A (HAV) - Can affect anyone and ranges from single cases to widespread epidemics\n- Hepatitis B (HBV) - A deadly disease causing cirrhosis, cancer, liver failure, and even death\n- Hepatitis C (HCV) - Easily spread by coming into contact with the blood of an infected person\n- Hepatitis D (HDV) - A virus found in the blood of affected individuals, and the patient usually has HBV as well\n- Hepatitis E (HEV) - Similar to HAV, but rarely found in the United States.\n4. Gilbert Syndrome\nGilbert Syndrome is caused by a defect in the uptake of bilirubin into the liver cells. Gilbert Syndrome can be very difficult to identify and is sometimes confused with liver disease.\nSymptoms of a Sluggish Liver\nWhen your liver isn’t functioning at peak performance, you may experience some or all of the following symptoms:\n- Chronic Fatigue\n- Digestive Difficulties\n- Chemical Sensitivities\nIt’s interesting that the symptoms for impaired hepatic detoxification are commonly the same as those attributed to patients suffering from exposure to toxic chemicals.\nCan Any Organisms Harm the Liver?\nYes! Liver flukes (Trematodes) are a type of flatworm which infects the digestive system and other organs in the body. Adult flukes have external suckers which allow it to attach and leech nourishment. Trematodes normally enter the body through the act of eating uncooked fish or plants and fish from fluke-infested waters. Trichinosis is another type of parasitic disease that can harm the liver. Trichinosis is contracted from eating under-cooked meat (usually pork) and it affects the bile ducts that drain the liver.\nA condition known as schistosomiasis occurs when parasites enter the body through the skin, usually from walking barefoot in infested fresh water. The organisms travel through the body, settling in the small veins of the liver. Ultimately, schistosomiasis results in cirrhosis of the liver due to the scarring caused by the inflammation. Also, many people may not realize Hepatitis A is a virus that enters the liver as an organism. The most common way to contract Hepatitis A is to eat food prepared or handled by someone who didn’t thoroughly wash their hands after using the bathroom.\nWhat Chemicals Affect the Liver?\nChemicals which can cause hepatotoxicity include Acetaminophen, Isoniazid, aflatoxins, arsenic, and carbon tetrachloride.  Taking more than the recommended amount of Acetaminophen causes the glutathione pathway to become overwhelmed with NAPQI—Acetaminophen’s metabolite. Ultimately, this toxic compound accumulates in the liver and the glutathione is unable to remove it, thus damaging the liver in the process.\nUsed to treat Tuberculosis, Isoniazid is a powerful medication necessitating the patient to undergo constant liver tests. Isoniazid is believed to cause granulomas (chronic inflammatory nodules) within the liver.\nAflatoxins occur naturally and can be found in soil, decaying vegetation, and other natural settings.  Aflatoxins typically flourish in areas of high heat and moisture and normally contaminate crops of cereals, spices, peanuts, and other dried goods. High exposure to aflatoxins causes carcinoma, cirrhosis, and eventual necrosis of the liver.\nArsenic is a well-known carcinogen and teratogen. The most common exposure for humans comes from drinking tap water. High levels of arsenic can be fatal within 12-48 hours after ingestion with some deaths occurring within one hour.  Once absorbed into the body, arsenic is stored in the liver where it can contribute to the onset of cancer.\nCarbon tetrachloride is very dangerous to humans and animals when inhaled. Clinical signs of carbon tetrachloride inhalation include elevated levels of the hepatic enzyme Aspartate Aminotransferase, elevated levels of bilirubin, decreased levels of albumin and fibrinogen, and a swollen and tender liver. \nGeneral Statistics about Liver Problems\n- Physical injuries to the liver are responsible for just 5% of all hospital admissions. \n- 80% of all cases of liver toxicity are caused by pharmacological drug reactions. \n- More than 900 prescription drugs are known to cause injury to the liver. Toxicity is the leading reason for withdrawing a drug from the market. \n- Liver and gallstones are present in 10 % to 20% of the population. \n9 Herbs That Promote Liver Cleansing\n1. Organic Chicory Root\nOrganic chicory root has a long history of assisting with liver problems, extending back to the ancient Romans and Egyptians who used this herb to help cleanse the blood. In more recent times, roasted Chicory root has served as a natural, caffeine-free coffee alternative.\n2. Organic Dandelion Leaf\nDandelion Leaf helps promote bile excretion from the liver. As such, the body can more efficiently process foods and liquids while purging harmful toxins. In addition, improved bile flow makes it possible for the body to better metabolize fat, which helps optimize the body’s cholesterol levels.\nDandelion Leaf is also effective at stimulating a sluggish gallbladder, which is responsible for storing and excreting bile as the body needs it. As such, this natural substance is effective at promoting blood purity which helps reduce the burden placed on the liver to filter out toxins.\n3. Organic Dandelion Root\nYou may consider dandelions as a weed, but the root has been used for many years to aid the body in purifying the blood and remedying liver problems. For people suffering from excess water in the liver due to health problems, dandelion root might help remove the water and improve overall liver function.\n4. Organic Greater Celandine\nCelandine is often used to assist with liver and gallbladder cleansing programs. Why? This valuable herb helps prevent the accumulation of foreign particles in the liver.\n5. Organic Milk Thistle Seed\nA number of studies suggest Milk Thistle Seed supports optimal liver function. For example, Milk Thistle Seed’s can promote the liver’s ability to regenerate new tissue after damage occurs. Milk Thistle Seed even shows promise in stimulating the liver to produce additional bile for improved digestive function.\n6. Organic Peppermint Leaf\nOrganic Peppermint Leaf helps to improve the flow of bile from your liver to the gallbladder. The volatile oil found in Peppermint (a hybrid of watermint and spearmint) also aids with the overall digestive process. Peppermint helps keep your liver in proper working order.\n7. Organic Turmeric\nOffering strong antioxidant effects, Organic Turmeric helps remove toxins from the body. As a result of this antioxidant effect, your liver does not have to work as hard to filter blood and keep your body healthy. Essentially, turmeric encourages a healthier liver by allowing it to focus on removing toxins from the body.\n8. Organic Yellow Dock Root\nYellow Dock Root helps eliminate the foreign substances that can overburden the liver. Yellow Dock Root is also used to address headaches, mental “fuzziness”, general irritability, skin blemishes, and blood and skin disorders—all of these problems are related to the liver not being able to operate efficiently.\n9. Wildcrafted Chanca Piedra\nWith the recent “discovery” of a very well known Amazon rainforest plant, Chanca Piedra, healthcare practitioners have acquired a powerful plant ally in helping their patients maintain optimal bladder, kidney, gallbladder, and liver health. Chanca Piedra is traditionally regarded as a health supplement that can promote the liver’s normal detoxification efforts for purging harmful substances.†\nWhat Diet Promotes A Strong Liver?\nBesides the intake of pharmaceutical drugs, a poor diet is a major contributor to most liver problems. High alcohol intake is the most common form of dietary danger to the liver. However, you can improve your diet a number of ways to promote overall liver function.\n- A diet too high in protein may cause a condition called hepatic encephalopathy or “mental confusion”. While the exact cause of hepatic encephalopathy is still unknown, it is theorized that when the body digests high amounts of proteins, excessive ammonia is formed, which affects the central nervous system.13 Your doctor should always be consulted before altering the protein in your diet.\n- A diet high in calories, especially carbohydrates and fatty foods, can equally result in liver problems. Excess caloric intake can lead to fatty deposits within the liver.\n- Watch your Vitamin A intake, as it is high in saturated fat and cholesterol and can be very toxic to the liver when taken in large amounts. However, Emulsified Vitamin A does not contain fat or cholesterol. Emulsified Vitamin A is a liquid form of the vitamin that is derived from the carotene found in green and yellow vegetables and egg yolk. Emulsified Vitamin A is a powerful antioxidant for supplementing your diet. Vitamin A also helps promote cell differentiation and supports the body’s normal immune system response.\nThe best diet for the liver is one low in protein and also sodium.14 Eating plenty of raw fruits and vegetables will help to keep your body, especially your liver from overtaxing itself. Approximately 30% to 40% of your diet should consist of dark green leafy vegetables and orange, red, purple, and yellow colored fruits and vegetables. A high fat diet causes the liver to work extra hard, so reducing your fat intake is critical to a healthy liver.\nAvocados and oily fish such as salmon, tuna, and sardines are excellent choices for promoting liver health. Many varieties of raw seeds such as flaxseed, sunflower, and alfalfa seeds are beneficial in this regard as well. Certain oils (primrose, black currant seed, and cold-pressed olive oil to name a few) assist the liver as well.\nAlong with a healthy and cleansing diet, you must routinely cleanse your liver to promote optimum function and health. A number of liver and even gallbladder cleansing products can be found in health stores and on the Internet, but the most effective of these are manufactured with only 100% organic and wildcrafted herbs and are produced using proprietary or Instruct® processes.\nLiver cleansers produced with these methods, such as Livatrex™, provide you with herbs extracted using pure Kosher-certified vegetable glycerin rather than alcohol, thus making them the safest and most effective class of supplements for supporting your health improvement initiatives.\n– Dr. Edward F. Group III, DC, ND, DACBN, DCBCN, DABFM\n- Bowen, R. Secretion of Bile and the Role of Bile Acids In Digestion. Online. (last accessed: 2007 October 4). www.arbl.cvmbs.colostate.edu/hbooks/pathphys/digestion/liver/bile.html\n- Lammert, F., Grunhage, F., Walier, M. and T. Wienker. Research team discovers gallstone gene. Hepatology. Issue 46. Pub. 2007 July 11. (last accessed: 4 Oct 2007). www.uni-bonn.de/en/News/93_2007.html\n- Centers for Disease Control and Prevention. Viral Hepatitus. (last accessed: 2007 October 5). www.cdc.gov/ncidod/diseases/hepatitis\n- Lawrence S. Friedman MD, Emmet B. Keeffe MD. Handbook of liver disease: expert consult. Elsevier Saunders. 2011 August 26. ISBN-10: 143771725X. Print.\n- Waliyar, Dr. F. Principal scientist (Pathology) and managing director for agri-science park. (last accessed: 2007 October 17). www.icrisat.org/aflatoxin\n- PDR Health. Arsenic. 2007.\n- Department of Health and Human Services. ATSDR. (last accessed: 2007 October 17). www.atsdr.cdc.gov/toxprofiles/tp30-c3.pdf\n- Peter R. McNally DO FACP FACG. GI/Liver secrets plus. Mosby Inc. 2010 April 2. IDBN-13: 978-0323063975. Print.\n- Pirmohamed M, Breckenridge AM, Kitteringham NR, Park BK. Adverse drug reactions. BMJ. 1998 Apr 25;316(7140):1295-8. Review.\n- Scott L. Friedman, James H. Grendell, Kenneth R. McQuaid. Current diagnosis and treatment in gastroenterology. Lange Medical Books/McGraw-Hill Pub. Division. 2003. ISBN:0-8385-1551-7. Print.\n- Virtual Medical Centre. Gallstones. 2003 September 21.","Hereditary cancer is caused by changes in certain genes. Genes are the instructions that our bodies use to grow and function, and this information is passed down from parents to children. According to the American Cancer Society, an estimated 5 to 10 percent of cancer diagnoses are due to an inherited form of cancer. This means that 5 to 10 percent of people who receive a cancer diagnosis had a genetic factor that caused them to have a higher likelihood of developing the disease.\nThe most common hereditary cancers include:\n- Breast cancer\n- Ovarian cancer\n- Uterine cancer\n- Colon cancer\n- Pancreatic cancer\nGenetic counseling is the first step in the genetic testing process. In a counseling session, you and a genetic counselor will discuss your family history, personal history with disease, symptoms you may be experiencing and other factors to assess possible health risks.\n- Estimate your risks for certain diseases\n- Estimate the risk of an inherited cancer in your family\n- Discuss ways to screen for and prevent cancer based on your risks\n- Review the pros and cons of genetic lab testing\n- Decide if genetic testing is right for you, given your particular risks and concerns\nOur team will also provide you with treatment options and resources to support you through your journey.\nWhile we offer genetic counseling services for a wide variety of diseases and disorders, many people use our services to better understand their cancer risks. Through genetic counseling and testing services, we can help you assess your risk of inherited cancers and empower you to lower your risks or detect cancer at its earliest stages.\nOne in three people develop cancer at some point in their lives, so chances are you or someone in your family has been affected. It’s important to remember that while a family history of cancer doesn’t mean you’ll develop cancer yourself, it may increase your risk. Genetic counseling can help you determine if you’re at a higher risk and if preventive and screening steps are right for you.\nOur genetic counseling teams recognize the importance of giving you a clear picture of your risks for cancer and other genetic conditions. Below are a few locations close to you.\nIf you or a close relative, such as a parent, child, brother, sister, aunt, uncle, niece, nephew, grandparent of grandchild, have any of the following risk factors, a referral to a genetic counseling clinic may be right for you.\nHereditary breast and ovarian cancer\n- Invasive breast cancer or ductal carcinoma in situ (DCIS) diagnosed at or under age 50\n- Two or more blood-related individuals with breast cancer\n- Male breast cancer\n- Ovarian, fallopian tube or primary peritoneal cancer at any age\n- Breast cancer and one or more other separate cancers\n- Breast cancer and large head circumference (57 cm or larger for women, 59 cm or larger for men) and/or mental retardation/autism\n- Breast cancer with Eastern European Jewish ancestry, or Hispanic/Latino ancestry from Southern Colorado or northern New Mexico\nCancers that accompany breast cancer in inherited syndromes include a second, new breast cancer; brain, thyroid, pancreatic and endometrial cancers; lymphoma/leukemia; and adrenal cortical or choroid plexus carcinomas.\nHereditary colorectal cancer\n- Colorectal OR uterine cancer diagnosed under age 60\n- Two or more separate colorectal cancers in the same person\n- Three or more blood-related individuals with colorectal or uterine cancer\n- Colorectal cancer or uterine cancer and a personal or family history of other cancers\n- Ten or more cumulative gastrointestinal adenomas or hyperplastic polyps\n- Hamartomas (non-cancerous growths), juvenile polyps or Peutz-Jeghers polyps\n- Pathology of the colorectal tumor shows microsatellite instability or evidence of a mismatch repair defect\nCancers/tumors that accompany colorectal cancer in inherited syndromes include uterine, ovarian, stomach, small intestine, pancreatic and urinary tract cancers, as well as brain tumors, sebaceous adenomas and keratoacanthomas.\nOther hereditary cancer syndromes\n- An identified inherited/genetic syndrome\n- An individual with bilateral or multiple primary cancers or brain tumors\n- An individual diagnosed with cancer at an unusually young age\n- Three or more blood-related family members with the same type of cancer\n- Rare cancers, such as medullary thyroid cancer, adrenocortical carcinoma, pheochromocytoma and paraganglioma\nGenetic counseling is the first step toward genetic testing. Information gathered during genetic counseling helps determine if genetic testing would be helpful in your situation.\nGenetic counseling involves recording a detailed family history and using that information to estimate your risks, helping to determine if testing is right for you and which tests to perform. Genetic testing may require a blood draw or cell sample from inside your cheek that are then sent to a laboratory for analysis.\nSome of the common questions asked in genetic counseling sessions include:\n- Have your relatives had genetic testing?\n- Which relatives have had cancer?\n- How old were they when they developed cancer?\n- What type of cancer(s) did they have?\n- Where did the cancer start in their body?\nYour test results, family history information and evaluation notes will only be released to someone if you sign a form stating that our office can release your information to that individual or office.\nThe Genetic Information Nondiscrimination Act of 2008 (GINA), and other laws help protect people who undergo genetic testing against health insurance discrimination and employment discrimination. .\nSome inherited gene changes cause several medical complications. Genetic counseling and testing may determine if you’re at risk for another type of medical complication or another type of cancer in the future.\nWe can then take a proactive approach to your health, considering ways to prevent your risk of developing other conditions. Many people with potentially hereditary conditions seek genetic counseling and testing to help their relatives. If you already have a condition, the results of your genetic evaluation will tell you if there’s reason for your relatives to be cautious as well.\nMost insurance companies will cover the cost of a genetic test if it will improve a person’s medical care and you have a reasonable chance of having a specific condition.\nAt Centura Health, we attempt to bill insurance for the cost of genetic counseling appointments and verify insurance coverage prior to a scheduled appointment.\nIndividuals whose insurance pays for genetic counseling will be responsible for any applicable co-pay. If you schedule a genetic counseling appointment, you should receive a call prior to your appointment to explain what your cost would be. (If you’d like to see if genetic counseling would be a covered benefit, contact your insurance company and ask if they cover the CPT code 96040.)\nBecause Medicare and some insurance companies are not yet paying for genetic counseling, Centura gives a discount to those who pay out of pocket for these services.. Depending on the length of the appointment, individuals who self-pay will be charged approximately $40 or $80. If you’re not able to make these payments, Centura may be able to help.\nGenetic testing for inherited cancer syndromes has advanced rapidly in the last few years. Expanded knowledge of genetics has also led to better insurance coverage and additional testing options that allow you to receive the testing needed to make informed choices about your health. Many former patients who pursued genetic testing previously have elected to undergo new testing that has provided new information. One such patient remarked after a cause of the cancer in her family was finally found, “My family has been looking for this explanation for 20 years.”\nIf you were seen for genetic counseling previously, even if you received normal test results, you may benefit from a follow-up appointment. If you declined genetic testing in the past because of cost, new options may allow you to receive testing at no or very little cost. If you already had genetic testing and the results were negative or uncertain, you may qualify for additional genetic testing. Personal and family history updates may also impact genetic testing options, as well as cancer risk estimates and screening options. We recommend contacting your genetic counselor every one to two years to update your family history and see if any additional genetic testing options are available to you.\nBefore 2012, genetic testing for hereditary cancer syndromes was often done one gene or one syndrome at a time. Since 2012, technology has improved to be able to test many different genes at once. We now call these tests “panels” since they analyze numerous genes of concern at one time. These panels have made genetic testing cheaper because now one test can be done instead of having to order multiple tests from different labs. Panels also allow us to test individuals for rare hereditary cancer syndromes that previously would not have been covered by insurance.\nBRCA1 and BRCA2 genes have been the center of conversations surrounding hereditary breast cancer since the mid-1990s. BRCA1/2 testing that was done before 2012 didn’t always include comprehensive rearrangement testing (also called “BART”) so it’s very possible that individuals who received normal BRCA1/2 test results before 2012 could still have a BRCA1 or BRCA2 mutation that was undetected at the time.\nOther hereditary breast cancer genes\nIn addition, many different genes have long been known to interact with BRCA1 and BRCA2. Studies have now shown that individuals with mutations in these other genes also have an increased cancer risk. Testing for these additional genes is now available.\nHereditary colorectal cancer/Lynch syndrome\nTesting for hereditary colorectal cancer has also seen great advances. The most frequently identified hereditary colorectal cancer syndrome is Lynch syndrome, which leads to an increased risk for colorectal, uterine and other cancers. Additional genes that cause Lynch syndrome have been identified and more comprehensive testing of these genes is now available. New causes of hereditary colorectal cancer also have been identified. Individuals who received Lynch syndrome testing in the past could still have a mutation that wasn’t known at the time, or they may have a mutation linked to another hereditary colorectal cancer syndrome that wasn’t tested.\nOther hereditary cancers\nFor people with other cancer types, we now have testing options that did not exist previously. For example, there are now panels available that check 19 genes for hereditary kidney cancer and 13 genes for hereditary pancreatic cancer.\nIn summary, genetic testing for inherited cancer syndromes has been significantly updated in recent years, and individuals may qualify for additional genetic testing even if they tested negative in the past."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:7f1c3b15-d3c3-4161-9be5-900e868edda8>","<urn:uuid:aab09745-8252-4b55-b4a5-177fbba10098>"],"error":null}
{"question":"What are the conservation objectives and visitor activities offered at these wildlife areas?","answer":"The areas have dual conservation and recreational goals. Conservation objectives include managing wetland habitats for wildlife, improving water quality, retaining Trees of Special Interest, increasing deadwood habitats, and protecting species like the bald eagle. For visitors, both locations offer various activities: the Rocky Mountain Arsenal provides a 11-mile Wildlife Drive auto tour, 20 miles of hiking trails, fishing opportunities, and archery facilities, while Sence Valley focuses on creating a safe welcoming environment with public facilities for quiet recreation, along with opportunities for local skills development through volunteer programs.","context":["Sence Valley Forest Plan\nThe Sence Valley Forest Plan (FP) summarises proposals by the Forestry Commission for the management of Sence Valley (109.9ha), Kelham Bridge (26ha), Mill Hill Wood (7ha), David Taylor Wood (3.5ha) and Heather Wood (10.9ha).\nAs part of the Forestry Commission Centenary celebrations that take place in 2019 an additional 48ha of arable land has now been purchased on the northside of Sence Valley increasing the Sence Valley site from 61.9ha to 109.9ha . The new woodland will be planted in 2019 with a mixture of conifer and broadleaves to showcase how sustainable forest management can be practiced and helps deliver an economically valuable timber resources while also delivering social and environmental benefits for years to come.\nThe FP lies within Leicestershire and South Derbyshire Coalfield landscape character area (LCA) which comprises of gently undulating sequence of mudstones and sandstones with seatearths and coal seams, which strongly influences both the physical and cultural patterns of the landscape. This is a landscape in continuing transformation, in part assisted by the regeneration initiative of The National Forest that covers most of the LCA.\nThe main objectives for the Sence Valley Forest Plan will be:\n- Diversify the current forest structure through the introduction of a thinning/felling programme and enrichment planting.\n- Use a cutting programme on the grassland and scrub habitats sympathetic to the habitats requirements of the flora and fauna and maximise the available feeding and nesting habitats.\n- Within the new woodland to be planted in 2019 use a variety of conifers interspersed by small blocks of single species broadleaves to provide strong visual changes in colour and texture for visitors and a wide variety of woodland habitats and cover for wildlife.\n- Manage the wetland habitats for the benefit of wildlife and to improve water quality in the River Sence.\n- Use a variety of silvicultural techniques to maintain soil structure, stability and site infrastructure.\n- Identify, retain and recruit Trees of Special Interest and increase deadwood habitats.\n- Emphasis on creating a safe welcoming environment to be enjoyed by local families and visitors to the area\n- Sympathetically design appropriately scaled interventions to improve and maintain the visual integration of the forest.\n- Provide public facilities on site to ensure easy access for quiet recreation activities.\n- Work with and support local businesses to provide public services on site.\n- Enable local people to develop their skills and their potential employability, through volunteer opportunities and training.\n- Production of commercial conifers and broadleaves managed on a sustainable basis to improve future timber revenues and diversify species to mitigate against the impacts of pest, disease and climate change.\n- Introduction of new species to help mitigate against the impact of climate change, pests and diseases and allow mixed broadleaf and conifer stands to develop.\n- Seek opportunities to develop appropriate income streams that will enable us to continue managing sites in a sustainable way.\nWhat we'll do\nThe plan details management operations including approved felling and restocking for the 10 years to 2029, with outline proposals for a 50 year period.\nThe two fields to the northern edge of Sence Valley will be planted up around a network of rides and open space. The existing young woodlands will begin to be thinned out to provide space for the crowns of the trees to develop and opportunities to increase species diversity. Clearfell operations will take place to open up three key view points and to begin the first phase of removal of Corsican pine that is now diseased.\n|Felling (strip felling of Corsican Pine)||0.5||0|\nIn addition to these defined operations, thinning of both conifers and broadleaves will begin during the plan period with conifers been thinned on a 5 year cycle and broadleaves on a 10 year cycle.\nThe proportions of open space, conifer and broadleaf woodland at the beginning of the plan period are shown in the bar chart. The gradual reduction of conifer cover and the increase in broadleaf woodland and open space expected within the plan period and anticipated over time is indicated in the middle and right hand columns of the chart.","The Rocky Mountain Arsenal National Wildlife Refuge is one of the largest urban refuges in the country. It is a sanctuary away from the hustle and bustle of busy urban life where time moves at nature's pace. With its expansive views, wildlife viewing opportunities, and recreation activities, it's easy to take a nature escape! The Refuge is open sunrise to sunset daily and closed on Thanksgiving, Christmas, and New Year's Day. Visiting the Refuge is free!\n- 20 miles of easy hiking trails.\n- Bicycle on select trails.\n- Drive the 11-mile Wildlife Drive auto tour while listening to the Refuge podcast.\n- Free nature programs.\n- Wildlife viewing.\n- Junior Ranger program.\n- Fish seasonally at Lake Mary and Lake Ladora for a small fee.\n- Photograph wildlife and landscapes.\n- Practice your archery skills at the archery range.\n- Explore the Visitor Center, which includes exhibits, wildlife activities, and Nature's Nest Books and Gifts.\nFor Refuge and Visitor Center hours see Location and Contact Information below.\nView the Refuge Wildlife Drive and Trails Map - English, Spanish\n*Trail Closure: First Creek Trail is Temporarily Closed Through May 29.\nLocation and Contact Information\nRocky Mountain Arsenal National Wildlife Refuge was established in 2004, in part, to protect our national symbol, the bald eagle. The land has a unique story - it has survived the test of time and transitioned from farmland, to war-time manufacturing site, to wildlife sanctuary. It may be one of the finest conservation success stories and a place where wildlife thrives.\nOne of the best ways to experience this 15,000 acre Refuge is by taking the 11-mile Wildlife Drive auto tour in your vehicle. Listen to the podcast, which corresponds with each mile marker along the way to discover more about the Refuge and its wildlife. The Wildlife Drive takes about 45 minutes to 1 hour to complete and is free.\nKnow Before You Go\n- The Wildlife Drive is open to motorized vehicles; speed limit is 30 mph.\n- Follow the directional symbols. Park vehicles only in designated areas on map.\n- Stay on paved roads and observe all posted signs such as \"Stay in Vehicle\" or \"Area Closed\".\n- Stay in your vehicle when the road becomes one way only.\n- Always stay at least 75 feet (23 meters) away from bison.\n- Be sure to view the Wildlife Drive and Trails Map - English, Spanish for map and details.\nWhat We Do\nWildlife conservation is at the heart of the National Wildlife Refuge System. It drives everything on U.S. Fish and Wildlife Service lands and waters managed within the Refuge System, from the purposes for which a national wildlife refuge national wildlife refuge\nA national wildlife refuge is typically a contiguous area of land and water managed by the U.S. Fish and Wildlife Service for the conservation and, where appropriate, restoration of fish, wildlife and plant resources and their habitats for the benefit of present and future generations of Americans.\nLearn more about national wildlife refuge is established to the recreational activities offered to the resource management tools used. Using conservation best practices, the Refuge System manages Service lands and waters to help ensure the survival of native wildlife species.\nAt Rocky Mountain Arsenal National Wildlife Refuge we are working for wildlife to continuously ensure that our natural resources are conserved for current and future generations to enjoy."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2f844117-5e4a-4741-ba26-6f1b74a24865>","<urn:uuid:db9d416b-8489-424f-b6de-1437b9d4c7eb>"],"error":null}
{"question":"What's the key difference between power inverters and inverter generators in terms of their power capabilities? 🤔","answer":"Power inverters and inverter generators have different power capabilities. A power inverter simply converts DC power from a battery into AC power and cannot generate power on its own - when the batteries feeding it run out, it cannot operate any machines. In contrast, inverter generators can actively generate power and provide a stable power output, creating more than 300 sine waves per engine rotation. While power inverters are useful for running appliances from battery power, inverter generators can continuously produce power as long as they have fuel, though they typically have lower power output compared to conventional generators.","context":["Many household appliances need AC power to run. The wall sockets at your home or office supply single phase AC (Alternating Current) power (voltage level depends on your geographic location) which can be used to plug in desired appliances.\nHowever, a battery cannot supply AC power. It can only supply DC (Direct Current) that is not usable for running common appliances like a refrigerator or a television. A power inverter, in short, is simply a device that converts DC power to AC power. Hence, in the absence of a reliable AC supply, the inverter can be connected to a battery and used to operate devices that would not be usable otherwise.\nFrom a technical perspective, a power inverter converts the DC waveform from a battery into a sinusoidal AC supply that can power home appliances. It is usually connected to a 12V battery (or multiple 12V batteries connected in parallel to increase the power rating) from which it draws power.\nThe 12 V DC supply is converted to 120 V AC supply. Inverters can also be supplied from 6 V, 24 V and 48 V batteries. You can plug in any device like a microwave, television, blender, coffee-maker and even power tools to the inverter now and get to work. As the battery is basically just a storage device, it needs to be recharged periodically after the inverter has drawn its juice.\nIn terms of usage, a power inverter is an extremely flexible device and can be used to power a variety of appliances, right from laser printers, entertainment units, laboratory equipment and the stuff in your kitchen to heavy duty machines like washers and dryers.\nThe inverter enables access to portable power, where you need it, when you need it. Due to its versatility, it may be used in many different ways- it can be used as emergency backup power in your home, it can be used in camping trips and also on mobile vehicles like RVs, food trucks and boats. Let us explore the many different ways in which you can use a power inverter further in this article.\nInverters are a great way to restore power at home in case of emergencies. During power outages (especially in the winter when you cannot afford to have the heating system break down), the inverter can be used to run essential devices like the refrigerator and heating or air-conditioning systems.\nMost people prefer to hook up the inverters to their car batteries, placing the inverter close to the car and away from weather elements. A 1750 Watt inverter is best suited for such applications. It is advisable to use ring terminals for connecting the inverter to the battery instead of jumper cables to avoid loose wires.\nSome commercially available inverters are also capable of being connected to the car battery through the cigarette lighter port on your dashboard.\nAlthough the inverter will work even if the car is not running, it is advised to keep the car running while the inverter is in use. This will ensure that the battery is not discharged during periods of prolonged use. Homes without vehicles can buy rechargeable 12V batteries for use during outages.\nThe most reliable solution would be having a battery bank in place for emergency backup, but such a solution would be pretty expensive. When using batteries, it is essential to charge at regular intervals. Otherwise the battery will go into deep discharge mode, which will deter battery life.\nAn inverter is rated in Watts. The manufacturer specifies two different ratings for the inverter- the continuous Wattage rating determines the power it can handle over a long period of time and the surge rating determines the amount of overload the inverter can withstand for a split second. A 1500-1750 W inverter is suitable for keeping home appliances like refrigerators and microwaves running for three-four hours, as long as the battery lasts. (The inverter rating has to be matched to the total load that you want to operate.\nHowever, the inverter itself does not produce any power, it simply converts the power from the battery into a form suitable for AC appliances. Hence, if the batteries feeding it run out, the inverter will not be able to operate any machine.\nPerhaps you want to enjoy a quiet weekend off the grid but would like to have access to power to charge your phone, laptop, camera and coffee maker.\nAn inverter can help you have such a portable power source. Since the power needs during a camping trip is not very high (unless you are planning to carry a refrigerator, air conditioning and washing machine with you into the woods), you can make do with a relatively inexpensive inverter.\nElectronic loads like laptop and cellphone chargers usually have very light power requirements. A modified sine wave inverter rated at about 1000 Watts would serve your purpose. It is best to carry a couple of batteries with you to supply the inverter, although you could use your car batteries (this might not be the best option if your camping trip is going to be relatively wrong and you run the risk of running out of gas).\nVRLA (Valve Regulated Lead Acid) or Lithium ion batteries are the safest options to use. These batteries can be recharged as well.\nYou might want to hit the road on a RV, go on a sailing trip on your yacht or maybe you operate your business out of a food truck. In all of these cases, a power inverter can be used to power up necessary appliances like refrigerators, lights, coffee makers and microwaves.\nUsing a power inverter along with a battery bank is more environmentally friendly than having a diesel generator mounted on your vehicle. Moreover, especially if you run a food truck, placement of diesel generators within the truck is in violation of health regulations. This is because many regulatory bodies consider exhaust from a generator hazardous.\nA 1750 Watt inverter seems well-suited for use within a mobile vehicle. However, the inverter rating must be decided based on the wattage of the appliances that need to be run. If you want a light duty inverter, you may opt for a modified sine wave inverter over a pure sine wave inverter. An inverter along with a battery bank is a robust and reliable power supply solution.\nThere are many reasons to use a power inverter. It is versatile, portable and can be easily carried to remote locations. The inverter generates less noise than a diesel generator (no complaining neighbors!), is environment-friendly and makes life really easy.\nIt is a mature technology and is available in a variety of configurations and ratings. The wide range of options makes finding something tailor-made for your intended operation really easy. However, one needs to remember that the inverter wattage rating needs to match the load that you wish to run.\nA reliable battery set up is required to feed the inverter when traditional AC supply is not available. If the batteries run out, the inverter will not be able to power up loads on its own.","Generally speaking, the generator is a useful machine that provides a powerful source of energy. These machines have been around for more than one hundred years. Fortunately, the generator has gone through its own evolution of sorts, now available in a lighter, compact design that still packs the type of power the generator is known for.\nModern generators are also much quieter than they were just five or ten years ago. These newer models of generators are described as whisper quiet and can be used where loud, conventional generators cannot. But what is a quiet generator DB levels? Most inverter generators already have a reputation as the leading choice for RV’ing and camping, mainly because they’re much easier to haul and transport compared to conventional generators, and they’re more fuel efficient. But aside from the fact that these generators are cheaper to run and easier to use, they’re also considered the quietest generators around. If you plan on using a generator in a public space, then you need a model that’s designed for whisper-quiet operation. That’s where the camp generator comes in.\nWhat is quiet generator DB levels? Forty-seven up to seventy decibels. The inverter generator is known as the quietest generator on the market, known widely for its quiet operation and its ability to safely charge and power sensitive electronics including laptops and smartphones. Of course, the noise level of the inverter generator can depend on engine speed and what you’re powering or charging. But, ultimately, the inverter generator will provide a much quieter operation compared to conventional generators, which is why they’re such a popular choice for camping and RV’ing.\nA generator’s noise level can depend on many factors, which is why you have to pay close attention to a spec sheet to learn what a machine’s noise level rating is, based on power load. Most models of camp generators still have a much lower than average power rating, even with a full load.\nNow that you know what type of noise rating to look for when you’re shopping for a new generator, read on to learn more about why this generator offers a whisper-quiet operation, what makes the inverter generator so different than conventional generators, and what they’re commonly used for.\nHow Quiet is Quiet?\nWhat is a quiet generator? It’s a generator that you can switch on and still carry a conversation. It’s a model that won’t disrupt your environment, and a model you can use in public and not violate a noise ordinance. If you’ve ever camped at a federal campground, then you’ll notice they have strict rules regarding decibel ratings, which is why you can’t use a conventional generator in this type of environment.\nWith this in mind, many campers have turned to inverter generators as the perfect option for use in public places. But how powerful are these generators really, and what can they be used for?\nGenerators can be used as a back-up power source in the event of an emergency, they can supply power when no other power source is available, and they can be invaluable when you’re traveling across country or camping. These portable generators are often relied upon when disaster strikes. When an inverter generator is on hand, you can have electricity, anywhere, anytime, when it’s needed the most.\nFor years, the biggest complaint surrounding generators was how loud they are. The sound of the loud engine was enough to cause most types of generators to be prohibited from use in certain public spaces such as federal parks.\nFinding a quiet running generator is now easier than ever before, thanks to the rise of the inverter generator.\nA More Reliable Power Source\nHow does the inverter generator measure up to the conventional generator? Are inverter generators better?\nConventional generators have a bad rap for their fluctuating levels of power and power surges. Power surges can be harmful to certain types of devices, such as sensitive electronics. They can actually damage these devices beyond repair. Sensitive devices include smartphones, tablets, and laptops.\nOut of any type of generator, the camp generator provides the most stable power output. This is due to the generator’s inverter technology. Unlike a large, traditional generator, the camp generator is able to create more than three hundred sine waves per engine rotation. The AC power is then converted into DC power, then inverted back to AC power.\nSo, what does that mean exactly?\nThis process means you’ll enjoy consistent, pure power with no power surges. This is why it’s safe to charge and run sensitive electronics such as smartphones and laptops.\nThe quiet-running operation is one of the biggest benefits this type of generator offers. Because of the smaller engine combined with the generator’s compact design, it’s able to run at a much lower volume compared to any other type of generator on the market. The ability to run at varying output levels is another noise reduction feature. When you’re powering up a larger appliance, the power level can be increased. For smaller devices, the power level and engine speed can be decreased. This decrease in power results in a quieter operation. Most inverter generators are able to recognize when more power is needed and can actually increase or even decrease engine speed, automatically.\nThe inverter’s housing should take most of the credit for the generator’s quiet running operation. These generators are made out of sound attenuating, insulated materials that work to absorb most of the noise that’s produced by the generator’s engine. This is why the camp generator is the best power source for use in residential areas and campsites.\nAny generator with automatic voltage regulation technology will run at a low noise level. Most models with this feature will run at the seventy-decibel mark, which is why they’re a popular option for the workplace, or backup power, when running sensitive equipment is required.\nIf you’re familiar with inverter generators, then you’ve heard they’re the quietest type of generator on the market. Some of the leading models will have a decibel rating of just fifty-eight when at the lowest power setting and up to sixty-two to sixty-seven decibels at full rated output. Both decibel ratings are much quieter than the volume at a grocery store or a loud conversation. As an example, speech at a normal volume is around sixty decibels, while the average vacuum has a decibel rating of seventy. On a busy city street, the decibel rating is around eighty to one hundred.\nSo, even at full output, the inverter generator is incredibly quiet.\nThe decibel rating for generators can vary from model to model. You may come across a generator that only emits forty-seven decibels on the lowest setting and another model that has a seventy-five-decibel rating. The amount of noise a generator produces is heavily dependent on its housing design, engine power and even the conditions it’s used in.\nThese generators are currently the first choice for caravanning and camping. In fact, they’re often referred to as camp generators. Along with the fact that they’re compact and lightweight, they allow you to easily power all of your appliances, without causing a disruption on your quiet getaway.\nWhat About Conventional Generators?\nAs you can see, inverter generators are a better option if the noise level is more of a priority than overall power. That’s not to say the inverter generator isn’t powerful, but they’re not designed to handle major jobs like a conventional generator is.\nConventional style generators definitely still serve a purpose and work for a wide range of applications such as running power tools on a construction site or for the use of other types of industrial appliances. However, these generators tend to easily produce over eighty decibels of sound. Yet, they’re cost effective and low maintenance if you need a generator you can rely on to power up an entire house in the event of a power outage, while the inverter generator will only be able to power one large appliance or a few sensitive electronics.\nPower or Noise Rating\nIf you need a generator you can use for major jobs, such as running large equipment or power tools, or as a source for emergency power, then a conventional generator is the best choice.\nThe inverter generator shines in terms of portability and quiet running operation. These are the generators you can use in public places, campsites, federal campgrounds, and so on because they will not cause a disturbance during use like a conventional generator can. However, they aren’t powerful enough to rely on for big jobs. Instead, they’re a better option if you need a generator for use during a camping trip, caravanning, as an emergency power source that can keep your phones charged, or for use for public events when you’re far away from traditional power sources.\nThe Pulsar 2,000W Portable Gas-Powered Quiet Inverter Generator has a noise rating of just fifty-nine decibels. It also weighs just under forty-seven pounds and features a compact suitcase design that makes it easy to transport.\nWhy We Love it:\n- Low noise rating\n- Two thousand peak watts with sixteen hundred running watts\n- Compact suitcase design\n- Eight hours of runtime\n- Parallel capable\nWhat We Didn’t Love:\n- Difficult assembly\nTo learn more about this generator, click the button below.\nWhat is One of the Leading Quietest Inverter Generators on the Market?\nThe Wen PowerPro 56101 Inverter Generator features a sixty-decibel rating at a moderate power level, which is better than average. If you’re looking for a particularly quiet inverter generator, then paying close attention to each model’s DB rating is essential. Of course, you’ll want to learn about the generator’s runtime in addition to the decibel rating at different engine speeds.\nTo learn how to pick out the perfect camp generator for your next trip, click here to read our buyer’s guide.\nHow Noisy is 60 Decibels?\nSixty decibels is pretty quiet and comparable to the noise of an air conditioner, a conversation in a public place, and the noise level in a retail store. The top-selling inverter generators typically have a sixty-decibel noise rating, when they’re running a fifty to eighty percent load. At the lowest setting, many models have a decibel rating of just forty-seven to fifty decibels.\nWhich is Better, Solar Panels or Inverter Generators?\nThe HQST 100 Watts 12 Volts Polycrystalline Solar Panel Off-Grid RV and Boat Kit is one of the best-selling solar panel kits for RVs and produces enough power to run most of your appliances unless you run into two or three consecutive cloudy days. Camp generators, on the other hand, can produce power anytime, anywhere, because their operation isn’t based on weather conditions like solar panels are. However, you cannot run a camp generator in the rain. With this in mind, you may want to consider purchasing both a solar panel kit and an inverter generator for your next RV road trip.\nTo learn more about RV solar panel kits, click here to read our buyer’s guide.\nCan Inverter Generators be Used at Campsites?\nYes, they can! However, make sure you check out the rules of any campsite before you bring your generator along. Most campsites have strict noise rules in place, such as noise should not emit beyond an immediate campsite. Each campsite will also have specific quiet hours, usually ranging from ten at night until six the next morning. The strictest campsites will have specific hours of operation in place for inverter generators, although this time restriction will vary from site to site.\nNow you know why inverter generators are the go-to style of generators for most outdoor events, especially in public places. The generator’s lightweight design allows you to easily transport, set up, and haul the generator, but these machines really shine in terms of noise level.\nWhat is quiet generator DB levels? An inverter generator that has a noise rating ranging from forty-seven to seventy decibels. A generator’s noise level rating can vary from model to model and can also be dependent on what you’re using the generator for. But even the loudest inverter generator is much quieter than a conventional generator. These models utilize inverter technology for cleaner energy, consistent power, and quiet operation."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:92e9a96a-2445-4c53-b43c-502c3e6fdf54>","<urn:uuid:c98a5da5-a98a-488d-a582-89244a7da112>"],"error":null}
{"question":"What strategies are being developed to reduce antibiotic use in aquaculture versus poultry production?","answer":"In aquaculture, researchers are developing probiotic solutions as an alternative to antibiotics. Specific probiotic bacterial species, S4Sm and RI06-95Sm, have been shown to colonize fish and reverse antibiotic effects by restoring microbiome diversity and providing colonization resistance against pathogens. In the poultry industry, the approach has been more market-driven, with major buyers like McDonald's demanding antibiotic-free poultry. While individual chicken farmers face competitive risks in eliminating antibiotics without contracted buyers, the industry is gradually shifting away from antibiotic use in response to consumer demands for 'green,' 'organic,' and 'humane' products.","context":["Over the last several decades the growth of the aquaculture industry has accelerated. Aquaculture can be defined as the farming of aquatic organisms by intervening in the rearing process which results in the enhancement of production. Aquaculture has helped to increase the production of food as well as increase the availability of aquaculture byproducts in the pharmaceutical industry.\nResearchers from Brown University and the University of Rhode Island suggest a probiotic solution for deaths associated with prophylactic use of antibiotics in fisheries. Credit: Aquaculture Coalition\nOutbreaks in aquaculture associated with viral, bacterial and fungal infections have resulted in devastating economic losses worldwide. Control of aquatic diseases has relied mainly on the use of chemical additives and antibiotics. Unfortunately, the increased use of antibiotics has had significant public health impacts by promoting the persistence of antibiotic-resistant bacteria in the environment.\nA recent study published in mSystems, discusses the effects of antibiotic use on the aquaculture microbiome. Use of antibiotics in mammals, particularly humans and mice, has been found to disrupt the host microbiome by decreasing microbial diversity. Decreasing microbial diversity reduces the number of “good” bacteria in the microbiome which can help fight off the “bad” bacteria, also known as “colonization resistance”. Therefore; repeated antibiotic treatment can lead to recurrent drug-resistant infections.\nInterestingly, previous studies have shown that transferring healthy microflora from the intestine of a healthy individual to the intestine of an unhealthy individual can help to restore the diversity of the host microbiome and prevent infection in the unhealthy individual. Until now, there has been very little research on colonization resistance in aquaculture. Scientists have hypothesized that the use of probiotics could help reduce the number of antibiotic-associated infections in aquaculture as well.\nIn this study, researchers determined the effects of antibiotics on the mortality of black mollie fish, which were selected due to their ability to live in a wide range of environmental conditions and ability to eat a variety of different diets. Fish were acclimated to sterilized seawater and placed into 4 treatment environments: (1) probiotics only; (2) no treatment (control), (3) antibiotics plus probiotics and (4) antibiotics only. Antibiotics were administered daily from days 1-13 and probiotics were administered daily from days 8-13. All 4 treatment groups were challenged on day 13 with a known aquaculture pathogen, Vibrio anguillarum, to determine the effects of the various treatments on mortality rates. After 48 days, microbial communities within each fish tank were collected and assessed using whole genome sequencing.\nThe authors of the study found that antibiotic treatment significantly increased black mollie mortality. They were also able to identify two specific probiotic bacterial species able to colonize black mollie and able to reverse the effects of the antibiotics. These two probiotic species, S4Sm and RI06-95Sm, were resistant to antibiotics and could help fight off the challenge bacteria through colonization resistance.\nAuthors of this study suggest that use of probiotics can help to restore aquaculture microbiome diversity, particularly following treatment with cycles of antibiotics. Use of probiotics may also help boost the number of “good” bacteria in the microbiome which can help fight off infection.","Arkansas, Alabama among top three poultry producers\nBy Jacque Kochak\nA few years ago, an urban legend circulated on the Internet, claiming that KFC’s chickens are really genetically manipulated organisms without feathers to be plucked or feet and beaks to be removed. Of course the story has been debunked, but the tale is a testament to how little people know about the origin of their food.\n“The population at large is almost fully divorced from animal agriculture and food production. They need to be educated so they’re not shocked when they learn how food is produced,” says Dr. Sarge Bilgili, an Auburn University poultry science professor and a specialist with the Alabama Cooperative Extension System. He says his own young daughter was devastated to learn that a chicken had to be killed before the bird ended up fricasseed on her dinner plate.\nIn the interest of educating eaters, here are 14 facts you probably didn’t know about the poultry industry: companies are more profitable. The individual chicken farmer who eliminates antibiotics without a contracted buyer is taking a big competitive risk. Yet public concerns are high that use of antibiotics in animals has contributed to the rise of antibiotic resistance in human pathogens.\n1. Big business. Poultry is big business in both Arkansas and Alabama. Broilers (poultry raised for their meat) are the top agricultural product in both states, in 2017 accounting for $3.8 billion in sales in Arkansas and $3.3 billion in sales in Alabama, according to USDA. Hens that produce eggs are called “layers.” This differentiation started in the 1920s and 1930s, when farmers noticed that some chickens were better for laying eggs while others produced more meat.\n2. Tops in the biz. Alabama and Arkansas are two of the top three broiler-producing states in the U.S. (the other top state is Georgia). In both states, the poultry industry is concentrated in the hilly northern counties. The rocky terrain of the Ozarks in Arkansas and the Appalachian foothills in Alabama limited widespread cultivation of cotton or other crops, so poultry production offered a way for hardscrabble farmers to earn a living.\n3. Back in the good old days. The normal mortality rate for poultry flocks was around 40 percent in the early 1900s when most poultry was raised in somebody’s backyard, according to a United Egg Producers timeline. Today, according to the National Chicken Council, mortality of broilers is about 3.8 percent. Before the 1920s, there was no broiler industry and people raised chickens mostly for their eggs. The chicken that went in the stewpot was usually a tough old hen no longer laying eggs.\n4. Life in the wild. Life in the wild wasn’t actually all that idyllic for chickens. You’ve heard of a “pecking order,” and the term comes from chicken behavior. More dominant, aggressive hens peck at other hens and keep more timid hens from getting to feed. A hen starts pecking and pulling at the feathers of other hens, which sometimes leads to serious injuries and cannibalism. Part of the reason egg-laying chickens are caged is to control this behavior. Broilers, however, do not reach sexual maturity and don’t engage in this kind of aggression because they are too young to have formed a dominance hierarchy.\n5. Moving indoors. The advent of specially designed indoor housing (which allows controlled feeding and protects a flock from predators and parasite infestations) was part of the reason chicken mortality dropped dramatically. Another reason was avian medications developed at researchers like those at Auburn and the University of Arkansas. Different species were cross-bred to create faster-growing, fuller-breasted birds, and over the years broiler production evolved into a major industry as consumers started to like the taste of young, specially bred birds.\n6. No cages. Animal rights activists often publish photos of chickens crowded into cages, but young meat chickens are not raised in cages. They spend their lives in large, open structures known as grow-out houses. The houses are equipped to deliver feed and water, as well as having ventilation systems and heaters. The house’s dirt floor is covered with bedding material (such as wood chips, rice hulls, peanut shells) called “litter.” Some houses have curtain walls that can be rolled up in good weather to admit sunlight and fresh air. A screen keeps the insects, rodents and wild birds out, because they are vectors to transmit disease from other sources.\n7. But flocks are large. According to the National Chicken Council, a typical broiler flocks includes about 20,000 birds, kept in a space that measure 400 feet by 40 feet. The Council for Agricultural Science and Technology (CAST) says that the minimum space that should be allocated for each bird is at least half a square foot, although the typical grow-out house has about eight-tenths of a square foot for each chicken.\n8. Tyson and vertical integration. In 1943, an Arkansas poultry farmer named John Tyson had a role in another significant industry innovation. At that time, poultry farmers operated pretty much on their own, with hatcheries producing chicks and feedmills producing poultry feed. Tyson took the first steps toward “vertical integration” when he started raising his own chicks and producing his own feed. By the 1960s, some 90 percent of broilers came from fully integrated operations, with the poultry producer controlling virtually every aspect of production, from hatchery to retail sales of broilers. By the way, Tyson’s company is now known as Tyson Foods, the world’s second largest second largest processor and marketer of chicken, beef, and pork.\n9. Contract farmers. Today, most U.S. broiler production is under contract with a broiler processor. The grower normally supplies the poultry house with all the necessary heating, cooling, feeding, and watering systems. The grower also supplies the labor needed in growing the birds. The broiler processor supplies the chicks, feed, and veterinary medicines. Farmers have always operated with a slim profit margin, and this arrangement gives them a stable market. The benefit to consumers is that vertical integration results in lower prices and a better supply. Processors often add value by taking raw chicken and deboning it, marinating it, cutting it into pieces, pressing it into patties, rolling it into nuggets, breading it, battering it, or cooking it and freezing it.\n10. Antibiotics. Although change can be slow, the poultry industry does respond to consumers, who more and more say they want “green,” “organic,” and “humane.” The industry is especially responsive to desires of large buyers like fast-food giant McDonald’s. When McDonald’s and other fast food chains wanted antibiotic-free poultry, they got antibiotic-free poultry. McDonald’s, in turn, was responding to consumer concerns about the use of antibiotics as a growth-promotant in agricultural animals. Low levels of antibiotics in feed modify the gut environment to utilize nutrients more efficiently, so animals grow more quickly and poultry companies are more profitable. The individual chicken farmer who eliminates antibiotics without a contracted buyer is taking a big competitive risk. Yet public concerns are high that use of antibiotics in animals has contributed to the rise of antibiotic resistance in human pathogens.\n11. Free range. Another example of the poultry industry’s sensitivity to niche markets is the growth of farms that provide “free-range” poultry, raised with access to the outdoors. Typically, free-range birds stay inside at night for protection from the elements and predators. Free-range chicken meat costs quite a bit more than the standard chicken on sale at the grocery store. This is because the husbandry system is labor intensive, and feed intake is usually greater—especially in cold wintry months—because the environment is less controlled. That’s an acceptable trade-off for consumers concerned about laying hens closely confined in pens, however.\n12. Organic. Just so you will know, the feed of “organic” poultry does not contain most conventional pesticides, fungicides, herbicides or commercial fertilizers. The feed is vegetarian and must be USDA-certified as “organic.” This label does not mean chickens are not caged. Be very wary of anybody advertising “hormone-free” poultry. It is illegal to use any kind of hormones in U.S. poultry production, and such a label is misleading because it implies other poultry products may contain hormones.\n13. Challenges. Auburn and the University of Arkansas boast two of the few remaining poultry science departments in the country, and scientists at both schools have been responsible for numerous advances benefiting the industry. Likewise, they will help poultry farmers and processors solve some of the challenges of the future. One of the industry’s biggest challenges is controlling the incidence of Salmonella, a common foodborne pathogen. Another challenge is solving the environmental concerns caused by poultry-waste runoff and disposal of litter, as well as controlling the ammonia fumes that can damage chickens’ lungs in big poultry houses. Researchers are also addressing consumer concerns about animal welfare.\n14. Ever-increasing consumption. In 1992, chicken surpassed beef as American’s most-consumed meat, and in 2018 it is estimated the average American will eat more than 90 pounds of chicken per capita. Poultry consumption has increased dramatically since 1965, when Americans ate almost 30 pounds per capita. This is partly because consumers perceive chicken as a healthy alternative and partly because the cost is attractive as a result of supply increasing so dramatically in the last hundred years. This is despite the fact that most people no longer live on farms and raise their own chickens (although recently backyard chicken flocks are increasing in popularity). A final challenge to the industry is to find ways to keep the cost of chicken affordable while addressing consumer concerns. Research at both the University of Arkansas and the University of Alabama will help."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:49eb75f8-7a89-423c-9380-33f29a49b51d>","<urn:uuid:df8f3723-5190-44b1-b094-dc087f626794>"],"error":null}
{"question":"How do phase separation mechanisms differ between cemented carbides and architected materials in terms of their failure behavior?","answer":"In cemented carbides, phase separation occurs through discontinuous precipitation (DP) or spinodal decomposition (SD), with strain energy being a key factor. For example, in (Ti,Zr)C systems, despite having a miscibility gap that should lead to SD, the material undergoes DP instead. In contrast, architected materials follow classical continuum mechanics for failure prediction, exhibiting a dual discrete-continuum nature. Their failure can be modeled as a continuous mass rather than individual particles, making their behavior more predictable compared to the complex phase separation mechanisms in cemented carbides.","context":["Phase field modeling of precipitation reactions in miscibility gap systems\nTime: Fri 2022-10-14 13.00\nLocation: Kollegiesalen, Brinellvägen 8, Stockholm\nSubject area: Materials Science and Engineering\nDoctoral student: Deepjyoti Mukherjee , Strukturer\nOpponent: Professor Benoît Appolaire, Université de Lorraine\nSupervisor: Professor Joakim Odqvist, Strukturer; Docent Henrik Larsson, Strukturer\nAs a backbone to the cutting tools and rock drilling industry, cemented carbides have been used widely due to their high hardness and wear resistance. Most commercially used cemented carbides contain a hard phase made of Tungsten carbide (WC) and a binder phase which is generally ductile. Lately, a secondary hard phase is desired by replacing W in WC partially with one or more metal substitutes such as Ti, V, Ta, Cr and Zr to better its mechanical properties. Some of these carbides are observed to exhibit unusual microstructures during ageing. For example, (Ti,Zr)C present along with WC, has been observed to undergo phase separation from a supersaturated phase, called $\\gamma$, to Ti-rich and Zr-rich domains leaving behind an array of precipitates morphologically manifesting as lamellae. This phase separation process has been termed as discontinuous precipitation(DP) as it resembles the classical DP reaction observed in certain binary and multi-component systems. The behaviour comes from the presence of a miscibility gap in the carbide Ref. Borgh et al. 2014 and Ma et al. 2016 due to which the usual response of the system should be to undergo spinodal decomposition(SD), however, the carbide chooses a different path which questions the governing mechanism behind its decomposition process. Several factors are believed to affect such a process and one of such factors is the strain energy, which is generated due to the difference in lattice parameters of the separating phases. When compared to a different miscibility gap system, such as Fe-Cr, where the strain energy is quite low and the response is SD. Other factors such as grain boundary diffusion, atomic mobility, and gradient energy coefficient (κ) are also believed to have an effect on the decomposition process. Therefore, a thorough investigation of the factors is required and, a powerful tool to study the spatio-temporal evolution of the microstructure such as the phase field method, should be used.\nAccording to some experiments the lamellae are generally observed to nucleate at grain boundaries and later grow with the help of grain boundary migration Ref. Borgh et al. 2014. The moving grain boundary leaves behind a series of alternate strands of Ti and Zr rich phases. The growth mechanism behind the moving boundary is believed to be assisted by diffusion of solutes along the grain boundary and generation of the elastic strain energy by it. The phenomenon is commonly known as diffusion induced grain boundary migration(DIGM) and it is believed to be a key part of DP Ref. Hillert and Purdy 1977 and Chongmo and Hillert 1981. In order to recreate DIGM and DP an energetic coupling between the mole fraction and phase field variable is required so that it accounts for the generated strain energy during the process. The main focus of this thesis will be to develop a phase field model accounting for such coupling which will predict DIGM in binary systems and use it further as a medium to model DP in (Ti,Zr)C. The model for DP could be used to predict and control its formation as its occurrence prone to increase the hardness of the carbides. Therefore, it can be used as a tool to design alloys and develop better alternatives. The alternatives could be used to prevent DP in the carbides which could be done by using different metal substitutes that will prefer SD over DP or vice versa.","What's the discovery?\nJulia Greer, professor of materials science, mechanics and medical engineering in Caltech's Division of Engineering and Applied Science, creates materials out of micro- and nano-scale building blocks that are arranged into sophisticated architectures that can be periodic, like a lattice, or arbitrary. Described as \"architected materials,\" they sometimes exhibit unusual properties. For example, Greer has created ceramics with foam-like recoverability, lightweight but ultrastrong frames that can bounce back after compression, and mechanically robust batteries.\nWorking with Yong-Wei Zhang of the Institute of High Performance Computing in Singapore, Greer has determined that the failure of architected materials—the point at which they break when compressed or stretched—can be described using classical continuum mechanics, which models the behavior of a material as a continuous mass rather than as individual (or \"discrete\") particles.\nThis finding implies a duality to the nature of these materials—in that they can be thought of both as individual particles and also as a single collective. Greer and Zhang's findings were announced in a paper published by the journal Advanced Functional Materials on December 13.\nWhat has been the problem so far?\nArchitected materials are interesting to engineers because of their often unusual properties, but their behavior can be difficult to predict. It is impossible to know how they will respond to stress until they are created in a lab and tested. As such, the creation of these materials has been largely trial and error: researchers would dream up new lattice structures and then crush and stretch them to see how strong they were. While this process has led to some interesting discoveries, being able to predict how a given lattice will perform under pressure before actually building it would make it easier for engineers to create purpose-built materials.\nHow did the team demonstrate the dual nature of architected materials?\nThe team fabricated a lattice of hollow, 50-nanometer-thick aluminum oxide beams, and then conducted \"failure\" tests: they placed the lattice under tension and recorded when and how it cracked. The tests showed that the material has a strength-over-density ratio, or \"specific strength,\" that is four times higher than that of any other reported material to date.\nImportantly, the failure tests allowed the team to create a theory for how architected materials fail in general.\"This new analysis gives us a very powerful approach to designing new materials that are particularly resistant to damage and tearing while maintaining exceptionally low weight,\" Greer says.\nWhy does this matter?\nUnderstanding when and how a material fails is critical if it is to be useful in real-world applications, where it would never be pushed beyond the failure point. Such information allows for the creation of new materials that are lighter and stronger than any yet produced—and that will fail in simple, predictable ways. By contrast, many conventional (that is, non-architected) materials fail suddenly and in ways that can be difficult to foresee and describe, Greer says.\nThe paper is titled \"Discrete-Continuum Duality of Architected Materials: Failure, Flaws, and Fracture.\" Other co-authors are former Caltech graduate student Arturo Mateos (MS '14, PhD '18) and Wei Huang of the Institute of High Performance Computing. The research was supported by the U.S. Department of Defense through Greer's Vannevar Bush Faculty Fellowship and the Army Research Office through the Institute for Collaborative Biotechnologies."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c44e7168-6b88-4358-9144-1d5f7579a5ce>","<urn:uuid:3f68bcab-7d5f-472b-89b7-c1d9e36351fd>"],"error":null}
{"question":"What's the main difference between gaming and server-focused memory modules?","answer":"According to the documents, server-focused memory modules, like SK Hynix's new DDR5 with 48 GiB and 96 GiB capacities, are initially targeted at high-performance servers for machine learning and HPC situations, with ECC and RDIMM designs. Gaming-focused memory, such as DDR4 3200, emphasizes faster clock speeds and tighter timings for improved gaming performance, focusing on faster load times and smoother gameplay. Server modules typically appear in the market first as that sector can absorb the early adopter costs of new memory technology.","context":["SK Hynix to Manufacture 48 GiB and 96 GiB DDR5 Modulesby Dr. Ian Cutress on December 14, 2021 8:37 PM EST\nToday SK Hynix is announcing the sampling of its next generation DDR5 memory. The headline is the commercialization of a new 24 gigabit die, offering 50% more capacity than the leading 16 gigabit dies currently used on high-capacity DDR5. Along with reportedly reducing power consumption by 25% by using SK Hynix’s latest 1a nm process node and EUV technology, what fascinates me most is that we’re going to get, for the first time in the PC space (to my knowledge), memory modules that are no longer powers of two.\nFor PC-based DDR memory, all the way back from DDR1 and prior, memory modules have been configured as a power of two in terms of storage. Whether that’s 16 MiB to 256 MiB to 2 GiB to 32 GiB, I’m fairly certain that all of the memory modules that I’ve ever handled have been powers of two. The new announcement from SK Hynix showcases that the new 24 gigabit dies will allow the company to build DDR5 modules in capacities of 48 GiB and 96 GiB.\nTo be clear, the DDR5 official specification actually allows for capacities that are not direct powers of two. If we look to other types of memory, powers of two have been thrown out the window for a while, such as in smartphones. However PCs and Servers, as least the traditional ones, have followed the power of two mantra. One of the changes in memory design that is now driving regular modules to non-power of two capacities is that it is getting harder and harder to scale DRAM capacities. The time it takes to figure out the complexity of the technology to get a 2x improvement every time is too long, and memory vendors will start taking those intermediate steps to get product to market.\nIn traditional fashion though, these chips and modules will be earmarked for server use first, for ECC and RDIMM designs. That’s the market that will absorb the early adopter cost of the hardware, and SK Hynix even says that the modules are expected to power high performance servers, particularly in machine learning as well as other HPC situations. One of the quotes on the SK Hynix press release was from Intel’s Data Center Group, so if there is any synergy related to support and deployment, that’s probably the place to start. A server CPU with 8x 64-bit channels and 2 DIMMs per channel gives 16 modules, and 16 x 48 GiB enables 768 GiB capacity.\nAs to when this technology will come to the consumer market, we’re going to have to be mindful of cost and assume that these chips will be used on high-cost hardware. So perhaps 48 GiB UDIMMs will be the first to market, although there’s a small possibility 24 GiB UDIMMs might make an appearance. Suddenly that 128 GiB limit on a modern gaming desktop will grow to 192 GiB.\nSource: SKHynix Newsroom","When it comes to DDR4 memory, there are two common types that are often compared: DDR4 2666 and DDR4 3200. These two types of memory have different speeds and can affect the overall performance of a computer. In this article, we will explore the differences between DDR4 2666 and DDR4 3200 and which one would be a better choice for your needs.\nDDR4 2666 vs DDR4 3200: What’s the difference?\nThe key difference between DDR4 2666 and DDR4 3200 is their clock speeds. DDR4 2666 has a clock speed of 2666MHz, while DDR4 3200 has a clock speed of 3200MHz. Clock speed refers to the speed at which data is transferred from the computer to the memory and back.\nAnother difference between the two types of memory is the timings. DDR4 3200 has tighter timings than DDR4 2666, which means that it can move data more quickly. Tighter timings generally translate to better performance, especially in applications that require a lot of memory access, such as gaming and video editing.\nApart from the above, DDR4 3200 also facilitates overclocking of the memory which enables to achieve even faster speeds than the native 3200MHz as compared to DDR4 2666.\nDDR4 2666 vs DDR4 3200: Which is better for gaming?\nWhen it comes to gaming, faster clock speeds and tighter timings can lead to better performance. In general, DDR4 3200 is better for gaming than DDR4 2666. This is because it can move data more quickly, which results in faster load times and smoother gameplay.\nFor gamers who are looking to build a high-performance system to play the latest games, DDR4 3200 would be the better choice. However, for gamers who are on a budget, DDR4 2666 could be a good option.\nDDR4 2666 vs DDR4 3200: Which is better for video editing?\nVideo editing is another task that requires a lot of memory access. When it comes to video editing, DDR4 3200 is generally better than DDR4 2666. This is because tighter timings can result in faster rendering times and shorter import/export times.\nFor professional video editors who work with high-resolution footage, DDR4 3200 would be the better choice. However, for casual video editors, DDR4 2666 would be sufficient.\nDDR4 2666 vs DDR4 3200: Which is better for everyday use?\nFor everyday use, the difference between DDR4 2666 and DDR4 3200 may not be noticeable. Both types of memory provide fast and reliable performance, so the choice comes down to personal preference and budget.\nIf you use your computer for browsing the web, streaming videos, and other basic tasks, DDR4 2666 would be sufficient. However, if you use your computer for more demanding tasks, DDR4 3200 could provide a noticeable performance boost.\nIn conclusion, DDR4 2666 and DDR4 3200 have different clock speeds and timings that can affect the overall performance of a computer. DDR4 3200 is generally better for tasks that require a lot of memory access, such as gaming and video editing. However, for everyday use, DDR4 2666 would be sufficient. The choice between the two comes down to personal preference and budget.\nKeywords: DDR4, DDR4 2666, DDR4 3200, clock speeds, timings, gaming, video editing, performance, everyday use, budget."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:4d8e4c19-6016-46a3-8be5-6320328ec9f4>","<urn:uuid:aa6d9f87-3d88-4c98-b854-f919b005b331>"],"error":null}
{"question":"How do you grow fennel in a container?","answer":"To grow fennel in a container, choose a sunny spot and use well-draining, acidic soil with pH 5.5-6.8. Sow seeds directly in the growing pots as fennel doesn't like being transplanted. Keep the soil moist and feed every 3 weeks with liquid organic fertilizer. Use mulch like grass clippings to retain moisture. The plant can grow 18-24 inches tall, so ensure adequate space.","context":["Otherwise, fennel isn’t a great companion plant for other plants in your garden in a traditional sense. We aim to enrich everyone’s life through plants, and make the UK a greener and more beautiful place. Harvesting. Plant Spacing. Plant the fennel outdoors. Harvest leaves as required from spring to autumn. It grows so large and aromatic that it can sometimes overpower nearby plants. Outdoors, in containers (sow directly in growing pots, do not transplant), and hydroponic cultures. Mon – Fri | 9am – 5pm, Join the RHS today and support our charity. Notes. Or they are chewed after a meal to help the digestion. Remedy: Use your finger and thumb to squash aphid colonies or use biological control in the greenhouse. However, growing fennel in containers must be done properly in order to have a successful harvest. It prefers a sunny spot in light, free-draining soil, and requires little maintenance apart from the removal of dead stems at the end of the growing season. Seed ca… In fact, this year is my first year growing fennel. Growing fennel in water is as easy as that. Fennel is a drought tolerant herb once it is established. Fennel is popular in Italian and other Mediterranean recipes. Feed every 3 weeks with a liquid organic fertiliser, and use a rich, biodegradable mulch such as grass clippings. Native to southern Europe and Asia Minor, fennel is cultivated in temperate regions worldwide. The Royal Horticultural Society is the UK’s leading gardening charity. However, the herb form is grown as a perennial, making a long-lived plant that is covered in pretty yellow flowers in early summer. If you have a heavy or clay soil, you can add in sand to help it drain. The flowers are attractive to a range of beneficial insects. Like their aforementioned cousins, the fennel herb has a long tap root that doesnt like to be divided or moved. Fennel usually grows to a height of 18 to 24 inches (45 - 60cm). Munch on raw sliced fennel instead of celery. You can also sow in mid summer for a fall harvest. Eat it raw. It prefers a sunny spot in light, free-draining soil, and requires little maintenance apart from the removal of dead stems at the end of the growing season. If you don’t have a lot of space in your garden, don’t worry – you can easily grow fennel in a container. The seeds are also used in desserts, breads, other baked goods and drinks. 1. It’s a great option for growing in raised garden beds, containers, and... Space fennel plants 4 to 12 inches apart, depending on the variety. Join the RHS today and support our charitable work, Keep track of your plants with reminders & care tips – all to help you grow successfully, For the latest on RHS Shows in 2020 and 2021, read more, RHS members get free access to RHS Gardens, Free entry to RHS members at selected times », Reduced prices on RHS Garden courses and workshops, Our Garden Centres and online shops are packed with unique and thoughtful gifts and decorations to make your Christmas sparkle, General enquiries\nSaute or braise sliced fennel to mellow out the flavor. Planting Fennel 1. Fennel has two main types – common fennel, and florence fennel – which are not to be confused. If so grow the fennel in a peat pot, but do not expect all the plants to survive. In just a few days, you should see new green shoots growing up from the base. The plant can be cut back after flowering to encourage new, bushy growth. Fennel hates having its roots being disturbed or being transplanted, so sow in-situ, either into pots or the ground where it is to grow. Water in well and keep moist. The leaves can be used in place of herb fennel. Seeds can be used fresh over the summer months or dried for winter use. http://learn-how-to-garden.comMy film is about how to start fennel off and how you grow it on. Sow one seed per module, as the seedlings don’t like having their roots disturbed when transplanted. A newer crop in the U.S. is bulb fennel, with a vaguely licorice-like flavor. The plant is a perennial that can survive winters between growing zones 5 and 10. Sweet fennel plant is the older of the two, and even now is barely domesticated –California residents are undoubtedly familiar with the invasive nature of wild fennel. After a little more time, new roots should start to sprout from the base of your fennel. Growing Cultures. Sow in early spring in small pots or modules under cover, or in mid-spring direct where it is to grow. For now, feel free to continue reading. Growing Fennel in a Container or Indoors. Before you plant fennel, prepare its soil. The seeds should be planted... 3. It can be direct seeded into your garden after your last frost. Baked Haddock with Roasted Tomato and Fennel, Growing Kohlrabi: The Complete Guide to Plant, Grow, & Harvest Kohlrabi, Growing Almonds: Best Varieties, Planting Guide, Care, Problems and Harvest, Growing Cranberries: How to Grow, Plant and Harvest Cranberries, Growing Peppers: Planting Guide, Care, Problems and Harvest, Betony Plant: Growing Guide, Care, Problems and Harvest, Growing Lettuce: How to Plant, Grow, and Harvest Fresh Lettuce, Growing Christmas Trees: Varieties, Planting, Care and Harvest, Growing Wheatgrass: How to Plant, Grow and Harvest This Nutritional Powerhouse, Growing Grapes: Varieties, Planting Guide, Care, Diseases, and Harvest, Growing Amaranth: The Complete Guide to Plant, Grow, & Harvest Amaranth, Growing Mesclun Greens: A Complete Guide to Planting, Care, Troubleshooting, and Harvest, Growing Wheat: Varieties, Planting Guide, Care, Problems, and Harvest, Growing Tamarillo: Best Varieties, Planting Guides, Care, Problems and Harvest, Growing Oats: Best Varieties, Planting Guides, Care, Problems and Harvest, Growing Mullein: Growing Guide, Care, Problems and Harvest, Growing Sweet Potatoes: The Complete Guide to Plant, Grow, and Harvest Sweet Potatoes, Growing Watermelon: Your Guide to Plant, Grow, and Harvest Watermelon, Erba Stella: Everything to Know About Growing This Unique Salad Green, Jicama Plant: Varieties, Growing Guide, Care, Problems, and Harvest, Growing Elderberries: A Complete Guide on How to Plant, Grow, & Harvest Elderberries. For this kind of planting method, you had better choose the fennel bulb. Plant it in the back of the herb garden or in your vegetable garden. Due to different plant characteristics among the two types of fennel plant, not all of them can be grown in containers. The refreshing flavor balances out the taste of meat. Fennel can be quickly grown in pots in a limited space and do not require a lot of maintenance. Plant Height. Today, I’m sharing what I have learned from years of reading and researching gardening, personal experience, and the like. Chop up fennel tops and add to salad dressing or use as a garnish. 222879/SC038262. Fennel are grown from seed. Plants may be divided, but this isnt as easy as it is with other garden plants and often proves unsatisfactory. Save There are two types of sweet fennel. 020 3176 5800\nThe herb fennel is closely related to the vegetable Florence fennel. Read: Growing Fruits Hydroponically. The fennel bulb is high in vitamin C, and is also a good source of calcium, fiber and potassium. Fennel Propagation. Some varieties of common fennel will not produce the fat bulb, so if you intend to harvest that as well you should plant Florence fennel. Starting from Seed Fennel Plant. The fennel type you select for growing will be completely based on the part of the fennel you desire to use like the seeds or the bulb or the fronds. I will have to plant it in a container. Native to southern Europe, the fennel herb is now naturalized throughout Europe, North America and Australia and grown in gardens all over the world. Fennel prefers well-drained, acidic soil with a pH between 5.5-6.8. Once you reach this stage, you have two choices. If you want to get an early start with Florence fennel, sow seeds into modules under cover. A wild profusion of the non-native plant is prevalent along the coastline. The process for planting both varieties of fennel is the same. All parts of the plant are used in flavoring, and the bulblike stem base of the Florence fennel … Keep the soil around plants weed free and earth up around the bulbs during the growing period to make them sweet and white. Growing fennel from seed. Umbelliferae ... Our Garden Planner can produce a personalized calendar of when to sow, plant and harvest for your area. I picked up one bulb and sadly, it has no home. It’s crunchy and fresh-tasting and a delightful snack if you don’t hate the strong anise-flavor. Stay tuned for the first newsletter in the morning, straight to your inbox. Fennel and other Foeniculum should be grown in a soil that has a pH of 6 to 8 and good drainage. You may prefer to start growing fennel indoors, to plant outside in the spring or autumn. Florence fennel is grown for its swollen bulb and eaten as a vegetable, whereas common (or sweet) fennel is a tall (up to 180cm) perennial herb grown for its seeds and feathery foliage and is often used as a substitute for dill. Also called Florence Fennel or Finuccio, it is easy to grow and very hardy, lasting well after the first frost. It is a hardy, perennial herb with yellow flowers and feathery leaves.It is indigenous to the shores of the Mediterranean but has become widely naturalized in many parts of the world, especially on dry soils near the sea-coast and on riverbanks.. Although fennel is a hardy spring vegetable plant, you would do well to plant it outdoors only once all danger of frost has passed in your zone. Save The second is Florence or bulb style, which produces crisp celery-like bulbs bursting with anise-like flavor in addition to fragrant foliage and seeds. Foliage and seeds have an anise-like flavor. Choose a fennel variety. With bright green, fern-like leaves and aromatic yellow flowers, this plant will grow three to four feet tall. Every part of the plant has a distinctive, aniseed-like scent and taste, and can be used in salads and cooking; particularly with fish. Germination requires a temperature of around 68℉. Fennel is easy to grow. Plantation of fennel: In the first place, you will need to select a variety of fennel to grow. If you would like to grow fennel for its leaves only, choose sweet, leaf, or bronze fennel. Remedy: There are many ways to control slugs and snails, including beer traps, sawdust or eggshell barriers, copper tape and biocontrols. Plant out pot-grown plants from early sowings in late spring and early summer. Caring for and Harvesting Fennel in the Garden Be sure to take its eventual size into account at planting time so it doesn't shade the rest of your vegetables. Otherwise, you may choose the bulb fennel. Fennel Growing Guide Crop Rotation Group. Fennel leaves can be harvested at any time they are available. Also, it can inhibit the growth of tomatoes and beans , so avoid planting them near either of those crops. When the bulb is the size of an egg, surround it with 6 inches of loose mulch to help it plump up. The first is the common or herb type, Foeniculum vulgare, prized for the anise-like flavor of its feathery leaves and robust seeds. Fennel grows in zones 5-10. In my family, this vegetable has always been a holiday treat known by its Italian name, “finocchio.” Save W… RHS Garden Hyde Hall Spring and Orchid Show, Free entry to RHS members at selected\nFennel, perennial herb of the carrot family grown for its aromatic shoots, leaves, and seeds. Quick Guide to Growing Fennel Plant fennel in spring after the last frost. Growing Fennel in Water. The fennel plant prefers an acidic, well-draining soil. Fennel hates having its roots being disturbed or being transplanted, so sow in-situ, either into pots or the ground where it is to grow. They suck sap and excrete sticky honeydew, encouraging the growth of black sooty moulds. Disclosure. Slugs and snails: These feed on the young seedlings and you'll see the tell tale slime trail on the soil around your crop, as well as on the leaves. Sow seeds early … The type of fennel you choose to grow will depend on what part of the fennel plant you wish... 2. Soil and Sun Requirements. This flavorful herb c an be paired with a variety of dishes and herbal teas, making it one of the most used culinary herbs! If you are growing fennel for seeds, the plant will bloom in the second year because it is a perennial. Bronze fennel:Hardier than the species, and highly ornamental, bronze fennel (Foeniculum vulgare 'Purpureum') makes a great addition to a flower border or herb garden. Timing is important when growing fennel outdoors, especially if you want the plant to grow bulbs before bolting stunts bulb growth. You can easily grow fennel plant from seed by sowing directly into the soil or a container. Youll find two methods of propagation when researching how to grow fennel. Continue reading about Growing Fennel in Container!. Plant the fennel indoors. Fennel plants should be spaced between 9 and 12 inches (22-30 cm) apart. Dig up the garden bed to loosen its soil, since fennel needs plenty of drainage. Bulb fennel is perhaps a slightly misleading name, as the ‘bulb’ is in fact the swollen stem base of the plant. Planting. The best fennel type to be grown in containers. It is used as both an herb, due to its medicinal properties, and as a vegetable, which can be eaten raw, stewed, grilled, baked or boiled. Directly sow Fennel seeds into your garden as early in the season as the ground can be worked. Aphids: Look for colonies of greenfly on the soft shoot tips of plants or on leaves. Fennel can grow up to 5 feet tall, depending on which variety you're growing. Longtime organic gardener, Renee Studebaker, is in love with fennel, and her affection for the plant has made her a bit of an expert in growing it in the Central Texas area. Fennel (Foeniculum vulgare) is a flowering plant species in the carrot family. Feed every two to three weeks in summer with a high potash plant food. Growing fennel — bulb, herb, pollen - Many people grow fennel as the herb, for leaves and seeds, for salads, soups, fish dishes and teas. times, RHS Registered Charity no. The best spot for fennel to grow is toward the back of the garden, in its own garden bed, or in pots in your garden. Fennel is a drought tolerant herb once it is established. Planting Fennel Growing Zones. Fennel is a gorgeous Mediterranean perennial herb that does well in winter and early spring gardens in moderate climates. You can even grow it in a pot if you are short on space. The best time to sow fennel is in early to mid spring. Planting Fennel. Fennel is a common plant that thrives particularly well in the Mediterranean and some areas of North America, Asia and Europe. But fennel has more strings to its bow than this – pop chunks into a stew to freshen things up, or finely slice a bulb over a garden-gathered salad using a chef’s mandolin. Planting fennel by seed is the much easier option. In most climates, bulb fennel or finocchio can be grown twice a year, in spring and fall, on a growing and planting schedule similar to that of broccoli. Read more: For more cool ideas on gardening and growing your own produce, check out The Hungry Gardener ‘s website. Harvesting Herb fennel. Preferred pH Range Fennel is quite popular for its sweet, aromatic dill-like leaves, seeds, and bulbs. Fennel also goes well with other meats. Growing Fennel.\n2020 fennel plant growing"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:37968570-71ab-46e2-8044-f199b81dbf8b>"],"error":null}
{"question":"What are the cost-saving benefits of installing solar panels for homes, and what are the factors affecting their performance in different weather conditions?","answer":"Installing solar panels can lead to significant cost savings. The upfront cost ranges from $2500 for a 1.5 kW system to $11,500 for a 5 kW system, but states offer incentive schemes that can reduce installation costs substantially. For example, some energy suppliers provide rebates to solar installers, which are then passed to homeowners through reduced installation fees. As for performance, while solar panels do not generate power at night and require the home to draw power from the main grid, they can still work during cloudy weather, though with reduced electricity generation compared to clear, sunny conditions.","context":["If you are looking for ways to cut back on your household bills, you might be surprised to note that making a few changes around your home could make a big difference. By making your property more energy efficient, you can reduce your energy consumption, saving you a good chunk of money, and helping the planet in the process. Here are 6 ways to make your property more energy efficient.\nReplace your lightbulbs\nIf you don’t already use LED light bulbs, you could save a lot of money by switching. LED’s use around 10% of the energy of a traditional incandescent and around 85% less than halogen. Even compared to CFL (compact fluorescent lamp) bulbs, which were the first energy-efficient alternative, you will use a whopping 40% less energy. Depending on the size of your home, that could result in a pretty noticeable annual cost saving.\nSwitching lights off when not in use will also reduce the amount of electricity you consume, so it’s another worthwhile change. If you tend to forget to switch the lights off, another option is to add timers or have motion sensor lighting fitting.\nDouble glaze your windows\nIf your home generally feels quite cold, it could be down to heat escaping through your windows and cold getting in. This issue is usually more severe in older properties that have single glazed windows. Although it might not be cheap, switching to double glazing is something to consider.\nAir is removed from the two panes of window glass, creating a vacuum that acts as a natural insulator. This insulator stops the circulation of air and reduces convection, which in turn reduces heat loss across the window.\nMany double-glazed units are made with a layer of gas inside instead of a vacuum, which increases the insulation even further. If you live in a particularly windy area or an upper-level apartment, for example, triple glazing will provide an additional layer of protection against the elements.\nHave solar panels fitted\nSolar panels (sometimes referred to as PV panels) are produced by adding lots of photovoltaic (PV) cells to a metal glass cased panel. The panels then convert sunlight into electricity, which then powers your home. Solar panels are usually fitted on the roof of your property and, compared to a standard electricity supply, could save you a lot of money in the long term. This renewable source of energy is not only cheaper but far better for the environment.\nMany states run incentive schemes, which can bring installation costs down substantially. For example, Oregon solar incentives allow homeowners in the greater Portland area to lower the upfront costs associated with having a new energy system installed. Energy suppliers will give a rebate to the solar installer, which will then be passed to the homeowner by way of a reduction in installation fees. The level of rebate is likely to differ depending on how many panels you are looking to have fitted. It’s a great way to reduce your month to month costs and will help to increase the energy efficiency of your home.\nHas your boiler seen better days?\nBoilers can be expensive to replace but, if yours is fairly old, it is unlikely to be working as efficiently as it should be. This is likely to result in higher energy bills and an increased carbon footprint. A new boiler will be far more energy efficient and could save you a fairly hefty sum on your energy bills each year.\nNew boilers also tend to be smaller and far less noisy, so if it something you have been considering for a while, speak to a gas safety approved engineer and find out how a new boiler could benefit your household.\nMake things easier with a smart thermostat\nIf you don’t already have a smart thermostat, you don’t know what you are missing! If you regularly forget to turn your heating off or are sick of coming home to a freezing house, this could be the gadget you have been waiting for.\nA smart thermostat works by using an app that can be controlled from your phone regardless of your location. Timers can be set, heating can be switched on and off, and temperatures adjusted as you please. Most boilers will come with a thermostat that has a timer function; however, a smart thermostat takes the flexibility you have to another level.\nMany systems will also remember your heating preferences and can be set to automatically adjust the temperate of your home to suit your usual needs. Smart thermostats can be fitted retrospectively, which makes the switch a fairly simple one.\nInsulate your loft\nIf your loft isn’t insulated or the existing insulation is old or thinning, it is worthwhile having new insulation fitted. The good news is it’s a pretty simple task that most people can do on their own. The thicker the loft insulation the more energy and money you will save, so it’s worthwhile spending a bit extra and getting insulation which is at least 270mm thick. This thickness alone could save you upwards of 14% on your heating bills each year.\nLoft insulation is purchased in rolls that cover a certain square area, so calculate what you need and then buy the required thickness. New insulation can be placed directly on top of old, so there is no need to remove anything. It is quite fibrous, so wear protective gloves when handling the insulation and wear a mask over your mouth.\nIf you want to use your loft as a storage area, then you should use loft storage boards. These are pre-cut lengths of wood that are elevated on plastic feet and screwed into the joists. This will ensure that there is a gap between your insulation and your loft boards and will allow air to circulate. It is not a good idea to pack your loft full of belongings as this can affect the energy efficiency, so keep it to a minimum and, where possible, store your belongings elsewhere instead.","Frequently Asked Questions\nQ: How much does it cost to install solar panels?\nA: The price of solar systems has dropped dramatically over the past few years, making it an increasingly attractive option for homes and businesses.\nThe upfront cost of your solar PV system is affected by a number of different factors, including:\n- government incentives and support schemes available\n- contractor installation costs\n- type and number of solar panels, which affect the output of your system in kilowatts (kW)\n- type and size of inverter (the part of the system that converts the electrical output of your solar panels into AC electricity for use in your home or business)\n- type of framing equipment and other system components\n- height and accessibility of roof and whether it is tiled, metal or concrete\n- any after-sales service agreements\nAs a very rough guide, the total cost of getting a home solar system up and running is between $2500 (for a small 1.5 kW system) and $11,500 (for a top-of-the-line 5 kW system).\nFor businesses, the approximate cost of installing solar ranges from around $15,000 (for a 10kW system) to more than $200,000 (for a high-capacity 100 kW system).\nQ: How much money will I save with solar panels?\nA: The amount of money your household will save on power bills by going solar is affected by a number of factors, including:\n- Your energy consumption and the size of your solar power system – if you use more power than your system is capable of producing, your savings will be reduced. This can be avoided by choosing the right-sized system for your needs.\n- Your feed-in tariff – this is the amount your electricity retailer pays you for any excess power your solar panels generate.\n- Your usage patterns – solar panels can only generate electricity while the sun is shining. This means that households that use a lot of power during the day may attract greater savings than those that consume most of their power at night. However, you will still receive a feed-in tariff for any excess electricity you generate during the day.\n- Where you live – some areas of Australia receive a lot more sunlight than others, so a solar PV system in Brisbane will usually generate more power than one in Hobart.\nBusinesses have a couple of other things to take into account, including the tax implications of any revenue received from feed-in tariffs.\nQ: What is a solar feed-in tariff (FiT)?\nA: A feed-in-tariff is the amount your electricity retailer pays you for any electricity your solar PV system generates that you don’t use, and is instead fed back into the grid.\nQ: What is a small-scale technology certificate (STC)?\nA: STCs are government incentives that help reduce the upfront cost of installing your solar PV system. The value of STCs your system receives differs depending on its size and location.\nTo be eligible for STCs, your solar system must be installed by Clean Energy Council accredited installer.\nQ: How can I tell if my installer is accredited?\nYou can find an accredited installer in your area using our find an installer tool.\nRemember, to be eligible for government rebates in the form of STCs, your system must be installed by a Clean Energy Council accredited installer.\nQ: Where can I find a list of approved solar PV modules and inverters?\nA: The Clean Energy Council maintains a list of all solar modules and inverters that meet Australian Standards for use in the design and installation of solar PV systems. Only systems that use products from the approved lists are entitled to rebates in the form of small-scale technology certificates (STCs).\nPlease note that the Clean Energy Council does not certify modules and inverters directly. For a product to be included on our approved lists, the manufacturer must provide a certificate of compliance from a recognised certifying body.\nQ: Can I recycle my solar panels?\nA: Reclaim PV Recycling operates an Australian solar panel take back and reclaiming scheme throughout Australia.\nWhen you are buying your solar panels, check with your supplier whether they have a recycling program in place.\nQ: Is solar power safe?\nA: The Australian solar industry is well regulated and safe.\nSolar panels and inverters sold in this country must comply with a range of standards that maximise safety and reliability. The Clean Energy Council maintains a list of currently approved solar panel modules and inverters.\nThe Clean Energy Council’s Solar Accreditation scheme ensures that the people who design and install solar PV systems are across all the latest safety requirements. Accredited installers are qualified electricians who have undergone additional training and assessment in the installation of solar PV systems. Systems must be installed by a Clean Energy Council accredited installer to be eligible for small-scale technology certificates (STCs).\nInitiatives such as the Clean Energy Council’s Approved Solar Retailer scheme are also ensuring that the Australian solar PV sector stays safe and reliable.\nTo keep your system running safely and effectively for many years, you will need to maintain it correctly. See our after installing solar PV section for details on inspecting, maintaining and upgrading your system.\nQ: Do solar panels work at night or during cloudy weather?\nA: Solar panels do not generate power at night. Once the sun goes down, your home or business will start to draw power from the main grid as usual.Solar panels still work on a cloudy day; however they will not generate as much electricity as when the weather is clear and sunny.\nQ: What should I do if my solar PV system stops working?\nA: If your solar PV system is still under warranty, you should contact the retailer you purchased your system from to arrange repairs. If you bought from a Clean Energy Council Approved Solar Retailer, you can rest assured that every part of your system is covered under warranty for at least five years.If your system is out of warranty, you should contact your retailer or an accredited solar installer. However, you may be responsible for the cost of any repairs.For more information on what to do if your system stops working, refer to solar PV warranties, complaints and disputes."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:0aa1137a-e74e-4884-bdc3-e79f88f5f40c>","<urn:uuid:81b5328a-b0a4-45f1-91a0-5ad6da341126>"],"error":null}
{"question":"What is the history behind gravitational wave detection, and how do we identify potential supermassive black hole binary sources?","answer":"Einstein's general relativity theory in 1916 first predicted gravitational waves as ripples in space-time created by moving masses. The first indirect evidence came in 1974 when Hulse and Taylor discovered a binary pulsar system whose orbit decreased exactly as predicted by gravitational wave emission, earning them the 1993 Nobel Prize. For direct detection of supermassive black hole binaries, scientists use both electromagnetic and gravitational wave observations. They identify candidates by analyzing light curves from active galactic nuclei, looking for periodic brightness changes that suggest orbiting black holes. Currently, 149 such candidates have been identified through various catalogs. Their distances and masses are then used to predict the frequency and strength of potential gravitational waves they might emit, which could be detected by future pulsar timing arrays.","context":["Authors: Chengcheng Xin, Chiara M. F. Mingarelli, Jeffrey S. Hazboun\nFirst Author’s Institution: Columbia University, Department of Astronomy, 550 West 120th Street, New York, NY, 10027, USA\nStatus: Submitted to ApJ\nDisclaimer: I am a member of the NANOGrav collaboration but was not involved in this work.\nThis year’s Nobel Prize in Physics was, in part, awarded to Andrea Ghez and Reinhard Genzel for their discovery of the supermassive black hole (SMBH) at the center of the Milky Way. This was an amazing discovery, and has led many astronomers, including the authors of today’s paper, to look for SMBHs in other galaxies. However the authors of today’s paper are looking not for a single SMBH, but for supermassive black hole binary systems (SMBHBs) which may be formed when two galaxies merge. To do this, they utilize multimessenger astrophysics. First they analyze electromagnetic observations to identify candidate SMBHBs. Then they determine the strength of the gravitational waves emitted by the orbits of these SMBHBs, and predict if they will be detectable with current and future pulsar timing arrays.\nCombing for Candidates\nSome SMBHBs candidates belong to a class called active galactic nuclei , or AGN, which emit massive amounts of energy that we observe as electromagnetic waves. As the SMBHBs orbit each other, they can cause periodic changes in the brightness, or magnitude, of the light emitted over time. This time-variable light emission is known as a light curve. Many large scale surveys such as the SDSS V or the Catalina Real-Time Transient Survey (CRTS) regularly monitor the light from sources across the sky, resulting in light curves from potential SMBHB sources that span years, like that shown in Figure 1.\nThe authors of today’s paper compile a total of 149 of these SMBHB candidates from various catalogs, and then use the source’s distance and masses to predict the frequency and strength, or strain, of the gravitational waves that may be emitted by the binary.\nA Galaxy Sized Gravitational Wave Detector\nThe gravitational waves emitted by SMBHBs will not be like those that have been detected by the Laser Interferometer Gravitational-Wave Observatory (or LIGO), which are emitted at frequencies between 10 and 10,000 Hz. These will be emitted at nanohertz frequencies, which means these binaries will take years to complete one orbit. We can’t detect these low frequency gravitational waves with ground based detectors, so instead we use pulsar timing arrays. Pulsars are neutron stars that emit radio pulses that can be timed, or have their time of arrival at Earth predicted, extremely precisely. As the gravitational waves from these SMBHBs pass between Earth and the pulsar, they will squeeze and stretch the space, making the pulses arrive slightly earlier or later, then predicted. This signal will be different in each pulsar depending on where it is in space, and is discussed in more detail in this astrobite.\nCurrent pulsar timing array experiments such as those done by North American Nanohertz Observatory for Gravitational Waves (NANOGrav), the European Pulsar Timing Array (EPTA), and the Parkes Pulsar Timing Array (PPTA), which collectively make up the International Pulsar Timing Array (IPTA), all regularly search their data for these individual sources of gravitational waves, but none have been found so far. This makes searching for potential candidates for gravitational wave detection through mulitmessenger methods crucial for these experiments.\nPredicting the Future\nThe authors of this paper use previous limits set by these pulsar timing arrays, and then use models to extrapolate how sensitive they might be in the future, with particular attention to the IPTA and the Square Kilometer Array (SKA), an array of radio telescopes currently being built which is expected to find and time hundreds of pulsars. The authors then look to see if any of their most promising SMBHB candidates will be detectable in the future, shown in Figure 2 by the candidate points and sensitivity curves and in Figure 3 by the gravitational wave strain needed for the pulsar timing array to detect the source with a signal-to-noise greater than 3 in different parts of the sky. We can see from both of these figures that some SMBHB candidates very well might be detectable, though not for at least a few more years.\nBeing able to identify and predict the detectability of potential SMBHB candidates from light curves is crucial for confirming them in the future, as gravitational wave emission is an undeniable signature that the candidate really is a SMBHB. With telescopes like the Vera Rubin Observatory Legacy Survey of Space and Time (LSST) coming online soon, more and more of these candidates will be identified by their light curves. Finding these candidates also allows for targeted gravitational wave searches which may increase the sensitivity by a factor of two! This paper has helped lay the groundwork for multimessenger astronomy as a critical component for detecting SMBHBs in future pulsar timing array data, and shows there is a lot to look forward to!\nAstrobite edited by: Abby Waggoner\nFeatured image credit: AUI/NRAO, NAOJ, and Science/Nicole Rager Fuller","Advancing Basic Science for Humanity\nWhat are gravitational waves?1\nGravitational waves are a key prediction of general relativity, a theory proposed by Albert Einstein in 1915 that is still our best explanation for the force of gravity. Einstein pictured space and time as interwoven aspects of the same underlying reality, known as space-time. Objects that possess mass, such as stars and planets, warp space-time much like how a heavy ball placed on a trampoline creates a bowl-like depression around itself. This curvature in the space-time trampoline, so to speak, is experienced by all matter in the universe as the force of gravity. Whenever any mass moves, it generates gravitational waves that swell through space-time like ripples radiating across a pond's surface. For these waves to be big enough to detect, however, extraordinarily massive, astronomical objects are required, such as accelerating black holes or neutron stars. The ability to measure the strength and frequency of gravitational waves is important because these measurements would provide vital details about the distant, exotic phenomena that unleashed the waves upon the cosmos.\nWhy are gravitational waves important for science?\nVirtually all of our knowledge about the universe has come to us in the form of light, or electromagnetic radiation. Gravitational waves, however, are an entirely different form of radiation, produced by some of the most violent events in the universe. The study of gravitational waves will reveal the inner workings of some of the most violent events in the universe, such as the collisions of black holes and the explosions of titanic stars. Gravity itself, as a force of nature, will be probed in new ways in these extreme astrophysical conditions that are un-reproducible on Earth in the laboratory. Gravitational waves should even let scientists see all the way back to the origin of the cosmos itself in the Big Bang. The opening of these unparalleled new vistas has a historical precedent. For nearly all of human history, we had to make do with the sunlight and starlight our eyes can see. However, visible light makes up a tiny sliver of the range of energies light can possess. In the 20th century, we learned how to detect the entire electromagnetic spectrum, from the highest-energy gamma rays on down to the lowest-energy radio waves. As a result, astronomers literally saw the universe in a whole new light. A slew of phenomena and happenings in the cosmos suddenly became evident to us, from the spasms of supermassive black holes in galaxy cores to the faint heat emanated by gas clouds where solar systems are born. Now, in the 21st century, a newfound ability to see in gravitational waves will further bolster our efforts to understand the universe.\nWhat is the history behind the search for gravitational waves?\nAlbert Einstein's general theory of relativity, published in full in 1916, revolutionized the understanding of gravity that had prevailed since Isaac Newton's time in the 17th century. Rather than treating space and time as absolute, independent entities as Newton did, Einstein's equations intertwined them into a \"space-time.\" The force of gravity, although described quite accurately by Newton's mathematics, was now conceived by Einstein to be a geometric property of space-time. According to general relativity, mass caused space-time to curve in a describable manner, manifesting as gravity. Application of Einstein's theory resolved numerous discrepancies with Newton's physics, such as tiny shifts in the orbit of the planet Mercury. Over the past century, the predictions of general relativity have been proven to extreme accuracy again and again, and have even made possible precision technologies such as the Global Positioning System.\nThe LIGO Laboratory operates two detector sites, one near Hanford in eastern Washington, and another near Livingston, Louisiana. This photo shows the Hanford detector site. (Credit: Caltech/MIT/LIGO Lab)\nOne of general relativity’s biggest unproven predictions is the existence of gravitational waves. Einstein's work held that no information could travel faster than the speed of light, including the positions of masses in the universe, which are communicated by gravity. When masses move, corresponding changes in the gravitational field move through the cosmos as gravitational waves at the speed of light, like ripples across a pond. But gravity is an extremely weak force so even the sources of the biggest gravitational waves, like the cataclysmic collisions of black holes, would only produce the tiniest of wiggles by the time they reached Earth. This movement induced by gravitational waves has been calculated to be thousands of times smaller than the nucleus of an atom. For many decades, the ability to measure on such a small scale was impossible.\nAgainst this challenge, strong, albeit indirect support for the existence of gravitational waves emerged in 1974. The astronomers Russell Hulse and Joseph Taylor were conducting research at the Arecibo Radio Observatory in Puerto Rico. They discovered the first binary pulsar star system, consisting of two rapidly spinning, neutron star remnants of once-giant stars, orbiting each other and sending out pulses of radiation. According to general relativity, the two pulsars would draw closer together over time as they radiated gravitational waves. Indeed, over eight years of observations, the pulsars' orbits decreased at exactly the rate called for by Einstein's equations. Hulse and Taylor were awarded the 1993 Nobel Prize in Physics for their discovery.2\nAs exciting as these and related pulsar findings have been, no gravitational waves had ever been directly detected on Earth. LIGO was built to finally test this prediction of general relativity and made the long-awaited detection in September 2015.\nWhat are the types of gravitational waves?3\nAny movement of a mass through space-time generates gravitational waves. But objects such as people or airplanes do not make waves that are anywhere near large enough to detect. Instead, scientists must look to the cosmos, where extremely massive objects capable of generating detectable waves are relatively commonplace.\nLIGO scientists have come up with four different types of gravitational waves that, depending on their origin, should each leave a distinctive signature on the experiment's detection equipment.\nContinuous gravitational waves\nThese waves should be produced by spinning, massive objects that are not perfectly spherical. An example of such an object is a neutron star—the dense, city-sized leftover of a colossal star that collapsed on itself, exploding as a supernova. As a neutron star whirls on its axis, any bumps on its surface will emit gravitational waves. Assuming the compact neutron star's spin rate remains constant, the ripples in space-time it sends out with also remain constant, or continuous.\nInspiral gravitational waves\nTwo massive objects in the universe can become locked gravitationally and will draw each other closer and closer in a winding, spiral-shaped dance. As the objects \"inspiral\" toward each other, their orbital distances decrease and their speeds ramp up, rather like how a spinning figure skater who draws in his or her outstretched arms whirls even faster. Huge gravitational waves are generated in this process, with the waves' frequency increasing until the two objects collide and merge into a single object. There are two kinds of massive astronomical bodies that should crank out detectable inspiral gravitational waves: neutron stars (described above); and even denser objects called black holes. So-called stellar black holes form in a similar manner as neutron stars, during the collapse of a giant star going supernova. But these remnant, compact masses contain so much mass, and thus generate so much gravity, that even light cannot escape their gravitational clutches. Any combination of neutron stars and black holes spiraling in toward each other—whether two neutron stars, two black holes, or one of each—should make distinctive gravitational wave signatures.\nStochastic gravitational waves\nGiven all the possible sources of gravitational waves in the universe, scientists expect there to be a sort of background gravitational wave \"hum.\" The waves composing this background \"noise\" are expected to be small and hard to detect. Intriguingly, one of the sources of this particular kind of gravitational wave could be the Big Bang itself, 13.8 billion years ago when the universe came into existence. Stochastic gravitational waves from the Big Bang could offer scientists an unprecedented view into the earliest moments of the cosmos.\nBurst gravitational waves\nThis last category is reserved for the \"expected unexpected.\" Scientists think it is likely that LIGO and similar gravitational-wave detection experiments will discover types of waves that theorists have not yet completely described or anticipated. Supernovae, for example, as well as energetic, puzzling phenomena known as gamma ray bursts could each produce telltale gravitational waves."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:b11b456d-db5c-41c0-8663-7ed46047766a>","<urn:uuid:6742c6db-d66f-4a04-aab2-724a28314749>"],"error":null}
{"question":"What material is better for making a fire pit heat deflector, aluminum or stainless steel?","answer":"Stainless steel is better than aluminum for making a fire pit heat deflector. While aluminum is cheaper and easier to work with, stainless steel is sturdier, blocks heat better, and improves efficiency. Additionally, stainless steel doesn't react with food, unlike aluminum which can chemically react with foods causing erosion and changes in color or odor.","context":["How to Build a Fire Pit Heat Deflector\nFire pits have become quite popular in modern homes. They serve several purposes, but heat output and efficiency are the top priority. Ultimately, a great fire pit can give off enough heat; that’s where heat deflectors come in. Heat deflectors redirect the fire pit heat released upwards, sideways where people are sitting around.\nA heat deflector for a fire pit consists of a metal disc with three or four stands that mount on top of the fire pit, blocking hot air from rising and redirecting it to the sides. Making a heat deflector for your fire pit is not as difficult. However, you will need some basic handyman skills as it requires materials, various tools, and machinery. This article will teach you how to make a DIY heat deflector suitable for gas and wood fire pits.\nBuilding a wood or gas fire pit heat deflector\nOur heat deflector will consist of a rectangular lid with four stands. Here’s how to build it.\nWhat you need\n- Protective gear (goggles, ear muffs)\n- Metal bars for the stand (non-hollow)\n- Malleable sheets of your desired metal (18 gauge )\n- Screws ( aluminum or steel)\n- Soldering gun\n- Metal shears or chop saw\n- Brake (to help bend the metal sheets).\n- Rivets and riveter\nMeasure your fire pit\n- Wear your safety goggles when cutting metals to avoid shards from getting into your eyes.\n- You need to take measurements of the fire pit for your metal sheets. First, measure the fire pit end to end, where you want the stands of the heat deflector to be, then add an inch and a quarter to all sides.\nChoose your material\nThe sheets for the heat deflector could either be aluminum or stainless steel.\n- Aluminum is a cheaper option to make a heat deflector and easier to weld the sheet to the legs than stainless steel. It is also more flexible and easier to bend. However, although aluminum is a better heat conductor, it also loses too much heat. It is also chemically reactive with foods that might erode the sheet and change its color or odor.\n- Stainless steel is the pricier option. It is sturdier than aluminum. It is a better heat blocker that will prevent your fire pit from losing heat, thus improving its efficiency. Finally, it doesn’t react with food.\nMake the lid\n- Cut your sheets to size using a pair of metal shears.\n- Measure half an inch from each corner, mark it with a scribe, and drill a small hole. The size of the hole doesn’t matter.\n- Use a clipper to trim the edges of the metal sheet circularly. Then use the same cutter to snip the corner right to the drilled holes, cutting the corner in half.\n- Using a brake, bend each side up to 60°.\n- Then use the pliers to bend the corners of each intersecting side until they overlap and then drill a pilot hole through them.\n- Use a riveter to insert a rivet through each hole, connecting the corners of each intersecting side. Alternatively, you can weld each side together with a soldering gun.\nMake a stand\n- You will need four metal bars that aren’t hollow with the smallest size as 0.5×1.5 inches.\n- You can cut the metal bars to your desired length; most people do 12 inches or more. However, note that a wood fire pit heat deflector should have longer legs to allow space for air circulation and to add wood.\n- Have someone hold all the four stands where you want the lid to stand. Then position the lid on the legs and mark where the bars touch the lid.\n- Hold a stand in its marked place under the lid, drill a hole through them, then set the screws to secure each bar. Repeat this for all stands. Use either aluminum or steel screws and not galvanized screws because zinc releases fumes that are toxic when heated and can buildup in the food.\n- If you have extra bars, you can connect the bottom of the stands for steadier support. Then, once you’re done, you can place your heat deflector above your fire pit and remove it for easy storage.\nIf making the heat deflector for yourself is too much of a hassle, you can invest in the Solo Stove heat deflector. It is a stainless steel heat deflector with a circular disc with three legs that only requires straightforward assembling. In addition, it comes in three sizes, allowing you to choose a sizeable one for your fire pit.\nDo fire pit heat deflectors work?\nYes, heat deflectors for fire pits work. A heat deflector is a shield or barrier that stops or limits heat from passing through. So heat moves in three ways; radiation, convection, and conduction. Convection is the process of heat transfer through air and the heat type from a firepit. The heat from the fire pit heats the air around it, and since warmer air is lighter, it rises upwards. A heat deflector above your fire pit blocks the rising hot air from moving further upwards and redirects it sideways instead. It magnifies the fire pit’s heat radius and allows you to keep larger groups of people gathered around it warm.\nHow do I make my fire pit radiate more heat?\nTypically, gas fire pits don’t radiate as much heat as a wood-burning fire. But if your wood fire pit is not heating up well, you can make some adjustments to get more heat out of it. A fire pit consists of three essentials; oxygen, fuel(wood or gas), and heat. Therefore, anything that affects these components will affect the amount of heat produced. Below are some easy ways to make your fire pit burn hotter.\nUse dry wood\nDry wood absorbs the fire more easily because of the lack of sap since tree sap is not flammable. Therefore, a fire pit with dry wood burns brighter, stronger, and produces more heat. Conversely, using wet wood causes the fire to waste a lot of heat to burn the less-flammable wood. As a result, the fire has less energy to convert to heat, therefore less heat coming out of your fire pit.\nDry wood is typically gray or brown, while wet wood is usually green.\nGive more oxygen\nA good solution for more heat is to place the fire pit in an area with good air circulation. The carbon dioxide will go out, and oxygen can enter, thus maintaining a much hotter fire pit. Oxygen oxidizes the fuel, which creates a chemical reaction to cause a fire. Therefore a fire will not burn without oxygen.\nIncreasing oxygen to your fire promotes the chemical reaction that causes the fire to burn bigger and hotter. Although providing more oxygen to your fire pit causes an increase in heat and light, it is not long-lasting. Once you stop providing more oxygen, the fire pit burns low and slow.\nThere are two kinds of wood: softwood and hardwood.\nSoftwood burns faster, producing a lot more heat and energy. It also dries faster and is great if you want a quick hot flame.\nOn the other hand, hardwood burns much slower and lasts longer; however, it produces less heat.\nIncrease the surface area\nA bigger fire usually burns hotter. Making your fire bigger will help the flames access more oxygen. And as explained earlier, the more oxygen a fire gets, the hotter the flames burn. You can make your fire bigger by adding more flammable twigs, leaves, and branches, then spread the ashes and fire around to increase the surface area of the flame. The flames will acquire more oxygen and make it easier for the produced carbon dioxide to escape, thus making the fire larger and hotter.\nEnsure you spread all the twigs and branches evenly on all sides to get a more consistent heat dispersion and build an even fire all across the perimeter.\nChoose the right type of wood\nThe amount of heat the fire produces depends on the type of wood you burn. So when picking wood for your fire, make sure the wood your choose is soft and dry. Some excellent variants for firewood include; ash, maple, oak, pine, sycamore, app, birch blackthorn, and yew. These trees are particularly soft, so they also dry quickly well.\nHowever, hardwood usually burns hotter than softwood. But the downside with hardwood is it takes a lot of time to reach such hot temperatures, while softwood doesn’t reach some high temperatures but reaches its maximum temperatures much faster. So unless you have time to burn your fire pit for long, you should go for soft, dry wood.\nWith the DIY steps of making a fire pit heat deflector, you will enjoy gathering your friends around the fire as you will all be warm throughout. You can also help your fire pit produce more heat by using soft and dry wood, increasing the surface area of the fire, and improving aeration. Note that making the fire pit defector requires skills and machinery handling; if you’re not confident, you can always invest in a commercial heat deflector custom-made for your fire pit."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:1c18ebfa-8c25-493d-82d8-16e8144348aa>"],"error":null}
{"question":"Hello! In physics class learning about energy and motion... can u help explain are angular velocity and angular frequency same thing or different? Need to understand difference!","answer":"Angular velocity and angular frequency are closely related but serve different purposes in physics. Angular frequency, specifically in simple harmonic motion (SHM), is measured in rad/s and represents how quickly a system completes one full oscillation - with one complete oscillation corresponding to 2π radians. Angular velocity, as shown in rotational motion examples like wind turbine blades, describes how fast an object rotates around an axis. While both are measured in rad/s, angular frequency is specifically used to describe oscillatory motion in SHM, where it helps determine position, velocity, and acceleration of an oscillating object, while angular velocity is used in rotational motion to determine how fast objects like turbine blades are spinning and contributes to calculating their rotational kinetic energy.","context":["Because of its simplicity, simple harmonic motion is a physical model that can be used to investigate many more complex systems where there are restorative forces, such as the electrical interactions of atoms and the gravitational attractions between planets.\nSummary about the SHM\nThe MHS is the result of the action of a force that tends to keep some particle, or system of particles, in an equilibrium position , as happens with a spring when stretched or compressed, which is subjected to the action of an elastic force . In this type of movement, the sum of kinetic energy and potential energy is always constant, so we say that there is conservation of mechanical energy.\nWe call frequency the number of oscillations performed by a system in MHS that are completed every second. The period, in turn, is calculated as the inverse of the frequency and is equal to the time taken for the system in MHS to complete an oscillation. The units of measurement of frequency and period of the MHS are, respectively, the hertz (Hz) and the second (s). The formulas used to calculate these quantities are as follows:\nf – frequency (Hz)\nT – period (s)\nn – number of oscillations\nΔt – time interval (s)\nIn addition to the frequency and period quantities, the MHS is defined from angular quantities . Such quantities allow us to know in which position a particle in the MHS is, as well as to specify its measurements of kinetic and potential energy at that instant. The most important of the angular quantities related to MHS is angular frequency , also known as angular velocity or pulsation.\nω – angular frequency (rad/s)\nThe angular frequency has the dimension of rad/s. Radians are one of the different ways to define angles on the trigonometric circle. It is known that one complete turn along the trigonometric circle corresponds to 360º , which in turn correspond to 2π radians .\nIn the case of MHS, the trigonometric circle serves as a reference for a complete oscillation . For example, compressing a spring and releasing it, we have a complete oscillation when it has returned to its initial position – in which case we say that it has traveled an angular displacement equal to 2π radians.\nThe angular frequency can also be calculated as a function of other parameters, according to the harmonic oscillator type . Among all the possible types of oscillators, the ones that stand out the most for their importance are the simple pendulum and the spring-mass oscillator . The formulas used to calculate the angular frequency in these cases of simple harmonic motion are shown below.\nSimple harmonic motion formulas\nThe most important formulas of simple harmonic motion are the time equations of position, velocity, and acceleration . These equations allow us to determine the position, velocity or acceleration of a mobile in MHS at a given instant of time.\nIn the formulas above, the amplitude (A) is equivalent to the maximum distance that a particle can travel in relation to its equilibrium position . The variable t refers to the instant of time, and Φ 0 is called the initial phase and is related to the position at which the system started the movement.\nIn addition to the formulas already mentioned, there are also formulas that are used to calculate the period of oscillation and the frequency of the simple pendulum and also the mass-spring oscillator, namely:\nSolved exercises on simple harmonic motion\nQuestion 1 – Calculate the angular frequency of a particle that develops a simple harmonic motion knowing that the period of this motion is equal to 0.5 s.\na) π/2 rad/s\nb) π rad/s\nc) 4π rad/s\nd) 3π/2 rad/s\nTemplate : letter C.\nTo solve the exercise and calculate the angular frequency of the particle, we need to use the formula that relates this quantity to the period of motion.\nAccording to the calculation made, the angular frequency of the movement is equal to 4π rad/s .\nQuestion 2 – A particle describes a simple harmonic motion of amplitude equal to 4 cm. Knowing that the initial phase of the motion is equal to 0 and that its angular frequency is equal to π rad/s, determine the position of this particle at the instant t = 0.5 s.\na) 2 cm\nb) 5 cm\nc) 0 cm\nd) 4 cm\nTemplate: letter C.\nTo find out the position of the mobile, it is necessary to use the hourly equation of the position in the MHS; doing so, we must solve the following calculation:\nSince the cosine of π/2 rad is equal to 0, the result obtained is equal to 0 .\nQuestion 3 – Determine the maximum speed of a mobile that describes a simple harmonic motion of amplitude equal to 5 m knowing that its angular velocity is equal to 2 π rad/s.\na) 2π m/s\nb) 10π m/s\nc) π m/s\nd) π/4 m/s\nTemplate: letter B.\nThe formula relating the velocity of the particle that develops a simple harmonic motion is shown below. It is noteworthy that, in order to obtain the maximum speed in this type of movement, the sine, on which the speed function depends, must have its value equal to -1; So, just do the following multiplication.","Here we will derive the Rotational Kinetic Energy Equation in a few easy steps. This equation expresses the kinetic energy of a rotating object just because of its rotational motion. So, let’s begin the derivation.\nHow to derive the Rotational Kinetic Energy Equation | Rotational KE formula derivation\nTo derive the rotational kinetic energy equation, here we will consider the rotating blades of a wind turbine. We can easily see that the blades of a wind turbine have no overall translational motion. Still, each particle in the blades is moving and hence has kinetic energy.\nNow if we add up the kinetic energy of all the particles that make up the blades, we will find the rotational kinetic energy of the blades. Remember that this kinetic energy is just due to rotation.\nIn Figure 1, we will consider the motion of two particles in the wind turbine blades. Say, the blade assembly rotates with angular velocity ω. Recalling from our post on linear motion and circular motion that a particle moving with angular velocity ω in a circle of radius r has a speed v = ω r.\nPlease note that depending on the position of the particle on the blades they will have different speeds and radius. But all of these particles will have the same angular velocity.\nThus particle 1, which rotates in a circle of radius r1, moves with speed v1 = ω r1 and so has kinetic energy = (½) m1v12 = (½) m1 (ω r1)2 = (½) m1 ω2 r12 .\nSimilarly, particle 2, which rotates in a circle with a larger radius r2, has kinetic energy = (½) m2 ω2 r22.\nThe object’s rotational kinetic energy is the sum of the kinetic energies of all the particles: Krot = (½) m1 ω2 r12 + (½) m2 ω2 r22 + ….. = (½) (Σmr2) ω2 ……….. (1)\nNow, Σmr2 is known as moment of inertia I, which is the rotational equivalent of mass or inertia.\nThus the Rotational Kinetic Energy = (½) I ω2 ……………… (2)\nThe above equation is the equation for the Rotational kinetic energy of an object with a moment of inertia I and angular velocity ω.\nSummary | Take Away | Suggested Study\nIn this post, we have adopted a step-by-step approach to derive the equation of the rotational KE. We have seen how angular velocity and moment of inertia have become parts of that KE formula.\nNow it’s your turn to share this using the share buttons on the page.\nHere, we would suggest a few related posts for your reading that might help for easier understanding. The suggested posts are:\nRelationship between linear motion and circular motion quantities\nAnupam M is a Graduate Engineer (NIT Grad) who has 2 decades of hardcore experience in Information Technology and Engineering. He is an avid Blogger who writes a couple of blogs of different niches. He loves to teach High School Physics and utilizes his knowledge to write informative blog posts on related topics. Anupam M is the founder and author of PhysicsTeacher.in Blog."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:2235bfd2-6ae5-4507-b792-727d6cc849c9>","<urn:uuid:8f1b850a-4ae4-467d-9289-b93bcc53389c>"],"error":null}
{"question":"What alternative management options are available for treating Irritable Bowel Syndrome?","answer":"Alternative management options for IBS include assessment for small intestinal bacterial overgrowth (SIBO), lactose intolerance, and fructose intolerance. If SIBO is positive, treatment may include antibiotics like Rifaximin, followed by probiotics such as Alflorex, Vivomixx, VSL#3, or Symprove. Other alternative approaches include CBT, hypnotherapy, and acupuncture. Working with an experienced gastroenterology dietician and setting specific goals for symptom control is also recommended.","context":["Irritable Bowel Syndrome (IBS) is considered one of the functional gastrointestinal disorders with major focus on the gut brain access and classified by gastrointestinal symptoms relating to motility disturbances, visceral hypersensitivity, altered mucosal and immune function, altered gut microbiota and altered central nervous system processing.\nScale of Challenge\n- Majority of patients affected are under the age of 35 and often uncommon in the over 50s.\n- Over 75% of the patients may remain symptomatic after 5 years of diagnosis.\n- One of the most common gastrointestinal presentation in general practice.\n- Very prevalent with greater occurrence in women than in men.\n- Have an average of three days of work missed per month with various degree of severity of presentation mainly from mild to severe symptoms.\nDiagnosis & Management\nIn general, it refers to recurrent abdominal pain at least one day a week in the last three months associated with more than two of the following:\n2. A change in frequency of stools.\n3. Change in the form of stools.\n- Often a sensation of incomplete emptying of the bowels, occasionally passing mucus per rectal and with abdominal bloating and swelling.\n- Need a full history and to ensure no red flag symptoms for sinister pathology.\n- To assess the impact of the symptoms on daily life as well as any other psychological comorbidity or other comorbidities, which may be related to the presentation, such as gynaecological or urological.\n- Trigger factors such as stress or diet needs to be also explored.\n- 30 – 40% of patients may have anxiety or depression with other atypical symptoms ranging from backache, lethargy, nausea, bladder symptoms or even having a stressful life event.\n- It is prudent to refer patients, once red flag symptoms are excluded, for symptoms such as; constant abdominal pain, constant diarrhoea and distension with a change in bowel habit to loose or more frequent stools or both, persisting for more than six weeks, especially in patients over the age of 60. Any rectal bleeding, unexpected weight loss or any concerning family history of bowel or ovarian cancer needs to be also considered. The presence of anaemia, abdominal masses, rectal masses or any uncertainty in diagnosis, or failure to control symptoms in primary care, would necessitate a referral as well as having an elevated CRP or ESR or raised faecal calprotectin.\n- Often positive diagnosis and could be further subdivided into diarrhoea pre-dominant, constipation pre-dominant, pain pre-dominant or pain pre-dominant. It is important to consider such classification as the treatment strategy for management differs accordingly.\n- Treatment strategies directed towards the main symptoms or a combination of symptoms.\n- It is also important to explore the pathophysiology of Irritable Bowel Syndrome to patients.\nIBS – Diarrhoea pre-dominant\n- Exclude bile acid malabsorption.\n- Consider treatment such as Loperamide or a low dose tricyclic antidepressant in a low dose.\n- Change of diet following a review by a dietician to consider exclusion diet or low FODMAP diet may be of help.\nIBS – Constipation pre-dominant\n- Consider Osmotic laxatives may be helpful such as Movicol or Laxido.\n- Newer agents to consider include Resolor (Prucalopride) or Constella (Linaclotide).\n- Stimulant laxatives may make symptoms worse.\n- Lactulose may exacerbate distention and flatulence.\nIBS – Pain pre-dominant\n- One may consider antispasmodics, peppermint capsule.\nIBS – alternative management to explore\n- Assessing for small intestinal bacterial overgrowth, lactose intolerance and fructose intolerance.\n- Use of antibiotics such as Rifaximin or alternative antibiotics may be of value in controlling the symptoms of IBS especially with a positive SIBO test or diarrhoea.\n- When SIBO is positive, giving a course of probiotic after completing the antibiotics course may be of help, using probiotics such as Alflorex, Vivomixx, VSL#3, Symprove.\n- Other areas to be considered include CBT, hypnotherapy and acupuncture.\n- To use the assistance of an experienced gastroenterology dietician and to set some goals in regards to achieving symptom control.\n- To establish good rapport with patients and to offer clear explanation about the level of evidence used in the management of IBS and likely outcomes following treatment."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:1d4e3e2b-18bc-45f8-89ef-a16663f11085>"],"error":null}
{"question":"How did the methods of cloth printing in 1670s England compare to the transfer printing techniques developed in the 1750s?","answer":"In the 1670s, cloth printing in England involved using cast iron rollers engraved for printing patterns on cloth, with William Sherwin patenting a method using a double-necked rolling press in 1676. In contrast, the transfer printing technique developed in 1756 by John Sadler and Guy Green was more sophisticated, involving copper plates with hand-engraved designs. The later process used tissue paper as an intermediary step, where the design was first printed onto tissue paper using an etching press before being transferred to the ware. The earlier cloth printing method was more direct, while the later transfer printing technique allowed for more detailed and precise patterns.","context":["In my previous post on this subject I talked about Hooke’s dealings with some of London’s artists. This time I’m going to talk about craftsmen (broadly defined). To set the scene I’d like to share a conversation Hooke had at Garraway’s coffehouse on Boxing Day December 1673 with Andrew Yarranton and ‘Captain Hamden’. These two gentlemen had been to Germany to see some ‘Lattin making works’ – ie. factories producing tin plate – and they described to Hooke what they had seen. This is how Hooke recorded it:\n‘many plates beat under ye Hammer at once like leaf gold or tinfoyle. the great difficulty is how to turne it under ye hammer quick enough. much discourse about ye great cast iron rowles softned turned & graven for stuffs. one of six foot long & six foot about. cast iron pillars for bridges. hardning iron into steel quite through. pressing of cloth &c’\nI love this conversation because you can smell the Industrial Revolution in the air – but I’m quoting it here because of the reference to cast iron rollers engraved for printing patterns on cloth (‘stuffs’). The first English patent for printing on cloth was taken out a few years later, in 1676, by the artist William Sherwin.\nSherwin is an interesting figure with whom Hooke kept in intermittent contact. He was a leading engraver – this is his portrait of Charles II produced in 1669, the first datable English mezzotint. Although Sherwin states here that Prince Rupert helped him with the new process historians believe he discovered it independently. In 1676 Sherwin patented a new method of ‘printing of broad calico and Scotish cloath with a double necked rowling press . . . the only true way of East India printing & stayning such kind of goods’. Hooke recorded in August 1676 ‘saw Sherwins new invented way of staining calico’.\nPrinting cloth was new in England, but Sherwin was not the only person working on the process and Hooke had discussed methods of staining calico with Theodore Haak FRS earlier in the year. Interestingly, Hooke also made some trials himself. In 1672 he recorded ‘Mr Barret shewd me his flowered printed cloth’. Unfortunately I’ve not been able to identify Barret but he was clearly a craftsman of some kind, possibly a metal-worker, and Hooke had regular contact with him. In August 1673 he ‘Saw new stuff at Barrets and new printers Black’, and shortly afterwards ‘harry at Barrets carryd pattern to be cast.’ This raises the tantalising possibility that Hooke had designed a fabric print (but of course it may have been something else). In March 1674 he wrote ‘at Barrets made tryall of Golding flowerd Shift wch succeeded.’ – which (I think!) means that they tried applying gold leaf to a shift already printed with a flower pattern. As late as 1679 Hooke was still discussing cloth printing with Barrett: ‘At Barrets, with him to Garways [ie. Garraway’s coffeehouse]. Discoursd to him the way of staining Sattin with Lead moulds and copper plates.’\nThis is just one of the points at which the world of Restoration art overlaps with the world of craft, or manufacturing. If anything, Hooke and his Royal Society colleagues were more interested in the latter. Hooke records very many conversations in coffeehouses and elsewhere with people who are keepers of trade secrets or innovators in fields related to manufacturing. In my previous post I mentioned the Royal Society’s interest in dyeing and colours, which Hooke shared; Hooke also discussed methods of transferring painted or printed images; staining marble; glass-painting; making marbled paper; enamelling of tiles; recipes for varnish; and even ‘a paint not to be washed from the face with wett’ (a longstanding desideratum of the cosmetics industry!).\nI will single out just one more of these connections for further discussion. Hooke several times mentions John Dwight, a well-known potter who took out a patent in 1672 for ‘making transparent earthenware, commonly known by the names of porcelane or China and Persian ware, and also the mistery of making the stone ware vulgarly called Collogne ware’. Dwight opened a pottery at Fulham, and attempted to make all the types of ceramics imported into England at the time. Like many others he was particularly keen to make porcelain, and excavations at his pottery show that he conducted experimental trials with clays and glazes. Dwight studied at Oxford where he apparently met Hooke and Robert Boyle, and he later said that these two men supported his endeavours. In 1673 he confided to Hooke his secret for making salt-glazed stoneware: ‘he told me he used salt to throw into his fire as the Dutch’; and in 1674 Hooke saw some of the results and was impressed: ‘Saw Mr Dwights english china. Dr Willis his head. a little boye wth a hauke on his fist. severall little jarrs of severall colours all exceeding hard as a flint very light of very good shape. the performance very admirable and outdoing any European potters.’\nDwight was unusual in that his figures were modelled rather than cast from moulds. Early in 1675 Hooke showed one at a meeting of the Royal Society: ‘Mr Hooke brought in an artifical head resembling china, made in England, of English clay, so hard and solid, that he said, that nothing would fasten on it, except a diamond; and that it received its polish in the fire’. This durability was possibily of interest to Hooke because of his various architectural commissions. Dwight was able to make life-sized portrait busts, and Hooke seems to have thought of him in connection with a bust of Dr Baldwin Hamey the younger for the Royal College of Physicians, whose Anatomy Theatre Hooke designed.\nHooke’s association with Dwight shows how connections between the scientific and arts/crafts communities in Restoration London could be mutually beneficial. Dwight’s methodical experiments with chemicals and materials at his pottery may have been influenced by his knowledge of Boyle and Hooke’s experimental methods in Oxford, and his success brought him to the attention of the Royal Society (full of wealthy gentlemen who were potential patrons). Hooke and the other Fellows were interested in Dwight’s work for its scientific value in investigating different types of clays and glazes. This is just one example of this kind of relationship, but I’m hoping to find others as I continue working on Hooke’s diary.\nAuthor’s note: These two posts on Hooke’s relationships with artists and craftsmen were taken from a conference paper I delivered at ICHSTM Manchester (July 2013) in a session organised by Dr Sachiko Kusukawa as part of her research network ‘The Origins of Science as a Visual Pursuit‘. I would like to thank Sachiko for inviting me to take part. I would also like to thank (and congratulate) the Trustees of the British Museum for allowing members of the public to use the Museum’s images on non-profit websites without special permission.","Transfer printed decoration on porcelain was begun in England in 1756 and was developed by John Sadler and Guy Green of Liverpool. It was then adopted by Josiah Wedgwood who used it on his ivory based \"Creamware\". Thomas Spode early introduced blue transfer printed wares, using an improved version of the method of Ralph Baddeley in Shelton.\nThe printing method greatly reduced the effort required to produce large numbers of the same pattern. The relatively low cost of printing allowed more people to purchase quality tableware and its popularity made many potters wealthy. Most of the transfer ware manufactured in England came out of the Staffordshire region.\nFrom 1773 the East India Company had begun to reduce their imports of chinaware, making it difficult for families to obtain replacements and additions.\nIn 1784, the enormous tax in England on tea, was dramatically reduced which significantly increasing the frequency and enjoyment of tea drinking, which in turn, increased the demand for tea services and pots. The same year, in 1784, to meet this demand for more porcelain decorated in the classical Chinese decorations, Josiah Spode I perfected the process of blue underglaze printing on earthenware, from hand-engraved copper plates.\nInitially, the patterns were reproductions of the Chinese porcelain designs, firmly establishing the popularity of blue and white themes, but others soon followed, including the earliest blue florals. This not only assured Spodes fame and the future prosperity of his company, but was essential to the phenomenal growth of the English tableware industry.\nThe Spode transfer printing process was almost identical with the printing of a fine etching and was done from copper plates in which the design had been deeply engraved by hand. Dot punching was used for softer shadings and provided tones over large areas.\nAfter the engraving was done color mixed with oil was worked into the depths of the engraving. This was done on a stove that kept the copper plate hot, so that the color would run more freely. Colour was rubbed into the engraving with a wooden \"dabber\". When the engraving was filled, the surface was carefully scraped clean, leaving the color only in the engraved lines and the punched dots. After the excess colour had been removed the copper plate was bossed to even eliminate the thin film that was left by the scraper.\nTo catch the decoration onto a softer media a sheet of strong tissue paper, \"sized\" with soft soap and water, was laid smoothly on the copper engraving, which was then run under the roller of an etching press. The roller was covered with a soft felt to force the paper into contact with every line and dot of the pattern.\nThe engraving was then put back on the hot stove and the printed tissue paper was pulled away from the engraving. When the tissue was lifted it would draw with it all of the color from the engraving and the design would now be on the tissue. Next the the separate parts of the printed decoration that would be needed for the piece to be decorated, would now be cut-out out of the sheet of paper.\nThe Transferrer, who were mostly skillful girls, would now place the pieces of printed tissue carefully into position on the ware, where they would be held stuck to the porcelain by the tackiness of the colour. The print would be vigorously rubbed down with a stiff-bristled brush, lubricated with soft soap. Finally, to remove the tissue, is would now be washed off with cold water. When the tissue was washed off, it would leave the design on the ware.\nThe decorated ware would now be fired for ten hours at between 680C and 750C in the \"hardening-on\" kiln, so that the oil from the color would evaporate and the decoration would be fixed to the ware. After hardening-on, the ware could be glazed and re-fired at 1050C, which is also when the design would turn blue.\nThree original patterns from the period 1790 to 1820 - Blue Italian, Tower Blue and Willow - are still produced at Spode today. The method of transfer printing was introduced by the Maastricht potteries around 1840 with copper plates engraved in England. In Japan a similar method using transfer patterns was introduced by the late 19th century called Inbante.\nOn Chinese porcelain transfer prints seems to have been introduced even later and most Chinese porcelain pieces which appears to have been transfer printed seems to date to the first decades of the 20th century. The method seems to have been abandoned before WWII."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:7e69e5e5-39c0-4a78-b904-772eaab74c9d>","<urn:uuid:f6a5fb99-59e7-4e2d-a674-ab0c040875c1>"],"error":null}
{"question":"How does obesity contribute to type 2 diabetes development, and what forms of exercise are most effective in addressing this condition?","answer":"Obesity leads to insulin resistance, with excess body fat causing cells to resist insulin's effects and requiring more insulin to maintain blood sugar levels. High levels of free fatty acids in obese people contribute to insulin resistance and decreased insulin secretion. For exercise intervention, a combination of approaches is recommended: aerobic exercise improves cardiorespiratory function and glycemic control, resistance training increases bone mass density, and interval training helps increase insulin receptor sensitivity. Regular physical activity increases glucose transport and skeletal muscle insulin sensitivity.","context":["There are two types of Diabetes type 1 and type 2. Both types cause blood glucose to rise to unhealthy levels.\nIn type 1 the pancreas produces very little or no insulin. People with type 1 diabetes must manage their blood sugar levels with insulin injections.\nTYPE 2 DIABETES\nPeople with type 2 Diabetes have insulin made in their body but the body does not use the insulin properly, or it produces insufficient insulin which is referred to as insulin resistance. This diabetes can be managed with nutrition, good health and exercise. Some people with type 2 diabetes need oral medication.\nToo much glucose in your blood can lead to heart problems, blindness and even kidney failure.\nType 2 Diabetes can be prevented. Simple lifestyle changes can help you prevent this deadly disease.\nBefore you get to type 2 diabetes you must have come from what the diabetes experts have identified as PREDIABETES. If ever you are told that you are prediabetes, this should be a wake up call because at this stage you are not yet diabetes and can prevent it.\nPREVENTION OF TYPE 2 DIABETES\nThere are things that we can not change in our lifes like, we can not chage our race, we can not change our ethinicity,we can not turn back time and change our age, or our genes but there are some things we can change to prevent,of developing diabetes or reduce your risk of developing diabetes.\nFor us to be able to prevent DIABETES we need to know who is at risk, what risk there is and then how to prevent the it\nWHO IS AT RISK OF DEVELOPING DIABETES\nWhen we talk about preventing diabetes we are referring to type 2 diabetes.\n– Those who are prediabetes are at high risk of developing diabetes. Prediabetes means that your blood glucose levels are higher than normal but not yet high enough to be considered diabetes.\n– Those who have had gestational diabetes.\n– Those who have high blood pressure.\n– Those whose cholesterol or triglyceride levels are not normal HDL (he good cholesterol) is 50 or lower\n– Those who are inactive and exercise fewer than two times per day.\nPre-diabetes is not just a warning of diabetes. Someone with blood sugars in the prediabetes range is 1.5 more times likely to have cardiovascular disease even if r she does not develop type 2 diabetes.\nMost strategies that help prevent diabetes will most likely help lower blood pressure and cholesterol and help to prevent heart disease as well when combined with medication.\nChanging your lifestyle can be very difficult. In most cases no one is willing to undertake change not unless they believe that the risk to their health is REAL\nIt is natural to appreciate the general concept of risk such like overweight people over the age of\n45 are at risk of developing type 2 diabetes but to think that it won’t happen to you.\nThe risk is REAL though one can lower it by taking concret prevention steps.\nThe progress from Pre-diabetes to diabetes is not inevitable provided that you change your life style.\nWe can prevent Diabetes type 2 by changing diet and increasing our physical activities. We can reduce the risk of type 2 diabetes by 58 percent with modest weight loss, about ( 5 percent of the body weight) improved diet, less fat more fiber and increased physical activities (moderate exercise thirty minutes per day).\nTYPE 2 DIABETES IN WOMEN\nThe findings of the Nurses Health Study that specifically concerns Type 2 Diabetes in Women stated that:-\n- Being overweight or OBese was the single most important predictor of type 2 Diabetes in women. Also associated with increased risk were lack of exercise and a diet low in cereal fiber and high trans fat. Moderate alcohol consumption (one drink a day ) was associated with decrease in risk.. If all this life factors were improved, the incidence of type 2 Diabetes might be 80 percent lower\n2. Walking around for one hour per day was associated with a greater risk reduction.\n3 . Cigarette smoking was great cause of certain cancers and heart diseases and also a risk factor of type two DABETES\n4 . As far as reducing the risk of developing type two DIABETES, walking is as effective as physical activities.\n5 . Eating more nuts and peanut butter (in place of red meat, refined grain products like wheat bread and processed meat like hotdogs was associated with lower risk of type two DIABETES.\n6. Women with long or very irregular menstrual cycles ( usual length is twenty six to thirty days) which is associated with Obesity and polycystic ovary syndrome had a significantly increased risk of developing type two DIABETES. This risk was further increased by Obesity.\n7. The amount of time spent watching TV or on your phone was associated with the increase in Obesity and type 2 DIABETES .\nWHY WEIGHT LOSS IS IMPORTANT\nThe rise in Obesity in the world and especialy in the United States of America has paralleled the rise in type two DIABETES. You realise that the two are related and they often go hand in hand.\nObesity results in insulin resistance which leads to pre- diabetes and a steping stone to type two diabetes. The body’s cells resist the effects of insuline and thus more insuline is required to maintain blood sugar levels. For every pound you gain, your body needs more insuline.\nMost obese people have high level of free fatty acids in their blood. Free fatty acids are a necessary source of fuel, but too high of the level has been found to contribute to INSULINE resistance and somehow decrease insulin esecretion from the pancreas.\nThe body’s metabolism is most severely altered in those whose excess body fat is stored centrally around the abdomen as opposed to around the hips and legs. This type of Obesity becomes common in women after menopause. A waist measurement more than thirty five inches is considered above normal in women.\nIf you are concerned about type 2 Diabetes, the best advice is to lose weight if you are overweight. Eat less fat and calories and get more physically active. Lifestyle risk factors can push your blood sugars up into DIABETES .Genes are not destiny with type 2 Diabetes.\nA diet rich in whole grain will protect against type 2 Diabetes.\nA variety of chocolate or cocoa products creates a reduction of SERUM insulin levels. This could be because COCOA is full of POLYPHENOLS. 20-45 grams of chocolate should be added to the diet of a person with type 2 diabates.\nThe good news is that even small weight losses can produce big health benefits. The Diabetes Prevention Program results shows that a weight loss as low as 7 percent prevented type 2 DIABETES and decreased blood glucose levels back towards the normal range. The amount of weight loss has also been linked to improvements in blood pressure and cholesterol levels which linked to heart disease and stroke.\nThe path to healthier living is gradual. Do not be intimidated by the amount of weight you need to lose. It took some years for you to become overweight and it might also take some time for you to lose some weight even the 7 percent.\nHead in the right direction and have a program that will help you lose weight . Set realistic goals and reward yourself when you attain even a fraction of your set goals. Have a list of pros. This will be a good personal motivation to your success. Tape this list on your refrigerator and always come back to it when you feel like giving up.\nThe key will be to regain better balance in your life. The mentra is “CALORIES IN CALORIES OUT)\nDo not take more calories than your body need to function. To lose weight you must burn more calories than you take.\nWhat you need to lose weight and maintain it is a gradual process and not a “DIET”. Introduce healthier habits into your life that will become part of your life.\nYour personal plan for healthier life must start with the food you like to eat and the activities you like to do.","Exercise as it relates to Disease/Is aerobic exercise the best form of training for preventing/reducing effects of type II diabetes?\n- 1 Background\n- 2 Effects of Type II Diabetes Mellitus\n- 3 Current Rehabilitation Methods\n- 4 Exercises and their ability to either prevent or reduce effects of type II diabetes\n- 5 Recommendations for Effective Exercise Rehabilitation\n- 6 Further reading\n- 7 References\nIn adults, type II diabetes mellitus (t2d) accounts for 90-95% of all diagnosed cases of diabetes. Of these individuals, 80-90% of them are considered overweight/obese. Type II diabetes is and will continue to be a major health care burden, by the year 2030 the number of people who have this disease is estimated to more than double in compared to the 2000 statistics where there were 171 million reported cases in America. Right now this disease effects 346 million people world wide.\nType II Diabetes Mellitus DefinedEdit\nThe initial/early characteristic of this disease is an onset of insulin resistance in body cells. T2d is known as a progressive disorder of glucose metabolism, therefore indicating that the individual may have a decreased β-cell function which would cause less insulin secretion.\n- Insulin resistance, genetic predisposition, obesity and physical inactivity.\n- Dysfunction in skeletal muscle and bone.\n- Genes can increase the likelihood of excessive weight gain when an individual has 1st degree family history of t2d.\n- Environmental factors and weight gain.\n- High levels of sedentary behavior are associated with increased risk of having t2d.\nEffects of Type II Diabetes MellitusEdit\n- Inability to control metabolic pathways and blood glucose levels.\n- Increased chances of getting coronary heart disease.\n- Low cardiovascular fitness found in patients with t2d increases their risk of developing a cardiovascular disease.\n- Bone quality is reduced thanks to a decrease in osteoblast cell growth, bone fractures are higher in individuals with t2d then those who have osteoperosis.\nCurrent Rehabilitation MethodsEdit\n- Follow the 2008 Physical Activity Guidelines.\n- Engage in some form of general exercise is recommended via American Diabetes Association, American College of Sports Medicine, and American Heart Association. Physical activity increases glucose transport and skeletal muscle insulin sensitivity.\nExercises and their ability to either prevent or reduce effects of type II diabetesEdit\n|Aerobic Exercise (AB)||Resistance Training (RT)||Anaerobic/Interval Training (AIT)||Sedentary Behavior (SB)|\nUsing larger muscle groups over extended periods of time, and doing so with a high volume of repititions.\n♦Improves functional capacity of cardiorespiratory system, decreases the onset of t2d.\n♦Improves glycemic control, body composition, vascular and ventricular function.\n♦No benefit when it comes to improving metabolic control.\nMovements of high loads using resistance from either machines or weights, generally a small number of repititions.\n♦Weight bearing exercises are key to improve bone health, therefore RT results in an increase of bone mass density.\n♦Does not improve aerobic capacity (AE).\nExercising at a high intensity for short bursts of time (example could include interval training), ensuring use of either phosphocreatine or glycolysis energy systems.\n♦Regular intervals help to increase sensitivity of insulin receptors helping to control ones blood glucose levels.\nActivities that are done sitting or in reclining posture that expend less than 1.5 times the basal metabolic rate, distinctly different than physical inactivity.\n♦Your body is automatically more vulnerable to diseases.\n♦SB impairs physiological functions and causes the body to be more susceptible to oxidative stress.\nRecommendations for Effective Exercise RehabilitationEdit\n|Exercise||Recommended Activity to Either Prevent or Reduce Effects of Type II Diabetes Mellitus|\n|Aerobic Exercise||30–60 minutes of exercise, 7 days a week, ensuring to reach 60-70% of heart rate max|\n|Resistance Training||Participate in a full body strength training program at a minimum of 3 days a week|\n|Anaerobic/Interval Training||Ensure intensity of workout is either: VO2 max= 40-59 & 60-84% or heart rate max= 50-69 & 70-89% |\nEnsure that all individuals complete a prescreening evaluation before they undergo any testing/physical exercise.\n- American Diabetes Association recommendations for treatment and prevention\n- American Heart Association general information\n- American College of Sports Medicine exercise and t2d\n- Videos: Combination of RT and AE benefits, Aerobic and Anaerobic blood glucose and diabetes and Type II Diabetes no case is the same\n- Wood, R.J., et al. (2012). ‘Resistance training in type II diabetes mellitus impact on areas of metabolic dysfunction in skeletal muscle and potential impact on bone’. Journal of Nutrition and Metabolism, pp. 1 - 13. doi:10.115/2012/268197\n- Jenkins, A. B., et al. (2013). ‘Segregation of a latent high adiposity phenotype in families with a history of type 2 diabetes mellitus implicates rare obesity susceptibility genetic variants with large effects in diabetes-related obesity’. PLoS ONE, Vol. 8, pp. 1 - 9.\n- Kadic, D., et al. (2013). ‘Function of β-cells and insulin resistance in long-standing type II diabetes mellitus’. Script Medica, Vol. 44, 79 - 82.\n- Lakerveld, J., et al.(2013). ‘The effects of a lifestyle on leisure-time sedentary behaviors in adults at risk: The Hoorn Prevention Study, a randomized controlled trial’. Preventative Medicine. Vol. 57, pp. 351 - 356.\n- Kiyatno. (2010). ‘Interval exercise with 1:1 work/rest ratio decreases the risk factors of type-2 diabetes mellitus and coronary heart disease’. Folica Medica Indonesiana, Vol. 46, pp. 229-232.\n- Johannsen, N. M., et al. (2013). ‘Categorical analysis of the impact of aerobic and resistance exercise training alone and in combination, on cardiorespiratory fitness levels in patients with type 2 diabetes’. Diabetes Care, Vol. 36, pp. 3305-3312.\n- Belli, T., et al. (2011).’Effects of 12-week overground walking training at ventilatory threshold velocity in type 2 diabetic women’. Diabetes Research and Clinical Practice, Vol. 93, pp. 337 - 343.\n- Brassard, P., et al.(2007). ‘Aerobic exercise training reverses diastolic dysfunction in uncomplicated well-controlled type 2 diabetics’. Diabetes. Vol. 56.\n- Kafkas, M. E., et al.(2013).‘The effect of aerobic and anaerobic swimming exercises on mda, sod and gsh levels of elite swimmers’. Health Med, Vol. 7, pp. 2459 - 2565.\n- Belli, T., et al.(2007).‘Lactate and ventilatory thresholds in type 2 diabetic women’. Diabetes Research and Clinical Practice, Vol. 76, pp. 18 - 23.\n- Radak, Z., et al.(2008).‘Systematic adaptation to oxidative challenge induced by regular exercise’. Free Radical Biology and Medicine, Vol. 44, pp. 153 - 159."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3eec7566-182d-4f16-862d-039454d863ab>","<urn:uuid:dea5012a-1059-479b-ae5a-4606ae5638cc>"],"error":null}
{"question":"How many countries in Asia has the fall armyworm pest spread to since 2018, and why is it considered a major threat?","answer":"Since mid-2018, the fall armyworm has spread to over 15 countries in Asia. It is considered a major threat because it affects maize, which is used as food, feed, and fodder by millions of smallholder farmers who depend on this crop for their livelihoods.","context":["CABI experts in invasive alien species and extension communication have contributed to a major new guide for the integrated pest management (IPM) of the fall armyworm (Spodoptera frugiperda) in Asia.\nThe resource, ‘Fall Armyworm in Asia: A Guide for Integrated Pest Management,’ has been produced under the Feed the Future initiative of USAID, CIMMYT and CGIAR Research Program MAIZE and offers evidence-based sustainable approaches to tackling fall armyworm (FAW).\nSince its arrival in Asia in mid-2018, the pest has already spread to over 15 countries on the continent and is a major threat to maize – used as food, feed and fodder –by millions of smallholder farmers who rely on the crop for their livelihoods.\nIt is feared that the FAW is likely to become endemic – due to the conducive environments offered by the region – and the guide is designed to help professionals in private and public plant protection organizations, extension agencies, research institutions and governments to support smallholder farmers to effectively monitor and manage the pest using IPM based approaches.\nThe guide features seven chapters focusing on practical knowledge and tools to control FAW in Asia. These include a chapter on fall armyworm scouting, action thresholds and monitoring, a chapter on host plant resistance in maize to FAW, and a chapter on biological control for FAW management in Asia.\nDr Roger Day, CABI’s Global Advisor, Plant Health, contributed to the first chapter: ‘Fall Armyworm in Asia: Invasion, Impacts, and Strategies for Sustainable Management,’ and Dr Malvika Chaudhary, Regional Coordinator Asia for the CABI-led Plantwise programme, and Tamsin Davis, Development Communications Specialist at CABI, provided their expertise to chapter seven: ‘Communications Framework for Integrated Pest Management of Fall Armyworm in Asia.’\nIn the guide’s forward, Rob Bertram, USAID Chief Scientist, Bureau for Resilience and Food Security, said, “FAW’s emergence in Asia is a high concern given that region contributes approximately 30% of global maize supplies, which are now at risk. Smallholder farmers are especially vulnerable, having limited access to the tools, technologies, and management practices that are necessary to sustainably manage the pest.\n“I hope this document provides the knowledge critical to meeting the FAW challenge and look forward to expanding our partnerships with many of you as we exchange knowledge, innovations, and experience and move forward in the fight against fall armyworm.”\nShortly after the outbreak of FAW in India in 2018, USAID, CIMMYT and the Fall Armyworm Research-for-development (R4D) International Consortium organized a series of workshops and stakeholder consultations in several countries across Asia, including Bangladesh, Nepal, and India.\nGovernments in Asia have also quickly responded to the challenge and set up National FAW Task Forces to rapidly mobilize actions to mitigate FAW damage. Several public- and private-sector institutions in Asia have organized massive campaigns to bring awareness about the diagnosis and management of FAW.\nThe editors of the guide in their preface said, “There is an increasing understanding that the pest needs to be sustainably managed, along with other maize pests, using an integrated pest management (IPM) strategy, without damaging human and animal health and the environment.\n“Similar to our approach in formulating the FAW Technical Guide for Africa in 2018, development of this publication is guided by the Rome Principles developed by leaders at the 2009 World Summit on Food Security to guide urgent action to eradicate hunger.”\nMain image: Fall armyworm on maize (Credit: CABI).\nFull guide reference\nB.M. Prasanna, Joseph E. Huesing, Virginia M. Peschke, Regina Eddy (eds). 2021. Fall Armyworm in Asia: A Guide for Integrated Pest Management. Mexico, CDMX: CIMMYT.\nYou can access the full document here.\nCIMMYT has also published an announcement of the guide launch which can be read here.\nRelated News & Blogs\nCABI scientists from its regional centre in Malaysia have collaborated on new research which suggests Asia is a ‘biosecurity hotspot’ and a ‘genetic melting pot’ for the devastating crop pest fall armyworm (Spodoptera frugiperda). Dr Muhammad Faheem, D…\n22 June 2022"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:75f6149c-681d-44c7-81e0-fc10fdf29b31>"],"error":null}
{"question":"How long do landowners have to be notified before a Timber Harvest Plan can be submitted?","answer":"Landowners within 1,000 feet downstream must be notified and given at least ten days before a Timber Harvest Plan may be submitted.","context":["Timber Harvest Plans\nCheck out the PDF to find a detailed presentation of Timber Harvest Plans in California: Review, Tracking, and Comment-Writing, which provides clear instructions and images on how to look up a THP on the CalTrees Website.\nFinding a Copy of the THP\nAll THPs and related documents submitted to CAL FIRE are now available on the CaLTrees Website\nClick “Search Timber Harvest Documents”\n*Hint: you do not need a user name or password!\nLogging on Private and Corporate Lands\nLogging on private and corporate non-federal land in California is regulated by the 1973 Z’berg-Nejedly Forest Practice Act. This Act established the Forest Practice Rules (FPRs) and a politically-appointed Board of Forestry to oversee their implementation. The California Department of Forestry (CALFIRE) works under the direction of the Board of Forestry and is the lead government agency responsible for approving logging plans and for enforcing the FPRs.\nTo log on private or corporate land, a Registered Professional Forester (RPF) must prepare a document which outlines the proposed logging operations, known as a Timber Harvest Plan (THP), and submit this to the state. The FPRs describe THPs as having two functions: to provide information for the CALFIRE director to determine if the proposed logging conforms to the rules; and to provide direction to logging operators who carry out the THP. These documents were certified as the “functional equivalent” of an Environmental Impact Report to comply with the California Environmental Quality Act (CEQA) and are supposed to evaluate all of the potential direct and cumulative impacts that might occur as a result of the logging plan and to implement any feasible measures which would reduce this impact to a level of insignificance.\nApproving the THP\nThe FPRs state that CALFIRE “shall disapprove a plan as not conforming to the rules” if it does not contain enough information to evaluate potential environmental effects, if it would cause “significant, long-term damage” or cause a “taking” of a threatened or endangered species or if it would cause irreparable harm to rare or endangered plant species (see Title 14, California Code of Regulations, 898.2 of the FPRs). 99 percent of the THPs that are submitted, however, receive CALFIRE’s reliable rubber-stamp approval. At most CALFIRE will encourage submitters to withdraw a THP if there are problems in giving it their approval, but most often a new THP is submitted and approved in its place which covers the exact same area and only differs from the original plan by small, cosmetic changes.\nWhen a THP is prepared, the submitter is required to notify landowners within 1,000 feet downstream of its boundary and request information on domestic water sources that could be affected by the proposed logging operations. At least ten days after providing this notice, the THP may be submitted, and CALFIRE then has ten days to accept it for filing or return it to the submitter. THPs are available for interested people to obtain, with a payment, through their local or regional office of CALFIRE; THPs in Humboldt County, for example, are available for purchase at the Fortuna and Santa Rosa CALFIRE offices.\nFirst Review and Pre-Harvest Inspection\nBefore being accepted for filing or within five days after filing, the plan undergoes “first review” in Santa Rosa to decide if an on-the-ground inspection, known as a “pre-harvest inspection” (PHI), is necessary and to help identify potential problems which should be considered during the review of the THP. The first review team includes a representative from CALFIRE and may include a representative from other agencies (i.e. the Department of Fish and Wildlife, Regional Water Board, etc). Agencies other than CAL FIRE are not required to participate. These other agencies are able to make recommendations on THPs, but CALFIRE is the lead agency in THP review and approval and is not obligated to include recommendations made by these other agencies.\nIf a PHI is deemed necessary, which in most cases it is, a CALFIRE Forester and other agency officials who choose to participate do an on-site inspection of the area to be subjected to logging. The CALFIRE Forester and any other agency officials who might have attended the PHI then prepare and submit a report on the inspection. These PHI reports, as well as any other reports and documents that are submitted in relation to the THP, are included in the official file for the THP and can be obtained at the local or regional office of CALFIRE. Often PHI reports reveal a little more of the details about the on-site conditions and the logging operations that are proposed than the THPs themselves. The RPF who prepared and submitted the THP must respond to any recommendations that are made by CALFIRE and other agencies, and this response is also included in the file and available for citizens to review and purchase.\nAfter the PHI, the THP goes through its “second review” in at the Unit level, and a recommendation for denial or approval of the THP is made. This meeting is again comprised of a CALFIRE representative, who is the Chair, and other agency officials who elect to participate. If agencies outside of CALFIRE do not agree with the Chair’s recommendation, they may file a “non-concurrence.” These meetings, although not considered a formal public hearing, are open for interested members of the public to attend and make comments. Comments, however, are not recorded in any way and do not become a part of the official administrative record of the THP.\nPublic comment periods vary from county to county, so to know the close of public comment one must call their local CALFIRE office and ask. This date is often extended, so the comment period may not necessarily close on the date that is first indicated. For THPs in Humboldt County, public comments are most often accepted until one week after second review. CALFIRE is required to consider and respond to these comments, and an “Official Response” is prepared and sent out upon the plan’s approval.\nCALFIRE is required to give notification on THPs that are filed to any interested party. These lists give very basic information about the THP, such as who the submitter is, the acreage and location of the proposed logging operation, and the silvicultural method that is proposed (i.e. clearcut, commercial thin, etc). To receive these notifications, one can call their regional CALFIRE office and request to be added to the mailing list. THPs are also now available online and on a new digital interface known as CALTREES, and can be found here.\nPlease contact EPIC at email@example.com, if you are interested in learning how to become more involved in protecting our forests, rivers and wildlife by monitoring timber harvest plans.\nPositive changes to state and federal laws and how they are implemented are most often spawned by active participation from individual citizens and public interest groups. This fact is readily apparent with forestry practices in California, as it has been public awareness, pressure, and advocacy most responsible for any of the improvements that have been made over the years. The following information is a brief overview of the process by which logging plans are approved. We encourage interested individuals to learn more about this process and to become involved in creating changes in our current laws that will better protect our forests, water quality, and other public trust resources.\nThe above photos are from an October 2012 overflight of Green Diamond clearcuts in their holdings in the Upper Maple Creek and Upper North Folk of the Mad River. These are Sustainable Forestry Initiative (SFI) certified clearcuts."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c274d4cf-b722-4a94-9a3c-1e5d6d480aa1>"],"error":null}
{"question":"I'm interested in sustainable forestry practices. How do forests contribute to climate change mitigation, and what are the environmental impacts of forest utilization?","answer":"Forests contribute to climate change mitigation in two key ways: by sequestering carbon dioxide as they grow and by providing renewable materials that replace fossil-based products. Growing forests act as carbon sinks, increasing carbon stocks yearly through active forest management. Additionally, wood-based products replace fossil-intensive materials like cement and plastic, creating a substitution effect that reduces fossil emissions. However, forest utilization can have environmental impacts at both forest and macro levels. At the forest level, activities like timber extraction, road construction, and logging camps can negatively affect plant and animal resources and ecological functions. These impacts vary in severity, from low-impact activities like collecting non-wood forest products to high-impact ones like commercial timber harvesting. At the macro level, environmental impacts are influenced by policies, plans, and programmes, requiring assessment and mitigation measures to maximize environmental benefits while supporting sustainable forest use.","context":["Environment and forest utilization\nAssessing environmental impacts of forest use\nFOREST UTILIZATION can have impacts on the environment both at the forest level and at the macro level.\nAt the forest level, several activities can have direct and indirect negative impacts on plant and animal resources and on ecological functions of the forests (including conservation of biological diversity and carbon and water cycles). These include poorly planned and implemented extraction of timber and non-timber products, the construction of logging and transport roads, the construction of facilities for logging camps or for recreational activities in the forests and waste accumulation. Active forest utilization can also have direct and indirect impacts on human health and on cultural and social foundations in nearby areas. Different forms of forest utilization vary in the severity, irreversibility, probability of occurrence and significance of their impacts. They range from low-impact activities such as occasional collection of non-wood forest products (NWFPs) to high-impact ones such as commercial timber harvesting or conversion of natural forest to plantations.\nAt the macro level, environmental impacts are determined by policies, plans and programmes. These impacts need to be assessed and mitigation measures promoted when necessary. Specific issues include, for example:\n- the linkages between environmental concerns and forest products trade;\n- the effects of the application of environmental standards in forest operations and industries;\n- ways of considering the environmental costs of forest operations and of compensating for environmental benefits of sound forest utilization;\n- increased recognition of the economic potential of national and international markets for the environmental services provided by forests;\n- promotion of policy tools to capture the value of these services.\nPolitical pressure by environment-conscious groups is at the root of policies influencing forest use, such as logging and log export bans, and the implementation of legally binding international conventions such as those on biological diversity, climate change and international trade in endangered species. These environmental conventions are the platform for policies and practices aimed at maximizing the local and global environmental benefits of sustainable forest use.\nFAO's work: awareness raising and capacity building\nFAO aims to promote forms of forest utilization that improve the livelihoods of people without compromising the environmental functions of forests. The objective is to address, in a systematic way, key environmental and social issues related to forest utilization and to promote best practices to maximize positive impacts, as well as mitigation measures for identified negative impacts. Awareness raising among a variety of stakeholders, including the public and private sectors (including industry) and civil society organizations, is a fundamental aspect of this work.\nFAO¿s work in environmental impact assessment and mitigation is related to the Organization¿s work in environmentally sound forest harvesting and operations, but with a wider scope, including issues such as the integration of biodiversity and carbon management considerations in environmental impact assessment; the role of green-market-driven mechanisms (certification and ecolabelling) in the adoption of environmentally sound forestry practices; ways to compensate for the environmental benefits of these practices; and developing markets for environmental services provided by forests. This area is thus also closely linked with FAO¿s work in forest products trade and marketing, forests and climate change, and wildlife and protected areas.\nMain activities include the following:\n- training, publications and workshops, and provision of technical and policy guidance and expertise, to help member countries maximize the positive contributions of forest utilization and reinforce their capacity to assess and mitigate negative environmental impacts of forest activities and policies;\n- seminars and networking about the environmental consequences of forest activities among a variety of stakeholders;\n- collecting and sharing of information and technical knowledge on the impacts of forest utilization activities through case studies and comparisons of country experiences;\n- developing indicators of environmental impacts of forest utilization, and participation in international initiatives in this area;\n- establishing partnerships and exploiting synergies with other organizations working in environmental impact assessment, such as the United Nations Environment Programme (UNEP), the Convention on Biological Diversity (CBD) Secretariat, the World Conservation Union (IUCN) and the World Wide Fund for Nature (WWF);\n- facilitating access to sources of funding for environmental assessment and to financial mechanisms for transfer of funds for environmental benefits of appropriate forest utilization.\nA central theme of this work is the environmental impact assessment (EIA), which is widely used as a tool to assist in decision-making. In several countries EIA is mandatory for forest activities. An important feature of EIA is public participation and transparency throughout the assessment process.\nAt the macro level, a tool called strategic environmental assessment (SEA) is used to assess the environmental impacts of policies and programmes. Such assessments help unravel the complexity of linkages between policies affecting forest utilization and environmental conditions.\nFAO¿s work on environmental impact assessment includes the following publications: Environmental impact of forestry (FAO Conservation Guide No. 7, 1992), Economic assessment of forestry project impacts (FAO Forestry Paper No. 106, 1992), Assessing forestry project impact: issues and strategies (FAO Forestry Paper No. 114, 1993), Valuing forests: context, issues and guidelines (FAO Forestry Paper No. 127, 1995), Environmental impact assessment and environmental auditing in the pulp and paper industry (FAO Forestry Paper No. 129, 1996) and A training manual for environmental assessment in forestry (Report for the FAO regional project ¿Forestry Planning and Policy Assistance in Asia and the Pacific\" 1996). Recent concern about environmental impacts of forest utilization at both the local and global levels has created new demand for assistance, and FAO is now renewing emphasis on this area.","Forests sequester large amounts of carbon dioxide as they grow. However, they also have considerable climate impact when they are felled because products produced using wood form forests replace fossil-based products so that oil and coal can be left in the ground. “We must consider both these parameters to be able to make political decisions that have the greatest positive impacts on the climate. There is a holistic calculation model that helps do just that,” says Peter Holmgren, an expert on forestry and climate.\nThe UN Framework Convention on Climate Change states that saving the climate rests on a two-pronged approach of reducing fossil emissions and increasing carbon storage.\n“The forestry industry contributes to both of these approaches, but at EU level there tends to be more focus on carbon storage in forests. The contribution forests make to reducing fossil emissions is overlooked, even though this is actually how the forestry sector provides the greatest benefit. When you use forests as a raw material to build houses and manufacture packaging and biofuels, for example, this replaces products made from cement and plastic that have a negative impact on the climate,” explains Peter.\nThe effect of renewable alternatives such as forestry raw materials reducing the need for fossil products is known as substitution. Substitution prevents the release of substantial amounts of fossil carbon dioxide into the atmosphere.\n“Substitution is a fairly complicated process because emissions reductions take place in industries other than forestry and often in other countries. However, it is incredibly important to include substitution effects because otherwise we’ll end up with policies that impact the climate negatively,” Peter notes.\nTogether with SCA, Peter has developed a model that indicates the forestry sector’s overall climate impact. The model reveals the extraordinary opportunities that the sector offers when it comes to contributing to climate initiatives. The model consists of three components:\nChange in the carbon stock in forests and wood-based products\nGrowing forests sequester carbon. Active forest management increases the volume of timber over time and this means that carbon stocks in forests grow every year, so-called carbon sinks. In addition, carbon is sequestered in products made from raw materials made from forests.\nThe Substitution Effect\nEmissions of fossil-based greenhouse gases are reduced due to the use of wood-based products and bioenergy instead of less climate-friendly alternatives.\nFossil emissions in the forestry sector’s value change\nThe forestry industry’s own fossil emissions from forestry, industrial processes and transport are deducted from the climate benefits that the industry creates.\n= Total climate benefit\nA recent report based on this model shows that the Swedish forestry industry creates a climate benefit that is twice as large as Sweden’s total climate emissions.\n“Roughly half of this climate benefit is due to wood replacing products such as concrete, plastic and oil. And this is something that can be done over and over again, so it has an incredible impact. So, it’s clear that forest products are needed for us to be able to limit global warming,” says Peter.\nEU focuses on carbon storage\nBut within the EU, focus is mainly on the first component of the model, that forests are important because they sequester carbon. The European Commission therefore wants more forests to remain untouched to increase carbon stocks. This is seen as compensating for other industries continuing to emit fossil carbon.\n“However, forests are at their most useful if both of these functions – being carbon sinks and being used sustainably – are used so that they contribute to the substitution effect. If we stop using forests and do not have the ability to make products made from wood from forests, what should we use to build houses, manufacture packaging, fuel and everything else that is needed in a welfare society?\nThe answer is that we would need to use more other materials, which have a negative impact on the climate. We thus get an increase in carbon storage in forests, but at the same time, emissions of fossil carbon from other activities increase,” explains Peter.\nConsiderable growth of cultivated forests\nPeter argues that forests can easily contribute with carbon sequestration and climate-smart products, just as they do today. An international study shows that it is precisely because forests are actively used that they sequester so much carbon. On the one hand, growth will be considerable in a cultivated forest, and on the other, there will be less damage and forest fires that contribute to high carbon dioxide emissions.\n“Timber stocks are constantly increasing in Sweden’s forests and in the rest of Europe, so carbon storage is increasing at the same time as we have access to large quantities of sustainable raw materials. In fact, there is no country in the EU that fells more trees at a faster rate than its forests grow.”\nOverall picture provides optimal benefit\nPeter developed the climate model together with SCA 2018. Today it is used by several Swedish forestry companies and by the Swedish Forest Industries Federation. It is also beginning to attract interest abroad.\nThe model is an important tool for helping politicians make decisions that optimise climate benefits.\n“It’s imperative that politicians and others involved in the forestry debate look at the whole picture. Then it becomes clear how forests can contribute in the best way.”\nFootnote: Peter Holmgren was previously head of the global climate efforts at the UN Food and Agriculture Organization and is today an advisor in sustainable development.\nPhoto: Michael Engman"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9aa021a7-ff88-4de0-9320-95f9eeb8b02a>","<urn:uuid:4615e2a9-f53a-405c-857b-99b99e5c6c60>"],"error":null}
{"question":"When comparing brick floor sealing and vinyl plank floor installation, which flooring type requires more extensive surface preparation before installation?","answer":"Vinyl plank flooring requires more extensive surface preparation. For LVP installation, all cracks and uneven surfaces must be addressed and leveled prior to installation, requiring self-leveling compounds and thorough repairs to achieve a completely flat surface. In contrast, for brick flooring, preparation mainly involves adding a 1-inch layer of fine screened sand over the sub-floor, compacting and leveling it using square tubing and a bridging piece of metal.","context":["Step #1: PrevNext\n- MATERIALS - Compressed Earth Blocks, Screened sand, Sealer – acrylic or oil based concrete sealer or varnish\n- TOOLS - Circular saw with masonry blade (optional), 2 levels, one small, one longer, Rubber mallet, 2 boards to stand on. You do not want to stand on the sand as your feet will make large dents. If you stand on a board, your weight is spread out and the smooth surface of the sand is not compromised. Surgical tape. The person laying the brick should consider taping their fingertips with surgical tape. This helps protect them without compromising dexterity. Roller and brush\nStep #2: PrevNext\n- PREPARATION Prepare and level your sub-floor, adding if you wish any vapor barrier, heating system, and insulation. Put a 1 inch layer of fine screened sand over the whole area. Compact and level the sand. The easiest way to do this is to bury and level a piece of square tubing in the sand on either side of the room, so that the top of the metal is flush with the level you want the sand to be.\n- You then bridge another piece of metal between the 2 pieces of square tubing, so that it sits on top of them, and drag it backwards and forwards over the area until it is smooth.\n- Choose your pattern before you start, and estimate the amount of bricks you will need. Running bond is often the easiest pattern to get your feet wet, but none of them are hard. The Herringbone can be difficult to visualize, but once you get going, it's not nearly at intimidating as it seems.\nStep #3: PrevNext\nCUTTING BRICKS - No matter which pattern you decide to use, you will need some cut bricks. Try and work out roughly how many you will need for your starting edge and cut those ahead of time. The ones needed at the other end of your rows, you can do once the rest of the floor is laid. Cut the bricks using a circular saw with masonry blade. If you are not too particular about the edges of your cut bricks, it is far easier to break them instead of cutting.\nStep #4: PrevNext\n- LAYING THE FLOOR - Place each brick, one by one, where you want it to go.\n- With the long level, check that it is level with previous bricks or existing floors. With the short level, make sure the brick itself is level in all directions. You also want to check that it is lined up well with the wall.\n- Use the rubber mallet to tap the brick tight against its neighbors. And tap down on it to get the level correct.\n- When you get to the opposite end of the wall from where you started, and you do not have a brick to fit in the space, leave it. You should do all the edge bricks at the end. Even before the floor is finished, you are able to walk on it. Do not tread near to unfinished edges.\nStep #5: PrevNext\n- PERIMETER - For the edges, you may have to measure each space and cut or break bricks to fit. Alternatively, you can fill the gaps with a very fine concrete when you do the perimeter.\n- Once you have all the bricks laid, you can fill the perimeter, in between the bricks and walls, with concrete. This does not use much concrete, and can be done in half an hour. Screen your sand and then trowel the concrete smooth and level with the tops of the bricks.\nStep #6: PrevNext\nFILLING CRACKS - Once all your bricks, including your edge bricks, are in place, sweep fine sand into the cracks. Allow the floor to settle a couple of days, and then sweep more sand into the cracks. Repeat this several times until the sand no longer settles.\nStep #7: PrevNext\n- SEALING - To seal the bricks so that you can sweep and mop them, use an acrylic or oil based concrete sealer or varnish, at least two coats. Until this is done, the bricks will be coated in a fine dust (as you gradually wear them down). This is okay for a patio or outside floor, but for inside it's not as acceptable.\n- Use a small can and/or brush to drip sealer in all the cracks first. Once that has set up, use a roller to spread the varnish on the brick surface.\n- The sealer will darken the natural color of the bricks. Most concrete sealers and varnishes smell strongly. Always provide adequate ventilation, and plan to keep those windows open for a few days.","How to install vinyl plank flooring? DIY Vinyl flooring.\nUpdated: Aug 6, 2019\nPrepare For Installation\nPrepare you home for installation by removing all of the furniture and fixtures that may get in the way of your LVP installation. Luxury Vinyl Plank flooring comes with different locking systems so please read manufactures recommendations prior to installation for correct locking method technique. Vinyl plank flooring can be installed over existing flooring such as tile, hardwood, and laminate, or it can be installed directly on concrete. LVP needs a level surface and all cracks and uneven surfaces must be addressed and leveled prior to LVP installation. A floor leveler will help you find if your flooring surface is level. The flooring prep is crucial to installing LVP correctly.\nUpon receiving your Vinyl Plank Flooring read the manufacture guide for acclimation requirements as some products will need to sit in your home prior to installation.\nPower Saw (Laminate Cutter)\nVinyl Plank Flooring\nAquabar/Underlayment (Some products have it installed)\n1) Remove flooring and prep your foundation if you have any uneven spots. If you are installing your LVP flooring over existing flooring make sure there are no uneven spots and patch all areas accordingly. LVP flooring requires a flat surface for installation. Self leveling dries quickly so if you have to do repairs you will need to mix and apply quickly. Make sure you vacuum and sweep and leave no debris on your floor prior to installation. Once the floor is prepped, dry, and level you will use a chalk line to create a starting point.\n2) Create a 5/16\" expansion gap from the end of each wall that is your starting point. Once you have marked this with the chalk line you can move onto measuring and calculating the width of your final row. This is only necessary if you are looking to have the same width on the starting and finishing rows. (Be aware that not all walls are even and this is why creating a starting point is important.)\n3) Starting your first row correctly will help in providing a solid starting point and is crucial to having a successful installation. You can remove the tongue on your 1st row and place the vinyl plank boards along your marked line. You may use spacers in assisting you with this part as planks will move while you are installing and this is why it's very important to have a starting point marked correctly.\n4) Follow the installation by installing the next plank to the side of the previous. Make sure you lock it into place (some require a mallet to lock in but most can be locked in by hand easily). Continue to the next rows and create off-setting starting vinyl plank pieces to provide varied plank look. You will get a staircase look if you are using the same sizes in repeating rows. To fit the final piece at the end of the row you will use a power saw/laminate cutter/utility knife.\n5) Continue your installation throughout and keep variable sizes throughout. When working in the kitchen or other areas you will have the option to put transitions and trim pieces in place around cabinets and other areas. You will need to nail the trim pieces into the wall and glue the transition pieces in into the floor.\n6) Once your project is complete go back and touch up any areas you need by using caulk or putty for nail holes. Clean the floor thoroughly and prepare to bring your furniture back into your home. While vinyl plank flooring is durable you should take precautions to protect your flooring by putting felt pads on the bottom of all of your chairs, tables, and anything that may be dragged across causing damage to your LVP."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:e0418979-a145-4b5e-9391-11b31b7fb7eb>","<urn:uuid:2aaa27aa-0831-4b60-b8c7-148f72537c0c>"],"error":null}
{"question":"How do you ensure the long-term sustainability of community projects?","answer":"To ensure long-term sustainability of community projects, you should maintain relationships within your core team to keep momentum going, regularly revisit assessments to look for new opportunities for change, and approach like-hearted organizations to collaborate and sustain successful initiatives. It's important to think about sustainability from the beginning of your planning. Additionally, you should celebrate accomplishments along the way to sustain the relationships, connections, and capacity you've built, which will help maintain benefits to communities for years to come.","context":["Welcome to the Alberta Healthy Communities Approach! This innovative five-step process can foster action and support you to create a healthy community.\nStep 1: Engage and Create Connections\nWant to make a change in your community? Build your team! Find other enthusiastic and passionate community members with diverse experience and perspectives to be a part of your team, or join an existing community group or committee and begin your journey.\nAs you build your team, consider finding people who are:\n- Invested in the process and the end goal\n- Represent different perspectives\n- Enjoy networking and building connections\n- See themselves and their skills as adding value to your team\nStart community conversations with individuals, groups, agencies and other stakeholders to understand what matters most to people and to generate interest in your team. Once you have your team in place establish a shared vision for your work together and create a Terms of Reference to clarify your purpose, describe your team and set roles and responsibilities.\nThe Power of Working Together - Cheryl's Story\nThrough their Eyes - Shana's Story\nNeed help getting started? Check out step 1 of the AHC Action Guide - for guidance on how to build, and maintain your team and create a vision for your work together.\nTools and Resources:\n- Collective Team Assets Inventory: PDF Download\n- AHCA Terms of Reference Template: Community Library Login\n“(It’s about) … working collaboratively, not in silos, sharing what we know, bringing the community together, building relationships, getting along, having open conversations, developing a plan, leadership, grit and hard work. Be patient with the process –it takes time to capture all the ideas!”\n~ Community team member\nStep 2: Understand Your Community\nCommunities are complex. We may think we know what makes up the places where we live, work and play. But do we?\nTaking a deeper look at your community by gathering data from different sources can help you better understand community strengths, needs and opportunities.\nThere are several different ways to build a greater understanding of your community. You can start simple by:\n- Creating a community profile\n- Developing a community asset map\nOr you can dig deeper using comprehensive assessments that take a deeper look at your community and the factors that influence health.\nBy layering your personal experiences in your community with the information you gathered you are ready to identify priorities and create a plan for your community.\nRelationships are the Greatest Asset - Molly's Story\nNeed help getting started? Check out step 2 of the AHC Action Guide for guidance on how to create a community profile, asset map or carry out a comprehensive community assessment.\nTools and Resources:\n- The Community Capacity Assessment Tool: an opportunity to reflect on and ask questions about engaging partners, leveraging resources, skills, knowledge, and creating a shared vision\nCommunity Capacity Assessment Tool\n- The Healthy Places Action Tool: provides an assessment of the following focus areas: physical activity, healthy eating, tobacco reduction, alcohol reduction, UV protection, and cancer screening\nHealthy Places Action Tool\n- CDLI asset map: a tool to map and visualize your community’s strengths and connections\n- Community Profile Template: PDF Download\n- Community Cancer Prevention and Screening Dashboard: provides you with the most-up-to date data on your community\n“Data can tell us about our communities: we were holding assumptions and not recognizing when change was happening.”\n~ Community member of Stettler\nStep 3: Prioritize and Plan\nUsing the information you’ve gathered and your amazing community wisdom it’s time to start setting priorities and creating an action plan. Bring your team together and use a priority-setting tool to help you determine where you want to focus your attention.\n- The level of importance of the issue\n- If it is something that can be changed in the short term\n- Your community's resources and capacity to address the issue\n- The impact it will have on the community\n- If there are any unintended harms or negative consequences\nKeep things manageable and remember to keep the community’s strengths, opportunities and interests in mind. Be creative.\nOnce you have set your priorities begin brainstorming ways to take action. Reflect on the make up of your team and determine if you need to invite others to join you. Need inspiration? Check out community stories and learn about what other communities are doing to help create environments that promote and support health.\nFinding your People - Brad's Story\nPlanners and Doers - Christina's Story\nNeed help getting started? Check out step 3 of the AHC Action Guide for a variety of tools to help your team brainstorm and set priorities for community-level action.\nTools and Resources:\n- Community Brainstorming Template: a tool to help your team map strengths, opportunities, challenges, importance, changeability and set priorities\n- Community Action Planning Template: a planning tool to help you set objectives, identify what success looks like and define roles and responsibilities\nStrategy Kits: tried, tested and useful strategies and resources to create healthier communities\n“Identify what is achievable and realistic...action plans help put ideas on paper, so others can catch the vision”.\n~ Community member of Wembley\nStep 4: Implement and Evaluate\nIt’s time to put your plan into action and watch for positive changes in your community! Implementation is where project-specific work begins and community excitement builds. This is where your team will need to mobilize resources, create new connections and partnerships to move work forward and tap into community assets. You will learn a lot along the way and find that new opportunities will emerge or you may face challenges that force you to change directions. Remember it is all part of the process.\nDiagram revised from Harmonization Cancer Prevention Team, (2016).\nEvaluation provides your team with an opportunity to learn from experience and to showcase the impacts of your initiatives. It demonstrates accountability to your funders and your community. It helps you to identify what is working well and where there is the opportunity for improvement. Use different data collection tools (surveys, focus groups, interviews) to track and monitor your progress. And don’t forget to share your successes and learnings along the way!\nNeed help getting started? Check out step 4 of the AHC Action Guide for a variety of tools to help your team implement their action plans and determine how they will measure success.\nTools and Resources:\n- Evaluation Planning Template: a tool to help your team establish evaluation questions, identify how they will measure success and what data they will need to gather\n- Strategy Kits: tried, tested and useful strategies and resources to create healthier communities\n“Evaluation shows what worked well, what didn’t and what can be done differently to get the desired outcome”\n~ Community team member\nStep 5: Sustain, Improve and Share\nBy celebrating your teams’ accomplishments along the way, you sustain the relationships, connections and capacity you’ve built, and will maintain benefits to communities for years to come.\nThinking about the sustainability of your initiatives at the onset of your planning will set your team up for success.\nWhen planning for sustainability:\n- Maintain relationships of your core team to keep the momentum going\n- Revisit your assessments and look for new opportunities to create change\n- Approach like-hearted organizations to collaborate and sustain initiatives that are successful\nLearn from the experience and make improvements along the way. Don’t forget to highlight your achievements and celebrate your successes!\nWhen the Funding Ends - Myrna's Story\nYou’ve accomplished amazing things and other communities want to learn from you.\n“It’s inspiring to see what we can do as a community when we unite and work toward common goals”\n~ Community member of Vulcan\nAlberta Healthy Communities Action GuideAccess full action guide\nDownload the full Guide. The Alberta Healthy Communities Action Guide, leads you through the Alberta Healthy Communities Approach and provides you with tools and resources to support you in every step of the process."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:f48b3cdb-7e36-4b32-9850-0a440c160fdc>"],"error":null}
{"question":"I'm studying urban design and I'm curious how Interior Urbanism and Biophilic Urban Acupuncture differ in their approach to improving city life?","answer":"Interior Urbanism and Biophilic Urban Acupuncture represent contrasting approaches to enhancing urban life. Interior Urbanism focuses on creating controlled indoor environments that protect from hostile climate conditions and extend public spaces indoors, as seen in examples like Hong Kong's indoor networks and Toronto's mall systems. In contrast, Biophilic Urban Acupuncture emphasizes integrating natural elements into urban spaces through small-scale interventions that connect people with nature, such as urban forests, water features, and tidal wetlands. While Interior Urbanism creates artificial environments that blur the indoor-outdoor boundary, Biophilic Urban Acupuncture aims to relieve urban stress by introducing natural elements into the built environment.","context":["What is Interior Urbanism?\nA Review of MONU #21\nIn 1969 Reyner Banham in his book The Architecture of the Well-tempered Environment marked the shift between the concept of interior to that of an artificial environment. Technology and new human needs in fact had become an integral part of architecture, defining a new paradigm to describe indoor space, that it was not any longer a concern of the singular living-cell but rather of its internal atmosphere.\nThe issue 21 of MONU describes the current development and the extreme consequences of what this Interior Urbanism means. As Brendan Cormier emphasizes in his article Some Notes Towards an Interior Archipelago: “90% of our lives are spent inside. Urban life is an interior affair.” This statement manifests the necessity to invert the canonical approach to read and plan cities, unfolding a new possible stream of research which considers how architecture affects our everyday life.\nClimate, or the need to erase the atmospheric conditions, is one of the trigger factors of the production of interior urbanism. Michael Piper & James Khamsi in Endless Architecture: Accidental Manifestos for the interior state that “the interior has grown to become an endless type of urban form” which provides an indoor urbanism between the malls of Toronto producing a protected shelter against a hostile climate. The system grew until the inclusion of the public buildings such as the station and the city hall overpassing the threshold of the commercial status of this air-conditioned environment.\nAs described in the essay of Inge Goudsmit and Adrienne Simons, maybe the most extreme scenario of indoor urbanism is the case of Hong Kong, where for specific contextual constraints such as the tropical climate and the lack of space, not only the city developed vertically but also the public space defined a network of inner connections where common life develops. Assuming as cases, the extremes of Canada and tropical China, it seems that the necessity for a hospitable public environment, despite the climate, is nowadays an unavoidable condition for the contemporary cities. This need for well tempered buildings represents an important factor for the homogenization of architecture worldwide, even stronger than the cultural one.\nIf it is true that certain internal conditions are able to create new urban spaces (as in the cases described above) the opposite is also true, that some buildings have assumed a character of indoor urbanity. One example is the article by Jonathan A. Scelsa Enfiladed Grids, The Museum as City, which highlights how museums are taking the configuration and the spatial experience of a city through the wise use of the intermezzo or the connective space between exhibition rooms such as in the work of OMA, REX, Jean Nouvel and SANAA.\nThis condition of blurring between interior and exterior is well described in the interview of Winy Maas, where the metaphor of a “3D Nolli”, in relation to the Nolli Map (1784) which first represented the enclosed publicly built surface as part of a continuum with the open spaces of Rome, is used as a tool to interpret a new generation of indoor public spaces like the Market Hall in Rotterdam. Scale and urban density, in the words of Winy Maas, are the “activators” of this kind of internal condition where the boundary between interior and exterior is totally blurred.\nReversing the traditional figure/ground opposition defined by Nolli Map, the poché which represents the private buildings unfold another, less porous, dimension of interior urbanism. In Some Notes Towards an Interior Archipelago, Brendan Cormier describes as an urban paradigm, the network of places that hosts the daily life of human beings. Far from the radical scenarios described by Archizoom in the No-stop City, our everyday life is not the one of the free man in an open indoor environment but rather it is confronted with the problems of ownership, differentiation and exclusivity, that define the gradient of permeability of this continuous interior. Visible and invisible boundaries restrict the possibility of wandering. In a moment in which, through the social networks, our lives have become public in almost every aspect, the interior has become the eminent space of privacy and thus intimacy and freedom.\nIn our opinion this different approach, so widely explored in MONU 21 in all its different aspects, represents a useful tool to overpass the dichotomy between the city as a system and the building as an object. If in fact we assume that there is a unifying field that relates to all the objects which compose the city, the urban dimension is no longer a matter of juxtaposition. With MONU 20 about Geographical Urbanism, this issue challenges the scale through which we are used to reading/to interpreting the city: from XS to XXL questions, there is a need to understand urban phenomena defining the new extents for urban life.\n— Claudia Mainardi and Giacomo Ardesio. Both of them graduated in Architecture at the Milan Politecnico, they are both part of the collective Fosbury Architecture and they are currently working at OMA in Rotterdam.","Jonce Walker, urban planner and sustainability professional at Terrapin Bright Green, articulates the need for biophilic interventions in urban places, offers examples found in New York City, and suggests solutions to integrate biophilic design into urban projects.\nBiophilia is our deep-seated connection to nature. It helps explain why the rhythm of crashing waves and the crackling of fire captivate us; why a view of nature can enhance our creativity; why shadows and heights instill fascination and fear, and why gardening and strolling through a park have restorative healing effects.\nRoutine connections with nature can provide opportunities for mental restoration, during which time our higher cognitive functions can sometimes take a break. Cognitive functioning encompasses our mental agility and memory, and our ability to think, learn and output either logically or creatively. For instance, directed attention is required for many repetitive tasks, such as routine paperwork, reading and performing calculations or analysis, as well as for operating in highly stimulating environments, such as when crossing busy streets and navigating urban places.\nThe mental health benefits of nature are important to understand as our world continues to rapidly urbanize and our cities simultaneously expand and densify. In less than fifteen years more than 60% of humans will live in urban places and there will be thirteen new megacity regions according to the UN. As we continue to select urban places to live, urgent attention must be given to embedding nature and natural systems into urban design to connect people to nature and safeguard our health and wellbeing. If we are not careful, our commute and daily experience within these urban places will be nothing more than glass, steel, and concrete.\nBiophilic Urban Acupuncture\nOn approach to this is urban acupuncture, a socio-environmental theory that combines contemporary urban design with traditional Chinese acupuncture, using small-scale interventions to transform the larger urban context. Just as the practice of acupuncture is aimed at relieving stress in the human body, the goal of urban acupuncture is to relieve stress in the built environment. Urban acupuncture is intended to to produce small-scale but socially catalytic interventions in the urban fabric. No needles necessary.\nBiophilic Urban Acupuncture (BUA) is the theory that threads and nodes of biophilic interventions in specific urban places can help improve people’s moods, connect people to place, and help improve mental health. Biophilic urban acupuncture blends two very important design concepts, biophilia and urban acupuncture.\nBUA has higher levels of effectiveness in dense cities versus suburban places due to the ease of pedestrian mobility. A resident that lives in a dense city will spend at least some time each day outside just by the fact that they will be walking to transit stations, walking to work, or walking to get a meal. (Though BUA is also of benefit in suburban places, the auto-centric street design and sprawled land-use in suburbs typically does not lend itself to high quality biophilic opportunities.)\nSmall BUA interventions\nBiophilic interventions do not need to be grand in scale to be effective. Positive impact on self-esteem and mood has been shown to occur in the first five minutes of experiencing nature (Barton &Pretty, 2010). Daily, unintentional exposure should be a priority when planning a BUA intervention. The intervention should be placed in a location that receives a large number of users but is embedded into an everyday habitat or commute. Smaller BUA interventions should be placed in locations throughout the city in a web-like structure, so that users with different destinations will encounter biophilic experiences, no matter their destination or purpose of travel.\nLarger BUA interventions\nThe larger biophilic experience should be placed in an area of the city that can serve a substantial proportion of the population and should include as many biophilic patterns as possible. These are typically parks such as the Olmsted designed Central in New York City or the Tommaso Francini designed Luxembourg Garden in Paris. Large parks that are centrally located within a city and connected by good transit will provide a robust BUA experience to a greater number of residents than parks located in the periphery.\nThe High Line Park in New York City is a converted disused elevated railroad tracks into a much-loved biophilic intervention. Image courtesy of Dean Shareski via Flickr.\nIn the urban environment, there are two ways to capitalize on the multi-sensory attributes of water to enhance the experience of a place. First, simulating or constructing water features (water walls, fountains, or falls; aquaria; water imagery) in the built environment—indoors and out—creates positive effects for inhabitants. Although, it is worth mentioning that water and energy-intensive installments may create other issues. Second, it is possible to amplify the presence of naturally occurring water (lakes and ponds; streams, creeks, and rivers; rainfall; arroyos) to help inhabitants become increasingly aware of the surrounding environment. The Fountains and Water Features of NYCA space with a good Presence of Water condition feels compelling and captivating. Fluidity, sound, lighting, proximity and accessibility each contribute to whether a space is stimulating, calming, or both.\nThe water wall at Paley Park is a wonderful and captivating intervention. Image courtesy of Wally Gobetz via Flickr.\nThe Trees of NYCA space with a good Visual Connection with Nature feels whole, it grabs one’s attention and can be stimulating or calming. It can convey a sense of time, weather and other living things.\nMillionTrees NYC is a citywide, public-private program that has planted one million new trees across the City’s five boroughs over the past decade. Beyond the numerous ecological benefits, strengthening New York City’s urban forest plays a positive role in helping inhabitants reduce stress and bolster self-esteem, mood, and parasympathetic activity.\nPopUp Forest: Times Square is emulating the pop-up restaurant experience by transforming a public plaza in Times Square into a large-scale, temporary urban forest installation. The goal is to foster a movement to re-define cities with nature in mind and to create an urban oasis for wildlife while helping New Yorkers get more familiar with nearby nature.\nA proposed art installation, PopUp Forest, will bring a forest into the ultra dense Times Square. Image copyright http://www.popupforest.org/\nBiomorphic subway art\nBiomorphic forms & patterns are symbolic references to contoured, patterned, textured or numerical arrangements that persist in nature. A space with good biomorphic forms & patterns feels interesting and comfortable, possibly captivating, contemplative or even absorptive.\nBiomorphic subway art illustrates how this concept has been implemented in New York City subway stations. The passageway between 42nd Street and 5th Avenue includes artistic depictions of natural systems such as tree roots and animal burrows, and the Jay Street/Metro Tech Station depicts glass mosaic art with various animal species, including starlings, sparrows, lion fish, parrots, tiger beetles, and koi fish.\nNatural scenes and biomorphic forms and patterns can transform a dreary subway passage. Image courtesy of Wally Gobetz via Flickr.\nBrooklyn Bridge Park Tidal Wetlands\nA space with a good connection with natural systems evokes a relationship to a greater whole, making one aware of seasonality and the cycles of life. The experience is often relaxing, nostalgic, profound or enlightening, and frequently anticipated.\nThe tidal wetlands at the recently expanded Brooklyn Bridge Park offers a prime example of connecting an urban landscape with the local ecosystem. The wetlands heighten awareness of natural properties of the East River and hopefully promote environmental stewardship of the Park and surrounding area.\nTidal wetlands at the Brooklyn Bridge Park integrate natural with built systems. Image courtesy of Julienne Schaer for Brooklyn Bridge Park.\nDIY Biophilic Urban Acupuncture\nBiophilic Urban Acupuncture does not need to hinge on large budgets or city agencies to have dramatic impact. You can play a role in integrating BUA elements in your neighborhood now. Here are a few strategies to help you get started:\nSeed bombs are balls made from volcanic red clay or compressed soil containing different varieties of native species seeds that can fit in the palm. Usually other additives are included in the ball such as compost or humus to provide microbial inoculants. They can be dropped or tossed onto vacant lots or public places that are in need of beauty and vegetation. Seeds that support pollinators such as honey bees or butterflies are better as they will reinforce the Visual Connection to Nature and Connection to Natural Systems biophilic patterns.\nWHERE: Seed bombs work well in places that have exposed soil and in places that are difficult to access.\nHOW: What was once strictly a DIY project, seed bombs can now be purchased online, in stores, or even from vending machines.\nTree pits are areas around urban trees that provide a small pervious surface for the roots to breath and absorb water. These can be transformed from a small often neglected patch of soil into a strong BUA intervention. If done with care, you can plant flowers or root bulbs in the pits. Additional interventions could be small benches around the tree pit which will create a reason for people to linger under the tree reinforcing the biophilic response.\nWHERE: Most trees that are located in public right-of-way (ROW) are the responsibility of the community to take care of. Check with your neighbors about which tree pits are available to improve.\nHOW: Using a hand cultivator, loosen the topsoil as this is usually compacted. Spreading a thin layer of mulch will help the tree absorb water and reduce evaporation. Plant in-season flowers and enjoy!\nGuerilla gardening is the act of planting vegetation in spaces that the gardeners do not have the legal rights to use. These sites are typically abandoned or areas that are be substantially neglected. BUA can have large impacts in these neglected areas via guerilla gardening because the intervention is typically noticed and appreciated by the community regardless of who did it, and taken care of for years. This intervention supports the connection with natural systems, visual connection with nature, and non-visual connection with nature biophilic patterns\nWHERE: Typically, guerrilla gardening occurs in spaces that are vacant or underutilized spaces. This intervention originated in NYC in the 1970’s by residents throwing balloons filled with local seeds, water, and fertilizer into empty lots.\nHOW: This BUA intervention is best done with a group of neighbors and/or friends. Locate the site that is in need of the garden and make a plan for the plantings and improvements. Pre-planting site work may need to be done such as cleaning up junk, trash, and debris.\nWe know that cities will continue to morph. We also know that we enjoy listening to a water fountain, seeing a butterfly, or watching leaves shake with the help of a slight breeze. Let’s work to ensure that Biophilic Urban Acupuncture is part of the toolkit to help shape the places where we want to live.\nAbout the Author\nSanity and Urbanity:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:99fbe97d-520d-46c0-a139-d03cc1f9ae1c>","<urn:uuid:5ca439ae-7417-4cbe-bb79-c94c76ae8fa7>"],"error":null}
{"question":"As someone interested in climate adaptation, I'd like to know how the UK's climate risks compare with the technological solutions being developed to address atmospheric CO2?","answer":"The UK faces significant climate risks including temperatures rising up to 3.8°C by 2070, increased flooding, and dangerous heatwaves (as seen in 2022 with 25,000 wildfires and 3,000 excess deaths). To address such risks, the UK is investing £5.2 billion in flood defenses and over £750 million in nature-based solutions. Meanwhile, technological solutions are being developed to remove CO2 from the atmosphere, with companies like Climeworks and Origen Power developing direct air capture technologies that can remove several tonnes of CO2 daily, though these currently cost between $94-232 per tonne and need to be scaled up significantly to make a meaningful impact.","context":["Britain has decarbonised faster than prosperous economies like the United States, Australia, Japan, France and Germany.\nIt has shut down its coal power plants, which has allowed it to slash its carbon emission faster than any wealthy country since 1990, which is down to 44%, according to the Department for Business, Energy and Industrial Strategy (BEIS), during a period while its economy by over 75%. The government will follow this strategy to meet the goal of zero emissions by 2050.\nThe government published, in October 2021, its Net Zero Strategy. The 360+ page document laid out the country’s long-term plan to end its domestic contribution to climate change, including removing carbon from its power, retiring the ICE cars, phasing out gas boilers from homes, and achieving a net zero GHG economy by 2050.\nClimate change in the UK and the need for climate adaptation\nThe United Kingdom is already seeing the effects of climate change. In the most recent decades, between 2012 and 2021, the country’s average temperatures have risen by 1.0°C warmer than the 1961 to 1990 average. All 10 of the warmest years in the UK have occurred since 2003. The year 2022 was the UK’s hottest year on record, with an average year-round temperature above 10°C seen for the first time.\nClimate models project that if GHG emissions continue to increase, in 2070, the UK will have warmer (up to 3.8°C) and wetter (up to 39%) winters compared to its 1990 climate. UK summers will also become hotter, with temperatures rising between 1.3 and 5.1°C depending on the region. Hotter summers can cause more heat waves and increase the health risks to vulnerable people.\nIn July 2022, the country’s temperatures surpassed 40°C, setting a new record. The chance of the UK seeing 40°C days could be ten times more likely in the current climate than in the past. Hot summers also increase wildfire risk, as seen in the summer of 2022. According to the BBC, last year’s intense heatwave, the worst the country has ever seen, brought 25,000 wildfires as hospitals struggled to cope with 3,000 more deaths in the over-65 than usual. Alongside flooding, overheating is one of climate change’s most significant risks to the UK’s population.\nThe scale of the climate impacts that the UK is witnessing, intense heatwaves, record wildfires and floods, make it clear that the government should treat climate adaptation and resilience as its national priority.\nOn 17 July 2023, the government released its third and latest climate change adaptation plan. “From helping homes, schools, and hospitals prevent overheating to safeguarding our food and energy supply chains from disruption, the publication of the third National Adaptation Programme marks a step-change in the UK government’s approach to climate adaptation, setting out in one place the ambitious programme the government is undertaking to address the key climate risks facing the country.”\nUK’s government is already investing billions in adaptation, including £5.2 billion in flood and coastal schemes in England, over £750 million for the Nature for Climate Fund, which supports nature-based solutions for climate resilience, and £80 million for the Green Recovery Challenge Fund which creates jobs in nature recovery and conservation – all of which play a crucial role in enhancing the country’s resilience to climate change. £5.2 billion is also being invested in new flood and coastal defences – and the number of government-funded projects, including nature-based solutions, will be doubled by 2027.\nNAP3 provides a comprehensive and forward-looking plan for the UK in adapting to the risks and opportunities of climate change.\nSecretary of State for Environment Therese Coffey describes the plan as a “step change in our approach to managing the risks of climate change, moving us from planning to action. It represents the beginning of a 5-year programme of work across government to build our resilience to climate change.”\nRead the full document by clicking this link: The Third National Adaptation Programme (NAP3) and the Fourth Strategy for Climate Adaptation Reporting.\nGovernment sets out adaptation programme to tackle climate impact. (2023 July 17). Retrieved from https://www.gov.uk/government/news/government-sets-out-adaptation-programme-to-tackle-climate-impact\nHow Britain decarbonised faster than any other rich country. (2021, February 15). The Economist. Retrieved from https://www.economist.com/britain/2021/02/15/how-britain-decarbonised-faster-than-any-other-rich-country\nRecord excess deaths in UK’s heatwave summer. (2022, October 7). BBC. Retrieved from https://www.bbc.com/news/health-63171417","Germany aims to cut its greenhouse gas emissions some 40 percent by 2020. This target rises to 55 percent by 2030 and 95 percent by 2050 when compared with 1990 levels. Many other countries across the world have issued similar pledges.\nThe bad news, though, is this won’t be good enough: efforts to decarbonise the global economy have been delayed to such an extent that reducing emissions now comes too little, too late.\nClimate prediction models indicate that, in order to meet the targets established by the Paris Agreement, carbon dioxide (CO2) will have to be removed from the atmosphere on a substantial scale.\nIn fact, scientists now estimate limiting the global temperature rise to two degrees Celsius above pre-industrial levels will require 10 billion tonnes of CO2 to be removed from the atmosphere every year from 2050 onwards.\nReforestation is the most natural way of sucking carbon out of the air, but this is time consuming and would need to take place on a gigantic scale. As global populations increase, convincing governments to plant trees on spare land, rather than build homes, is a difficult sell.\nA combination of renewable energy investment and carbon-removal solutions may be the only way to avoid an environmental disaster in the coming years\nOther businesses are looking at carbon capture and sequestration techniques that trap CO2 before it reaches the atmosphere, but this approach is often only carbon-neutral at best.\nIn order to solve the planet’s carbon conundrum, a number of businesses have started thinking outside the box, employing cutting-edge technology to remove existing CO2 from the planet’s atmosphere. In many cases, their ideas have proved to be effective; whether they are financially feasible is another matter entirely.\nWith the likelihood that humanity will be able to curb its CO2 emissions in time to avert catastrophe looking increasingly slim, a number of businesses have begun to search for alternative options. Their approaches to CO2 removal vary widely, with each methodology bringing its own advantages and disadvantages.\nOne of the more unusual ideas involves supplying natural gas to a high-temperature fuel cell. Half of the energy created is converted into electricity, and the other half is used to decompose limestone into lime and CO2.\nAs this activity produces CO2, it may not initially be clear how it helps with the planet’s greenhouse gas problem. However, the entire process is actually carbon-negative. As all of the CO2 being produced – both from the fuel cell and the lime kiln – is pure, it can be used or stored underground at a low cost. The lime can also be used to trap CO2.\nThe company pioneering this method of carbon air capture, Origen Power, estimates it can remove 600g of CO2 from the air for every kWh of electricity generated. In contrast, electricity produced by burning natural gas typically emits around 400g of CO2 for the same energy output.\nOrganisations like Canada’s Carbon Engineering and the Zurich-based Climeworks have opted for a different approach, utilising direct air capture technology. The latter develops, builds and operates plants that capture CO2 from ambient air through a cyclic adsorption-desorption process.\n“At the heart of the process is a filter material, which selectively captures CO2,” Louise Charles, Communications Manager at Climeworks, told The New Economy. “The first step involves air being drawn into a collector using a fan.\n“CO2 is then captured on the surface of a filter material – also known as the adsorption stage. The second step begins when saturation has occurred. Then, the collector is closed and the temperature increased to 100 degrees Celsius, releasing the CO2 (desorption) at a purity of over 99 percent. The CO2 is then cooled to 45 degrees Celsius, collected and delivered where necessary.”\nCost of carbon removal (per tonne)\nIt should be remembered there is no silver bullet to the problem of climate change. The disparate ideas that businesses like Origen Power and Climeworks are exploring may all need to be deployed together if a decarbonised future is to be achieved.\nUltimately, a combination of renewable energy investment and carbon removal solutions may be the only way to avoid an environmental disaster in the coming years.\nCosting the Earth\nThe suitability of carbon-removal projects is not in doubt – their affordability is. In 2011, a study led by researchers from the Massachusetts Institute of Technology found that, when focusing on ambient air, the cost of carbon removal was likely to be more than $907 per tonne of CO2 removed.\nAlthough they are not truly carbon-negative, powerplant scrubbers can boast a figure of between $50 and $100 for every tonne of CO2 prevented from reaching the atmosphere.\nRecently, however, scientists have become increasingly optimistic that carbon removal is not as expensive as first feared. An updated analysis, published in June, suggested carbon capture could be achieved at a price between $94 and $232 per tonne if existing technologies were scaled up. Many of the businesses working in this field are working on the assumption that their current costs can be significantly reduced.\n“We are already commercially viable and, as we continue to develop our technology, we will become increasingly cost competitive across the world,” Charles explained. “We have a detailed cost-reduction roadmap in place and are confident we’ll reach a price level of $200 per tonne of CO2 in three to four years’ time. Our long-term goal of $100 per tonne of CO2 is achievable within the next five to 10 years – or by 2030 at the very latest.”\nScaling up will be key to optimising outgoings, and progress on this front has been steady, if not spectacular. When Climeworks was founded in 2009, it was only capable of removing a few milligrams of CO2 in a 24-hour period; now, several tonnes are being captured every day.\nThe company is aiming to scale up by a factor of one million in the next five to 10 years in order to make a broader impact. To achieve this, more customers and additional funding will be required.\nThe next step\nCarbon capture, in one form or another, has been implemented since the 1970s, but the technology’s progress has been slow. Globally, there are 17 large-scale plants dedicated to carbon capture, most of which are in the US. Altogether, they remove around 40 million tonnes of CO2 per annum – equivalent to just one percent of yearly global emissions.\nGuaranteeing these facilities receive the right government support will be vital to their development. Subsidies for carbon removal need to be implemented to ensure the right economic incentives are in place for firms. Renewable energies have benefitted hugely from this approach and there is no reason why CO2 removal wouldn’t similarly profit.\nBusinesses also need to think carefully about what they do with the CO2 once they remove it from the air. Making it into synthetic fuels is one option; burying it underground is another.\nCharles said her company “sells high-purity, high-concentration, air-captured CO2 to the food, beverage and agriculture market, [as well as] for the synthesis of renewable fuels and materials”. By forging external partnerships, carbon-removal firms gain access to additional revenue streams and ensure companies can purchase sustainable supplies of CO2.\nIt is regrettable that mankind has reached a stage where reducing carbon emissions will not be sufficient to stop temperatures exceeding globally agreed targets. But all hope is not lost: while CO2 removal can’t turn back the clock on decades of polluting human activity, it could provide a way to avoid the potentially devastating consequences of climate change."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c36bbff6-4e70-4bb8-9df9-fa82526109fe>","<urn:uuid:aa0edff0-4d25-4ea5-9b0d-02b5098abec5>"],"error":null}
{"question":"I'm doing research on marine ecosystems in New Zealand. How is climate change affecting kelp forests along NZ's coastline?","answer":"Climate change is severely impacting New Zealand's kelp forests through multiple threats. Rising ocean temperatures and frequent marine heatwaves are placing kelp forests under thermal stress, affecting their range and physiology. Additionally, coastal 'darkening' from increased sediment levels (due to extreme weather events) is forcing seaweeds to retreat to shallower waters as they can no longer survive in deeper reefs due to low light. Ocean acidification is also threatening calcareous seaweeds, particularly coralline algae, by slowing their growth and ability to spread. These combined stressors are particularly concerning for sensitive species like giant kelp and bull kelp, especially at their northern limits.","context":["NZ’s Vital Kelp Forests are in Peril from Ocean Warming\nYears of almost non-stop marine heatwaves are stressing New Zealand’s kelp forests. But as we show in our new research, ongoing ocean warming is only one of several threats to these unique and important coastal seaweed ecosystems.\nMany seaweed species are sensitive to changes in the ocean’s acidity and coastal “darkening” – changes in colour and clarity – is forcing some to retreat to shallower waters. All these stress factors combined place these crucial habitats in peril, with consequences for all species that depend on them.\nNew Zealand has the ninth longest coastline in the world (at about 15,000 kilometres). This is almost twice the length of Australia’s great southern reef, which has been valued at AUS$10 billion annually.\nNo equivalent valuation has been calculated for New Zealand’s seaweed-dominated rocky reefs, but we know they provide crucial habitat for economically and culturally important species such as pāua (abalone), kina (sea urchins), rock lobster and near-shore finfish.\nOur ability to predict the future impacts on these species depends on our understanding of how the coastal habitats they require are changing.\nRising heat and dimming light\nNew Zealand’s seaweed ecosystems include canopy-forming large brown algae as well as several understorey species. Driven by rising ocean temperatures, more frequent, intense and longer marine heatwaves already place these kelp forests under thermal stress.\nWe predict that marine heatwaves will change the range and basic physiology of many seaweed species, removing sensitive ones from the northern edges of their ranges and slowing growth rates of most.\nSeaweeds require sunlight to photosynthesise and grow. But increasingly frequent erosion events from extreme weather like cyclone Gabrielle raise sediment levels in coastal seawater, leading to coastal darkening.\nSediment running off the land, made worse by storm events, has already lowered photosynthetic rates of seaweeds in several regions. New Zealand’s geologically young landmass is eroding rapidly in some areas and research shows seaweed communities are creeping further up the reef into shallow waters. They can no longer survive in deeper reefs because of low light.\nThis darkening will intensify if we don’t halt erosion and remediate lands. The effects of sedimentation have likely already limited the distribution of some seaweeds and will continue to do so in future.\nWhen the sea becomes more acidic\nOcean acidification, which makes seawater more acidic because of the excess carbon dioxide it is absorbing, threatens the ability of calcareous seaweeds to grow. It puts the survival of sensitive coralline algae (the pink algae covering coastal rocky reefs) at risk, slowing their growth and ability to spread to new space.\nThese red seaweeds form skeletons made of calcium carbonate and represent the seaweed group most sensitive to ocean acidification. Within coralline algae, some species are more sensitive than others.\nIn New Zealand, coralline algae provide important surfaces for larval settlement of many species, including pāua and kina. Whether the species of coralline algae that provide vital settlement substrates are sensitive or robust to ocean acidification remains unknown.\nEcological impacts on seaweed ecosystems\nSeaweed communities provide enormous benefits to New Zealand. They provide food and habitat for several marine invertebrates and finfish species of both cultural and commercial value, including pāua, kina, moki, snapper, rocklobster, blue cod and butterfish.\nIf seaweed ecosystems are altered, these species will experience changes in food supply and habitat availability.\nGiant kelp (Macrocystis pyrifera) and bull kelp (Durvillaea spp.) are important habitat formers and food suppliers for other species. But they are also extremely sensitive to the impacts of temperature stress.\nMarine heatwaves and ongoing ocean warming likely threaten them throughout their ranges, especially at their northern limit.\nShifting population ranges and invasions\nWarming oceans also facilitate the spread of Australian long-spined urchins (Centrostephanus rodgersii) which will threaten seaweed communities in northern New Zealand.\nPopulations of this warm-water species have already expanded in some parts of New Zealand, similar to Tasmania where it invaded during the 1950s and caused widespread “urchin barrens” where the urchins graze through kelp beds, leaving few seaweeds.\nNew Zealand is home to around 1,100 species of seaweeds. Many are poorly documented and with unknown ranges.\nThe combined effects of climate change will likely result in the extinction of some species with very narrow ranges. This is especially true for species that are sensitive to changes in light or temperature and currently live near the edges of their physiological limits.\nTo add to all of this, we don’t know how the impacts of the invasive caulerpa seaweeds Caulerpa brachypus and Caulerpa parvifolia will interact with climate change.\nThe degradation of coastal ecosystems is another reminder that we must move to eliminate the use of fossil fuels, as well as limiting overfishing and sedimentation.\nThere is potential to breed high-temperature, low-light and low-pH resistant strains of major species to help restore these ecosystems in the future, but we need strategic government investment in integrated coastal management and climate adaptation to save them in the first place.\nChristopher Cornwall, Lecturer, Te Herenga Waka — Victoria University of Wellington\nWendy Nelson, Senior Research Fellow, Auckland War Memorial Museum\n(Source: The Conversation)"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:cb9d915d-17a5-4459-8d29-65b4fbf54c40>"],"error":null}
{"question":"What was the breakthrough discovery about Efimov states in 2006?","answer":"In 2006, scientists achieved the first experimental observation of Efimov states, which were mysterious quantum states that had been theoretically predicted by Russian scientist Vitali Efimov in the early 1970s. This discovery helped solve an old mystery in physics.","context":["10 November 1961 |\n|Nationality||Austria (since 17 Dec 2007) Germany (since 10 Nov 1961)|\n|Institutions||University of Innsbruck|\n|Alma mater||University of Hannover|\n|Thesis||Light-pressure-induced phenomena in an atomic gas : modification of absorption and dispersion profiles (1989)|\n|Doctoral advisor||Jürgen Mlynek\n|Known for||ultracold atoms, Bose–Einstein condensation|\n|Notable awards||Wittgenstein Award (2005)|\nRudolf Grimm (born 10 November 1961, in Mannheim, West Germany) is an experimental physicist from Austria. His work centres on ultracold atoms and quantum gases. He was the first scientist worldwide who, with his team, succeeded in realizing a Bose–Einstein condensation of molecules.\nGrimm graduated in physics from the University of Hannover in 1986. From 1986 to 1989 he was a post-graduate researcher at the ETH Zurich (Swiss Federal Institute of Technology), then went on to the Institute of Spectroscopy of the USSR Academy of Sciences in Troitsk near Moscow for half a year. He spent the next ten years in Heidelberg as a researcher at the Max Planck Institute for Nuclear Physics. In 1994, Grimm applied to the University of Heidelberg to qualify as a professor by receiving the \"venia docendi\" in experimental physics. In the year 2000, he was appointed to a chair in experimental physics at the University of Innsbruck, where he has been Dean of the Faculty for Mathematics, Computer Science and Physics since 2005 and Director of the Research Center for Quantum Physics from 2006. Since 2003, Grimm has also held the position of Scientific Director at the Institute for Quantum Optics and Quantum Information (IQOQI) of the Austrian Academy of Sciences (ÖAW). Grimm is married, with three children.\nThe work of the experimental physicist concentrates on Bose–Einstein condensation of atoms and molecules and on fermionic quantum gases. In 2002 his working group succeeded for the first time ever to produce a Bose–Einstein condensate from caesium atoms. In the following year, the team produced the first Bose–Einstein condensate of molecules (simultaneously with Deborah S. Jin's group at JILA, Boulder, Colorado). In 2004, the Innsbruck scientists achieved a Fermionic condensate. Their work was ranked among the top ten global achievements in the natural sciences for that year by Science. In his work on collective oscillations and pairing energies, Grimm found first evidence of the flow of particles without any loss of energy (superfluidity) in Fermi condensates. Meanwhile Grimm and his team have succeeded in producing more complex molecules in ultracold quantum gases. Currently Grimm is concentrating his efforts on producing mixed condensation from atoms of different elements. In 2006, his working group also managed to lift the veil on an old mystery of physics: they succeeded in the first experimental observation of Efimov states, mysterious quantum states that the Russian scientist Vitali Efimov had theoretically predicted in the early 1970s.\nGrimm has received numerous awards for his achievements. In 2005 he was presented with the Wittgenstein Award, Austria's highest scientific accolade. In the same year, the Austrian daily paper Die Presse made him „Austrian (Researcher) of the Year 2005\". Years before, he had won the Gerhard Hess Prize, a new blood stipend of the German Research Foundation (DFG) (1996), and the Silver Medal of the ETH Zurich (1989). Recently he received the Beller Lectureship Award of the American Physical Society (APS) (2007), the Science Award of the Region of Tyrol (2008) and was named „Austrian Scientist of the Year 2009\" by the Austrian Club of Education and Science Journalists. In 2006, Grimm became a full member of the Austrian Academy of Sciences.\n- Bose–Einstein condensation of cesium, T. Weber, J. Herbig, M. Mark, H.-C. Nägerl, R. Grimm. Science 299, 232 (2003)\n- Bose–Einstein condensation of molecules, S. Jochim, M. Bartenstein, A. Altmeyer, G. Hendl, S. Riedl, C. Chin, J. Hecker Denschlag, and R. Grimm. Science 302, 2101 (2003)\n- Breakthrough of the Year 2004\n- Evidence for Efimov quantum states in an ultracold gas of caesium atoms. T. Kraemer, M. Mark, P. Waldburger, J. G. Danzl, C. Chin, B. Engeser, A. D. Lange, K. Pilch, A. Jaakkola, H.-C. Nägerl and R. Grimm. Nature 440, 315 (2006)"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:fa6ad8b3-ad24-4090-a5a0-f2ef3d41ac57>"],"error":null}