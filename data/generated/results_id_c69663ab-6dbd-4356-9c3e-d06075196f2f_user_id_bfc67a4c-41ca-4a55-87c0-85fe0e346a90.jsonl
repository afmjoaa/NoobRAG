{"question":"How are shooting fouls penalized, and what distinguishes them from regular personal fouls in basketball?","answer":"A shooting foul occurs when a player is fouled in the act of shooting. If the shot goes in, the shooter gets one free throw. If the shot is missed, the shooter receives two free throws, or three free throws if fouled beyond the 3-point line. In contrast, regular personal fouls result in the ball being awarded out of bounds to the opposing team, unless the team has reached their foul limit. Once a team accumulates 7 team fouls, their opponent enters a bonus free throw situation. According to NBA rules, any illegal contact that affects a player's speed, direction, balance, or rhythm is considered a personal foul.","context":["Learn the common basketball fouls and referee hand signals found in college, high school, and youth basketball rules.\nNot all contact on the basketball court is illegal, but there are some specific fouls outlined in the rules that are designed to keep the contact from becoming too aggressive.\nIn the early history of basketball, before there were any standardized regulations, play could pretty rough at times.\nOver time, rules regarding basketball fouls and violations were put into place to keep the game under control.\nThe first thing you need to know is that basketball fouls and violations are not the same thing.\nA foul is a penalty called by the referee for rough play to keep a player from gaining an advantage over another player.\nIt's an infraction of the rules that results in a player being charged and penalized.\nEach player is allowed 5 basketball fouls before they're removed from the game.\nWhen a player commits a foul, the ball is awarded out of bounds to the opposing team, unless the foul occurs during the act of shooting.\nA shooter who is fouled is awarded 1, 2 or 3 free throws depending on the location of the foul and whether or not the shot went in.\nA violation, on the other hand, is an infraction of the rules of basketball that cause a team to lose possession of the ball. There are no free throws taken when a violation occurs.\nBasketball violations include such things as:\nThese are just a few of the violations that occur during a game.\nWhen the defender makes illegal personal contact with an opponent who may or may not have the ball. Blocking is called when the defender impedes the progress of the opponent.\nWhen an offensive player makes contact with a defender who has already established a set position. A player with the ball must avoid contact with a stationary defender by stopping or changing direction.\nWhen two opponents commit fouls against each other at the same time.\nWhen a player with the ball swings his elbows excessively and makes contact with a defender.\nA personal or technical foul, which is violent in nature.\nExamples: fighting, striking, kicking, or kneeing an opponent.\nConsequences: Fouled player shoots two free throws, his team gets possession of the basketball out of bounds, and the player committing the foul is kicked out of the game.\nWhen a defender makes repeated contact with her hands on her opponent.\nGrabbing or touching an opponent to interfere with his freedom of movement.\nForm of blocking that occurs when a player setting a screen is still moving at the time the defender makes contact with her. The intention is to prevent the defender from getting around the screen.\nA personal or technical foul, which keeps the opponent from capitalizing on an advantageous situation.\nCould be contact away from the ball or contact when a defender is not making a legitimate attempt to play the ball or a player. Also occurs when a player causes excessive contact with an opponent.\nAn intentional foul results in two free throws and possession of the basketball out of bounds for the player that was fouled.\nA foul caused by an offensive player, usually in the form of charging.\nWhen a player jumps on or over the back of an opponent. It usually occurs when players are battling for rebounds or when a defender takes a shooter's pump fake and comes down on top of him.\nIllegal contact with an opponent while the ball is live, which hinders the opponent's offensive or defensive movement. Also includes contact by or on an airborne shooter when the ball is dead. It results in possession of the ball out of bounds and a 1-and-1 free throw situation for the team that was fouled if they are above their foul limit.\nWhen an offensive player in possession of the ball makes contact with a defender who has already established a set position. A player with the ball must avoid contact with a stationary defender by stopping or changing direction.\nWhen a player excessively pushes or bumps into an opponent. It usually occurs when players are fighting to get position for a rebound or when a defensive player shoves a shooter or dribbler.\nWhen a defender attempting to steal the basketball reaches in with her hands and makes contact with the ball handler.\nA foul called when a player is in the act of shooting. If the shooter makes the shot, she is awarded one free throw. If she misses the shot, she gets two free throws unless she was fouled beyond the 3-point line in which case she is granted three free throws.\nAny foul charged to a team. Once a team reaches 7 team fouls, its opponent is in a bonus free throw situation.\nA non-contact foul by a player; an intentional or flagrant contact foul while the ball is dead; or a violation charged to the head coach because of violations on the sideline or from bench personnel. A technical foul results in two free throws and possession of the ball out of bounds.\nUsing the foot or leg to cause an opponent to fall or lose his balance.\nClick here to go to the National Federation of State High School Associations (NFHS) web page where you can find more info about basketball fouls and violations along with the latest rule changes and interpretations.","Newcomers to basketball may be wondering – what is a reach-in foul in basketball?\nIf you are confused about the concept of “reaching-in”, then you aren’t alone. Reach-ins are sometimes legal and sometimes not, which can drive basketball newbies crazy!\nIn this post, I will talk about what reaching-in is when it is considered a foul, and cover everything else that you should know about it.\nWhat Is The Reach-In Foul In Basketball?\nFirst things first, the reach-in foul is not formally defined by any basketball rules (including NBA rules). Fans and journalists commonly use the terms “reach-in” or “reaching in” when defenders come in illegal contact with an opponent in possession of the ball.\nThe so-called reach-in foul occurs when a defending player reaches toward the orange in an attempt to steal the ball but instead illegally contacts the ball handler.\nThe act of reaching in by itself is not considered a foul in any rulebook, be it NBA or FIBA rulebooks. However, depending on the circumstances, it may indeed be a foul.\nWe’ll have to take a look at the NBA rulebook to understand when it is and when it is not an offense.\nNBA’s definition of illegal contact\nIllegal contact is defined by Rule 12B (Personal Foul) of the NBA 2019-2020 rulebook.\nRule 12B Section I point (a) of the NBA rulebook states that holding, pushing, charging into, or preventing the progress of an opponent by bending the body into a position that is not normal or extending a hand, arm, leg, or knee is not allowed.\nDefending players who are guarding an opponent in possession of the ball are not allowed to initiate contact with him as well. Touching forearms, hands, or body checks are not allowed.\nThere are some exceptions to this, however, defined under Section I point (b) subpoint (a). For example, incidental contact with an offensive player’s hand is not a reach-in foul if it doesn’t affect the player’s speed, direction, balance, or rhythm.\nRule 12B Section I point (c) also defines that if the actions of a player against an opponent cause illegal contact with another opponent, the very first player is to be charged a personal foul.\nFinally, point (e) states that contacting the hand of an offensive player while that part of the hand touches the ball is legal. And this is exactly where the vast majority of reach-in fouls occur.\nIn the section “Comments on the rules”, it is also defined that the hand is considered part of the ball when it contacts the ball. Touching that part of the hand is therefore considered legal.\nWhat are the penalties for “reaching in”?\nIf illegal contact with the hand of an offensive player occurs:\n- The offensive player gets a personal foul.\n- If the illegal contact was caused by the defender, the offended team is charged with a team foul.\n- No team fouls are called if one member of each team has a personal foul or if the foul was committed against an offensive player.\nAfter a reach-in foul is called, the offended team is typically awarded a ball-out-of-bounds. But depending on the circumstances, the offended team may be awarded up to three free throws as well.\nFrequently Asked Questions\nFinally, in this section, I’ll give answers to some FAQs to give you a better idea of when reaching in is legal and when it is a foul.\nCan you grab the ball out of someone’s hands in basketball?\nThis is indeed allowed as long as you:\n– Only touch the ball.\n– Touch the opponent’s hand only in the area that is contacting the ball. For example, if just the fingertips of a player are contacting the ball, then only hitting their fingertips will be considered legal.\n– Do not touch the opponent.\nIf you meet these three conditions, then touching the ball is legal, and no reach-in foul will be called against you. This is how you can legally steal the ball.\nCan you slap the basketball out of someone’s hand?\nAs long as you only touch the ball or the part of the ball handler’s hand that touches the ball, you are allowed to do this.\nCan you punch the ball in basketball?\nNo, Rule 10 Section IV point (a) states that “a player shall not kick the ball or strike it with a fist”\nIs slapping the hand a reach-in foul in basketball?\nSlapping or otherwise contacting the hand of a player in possession of the ball is only legal in two cases:\nWhen that part of the player’s hand touches the ball. For example, if only the fingertips touch the ball, then touching the offensive player’s hand anywhere else is a reach-in foul.\nWhen the hand contact is incidental – that is, if the ball handler isn’t slowed down and doesn’t lose his balance or rhythm as a result of the hand contact.\nThe 4th Quarter\nIn the end, reaching in basketball may be called as a foul if any illegal contact occurs between the defender and the ball handler. This explanation obviously omits a lot of details, but you get the point.\nReaching in can lead to free throws for the other team if done improperly, but they give you the best chances to steal the ball from your opponent and turn defense into offense in the blink of an eye.\nI suggest that you go read the NBA rulebook on your own too. I’ve only covered the most important details – there are plenty of other things in the rulebook that might be useful for you."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:629a7973-cb0d-404a-b7f3-fed2292ac945>","<urn:uuid:c186197b-0e4e-4e7d-80f2-6a05deecac1f>"],"error":null}
{"question":"As someone researching leadership styles in public service, what were the key differences in how Ernest Clark Jr. and Dan Stroh approached community development in their respective roles, and what lasting impacts did they leave on their communities? Please provide specific examples from their careers.","answer":"Ernest Clark Jr. focused on direct mentorship and individual empowerment, recognized as a 'Servant Leader' by Toastmasters International. He demonstrated this through mentoring new Toastmasters members, providing affordable workspace for starting attorneys and real estate agents, and serving in leadership roles in organizations like NARAB and OCCUR. Dan Stroh, on the other hand, approached community development through systematic urban planning, focusing on managing Bellevue's growth from 87,000 to 140,000 residents and 90,000 to over 150,000 jobs. He worked on focusing growth in appropriate areas while preserving others, and helped shape Bellevue's development into a dynamic multi-center city with several major job hubs beyond downtown.","context":["Ernest Clark, Jr. Photo by Anintard Henderson, Photographers@large.\nBy Randie Ellington\nIf you have ever met Mr. Ernest Clark, Jr., you would remember him for many things. In particular, you will remember his constant, sincere smile and his dedication to help others be successful. Despite his untimely passing in 2017, there is so much more to know about the well known and respected Oakland real estate and non-profit community leader.\nClark was born in Fresno and raised in Long Beach andgraduated from Long Beach Polytechnic High School. He served in the Air Force, where he obtained his Bachelor’s Degree in Elementary Education. In 1969, he began his professional career as a grade school teacher while he earned his Master’s Degree. In 1972, he moved his family to Castro Valley and accepted employment with the U.S. Department of Education, where he worked to open education centers in developing countries throughout the world.\nThe family soon moved to Oakland in 1978 and Clark decided to change careers. He started selling real estate first from his home, and then for Red Carpet. Eventually, Mr. Clark obtained a broker’s license and opened Seville Real Estate, in 1986.\nAs a local real estate professional, Clark was involved in countless organizations and causes. He served as the president of the National Association of Real Estate Brokers (NARAB) and the Associated Real Property Brokers (ARPB). He also served on the Advisory Board of the Federal Home Mortgage Association.\nClark’s community work included executive director of the Long Beach Community Improvement League, board chair of the East Bay Volunteers of America, And most recently he was a board member and vice president of the Oakland Citizens Community for Urban Renewal (OCCUR). He was one of the first independent entrepreneurs in the East Bay to provide small affordable working spaces and a law library for attorneys and real estate agents just starting out in their careers. In every role, Mr. Clark demonstrated his belief in helping others.\nToastmasters International recognized Clark as a respected “Servant Leader” and he remained an active member for 25 years. He attended weekly meetings and unselfishly offered his insight and experience. He mentored the newer members and club leaders. He would often invite and sponsor new members who were seeking to improve their communication and leadership skills.\nShortly before his passing, ARPB Toastmasters club hosted an open house to celebrate the club’s 25th anniversary and show appreciation to Clark. He was presented with a district-wide appreciation award for his years of support and leadership within Toastmasters and the community.\nClark’s daughters also gave him a surprise 80th birthday party at his home just two weeks prior to his death. Countless close friends, community leaders, industry professionals and family members came to share how Mr. Clark had affected their lives.\nMr. Clark passed on September 30, 2017. Former wives Betty Rhone and Harriet Wedgeworth preceded him in death. Still carrying his memory are his children: Patrice Clark, Keith Clark, Melanie Sowell and Maya Clark, and a host of grandchildren and great-grandchildren.\nThrough it all, his ever-present, and sincere smile shines as bright as ever in the memory of those who knew him.\nAs he lived, he unselfishly helped others become successful and he will not be soon forgotten for making a difference in the lives of others.\nAnd in the words of Booker T. Washington, “Those who are happiest are those who do the most for others.”","Few people have witnessed Bellevue’s development over the past three decades quite like the city’s planning director, Dan Stroh. Employment and residential booms downtown, boundary-moving annexations, and increasing demands on transportation and housing — Stroh has dealt with all of it.\nThat changes today when Stroh, 62, retires after working at Bellevue City Hall for the past 27 years. Michael “Mac” Cummins, a veteran planning manager from the City of Westminster, Colorado, fills the position this month.\n“It’s tough to leave, though I never thought I would stay here this long,” said Stroh during an hour-long interview earlier this month at City Hall. “It’s been a great 27 years. I hope I have done good things for the community, and realize that anything that I contributed is mostly due to the contributions of other people here, the city council, and many other parties.”\nStroh’s professional planning career started in 1982 in North Carolina, where he spent five years in community development and urban planning before he moved to the Pacific Northwest and was hired by the City of Kent as a senior planner. He joined the City of Bellevue two years later, first as a senior planner, then as the planning director beginning in 1998.\nStroh recently discussed his interest in urban development, Bellevue’s growth, and his future plans once he leaves City Hall.\n425 BUSINESS: How did you get into planning? What were those early years like for you in college? Was it something that you were always interested in?\nDAN STROH: I’ve always been fascinated with cities. I studied archaeology and anthropology in college and did an internship with a visiting professor who was an economic historian. We traveled around England and France and looked at the evolution of cities — how they grew and changed over time, how they dealt with issues and opportunities over the centuries — and this became enormously interesting to me.\nI was also pulled into this field by wanting to make a difference. Growing up in Central Virginia at that time, I was seeing “galloping growth” that was moving way outside the core city and really impacting some areas that were quite beautiful, and losing some of the character that made them real special. I thought, gosh, is there any way I can intervene in this? I discovered city planning as the ticket both for my interest in cities and a way to help make them better. After my undergraduate work at the College of William and Mary, I went to the University of North Carolina at Chapel Hill and (completed) a master’s degree in planning. That’s how I got into it.\n425 BUSINESS: Bellevue has seen tremendous growth since you began working at City Hall. What have been the challenges in keeping up with that growth? Have you felt like you have been in the hot seat in terms of planning for and responding to that growth?\nSTROH: Bellevue had about 87,000 people in 1990, and it’s pushing 140,000 people now. The bigger dynamic has been in job growth. Bellevue was at about 90,000 jobs in 1990 when I came here, and now is well north of 150,000 jobs. Sometimes people compare Bellevue to another community of 140,000 people, but it’s really that combined effect of population plus an incredibly dynamic job engine here, which gives us a daytime population that is much, much higher than the residential population\nSo, in terms of being in a hot seat, yeah, it’s been a constant series of challenges about how to grow gracefully. That’s the question for a community like this. What we are trying to do with growth management is to focus growth where it should happen, keep it out of places where it shouldn’t happen, and ultimately create a higher quality of life in both places. In a nutshell, that’s what the job of planner is really about.\n425 BUSINESS: What is one big challenge still facing Bellevue’s future in terms of planning and development?\nSTROH: Keeping up with transportation systems is always a big challenge. As we densify and there is more development in the urban area, we reach a level of density where transit can work, but it’s hard to keep up with the needed regional transit system. We’re just now bringing light rail to the Eastside, but it won’t be available until 2023. The growth pains between now and then of keeping pace with transportation systems for transit and for the rest of the regional transportation system have been a continued challenge. In a place as dynamic as Bellevue, that’s not surprising.\nIt’s interesting how there have been some recent behavioral changes that are particularly heartening for an urban planner. Millennial generations and beyond are embracing urbanism in a way that Baby Boomers did not. What that has meant raises some pretty interesting questions in terms of how our dense urban areas are going to prosper. (Millennials) have embraced urbanism more, they are living in central city areas more, they are tending to have lower rates of car use, car-sharing is much more prevalent, and they are making much higher use of transit. Cities are needing to catch up with some of those behavioral changes. These are basically the consumers and the workforce of the future. Those behavioral trends may prove to be really helpful to urban areas and help us through some of our transportation issues.\n425 BUSINESS: What are some misperceptions people might have about Bellevue in terms of planning and development? Are there aspects, demographics, or trends about Bellevue planning that might surprise people?\nSTROH: I’m constantly surprised how little that people understand the demographics of Bellevue and how this community has changed. There is still a perception out there that Bellevue is a place of predominantly Caucasian families with kids. The reality is that by 2015 Bellevue has become a place where predominantly non-Hispanic whites are in the minority. There is just tremendous ethnic and cultural diversity in this city. People from all over the world come here and become contributing parts of the community, and are welcomed here. I think that level of cultural and ethnic diversity is something that the rest of the region doesn’t understand well about Bellevue. People in Bellevue know that, but people in the rest of the region don’t.\nThe other misperception is that people see the high-rises downtown and assume that all of Bellevue’s jobs are downtown. In fact, only about one in three Bellevue jobs are downtown. We have several other major job centers: Bel-Red, Eastgate, Factoria, Crossroads, Wilburton. Downtown Bellevue certainly is where the lion’s share of our growth will be, but there are a lot of jobs, including regional jobs, in other parts of Bellevue that are less on people’s radar screens.\n425 BUSINESS: Why are you leaving the City of Bellevue? What is next for you?\nSTROH: I want to take advantage of a lot of other opportunities while I’m still healthy and still relatively young. I want to do some extensive world travel with my family. I feel really fortunate to be able to do that at this point.\nThere are a lot of things that are working very well for us (at the City of Bellevue), so wanting to leave at a high point (is important). It’s as simple as that. You never want to overstay your welcome. My time in Bellevue has been incredibly rewarding. Bellevue has been a really interesting place to be, where you feel like you can get things done. It’s been dynamic in a way that I get a charge out of. Staying in one place for a long time as a city planner, you actually have a role in getting things implemented and can see the results on the ground. That is rewarding and something that not all planners get to do."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:1b3732c7-fad7-4bfa-bc56-7a1dddff2fda>","<urn:uuid:1c715d61-34d5-4af8-a7c1-8316bfe6dd71>"],"error":null}
{"question":"Compare water infrastructure challenges between Sub-Saharan Africa and developed regions like US","answer":"While both regions face infrastructure problems, Sub-Saharan Africa experiences more severe challenges. In the US, issues mainly involve bursting pipes and malfunctioning sewage systems that cause boil alerts and beach closures. In contrast, Sub-Saharan Africa faces economic water scarcity where sufficient water exists but lack of infrastructure severely constrains access. This has made Sub-Saharan Africa the only region where the population without access to basic water services has increased from 2000-2020, with 70% lacking safely managed drinking water services compared to just 4% in Europe and North America.","context":["This week we continue counting down the 19 best solutions to the global freshwater crisis captured by a GlobeScan and SustainAbility poll of more than 1200 leading international experts in 80 countries. Here’s the final list.\nPhoto by Brent Stirton/Reportage by Getty Images for Circle of Blue\nImperial Valley, California 2009: Runoff agricultural waste water contaminated with a high saline and fertilizer content on its way to be dumped in the Salton Sea.\nVirtually every industry in the world anticipates sweeping systemic transformation over the next decade in their strategic planning, production practices, and business models, according to the Sustainability Survey Poll on Water. The global opinion poll, which released a survey of more than 1,200 sustainability experts in March, concludes that water shortages will shift public perception of the value of water, prompting governments and companies to view clean water not as a commodity to exploit but as a precious resource.\nConducted by GlobeScan, an international public and stakeholder opinion research firm, and SustainAbility, a think tank and business strategy consultancy, the poll asked, “What are the technologies or changes in behavior which show the most promise for addressing water shortages over the next 10 years?” THE experts’ responses generated 19 consensus solutions.\nJeff Erikson, senior vice president at SustainAbility, told Circle of Blue that the decisions executives make to respond to freshwater scarcity will penetrate almost every aspect of their business operations. The varied solutions reveaedl the complexity of coping with water scarcity. Population growth, urban development, farm production and climate change are increasing competition for fresh water and producing shortages. Here’s a look at the first 10 areas where experts feel needed solutions will come.\nEducate to change consumption and lifestyles\nIn the end, changing the face of this crisis involves education to motivate new behaviors. Coping with the coming era of water scarcity will require major overhaul of all forms of consumption, from individual use to the supply chains of major corporations, like GE\n. Some regions led by India, Australia and the Southwest U.S., are already facing the freshwater crisis. The most critical task is making sure the problem is much better understood worldwide.\nInvent new water conservation technologies\nIn areas where aquifers are drying up and rainwater is increasingly unpredictable, innovation is needed. But as we attempt to cope with freshwater scarcity and develop conservation technologies, energy consumption is an important consideration\nIn March, World Water Day panelists urged a new mindset for wastewater treatment\n. Some countries, like Singapore\n, are trying to recycle to cut water imports and become more self-sufficient. The rich East Asian republic is a leader in developing advanced technology that cleanses wastewater for other uses, including drinking.\nImprove irrigation and agricultural practices\nSome 70 percent of the world’s freshwater is used for agriculture. Improving irrigation can help close supply and demand gaps. In certain cases profligate irrigation practices meant for an earlier era has weakened the ability of farmers to provide food and fiber to a growing world. Examples include the Murray-Darling basin in Australia\n, Central Asia’s Aral Sea\n, and the American Southwest\n. Although new technology has become an appealing solution, global water experts like Peter Gleick note that in some cases, such as the agricultural systems in California, success stories can happen by improving what’s already in place\nAppropriately price water\nWater pricing and rights go hand in hand, with consumers questioning the benefit of higher prices. According to experts from the Organization for Economic Co-operation and Development (OECD), an international economic forum of 31 of the world’s richest countries, raising prices will help lower waste and pollution\n. But Circle of Blue’s May investigation into water pricing systems in major U.S. cities\n, show current utility pricing systems are obsolete, send the wrong signals, and need reform.\nImprove water catchment and harvesting\nWater catchment systems are essential for areas with no other reliable water sources. Pakistan\n—two countries that contend with some of the worst effects of climate change—are overhauling rainwater harvesting systems. These efforts provide independent control of water resources.\nLook to community-based governance and partnerships\nCommunity organizations elevate the experiences of those whose voices merit more influence. In April, for instance, indigenous groups met at the alternative climate change conference in Bolivia, a gathering meant to foster international partnerships among underrepresented groups\n. Ensuring more effective governance at the grassroots-level gives communities stature, and can lead to effective policy changes on a national scale.\nDevelop and enact better policies and regulations\nAs water scarcity complicates food security\nand pollution, governments need to redefine their role. The U.S. government is considering expanding the Clean Water Act\nto ensure more protections. In Russia, meanwhile, Prime Minister Vladimir Putin has approved waste discharges in Lake Baikal, one of the world’s largest bodies of freshwater. Regardless of what path elected leaders take–the Circle of Blue/GlobeScan WaterViews survey\nindicates they are considering multiple approaches–the survey also found that most people say it is up to the government to ensure communities have access to clean water.\nHolistically manage ecosystems\nSimply put, holistic management applies to a practical, common-sense approach to overseeing natural resources that takes into account economic, cultural, and ecological goals. In essence, the whole is greater than the sum of its parts, and each facet is related to and influences the others. Good examples of holistic management are communities that operate sewage treatment plants while pursuing partnerships with clean energy producers to use wastewater to fertilize algae and other biofuel crops. The crops, in turn, soak up nutrients and purify wastewater, significantly reducing pumping and treatment costs.\nImprove distribution infrastructure\nPoor infrastructure is devastating to health and the economy. It wastes resources, adds costs, diminishes the quality of life, and allows preventable water-borne diseases to spread among vulnerable populations\n, especially children. The problem is not confined to the developing world. Pipes burst on a regular basis in the U.S., prompting boil alerts. Sewage treatment systems regularly overflow and malfunction, causing beach closures.\nShrink corporate water footprints\nIndustrial water use accounts for approximately 22 percent of global consumption. The corporate footprint includes water that is directly and indirectly consumed when goods are produced. ?As sustainable manufacturing becomes more important, given the increasing severity of water scarcity, Peter Gleick and other experts question the costs of one industry sector in particular: bottled water\nR&D / Innovation\nAccess to water in a water-scarce world will become a much higher priority in business decisions. Communities are likely to pursue public-private partnerships that draw on the innovative capacities of companies. One example— cities that operate sewage treatment plants are likely to pursue partnerships with clean energy producers to fertilize algae and other biofuel crops\nWater projects in developing countries / transfer of technology\nClimate change and water scarcity are producing the most dramatic consequences in developing regions, such as northwest India\nand Sub-Saharan Africa\n. One proposed solution is to transfer water conservation technologies to these dry areas. Doing so is tricky because economies are weak and there are gaps in skills that often compel government and business authorities to impose these changes on local citizens.\nClimate change mitigation\nClimate change and water scarcity go hand-in-hand\nto cause some of the biggest contemporary challenges to the human race. These issues have a reciprocal relationship, identified by the Intergovernmental Panel on Climate Change (IPCC), in which, “water management policies and measures can have an influence on greenhouse gas (GHG) emissions.” As renewable energy options are pursued, the water consumption of these mitigation tactics must be considered in producing alternatives ranging from bio-energy crops to hydropower and solar power plants\nPopulation growth control\nBecause of the accelerating growth in global population, parts of the world could see a supply-demand gap of up to 65 percent in water resources by 2030\n. Currently, more than one billion people don’t have access to clean water. And with 70 percent of the world’s freshwater used for agriculture, water’s critical role in food production must be considered as climate and resource conditions change.\nTAGS: Action on climate change\n, Carbon finance\n, climate change\n, Environmental Issues\n, global warming\n, Mitigation of global warming\n, solutions to global water crisis\n, Water Crisis\n, Water Resources\n, Water Supply","Water Cooperation Essential as Africa’s Water Crisis Intensifies\nThere is a growing global water crisis, driven by climate change, population growth, increased water-intensive production, and the neglect of water infrastructure. The latest edition of the UN World Water Development Report states that globally, 2 billion people (26% of the population) do not have safe drinking water and 3.6 billion (46%) lack access to safely managed sanitation.\nWater availability per capita has been decreasing worldwide. Sub-Saharan Africa (SSA) has seen a 40% decline in per capita internal renewable water resources between 2000 and 2018, the largest decline of all regions in the world, albeit from a low base. From 2008-2018, water stress increased by 14.2% in SSA and 13.3% in Northern Africa, the largest increase of all regions after South-Eastern Asia (15.5%) and Latin America and the Caribbean (15.4%). Economic water scarcity (the condition where there is sufficient water to meet human and environmental needs, but access is constrained by a lack of water infrastructure or poor water resources management) is most severe in SSA. This is reflected in service coverage statistics: while the rest of the world has seen a decline in the population without access to basic water services from 2000-2020, SSA is the only region where these numbers have been rising. In 2020, 70% of the population in Sub-Saharan Africa did not have access to safely managed drinking water services (compared to just 4% in Europe and North America).\nTo address the water challenges the world currently faces related to droughts, floods, and a lack of sanitation and drinking water, the UN held the 2023 Water Conference, the first freshwater conference hosted by the UN since 1977. An outcome of the Conference was the Water Action Agenda, a collection of over 700 voluntary pledges to help deliver on water action from governments, businesses, NGOs and others.\nAn analysis of all of these pledges by the World Resource Institute finds that while more than one-quarter of the commitments are potential game-changers, the rest of the pledges are unlikely to generate real change. A stand-out pledge identified by the WRI is a joint commitment from the Niger River Basin Authority and the German Federal Ministry for the Environment, Nature Conservation, Nuclear Safety and Consumer Protection (BMUV). It includes financial backing of $21.2 million through 2029 to strengthen climate change adaptation and mitigation throughout all nine countries the Niger River runs through (Benin, Burkina Faso, Cameroon, Chad, Ivory Coast, Guinea, Mali, Niger, and Nigeria). The strategy includes nature-based solutions, such as climate-smart agriculture and wetland restoration, to address the challenges posed by the region’s erratic rainfall patterns and desertification.\nThis type of project is made possible by the existence of an operational arrangement (legal and institutional structures) for governing the transboundary marine resource. Additionally, the development of a Climate Resilience Investment Plan by the Niger Basin Authority created a framework for successfully leveraging investments. Many transboundary water (TBW) resources on the continent lack these kinds of arrangements and vision, stifling the potential for innovative transboundary actions to be taken. Only 29% of transboundary river basins and less than 10% of transboundary aquifers on the continent have TBW agreements, with only 19% having basin-wide agreements. Moreover, many TBW agreements do not account for the effects of increasing climate-induced water variability, constraining their flexibility to adapt to changes in water quantity and distribution over time.\nThe escalating tension between Ethiopia and Egypt over the Grand Ethiopian Renaissance Dam (GERD) on the Blue Nile is illustrative of what can go wrong without a fair, meaningful, agreement to govern the use of shared water resources. Ethiopia, Egypt, and Sudan have been in negotiations over the use of the Nile River Basin’s waters for over a decade. Tensions between Egypt and Ethiopia have been escalating since 2020 when Ethiopia filled the GERD reservoir despite Egypt’s demands that the dam should remain empty until an agreement was reached. Egypt is concerned that the Dam will affect the flow of water into the Nile which Egypt depends on heavily for its household and commercial water supply. While Ethiopia’s highlands supply over 85% of the water that flows into the Nile River, Egypt has long referred to colonial-era 1929 and 1959 agreements to stop the construction of any large infrastructure projects on the tributaries of the Nile. The 1959 agreement allocated the entirety of the Nile River’s waters to Sudan and Egypt, leaving no water to Ethiopia or other upstream riparian states. Negotiating a modern, equitable, basin-wide agreement to ensure the between the 11 riparian countries (drawing on the experiences of successful African management frameworks, including the Niger Basin Authority) is critical for avoiding further disputes.\nAs the World Water Development Report highlights, water cooperation is critical for combatting the water crisis in Sub-Saharan Africa and ensuring peace and security. Water security in the region’s many transboundary basins and aquifers can be improved by creating strategies and platforms for strengthening communication and the exchange of information not only between governments but all water stakeholders from the community, academia, and business.\n WHO, UNICEF, World Bank. (2022). State of the world’s drinking water: an urgent call to action to accelerate progress on ensuring safe drinking water for all. Geneva: World Health Organization.\n AfDB. (2022). Climate proofing Transboundary Water Agreements in Africa. https://www.afdb.org/en/documents/climate-proofing-transboundary-water-agreements-africa\n Mbaku. (2020). The controversy over the Grand Ethiopian Renaissance Dam. Brookings. https://www.brookings.edu/blog/africa-in-focus/2020/08/05/the-controversy-over-the-grand-ethiopian-renaissance-dam/\nAbout the Author(s)\nLeave a comment\nThe Trade Law Centre (tralac) encourages relevant, topic-related discussion and intelligent debate. By posting comments on our website, you’ll be contributing to ongoing conversations about important trade-related issues for African countries. Before submitting your comment, please take note of our comments policy."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:b52fd51d-87db-41c2-a1d6-4d47ca1f0741>","<urn:uuid:3e1e5d80-e18a-4c75-9c24-235119817ba3>"],"error":null}
{"question":"How do the measurement recording conventions compare between windows and French doors when preparing for blind or covering installations, including the key dimensions and notations used?","answer":"For windows, measurements are recorded in a width by length format (e.g., '60 x 20' spoken as 'sixty by twenty'), with measurements taken to the nearest 1/16 inch. Window measurements include both inside frame measurements (taking the smallest of three measurements at top, middle, and bottom) and outside frame measurements. For French doors requiring blind installation, while the same width by length format is used, measurements focus specifically on the glass portion plus an additional inch for overlap on both dimensions. French doors require particular attention to mounting locations and need measurements for two separate doors, even if they appear identical. Both types require precise measurements, but French doors have additional considerations for handles, hinges, and mounting brackets that windows don't require.","context":["How to Measure Doors & Windows\nA home handyperson must, on occasion, measure a window or a door. Perhaps it is for a replacement item, or perhaps it is for drapes, or to calculate the amount of paint needed to cover a door. No matter what the requirements, careful, accurate measuring starts with a reliable tape measure and an understanding of the standards of measuring for doors and windows.\nRecord measurements to the nearest 1/16 inch and measure the window, not the glass.\nRecord all measurements following the format of width by length. For example, a window that is 60 inches wide and 20 inches long is recorded as “60 x 20.” A window that is 10 inches wide and 55 inches long is recorded as “10 x 55” and spoken as “ten by fifty-five.”\nMeasure the width of the window as the measurement from left to right. For example, the width of a window is the measurement from the left edge to the right edge.\nMeasure the length of the window as the measurement from the top to bottom.\nTake one set of measurements inside the window frame. A window may have several steps within the frame; measure from side to side at the widest place. Measure the width at the top, middle and the bottom, and use the smallest of the three measurements as the inside width.\nMeasure the window length at the left side, the center and the right side inside the window frame and record the smallest of these as the inside length measurement.\nRecord the window outside measurements as the from the far left edge of the window frame to the far right edge. Measure the outside length measurement as from the top of the window frame to the bottom of the frame.\nTake a picture of a multipaned window and record the measurements on the print to eliminate confusion. Record the overall window width and length, both inside and outside the frame. Measure and record the width and length of each window section and the measurements of any subsections. For example, the overall the window may be 60 x 20, composed of two windows 30 x 20.\nRealize that the door is the part that opens and closes. Anything else is the frame or trim, not part of the door.\nMeasure the door from the left to the right as the width. Measure the door near the top, at the middle and near the bottom. Use the largest measurement as the door width.\nMeasure the door length from the top to bottom. Measure at the left side, the center and the right side, and use the largest measurement as the door length.\nRecord the door measurements as “width by length.” For example, a door 32 inches from side to side and 80 inches from top to bottom is written as “32 x 80” and spoken as “thirty-two by eighty.”\nMeasure multi-sectional doors with overall width and length first, and each component door separately.\n- Use a 1-inch wide metal tape measure, marked in 1/16-inch increments.\nLinda Erlam started writing educational manuals in 1979. She also writes a biweekly newspaper column, \"Design Dilemmas,\" in the \"Lakeshore News\" and has been published in \"Design and Drapery Pro\" magazine. Erlam is a graduate of the Sheffield School of Interior Design and is a practicing interior decorator and drapery workroom operator.","What are French Doors & Why Do They Need Blinds?\nIf you have a porch or a patio you need doors that help to connect these spaces to the house. The door can be either patio doors or French doors. Both are beautiful and absolute show-stealers. French doors have glass panes that extend for the greater part of their length. While French doors open outwards on hinges, patio doors slide sideways along a track. Both are popular choices for doors.\nWhile French doors look gorgeous, allowing lots of natural light to stream through into the house and gives an unobstructed view of the outdoors, you may want to cover the doors with some form of window treatment. Regardless of how pretty they look as it is, a window covering is required to make the door more functional. You cannot leave them bare or naked as they otherwise bring a number of concerns to address. You need a window treatment to protect your privacy, insulate your doors against energy loss, block out the harsh rays of the sun and the harmful UV rays that can cause damage to health, your interiors and electronics. The glare of the sun’s rays can cause irritation and interfere with your work and sleep. Getting the right window treatment that will help elevate the look and appeal of your French doors is a challenge for homeowners.\nInstalling Blinds on French Doors\nStep 1: Choice of Tools\nBefore you start work on installing the blinds it is advisable to get all the tools needed in one place to prevent confusion while work is in progress.\nDisclaimer: Please follow these instructions at your own risk. ZebraBlinds takes no liability for any issues or damaged caused through following DIY methods. Since all blinds and shades are different, we always recommend checking with your blind manufacturer or retailer first before making any modifications to your blinds. As well, if you are uncomfortable on your own, look for the help of a professional.\nTo install blinds successfully on your French doors you need the following tools:\n• Measuring tape\n• Carpenter’s level\n• Electric Drilling machine\n• 1/16 inch drill bit\n• Flat screwdriver\n• Screws brackets\nTo get the right fit and install the blinds perfectly on your French doors adhere to the steps given below. It is preferable to use a drill bit that is smaller than the screws to be used. When you are making pilot holes make them below the surface.\nStep 2: Measurements\nGetting the measurements right is the first step towards getting the best-fitted blinds for your French doors. These doors are different from traditional windows and measurements will vary. Take a measuring tape and firstly measure the glass portion of the door. You need to add at least an inch to both the width and the length to get overlap for privacy and light control. Allow a sufficient buffer for handles and hinges when taking the measurements. You do not need to make space for the center molding.\nStep 3: Where to Mount\nThe blinds should be mounted a couple of inches above the glass. On each side of the glass make markings for the brackets using a pencil. Repeat the procedure at the bottom for hold-down brackets. When you open and close the doors these brackets will help to keep the blinds in place. The blinds need to be installed at the top center of the French doors. This is considered the best place for installing the blinds as it is the strongest section of the door. Installing the blinds here will prevent any damage to the wood and also prevent splintering. French doors are normally made of cedar, oak and other forms of solid wood that are known to split.\nStep 4: Make Markings\nTake a mounting bracket and make markings of the places where the screws will go using a pencil. Repeat these steps for all mounting and hold-down brackets. With the help of a carpenter’s level ensure that the markings are aligned so that the blinds make a straight fit.\nStep 5: Drill\nUsing a 1/16 inch drill bit make pilot holes in every screw mark that you made on the doors a while ago.\nStep 6: Secure Brackets\nTake the brackets, hold them against the screw holes and secure the screws using your screwdriver. Repeat this step for all the brackets that need to be installed.\nStep 7: Install the Blinds\nTake the headrail of the blind and slide it into the grooves on the brackets. The headrail will snap into place. Do the same for the bottom rail of the blind.\nPoint to Remember\nFrench doors, unlike sliding doors, have two doors. You will need two blinds. Ensure that you have taken correct measurements of both the doors. While it is likely that both doors will measure the same, it is better to double-check as there might be a slight difference between the two which needs to be considered when installing the blinds.\nAs you place the headrail and bottom rail into the grooves, the blinds are ready to use. Installing and fitting blinds on to your French doors are easier than you realize and are completely doable at home. Measurement is the key and once you cross the hurdle, the rest is easy."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:40d39777-df92-48c0-8386-4a60c87e789c>","<urn:uuid:b5332332-94d2-400a-88eb-58027dd9d274>"],"error":null}
{"question":"I'm worried about my heart health. Which is more important to track - my 9p21 gene status or my HDL cholesterol levels?","answer":"Both measurements are important risk indicators for heart disease. The 9p21 gene increases risk of atherosclerosis and coronary heart disease, with heterozygote SNP carriers having a 50% increased risk of myocardial infarction and homozygote SNP carriers having approximately 100% increased risk. HDL cholesterol levels are also crucial indicators - levels below 40 mg/dL increase heart disease risk while levels above 60 mg/dL help protect against heart disease. Rather than prioritizing one over the other, both should be monitored as part of comprehensive cardiovascular risk assessment.","context":["Nutrigenomics provides us with an expanded perspective on the prevention and treatment of cardiovascular disease. In cardiovascular management, nutrigenomics encompasses genetic testing, metabolomics, the identification of single nucleotide polymorphisms (SNPs) and nutrient-genetic interactions, and the newest concept, gene expression testing. These tests provide indication of whether or not your patients’ genetics are expressed and their risk of cardiovascular disease.\nMost genetic expression is driven by inflammation, and the majority of the genes, once turned on, promote an inflammatory response. Most of the loci on genes associated with myocardial infarction (MI), Coronary Heart Disease (CHD), and Congestive Heart Failure (CHF) are expressed through inflammation, oxidative stress, and immune-vascular dysfunction. This dynamic starts in the vascular endothelium and vascular smooth muscle and cardiomyocytes leading to angina, coronary artery vasospasm, obstructive coronary heart disease, diastolic and systolic dysfunction and cardiomyopathy. Regardless of the type of insult, blood vessels respond to insults via these same three mechanisms: inflammation, oxidative stress, and immune-vascular dysfunction.\nConsequently, the inflammatory pathways have become the primary focus in the management of genetic expression and of genetic risk for CVD.\nNutritional factors provide information that determines whether our genes are turned on or turned off, with a corresponding beneficial or detrimental outcome. One change in a single ubiquitous nutrient such as magnesium may cause 300 or 400 different changes in downstream metabolic pathways and cardiovascular function and health.\nThere are several issues we want to define when we initially examine patients. One is their genetic profile, the genes they were dealt. There are also epigenetic influences that are not genetic such as DNA methylation, histone modification, and non-coded messenger RNA. The final aspect is gene expression, as genes express themselves in response to nourishment or insults from different types of information coming in from the environment. Genetics have become important in determining not only dietary intake, but also medication intake in many patients, based on their genetic profile.\nThere are more than 400 known risk factors for cardiovascular disease. They all ultimately result in the same three finite responses in the body: inflammation, oxidative distress, and/or immune dysfunction. These risk factors ultimately translate into vascular disease.\nGenes Relevant to Cardiovascular Risk\nOne of the primary genes we are now measuring is the 9p21 gene, which increases the risk of atherosclerosis and coronary heart disease. Patients who have a heterozygote SNP for 9p21 have a risk for MI that is increased by 50%. When a patient has a homozygote SNP, risk goes up to approximately 100%, so this is one of the top genetic risks that we measure for CHD and MI.\nApo E4 Genotype\nThe Apo E genotype is not new information, but we must remind ourselves that this genotype increases risk for CVD and people with the genotype have varied responses, particularly to different types of fats in their diet.\nOne of the newest genes that we’re looking at is COMT (catechol-O-methyltransferase) which provides instructions for the breakdown of norepinephrine and epinephrine. If this genetic SNP is present, the patient will have higher levels of norepinephrine and epinephrine and increased risk of hypertension and coronary heart disease. There is a variation in response depending on which of the specific COMPT SNPs the patient carries; for example, aspirin or vitamin E may be beneficial for patients with one type of COMT SNP, but detrimental if one of the other SNPs is present.\nThe risk of myocardial infarction can be increased by 71% if a SNP affecting glutathione metabolism (GSH-Px) is present. This selenium-dependent enzyme expresses different capacities to neutralise hydroxyl radicals and other oxidative molecules related to increases in oxidative stress and CVD. For these patients, glutathione peroxidase and selenium levels would be key measurements to track for the risk of CVD.\nThere is a whole host of genetic influences on blood pressure, probably 30 different genes that we have recognised to date, all of which are helpful in determining both risk for hypertension and risk for cardiovascular target organ damage, as well as response to nutrients, caffeine, medications, and various types of diets.\nWe know, for example, that someone who consumes large amounts of caffeine and has the SNP, cytochrome P-450-1A2, will increase their risk of tachycardia, hypertension, aortic stiffness, and myocardial infarction. Of course, one could have the right type of SNP for caffeine detoxification and that will reduce their risk. Approximately 60% of the population has cytochrome P-450-1A2FF, which is the wrong kind of gene to have, because they are slow metabolisers and their risk for CHD and MI actually go up with caffeine consumption.\nBefore you tell patients it’s okay to be drinking caffeine, you need to check the gene for cytochrome P-450 function.\nGenetic assessments can play a role in both undertanding and mitigating risks associated with cardiovascular disease. This is an emerging field of medicine, and one that will shape the way we practice in the future.\nDr Houston’s Early Detection and Prevention Checklist for Cardiovascular Disease\n- Genetic Expression Scoring and Testing: Corus CAD.\n- Top five CHD Risk Factors treated to new goals\n- Hypertension: 24 hour ambulatory blood pressure monitor (ABM)\n- Dyslipidaemia: Advanced lipid testing.\n- Dysglycaemia: FBS, 2h GTT, HbA1c, insulin, proinsulin, C-peptide.\n- Obesity: BW, BMI, WC, WHR, body impedance analysis (BIA)\n- **Tobacco: stop all forms.","Alternative Names: High-density lipoprotein test\nHDL stands for high-density lipoprotein. It's also sometimes called \"good\" cholesterol. Lipoproteins are made of fat and protein. They carry cholesterol, triglycerides, and other fats, called lipids, in the blood from other parts of your body to your liver.\nThis article discusses the blood test used to measure the level of HDL cholesterol in your blood.\n- LDL (\"bad\") cholesterol test\n- Lipid profile\n- High blood cholesterol and triglycerides\n- Total cholesterol test\nWhy is the Test Performed?\nThis test is done to check the level of cholesterol in your blood and to see if you are at high risk for a heart attack, stroke, or other cardiovascular problem. Studies of both men and women have shown that the higher your HDL, the lower your risk of coronary artery disease. This is why HDL is sometimes referred to as \"good\" cholesterol.\nThe main function of HDL is to help soak up excess cholesterol from the walls of blood vessels and carry it to the liver, where it breaks down and is removed from the body in the bile.\nThe laboratory test for HDL actually measures how much cholesterol is in each high-density lipoprotein particle, not the actual amount of HDL in the blood.\nHow is the Test Performed?\nA blood sample is needed.\nBlood is typically drawn from a vein, usually from the inside of the elbow or the back of the hand. The site is cleaned with germ-killing medicine (antiseptic). The health care provider wraps an elastic band around the upper arm to apply pressure to the area and make the vein swell with blood.\nNext, the health care provider gently inserts a needle into the vein. The blood collects into an airtight vial or tube attached to the needle. The elastic band is removed from your arm.\nOnce the blood has been collected, the needle is removed, and the puncture site is covered to stop any bleeding.\nPreparation for the Test\nYou may be told not to eat or drink anything for 9 - 12 hours before the test.\nThe health care provider may tell you to stop taking certain drugs before the procedure.\nHow will the Test Feel?\nWhen the needle is inserted to draw blood, some people feel moderate pain, while others feel only a prick or stinging sensation. Afterward, there may be some throbbing.\nHDL test Risks\nThere is very little risk involved with having your blood taken. Veins and arteries vary in size from one patient to another and from one side of the body to the other. Taking blood from some people may be more difficult than from others.\nOther risks associated with having blood drawn are slight but may include:\n- Excessive bleeding\n- Fainting or feeling light-headed\n- Hematoma (blood accumulating under the skin)\n- Infection (a slight risk any time the skin is broken)\nHDL may be done as part of an overall lipid profile, where \"bad\" cholesterol (LDL) and triglycerides will also be measured. The combined information gathered from all of these tests may help your risk of heart attack, stroke, and peripheral vascular disease.\nYour health care provider may recommend therapy if your risk is found to be high. Regular exercise can increase HDL levels by several points.\nNormal Results for HDL test\nIn general, your risk for heart disease, including a heart attack, increases if your HDL cholesterol level is less than 40 mg/dL.\nAn HDL 60 mg/dL or above helps protect against heart disease.\nWomen tend to have higher HDL cholesterol than men.\nNote: Normal value ranges may vary slightly among different laboratories. Talk to your doctor about the meaning of your specific test results.\nThe examples above show the common measurements for results for these tests. Some laboratories use different measurements or may test different specimens.\nWhat Abnormal Results Mean\nLow HDL levels may be a sign that you have an increased risk for atherosclerotic heart disease.\nA low HDL level may also be associated with:\nReviewed By: David C. Dugdale, III, MD, Professor of Medicine, Division of General Medicine, Department of Medicine, University of Washington School of Medicine. Also reviewed by David Zieve, MD, MHA, Medical Director, A.D.A.M., Inc.\nCopyright 2015 A.D.A.M., Inc."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:fd86886e-16c0-4be4-a29b-1083181c48e8>","<urn:uuid:7deabc66-a1b3-400d-bdba-e3b9c761d16f>"],"error":null}
{"question":"What significant organizational changes occurred at both the Cannes Film Festival and Festival dei Popoli in response to their respective historical moments?","answer":"The Cannes Film Festival introduced the Directors' Fortnight section in 1969 as a direct response to the 1968 protests, creating 'a festival inside the festival' that became a showcase for radical and revolutionary voices. This new section helped represent the fight against censorship and broadened the festival's scope beyond traditional European and American films. Similarly, the Festival dei Popoli, founded in 1959, evolved by creating an archive of over 10,000 titles and launching filmmaking education initiatives. It has adapted to modern times by examining how documentary film faces new technologies and forms of reproducing reality, while organizing retrospectives and maintaining its focus on social and anthropological themes.","context":["The Cannes Film Festival today- it’s going on right now – has turned into such total glitz and glamour that it’s become a shadow of what it used to be. It was always a marketplace, but it was also a place of ideas, where revolutionary ideas in the cinema were discussed, and sometimes put into practice.\nWithin days, the trade unions had joined in, millions of people around the country were demonstrating and France was brought to the verge of standstill. In Cannes, meanwhile, life was—initially at least—proceeding as normal. The 21st edition of the world’s most prestigious film festival kicked off on May 10 with a restored version of Gone with the Wind. As the protests spread across the country, however, so too did the enfants terribles of French cinema, Jean-Luc Godard and François Truffaut, who hit the Croisette with one goal: to shut down the festival.\nOn May 13, the French Critics Association issued a statement calling on those present to demonstrate in solidarity of the students, protest against the heavy-handed tactics of the police, and demand the festival be suspended. Festival founder and longtime president Robert Favre le Bret refused. As a concession, he offered to cancel parties and cocktails. That wasn’t enough, however, for the impassioned leaders of the French New Wave, one of whom—Claude Lelouch—actually reported for revolutionary duty in Cannes on-board his private yacht.\nFervor was spreading as the three musketeers of Godard, Truffaut and Lelouch set about disrupting the festival, enlisting members of the jury—including Roman Polanski—and filmmakers, some of whom like Carlos Saura even had their own films in the festival, to the cause. During one heated debate, Godard lost his cool, screaming at someone against cancelling the festival: ‘We’re talking about solidarity with the students and the workers and you’re speaking about travelling shots and close-ups’ . . .\nWhen the festival tried to go through with the screening of Carlos Saura’s Peppermint Frappė against the wishes of the filmmakers, Saura and leading lady Geraldine Chaplin, along with Truffaut and Godard, tried to grab hold of the curtain in front of the screen to prevent it from opening; hanging on like leaves on a tree. There were fist fights. Godard lost his glasses while Truffaut took a tumble.\nEventually, Le Bret relented, reluctantly, and cancelled the festival on May 19, five days before its intended close. Cannes would never be the same again. The following year, a new section was introduced, Directors’ Fortnight, that would become a showcase for radical, daring and revolutionary voices . . . ‘We started Directors’ Fortnight because we wanted to have a festival inside the festival. Cannes did not agree to change some of the regulations,’ says Pierre-Henri Deleau, who ran it for three decades.\n‘The first year, we didn’t even know we had to ask for permission from the French customs to allow 35mm prints into the country, so the first two films we had scheduled were delayed. We didn’t even have a catalogue. Just a poster with the names of the films. But, to our surprise, it was a big success. So we kept on doing it.’\nOver the years, the selection of Directors’ Fortnight, or the Quinzaine, would continue to seek to push the envelope, whether in terms of showing creatively bold films or simply films from countries never selected for a major festival before. ‘We showed the first films from Cuba post-revolution, for example, or Asia and Latin America. Back then, the competition was quite conservative,’ says Deleau.\n‘It was always France, Germany, Spain, Italy, the US and the UK. The selection was like diplomacy. You have to remember in those days there were only three unions: the producers, distributors and exhibitors. There was no voice for the creators and directors. We wanted Directors’ Fortnight to represent the fight against censorship.’\nAs for the long-term legacy of 1968, there is no doubt that the events in Paris, the country as a whole, and Cannes that year, changed the festival, even if not ultimately exactly the way the great agitators initially envisaged. Ironically, the political fight may have contributed to the eventual breakdown in the friendship between Truffaut and Godard. Godard’s strident declarations and behavior marked him out as a genuine political radical, in contrast to Truffaut, whose main concern was, and remained, cinema.\n‘Truffaut was never political,’ says Deleau. ‘He always refused to be associated with one specific party. Ultimately, 1968 was not a revolution. It was not even the beginning of a revolution. It was a happening. The festival did change over the years, in some ways for the better, especially under Gilles Jacob when it became the festival that was choosing the films in selection, and not the producer countries.\nBut what happened in 1968 could never happen again today. Now, it’s all a question of business and promotion. There are too many films. How can a critic see 70 or 80 films? The real power isn’t in the hands of the director or the producer anymore. The people selling the films are in charge.’”\nYes, today Cannes is a commercial market above all else. But then again, things that go around come around, so to speak, and every so often, the cinema – like all the other arts – reinvents itself. Perhaps something like this, at another festival, with other directors who refuse to accept the status quo of the comic book movie DC / Marvel Universe present may eventually assert itself.\nIn the meantime, this article, and the events of May 1968, serve to remind us that film has always been torn between two polar opposites; it’s a business, and it’s an art form. Right now, the business end is winning. But as history has shown us time and again, all overblown regimes eventually collapse under their own weight, and commercial cinema has always been – as Jean Cocteau once put it – “a little overripe.” What will happen next is anyone’s guess, but as long as the struggle between art and commerce continues, and the underlying tensions remains, change is always possible.\nYou can read the entire article by clicking here, or on the image above.","FESTIVAL DEI POPOLI: BROADCASTING A LEGACY\nIt all began in 1959 when a group of Communications Studies scholars founded the Festival dei Popoli with the intention of promoting documentary films with a social edge. First, they organized an international documentary film festival, today regarded as one of the most important festivals of its kind. At the same time, they also created an archive that has grown in the last 49 years to number over 10,000 titles, including video and film reels. Such an archive has allowed the association to launch an initiative to teach filmmaking, with courses and workshops for aspiring filmmakers and documentarians, besides the important work in film restoration. Year after year, the Festival introduces to the public the most notable filmmakers in the genre by organizing retrospectives of their work (among them, Jean Rouch, Ken Loach, Vittorio de Seta, Fred Wiseman, Richard Leacock, Lindsay Anderson, Abbas Kiarostami, Nagisa Oshima, Artavazd Pelechian and Aleksandr Sokurov). Meanwhile, it also organizes conferences and roundtable discussions with esteemed guests from various branches of the humanities (Roland Barthes, Umberto Eco, Francesco Alberoni, Edgar Morin, Mary Douglas, Elemire Zolla, Carlo Tullio Altan and Jean-Louis Comolli). Following the suggested themes of the Festival, the guests analyze myriad aspects of the contemporary world, broadening the films with their reflections on the nature of representation.\nWith these reflections in mind, the next edition of the Festival dei Popoli will focus on examining the profound changes that documentary film faces in an era of new technologies, new forms of reproducing reality, and new means of distribution. Without abandoning its basic anthropologic and social roots, the Festival will open itself up to the world once again through a particularly defined program, while not forgetting the plurality of existing cinematographic forms. Through a discussion of mise-en-scene, our goal is to understand how the line between reality and fiction has been blurred over time, often at the risk of creating a “small hole” in the certitude of its audience. Although the festival’s core focus remains the material and spiritual life of man, there also exists a drive to examine formal exploration, the language used to represent different subjects, and the specific aesthetic of filmmakers, as well as the human and social elements they choose to film. In this sense, the new course of the Festival dei Popoli will balance continuity and innovation, concentrating its program on a few key points: understanding the world, exploring the form, and producing films of the future.\nTo achieve such, the Festival will adopt a spirit of fervent dialectics; beyond the regular structure and organization of the program, the festival will also act as an artistic retreat. More than just organizing post-screening talks, we want to invite filmmakers and producers to “live together” for a week in the hopes that they make long lasting connections through workshops, roundtable discussions and social gatherings. Therefore it will take the form of a studio, a place not only to watch films and talk about world topics, but to help create the films of tomorrow. In this sense, the aim is to create a fund that will select high-quality creative projects to partially finance, offering filmmakers residencies, tutelage and meetings with producers. To see its mission through, the Festival will expand a local promotional campaign (city, province, region) to a national (opportunities in other cities) and international stage (as happened in June in New York, with screenings of our best archival films). The Festival will collaborate with other local cultural institutions on developing contacts and supporting “established” projects in the arena of film. Also, it will organize screenings abroad to showcase documentaries made in each host country. In exchange, participating countries will become an outlet for the screening of Italian documentaries. Finally, in collaboration with other festivals, the Festival dei Popoli will transfer documentaries to digital, making a collection of DVDs available online or through a web of specialized libraries.\nAlong with its plans to meet the new goals, the 49th edition of the Festival dei Popoli will kick things off with a roundtable discussion, Nanook’s Legacy, which will ask international participants (filmmakers, producers, critics and festival directors) to reflect on the state of the documentary and how the term coined by Grierson in his paper on Flaherty’s film no longer corresponds to the same thing, inasmuch as the focus of nonfiction film has shifted to employ other forms.\nSuch a transformation will be made more evident by the retrospective A Baltic Diagonal, which takes a look at the last forty years of documentary production in Estonia, Latvia and Lithuania (the idea is to examine the evolution of this cinematic form from a technical and linguistic revolution—live cinema—to that of digital technology, covering an arc of time equivalent to the festival’s). The retrospective will highlight the personalities in the Works of Claire Simon, a filmmaker who alternates documentary and fiction with intriguing, effecting structural and linguistic hybrids; and the series The Faces of Power, which showcases numerous titles in the history of cinema in order to examine the cinematic representation of power.\nAlong the same lines, the two competitions are the International Competition, made up of twenty-four films that depict the look and spirit of the world; and the Free Style Competition, consisting of twenty films that experiment with documentary form."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:b7b768f9-b23a-41ef-8b80-ed0490382ca1>","<urn:uuid:8f810d59-b058-4ac8-b6ee-f169d322fb98>"],"error":null}
{"question":"How do particle arrangements and forces differ between solids and gases?","answer":"In solids, particles are packed together as tightly as possible in a neat and ordered arrangement (lattice), with strong forces holding them in fixed positions although they do vibrate. In contrast, gas particles are widely separated with negligible forces between them, allowing them to move rapidly in all directions. While solid particles can only vibrate about fixed positions, gas particles have no fixed positions and move chaotically, frequently colliding with each other and container walls. However, the attractive forces don't necessarily get weaker when moving from solid to gas state - rather the particles' increased kinetic energy allows them to overcome these forces.","context":["This focus idea is explored through:\nContrasting student and scientific views\nStudent everyday experiences\nAt this level, students are expected to 'explain the behaviour and properties of materials in terms of their constituent particles and the forces holding them together’ (VELS standards Level 6). However, the fact that students may be able to draw the usual static arrangements of particles in solids, liquids and gases does not mean that they hold a fully particulate view of matter. Research evidence suggests that many students at this age and older still hold a number of alternative conceptions about particles which prove difficult to extinguish. They often lack an appreciation of the very small size of particles, attribute macroscopic properties to microscopic particles, have difficulty appreciating the motion of particles in all states of matter and have problems understanding forces between particles.\nResearch: Driver (1987)\nMany students who appreciate that matter is particulate still retain some former views and consider that particles can change their form (solid to liquid), explode, burn, expand, change shape and colour or shrink. Students visualise atoms, molecules and ions to be little ball-like objects (perhaps because of the way the information has been presented) and this contributes to them confusing the properties of the particles with the macroscopic nature of the materials that they make up.\nResearch: Happs (1980)\nThese ideas are also explored in the focus idea\nMacroscopic and microscopic properties.\nStudents frequently fail to understand the dynamic nature of particles; they tend to think of them as static. Students may believe that gas particles are moving slowly in ways similar to what they observe when they see suspended dust particles in a beam of light. Random particle motion in liquids and gases is a difficult concept for students to appreciate. When asked, “Why don't gas particles fall to the bottom of a vessel?” only about 50% of students thought that the particles were in constant motion. Students stated that particles were forced apart (by heat acting as a substance) when gases were heated. When gases condensed to a liquid, many students attributed this to increased attractive forces between particles.\nResearch: Novick & Nussbaum (1981)\nStudents frequently find it difficult to appreciate particle movement in solids and this leads to different conceptions about freezing and melting. Some examples of students’ thinking about the behaviour of particles in a melting ice block are:\nStudent 1: “The particles start to break away from each other because of the rise in temperature. When they have broken away from each other, they turn from a crystal form to a solution form.\"\nStudent 2: \"When a block of ice is taken out of a freezer, the sudden change of temperature reacts on the particles making them decrease in size.\"\nAtoms are incredibly small and cannot be seen with even the most powerful light microscope. We use multiple models of atoms to help explain chemical processes and describe their behaviour.\nIn gases the particles move rapidly in all directions, frequently colliding with each other and the side of the container. With an increase in temperature, the particles gain kinetic energy and move faster. The actual average speed of the particles depends on their mass as well as the temperature – heavier particles move more slowly than lighter ones at the same temperature. The oxygen and nitrogen molecules in air at normal room temperature are moving rapidly at between 300 to 400 metres per second. Unlike collisions between macroscopic objects, collisions between particles are perfectly elastic with no loss of kinetic energy. This is very different to most other collisions where some kinetic energy is transformed into other forms such as heat and sound. It is the perfectly elastic nature of the collisions that enables the gas particles to continue rebounding after each collision with no loss of speed. Particles are still subject to gravity and hit the bottom of a container with greater force than the top, thus giving gases weight. If the vertical motion of gas molecules did not slow under gravity, the atmosphere would have long since escaped from the Earth.\nIn liquids, particles are quite close together and move with random motion throughout the container. Particles move rapidly in all directions but collide with each other more frequently than in gases due to shorter distances between particles. With an increase in temperature, the particles move faster as they gain kinetic energy, resulting in increased collision rates and an increased rate of diffusion.\nIn a solid, the particles pack together as tightly as possible in a neat and ordered arrangement. The particles are held together too strongly to allow movement from place to place but the particles do vibrate about their position in the structure. With an increase in temperature, the particles gain kinetic energy and vibrate faster and more strongly.\nThe attractive force in solids need not be stronger than in liquids or gases. For example the forces between solid helium particles (at -270 degrees C) are still very weak. By comparison, the forces between iron vapour particles (requires very high temperatures) are very strong. If you compare different substances that are at the same temperature, then the average kinetic energy of the particles will be the same (i.e. if the particles have the same mass then they will move with the same speed), but the attractive forces in solids will be greater than those in liquids, which will be greater than those in gases. Attractive forces don't get weaker when a substance moves from the solid to the liquid to the gas state, rather the kinetic energy of the particles increases (implying faster motion), allowing them to overcome the attractive forces.\nCritical teaching ideas\n- All matter is made up of atoms which are far too small to see even with the most powerful light microscopes.\n- Particles in all states of matter are in constant motion and this is very rapid at room temperature. A rise in temperature increases the kinetic energy and speed of particles; it does not weaken the forces between them.\n- The particles in solids vibrate about fixed positions; even at very low temperatures.\n- Individual particles in liquids and gases have no fixed positions and move chaotically.\n- The collisions between particles differ from collisions between macroscopic objects in that they are perfectly elastic: i.e. the kinetic energy of the particles remains constant and no energy is transformed into other forms during collisions.\nExplore the relationships between ideas about movement of particles in the\nConcept Development Maps - (Chemical Reactions, States of Matter)\nStudents at this level have been exposed to ideas about particles (including atoms, ions and molecules) on a number of occasions, yet many of them retain alternative or naïve views about the nature of particles and these can inhibit their understanding. Aim to adopt teaching strategies that promote dissatisfaction in students with their existing ideas, and promote a scientific conception that is plausible, consistent and useful in a variety of situations.\nBring out students’ existing ideas\nIt is important to ascertain the majority of students' prior views at the commencement of teaching to establish their existing understanding of the particle model of matter.\nAsk students for their ideas about the size of atoms compared with other small things such as cells, bacteria and viruses. This can be done by asking them to draw the relative size of these on the same scale (a scale where a human cell is the size of a page or poster). Bring out the idea that atoms are so much smaller again. Look for other activities that can help reinforce the idea that particles are very, very small.\nShow students the conventional drawings of particles in solids, liquids and gases and ask them if and how fast they think they are moving.\nChallenge some existing ideas\nA number of the issues raised in the focus idea ‘Conservation of mass’ are relevant here and the weighing of a flask containing a small amount of acetone before and after evaporation can be used to challenge students’ ideas about matter being lighter in the gas state and to raise problems with the static drawings of gas particles in texts. For more information see:\nConservation of mass.\nHelp students work out some of the ‘scientific’ explanation for themselves\nWith a little encouragement, a class can usually work out by discussion that the particles in gases must be hitting the bottom of the flask harder than the top and hence that they are affected by gravity. This can be extended to explaining why the Earth’s atmosphere thins and eventually ends – the upward vertical motion of the particles ceases.\nPromote reflection on and clarification of existing ideas and encourage students to identify phenomena not explained by the (currently presented) scientific model or idea\nAs particles cannot be directly observed, much of the teaching involves looking for apparent problems or inadequacies with the sorts of static pictures of particles given in earlier years. Encourage students to identify these and talk through possible explanations. Some prompts:\n- What holds air particles up?\n- Are air particles moving faster on a windy day?\n- How can gases have weight?\n- Why don’t air molecules fly off into outer space?\nIf needed, raise issues such as these, which will open up discussion, but it is better if the students themselves come up with some. Note that many of the issues are to do with gases – it is their properties that we most need a particulate model to explain.\nTo reinforce the notion of elastic collisions, ask what would happen if collisions between gas particles were not elastic. What practical consequences would there be for people? This can be introduced by dropping different types of balls (such as a soccer ball, a table tennis ball and a bouncy ball (from toy shops)) and explaining that a bouncy ball behaves more like gas particles.\nOpen up discussion via a shared experience\nUsing activities like POE (Predict-Observe-Explain) can help students think about and then question their existing ideas. The following activity will help students consider their ideas about the movement of particles.\nSet up two pairs of flasks each connected by a valve (see diagrams below). Both pairs have brown nitrogen dioxide in the left hand side flask.\nThe first pair also has air in the right hand side flask. Students are asked to predict what will happen when the valve between the two flasks is opened. The brown colour will spread very slowly from one flask to the other because the particles have frequent collisions with the air particles.\nThe second pair of flasks has brown gas in the left hand side flask but the right hand side flask is completely evacuated. Students are asked again to predict what happens when the valve is opened. The very fast speed of the molecules means that they fill the evacuated flask very quickly.\nDiffusion experiments can reinforce the idea of movement of particles. These can also be used as POEs.\n- a crystal of copper sulphate is placed in agar gel; the blue colour slowly diffuses through the gel\n- a potassium permanganate crystal is placed in a glass and water is slowly added. See the image. Alternatively water is very slowly added to a solution of potassium permanganate in a burette.\nBrownian motion can also be observed using stereo microscopes when sulphur powder or camphor is sprinkled on the surface of water or ethanol.\nPractise using and build the perceived usefulness of a scientific model or idea\nA cotton wool piece soaked in ammonia is placed at one end of a long glass tube with another soaked in hydrochloric acid (HCl) placed at the other end. Eventually a white ring will form where the two gases meet. The two gases are at the same temperature and thus the particles have the same kinetic energy; the ring forms closer to the source of heavier and thus slower moving HCl. This is predicted by a comparison of the relative molecular masses. Including a strip of universal indicator paper in the tube allows the gas diffusion to be tracked. This is an example of a POE where it is useful to draw students’ attention to a relevant piece of science before they make their prediction as it builds usefulness for the concept of relative molecular mass (Mr values).\nStudents need to be given the opportunity to use the scientific conceptions about particle theory in other settings. Ask students to observe and then explain the changes in terms of particle movement in scenarios such as melting wax or plastic, mothballs (naphthalene) vanishing in a cupboard and the scent of perfume spreading through a room.","Kinetic Molecular Theory of Matter\nMatter is made up of particles which are in constant, random motion. It can be defined as anything which has mass or occupies space. Matter is classified by its state and type, of which there are three main types – solid, liquid and gas. These lead us to the Kinetic Molecular Theory of Matter.\nParticle Arrangement in Matter\n- Particles in the solid state are closely packed, in a regular arrangement, known sometimes as a lattice.\n- Particles in a liquid state are not as closely packed and are irregular in their arrangement.\n- In a gas, particles are separated.\nForces Between Particles\n- In the solid state, the forces are strong enough to keep the particles in a fixed position. Particles do, however, vibrate and rotate in their positions.\n- In the liquid state, there are weak forces which hold it together. The greater energy of the particles and the weaker forces allows for the disruption of the lattice and particles are, therefore able to slide past one another.\n- In the gaseous state, particles possess even higher energy levels and the forces which hold the gas together are negligible. This explains why gases are able to isolate themselves completely from one another and have no fixed size or shape.\nChanges in State\nChanges in state (also called phase transitions) involve heat energy being supplied to or removed from the substance. Increasing the amount of heat energy in a substance increases its kinetic energy since temperature is a measure of the amount of kinetic energy possessed by a substance. In a solid, heat energy causes the particles to vibrate at a greater rate until they possess sufficient energy to break away from their fixed position and become a liquid (known as melting). The temperature at which this occurs is called a substances melting point. As heat is removed from the liquid, the particles return to their closely-packed, fixed positions, this process is called freezing.\nWhen heat energy is supplied to a liquid, the particles also take on more heat energy which causes them to move around at a greater speed. Fast-moving particles at the surface of the liquid eventually have sufficient energy to escape from the liquid and move into the gaseous state. Here, these particles move rapidly, at a large distance from the other particles. This process is known as vaporisation. The point at which a substance moves from the liquid to the gas state is known as its boiling point. As heat is removed from the substance the particles move closer together once more, this process is called condensation.\nSome substances e.g. iodine crystals are able to change from a solid directly into a gas, without moving through the liquid state. This process is called sublimation. Deposition is the reverse of sublimation and occurs when a substance moves directly from a gas to a solid, omitting the liquid state. An example of this can be seen in sub-zero temperatures, where water vapor in the air changes directly into ice, without first becoming a liquid.\nSelect the best answer for each of the following questions.\n- Which of the following is an example of a molecule, but not a compound?\n- Which of the following is the correct definition for an atom?\n- The smallest building block of matter\n- The smallest building block of matter that retains the chemical properties of the element.\n- The smallest, indivisible building block of matter.\n- The purest type of matter.\nDecide whether the following statements are true or false.\n- All molecules are compounds, but not all compounds are molecules.\n- Compounds are pure substances.\nTheobromine, is a bitter alkaloid of the cacao plant found in chocolate. It has the chemical formula C7H8N4O2.\n- Calculate how many atoms are in two molecules of C7H8N4O2.\nPlasma – the 4th State of Matter\nPlasmas, like gases, have no fixed shape or volume. A gas can reach the plasma state when its atoms become ionized. This occurs when the atom loses some or all of the electrons leaving a positively charged nucleus. This process is known as ionization and explains why plasmas are able to conduct electricity since the electrons are free to move around. Recombination occurs when plasmas return to the gaseous state.\nNeon signs are an example of plasma. The electricity flows through the glass tube containing the gas, stripping the atoms of their electrons. The electricity promotes the electrons to a higher energy level. As the electron returned to its former energy level the excess energy is carried away as a photon, which we see as coloured light.\nVideo Lesson States of Matter\n- Decide if the following are true or false.\n- Solids possess a greater level of kinetic energy than gases.\n- The forces which hold a liquid together are weaker than those which hold a solid together.\n- Some solids exist in a regular structure known as a lattice.\n- Gases have a fixed shape when in a container.\n- Decide whether energy needs to added or removed for each of the following changes of state.\n- Decide which of the properties described below can be attributed to solid, liquid or gas. (Note some properties will be relevant for more than one state.)\n- Can be compressed\n- Requires large amounts of energy input in order to change state.\n- Particles are isolated\n- Fits the shape of the container\nSummarize the differences between solids, liquids and gases for the following:\n- Arrangement of particles\n- Shape (draw a diagram)\n- Shape of substance\n- Level of energy possessed by particles\n- Strength of forces involved\n- Processes involved to change state\nHere is your Free Content for this Lesson on Kinetic Molecular Theory of Matter!\nKinetic Molecular Theory of Matter - PDFs\n- 1-2 Additional Resources - Kinetic Molecular Theory of Matter (FREE)\n- 1-2 Bell Ringer SE - Kinetic Molecular Theory of Matter (FREE)\n- 1-2 Bell Ringer Teacher Edition - (MEMBERS ONLY)\n- 1-2 Guided Notes SE - Kinetic Molecular Theory of Matter (FREE)\n- 1-2 Guided Notes Teacher Edition - (MEMBERS ONLY)\n- 1-2 Homework Questions SE - Kinetic Molecular Theory of Matter (FREE)\n- 1-2 Homework Questions Teacher Edition - (MEMBERS ONLY)\n- 1-2 Lesson Plan - Kinetic Molecular Theory of Matter (FREE)\n- 1-2 Slide Show - Kinetic Molecular Theory of Matter (FREE)\n- 1-2 Vocabulary Doodle Notes SE - Kinetic Molecular Theory of Matter (FREE)\n- 1-2 Exit Quiz SE - Kinetic Molecular Theory of Matter (FREE)\n- 1-2 Exit Quiz Teacher Edition - (MEMBERS ONLY)\n- 1-2 Vocabulary Worksheet SE - Kinetic Molecular Theory of Matter (FREE)\n- 1-2 Vocabulary Worksheet Teacher Edition - (MEMBERS ONLY)\nKinetic Molecular Theory of Matter - Word Docs & PowerPoints\nTo gain access to our editable content Join the iTeachly Chemistry Teacher Community!\nHere you will find hundreds of lessons, a community of teachers for support, and materials that are always up to date with the latest standards.\nWant access to all of our Chemistry Lessons?\nSimply click the image below to GET ALL OF OUR LESSONS!\nDon't Forget to Pin this Lesson on the Structure and Properties of Matter - Kinetic Molecular Theory of Matter...\nThis lesson is from..."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:b94c51f8-7757-40f6-91e9-e52a6cf11272>","<urn:uuid:c8d0dda1-c8bf-45b9-a61e-ae8eb2301611>"],"error":null}
{"question":"How do the ancient Aramean and Hindu deities compare in their representation of divine power through multiple aspects or forms?","answer":"Both Aramean and Hindu deities demonstrate divine power through multiple aspects, though in different ways. The Aramean panel shows eight gods with distinct roles and attributes - including Hadad as the storm god with a lightning fork, Atargatis as the mother figure, Sîn as the moon god with a crescent crown, and Šamaš as the sun god with a winged sun. Similarly, Hindu deities express multiplicity of power - particularly Shiva, who is depicted with multiple heads/arms, a third eye, and various attributes including a trident. He embodies duality through multiple roles as both Destroyer and regenerative force. The multiplicity in both cultures' divine representations suggests different approaches to expressing supernatural power - the Arameans through a procession of distinct deities, and Hinduism through individual deities taking multiple forms.","context":["While constructing a home in Turkey, looters realized they’d been building atop an ancient chamber. Instead of alerting local authorities, they began to dig themselves—and only after the heritage criminals were apprehended did archaeologists arrive to save the day (and the artifacts within).\nNow, a stone panel uncovered during their emergency excavation sheds new light on the melting pot of cultures that coexisted during the early days of the expansion of Mesopotamia’s Neo-Assyrian Empire nearly 3,000 years ago.\nAn article published this week in Antiquity reveals more about the panel, which likely dates to between the ninth and eighth centuries B.C.E. The symbols inscribed there represent a variety of cultures—the inscriptions are written in Aramaic, while its carvings reflect a Neo-Assyrian style with a local Syro-Anatolian flair.\nThe panel shows a procession of eight Aramean gods. Among them are storm god Hadad, who is depicted with a trident-esque lightning fork and looms larger than the rest. The goddess Atargatis, known as the mother figure and frequently depicted as the companion of Hadad; the moon god Sîn, who wears a crown of a crescent and full moon; and the sun god Šamaš, sporting a winged sun on his head, are also present.\nScholars believe the work was left unfinished, since only the gods’ upper bodies or heads in profile are shown.\nThe panel shows “ … a local cohabitation and symbiosis of the Assyrians and Arameans in a region and period under firm Assyrian imperial control,” the authors write. They call the panel “ … a striking example of regional values in the exercise of imperial power”—a relationship that could have involved a give and take between Assyrians who wanted to impress their new subjects and Arameans eager to please their new overlords.\nThe site was likely uncovered when the home in the southeastern Turkish village Başbük was being built, reports National Geographic’s Tom Metcalfe. Instead of notifying authorities, looters cut a hole into the ground floor of the two-story home. On the other end of that makeshift entryway lay an ancient chamber hewn out of the limestone bedrock that led by staircase to another lower room.\nExcavations went on for two months in 2018 before archaeologists were forced to stop due to the “instability of the site,” according to the study.\nThe rock art is from an active time in Assyrian history. Between 900 and 600 B.C.E., the Neo-Assyrian empire expanded its territory within southeastern Anatolia. The region was once largely ruled by city-state dynasties whose residents spoke Aramean and Luwian. But gradually, Neo-Assyrian King Ashurnasirpal II took control of notable Aramean settlements.\nThis meshing of cultures is evident in not just the language of the inscriptions but the way in which the gods are portrayed. The panel’s dating captures the Assyrian takeover of power “in its early phases,” study author Selim Ferruh Adali, a history professor at the Social Sciences University of Ankara, tells Live Science’s Emily Staniforth.\n“Although some features of the gods are distinctly Assyrian—such as their rigid poses, and the particular style of their hair and beards—many details of the carvings show strong influences from the local Aramaic culture,” Metcalfe writes for National Geographic.\nArchaeologists also found an inscription that might offer insight into the work’s purpose. Aramaic writing on the panel appears to include the name of Mukīn-abūa, an Assyrian official in charge of the province of Tušhan around 811 to 783 B.C.E. It may mean that the complex was created as a way to engender favor with local residents, says Adali to CNN’s Ashley Strickland.\n“The panel was made by local artists serving Assyrian authorities who adapted Neo-Assyrian art in a provincial context,” Adali says. “It was used to carry out rituals overseen by provincial authorities. It may have been abandoned due to a change in provincial authorities and practices or due to an arising political-military conflict.”\nThough it’s unclear what caused artisans to abandon the panel, the art they began is effective, even thousands of years later. Even now, its depictions hold an emotional power over those who cast their eyes on them.\n“I felt as if I was in a ritual,” says lead author Mehmet Önal, archaeologist at Harran University, to National Geographic. “When I was confronted by the very expressive eyes and majestic, serious face of the storm god Hadad, I felt a slight tremor in my body.”","Presentation on theme: \"South, East, Southeast Asia 1 images 192-197 Content Area 8 Themes: Sacred architecture, After life, Representation of Deities, Landscape, Royal Portraiture,\"— Presentation transcript:\nSouth, East, Southeast Asia 1 images 192-197 Content Area 8 Themes: Sacred architecture, After life, Representation of Deities, Landscape, Royal Portraiture, Power and Authority, Propaganda\nGuiding Questions 1.How do art and architecture reflect beliefs and practices? 2.How do art and architecture reveal cross- cultural connections and influences?\nRequired Images (21) 1.Great Stupa at Sanchi. Madhya Pradesh, India. Buddhist; Maurya, late Sunga Dynasty. c. 300 BCE – 100 CE. Stone Masonry, sandstone on dome ( 4 images) 2.Terra cotta warriors from mausoleum of the first Qin emperor of China. Qin dynasty. c. 221-209 BCE. Painted terra cotta. (2 images) 3.Funeral banner of Lady Dai (Xin Zhui). Han Dynasty, China. c. 180 BCE. Painted silk. 4.Longmen caves. Luoyang, China. Tang dynasty. 493-1127 CE. Limestone. (3 images). 5.Gold and jade crown. Three Kingdoms Period, Silla Kingdom, Korea. Fifth to Sixth century CE. Metalwork. 6.Todai-ji. Nara, Japan. Various artists, including sculptors Unkei and Keikei, as well as the Kei School. 743 CE; rebuilt c. 1700. Bronze and wood (sculpture); wood with ceramic tile roofing (architecture). (5 images). 7.Borobudur Temple. Central Java, Indonesia. Sailendra dynasty. c. 750-842 CE. Volcanic-stone masonry. (3 images)\nRequired Images p. 2 8.Angkor, the temple of Angkor Wat, and the city of Angkor Thom, Cambodia. Hindu, Angkor dynasty. c. 800 – 1400 CE. Stone masonry, sandstone. (5 images) 9.Lakshmana Temple. Khajuraho, India. Hindu, Chandella dynasty. c. 930-950 CE. Sandstone. (4 images) 10.Travelers among Mountains and Streams. Fan Kuan. c. 1000 CE. Ink and colors on silk. 11.Shiva as Lord of Dance (Nataraja). Hindu; India (Tamil Nadu), Chola dynasty, c. 11 th century CE. Cast bronze. 12.Night Attack on the Sanjó Palace. Kamakura Period, Japan. c. 1250-1300 CE. Handscroll (ink and color on paper). (2 images) 13.The David Vases. Yuan Dynasty, China. 1351 CE. White porcelain with cobalt-blue underglaze. 14.Portrait of Sin Sukju (1417 – 1475). Imperial Bureau of Painting. c. 15 th century CE. Hanging scroll (ink and color on silk).\nRequired images p. 3 15.Forbidden City. Beijing, China. Ming dynasty. 15 th century CE and later. Stone masonry, marble, brick, wood, and ceramic tile. (5 images) 16.Ryoan-ji. Kyoto, Japan. Muromachi Period, Japan. c. 1480 CE; current design most likely dates to the 18 th century. Rock garden. (3 images) 17.Jahangir Preferring a Sufi Shaikh to Kings. Bichitr. c. 1620 CE. Watercolor, gold, and ink on paper. 18.Taj Mahal. Agra, Uttar Pradesh, India. Masons, marble workers, mosaicists, and decorators working under the supervision of Ustad Ahmad Lahori, architect of the emperor 1632-1653 CE. Stone masonry and marble with inlay of precious and semiprecious stones; gardens. (2 images) 19.White and Red Plum Blossoms. Ogata Korin. c. 1710-1716 CE. Ink, watercolor, and gold leaf on paper. (2 images) 20.Under the Great Wave off Kanagawa, also known as the Great Wave, from the series Thirty-Six views of Mt. Fufi. Katsushika Hokusai. 1830- 1833 CE. Polychrome woodblock print; ink and color on paper. 21.Chairman Mao en Route to Anyuan. Artist unknown; based on an oil painting by Liu Chunhua. c. 1969 CE. Color Lithograph.\nHinduism 101 Monotheism displayed through polytheistic views literary origins of Hindu date to the Vedic period and to the Indus Valley Civilization Multiplicity of deities suggest the all-pervasive nature of the Hindu gods. Three main gods: Shiva, Vishnu, and Devi The Hindu Trimurti consists of Brahma the Creator, Vishnu the Preserver, and Shiva the Destroyer. Hinduism is bound to the hierarchal structure of the caste system: Scholars (Bramans), Warriors/princesses/princes (Kshateriya), Merchants (Vyshas), and farmers and laborers (Shudras) Position in caste system is reflection of accumulated merit in past lives, cause and effect = karma. Samsara is the cyclical reincarnation Ultimate goal is liberation and release from samsara, known as Moksha\nHinduism 102 - Shiva Shiva is the Destroyer as well as a regenerative force. Duality through multiplicity of roles. Shiva can be represented in the form of a linga, a phallic or cosmic pillar, emphasizing his regenerative nature. Frequently depicted with many limbs or heads, again multiplicity in form and power – suprahuman Shiva as Nataraja, Lord of dance Attributes: multiple heads/arms, third eye on forehead, carries trident, dreadlock hair, represented as great yogi and teacher, linga, serpent scarf, rides the bull, Nandi Son is Ganesha, god of good fortune and remover of obstacles, has the head of an elephant\nHinduism 103 - Vishnu Vishnu is the Preserver of the Universe, embodiment of mercy and goodness, and maintains the cosmic order Often depicted with four arms holding different attributes, such as conch-shell, disc, a club, and a lotus Often laying on a coiled serpent Shesha in the cosmic sea Has transformed into 9 out of 10 avatars to help bring balance back to earth: fish, turtle, boar, lion, dwarf, Parasurama, Ram, Krishna, Buddha, and the tenth is yet to come (Kalki).\nHinduism 104 - Devi Devi the Great Goddess who take many forms Parvati is the wife of Shiva Lakshmi is the wife of Vishnu Radha is the lover of Krishna In on manifestation, Devi is Durga, a multi- armed goddess who often rides a lion. Devi creates and destroys Son is Ganesha\nBuddhism 101 - Buddha Buddha (the Enlightened One) was born Siddhartha Gautama, eldest son of a royal family. Was a Hindu At the age of 29, the prince abandoned everything and everyone, witnessed pain and suffering, and became an ascetic Reached enlightenment at age of 35 through meditation under the Bodhi tree at Bodh Gaya in eastern India.\nBuddhism 102 – Four Noble Truths The Wheel of the Law, or dharmachakra, are the teachings of the Buddha Four Noble Truths: 1.Life is suffering 2.The cause of suffering is desire 3.one can overcome and extinguish desire 4.the way to conquer desire and end suffering is following Buddha’s Eightfold Path\nBuddhism 103 – Eightfold Path 1.Right understanding 2.Right thought 3.Right speech 4.Right action 5.Right livelihood 6.Right effort 7.Right mindfulness 8.Right concentration Buddha’s path leads to nirvana. Life is cyclical until nirvana is reached.\nQuestions addressed by architects of sacred structures: 1. Is there communal ritual? 2. Is there movement from point to point by ritual participants? 3. Is there a focal point participants must be able to see during the ritual? 4. How can transitions into more sacred space be provided? 5. How can the plan and decoration reflect beliefs of the participants?\nGreat Stupa at Sanchi. Madhya Pradesh, India. Buddhist; Maurya, late Sunga Dynasty. c. 300 B.C.E.–100 C.E. Stone masonry, sandstone on dome. 192\n-represents burial mound of the Buddha where relics were enclosed -focal point of worship -perched on a hill, surrounded by other smaller Stupas -4 Gateways (toranas) mark the cardinal directions – added later -Worshippers circumambulate in a clockwise direction -Toranas were elaborately carved, connecting secular world with spiritual realm (next slide) -Local Fertility deities, served to sanctify the site Great Stupa at Sanchi. Madhya Pradesh, India. Buddhist; Maurya, late Sunga Dynasty. c. 300 B.C.E.–100 C.E. Stone masonry, sandstone on dome. Built by King Ashoka, a converted Buddhist Buddhist Monuments at Sanchi (UNESCO/NHK) 2:53\nNorth Torana There are no human depictions of Buddha here because he has reached enlightenment Women depicted are Yakshi, Pre-Buddhist personifications of water, fertilit,y and vegetation\nGreat Stupa at Sanchi. Madhya Pradesh, India. Buddhist; Maurya, late Sunga Dynasty. c. 300 B.C.E.–100 C.E. Stone masonry, sandstone on dome. 1.Harmika: square area symbolizes the sacred domain of the gods 2.yashti is a pole = axis of universe 3.chatras represent Buddhas past, present, and future\nBuddhists venerated Buddha’s remains by circumambulation Stupa WAS NOT entered Circular movement echoed that of the earth and sun – brought the devotees in harmony with the cosmos Stupa represents a mountain, a axis mundi – connecting earth with the heavens Great Stupa at Sanchi. Madhya Pradesh, India. Buddhist; Maurya, late Sunga Dynasty. c. 300 B.C.E.–100 C.E. Stone masonry, sandstone on dome.\nCross Cultural Connections Take a moment with your partner(s) and connect the Great Stupa at Sanchi with another sacred site that incorporates the idea of axis mundi. Please fully identify your site, justify your decision, and be ready to share in a few minutes!\nTerra cotta warriors from mausoleum of the first Qin emperor of China. Qin Dynasty. c. 221–209 B.C.E. Painted terra cotta. 193\n8,000 or more terracotta soldiers and horses, bronze horses and chariots guard the tombs of China’s First Emperor, Qin Shi Huangdi Emperor Qin built the Great Wall to keep nomadic invaders out from the north (Huns) He restored order and consolidated power BRUTALLY The basic but considerable difference between the state of Qin and the states that had been conquered was that privileges of the nobility were abandoned and officials who were assigned for government positions were selected according to merits.\nArtists varied the combination of parts and the coloration, thus individualizing each figure Uniformity + individuality Even shows different ethnic groups that made up Emperor Qin’s army Emperor Qin conscripted more than 700,000 laborers to work on his tomb construction About 2,000 statues of cavalry, archers, lancers, chariots, and hand-to-hand fighters Undeniable connection to the POWER AND AUTHORITY of Emperor Qin Also, kudos to his imperial workshop Aligned to the road to the Mausoleum, army is to guard the Emperor in the after life VideoVideoes\nIn order to consolidate his power the Qin emperor standardized: - the script, - weights and measures, - the currency, and - the length of the cart axles. - He standardized a law code which everybody had to obey to and which prohibited the private possession of arms, and he - installed a state police and a secret service as government agencies. - Roads and canals were built to link all areas of the territory and move soldiers and supply fast.\nEarly China The Han Dynasty Lady Dai 12:22 Funeral banner of Lady Dai (Xin Zhui). Han Dynasty, China. c. 180 B.C.E. Painted silk. 194\nThis T-shaped silk banner covered the coffin of Lady Dai Lady Dai was the wife of Han ruler Marquis Li Cang Also in her tomb was additional embroidered silk gowns, 154 lacquer dishes, 51 ceramics, 48 bamboo suitcases of clothing and household goods, baskets of gold pieces and bronze coins. All things needed to be comfortable in the AFTERLIFE There was also an inventory of food items offered in her tomb: rice, wheat, barley, millet, soybeans, red lentils, 13 different meat dishes made from a variety of 7 kinds of meat. The scenes are interpreted as showing the modes of existence of the soul after death. The corpse is placed in the tomb where it is served by underworld attendants. The body soul enjoys and consumes the burial objects and offerings. At the same time the spirit soul (hun) ascends to the realm of the immortals and seems to rejuvenate during this process.\nThe lower section of the banner shows the offerings and ceremonies devoted to her body soul (po). Sacrificial vessels are provided for her and attendants are standing next to her, ready to serve her soul which resides in the tomb. Beneath the tomb we get a glimpse of the creatures living in the underworld: A deity of the earth carries the foundation of the tomb, her netherworld dwelling.\nThe central part of the banner shows Lady Dai in a standing position. She leans on a cane, while two persons crouch or kneel in front of her and three women, presumably female attendants, stand behind her.\nThe upper part of the banner is said to show the realm of the immortals. The entrance is guarded by two deities holding the records of the life span of Lady Dai. They are identified as deities of destiny. In the top section we can see a standing woman. She is surrounded by a creature with a snake-like body and flanked by the depictions of the moon with a toad and a rabbit (which is said to pound the elixir of immortality) and the sun with a raven. Five birds seem to keep her company which may represent the figures of the lower parts of the banner.\nSilk Road is established during Han dynasty Tomb was found as a mound Body and tomb was very well preserved Heavens Earthly World Spiritual World\nChinese Buddhist Cave Shrines 4:35 Longmen caves. Luoyang, China. Tang Dynasty. 493–1127 C.E. Limestone. 195 b/c 2300 caves were carved into the cliffs with Buddha's of all sizes Central cosmic Vairocana Buddha is more than 44ft. tall – seated Commissioned by Empress Wu Zetian flanking the central Buddha are bodhisattvas, monks, and guardian figures\nThe central Vairocana Buddha (more than 55 feet high including its pedestal) is flanked on either side by a bodhisattva, a heavenly king, and a thunderbolt holder (vajrapani). Vairocana represents the primordial Buddha who generates and presides over all the Buddhas of the infinite universes that form Buddhist cosmology. This idea—of the power of one supreme deity over all the others—resonated in the vast Tang Empire which was dominated by the Emperor at its summit and supported by his subordinate officials. These monumental sculptures intentionally mirrored the political situation. The dignity and imposing presence of Buddha and the sumptuous appearance of his attendant bodhisattvas is significant in this context.\nVaiśravana, one of The Four Heavenly Kings, is on the left (indicated by the stupa in his right hand). Vajrapāṇi (on the right) are spiritual beings that wield the thunderbolt, 673-675 C.E., Tang dynasty, limestone, Luoyang, Henan province\n196 Gold and jade crown. Three Kingdoms Period, Silla Kingdom, Korea. Fifth to sixth century C.E. Metalwork. Silla Kingdom was the most powerful Kingdom out of three rivaling Kingdoms in Korea Known as a Kingdom of Gold Crown is very carefully constructed, looks fragile in form Worn for burial purposes & special ceremonial rites of the Silla Kingdom Before Buddhism arrived in Korea, Silla Kingdom practiced shamanism – connection to nature worship through Shaman who acts as intercessor\nSilla royalty upheld shamanistic practices in ceremonial rites such as coronations and memorial services. In these sacred rituals, the gold crowns emphasized the power of the wearer through their precious materials and natural imagery. Three-pronged crown symbolized the sacred tree found in the Silla capital city, Gyeongju This sacred tree was conceived of as a “world tree,” or an axis mundi that connected heaven and earth. Silla gold crowns were used both above ground and below, and their luxurious materials conveyed the social status of the tomb occupant in the afterlife.\nTake a moment to Reflect Considering symbolism and significance of the material GOLD, compare and contrast two different cultures with regards to their symbolic connection to GOLD. How is GOLD a symbol of power and authority, and how is GOLD used within that culture?\nTodai-ji. Nara, Japan. Various artists, including sculptors Unkei and Kaikei, as well as the Kei School. 743 CE; rebuilt c. 1700. Bronze and wood (sculpture); wood with ceramic tile roofing (architecture). (5 images). 197a VideoVideo, :30 – 2:15\nCommissioned by Emperor Shomu 2/3 the size of the original! Original had 11 bays, this one has 7\nBuddhism was introduced to Japan in 552 CE to the ruling elite from the Korean king, product of silk road Seated Cosmic Buddha is larger than 53 feet. Same Buddha as Longman Caves 53 feet Associated with the sun Scale implies consolidation of Imperial authority + penetration of Buddhism 197b\nLarge scale god representation to elicit power and authority of Emperor Shomu (left) and Empress Wu Zetian (right) as well as power and authority of developing Buddhism\nNandaimon (Great South Gate), built during Kamakura Period 12 th century, Nara Highly inspired by Chinese architecture (product of Chogen’s travels to China) Houses two Guardian figure sculptures… 197e\nLeft: Ungyo, sculpted by Unkei, Kamakura Period Right: Agyo, sculpted by Kaikei, Kamakura Period Both artists were products of the Kei School of sculpture, most prominent during the 12 th century when Todai-ji was being rebuilt. 1 st rebuilt during the Kamakura Period (1185-1333) Buddhist Priest, Shunjobo Chogen, was put in charge of rebuilding 197c /d Kongo-rikishi: Guardian statues 8 meters tall = 26.2 feet\nJapanese Architecture The grand Buddhist architectural and sculptural projects of early Japan share a common material—wood–and are thus closely linked to the natural environment and to the long history of wood craftsmanship in Japan. CONNECTION TO NATURE"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:52f2f7d4-4d0a-49ad-a13a-32f9d27a47a3>","<urn:uuid:fc70de2d-b16d-43cf-bfd3-d38195f11339>"],"error":null}
{"question":"How can I create a more engaging scientific poster using modern technology? I'm presenting at a conference next month.","answer":"You can enhance your scientific poster using several modern technologies: 1) Convert it to an e-Poster format, which uses a large monitor and computer to display content and allows for multimedia elements. 2) Add QR codes to allow attendees to capture electronic versions for later viewing. 3) Implement Augmented Reality through smart device apps, which can add digital content like videos, additional graphics, text, tables, and 3D experiences to a traditional paper poster. This lets viewers access additional information beyond what's visible on the printed poster. However, note that augmented reality requires 2-3 weeks of additional development time and may incur extra costs depending on complexity.","context":["Lana Vegman, PharmD, Envision Pharma Group\nStrictly defined, a poster is a paper print designed for display on a wall or vertical surface.1,2 Posters can include visual images, text, or both. The printed poster dates back to the 1840s, following the advent of color lithography, which the printing industry refined and ultimately automated to make mass production possible.2\nOver time, the media for these creative visual pieces has evolved from paper, cardboard, photographs, laminated paper, and cloth, to electronic posters (e-Posters), all in an effort to increase reach and bring attention to the information being conveyed. A successful poster captures the viewer’s attention, and communicates the key points clearly and succinctly.\nMedical conferences generally include sessions for scientists to display the results of their research in a poster format. These research posters are a good way of presenting information to reach a large audience, particularly when they are made available in an electronic format after the meeting to individuals who did not attend the congress.\nAt a congress, poster presentations are less coveted than oral presentations by researchers because posters are less prestigious and generally have more limited impact and reach.3 However, posters play a critical role in medical conferences because they are more numerous than oral presentations, and they provide an opportunity to display not only completed research and novel data that might impact clinical decision-making, but also preliminary findings and ongoing research that provides the larger scientific community with the latest developments in their field.\nWhile posters at congresses certainly provide value, there are areas that can be, and are being, improved upon. A study published in 2009 demonstrated that among 142 posters reviewed at a national meeting, 33% were cluttered or sloppy, 22% had fonts that were too small to be easily read, and 38% had research objectives that could not be located in a one-minute review.4\nRecent trends have shown a shift away from traditional paper-based posters to e-Posters, which were introduced and have grown in popularity in the past five years at medical congresses. An e-Poster utilizes a large monitor and computer to display the poster, and may allow for some multimedia content. While it provides an opportunity to more effectively convey the information and enhance visualization and appeal, it also requires both computer-savvy presenters and viewers to be able to make the most of the content. Additionally, the graphics in an e-Poster become more critical components of the presentation, and these must be appropriately designed to ensure maximal functionality and ease of use; issues with these elements may deter viewers from being able to review the entire e-Poster content, thus missing critical information.\nAround the early 2010s, quick response (QR) codes were introduced as a novel way to extend the reach of printed poster presentations beyond the congress. Anyone who has walked through a crowded congress hall full of posters, but with limited time, immediately recognized the impact of quickly scanning QR codes to capture electronic versions of the posters for reading in detail later, and spending more time interacting with presenters. An added advantage of QR codes is that they eliminate the need for printed handouts, which can be burdensome to organize and store, or all too quickly become trash.\nAs the poster has evolved, one unresolved question remains at the forefront – how does one combine engaging graphical elements with explanatory text, while still providing the necessary details without any clutter? One solution may be Augmented Reality. While this concept has been used in cinema for some time, augmented reality more recently has extended its reach to scientific conferences.\nAugmented reality is a way to add digital content, such as videos, additional graphics, text, tables, figures, and other 3D experience to the traditional paper poster (Figure 1). This is done through use of various apps that are launched using a particular launch point on the poster through a smart device (eg, phone, iPAD). The launch point is treated as a “hot spot” that transforms sections of the poster from its static form to provide user access to information beyond what is visible on the printed poster itself. Augmented reality provides an opportunity to add more depth to and interaction with a poster, without disrupting conveyance of the main points.\nFigure 1: Evolution of a poster – use of Augmented Reality 5\nPresenting authors are generally required to stand by their poster for one to two hours, sometimes longer, to provide an overview of the findings and address questions. Often, they are asked the same question repeatedly by different viewers because space limitations on the printed poster did not allow inclusion of enough detail. Adding an icon to a critical figure, table, or piece of text can give the author the ability to provide information that is not physically printed on a poster that will enhance understanding of the content, stimulate discussions, and preempt questions.\nAugmented reality also allows the attendee to interact with the poster, even when the presenting author is not available. And, if the printed poster also includes QR codes, attendees can capture electronic versions of the poster, along with the additional augmented reality content, as a take away.\nWhile there are several benefits to exploring this new technology, when considering augmented reality, additional timing and cost for development of the interactive components and graphical elements need to be carefully weighed. Depending on the level and complexity of the animation and graphics, an additional 2-3 weeks should be planned to allow sufficient time for development and quality check of the added components and final poster. While the costs for development vary, an assessment of the complexity and number of augmented reality “hot spots” desired in a given poster can help guide the additional cost required.\nAlthough platform presentations are widely considered more prestigious, poster presentations can offer a better opportunity for attendees to engage with their colleagues to more fully assimilate the newest advancements in their area. From its start in the 1840s as a simple paper image to the present, where use of technology makes it possible to create additional interest and provide more depth and dimension, the scientific poster has had the same goals of conveying the latest research findings, stimulating further research, and for key findings, impacting clinical decision-making. The only difference being in how the information is served up and taken away.\nAll this makes you wonder, what will tomorrow bring?\nDisclosure: Editorial support for this article was provided by Alison Gagnon, PhD, of Envision Pharma Group\n- Gosling, Peter. (1999). Scientist’s Guide to Poster Presentations. New York: Kluwer.\n- Wikipedia, the free encyclopedia. Available at https://en.wikipedia.org/wiki/Poster. Accessed on January 8, 2016.\n- Hess GR, Tosney KW, Liegel LH. Creating effective poster presentations: AMEE guide no. 40. Med Teach 2009;31:319-21\n- Swales, J.M., & Feak, C. (2000). English in Today’s Research World: A Writer’s Guide. Michigan: Michigan University Press.\n- Figure courtesy of Envision Pharma Group. All rights reserved"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:b47a2e40-3b4e-45b3-a70c-8fc206118a72>"],"error":null}
{"question":"Which route has more wildlife viewing opportunities - Vancouver Island Oceanside or Icefields Parkway?","answer":"Both routes offer excellent wildlife viewing, but the Icefields Parkway has greater diversity. While Vancouver Island Oceanside Route is notable for having one of the world's densest black bear populations (over 7,000) and opportunities to see black-tailed deer, cougars, grey wolves, and grizzly bears, the Icefields Parkway through Banff and Jasper National Parks hosts over 53 species of mammals and more than 260 species of birds. Along the Icefields Parkway, visitors commonly see mountain goats, caribou, moose, black bears and grizzly bears, making it the route with more diverse wildlife viewing opportunities.","context":["Could Canada be the best safari location\nyou've never thought of?\nREAD TIME: 12 minutes\nWhen you hear ‘safari’, you might think of the Masai Mara in Kenya, the Serengeti National Park in Tanzania or the Kruger National Park in South Africa. Canada is a destination that often gets overlooked.\nBut, considering that Canada is home to some of the best scenery, road trips and varied wildlife in the world, we think Canada is the best safari location that you’ve probably never considered! There’s no doubt that Icefields Parkway in Alberta and the Cabot Trail in Nova Scotia are two of the most popular places to go on your own safari, but there are lots of other intimate and authentic safari experiences to try in lesser-known areas of Canada.\nHere, we take you through some of the best locations in Canada where you can enjoy your very own safari.\nClick on our map to find out the locations and read below for more information about what you can see on each route.\nThe best destinations for a Canadian safari\nTravel writer and international television host Robin Esrock is the bestselling author of The Great Canadian Bucket List: Once-in-a-Lifetime Travel Experiences. He has visited all of the below routes and tells us that they are great for spotting wildlife.\n“I've visited all these amazing routes which put you in close proximity to a who's who of Canadian wildlife. In Prince Albert National Park, it's possible to encounter bison while being on horseback, the advantage being you can get much closer to these magnificent animals. You can snorkel with migrating salmon in Campbell River on Vancouver Island, and always be on the lookout for bears and moose in Newfoundland and on the Icefields Parkway (if you can tear your eyes away from the spectacular scenery).”\nVancouver Island Oceanside Route\nA great destination for wildlife watching is along the Vancouver Island Oceanside Route, which runs from Nanaimo (where the ferry from Vancouver arrives into) up to Campbell River.\nIf you are using our services for car hire in Vancouver, then driving along this route is a must as the Oceanside Route of Highway 19A is a quiet, nature-focused journey. It boasts a unique landscape: one minute you’ll be travelling through the valleys and beside the ocean and the next you’ll be heading over tall peaks.\nThe road hugs the coast and, as a result, there are plenty of beachfront towns along the road, such as the picturesque town of Nanaimo.\nAlthough you are still close to some urban areas, there’s lots of Vancouver wildlife for you to see. According to Discover Vancouver Island, there are more than 7,000 black bears on Vancouver Island, making the area one of the most densely populated places in the world for this type of bear. The black-tailed deer is another common sighting along the route and if you’re really lucky, you may even get a sighting of a cougar, grey wolf, or grizzly bear.\nMUST READ: Top places to visit on Vancouver Island\nOne of the most iconic driving routes not only in Canada but the world is the Icefields Parkway, as you journey through the spectacular Banff National Park and Jasper National Park.\nRated as one of the top drives in the world, the trip takes in the likes of Lake Louise, the Rocky Mountains and the Columbia Icefield.\nIt is also a great place to go on a safari in Canada and seeing wildlife up close and personal is often a highlight of a visit to the Canadian Rockies. With over 53 species of mammals and more than 260 species of birds in the national parks, you are very likely to see animals as you travel down this route.\nMountain goats, caribou, moose, black bears and grizzly bears are all commonly seen along the spectacular route so keep your eyes open!\nMUST READ: A Guide to the Icefields Parkway’s Top Stops\nHighway 60 Corridor\nThe Highway 60 Corridor passes through the southern section of Algonquin Provincial Park in Ontario and is open year-round. It is the most accessible area of the park and goes past a number of attractions and picnic areas, just a few hours north of Toronto.\nThe highway is also a well-known route for spotting wildlife, which should come as little surprise as Algonquin is home to 55 mammal species, 32 kinds of reptiles and amphibians, and more than 140 species of breeding birds. Moose, deer and wolves can all be seen from the Highway 60 Corridor.\nYou should aim to go in the early morning to see the birds and mammals. Plus, by heading out early, you will have the entire route to yourself with very few people about. When looking for wildlife, you should look for low-lying, wet areas like ponds or bogs as these are often the preferred habitat for many wildlife species and provide breaks in the thick forest.\nTo really see the wilderness of British Columbia’s north you should travel along the historic Alaska Highway. The highway has been described as the largest and most difficult construction project since the creation of the Panama Canal.\nThe entire Alaska Highway stretches from Dawson Creek in British Columbia to Delta Junction in Alaska (over 1,400 miles), but during the Canadian portion of the route, you will experience some spectacular scenery of the northern Canadian Rockies.\nEn route to Muncho Lake, you can stop off at Stone Mountain Provincial Park for incredible views and amazing wildlife viewing opportunities. So, keep your eyes peeled and take your binoculars to see if you can get a glimpse of moose, bison, eagles and other bird species.\nHighway 2 is a provincial highway in Saskatchewan that runs from Regina to Waskesiu and cuts through an area rich in history. If you’re driving along this route you should look to visit the Big Muddy Badlands, an area famed for being used by outlaws and rum runners to hide out during the 19th and early 20th century.\nAlong the route, you will wind up and down hills created by glacier movement and will get to see lots of wildlife. As you approach Waskesiu, you will be entering Saskatchewan’s bison country.\nThe bison here roam free to the west of Prince Albert National Park and it is now one of the few areas in the world where you can see plains bison within their historical range. After almost becoming extinct, the bison population has improved with around 1,000 of them now living peacefully in the area.\nLabrador Coastal Drive\nThis popular driving route goes through a rugged and pristine region of Labrador along the edge of the North Atlantic Ocean.\nBy driving along this route, you will be retracing the footsteps of ancient mariners and the Native Americans before them.\nThe Labrador Coastal Drive blends history, culture, scenery and wildlife together. There is lots of wildlife native to the area that you could see. Here are some of the animals you can see and some great places you could stop-off at along the coastal drive:\n- Whales at Point Amour\n- Sanderlings at Pinware\n- Seabirds like razorbills and gannets around Wonderstrands\nAs well as the wildlife, there are lots of historic attractions you can see along the route. The\nPoint Amour Lighthouse is the perfect viewpoint to see spectacular icebergs. The Red Bay National Historic Site of Canada is where you can hear stories and view artefacts from a Basque Whaling Station. The Battle Harbour is a charming restored fishing village.\nThe Cabot Trail\nWith its picturesque coastline and stunning highlands, the Cabot Trail in Nova Scotia is now regarded as one of the best driving routes you can experience.\nSomething that many people might not associate with the Cabot Trail is the fact it is also one of the best destinations for your own safari. There is a wide assortment of wildlife you can see along the route and it is not unusual to see whales or Atlantic puffins in the Gulf of St. Lawrence from viewing decks along the trail. If you’re lucky, you might see black bears and bald eagles too.\nIf you’re a keen hiker then you will be spoilt for choice as there are lots of scenic trails you can stop-off at and walk on. These include the Lone Shieling Trail at the base of North Mountain (a 15-minute walk) and the Skyline Trail which loops around part of the island.\nThe Fjord Route\nThe Fjord Route (or Route du Fjord as it is also known) is another great destination to see animals in the wild.\nThe route travels through a national park that is just teeming with wildlife, in a region with craggy mountains, rugged cliffs and dense forests. In Baie-Sainte-Catherine you might be able to spot a host of marine life including bluefin, minke and beluga whales as well as seals. The best time to go whale watching is from November to May.\nOn top of the incredible wildlife, you can visit quirky and historic French-Canadian villages such as Petit-Saguenay and Rivière-Éternité.\nAny time of year is good to drive this four-season destination, but one thing to be aware of is that the area is not touristy and many restaurant menus along the route are in French only, so it might be worth brushing up on your French or bringing a pocket dictionary with you!\nWhat are the top tips for photographing wildlife?\nIf you’re planning on hiring a car in Canada for your holiday and driving one of the aforementioned routes, you should follow some of these easy-to-apply tips and advice for photographing wildlife.\nTanya Stollznow, who runs her own wildlife photography blog, recommends the following tips when photographing wildlife:\n- “Research the location so that you make the most of opportunities to locate wildlife and the best times and places to photograph them.\n- “Aim for simple backgrounds. The most compelling wildlife photos are often the ones where the subject stands out without any distraction.\n- “Don’t be afraid to try something different. Aim for different viewpoints and composition.\n- “Respect the wildlife and the natural environment.\n- “Be aware of and adhere to rules, guidelines and advice offered by the local government and their park rangers.”\nRobin Esrock, who has explored more than 100 countries, tells us that photographing wildlife is about moments:\n“Capturing wildlife is all about moments, and each moment can be different from one to the next. It’s why people return year after year to their favourite spots, as you never know what you will see. You'll definitely need a good zoom, and it's rare to get super-close to animals, although I have had bears, moose and elk come right past my car. Luck is when preparation meets opportunity. To get the lucky shot you'll be showing people for years, prepare with a decent camera set on a fast shutter speed, a tripod if necessary, a good lens, and plenty of patience.”\nThe great safari locations in Canada\nJust to recap, below are the top places to see animals in the wild in Canada:\n- Icefields Parkway\n- Highway 60 Corridor\n- Highway 2 (Regina to Waskesiu)\n- Labrador Coastal Drive\n- The Cabot Trail\n- Alaska Highway\n- Vancouver Island Oceanside Route\n- The Fjord Route\nIf you’ve been inspired to make future travel plans and experience Canada’s incredible wildlife in its natural habitat, then check out our Canada holiday packages. That dream Canadian holiday is just a few steps away.","The drive from Banff to Jasper in the Canadian Rockies is rated as one of the most beautiful drives in the world. Road trips are the best way to experience the National Parks as you can explore at your own pace and stop whenever and wherever you want! The drive can be broken up into two parts: The Bow Valley Parkway and the Icefields Parkway. Keep reading for the best places to stop while driving from Banff to Jasper along the Bow Valley and Icefields Parkway!\nThings to Know Before Leaving Town –\n- There is limited gas on the drive from Banff to Jasper. I would recommend filling up before leaving Banff, because the only other places to stop at are Lake Louise and at the Saskatchewan River Crossing, which is about 135km into the drive and way more expensive.\n- There is no cell phone reception or data available on a lot of the drive after leaving Lake Louise. Download offline maps before leaving and do all of your research beforehand.\n- In the wintertime the drive from Banff to Jasper can be a little more risky. Winter tires are required on the Icefields Parkway (road between Lake Louise and Jasper). Also check for road closures before setting out in the winter.\n- Charge your camera and pack snacks before leaving Banff! The only places to buy food are at Lake Louise and the Saskatchewan River Crossing.\n- Ideally you will want a car to complete the Banff to Jasper drive, however Sundog Tours offers a shuttle from Banff to Jasper, where you can just enjoy the ride and look out the window at the amazing views.\nWhere to Stop on the Drive from Banff to Jasper –\nThese are the best stops to make while driving between Banff National Park and Jasper National Park.\nYou’re most likely starting your road trip from the incredible Town of Banff. Definitely spend time exploring downtown Banff, but you can also spend your time exploring Banff National Park and the first couple of stops on the Banff to Jasper Drive – such as day trips through the Bow Valley Parkway, exploring Lake Louise, and also Moraine Lake!\nThe most incredible place to stay in Banff is the Fairmont Banff Springs Hotel, however, if you’re on a budget I would highly recommend staying in the Town of Canmore or camping if you already have the gear.\nBow Valley Parkway\nYou can take the Trans Canada Highway from Banff to Lake Louise, but the Bow Valley Parkway is waaaay more scenic and worth the little bit of extra time! The Bow Valley Parkway is a road that runs parallel to the Trans Canada, but has a couple of scenic spots to stop at along the highway. While driving make sure to stop at Backswamp Viewpoint, Hillsdale Meadow Viewpoint, Johnston Canyon (super popular, easy hike), Castle Mountain Viewpoint and Silverton Falls, and Morant’s Curve.\nYou can easily spend a full day just exploring the Bow Valley Parkway, so plan your time accordingly and stay at Protection Mountain Campground or Baker Creek Mountain Resort if you want a little extra time to explore.\nNote: The Bow Valley Parkway is usually closed to vehicles in the Spring. You can check at the Visitors Centre before leaving Banff.\nLake Louise & the Fairmont Chateau\nLake Louise is one of the most popular stops on the route between the towns of Banff and Jasper. So why not stop and make a visit to this iconic Canadian landscape! While visiting the lake, the beautiful Fairmont Chateau Lake Louise is right there as well, so head inside and take a look at one of the most luxurious hotels in Banff. If you have extra time to spend here, hike to the Lake Agnes Teahouse or rent a canoe on this iconic lake.\nOne of the most visited lakes in Alberta, Moraine Lake is seriously breathtaking. The parking lot usually fills up by 5:30 am in the summer, so make sure to go EXTRA early, or try later in the evening. Another option is to take the Moraine Lake shuttle from the Lake Louise parking lot (which is a lot bigger and just a short distance away). If you’re planning to take the shuttle, make sure to book in advance.\nWander the trail beside the lake, or take the easy hike up Rockpile Trail to get those photos you see all over Instagram.\nNote: The Moraine Lake Road closes during the wintertime. Check for road closures before planning your visit in the fall, winter, and spring. Bike up Moraine Lake Road while it’s still closed to vehicles in May to escape the crowds.\nBanff National Park is known for its lakes, and Herbert Lake is no exception. You don’t need much time to explore Herbert Lake, but it does offer great photo opportunities of the mountains, lake, and nature, but with fewer crowds. Snap a few photos here for a great start to your road trip!\n23 minutes up the road from Herbert Lake is Bow Lake, which is another less visited/touristy lake in Banff National Park, but still delivers on the beautiful blue water and mountain views! Stop here to take in the beauty and nature away from the crowds. If you have extra time you can hike to Bow Glacier Falls from here to get up close to a waterfall!\nSuper busy. Super well-known. But still incredible! To get to Peyto Lake, you essentially park at the Peyto Lake parking lot and walk up a well-maintained trail to the viewpoint. It’s not a crazy hike and is actually a very accessible viewpoint. (Get here early or around sunset to try to escape the crowds). Hands down one of the best places to stop along the Icefields Parkway – which is probably why it’s always so busy!\nWaterfowl Lakes Viewpoint\nThe Waterfowl Lakes are beautiful! They have the stunning aqua colour Canada’s lakes are known for, so for that reason alone, you have to stop here. The viewpoint is easily seen from the highway, so you can’t miss it!\nMistaya Canyon isn’t as popular of a stop as some of the other ones along the Icefields Parkway, and I have no idea why! It’s a pretty quick hike to the main area of the canyon, and it is absolutely stunning! Make a stop here to see some roaring water carve through the canyon and awesome mountain views.\nAlthough technically not on the Icefields Parkway, it is nearby! If you’re visiting the area in the wintertime you should definitely plan a stop at Abraham Lake. Take the David Thompson Highway heading East at the Saskatchewan River Crossing to get here. The plants at the bottom of Abraham Lake release bubbles of methane, and as these bubbles rise, they freeze, making the frozen lake look super cool and bubbly!\nThere are a few different hikes along the Icefields Parkway, so if you have more than one day to explore, I would definitely plan to do at least one hike! A few hikes to check out are:\nCIRQUE PEAK TRAIL\nA pretty easy hike to start out with, but gets harder once you get to Helen Lake. This hike takes a good chunk of the day, but you’ll be rewarded with great views once you make it to the top.\nA well-marked and popular hike off the parkway, Parker Ridge is a moderate hike that you can complete in a few hours depending on your physical ability. The hike offers great, picturesque views – you will not be disappointed!\nWilcox Pass is a great trail where bighorn sheep frequent, so be prepared for wildlife! The parking lot to this hike is pretty small and fills up quite fast. If you stay at the Wilcox Creek campground overnight (first-come first-serve) you won’t have to worry about parking and crowds.\nBEAUTY CREEK TRAIL\nA relatively easy hike (and one of my favourites), Beauty Creek is a lesser known hike that takes you along a beautiful river and a few waterfalls. The endpoint is Stanley Falls and takes about 1-1.5 hours to complete.\nColumbia Icefield & The Athabasca Glacier\nAt the Columbia Icefield Center, you can have a bite to eat at the restaurant or take in the views from their patio. There are also a few picnic tables at the parking lot if you want to have your own lunch/snack and sit down to enjoy it. Across from the center, you’ll find another parking lot, and this is where the hike to the Columbia Icefields starts from. You can hike close to the glacier for free and at your own leisure, or pay for a tour on one of the big ice explorer trucks that will take you up and closer to the glacier, where you’ll have a chance to walk on it!\nTicket price: Varies. Depending on what you want to do there are a few different packages/tours to choose from.\nHeading to Jasper? Read our Jasper Bucketlist Here.KEEP READING ->\nColumbia Icefield Skywalk\nThe skywalk is a unique way to experience Alberta’s rocky mountains. It’s a glass-floored walkway hanging out from the side of a cliff that visitors can walk on, and provides great views of the icefields and mountains around the skywalk. You cannot visit without a ticket, as you have to park at the Icefield center and shuttle up to the walkway because it is a narrow highway and there’s no parking lot.\nIf you are driving the parkway though, you’ll catch a glimpse of it as you drive by, regardless of whether you visit or not!\nTicket price: Varies from $27 CAD – $32 CAD for adult admission. Alberta residents receive a 20% discount at the time of writing (with proof of residence)\nTangle Creek Falls\nA pretty, multi-tiered waterfall on the side of the Icefields Parkway, Tangle Creek Falls is a quick stop you can make on your road trip. The parking lot is across the road from the falls; it comes up fast and is easy to miss, so make sure to program the falls into google maps so you know when it’s coming up and you can prepare to enter the parking lot.\nFed by the Athabasca Glacier, Sunwapta Falls is beautiful and is definitely one of the best places to stop along the Icefields Parkway. With a tree-filled island centered just before the waterfall and mountains in the background, it would be hard for this view to be more picturesque. If you have the time, I highly recommend hiking to the lower falls as well.\nThe Athabasca Falls are fierce, there is so much water moving here it’s crazy. There are a few trails in the area to explore as well that I would highly recommend exploring if you have time.\nCliff Jump at Horseshoe Lake\nA hidden gem worth mentioning – on a hot, sunny day, cliff jumping into the icy blue waters of horseshoe lake is so refreshing, and not to mention so picturesque. Come with your bathing suit under your clothes because there is just an outhouse and nature to change in (and it’s just easier). I really only jump from the smaller cliffs and it is lots of fun! Horshoe Lake is definitely Jasper National Parks ultimate gem.\n* The area is unsupervised, so jump at your own risk. Cliff jumping can be dangerous if not done properly and people have unfortunately died here before\nWatch for Wildlife Along the Way\nThe best chance to see wildlife in the Canadian Rockies is definitely along the Icefields Parkway! Keep your eyes peeled on your drive to Jasper – I saw 3 black bears and a ton of goats and elk over my 2 days on the Parkway.\nBudget Time for Other Viewpoints\nThere are viewpoints the whole way along the Parkway. If you stop at all of them the time adds up – so make sure you budget your time while driving because it is so tempting to stop at all of them!\nOnce you make it to the Town of Jasper in Jasper National Park, there is so much more to do!! My favourite area in Jasper is exploring Maligne Lake and Maligne Canyon. Another popular place that is only a 12 minute drive away is the Pyramid Lake Resort and iconic bridge leading to Pyramid Island. The Fairmont Jasper Park Lodge is another pretty place to visit, with a pretty lake you can canoe on in the summertime or snowshoe trails in the winter. Jasper National Park is incredible to visit in the winter as well, there are a ton of outdoor activities to do (don’t miss the Maligne Canyon Icewalk). I highly recommend spending a couple of nights exploring the area around Jasper.\nWhere to Stay While on a Banff to Jasper Road Trip –\nIf you’re looking to spend more than one day driving from Banff to Jasper on the Icefields Parkway, I highly recommend staying at one of the many options along the drive. Depending on your budget, these are a couple of options:\nCamp at a first-come first-serve campsite\nCamping at one of these campsites is a great option if you’re planning to spend multiple days on the parkway (which I would recommend at least 2 days!). You cannot reserve a site in advance (at most campgrounds on the Parkway), so if you show up later in the day it might be harder to secure a spot. The campgrounds along the parkway include:\n- Mosquito Creek\n- Silverhorn Creek\n- Waterfowl Lakes\n- Rampart Creek\n- Wilcox Creek\n- Columbia Icefield\n- Jonas Creek\n- Honeymoon Lake\n- Mount Kerkeslin\nBasing yourself at a campground near Jasper and Banff is also an option as well. Camping is the cheapest option (if you already have the equipment)\nStay in a Hostel\nThere are a couple of hostels along the Icefields Parkway:\n- HI Mosquito Creek Wilderness Hostel\n- HI Rampart Creek Wilderness Hostel\n- HI Beauty Creek Wilderness Hostel\n- HI Athabasca Falls Wilderness Hostel\nStay in a Hotel\nStaying in a hotel/lodge is the most expensive option for accommodation, but it is almost the most luxurious. I would check out:\n- Simpson’s Num-Ti-Jah Lodge\n- The Crossing Resort\n- Glacier View Lodge\n- Sunwapta Falls Rocky Mountain Lodge\nGlacier View Lodge and Sunwapta Falls Rocky Mountain Lodge are both very highly rated.\nOther Questions About the Banff Jasper Road Trip –\nIS IT WORTH GOING TO JASPER FROM BANFF?\nYes!!!! The Banff to Jasper highway is one of the prettiest highways to drive in the world, I highly recommend it. You have a way higher chance for animal sightings and the viewpoints and stops along the way are amazing. Even if you don’t stop, the drive from Banff to Jasper is well worth it.\nBANFF TO JASPER OR JASPER TO BANFF?\nIt doesn’t really matter! You’ll be taking the same highway either way, so it really depends on your starting point. If you’re starting your trip in Edmonton, Alberta, you’ll most likely travel from Edmonton to Jasper to Banff to Calgary and back to Edmonton. If you’re starting in Calgary, you can go Calgary to Banff to Jasper to Edmonton and back to Calgary (or skip Edmonton entirely, and drive from Jasper to Banff for more mountain time).\nHopefully, this guide on driving from Banff to Jasper National Park helps you plan your trip to the area or inspires you to visit!\nHey! I’m Kat. Based in Alberta, Canada I love to travel to the Rockies and explore new places around the world. Follow along on my adventures and find inspiration and tips for your own travels."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:afc4c3fd-36d9-46b9-8af0-9d13e847a1eb>","<urn:uuid:51c22f9d-1f7a-4fae-848c-8bb9f5b424cf>"],"error":null}
{"question":"How has the historical development of Kyo-ware and Bizen pottery influenced their current production methods?","answer":"Both pottery traditions maintain strong connections to their historical roots while adapting to modern times. Kyo-ware, which originated in the Nara and Heian periods (710-1185), continues its tradition of handmade production despite the prevalence of mass production, using traditional techniques while incorporating European methods adopted during the Meiji period. Bizen pottery, one of Japan's six ancient medieval pottery styles, has maintained continuous production throughout its thousand-year history and evolved from its origins in Sueki pottery to now include both practical products and art pieces. Today, while Kyo-ware emphasizes various traditional casting and decoration methods, Bizen pottery is carried forward by nearly 300 artists who maintain its characteristic unglazed style and firing techniques.","context":["Kyo ware/Kiyomizu ware Kyo yaki Kiyomizu yaki\nMany kinds, small quantity handmade production\nwe produce pottery as sophisticated and delicate as the town of Kyoto\nWhat is Kyo ware/Kiyomizu ware ?\nKyo-ware/Kiyomizu-ware are ceramics and porcelain produced in the Kyoto area. Originally, Kyo-ware was a general term for all pottery produced in Kyoto, while Kiyomizu-ware referred to pottery produced on the road leading to the Kiyomizu temple. Kyo-ware is also known as Kyoto-ware. Today, Kyo-ware designs a large variety of potteries made in specific areas of Kyoto. Kiyomizu-ware is one of the many Kyo-ware categories and refers to potteries produced around the Kiyomizu temple.\nKyo-ware and Kiyomizu-ware are interesting because they are not just one type of pottery. Many distinct techniques are used to create different kinds of potteries but as long as they are produced in certain areas of Kyoto, they are considered Kyo-ware or Kiyomizu-ware. Each kiln also has its unique traditions and specialties.\nKyoto is known as a traditional city that perpetuates many long-established arts. Several types of tea ceremonies, flower arrangements or incense-smelling ceremonies during which the smell of the incense is tasted with traditional Kyoto cuisine or sweets are among the most famous traditions. Kyo-ware and Kiyomizu-ware were developed together with these cultures and that is why they fit so well in such historic environments.\nEven today, when mass production has become the norm, Kyo-ware and Kiyomizu-ware are still making every piece by hand using traditional techniques.\nKyo-ware as we know it today was first created during the Nara and Heian periods (710-1185) and its production grew a lot as the tea ceremonies became more and more popular duing the Azuchi-Momoyama period (1573-1600).\nThe first highly skilled artisans appeared at the beginning of the Edo period (1603-1868), triggering the rapid development of modern Kyo-ware.\nFamous names like Ninsei NONOMURA a potter from the Hyogo prefecture who completed magnificent painted ceramics or Kenzan OGATA, the younger brother of painter Korin OGATA, created masterpieces using original designs in collaboration with his older brother. During the latter part of the Edo period (middle of the 19th century), Eisan OKUDA became famous for his beautiful fired porcelain. Master artisans such as Mokubei AOKI, Douhachi NINNAMI or Hozen EIRAKU's masterpieces are also a must-see.\nMoving into the Meiji period (1868-1912), advances were also made outside of Japan with the adoption of European porcelain production methods.\nKyo-ware and Kiyomizu-ware keep on using traditional techniques for making various types of high-quality potteries.\nGeneral Production Process\n- 1.Kneading the clay\nThe first process is to thoroughly knead the clay by hand. Kneading the clay is a very important step as, when it is done properly, it removes the air, ensures an even hardness and increases the viscosity.\nSince potter’s clay is no longer produced in Kyoto, it is imported from different regions in Japan. It is then mixed with kaolin, kibushi clay, silica and feldspar.\nThere are different casting methods such as lathe casting, hand forming or molding and all have different results.\nLathe casting is a method of casting where the kneaded clay is placed centrally on a disc-shaped rotating potter’s wheel, using centrifugal force while soaking the clay with water. Different types of potter’s wheel include hand potter’s wheel, kick potter’s wheel and mechanical potter’s wheel. This is a high-skilled method that usually requires many years of experience.\nHand forming is a method of casting where the clay is casted while twisting, using the fingertips and a bamboo spatula. This is the easiest method and it does not require the use of a lathe.\nWhen molding, the mud that was made by mixing base clay with water and silicic acid soda is poured into a plaster mold. This enables the casting of delicate forms and the production of many same-shape works.\n- 3.Drying and planing\nThe planing is done in one time, once cast pieces have reached a semi-dry state after drying away from the sunlight for several days. A stand called a \"chuck\" is installed on the potter’s wheel, and the cast piece is hung upside-down. The foot is trimmed and the whole body is finished using a metal plane or bamboo spatula while rotating the wheel.\nDecoration is then applied using finishing tools, and pieces are dried in the sunlight.\nThe bisque firing refers to firing the clay for the first time. This first firing is carried out at low temperature to reinforce the cast items and make it easier to decorate and glaze afterwards.\nThe undercoating is done before the glost firing. All the drawing is hand-made with specific brushes and different metal colorants such as asbolite which makes an astringent blue color, and iron oxide become the base of this step.\nColored glazes, transparent glazes and frosted glazes are applied to the work. Colors, transparency and luster appear afterwards, during the firing.\nThis process is as important as the molding as the charm of the finished work depends on it.\n- 7.Glost firing\nIt consists in fusing the glaze and the clay in a separate firing. Cast pieces that have been glazed are loaded into the kiln and baked at a high temperature.\nInstead of old-fashioned climbing kilns, gas/electric kilns are the mainstream today.\nDepending on the desired texture, either oxidation firing or reduction firing can be used.\nThis is the process of decorating after the glost firing (some items are not overglazed).\nDecoration and coloring are carried out with a fine brush, using all kinds of metal colorants. Gold and silver are also often applied.\n- 9.Overglaze firing\nAfter overglazing, the pieces are baked again at a low temperature. This process produces the colors, luster and prevents damages such as peeling. This is a very meticulous step and the work and temperature have to constantly be checked in order to make sure the glaze has dissolved.\nAfter firing, the kiln is cooled and the pieces are then removed from the kilns.\nMany kinds, small quantity handmade production With assiduity, we produce pottery as sophisticated and delicate as the town of Kyoto\nBusiness Hours9.30am to 5.30pm\nWe have a large variety of items, from the traditional-style Kiyomizu-ware first created right in front of the Sennyuji temple by Ninsei NONOMURA and Kenzan OGATA to modern Kyoto pottery.\nClosedThursdays / Around the New Year / August 7 to 16\nWhere to Buy & More Information\nKyoto Museum of Traditional Crafts\nClosedAround the New Year\nBusiness Hours9am to 5pm\nSee more Ceramic\n- Imari ware/Arita ware\n- Hasami ware\n- Kutani ware\n- Mashiko ware\n- Shigaraki ware\n- Bizen ware\n- Hagi ware\n- Koishiwara ware\n- Mino ware\n- Tobe ware\n- Tokoname ware\n- Karatsu ware\n- Kasama ware\n- Satsuma ware\n- Iga ware\n- Mikawachi ware\n- Agano ware\n- Otani ware\n- Obori-soma ware\n- Tsuboya ware\n- Aizu-hongo ware\n- Shodai ware\n- Echizen ware\n- Akatsu ware\n- Tamba-tachikui ware\n- Yokkaichi-banko ware\n- Izushi ware\n- Kyo ware/Kiyomizu ware\n- Iwami ware\n- Amakusa ceramics\n- Seto-sometsuke ware\nSee items made in Kyoto\n- Nishijin brocade\n- Kyo textiles\n- Kyo folding fans\n- Kyo doll\n- Kyo uchiwa fans\n- Kyo ware/Kiyomizu ware\n- Kyo laquerware\n- Kyo braided cords\n- Kyo woodworks & joinery\n- Kyo-komon textiles\n- Kyo Buddhist altar\n- Kyo embroidery\n- Kyo art preservation\n- Kyo Buddhist altar equipment\n- Kyo dyed textiles\n- Kyo-ishi craft\n- Kyo kimono-dyeing","|That's Bizen Pottery|\n|Bizen Pottery and Its Beauty|\n|Bizen Pottery is one of the six famous ancient medieval pottery styles in Japan, including Seto, Tokoname, Tamba, Shigaraki and Echizen. It is also known as \"Imbe Pottery\" based on the name of the area. Bizen Pottery traces its long history back to Sueki Pottery (earthenware fired with no glaze) in the Tumulus Period. From the Heian Period to the early Kamakura Period, potters started to produce more practical and durable wares for everyday use. This is believed to be the beginning of Bizen Pottery.|\nThe beauty of Bizen Pottery is in its unadorned simplicity. Its unglazed austere appearance caught the attention and admiration of tea ceremony masters in Sakai and Kyoto. In the Momoyama Period, a number of masterpiece tea bowls were created.\nExperiencing hard times ever since, Bizen Pottery has begun a new stage of its development. It produces many art pieces in addition to its practical products. Not a single day has passed without smoke rising from the kilns of Bizen in its thousand year history. Four national treasures have been designated: the late Toyo Kanashige, the late Kei Fujiwara, the late Toshu Yamamoto and Yu Fujiwara. The brownish surface produced by the combination of clay, fire and man is created in a kiln that continues firing for two weeks at a temperature of about 1,300°C. Its mysterious and simple heart-warming elegance retains nature and our minds which have been lost in modern society, and appeals to many people including Bizen enthusiasts overseas. Today the unparalleled long history and tradition of Bizen Pottery, as well as its infinite beauty are firmly inherited by nearly 300 excellent artists and potters who produce numerous great pieces from their kilns in Bizen.\n|Kiln Effect Variation of Bizen Pottery|\n|When a piece gets buried in the ashes at the bottom of the kiln, it gets only indirect fire and poor air circulation, causing oxidation-reduced firing. It creates colors of gray, dark gray and blue. |\n|When pine ashes melt in the high heat, they create an ash glaze on the surface of a piece which looks as if the it were covered with sesame seeds. Many Goma pieces are placed on the shelves near the fire, and they get covered with plenty of ash. When the ashes run on the surface of a piece, the piece is called \"Tamadare\". |\n|When a small piece of clay is placed on a plate or bowl in firing, the small spot leaves an unfired red spot. This is called Botan-mochi. |\n|A white or light brown piece with red lines. This effect is made by placing rice straws between pieces and wrapping a piece with them. When firing a large piece, the piece is placed in a rice straw sack so that it does not get exposed to the fire directly. |\n|When a piece is placed in a rice straw sack in a certain part of the kiln, and smothered in strong firing, it turns blue gray. This is called Ao-Bizen. Ao-Bizen fired with salt is called Shokuen-ao. |\n|When a piece is covered with another piece on top, it creates two different colors at the top and bottom. This is called Fuseyaki. Sake bottles are often fired with this method. |"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:91ae0618-1628-41dc-a08f-cc579bf5c90c>","<urn:uuid:efec3337-a367-4305-8ce6-b9220bf48860>"],"error":null}
{"question":"I'd like to understand the performance evaluation methods in farming: what are the differences between using water-sensitive paper for spray assessment versus organic farming yield measurements, and how do these approaches compare?","answer":"Water-sensitive paper provides immediate feedback on spray coverage through a color-changing mechanism (yellow to blue), allowing farmers to assess distribution patterns and make quick adjustments to sprayer settings. While versatile and cheap, it has limitations such as difficulty showing fine droplets under 50 μm and challenges with paper placement in dense canopies. In organic farming, performance is primarily measured through crop yields, which initially decrease during the 3-5 year transition period from conventional farming but typically increase afterward. Organic yields vary by crop type - cereal and forage crops are relatively easy to grow organically with yields close to conventional levels, while horticultural crops often have lower marketable yields. However, organic products can command higher prices to offset these yield differences.","context":["It’s nearing the end of a long morning of spraying and you just want to get it done.\nAs the tank empties and you watch the last of the spray cloud waft through the row, you’re thinking about rinsing out and moving on… but did the spray land where you wanted?\nHow do you really know if you hit the target?\nMaybe you’re content with the occasional “shoulder checks” you made from the cab while spraying.\nPerhaps you stop at the end of the row and get out of the tractor to look for wet foliage during.\nMaybe you plan to return once the product is dry and look for white residue.\nThese are all good feedback practices, but a more accurate method is the use of water-sensitive paper, which turns from yellow to blue wherever spray touches it. You can easily see the distribution of the spray and the overall area covered, and it can be quantified so you can compare one sprayer set-up to another, or see the impact of weather, or even the effects of nozzle choice, pressure and water volume.\nDraw a map\nBegin by creating a simple drawing of the tree, cane, bush,vine, etc. you wish to spray. Label the drawing with unique numbers that correspond to where you are going to place the papers. Write the numbers on the back of each paper so you can see where they came from after they are collected. You should also note the pass number, so you can differentiate between each sprayer setup and corresponding pass. You might make a change and want to see how it affects coverage, and it’s very easy to mix up the papers if you haven’t record everything clearly. Plan to do this for at least two plants upwind from the sprayer to ensure you will get an accurate representation of average coverage. Be sure to wear disposable gloves and avoid dew so the papers don’t react prematurely.\nDistribute the papers\nIt is critical to distribute the papers evenly throughout each target canopy. They should be placed in key locations where pest damage has been an issue in the past (e.g. scab at the top of a tree, or spotted-wing drosophila at the bottom-centre of highbush blueberry), or anywhere coverage is notoriously difficult. Our preference is to place them at the top, centre and bottom of a tree canopy as well as laterally from the outer edge of the canopy beside the sprayer moving in towards the trunk.\nWe use spring-back paper clips attached to alligator clips at 90 degrees to attach the papers to small branches. You can also staple them to the upper or lower face of the leaves (as long as they don’t cause leaf to droop). You can wrap them around stems for panoramic coverage or to monitor drenches. They can be stapled the trunk to show if spray is aimed into the canopy or being wasted. You can even skewer to the ground using wire flags to to illustrate poor lower-nozzle positioning and/or canopy run-off. Put them wherever you want to know about spray coverage!\nWe typically orient them facing the alleys so their sensitive faces are square to the sprayer as it passes. We often use two in each location, oriented back-to-back facing each alley so you can resolve coverage from both sides. The important part is to ensure you are consistent. Mark the location in the canopy with some colourful flagging tape so you can find the papers after you spray, and if you wish to replace them with fresh papers to evaluate another pass, orient them the same way to make the comparison fair.\nSpray, check and spray again\nOnce the papers are in place, pass by on one side with both booms open (as you would normally spray). Be sure to start spraying well before passing the target, and keep spraying afterwards to ensure the resultant coverage represents an actual application. It is very informative to get out of the cab and examine the papers before passing by on the other side. You can learn a lot about how the wind is affecting the spay.\nInterpret the patterns\nYou might notice the outer portions of larger canopies receive more spray than the inside. This is hardly surprising given that spray must pass through the outside to get to the inside. As a result, inner papers often receive proportionally less spray and should be the basis for determining if you have sufficient spray coverage. This is also why the label recommendation of “spraying to the point of runoff” is unhelpful: the outer portion of wide, dense canopies often begin to drip before the inner portion receives sufficient coverage. Further, how do you spray to the point of runoff? How do you know when to stop before it’s too late? Label language can be frustrating…\nWhen assessing coverage, don’t follow the droplet counts in the small guide that comes with the paper sensitive paper kit – they haven’t been updated for a very long time and are more appropriate for field crop applications – not airblast applications. Research and experience suggest that 85 discrete fine/medium-sized droplets per square centimetre and a total coverage of 15% should be sufficient for most foliar insecticides and fungicides. Remember, this is only a suggested threshold and in the case of coarser sprays, focus more on even distribution and the 15% coverage.\nMake a change and try again\nThere’s no easy way to define a threshold between sufficient and insufficient spray coverage. When you retrieve and examine the papers, think about how the product is intended to work: “Is it a contact, trans-laminar or locally systemic pesticide? What are the odds that an insect or spore will come in contact with residue? Will I be spraying again soon (e.g. fungicide) and will the spray already on the leaves have residual activity?” Regarding that last thought, protectant fungicide applications are often layered, so what one spray misses, the next will catch. Quite often, “sufficient coverage” is less than most sprayer operators think.\nIf you are content with the coverage, record your sprayer settings to use them again in that block (in similar weather, and assuming the crop canopy doesn’t change significantly before the next spray day). If you are not content, make a change to the sprayer to improve matters, reset the papers, and go again. It can take time and some effort to get it right, but improved coverage and reduced waste are ample financial reward for your efforts.\nOther methods of evaluating coverage\nIt should be noted that while water-sensitive paper is versatile, cheap and easy to use, it has its shortcomings. Placement and orientation of the paper is very important; it’s easy to hit papers on the outside of the canopy with the sensitive-side facing the sprayer. It’s considerably harder when they are at the very centre of the canopy, or hiding behind fruit. When the thin edge of the paper is oriented to the spray (i.e. oriented facing the ground), it presents very little surface and can be difficult to hit.\nFurther, the papers won’t show the finest droplets (<50 µm), so there may be spray even though you can’t see it. Taken collectively with the product’s mode of action (i.e. contact or locally systemic), and any possible re-distribution by rain or dew, spray coverage becomes a good indicator for protection, but it isn’t definitive. While coverage is a good indicator, improved coverage does not always mean improved efficacy.\nSome sprayer operators use other methods to confirm their coverage. Kaolin clay is an inert compound that leaves white residue when dry. Red, yellow or green water-soluble, food-grade dyes will also indicate coverage. Even fluorescent dyes such as phosphorus can be sprayed at night and illuminated under black lights.\nThese methods give the sprayer operator a lot of information because they land on the actual target, not a piece of paper hung in the canopy. But, they require a lot of time and effort and are typically out of reach for most operators. Further, they do not allow multiple applications on the same canopy to compare the effect of sprayer settings on coverage – once the target is sprayed, it’s sprayed.\nNo matter which method you choose to use, understanding how changes to you sprayer, or the impact of weather, affect coverage is a critical piece of information. Operators should make an effort to evaluate spray coverage. Here are a few videos describing the process:","Introduction to Organic Farming\nOrganic farming is a method of crop and livestock production that involves much more than choosing not to use pesticides, fertilizers, genetically modified organisms, antibiotics and growth hormones.\nOrganic production is a holistic system designed to optimize the productivity and fitness of diverse communities within the agro-ecosystem, including soil organisms, plants, livestock and people. The principal goal of organic production is to develop enterprises that are sustainable and harmonious with the environment.\nThe general principles of organic production, from the Canadian Organic Standards (2006), include the following:\n- protect the environment, minimize soil degradation and erosion, decrease pollution, optimize biological productivity and promote a sound state of health\n- maintain long-term soil fertility by optimizing conditions for biological activity within the soil\n- maintain biological diversity within the system\n- recycle materials and resources to the greatest extent possible within the enterprise\n- provide attentive care that promotes the health and meets the behavioural needs of livestock\n- prepare organic products, emphasizing careful processing, and handling methods in order to maintain the organic integrity and vital qualities of the products at all stages of production\n- rely on renewable resources in locally organized agricultural systems\nOrganic farming promotes the use of crop rotations and cover crops, and encourages balanced host/predator relationships. Organic residues and nutrients produced on the farm are recycled back to the soil. Cover crops and composted manure are used to maintain soil organic matter and fertility. Preventative insect and disease control methods are practiced, including crop rotation, improved genetics and resistant varieties. Integrated pest and weed management, and soil conservation systems are valuable tools on an organic farm. Organically approved pesticides include “natural” or other pest management products included in the Permitted Substances List (PSL) of the organic standards. The Permitted Substances List identifies substances permitted for use as a pesticides in organic agriculture. All grains, forages and protein supplements fed to livestock must be organically grown.\nThe organic standards generally prohibit products of genetic engineering and animal cloning, synthetic pesticides, synthetic fertilizers, sewage sludge, synthetic drugs, synthetic food processing aids and ingredients, and ionizing radiation. Prohibited products and practices must not be used on certified organic farms for at least three years prior to harvest of the certified organic products. Livestock must be raised organically and fed 100 per cent organic feed ingredients.\nOrganic farming presents many challenges. Some crops are more challenging than others to grow organically; however, nearly every commodity can be produced organically.\nThe world market for organic food has grown for over 15 years. Growth of retail sales in North America is predicted to be 10 per cent to 20 per cent per year during the next few years. The retail organic food market in Canada is estimated at over $1.5 billion in 2008 and $22.9 billion in the U.S.A. in 2008. It is estimated that imported products make up over 70 per cent of the organic food consumed in Canada. Canada also exports many organic products, particularly soybeans and grains.\nThe Canadian Organic Farmers reported 669 certified organic farms in Ontario in 2007 with over 100,000 certified organic acres of crops and pasture land. This is an annual increase of approximately 10 per cent per year in recent years. About 48 per cent of the organic cropland is seeded to grains, 40 per cent produces hay and pasture and about five per cent for certified organic fruits and vegetables. Livestock production (meat, dairy and eggs) has also been steadily increasing in recent years.\nThe main reasons farmers state for wanting to farm organically are their concerns for the environment and about working with agricultural chemicals in conventional farming systems. There is also an issue with the amount of energy used in agriculture, since many farm chemicals require energy intensive manufacturing processes that rely heavily on fossil fuels. Organic farmers find their method of farming to be profitable and personally rewarding.\nConsumers purchase organic foods for many different reasons. Many want to buy food products that are free of chemical pesticides or grown without conventional fertilizers. Some simply like to try new and different products. Product taste, concerns for the environment and the desire to avoid foods from genetically engineered organisms are among the many other reasons some consumers prefer to buy organic food products. In 2007 it was estimated that over 60 per cent of consumers bought some organic products. Approximately five per cent of consumers are considered to be core organic consumers who buy up to 50 per cent of all organic food.\n“Certified organic” is a term given to products produced according to organic standards as certified by one of the certifying bodies. There are several certification bodies operating in Ontario. A grower wishing to be certified organic must apply to a certification body requesting an independent inspection of their farm to verify that the farm meets the organic standards. Farmers, processors and traders are each required to maintain the organic integrity of the product and to maintain a document trail for audit purposes. Products from certified organic farms are labelled and promoted as “certified organic.”\nIn June 2009, the Canadian government introduced regulations to regulate organic products. Under these regulations the Canadian Food Inspection Agency (CFIA) oversees organic certification, including accreditation of Conformity Verification Bodies (CVBs) and Certification Bodies (CBs). This regulation also references the Canadian Organic Production Systems General Principles and Management Standards (CAN/CGSB-32.310) and the Organic Production Systems – Permitted Substances List that were revised in 2009.\nThe Canadian organic regulations require certification to these standards for agricultural products represented as organic in import, export and inter-provincial trade, or that bear the federal organic agricultural product legend or logo. (Figure 1) Products that are both produced and sold within a province are regulated by provincial organic regulations where they exist (Quebec, British Columbia and Manitoba).\nFigure 1. Canadian Agriculture Product Legend (logo)\nThe federal regulations apply to most food and drink intended for human consumption and food intended to feed livestock, including agricultural crops used for those purposes. They also apply to the cultivation of plants. The regulations do not apply to organic claims for other products such as aquaculture products, cosmetics, fibres, health care products, fertilizers, pet food, lawn care, etc.\nFood products labelled as organic must contain at least 95 per cent organic ingredients (not including water and salt) and can bear the Canada Organic logo. Multi-ingredient products with 70 per cent to 95 per cent organic product content may be labelled with the declaration: “% organic ingredients”. Multi-ingredient products with less than 70 per cent organic content may identify the organic components in the ingredient list.\nExported products must meet the requirements of the importing country or standards negotiated through international equivalency agreements. Products exported to the U.S. must meet the terms of the Canada-U.S. equivalency agreement signed in June 2009. All products that meet the requirements of the Canada Organic Regime can be exported to the U.S. with the exception that agricultural products derived from animals treated with antibiotics cannot not be marketed as organic in the U.S. Canada is also exploring other international equivalency agreements with other trading partners to enhance trade opportunities for export and to assure the organic integrity of imported products.\nWhen considering organic certification, know the requirements and accreditation(s) needed in the marketplace where your products will be sold. When comparing certification bodies, make sure they have the certification requirements and accreditations needed to meet market requirements. As a minimum certification bodies should be accredited under the Canadian Organic Products Regulations. Some markets may require accreditation or equivalency agreements with countries in the European Union, or with the Japanese Agricultural Standard (JAS), Bio-Swisse or other international organic certification systems. As Canada develops international equivalency agreements the need for the certification body to have these international accreditations will diminish.\nFor more information on certification and links to Canadian regulations and standards see the Organic Agricultural section of the OMAFRA website at www.ontario.ca/organic or the CFIA website at www.inspection.gc.ca.\nThe first few years of organic production are the hardest. Organic standards require that organic lands must be managed using organic practices for 36 months prior to harvest of the first certified organic crop. This is called the “transition period” when both the soil and the manager adjust to the new system. Insect and weed populations also adjust during this time.\nCash flow can be a problem due to the unstable nature of the yields and the fact that price premiums are frequently not available during the transition since products do not qualify as “certified organic.” For this reason, some farmers choose to convert to organic production in stages. Crops with a low cost of production are commonly grown during the transition period to help manage this risk.\nCarefully prepare a plan for conversion. Try 10 per cent to 20 per cent the first year. Pick one of the best fields to start with and expand organic acreage as knowledge and confidence are gained. It may take five to 10 years to become totally organic, but a long term approach is often more successful than a rapid conversion, especially when financial constraints are considered. Parallel production (producing both organic and conventional versions of the same crop or livestock product) is not allowed. Use good sanitation, visually different varieties, individual animal identification and other systems to maintain separation and integrity of the organic and conventional products. Good records are essential.\nIn organic production, farmers choose not to use some of the convenient chemical tools available to other farmers. Design and management of the production system are critical to the success of the farm. Select enterprises that complement each other and choose crop rotation and tillage practices to avoid or reduce crop problems.\nYields of each organic crop vary, depending on the success of the manager. During the transition from conventional to organic, production yields are lower than conventional levels, but after a three to five year transition period the organic yields typically increase.\nCereal and forage crops can be grown organically relatively easily to due to relatively low pest pressures and nutrient requirements. Soybeans also perform well but weeds can be a challenge. Corn is being grown more frequently on organic farms but careful management of weed control and fertility is needed. Meeting nitrogen requirements is particularly challenging. Corn can be successfully grown after forage legumes or if manure has been applied. Markets for organic feed grains have been strong in recent years.\nThe adoption of genetically engineered (GMO) corn and canola varieties on conventional farms has created the issue of buffer zones or isolation distance for organic corn and canola crops. Farmers producing corn and canola organically are required to manage the risks of GMO contamination in order to produce a “GMO-free” product. The main strategy to manage this risk is through appropriate buffer distances between organic and genetically engineered crops. Cross-pollinated crops such as corn and canola require much greater isolation distance than self-pollinated crops such as soybeans or cereals.\nFruit and vegetable crops present greater challenges depending on the crop. Some managers have been very successful, while other farms with the same crop have had significant problems. Certain insect or disease pests are more serious in some regions than in others. Some pest problems are difficult to manage with organic methods. This is less of an issue as more organically approved biopesticides become available. Marketable yields of organic horticultural crops are usually below non-organic crop yields. The yield reduction varies by crop and farm. Some organic producers have added value to their products with on-farm processing. An example is to make jams, jellies, juice, etc. using products that do not meet fresh market standards.\nLivestock products can also be produced organically. In recent years, organic dairy products have become popular. There is an expanding market for organic meat products. Animals must be fed only organic feeds (except under exceptional circumstances). Feed must not contain mammalian, avian or fish by-products. All genetically engineered organisms and substances are prohibited. Antibiotics, growth hormones and insecticides are generally prohibited. If an animal becomes ill and antibiotics are necessary for recovery, they should be administered. The animal must then be segregated from the organic livestock herd and cannot be sold for organic meat products. Vaccinations are permitted when diseases cannot be controlled by other means. Artificial insemination is permitted. Always check with your certification body to determine if a product or technique is allowed in the Permitted Substances List and the organic standards. Organic production must also respect all other federal, provincial and municipal regulations.\nOrganic produce can usually qualify for higher prices than non-organic products. These premiums vary with the crop and may depend on whether you are dealing with a processor, wholesaler, retailer or directly with the consumer. Prices and premiums are negotiated between buyer and seller and will fluctuate with local and global supply and demand.\nHigher prices offset the higher production costs (per unit of production) of management, labour, and for lower farm yields. These differences vary with commodity. Some experienced field crop producers, particularly of cereals and forages, report very little change in yield while in some horticultural crops such as tree fruits, significant differences in marketable yield have been observed. There may also be higher marketing costs to develop markets where there is less infrastructure than for conventional commodities. Currently, demand is greater than supply for most organic products.\nOrganic farming can be a viable alternative production method for farmers, but there are many challenges. One key to success is being open to alternative organic approaches to solving production problems. Determine the cause of the problem, and assess strategies to avoid or reduce the long term problem rather than a short term fix for it.\nCOG – Canadian Organic Growers Inc.\n323 Chapel St., Ottawa ON K1N 7Z2\nPhone: (613) 216-0741, 1-888-375-7383\nEFAO – Ecological Farmers Association of Ontario\n5420 Highway 6 North,\nRR 5, Guelph, ON N1H 6S2\nPhone: (519) 822-8606\nOMAFRA – Ontario Ministry of Agriculture, Food and Rural Affairs\n1 Stone Road W., Guelph, ON N1G 4Y2\nAgr. Information Contact Centre\nOACC- Organic Agricultural Centre of Canada\nNova Scotia Agricultural College\nBox 550, Truro, Nova Scotia, B2N 5E3\nPhone: (902) 893-7256, Fax: (902) 893-3430\nGuelph Organic Conference\nFor information contact:\nTomás Nimmo, Box 116,\nCollingwood, ON L9Y 3Z4\nPhone: (705) 444-0923, Fax (705) 444-0380\nOCO - Organic Council of Ontario\nRR 5 Guelph, ON N1H 6J2\nPhone: (519) 827-1221, Fax: (519) 827-0721\n© Queen's Printer for Ontario, 2015\nThe information on this page was written and copyrighted by the Government of Ontario. The information is offered here for educational purposes only and materials on this page are owned by the Government of Ontario and protected by Crown copyright. Unless otherwise noted materials may be reproduced for non-commercial purposes. The materials must be reproduced accurately and the reproduction must not be represented as an official version. As a general rule, information materials may be used for non-profit and personal use."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:e6dd6f9e-4ae7-4afa-a110-aeed99d782a6>","<urn:uuid:e6ddb9b3-68e1-422b-adc0-49ee5dd47471>"],"error":null}
{"question":"Compare NASA Astrobiology funding start vs modern deep ocean trench exploration beginnings?","answer":"NASA established its Astrobiology Program in 1996, though its predecessor exobiology studies dated back to the beginning of the U.S. space program. In contrast, deep ocean trench exploration began earlier, with Felix Andries Vening Meinesz conducting pioneering gravity measurements from submarines in the 1920s and 1930s, leading to the discovery that trenches are sites of downwelling in the solid Earth.","context":["\"For thousands of years, humans have looked up at the stars and wondered whether life exists beyond our home planet.\"\nDecember 4, 2013\nStatement of Dr. Mary A. Voytek Senior Scientist for Astrobiology\nNational Aeronautics and Space Administration\nbefore the Committee on Science, Space and Technology U.S. House of Representatives\nMr. Chairman and Members of the Committee, thank you for the opportunity to appear today to discuss the topic of Astrobiology. For thousands of years, humans have looked up at the stars and wondered whether life exists beyond our home planet. This curiosity was renewed with the latest discovery by NASA's Kepler mission of 833 new candidate planets outside our solar system. Ten of these candidates are less than twice the size of Earth and orbit in their star's habitable zone. With Kepler's help, more than 3,500 potential worlds have now been identified orbiting stars other than our Sun, reminding us just how important NASA's work is to understand the universe and the potential for life beyond our solar system.\nEven today, children wonder, where did I come from? Astrobiology seeks to answer this enduring question.\nWhat is astrobiology?\nAstrobiology is the study of the origin, evolution, distribution, and future of life in the universe. It addresses three basic questions that have been asked in various ways for generations:\nHow does life begin and evolve? Does life exist elsewhere in the universe? What is the future of life on Earth and beyond?\nIn striving to answer these questions, experts in astronomy and astrophysics, Earth and planetary sciences, biology and chemistry, and other relevant disciplines participate in astrobiology research to achieve a comprehensive understanding of biological, planetary, and cosmic phenomena and the relationships among them.\nThis multidisciplinary field encompasses the search for habitable environments in our\nSolar System as well as habitable planets outside our Solar System. Astrobiology embraces the search for evidence of prebiotic chemistry and life on Mars and other bodies, laboratory and field research into the origins and early evolution of life on Earth, as well as studies of the potential for life to adapt to future challenges, both here on Earth and beyond. Astrobiology is a community science\nAstrobiology is a cross-cutting theme in all of NASA's space science endeavors, knitting together research in astrophysics, Earth science, and heliophysics as well as planetary science. As such, astrobiology is guided by a community-constructed roadmap generated every five years, most recently in 2008. The ongoing development of astrobiology roadmaps embodies the contributions of diverse scientists and technologists from government, universities, and private institutions. These roadmaps outline multiple pathways for research and exploration and indicate how they might be prioritized and coordinated.\nNASA's Astrobiology Program also solicits advice from the Space Studies Board of the National Research Council (NRC). The NRC conducts studies that provide science community consensus on key questions posed by NASA and other agencies. This coordinated, collective approach to research planning has contributed to the NRC's decadal surveys for Planetary Science and for Astronomy and Astrophysics, both of which incorporate astrobiology as a key component of their programs. Within these surveys, questions encompassed by astrobiology serve as overarching themes for future planetary and astrophysics missions as a whole.\nHistory and status of the science\nNASA established its current Astrobiology Program in 1996. However, NASA studies in the field of exobiology - a predecessor to astrobiology - date back to the beginning of the U.S. space program.\nLong before NASA was established, astronomers were already documenting increasingly complex organic molecules distributed throughout the universe. Similar compounds found in some meteorites and interplanetary dust particles suggest these chemicals could have been delivered to the early Earth by comets and asteroids. NASA's Viking missions to Mars in the 1970s included three biology experiments designed to look for possible signs of life.\nIn the 21st century, astrobiology is a focus of a growing number of NASA missions. As mentioned earlier, with NASA's Kepler mission, we have been able to detect Earth-size planets within the habitable zones around distant stars. These potentially habitable planets will expand our search for life beyond our Solar System. Mars also continues to be an area of interest, with the Mars Science Laboratory mission currently assessing the potential habitability of that planet. Recently, astrobiologists studying the Mars meteorite ALH84001 determined how and when the rock interacted with water on ancient Mars. Reconstructing the history of water on Mars is important for understanding the evolution\nof the atmosphere, and the potential for ancient habitats capable of supporting life. These results provided evidence that the surface of Mars was wet and clay-rich prior to 4.2 billion years ago.\nHowever, since Earth is the only known example of an inhabited planet, the search for life in the cosmos begins with our understanding of life on Earth. Studying the origins and evolution of life on Earth improves our ability to recognize and characterize life in its many \"illusory\" - that is, imaginary unique -- forms.\nFor example, in 1977, oceanographers discovered an oasis of life around a hydrothermal vent system at the bottom of the ocean. They found communities of organisms that thrived despite high pressure, temperatures upwards of 130 degrees Celsius (water boils at 100 degrees) and the absence of sunlight. The deepest hydrothermal vents found since then were discovered along the approximately 110 kilometer long, ultra-slow spreading Mid-Cayman Rise in the Caribbean Sea. Characterization of the life at these vents has filled in a critical piece of our understanding of organisms that are fueled by chemicals rather than the Sun. Further study of these sites, which are analogs for early Earth, can help determine whether deep-sea hydrothermal vents provided an environment for the origin of life early in our planet's history.\nIn 2012, astrobiologists found that microbes from Earth can survive and grow in the low pressure, freezing temperatures and oxygen-starved conditions seen on Mars. Their research found that microbes from permafrost soil collected in northeastern Siberia could grow at 7 millibars of pressure. In comparison, the atmospheric pressure at the summit of Mount Everest is approximately 300 millibars, more than 40 times the global average surface pressure of Mars. In a companion study, these same scientists investigated 26 strains of bacteria commonly found on spacecraft. Incubating them under Mars-like conditions, they found that one particular bacterium, Serratia liquefaciens, could survive and even reproduce under these extreme conditions.\nOverall, astrobiologists have discovered life in numerous extreme environments on Earth such as volcanic lakes, glaciers, and sulfur springs. We have also found life in extraordinary forms ranging from bacteria that consume chemicals toxic to most life to microbes that live under high levels of gamma or ultraviolet radiation. These discoveries have taught us that life is tough, tenacious, and metabolically diverse and highly adaptable to local environmental conditions. Knowledge gained through astrobiology research reveals new possibilities of what else might be out there and how we might be able to find and recognize it.\nBenefits to Society\nAstrobiology is about more than just scientific discovery. Astrobiology research and technology development has an impact on our daily lives and benefits society as a whole. We are all familiar with the Deepwater Horizon spill of 2010 - the largest offshore spill in U.S. history. In April of that year, the United States was faced with the challenge of\ndetermining the extent of the spill, both in regard to how much oil was leaking and where the oil was moving. Astrobiology had a role in analyzing the spill. Using detectors and autonomous operation technology funded by NASA's Astrobiology Program, along with a National Science Foundation robotic submersible vehicle, scientists were able to map the underwater plume. Technology initially developed to search autonomously for environments capable of supporting life allowed the submersible to navigate along a guided path to search for the plume.\nAnother astrobiology technology that has proved useful for broader application is the Chemistry and Mineralogy (CheMin) instrument on NASA's Mars Science Laboratory Curiosity Rover. CheMin is a highly sensitive instrument that can identify and quantify the minerals present in Martian rocks and soil, which may provide valuable clues of where to look for biosignatures. Commercial spin-offs of CheMin technology have proved useful for a variety of purposes, including hazardous material identification, mud logging at oil drilling sites, artifact preservation in museums and even the detection of counterfeit pharmaceuticals in developing countries.\nLife is a central theme that unifies NASA's science program. A golden age has begun for the life sciences, an age in which science and technology will benefit enormously from a fundamental understanding of the full potential of living systems. The science of astrobiology aims to achieve a better understanding of our own world and the life it hosts and also of potential habitable worlds and life beyond Earth. This is an agenda for inspiring the next generation of explorers and stewards to sustain the NASA mission of exploration and discovery.\nAgain, thank you for the opportunity to testify today, and I look forward to responding to any questions you may have.\nPlease follow Astrobiology on Twitter.","Trenches define one of the most important natural boundaries on the Earth’s solid surface, that between two lithospheric plates. There are three types of lithospheric plate boundaries: divergent (where lithosphere and oceanic crust is created at mid-ocean ridges), convergent (where one lithospheric plate sinks beneath another and returns to the mantle), and transform (where two lithospheric plates slide past each other). Trenches are the spectacular and distinctive morphological features of plate boundaries. Plates move together along convergent plate boundaries at convergence rates that vary from a few millimeters to ten or more centimeters per year. A trench marks the position at which the flexed, subducting slab begins to descend beneath another lithospheric slab. Trenches are generally parallel to a volcanic island arc, and about 200 km from a volcanic arc. Oceanic trenches typically extend 3 to 4 km (1.9 to 2.5 mi) below the level of the surrounding oceanic floor. The deepest ocean depth to be sounded is in the Challenger Deep of the Mariana Trench at a depth of 10,911 m (35,798 ft) below sea level. Oceanic lithosphere disappears into trenches at a global rate of about a tenth of a square meter per second.\nThere are about 50,000 km of convergent plate margins, mostly around the Pacific Ocean – the reason for the reference “Pacific-type” margin - but they are also in the eastern Indian Ocean, with relatively short convergent margin segments in the Atlantic Ocean and in the Mediterranean Sea. Trenches are sometimes buried and lack bathymetric expression, but the fundamental structures that these represent mean that the great name should also be applied here. This applies to Cascadia, Makran, southern Lesser Antilles, and Calabrian trenches. Trenches along with volcanic arcs and zones of earthquakes that dip under the volcanic arc as deeply as 700 km are diagnostic of convergent plate boundaries and their deeper manifestations, subduction zones. Trenches are related to but distinguished from continental collision zones (like that between India and Asia to form the Himalaya), where continental crust enters the subduction zone. When buoyant continental crust enters a trench, subduction eventually stops and the convergent plate margin becomes a collision zone. Features analogous to trenches are associated with collisions zones; these are sediment-filled foredeeps referred to as peripheral foreland basins, such as that which the Ganges River and Tigris-Euphrates rivers flow along.\nDuring the 1920’s and 1930’s, Felix Andries Vening Meinesz developed a unique gravimeter that could measure gravity in the stable environment of a submarine and used it to measure gravity over trenches. His measurements revealed that trenches are sites of downwelling in the solid Earth. The concept of downwelling at trenches was characterized by Griggs in 1939 as the tectogene hypothesis, for which he developed an analogue model using a pair of rotating drums. World War II in the Pacific led to great improvements of bathymetry in especially the western and northern Pacific, and the linear nature of these deeps became clear. The rapid growth of deep sea research efforts, especially the widespread use of echosounders in the 1950’s and 1960’s confirmed the morphological utility of the term. The important trenches were identified, sampled, and their greatest depths sonically plumbed. The heroic phase of trench exploration culminated in the 1960 descent of the Bathyscaphe \"Trieste\", which set an unbeatable world record by diving to the bottom of the Challenger Deep. Following Robert S. Dietz’ and Harry Hess’ articulation of the seafloor spreading hypothesis in the early 1960’s and the plate tectonic revolution in the late 1960’s the term ‘trench’ has been redefined with plate tectonic as well as bathymetric connotations!\nTrenches are centerpieces of the distinctive physiography of a convergent plate margin. Transects across trenches yield asymmetric profiles, with relatively gentle (~5°) outer (seaward) slope and a steeper (~10-16°) inner (landward) slope. This asymmetry is due to the fact that the outer slope is defined by the top of the downgoing plate, which must bend as it starts its descent. The great thickness of the lithosphere requires that this bending be gentle. As the subducting plate approaches the trench, it is first bent upwards to form the outer trench swell, then descends to form the outer trench slope. The outer trench slope is disrupted by a set of subparallel normal faults which staircase the seafloor down to the trench. The plate boundary is defined by the trench axis itself. Beneath the inner trench wall, the two plates slide past each other along the subduction decollement, the seafloor intersection of which defines the trench location. The overriding plate contains volcanic arc (generally) and a forearc. The volcanic arc is caused by physical and chemical interactions between the subducted plate at depth and asthenospheric mantle associated with the overriding plate. The forearc lies between the trench and the volcanic arc. Forearcs have the lowest heatflow from the interior Earth because there is no asthenosphere (convecting mantle) between the forearc lithosphere and the cold subducting plate.\nThe inner trench wall marks the edge of the overriding plate and the outermost forearc. The forearc consists of igneous and metamorphic crust, and this crust acts as buttress to a growing accretionary prism (sediments scraped off the downgoing plate onto the inner trench wall, depending on how much sediment is supplied to the trench). If the flux of sediments is high, material will be transferred from the subducting plate to the overriding plate. In this case an accretionary prism grows and the location of the trench migrates progressively away from the volcanic arc over the life of the convergent margin. Convergent margins with growing accretionary prisms are called accretionary convergent margins and make up nearly half of all convergent margins. If the sediment flux is low, material will be transferred from the overriding plate to the subducting plate by a process of tectonic ablation known as subduction erosion and carried down the subduction zone. Forearcs undergoing subduction erosion typically expose igneous rocks. In this case, the location of the trench will migrate towards the magmatic arc over the life of the convergent margin. Convergent margins experiencing subduction erosion are called nonaccretionary convergent margins and comprise more than half of convergent plate boundaries. This is an oversimplification, because different parts of a convergent margin can experience sediment accretion and subduction erosion over its life.\nThe asymmetric profile across a trench reflects fundamental differences in materials and tectonic evolution. The outer trench wall and outer swell comprise seafloor that takes a few million years to move from where subduction-related deformation begins near the outer trench swell until sinking beneath the trench. In contrast, the inner trench wall is deformed by plate interactions for the entire life of the convergent margin. The forearc is continuously subjected to subduction-related earthquakes. This protracted deformation and shaking ensures that the inner trench slope is controlled by the angle of repose of whatever material it is composed of. Because they are composed of igneous rocks instead of deformed sediments, non-accretionary trenches have steeper inner walls than accretionary trenches.\nThere an evolution in trench morphology can be expected as oceans close and continents converge. While the ocean is wide, the trench may be far away from continental sources of sediment and so may be deep. As the continents approach each other, the trench may become filled with continental sediments and become shallower. A simple way to approximate when the transition from subduction to collision has occurred is when the plate boundary previously marked by a trench is filled enough to rise above sealevel.\nThe slope of the inner trench slope of an accretionary convergent margin reflects continuous adjustments to the thickness and width of the accretionary prism. The prism maintains a ‘critical taper’, established in conformance with Mohr-Coulomb Theory for the pertinent materials. A package of sediments scraped off the downgoing lithospheric plate will deform until it and the accretionary prism that it has been added to attain a critical taper (constant slope) geometry. Once critical taper is attained, the wedge slides stably along its basal decollement. Strain rate and hydrologic properties strongly influence the strength of the accretionary prism and thus the angle of critical taper. Fluid pore pressures modify rock strength and are important controls of critical taper angle. Low permeability and rapid convergence may result in pore pressures that exceed lithostatic pressure and a relatively weak accretionary prism with a shallowly tapered geometry, whereas high permeability and slow convergence result in lower pore pressure, stronger prisms, and steeper geometry.\nThe Hellenic trench system is unusual because this convergent margin subducts evaporites. The slope of the surface of the southern flank of the Mediterranean Ridge (its accretionary prism) is low, about 1°, which indicates very low shear stress on the decollement at the base of the wedge. Evaporites influence the critical taper of the accretionary complex, as their mechanical properties differ from those of siliciclastic sediments, and because of their effect upon fluid flow and fluid pressure, which control effective stress. In the 1970s, the linear deeps of the Hellenic trench south of Crete were interpreted to be similar to trenches at other subduction zones, but with the realization that the Mediterranean Ridge is an accretionary complex, it became apparent that the Hellenic trench is actually a starved forearc basin, and that the plate boundary lies south of the Mediterranean Ridge.\nChemosynthetic communities thrive where cold fluids seep out of the forearc. Cold seep communities have been discovered in inner trench slopes down to depths of 6000 m in the western Pacific, especially around Japan, in the Eastern Pacific along North, Central and South America coasts from the Aleutian to the Peru-Chile trenches, on the Barbados prism, in the Mediterranean, and in the Indian Ocean along the Makran and Sunda convergent margins. These communities receive much less attention than the chemosynthetic communities associated with hydrothermal vents. Chemosynthetic communities are located in a variety of geological settings: above over-pressured sediments in accretionary prisms where fluids are expelled through mud volcanoes or ridges (Barbados, Nankai and Cascadia); along active erosive margins with faults; and along escarpments caused by debris slides (Japan trench, Peruvian margin). Surface seeps may be linked to massive hydrate deposits and destabilization (e.g. Cascadia margin). High concentrations of methane and sulfide in the fluids escaping from the seafloor are the principal energy sources for chemosynthesis.\nIgneous basement of a nonaccretionary forearc may be continuously exposed by subduction erosion. This transfers material from the forearc to the subducting plate and can be accomplished by frontal erosion or basal erosion. Frontal erosion is most active in the wake of seamounts being subducted beneath the forearc. Subduction of large edifices (seamount tunneling) oversteepens the forearc, causing mass failures that carry debris towards and ultimately into the trench. This debris may be deposited in graben of the downgoing plate and subducted with it. In contrast, structures resulting from subduction erosion of the base of the forearc are difficult to recognize from seismic reflection profiles, so the possibility of basal erosion is difficult to confirm. Subduction erosion may also diminish a once-robust accretionary prism if the flux of sediments to the trench diminishes.\nNonaccretionary forearcs may also be the site of serpentine mud volcanoes. These form where fluids released from the downgoing plate percolate upwards and interact with cold mantle lithosphere of the forearc. Mantle peridotite is hydrated into serpentinite, which is much less dense than peridotite and so will rise diapirically when there is an opportunity to do so. Some nonaccretionary forearcs are subjected to strong extensional stresses, for example the Marianas, and this allows buoyant serpentinite to rise to the seafloor where they form serpentinite mud volcanoes. Chemosynthetic communities are also found on non-accretionary margins such as the Marianas, where they thrive on vents associated with serpentinite mud volcanoes.\nThere are several factors that control the depth of trenches. The most important control is the supply of sediment, which fills the trench so that there is no bathymetric expression. It is therefore not surprising that the deepest trenches (deeper than 8,000 m) are all nonaccretionary. In contrast, all trenches with growing accretionary prisms are shallower than 8000 m. A second order control on trench depth is the age of the lithosphere at the time of subduction. Because oceanic lithosphere cools and thickens as it ages, it subsides. The older the seafloor, the deeper it lies and this determines a minimum depth from which seafloor begins its descent. This obvious correlation can be removed by looking at the relative depth, the difference between regional seafloor depth and maximum trench depth. Relative depth may be controlled by the age of the lithosphere at the trench, the convergence rate, and the dip of the subducted slab at intermediate depths. Finally, narrow slabs can sink and roll back more rapidly than broad plates, because it is easier for underlying asthenosphere to flow around the edges of the sinking plate. Such slabs may have steep dips at relatively shallow depths and so may be associated with unusually deep trenches, such as the Challenger Deep.\n|Mariana Trench||Pacific Ocean||10,911m (32,733)|\n|Tonga Trench||Pacific Ocean||10,882m (32,646)|\n|Kuril Trench||Pacific Ocean||10,542m (31,626)|\n|Philippine Trench||Pacific Ocean||10,540m|\n|Kermadec Trench||Pacific Ocean||10,047m|\n|Izu-Bonin Trench (Izu-Ogasawara Trench)||Pacific Ocean||9,780 m|\n|Japan Trench||Pacific Ocean||9,000m|\n|Puerto Rico Trench||Atlantic Ocean||8,605 m|\n|Peru-Chile Trench or Atacama Trench||Pacific Ocean||8,065 m|\n|Aleutian Trench||West of Alaska|\n|Bougainville Trench||South of New Guinea|\n|Cayman Trench||Western Caribbean Sea|\n|Cedros Trench (inactive)||Pacific coast of Baja California|\n|Hikurangi Trench||East of New Zealand|\n|Japan Trench||Northeast Japan|\n|Kuril-Kamchatka Trench||Near Kuril islands|\n|Mariana Trench (deepest known part of the oceans)||Western Pacific ocean; east of Mariana Islands|\n|Middle America Trench|\n|New Hebrides Trench||West of New Caledonia|\n|Puerto Rico Trench (deepest known part of the Atlantic Ocean)||Boundary of Caribbean Sea and Atlantic ocean|\n|Peru-Chile Trench||Eastern Pacific ocean; off coast of Peru & Chile|\n|Philippine Trench||East of Philippine Islands|\n|Ryukyu Trench||Eastern edge of Japan's Ryukyu Islands|\n|South Sandwich Trench|\n|Sunda Arc and Java Trench|\n|Tonga Trench||North-east of Australia|\n|Yap Trench||Western Pacific ocean; between Palau Islands and Mariana Trench|\n|Intermontane Trench||Western North America; between Intermontane Islands and North America|\n|Insular Trench||Western North America; between Insular Islands and Intermontane Islands|\n|Farallon Trench||Western North America|\n|Tethyan Trench||South of Turkey, Iran, Tibet and Southeast Asia|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:bf09b15b-bffe-40ca-93f1-53a1ce4618ef>","<urn:uuid:d936b3af-fff6-427a-bb02-458add29833e>"],"error":null}
{"question":"How did the approach to minimal editing in 1930s French cinema compare with Chaplin's editing style in the factory scenes of Modern Times?","answer":"In the 1930s, French director Jean Renoir favored shots of long duration with minimal editing, as seen in films like Grand Illusion and Boudu Saved from Drowning. This contrasts with Chaplin's approach in Modern Times' factory scenes, which required careful rehearsal but still maintained relatively few cuts while focusing on continuous action. For example, in the assembly line scene, Chaplin performed carefully choreographed movements at high speed with limited editing, allowing the natural flow of the scene to create both comedy and social commentary about industrialization.","context":["Movies and Film: Fade In: A Brief History of Editing\nFade In: A Brief History of Editing\nAs with most other film techniques, editing has evolved over time as the technology and audience expectations change. The following is a brief history of this technique.\nLike almost every basic idea about movies, the idea of editing has its precursors. Flashbacks had existed in novels; scene changes were already part of live theater; even narrated sequences had been a part of visual culture from medieval altar triptychs to late nineteenth-century comic strips.\nBut the very earliest filmmakers were afraid to edit film shots together because they assumed that splicing together different shots of different things from different positions would simply confuse audiences.\nShot: The basic temporal unit of film photography and editing. A shot consists of the celluloid used from the moment a camera begins rolling on a scene to the moment it stops.\nSequence: A number of shots edited together and unified, either through the plot, the character(s), the time and/or space, or the theme.\nHowever, filmmakers quickly discovered that editing shots into a sequence not only contributed to the audience's sense of tale, but also enabled them to tell more complex stories as a result. You can see primitive instances of editing in films like Rescued by Rover (Great Britain, 1904) and The Great Train Robbery (1903).\nEarly on the cuts were made in the camera, so that the cameraman would simply stop cranking at the exact end of a shot, and begin cranking again when it was moved somewhere else, or when something else was put in front of it. This kind of editing could allow for some early special effects. In movies he is making at the turn of the century, Georges Mlis stops the camera after detonating a magic puff of smoke in front of his actor, then begins the camera again after the actor has left the stage, making it seem as if the actor has magically vanished.\nGriffith and Beyond\nDid you know that the first real film school in the world—the Moscow Film School—was founded as a propaganda device? Lenin knew early on that the cinema was going to be an important ideological tool for communicating ways of seeing the world. Lenin's way—Marxism—was so controversial in the early part of the century that the United States and Western Europe blockaded Russia after that country's communist revolution.\nWe will read in greater detail about D. W. Griffith's contribution to editing. Here we can just note that, though he did not invent any of the editing techniques he used, he made them emotionally and thematically significant. So much so that he influenced the art of editing worldwide. The Moscow Film School of the 1920s, for example, played his Intolerance (1916) over and over again in order to use Griffith's techniques for the films of its students. One of the most notable of the Soviet directors of this era was Sergei Eisenstein, who transformed the principles of classical editing into something more consciously intellectualized he called montage.\nThough the idea of putting together shots to forward theme as well as action—one way of seeing montage—had occurred to other filmmakers before Griffith and the early Soviets, Griffith made it a regular practice and the Russian filmmakers theorized its meaning.\nThe first rigorous use of the term is by Soviet filmmakers like V.I. Pudovkin and Sergei Eisenstein, who saw montage principally as a useful propaganda film tool. Montage was a way to put together a number of shots, more or less quickly, in a manner that pointed out a moral or an idea. In Charlie Chaplin's Modern Times (1936), a shot of a faceless, crowded group of men emerging from a subway on their way to work is followed by a shot of a herd of sheep being led to slaughter. There is one black ram in the middle of the herd. We immediately cut back to Charlie emerging in the midst of the crowd: the one black sheep in the fold.\nIn Editing, Sometimes Less Is More\nSome filmmakers chose to minimize editing, seeing it as the \"death of 1,000 cuts\" for realism. For example, though some documentarists saw editing as a way to make their anthropological visions appear more interesting, others saw minimal intrusion as the more authentic way to go. Other documentary styles emerged in which editorial intervention was minimal, if never entirely absent.\nMontage is a confusing term because, like love, it means different things to different people. In Hollywood it most often simply means a number of shots edited quickly together in order to form a brief impression of a character, place, or time. The Madonna musical number \"Back in Business\" in Dick Tracy (1990) underscores a visual montage of several quick shots of gangsters engaged in various illegal rackets: gambling, robbery, and so on. This montage simply conveys the idea that a lot of illegal activities are going on in a compressed time.\nBut even in feature filmmaking some directors chose to avoid the manipulation of reality that montage and heavy editing seemed to imply. In the silent era, some American comics such as Buster Keaton and Charlie Chaplin often relied on long takes in order to demonstrate that no special effects had been used and the acrobatics of the comedian were not camera tricks but dangerously real events.\nIn the 1930s, Jean Renoir's films were filled with shots of long duration. The best examples are probably Grand Illusion (La Grande Illusion, France, 1937) and Boudu Saved from Drowning (Boudu Sauv Des Eaux, France, 1932). The subsequent movements most associated with less emphasis of montage are Italian neorealism and the French Nouvelle Vague (New Wave) and cinma verit.\nThe laws of gravity and insurance prevent most contemporary he-man stars from performing a tenth of the feats the very small Keaton performed, which is one reason that action sequences tend to be so heavily edited. Harrison Ford and Arnold Schwarzenegger could not—even if they had the skill—have a house fall on them, leap around on top of a moving train, or actually tumble head-over-heels down a hill. The feats that the he-men seem to do in their films are, most of the time, special effects. While also a master of editing effects, Keaton was very careful to make sure the camera continued cranking and focusing on him when he took real chances.\nEven in an era of incredibly advanced special effects, some filmmakers are still enamored of the photographic realism in sustained shots. Perhaps the most conspicuous is Jim Jarmusch, who will hold his camera on his subjects for an agonizingly hilarious amount of time.\nBut the past 20 or so years has also seen the rise of \"digital editing\" (also called nonlinear editing), which makes any kind of editing easier. The notion of editing film on video originated when films were transferred to video for television viewing. Then filmmakers used video to edit their work more quickly and less expensively than they could on film. The task of cleanly splicing together video clips was then taken over by computers using advanced graphics programs that could then also perform various special effects functions. Finally, computers convert digital images back into film or video. These digital cuts are a very far cry from Mli's editing in the camera.\nThe unkindest cut of all: editing and censorship. Films can and have traditionally been censored even after release simply by cutting out anything deemed unsavory. In the first three decades of film, even individual American cities routinely cut out parts of films with overt sexual content or controversial subject matter.\nExcerpted from The Complete Idiot's Guide to Movies and Film © 2001 by Mark Winokur and Bruce Holsinger. All rights reserved including the right of reproduction in whole or in part in any form. Used by arrangement with Alpha Books, a member of Penguin Group (USA) Inc.\nTo order the e-book book direct from the publisher, visit the Penguin USA website. You can also purchase this book at Amazon.com.\nHere are the facts and trivia that people are buzzing about.","Modern Times, 1936, Charles Chaplin\nIn many ways, Modern Times was both an ending and a beginning. For Chaplin, it was the end of his silent movie star career and his popular character, the tramp. It was also the last major silent film release. It was at the height of the depression, and the underlying themes represented Chaplin’s critical feelings of industry and the exploitation of the working man. Little could he know that everything would change in a few years with a war, which would devastate the world and end the depression. Yet, the changing times did not date the picture. With historical perspective, Modern Times can be seen as a nostalgic and sentimental transitional film.\nIf there are any questions about Chaplin’s thoughts about industrialization, then they are answered within the first 15 minutes. Chaplin effective turns a social issue (one that we felt strongly above) into comedy, and as expected, he was completely successful. The early factory scene delivers the laughs. Chaplin becomes both obsessed and complacent with the act of riveting. Occasionally he’ll sneeze or otherwise be forced to miss an item and the assembly line will go out of whack, forcing the two other employees working behind him to get frustrated with his antics. The Tramp keeps on turning knobs with his two wrenches, sometimes even when his hands are not over the assembly line. He gets distracted by a woman’s attire with dark, loud buttons, and tries to turn them too. This scene works flawlessly. There are not many cuts, so the action must have been carefully rehearsed and difficult to carry out, and the speed at which the scene flows thanks to the customary 16 frames per second in silent films make it seem all the more hurried and manic.\nThe scene with the most laughs, at least for me, is the lunch efficiency machine. Again, Chaplin is poking fun of industrialization, specifically the fact that they value production so highly that they will compromise the worker’s free time and convenience by automating their lunch process. The machine dumps soup on Chaplin, forces him to eat corn on the cob at a rapid fire pace, and smacks him on the face when it is intended to merely wipe his chin. The device itself is funny simply due to its absurdity, but it is Chaplin’s performance that cements the scene as being so memorable in his cannon. He recalls the lunch machine later in the film by becoming a lunch machine himself, yet he is just as effective (or ineffective) as the automated version. Again, this is funny, but he is still making a cultural statement. People do not need external assistance, whether from a machine or a human, to perform basic duties.\nChaplin’s politics seem clear in some instances and hazy in others. He is clearly portraying modernity from a leftist perspective, and that echoes some of the activism he was undertaking outside of the film industry. However, he was careful not to go too far to the left. There is another scene where he takes a flag from a truck. A communist mob marches behind him, and he is swept up with them. As the flag waver in the front, he is mistaken as the front-runner in yet another hilarious gag. Despite the scene’s humor, he is distancing himself from the Communist movement. He was leftist, but not that left wing.\nUnlike other Chaplin pictures, in Modern Times he has a co-star – a trampette if you will – in the form of Paulette Goddard, his off-screen lover. Even though Chaplin is always the funniest, she provides a welcome equilibrium to his antics, along with a motivation for him to pursue his character arc. Before meeting her, he was perfectly content being in jail because, after all, they served food and gave him a roof over his head, which wasn’t always the case on the outside (and this was another comment about modern times.) After meeting Goddard as The Gamin, he wants to succumb to the lures of society. He wants a good job so that he can afford a nice house, even if his dream house still rejects modernity by extracting milk directly from a passing cow rather than buy the processed product.\nAs Chaplin and Goddard pursue normal lives, and even find themselves living in a crude, fragile shack, they cling more to a life of poverty. In Chaplin’s vision, having less allows one to live in opposition to the modern trappings of society. They find themselves in plenty of other comic scenes, including a department store and most famously, a restaurant where Chaplin sings for the first time, but this is not the life for them. As everything they aim for falls apart, they are content simply walking away, hand in hand, comfortable in each other’s company.\nFilm Rating: 9.5\n- Modern Times definitively identifies Chaplin’s transition from silent films into sound. He had intentions of making a talkie and even wrote a script, but trashed the idea after filming a couple scenes.\n- After City Lights, he went on an 18-month world tour where he was treated as a celebrity. He saw economic collapse and nationalism. He published a number of social articles when he returned, including those about the tyranny of the assembly line.\n- The lunch machine scene took 7 days. It isn’t known for sure because Chaplin never revealed his methods, but it is thought that there was an operator somewhere, although stills show Chaplin operating the lever.\n- He was accused of ripping off Rene Clair’s À Nous la Liberté. Some argued that the similarities were obvious with any industrial story. Clair was not pleased with the lawsuit because he respected Chaplin. He didn’t think Chaplin was guilty, and if so, was flattered. The suit was out of Clair’s hands and went on. It didn’t resolve until 1947, where Chaplin paid a modest settlement.\n- Goddard had been a struggling actress and a divorcee when she met Chaplin, when their affair and collaboration began. He convinced her to go back to her natural brunette color instead of the platinum blonde.\n- One of the few critical complaints is that Modern Times is a series of 2-reelers, which is true to an extent (factory, furniture store, factory again, restaurant).\n- Like City Lights, he was credited as composer. People have criticized him for taking too much credit away from his arrangers. He could play instruments, but could not read music. The arrangers all confirmed that he directed the compositions through them.\n- The FBI, trying to establish a link with him and the Communist Party, investigated Chaplin. It was more that he found left leaning individuals to be better dinner companions. The FBI never found anything despite their pursuits. He was never tied to the party, so their efforts were futile.\n- Chaplin’s song became famous. In 1939 it was released as a song about who had the better mustache, Chaplin or Hitler. It was most famous as being the first time his voice is heard on screen. He sings a gibberish of his own invention.\nModern Times: A Closer Look: Visual essay from Chaplin historian Jeffrey Vance.\nChaplin was highly secretive about how he worked. He did not allow people to film him during the process. “If people know how it’s done, the magic is gone.” Still photos survive as the background of the making of Modern Times.\nHe spoke with great minds (Churchill, Einstein, others), and wanted to make some sort of social cinema. He nixed the idea of a Napoleon film when he befriended Paulette Goddard. This would begin an 8-year collaboration with Chaplin and Goddard, which included a common law marriage. They treated each other as equals, and he cast her in that manner in the film.\nThe film was steeped in the political and social realities of the time. He met Henry Ford in 1923 and found that people who were hired from farms to factories often had nervous breakdowns.\nGoddard later called it her favorite film. “Charlie could be difficult at times, but charming” and he gave her valued education and experience. Their collaboration would end due to a falling out after The Great Dictator.\nA Bucket of Water and a Glass Matte: Craig Baron and Ben Burtt talk about visual and sound effects.\nChaplin isn’t thought of in terms of visual effects, but he used them effectively. He was a visual director because of his roots in silent film. He used techniques like miniatures, rear projection, glass shots, matte paintings, and many more. He built large sets, like he did with the factory. He used a lot of hanging miniatures, even during the factory sequence. They are smaller, yet they give the impression of appearing full-size, and they make the set look larger.\nSounds were used as needed for dramatic or comic effect, but no more. He preferred to use them only when necessary, such as the feeding machine and flatulence jokes.\nThey show the roller skating shot in detail. Chaplin used a glass matte painting shot. Camera shoots through a sheet of glass with a painting. The empty “cliff” is the painting. Chaplin was an exception skater, but was never in any danger.\nSilent Traces: Modern Times: Visual essay with John Bengtson as he tours the locations that Chaplin used.\nChaplin began in Los Angeles, and many of the locations still exist today. He filmed factory scenes near gas storage tanks. The north of which was demolished in 1973. The landmark also appears in The Kid, Buster Keaton’s The Goat. The southern plant was smaller and used in the worker lineup scene.\nToday the Chaplin studio in Hollywood is home to Jim Henson company, where Kermit pays tribute to Chaplin by dressing as the tramp.\nDavid Raksin and the Score: 1992 interview with the composer for the film..\nAlfred Newman did the conducting and was brilliant. Raksin was credited as music arranger. Charlie was autocratic, not used to people disagreeing with him. Initially he did not get along with Raksin because his taste and authority were challenged. Raksin was at one point fired due to these disagreements. Later Newman was looking at his Raskin’s sketches and thought they were marvelous, and he talked Charlie out of firing him. Charlie and David had to talk privately and work things out before he could come back.\nCharlie did not know how to develop music, but he was excellent at working it out with someone who knew about music. He had an understanding of instruments that most non-musicians wouldn’t have. Raskin would generally like what Chaplin did, and prior disagreement were his just acting out of instinct.\nTwo Bits: These are two deleted scenes.\nCrossing the Street – Funny scene with the tramp not understanding the stop and go signs, and the cop chiding him along. Even though it is funny, it does not fit too well with the theme of the film. I understand why it was cut.\nThe Tramp’s Song, unedited. – The last verse was removed when Chaplin re-edited the film. This 4-minute full sequence restores it. The last verse doesn’t add much and I expect he cut it for brevity.\nAll At Sea: This is a short filmed by Alistair Cooke of a yacht trip with Chaplin and Goddard with an added film score.\nWe see their mugs playing to the camera. Mostly it is Charlie doing the comic antics, but we also see Alistair showing a sense of humor that would surprise most fans of Masterpiece Theater.\nIt is strange seeing Chaplin out of his element, dressed well with perfectly combed hair. He looks just like a wealthy man on a yacht and nothing like the tramp. That doesn’t mean he isn’t funny. He does a series of routines with a broom, impersonating people that were in the headlines such as Gaynor, Garbo, and Harlow.\nThis documentary really is a treasure and I’m glad they added it to the disc.\nSusan Cooke Kittredge Interview: When her father died in 2004, she was responsible for sorting through his old belongings. She found a treasure trove. Behind everything was a reel of film labeled “Chaplin film.” He had told his children that he had made a film with Chaplin, but they thought he was making it up. Cooke thought had he lost it, and it was unfortunate that it was found after his passing.\nCooke wanted to be a film critic early. He approached Chaplin and told him he was with the London Observer and asked to schedule an interview. Chaplin says yes, and Cooke pitched it to the Observer to get them to hire him. They hit it off well, and the interview turned to lunch and then dinner, and then they became inseparable.\nThey spent the weekend cruising around Catalina Island. Cooke happened to have a 8mm camera so they just thought they would shoot a film. It was just something to do.\nTheir friendship did not continue because their careers went in different directions. They saw each other occasionally and would reminisce, but the intensity of the friendship passed.\nThe Rink: – This has plenty of slapstick comedy and subtle gags. Some jumped out at me, like when he is working at a restaurant and tells his boss,“I’m going to lunch,” and promptly leaves the restaurant. The antics in the skating rink make it a fitting companion to Modern Times. This short shows off Chaplin’s skating ability, which was quite impressive.\nFor the First Time: 1967 Cuban documentary about showing motion pictures to rural communities that haven’t ever seen a movie. They showed Modern Times.\nThis was my second favorite supplement on the disc, with the Cooke film as the first.\nThe crew traveled to rural areas near Guantanamo and Baracoa. Many peasants had never seen a movie.\nThere is ecstatic laughter at the lunch scene! When the corncob goes in his mouth, people seem about to lose themselves with joy. Some kids yawn and then fall asleep. Some people are so blown away by what they are seeing that you can see tears in their eyes. My only complaint is that they don’t have interviews afterward to hear their thoughts.\nChaplin Today: “Modern Times.”: Philippe Truffaut documentary in 2003 with the Dardenne brothers.\nThe famous filmmakers dissect the film. They identify that he uses hunger in most of his films, and bread brings him together with the girl and is a prop in prison. Even the furniture store “burglars” are only looking for some food.\nThe assembly lines of Ford’s auto plants were mechanized labor, and during the depression that was no hiring because there was no demand for product. Chaplin was inspired by the assembly lines in Detroit to make the movie. Dardennes: “Man becomes a cog in the machine.” Chaplin sabotages the system, which is the ultimate rebellion. Dardennes talk about how when he does his ballet, he distracts the men from the machine, but they are still chained to it and resume work when he reminds them.\nCriterion Rating: 10/10\nPosted on June 14, 2015, in Criterions, Film and tagged Alistair Cook, charles chaplin, charlie chaplin, criterion, dardennes, film, industrialization, paulette goddard, silent films, talkies, the criterion collection, the great depression. Bookmark the permalink. 7 Comments."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:7d7253ad-f9f3-49c6-aa4e-863caf1a0c68>","<urn:uuid:1c218b09-723d-4daf-929f-10154f8f9a94>"],"error":null}
{"question":"What are the characteristics of Salmonella outbreaks in food establishments, and what safety measures are required for handling hazardous chemicals in these settings?","answer":"Salmonella outbreaks in food establishments can be large and severe - for example, a 2015 outbreak affected 192 people across five states, with 17% requiring hospitalization. The outbreak was linked to cross-contamination issues including inadequate employee handwashing and insufficient cleaning of food contact surfaces. Regarding hazardous chemical handling in food establishments, OSHA requires comprehensive hazard communication programs that include proper container labeling with hazard warnings, Material Safety Data Sheets (MSDS) for every hazardous chemical, and mandatory employee training on safe chemical use and emergency procedures. The MSDS must contain detailed information about chemical characteristics, hazards, safe handling protocols, and emergency response measures.","context":["Notes from the Field: Outbreak of Multidrug-Resistant Salmonella Infections Linked to Pork — Washington, 2015\nWeekly / April 15, 2016 / 65(14);379–381\nViews: Views equals page views plus PDF downloadsMetric Details\nVance M. Kawakami, DVM1,2; Lyndsay Bottichio, MPH3; Kristina Angelo, DO2,3; Natalie Linton, MPH4; Bonnie Kissler, MPH5; Colin Basler, DVM3; Jennifer Lloyd, MS1; Wendy Inouye, MPH1; Elysia Gonzales, MPH1; Krista Rietberg, MPH1; Beth Melius, MPH4; Hanna Oltean, MPH4; Matthew Wise, PhD3; Jennifer Sinatra, DVM5; Paula Marsland, MS6; Zhen Li, PhD6; Roxanne Meek6; Meagan Kay, DVM1; Jeff Duchin, MD1,7; Scott Lindquist, MD4 (View author affiliations)View suggested citation\nDuring June–July 2015, Public Health–Seattle & King County (PHSKC) and Washington State Department of Health (WADOH) investigated 22 clusters of Salmonella serotype I 4,, 12:i:- infections. Serotype I 4,, 12:i:- is the fifth most frequently reported Salmonella serotype in the United States, but is uncommon in Washington.* On July 29, 2015, WADOH and PHSKC requested assistance from CDC to identify the infection source, determine risk factors, and make recommendations for prevention.\nA confirmed case was initially defined as a gastrointestinal illness with onset during April 25–September 25, 2015, with documentation of a Salmonella serotype I 4,, 12:i:- isolate from one of five closely related pulsed-field gel electrophoresis (PFGE) XbaI patterns (JPXX01.1314, JPXX01.2311, JPXX01.2429, JPXX01.3161, or JPXX01.3336) in a Washington resident, or with an isolate matching one of the outbreak PFGE patterns with highly related whole genome sequencing, in a non-Washington resident. Later in the investigation, an additional PFGE XbaI pattern (JFXX01.0046) was added to the case definition.\nA total of 192 confirmed cases were reported from five states; 184 (96%) occurred in Washington (Figure). Patients ranged in age from <1 to 90 years (median = 35 years), and 97 (51%) were female. Among 180 patients for whom information about hospitalization was available, 30 (17%) were hospitalized; no deaths were reported.\nOn the basis of cases investigated before August 2015, a supplemental questionnaire that went into more detail in addressing meat and livestock exposures was developed. Among 80 patients (42% of all confirmed cases) who were interviewed, 59 (74%) reported eating pork during the 7 days preceding illness. This was significantly higher than the most recently published (2007) Foodborne Diseases Active Surveillance Network (FoodNet) population survey of healthy persons, in which 43% reported eating pork in the week before they were interviewed (p <0.001) (1).\nWADOH and PHSKC investigation into the source of pork traced the pork consumed by 35 (59%) of the 59 interviewed patients who reported eating pork back to a U.S. Department of Agriculture’s Food Safety and Inspection Service–inspected pork slaughter establishment in Graham, Washington. During the outbreak period, the establishment distributed whole hogs and pork parts, primarily from five farms in Montana and one in Washington, to Washington, Oregon, and Alaska. Among the 21 interviewed patients who did not report consuming pork before becoming ill, 13 had eaten at one of two restaurants or had shopped at one market where pork from the establishment was served. During June and July 2015, PHSKC inspections of these three facilities identified potential opportunities for cross-contamination of raw pork with other meat and produce, including inadequate employee handwashing and insufficient cleaning and sanitization of food contact surfaces and utensils used for raw meat. Food and environmental sampling by PHSKC at all three facilities yielded the outbreak strains.\nEight of 11 pooled environmental samples collected on July 31, 2015, from the slaughter establishment by WADOH yielded one of the outbreak strains. A parallel Food Safety and Inspection Service investigation of the establishment, conducted during August 10–14, cited insanitary conditions, supported by isolation of outbreak strains from samples taken before the start of daily operations, consistent with WADOH results. Additionally, the Food Safety and Inspection Service isolated Salmonella Infantis (XbaI pattern JFXX01.0046) from the establishment, which was subsequently added to the case definition. Four patients (2% of all confirmed cases) were identified using the updated case definition. On August 13, 2015, the establishment recalled an estimated 116,262 pounds of whole hogs produced during April 18–July 27, and on August 27, expanded the recall to include approximately 523,380 pounds of pork products produced during April 18–August 26 because of potential contamination with Salmonella I 4,, 12:i:- (2). On August 27, the slaughter establishment voluntarily ceased operations.\nTen clinical isolates of the outbreak strains from Washington were submitted to CDC’s National Antimicrobial Resistance Monitoring System for resistance testing. All 10 exhibited resistance to ampicillin, streptomycin, sulfisoxazole, and tetracycline (ASSuT resistance). In 2009, the National Antimicrobial Resistance Monitoring System reported <1.5% of Salmonella I 4,, 12:i:- human isolates had the ASSuT resistance pattern; in 2013, this number had increased to 45.5% (3). Regarding future Salmonella I 4,, 12:i:- outbreaks, increasing ASSuT resistance is concerning because infections with antimicrobial-resistant Salmonella strains are associated with an increased risk for hospitalization, bloodstream infection, and treatment failure (4,5). Further study of the epidemiology and etiology of ASSuT resistance and Salmonella I 4,, 12:i:- is recommended.\nThis was the largest Salmonella outbreak in Washington in recent history, and highlights that pork is an important source for human Salmonella infections (6). Best practices in all parts of the pork production industry, from farm to processing plant, can help reduce the risk for future outbreaks (7). In addition, prevention strategies that include rigorous Salmonella control in pork slaughter establishments in conjunction with food handling education at the wholesaler and restaurant level should be strengthened.\nOffice of Communicable Disease Epidemiology, Washington State Department of Health; Public Health–Seattle & King County, Washington; Public Health Laboratories, Washington State Department of Health; U.S. Department of Agriculture–Food Safety and Inspection Service; Washington State Department of Agriculture; Division of Foodborne, Waterborne, and Environmental Diseases, National Center for Emerging and Zoonotic Infectious Diseases, CDC; U.S. Department of Agriculture-Animal and Plant Health Inspection Service; Montana Department of Public Health & Human Services; Montana Department of Livestock; Alaska Department of Health and Social Services; Oregon Department of Health; California Department of Public Health.\nCorresponding author: Vance Kawakami, email@example.com, 206-423-8160.\n1Public Health–Seattle & King County, Seattle, Washington; 2Epidemic Intelligence Service, Division of Scientific Education and Professional Development, CDC; 3Division of Foodborne, Waterborne, and Environmental Diseases, National Center for Emerging and Zoonotic Infectious Diseases, CDC; 4Office of Communicable Disease Epidemiology, Washington State Department of Health, Shoreline, Washington; 5U.S. Department of Agriculture-Food Safety and Inspection Service, Washington, D.C.; 6Public Health Laboratories, Washington State Department of Health, Shoreline, Washington; 7University of Washington Medical Center, Seattle, Washington.\n- CDC. Foodborne Diseases Active Surveillance Network (FoodNet) population survey atlas of exposure, 2006–2007. Atlanta, GA: US Department of Health and Human Services, CDC; 2008. http://www.cdc.gov/foodnet/surveys/foodnetexposureatlas0607_508.pdf\n- US Department of Agriculture Food Safety and Inspection Service. Class I recall—news release: Kapowsin meats recalls pork product due to possible Salmonella contamination. Washington, DC: US Department of Agriculture, Food Safety and Inspection Service; 2015. http://goo.gl/osPJOl\n- CDC. National Antimicrobial Resistance Monitoring System for Enteric Bacteria (NARMS): human isolates final report, 2013. Atlanta, GA: US Department of Health and Human Services, CDC; 2015.\n- Varma JK, Molbak K, Barrett TJ, et al. Antimicrobial-resistant nontyphoidal Salmonella is associated with excess bloodstream infections and hospitalizations. J Infect Dis 2005;191:554–61. CrossRef PubMed\n- Crump JA, Medalla FM, Joyce KW, et al. ; Emerging Infections Program NARMS Working Group. Antimicrobial resistance among invasive nontyphoidal Salmonella enterica isolates in the United States: National Antimicrobial Resistance Monitoring System, 1996 to 2007. Antimicrob Agents Chemother 2011;55:1148–54. CrossRef PubMed\n- CDC. Foodborne outbreak online database. Atlanta, GA: US Department of Health and Human Services, CDC; 2015. http://wwwn.cdc.gov/foodborneoutbreaks\n- Dickson JS, Hurd HS, Rostagno MH. Salmonella in the pork production chain. Report no. 03558-3/13. Des Moines, IA: National Pork Board (US); 2013. http://www.pork.org/wp-content/uploads/2010/05/salmonellaproductnchn.pdf\n* National Enteric Disease Surveillance: Salmonella Annual Report, 2012. http://www.cdc.gov/ncezid/dfwed/pdfs/salmonella-annual-report-2012-508c.pdf.\nFIGURE. Date of illness onset* among 192 persons† infected with the outbreak strains of Salmonella I 4,, 12:i:- or S. Infantis, by state residency status — Washington, 2015\nAbbreviation: WA = Washington.\n*When unknown, illness onset dates were estimated by the following formula: (isolation date of outbreak strains of Salmonella I 4,, 12:i:- or S. Infantis) – 3 days.\n†N = 192 for whom information was reported as of November 24, 2015.\nSuggested citation for this article: Kawakami VM, Bottichio L, Angelo K, et al. Notes from the Field. Outbreak of Multidrug-Resistant Salmonella Infections Linked to Pork — Washington, 2015. MMWR Morb Mortal Wkly Rep 2016;65:379–381. DOI: http://dx.doi.org/10.15585/mmwr.mm6514a4.\nMMWR and Morbidity and Mortality Weekly Report are service marks of the U.S. Department of Health and Human Services.\nUse of trade names and commercial sources is for identification only and does not imply endorsement by the U.S. Department of Health and Human Services.\nReferences to non-CDC sites on the Internet are provided as a service to MMWR readers and do not constitute or imply endorsement of these organizations or their programs by CDC or the U.S. Department of Health and Human Services. CDC is not responsible for the content of pages found at these sites. URL addresses listed in MMWR were current as of the date of publication.\nAll HTML versions of MMWR articles are generated from final proofs through an automated process. This conversion might result in character translation or format errors in the HTML version. Users are referred to the electronic PDF version (https://www.cdc.gov/mmwr) and/or the original MMWR paper copy for printable versions of official text, figures, and tables.\nQuestions or messages regarding errors in formatting should be addressed to firstname.lastname@example.org.\n- Page last reviewed: August 25, 2017\n- Page last updated: August 25, 2017\n- Content source:","How to Develop a Hazard Communication Program in Your RestaurantHow to Develop a Hazard Communication Program in Your Restaurant\nHazard communication programs are essential for any operation in which hazardous chemicals are used. When developing a hazard communication program for your food service operation, be sure you follow all standards set by OSHA, specifically Standard 1910.1200. This standard ensures that information regarding the hazards of any chemicals produced or imported is communicated to all workers by way of a comprehensive hazard communication program. This shall include container labeling and other warnings, Material Safety Data Sheets (MSDS) and employee training.1 Essentially, there must be proper labels, written warnings, MSDS and employee training in place.\nWhat is in a Written Hazard Communication Program?\nProper Labels. Although the employer is not required to label portable containers into which hazardous chemicals have been transferred, it is helpful to appropriately label every container to provide additional information about the chemicals contained within. Signs, operating procedures or other informational material can be posted on the walls to help identify chemicals used on the premises. Appropriate hazard warnings, words, pictures, symbols to provide specific information regarding the physical and health hazards of that chemical.\nHazardous Chemicals. According to the Occupational Safety and Health Administration (OSHA), hazardous chemicals are any that could potentially cause a physical or health hazard, specifically pertaining to chemicals with carcinogens, or toxins, such as chlorine, ammonia, formaldehyde or ethanol. OSHA provides a full list of chemical types that are considered hazardous2, including the following:\n- Toxic agents\n- Reproductive toxins\n- Agents damaging to lungs, skin, eyes or mucous membranes\nMaterial Safety Data Sheets\nMSDS Requirements. You must have a Material Safety Date Sheet (MSDS) for every hazardous chemical on the restaurant premises. When you order chemicals for the first time, the chemical manufacturer will often provide the appropriate Material Safety Data Sheet from the chemical manufacturer. Every MSDS must include the following information, in any order:\n- The chemical and common names of the substance\n- Physical and chemical characteristics of the hazardous ingredients (such as odor, appearance, vapor pressure)\n- Physical hazards (such as combustibility)\n- Health hazards (such as symptoms of exposure and any medical conditions exacerbated by exposure\n- Primary means of entry (such as inhalation)\n- The OSHA permissible exposure limit (such as the ACGIH Threshold Limit Value (TLV®)*)\n- Precautions for safe handling and use (regarding storage, use and disposal)\n- First aid and emergency procedures\n- The manufacturer name\n- Date the MSDS was prepared and contact information for the chemical manufacturer\nMSDS are not needed for the following products5:\n- Tobacco products\n- Wood or wood products (not including saw dust)\n- Typically used consumer products (such as pens or pencils)\n- Articles (such as plastic chairs)\n- Personal food, drugs or cosmetics\n- Retail food, drugs, cosmetics or alcohol\n- Drugs in solid form (such as pills)\n* The American Conference of Governmental Industrial Hygienists (ACGIH) determines that exposure to a chemical at or below its TLV® will not create unreasonable risk of injury or illness.\nSince OSHA standards mandate employee training within a hazard communication program, you need to implement training procedures in your restaurant. The best way to do this is to introduce all new employees to the chemicals you have on hand, procedures for how to use them safely, and how to use the material safety data sheets in case of a crisis. Make sure the following training points are included in all employee training:\nTeach employees to use MSDS. First off, teach your employees where to find the MSDS. In a critical situation, they need to know where the MSDS is located, how to look up the chemical in question, and how to read the chemical information.\nPoint out labels and posters. All original containers should be labeled appropriately, identifying the chemical inside. Be sure all employees know how to read and understand the chemical information on the label, as well as any procedures or warnings on the posters.\nDemonstrate proper use. Perhaps the most important part of training is the opportunity to demonstrate the correct usage of all chemicals. This is particularly important for brand new employees, but can also be a good refresher for seasoned employees.\n- Managing Operational Risks\n- Top Ten Safety Tips for the Restaurant Employee\n- Are Your Workers Safe? Why Personal Protective Equipment is a Necessity for Restaurants\n- Hazardous Chemicals and Restaurant Safety\n- Food Preparation Hazards in the Commercial Kitchen\n- Strain and Sprain Hazards in the Commercial Kitchen\nMore from How to Develop a Hazard Communication Program in Your Restaurant ...\n- Food Safety Temperatures and The Danger Zone\n- 6 Food Quality Control Tips for Restaurants\n- 8 Tips for Safe Food Storage in Your Restaurant\n- Top 10 Food Safety Tips for the Commercial Kitchen\n- Types of Restaurant Food Safety Certification\n- Proper Fruit and Produce Washing\n- Safe Ice Handling\n- Filtered Water Makes The Best Ice\n- When to Accept or Reject Fresh Meat, Poultry and Seafood\n- How Commercial Kitchen Operators Can Obtain a Food Handler's Permit\nBack to How to Develop a Hazard Communication Program in Your Restaurant"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:b76828e5-2a06-49fb-bb80-c89ef84bc402>","<urn:uuid:d4e75f13-2705-46ec-9e7a-87263cb58f03>"],"error":null}
{"question":"I'm interested in the preservation of historical artifacts - can you list both the Civil War Museum's exhibits and the USS Sterett's preserved items from WWII?","answer":"The Civil War Museum contains over 8,000 square feet of authentic period exhibits covering the Western Theater of the American Civil War, from the Appalachian Mountains to the Mississippi and from Georgia to the Gulf of Mexico. As for the USS Sterett, the only preserved item specifically mentioned is a tattered 5 foot by 3 foot 48-star American flag that was flown by USS Sterett (DD 407) during the 1942 Guadalcanal campaign.","context":["Get ready for history times FOUR with Museum Row, where your admission allows you entry into four fascinating and inspiring museums in the center of beautiful Bourbon country!\nFirst, let's start with the Civil War Museum and Women's Museum of the Civil War! These are the largest and most complete museums dedicated to the Western Theater of the American Civil War in the entire USA!\nInside, you'll find over 8,000 square feet of authentic period exhibits, that tell the story of the successes and the struggles between the Union and the Confederacy, all the way from the stunning Appalachian Mountains to the Mississippi, from Georgia to the Gulf of Mexico.\nNext, the inspiring Women's Museum highlights the incredible achievements of women of all races, backgrounds and creeds during the Civil War and through the entire 1800s. Kids can be inspired as they learn all about their achievements in medicine, writing, science, bursing, civil rights, journalism, the arts, suffrage, military service and so much more!\nIt really is a complelling empowerment experience for all young girls and women alike - and it's believed to be the only museum of its kind in the entire country!\nAdjacent to these museums is the Old Bardstown Village, which is where kids can walk through history as they explore the collection of ten original 18th and 19th century log structures that form a Colonial period settlement. History really does come to life! It's always fun for kids to see how differernt life would have been all those years ago (and we're sure they won't spot an iphone in sight!)\nIf all of that wasn't enough, Museum Row also features The General Hal Moore Military Museum. Here you'll find exhibits that tell the story of conflicts from the American Revolution, to the Mid-East battles of today. The exhibits are also centered on the contributions of many Kentukians who served in these battles. Recently, this museum expanded the World War I exhibit, so it really is an eye opening experience for all.\nAre you ready to explore not one, but FOUR museums in Bardstowns' Museum Row? Get those walking shoes on and learning hats at the ready, because history is waiting for you!\nBardstown is located in Central Kentucky between I-65 and I-75, just off the Bluegrass Parkway. From Lexington take Route 60 to the Bluegrass Parkway. Continue on the Bluegrass until you reach exit 25. Take a right at the exit and follow the road into town. From Louisville take I-65 South to exit 112, and turn left at the exit. Follow this road about 16 miles until you reach Bardstown. The Civil War Museum is located at 310 E. Broadway, Bardstown, KY 40004.Get directions\n$12 - for all 4 museums\n$6 (Ages 6 to 15) - for all 4 museums\nTickets are good for all four venues (Civil War Museum, Historic Bardstown Village, Museum of MidAmerica and Women of the Civil War Museum) and are valid for two consecutive days in order to ensure time to enjoy all museums!\nThere is of course a gift shop where you can pick up some fun merchandise and souvenirs, as well as books and educational items, and a guided tour DVD too if you want to re-live your experience!\nOld Bardstown Village is open to school groups/ educational institutes of 20 or more, seven days a week, 12 months a year with prior reservations. Group per-student price is $5.00 for all four attractions, with parents and chaperones paying the same price!","Preservation, Education, and Commemoration of Naval History\nNHF Historian Wins LEGO Shipbuilding Contest with 5 Foot Long Fletcher Class Destroyer\nThis past Saturday, the Hampton Roads Naval Museum (HRNM) held their second annual “Brick by Brick: LEGO Shipbuilding” contest. The event brought together those who love naval history, and those who love the iconic children’s construction blocks. The day-long event encouraged builders of all ages to bring in LEGO ships they had created at home, to enter into a contest. Over 100 creations were entered, and nearly 1,500 people visited the Museum for the event. We’re happy to announce that one of our own was judged the grand prize winner for the day. Naval Historical Foundation Digital Historian Dave Colamaria built a five foot long replica of a Fletcher class destroyer, and was selected by representatives of HRNM and the Hampton Roads LEGO User Group as the overall winner.\nSaving Historic Ships: NHF Historian Pens Article in Current Issue of Proceedings\nThe February Naval Institute Proceedings features an article by Naval Historical Foundation Historian Dr. David F. Winkler who looks at Historic Ships as an underutilized asset for the Navy in telling the Navy’s heritage story.\nImage: USS Olympia, 1902. NH 42514\nOn 19 January 1840 an expedition led by Lieutenant Charles Wilkes, USN became the first Americans to discover the Antarctic coast. This engraving by Jorban & Halpin is after a sketch by Wilkes himself, showing the men and dogs of the expedition ashore, with USS Vincennes in the background. NHHC image NH 51495.\nCall for Papers: Historic Naval Ships Association 2013\nThe Historic Naval Ships Association 2013 Annual Conference will be held in Camden, NJ/Philadelphia. The Battleship New Jersey and Independence Seaport Museum will be co-hosting.\nOn 12 November 1942 three days of fighting began, in what came to be known as the Naval Battle of Guadalcanal. The battle began with Japanese air attacks on American ships which had just landed reinforcements, including units from the US Army’s Americal Division (learn more here). Over the course of the next three days, the battle would evolve into a monumental engagement between battleships, cruisers, and destroyers in the narrow confines of Iron Bottom Sound.\nIn this photo, USS President Jackson (AP 37) maneuvers while under Japanese air attack off Guadalcanal on 12 November. In the center background is smoke from an enemy plane that had just crashed into the after superstructure of USS San Francisco (CA 38), which is steaming away in the right center. National Archives image 80-G-32366.\nBOOK REVIEW: Steam Coffin – Captain Moses Rogers and the Steamship Savannah Break the Barrier\nBy John Laurence Busch, Hodos Historia, (2010), 726 pp.\nReviewed by Mark Lardas\nOn Thursday June 17, 1819 lookouts at the Cape Clear Island semaphore station sent a report to the Royal Navy base at Cork, Ireland that a ship was afire off Cape Clear. A revenue cutter sent to investigate discovered not a vessel in distress, but the steamship Savannah completing the first steam-assisted crossing of the Atlantic Ocean.\nMost with even casual interest in maritime history have heard of Savannah. When pressed, many can add that they heard the ship completed only one voyage, that it was an economic failure, and that it was converted back to a sailing vessel upon its return to the United States.\nBOOK REVIEW: The Role the USS Casablanca (CVE-55) Played in World War II in the Pacific\nBy Dr. Barbara G. Jones. 2010, The Edwin Mellen Press, Box 450, Lewiston, NY., 515pp.\nReviewed by Charles H. Bogart\nDr. Barbara Jones has penned a well-written and interesting history of the escort carrier USS Casablanca (CVE 55). The story is told using official records, personal reminiscences, and secondary sources. The author divides her account into four periods: an account of how the United States got into the war, the development of the escort carrier concept, a description of the Casablanca enlivened by accounts of the day-to-day routine on board, and her war time service.\nUSS Sterett Flag From World War II Finds Its Way Home\nWe were contacted last month by the Commanding Officer of the destroyer USS Sterett (DDG 104) about an historic artifact that once belonged to the Naval Historical Foundation. Commander Stewart L. Bateshansky, USN, recently assumed command of the Arleigh Burke class guided missile destroyer, homeported in San Diego. He was shown a tattered 5 foot by 3 foot 48-star American flag, and informed that it had been flown by USS Sterett (DD 407) during the 1942 Guadalcanal campaign. On the packaging for the flag, he noted that the item was marked as once being in the possession of the Naval Historical Foundation.\nOn 24 September 1960, the world’s first nuclear powered aircraft carrier, USS Enterprise (CVAN 65) was launched at Newport News, VA. This image shows Enterprise underway in March 2012.\n(watch “The Nuclear Navy,” which tells the story)\nBOOK REVIEW: The Shenandoah Affair\nBy Paul Williams, Fantascope Pty. Ltd., Australia (2012)\nReviewed by Diana L. Ahmad, Ph.D. Missouri University of Science and Technology\nIn the 2012 edition of The Shenandoah Affair, author Paul Williams provides lay historians with an expanded edition of his 1992 historical novel about the adventures of CSS Shenandoah in Australia. There are basically two themes to the work. The first considers the debate about whether or not Australia should provide aid to Shenandoah in matters of repairs to the vessel and in recruiting crew members that might constitute violations of neutrality laws. This section describes the machinations of the American Consul who tried to get the Australian authorities to declare the Confederate vessel a pirate ship and have it seized. Newspapers in Australia covered the ship’s arrival and took either a pro-CSA or pro-USA position. Like the newspapers, some Australians supported the CSA, while others the USA. The second focus of the novel features the illicit romance between the commanding officer of Shenandoah, Lieutenant-Commanding James Waddell, and the wife of Captain William Nichols, Lillias Nichols. Captain and Mrs. Nichols came from multiple generational Maine shipping families. The two lovers meet when Waddell ordered the Nichols’ Yankee trading vessel, Delphine, sunk. Waddell took Delphine’s crew and passengers, as well as cargo, on board Shenandoah. In a short time, Waddell and Lillias found themselves in a love affair that began aboard ship and continued while Shenandoah was repaired near Melbourne."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:05d18615-6510-43ba-9ada-b76c847f9ba4>","<urn:uuid:f3af508b-f1d1-4772-b31a-68c348fe8084>"],"error":null}
{"question":"How do boat lighting solutions differ between night walleye fishing and navigation safety requirements?","answer":"Night walleye fishing requires specific lighting setups like LED head lamps, LED rope lighting around the gunwale for ambient light, and specialized systems like the Cisco Sur-Lok Boat Lighting System for task lighting. In contrast, navigation safety lighting must meet strict regulatory requirements - Aqua Signal LED navigation lights are designed with proprietary prism lenses to meet USCG visibility requirements and ABYC A-16 standards, providing increased visibility for safer boating. The navigation lights operate on both 12 or 24 volt systems and are specifically designed to meet worldwide regulations for light output, angles of visibility, and color compliance.","context":["Captain Terry Kunnen of TKO Charters caught this monster walleye\nnight trolling using crankbaits and Off Shore Tackle Side-Planer boards.\nThe best boat for night walleye trolling is one that has a place for everything and everything is in that place. On a night walleye fishing adventure just take the necessary gear and nothing else. Keeping things simple is the best way to be successful and avoid frustrations when fishing after dark.\nWhen fishing in the dark an angler is going to need some dependable lighting options. The cockpit lights in most fishing boats aren’t adequate to perform important tasks like tying on fishing lures or monitoring the lead lengths on a line counter reel.\nMost anglers use an LED head lamp for these purposes. I keep mine, plus a spare, stored in the glove box of the boat along with plenty of spare batteries.\nAdding LED rope lighting around the gunwale of the boat is a good way to flood light onto the floor of the boat and provide enough ambient light for tasks like hooking up planer boards, unhooking fish from the landing net, setting lines, etc.\nTo really light up the night, anglers might want to consider more powerful LED lighting options. Cisco Fishing Systems produces the Sur-Lok Boat Lighting System that mounts to risers which slide into their track system. These lights can literally be mounted to any flat surface in the boat, turning night into day. Like everything produced by Cisco these lights are high quality and made right here in America. For more details on these very cool lights for night fishing, visit www.ciscofishingsystemsltd.com.\nWhen the sun sets walleye go on the prowl. Trolling\ncrankbaits after dark is one of the best ways to target\ntrophy walleye that get conditioned to feed\nmore at night than during the day.\nLike most open water trolling applications a good fiberglass or graphite/fiberglass composite trolling rod matched up to a dependable line counter reel and 10 to 12 pound test monofilament line is the right set up for night walleye trolling.\nMy set up consists of a 8.5 foot telescopic rod that fishes nicely when extended and fits neatly into my rod locker when telescoped down. A 20 size line counter reel works best for walleye fishing and I can’t express enough the importance of running premium fishing lines designed with trolling in mind.\nI fish 12 pound test Maxima in the Ultra Green color and have found this line to be tough as nails when fishing around rip rap and dealing with the abrasion created by putting a planer board on the line and taking it off countless times. Maxima is very popular among steelhead fishermen who know how tough this line is, but for the most part walleye anglers have not discovered this super premium line choice.\nNight walleye trolling is a game placed with crankbaits and the brands/models of lures that routinely produce fish is an amazingly short list. Most night walleye fishing takes place at or near the surface so shallow diving stickbaits or what bass anglers refer to as jerkbaits are among the best choices.\nThe list of “must have” stickbaits for walleye trolling include the Rapala Husky Jerk 12 and 14, Rapala No. 18 Minnow, the Smithwick Perfect 10 and the Yo-Zuri Crystal Minnow series of baits. Obviously other stickbaits on the market will catch walleye, but this “short list” are the baits the hard core fishermen carry every time on the water.\nIn some situations it might be necessary to fish a little deeper than these stickbaits naturally dive. The depth of these lures can easily be increased by adding a one ounce Off Shore Tackle Snap Weight to the line about 20 to 25 feet ahead of the lure. A one ounce weight will add about 1/3 to the natural diving depth of a floating/diving style crankbait. In other words a bait that dives 15 feet without weight will reach about 20 feet when a one ounce weight is used.\nNotice the planer boards in this picture. They have been\nequipped with strips of reflector tape which makes them highly\nvisible when night trolling. Other anglers prefer to tape a cyalume\nstick to the flag of their boards when night trolling.\nThe Xi5 also features the GPS navigation powers of the Pinpoint Auto-Pilot System which makes it easy to follow a compass heading and keep the boat traveling in a desired direction hands free. Even better, once a few fish have been caught and the location of those fish saved as waypoints on a Lowrance HDS sonar/GPS unit, the Xi5 can be programmed to duplicate a precise trolling track. Thanks to the Xi5 it possible to troll from waypoint to waypoint like a beagle following a rabbit track!\nThis amazing technology is made possible by combining the GPS features of the Xi5 electric motor with the Sonar/GPS capabilities of any Lowrance Generation Two or Three HDS units. Essentially the electric motor and sonar/GPS unit are communicating using a system called Gateway, that allows the electric motor to be controlled using a foot control, a key fob and also using the touch screen on Lowrance HDS units.\nCatching walleye by trolling after dark is about finding fish, but it’s just as important to stay on those fish once they are found. With the Gateway System it’s easy to duplicate a productive trolling pass time and time again.\nBOARDS ARE BEST\nAn angler can long line his favorite crankbaits out the back of the boat and catch a few night time walleye. Incorporating in-line boards like the famous Off Shore Tackle OR12 Side-Planer is the fast track to night trolling success.\nIn order to see these boards in the dark and detect strikes, anglers use some ingenious tricks. Ordinary cyalume sticks sold at dollar stores are a good value and they also make a great board light option when taped to the flag of an OR12 Side-Planer. The typical cyalume stick lasts for six or eight hours and provides plenty of light to easily monitor boards in the dark.\nAnother option is to purchase Coast Guard approved reflector tape and place a few strips on the board flag. This tape will reflect even the subtle light produced by a boat’s bow and stern lights and also head lamps.\nAnother little known trick is to set the bait clicker function on the trolling reels and then to back off on the reel drag until just enough tension in the drag is achieved to hold the boards in place while trolling. When a fish is hooked, the extra pressure on the line causes the line to slip a little and the bait clicker gives an audible clue as to which rod/reel combination has hooked a fish.\nWHERE IS THE ACTION\nWalleye can be caught at night anywhere these fish are found. In-land lakes, impoundments, rivers and of course the Great Lakes are all great places to try your hand at “night shift” walleyes.","Aqua Signal Lighting\nAqua Signal has been producing marine lighting & electronics for over 130 years. Their range of conventional and LED lighting meets the needs of recreational, commercial, and naval vessels ranging from small sailboats to cruise ships, ferries, and even offshore drilling platforms.\nAqua Signal LED Lighting\nDid you know Aqua Signal was the first marine lighting manufacturer to offer a complete line of LED navigation lights, and the first to receive worldwide approval for this technology?\nCombined with Aqua Signal's proprietary prism lens, these LED navigation lights give your boat markedly increased visibility and contribute to safer boating. The LED fixture features a single high quality diode with patented prism to meet industry requirements for light output, angles of visibility, and color. Additionally this fixture also helps to reduce the risk of premature failure of the diode by featuring a machined aluminum heat sink to dissipate heat build-up.\nCompared to conventional incandescent bulbs, LED lights have lower power consumption (up to 80% less), burn brighter and last longer. LED navigation lights require no modification to operate with 12 or 24 volt systems. All Aqua Signal lights exceed USCG visibility requirements, are certified ABYC A-16, and are easy to install.\n- Navigation lights\n- Control lights\n- Outdoor lighting\n- Indoor lighting\n- Emergency lighting systems\n- Computerized illumination systems\n- Decorative lighting\nDo Aqua Signal lights meet USCG approvals?\nYes, all Aqua Signal LED's meet all worldwide regulations, they also meet or exceed the specific regulations promulgated by the USCG and ABYC for the US and Canadian market.\nWhat are the benefits of LED technology?\n- Compact LED size allows for a wide range of housings designs and sizes\n- Increased life expectancy over incandescent (50,000 to 100,000 hours, defined as > 50% of original output)\n- Safe Against Mechanical Shock and Vibration which makes it an excellent choice for automotive and marine applications\n- Unlike conventional lighting, there is no risk of implosions\n- Low maintenance costs (exchange or cleaning)\n- Low operating costs (power consumption)\n- Saves low voltage power supply (energy efficient)\n- LED's produce more light/watt than incandescent light\n- Can emit light of an intended color without the use of color filters that traditional lighting methods require\n- LED's inherently focus the light\n- No ill effects or premature failure from cycling on and off of LED lights\nWhat are candelas in relation to navigational lights?\nCandela (or candle power) is a measurement of light output and is often used for searchlights, floodlights and in some cases interior lights. This is the light output of the light or the measurable intensity of the light output. Navigation lights on the other hand, certified and measured by the distance they can be seen. The difference is light output vs. visible light detection. Aqua Signal does not publish candela rating measurements for their navigation lights.\nStrobe light and LED light combination's\nAqua Signal LED fixtures do not include strobes. There are several reasons for this because of the smaller conductor sizes for the LED and the size of heat sinks need to cool the diodes. Because of the size of the heat sinks, it is not possible to fit the strobe into the LED fixture."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:1a26a0ae-6694-4101-a72d-1361a3bed8b8>","<urn:uuid:583e4568-887e-4bf6-9e98-ca7e50cbcb5a>"],"error":null}
{"question":"How do structural requirements differ between a Dyson sphere shell and Farrat structural thermal break plates?","answer":"A Dyson sphere shell needs to be several yards thick of chrome steel to encompass an entire star at one astronomical unit radius. In contrast, Farrat structural thermal break plates are much smaller, with maximum thicknesses of 25mm and a characteristic compressive strength of 312 N/mm² for TBK grade. While both structures serve containing/insulating purposes, they operate at vastly different scales and load requirements.","context":["Dyson sphere n.\nan artificial structure in the form of a hollow shell surrounding a star, built esp. in order to capture the star’s energy output\nHow thick would the shell of a Dyson sphere be? [Ibid. 473] An extraplanetary civilization surrounded by a Dyson sphere would be a very powerful source of infrared radiation.\ntr. I. S. Shklovskii & C. Sagan Intelligent Life in the Universe 472\nThe Ringworld is a compromise, an engineering compromise between a Dyson sphere and a normal planet. Dyson was one of the ancient natural philosophers, pre-Belt, almost pre-atomic. He pointed out that a civilization is limited by the energy available to it. The way for the human race to use all the energy within its reach, he said, is to build a spherical shell around the sun and trap every ray of sunlight.\nBut there’s more to a Dyson sphere than collecting solar power. Say you make the sphere one astronomical unit in radius. You've got to clear out the solar system anyway, so you use all the solar planets in the construction. That gives you a shell of, say, chrome steel a few yards thick. Now you put gravity generators all over the shell. You'd have a surface area a billion times as big as the Earth’s surface. A trillion people could wander all their lives without ever meeting one another.\nI thought that’s where you might be heading. A Dyson sphere.\nStarless World v. 19\nBob Shaw’s Orbitsville, which also was based on Dyson’s speculations about advanced civilizations converting planetary matter into a sphere about a sun and utilizing all that star’s energy and all the converted planet’s area as living space was not as spectacularly popular as Ringworld but kept the concept of the Dyson sphere at the center of the novel.\nReaders of Hard Science Fiction in G. E. Slusser & E. S. Rabkin Hard Science Fiction 78\n1993 Science Fiction Age Jan. 20/3\nThe same episode will also feature breath-taking footage of the Dyson Sphere, an effect meant to replicate a device two hundred million miles in diameter that was built around the sun of a now vanished alien race.\nOne must think about the physical possibilities of a Dyson Sphere habitat, to the point where one can see the implications for life on the inner surface.\nAfterword in C. Pellegrino & G. Zebrowski Star Trek: Next Generation: Dyson Sphere (1999) 201\nFor the first time, the Dyson sphere really did look like a planet. It was as small as Earth now, and growing smaller with each passing second. The instruments suggested that it did not exist at all, except visually.\nStar Trek Next Generation: Dyson Sphere xii. 189\nWhy anyone would create such an artefact in the first place was beyond him. It lacked the practicality of a Dyson sphere or a Niven ring.\nPandora’s Star xxii. 722\nThis particular fragment of the nascent, fully enclosing Dyson sphere was diamond-shaped, 160,000 miles long and 100,000 wide.\nPolity Agent i. 34\nResearch HistoryMike Christie found a citation in a 1984 printing of Larry Niven's \"Ringworld\". Brian Ameringen subsequently verified the cite in the 1970 first edition.\nSue Surova contributed various cites from 1996-2002.\nBill Mullins submitted a 1966 cite from I.S. Shklovskii and Carl Sagan's \"Intelligent Life in the Universe\".\nMichael Simons provided a link to the Dyson sphere FAQ, which mentions Freeman Dyson's article \"Search for Artificial Stellar Sources of Infrared Radiation\" in 1959 in Science as the original source. Jeff Prucher located a copy of this article and found that the term \"Dyson sphere\" does not appear there, nor does it appear in the correspondence that subsequently appeared in the letter column of the magazine.\nLast modified 2023-03-29 09:53:40\nIn the compilation of some entries, HDSF has drawn extensively on corresponding entries in OED.","Farrat Structural Thermal Break Plates/Pads are high performance thermal insulators used between horizontal and vertical connections of internal and external elements to prevent thermal or cold bridging.\nStructural Thermal Breaks provide a simple, economical and extremely effective solution to meeting Part L of the Building Regulations by way of reducing heat loss and the risk of internal condensation. Farrat Structural Thermal Breaks have British Board of Agrement Certification [BBA]. This is important in a market where there are materials offered which have not had independent evaluation to ensure suitability in structural connections.\nFarrat Structural Thermal Breaks also meet the NHBC’s technical requirements.\nChanging Legislation in response to climate change and energy saving has meant that Farrat now supply Structural Thermal Break Plates for the UK and overseas market.\nConstantly driven by engineering excellence, we continue to lead the way in the development of the Structural Thermal Break Plate market with the following Certifications and membership:\nFarrat Structural Thermal Breaks have British Board of Agrèment Certification [BBA].\nFarrat Structural Thermal Breaks meet the NHBC’s Technical Requirements. This is referenced in the BBA Certification.\nFarrat is a member of BRE’s Certified Thermal Details and Products Scheme.\nFarrat Structural Thermal Breaks can be found on NBS Plus and NBS National BIM Toolkit and Library.\nFarrat operates under an ISO 9001:2008 Quality Assurance System. This also incorporates BBA’s Product Quality Plan.\nFarrat operates under an ISO 14001:2004 Environmental Management System.\nFarrat is a member of The Steel Construction Institute [SCI].\nFarrat Structural Thermal Breaks are manufactured from high performance materials. We only use materials specifically developed for use within the building envelope and have British Board of Agrement Certification [BBA] to ensure that designers and clients have confidence in the product which is used in structural connections. Every order is accompanied with a Certificate of Conformance.\nWe offer two grades, Farrat TBK and Farrat TBL.\n|Farrat TBK||Farrat TBL|\n|Characteristic Compressive Strength, fck (N/mm² , MPa)||312||89|\n|Design Value for Compressive Strength, fcd (N/mm² , MPa)||250||70|\n|Elastic Modulus (N/mm² , MPa)||4100||2586|\n|Water Absorption (%)||0.14||0.48|\n|Thermal Conductivity (W/m-k)||0.187||0.292|\n|Colour (may vary)||Amber||Black|\n|Thicknesses available (mm) +||5, 10 ,15 ,20 & 25||5, 10 ,15 ,20 & 25|\n|Thickness Tolerances (mm) ++||0 to +0.3||0 to + 0.25 (TBL5)\n+0.2 to +1.5 (TBL10)\n+0.3 to +2.5 (15, 20 & 25)\n|Maximum sheet size (mm)||2400 x 1200||2500 x 1250|\n|+||Multiple plates can be provided for applications where thicknesses greater than 25mm are required. Both materials can be supplied in non-standard thicknesses (please contact Farrat for further details).|\n|++||Farrat TBL can be supplied to tighter tolerances (please contact Farrat for further details).|\nFor further details, please refer to our Structural Thermal Breaks brochure.\nThere are few standard construction details between projects therefore detailing of the building envelope and penetrations can vary significantly. As a result, the calculation of thermal performance and compliance with requirements can be complex.\nThere are two aspects to the thermal performance of the building envelope; heat loss and condensation risk. Both issues are covered by Building Regulations and guidance on meeting them is provided in various Approved Documents (England and Wales), Technical Handbooks (Scotland) or Technical Booklets (Northern Ireland). These documents currently require heat loss and condensation risk to be assessed in accordance with the same British Standards, European Standards and BRE Publications.\nUnlike proprietary mechanical thermal break systems, the plate type thermal break is very simple to incorporate into most details. This flexibility means that it can be used for a wider variety of applications and is not restricted by the modular nature or the space required for proprietary mechanical systems. This flexibility also provides the Designer with greater freedom to develop a bespoke solution.\nThermal Design Considerations:\nHow thick does the thermal break need to be?\nIdeally the detail should be thermally modelled. This requires not only the members and connections, but the entire fabric of the envelope local to the connection to be included in the model. This applies to both mechanical and plate type thermal breaks.\nThis issue is often forgotten or considered late in the construction process and due to the cost and time implications, modelling is often not undertaken. However, modelling should be considered where:\n- the environmental conditions pose a greater risk (e.g. swimming pools)\n- the detailing of the planar elements local to the connection are considered to have an inferior thermal performance to that of the main building envelope\n- there is significant repetition of the same detail (e.g. balconies).\nIf thermal modelling is not undertaken, the following should be considered:\n- The thermal break should be located within the insulated zone of the building envelope.\n- Selection of the thickest thermal break (up to 25mm) considering cost, thermal performance and structural requirements (limitations).\n- Minimisation of the cross sectional area/ mass of the steelwork penetrating the building envelope where possible.\n- The performance of the connection detail against the BRE’s Certified Farrat details – information provided below.\nStainless steel bolts are sometimes specified for durability reasons. Isolation using normal methods may need to be considered because of bi-metallic action and corrosion. Isolation using thermal washers and thermal bushes will provide minimal additional thermal performance.\nPoint thermal bridge\nThe quantity which describes the heat loss associated with a single penetration is a point thermal bridge(χ-value, W/K). This is pronounced as ‘chi-value’. This is a property of the thermal bridge and is the rate of heat flow per penetration that is not accounted for in the U-values of the plane building elements containing the point thermal bridge.\nLinear thermal bridge\nThe quantity which describes the heat loss associated with a thermal bridge is its linear thermal transmittance (Ψ-value, W/m·K). This is pronounced as ‘psi-value’. This is a property of a thermal bridge and is the rate of heat flow per degree per unit length of the bridge that is not accounted for in the U values of the plane building elements containing the linear thermal bridge.\nThe Specifier will usually identify indoor and outdoor temperatures and relative humidity conditions under which condensation must not occur. Guidance on suitable conditions is given in BS 5250 Code of Practice for the Control of Condensation in Buildings. From these conditions it is possible to determine the allowable minimum temperature on the construction detail below which there would be a risk of condensation. Finite Element Analysis and similar analysis methods allow the temperature distribution to be predicted.\nThe temperature factor (f) is used to assess the risk of surface condensation or mould growth and is calculated under steady state conditions. To avoid problems of surface condensation or mould growth, the fRsi should not be less than a critical temperature factor (fCRsi).\nA range of appropriate critical temperature factors are identified in BRE Information Paper IP 1/06 and listed below:\n|Building type||Critical Temperature Factor (fCRsi)|\n|office, retail premises||0.50|\n|dwellings, residential buildings, schools||0.75|\n|sports halls, kitchens, canteens||0.80|\n|swimming pools, laundries, breweries|\nBRE Certified Thermal Products Scheme\nFarrat is a member of BRE’s Certified Thermal Details and Product Scheme.\nThe scheme database includes for both BRE Certified Thermal Details and Products and Government Accredited Details, and this provides a freely accessible and independently assessed and certified resource for users. The third-party BRE Global certification can distinguish products and services from their competitors, and give customers confidence about the thermal performance of the products.\n- storage buildings\n- retail premises\n- residential buildings\n- schools and sports halls\n- kitchens and canteens\n- swimming pools\nThermal Modelling Specialists\nGraeme A. Hannah (MEng) – Technical Manager\nBRE Certified Thermal Details and Products Scheme\nT: +44 (0) 1355 576 000 E: email@example.com\nAnnalisa Simonella – Director\nLoud1 Design | Building Physics\nT: +44 (0)7906 082 828 E: firstname.lastname@example.org\nDr. Richard Harris – Partner, Consultancy Department\nT: +44 (0)20 7565 7066 E: email@example.com\n- Thermal bridge in a link without a Farrat Thermal Break. The temperature of the steel is on the hot side of the outer-wall system (9.8°C) and heat loss (χ value) is 1,31W / K.\n- Distribution of temperature with Farrat Structural Thermal Break plate (TBK). The temperature on the hot side of the facade system has been improved to 16.5°C and the heat loss is limited to 0.35 W/K = 73% less heat loss.\nUnder the SCI Assessed Product Scheme the technical data and structural design methodology for Farrat Structural Thermal Breaks has been independently verified by the SCI. The design considerations are set out in the Farrat Structural Thermal Breaks Connections Guide.\nUnlike proprietary mechanical thermal break systems, the plate type thermal break is very simple to incorporate into most details. This flexibility means that it can be used for a wider variety of applications and is not restricted by the modular nature or the space required for proprietary mechanical systems. This flexibility also provides the designer with greater freedom to develop a bespoke solution.\nStructural Design Summary (steel connections)\nConnections that include thermal break plates should be designed in accordance with the relevant design standards (e.g. BS EN 1993-1-8) or industry guidance (e.g. SCI publications). The following additional checks should also be undertaken, check that:\n- the thermal break plate can resist the applied compression forces.\n- any additional rotation due to the compression of the thermal break plate (including allowance for long term creep) is acceptable.\n- the shear resistance of the bolts is acceptable given that there may be a reduction in resistance due to:\n- PACKS – Clause 22.214.171.124 of BS 5950-1 or clause 3.6.1(12) of BS EN 1993-1-8\n- LARGE GRIP LENGTHS – Clause 126.96.36.199 of BS 5950-1 or BS EN 1993-1-8\nStructural Design Considerations:\nThermal break plates are contained within the protective envelope of the building and in general Building Regulations do not require them to be fire protected or have a fire performance rating. Where the connection containing the thermal break requires fire protection then the following options can be considered:\n|Board Protection||A number of proprietary fire protection board systems are available on the market.|\n|Sprayed Fire protection||A number of proprietary sprayed fire protection systems are available on the market. The manufacturer should be consulted regarding the compatibility between the system and the thermal break materials. Alternatively consideration can be given to recessing the thermal break and providing a continuous fire protection strip (Nullifire etc.)|\nThe majority of thermal break connections are related to secondary elements only.\nThe Structural Engineer will consider robustness during the design process and will refer to local codes and standards. Where a thermal break is located within a key critical element this may need further analysis leading to either consideration of the complete loss of the thermal break or inclusion, for example, of a physical “fail safe”. The detailing of this can often be undertaken whilst maintaining the thermal performance of the connection.\nHandling on site\nThermal breaks are normally procured by the steel fabricator as part of the steel frame package on a project. The delivery from Farrat is normally co-ordinated with the steel work contractor erection schedule. They are delivered to site with each one labelled with a unique reference linked to the steel work contractors drawings.\nFor identification purposes Farrat TBK and TBL are different in colour. If it is essential to the project that both materials are used on the same project, Farrat normally advise that the connection arrangement (e.g. bolt positions) is unique to ensure that no errors are made during installation. This is in addition to Farrat’s normal labeling protocol.\nThe general handling requirements for thermal breaks should be in line with other component accessories expected to be handled with the primary steel work. This is covered in the NSSS: Section 8 Workmanship – Erection. The NSSS also sets out the requirements of the Quality Management System expected to be adopted by all competent steelwork contractors working on UK construction projects.\n- Structural Thermal Break Plate (TBK) with 4-hole connection, steel-to-steel.\nFarrat Structural Thermal Breaks can be used in a wide variety of applications where there is a structural requirement of the thermal insulation:\n- Steel to steel\n- Steel to concrete / masonry\n- Steel to timber\n- Concrete to concrete\n- Facade system connections to the primary frame\n- Brise Solei and Canopies\n- Roof plant room columns\n- Connections of external to internal primary building elements\n- Isolation of sub-structure & basement structure elements\n- External staircases or external balconies\n- Man-safe systems\n- BMU Systems\n- Connections to existing structures\nFarrat’s market leading Structural Thermal Break Plates & Pads (FTB) are high performance thermal insulators, used between horizontal and vertical connections of internal and external elements to prevent thermal/cold bridging.\n- Mechanical properties of the materials aligned for building applications\n- High or very high strength options\n- Low thermal conductivity (k)\n- Ability to be manufactured in 2D or 3D (i.e. recess, chamfer, etc.)\n- Variety of thicknesses available. Special thickness/tolerances available to specifiers\n- British Board of Agrement Certification [BBA]\n- A simple and effective solution to meeting Building Regulations\n- Not a proprietary modular mechanical system – so offers the designer scope to develop bespoke connection detailing\n- Supported by technically qualified staff\n- Supported by external organisations including BBA, NHBC, NBS and BRE.\n- Manufactured under Farrat’s ISO9001 & ISO14001 Systems\n- Manufacturing capacity allowing us to meet your lead time\nShould show a fully detailed connection or one communicating the design intent with a supporting specification (NBS or similar).\nIs normally responsible for ensuring that the connection meets the requirements of the Building Regulations Part L (SAP).\nDesign Output – Thermal performance/ Thickness (Farrat TBK or Farrat TBL).\nThe Structural Engineer\nIs normally responsible for designing the connection or providing a performance specification for the steel work fabricator.\nDesign Output – Strength (Farrat TBK or Farrat TBL)\nSample Specification for project using Farrat TBK – National Building Specification (NBS)\nNBS Clause: G10/ 350 Structural Thermal Break Connection Plate\n- Manufacturer: Farrat Isolevel Ltd, Balmoral Road, Altrincham, Cheshire, WA15 8HJ, Tel: +44 (0)161 924 1600, Fax: +44 (0)161 924 1616 www.farrat.com\n- Product Reference: Farrat TBK\n- Thickness: 25 mm\n- Plate Size: As Drawing number – or to be determined by the connection designer\n- Hole Size & Positions: As Drawing number – or to be determined by the connection designer\n- Certification – BBA\nPlease be aware there are cheaper materials on the market that the supply chain may provide as an alternative, but in our view not of equal performance or certificated for use in building applications (structural).\nTo enable us to provide a quotation, we will require the following information for each plate:\n- Material – Farrat TBK or Farrat TBL\n- Plate Dimensions\n- Plate Thickness\n- Size and Number of Holes\n- Any Special Requirements\n- Delivery Location\nTo accept orders, our manufacturing facility will require a fully dimensioned drawing with each plate type having a unique customer reference (drawing number).\nWe aim to start manufacturing within 3 working days of receiving the order and you will be advised of a despatch date.\nWe can very often start manufacturing sooner, and can work with you on very large orders to meet your programme and requirements.\n- Each plate has a label attached [Farrat/BBA]\n- Each order will be accompanied by a Certificate of Conformance under our British Board of Agrement Certification.\nQMU Graduate Centre\nType: TBK Thermal Breaks\nType: Structural Thermal Breaks\nType: Structural Thermal Breaks\nAIRC Cranfield University\nType: TBK Thermal Breaks\nNottingham Trent University\nType: TBL Thermal Breaks\nTo provide a quotation please submit the following information:\n- Material Type – Farrat TBK or Farrat TBL\n- Plate Dimensions & Thickness\n- Size & Number of Holes*\n- Delivery Postcode\nWe aim to start manufacturing within 3 working days from an order being placed.\n*A fully dimensioned drawing will be required for each type of plate with a unique reference prior to fabrication.\nStephen Blundell BEng(Hons) CEng MICE MIStructE\nTechnical Director – Structural Thermal Breaks\nDD: +44 (0)161 924 1600\nStructural Thermal Break Connections\nStephen is a Chartered Engineer with over 35 years experience in civil and structural engineering. As Technical Director for Structural Thermal Break Connections at Farrat, Stephen is the technical lead for thermal break systems and specification."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:c93c081b-c312-41fe-8acb-f62f0c568f3c>","<urn:uuid:5418009a-1a97-44de-9c7c-a25030b5fa55>"],"error":null}
{"question":"As someone interested in orchestral composition, I'm curious how Zibuokle Martinaityte's approach to layered textures compares to Ligeti's micropolyphony technique?","answer":"Both composers work with layered orchestral textures, but in distinct ways. Martinaityte creates a 'musical canvas' with multiple layers of sound, including an ever-present sounding mass that acts as a background for more prominent foreground textures. She acknowledges Ligeti as one of her strongest influences. Ligeti's approach, specifically in works like Atmosphères, involves dense canonic structures and micropolyphony, where individual instrumental voices are woven so tightly that they lose their individuality and become an impenetrable texture. While Ligeti's polyphonic structure remains hidden in what he calls a 'microscopic underwater world,' both composers achieve similar effects of creating complex, multi-layered sonorities that blur individual voices into a larger textural whole.","context":["[Zibuokle Martinaityte is a Lithuanian composer now based in New York. Her orchestral work Horizons will be featured in Composers, Inc.’s opening concert November 17. Currently in Paris on a creative residency, she responded by email to Allen Shearer’s questions.]\nWhat is your compositional process like? What is a typical starting point for a new piece?\nIdeas for my compositions often emerge from non-musical sources. A sentence in a book, a line of poetry, or watching forces of nature unfold can trigger the imagination. The phrase “the blue of distance” from Rebecca Solnit’s A Field Guide to Getting Lost turned into a title for my choral composition. This beautiful, poetic phrase seemed to open the door to a world of endless musical possibilities. Or the basis for a new piece can be a purely constructive compositional idea, as with my brass sextet Osmosis, in which the instruments are in osmotic relationship to one another and assimilate more and more as the piece progresses. Or life events or psychological states may need to be expressed in sound. Completely Embraced by the Beauty of Emptiness reflects the world of emotions associated with the passing of my father.\nOnce I commit to an idea, a different type of work begins. The creative process is usually quite slow despite moments or even hours of burning inspiration. What takes time is not the composing itself, but dealing with inner and outer disturbances evoked by the creative process. When one enters that territory of the unknown, there are dark forces one has to conquer. The process has its phases, yet at times it seems to be endless.\nThere are lots of details to be dealt with. The first round of revisions is mostly editing the sequence of acoustic events, prolonging some sections, shortening others or eliminating them altogether. At some point the piece seems to have become a living organism which may have imperfections yet gives a certain sense of wholeness. The second round of revisions comes after hearing the first performance of the piece. Here I fine-tune the timing, fix minutiae of instrumental technique, or adjust dynamics.\nYou are a classically trained pianist. Did that help prepare you to become a composer? Has the music of past eras provided lessons?\nThe past is ever-present, at least on the subconscious level. Although I don’t think I draw influences from particular pieces, there are layers of musical residue, pieces that I’ve learned to play on the piano or ones I’ve listened to repeatedly. Composers I’ve admired are J. S. Bach for his sense of symmetry, proportion and incessant continuity; Mozart for his capricious nature, rapidly shifting emotional states and for his joyful “lightness of being,” which for me was an antidote to darker moods and depression associated with adolescence; and Scriabin for the aura of mystery in his harmonies. Other composers and pieces that left a trace are Giacinto Scelsi, Gyorgy Ligeti’s Atmosphères and Gérard Grisey’s Vortex Temporum.\nAs you are a Lithuanian composer, one looks for the influence of the “Polish school” prevalent in the sixties and seventies – Penderecki, Lutoslawski, Gorecki. In Horizons and other of your pieces I do find the massed sonorities those composers are known for, but more as a background, a kind of “whoosh” or distant roar, as if the passing of time itself were given a voice.\nYour insightful observation is relevant to many of my pieces. Yes, perhaps voice is given to the passing of time, to the experience of time itself as well as the events that happen in that time. Just as the depth of the ocean is underneath the waves, there is this ever-present sounding mass, at times an almost inaudible hidden background. I prepare a “musical canvas” consisting of a few layers of sound upon which I build up the more prominent foreground textures. Ligeti, with his multi-layered micropolyphonic orchestral textures, has been one of my strongest influences, and the “Polish school” as well, since I had great exposure to it.\nYou have participated in conferences on new music in various European countries, and you have been to several artist colonies in the United States. How has this helped you as a composer?\nI simply love encountering new music of various countries that I’ve never heard before, especially performed live. It proves again and again that there is no limit to human imagination! Listening to new pieces and analyzing the scores, observing the rehearsal process, attending composers’ talks about their work – I find all of this incredibly stimulating.\nMy best work has been done at artist colonies, where distractions of the outer world are minimized. The MacDowell Colony provides no Internet connection in the studios. Though hard to cope with at first, this frees up internal space in our Internet-addicted minds. In hearing tests it was discovered that I am a “super-hearer,” someone who can hear extreme ranges of frequencies especially at the high end of the spectrum. This can lead to aural overstimulation. Silence during creative residencies is a much needed restorative for the brain and for general well-being.\nDuring a residency one can balance solitude and human exchange with other artists, gaining exposure to their work and sharing secrets of the creative process. A random conversation with an artist of a different discipline can lead to a future collaboration or simply “whisper” the solution to your own creative puzzle.\nYou have had orchestral and other works performed in Lithuania. Are there more performance opportunities for a composer in Europe than in the United States?\nAt the beginning of a career it is easier to get performances in your native environment, whether it’s Europe or the United States. Later the horizons start to expand. It is true though that the arts in Europe are more supported by the government. In the US, where funding is mostly from private sources, artists have to develop entrepreneurial skills for their art to survive and reach audiences.\nWhat’s it like to be a composer in New York City?\nNew York contains the entire world! It must have the world’s highest density of composers and other creative types, which is both stimulating and nerve-wracking. But there is such excitement around new music! When I moved to New York, one of the first concerts I attended was the JACK Quartet performing four string quartets by Xenakis. The 250-seat hall at Morgan Library was completely filled with new music aficionados, with many people waiting to get in. The atmosphere was charged with energy and excitement from the audience and the performers, who exhibited both virtuosity and a passion for that music. It’s an affirmation of the necessity of music and art in general. For us composers, it adds an extra push to our creative development. After all, music is meant to be heard and shared with other people.","You are here\nLength: c. 10 minutes\nOrchestration: 4 flutes (all = piccolo), 4 oboes, 4 clarinets (4th = E-flat clarinet), 3 bassoons, contrabassoon, 6 horns, 4 trumpets, 4 trombones, tuba, 2 pianos, and strings\nFirst Los Angeles Philharmonic performance: September 9, 1971, Lawrence Foster conducting\nGyörgy Ligeti, in a 1978 interview with his Hungarian compatriot, the musicologist Péter Várnai, observed: “I have always approached musical texture through part-writing. Both Atmosphères and Lontano  have a dense canonic structure. But you cannot actually hear the polyphony, the canon. You hear a kind of impenetrable texture, something like a densely woven cobweb. I have retained melodic lines in the process of composition, they are governed by rules as strict as Palestrina’s – but the rules of this polyphony are worked out by me. The polyphonic structure does not actually come through, you cannot hear it; it remains hidden in a microscopic underwater world… I call it micropolyphony… All in all, you cannot hear my music as it appears on paper… The technical process of composition is like letting a crystal form in a supersaturated solution. The crystal is potentially there in the solution but becomes visible only at the moment of crystallization. In much the same way, you could say that there is a state of supersaturated polyphony, with all the ‘crystal culture’ in if but you cannot discern it. My aim was to arrest the process, to fix the supersaturated solution just at the moment before crystallization… ”\nAtmosphères was widely known before general familiarity with Ligeti’s name, today among the most honored in the musical world, when it was employed as “space-age music,” to evoke distance, loneliness, foreignness, in Stanley Kubrick’s 1968 film 2001: A Space Odyssey. (Ligeti, it should be noted, strenuously objected to its use there.) Of this score, commissioned in 1961 by the Southwest German Radio, Baden-Baden and first performed at Donaueschingen that year under Hans Rosbaud, the composer observed, well after the conversation quoted above:\n“My most basic aim… is the revivification of the sonorous aspect of musical form. Those factors of contemporary composition which do not manifest themselves directly as acoustical experience seem to me of only secondary importance… In Atmosphères I have attempted to supersede the ‘structural’ approach to music which once, in turn, superseded the motivic-thematic approach, and to establish a new textural concept of music [in which] there are no ‘events,’ but only ‘states,’ no contours or forms, but instead an uninhabited, imaginary musical space. Tone color, usually a vehicle of musical form, is liberated from form to become an independent musical entity.\n“This so-to-speak ‘informal” music I embodied in a new type of orchestral sound: the sonorous texture is so dense that the individual interwoven instrumental voices are absorbed into the general texture and completely lose their individuality… The ‘interwoven’ treatment of the orchestra is the reason for the omission of all percussion and for the unusual format of the orchestral score which is notated on 87 staves, since the string instruments are written completely divisi, that is, with an individual part for each player….”\nHerbert Glass, after many years as a columnist for the Los Angeles Times, has been the English-language annotator and editor for the Salzburg Festival for more than a decade."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:98e2219b-ce3c-46ee-9770-c53b7aa85c63>","<urn:uuid:5253e74e-32df-4dd5-885c-a3e5e3d06898>"],"error":null}
{"question":"I study theoretical physics and need clarity - how has universe's expansion changed through time, and what new theories explain this acceleration?","answer":"The universe's expansion has gone through distinct eras: It started with an inflationary era of rapid volume growth lasting from 10-35 to 10-32 seconds after the Big Bang. This was followed by a radiation-dominated era lasting 50,000 years, then a matter-dominated era for 5 billion years where matter density restrained expansion. Currently, we're in the cosmological constant dominated era (from 5 billion years to present) where diluted matter density allows accelerated expansion. Recent theoretical work proposes that this acceleration is driven by spacetime vorticity coupled with magnetic fields in galaxies. This model suggests that galactic rotations generate vorticity which, combined with local magnetic fields, creates a repulsive force causing galaxies to recede. The acceleration isn't indefinite - as magnetic energy generation via accretion decreases, gravitational attraction may eventually cause cosmic contraction.","context":["Want to stay on top of all the space news? Follow @universetoday on Twitter\nCosmologists tend not to get all that excited about the universe being 74% dark energy and 26% conventional energy and matter (albeit most of the matter is dark and mysterious as well). Instead they get excited about the fact that the density of dark energy is of the same order of magnitude as that more conventional remainder.\nAfter all, it is quite conceivable that the density of dark energy might be ten, one hundred or even one thousand times more (or less) than the remainder. But nope, it seems it’s about three times as much – which is less than ten and more than one, meaning that the two parts are of the same order of magnitude. And given the various uncertainties and error bars involved, you might even say the density of dark energy and of the more conventional remainder are roughly equivalent. This is what is known as the cosmic coincidence.\nTo a cosmologist, particularly a philosophically-inclined cosmologist, this coincidence is intriguing and raises all sorts of ideas about why it is so. However, Lineweaver and Egan suggest this is actually the natural experience of any intelligent beings/observers across the universe, since their evolution will always roughly align with the point in time at which the cosmic coincidence is achieved.\nA current view of the universe describes its development through the following steps:\n• Inflationary era – a huge whoomp of volume growth driven by something or other. This is a very quick era lasting from 10-35 to 10-32 of the first second after the Big Bang.\n• Radiation dominated era – the universe continues expanding, but at a less furious rate. Its contents cools as their density declines. Hadrons begin to cool out from hot quark-gluon soup while dark matter forms out of whatever it forms out of – all steadily adding matter to the universe, although radiation still dominates. This era lasts for maybe 50,000 years.\n• Matter dominated era – this era begins when the density of matter exceeds the density of radiation and continues through to the release of the cosmic microwave background radiation at 380,000 years, when the first atoms formed – and then continues on for a further 5 billion years. Throughout this era, the energy/matter density of the whole universe continues to gravitationally restrain the rate of expansion of the universe, even though expansion does continue.\n• Cosmological constant dominated era – from 5 billion years to now (13.7 billion) and presumably for all of hereafter, the energy/matter density of the universe is so diluted that it begins losing its capacity to restrain the expansion of universe – which hence accelerates. Empty voids of space grow ever larger between local clusters of gravitationally-concentrated matter.\nAnd here we are. Lineweaver and Egan propose that it is unlikely that any intelligent life could have evolved in the universe much earlier than now (give or take a couple of billion years) since you need to progressively cycle through the star formation and destruction of Population III, II and then I stars to fill the universe with sufficient ‘metals’ to allow planets with evolutionary ecosystems to develop.\nSo any intelligent observer in this universe is likely to find the same data which underlie the phenomenon we call the cosmological coincidence. Whether any aliens describe their finding as a ‘coincidence’ may depend upon what mathematical model they have developed to formulate the cosmos. It’s unlikely to be the same one we are currently running with – full of baffling ‘dark’ components, notably a mysterious energy that behaves nothing like energy.\nIt might be enough for them to note that their observations have been taken at a time when the universe’s contents no longer have sufficient density to restrain the universe’s inherent tendency to expand – and so it expands at a steadily increasing rate.\nFurther reading: Lineweaver and Egan. The Cosmic Coincidence as a Temporal Selection Effect Produced by the Age Distribution of Terrestrial Planets in the Universe (subsequently published in Astrophysical Journal 2007, Vol 671, 853.)","Babur Mirza and colleagues presented a general relativistic mechanism for accelerated cosmic expansion and the Hubble’s constant. They showed that spacetime vorticity coupled to the magnetic field density in galaxies causes the galaxies to recede from one another at a rate equal to the Hubble’s constant.\nAccelerated expansion of the universe, as observed, for example, in the cosmological redshift measurements using type-Ia supernovae (SNe Ia) as standard candles, implies the need for an expansion energy effective at least up to the Mpc scale. A number of independent observations (including the SNe Ia redshift, the Hubble’s constant measurements, the cosmic microwave background (CMB), baryon acoustic oscillations, and various cosmological probes), have measured the contributions of matter and the cosmological constant to the energy density of the universe, providing an accurate measurement of the cosmic acceleration. However the amount of energy for this acceleration implies a hidden or dark form of energy which is approximately three times of the observed gravitational mass-energy density in the universe.\nWithin Einstein’s general theory of relativity, the observed expansion rate can be accounted for by including a cosmological constant, whose origin remains somewhat mysterious. In this context various mechanisms have been postulated, including new forms of hypothetical particles, or modifications of the Newtonian-Einsteinian law of gravitation at large distances, among others. However these theories are specialized in the sense that they fail to account for other observed features of the universe, such as the high degree of isotropy in CMB, or even some feature of the expansion, such as the correct value of the Hubble’s constant.\nNow, Babur and colleagues in their work showed that the specific form of the cosmological constant, hence cosmic acceleration, which can be described by spacetime vorticity, is generated by galactic rotations. They showed that this vorticity coupled to the local (galactic) magnetic field provides the requisite push (repulsive energy) causing the individual galaxies to recede at an accelerated rate. They are therefore led to an oscillatory universe, where expansion and conversely contraction rate is determined by local spacetime vorticity, rather than global geometry (curvature) of the spacetime.\n“To recapitulate we remark that, within the above model of the accelerated expansion of the universe, local spacetime vorticity and magnetic field energy generation within galaxies and galactic clusters act as the feedback mechanism for expansion. Thus, contrary to some recent suggestions that accelerated expansion must imply a violation of the law of conservation of energy, we see that energy conservation remains strictly valid not only locally but globally as well. The continued universal acceleration depends on the energy generation within galaxies, which in turn is determined by accretion rate in galactic nuclei.”, said Babur.\nFriends, conversion of matter-energy density into the magnetic field energy under such conditions can only take a finite amount of time, hence the magnetic field driven acceleration cannot continue indefinitely for a finite total mass. Since the acceleration aB ∼ B’², where B’² is the magnetic energy density per unit volume, they saw that with the decreasing feedback magnetic field, universal acceleration after reaching an maximum will gradually decrease. With the decrease of Magnetic energy generation via accretion, a gradual deacceleration under gravitational attraction is likely to cause cosmic contraction. They therefore proposed that they have an oscillatory universe, where magneto-vorticity coupling rather than global spacetime curvature causes the expansion and contraction phases.\nAccording to Babur, “A very high degree of entropy must have existed at the early stage of the universe, as inferred from the Planckian shape of the CMB radiation. This raises the paradox for other cosmological models, since entropy should decrease closer to the initial singularity (big bang). Our model implies that this must be so due to the expansion started before the cross-over R = Rs, where Rs is Schwarzschild singularity. Subsequently, as this expansion (inflation) stops, and matter formation starts, expansion under spacetime vorticity must now cause matter entropy to gradually increase with time. As deduced in our paper, this explains the high isotropy and the Planckian profile of the CMB spectrum, carrying the imprint of this initial inflation over R > Rs.”\nReference: Babur M. Mirza, “Can accelerated expansion of the universe be due to spacetime vorticity?”, Modern Physics Letters A, Vol. 33, No. 40, 1850240 (2018). https://www.worldscientific.com/doi/abs/10.1142/S0217732318502401 https://doi.org/10.1142/S0217732318502401\nCopyright of this article totally belongs to our author S. Aman. One is allowed to reuse it only by giving proper credit either to him or to us."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:1ce56b11-91d7-4ae3-b522-c96727a53077>","<urn:uuid:066dc218-007f-463a-a4eb-7e19e61a2996>"],"error":null}
{"question":"What are the essential techniques for capturing photos in low-light conditions?","answer":"For low-light photography, several techniques are important. While digital cameras typically have built-in flashes for average photographs, a more professional look requires an external flash. If using a camera with a 'hot shoe' mount, consult a professional to find compatible external flashes. When using a cell phone camera in low light, keep in mind that their flashes are often ineffective or nonexistent. In such cases, you'll need to rely on ambient light sources and can improve your shots by zooming in on your subject. You can also experiment with slower shutter speeds like 1/30, which can create artistic effects with movement while maintaining focus on stationary elements.","context":["There is more to photographic excellence than a focused shot with good lighting. Creating beautiful photos with a camera is a form of art. It has just as many skills and techniques to learn as any other art form. Good photographers have an eye for the art and the ability to see the details that the things of the world contain. Follow these great photo tips.\nTo create pictures that resemble things like water colors, sketches, or oil paintings, use digital methods. There are many options for digital editing software, although Adobe Photoshop is considered to be of the highest quality. It is easy to convert pictures to nice art pieces by choosing the “filter” button, picking your favorite medium, then clicking selection.\nKeep your arms close to you while holding the camera, and position your hands on each side and the bottom of the camera. This will minimize shaking and produce clearer shots. Keeping your hands under the lens and camera, instead of holding it at the top, will also help you avoid dropping the camera by accident.\nWhen you feel as though you are ready for a high end camera, look for a good quality digital single lens reflex camera. A DSLR camera is the best one to use for taking shots as you can look at them as soon as you snap the photo. You should get a full-frame DSLR, as they have big image sensors and capture the most detailed shots.\nMake sure you pack your photography equipment with care when going on a trip. Take all different kinds of lenses, and make sure you take cleaning accessories and enough batteries. Never pack more than you need. Think about which items will be convenient for taking on your trip.\nAll parts of the landscape will be visible in your picture, so be sure to pay attention to what will be at the front of the image. Add some interesting elements or colors to your foreground to create a better frame for your landscape.\nMove and look at your subject from different angles. Try getting shots of the subject from all around it, like above it or below it and so on.\nDon’t pack your equipment carelessly when traveling. Bring as many lenses you think you’ll need and never forget to have spare batteries and maintenance accessories. Do not take more than what you need and think about what will be convenient to transport with you on your trip.\nBy focusing your camera before taking the actual picture and then switching the angle or moving to the side, it will cause the subject to no longer be the central point in your photo. Perfect composition is not necessarily the most interesting or artistic photographic technique. By using this technique, you will find that your photo and the subject matter give off a more interesting appeal to the viewer.\nDigital cameras automatically adjust for low light situations by using flash components. Built in flashes are great for your average photographs, but for a more professional look, you may want to consider an external flash. If you decide you do need an external flash, invest in a camera with a “hot shoe” to fit the flash into, and consult a professional to learn which flashes are a good fit for your camera.\nConsider finding a club that take pictures, or find someone who is also into photography to buddy up with. While you may learn new techniques, make sure you keep your own signature style. When joining forces with another photographer, compare and contrast your photos of the same subjects, so you can get an idea of how images of identical objects can vary in appearance when taken through the perspective of two different people.\nFocus your camera with the subject in the middle and then shift it to the left or right before taking the picture. A centered subject is the norm and most people will not find it interesting or artistic. Off-centering your photos makes them more interesting to those viewing them.\nHaving good skills in photography does not involve a big secret. Keep taking pictures and gain experience. You do not have to develop all your pictures or keep them, especially with the digital format. Over time, as you continue to take photographs of everything, you will get better at analyzing them and determining what could make each photo even better.\nPhotography isn’t an arcane art that only a select few can master. You will improve as you experiment. Luckily, with digital cameras, you can get lots of no-risk practice. You don’t necessarily have to develop every single picture, just keep what you like. You want to constantly experiment with new subjects and techniques, then judge and compare the results to see what worked best.\nIn life we are taught that even and centered is the way things should be. To create photographs that are more interesting, try aiming your camera so that your subject is slightly off center. To create asymmetry, you may need to disable your camera’s auto-focus feature, because it always uses the lens’ center as focal point. Instead, focus the camera manually, then secure the focus prior to taking the photo.\nVisit a thrift store to buy a film camera if you would like to test out the older film-based photography. Using black and white film (200 speed), can also create that old-time look. By getting your single prints on multiple types of paper, you can view the differences and decide which you prefer.\nAre you itching to shoot some dewy, rain-spattered subjects? There’s nothing wrong with making your own rain. Pack a spray bottle along with your photography gear, and give your subject a light misting before shooting it.\nLighting is one of the most important considerations when taking pictures. The ideal lighting for outside photos is provided by the sun when it is low on the horizon. When the sun sits high in the sky, harsh shadows or squinting subjects can become an issue. Give yourself and your subject a break by positioning them parallel to the sun so that light enters the picture from the side.\nYou need to shoot fast when you are taking a photo. You can never tell how fast that fleeting moment will flee, so always be ready for it. Candid expressions disappear, smiles fade or get strained, and beautiful animals will flee if you take too long with the shot. Try not to worry about getting all the camera settings correct, otherwise you risk missing the shot.\nWhite is the worst color to choose for an outfit when getting your photograph taken. Cameras that use autofocus try and determine all the different shades that are present within the photograph. This causes white clothing to usually get washed out in photographs.\nTry creating a silhouette in your photo. A lot of photographer wait until the sunset to take pictures of a natural silhouette, but you can do it differently. If your background is much brighter than your subject, you’ll notice a silhouette forming. You can make a silhouette by creating a flash from outside of the frame or also by directing the subject to stand before a brightly lit window. You should be aware, however, that a silhouette could highlight a subject’s most unflattering feature.\nDon’t hesitate when taking photos; however, squeeze the trigger, don’t jerk it. You never know how fast that perfect moment will leave you, so be ready to capture at any moment. People can tire holding a smile, animals can run, or you could lose that “perfect” candid moment and then the moment will have passed. Adjust your settings as quickly as possible, and do your best to snap your picture while the scene still looks natural.\nGet creative with your shutter speeds. It is normal to use the quickest shutter speed if you want to freeze action. In the same way, try to consider the things you can do with slower shutters speeds like 1/30. Did you see the person riding a bike who was going past at a fast pace? If implemented correctly, you can end up with an image where the backdrop is blurred while the cyclist remains in focus, indicating the speed at which he propelled himself.\nWatch for fixed patterns in the pictures that you take, and make the most of them. You’ll find that these patterns lead to more intriguing prints in the end. Try using patterns for unique angles or backgrounds on a subject matter.\nUse a tripod if you want to take pictures of a landscape. Having a sturdy tripod for your camera to sit on is imperative for taking any sort of picture, especially those that deal with landscapes, since you’ll be able to change the settings without your camera shaking.\nKeep your subject in focus when you want to take great pictures. A focused photo will have good composition as well as personal style. Particularly when you are just beginning, you should try and have your subject in the center of your pictures. Let the background take care of itself.\nTry to visualize a concept prior to actually starting to take photos. Spend some quiet time jotting down ideas and notes about ways to improve your shots. Photography is art and it really shines through when you pay close attention to all the little details that you planned out. You will be inspired and see much better results, if you take this approach.\nUse a tripod to capture the most clear and precise landscape shots. You don’t want a perfect landscape shot to be ruined because the camera jiggled at the wrong moment, so make sure your camera is resting on a steady base.\nTo give your subject a look of power, shoot upward from a low angle. If you desire your subject to project a weaker image, shoot the photo from above. Each of these techniques has their benefits, and experimentation and experience will help you see when these techniques can enhance your photographic subjects.\nIf ou want a subject to look more powerful, aim from a low level going upwards. If you’d like to make your subject appear weaker, shoot from a higher perspective. Experimenting with these photography techniques will teach you when you should and should not shoot from high or low angles.\nExercise patience with setting up your subject into the right pose. Although candid photos serve a purpose, you’ll get better results with nicely posed photographs. If you aren’t happy with the photos you’re taking at birthday parties and family get-togethers, try asking family members to pose instead of trying to take pictures while they aren’t looking. This increases the odds that more of your subjects will look good in your photographs.\nTaking a photo with the camera looking down at the children is not very flattering for them, but getting on their level will give you much-improved photographic results. Doing so provides a simple solution that makes a rather huge difference.\nBy reading this article you now understand that photography is much more than taking technically correct, well-lit pictures. Doing so will help take your photography to new levels.\nIt is possible to use a cell phone camera in a pinch to get decent photos, but remember your lighting. On cellphone cameras, flashes are either ineffective or nonexistent, so you will need to exercise some old-school photographic talent and make the most of ambient light sources. One way to work around a low-light situation is to zoom in on your subject."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:0b5df978-679c-43e3-818a-bf3f8364e309>"],"error":null}
{"question":"How does the durability and maintenance of metal fencing compare to wooden fencing for securing property?","answer":"Metal fencing is significantly more durable and requires less maintenance than wooden fencing. Steel fences typically come with a 25-year guarantee and only need occasional hosing, while wooden fencing requires regular care and maintenance including repainting or finishing with stain every three to four years. Wood can be broken through easily and set on fire, whereas metal fencing is highly resistant to damage. Steel fencing is also approved by most high street insurance companies, which could reduce insurance costs for properties with expensive equipment.","context":["This article looks at allotment security from another angle, concentrating on security fencing and discussing various types.\nThe security of your allotment is paramount if you want to protect the contents of your shed, your produce and any livestock that live there. There are often news stories of allotment break-ins causing hefty repair bills and unneeded stress.\nThis has worsened since the value of scrap metal has increased, meaning opportunistic thieves may seize any opportunities that come along.\nEven if theft isn’t a concern you may have other issues such as gangs of youths congregating at the allotments on an evening or flying footballs squashing your seedlings!! The question is what can you do to increase security at your allotment?\nMetal Sheds Increase Allotment Security\nOne of the first things is to ensure that any expensive equipment is kept out of sight and is housed in a secure shed. Metal sheds are more of a deterrent to thieves as they are harder to break into than traditional wooden sheds.\nAlthough getting a metal shed may secure your items, neighbouring allotments may encourage thieves meaning your produce could still be damaged by someone walking over it or needlessly damaging it.\nIf you have livestock then try to keep them fully secured so thieves cannot get to them easily. If this is not possible then consider things such security tagging which means livestock can be traced back to you if they are stolen.\nConsider Allotment Perimeter Fencing\nIt is perhaps a good idea to get together with everyone from your allotment to discuss a plan of action so everyone is protected and working together. Rather than replacing sheds that are serving their purpose another option could be replacing your perimeter fencing.\nTo act as a deterrent you need to choose fencing that looks like it means business. If it looks easy to climb over or snip through then it won’t be as effective as a tall, sturdy fence.\nSteel Allotment Fencing\nSteel railing such as the fence in the photograph is tall enough to act as a deterrent to thieves but it will also stop the likelihood of things such as footballs being kicked over and flattening your plants. The top can come in a range of finishes so you could even get a spiked top if theft is common in your area.\nAnother fence that will act as a deterrent is a steel palisade fence. Steel palisade fencing is approved by most high street insurance companies which means if you do have expensive equipment on site having this form of fencing may reduce any insurance that you have on it.\nIf you choose these type of steel fences they generally come with a 25 year guarantee and are very low maintenance. This contrasts with wooden fencing that requires regular care and maintenance. The other downside of wooden fencing is that it can be set on fire and easily broken through.\nIf security is your priority then steel fencing is definitely worth considering.\n- Allotment & Garden Paths\n- Allotment Growing as You Get Older\n- Allotment History – A Brief History of Allotments in the UK\n- Allotment History – Cultivating a 19th Century Allotment by Dr Lesley Acton MA Ph.D\n- Allotment History – The First Allotments by Dr Lesley Acton MA Ph.D\n- Allotment Journey – A Step to Sustainable Living\n- Allotment Regeneration – Case Study\n- Allotments & Children\n- Allotments & The Law – Legal Aspects of An Allotment\n- Allotments – Some Tips to Get You Started\n- Clearing a New Allotment or Vegetable Plot\n- Cuban Vegetable Growing Practices can Benefit your Allotments\n- Finding an Allotment – How to Find an Allotment\n- Health and Safety in the Allotment & Garden\n- How to Ensure the Security of your Allotment\n- How To Pick The Right Shed For Your Allotment\n- Improving Security on Allotments to combat Vandalism and Theft\n- The Allotment – The City Dwellers bit of Country\n- Vacant Allotment Plots – What To Do With Them?\n- Why People Grow Our Own – Our Plots\n- You Have a New Allotment!","Wood or chain-link? Picking the right fence for your home\nBy Joe Provey\nThrough the ages, fences have been made from a variety of materials. Around my neighborhood, I see vinyl, cedar, metal and a number of other fence types. Which is best for you? The answer depends on your budget and stylistic preferences. Here are the pros and cons of homeowners’ most popular choices:\nThere’s much to recommend when it comes to vinyl fencing. It is manufactured in a range of styles, with decorative post caps to match, and there’s now a greater variety of colors and finishes to choose from. Perhaps the most appealing aspect is that you do not need to repaint vinyl. A hard-wearing material, it won’t warp, splinter, rot, split or blister. And it’s easy to clean; dirt can be washed away using only a sponge and hose (or a pressure washer). Though vinyl may initially cost more than wood, it’s less expensive over time.\nThere are of course negatives to be aware of. Dozens of styles are available, but design options are not unlimited with vinyl fencing, and its plastic appearance is not everyone’s favorite.\nIt’s tough stuff, to be sure, but vinyl can break — under high winds, for example, or upon impact from a kicked soccer ball. Minor damage (holes, cracks, etc.) is repaired with body filler followed by sanding and repainting. More extensive damage may require a replacement component, so if and when you install vinyl fencing, hang on to any spare parts. If your fence style is discontinued, that could mean you’re out of luck.\nThe construction of vinyl fencing is more complicated than you might expect. Rails are attached using specialized brackets or crimping tools, and posts must often be reinforced with concrete or metal stiffeners.\nLast but not least, it’s important to note that vinyl is generally considered environmentally unfriendly. Toxins are produced when it’s manufactured, and the material is difficult to recycle.\nComposite fencing (an engineered wood product) comes in a bewildering number of variations. Some fences have solid, not hollow, boards. Some are “capped” or “co-extruded” with a layer of PVC. And while multiple components are often required, construction in some cases is similar to that of a wood fence. Common to all composites is the fact they are made with recycled fibers, plastics and binding agents.\nDue to manufactured textures and colors, composite fencing simulates wood more effectively than vinyl does, but if you opt for a solid color, there are fewer choices among composites. Like vinyl, composite fences require no staining or painting, and can easily be taken care of — only mild detergent and a hose are needed for cleaning. Durable and often backed by warranty, composite fencing is assembled, not with special brackets, but with traditional fasteners.\nAnother pro: Composite fencing is environmentally friendly. Up to 95 percent of materials used to make it are recycled, and some makers employ a nearly waste-free manufacturing system. Unfortunately once manufactured, however, composite products cannot be recycled easily.\nAnother con: Compared to vinyl or wood, fewer styles are available with composite fencing, with designs mainly limited to fence types that involve boards — privacy, shadow box and post-and-rail fences, not to mention those with simple dog-ear pickets. Composites are also susceptible to scratching, staining, and fading.\nCost? Composite boards go for about twice as much as pressure-treated wood, and between the two, wood is easier to work with.\nWood fences have been a mainstay of the American landscape since the Colonial period. They can be crafted in a wide variety of styles, and painted or stained with innumerable colors. In some regions, cedar and redwood are the preferred material on account of their resistance to rot and insects, but several other wood species are also used. For longevity, pressure-treated wood is best, at least when it comes to structural members. Any non-pressure-treated pickets or boards should be coated with a preservative prior to finishing.\nInitially, wood costs significantly less than either vinyl or composite, and if properly constructed and maintained, a wood fence will last for many years. And, unlike petro-based materials, wood is renewable if sustainably harvested.\nOn the other hand, wood requires more maintenance than other materials. A couple times per year, it should be rinsed off, and every three or four years, it should be repainted or finished with a stain. Cedar and redwood fencing may be left to weather naturally, but even so, a clear preservative should be applied every few years. Because of the additional maintenance required, the cost of wood fencing may ultimately equal or exceed that of other fence types.\nIn many ways, ornamental metal fencing combines the best qualities of other materials. Available in a wide variety of styles, it’s very low-maintenance and unsurpassed in durability. While cast iron was the norm for many decades, today’s standard is powder-coated, galvanized steel, aluminum, or a combination of aluminum and solid metal.\nMaintenance is limited to an occasional hosing. If rust appears, it can be brushed off, or the metal can be treated with a rust-inhibiting primer and a fresh coat of paint. Even after many decades, metal fencing can easily be recycled.\nOrnamental fencing is not without drawbacks. For one thing, repair work can be tricky. But metal fences are highly resistant to damage. Indeed, you can buy Civil War-era cast iron fencing that has outlasted the house it once surrounded.\nSteel fencing takes many forms, the most popular of which is chain-link. Though it’s not normally thought of as pretty, chain-link fencing can certainly be used without becoming an eyesore. For starters, the mesh is immediately useful as a trellis for everything from moonflowers to morning glories.\nChain-link is sturdy, maintenance-free, durable and economical. Plus, it’s ideal for situations in which you want your fence to be see-through (burglars cannot hide behind chain-link, after all).\nInstallation is easy. Most of the fittings are tightened down with a socket wrench. The only special tool you may need is a second pair of hands to assist in pulling the mesh tight. While it’s often possible to re-stitch damaged mesh fabric, repairing a chain-link fence is relatively easy.\nAnother pro: Chain-link fencing is considered “green,” since any scrap metal dealer will be happy to receive (and may even pay for) one you’re discarding. Try that with an old vinyl or composite fence!\nInevitably, chain-link fencing possesses a utilitarian aesthetic, but style options exist. Different mesh sizes and wire gauges are available, and the polymer coatings now come in colors, such as brown, green, and black (any of these provides a softer look than silver). When installed among shrubs or along the border of wooded areas, it’s possible for a chain-link fence to be nearly invisible, especially if outfitted with fabrics or lattice panels.\nGet breaking and town-specific news sent to your phone. Sign up for text alerts from the Kane County Chronicle."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:562998a4-7b92-4e74-acb4-68f220b7e184>","<urn:uuid:d66052df-259e-4a15-ba12-86d463e3c937>"],"error":null}
{"question":"I'm evaluating earthquake safety measures - what's the difference between traditional pile design approaches and modern soft-story retrofit solutions?","answer":"Traditional pile design approaches only considered inertia forces at the pile cap, but modern analysis shows that greatest damage occurs at interfaces between ground layers of different stiffness. Modern solutions include increasing longitudinal steel or adding steel hooping in critical areas of concrete piles, or using more ductile steel piles. For soft-story retrofits, solutions include conventional plywood shear walls where space permits, or pre-manufactured narrow light gage/structural steel shear wall panels and Special Moment Frames that can endure significant inelastic deformation in areas with limited wall length.","context":["Building codes for designing structures in earthquake zones have until now focused on the superstructure, with little regard for how the foundations perform. This is because codes are geared up to prevent loss of life and very few deaths in earthquakes have been attributed to foundation failures.\nConsequently it is quite possible to design and build structures that can safely withstand any feasible earthquake.\nHowever the bigger issue in countries with mature earthquake engineering, such as Japan and US, has recently switched on to how to avoid huge repair costs.\nIn the California Northridge earthquake of 1994, loss of life was minimal, and in this respect the US seismic codes could be considered to have fulfilled their purpose. Yet the cost of damage was huge, in the order of $200bn, with much of the cost coming from work to repair or replace damaged foundations. From this has come the recognition by building owners that the existing codes are not acceptable. And for insurance companies the high sums are not acceptable.\nSuddenly building codes are changing rapidly, and European code writers are leading the way. The draft of Euorcode 8 Design provision for earthquake resistance of structures is the first to give more explicit reference to designing more earthquake resistant foundations.\n'Seismic engineers have in the past not thought too deeply about foundations. Most seismic codes are based on US practice, in which foundations were ignored or were a non-issue,' says Ted Piepenbrock of consultant Ove Arup.\nConventionally, when it comes to foundation design, seismic engineers have looked only at inertia forces, these being the forces the superstructure, in moving, exerts on the foundation.\nEurocode 8, codifies for the first time, the concept of kinematic forces in the piles resulting from the soil pile interaction during dynamic ground movements.\n'What we are seeing is a move towards performance based approach to seismic engineering,' says Piepenbrock.\nPreviously seismic codes assumed that as far as piles were concerned, the maximum shear on the piles resulted from inertia forces and occurred at the underside of the pile cap. But from recent efforts to monitor yield and cracking in piles, it is clear the problem has been under thought.\nNow developments in computing power mean that what was, until recently, painstaking university research is now within the capabilities of the best equipped consultants.\nAccording to Piepenbrock 'guessing the answer is no longer acceptable'.\nConsultants using methods such as non-linear dynamic soil-structure analyses can now get a reasonable idea of how the foundations behave during an earthquake. Such approaches have shown that greatest pile damage is unlikely to occur at the top of the pile, but at the interface between ground horizons of different stiffness. This observation is with hindsight hardly startling.\nOn a practical level the implications for pile design are surprisingly straightforward. A pile may include increased longitudinal steel or contain steel hooping to confine the concrete in the area at levels in the pile where the greatest bending moments are predicted.\nAlternatively, steel piles are more ductile than concrete and so provide another option.","It is common knowledge that California is vulnerable to earthquakes. Geological records show a pattern of seismic events that highlight areas most susceptible to an earthquake and give the expected intensity. Recent experience with California earthquakes within our lifetime have demonstrated the structural vulnerabilities in building types described as “weak” or “soft-story” structures. Having identified the more vulnerable building type, we must ask what corrective measures can be taken to strengthen them in order to avoid a housing catastrophe on a similar scale to Hurricane Katrina?\nSoft-story structures (also known as weak-story) are identified as buildings with inadequate stiffness (soft) and/or strength (weak) at the first floor to prevent significant damage or collapse in an earthquake. These structures have been designed for open, common space at the first floor to accommodate parking cars or for use as retail space. The first floor framing along open areas is typically post and beam construction designed to support vertical loads from the structure above but providing very little lateral support. Levels above the first floor are typically apartment spaces that have more walls dividing the living area, which provides more lateral strength and stiffness than is available in the soft/weak first story.\nIn an effort to increase the resiliency of the city and decrease the potentially devastating economic fallout from a significant earthquake, the City of San Francisco adopted a soft-story ordinance in 2013. With a similar goal, the City of Los Angeles adopted a comparable mandate two years later. Other jurisdictions — including the cities of Oakland, Berkeley, Fremont, Alameda, and Santa Monica — also have adopted policies to identify, evaluate, and/or strengthen vulnerable buildings.\nThese soft-story retrofit ordinances concern multiple-unit, multifamily wood-framed residential structures such as apartment buildings, condominiums, and residential/retail mixed use that were common construction prior to the adoption of the 1978 building code provisions. Typically the first floor space in these buildings has one or more exterior faces with very narrow wall lengths, if there are any walls at all. The lack of lateral force-resisting elements along the exterior face makes the building very susceptible to excessive damage or collapse in the event of a significant earthquake.\nScope of timeline of ordinances\nThere are currently 18,000+ wood frame buildings identified that will require retrofitting under their local ordinance. The LA Times reported that in the City of Los Angeles alone there are an estimated 13,500 buildings identified as soft story, all of which are required to be seismically retrofit.\nThe amount of time building owners have to complete the seismic upgrades varies depending on the city in which they are located, building size, and number of occupants. In Los Angeles, owners have as long as seven years to complete retrofit requirements. In San Francisco, the due date depends on the type of building (http:// sfdbi.org/softstory). Type I buildings defined as “any building containing educational, assembly, or residential care facility uses” must be retrofit by Sept. 17, 2017. Type IV buildings (“any building containing ground floor commercial use”) require permit applications to be submitted by Sept. 15, 2018 and the work to be completed by Sept. 15, 2020.\nWhile there is not a single lateral force structural solution that works for all buildings nor, in many cases, is there a single solution that works within all locations of a particular building, there is a small group of lateral force-resisting elements that are most often considered. Where space permits, conventional plywood shear walls are often added to increase the lateral force-resisting capacity of the building for seismic loads.\nFor the more common conditions where long lengths of wall are not available, pre-manufactured, narrow light gage or structural steel shear wall panels may be appropriate. Given that many softstory structures are two or more stories above ground level, have narrow vertical supports, and need to protect parking spaces, Special Moment Frames (SMFs) designed to endure the “significant inelastic deformation” caused by earthquakes are frequently the best solution.\nThe Hardy Frame prefabricated SMF was made available in 2008 using the SidePlate SMF connection, the first to be approved by AISC’s Connection Preapproval Review Panels for inclusion in the AISC 358 Prequalified Moment Connection Standard. Connections that are included in AISC 358 have gone through a rigorous review by industry experts and academia and have been proven to meet seismic design requirements and performance expectations of the building standards.\nAdditionally for the Hardy Frame SMF with SidePlate Connections, AISC 358 includes the use of square and rectangular hollow structural section (HSS) beams. The inclusion of HSS sections was based on full-scale seismic testing of Hardy Frame moment frames with HSS beams included. The tests highlighted that lateral bracing is not required as it is with other SMFs using conventional wide-flange beams.\nAside from design advantages, the ability to have SMFs delivered to the jobsite completely preassembled reduces installation time and eliminates onsite welding along with the special inspection it requires. Installation can be as easy as excavating and constructing new concrete pads or grade beams for the anchorage and overturning forces associated with the new lateral force-resisting elements, installing the frame on the anchorage as a single element, and, once in place, connecting the frame to the existing building at the floor level.\nDAVID LOPP is vice president of Technical Support for the Hardy Frame Shear Wall System. (http://www.hardyframe.com/softstory)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:f7824dca-ea4a-4390-b723-6cbc5224e14c>","<urn:uuid:b4b9e7ac-25ab-4ad2-b5b1-02aa8d9d08e3>"],"error":null}
{"question":"Which vibration measurement technique requires more complex equipment setup: the combine harvester's mount stiffness method or the Modal Exciter 100 lbf system?","answer":"The combine harvester's mount stiffness method requires more complex equipment setup, as it needs acceleration measurements on both engine side and mainframe side, force sensors, and complex data processing including FFT analysis with 30-time averaging. In contrast, the Modal Exciter 100 lbf features a simpler setup with a through-hole armature design, chuck and collet attachment system, and straightforward connection to force sensors using modal stinger rods or piano wire stingers.","context":["Engine Development Division\nPower System Operations Business\nYANMAR Technical Review\nVibration Transfer Path Analysis for Combine Harvester Using Multibody Dynamics for EngineHybrid Method Combining CAE and Experiment\nDiesel engines are the main power source used in agricultural and construction machinery and also a source of vibration. Engine vibration resulting from combustion pressures in the cylinder, the motion of the crankshaft system, and other sources transfers via the engine mounts to the machine, influencing the vibration of the entire vehicle. To reduce the vibration at evaluation points represented by the operator position, it is important to take any measures to the most contributory vibration transfer path among the vibration transfer paths from the source to the evaluation points. The transfer path analysis is used as a method to analyze the contribution of vibration transfer paths.\nIn the transfer path analysis, the vibration system is divided into two parts, and the part of the vibration source such as the engine and other is treated as the active part and the transfer path including evaluation points such as the operator position is treated as the passive part. To perform the transfer path analysis, inputs from the active part to the passive part and Frequency Response Functions (hereafter FRF) from the input points to the evaluation points are required. The FRFs can be measured by excitation tests using an impact hammer or a vibration exciter. In contrast, it is often difficult to directly measure the input to the passive part. This is because force sensors are required to be fitted at the input points but there is special constraints during the experiments. Instead, the mount stiffness method or the inverse matrix method are widely used for indirect input identification(1). However as these input identification methods are experimental methods, the input parameter study of vibration at the evaluation points cannot be performed.\nProgress in CAE in recent years has made it possible to analyze engine dynamic behavior using multibody dynamics(2) (3). This makes it possible to obtain the transmitted vibration force from the engine to the machine with good accuracy(4) (5).\nThis paper describes a hybrid transfer path analysis method using the transmitted vibration force obtained by engine multibody dynamics and the FRF obtained experimentally. The advantage of the proposed hybrid method is shown by applying it and the current mount stiffness method to the vibration at the operator position of a conventional combine harvester.\n2.1. Theory of Transfer Path Analysis\nThe transfer path analysis is a method to identify contributory paths for the response at the evaluation points(1). The response vibrationat the evaluation pointis expressed by the following equation using the inputto the path\nHere,is the angular velocity,is the FRF for the pathfrom the input point to the evaluation point, andis the contribution vibration by the path.\nThe contribution of each transfer path to the evaluation point can be defined as the projection of contribution vibrationon the response vibration, which can be expressed as follows.\nHere,is the contribution of the pathto the vibration at the evaluation point, andis the conjugate complex of\nThe mount stiffness method is used to compare with the proposed method as an input identification method. This method performs input identification by using the displacement difference between the active and passive parts at the input point and the complex dynamic stiffness at the input point. The input of the mount stiffness method can be expressed as follows.\nHere,is the complex dynamic stiffness at the input point,is the active part displacement at the input point for the path, andis the passive part displacement at the input point for the path,is the active part acceleration at the input point for the path, andis the passive part acceleration at the input point for the path.\n2.2. Combine Harvester Used in Experiment and Vibration Transfer Path\nFig. 1 shows the conventional combine harvester used in the experiment. The evaluation points are the seat mounting position, floor, and steering. The evaluation condition is the vibration at the engine idling while the thresher and reaper are not operating. Fig. 2 shows the engine mounting layout. The engine room is located below the operator’s seat and an inline 3-cylinder vertical water-cooled diesel engine (see Table 1) is installed. The engine is mounted to the mainframe by vibration isolating rubbers (hereafter engine mount) at four points. These engine mounts are mounted at an angle of 30° to the horizontal plane. The designations of engine mounts are shown in Fig. 2. Totally 12 paths which are three-axis directions (X, Y and Z direction) for four engine mounts are considered as the vibration transfer paths. X direction is the axial direction of the crank shaft, Z direction is the compression direction of the engine mount and Y direction is the direction of perpendicular to X and Z.\n2.3. Analytical Model of Transmitted Vibration Force\nThis section describes a multibody dynamics model that calculates the transmitted vibration force from the engine to the main frame of the combine harvester. Fig. 3 shows an overview of the multibody dynamics model. The definition of engine components and joints connecting them are as follows.\n- Crankshaft, engine body: Flexible body model with reduced degrees of freedom using the Craig-Bampton method\n- Piston and connecting rod: Mass properties and center of gravity obtained by a 3D model\n- Cylinder pressure: Experimental data\n- Main bearing and crank pin bearing: Rotating joint\n- Thrust bearing: Constrained in the crank shaft axial direction only\n- Piston-cylinder block: Translational joint\n- Engine mount: Considers characteristics of frequency dependence and amplitude dependence\n- Mainframe: None (ground part)\nThe components are located in 3D space and connected by joints. Fig. 4 shows the engine mount model used in multibody dynamics. A generalized Maxwell model is used to express the frequency dependent characteristic of the engine mount (6). To represent the amplitude dependent characteristic of the engine mount, a non-linear spring which load varies with displacement is added to the generalized Maxwell model. Fig. 5 shows the complex dynamic stiffness of the engine mounts measured in unit test(7) and the complex dynamic stiffness used in the analytical model. The number of Maxwell models is defined as three, with the spring and dashpot values identified from the results of the unit test of engine mounts.\n2.4. Identification of Transmitted Vibration Force and Measurement of Frequency Response Function\nTo compare with the transmitted vibration force obtained by multibody dynamics, the transmitted vibration force while the engine is operating is identified using the mount stiffness method. As shown in Fig. 6, the acceleration of the engine mounts is measured on both the engine side and mainframe side. The measured acceleration signals are then converted from the time to the frequency domain using a Fast Fourier Transform (FFT). The frequency analysis resolution is 1 Hz and number of averaging is 30 times. The phase reference for each signal during averaging is the phase of the up and down direction of mount 1. As indicated by equation (3), the transmitted vibration force is obtained by twice integrating the measured accelerations in the frequency domain and multiplying by the complex dynamic stiffness.\nAn impact hammer and accelerometer are used for the measurement of the FRF as shown in fig. 7. Because it is not possible to excite directly to the engine mount positions, the FRFs are measured using the reciprocity principle. Using the reciprocity principle, the FRFfrom the input point to the evaluation point is expressed by the following equation.\nHere,is the response acceleration at pointwhen the pointis excited by the forceandis the response acceleration at pointwhen the pointis excited by the force.\n3. Results and Discussion\n3.1. Transmitted Vibration Force\nThis section describes the accuracy of the transmitted vibration force obtained by the multibody dynamics model. Fig. 8 compares the transmitted vibration forces obtained from the multibody dynamics and from experiment. As the results obtained by multibody dynamics are in the time domain, they are converted to the frequency domain by FFT to compare. The engine speed is 1500 min-1 at idling. The results show that the peak transmitted vibration forces occur at half-order harmonics of the engine speed. In particular, the peaks are large at the first order (25 Hz) and the 1.5th harmonic (37 Hz). These are the same to both the analytical and experimental results. A comparison of peaks shows a good agreement between analytical and experimental results.\n3.2. Transfer Response Function Measurement Results\nThe reciprocity principle is used for the FRF measurement. At the beginning of the test, it is confirmed that the reciprocity principle can apply. Because it is not possible to excite at the engine mount positions, the applicability of the reciprocity principle was confirmed by exciting to the frame near the engine mounts. Fig. 9 shows the results of confirming reciprocity principle for the FRF from the vicinity of the engine mounts to the floor. The solid lines in the figures indicate the FRF when the input point is near the engine mount and the response point is the floor. The dashed lines indicate the FRF when the input point is the floor and the response point is near the engine mounts. A comparison of the FRFs show good agreement below 150 Hz for both amplitude and phase, and approves the reciprocity principle.\nFig. 10 shows the measured FRFs. Figs. 10a and 10b show the amplitude and the phase respectively and Fig. 10c shows the coherence. The coherence value is close to 1.0, except below 10Hz and at anti-resonance points, indicating a good correlation between input and output. Reasons for poor coherence are considered to be the low level of the response signal at anti-resonance points, and insufficient excited force below 10 Hz.\n3.3. Transfer Path Analysis Results\nThis section describes the results of the vibration transfer path analysis for the operator position. Fig. 11 shows the contribution of each path to the vibration at the floor position. Fig. 11a is the contribution based on the inputs obtained using multibody dynamics and Fig. 11b is the contribution based on the inputs identified using the mount stiffness method. The actual vibration amplitudes are also shown at the top of each figure. The main frequency of the actual vibration is 25Hz that is the first order component of the engine speed. Similarly, 25 Hz component of the total vibration amplitude by the transfer path analysis is big. When focusing the 25 Hz vibration, the most contributory path is the Z direction of mount 3. X direction of mount 4 shows negative contribution. For 37 Hz vibrations which is the 1.5 order component of the engine speed, the contribution of the Y direction of mount 1 is biggest, and the Y and Z directions of mount 3 show negative contribution. Both the mount stiffness method and proposed method show the same tendency. Figs. 12 and 13 plot the contribution vibration of each path on the complex plane for 25 Hz and 37 Hz respectively. The actual vibration measurement values are also plotted in the same figures as well. The phase reference for each path is the Z direction of mount 1. A comparison of theses methods shows good agreement for both amplitude and phase. These results reveals the effectiveness of the vibration transfer path analysis based on inputs obtained from multibody dynamics.\n3.4. Prediction of Vibration Level\nThe vibration level at the operator position is predicted using the transmitted vibration force obtained by multibody dynamics and the measured FRF. Fig. 14 shows a comparison of measured and predicted vibration levels. The vibration of the steering is corrected by hand-transmitted vibration weighting factor(8), and the vibrations of the seat mounting position and floor are corrected by whole-body vibration weighting factor (9). The measured and predicted vibration levels show good agreement, with differences being within 3dB. It can be said that the vibration level of the operator position can be predicted using multibody dynamics for engine and measured FRFs.\nThe effectiveness of the hybrid transfer path analysis method using transmitted vibration forces obtained by engine multibody dynamics and FRFs obtained experimentally is confirmed. The conclusions are as follows.\n- The contributions of each path to the vibration in the operator position obtained using the proposed method show the same tendency as those obtained by the current mount stiffness method. The proposed method and mount stiffness method also show good agreement for the amplitudes and phases of each path.\n- The vibration level of the operator position predicted using the proposed method agrees with measurements to an accuracy of within about 3 dB. It is possible to perform a parameter study of the inputs for the vibration level of the operator position.\n- (1)Peter, A. G., “Advanced Transfer Path Analysis Methods”, 2012, LMS Japan, p.7-12 in Japanese.\n- (2)Raub, J. et al., “Analytical Investigation of Crankshaft Dynamics as a Virtual Engine Module”, 1999, SAE Technical Paper 1999-01-1750.\n- (3)Sonntag, H-D. et al., “Acoustical Optimization based on modern CAE tools”, 2001, JSAE Annual Congress Proceedings, NO.59-01, p.1-4.\n- (4)Kannan, M. et al., “Analytical Prediction and Measurement of Engine Mount Forces”, 2007, SAE Technical Paper 2007-32-0105.\n- (5)Masahiro Akei, et al., “Study of Experimental and Simulation Method for Prediction of Engine Transfer Force” 2012, Dynamic and Design Conference 2012, NO.745 in Japanese.\n- (6)David, I.G. Jones, “Handbook of Viscoelastic Vibration Damping”, 2003, Maruzen, p.49-52 in Japanese.\n- (7)JIS K 6394: 2007, Rubber, vulcanized or thermoplastic -- Determination of dynamic properties in Japanese.\n- (8)ISO Standards 5349-1: 2001, Mechanical vibration -- Measurement and evaluation of human exposure to hand-transmitted vibration.\n- (9)ISO Standards 2631-2: 2003. Mechanical vibration and shock -- Evaluation of human exposure to whole-body vibration.\nThe original technical report is written in Japanese.\nThis document was translated by R&D Management Division.\nEngine Development Division","Modal Exciter 100 lbf\nWhen performing experimental modal analysis, the choice of excitation function and system can make the difference between a good measurement and a poor one. For many modal test applications, an electrodynamic shaker system is best suited for creating an appropriate input forcing function. Distributing adequate input force energy across the test structure and obtaining accurate and reliable input force measurements is critical for successful modal testing. This often requires a shaker that is highly portable, rugged and easy to setup in order to facilitate the best exciter location (relative to the test structure) while minimizing any unwanted interaction between the exciter and test structure.\nThe Modal Shop 2100E11 Modal Shaker, a lightweight electrodynamic modal exciter, is capable of providing 100 lbf (440 N) of peak force excitation in a small footprint weighing just 33 pounds (15 kg). With a 1” stroke and useful frequency range beyond 5400 Hz, the 2100E11 is suitable for structural testing and experimental modal analysis applications, including single and multiple inputs (SIMO and MIMO) using random, burst random, sine dwell or chirp excitation signals.\nThrough-hole armature with chuck and collet attachment provides simple setup with modal stingers.\nLightweight and portable – weighing just 33 lbs (15 kg).\nTrunnion base provides flexibility when choosing best exciter location(s).\n1” stroke and broad frequency range supply adequate input energy for most modal test applications.\n- Forced air cooling sufficient to meet full shaker performance (100 lbf pk) specifications.\nThe 2100E11 Modal Shaker is supplied in a trunnion base allowing full rotation for easy setup. The through-hole armature design with chuck and collet attachment is ideal for use with either traditional modal stinger rods, like the TMS 2150G12 1/16” diameter stingers, or piano wire stingers, like the TMS K2160G Piano Wire Stinger Kit with force sensor quick disconnect. These stingers greatly simplify test setup with an easy connection to the force sensor and test structure, and help decouple cross-axis force inputs, minimizing or eliminating input force measurement errors while using the modal exciter. For horizontal force inputs, the 2100E11 adapts directly to The Modal Shop’s 2050A Lateral Excitation Shaker Stand. Using four turnbuckles, the modal shaker is supported by its trunnion base and can be adjusted over a wide horizontal and vertical area relative to the test structure.\nFor a discussion of Frequently Asked Questions (FAQ) on Modal Shakers or related force excitation topics, please click here.\n2100EA13 Shaker Cable, 20 ft (6 m).\nShaker Accessory Kit: includes 2100E14 chuck with collets, 2100EA07 10-32 adapter, 2150G12 modal stingers (1/16” diameter rod with 10-32 threaded end, pack of three), 2155G12 modal stingers (3/32” diameter rod with 10-32 threaded end, pack of three), K2160G Piano Wire Stinger Kit (includes quick disconnect load cell adapter), and spare fuses & flexures.\nSUGGESTED ACCESSORIES AND RELATED EQUIPMENT:\n2050E05 Shaker Amplifier, selectable voltage / current control\n2100E21 SmartAmpTM Amplifier\n2100E18 Power Amplifier\n2050A Lateral Excitation Stand\n2100E13 Modal Accessory Kit, for use with 2050A excitation stand\n2050E03 Cooling Package, 110V portable\n2100E16 Cooling Package, 220V portable\nPCB 288D01 ICP® Impedance Head driving point sensor\nPCB 208C01 and PCB 208C02 ICP® force sensors"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:b79e1294-7bf6-4b25-86c9-4c047b1c7075>","<urn:uuid:1d491c66-ee58-4150-bdaa-3845ce7ebd2c>"],"error":null}
{"question":"I'm researching physiological changes in organisms. Can you explain how animals develop specialized adaptations over time for survival, and how these compare to the adaptation mechanisms seen in coastal ecosystems?","answer":"Organisms develop various specialized adaptations for survival. In animals, we see specialized neurons that have adapted to detect small changes in chemical concentrations in specific body areas like the respiratory system and carotid artery. Similarly, in coastal ecosystems, organisms have developed specialized adaptations - for instance, seagrasses have evolved specialized systems for carbon capture, being able to store carbon in their roots and sediment, making them highly efficient carbon sinks. Both these examples show how organisms develop specific physiological and anatomical features in response to their environmental needs, whether it's for sensing chemicals or storing carbon.","context":["Anatomical Adaptations of Xerophytes: ø Most of the cases, the stem will be photosynthetic and contains chlorenchymatous cells in the outer cortex. ø In the stem of Casuarina, the chlorenchymatous cells are radially elongated and palisade like tissue in appearance.\nAnatomy Chapter 18. Specialized neurons that can detect small changes in concentration of specific chemicals or compounds In general: Respond only to water-soluble and lipid soluble substances dissolved in surrounding fluid Found in respiratory system, carotid artery, medulla oblongata, aorta Important for these receptors Carotid Bodies.\nPhysical Adaptations (these include ones that effect the population) Behavioral Adaptations (these include behavioral adaptations that effect the population) Habitat With Dense Vegetation A behavioral adaptation of the Bengal tiger is moving into areas with dense vegetation. The tigers do this because the kind of animals.\nOrganisms Vs Non Organisms Define Observation In Scientific Method Jun 14, 2017. The scientific method has four steps: 1.Observation and description of a phenomenon (a concept), 2.Formulation of a hypothesis to explain the. scientific method is the process created in the seventeenth century through which hypotheses are developed, tested and either proven or disproven. Scientific observation is the central element of scientific method or process. After you have asked the question give a possible answer; Try to explain what was. Darwin S Biggest Crocodile\nThe salt treatments used were: control (no salt), 50, 100, 150 and 200 mM of NaCl in Hoagland’s nutrient solution. salt stress than its counterpart from the Faisalabad region. Anatomical.\nmarkings and anatomical features, but also used the latest DNA methods to create unique genetic fingerprints for most of the species in the form of DNA barcodes. What’s in a name? A particular.\nAlthough most of the above‐mentioned adaptations have been discussed at length in previous publications, the articles in this special issue present some new findings regarding aquatic adaptations. This special issue focuses on a common hypothesis: the described anatomical specialization confers a selective advantage to an aquatic existence.\nlight adaptation adaptation of the eye to vision in the sunlight or in bright illumination (photopia), with reduction in the concentration of the photosensitive pigments of the eye. physiological adaptation the ongoing process by which internal body functions are regulated and adjusted to maintain homeostasis in the internal environment.\nOne of the definitions of AA is \"Anatomical Adaptation\". Q: A: What does AA mean?. means \"Anatomical Adaptation\". Q: A: What is shorthand of Anatomical Adaptation? The most common shorthand of \"Anatomical Adaptation\" is AA. APCOTS – Anatomical Pathology Commercial Off the Shelf; ATC/DDD – Anatomical Therapeutical Chemical Classification.\nAA stands for Anatomical Adaptation. AA is defined as Anatomical Adaptation somewhat frequently. Printer friendly. Menu Search. New search features Acronym Blog Free tools. Proust moves to further explore the anatomical adaptations made to accommodate for reading development.\nIt’s the evolutionary equivalent of using a pretty over-confident dating profile to impress potential partners. When an anatomical structure appears. that is a by-product from some other functional.\nJack Tseng, an evolutionary biologist and assistant professor of pathology and anatomical science in the Jacobs School of Medicine and Biomedical Sciences at UB, talks to UBNow about why this finding.\nGeneral Adaptations. Another anatomical adaptation which allows the saguaro to survive in the desert is it’s spines, which are modified leaves and are common amongst most cacti plants. The spines of the saguaro protect it from animals that would otherwise eat the saguaro, or use it as a water source, and they help provide protection from the sun.\nAtomic X9 For Sale Jan 30, 2019. Atomic Country Commercial Manager Jake Strassburger says the new Atomic Redster X9 WB is perfect for ski instructors because, “The overall. As if being an awkward, bullied 15-year-old isn’t bad enough, “Fergie” Ferguson suddenly discovers he can see dead people. Well, one dead person specifically—the ghost of a certain punk rocker named. Search the history of over 351 billion web pages on the Internet. Cessna brought its new Corvalis TTx cockpit mockup to Oshkosh this year. The\nLearn adaptation biology with free interactive flashcards. Choose from 500 different sets of adaptation biology flashcards on Quizlet.\nAnatomical Adaptation definition, categories, type and other relevant information provided by All Acronyms. AA stands for Anatomical Adaptation Search for acronyms, abbreviations,\nhomology and other broad themes—while Gries’ striking photos isolate the essence of each animal’s unique adaptations. Simultaneously, though, the photos highlight the common anatomical characteristics.\nBasilosaurids and dorudontids showed further aquatic adaptations of the ossicular chain and the acoustic. 1976;Uhen, 2004). Anatomical evidence for low frequency sensitivity in an archaeocete.\nTendons and ligaments grow strong through anatomical adaptation. Anatomical adaptation prepares the body for work by increasing tendon and ligament strength and correcting muscular imbalances. When commencing a new training program it is necessary to prepare the.\nThere’s little squirrels won’t sink their teeth into—and. but even tree squirrels are equipped with extraordinary anatomical adaptations that enable them to clear ten-foot gaps between branches and.\n\"Said brain connectivity patterns can serve as biological indicators for predicting the risk of the appearance of behavioral problems and social adaptation difficulties. (2014, July 22). Children’s.\nThe dinosaurs, of course, were the major draw when I picked up the novel in advance of the screen adaptation. bone that’s sitting in your throat, hidden between your neck vertebrae and your lower.\nA: Behavioral adaptation is the process by which an organism or a species changes its pattern of action to better suit its environment. It is contrasted with structural adaptation, which is the appearance of physical features that confer an advantage upon a species. Continue Reading.\nIn addition to anatomical. adaptations of their ancestors, but other inherited traits limited the ways they could climb through the canopy. For more on sloth evolution and locomotion, see this.\nIts a phase of training that is usually done early on. Anatomical adaptations focus on circuit training with the load intensity being around 40% to 60% of a 1RM.\nAnatomical adaptation is what some refer to as general conditioning. Preparing the body for activity and more specific training is critical to preventing injuries and physically preparing for the activities of softball.\nNancy It’s true there’s a serious need to address CTE. Woodpeckers show a number of anatomical adaptations that help distribute the energy of repeated high-intensity impact. But from a.\nThree years ago, when Harvard biologist Jonathan Losos settled in at the Geological Lecture Hall for a talk by fellow scientist Richard Lenski, he was toying with the idea of writing a book on.\nSecure fixation of these implants in bone is essential for the procedure’s success. so they cannot measure bony ingrowth.\n“It’s not so much that what we thought we knew about sauropods. feet long and about ten tons—a staggering size for an animal that preceded the sauropod anatomical “toolkit.” These older cousins of.\nInstead a settled for a few cherished paperbacks, including George Gaylord Simpson’s posthumously-published sci-fi yarn The. Despite the lack of anatomical change seen among pelicans, though, it.\nEvolution of Human Adaptations • Humans face basically the same adaptive challenges as all organisms. earth’s terrestrial habitats Expansion of NE Asian people into the Americas around 14,000 years ago Polynesian expansion: the last great spread of\nFull Answer. A bird in high altitude adapts to use less oxygen, while a camel adapts to the desert to store nutrients. Another way that animals can physiologically adapt is through their predations strategies. Snakes physiologically adapted to their environments by evolving to produce venom. The BBC says that animals must physiologically adapt.\nAdaptations are special characteristics that an organism is born with and which enable it to survive in its natural habitat. Adaptations are not developed in the course of an organism’s life. The following adaptations show that the camel is specially suited to live in.\nAnswer Wiki. Physiological adaptations are changes to the way an animal functions in response to its environment. So, for example, an animal living in a cold climate can have physical adaptations, such as thick fur and short ears to reduce heat loss, but a physiological adaptation might be shivering to generate more heat when it is really cold.\nCompare Historian And Geographer Due to a special event taking place in the U.S. Capitol on Wednesday, April 3, 2019, areas regularly included in the tour of the Capitol, such as the Rotunda and National Statuary Hall, will not be available for tours until the conclusion of the event at approximately 12:30 p.m. 4 Organisms In A Food Chain Government scientists have not identified a food item, grocery store or restaurant chain as the source of these infections. in foods and in the intestines\nBenefits of the Anatomical Adaptation phase. It has been shown that when players add an individualised conditioning programme that takes account of the individual player’s physical needs in addition to their team skill and practice training that greater gains in terms of fitness occur (Andrzejewski et al, 2011, Rhea et al, 2009, Gilligan et al, 2005).\nBut the men have not been taught how to handle that or how to keep up with the modern, forward-thinking woman” I was recently working on a play that is an adaptation of Shakespeare. is what and.\nCan You Pronounce Geneticist Compare Historian And Geographer Due to a special event taking place in the U.S. Capitol on Wednesday, April 3, 2019, areas regularly included in the tour of the Capitol, such as the Rotunda and National Statuary Hall, will not be available for tours until the conclusion of the event at approximately 12:30 p.m. 4 Organisms In A Food Chain Government scientists have not identified a food item, grocery store or restaurant chain as the source of these infections. in foods\nPeer Review Psychology Journals Health Psychology ® is the official scientific journal of the Society for Health Psychology (Division 38 of the American Psychological Association). Its mission is to advance the science and practice of evidence-based health psychology and behavioral medicine. It publishes peer-reviewed articles on. American Journal Of Hypertension Salt Meta Analysis A bit of salt can make certain healthy foods, particularly bitter vegetables, far more palatable. Considering the evidence I’ve presented in this series, I believe that salt restriction for the general\nas well as the Yukon giant camel, which is thought to be Paracamelus, the ancestor of modern camels. The collagen information, combined with the anatomical data, allowed the group to conclude that the.\nAmerican Journal Of Hypertension Salt Meta Analysis A bit of salt can make certain healthy foods, particularly bitter vegetables, far more palatable. Considering the evidence I’ve presented in this series, I believe that salt restriction for the general population is not only unnecessary, but potentially dangerous. Now, I’d like to hear from you. And for the same decades, a vocal opposition has challenged the guidelines as unscientific: No solid evidence directly links salt intake to heart disease over the long term. The vitriol of the salt. Approximately\nThen there’s Red and the Big Bad Wolf; their tale, which occurs in Act One, is sexually charged, a point that is further made by the wolf’s anatomically correct costume. that was changing for the.\nAnd anyway, it’s possible that their divergence from our most recent common ancestor brought with it some fundamental anatomical or cognitive adaptations that make human-like intelligence incredibly.\nIn the ancestors of remoras, such an anatomical fluke may have allowed. Sometimes, when we look at an adaptation in living animals, it seems to exquisitely well-suited to the animal’s life that we.\nNov 19, 2013 · Anatomical Adaptation of Animals UNTV Web. Loading. Unsubscribe from UNTV Web?. Animal Adaptations for Kids -Lesson with Quiz – Duration: 7:16. makemegenius 296,116 views.","When we think about carbon and climate change, our minds often go to fossil fuels, deforestation, and greenhouse gas emissions. However, there’s another side to the carbon story that’s just as important – the role of coastal ecosystems in storing and sequestering carbon. This is where the concept of “Blue Carbon” comes into play.\nThe Role of Blue Carbon in Climate Change\nCarbon Sequestration in Coastal Ecosystems\nBlue carbon refers to the carbon captured and stored by the world’s coastal and marine ecosystems. These ecosystems, including mangroves, seagrasses, and salt marshes, act as carbon sinks, capturing and storing large amounts of carbon dioxide from the atmosphere. In fact, they can sequester carbon at a rate up to 35 times faster than tropical rainforests.\nThe Blue Carbon Initiative\nRecognizing the importance of these ecosystems, the Blue Carbon Initiative was launched to promote their conservation and restoration. The initiative brings together governments, research institutions, NGOs, and communities to protect and restore these vital ecosystems, which not only help in climate change mitigation but also provide numerous other ecosystem services such as biodiversity conservation, water purification, and coastal protection.\nCoastal Ecosystems and Blue Carbon\nMangroves are one of the most important blue carbon ecosystems. These unique trees, which grow in intertidal zones, have complex root systems that trap sediments, slowing down water flow and allowing organic material to settle. This creates a carbon-rich soil that can store significant amounts of carbon.\nSeagrasses, another vital blue carbon ecosystem, are flowering plants that grow in shallow coastal waters. They have the ability to capture and store large amounts of carbon in their roots and sediment, making them highly efficient carbon sinks.\nSalt marshes, found in the intertidal zone of coastal areas, are another important blue carbon ecosystem. These marshes are highly productive, with plants that capture and store carbon in their biomass and soil.\nThreats to Blue Carbon Ecosystems\nOne of the main threats to blue carbon ecosystems is coastal development. As coastal areas become more developed, these ecosystems are often destroyed or degraded, leading to the release of stored carbon back into the atmosphere.\nPollution, especially nutrient runoff from agriculture and sewage, can also harm blue carbon ecosystems. Excess nutrients can lead to algal blooms, which deplete oxygen levels in the water, harming seagrasses, mangroves, and other organisms.\nClimate change poses a significant threat to blue carbon ecosystems. Rising sea levels and temperatures can lead to the loss of these ecosystems, while increased storm intensity can cause physical damage.\nConservation Efforts for Blue Carbon Ecosystems\nTo counter these threats, various restoration projects are underway to bring back lost or degraded blue carbon ecosystems. These projects involve planting mangroves, seagrasses, and salt marshes, and are often done in collaboration with local communities.\nPolicy measures, such as designating protected areas and creating incentives for conservation and restoration, are also crucial in protecting blue carbon ecosystems. By recognizing the value of these ecosystems, we can ensure their conservation and restoration, leading to long-term benefits for the climate and biodiversity.\nThe Future of Blue Carbon\nThe concept of blue carbon is still relatively new, but it’s gaining recognition and momentum. As we continue to understand the importance of these ecosystems in climate change mitigation, we can expect more efforts to conserve and restore them. With collaboration between governments, researchers, NGOs, and communities, we can protect and enhance these vital ecosystems for future generations.\n- Biology Niche: Exploring the Fascinating World\n- Biofuels: The Future of Sustainable Energy\n- Biodiversity: Exploring Nature’s Tapestry\nIn conclusion, blue carbon is a crucial part of the global carbon cycle, with coastal ecosystems playing a key role in capturing and storing carbon. By conserving and restoring these ecosystems, we can mitigate climate change while also enjoying other ecosystem services. The future of blue carbon is promising, and with continued efforts, we can ensure these ecosystems are protected for generations to come.\nWhat is blue carbon?\nBlue carbon refers to the carbon captured and stored by the world’s coastal and marine ecosystems, including mangroves, seagrasses, and salt marshes. These ecosystems act as carbon sinks, helping to mitigate climate change by sequestering large amounts of carbon dioxide from the atmosphere.\nWhy are coastal ecosystems important in capturing carbon?\nCoastal ecosystems, such as mangroves, seagrasses, and salt marshes, are crucial for capturing carbon because they have unique properties that allow them to trap and store carbon in their biomass and sediments. These ecosystems can sequester carbon at a rate up to 35 times faster than tropical rainforests, making them highly efficient carbon sinks.\nWhat is the Blue Carbon Initiative?\nThe Blue Carbon Initiative is a global program that aims to promote the conservation and restoration of coastal and marine ecosystems to enhance their capacity to capture and store carbon. The initiative brings together governments, research institutions, NGOs, and communities to work towards protecting and restoring these vital ecosystems for climate change mitigation and other ecosystem services.\nWhat are some threats to blue carbon ecosystems?\nCoastal development, pollution, and climate change are the primary threats to blue carbon ecosystems. Coastal development can lead to the destruction or degradation of these ecosystems, while pollution, especially nutrient runoff, can harm their health. Climate change, with rising sea levels and temperatures, can also result in the loss of these ecosystems and increased storm intensity can cause physical damage.\nHow can we protect and restore blue carbon ecosystems?\nTo protect and restore blue carbon ecosystems, conservation efforts include restoration projects, policy measures, and community involvement. Restoration projects involve planting mangroves, seagrasses, and salt marshes, often in collaboration with local communities. Policy measures, such as designating protected areas and creating incentives for conservation and restoration, are crucial in protecting these ecosystems. Additionally, raising awareness and involving local communities in conservation efforts are essential for the long-term success of these initiatives."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:a4ba7100-e3c5-40da-a861-7018aac736c7>","<urn:uuid:98890e9d-0424-4365-bfae-27add36ce21c>"],"error":null}
{"question":"Which poses a greater risk to pets during holidays: turkey bones at Thanksgiving or candy during Halloween?","answer":"Both pose significant risks but in different ways. Turkey bones can be deadly if pets access the carcass from the carving table or trash, while holiday candy presents multiple hazards - chocolate is harmful for pets, candy with xylitol sweetener can be deadly, and candy wrappers risk tearing the esophagus or intestines if swallowed.","context":["THANKSGIVING PET HAZARDS\nThanksgiving is the beginning of the holiday season. There are some hazards that we have to be mindful of when preparing food we feed our guests. Not hosting for Thanksgiving? We have some helpful travel information for you as well.\nThis can cause a condition called pancreatitis. Dog’s bodies can’t digest greasy, fatty foods. Many dogs can vomit, stop eating and become dehydrated.\nChocolate can be harmful for pets, even though many dogs find it tempting and will sniff it out and eat it. The artificial sweetener called xylitol – commonly used in gum and sugar-free baked goods – also can be deadly if consumed by dogs or cats.\nYeast dough can cause problems for pets, including painful gas and potentially dangerous bloating.\nA turkey carcass sitting out on the carving table, or left in a trash container that is open or easily opened, could be deadly to your family pet. Dispose of turkey carcasses and bones – and anything used to wrap or tie the meat, such as strings, bags and packaging – in a covered, tightly secured trash bag placed in a closed trash container outdoors (or behind a closed, locked door).\nHosting family or friends\nVisitors can upset your pets. Some pets are shy or excitable around new people or in crowds, and Thanksgiving often means many visitors at once and higher-than-usual noise and activity levels. If you know your dog or cat is nervous when people visit your home, put him/her in another room or a crate with a favorite toy. This will reduce the emotional stress on your pet and protect your guests from possible injury. If your pet is particularly upset by house guests, talk to your veterinarian about possible solutions to this common problem.\nWatch the exits. Even if your pets are comfortable around guests, make sure you watch them closely, especially when people are entering or leaving your home. While you’re welcoming hungry guests and collecting coats, a four-legged family member may make a break for it out the door and become lost.\nIdentification tags and microchips reunite families. Make sure your pet has proper identification with your current contact information – particularly a microchip with up-to-date, registered information. That way, if they do sneak out, they’re more likely to be returned to you. If your pet isn’t already microchipped, talk to your veterinarian about the benefits of this simple procedure.\nTraveling with your pet\nYour pet needs a health certificate from your veterinarian if you’re traveling across state lines or international borders, whether by air or car. Learn the requirements for any states you will visit or pass through, and schedule an appointment with your veterinarian to get the needed certificate within the time frames required by those states.\nNever leave pets alone in vehicles, even for a short time, regardless of the weather.\nPets should always be safely restrained in vehicles. This means using a secure harness or a carrier, placed in a location clear of airbags. This helps protect your pets if you brake or swerve suddenly, or get in an accident; keeps them away from potentially poisonous food or other items you are transporting; prevents them from causing dangerous distractions for the driver; and can prevent small animals from getting trapped in small spaces. Never transport your pet in the bed of a truck.\nPack for your pet as well as yourself if you’re going to travel together. In addition to your pet’s food and medications, this includes bringing medical records, information to help identify your pet if it becomes lost, first aid supplies, and other items.\nInformation proved by AVMA.\nInformation proved by AVMA.","11 of the Most Common Household Health Risks For Pets\nCould the biggest danger to your fur baby be in your own home? Here are 11 health risks for pets you can eliminate right now.\nFlowers and plants that pose a health risk to pets\nWhile they may be pretty, lilies are one of the most poisonous plants for cats. Petside suggests keeping them out of the house (or better yet, purchase artificial flowers). Be aware of symptoms of lily poisoning which include vomiting, lethargy, and loss of appetite. Call your vet as soon as possible if you think your pet has ingested lily. The American Society for the Prevention of Cruelty to Animals (ASPCA) says that without immediate care, cats who eat lily may develop life-threatening kidney failure within 36 to 72 hours of ingestion. (By the way, here are 50 secrets your pet wishes they could tell you.)\nHoliday poinsettias are also dangerous for pets, though not as worrisome as the lily. This doesn’t mean your pet should eat this pretty red Christmas decoration, since doing so will likely lead to stomach pain and discomfort, including vomiting.\nThe ASPCA’s compiled a searchable plant database of dangerous plants (listing over 400 items). Check it out if you are considering bringing a new plant home.\nPsst—this is why cats are afraid of cucumbers.\nFoods that pose a health risk to pets\nChocolate might have plenty of health benefits for humans, but it’s a harmful food for pets. Petside says most adults know this, but that it’s adults’ responsibility to make sure children know, too. Keep little ones from giving chocolate to pets and do your best to supervise.\nAll kinds of candy—including candy wrappers\nToo much sugar can give your pet a bellyache, but worse, if wrappers are swallowed, your pet risks tearing of the esophagus or intestines. Clean up as best and frequently as you can when candy is being unwrapped.\nMore harmful foods\nYour pets should also steer clear of chewing gum, grapes, raisins, macadamia nuts, avocados, onions, garlic, salt, raw yeast dough, and fatty foods.\nNext, learn to read the signs your dog is mad at you.\nHoliday health risks for pets\nEaster and Christmas decorations\nPlastic eggs, if ingested, can rip tears in the digestive system. Likewise, spoiled hard boiled eggs, if ingested, can make pets ill. Easter grass and tinsel are attractive, but deadly. Pets who attempt to eat these garlands and garnishes can choke, or lethally damage their intestines. At Easter, try real grass or crumpled paper instead. At Christmas, cat-proof your tree by avoiding tinsel.\nOther holiday safety tips for pets:\nNew Year’s: Forego confetti and keep an eye on balloons. If they deflate, they become a choking hazard.\nValentine’s Day: Keep their paws off the chocolates and far from the flowers.\nThanksgiving: Throw turkey bones in the trash.\nHalloween: Use flameless candles, and keep candy out of harm’s way. (Speaking of Halloween, you won’t want to miss these adorable Halloween dog costume ideas.)\nChristmas: Keep pets out of tree water, and be attentive when they show interest in ornaments, decoration hooks and ribbon. Here are more holiday safety mistakes you didn’t realize you were making.\nToys can pose a health risk to pets\nSmall, brightly coloured toys hold the same appeal for pets as they do children. The problem is that they are choking hazards. Petside’s advice is to keep small toys in a place safely hidden from pets.\nLearn how to spot the signs of cancer in cats.\nDrinks that are health risks for pets\nCoffee, tea, and alcohol\nCoffee and tea leaves are on the ASPCA’s list of People Foods to Avoid Feeding Your Pet, as is alcohol. Alcoholic beverages can cause vomiting, diarrhea, decreased coordination, central nervous system depression, and breathing difficulty, among other things.\nCould you be stressing out your pooch without realizing it? Check out these surprising reasons your dog is anxious.\nBatteries (and other small items) can be health risks for pets\nMany small items can lead to choking—even things you would never expect your pet would attempt eating. Be mindful of buttons, small batteries, twist ties, and rubber bands. In the bathroom, keep hairpins, cotton swabs, and dental floss out of reach from your pet. Cut down on clutter throughout your home with these organization tips from Marie Kondo.\nHealth risks for pets in the garage\nIf your pet is your shadow and frequently follows you around the house, remember that garage and storage areas need special attention, too. Keep cleaning supplies, antifreeze, fertilizer, de-icing materials and pesticides in a place pets can’t easily access. “Products containing metaldehyde, such as some slug pellets and firelighters, are extremely toxic, and should be kept away from pets,” according to Blue Cross. “Antifreeze and de-icer fluids taste sweet, but are also poisonous.” (As if having a furry companion wasn’t enough of a reason to grin, did you know that your dog actually loves when you smile?)\nBones pose a health risk for pets\nWhile eating meat off the bone might be tastier, if your pet gets a hold of one of those bones it could be bad news. Just like hazardous objects that might be laying around the house, it’s especially important to keep an eye on where your food leftovers end up. “Cooked bones splinter and can cut your dog’s mouth,” says Dana Humphrey, A.K.A. The Pet Lady. “If swallowed, they can puncture their stomach or esophagus too.” The same goes for bone “toys” you find in pet stores—which is why you should never, ever buy one.\nLearn how to read your dog’s facial expressions.\nSticks can be a health risk for pets\nYour dog might love to play fetch, but you might want to think twice before you pick up that stick outside. Sticks, especially small ones, can pose as serious choking hazards. Instead, Blue Cross suggests throwing a plastic, indestructible object that’s too big for your pet to accidentally swallow.\nPsst—this is why dogs spin around before they poop.\nIt’s easy to forget that trash cans can be health risks for pets, too. Your garbage can might have bones, chocolate, coffee grounds—essentially, a checklist of dangerous items that your fur baby should be nowhere near. “Make sure your garbage pail comes with a secure lid so you don’t have to worry about Fido or Fluffy getting their paws on discarded rib bones or leftover chocolate cake,” says The Pet Lady, Dana Humphrey.\nThink you’re raising a four-legged genius? Here’s how to tell if your dog is smart.\nMedication can pose a serious health risk for pets\nJust like humans, if you take medication that isn’t meant for you, it’s probably not a good idea. Human medication isn’t meant for your pets, and might even cause more harm than good. “Painkillers such as ibuprofen and paracetamol are particularly dangerous,” says Blue Cross. “Vitamin and mineral supplements can also be dangerous, particularly iron tablets and products containing zinc.” The same concept applies to different animals: never give your dog cat medication, and vice versa.\nBe sure to check out the ASPCA site for tips on keeping your pet safe and poison-proofing your home.\nNext, find out the secrets your dog’s tail is trying to tell you."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:4cedf726-4495-485c-aa69-9505211c0c29>","<urn:uuid:1082d9f9-b7d7-4f14-ac3d-56dd4f59394c>"],"error":null}
{"question":"What are the essential safety protocols for blood transfusions, and what types of transfusion reactions can patients experience?","answer":"Blood transfusions have multiple safety protocols: all blood donors must complete medical history questions and undergo physical examination, and donated blood is tested for various viruses including hepatitis B/C, HIV, HTLV I/II, syphilis, and West Nile virus. Before transfusion, blood is crossmatched with the patient's blood to ensure compatibility. During the procedure, patient's temperature, blood pressure and heart rate are monitored frequently. Regarding transfusion reactions, patients can experience two main types: hemolytic and non-hemolytic reactions. Hemolytic reactions occur due to blood incompatibility and present with symptoms like chills, fever, chest pain, dyspnea, and hypotension. Non-hemolytic reactions include febrile reactions (with symptoms like chills and muscle pain), allergic reactions (ranging from mild itching to severe dyspnea), fluid overload (presenting with cough and distended neck veins), and sepsis (showing high fever and hypotension).","context":["Prior to a patient’s surgery or procedure, the surgeon may discuss the need for a blood transfusion. Below are frequently asked questions about blood transfusions at CHOC. For more information about a specific patient’s need for blood during or as a result of a surgery or procedure, it is best to speak with the child’s surgeon.\nWhat is a blood transfusion?\nWhen a patient receives blood through a needle or catheter, it is called a blood transfusion. Patients may require a blood transfusion because they have:\n- a low blood count before, during or after surgery\n- severe heart or lung disease\n- had bone marrow failure\n- moderate to severe anemia\nPatients may also receive a blood transfusion when doctors predict the patient will lose blood during surgery.\nThere are several different components of the blood that can be transfused. Red blood cells are the most common type of transfusion. If the child’s physician has decided the patient might need a transfusion of blood, or blood products, he or she will explain the reasons for the transfusion.\nAre blood transfusions safe?\nAll blood donors must answer medical history questions and will receive a limited physical examination before being accepted as a donor. The donated blood is carefully tested for hepatitis viruses B and C, human immunodeficiency virus (HIV), human T-lymphotrophic viruses (HTLV) I and II, syphilis, and West Nile virus. These tests decrease the chances of transfusion-related infections.\nHow is blood transfused?\nBlood is collected and stored in sterile bags that are used once and then thrown away. Before blood is given to any patient, it is crossmatched with his or her own blood to make sure it is compatible. The blood will be given through a needle or catheter placed in the vein. The child’s temperature, blood pressure and heart rate are checked many times while the blood is being given. It can take a few hours to complete the process.\nWhat is designated donation?\nCHOC offers a Designated Donor Program that allows families and friends to donate blood for a specific patient in need. Like all blood donations, designated donations are tested and if accepted and compatible will be available for the specific child. No blood is ever wasted and if the donation is not compatible or the child does not need the blood at the time of his or her surgery or procedure, it will be released to another child in need. Blood donations should be received at least five days before the child’s anticipated need. Read more about the Designated Donor Program.\nWhat is autologous donation?\nCHOC offers an Autologous Blood Donation Program that allows patients to donate their own blood prior to their surgery or procedure in the case that they will need a transfusion. No blood is ever wasted and if the child does not need the blood at the time of his or her surgery or procedure, it will be released to another child in need. Blood donations must be received at least one week prior to surgery. Read more about the Autologous Blood Donation Program.\nAre there risks in receiving a blood transfusion?\nMost transfusions are performed without any problems. Mild side effects may include symptoms of an allergic reaction such as headache, fever, itching or rash. This type of reaction can usually be treated with medication, should the child require additional transfusions. Serious side effects are rare and may include difficulty breathing and sudden drops in blood pressure. Transfusion with blood of the wrong type can be fatal, but this is unlikely to occur because all blood is checked multiple times by medical personnel.","This action might not be possible to undo. Are you sure you want to continue?\nWhat is Blood transfusion?\nIntroduction of whole blood or components into the venous circulation Blood Transfusions can save LIVES ! A mainstay in the treatment of patients whose low blood count can compromised their lives Carries risk in various degrees of transfusion reactions as well as transmission of blood-borne pathogens\n1/30/2011 v3/CoN/CvSU 2\nPurposes of Blood Transfusion To restore blood volume after severe hemorrhage To restore the capacity of the blood to carry oxygen To provide plasma factors (antihemophilic factor . factor VIII) or platelet concentrates which prevent bleeding 1/30/2011 v3/CoN/CvSU 4 .\nB . The presence of a specific antigen in the erythrocytes¶ surface determines the blood type of the person Antigens can cause antibody reactions when in contact with mismatched blood Mismatched blood can cause a hemolytic reaction 1/30/2011 v3/CoN/CvSU 5 . AB and O.Blood groups Human blood is classified into 4 groups Blood type A.\nRh Factor Rh or Rhesus factor was discovered in 1940 Rh antigens are also present in the surface of erythrocytes Present in about 85% of the population Can cause hemolytic reactions in persons with antibody to that antigen Referred to as Rh positive or Rh negative 1/30/2011 v3/CoN/CvSU 6 .\nBlood Products for Transfusions BLOOD COMPONENTS 1/30/2011 v3/CoN/CvSU 7 .\n1/30/2011 v3/CoN/CvSU 8 .\n) and plasma One unit of whole blood is 500 ml Replaces blood volume and blood products: ( RBCs.Whole blood Includes all the blood cells ( RBCs. plasma protein. WBCs. plasma. fresh platelets and other clotting factors Primarily used for cardiac surgery or acute hemorrhage 1/30/2011 v3/CoN/CvSU 9 .\nPacked Red blood cells PRBCs are left after the plasma is separated out of whole blood Used for routine blood replacement during surgery Used to increase the oxygen.carrying capacity of blood in anemias and disorders with slow bleeding 1 unit raises hematocrit by approx 4 % 1/30/2011 v3/CoN/CvSU 10 .\nPlatelets May come as a single unit from multiple donors or multiple units from a single donor Used in patients with bleeding disorders from illness. trauma or organ dysfunction or with platelet deficiency Fresh platelets are most effective 1/30/2011 v3/CoN/CvSU 11 . medications.\nPlasma Expands blood volume and provide clotting factors No need to be typed and crossmatch 1/30/2011 v3/CoN/CvSU 12 .\nAlbumin A protein manufactured by the liver Blood volume expander and provides plasma proteins Maintains osmotic pressure that causes fluid to remain within the bloodstream instead of leaking out into the tissues Needs to be transfuse if albumin gets low Is not a component that must be cross-matched but is considered as a blood product v3/CoN/CvSU 13 1/30/2011 .\nClotting factors and cryoprecipitate Obtained by slowly thawing a unit of FFP or fresh frozen plasma ³Cryo´ is the recovered cold precipitate which is rich in certain clotting factors Used for clients with clotting factor deficiencies ( factor VIII and XIII) and those with DIC syndrome 1/30/2011 v3/CoN/CvSU 14 .\nTransfusion Reactions 1/30/2011 v3/CoN/CvSU 15 .\nTransfusion Reactions Blood from the donor and from the recipient are tested for compatibility Referred to as typing and crossmatching Used to assess a client closely for transfusion reactions 2 types of transfusion reaction: Hemolytic Nonhemolytic 1/30/2011 v3/CoN/CvSU 16 .\nbackache. dyspnea. Keep the vein open with normal saline or accdg to agency protocol 3. fever. and a urine sample to the laboratory 4. headache. Hemolytic Reaction Incompatibility between client¶s blood and donor¶s Clinical signs: Chills.I. Monitor vital signs 1/30/2011 v3/CoN/CvSU 17 6. Monitor intake and output .Discontinue the transfusion immediately 2. a sample of the client¶s blood . Notify the physician immediately 5. hypotension Nursing intervention 1. Send the remaining blood. cyanosis. tachycardia . chest pain.\nII. Nonhemolytic reactions Febrile reactions Allergic reactions Fluid overload Sepsis 1/30/2011 v3/CoN/CvSU 18 .\nplatelets. Notify the physician 1/30/2011 v3/CoN/CvSU 19 . Give antipyretics as ordered 3. headache. Discontinue the transfusion immediately 2. flushed skin. anxiety. or plasma proteins Clinical signs: chills.Febrile reactions Febrile reactions Sensitivity of the client¶s blood to white blood cells. warm. muscle pain Nursing intervention 1.\nadminister medication (antihistamine) as ordered v3/CoN/CvSU 20 1/30/2011 . depending on agency protocol 2. urticaria.Allergic reactions (mild) Sensitivity to infused plasma proteins Clinical signs: flushing. stop or slow the transfusion. bronchial wheezing Nursing intervention 1. notify the physician 3. itching.\nchest pain. Administer CPR if needed 5.Allergic reactions (severe) Antibody-antigen reactions Clinical signs: dyspnea. Monitor vital signs. Keep the vein open with PNSS 3. Notify the physician immediately 4. Administer medications and/or oxygen as ordered 1/30/2011 v3/CoN/CvSU 21 . stop the infusion 2. cardiac arrest Nursing intervention: 1. circulatory collapse .\ndistended neck veins.Circulatory Overload When blood is administered faster than the circulation can accommodate Clinical signs: cough. tachycardia.with feet dependent 2. Stop or slow the transfusion 1/30/2011 v3/CoN/CvSU 22 . Place the client upright . Notify the physician 4. dyspnea. Administer diuretics and oxygen as ordered 3. hypertension Nursing intervention: 1.crackles (rales).\ndiarrhea. Stop the infusion 2. Send the remaining blood to laboratory 3. antibiotics 1/30/2011 v3/CoN/CvSU 23 . hypotension Nursing intervention 1. Administer IV fluids. vomiting . Notify the physician 4.Sepsis Contaminated blood administered Clinical signs: High fever. Obtain a blood specimen from the client for culture 5. chills.\n1/30/2011 v3/CoN/CvSU 24 .\nBlood Administration 1/30/2011 v3/CoN/CvSU 25 .\na unit f PRBC is 200-250 mL Ensure that blood is typed and cross matched properly Blood administration is not delegated to a UAP.Important considerations Gather the pertinent data. schedule the administration (usually 30 minutes prior to transfusion) v3/CoN/CvSU 26 1/30/2011 . place the blood in a tray covered with a towel One unit of whole blood is 500 mL. but UAP must know the complications or adverse effects of blood transfusion and report it to the nurse Note any premedication ordered by the physician. know the purpose of the transfusion Confirm the physician¶s order for the number and type of units and the desired speed of infusion Obtain blood in plastic bag from the blood bank.\ng 19 in some agencies Alcohol swabs Tape Sterile gloves v3/CoN/CvSU 27 1/30/2011 . or other component Blood administration set with 170-200 u filters Supplemental blood filters . packed RBCs. if needed IV pump if needed 250 ml normal saline for infusion IV pole Venipuncture start kit ( including a gauge 20 needle or catheter).Equipment Unit of whole blood.\nEquipment 1/30/2011 v3/CoN/CvSU 28 .\ncheck whether the needle is appropriate to administer blood 1/30/2011 v3/CoN/CvSU 29 .Preparation Verify client consent and obtain baseline data before the transfusion Assess vital signs for baseline date Determine any known allergies or previous reactions to blood Note specific signs related to the client¶s pathology and the reason for the transfusion Establish the intravenous line.\n1/30/2011 v3/CoN/CvSU 30 .\nCheck the client¶s identification. Provide for client privacy and prepare the client. start the saline solution 1/30/2011 v3/CoN/CvSU 31 . ensure the right patient Assist the client in a comfortable position. why it is necessary and how she can cooperate. ensure that blood filters inside the drip chamber is suitable for the blood component Put on gloves. expose the IV site but provide for client privacy Wash hands and observe appropriate infection control procedures Prepare the infusion equipment. close all clamps on the Y-set Insert the piercing pin (spike) into the saline solution and hang on IV pole about 36 inches above the venipuncture site Prime the tubing.Preparing the IV infusion Explain to the client what you are going to do.\nblood donor number and expiration date of the blood Observe the blood for abnormal color. gas bubbles and extraneous material Compare the laboratory blood bag label with client¶s data If any information does not match exactly. notify the charge nurse and the blood bank v3/CoN/CvSU 32 1/30/2011 .Performance Obtain the correct blood component for the client. blood type and Rh group. clumping. ID no. check the physician¶s order with the requisition Check the requisition form and the blood bag label with a lab tech or according to agency policy Check the client¶s name.\nImportant considerations Make sure that RBC are left at room temperature for no more than 30 minutes before starting the infusion If the start of the transfusion is unexpectedly delayed return the blood into the blood bank Do not store blood in the unit refrigerator DO NOT ADMINISTER BLOOD UNTIL DISCREPANCIES ARE CORRECTED OR CLARIFIED 1/30/2011 v3/CoN/CvSU 33 .\nopen the upper clamp below the blood bag. closed the upper clamp below the IV saline solution.Prepare for transfusion Invert the blood bag gently several times to mix the cells with the plasma Expose the port on the blood bag by pulling back the tabs Insert the remaining Y spike into the blood bag Suspend the blood bag Establish the blood transfusion. readjust the flow rate with the main clamp Run the blood slowly for the first 15 minutes at 20 drops/ minute Observe the client closely for the first 15 minutes Note any adverse reactions and remind the client to call a nurse immediately if any unusual symptoms are felt during the transfusion v3/CoN/CvSU 34 1/30/2011 .\nDocument relevant dta Record starting the blood. type of blood. blood unit. 1/30/2011 v3/CoN/CvSU 35 . including vital signs. site of venipuncture. sequence number. size of the needle and drip rate.\ndepending on the health status Terminate the infusion if no infusion is to follow Discard the administration set according to agency policy Fill in the time the transfusion was completed on the requisition or monitoring sheet and the amount transfused Document relevant data v3/CoN/CvSU 36 1/30/2011 .Care of the client Monitor the client fifteen minutes after initiating the transfusion Establish the required flow rate if there are no signs of reaction Do not transfuse a unit of blood for longer than 4 hours Assess the client including V/S every 30 minutes or more often.\n10 ml /minute 1/30/2011 v3/CoN/CvSU 37 . Do not refrigerate platelets and keep them agitated at all times Fresh frozen plasma: 200 to 250 mL/unit. at 5. infuse within 24 hours of thawing .Variation: Infusing other blood components Platelets: pooled platelets usually contain 200 ± 400 mL.\net al Brunner and Sudarth Medical.References: Clinical Nursing Techniques :Kozier et al Contemporary Medical-Surgical Nursing: Daniels.Surgical Nursing : Smeltzer and Bare 1/30/2011 v3/CoN/CvSU 38 .\nThank you« Vina Virgo-Velasco RN 1/30/2011 v3/CoN/CvSU 39 .\nThis action might not be possible to undo. Are you sure you want to continue?\nWe've moved you to where you read on your other device.\nGet the full title to continue reading from where you left off, or restart the preview."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:2c6789ac-1d32-40ff-aa0d-e0bdddb552dc>","<urn:uuid:7f05a686-a8f6-49fd-9e90-565f504a5f42>"],"error":null}
{"question":"How do tribal courts compare to state/federal courts for handling injury claims against tribal entities?","answer":"Tribal courts like the Mohegan Gaming Disputes Court handle tort claims arising from tribal activities, as demonstrated in the Lewis v. Clarke case where injured parties could pursue compensation through tribal courts. In contrast, state and federal courts may be barred from hearing such cases due to tribal sovereign immunity, though exceptions exist. The Supreme Court in Bay Mills noted that states have alternative means of enforcement including denial of licenses, suits against individual employees, and criminal prosecution of tribal officials for state law violations.","context":["November 01, 2014\nOctober 31, 2014\nOctober 30, 2014\nOctober 29, 2014\nThe Supreme Court Decides Bay Mills Case, Leaves Tribal Sovereign Immunity Intact\nIn its long-awaited decision in Michigan v. Bay Mills Indian Community, the U.S. Supreme Court today re-affirmed its 1998 holding in Kiowa Tribe v. Manufacturing Technologies, Inc. 523 U.S. 751 (1998) that tribal sovereign immunity extends to tribes’ governmental and commercial activities, both on reservation and off. In a 5-4 decision, the Court affirmed the Sixth Circuit’s decision that the Indian Gaming Regulatory Act waiver of tribal sovereign immunity for suits to enjoin gaming on Indian lands in violation of a tribe’s gaming compact with a state did not apply to tribal gaming on non-Indian lands and that the State was barred by the doctrine of sovereign immunity from suing the Tribe directly for damages.\nThe Court rejected Michigan’s arguments that Kiowa was wrongly decided and that tribes should enjoy no immunity with respect to their commercial, off-reservation activities. The majority opinion, authored by Justice Kagan and joined by Justices Roberts, Kennedy, Breyer and Sotomayor, emphasized the doctrine that the court should not overrule its previous decisions without special justification (stare decisis), pointing out that (1) the Court had explicitly invited Congress to consider limitations on tribal sovereign immunity its Kiowa decision, (2) Congress had, in fact, considered several bills that would have imposed broad limits but had not enacted any of them (\"Having held in Kiowa that this issue is up to Congress, we cannot reverse ourselves because some may think its conclusion wrong\" Slip. Op. 20), and (3) Michigan had other means of enforcing state law against Bay Mills, including denial of required licenses, suits against employees and officials of the tribe to enjoin violations of state law and criminal prosecution of tribal officials and employees and for violations of state criminal laws.\nThe Court concluded:\nAs \"domestic dependent nations,\" Indian tribes exercise sovereignty subject to the will of the Federal Government. …Sovereignty implies immunity from lawsuits. Subjection means (among much else) that Congress can abrogate that immunity as and to the extent it wishes. If Congress had authorized this suit, Bay Mills would have no valid grounds to object. But Congress has not done so: The abrogation of immunity in IGRA applies to gaming on, but not off, Indian lands. We will not rewrite Congress’s handiwork. Nor will we create a freestanding exception to tribal immunity for all off reservation commercial conduct. This Court has declined that course once before. To choose it now would entail both overthrowing our precedent and usurping Congress’s current policy judgment.\nIn her concurring opinion, Justice Sotomayor asserted that the historical basis for tribal sovereign immunity is stronger than the dissent recognized and that the result reached by the majority is consistent with comity in view of the fact that State sovereign immunity prevents states from being sued by tribes. Justice Sotomayor also pointed out the special challenges that tribes face with respect to raising revenue and the role that their commercial enterprises play in funding government.\nThe unmistakable premise of the dissenting opinion, authored by Justice Thomas and joined by Justices Scalia, Ginsburg and Alito, is that sovereign immunity as currently exercised by tribes under the rule of Kiowa has led to widespread, grave and intolerable injustices. These injustices, according to the dissenters, warrant departing from the rule of stare decisis to correct the Court’s \"mistake\" in the Kiowa decision:\nIn Kiowa, this Court adopted a rule without a reason: a sweeping immunity from suit untethered from commercial realities and the usual justifications for immunity, premised on the misguided notion that only Congress can place sensible limits on a doctrine we created. The decision was mistaken then, and the Court’s decision to reaffirm it in the face of the unfairness and conflict it has engendered is doubly so.\nDissent, slip Op. 18.\nJustice Thomas’ opinion highlights areas, including taxation, tobacco commerce, payday lending and campaign finance, in which tribes have \"exploited\" immunity to avoid state regulation. In a separate dissenting opinion, Justice Ginsburg expressed her view that the Court had gone too far not only in expanding the scope of tribal sovereign immunity in Kiowabut also in expanding state sovereign immunity from suits by tribes to enforce federal laws in Seminole Tribe v. Florida, 517 U.S. 44 (1996). Justice Ginsburg would impose greater limits on the immunity of both sovereigns.\nThe Court’s decision leaves unclear two areas of tribal sovereign immunity jurisprudence. First, the Court expressly acknowledged that it has never \"specifically addressed\" whether immunity \"should apply in the ordinary way if a tort victim, or other plaintiff who has not chosen to deal with a tribe, has no alternative way to obtain relief for off-reservation commercial conduct. The argument that such cases would present a ‘special justification’ for abandoning precedent is not before us\" Slip. Op. 16, n.8. The Court’s comment will be viewed as an invitation for a plaintiff to make the argument that this situation does indeed present a \"special justification\" for an exception to immunity.\nThe Court’s opinion also leaves unaddressed the extent to which tribal sovereign immunity applies to subsidiary entities. In a footnote, the dissent observed, without comment, that \"[l]ower courts have held that tribal immunity shields not only Indian tribes themselves, but also entities deemed ‘arms of the tribe.’ … In addition, tribal immunity has been interpreted to cover tribal employees and officials acting within the scope of their employment.\"\nThe consequences of the Court’s decision are likely to be (1) arguments by state lobbyists that Congress should take action to limit tribal immunity for the reasons set forth in the dissenting opinion and (2) suits based on the assertion that there should be an exception to tribal sovereign immunity for a \"tort victim or other plaintiff who has not chosen to deal with a tribe\" who has \"no alternative way to obtain relief for off-reservation commercial conduct.\"","Does a lawsuit against a tribal employee for an act he committed within the scope of his employment by the tribe violate tribal sovereign immunity?\nWith Lewis and Clarke, the Supreme Court will venture into the relatively unfamiliar legal territory of tribal sovereign immunity for individuals employed by Indian tribes. The case arises out of an automobile accident between Brian and Michelle Lewis and William Clarke, an employee of the Mohegan Sun Casino, which is owned by the Mohegan tribe. In a lawsuit brought by the Lewises, Clarke successfully convinced the Connecticut Supreme Court that he was entitled to tribal sovereign immunity. The Lewises argue that sovereign immunity does not apply when a tribal employee is sued in his individual capacity because the finances of the tribe are not formally at risk. Clarke counters that the finances of the tribe are at risk in this suit, and thus, the sovereign immunity of the tribe should extend to him because he was acting within the scope of his tribal employment. To some, the voyage of Lewis and Clarke into the obscure realm of tribal sovereign immunity for individuals imperils tribal coffers; to others, the regulatory power of the states is at stake.\nQuestions as Framed for the Court by the Parties\nWhether the sovereign immunity of an Indian tribe bars individual-capacity damages actions against tribal employees for torts committed within the scope of their employment.\nLewis and Clarke’s five-year voyage to the Supreme Court began on October 22, 2011 with the chance encounter of Brian and Michelle Lewis (“Lewis”) and William Clarke (“Clarke”) in Norwalk, Connecticut. See Lewis v. Clarke, 135 A.3d 677, 679 (Conn. 2016). At the time, Clarke was an employee of the Mohegan tribe and was responsible for transporting patrons of the Mohegan Sun Casino in a limousine to their homes. See id. While Clarke was thus navigating Interstate 95, he collided with Lewis’s vehicle, propelling Lewis onto a concrete barrier on the left side of the highway. See id. Thereafter, Lewis set out to sue in the Connecticut Superior Court, alleging the accident was a tort as it occurred due to Clarke’s negligence and carelessness. See id.\nClarke sought to dismiss the lawsuit, asserting a right to tribal sovereign immunity and claiming that he could not be sued because he was an employee of the Mohegan tribe. See Lewis, 135 A.3d at 679. According to Clarke, Lewis should have sued in the Mohegan tribe’s Gaming Disputes Court, which the tribe established pursuant to an agreement with the state of Connecticut for the purpose of deciding tort claims arising out of the tribe’s gambling activities. See Brief for Respondent, William Clarke at 4–5. The Superior Court denied Clarke’s motion to dismiss, concluding that tribal sovereign immunity did not extend to Clarke as an individual. See Lewis, 135 A.3d at 679–80. According to the Superior Court, sovereign immunity would have applied had Lewis sued Clarke as an official of the Mohegan Tribal Gaming Authority. See id. The Superior Court held that, because Lewis sued Clarke seeking compensation not from the Mohegan tribe but from Clarke personally, tribal sovereign immunity did not bar this suit. See id. Clarke disagreed with the Superior Court’s holding and so journeyed onward via appeal to the Connecticut Supreme Court. See id.\nOn appeal, the Connecticut Supreme Court reversed course. See Lewis, 135 A.3d at 685–86. As the U.S. Supreme Court had yet to chart the boundaries of tribal sovereign immunity, the Connecticut Supreme Court relied on guidance from lower federal courts. See Brief for Respondent, William Clarke at 8. Based on the decisions of those courts, the Connecticut Supreme Court concluded that tribal sovereign immunity bars Lewis’s suit, solely because Clarke was an employee of the Mohegan Tribe and was performing an official duty. See id. Lewis was not content with ending this legal journey there and petitioned the U.S. Supreme Court for a writ of certiorari, which the Court granted on September 29, 2016. See id. With this case, the Supreme Court must now demarcate the boundaries of tribal sovereign immunity.\nDOES SOVEREIGN IMMUNITY APPLY WHEN AN INJURED PARTY SUES A TRIBAL EMPLOYEE IN HIS INDIVIDUAL CAPACITY?\nLewis maintains that applicability of sovereign immunity is based upon whose assets are at risk in a suit, as determined by state and federal law. See Brief for Petitioners, Brian Lewis et al. at 9, 24. For example, if an injured party chooses to sue a government employee in his official capacity, then the government’s assets are at risk in the suit, and the doctrine of sovereign immunity would bar the suit. See id. at 8–9. If, on the other hand, an injured party chooses to sue a government employee in his individual capacity, then it is the employee’s own assets that are at risk, and the doctrine of sovereign immunity does not bar the suit. See id. Lewis illustrates this distinction by noting that, despite sovereign immunity protecting the federal and state governments from suit, federal and state employees can still be sued in their individual capacities in actions such as Bivens claims and Section 1983 suits. See id. at 13–14, 18–19. Lewis argues that the sovereign immunity doctrine should apply in the same manner for tribal sovereign immunity cases. See id. at 21. Additionally, Lewis states that a tribe cannot alter the sovereign immunity doctrine of state and federal courts by covering any damages held against tribal employees acting within their scope of employment. See id. at 24. Thus, despite the Mohegan Tribal Code’s indemnification of its employees for damage awards, Lewis argues a suit should be allowed so long as the injured party sues the employee in his individual capacity. See id.\nClarke counters with a different interpretation of the sovereign immunity doctrine. See Brief for Respondent, William Clarke at 14. He states that the analysis for the immunity of a tribal employee takes a different path from the analysis for a state or federal government employee, noting that previous lower court tribal immunity cases have applied sovereign immunity and official immunity almost interchangeably. See id. at 12–14. Further, Clarke argues that in the tribal immunity context, the relevant inquiry for immunity is not whether an employee is being sued in an official or individual capacity, but whether he was acting in his official capacity at the time he caused an injury. See id. at 14. If a tribal employee was acting within the scope of his employment at the time of the injury, Clarke argues that the inquiry then turns to whose assets are at risk in the suit. See id. According to Clarke, if a tribe’s assets are at risk, then the doctrine of sovereign immunity bars the suit in State or Federal court. See id. Clarke maintains that this suit should be barred on grounds of sovereign immunity because the Mohegan Tribal Code indemnifies damages against its employees for actions performed within the scope of their employment. See id. at 15–16, 20. Clarke argues that it should make no legal difference whether the tribe is obligated to pay damages based on state law or tribal law, because Supreme Court precedent has indicated that sovereign immunity applies to cases in which the sovereign is not formally held liable but is held liable “in effect” because it is ultimately responsible for paying the damages. See id. at 16–17. Further, Clarke states that the Court has treated the application of sovereign immunity to state and tribal instrumentalities as a functional question of who is ultimately responsible for paying the damages, based on state or tribal indemnity laws. See id. 17–18, 21.\nDOES OFFICIAL IMMUNITY APPLY WHEN AN INJURED PARTY SUES A TRIBAL EMPLOYEE IN HIS INDIVIDUAL CAPACITY?\nLewis argues that the Court should not explore whether official immunity applies in this case. See Brief for Petitioners at 16, 22. Lewis states that Clarke did not make the official immunity argument and that the argument goes beyond the question before the Court. See id. at 22. Further, Lewis notes that the Court has never extended official immunity to tribal officials. See id. According to Lewis, the official immunity doctrine is ill-suited to the context of tribal employment because the tribal context does not implicate the supremacy concerns present in the federal context. See id. at 22–23. Lewis also notes that official immunity is typically only applied in cases in which a government employee was acting within their discretion, which would not apply in this case. See id. at 17, 19–20, 23.\nClarke responds that the Court should extend the common-law doctrine of official immunity that covers federal employees to the context of tribal employees. See Brief for Respondent at 26–28. First, he argues that the Court can consider official immunity in this case because it has been treated as interchangeable with sovereign immunity in tribal immunity cases and played a critical role in the outcome of the decision below. See id. at 27–28. Clarke maintains that tribal officials, like government officials, require official immunity in order to effectively carry out their duties. See id. at 29. In fact, he argues that the need for protection in carrying out official duties is even stronger in the context of an Indian Tribe. See id. at 42. Further, Clarke states that this immunity should not be limited to cases in which the tribal official was acting within his discretion, because Congress has rejected such a distinction for government employees in immunity cases. See id. at 30–32, 36–37. Clarke notes that a tribe’s sovereignty is most akin to that of foreign sovereigns, whose officials are granted complete official immunity. See id. at 43. Clarke also argues that refusing to extend immunity in this context would uniquely disadvantage tribal employees because they would not be eligible for the same protections as are afforded to the employees of all other governments. See id. at 45–46.\nSTATE’S POWER TO REGULATE VS. TRIBE’S POWER TO GOVERN\nClarke asserts that a narrow interpretation of tribal sovereign immunity will hinder effective tribal governance. See Brief for Respondent, William Clarke at 42. According to Clarke, if a tribe’s employees are not afforded sovereign immunity, they may become paralyzed by the fear of a lawsuit and avoid discretionary action. See id. Clarke posits that such a result would cripple a tribe’s ability to govern effectively. See id. Additionally, Clarke asserts that prospective employees would steer clear from tribal employment if potentially bankrupting liability was at stake. See id. Clarke thus cautions the Supreme Court to weigh the tribe’s ability to effectively govern when charting the boundaries of tribal sovereign immunity. See id.\nThe United States, as amicus in favor of reversal, agrees with Clarke’s conclusion but disagrees as to the method. See Brief of Amicus Curiae the United States, in Support of Reversal at 26. According to the United States, the common law doctrine of official qualified immunity is more appropriate in this context. See id. The United States urges the Court to send the case back to the Connecticut Supreme Court to reconsider it under the doctrine of qualified immunity. See id.\nLewis foresees an alternate problem, asserting that broadly delineated tribal sovereign immunity intrudes upon States’ regulatory authority. See Brief for Petitioners, Brian Lewis et al. at 6, 20. According to Lewis, states utilize tort actions as a tool to advance their regulatory scheme. See id. at 27–28. Lewis cautions that should Clarke prevail, the result would weaken the regulatory power of the States. See id.\nClarke disagrees, asserting that the State of Connecticut’s regulatory powers are adequately preserved. See Brief for Respondent at 53–54. According to Clarke, Connecticut had an opportunity to negotiate for a waiver of sovereign immunity with the tribe, but chose not to do so. See id. Clarke thus argues that Connecticut’s power to regulate would not be diminished. See id.\nTRESPASSING ON THE DIGNITY OF SEMI-SOVEREIGN TRIBES\nAccording to Clarke, establishing narrow boundaries on tribal sovereign would constitute a trespass on the dignity of the tribes as “separate sovereigns pre-existing the Constitution.” Brief for Respondent at 43–44. Clarke asserts that inasmuch as law treats foreign dignitaries and officials with sovereign immunity, so it must afford the same to the tribes, which have long been recognized as “‘retain[ing] their historic sovereign authority’ unless Congress abrogates it.” Id. at 44.\nLewis, however, finds no such concern. See Brief for Petitioners at 23. According to Lewis, the framers of the Constitution devised sovereign immunity to ensure that States would not suffer the humiliation of appearing as parties in federal courts. See id. Lewis concludes that as the present lawsuit was filed against an individual, it would not force the tribe itself to appear in American courts. See id. Thus, according to Lewis, sovereign immunity concerns are not implicated here. See id.\nTHREATENING THE WEALTH OF THE TRIBES\nClarke also asserts that a narrow delineation of tribal sovereign immunity would threaten the financial integrity of a tribe. See Brief for Respondent at 50. According to Clarke, with sovereign immunity, tribes would be allowed to set their own rules of recovery, allowing them to avoid potentially ruinous liability. See id.\nIn response, Lewis asserts that the present lawsuit poses no danger to the financial security of the tribe. See Brief for Petitioners at 23–24. According to Lewis, the only reason the Mohegan tribe may suffer is because it agreed to cover the damages awarded against Clarke. See id. Lewis thus concludes that the tribe could avoid any potential liability by simply refusing to indemnify its employees. See id.\nINJUSTICE TO THE VICTIMS\nIn support of Lewis, the Connecticut Trial Lawyers Association and American Association for Justice (“Associations”) assert a verdict for Clarke could leave many injured parties without a remedy. See Brief for Connecticut Trial Lawyers Association et al., in Support of Petitioner at 5. Specifically, the Associations assert that barring Lewis’s suit would foreclose individuals injured by other tribal employees from having an opportunity to be heard in state and federal courts. See id. at 15. Lewis likewise repeats this concern, noting that it is inherently unfair to bar injured parties from American courts solely because of the employment status of their assailant. See Brief for Petitioners at 28–29.\nClarke challenges the Associations’ concerns as inconsistent with the facts of the case. See Brief for Respondent at 54–55. Clarke notes that the Lewis family would not be left without a remedy even if tribal sovereign immunity barred their suit in Connecticut courts. See id. According to Clarke, the Lewis family could have sued in the Gaming Disputes Tribal Court, to which other parties that were injured in the same accident appealed and received substantial compensation. See id. Thus, Clarke asserts that injured parties would not be left without a remedy even if the Court were to extend tribal sovereign immunity in federal courts to employees like Clarke. See id.\n- Matt Ford, Lewis and Clarke Get Their Day in Court, The Atlantic (Sep. 29, 2016).\n- Stella Shannon, Stella on SCOTUS: Lewis v. Clarke, The Politic (Oct. 11, 2016).\n- Andrew Westney, Justices May Open Tribal Immunity Loophole in Mohegan Suit, Law 360 (Sep. 30, 2016)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:201ab7fb-fceb-454c-8abf-6118074caa9b>","<urn:uuid:3a2431e6-c2c8-466b-9eae-5a31dc91b1da>"],"error":null}
{"question":"What is the key difference between making cuts above vs below plant nodes when pruning?","answer":"Cuts made above the leaf node are used to prevent new growth and maintain plant health, while cuts made below the leaf node are specifically used for propagation purposes - to start new plants. When pruning to keep a plant healthy, cuts should be made a quarter inch to half inch above the leaf node. For propagation, cuts need to be made at least a quarter-inch below the leaf node, with a 45° diagonal cut that allows roots to develop.","context":["There are several advantages of taking cuttings to generate new plants. Firstly, the cuttings will have identical characteristics to the mother plant, so the sex, size, quality, smell, taste, strength etc will be known. The plants will also grow to a similar shape and size, so space can be used more efficiently. A further benefit is that once the initial outlay has been made for the seeds, new plants can be grown for a minimal cost.\nAlthough any part of a plant can be used as a cutting, some parts will take longer to develop roots than others. The main head or arm tips have the highest concentration of growth hormones (auxins) and therefore more likely to root. Cuttings should be made from softwood stems and not the older, harder stems. ‘Softwood’ is the term which refers to the younger, soft, green stems. These are the easiest to root. Once these stems mature and age they are then known as ‘semi-ripe’’ and when fully mature, ‘hardwood’. Choose stems that are healthy and have at least three sets of nodes – smaller cuttings will root but they can be more difficult and may take longer, the ideal length is around 5 – 8 cms. Cuttings can be taken at any point in the vegetative stage of growth and in the first 3 or 4 weeks of the budding period. After this it may be difficult to get the cutting to root. NOTE: It is generally best to take more cuttings than required, you can then select the best ones or compensate for any that do not root.\nMAKING THE CUT\nUse a tool with a sharp blade to make the cut, a scalpel or razor blade for example. The blade should be sterilised to prevent contamination by bacteria etc, using a flame, alcohol or a bleach solution. Clean new blades to remove any grease etc. Make a 45° diagonal cut below the nodes, leaving a good section of stem underneath. Select as thick a stem as possible as thin cuttings can take longer to root.\nIf you are taking several cuttings at once, or if there will be a time delay between cutting and planting, take a larger cutting than is needed initially, and make the final cut later. Cuttings should be left sitting upright in water so that air does not get to the roots. Use a pair of scissors for the first cut, as the opposing blades will ‘seal’ the end. Make the final cut with a sharp blade (not scissors), this leaves an open cut which allows roots to develop.\nREMOVING THE LOWER LEAVES\nThe larger, lower leaves should be removed, so the cutting does not have to expend energy maintaining them. They will probably die anyway. At least the top two sets of leaves must be left. It is preferable (if possible) to remove leaves before making the cut, to avoid embolisms. An embolism occurs when a small bubble of air is sucked up into the stem, preventing the cutting from drawing up the water, nutrients etc that it needs. This will also allow you to plant the cutting as soon as possible after it has been taken from the mother plant.","Nothing quite strikes fear in the heart of gardeners than taking a set of blades to a plant.\nWe want our plants to flourish, yet, weirdly, the one thing that helps plants grow better and healthier is cutting bits of it off.\nWhat parts though? Where do you cut plants when pruning? Above the leaf node on a soft-stemmed plant, or a fraction of an inch from a branch collar on a fruiting tree?\nTable of Contents\nWhere Do You Cut Plants When Pruning?\nWhere you make the cuts depends on why you’re making the cuts. For deadheading, cuts are made a quarter inch below the flower head. Pruning plants intended for propagation requires cuts to be made a quarter inch below a leaf node. Cuts above the leaf node are done to prevent new growth. Pruning trees and shrubs requires a combination of reduction cuts, thinning cuts, and heading cuts. Where the cuts are made depends on the direction of growth buds. Inward facing buds should be removed with cuts made below the node to prevent inward growth that would hinder light penetration.\nA Note about Nodes (and Internodes)\nNodes are on every plant, whether it has soft-stems or woody branches. The nodes are the part on the stem (or trunks) where new branches shoot out.\nInternodes refer to the stem section between nodes.\nWhen pruning to keep your plant healthy, cuts should always be made a quarter inch to a half inch above the leaf node.\nThe only time cuts are made below a leaf node is for propagation – cutting part of a plant to start a new plant.\nMaking cuts for propagating plants is always done at least a quarter-inch below the leaf node.\nWhen you want the growth to be on the same plant, make the cut above the node.\nThe Different Types of Pruning Cuts\nReduction cuts are used on trees and shrubs to restrict height. These should only be used on young trees so this is the earliest type of pruning cut to use.\nThe advantage is that it will limit height making future pruning more manageable.\nThe major downside and the reason this type of cut shouldn’t be made on mature trees is that it leaves the plant more susceptible to decay.\nNodes on trees are different from those on soft-stemmed plants.\nNodes on branches have collars that stick out from the main trunk or the leading branch. This is part of the branch’s defense system.\nWhen you cut outside of the branch collar, the wood wound heals faster.\nCutting flush to the bark of a tree removes the protective branch collar, leaving the plant susceptible to infections or insect damage.\nWhen making reduction cuts, since it is cutting the branch down from the top to restrict height, there is no branch collar. As such, any reduction cuts you make will take longer to heal.\nYounger trees that are actively growing can handle these types of cuts, but they should be used with care. The main purpose of reduction cuts is for training younger plants by controlling the direction of growth.\nHeading cuts are made to reduce the size of branches. As the branches are established, there will be growth buds. Making a cut right above growth buds prevents branches from growing longer or taller.\nOn fruiting trees such as apple trees and pear trees, there will be growth buds and flowering buds.\nFlowering buds produce fruits. Growth buds produce wood.\nGrowth buds are shaped like teardrops, whereas flowering buds are round and swollen.\nWhen pruning branches on fruiting trees, cut about a quarter-inch below a growth bud to stop it from growing.\nCuts on these should be angled to 45-degrees to prevent water from sitting on an open branch.\nThe direction the growth bud is pointing to is the direction it will grow.\nAny inward-facing growth buds should be removed to prevent too dense a growth.\nAs a guideline, make cuts with buds facing outwards above the node.\nBuds facing inward should be cut below the node because inward growth isn’t healthy on trees or shrubs.\nThinning cuts are a type of structural pruning as it lessens the weight on the main trunk of trees.\nOn shrubs, such as when pruning roses, thinning cuts are needed to increase light penetration. They’re also great for letting you control the direction of growth shoots.\nUnlike heading cuts which reduce the length or height of a branch, a thinning cut removes the entire branch right back to the main branch.\nLarge branches with a lot of weight should be pruned in three stages to prevent the branch from falling and accidentally tearing healthy bark.\nWhen removing large heavy branches, cut it back by at least one third first, then make a second cut 12 to 15 inches shorter, then a final cut a quarter inch from the branch collar.\nThe Right Pruning Tools for the Job\nBypass pruners have two sharp curved blades that work like scissors. These are better suited to pruning live plant tissue because both blades cut like scissors.\nAnvil pruners only have one sharp blade that pushes against a flat plate, similar to pushing a knife through something against a flat object. These are better suited to pruning dead wood.\nLoppers have longer blades and handles and need two hands to use.\nLoppers are better suited to thicker branches up to two inches in diameter.\nPruning saws are suited to branches thicker than two inches.\nThe type of pruner to use depends on the thickness of the material you’re cutting.\nAs a guideline for which type of pruner is best for the task you need to do, use your thumb as a guide.\nThe average thumb is one inch in diameter.\nIf a branch is smaller than your thumb, hand pruners should be suited.\nWider than your thumb, use a pair of loppers. Unless it’s over two inches in diameter, in which case, the pruning saw would be the better tool for the job.\nPlant Pruning FAQs\nWill making too many cuts when pruning kill a plant?\nOver pruning can cause plants too much stress, which can indirectly kill them. It won’t happen immediately. To avoid stressing plants, the recommended maximum to remove from any plant, shrub, or tree is no more than a third of its total size.\nWhen pruning trees and shrubs with thick branches, does a sealer need to be applied to dress wounded wood?\nHealthy wood will heal on its own. Pruning sealers have been known to have the opposite effect they were intended for. Instead of sealing out potential infections, they can seal in moisture creating more problems than they prevent. It’s better to use safe cutting practices on trees that leave the branch collars in place so that the wound can callus over.\nDaniel has been a plant enthusiast for over 20 years. He owns hundreds of houseplants and prepares for the chili growing seasons yearly with great anticipation. His favorite plants are plant species in the Araceae family, such as Monstera, Philodendron, and Anthurium. He also loves gardening and is growing hot peppers, tomatoes, and many more vegetables."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:feaf33bc-6c3a-4362-9420-b6ed8d934f6d>","<urn:uuid:fd2ebe94-07d1-4ac6-b061-13e35f3523a8>"],"error":null}
{"question":"What are the core principles of Clinical Governance in healthcare delivery, and how does it relate to organizational change management?","answer":"Clinical Governance is a framework that ensures healthcare organizations maintain high standards of care through continuous quality improvement. Its core principles include patient-centric care, quality assurance, evidence-based practice, risk management, and professional development. It requires strong leadership to foster a culture of safety and excellence, with regular training and audit programs. Regarding change management, healthcare organizations must carefully manage people's fears during transitions, as failed organizational changes can lead to extinction. Change management in healthcare is particularly complex due to the confluence of different professions and requires structured approaches to implementation, especially when dealing with strategic transformations in patient safety and care delivery.","context":["Why Clinical Governance?\nClinical Governance was introduced in response to a series of high-profile failures in care that were highlighted in the UK during the 1990s. Incidents like the Bristol heart scandal, where substandard practices led to the deaths of numerous children, underscored the need for a structured framework to oversee clinical excellence and patient safety.\nWhat is Clinical Governance: A Definition\nClinical Governance is a framework through which healthcare organizations are accountable for continuously improving the quality of their services and safeguarding high standards of care. It integrates fundamental principles that ensure patient safety, effective clinical practice, and high-quality healthcare.\nKey Components of Clinical Governance\n- A Strategic Framework: Aims to create an environment where excellence in clinical care will flourish.\n- Quality Assurance: Involves systematic review, monitoring, and improvement of all aspects of patient care.\n- Patient-Centric: Focuses on delivering care that is patient-focused and driven by patient needs and outcomes.\nThe Five Pillars of Clinical Governance\n- Involving patients in their own care decisions, like having input into treatment options.\n- Ensuring patient rights and providing access to information, exemplified by transparent care plans and open communication channels.\n- Delivering care based on the best available evidence, such as adhering to research-backed protocols for disease management.\n- Utilising clinical guidelines, like using standardised stroke care pathways to ensure best practices are followed.\n- Identifying potential risks to patient safety and implementing strategies to prevent harm.\n- Performing regular risk assessments and learning from incidents.\n- Identifying potential risks to patient safety, such as assessing the risks of falls in elderly patients and implementing preventative measures.\n- Performing regular risk assessments and learning from incidents, like reviewing medication errors to enhance pharmacy practices.\n- Ensuring all healthcare staff have the necessary qualifications and skills through credentialing processes and competency assessments.\n- Fostering a culture of continuous learning and professional development, like providing regular clinical skills workshops.\n- Continuously assessing and improving healthcare processes, such as streamlining patient admission workflows to reduce wait times.\n- Encouraging innovation and embracing changes that enhance patient care, like adopting telehealth services for remote monitoring.\nImplementing Clinical Governance\n- Leadership and Culture: Strong leadership is vital to foster a culture of quality and safety.\n- Education and Training: Regular training programs to update staff on the latest guidelines and practices.\n- Audit and Feedback: Regular audits to assess performance, followed by constructive feedback and action plans.\nThe Role of Clinical Governance in Radiography\n- Enhancing Diagnostic Accuracy: Ensuring protocols are followed, like double-checking patient IDs to improve the accuracy of diagnostic imaging.\n- Radiation Safety: Implementing strict protocols, such as ALARA (As Low As Reasonably Achievable) principles, to minimise patient exposure to radiation.\n- Continual Professional Development: Keeping radiographers up-to-date with the latest techniques and safety measures through annual certifications and training.\nAccountability and Transparency\n- Duty of Candour: Being open and honest when things go wrong, such as disclosing adverse events to patients and families.\n- Record-Keeping: Accurate documentation, like maintaining detailed patient records, is a legal requirement and facilitates quality assurance activities.\nClinical Governance is not just about policies and procedures; it’s about creating a culture where quality care and patient safety are at the heart of healthcare delivery. It involves everyone within the organisation, from frontline staff to senior management, all working together towards a common goal of excellence in patient care. It’s a commitment to continuous improvement, transparency, and a holistic approach to healthcare that benefits patients and providers alike.","Change management in health care robert james campbell, edd this article introduces health care managers to the theories and philosophies of john kotter and. Transforming healthcare organizations organizational change in generic organizations understates the difficulty of managing change in healthcare organizations. Managing change means managing people's fear history is full of examples of organizations that failed to change and that are now extinct. Advances in health care organization behavior and their application to the management of health care organizations and organizational culture change. Applying structured change management to ehr implementations so how do we compel the people in the healthcare organizations to change to electronic healthcare. Implementing strategic change in a shifts in the environment can compel health care organizations to change their shown that when members of a management team. Professor worley and a group of organization change practitioners leading and managing change health care-american healthways, st joseph health care.\nEvidence-informed change management in canadian healthcare organizations i table of contents key messages. Roles of the governing body and senior management contributing to the organization’s leadership rather than being silos 2 leadership in healthcare organizations. Promoting a culture of health and wellness transformation planning and organizational change lifecycle development, organizational change management. Effecting and leading change in the knowledge base regarding successful change in health care organizations can a knowledge of change management can.\nMaking a case for organizational change in patient safety a health care organization might develop a strategy organizational change in patient. Issues distr general in health services this document is not a formal publication of the world health organization implementing and managing the change. Organizational change boston, ma: management decision and research center washington, dc: va health services research and development service, office of research and.\nOne of the key concerns in health care management is management of change and health care professionals are obligated both to acquire and to maintain the expertise needed to undertake their. Organizational change occurs when a company makes a transition from its current state to some desired future state managing organizational change is the process of. Top 7 healthcare trends and challenges from our a significant change in the healthcare industry’s population health management, and personnel.\nThis paper addresses the health care system from a global perspective and the importance of human resources management (hrm) in improving overall patient health. Tackling organizational change in health information management: two canadian experiences david thompson, mhsc, and karen adams introduction health information management in canada has.\nManaging change in the nhs making informed decisions on change key points for health care managers and professionals. Managers & supervisors play a critical role in times of change learn how manager & supervisors impact the outcome of a change management strategy.\nChange management in healthcare 2 abstract change management is a critical part of organizational and strategic transformation it is an intricate process that. Change management in healthcare literature review 2 healthcare organizations are complex, in part because of a confluence of professions, including. Navigating organizational change can cause concern in a health-care facility's nursing staff and strain employee morale by carefully planning strategies for. Strategies for managing health-care costs the organization may also benefit implementing the plan incrementally limits the amount of change employees. Wwwniceorguk how to change practice getting started about nice the national institute for health and clinical excellence (nice) is the independent."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:f958f413-97d2-4957-8764-b0085a4328c3>","<urn:uuid:d126901f-abb7-4525-a06e-1f624daddab1>"],"error":null}
{"question":"How do adaptation services differ between private homeowners and local authority tenants in terms of cost coverage?","answer":"For private homeowners, adaptation costs are covered through means-tested Disabled Facilities Grants up to £30,000, with grant levels depending on savings and income but not home value. Councils can add top-up adaptation grants or loans, and may put a grant repayment charge on the property. In contrast, for local authority tenants, the local authority meets the entire cost of housing adaptations, providing full coverage without means-testing.","context":["New checklist to help improve local housing adaptations provision throughout England\nCare & Repair England has launched a new guide which offers practical information and advice about housing adaptations and how services can be reviewed and improved to facilitate independent living.\nEntitled ‘Help with home adaptations: Improving local services’, the checklist is intended for local older people’s forums and groups who want to influence local policy, plans and actions.\nWritten by Care & Repair England, the checklist draws on the experience of members of the Older People’s Housing Champions network and sets out local authorities’ responsibilities for providing help with home adaptations; what a ‘good’ adaptation service is; and opportunities to improve local home adaptations provision.\nHousing adaptations are an essential part of enabling elderly and disabled people to remain independent in their own homes. Adaptations can provide vital equipment, such as a stairlift, to ensure people have access in and around their homes.\nThe guide states that whilst many people pay for housing adaptations themselves, there are a number of people who would benefit from independent advice, practical assistance with managing the building works or help covering the costs of adaptations.\nHowever, Care & Repair England says that whilst local authorities are legally required to assist people with housing adaptations, the quality of provision of housing adaptations varies significantly throughout England.\nIn light of this, the housing adaptations guide lays out:\n- The help with home adaptations that should be provided everywhere\n- Examples of good home adaptation services\n- Ideas for working with local authorities to improve local provision – including a list of questions to ask about current services and a good practice checklist\nLocal authorities’ duties\nBefore discussing what a good housing adaptation service looks like, the guide first establishes the different roles and responsibilities of local authorities and how they join up to work together to deliver housing adaptations.\nThe guide outlines that all local authorities (County Councils, Unitary Authority and District/Borough Councils) have legal duties to help disabled people with home adaptations.\nHousing authorities are required to offer mandatory, means-tested grants to help to meet the costs of essential home adaptations (Disabled Facilities Grants).\nSocial services authorities are required to provide assistance with community equipment and minor adaptations (such as grab rails) as well as offering related information and advice. Social services authorities also have discretionary powers with regard to helping with the costs of home adaptations (e.g. providing loans or grants) and their occupational therapists are usually involved in assessing need for a Disabled Facilities Grant (DFG).\nCounty Councils are responsible for social services, District/Borough Councils (which operate within a County) are responsible for housing functions, and Unitary Authorities are responsible for both social services and housing.\nAs both the housing authority and the social services authority are involved in home adaptations provision, the guide says that good links between these two councils is an important step in providing a good housing adaptations service.\nThe Disabled Facilities Grant\nThe DFG is a mandatory, means-tested grant that helps to pay for essential home adaptations which enable disabled people to remain independent in their homes.\nGrant level is dependent upon savings and income but not the value of the home and councils can put a grant repayment charge on the property. The mandatory DFG limit is £30,000 but councils can provide top up adaptation grants and/or loans.\nGood home adaptation services\nThe guide says that in order to provide a good housing adaptation service, the following factors need to be incorporated:\n- People should have access to reliable, impartial information and advice about suitable home adaptations\n- It should be easy for people to find out about the adaptations help available and adaptation costs\n- The adaptations process should be transparent and there should be clear communication with the individual\n- People should be able to get the adaptations help they need easily and professionals should listen to their wants and needs\n- The home adaptation should be carried out quickly and efficiently to avoid delays\n- The adaptation should be delivered by a reputable contractor at a reasonable price\nThe guide builds on various examples of good housing adaptation services to provide a helpful checklist for local older people’s groups and other stakeholders so they can support and encourage local authorities to review and improve home adaptations provision.\nExamples within the checklist include:\nYou can read the full guide by Care & Repair England here","A: Basic Living Skills\nThe service user may have concerns or difficulties with maintaining their accomodation, shopping, cooking and laundry. It is important to take the time to address these issues.\n1 Training for basic independent living skills\nSome homeless accommodation providers, and in particular transitional accommodation providers, may provide independent living skills courses offering basic training in areas such as managing change, furnishing a flat, paying bills and money management, communi-cation skills, basic health, cooking and nutrition, basic safety precautions etc. Check with the various services to see what courses may be available to service users. (See Listings: Accommodation/ Homelessness).\nTraining programmes, basic cookery programmes and money management programmes are also provided by many community centres and faith-based organi-sations; these programmes are often free of charge. Check with local Citizens Information Centres for more information on local courses.\n2 House maintenance and furniture\nGood quality and inexpensive furniture can be bought in various places such as the Society of St Vincent de Paul (SVP) and Oxfam second-hand shops.\nAlternatively, they may be available for free on www.dublinwaste.ie or on www.jumbletown.ie. The furniture advertised on these websites must be collected and the service user may need someone to help them with this task.\nThe Supplementary Welfare Allowance scheme may also provide assistance with certain once-off or urgent costs if furniture needs to be purchased. Contact the CWO for more information.\nSVP can also provide furniture or funding for new homes. Further information can be found on www.svp.ie/Contact-Us.aspx\nWhere the service user needs to know how to report maintenance issues the case manager should discuss this issue with them and provide them with infor-mation on which agency or landlord they should contact.\n3 General cleaning and housekeeping\nHow a person presents themselves often provides a good indication of their well-being. If someone is having a problem with managing their cleanliness, it may indicate that other health issues need to be addressed.\nAs a case manager, your first step is to make the person aware that a serious problem with lack of cleanliness in their accommodation could affect their residency. If children are living in the home, there may also be an issue in relation to child protection.\nIf the problem persists, it may be necessary to contact a public health nurse. Vulnerable people in the community, particularly the elderly who are in need of help with day-to-day tasks can avail of a home help service. The home help may visit for a couple of hours a day to do some housework or shopping, or they may provide personal care assistance with dressing, bathing etc. For further information on this service, contact the public health nurse.\n4 Disability and housing adaptation\nWhere housing has to be adapted in order to make it suitable for a person with a physical, sensory or intel-lectual disability, the local authority will provide a housing adaptation grant. This will enable the service user to carry out changes to the property such as making it wheelchair accessible, adding a ground floor bathroom or toilet, installing a stair lift etc.\nGenerally, there is no income limit for housing adaptation grant eligibility. However, local authorities may apply income limits at their discretion and therefore it will be necessary to check with the relevant local authority to see whether an income limit applies. If the service user is a local authority tenant, the local authority will meet the entire cost.\nThe three grant aid schemes provided are as follows:\n- Housing Adaptation Grant Scheme for People with a Disability\n- Mobility Aids Housing Grant Scheme\n- Housing Aid for Older People\nGrant application forms are available directly from the local authority."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:c5b9a1e6-deea-41b0-b6ab-2fe00e735391>","<urn:uuid:66e12f8e-691a-4778-9b20-b65ec457a544>"],"error":null}
{"question":"What are the key benefits of genebanks for agricultural sustainability, and how do they contribute to public research commercialization?","answer":"Genebanks contribute to agricultural sustainability by preserving genetic materials from various plants to help develop crops resistant to diseases, pests, and climate change. They maintain plant diversity through controlled storage conditions (-18°C to -20°C for seeds) and make samples available to researchers, breeders, and farmers. Regarding research commercialization, genebanks are part of public research organizations (PROs) that increasingly engage in knowledge transfer through formal channels like research collaboration and licensing. Data shows that patent filings by PROs have grown steadily, increasing by 29% under the Patent Cooperation Treaty, particularly in biomedical and pharmaceutical sectors. This has helped emerge new industries and create high-technology clusters, though measuring exact economic impact remains challenging.","context":["Plant Genebanking: Investing Seeds for the Future\nFor many years, the agricultural sector has worked on the continuous development of sustainable practices to provide sufficient food and medicine supply for a growing population. Among the many challenges they aim to resolve are the issues inflicted by plant disease outbreaks and upsurge, pests, and climate change. The conservation and increase of diversity of plant species are recognized feasible solutions to attain sustainability—and this is where genebank plays a part.\nA plant genebank is a type of biorepository that preserves genetic materials from various plants. The banked seedlings, cells, tissues, or other forms that contain genetic information are used by researchers, breeders, and farmers alike for the research and development of crops and medicines. As global climate changes, this approach is vital for plant varieties to withstand unprecedented weather and natural disasters.\nA Glance at Genebanking Process\nAfter the sample acquisition, a unique identification number is assigned to every material. This allows genebanks to properly manage and document their samples the moment they enter the process up to the time they are distributed.\nThe samples are prepared for conservation wherein methods differ per sample type. Seeds undergo cleaning, drying, moisture content determination, and packing. As for plant materials to be used in tissue culture or cryopreservation, extraction and disinfection are done.\nThe seeds’ and plant materials’ quality are tested to ensure that they are viable and free of pests and diseases.\nThere are two types, the in situ and ex situ. In in situ, germplasms are conserved and maintained in their natural habitat. This type is not deemed to be the best option. On the other hand, ex situ conservation offers an efficient and effective solution. Materials are placed under artificial conditions in a controlled environment such as cold storage. Seeds are stored at -18°C to -20°C to maintain viability and specially prepared in vitro culture samples are stored long-term at -196°C, usually in liquid nitrogen.\nThe expression of highly heritable characters ranging from morphological or agronomical features to seed proteins or molecular markers in plant germplasm is determined. This is done by growing a representative number of plants in the field.\nThis is done to increase the number of initial samples and replenish stocks. This process is very tedious as it requires careful adherence to special requirements to prevent loss of genetic integrity.\nThe preserved samples are made available for germplasm users, breeders, researchers, and farmers.\nThe seeds are investments that are soon to bear the fruits of labor. As the global demands for genebanking continue to increase, this calls for the utilization of engineering controls that are up-to-date and efficient. Taking part in guarding diversity, Esco Scientific innovates its products to help combat undernutrition, discover novel drugs and climate-proof crops, and ultimately, supply food to every table.\nA Horizontal Laminar Flow Cabinet is used to provide sample protection in preparing explants and media for tissue culture.Learn More Request for Quote\n Genebank Procedures Overview. (n.d.). https://cropgenebank.sgrp.cgiar.org/\n Guardians of Diversity: The Network of Genebanks Helping to Feed the World. (2019, November 07). CGIAR. https://www.cgiar.org/news-events/news/guardians-of-diversity-the-network-of-genebanks-helping-to-feed-the-world/","Harnessing the Benefits of Publicly-Funded Research\nby Pluvia Zuniga, UNU Maastricht Economics and Social Research Institute on Innovation and Technology and\nSacha Wunsch-Vincent, Senior Economic Officer, Economics & Statistics Division, WIPO\nOver the last 30 years, high-income economies have sought to maximize the benefits of publicly-funded research to accelerate knowledge transfer and entrepreneurship and to fuel innovation and economic growth. As a consequence, universities and public research organizations (PROs) in these countries are becoming more strongly business-focused. In light of the perceived benefits of strengthening university-industry links, particularly in terms of stimulating innovation and promoting technology transfer, many middle and low-income economies are adopting similar approaches. This is causing analysts to look more closely at these policies. Can they be readily exported from one setting to another? Is university patenting an efficient driver of business innovation? What is the impact of such policies in terms of economic growth and knowledge generation? This third article in WIPO Magazine’s Innovation Trends series takes a closer look at the evolving landscape and considers the merits of more active use of the intellectual property (IP) system by universities and PROs in middle and lower-income settings.\nEvolving policy frameworks\nPublic-private knowledge transfer occurs through a large number of formal channels (including research collaboration, licensing university inventions, joint ventures, and hiring university students and researchers) and informal channels (including publications and conferences). IP can also play a key role in terms of fuelling innovation and driving business development through, for example, incubators, science parks and university spin-offs.\nThere has been a marked trend over the past three decades in high-income economies – also more recently, in selected middle and low-income economies – toward institutional ownership and commercialization of university and PRO inventions. Policy frameworks and practices are constantly evolving in both more and less developed countries giving rise to a broadly diverse range of legal and policy approaches for maximizing returns on publicly-funded research.\nSpecific rules defining the scope of university patenting, invention disclosure and incentives for researchers (such as royalty sharing) also vary. One clear message that emerges from this web of policy and practice is that changes to the legal framework alone are not sufficient to trigger sustained patenting by research institutes anywhere. In the US, for example, university patenting is being driven not only by a favorable legal environment, but also by expanding technological opportunities in the biomedical and other high-tech fields.\nFigure 1: Universities’ and PROs’ patents are increasing under the PCT. PCT applications from PROs and\nuniversities worldwide, absolute numbers (left) and as a percentage of total PCT applications (right), 1980-2010\nSource: WIPO Statistics Database.\nPatent filings by universities and public research organizations increasing\nIn the absence of comprehensive data on formal and informal university-industry relationships, data on patents and licenses offer useful insights into the scale of university knowledge transfer and research performance. Since 1979, the number of international patent applications filed under the Patent Cooperation Treaty (PCT) by universities and PROs has increased steadily by 5 percent and 29 percent respectively (see Figure 1), outpacing the overall rate of growth in PCT applications.\nThis growth has been driven largely by high-income economies, among which France, Germany, Japan, the UK and the US represent approximately 72 percent of all university and PRO PCT applications.\nData for the period 1980-2010 show that patenting by universities and PROs is highly concentrated and confined to the science-driven biomedical and pharmaceutical sectors. Universities and PROs in the US filed 52,303 and 12,698 international applications respectively. PROs in France filed the second largest number of international applications with 9,068, followed by Japan with 6,850. Among middle-income countries, Chinese universities led the way with 2,348 international applications followed by Brazil, India and South Africa. China and India together accounted for 78 percent of all international applications filed by PROs from middle-income economies.\nLicensing by universities and public research organizations growing but from low levels\nLicensing activity – the number of agreements concluded and revenues generated – is a good indicator of university technology transfer. While sparse, data for high-income economies support the view that university and PRO licenses and related income are growing, albeit from low levels. Outside the US, however, licensing activity is modest compared to the number of patent applications filed by PROs, income derived from research and development (R&D) contracts, and consulting or R&D expenditure. Licensing-derived revenue is largely driven by a few institutions operating in the pharmaceutical, biomedical and software sectors. In middle and low-income countries, IP commercialization is limited to just a few patenting institutions. Other forms of IP (e.g. copyright, trade secrets) and know-how are more commonly used to transfer knowledge to businesses in these settings.\nImpacts and challenges\nThe jury is still out on the economic impacts of IP-based technology transfer laws and practices.\nSome experts favor encouraging universities and PROs to patent inventions arguing that it enables them to “reveal their inventions”, encourages follow-on innovation and helps create a market for such inventions. The rationale is that university inventions often need further development to be useful, and firms are unlikely to invest in further development without an exclusive license.\nOthers argue that patents can slow the diffusion of these university inventions (through the exclusive licensing of patents to a single firm) and can stifle innovation and technology transfer by limiting the diversity of research and by negatively impacting other informal channels for knowledge exchange.\nThe possible benefits and costs to firms, universities and PROs, as well as the broader systemic impacts on science, the economy and society, are outlined in Table 1.\n|Table 1: Systemic impacts of IP-based technology transfer policies|\n|Potential Benefits||Potential Costs (or Investments)|\n|Universities & PROs||\n1) Increased IP ownership facilitates entrepreneurship and economic specialization\n2) Faculty-industry cross-fertilization\n3) Increased student intake and ability to place students in firms\n1) Diversion of time away from academic research\n2) IP-related costs & resource requirements associated with\n|Firms||1) Easy access to useful university inventions\n||1) Blocked access to university inventions\n|Potential Benefits||Potential Costs|\n|Broader impacts on science||1) Increased impact of more focused and relevant applied research\n2) Improved innovation system linkages\n3) Improvement in the quality of research and education\n|1) Reorientation of research\n2) Negative impacts on open science\n3) The promise of university income can reduce government commitment to funding\n|Innovation and growth||\n1) Commercialization of inventions with economic and social impacts\n2) (Localized) positive impacts on R&D, technology spillovers, entrepreneurship, employment and growth\n3) Higher competitive position of country in the global market\n1) Long-run negative effect as attention is diverted away from academic knowledge production\n2) Long-run negative effects of IP on open science and follow-on innovation\n3) Focus on IP might inhibit rather than promote commercialization of inventions\nExperience of high-income countries\nResearch relating to the experiences of high-income economies confirms that university and PRO patenting and efficient technology transfer policies and institutions are an important precondition for increasing opportunities to commercialize university inventions. Access to early-stage research is critical to firms, in particular in the science-intensive sectors. Closer university-industry linkages have also proven effective in fostering research into more socially relevant outputs.\nStudies show that university patenting and licensing have underpinned the emergence of new industries (e.g., the scientific instruments industry, semiconductors, computer software and biotechnology industries), as well as the creation of high-technology clusters. It is, however, difficult to demonstrate with any certainty the contribution that commercialization of university IP makes to economic development. Constructing data that effectively capture other dimensions of the impacts of IP-based technology transfer – for example, productivity gains of downstream firms using or building on such IP, or a consumer surplus from resulting innovation – remains a challenge.\nOn top of this, there are no clear signals as to the most adequate IP ownership model for universities. For example, it is not clear whether the university-ownership model is superior to one in which faculty retains ownership of inventions, or one in which the individual scientists retain IP rights. It is equally challenging to identify the long-term implications of university patenting for other knowledge transfer channels and more globally for the broader science system.\nChallenges facing low and middle-income economies\nLow and middle-income countries vary substantially with regard to the R&D capacity of PROs, infrastructure and policy frameworks for technology transfer and science-industry cooperation.\nInnovation systems in these economies are characterized by a lower level of science and technology activity (S&T); a greater share of publicly-funded R&D with less relevant outputs; and limited science-industry linkages. This can be attributed to the low absorptive capacity of firms, combined with an ensuing lack of “business” demand for science and technology, as well as a range of other constraints relating to entrepreneurship and access to financing for innovation.\nTechnology transfer policies unaccompanied by policies seeking to strengthen both the R&D capabilities of firms and industry-science linkages are unlikely to be successful. Broader institutional reforms are also needed, for example, to enhance the autonomy of universities and ease regulations governing the terms of employment of scientists so as to encourage more proactive participation in technology transfer activities.\nPolicymakers in middle and low-income economies face low levels of awareness within universities and few incentives encouraging participation in IP-related technology transfer. Few universities and PROs have clear technology transfer policies, and efforts to strengthen university-industry linkages are further hamstrung by inadequate resources and skills shortages. However, these characteristics are not shared equally across all middle and low-income countries. In general, work is ongoing to improve the systemic weaknesses in national innovation systems to give increased autonomy to universities. Many are in the midst of setting up technology transfer policies and practices, some of which are already having a significant impact. For example, Brazil and Mexico have enacted explicit regulations regarding IP ownership and university technology transfer. In India, institutional policies have recently been developed at key national academic and research organizations. While Nigeria and Ghana do not have specific legislation relating to university patenting, both are in the process of establishing technology transfer offices within institutions of higher education.\nThere is growing evidence that IP-based technology transfer policies and institutions are instrumental in increasing opportunities for commercializing university inventions and in securing university-industry synergies. Amid the broadly diverse national policies being adopted to maximize the impact of publicly-funded research, however, there is, as yet, no clear blueprint for success. The ongoing experiences of both high-income and selected middle and low-income economies in this area, will, no doubt, offer important and useful insights to all those involved in crafting and implementing optimal innovation systems for the future.\nThe WIPO Magazine is intended to help broaden public understanding of intellectual property and of WIPO’s work, and is not an official document of WIPO. The designations employed and the presentation of material throughout this publication do not imply the expression of any opinion whatsoever on the part of WIPO concerning the legal status of any country, territory or area or of its authorities, or concerning the delimitation of its frontiers or boundaries. This publication is not intended to reflect the views of the Member States or the WIPO Secretariat. The mention of specific companies or products of manufacturers does not imply that they are endorsed or recommended by WIPO in preference to others of a similar nature that are not mentioned."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:0647747c-d18e-447c-81b7-a6d9bf39a397>","<urn:uuid:fdeadc09-da5e-4001-aa4e-bb0d642b87a7>"],"error":null}
{"question":"How does otitis media differ from Meniere's disease in terms of symptoms and affected parts of the ear?","answer":"Otitis media and Meniere's disease affect different parts of the ear and have distinct symptoms. Otitis media is an inflammation or infection of the middle ear, usually associated with fluid effusion that may remain for a long time. In contrast, Meniere's disease affects the inner ear and is characterized by three main symptoms: vertigo (spinning sensation lasting at least 20 minutes), hearing loss (usually affecting one ear and lower frequencies), and tinnitus (ringing or noise in the ears). While otitis media is very common in childhood, Meniere's disease typically occurs in the 40-60 year age group and is more common among females.","context":["Anatomy of the ear\nThe ear is the organ of hearing. It is divided into 3 parts, the external, middle and inner ear. Sound travels through the outer ear, then into the external auditory canal to the eardrum. Sound causes the eardrum to vibrate and these vibrations are transmitted, via the middle ear bones, to the fluid filled hearing organ, the cochlea, where sound is transformed into nerve impulses. The impulses are then sent to the brain via the auditory nerve and there they are translated to sound.\nOtitis media is an inflammation or infection of the middle ear. It is very common in childhood, but may appear at any age. It is usually associated with a fluid effusion that may remain for a long period of time. This is called Chronic Otitis Media with Effusion.\nInflammation or infection of the outer ear and ear canal. Otitis externa usually presents with earache and discharge.\nThis is the sensation of “ringing” or \"noise\" in one or both ears, when no sound is actually present. It is usually a result of hearing loss and is a common complaint.\nThis condition occurs when sound signals are not reaching the brain. The are 2 types of hearing loss, sensorineural and conductive. The type of loss depends on where the problem is located.\nEar wax (cerumen)\nIt is the substance that protects the skin of the ear canal from infections, fungi and bacteria. It also helps in cleaning and lubrication of the ear. When it is excessive, it may block the eardrum and cause reduced vibrations and hearing loss. It can also be very painful when impacted in the external ear canal.\nAcoustic neuroma (Vestibular schwannoma)\nThis is a slow growing, benign tumor arising from the Schwann cells of the vestibulocochlear nerve travelling from the inner ear to the brain. It is most common in the 5th and 6th decades and affects both sexes. It usually presents with hearing loss and dizziness.\nThis is an infection that affects the air cells of the skull behind the ear. It is caused by untreated otitis media. Its incidence has fallen down considerably with the use of antibiotics.\nClassically, this is a syndrome of vertigo, tinnitus and hearing loss. It affects the inner ear and can be quite debilitating. It tends to recur and can cause worsening hearing loss.\nBening paroxysmal positional vertigo (BPPV)\nIs a disorder that is thought to be due to loose particles in the labyrinth. It is one of the commonest causes of vertigo. Episodes last several seconds to a minute and are caused by rotating the head to the side, usually in bed.\nThis is a hole in the eardrum, which may be a result of trauma, otitis media or proximity to an explosion. It usually leads to conductive hearing loss, which is temporary and resolves when the tympanic membrane has healed.\nThis is a condition where a cyst-like skin growth develops in the ear. If it is not treated, it continues to grow and damages the structures of the ear, such as the ossicles.","There ear is divided into three parts – the outer, middle and inner ear. The first two parts are mainly responsible for hearing while the inner ear is responsible for both hearing and balance. Conditions affecting the inner ear will therefore lead to symptoms such as hearing loss, tinnitus and lead to balance and movement-related symptoms like vertigo. Meniere’s disease is one such condition.\nWhat is Meniere’s disease?\nMeniere’s disease is a condition where there is increased pressure within the inner ear. Since the inner ear is responsible for both the sense of hearing and for balance, Meniere’s disease will often affect these functions. Therefore symptoms such as hearing loss, tinnitus (ringing in the ears) and dizziness are often present in Meniere’s disease. It may be of varying intensity and fluctuate during the course of the disease.\nOverall Meniere’s disease is not very common but is not a rare condition either. It is known to affect about 1 in 100 people but the prevalence could be much higher as the condition may be under reported. Although Meniere’s disease has been reported in children as young as 4 years, it is most likely to occur in the 40 to 60 year age group. It tends to be more common among females.\nCauses of Meniere’s Disease\nThe cause of Meniere’s disease is not exactly known. In fact the medical term for Meniere’s disease is idiopathic endolymphatic hydrops, and the term idiopathic means of unknown cause. To some extent the exact mechanism is also not clearly understood but the condition appears to be due to increased fluid pressure within the structure of the inner ears.\nThe labyrinth is a hollow cavity within the ear which is filled with fluid. It has different parts, one of which is the cochlea and the other being the vestibule. The cochlea is the hearing apparatus while the vestibule is reponsible for balance. There are two types of fluid within the labyrinth – endolymph and perilymph – which are separate by a membrane.\nIn Meniere’s disease, it appears that there is an increase of endolymph which raises the pressure within the labyrinth of the inner ear. It is unclear as to whether this increase of fluid is the cause of the disease or occurs as a result of the disease. In fact, there are patients who have increased fluid pressure within the inner ear but do not experience any of the symptoms that are characteristic of Meniere’s disease.\nThe increased fluid pressure appears to cause disturbances within the organs of hearing and balance. Although the exact reason for this increase in fluid is unclear in Meniere’s disease, it can be occur with infections, allergies and trauma. Blocked drainage of fluid, immune disturbances, genetic factors, head injury and migraines may also play a role in a rise in fluid pressure within the ear but this may not always lead to Meniere’s disease.\nSigns and Symptoms\nThe symptoms of Meniere’s disease are usually episodic. These episodes come and go with symptom-free gaps that may last for months or even years. The three main symptoms of Meniere’s disease are vertigo, hearing loss and tinnitus. Other symptoms like a feeling of fullness in the ear may also be present. The auditory symptoms are usually one-sided but can sometimes affect both sides (bilateral).\nVertigo is a sensation of moving despite being stationary. Most people describe it as a spinning sensation. With Meniere’s disease, vertigo can come and go suddenly. It may last anywhere from a few minutes to several hours. These episodes of vertigo must last for at least 20 minutes for a diagnosis to be made. It may be accompanied by nausea and vomiting.\nThe hearing loss in Meniere’s disease may vary in intensity and duration. It is sensorineural hearing loss and usually only affects one ear. However, it can occur in both ears. The hearing loss is not always obvious. Often it is only the ability to hear lower frequencies that are normally audible to humans which is lost. In the long term there tends to be some degree of permanent hearing loss.\nTinnitus is the perception of sound despite an external source. It is often described as a ringing sensation but may also be a swooshing, roaring, whistling or buzzing sound in the ears. This abnormality usually correlates with the hearing loss and can be continuous or intermittent.\nTreatment of Meniere’s Disease\nAlthough there is no cure for Meniere’s disease, treatment can help control the symptoms. Some drugs may also reduce the frequency of Meniere’s disease attacks. In severe cases that do not respond to medication and other non-invasive techniques, surgery may be considered.\n- Antiemetics typically used for nausea and vomiting can help to suppress the vertigo. Drugs used for motion sickness may also be helpful in this regard.\n- Diuretics increase the passing of water through urine and can be helpful in reducing the fluid pressure within the inner ear. These drugs may be used for long term management.\n- Corticosteroids reduce inflammation and appear to help ease the fluid pressure in the inner ear. It does not resolve the condition but helps reduce the severity of symptoms.\n- Gentamicin is an antibiotic that may help reduce the severity and duration of vertigo. It may be administered as an injection.\n- Endolymphatic sac procedure helps reduce the pressure (decompression) within the inner ear and a shunt may also be done to drain excess fluid.\n- Vestibular nerve section involves cutting the vestibular nerve which relays signals about movement and balance back to the brain. This helps with reducing vertigo.\n- Labyrinthectomy involves removing a portion of the inner ear but impacts on hearing and will only be considered when there is complete hearing loss, or almost complete loss, in the affected ear.\nOther therapies focus on helping a person suffering with Meniere’s disease to cope with the symptoms of the disease. This may include:\n- Hearing aids increase the volume of environmental sound to help improve impaired hearing.\n- Meniett device to apply pressure to the middle ear which aids with fluid exchange in the inner ear. It helps with vertigo."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:8c621d95-8d0a-4ea6-a2eb-cee5fc350885>","<urn:uuid:f8798068-6fa4-46a5-b439-9359d2bea29c>"],"error":null}
{"question":"What is the key difference between meiosis and minimalism in their fundamental concepts?","answer":"Meiosis and minimalism represent very different concepts. Meiosis is a type of cell division where a diploid cell nucleus generates four haploid nuclei with half the complement of chromosomes, crucial for sexual reproduction and genetic diversity. Minimalism, on the other hand, is the principle of using the minimum means necessary to achieve a desired result, originally associated with various artistic fields from the 1920s onwards, including pictorial arts, sculpture, architecture, and literature.","context":["Minimalism, the principle of using the minimum means necessary to achieve a desired result, was originally associated with various pictorial artists from the 1920s on, and was later introduced into sculpture, architecture, and literature. Michael Nyman introduced the idea of “minimal music” in an article in the Spectator in 1968. Exponents include the artists Anne Truitt and Frank Stella, the musicians Steve Reich and Philip Glass, the writers Raymond Carver and Samuel Beckett, and the film makers Robert Bresson and Jim Jarmusch.\nYves Klein’s minimalist painting, IKB 79 (1959), painted in his typical choice of colour, IKB (International Klein Blue)\n“Minimalism” comes from the IndoEuropean root MEI, meaning small. In Greek μειοῦν meant to lessen; and μείων (adjective) and μεῖον (adverb) meant less.\nMeiosis was originally a figure of speech in which something is intentionally represented as being smaller or less important than it really is; by extension it came to mean understatement, often with ironic intent. As you might, for example, say “a bit of a headache” for an intense bout of migraine. Meiosis has also sometimes been used to mean litotes, which is from another Greek word meaning small, λῑτός. But litotes is a different figure of speech, in which understatement is used for emphasis. If, for example, you say “that’s not the smallest tumour I’ve ever seen”, you really mean that it’s the biggest.\nMeiosis was once a term, now obsolete, for the stage of a disease at which symptoms start to abate. Meiosis is also a stage in cell division, in which a diploid cell nucleus generates four haploid nuclei, with half the complement of chromosomes. Miosis, on the other hand, is constriction of the pupil of the eye. It was originally spelt myosis, since it comes from another Greek word altogether, μύειν, to shut the eyes. However, confusion with meiosis resulted in the current spelling.\nThe prefix meio- sometimes indicates a stage between macro and micro. The meiobenthos, for instance, includes animals somewhere in size between microfauna and macrofauna, found at or near the bottom of the sea. And the geological Miocene period came between the Oligocene and the Pliocene (Greek ὀλίγος few and πλείων more).\nNow add an n to MEI and you get words with small connotations, such as mince (meat chopped up small), minor, minuet, minuscule, minute, minutiae, comminute and diminish (Latin minuere, to reduce), minim and minimum.\nA minister was originally an inferior or a servant, contrasted with magister, a master; and minestrone soup was something served at table.\nWhen Lenin’s party won a debate by a single vote at the end of the second congress of the Russian Social-Democratic Workers’ Party in 1903, they were called Bolsheviks or members of the majority (больше, more); their opponents were called Mensheviks, members of the minority (меньше, less).\nThe word “minimus” entered English in the late 16th century, meaning a very small or insignificant creature. It is first recorded, according to the OED, in A Midsummer Night’s Dream (III.ii.330), when Lysander says to Hermia “Get you gone, you dwarf; You minimus of hindering knotgrass made, You bead, you acorn.” Lysander has taken his cue from Helena, who taunts Hermia over her height: “O, when she’s angry, she is keen and shrewd! She was a vixen when she went to school; And though she be but little, she is fierce,” to which Hermia retorts “’Little’ again! nothing but ‘low’ and ‘little’! Why will you suffer her to flout me thus?”\nHermia and Helena (1818) painted by Washington Allston, “the American Titian” (1779–1843)\nThe next appearance of the word is not until the late 18th century, when it was used as a postnominal denoting the rank of an individual in a family of siblings. As Charles Rowcroft put it, in Confessions of an Etonian (1852), “The boys at Eton are not known by their Christian names, and when there are more than one bearing the same surname, … the individuals are distinguished by the addition of maximus, major, minor, and minimus.”\nIn the middle of the 19th century, minimus started to be used to describe various very small, ancient, usually Roman, bronze or silver coins found in Britain. The term was later shortened to “minim” a term that is still used today.\n“Minimus” meaning an animal’s little finger or toe, from the Latin digitus minimus, is a late entry in English, first appearing in the late 19th century. I shall discuss it next week.\nJeffrey Aronson is a clinical pharmacologist, working in the Centre for Evidence Based Medicine in Oxford’s Nuffield Department of Primary Care Health Sciences. He is also president emeritus of the British Pharmacological Society.\nCompeting interests: None declared.","Sign In / Sign Out\n- ASU Home\n- My ASU\n- Colleges and Schools\n- Map and Locations\nCell: a tiny building block that contains all the information necessary for the survival of any plant or animal. It is also the smallest unit of life... more\nChromosome: a long, thread-like molecule made of the chemical called DNA (deoxyribonucleic acid) that is held together with special proteins and is visible (with strong microscopes) during cell division... more\nDiploid cell: a cell with two sets of chromosomes (46 chromosomes total)... more\nDNA: deoxyribonucleic acid is the information \\\"blue-print\\\" of the cell. It is a nucleic acid and is made from building blocks called nucleotides. This genetic information is passed from parent to child... more\nSometimes you accidentally bite your lip or skin your knee, but in a matter of days the wound heals. Is it magic? Or, is there another explanation?\nEvery day, every hour, every second one of the most important events in life is going on in your body—cells are dividing. When cells divide, they make new cells. A single cell divides to make two cells and these two cells then divide to make four cells, and so on. We call this process \"cell division\" and \"cell reproduction,\" because new cells are formed when old cells divide. The ability of cells to divide is unique for living organisms.\nCells divide for many reasons. For example, when you skin your knee, cells divide to replace old, dead, or damaged cells. Cells also divide so living things can grow. When organisms grow, it isn't because cells are getting larger. Organisms grow because cells are dividing to produce more and more cells. In human bodies, nearly two trillion cells divide every day.\nWatch cells divide in this time lapse video of an animal cell (top) and an E. coli bacteria cell (bottom). The video compresses 30 hours of mitotic cell division into a few seconds. (Video by the National Institute of Genetics)\nYou and I began as a single cell, or what you would call an egg. By the time you are an adult, you will have trillions of cells. That number depends on the size of the person, but biologists put that number around 37 trillion cells. Yes, that is trillion with a \"T.\"\nIn cell division, the cell that is dividing is called the \"parent\" cell. The parent cell divides into two \"daughter\" cells. The process then repeats in what is called the cell cycle.\nCells regulate their division by communicating with each other using chemical signals from special proteins called cyclins. These signals act like switches to tell cells when to start dividing and later when to stop dividing. It is important for cells to divide so you can grow and so your cuts heal. It is also important for cells to stop dividing at the right time. If a cell can not stop dividing when it is supposed to stop, this can lead to a disease called cancer.\nSome cells, like skin cells, are constantly dividing. We need to continuously make new skin cells to replace the skin cells we lose. Did you know we lose 30,000 to 40,000 dead skin cells every minute? That means we lose around 50 million cells every day. This is a lot of skin cells to replace, making cell division in skin cells is so important. Other cells, like nerve and brain cells, divide much less often.\nDepending on the type of cell, there are two ways cells divide—mitosis and meiosis. Each of these methods of cell division has special characteristics. One of the key differences in mitosis is a single cell divides into two cells that are replicas of each other and have the same number of chromosomes. This type of cell division is good for basic growth, repair, and maintenance. In meiosis a cell divides into two cells that have half the number of chromosomes. Reducing the number of chromosomes by half is important for sexual reproduction and provides for genetic diversity.\nMitosis is how somatic—or non-reproductive cells—divide. Somatic cells make up most of your body's tissues and organs, including skin, muscles, lungs, gut, and hair cells. Reproductive cells (like eggs) are not somatic cells.\nIn mitosis, the important thing to remember is that the daughter cells each have the same chromosomes and DNA as the parent cell. The daughter cells from mitosis are called diploid cells. Diploid cells have two complete sets of chromosomes. Since the daughter cells have exact copies of their parent cell's DNA, no genetic diversity is created through mitosis in normal healthy cells.\nBefore a cell starts dividing, it is in the \"Interphase.\" It seems that cells must be constantly dividing (remember there are 2 trillion cell divisions in your body every day), but each cell actually spends most of its time in the interphase. Interphase is the period when a cell is getting ready to divide and start the cell cycle. During this time, cells are gathering nutrients and energy. The parent cell is also making a copy of its DNA to share equally between the two daughter cells.\nThe mitosis division process has several steps or phases of the cell cycle—interphase, prophase, prometaphase, metaphase, anaphase, telophase, and cytokinesis—to successfully make the new diploid cells.\nWhen a cell divides during mitosis, some organelles are divided between the two daughter cells. For example, mitochondria are capable of growing and dividing during the interphase, so the daughter cells each have enough mitochondria. The Golgi apparatus, however, breaks down before mitosis and reassembles in each of the new daughter cells. Many of the specifics about what happens to organelles before, during and after cell division are currently being researched. (You can read more about cell parts and organelles by clicking here.)\nMeiosis is the other main way cells divide. Meiosis is cell division that creates sex cells, like female egg cells or male sperm cells. What is important to remember about meiosis? In meiosis, each new cell contains a unique set of genetic information. After meiosis, the sperm and egg cells can join to create a new organism.\nMeiosis is why we have genetic diversity in all sexually reproducing organisms. During meiosis, a small portion of each chromosome breaks off and reattaches to another chromosome. This process is called \"crossing over\" or \"genetic recombination.\" Genetic recombination is the reason full siblings made from egg and sperm cells from the same two parents can look very different from one another.\nMeiosis has two cycles of cell division, conveniently called Meiosis I and Meiosis II. Meiosis I halves the number of chromosomes and is also when crossing over happens. Meiosis II halves the amount of genetic information in each chromosome of each cell. The end result is four daughter cells called haploid cells. Haploid cells only have one set of chromosomes - half the number of chromosomes as the parent cell.\nBefore meiosis I starts, the cell goes through interphase. Just like in mitosis, the parent cell uses this time to prepare for cell division by gathering nutrients and energy and making a copy of its DNA. During the next stages of meiosis, this DNA will be switched around during genetic recombination and then divided between four haploid cells.\nSo remember, Mitosis is what helps us grow and Meiosis is why we are all unique!\nBianconi E, Piovesan A, Facchin F, Beraudi A, Casadei R, Frabetti F, Vitale L, Pelleri MC, Tassani S, Piva F, Perez-Amodio S, Strippoli P, Canaider S. Ann. An estimation of the number of cells in the human body. Retrieved March 14, 2014 from http://www.ncbi.nlm.nih.gov/pubmed/23829164.\nDr. Biology. (2014, February 03). Cell Division. ASU - Ask A Biologist. Retrieved April 22, 2018 from https://askabiologist.asu.edu/cell-division\nDr. Biology. \"Cell Division\". ASU - Ask A Biologist. 03 February, 2014. https://askabiologist.asu.edu/cell-division\nDr. Biology. \"Cell Division\". ASU - Ask A Biologist. 03 Feb 2014. ASU - Ask A Biologist, Web. 22 Apr 2018. https://askabiologist.asu.edu/cell-division"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:e3a8e7eb-a7cb-446a-b91c-c247ecec4588>","<urn:uuid:cdce2445-7f43-4060-abcb-7b13bc748a8f>"],"error":null}
{"question":"I'm interested in understanding why Turin is considered historically significant - what are its major historical landmarks from different eras?","answer":"Turin has historical significance dating back over 2000 years. The city contains archaeological remains from the Roman period, Romanesque churches, and palaces from the Baroque period. It was also the first capital of Italy and is home to the richest Egyptian Museum in the West, second only to Cairo's. Other significant historical palaces include the Palazzina di Caccia di Stupinigi and the Venaria Reale, which are located in wooded areas around the city.","context":["Piedmont and Lombardy accessible are among the most industrialized Italian Regions. Located in the North Western part of Italy at the foot of the Alps.\nThey are regions rich in natural beauty even if they differ from the classical iconography of the Italian environment. High mountains (the highest in Europe), large lakes, woods, villages and large cities:\nMilan & Turin:\nMilan (the economic capital of Italy), Turin (the first capital of Italy and seat of the largest Italian industry, FIAT now FCA) .\nDespite being modern cities, they retain monuments from their long past that dates back to over 2000 years ago.\nIn Turin there are archaeological remains of the Roman period, Romanesque churches, palaces of the Baroque period.\nIn Milan the oldest remains are few traces of an imperial palace of the Western Roman Empire of the third century AD metre more are the Romanesque churches, buildings of the medieval period and the last centuries.\nBoth cities have important museums and cultural institutions.\nTurin hosts the richest Egyptian Museum in the West second only to that of Cairo. Other palaces include the Palazzina di Caccia di Stupinigi and the Venaria Reale located in wooded areas around the city.\nIn addition to the aforementioned Romanesque churches, Milan has interesting museums. One of the most interesting is the “Leonardo da Vinci” Science Museum.\nIn general, both cities have good accessibility with walkable sidewalks and, especially in the central area of Turin, covered by arcades.\nThere are slides at the crossings.\nMilan has the largest underground network in Italy and most of the stations are accessible.\nTurin has only a sketch of a metropolitan network with a single line that crosses it and is, in some points connected with the railway network, thus increasing the number of points in the city that can be reached.\nAll the stations are accessible.\nBoth cities have accessible surface public transport even if the buses are crowded and therefore access with wheelchairs is difficult.\nThere are no accessible taxis, but accessible transport with driver for private services can be booked on request.\nBoth cities host important trade fairs. Milan which has 2 exhibition centers is the Italian city which hosts the largest number of trade fairs, exhibitions, fashion shows, …\nFor any service connected with participation in Fairs and Exhibitions both in Milan and Turin for exhibitors or visitors with reduced mobility, we can provide private transport (with accessible vehicles), car rental with manual controls, accessible hotels of any category, personal assistance and accompaniment carried out by professionals in the desired language.\nMilan is where, the main Italian Opera Theater and one of the first in the world, La Scala Theater is located.\nAccessibleItaly can arrange stays combined with shows in La Scala, organizing accessible transports, booking accessible hotels in a convenient position to reach the theater and book seats in positions that can be easily reached by wheelchair.\nPiedmont and Lombardy accessible to travelers by wheelchair\nLombardy has many cities rich in history and with fascinating historical centers: Bergamo, Mantua, Pavia, Cremona, … 4 large lakes: Lake Maggiore, Lake Como, Lake Iseo and Lake Garda that create a unique habitat with small villages and beautiful lake landscapes and are crossed by navigation.\nPiedmont is known for its mountains in whose ski resorts even people with disabilities can practice skiing.\nSome of the areas of Piedmont are highly reputed for food and wine tourism such as the Langhe where some of the most famous Italian wines are produced: Barolo, Barbaresco, Nebiolo and gastronomy based on rare and precious foods such as white truffles.\nTours & Stays:\n=====PIEDMONT , LOMBARDY AND TURIN, MILAN MUSEUM INFO=====\nPiedmont & Turin:\nMuseums Turino: www.museireali.beniculturali.it\nEgyptian Museum: www.museoegizio.it\nStupinigi hunting lodge: www.ordinemauriziano.it (only Italian)\nRoyal Palace of the Venaria Reale: www.lavenaria.it\nTransports Turino (Map): www.gtt.to.it\nLanghe wines and cuisine: www.langheroero.it\nMont Blanc skyway: www.montebianco.com\nNavigation Piedmont & Lombardy Lakes: www.navigazionelaghi.it (In English too)\nLombardy & Milan:\n“Leonardo da Vinci” Science Museum: www.museoscienza.org\nLa Scala Theater: www.teatroallascala.org\nTransports Milan: www.atm.it\nTransports Milan (Map): giromilano.atm.it\nAccessible Toilets: www.lonelyplanet.com"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:82b4223c-2b21-42d6-8773-f35120ab9ee0>"],"error":null}
{"question":"In the context of both thermodynamic processes and marine safety, what are the key differences between reversible and irreversible processes, and how do they impact engine room safety protocols?","answer":"In thermodynamic processes, reversible processes occur when opposing pressures differ by only an infinitesimal amount, yielding maximum work output. In contrast, irreversible processes, such as rapid gas expansion against negligible pressure, produce minimal or no work. This principle has critical safety implications in ship engine rooms, where uncontrolled pressure differences can lead to catastrophic accidents. For instance, compressor airline explosions can occur when discharge valves are improperly closed and relief valves fail, creating dangerous pressure differentials. Similarly, boiler explosions can result from improper pressure management, such as overheating due to loss of water circulation or inadequate pre and post purging procedures.","context":["Heat, work and law of conservation of energy\nHeat is another mode by which a system can exchange with the surroundings. Wherever a temperature difference exists between the system and surroundings heat either flows in or out of the system. It is not a state function because the quantity of heat involved in a process depends upon the path. Heat like work is an extensive property.\nIt is not a property of system or surroundings. Heat as such cannot be measured at all, but the effects which it produces are measured. It was formerly measured by the increase in temperature of water. The amount of heat required to raise the temperature of 1 g of water by 1°C becomes a unit of heat, the Calorie.\nIn modern practice, it is defined in terms of Joule. Because Joule, in 1850, showed that there is a definite relationship between mechanical work done (w) and heat produced (H).\n‘J’ is known as mechanical equivalent of heat. Its numerical value is taken as ergs = 4.185 Joules.\nHeat is an algebraic quantity and the convention used for heat is q. A + q shows that heat is added to the system and a – q means that the system has lost the heat.\nMechanical work is done whenever there is a change (increase or decrease) in the volume of the system i.e. expansion or compression of a gas.\nThis is known as Pressure volume work or PV work or Expansion work.\nConsider a gas enclosed in a cyclinder provided with a piston.\nIf P is the pressure of the gas, it exerts a force F on the piston given by:\nF = PA . .. (i)\nwhere A is area of cross-section of the piston. This force can be balanced by an equal force, Fem acting on the piston, (fig). If there is an infinitesimally small movement of the piston (dl) outward, the small amount of work done (dw) by the gas (system) on the surroundings will be given by,\nwhere dV is the small increase in volume of gas that has taken place in the process. This process of expansion may be carried out in infinitesimally slowly (i.e. in a thermodynamic reversible) manner in a series of steps. The work done in each step will be given by PdV.\nIf, ultimately, the volume of the system changes by a finite quantity, say, from then the total work (w) done by the system on the surroundings will be obtained by the integration of the factor PdV.\nwhere P is a variable factor.\nOn the other hand, if there is infinitesimal contraction of the gas resulting from infinitesimal movement of the piston inward, then the small amount of work done by the surroundings on the system will be given by:\nDw = PdV\nwhere ‘dV’ is the decrease in volume of the gas that has taken place in the process. If the work of contraction is carried out in the above manner in a series of steps, the work done (w) by the surroundings on the system is given by integration of the factor PdV.\nWhen the volume of the system decreases from .\nWork done at constant pressure\nIf the-pressure P remains constant throughout the process, the above integration gives\nwhere is the volume of the system in the initial state,. Then , evidently, is the change in volume of the system.\nIf is positive i.e. the gas expands in the process, w will have a positive value. The work, in this case, is done by the system on the surroundings.\nIf is negative i.e. the gas undergoes contraction, w will have a negative value. The work, in this case, is done by the surroundings on the system.\nWork done at constant volume\nIf the volume is constant,\ndV = 0\nthen w = 0 …..(v)\nIn the above example, suppose the pressure applied on the piston is negligibly small in comparison with the pressure of the gas inside the cylinder. The gas will then expand rapidly i.e. irreversibly. In this case, the work done by the system will be negligibly small since the opposing force has been negligibly small.\nIf the opposing pressure on the piston is zero, the work done by the system will be zero.\nPdV = 0 ……(vi)\nHence, it follows that when a gas expands freely (free expansion ) i.e. when it expands against vacuum such that P = 0, no work is done by the system.\nIt also follows from the above discussion that the magnitude of work done by a system on expansion depends upon the magnitude of the opposing (external) pressure. The close is the opposing pressure to the gaseous system in the cylinder, the greater will be the work performed by it on expansion.\nIn other words, maximum work is obtained when the two opposing pressures differ only by an infinitesimally small amount from one another.\nThis condition, evidently, is demanded for an ideal reversible process. Hence the condition for maximum work coincides with that for thermodynamic reversibility.\n- Heat Capacity of gases Specific and molar heat capacity of gases Specific heat (or...\n- Gas Law Woorksheet When scientists began to realize the relationship between the pressure,...\n- Internal Energy of a System Each substance is associated with a certain amount of energy...\n- Boyle’s Law Boyle’s Law (1662) This law states that, “For a given...\n- Hennery’s law According to Henry, “The ratio of concentration of a gas...","The ship’s engine room is the home to a variety of machinery and systems, which work together to move the ship from one port to another. Engine room professionals have to continuously work amidst such high temperature and pressure systems, which make an extremely hostile working environment.\nIn spite of taking all the precautions and safety measures while handling engine room machinery systems, accidents are bound of take place in the ship’s engine room. Many of these accidents are extremely dangerous not only to the ship’s properly but also the the lives of seafarers. Mentioned below are ten such types of extremely dangerous engine room accidents that occur in ship’s engine room.\n1. Crankcase Explosion of Ship’s Engine\nExplosion of ship’s crankcase is one of the most dangerous accidents in the ship’s engine room which has led to devastating consequences, including loss of lives in the past.\nIn the engine crankcase, oil particles are churned into smaller particles of up to 200 micro meters in diameter. These small particles cannot ignite readily even with some naked flame. However, if a hot spot comes in contact with these small particles, it reduces the size of the particles, resulting in the formation of mist, which can be readily ignited with a hot spot.\nIn the crankcase, all the three elements required for fire are available – lubricating oil (fuel source), air, and heat from a hot spot. Coming together of all these three elements can lead to a major explosion that will not only damage the engine but also take lives of crew members.\n2. Over-Speeding of Generators\nThis kind of accidents though rare have occurred in the past, causing heavy damage and loss of lives. When the ship’s generator starts, there are high changes of it to over-speed. If this occurs and the over-speed trip fails to work properly, the high RPM of the generator leads to failure of internal parts. When such situation go out of control, the internal parts such as crank shaft, connecting rod, nut-bolts etc. become loose, get detached, and are thrown away because of the high speed. If crew members do not evacuate the surrounding place in time, the loosen parts can severely harm the crew members.\n3. Boiler explosion\nEveryone working on ships has heard about boiler explosion as one of the most deadly accidents in the ship’s engine room. A highly pressurized equipment on board ships, boiler has been attached to different kind of accidents as a result of mistakes while operating them. Boiler explosion is one such dangerous accident which is caused because of the following reasons:\n– Fuel dripping inside the furnace of the boiler. If the dripping is more and the boiler is fired after an interval, it can lead to blowback and even explosion.\n– Overheating of boiler due to loss of water circulation\n– No pre and post purging\n4. Compressor Airline Explosion\nAir compressor on ships is also a highly pressurized equipment that can cause deadly accidents. Compressor’s airline explosion is one accident everyone is afraid of. Such explosions usually occurs when during maintenance, the discharge air valve in the line is closed. There is also a common practice among seafarers to shut the discharge valve of the air compressor to minimize air leakage. But when this discharge valve is not opened again while starting the compressor and if the relief valves fail to operate, the airline gets over-pressurized and explodes.\n5. High Pressure Fuel Line Bursting\nAll high pressurized lines and equipment on board ships are accident prone. The high temperature and pressure fuel line which supplies fuel to the combustion chamber of marine engines can explode if proper maintenance is not carried out. Also, if the lines are not adequately secured, they can burst due to continuous vibrations and friction. Fuel line bursting leads to severe burns, injury and even death of seafarers.\nAs per regulations, all high pressurized pipes must be jacketed type to avoid chances of fuel leakages and sprays from the pipe joints.\n6. High Pressure Steam Leakages\nHigh pressurized steam lines are present in several parts of the ship’s engine room. These high temperature steam lines when burst or crack, lead to leakage of steam at extremely high pressure. Steam burns are extremely dangerous and can even cause instant death. Accidents due to steam leakages can occur because of following reasons:\n- Failure of steam joints\n- Steam burns or scalding from opening of boiler mounting valves if not properly isolated or de-pressurized\n- Steam line bursting due to failure of material or crack from vibration or if not properly secured\n7. Hydraulic High Pressure Components Bursting\nHydraulic high pressure equipment tools are used during overhauling of ship’s machinery and other important systems. If these high pressure systems are not properly tested before use, it can lead to bursting of their high pressurized parts and causing serious injury to the ship’s crew operating them. Some of the major types of hydraulic high pressure accidents are:\n- Hydraulic jack oil seal leakage\n- Hydraulic jack oil pipe fracture resulting in high pressure jet of oil\n- Loose or worn-out connection between jack and pipe causing snapping of pipe which can harm the user\n8. Turbo Charger Explosion\nTurbo charger explosion on ships is caused when turbochargers are not cleaned for a long time. When the parts of turbo charger are not cleaned properly, the carbon deposits do not allow the parts to cool down properly. As a result, when the oil gets into the exhaust side of the turbo charger through the cracks, the heated parts and fuel source form the perfect combination of an explosion.\n9. Electrical Shocks\nEquipment and cables carrying high electrical power are extremely dangerous for people working on ships. If any kind of maintenance is carried out on such systems without isolating them properly, then there are high chances of getting electrical shocks. Moreover, accidental starting of electrical equipment during maintenance has also been a serious cause of seafarers deaths in the past. Electrical shocks frequently occur on board ships and therefore adequate precautions must be taken to prevent them.\n10. Accidental CO2 Release\nCO2 system is used to release CO2 in the ship’s engine room during fire emergencies only after all the crew has left the engine room. But accidental release of CO2, when the crew members are still present in the engine room, would lead to instant and tragic death of all. Cases of accidental CO2 release in the ship’s engine room has caused several deaths in the past.\nNote: While testing engine room CO2 alarm, the CO2 pilot bottles should be properly isolated.\nDo you know any other type of dangerous accident that can occur in the ship’s engine room?"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:eeaf979d-645d-40ba-b0db-7fec6a33bb55>","<urn:uuid:f3c0a6ad-6901-475a-bab9-4fe67328f167>"],"error":null}
{"question":"How do European consumer protection mechanisms balance online dispute resolution efficiency with data privacy concerns?","answer":"European consumer protection mechanisms achieve this balance through complementary systems. The dispute resolution system includes an ODR platform launched in February 2016 that processes online complaints within 90 days, with an average resolution time of 40 days. In 68% of cases where companies participated, agreements were reached. For data protection, the GDPR framework ensures personal data is transferred, processed, and stored with appropriate technical safeguards, while giving individuals rights to be informed about data collection, access their data, and have it returned or deleted. Both systems apply to EEA residents and involve significant oversight - dispute resolution through eight recognized offices in Austria, and GDPR compliance through substantial penalties of up to €20M or 4% of global revenue for violations.","context":["Consumer Dispute Resolution Offices\nOut of court dispute resolution in conflicts between consumers and businesses is an important alternative to legal action.\nDispute Resolution Offices in Austria\nDirective 2013/11/EU on the alternative resolution of consumer rights disputes was implemented in Austria via the Alternative Dispute Resolution Act (Sag, Federal Law Gazette I no. 105/2015). Since 9 January 2016 there has been an alternative dispute resolution office available in Austria for almost every dispute resulting from a contract between a consumer and a business. For further information please see a special brochure (in German) which you can download or order from the Brochure Service.\nThere are a total of eight recognised dispute resolution offices which are responsible for different fields. Some of them already existed previously, such as those in the areas of energy, telecommunications, passenger rights and the post office. These eight recognised dispute resolution offices are characterised by the guarantees laid down in the Alternative Dispute Resolution Act, such as independence, transparency and effectiveness. As part of the AStG, a new ‘fallback dispute resolution office’ (conciliation for consumer transactions) was created.\nOutside this system, other mostly regional dispute resolution offices exist in some fields, such as matters relating to chimney sweeps. These are, however, not subject to the statutory requirements of the AStG.\nA list of the offices and their contact details is available from the Consumer Portal.\nNew Platform for Online Disputes\nSince 15 February 2016 there has additionally been a platform of the European Commission, via which consumers can also apply for dispute resolution. The so-called ODR platform can, however, only be used if your problem as a consumer results from a contract which was concluded online.\nHow does the ODR platform work?\nYou can enter via the ODR area of the website of the der European Commission.\nFirst you fill in the online complaints form. This is then sent to the company in question, which proposes a dispute resolution office. If you agree to the proposal, the complaint is forwarded to the relevant office, which initiates the conciliation procedure. The procedure may only last for a maximum of 90 days from the receipt of all relevant documents by the dispute resolution office.\nIf there are problems (e.g. with the submission of the complaint), you can, as a consumer in Austria, turn to the Austrian branch of the European Consumer Centre (EVZ), which acts as a contact point for online dispute resolution. Further information on the new ODR platform is available from the website of the EVZ.\nReport of the Federal Ministry for the Years 2016/2017\nThe Ministry of Social Affairs is the main contact point, and according to Art. 27 of the ADR Directive (Section 27 AStG) it is obliged to create a report for the European Commission on the development and function of alternative dispute resolution offices in accordance with Art 20/6, and to publish it.\nThis report (in German) (PDF, 285 KB) contains:\n- an overview of the dispute resolution system in Austria (responsibilities of the eight dispute resolution offices, their functioning and development on the basis of numbers of cases in the years 2016/2017);\n- the description of tried and tested procedures, and\n- a description of deficits (supported by statistics).\nNo figures for the individual offices are shown here. However, if you are interested you can view this data in the annual reports of the dispute resolution offices on their websites.\n- A total of 12,000 consumers contacted a dispute resolution office with a consumer complaint in the years 2016 and 2017.\n- In 68 percent of cases in which companies participated, an agreement was achieved.\n- The statutory maximum duration of proceedings (90 days) was clearly undercut (an average of 40 days).\nFurther details can be obtained from the bi-annual report (in German) (PDF, 695 KB).","What is the General Data Protection Regulation (GDPR)?\nThe General Data Protection regulation or the GDPR is a European Union (EU) regulation designed to protect the privacy rights of Individuals in the European Economic Area (EEA), which includes the European Union, Iceland, Norway, and Lichtenstein. It is intended to be an overarching privacy regulation for all EU Member States and replaces prior EU privacy regulations and goes even further than benchmark United States privacy laws governing health care and educational records, such as the Health Insurance Portability and Accountability Act (HIPAA) and the Family Education Rights and Privacy Act (FERPA).\nWhat does GDPR do?\nGDPR expands privacy rights for individuals located in the EEA regardless of citizenship. Specifically, it guarantees certain rights, depending on how the data is used:\nThe right to be informed regarding the collection and intended use of a subject’s personal data,\nThe ability to make informed decisions regarding the use and disclosure of the data,\nThe right to access the data upon request or have the data transferred to a third party, and\n- The right to have the data returned or deleted.\nIt also impacts data pertaining to these individuals even when the data is located in other countries, regardless of the citizenship of the individuals. Specifically, the GDPR establishes a framework for safeguarding how personal data is used, such as:\nEnsuring that the data is transferred, processed, stored and eventually disposed of using appropriate technical safeguards,\nLimiting the use/processing of the data to purposes that comply with GDPR requirements (e.g., managing the academic records of UC students studying in the EEA as part of Education Abroad),\nRequiring third parties who receive the data to adopt UC’s GDPR protections and safeguards through changes to contract terms.\nWho does GDPR apply to?\nGDPR applies to all organizations that are established in the EEA, including higher education institutions (e.g., a study center in Europe). It also applies to organizations not physically in the EEA when goods or services are offered to individuals in the EEA (e.g., applications for admissions), or when the behavior of individuals in the EEA are monitored by individuals either inside or outside of the EEA (e.g., research that includes EU citizens).\nAre there penalties for GDPR non-compliance?\nYes, GDPR imposes significant monetary penalties for organizations that do not comply with the regulation. The fines are up to €20M ($28M) or 4% of global revenue.\nWhat is the UCB Privacy Office doing to comply with GDPR?\nThe Privacy Office established a Working Group to address issues that are specific to the impact of GDPR at our campus.\nThe Working Group is comprised of representatives from key sectors likely to be impacted by the regulation and who will drive implementation efforts here at Berkeley, including Privacy, Compliance, Legal Affairs, Information Technology, Security, Risk Management, Insurance Services, and others. The primary goal of the Working Group will be to develop guidelines, processes, and policy changes to be implemented at Berkeley to promote compliance with GDPR.\nThe Privacy Office is also working closely with the UC Office of the President (UCOP) and the Office of the General Counsel (OGC) in accordance with their system-wide GDPR efforts as listed below.\nUC Berkeley's Statement of Privacy Practices - GDPR\nUC Berkeley's Statement of Privacy Practices for Persons in the European Economic Area Subject to GDPR explains how we comply with GDPR and indicates your rights regarding your Personal Data\nWhat is the University of California (UC) doing to comply with GDPR?\nUC’s compliance, privacy and informational technology programs are working together to develop an effective GDPR compliance program. This program is specifically designed to enhance the existing robust privacy infrastructure at UC to ensure compliance with this new regulation.\nProgram activities include:\n- Assessing how GDPR will affect UC programs\n- Developing tools and templates to assist UC programs with GDPR compliance\n- Developing communication tools to provide greater transparency to UC students, employees, and other UC program participants regarding the collection and use of personal data\n- Ensuring that appropriate physical and technical safeguards are in place to protect the personal data of individuals\n- Working with our partners and vendors to ensure that data protections are maintained when personal data is transferred outside UC\nWhat can I do to prepare my office for GDPR?\n- Understand GDPR:\n- Determine if GDPR applies:\n- Develop a plan:\n- Review Tools & Resources created by UCB, and UCOP, and start working on a plan for your Department\n- Stay informed:\n- Review FAQs from UCB, UCOP, and selected external partners\n- Visit GDPR.berkeley.edu regularly for updates\nFor questions relating to GDPR and its impact at UCB, please contact the Privacy Office"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:a2f0b2f6-1013-44d6-94d4-d9426aa4d875>","<urn:uuid:f9242506-b63b-4295-b069-8fa423e70d6d>"],"error":null}
{"question":"I manage a large company and want to know: how does poor communication affect strategic planning, and can social media make it worse?","answer":"Poor communication is a root cause of most organizational problems, creating issues like unclear expectations, declining performance, interdepartmental rivalries, and power struggles. This problem can be amplified through social media vulnerabilities, where attackers can exploit communication channels by impersonating brands or high-level executives, sending spoofed emails to trick employees into sharing sensitive data or wiring money, and using collected information from social media platforms to make their deceptive communications more convincing and targeted.","context":["Hoshin Kanri is a popular approach to strategic planning and deployment. It provides a roadmap to achieving an organization’s most important three to five-year objectives, while at the same time achieving continuous, daily improvement.\n(If the approach is new to you, start here for the background.)\nWhile the Hoshin Kanri methodology is relatively straightforward, there are a few common errors that send it off the rails. We’ve looked to the literature to see what the experts have to say about each frequent mistake. Hopefully this post will help you avoid or correct them. (It will also make a handy reading list.)\n1 – Failure to Engage Front Line Employees in Developing the Strategic Plan\nA Hoshin plan is not something that is developed in the boardroom and handed down to employees as an edict. Instead, it is the result of collaboration and use of the Catchball technique. Management sets the direction and chooses the destination, but everyone should be involved in the discussion about how to get there.\n“The term Policy Development is taken from the Japanese Hoshin Kanri, meaning “to show the way.” It is a management tool, or rather a management philosophy, where the whole organization is involved in the strategic development process. Hoshin Kanri is based on the idea that every associate is an expert in his own job. This means that it is essential to utilize this expertise -also when you do strategic development – to be delegating as much responsibility to the associates as possible.\n- From Strategy to Action: Policy Deployment in Daily Management – Lars Tegl Rasmussen and Kurt Ottesen.\n2 – Poor Communication\nToo often, communicating the strategic plan becomes an afterthought. It may be introduced in the annual company meeting, but never fully explained in a way that is meaningful for employees. This degrades its value and impedes success. Effective and frequent communication with a two-way feedback loop is essential.\n“The root cause of the majority, if not all, of the organizational problems can be found in poor communication. Typically, when mapping value streams, regardless of the level of aggregation, the greatest sources of variability lies within the interfaces between organizational groups (i.e., departments, buildings, factories, etc.). Poor communication leads to a number of interpersonal problems ranging from unclear expectation, declining performance, finger pointing, interdepartmental rivalries, power struggles and more.”\n- The Hoshin Kanri Forest: Lean Strategic Organizational Design – Javier Villabla-Diez PhD\n3 – Infrequent Progress Reports\nLike all other long-term strategic planning techniques, Hoshin Kanri is only useful if it becomes part of how the business is managed on a day-to-day basis. Putting the plan in a drawer and hoping for the best is sure to disappoint. The approach is meant to be active with constant review and adjustment.\n“Hoshin Kanri encourages the ability to be adaptive. Since long-range planning is based on information monitored from the business system, the planning process itself must be adaptive and capable of responding to changes in the business system. This means that management of the business system must include a regular review or assessment that indicates whether or not the system plan is faltering and must be adjusted or changed. This is achieved through regular reviews of both the planning-implementation progress, as well as the application of the planning methodology itself. Thus, Hoshin becomes an enabling feature in the continuous improvement of the company’s management process and serves as an information feedback loop that permits continuous responses to the winds of business change.”\n- Hoshin Kanri: Policy Deployment for Successful TQM – Yoji Akao\nClose alignment between individual objectives and the strategic goals of the organization is essential, primarily when working toward long-term breakthrough objectives. When individuals feel disconnected from the mission, employee engagement suffers, and decision making is impeded. On the other hand, when people see how their work fits into the broader strategy, they are more willing to contribute discretionary effort in furtherance of the goals.\n“All the members of an organization must have a clear understanding of the organization’s vision and goals. With all members in perfect alignment and clearly understanding their own role in the achievement of those Goals as they are trained and encouraged to work together to achieve them, then the productive power of the organization would be optimal.\n- Hoshin Kanri: The Strategic Approach to Continuous Improvement – David Hutchins.\nIf you’ve struggled with any of these challenges, don’t give up. Revisit the Hoshin principles and pay attention to these common pitfalls. Take advantage of the many available resources, such as the works we’ve mentioned here to support your journey to your game-changing achievements.","Social Media Threat Definition\nSocial media offers an outlet for people to connect, share life experiences, pictures and video. But too much sharing—or a lack of attention to impostors—can lead to a compromise of business and personal accounts.\nAttackers often use social media accounts during the reconnaissance phase of a social engineering or phishing attack. Social media can give attackers a platform to impersonate trusted people and brands or the information they need carry out additional attacks, including social engineering and phishing.\nHow Social Media Threats Happen\nBusinesses can’t control what people do in their private lives. But unfortunately, attackers can take advantage of employees who post too much information on social media.\nThe methods used by an attacker depend on the social media platform targeted. Facebook allows users to keep their images and comments private, so an attacker will often friend a targeted user’s friends or directly send a friend request to a targeted user to access their posts. If an attacker can connect to several of the targeted user’s friends, then it’s more likely that the targeted user will accept the friend request based on the number of connected friends.\nLinkedIn is another common social media target. LinkedIn is known for business networking, and users’ networks are typically filled with colleagues and other employees within the same organisation. If an attacker targets a business, LinkedIn is an excellent social media site to collect business emails for a phishing attack. A large enterprise could have several networked employees who list their employer and their titles. An attacker can use this public information to find several employees who have access to financial information, private customer data or high-privilege network access.\nCollecting information to steal data isn’t the only reason to use social media for reconnaissance. The information posted on social media could be used to obtain passwords or impersonate business users. Many online accounts allow users to reset passwords if they enter a security question. With enough information from social media posts, an attacker could guess the answer to these security questions based on the private information posted by a targeted user.\nBrand impersonation is another social media threat. With enough gathered information, an attacker can impersonate a business brand to trick users into sending money, divulging private information or provide an attacker with account credentials. Attackers also use this threat to perform cross-site scripting (XSS) or cross-site request forgery (CSRF) attacks. These attacks can lead to more massive data breaches and business infrastructure compromise.\nExamples of Social Media Threats and What They Look Like\nBecause many social media platforms publicly display user posts, attackers can silently collect data without a user’s knowledge. Some attackers will take further steps into gaining access to user information by contacting targeted users or their friends.\nThe way a social media threat is carried out by an attacker depends on their goals.\nIf an attacker is looking for a high-stakes reward, the best way to quickly earn monetary rewards for their efforts is to target businesses. An attacker might first review LinkedIn for a list of possible targets. Targets can be a mix of high-level corporate employees and low-privilege users who could be tricked into sending additional corporate data or fall for a phishing attack that gives the attacker access to account credentials.\nWith a list of targets, an attacker could then review social media accounts for personal information. Personal information can help the attacker gain the target’s trust in a social engineering attack. It can also be used to guess answers to security questions for an account takeover or used to get closer to a user with higher privileges. The names of pets, favourite sports teams and education history are all potential password clues or answers to questions used to verify the user’s identity to reset a password.\nAfter the attacker collects all the data needed, the next step is to launch the attack. An attacker can use any of the following methods:\n- Social engineering. An attacker might call employees to trick them into sending private data, proving credentials or wiring the attacker money. In a complex attack, the attacker can pretend to be a high-level executive to trick the targeted user into transferring money to the attacker’s account.\n- Phishing. An attacker may use collected social media information to spoof the sender of an email message and trick users into clicking links or sending the attacker private data. A high-level employee’s spoofed email address could send a message instructing the recipient to send money, click a malicious link or reply with sensitive data.\n- Brand impersonation. Using brand employee names, the attacker can trick customers into thinking requests are from the legitimate brand. This type of brand fraud could be used to trick users into divulging personal information or account credentials.\n- Site compromise and data theft. With enough information from social media, an attacker could write malware explicitly targeting the business or perform an attack that would provide internal network access where the attacker can then exfiltrate data.\n- Spread malware. Like brand impersonation, an attacker could create domains and websites that claim to be the legitimate business and trick users into downloading malware or providing credentials.\n- Data breach. If an attacker gains access to account credentials, it could lead to a significant data breach targeting an organisation.\nBecause there are several social media platforms on the internet, an attacker can perform social engineering and phishing using a variety of threat methods. There is no “one size fits all” social media threat for an attacker. But basic reconnaissance and research using social media are the same. Any public information on private and business social media accounts could be used in further attacks.\nWays to Prevent Social Media Threats\nMost social media security threats stem from employees disclosing too much private and business information publicly. These accounts are personal, so businesses can’t stop users from having a social media presence. But they can educate users on the best ways to protect data and their credentials.\nEducation is key to stopping social media security threats. Individuals can educate themselves. But businesses must conduct training programs for every employee so that they can detect and prevent social engineering and phishing. The first step is educating users on the dangers of disclosing too much information online to the public. Even social media accounts set to private could be used in an attack should the attacker gain access to private feeds. Users should never post private corporate information on their social media accounts or information that could be used in an account takeover.\nSome organisations hand out mobile devices and allow users to install social media apps. These companies should provide an acceptable usage policy that determines what users can post using company devices. It’s also critical to protect these devices from malware to avoid company social media accounts from being hacked. Remote wiping software should be installed should an employee physically lose their device or it gets stolen.\nSome other educational points for employees include:\n- Use ad blockers on corporate devices. If ad blockers are not feasible, instruct employees to avoid clicking ads, especially on popups that instruct users to download software to view content.\n- Employees should not share passwords—even if it’s within the same department.\n- Attackers use fear and urgency in their engagements, and employees should recognise this tactic as suspicious. Any messages or social media posts that urge employees to act quickly should be ignored.\n- Don’t accept friend requests from unknown people even if the user has several friends in common.\n- Avoid using social media sites on public Wi-Fi hotspots. Public Wi-Fi is a common location for attackers to snoop on data using man-in-the-middle (MitM) attacks.\n- User account passwords should change regularly. But users should also be encouraged to change their own private social media account passwords.\nIT staff should have social media cybersecurity defences in place to help users avoid being victims of an attack. Email servers can use artificial intelligence applications to catch suspicious emails with malicious attachments and links.\nSuspicious messages can be quarantined and reviewed by administrators to determine if the organisation is the target of an attack. Browser isolation is also an option for organisations that let users browse the internet. This technology allows users to freely browse the internet, but confines personal web activity to a protected container that prevents downloads, uploads and form fills to keep threats out of the environment."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:59574899-1e9f-4328-8119-7c35169ae026>","<urn:uuid:a9a72fed-011a-491c-8fdd-818b9d95a65f>"],"error":null}
{"question":"What environmental threats face coastal regions, and how do historical trading patterns influence modern maritime challenges?","answer":"Coastal regions face numerous environmental threats including oil pollution, toxic contaminants, marine debris, sewage discharge, and alien species introduction. These challenges are particularly evident in historic trading ports, from the coral stone cities of East Africa to modern concrete ports like Berbera. The impact of maritime trade has evolved from historical patterns of commerce to modern environmental concerns, with shipping activities contributing to various forms of pollution through operational discharges, accidental releases, and indirect sources. Traditional trading routes and ports now grapple with new challenges like oil spills, toxic waste dumping, and the introduction of non-indigenous species, which can lead to ecosystem changes and the degradation of coastal environments. These issues are compounded by the fact that coastal waters are being degraded at an alarming rate, affecting both commercial activities and ecological balance.","context":["Indian Ocean port cities are worlds of coral and glass. Consider Lamu or Zanzibar on the East African coast with their labyrinthine old towns where ornate buildings made of smooth grey coral stone stand in various states of ruin along narrow and winding lanes echoing long histories of trade and mobility. If coral indexes a distant past of commerce and cosmopolitanism, the cityscapes of Dubai and Singapore with their gleaming glass structures conjure up the “fake” futurity that Pamila Gupta notes in her reflections on the World Islands project in Dubai. In contrast to these ruins of coral or the shimmer of glass, Berbera, like other port towns that dot the Northern Somali coast, is by every measure architecturally unremarkable. Nestled between parched and jagged mountains and hazy blue of the Red Sea, this semi-bustling port city, neither features in tourist itineraries nor in the manifests of container ships and is a ramshackle collection of drab concrete buildings, shops, and warehouses. But, these worlds of ramshackle concrete, like those of coral and glass are equally defined by arrivals and departures. Saints, sovereigns, pilgrims, merchants, adventurers and pirates arrive in addition to cargo that ranges from pasta to Land Rovers. Some regulated, some clandestine these arrivals and departures give a rhythm to life along this threshold of land and sea. Sometimes, certain arrivals muddle distinctions and temporalities—objects wash ashore; tsunamis churn up rubble from the hidden depths of the sea. In these moments legal classifications and distinctions are sought and made. Flotsam and jetsam, gift and theft are not mere descriptors but projects and end results of legal and ethical arguments that emerge from within encounters and arrivals. Thinking through the “rubble” (Gordillo, 2014) on land and sea is both to make visible the sinews of connectivity that shape Berbera and other seemingly marginal littoral spaces and a meditation on worlds of toxicity and obligation created in the wake of these connections—these arrivals and departures.\nIn 2010, when I conducted research in Berbera, the main road leading to the port was lined with shelled-out and bullet-pocked buildings and an abandoned and rusted tank—artifacts of the civil war that began in 1980s pitting the inhabitants of Somaliland against the dictatorial regime of Siyad Barre (1969-91). Adjacent to the port, half-sunken ships were visible on the horizon, reminders of the aerial bombing campaigns that marked the twilight years of the Barre regime, when all dissent was viciously quelled.\nLegacies of the Cold War, when Somalia oscillated between the Soviet Union and the United States, also lingered in the shadows of the city. The seemingly endless airport runway constructed by the Soviet Union and used by NASA as an emergency space shuttle landing site was perhaps the most visible reminder of Berbera’s then-strategic importance. But there were also smaller imprints of the Cold War: health clinics, fisheries cooperatives, and schools mostly reduced to rubble due to the shelling or just neglect. One such building, a former Cuban health clinic at the edge of town, had been recently reoccupied and renamed the Somaliland Fishing Association (Somafish).\nAfter a week of back-and-forth phone calls and text messages, Muse, the director of Somafish, agreed to meet with me right after the asr (afternoon) prayer. To get to his office, I had passed a graveyard of fiber fishing skiffs—the preferred vessels of local fishermen as well as the pirates who roamed farther out in the Gulf of Aden. Sitting across a desk littered with papers and a somewhat odd trinity of a Somaliland flag, shark jaw with teeth intact, and a replica of an anchor, Muse recounted his personal biography, one intertwined with these histories of geopolitics and mobility. Born in Mogadishu, Muse had moved to the Northern Somali coast in the 1970s drawn to the sea by Siyad Barre’s vision of transforming the Red Sea into a sea of profit and calories for the local population. Forged through a dance of maritime currents and wind patterns, the waters off the coast of Somalia are teeming with tuna, snapper, and over coveted piscine delights. Beginning in the 1970s, the Barre regime sought with Italian and Soviet assistance to harness this bounty of the sea. By the 1980s long-distance trawlers from as far as Japan and Norway were drawn to this coast partly fleeing the tightening embrace of maritime regulation and the aftereffects of overfishing in the Atlantic and Pacific Oceans. When Somaliland announced its independence in 1994, revenues from the Berbera port and fisheries were seen as crucial to securing the newly independent, though unrecognized, nation’s economic sovereignty. “We established Somafish with the goal of harnessing the sea like it was in the early years of the Siyad Barre regime.” But, Muse explained, this project had encountered many difficulties. “As you can see, we are not very far from Yemen and Yemeni fishermen often come to Somaliland to steal our fish. We also have Korean trawlers that are responsible for overfishing and Italian mafia ships that dump hazardous waste.”\nThese stories and the material remainders that litter the landscape, bodies, and biographies of those who inhabit the Northern Somali coast point to the worldedness of Berbera—a kind of rubble cosmopolitanism that exists alongside the aesthetics of coral and glass in the Indian Ocean littoral. Forged through histories of encounter, this rubble cosmopolitanism beckons to worlds beyond Berbera and the Somali coast and the ways in which objects that wash ashore, often unexpectedly, are domesticated and contested. As Charne Lavery shows the distant regions of the world’s oceans and their dark, cold depths create a vision of oceanic space as limitless and allow for possibilities of disposal whose effects are ‘delayed and distant, exhibiting not only the effects of latency but also drift.’ For Lavery when objects surface from these depths, they highlight simultaneously the ideology of oceanic space as empty and its limits. But, surfacing is also sometimes about profit and at other times peril. Surfacing is about making visible entanglements and obligations, including toxic obligations.\nIn the early nineteenth century, about a week after leaving the relatively calm seas of Aden or Berbera a ship would find itself at the promontory of Cape Guardafui where the Red Sea meets the Indian Ocean. From May to November, when strong currents and gusty winds blow from the south, ships would often get caught in a whirlpool that developed northeast of Ras Haafun. An unfortunate ship caught in this whirlpool would find itself thrown westward to the rocky coast between Haafun and ‘Alula. By the beginning of the nineteenth century, the coast around Ras Haafun and the Majeerteen inhabitants of the area had developed into a shipwreck coast. These ships were seen as “gifts from the ocean” and the wreckage that washed ashore created shipwreck villages where inhabitants waited for ships to come onshore. Of course the British and later the Italians did not often see these as gifts. Drawing on the law of shipwrecks that distinguished between flotsam (floating wreckage of a ship or its cargo still considered property of the ship owner) and jetsam (willfully jettisoned wreckage with no property claim), the British insisted that all that appeared on the Majeerteen coast was flotsam under the legal protection of the British. This was not gift exchange, but theft and as protectors of free trade, the British blockaded this coast until the Majeerteen gave up their claims on these “gifts.”\nSometimes, gifts are not only contested, they are poisonous. In 2005, the aftermath of the Indian Ocean tsunami washed ashore wreckage at these former shipwreck villages from as far as coastal Sri Lanka, including mysterious containers. Rumors of toxic waste dumping had circulated in this region for many years, but were seldom investigated. Unlike shipwrecks, the containers that washed ashore at these villages were a different kind of waste. Soon stories of mysterious sicknesses and toxic exposure emerged. In 2007, a UN commission on the Indian Ocean tsunami confirmed exposure to radioactive substances in some of those villages in Northern Somalia. In recent years there has been a demonstrated increase in cancer and birth defects in Northern Somalia. Yet, on questions of culpability the report has little to say. In the case of 19th century shipwrecks, coastal populations through a vocabulary of gift and bounty sought to disassociate histories of ownership. These histories and claims to property were reinstated when the British insisted that wrecks were flotsam, and not jetsam. They were in other words not gifts. In the case of containers, it was precisely the reverse. Coastal villagers insisted that these objects were not jettisoned artifacts, but a specific kind of gift, a curse from owners who could be identified and thus this gift of toxic waste could be returned. A language of jetsam and wreck in the UN report erased these counter-claims. However undesirable, the report suggested, toxic containers were merely a gift of the tsunami.\nToxic containers still dot the Somali coast, though shipwreck villages have faded into obscurity. These villages briefly boomed when stories of pirates captured the global imagination. During fieldwork in one such village, I asked a group of men about the history of shipwrecks and toxic containers:\nWe used to sit patiently hoping and praying to get a gift from the sea. Our ancestors found shipwrecks, but the Europeans ended that when they came. Then came containers, but no one took them away. I think the pirates maybe have an answer. Today you cannot just wait for a gift from the sea; you might get poison. You have to be like these men [pirates], you have to go out to sea and bring the gift of the sea home.\nJatin Dua is a socio-cultural anthropologist whose research focuses on maritime piracy in the Indian Ocean and projects and processes of governance, law, and economy along the East African coast. His current book project, Protectors of the Sea: Piracy and Economies of Capture in the Indian Ocean, explores maritime piracy in the Western Indian Ocean within frameworks of protection, risk and regulation by moving between the worlds of coastal communities in northern Somalia, maritime insurance adjustors in London, and the global shipping industry. His second project, Navigating the Bab-el-Mandeb, focuses on the materiality and mobility of navigation, including technologies of risk calculation, credit extension, and the daily forms of circulation and governance that occur across the Bab-el-Mandeb strait, a key maritime chokepoint connecting the Red Sea to the Indian Ocean.\nGordillo, Gastón R. Rubble: The afterlife of destruction. Duke University Press, 2014.","State of the planet2 - Marine environmental polution\n\"It does not matter where on Earth you live; everyone is utterly dependent on the existence of that lovely, living saltwater soup. There's plenty of water in the universe without life, but nowhere is there life without water. The living ocean drives planetary chemistry, governs climate and weather, and otherwise provides the cornerstone of the life-support system for all creatures on our planet, from deep-sea starfish to desert sagebrush. That's why the ocean matters. If the sea is sick, we'll feel it. If it dies, we die. Our future and the state of the oceans are one.\"\n• Sea Change A Message of the Oceans\nSylvia Earle, 19\nIn the beginning it was all water ,Human civilization itself start along all coat of the world , be it river , lake, ocean - and all of the world are linked to the ocean -not only linked together but also almost everything we use eventually get in touched with the water- under cycle water in the ecosystem - all activities we do on land eventually get washed buy the rain and pass through the ground and have encounter with the water table that merge to the inland water which then run into the ocean- the ocean get evaporated , for cloud which eventually turn to rain - all In the name of supporting human\nHuman benefit from marine and coastal ecosystem and activities\n• Coastal tourism =161 billion American dollars\n• Trade and shipping =155 billion American dollars\n• Offshore oil and gas = 132 billion American dollars\n• Fisheries = 80 billion American dollars\nTherefore it is important to be careful and maintain balance in dealing our activities The popular media attention is concentrated on loss of life and property. There is little prospect for preventing many of the disasters from occurring although much could be done to reduce their severity. Many impacts could be mitigated through better vulnerability and risk assessment, predictive modeling, information dissemination, and policy development.\nMajor source of pollution are:\n- air pollution\n- dredge disposal and contaminated sediments .\n- endangered and threatened species\n- landbased water pollution\n- oil pollution - regulatory compliance\n- ship/port generated waste - partnerships\nEffect from industry and household that run of into the river • Oil pollution • Chemical pollution • Harmful substances in package form • Sewage • Ballast water • Garbage • emission • Dumping of wastes liquid,solid)\nMain source of marine pollution:\nMarine Pollution I -Point form polution - Oil Pollution • ,Toxic Contaminants ,• Marine Debris • Mining and Dumping •\nMarine Pollution II - Nonpoint Pollution, • Sewage ,• Alien Species • Watershed Issues\nMain source from ships is in form of:\n•Operational - Through socio - economic impacts to marine ecology, habitat, and coastal infrastructures are affected though operational activities that results from: Oil spill, Emission, Ballast Water, Garbage, contamination, Antifouling Dredging activities.\n• Accidental risk - marine accident that could result to oil spills which then, end up degrading our environment about 400-300 thousands of oil entered the world ocean, collision with marine mammal, which then cause propeller injuries through : Grounding ,Stranding, Loss of oil, Hazardous cargo, Noxious liquid, collision with marine mammals.\n• Design - Risk associated with environmental issue n ship and in ship designing are : In the context of ship design the impacts areas are: Shipping Trends, Channel Design Criteria, Ship Maneuverability, Ship Controllability, and Use of Simulators in Channel Studies. Since world II many nations built port but forget about maintaining them while shipyard continues to build larger ships. Physical dimension and ratio of ships to channel has got impact in today's ship controllability.\nDischarge could be:\n• Intentional and unintentional discharge (oil, garbage, antifouling paint, air emission, on indigenous species from ballast water • Environmental damage and pollution due to port activities • Disturbance of marine environmental (collision and noise) • Emission from scraping of ships at the end of their life cycle Direct Discharges • Direct discharges are defined here to include releases from vessels, discharges of municipal and industrial wastewater via pipelines, and dumping of waste materials, such as dredged material, into ocean waters\nIndirect discharge • One to two-thirds of pollutants contributing to the degradation of coastal and marine waters are from indirect sources, and include sediments, nutrients, pathogens, and toxic compounds. Pollutants from agricultural and pasture lands include sediments, fertilizers, pesticides, herbicides, and animal wastes which contain bacteria and nutrients\nAccidental Releases- • Oil spill and bunkering fuel, Emmsion)Sox,Nox,CFC &VoC,Antifouling toxins,Ballast water discharges,Noise,Watse disposal at sea,Dredging @dispersal of soil Impacts • Habitat Destruction (overview) • Loss of Wetlands • Tourism and Recreation • Deforestation • Fresh Water Alterations • Fishing Issues (overview) • Over fishing • Ecosystem Changes • Bombs, Poison, Scrapes • By catch • Global Change Climate Change • Ozone Depletion • Coastal Development • Population\nOther impacts are -\nthe introduction of pathogens to coastal waters - alteration of water tables - modification of nutrient cycles or soil fertility - increased erosion - interference with navigation - a reduction in sport and commercial fishing yields - negative impacts on recreational boating and beach use .\nThe introduction of non-indigenous species often results in unexpected ecological, economic, and social impacts to the coastal and marine environment. Predation and competition by non-indigenous species has resulted in the eradication of some native populations and the drastic reduction of others, thereby altering local food webs. This process is often compounded by the exploitation of commercial fish. Overpopulation of some non-indigenous species has resulted in the degradation and loss of wetland vegetation and other submerged aquatic vegetation as a result of overgrazing.\nBecause industrialized society depends on petroleum products to maintain its accustomed standard of living, large volumes of petroleum are transported each day in the coastal and marine environment. Spills and leaks cause the formation of tar balls, oil slicks, and tar mats, and can impact the micro-layer, the benthos, the coast, and marine life.\nSustainability capacity building, efficiency optimization of development, practice and operations that meets the needs of the present generation without compromising the ability of future generation to meet their need\nGood environmental quality is essential for sustaining coastal and marine ecosystems, commercial and recreational fisheries, and economic growth in coastal communities.\nIt is also an important means of providing natural protection against rising sea levels and storm damage. The health of coastal and marine ecosystems is affected by water quality, and in turn, water quality is dependent upon ecosystem health. If one is impaired, the other is threatened.\nDespite their value and the programs designed to protect them, many coastal waters are being degraded at an alarming rate in addition to this, other advantages are: • compliance with all applicable environmental laws and regulations; • No significant adverse environmental impacts; • Wastes treated or destroyed on board to the extent practicable; • No inappropriate dependence on shore facilities for waste off-load and disposal; • Minimal energy consumption; • Minimal logistical costs for waste management; and • Minimal use of hazardousmaterials.\n• Low exhaust emission diesel engine achieves a 25% reduction in air emissions\n• Expected to result in reduced maintenance, better engine performance, and decreased fuel consumption\n• Hull of boat was coated with a Teflon-based coat that contains no toxic chemicals • Holding tank for waste prevents discharges\n• Port's attempt to reduce emissions from marine vessel engines.\n• Particular features of existing tug engines were modified to keep combustion temperatures as low as possible; this optimizes engine efficiency and produces fewer emissions.\n• With a Teflon coat, hulls are easy to clean, contain no toxics, and no longer need to be painted, reducing pollution from waste products generated during painting.\n• Surface sediments contaminated with metals, PAHs, PCB, and other organics\n• All clean material deposited in Mass Bay Disposal Site - innovation/uncommon\nothers - transferable - response to EPA or government initiative - significance and breadth of benefits - effectiveness and results - acknowledgement by others - Beneficial disposal of dredged material - Treating waste as resources and put them under recycling\nStrategies for shipboard control\n• Shipboard and waste emission outline -treatment and elimination - Pollution Prevention (P2) or Pollution Control-this is backbone of the thrust in achieving clean ship. Pollution Prevention Use fewer environmentally harmful substances and generate less waste on board.\nPollution Control: Increase treatment, processing, or destruction of wastes on board. The basic P2 principles follow:\nEliminating the use of environmentally harmful chemicals, such as ozone-depleting substance (ODSs), toxic antifoulant hull coatings, and other hazardous materials, may be the best approach for some potential of :\n• Sulfur reduction in bunker fuel\n• Nitrogen reduction to choice of propulsion system\n• On board Cataleptics system like charlatanic converter, water injection, emulsion\n• Operationally sped reduction and use of shore power connection has\nWhat is expected from youWe're all responsible for this mess, and it will take all of us to stop it from getting worse. It's time to completely rethink how we as a society use (or abuse) plastic. Here are some things that you can do right now:\nEvery time you see litter, pick it up and dispose of\nReduce, Reuse, Recycle - you've heard it before, but\nnow you know what happens when you don't. Be conscious of all\nthat you buy, and be sure to avoid products with excessive\npackaging, especially in disposable products.\nDemand more and better recycling facilities in your\nTake part in local stream, river and beach cleanups -\nor organize one yourself. Though these don't solve the\nproblem, they are very effective at drawing attention to the\ngreater problem offshore.\nIf you live near the ocean, or a river that drains\ninto it, your storm drains are probably washing garbage right\nout to sea. Be conscious of this and any other potential\nsources of marine litter in your area. Demand that these are\n- Be very conscious of your ecological footprint. Encourage change though your decisions and do no accept the current paradigm of use and waste.\nFeature environmental technology\n- Ozone safe substances- 200-Ton Air-Conditioning Plant Conversion Kit -The CG-47and DDG-51 plants have been successfully converted to the ozone-friendly refrigerant HFC- 236fa conversion kit .\n-Solid waste - Solid-Waste Pulpers -The\npulper (especially the large pulper) is the machine into which\nyou dump tremendous quantities of paper, cardboard, or food\nwaste.The waste mixes with seawater to form slurry, which is\nthen discharged overboard. Studies show an immediate\n100,000-to-1 dilution when discharged into the wake of a ship.\nShips equipped with a pulper can dispose off their paper,\ncardboard, and food waste just about anywhere and at anytime-at\nsea including MARPOL areas.\nLiquid waste - OWS and Bilge water Polishers: Many bilge cleaners the Navy uses today contain long-lasting emulsifying agents, which produce stable oil-in-water emulsions that shipboard OWSs cannot effectively process. The popular media attention is concentrated on loss of life and property.\nThere is little prospect for preventing many of the disasters from occurring although much could be done to reduce their severity. Many impacts could be mitigated through better vulnerability and risk assessment, predictive modeling, information dissemination, and policy developmeent .\n\"... [M]an's fingerprint is found everywhere in the oceans. Chemical contamination and litter can be observed from the poles to the tropics and from beaches to abyssal depths...But conditions in the marine environment vary widely. The open sea is still relatively clean...In contrast to the open ocean, the margins of the sea are affected by man almost everywhere, and encroachment on coastal areas continues worldwide...If unchecked, this trend will lead to global deterioration in the quality and productivity of the marine environment.\" The State of the Marine Environment, 1989; Group of Experts on the Scientific Aspects of Marine Poll comments will appreciated -"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:33a57865-31f0-4184-b671-5ddb579a98dc>","<urn:uuid:3c66f1d5-b4e8-4e49-8d6d-90b988085123>"],"error":null}
{"question":"Thangka schools and mounting styles comparison?","answer":"Thangka paintings have different schools and styles influenced by geography, raw material availability, socio-cultural factors and Buddhist schools followed. In terms of mounting, there are two traditional styles: those mounted in plain, moderately coarse cotton or wool cloth, which often show rodent damage and tears at top and bottom; and those mounted in brocade fabric with fine silk covers and ribbons, which tend to develop tears and losses in stressed areas.","context":["Anupam Sah (R) is the Head of Art Conservation at Chhatrapati Shivaji Maharaj Vastu Sangrahalaya's Art Conservation Centre (L) in Mumbai. (Robert Cutts/Flickr)\nMuch of Anupam Sah's work happens behind the scenes at Mumbai's Chhatrapati Shivaji Maharaj Vastu Sangrahalaya (CSMVS), formerly the Prince of Wales Museum of Western India. Sah is a distinguished art conservation-restoration practitioner and Head of Art Conservation at the museum's acclaimed Art Conservation Centre who has received the Sanskriti Award for Social and Cultural Achievement in recognition of his community engagement efforts and work in heritage conservation.\nJoin Asia Society India Centre's AsiaLens programme this Saturday, July 13 as Sah gives an exclusive glimpse of his very own studio at the CSVMS Museum Art Conservation Centre. He will specifically focus on conservation treatments and techniques of Nepalese thangka paintings in the CSMVS permanent collection. The event is organized in partnership with CSMVS. RSVP is required; Asia Society members will be given seating priority.\nHow did you become interested in becoming an art conservation-restoration practitioner?\nSince childhood I wanted to take up a profession that allowed me to study and strive to excel in multiple disciplines ranging across the sciences, humanities and including working with one's hands. Art conservation provided me this opportunity.\nWhat are some of the greatest challenges you face on the job? Is your experience working at an art museum in Mumbai different from that of a colleague in, say, Paris or Tokyo?\nContemporary art conservation training and dissemination in India is essentially in the English language and thus even the sharing of this information skims a very small section of Indian society. There are very few structured training courses in India at the moment and few trainers.\nThe experience at the CSMVS museum is extremely positive. There is a lot of flexibility and room to implement innovative ideas. This museum is a great team place where the conservation, curatorial, logistic, security and other departments work in tandem, in a quite informal manner. Personal communication plays a greater and more effective role in India, I feel. This may or may not be different for a colleague in, say, Paris or Tokyo. :)\nThangka is a traditional form of Buddhist art with Tibetan, Indian, Chinese, and Nepalese influences. Does this imply that there are different schools within thangka painting?\nYes, there are different schools and styles. Various factors determine this — geography, availability of raw materials, socio-cultural influences, [the] schools of Buddhism they follow.\nWhat kind of special considerations do you take in mind when conserving and restoring thangka paintings as opposed to other media in the CSMVS Museum's collection?\nThe thangka, first and foremost, are means for the Buddhists to achieve their aim of spiritual development, and are not just material works of art. As such they need to be treated at the conservation centre with that sense of respect, regard and that thought of sacredness.\nHow do you go about maximizing the \"lifespan\" of a thangka painting?\nBy first of all improving the structural stability of the thangka support and the paint layers.","Thangkas are iconographically and structurally complex devotional images used in households, monasteries, temples and other traditional locations. The two primary sections, the painting and the mounting, are integral to the icon. The painting is done on a fine cloth, which is sewn into a textile mount comprising multiple layers and sections of textile finished at the top and bottom with a stiff bar. The face is protected with a fine cover cloth secured at the top and held open with cords and ribbons. The thangka is consecrated repeatedly; empowerment symbols may be written on the reverse of the painting.\nDuring use, thangkas acquire patinas of soot, oil and water stains from exposure in temples and the elements. The thangka is rolled up for carrying. The repeated rolling and unrolling of a multilayered structure of brittle paint and stiff textile causes considerable damage to the paint and textiles. If handled in their current state, many of the Field Museum thangkas would loose more paint particles and textile fragments, and develop further breaks in the painting and mount textile.\nThe Field Museum thangka collection comprises 382 thangkas, of which 373 were collected by Anthropology Curator Dr. Berthold Laufer in1909 during the 3-year Blackstone Expedition to China and Tibet. The collection, amongst the three largest in the United States, has a number of thangkas of very high cultural and artistic significance, but is especially important for its anthropological value. Collected during one year, the collection provides a cross section of thangka use in eastern Tibet at that time: it includes stylistic and functional thangka sets, household and temple thangkas, new and well used thangkas, thangkas of exquisite and humble aspect. The collection is also significant for containing a high proportion of Bon thangkas – those whose images relate to popular folk culture in which features of the earlier indigenous religions in Tibet were incorporated into Buddhist iconography. These have received less scholarly attention than other categories of thangkas and it would be valuable to make the collection stable enough to be available to scholars.\nThe Anthropology thangka collection comprises about 140 traditionally mounted paintings, 175 unmounted paintings, 30 scroll mounted paintings, and 15 framed or otherwise previously restored or nontraditionally mounted paintings. We are conducting a condition survey of the whole collection to determine the treatment needs of each and develop a budget and strategy for funding. During the survey, preservation of the thangkas and paintings is improved by archival housing, and high resolution digital imaging is done to document their condition, document the painted images to make them more available to scholars. The survey includes details on presence and condition of each component of the mount and painting, and an estimate of the type and time of conservation treatment needed.\nRehouse to reduce factors causing damage to the thangkas.\nA unique housing system was designed and implemented for the traditionally mounted thangkas. Most damage to the thangka occurs as it is rolled and handled, cracking and flaking the paint and creasing and tearing the textile. A special set of 12 thangkas depicting the Dalai Lamas and Arhats are stored flat in map cases to eliminate rolling. Some other thangkas were previously restored and mounted in such a way that they are extremely stiff; these can only be housed flat in boxes. We do not have room to store the whole collection flat, so remaining mounted thangkas have been rolled on dowels with 2” of soft padding (soft to accommodate cockles in the painting, and large diameter to reduce the curvature of the rolled painting). The dowels are suspended in clam shell archival boxes so that the weight of the thangka is not carried by parts of the painting. The clam shell boxes protect the thangkas from dust, light and pressure and allow efficient use of storage space.\nThe 175 unmounted paintings were previously held with corner strips on heavy paper sheets and put in a glassine folder. These materials were tested and found to be archival (neutral pH), but provided insufficient support and were difficult to handle. Drop front archival boxes were made to accommodate the folders, providing a rigid, dust and light free storage environment.\nHigh resolution digital imaging of obverse and reverse of each thangka to assist scholars in low impact access to the thangkas.\nDuring the survey, high resolution tiff images are made of obverse and reverse, in incident and raking light, of every thangka surveyed. The availability of digital images renders the thangkas potentially accessible to scholars around the world and reduces the amount of damaging handling to which the thangkas are subjected. Imaging also documents the thangkas’ condition at this point in time, greatly increasing our ability to detect changes in condition over time.\nImages of the Field Museum paintings are posted on the website of Himalayan Art Resources (www.himalayanart.org) curated by Jeff Watt and David Pritzker.\nDevelop conservation treatment protocols appropriate to the specific thangka styles and types of damage to allow accurate estimate of resources needed.\nTraditionally mounted thangkas were of two styles: those mounted in plain, moderately course cotton or wool cloth; and those mounted in brocade fabric with fine silk covers and ribbons. The coarse cloth tended to have considerable loss from rodent eating, and to have tears at the top and bottom. The brocade and silk tended to have tears and losses where the fabric was stressed. In both cases, standard treatment comprised flattening and aligning the fabric and attaching a support patch or lining to the reverse using sewing techniques. The fabrics and threads used matched the weight of the original fabric – polycotton for the coarser cloth and silk crepeline and hair silk thread for the finer silks and brocades.\nThe support fabric of some paintings was torn or fractured. These were treated with a heat reactivated patch on the reverse; the weight of the patch paper matching the weight of the painting support fabric and thickness of ground.\nMany paintings suffered losses and instability from cracking, cupping and flaking of the ground and paint. Most paint is matte and thick, considerably restricting the type and application of consolidant that can be used to secure the paint but not cause darkening or sheen. Repeated applications of a very dilute methylcellulose paste in water/ethanol was found to penetrate the paint, provide enough plasticity to allow setting down of cupped paint, and dry without altering the paint appearance."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:4d77616b-e349-4d9a-9e8a-eaee264bca0b>","<urn:uuid:4af72f61-62dd-4c79-96a3-eb379d6423e5>"],"error":null}
{"question":"What is the monitoring threshold that triggers insecticide application for these two pests - Spotted Wing Drosophila and Apple Leaf Midge?","answer":"The monitoring thresholds differ between the two pests. For Apple Leaf Midge, researchers have established a specific threshold of 30 adults per trap per week for timing insecticide sprays. For Spotted Wing Drosophila, no specific numerical threshold is mentioned - the documents only indicate that monitoring should begin when fruit are noticeable and treatment should start once SWD is detected in the garden.","context":["Spotted Wing Drosophila in home gardens\nSpotted Wing Drosophila (SWD), Drosophila suzukii, is an invasive small fruit fly (sometimes called vinegar fly) in the U.S. In Minnesota, SWD is a pest that primarily attacks raspberries, blackberries (and other cane berries), and blueberries but may also infest strawberries, grapes, and stone fruit. Native to Asia, SWD was first found in California in 2008, and is currently found in most if not all of the primary fruit growing regions of the U.S.\nIn August 2012, the first confirmation of SWD was made in Minnesota in Ramsey, Hennepin, Anoka, and Olmsted Counties and has since been found in the majority of counties in the state. Follow this link to view the current distribution of SWD in Minnesota.\nMartin Hauser, California Dept. of Food and Agriculture\nFigure 1. Male SWD with distinctive wing markings\nSWD is a small fly, only 2 - 3 mm (1/12 - 1/8 inch) long, with yellowish-brown coloration, dark colored bands on the abdomen, and prominent red eyes. They can be difficult to distinguish from other species of small fruit flies. Male SWD are relatively easy to identify as they have clear wings and a dark spot along the first vein near the tip of each of wing (Figure 1).\nHowever, while female SWD also have clear wings, they lack any spots on them. They are more difficult to identify. They can only be identified by their saw-like ovipositor which has two rows of dark-colored teeth (the ovipositor is the structure used by the female fly to insert eggs into berries). However, high magnification is needed to see the ovipositor.\nSWD larvae (also called maggots) are white with a cylindrical body that tapers on one end. (Figure 2). They lack legs and a conspicuous head.\nAdult flies insert eggs into soft fruit where the larvae develop (Figure 3). The larvae eventually leave the fruit and drop to the ground where they pupate. They later emerge as adults. SWD can complete its life cycle in as little as 7-10 days, although in cooler weather it could take up to 25 days or longer.\nTrap catches have first found SWD in Minnesota in mid to late June. In the upper Midwest, it is believed about 10 - 12 generations occur during the growing season creating essentially continuous activity. Populations build through the growing season, peaking in late summer. SWD overwinters as an adult; however, its ability to survive Minnesota winters is unknown at this time.\nSWD larvae feed on healthy, intact, ripening fruits. In particular, SWD will feed on thin-skinned, soft fruits such as raspberries, blackberries, blueberries, strawberries, grapes, plums and cherries (Figure 4). SWD larvae feed within the fruits causing brown, sunken areas.\nIt is possible that larval feeding symptoms won't show until after the crops are harvested. In addition to the damage caused directly by the larvae, the feeding makes the fruits susceptible to infestation by other insects, rot fungi, and bacteria.\nMonitoring for SWD adults helps determine whether SWD is in your area and when it is first present. This information helps to time insecticide applications so they can be most effective. The best method for monitoring SWD is the use of traps.\nTraps should be set out as soon as fruit are noticeable. Traps should be monitored from just after blossom petals have fallen (fruit set) until the end of harvest. This allows the home gardener to identify the start and end of fly activity. The most critical time to monitor is when fruit color first starts to develop until the crop is harvested. This appears to be when fruit are most vulnerable to SWD infestation.\nAdult SWD flies can be monitored with a homemade plastic trap filled with apple cider vinegar. To construct the trap you will need a 32 ounce plastic cup with a top (like a deli container), apple cider vinegar, and a small piece of yellow sticky card. Drill or poke several 3/16'-3/8\" size holes around the upper side of the cup, leaving a 3-4 inch section without holes to hold the vinegar (Figure 5). The small holes allow access to SWD, but keep out larger flies and other insects.\nPour one inch of apple cider vinegar into the trap as bait. To help capture the flies, place a small yellow sticky card inside. Yellow sticky cards can be purchased from local garden supply companies and from Gempler's. The traps will also work without the yellow sticky insert, but if this is done make sure to add a drop of unscented dish soap to the vinegar to trap the flies in the liquid.\nTraps should be hung in the fruit zone, in a shaded area of the canopy, using a wire attached to the top of the trap (Figure 5). Make sure the trap is clear of vegetation with holes exposed so that SWD can easily enter the trap. Traps should be checked every other day, or as often as possible.\nIf it is desirable to test fruit for the presence of maggots, there are several sampling methods to test for larvae. For more information, see Sampling berries for spotted wing drosophila larvae (Michigan State University).\nSanitation is important to reduce the local buildup of SWD populations. The best sanitation practice is to frequently harvest crops to ensure ripe fruits are not in gardens for extended periods of time. It is also important to remove and destroy any old fruit that remains on stems or that has fallen to the ground.\nOnce infested or fallen fruit has been collected, place it in a plastic bag and seal it tightly. Fruit in clear bags can be left outdoors where the heat from the sun will kill any flies in the bag. Plastic bags can also be placed in the trash. Do not compost this material as that method is unreliable in killing SWD. Also, do not bury infested material as research has found that SWD can survive being buried as deep as 18 inches.\nIf your fruit looks intact, but you still suspect an infestation, place it in your refrigerator. Chilling can help slow and even stop the development of larvae. Note that these fruit are safe to eat. There is no known risk to human health posed by ingesting SWD.\nAnother cultural control method that can be used to manage SWD is exclusion through the use of netting or floating row covers. Netting can be used over a more permanent structure, such as a small high tunnel (Figure 6), or placed directly over the row similar to a floating row cover. The cover prevents SWD access to the developing berries, and can potentially reduce the infestation.\nOne drawback is that the covering would have to be opened at each harvest, which could provide SWD access to the berries. However, for raspberries, the covers should not need to be removed to allow for pollination because raspberries are primarily wind pollinated. A fine mesh netting should be used and an 80 gram insect netting has been shown to provide good results in University testing. Coarser 60 gram insect netting will allow SWD to pass through and infest fruit. Netting can also provide protection from birds and hail.\nOnce SWD is detected in your garden, an insecticide is necessary to protect susceptible fruit. More than one application will likely be necessary to protect fruit throughout the growing season. Insecticide resistance is a concern with this pest and you should rotate classes of insecticides if possible. Keep in mind that insecticides are targeting the adults, before they lay eggs, and will not control larvae already in the fruit. Once fruit is infested, there is not any effective control other than using sanitation to prevent SWD from emerging.\nThere are several insecticides approved for organic use that are available for treating SWD. Home gardeners still need to be judicious in rotating between insecticide classes, if possible. Intervals between applications may need to be shortened, but you still need to follow the label and be certain that all instructions, including PHI (pre-harvest interval) are allowed.\n|*Very short residual lasts less than 1 day. Short residual lasts 3-5 days. Medium residual lasts 7-10 days.|\nThe information given herein is supplied with the understanding that no discrimination is intended and no endorsement by the University of Minnesota Extension. Mention of a pesticide or use of a pesticide label is for educational purposes only.\nA pesticide label is a legal document. Always follow the pesticide label directions attached to the pesticide container you are using. Pesticide labels may change frequently. Internet labels may not match the label on the container you are using. The site of use or plant to which the pesticide is to be applied must be listed on the label or the pesticide cannot be used. Remember, the label is the law.","- IFP & Organics\n- Change Log\nApple leaf midge is an introduced pest from Europe first observed in Okanagan Valley in 2003 but present in Fraser Valley since early 1990s.\nLarval feeding causes leaves to curl tightly upwards and the tissue to thicken, often displaying a purplish color (Figs. 1, 2). Damage is easily confused with aphid infestations. Feeding on terminal leaves reduces terminal growth and may distort limb growth; leaves may drop prematurely. There is no evidence of reduced fruit quality or quantity in bearing trees. Primary impact is to delay or stunt structural development of nursery and young bearing trees.\n|Figure 1. Apple leaf midge damage. (H Philip)||Figure 2. Apple leaf midge damage. (H. Philip)|\nLarva - White to orange-red (depending on age), legless, maggots up to 3 mm long (Fig. 3).\n|Figure 3. Mature apple leaf midge larvae. (H. Philip)|\nAdult female - Delicate mosquito-like fly, dark brown body with reddish abdomen, about 2-3 mm long.\nAdult male - Male resembles female but lacks reddish abdomen.\nThere are at least two generations per year in British Columbia. It is distributed throughout the Fraser Valley and in central parts of the Okanagan Valley. Apple leaf midge overwinters as pre-pupae or pupae in cocoons in the soil, and occasionally in curled leaves or other protective sites beneath host trees. Adults begin to emerge in late May to early June and during their one week life span, mate and lay eggs on the edge of terminal apple leaves. Larvae feed on the upper surface of leaves for 2-3 weeks. Pupation occurs in early July with second-generation larvae appearing in August. They feed for up to 4 weeks before dropping to the ground to pupate in the soil for the winter.\nInspect developing shoots of nursery and young trees less than 2-3 years old for curled, often purplish, curled terminal leaves containing white to bright orange maggots. In the UK, researchers have identified female sex pheromones for trapping males. A threshold of 30 adults/trap/week is used for timing insecticide sprays.\nPirate bugs actively feed on larvae (campylomma have also been observed within infested leaves). Native and introduced parasitoids attack midge larvae in New Brunswick but no information is available on parasitoids attacking apple leaf midge in BC.\nThere are no cultural methods that will adequately reduce the risk or severity of apple leaf midge infestations. Hand removal and destruction of infested leaves may help but removing terminal leaves can result in the same impact on tree development. Proper management of susceptible trees will help minimize the impact of the midge.\nRipcord (cypermethrin), Decis (deltamethrin) and Movento (spirotetramat) are registered for control of apple leaf midge. These products are not effective against larvae in leaf curls. Ripcord and Decis are harmful to beneficial insects and mites and would lead to mite flare ups. Field trials in Ontario indicated Movento provides good control of the midge at petal fall timing. Application interval for Movento is a minimum of 14 days with a maximum of 2 applications per year."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:353d9303-fb59-4a2a-a708-ec998e660844>","<urn:uuid:764fa4a3-0e6f-4be8-82b0-a7ba93c650ee>"],"error":null}
{"question":"How do researchers approach brain analysis differently in pig studies versus Alzheimer's research - what are their main measurement techniques?","answer":"In pig studies, researchers use MRI scans, diffusion tensor imaging (DTI) to track neural development, and measurements of neurochemicals like creatine and acetylcholine to analyze brain metabolism. They also use 3D visualization software to manually segment brain regions. In contrast, Alzheimer's research focuses on measuring biological markers, specifically analyzing amyloid-beta peptide concentrations in spinal fluid, and using machine learning to identify predictive proteins in blood samples that could indicate disease risk.","context":["Pig Brain Models Provide Insights into\nHuman Cognitive Development\nURBANA, March 14, 2013 –A mutual curiosity about patterns of growth and development in pig brains has brought two University of Illinois research groups together. Animal scientists Rod Johnson and Ryan Dilger have developed a model of the pig brain that they plan to use to answer important questions about human brain development.\n“It is important to characterize the normal brain growth trajectory from the neonatal period to sexual maturity,” said Johnson.\n“Until we know how the brain grows, we don’t know what is going to change,” added Dilger.\nIn cooperation with the Beckman Institute, they performed MRI scans on the brains of 16 piglets, starting at the age of 2 weeks, then at 4 weeks, and then at 4-week intervals up to 24 weeks.\n“We have world-class people at the Beckman Institute who are pushing and developing the next generation of neuroimaging technology, so we’re able to connect with them and take advantage of their expertise,” said Johnson.\nMatt Conrad, a student in Johnson’s lab, used three-dimensional visualization software on over 200 images to manually segment each region on three planes. The software put the information together into a three-dimensional image of the pig brain. This is used to determine the volume of the different structures.\nWhen the piglets were at Beckman for their imaging sessions, Dilger performed other tests, including diffusion tensor imaging (DTI), which shows how neural tracks develop, allowing the exploration of brain complexity and of how neurons form. It was also possible to measure neurochemicals, including creatine and acetylcholine, in the brain, which provides a unique insight into brain metabolism.\nThe end result of this work is what they call the deformable pig brain atlas.\n“We are taking 16 pigs and averaging them, so it’s more representative of all pigs,” said Dilger. “You can then apply it to any individual pig to see how it’s different.”\n“It’s called a deformable brain atlas because the software takes information from an individual and deforms it until it fits the template, and then you know how much it had to be deformed to fit,” Johnson explained. “So from that, you can tell whether a brain region is larger or smaller compared to the average.”\nJohnson and Dilger said that the goal is to develop a tool for pigs that is equivalent to what is available for the mouse brain and make it publicly available. But they don’t want to stop with tool development.\n“We want to use this to address important questions,” Johnson said.\nOne research direction being pursued in Johnson’s lab is to induce viral pneumonia in piglets at the point in the post-natal period when the brain is undergoing massive growth to see how it alters brain growth and development. They are also looking at effects of prenatal infections in the mother to see if that alters the trajectory of normal brain growth in the offspring. The risk for behavioral disorders and reduced stress resilience is increased by pre- and post-natal infection, but the developmental origins are poorly understood.\nDilger’s group is interested in the effects of early-life nutrition on the brain. They are looking at the effects of specific fatty acids as primary structural components of the human brain and cerebral cortex, and at choline, a nutrient that is important for DNA production and normal functioning of neurons.\n“Choline deficiency has been tied to cognitive deficits in the mouse and human, and we’re developing a pig model to study the direct effects choline deficiency has on brain structure and function,” Dilger said. “Many women of child-bearing age may not be receiving enough choline in their diets, and recent evidence suggests this may ultimately affect learning and memory ability in their children. Luckily, choline can be found in common foods, especially eggs and meat products, including bacon.”\nMore information about the model is available in “Brain growth of the domestic pig (Sus scrofa) from 2 to 24 weeks of age: a longitudinal MRI study,” by M.S Conrad, R.N. Dilger, and R.W. Johnson, which was recently published in Developmental Neuroscience.\nMarch 14, 2013","March 11, 2019 | Written by: Ben Goudey\nCategorized: Healthcare | IBM Research-Australia\nShare this post:\nAlzheimer’s disease, a terminal neurodegenerative disease, has historically been diagnosed based on observing significant memory loss. There is currently no cure or disease-modifying therapy for this illness, despite hundreds of clinical trials being conducted since 2002. It is thought that the high failure rate of these trials may be because the people enrolled are in the latest stages of the disease, and have likely already suffered a level of brain tissue loss that cannot easily be repaired. The question is how to detect this disease earlier, while there is still a chance to slow its progression.\nRecent research has shown that a biological marker associated with the disease, a peptide called amyloid-beta, changes long before any memory-related issues are apparent. Examining the concentration of the peptide in an individual’s spinal fluid provides an indication of risk decades before any memory related issues occur 1. Unfortunately, accessing spinal fluid is highly invasive, requires an anaesthetist and is expensive to conduct on large segments of the population. Hence, there is a strong effort in the research community to develop a less invasive test, such as a blood test, that can yield information about Alzheimer’s disease risk.\nA recent paper by my team at IBM Research – Australia, published today in Scientific Reports, used machine learning to identify a set of proteins in blood that can predict the concentration of amyloid-beta in spinal fluid. The models we built could one day help clinicians to predict this risk with an accuracy of up to 77 percent2. While the test is still in the early phases of research, it could potentially help improve the selection of individuals for drug trials: individuals with mild cognitive impairment who were predicted to have an abnormal concentration of amyloid in their spinal fluid were found to be 2.5 times more likely to develop Alzheimer’s disease2.\nWhile a wide range of other proposed blood tests for Alzheimer’s disease are being developed, this is the first study to use a machine learning approach to identify sets of proteins in blood that are predictive of a biomarker in spinal fluid. This approach is easily extended to model other spinal fluid-based biomarkers – in fact, my team is presenting new work on a blood test for another key Alzheimer’s biomarker, tau, at the 14th International Conference on Alzheimer’s and Parkinson’s Diseases in Lisbon at the end of March.\nThe publication of this research comes just in time for Brain Awareness Week, happening this week. As our population lives longer, neurodegenerative diseases such as Parkinson’s, Alzheimer’s and Huntington’s are affecting millions of people around the world. While these mysterious and crippling diseases do not yet have a cure, the answer to slowing their growth may lie in prevention. At IBM Research, our mission is to use AI and technology to understand how to help clinicians better detect and ultimately prevent these diseases in their early stages. Whether that’s through retinal imaging, blood biomarkers or minor changes in speech, we envision a future in which health professionals have a wide array of easily accessible data available to more clearly identify and track the onset and acceleration of these conditions.\n- Palmqvist, S. et al. Earliest accumulation of β-amyloid occurs within the default-mode network and concurrently affects brain connectivity. Nat. Comm. 8, 1214 (2017).\n- Goudey, B. et al. A blood-based signature of cerebrospinal fluid Aβ1–42 Scientific Reports, www.nature.com/articles/s41598-018-37149-7 (2019)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:98b6603c-6c56-48f2-9d3c-61ad3e009b17>","<urn:uuid:0474c575-b6a2-47e1-b838-f4fd270b68bd>"],"error":null}
{"question":"Looking at historical data on microfinance effectiveness: how did ownership structures affect institutional performance, and what risk management frameworks were developed to address these differences?","answer":"Historical data shows that ownership structure significantly impacted performance, with non-bank financial intermediaries and banks demonstrating higher technical efficiency than NGOs and cooperatives in Latin America, based on analysis of 315 institutions across 18 countries. To address these performance differences, risk management frameworks were developed that included comprehensive compliance functions covering financial security (combating fraud, money laundering, terrorism financing), customer protection, and ethics. These frameworks required establishing internal audit, risk management, and compliance control functions independent of operational bodies, along with policies and risk mapping to manage institutional, operational, and external risks specific to microfinance institutions.","context":["Ownership and technical efficiency of microfinance institutions: Empirical evidence from Latin America\nAbstractBy using stochastic frontier analysis, this article examines the technical efficiency of different types of microfinance institutions in Latin America. In particular, it tests whether differences in technical efficiency, both intra- and interfirm, can be explained by differences in ownership. With a focus on non-governmental organizations, cooperatives and credit unions, non-bank financial intermediaries, and banks, the data set contains 1681 observations from a panel of 315 institutions operating in 18 Latin American countries. The results show that non-governmental organizations and cooperatives have much lower interfirm and intrafirm technical efficiencies than non-bank financial intermediaries and banks, which indicates the importance of ownership type for technical efficiency.\nDownload InfoIf you experience problems downloading a file, check if you have the proper application to view it first. In case of further problems read the IDEAS help page. Note that these files are not on the IDEAS site. Please be patient as the files may be large.\nAs the access to this document is restricted, you may want to look for a different version under \"Related research\" (further below) or search for a different version of it.\nBibliographic InfoArticle provided by Elsevier in its journal Journal of Banking & Finance.\nVolume (Year): 36 (2012)\nIssue (Month): 7 ()\nContact details of provider:\nWeb page: http://www.elsevier.com/locate/jbf\nMicrofinance; Efficiency; Technology; Stochastic frontier; Ownership type; Latin America;\nFind related papers by JEL classification:\n- D24 - Microeconomics - - Production and Organizations - - - Production; Cost; Capital; Capital, Total Factor, and Multifactor Productivity; Capacity\n- G21 - Financial Economics - - Financial Institutions and Services - - - Banks; Other Depository Institutions; Micro Finance Institutions; Mortgages\n- L31 - Industrial Organization - - Nonprofit Organizations and Public Enterprise - - - Nonprofit Institutions; NGOs\nPlease report citation or reference errors to , or , if you are the registered author of the cited work, log in to your RePEc Author Service profile, click on \"citations\" and make appropriate adjustments.:\n- Cull, Robert & Demirguc-Kunt, Asli & Morduch, Jonathan, 2008.\n\"Microfinance meets the market,\"\nPolicy Research Working Paper Series\n4630, The World Bank.\n- Lensink, Robert & Meesters, Aljar & Naaborg, Ilko, 2008.\n\"Bank efficiency and foreign ownership: Do good institutions matter?,\"\nJournal of Banking & Finance,\nElsevier, vol. 32(5), pages 834-844, May.\n- Robert Lensink & Aljar Meesters & Ilko Naaborg, 2008. \"Bank efficiency and foreign ownership: do good institutions matter?,\" ULB Institutional Repository 2013/14283, ULB -- Universite Libre de Bruxelles.\n- Gutiérrez-Nieto, Begoña & Serrano-Cinca, Carlos & Mar Molinero, Cecilio, 2007. \"Microfinance institutions and efficiency,\" Omega, Elsevier, vol. 35(2), pages 131-142, April.\n- Kabir Hassan, M. & Tufte, David R., 2001. \"The X-Efficiency of a Group-Based Lending Institution: The Case of the Grameen Bank,\" World Development, Elsevier, vol. 29(6), pages 1071-1082, June.\n- Loretta J. Mester, 1992.\n\"Efficiency in the savings and loan industry,\"\n92-14, Federal Reserve Bank of Philadelphia.\n- Loretta J. Mester, . \"Efficiency in the Savings and Loan Industry,\" Rodney L. White Center for Financial Research Working Papers 26-92, Wharton School Rodney L. White Center for Financial Research.\n- Meesters, Aljar & Lensink, Robert & Hermes, Niels, 2008.\n\"Outreach and Efficiency of Microfinance Institutions,\"\n08002, University of Groningen, Research Institute SOM (Systems, Organisations and Management).\n- Hermes, Niels & Lensink, Robert & Meesters, Aljar, 2011. \"Outreach and Efficiency of Microfinance Institutions,\" World Development, Elsevier, vol. 39(6), pages 938-948, June.\n- Rients Galema & Robert Lensink & Roy Mersland, 2012. \"Do Powerful CEOs Determine Microfinance Performance?,\" Journal of Management Studies, Wiley Blackwell, vol. 49(4), pages 718-742, 06.\n- Allen N. Berger & David B. Humphrey, 1990.\n\"The dominance of inefficiencies over scale and product mix economies in banking,\"\nFinance and Economics Discussion Series\n107, Board of Governors of the Federal Reserve System (U.S.).\n- Berger, Allen N. & Humphrey, David B., 1991. \"The dominance of inefficiencies over scale and product mix economies in banking,\" Journal of Monetary Economics, Elsevier, vol. 28(1), pages 117-148, August.\n- Meeusen, Wim & van den Broeck, Julien, 1977. \"Efficiency Estimation from Cobb-Douglas Production Functions with Composed Error,\" International Economic Review, Department of Economics, University of Pennsylvania and Osaka University Institute of Social and Economic Research Association, vol. 18(2), pages 435-44, June.\n- Hartarska, Valentina, 2005. \"Governance and performance of microfinance institutions in Central and Eastern Europe and the Newly Independent States,\" World Development, Elsevier, vol. 33(10), pages 1627-1643, October.\n- Aigner, Dennis & Lovell, C. A. Knox & Schmidt, Peter, 1977. \"Formulation and estimation of stochastic frontier production function models,\" Journal of Econometrics, Elsevier, vol. 6(1), pages 21-37, July.\n- Bhattacharyya, Arunava & Lovell, C. A. K. & Sahay, Pankaj, 1997. \"The impact of liberalization on the productive efficiency of Indian commercial banks,\" European Journal of Operational Research, Elsevier, vol. 98(2), pages 332-345, April.\n- Tim Coelli & Sergio Perelman, 2000. \"Technical efficiency of European railways: a distance function approach,\" Applied Economics, Taylor & Francis Journals, vol. 32(15), pages 1967-1976.\n- Battese, G E & Coelli, T J, 1995. \"A Model for Technical Inefficiency Effects in a Stochastic Frontier Production Function for Panel Data,\" Empirical Economics, Springer, vol. 20(2), pages 325-32.\n- Laurent Weill, 2004. \"Measuring Cost Efficiency in European Banking: A Comparison of Frontier Techniques,\" Journal of Productivity Analysis, Springer, vol. 21(2), pages 133-152, March.\n- Mersland, Roy & Øystein Strøm, R., 2009. \"Performance and governance in microfinance institutions,\" Journal of Banking & Finance, Elsevier, vol. 33(4), pages 662-669, April.\n- Alfons Lansink & Elvira Silva & Spiro Stefanou, 2001. \"Inter-Firm and Intra-Firm Efficiency Measures,\" Journal of Productivity Analysis, Springer, vol. 15(3), pages 185-199, May.\n- Hartarska, Valentina M., 2005. \"Governance and Performance of Microfinance Institutions in Central and Eastern Europe and the Newly Independent States,\" 2005 International Congress, August 23-27, 2005, Copenhagen, Denmark 24568, European Association of Agricultural Economists.\n- Valentina Hartarska & Steven B. Caudill & Daniel M. Gropper, 2006. \"The Cost Structure Of Microfinance Institutions In Eastern Europe And Central Asia,\" William Davidson Institute Working Papers Series wp809, William Davidson Institute at the University of Michigan.\n- Edward L. Glaeser, 2003. \"Introduction to \"The Governance of Not-for-Profit Organizations\",\" NBER Chapters, in: The Governance of Not-for-Profit Organizations, pages 1-44 National Bureau of Economic Research, Inc.\n- Julia Paxton, 2007. \"Technical Efficiency in a Semi-Formal Financial Sector: The Case of Mexico,\" Oxford Bulletin of Economics and Statistics, Department of Economics, University of Oxford, vol. 69(1), pages 57-74, 02.\n- Arrassen, Wassini, 2013. \"La microfinance : quelles leçons tirées des expériences des pays en développement ?,\" Economics Thesis from University Paris Dauphine, Paris Dauphine University, number 123456789/12692 edited by Avouyi-Dovi, Sanvi, September.\n- Goddard, John & Molyneux, Philip & Williams, Jonathan, 2014. \"Dealing with cross-firm heterogeneity in bank efficiency estimates: Some evidence from Latin America,\" Journal of Banking & Finance, Elsevier, vol. 40(C), pages 130-142.\n- John Goddard & Phil Molyneux & Jonathan Williams, 2013. \"Dealing with Cross-Firm Heterogeneity in Bank Efficiency Estimates: Some evidence from Latin America,\" Working Papers 13011, Bangor Business School, Prifysgol Bangor University (Cymru / Wales).\nFor technical questions regarding this item, or to correct its authors, title, abstract, bibliographic or download information, contact: (Zhang, Lei).\nIf you have authored this item and are not yet registered with RePEc, we encourage you to do it here. This allows to link your profile to this item. It also allows you to accept potential citations to this item that we are uncertain about.\nIf references are entirely missing, you can add them using this form.\nIf the full references list an item that is present in RePEc, but the system did not link to it, you can help with this form.\nIf you know of missing items citing this one, you can help us creating those links by adding the relevant references in the same way as above, for each refering item. If you are a registered author of this item, you may also want to check the \"citations\" tab in your profile, as there may be some citations waiting for confirmation.\nPlease note that corrections may take a couple of weeks to filter through the various RePEc services.","CHALLENGES OF CONPLIANCE FOR MICROFINANCE INSTITUTIONS\nI. The compliance\nCompliance is a recent concept that has introduced new obligations for businesses in general and particularly for financial institutions. It is defined as the obligation to ensure that the financial institution continuously complies with:\n- Legislative and regulatory provisions specific to financial activities;\n- Professional and ethical standards and practices;\n- Codes of conduct, including ethical codes and internal procedures.\nNon-compliance risk is defined as the risk of legal, regulatory, or disciplinary sanctions, significant financial losses, or reputational damage resulting from a failure to comply with specific professional provisions, whether legislative, regulatory, professional, ethical, or executive instructions.\nWithin the Financial Institution, the Compliance function covers:\n- Financial Security: Combating fraud, money laundering, terrorism financing, market abuse, and embargoes;\n- Customer Protection: Continuous protection of customers, preserving their interests and those of the markets or the Financial Institution;\n- Ethics: Adherence to the establishment’s ethical rules and handling reports from all employees.\nThe notion of compliance and its objectives are clearly illustrated in the original definition by the Basel regulator: “The purpose of the compliance function is to assist the bank in managing its compliance risk, which can be defined as the risk of legal or regulatory sanctions, financial loss, or loss to reputation a bank may suffer as a result of its failure to comply with all applicable laws, regulations, codes of conduct, and standards of good practice.”\nII. Importance of Compliance for Microfinance Institutions\nThe compliance control function emerged in the financial sector in the late 1980s in Anglo-Saxon countries. Nowadays, it has become a strong international requirement, with international organizations paying great attention to it, and regulators attaching increasing importance to it. The entire society demands more transparency and ethics.\nIn Tunisia, the compliance control function, commonly referred to as the “compliance function,” was established in 2006 under Article 34 quarter of Law No. 2006-19 dated May 2, 2006. Several texts and circulars subsequently reinforced the positioning of this function:\n- Law No. 2016-48 dated July 11, 2016, relating to banks and financial institutions, stipulating that banks and financial institutions must establish internal audit, risk management, and compliance control functions independent of operational and support bodies.\n- Circular BCT 2017-08 dated September 19, 2017, which sets internal control rules for managing the risk of money laundering and terrorism financing. The designated representative of the Financial Analysis Commission (CTAF) and their deputy must be part of the compliance control body.\n- Circular BCT 2021-05 dated August 19, 2021, titled “Governance Framework for Banks and Financial Institutions.” It highlights the advisory role of the compliance control function to the administrative and management bodies regarding compliance with applicable laws and regulations and requires various compliance-related actions.\nIII. Microfinance Institutions facing Non Compliance Risk\nMicrofinance institutions (MFIs) are financial entities specialized in assisting individuals who do not have access to traditional banks. Like a bank, an MFI provides loans, but the loan amounts are smaller than those granted by traditional banks. MFI clients are considered risky by traditional banks due to the lack of tangible assets to secure the loans.\nMFIs face specific risks that increase their compliance responsibilities, including institutional risks related to their social or commercial mission and dependence on international organizations, operational risks such as credit risk, fraud, or security, financial management risks, and external risks related to regulations, competition, demographics, physical environment, and macroeconomic conditions.\nCompliance with regulations is critical for MFIs due to their ethical commitment and humanitarian responsibility. Ensuring compliance requires establishing a robust non-compliance risk management system and regulatory monitoring within the MFI. This includes adopting various policies, codes of conduct, and risk mapping, as well as providing training and guidance to staff regarding regulatory compliance.\nSome analysts believe that the compliance function within MFIs should go beyond defining rules and monitoring risks. It should contribute to developing a strong ethical culture, exemplifying best business practices, and ensuring adherence to rules for sustainable performance. The compliance function should act as a partner to the business, translating regulations into specific operational actions, monitoring real residual risks, and facilitating change initiatives. Moreover, it should be proactive in anticipating changes in the profession and defending the interests of the MFI through lobbying efforts"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:0e65856d-970e-4bca-a5e6-08a077b061cd>","<urn:uuid:5620cfd5-6149-411e-ac6b-3eb2410ad53f>"],"error":null}
{"question":"For my research on data validation methods, how does the verification process differ between Dr. Snow's cholera data investigation and modern blockchain verification?","answer":"Dr. Snow's cholera data verification and blockchain verification use fundamentally different approaches. Dr. Snow collected and verified data by personally speaking to local residents and manually plotting locations on a map to identify patterns connecting cholera deaths to water pumps. In contrast, blockchain verification is an automated process where miners or validators verify transactions within blocks using complex computational methods. In blockchain systems, verification involves miners competing to solve a hash (a hexadecimal number) through trial and error calculations, with specific timeframes varying by cryptocurrency (10 minutes for Bitcoin, 14 seconds for Ethereum). While Dr. Snow's method led to a single verified conclusion about contaminated water pumps, blockchain verification is an ongoing process that continuously creates new blocks of verified transactions.","context":["Data in the time of Cholera\nVisualise this: We are generating, gathering and using more data than ever.\nGlobally, we produce some 2.5 million terabytes of information every single day — 90 percent of which was gathered just within the last two years, according to a 2017 report from IBM Marketing Cloud.\nHow then, can we ever make sense of such massive amounts of data?\nAccording to Mr William Teo, Visualisation Engineer at the Government Technology Agency of Singapore (GovTech), you can’t — at least not without some data visualisation chops.\n“Numbers do not give us a complete picture. They only provide a summary that is not sufficient for our understanding of the data,” said Mr Teo to the 52 participants of a data visualisation workshop organised by GovTech.\nThe four-hour workshop, conducted as part of National Engineers Day (NED) 2017, was held at Suntec Singapore on 19 July 2017.\nIn it, Mr Teo unpacked the concepts and grammar of data visualisation (‘data viz’ to its friends), giving spectacular examples of visuals along the way.\nDivining insights from data\nWhat a user really gets from a good visualisation is insight, said Mr Teo.\nTo understand a dataset in a meaningful and intuitive way, the data needs to be transformed into something that is palatable to the user.\nPreviously a freelance illustrator, Mr Teo sees data visualisation as a discipline that lies at the intersection of data science, communications and design.\nHowever, while data visualisation certainly has design elements to it, the focus and appeal of a visualisation is not merely in its aesthetics.\n“In the data viz world, there’s an ongoing obsession with the process over the outcome,” Mr Teo told the participants.\n“But the outcome is not about building a nice visualisation. Instead, it’s about helping someone make a correct or good decision.”\n“We want to create a good visualisation that amplifies our cognition, supports our decision-making and enhances our ability to understand the world.”\nJohn Snow and the outbreak of data\nMuch more than just pretty pictures, data visualisation can serve important practical purposes.\nMr Teo shared a well-known example of this: that of Dr John Snow, an English physician who in 1854 investigated the source of a serious Cholera outbreak in the Soho area of London.\nSceptical of the prevailing view that cholera was spread through ‘bad air’, Dr Snow set about collecting data on the outbreak by speaking to local residents.\nHis investigations led him to create a data visualisation in the form of a dotmap, which revealed a connection between the locations of Cholera clusters and water pumps.\n“Dr Snow plotted the locations of Cholera deaths and water pumps on the map, and used it to support his hypothesis that these wells were the source of the outbreak,” Mr Teo said.\nUpon further investigation, it turned out that the water company that supplied the pumps had contaminated them with sewage-polluted river water.\nDr Snow’s study eventually helped convince government officials to take action by disabling the water pumps; after that, the Cholera epidemic gradually subsided.\n“When visualising data, you want to support decision-making and analysis, and find patterns within the data. You also want the visualisation to communicate a message, and for it to result in some action.”\nWhat happens in Vega\nAfter learning about the purpose, concepts and grammar of data visualisation, the workshop participants were eager to try it out for themselves.\nFor the hands-on part of the workshop, the participants used Vega, an open-source web-based tool, to build their own visualisation designs.\nThe task set by Mr Teo: create visualisations for a dataset consisting of rainfall patterns over several years.\nHe guided the participants towards creating different types of visuals, each tailored for delivering a specific message.\n“To show trends in the rainfall patterns, it’s best to use a line plot because the lines show movement,” said Mr Teo, typing ‘line’ into the Vega editor and setting the visual to a line chart.\n“But if you wish to show, say, the average rainfall per month for a specific year, then a bar chart would be more appropriate,” he continued, replacing ‘line’ with ‘bar’ in the editor.\n“Vega was very user-friendly and easy to use,” said Ms Grace Hwang, an engineering graduate who now works at the Ministry of Education, adding that she looks forward to applying data visualisation techniques in her work.\nMs Nan Jiayin, a chemical engineering undergraduate at the Nanyang Technological University (NTU), said: “I felt that the workshop was well structured, and I was amazed by the charts that can be produced using data viz techniques.”\nBy applying their skills to publicly available datasets (Data.gov.sg), budding data visualisation engineers like Ms Hwang and Ms Nan can address the questions that matter to Singaporeans.\nSuch as, we posit, “When are the best times to hit the gym?” — or “Which is the right school for my child?”\nIndeed, with tools such as Vega, one would imagine an outbreak of data visualisation projects, since virtually anyone with a computer and some understanding of the concepts can start creating such visualisations to communicate ideas and answer the tough questions.\nJust as the pioneering Dr John Snow once did with ink and paper.\n- Image of Dr John Snow: Creative Commons John Snow by Rsabbattini licensed under CC By 4.0.","What Is Block Time?\nBlock time is the measure of the time it takes the miners or validators within a network to verify transactions within one block and produce a new block in that blockchain.\nBlockchains were first popularized by Bitcoin when it was introduced in 2009. The technology has grown as more cryptocurrencies are created, each of which can use different or the same blockchain, validation methods, and techniques for creating new blocks.\n- Block time is the length of time it takes to create a new block in a cryptocurrency blockchain.\n- A block is verified by miners, who compete against each other to verify the transactions and solve the hash, which creates another block.\n- Under the proof-of-work consensus mechanism, cryptocurrency is rewarded for solving a block's hash and creating a new block.\nUnderstanding Block Time\nA blockchain is a distributed database that records all transactions within a cryptocurrency network. You can think of a block within the database as a cell in a spreadsheet where transaction information is stored. Miners verify the transactions, which takes time because finding the solution to the block requires the computers to make a vast amount of trial and error calculations.\nThis is called hashing—using an algorithm to verify all the transactions within a block, which validates the authenticity of the transactions and stored information. When the block solution is found, a new block is created. The amount of time to find the solution and create a new block is the block time.\nHere are a few key points to remember if you're trying to understand block time:\n- A block is a file that records a number of the most recent cryptocurrency transactions.\n- Each block contains a reference to the block that preceded it (that's why it is theoretically impossible to alter cryptocurrency).\n- Cryptocurrency \"miners\" race against each other to solve the hash, which is the hexadecimal number generated that verifies the transactions. The winner receives a crypto coin.\nHow Is Bitcoin’s Block Time Different Than Ethereum's?\nEach cryptocurrency has a different block time—Bitcoin takes around 10 minutes, while Ethereum only takes around 14 seconds. The exact amount of time it takes for block generation varies and depends on the difficulty of the hash (the hexadecimal number generated by the hashing algorithm). In other words, block times will not always be the same.\nConsensus mechanisms exist to allow a network to agree that a transaction is valid. Cryptocurrencies can use different consensus mechanisms, which, among other factors, affect the time it takes to verify transactions and create new blocks. Proof-of-work and proof-of-stake are two types of consensus mechanisms that use different methods for verifying a transaction. Ethereum is transitioning to a proof-of-stake consensus mechanism throughout 2022, while Bitcoin remains on the more popular and energy-intensive proof-of-work mechanism.\nHow Many Bitcoins Will Ever Be Created?\nBitcoin has a limit of 21 million. There are nearly 19 million Bitcoins in circulation, and the number of Bitcoins created per year halves every four years. This slows down Bitcoin creation.\nHow Many Ethereum Will Ever Be Created?\nEthereum, unlike Bitcoin, doesn't have an upper limit on the number of coins that will be created.\nHow Do I Get a Bitcoin Block?\nYou never actually receive a Bitcoin block since it is part of Bitcoin's framework. Instead, you receive a Bitcoin when your miner solves the hash and creates another block.\nInvesting in cryptocurrencies and other Initial Coin Offerings (“ICOs”) is highly risky and speculative, and this article is not a recommendation by Investopedia or the writer to invest in cryptocurrencies or other ICOs. Since each individual's situation is unique, a qualified professional should always be consulted before making any financial decisions. Investopedia makes no representations or warranties as to the accuracy or timeliness of the information contained herein."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:4862120d-3ba4-454f-be63-e0a2f5f4b059>","<urn:uuid:22918f0a-c623-468e-88c4-df7956c6ab78>"],"error":null}
{"question":"How has the evolution of civilian cryptography been influenced by increasing computational power since the 1940s? Starting with Shannon's work at Bell Labs and leading up to modern asymmetric encryption technologies.","answer":"The evolution of civilian cryptography has been strongly tied to available computing power. In the 1940s, Claude Shannon at Bell Labs established the foundation of modern cryptography with his 1949 paper on mathematical cryptography. Initially, civilian encryption remained primarily manual since computers in the 1960s were room-sized and limited to large companies. As computational power became more accessible in the 1970s and 'working class' civilians could afford computers, cryptographic techniques evolved significantly. This led to the development of asymmetric encryption methods like RSA, which relies on the computational difficulty of factoring large numbers. Today, increasing computing power continues to impact encryption, requiring longer key sizes - RSA keys have moved from 1024 to 2048 bits to maintain security against more powerful factoring capabilities.","context":["I'm not talking about scytale, but encryption like RSA, DES, etc...\nHow did exactly civil cryptography evolve after WWII?\nclosed as too broad by e-sushi, archie, John Deters, rath, D.W. Nov 6 '13 at 6:48\nThere are either too many possible answers, or good answers would be too long for this format. Please add details to narrow the answer set or to isolate an issue that can be answered in a few paragraphs.If this question can be reworded to fit the rules in the help center, please edit the question.\nI won't answer your question up to every detail, as I would have to write a book to answer the pretty broad question to full length. But I'll give you some hints as it would be wrong to let you think that non-military cryptography has appeared in 50's and 60's only thanks to leaks from the NSA!\nBefore the 30's…\nOne of the earliest descriptions of encryption by substitution appears in the Kama-sutra, a text written in the 4th century AD by the Brahmin scholar Vatsyayana, but based on manuscripts dating back to the 4th century BC. The Kama-sutra recommends that women should study 64 arts, including \"mlecchita-vikalpa\" — the art of secret writing*, advocated in order to help women conceal the details of their liaisons. So, \"non-military cryptography\" has been known to civilians for more than 5000 years!\nWhere \"Modern\" crypto came from…\nAs for \"modern-day\" encryption, I'll only add that in the 30's and 40's, things like the Enigma cryptographic machine series were available to the general public, but they were too expensive for the average working man. Even after WW2, most military cryptography still was not all too different from the substitution ciphers known from the past, which were - theoretically - also available to every civilian. As an example, you can look at the work of Claude Shannon, who worked for Bell Labs and not the military.\nShannon worked at Bell Labs for several years. During his time there, he produced an article entitled “A mathematical theory of cryptography”. This article was written in 1945 and eventually was published in the Bell System Technical Journal in 1949. This is regarded to be the start of modern crypto… which is why Claude E. Shannon is considered by many to be the father of mathematical cryptography.\nThe (r)evolution of civilian modern crypto…\nThe cryptographic techniques and algorithms evolved in the 70's as computational power became available… and even \"working class\" civilians could afford computers. What leaked in those decades was nothing more than a hand full of different military cryptographic algorithms; but it's not as if there weren't any civilian cryptographic algorithms available at that time. As an example, you can look at the DES cipher you mention. DES was created by a research group at IBM, not the military.\nRe-reading your question, I guess you don't know (or forgot) that it was not until the 1970s, when the US government started treating cryptographic algorithms and software as munitions and interfering with university research in cryptography, that cryptography was actually pretty \"non-military\"; especially from a research and development point of view. My answer to \"Why are cryptography algorithms not exported to certain countries?\" dives a bit more into this.\nBut generally it's a known fact that there has always been ample civil research and development of cryptographic algorithms and techniques and most military cryptography is based on and/or derived from such civilian research and development. An example of a modern crypto invention in the civilian area actually resulted in Bernstein v. US Department of Justice (which made it famous in the first place). Bernstein's \"Snuffle\" was not and never has been created, influenced, manipulated, or leaked by the NSA or the US government… nevertheless, if got hit hard by US legislation. So yes, there was influence by governments and their military… but they were not the ones who gave birth to modern civilian crypto. Shannon did that in '49 with his publication, and - just like thousands of years before - it's mostly civilians (especially academics and the likes) that invented and still invent modern crypto algorithms.\nIf you need more details, you should probably check Wikipedia's \"History of cryptography\" with a section called \"Modern Cryptography\". If that's not enough, maybe it's time to buy a book about crypto history. Wikipedia has a nice listing or related books at \"Books on cryptography ~ History of cryptography\".\nLast but not least, I would like to add that it should be clear that the NSA (since you explicitly mentioned them in the question) had no chance to influence crypto before November 4, 1952… as they were founded on that date.\nThe history of RSA is that somebody working for British intel created something like it a few years earlier than the MIT guys who published publicly. Unfortunately for him, his work was so good that it was classified and kept secret until much later. That's the opposite of a leak, so your answer is no. Historically, cryptography was far less sophisticated than anything today but far less in use because it had to be manually implemented by some human being. That human had to be both pretty smart/educated and you had to trust them, or do it yourself if you could, so it was mostly (with some exceptions) for military or royal/government/diplomatic purposes centuries ago for that reason. It's more in use nowadays because computers just do it for you automatically, and anyone can set up an SSL connection by just clicking on the link to go an HTTPS protected page.\nFrom the 40s through 60s, civilian encryption simply wasn't a pressing need for the world of its day. Digital communications networks didn't exist, so manual cryptosystems like book codes sufficed for most commercial operations. International transactions were carried by voice over long distance lines, not by machines.\nHebern and other rotor machines were commercially available, although they were primarily used for diplomatic missions. Shannon published seminal work in his 1949 paper, \"Communication Theory of Secrecy Systems\", in which he proved that the one-time pad was the only theoretically secure cryptosystem. As more digital communications developed in the 60s, as the rise of multinational corporations arrived, and as information became ever more valuable, the need for security grew.\nShannon's discoveries also included theories of compression and hashing systems, both used in cryptographic systems today.\nKeep in mind that in the 1960s, a computer would fill a room, and one would serve a large company. Until the arrival of integrated circuits, computing power was not something available to individual people. Civilian cryptographic systems of the day remained primarily manual. And computer security was something achieved by keeping the door of the computer room locked.\nBut networking of computers wasn't far away. Remote terminals were spreading, and people began doing valuable work away from the office. There still wasn't much thought to security - simple passwords that prevented access were the rule of the day.\nIn the 1960s and 1970s, Horst Feistel laid the groundwork for block cypher systems, culminating in the creation of Lucifer for IBM. As e-sushi mentions, the NSA assisted by strengthening it against an attack that remained unknown in the civilian cryptographic world. Lucifer was adopted as the DES. (Ironically, until differential cryptanalysis was independently discovered in the 1990s, the NSA was accused of intentionally weakening DES with their tweaks.)\nIn 1976, Diffie and Hellman published their paper on asymmetric key exchanging, and the civilian cryptographic world changed again.\nOther people have contributed heavily, of course, but those were the key players, and much of what we know today was built on those foundations.","Asymmetric cryptography, also known as public key cryptography, uses public and private keys to encrypt and decrypt data. The keys are simply large numbers that have been paired together but are not identical (asymmetric). One key in the pair can be shared with everyone; it is called the public key. The other key in the pair is kept secret; it is called the private key. Either of the keys can be used to encrypt a message; the opposite key from the one used to encrypt the message is used for decryption.\nMany protocols like SSH, OpenPGP, S/MIME, and SSL/TLS rely on asymmetric cryptography for encryption and digital signature functions. It is also used in software programs, such as browsers, which need to establish a secure connection over an insecure network like the Internet or need to validate a digital signature. Encryption strength is directly tied to key size and doubling the key length delivers an exponential increase in strength, although it does impair performance. As computing power increases and more efficient factoring algorithms are discovered, the ability to factor larger and larger numbers also increases.\nFor asymmetric encryption to deliver confidentiality, integrity, authenticity and non-repudiability, users and systems need to be certain that a public key is authentic, that it belongs to the person or entity claimed and that it has not been tampered with nor replaced by a malicious third party. There is no perfect solution to this public key authentication problem. A public key infrastructure (PKI) -- where trusted certificate authorities certify ownership of key pairs and certificates -- is the most common approach, but encryption products based on the Pretty Good Privacy (PGP) model -- including OpenPGP -- rely on a decentralized authentication model called a web of trust, which relies on individual endorsements of the link between user and public key.\nHow asymmetric encryption works\nAsymmetric encryption algorithms use a mathematically-related key pair for encryption and decryption; one is the public key and the other is the private key. If the public key is used for encryption, the related private key is used for decryption and if the private key is used for encryption, the related public key is used for decryption.\nOnly the user or computer that generates the key pair has the private key. The public key can be distributed to anyone who wants to send encrypted data to the holder of the private key. It's impossible to determine the private key with the public one.\nThe two participants in the asymmetric encryption workflow are the sender and the receiver. First, the sender obtains the receiver's public key. Then the plaintext is encrypted with the asymmetric encryption algorithm using the recipient's public key, creating the ciphertext. The ciphertext is then sent to the receiver, who decrypts the ciphertext with his private key so he can access the sender's plaintext.\nBecause of the one-way nature of the encryption function, one sender is unable to read the messages of another sender, even though each has the public key of the receiver.\nExamples of asymmetric cryptography\nRSA (Rivest-Shamir-Adleman) -- the most widely used asymmetric algorithm -- is embedded in the SSL/TSL protocols which is used to provide communications security over a computer network. RSA derives its security from the computational difficulty of factoring large integers that are the product of two large prime numbers.\nMultiplying two large primes is easy, but the difficulty of determining the original numbers from the product -- factoring -- forms the basis of public key cryptography security. The time it takes to factor the product of two sufficiently large primes is considered to be beyond the capabilities of most attackers, excluding nation-state actors who may have access to sufficient computing power. RSA keys are typically 1024- or 2048-bits long, but experts believe that 1024-bit keys could be broken in the near future, which is why government and industry are moving to a minimum key length of 2048-bits.\nElliptic Curve Cryptography (ECC) is gaining favor with many security experts as an alternative to RSA for implementing public key cryptography. ECC is a public key encryption technique based on elliptic curve theory that can create faster, smaller, and more efficient cryptographic keys. ECC generates keys through the properties of the elliptic curve equation.\nTo break ECC, one must compute an elliptic curve discrete logarithm, and it turns out that this is a significantly more difficult problem than factoring. As a result, ECC key sizes can be significantly smaller than those required by RSA yet deliver equivalent security with lower computing power and battery resource usage making it more suitable for mobile applications than RSA.\nUses of asymmetric cryptography\nThe typical application for asymmetric cryptography is authenticating data through the use of digital signatures. Based on asymmetric cryptography, digital signatures can provide assurances of evidence to the origin, identity and status of an electronic document, transaction or message, as well as acknowledging informed consent by the signer.\nTo create a digital signature, signing software -- such as an email program -- creates a one-way hash of the electronic data to be signed. The user's private key is then used to encrypt the hash, returning a value that is unique to the hashed data. The encrypted hash, along with other information such as the hashing algorithm, forms the digital signature. Any change in the data, even to a single bit, results in a different hash value.\nThis attribute enables others to validate the integrity of the data by using the signer's public key to decrypt the hash. If the decrypted hash matches a second computed hash of the same data, it proves that the data hasn't changed since it was signed. If the two hashes don't match, the data has either been tampered with in some way -- indicating a failure of integrity -- or the signature was created with a private key that doesn't correspond to the public key presented by the signer -- indicating a failure of authentication.\nA digital signature also makes it difficult for the signing party to deny having signed something -- the property of non-repudiation. If a signing party denies a valid digital signature, their private key has either been compromised or they are being untruthful. In many countries, including the United States, digital signatures have the same legal weight as more traditional forms of signatures.\nAsymmetric cryptography can be applied to systems in which many users may need to encrypt and decrypt messages, such as encrypted email, in which a public key can be used to encrypt a message, and a private key can be used to decrypt it.\nThe SSL/TSL cryptographic protocols for establishing encrypted links between websites and browsers also make use of asymmetric encryption.\nAdditionally, Bitcoin and other cryptocurrencies rely on asymmetric cryptography as users have public keys that everyone can see and private keys that are kept secret. Bitcoin uses a cryptographic algorithm to ensure that only the legitimate owners can spend the funds.\nIn the case of the Bitcoin ledger, each unspent transaction output (UTXO) is typically associated with a public key. So if user X, who has an UTXO associated with his public key, wants to send the money to user Y, user X uses his private key to sign a transaction that spends the UTXO and creates a new UTXO that's associated with user Y's public key.\nAsymmetric vs. symmetric cryptography\nThe main difference between these two methods of encryption is that asymmetric encryption algorithms makes use of two different but related keys -- one key to encrypt the data and another key to decrypt it -- while symmetric encryption uses the same key to perform both the encryption and decryption functions.\nAnother difference between asymmetric and symmetric encryption is the length of the keys. In symmetric cryptography, the length of the keys -- which is randomly selected -- are typically set at 128-bits or 256-bits, depending on the level of security that's needed.\nHowever, in asymmetric encryption, there has to be a mathematical relationship between the public and private keys. Because hackers can potentially exploit this pattern to crack the encryption, asymmetric keys need to be much longer to offer the same level of security. The difference in the length of the keys is so pronounced that a 2048-bit asymmetric key and a 128-bit symmetric key provide just about an equivalent level of security.\nAdditionally, asymmetric encryption is slower than symmetric encryption, which has a faster execution speed.\nHistory of asymmetric cryptography\nWhitfield Diffie and Martin Hellman, researchers at Stanford University, first publicly proposed asymmetric encryption in their 1977 paper, \"New Directions in Cryptography.\" The concept had been independently and covertly proposed by James Ellis several years earlier, while he was working for the Government Communications Headquarters (GCHQ), the British intelligence and security organization. The asymmetric algorithm as outlined in the Diffie-Hellman paper uses numbers raised to specific powers to produce decryption keys. Diffie and Hellman had initially teamed up in 1974 to work on solving the problem of key distribution problem.\nThe RSA algorithm, which was based on the work of Diffie, was named after its three inventors -- Ronald Rivest, Adi Shamir and Leonard Adleman. They invented the RSA algorithm in 1977, and published it in Communications of the ACM in 1978.\nToday, RSA is the standard asymmetric encryption algorithm and it's used in many areas, including TLS/SSL, SSH, digital signatures and PGP.\nBenefits and disadvantages of asymmetric cryptography\nThe benefits of asymmetric cryptography include:\n- the key distribution problem is eliminated because there's no need for exchanging keys.\n- security is increased as the private keys don't ever have to be transmitted or revealed to anyone.\n- the use of digital signatures is enabled so that a recipient can verify that a message comes from a particular sender.\n- it allows for non-repudiation so the sender can't deny sending a message.\n- it's a slow process compared to symmetric crytography, so it's not appropriate for decrypting bulk messages.\n- if an individual loses his private key, he can't decrypt the messages he receives.\n- since the public keys aren't authenticated, no one really knows if a public key belongs to the person specified. Consequently, users have to verify that their public keys belong to them.\n- if a hacker identifies a person's private key, the attacker can read all of that individual's messages."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:427c0d4a-e2df-44b8-b5c4-f0de05a5fe81>","<urn:uuid:d2b20f21-e9e6-4c7d-aeeb-b6a930f885e8>"],"error":null}
{"question":"How do zebrafish and manta rays differ in their reproductive strategies and offspring numbers?","answer":"Zebrafish and manta rays have vastly different reproductive strategies. Zebrafish breed year-round, with females releasing multiple eggs during courtship when their genital organ contacts the male. In contrast, manta rays reproduce only every 3-5 years, with females giving birth to just one pup (rarely two) after a year-long pregnancy. Manta ray pups are quite large at birth, measuring about 1.5 meters in width, and swim off immediately without any parental care.","context":["Classification / Names\nCommon names | Synonyms | Catalog of Fishes (gen., sp.) | ITIS | CoL | WoRMS | Cloffa\nActinopterygii (ray-finned fishes) > Cypriniformes\n(Carps) > Cyprinidae\n(Minnows or carps) > Danioninae\nEtymology: Danio: Vernacular name fron India and Sri Lanka . More on author: Hamilton.\nEnvironment: milieu / climate zone / depth range / distribution range\nFreshwater; benthopelagic; pH range: 6.0 - 8.0; dH range: 5 - 19. Tropical; 18°C - 24°C (Ref. 1672); 33°N - 8°N, 66°E - 98°E\nAsia: Pakistan, India, Bangladesh, Nepal and Myanmar (Ref. 41236). Reported from Bhutan (Ref. 40882). Appearance in Colombian waters presumably by escape from an aquarium fish rearing facility (Ref. 1739).\nLength at first maturity / Size / Weight / Age\nMaturity: Lm 2.5  range ? - ? cm\nMax length : 3.8 cm SL male/unsexed; (Ref. 41236)\nVertebrae: 31 - 32. Five uniformly, pigmented, horizontal stripes on the side of the body, all extending onto the end of caudal fin rays. Anal fin distinctively striped. Lateral line absent. Rostral barbels extend to anterior margin of orbit; maxillary barbels end at about middle of opercle. Branched anal fin rays 10-12. Vertebrae 31-32.\nAdults inhabit streams, canals, ditches, ponds and beels (Ref. 1479). Occur in slow-moving to stagnant standing water bodies, particularly rice-fields (Ref. 4832); and lower reaches of streams (Ref. 58912). Common in rivulets at foot hills (Ref. 41236). Feed on worms and small crustaceans (Ref. 7020); also on insect larvae. Breed all year round (Ref. 58913). Appears to be primarily an annual species in the wild, the spawning season starting just before the onset of the monsoon (Ref. 72224). Domesticated zebrafish live on average 3.5 years, with oldest individuals surviving up to 5.5 years (Ref. 58923). Spawning is induced by temperature and commences at the onset of the monsoon season (Ref. 58913). Food availability also acts as cue for breeding (Ref. 58913). Growth rate is a vital guiding environmental factor for sexual differentiation for this species as observed in a study (Ref. 58948). In this same study, frequency and amount of food prior to and throughout gonadal differentiation period resulted in more individuals differentiating to become females and is more pronounced in hybrid than pure bred groups (Ref. 58948). Often used for mosquito control (Ref 6351). Popular for aquarium purposes (Ref. 44325). Used as a model system (=organism) for developmental biology (Ref. 47810). Aquarium keeping: in groups of 5 or more individuals; minimum aquarium size 60 cm (Ref. 51539).\nBreed all year round (Ref. 58931). From Johnson (1932), 'a female never extrudes eggs during active courtship until the genital organ comes in contact with that of the male, whereupon a small stream of eggs is ejected' (Ref. 205). Violent dashing and chasing characterise courtship finally culminating in eggs being shed a few at a time, settling freely without adhering to the bottom surface (Ref. 205).\nTalwar, P.K. and A.G. Jhingran, 1991. Inland fishes of India and adjacent countries. vol 1. A.A. Balkema, Rotterdam. 541 p. (Ref. 4832)\nIUCN Red List Status (Ref. 120744)\nCITES (Ref. 118484)\nThreat to humans\nFisheries: of no interest; aquarium: highly commercial\nEstimates based on models\nPhylogenetic diversity index (Ref. 82805\n= 0.5000 [Uniqueness, from 0.5 = low to 2.0 = high].\nBayesian length-weight: a=0.00724 (0.00330 - 0.01589), b=3.07 (2.89 - 3.25), in cm Total Length, based on LWR estimates for this (Sub)family-body shape (Ref. 93245\nTrophic Level (Ref. 69278\n): 3.1 ±0.1 se; Based on diet studies.\nResilience (Ref. 120179\n): High, minimum population doubling time less than 15 months (tmax=5.5 (in captivity); tm<1; multiple spawning per year; Fec=400-500).\nVulnerability (Ref. 59153\n): Low vulnerability (11 of 100) .","Everything You Need to Know About Diving with Manta Rays28th April 2016 | Mario Passoni\nManta rays belong to the elasmobranch group, just like sharks. They have existed for around 5 million years, and they’re closely related to other rays you may find in the ocean. Their body is flattened, but they no longer live along the bottom of the sea. Instead, they move within the water column as if they are flying, elegantly using their modified, wing-like pectoral fins.\nThere are two species of manta: the Resident Reef Manta Ray and the Giant Oceanic Manta Ray. Both are plankton feeders, so instead of teeth, they use their gills in order to filter the microscopic food from the water. It’s thought that they can live between 40 and 50 years, but we still need more information to be able to establish their exact longevity.\nNow let's see how to distinguish between the two species of manta ray.\nResident Reef Manta Ray\nOf the two species, the Reef Manta is the smallest, with an average disc-width of 10 to 11.5 feet (3-3.5 meters). The exception to this rule is the population of Mozambique, in which they can grow up to 13 to 15 feet (4-4.5 meters) wide. This species weighs around 2,900 pounds (1,300 kilos).\nFemales are larger than the males to accommodate their pup during “pregnancy.” The ventral part of the body is white with black spots, while the dorsal side is darker, forming a distinctive white or grey Y-shape along the ‘shoulders.’\nAs the name suggests, this species usually lives along coral reefs and makes short migrations according to the abundance of food.\nThe sting that is present in Stingrays is absent from all manta rays, so relax. Mantas are huge but not dangerous at all!\nPhoto credit: Jürgen Gangoly\nGiant Oceanic Manta Ray\nThis manta ray can reach a considerable size, with an average width of 13 to 16.5 feet (4-5 meters) and sometimes up to 23 feet (7 meters). They can weigh up to 4,400 pounds (2,000 kilos)!\nIts ventral part is white with black spots, but unlike the Resident Reef Manta, the pattern is not present in the area between the gills. Furthermore the lighter design on the dorsal side is T-shaped.\nAt the base of the giant manta’s tail is a fist-sized lump of cartilage. It’s an evolutionary remnant of the time they were sting rays (mantas evolved from sting rays). It’s nonfunctional.\nCompared to the reef manta, this mysterious creature is harder to see because it tends to live in the open ocean and makes longer migrations. Consequently interactions with this animal are more difficult.\nIt’s worth mentioning that research regarding a third species of manta ray, tentatively referred to as the Atlantic or Caribbean Manta, is under way. This possible species has characteristics between the Oceanic and Reef Manta Rays. Genetic testing will likely confirm or reject this hypothesis in the near future.\nPhoto credit: Elias Levy\nThe \"Black Morph\" phenomenon\nBoth species of manta present three color morphs. The first morph is called “Chevron.” This is the classic manta color pattern. The dorsal part of the body is black with some white parts on the head and on the tips of the lateral fins. The belly is white with some black spots.\nThe second morph is astonishing. It is called “Black Morph.” The mantas presenting this particular morph are completely black dorsally and almost completely black ventrally, with white patches only near the gill areas.\nThe last color morph is “Leucism” or leucitic mantas. These mantas have less pigment than usual, and can look very white. They are not albino, as they still have some pigment (their eyes aren’t red). They might have some black on their tops, and very faded markings on their bellies. They are simply much much paler than a normally colored manta.\nPhoto credit: Tam Warner Minton\nManta or Mobula?\nThere are 9 species generally called Mobula. How do we distinguish them from manta rays? First of all they are usually smaller, with a maximum disc width of 10 feet (3 meters). Mantas and mobulas are both filter feeders, but mobula rays have a bottom jaw which is undercut, so that when their mouths are closed, the edge of the lower jaw rests much further back than the upper jaw. Manta rays’ jaws are aligned.\nThe cephalic fins of mobulas are also slightly different. When they are rolled up on themselves, they resemble horns. In fact, it is this feature that gives them the name devil rays.\nThe sting is absent in mobulas as well, except in the Spine-tail Devil Ray, where it is present but short.\nMobulas tend to travel in large groups. When found, divers usually enjoy swimming amongst hundreds of individuals at one time. However, mobulas are very shy and only approachable for a few moments.\nFilter feeding machine\nManta rays feed primarily on plankton. Do you know what that is? Plankton is “the aggregate of passively floating, drifting, or somewhat motile organisms occurring in a body of water.” We can divide it into two groups: phytoplankton, made of microscopic plants and algae, and zooplankton, made mostly of microscopic animals.\nManta rays feed on zooplankton, favouring copepods, arrow worms, mysid shrimps and fish larvae. A resident Reef Manta Ray can eat a daily average of 11 pounds (5 kg) of this food!\nManta rays filter plankton through their modified gills, using their special cephalic fins as a funnel to increase the amount of food entering their mouth. Because they need to eat so much plankton, they have devised some fantastic feeding strategies:\nSurface feeding: Manta rays swim at the surface of the water and feed by opening their mouth, swallowing large amounts of plankton. In these cases, it is easy to locate and observe them directly from the boat.\nSomersaulting: One of the most beautiful spectacles that nature can offer. When there is a dense patch of plankton in the water column, some manta rays will do a backwards somersault in the water, circling around the plankton to optimize food intake. It is a wonderfully elegant show.\nBottom feeding: Individuals seem to scrape the bottom of the seabed. In reality they hover up to a few centimeters from the seabed where a lot of plankton is concentrated.\nForming feeding chains: When many manta rays are feeding in the same area and the concentration of plankton is high, they can form a “single file line.” In this way, the zooplankton which is missed by the first manta will be scooped up by the individual behind it. By cooperating with each other, mantas can increase the amount of food ingested.\nCyclone feeding: An incredible sight and a rare phenomenon, which can, for example, be seen a few times a year in the Maldives. When concentrations of plankton are extremely high, more than 30 manta rays create one huge feeding chain. Up to 200 manta rays can be seen in this formation. They start swimming in a spiral, creating a vortex. This pulls the plankton into the open mouths of the waiting mantas!\nPhoto credit: Niv Froman\nCourtship and Reproduction\nBeing able to witness the courtship and reproduction of mantas in nature is a rare sight. Courtship usually starts at a cleaning station. When the female comes into estrus, she releases hormones that attract males.\nMales usually follow the female for 20 minutes up to 48 hours, as she ‘tests’ the males’ strength and stamina by leading them on a “dance,” looping, swerving, and diving incredibly fast until there is only one male left. This male will take hold of the female’s left “wing” in his mouth, then move underneath the female so they are facing each other. Finally, copulation begins and takes just a few seconds.\nThanks to the marks left by the bite, it can be established if a female has already mated. After about a year, the female gives birth to one pup (rarely two). The pups are about 5 feet (1.5 meters) in width, and swim off without any nursing from the females. Unfortunately, it’s estimated that manta rays breed only every 3-5 years, resulting in a serious risk of extinction.\nThe number of pups who are born and survive is far lower than the number of manta rays killed by man.\nPhoto credit: Sarah Lewis\nWhere to See Manta Rays\nMantas regularly visit areas where they can eat in abundance (feeding sites) and where they can be cleaned (cleaning stations). We've collected a list of the 10 best places in the world to dive with manta ray. Check it out!\nThese are areas where, during a certain period of the year, there is a greater concentration of food. These areas are usually seasonal places, because zooplankton follow the currents influenced by weather patterns (e.g. the monsoons in the Maldives).\nPhoto credit: Niv Froman\nComparable to car washes! These are where manta rays come to have cleaner wrasse and other fish bite off all their dead skin, parasites and food detritus. It was observed that females, on average, spend more time in this kind of beauty center… a coincidence? ;)\nPhoto credit: Jürgen Gangoly\nHow to identify a manta\nEach manta is different. How can you distinguish them? Simple, you have to look at the ventral part. The black spots that you see are unique as human fingerprints. The next step is to take a picture and send it to the research centers which will analyze and compare it with the data already included in their huge database and find out if this manta has already been identified. If it's brand new, you can give it a name! This non-invasive method allows us to study and learn more about these creatures.\nPhoto credit: Jürgen Gangoly\nCode of Conduct for Diving with Manta Rays\nIt is always good to keep in mind that the marine world is not ours. We are only guests. As such there are important guidelines for interacting with manta rays:\n- Approach slowly and avoid noise.\n- Avoid excess flash photography. Do not point your flash directly into their eyes.\n- Keep your distance. Try to stay at least 5 feet (3 meters) from them. If they come close to you, remain calm. Remember, they cannot harm you.\n- Please look and do not touch. Avoid disturbing them and reduce interaction times.\n- Always try to use common sense and bring respect for these wonderful creatures.\nDid you know…?\nOrigin of the Name\nThe origin of their name is related to a legend. It says that centuries ago Spanish fishermen were frightened by a huge fish whose aspect resembled a large cloak (manta in Spanish). They feared that if they fell into the sea, the creature would wrap them up and drown them.\nIf mantas stop swimming, they will sink! Mantas, like sharks, have a cartilaginous skeleton that is very light, saving them valuable energy. However, they still weigh more than a ton. They can control their buoyancy with the variation of an oily substance located in the liver and by simply swimming.\nAnother reason for their life in motion is the need to breathe. Swimming creates a current of oxygen-rich water through the gills. They are not able to create this current themselves.\nReproduction is the Driving Force of Existence\nMother Nature gave two penises or claspers to male mantas. (What are claspers? And why do they have two? Click here for the answer).\nYou might think that if males have two penises, females have two vaginas. But this isn’t true. Females have only one vagina called a “cloaca,” an internal room where the reproductive, digestive and urinary ducts excrete their products.\nCan Mantas Sting?\nMantas have no barbs. Rarely some individuals can show a useless barb on the posterior part of the dorsal fin. They are absolutely harmless. In fact, snorkeling and diving with them creates a close connection. However, it is always important to use good conduct while in the water with mantas. See the section “Code of Conducts.”\nAnatomical studies have shown that the manta brain is very large. In fact, it has the biggest brain to body ratio among the studied fish. It is thought that they have highly developed cognitive abilities and interesting studies are in progress. Do you remember the Whale Shark? The size of a whale shark can be ten times the size of a manta, but their brain is only a third of the size of a manta brain.\nMost divers that have seen mantas note their strange behaviour. It seems that they are curious, and they tend to swim close to observe divers.\nNo one knows yet exactly why mobulas jump out of the water. Researchers think this behavior is a spectacular courtship ritual. Similar to humpback whales, it is thought that the jump is used to attract other individuals or display dominance through the noise produced. Jumping may also be a method for eliminating parasites or escaping from predators. Even mantas can do it, but it is much more rare to observe a jumping manta.\nThreats and Solutions\nThe natural predators of manta rays are a few types of sharks, killer whales and false killer whales. Occasionally you may see a manta with the characteristic ‘half-moon’ shark bite on it’s wing. But the real danger to these sea creatures is, as always, humans and their activities. Hooks and fishing nets wound and trap mantas, often leading to death (bycatching).\nThe flavor and texture of their meat is not highly sought-after. However, their gills are. In Chinese traditional medicine, it is believed that their gills have medicinal properties, such as improving the immune system, preventing cancer and aiding nursing mothers. Without criticizing this holistic medicine, there is no scientific evidence supporting these uses and recent studies have shown manta rays possess toxic levels of heavy metals harmful to humans. (More that 20 times the WHO recommendation of safe arsenic level has been found in manta gill rakers.) The manta fishing industry is highly developed in countries such as Sri Lanka and India.\nHow can we stop this? Through awareness campaigns, sustainable fishing strategies, manta conservation projects that work to introduce sustainable alternatives and by avoiding products that use parts of mantas. But above all, you can travel and enjoy manta rays in their natural habitat. This will give a commercial value to them (ecotourism). In such a way, it will be more advantageous to keep mantas alive rather than to kill them.\nIn the past few decades, the effort to study and protect mantas has increased. People, driven by their passion for the ocean, have created several associations. They have unified these energies to create networks with the purpose of the conservation of these wonderful creatures.\nThis article was written by Mario Passoni and Luca Saponari - two marine biologists involved in several projects concerning ocean conservation and education. Special thanks to Niv Froman, Tam Sawers, Sarah Lewis and Nicola Bassett from Manta Trust, Asia Armstrong from Project Manta and Anna Flam from the Marine Megafauna Foundation."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:ddece2b7-8f9d-44a8-a204-d7b996199047>","<urn:uuid:4e6eb090-8d5f-41d3-a6ab-89afb4015141>"],"error":null}
{"question":"How do Leibniz's philosophical principles and postmodernist thinkers differ in their approach to absolute truth?","answer":"Leibniz, as a rationalist philosopher, believed in discovering innate principles through logic and reason that could lead to demonstrable truths. In contrast, postmodernist thinkers reject the pursuit of absolute standards or ideal consensus, arguing that any new configuration of ideas is just as 'contingent' as its predecessor and cannot be evaluated according to any absolute standard. They prefer non-teleological approaches that don't aim at a single, stable representation of reality.","context":["The term ‘postmodernism’ exerts an instant fascination. For it suggests that ‘modernity’ is, paradoxically, already in the past; and consequently that a new form of consciousness is called for, corresponding to new social conditions. But of course it does not tell us what the distinctive character of these new conditions, or of the accompanying consciousness, is supposed to be. Expositions of postmodernism in the context of political and cultural theory often take as a negative point of reference the idea of ‘Enlightenment’. I therefore propose to look at some recent examples of anti-Enlightenment polemic and to consider their meaning from a feminist point of view. I shall use as source material the writings of three well-known philosophers—JeanFrançois Lyotard, Alasdair MacIntyre and Richard Rorty—who are among the most forceful exponents of the arguments and values which constitute postmodernism within academic philosophy.footnote1 Inevitably, then, my response to their work will also be a response tothe bigger picture which I shall trace in it. But this does not mean that I believe the whole of postmodernism, even\nMy chosen texts undoubtedly show certain common preoccupations, of which perhaps the most striking is an aversion to the idea of universality. The Enlightenment pictured the human race as engaged in an effort towards universal moral and intellectual self-realization, and so as the subject of a universal historical experiencÈ; it also postulated a universal human reason in terms of which social and political tendencies could be assessed as ‘progressive’ or otherwise (the goal of politics being defined as the realization of reason in practice).footnote2 Postmodernism rejects this picture: that is to say, it rejects the doctrine of the unity of reason. It refuses to conceive of humanity as a unitary subject striving towards the goal of perfect coherence (in its common stock of beliefs) or of perfect cohesion and stability (in its political practice).\nAll of our three philosophers illustrate, in their different ways, the postmodernist advocacy of pluralism in morals, politics and epistemology. All are struck by the thought that justification or ‘legitimation’ are practices, sustained in being by the disposition of particular, historical human communities to recognize this and not that as a good reason for doing or believing something; and all associate ‘enlightenment’ with a drive to establish communication between these local canons of rationality and to make them answerable to a single standard. But this is just what postmodernist thinkers complain of, for they question the merit of consensus as a regulative ideal of discourse. The policy of working for it seems to them to be objectionable on two counts: firstly as being historically outmoded, and secondly as being misguided or sinister in its own right.\nThe first claim frequently appears in the shape of triumphalist comments\nThe second claim—namely, that the pursuit of ideal consensus is misguided—finds expression in arguments for a more accepting attitude towards the contingency and particularity of our ‘language-games’. It is not that postmodernism subscribes to the view that whatever is, is sacrosanct: quite the reverse, in fact, in the case of Rorty and Lyotard, who prize innovation for its own sake. It does, however, deny that the replacement of one ‘game’ by another can be evaluated according to any absolute standard (e.g. as being ‘progressive’ or the reverse, in the sense fixed by a teleological view of history). The thought is that since history has no direction (or: since it is no longer possible to think of it as having a direction), any new configuration of language-games which we may succeed in substituting for the present one will be just as ‘contingent’ as its predecessor—it will be neither more nor less remote from ‘realizing (universal) reason in practice’.\nIt is not surprising then to discover in this literature a leaning towards non-teleological descriptions of discursive activity. Rorty wishes to transfer to conversation the prestige currently enjoyed by ‘enquiry’;footnote6 MacIntyre’s reflections on morality lead him to the conclusion that mythology, the range of narrative archetypes through which a culture instructs its members in their own identity, is ‘at the heart of things’.footnote7 Neither ‘conversation’ nor ‘mythology’ is naturally understood as aiming at a single, stable representation of reality, one which would deserve the name of ‘truth’ in something more than a contextual or provisional sense. And it is this negative feature which fits the terms in question for their role in expounding a ‘postmodernism of the intellect’.\nBut the divorce of intellectual activity from the pursuit of ideal consensus is too important a theme to be entrusted to one or two happily chosen words. Rorty, as we shall see later, explicitly states that a form of life which no longer aspires towards a more-than-provisional truth will be better, on broad cultural grounds, than one which continues to do so; while Lyotard goes further and equates that aspiration with ‘terror’, believing as he does that it leads inevitably to the suppression","The enlightenment rationalist G. W. Leibniz (1646-1716) was a master at articulating certain general and fundamental principles and applying these principles to various philosophical problems. Principles are statements of basic laws, truths, or rules from which other laws, truths, or rules are derived. In many cases, principles are not the results of demonstration: they are the things we use to demonstrate something. Now, Leibniz is a rationalist philosopher not an empiricist or someone who thinks all knowledge must come directly or indirectly from sense experience. This means he believes there are certain principles we can discover which are innate to our minds, that is, principles which we do not learn from sense experience. Once we are in possession of these principles, we can just think and use logic to see how they relate and how we can construct arguments using them.\nIn what follows, I present five of Leibniz’s principles and show how they can be applied to the question of whether or not there are atoms or indivisible material substances. These arguments can, perhaps, be used to think more philosophically about the current debate over whether, for example, Quarks can be further divided into point-like Preons (for some insights about this issue go here). I will be paraphrasing from Leibniz’s work Monadology as well as from Nicholas Rescher’s helpful overview of Leibniz’s principles. I will give the respective section number(s) of the Monadology after each principle for reference and some quotations will be from Leibniz’s Discourse on Metaphysics and Other Essays (Hackett Publishing). In future blogs I will cover different principles and topics. If you are interested, you can read Leibniz’s Monadology here.\nLeibniz’s favorite argument against atoms is based on the Principle of Continuity which asserts that nature never makes any leaps: “a motion never arises immediately from rest nor is it reduced to rest except through a lesser motion” (Discourse, 56). “To judge otherwise is to know little of the immense subtlety of things, which always and everywhere involves an actual infinity” (Discourse, 57). See Monadology 53-58, as well as 13, 61, 67, 72. Here is the argument:\nIf atoms are indivisible material substances not made of parts then when they collide they would instantaneously change their direction and speed, that is, they would change discontinuously.\nBut according to the Principle of Continuity no motion “arises immediately from rest nor is it reduced to rest except through a lesser motion”, that is, no motion is discontinuous.\nTherefore atoms cannot exist.\nLeibniz is saying this: when one thing collides with another and moves another, the movement can only take place continuously if each object has parts that are in turn made of parts, etc. It is the movement of these parts, however subtle and unseen, that allows for the continuous and partially elastic movement that is experienced in the collision. If atoms exist then they have no parts and therefore their movement—whether acceleration of deceleration—would not occur in a gradual, continuous manner as their internal parts shift this way and that; rather, their movement would be an immediate leap from one state to another. But this leap would violate the principle of continuity and thus atoms cannot exist. Go here for a ten second video that might help make this point about continuity, collision, and elasticity clear.\nOf course, one might be willing to accept that nature does makes leaps; certainly quantum mechanics maintains energy can be emitted in discrete, non-continuous packets. But Leibniz would say the acceptance of such leaps in nature is akin to accepting miracles. We would be accepting that there are events that cannot, in principle, be understood since their existence, not being continuous with any past event, would be grounded in nothing. This would violate the Principle of Sufficient Reason which asserts that for every contingent fact (something that could have been different) there is a sufficient reason why the fact is thus and not otherwise (Monadology 32). To violate this principle is to admit there are absurd events occurring out of connection with all other things. And this, for Leibniz, is irrational. John Dewey describes Leibniz’s point well in his 1888 book Leibniz’s New Essays Concerning the Human Understanding:\n“It is the very nature of matter to be infinitely divisible: to say this is to deny the existence of any true principle of unity. The world of nature is the world of space and time; and where in space and time shall we find a unity where we may rest? Every point in space, every moment in time, points beyond itself. It refers to the totality of which it is but a part, or, rather, a limitation. If we add resistance, we are not better situated. We have to think of something which resists; and to this something we must attribute extension,–that is to say, difference, plurality. Nor can we find any resistance which is absolute and final. There may be a body which is undivided, and which resists all energy now acting upon it; but we cannot frame an intelligible idea of a body which is absolutely indivisible. To do so is to think of a body out of all relation to existing forces, something absolutely isolated; while the forces of nature are always relative to each other….Nor do we fare better if we attempt to find unity in the world of nature as whole. Nature has its existence as whole in space and time. Indeed, it is only a way of expressing the totality of phenomena of space and time. It is a mere aggregate, a collection.” (pp. 289-290)\nA second argument against atoms can be formulated based on Leibniz’s Principle of Agency that asserts everything that exists is active to some degree: if something doesn’t act then it can’t exist (Discourse, 54, 65 and Monadology 8 and 49). Here is the argument:\nIf atoms (indivisible material things) exist then they would be completely inert: they would have no ability to actively resist and push back and thus they would be completely passive.\nBut according to the Principle of Agency (to be is to act) nothing can be completely passive.\nTherefore, atoms cannot exist.\nA third argument is based on Leibniz’s Principle of Individuality or Identity of Indiscernibles: Substances are identified through their unique properties. Thus there cannot be two beings in nature perfectly alike: if there were, we would not have two things but one (Monadology 9). Daniel Garber summarizes his argument against atoms based on this principle: “In some places he [Leibniz] argues against the existence of atoms from his principle that no two things in the world can be perfectly similar. While the argument is not altogether clear, his idea seems to be that if there were only one kind of matter, and it was always perfectly hard, then there would be no physical features to distinguish pieces of it from the same volume” (see his article “Leibniz: physics and philosophy” in the Cambridge Companion to Leibniz, pp. 321-233). If we ask why two things cannot have exactly the same properties, Leibniz would respond with reference to aforementioned Principle of Sufficient Reason: if there are no differences between x and y, then there can be no reason for x having the properties it has and y having the properties it has. And this, again, would commit us to the existence of something absurd without sufficient reason.\nAt least three interesting implications follow from the above arguments: (1) If nothing is inert then everything is active and thus alive: “Thus there is nothing fallow, sterile, or dead in the universe, no chaos and no confusion except in appearance, almost like it looks in a pond at a distance, where we might see the confused and, so to speak, teeming motion of the fish in the pond, without discerning the fish themselves” (Monadology 69; see 67 and 68 as well). Therefore we see that the Principle of Agency entails the Principle of Organicism as well: Everything that exists has life (Monadology 66-73); (2) if there are no absurd leaps in nature then all matter is continuous with all other matter and thus we see that the Principle of Community applies as well, namely, that every agent is affected by everything that happens in the universe: “All things conspire” (sympnoia panta) as the ancient Greek Hippocrates asserted (Monadology section 61); and (3) if there are no atoms or indivisible material particles then matter is infinitely divisible and we can never “get to the bottom of it” so to speak.\n Nicholas Rescher, G. W. Leibniz’s Monadology: An Edition for Students (Pittsburgh: University of Pittsburgh Press, 1991). See pp. 43-44. Rescher’s book is an indispensible resource for studying Leibniz. A very helpful collection of essays on Leibniz is The Cambridge Companion to Leibniz, ed. Jolley (New York: Cambridge University Press, 1995).\n This paraphrase draws upon Daniel Garber’s excellent analysis in his essay Leibniz: physics and philosophy, in The Cambridge Companion to Leibniz, ed. Jolley (New York: Cambridge University Press, 1995), p. 323."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:69a127ec-8225-4971-a8ed-b86da989d5c4>","<urn:uuid:a0c20287-3cb6-4464-a9f6-c3d187f30357>"],"error":null}
{"question":"What is the relationship between Denver's local transit planning and community engagement in transportation projects?","answer":"Denver combines transit planning with community engagement in several ways. The city has organizations like the Transit Alliance that helps train regular Denver residents to become transit advocates through its Citizens' Academy. Additionally, through projects like Eagle P3, extensive community outreach is conducted, including public information meetings and direct contact with affected businesses and residents. They also actively seek community input, as demonstrated by the installation of a railcar mock-up at Denver Union Station to gather feedback on design, which led to modifications such as improved wheelchair maneuverability, better luggage space, and enhanced bicycle storage.","context":["Jarrett Walker on How Denver Can Become a Great Transit City\nIn the last few years, Jarrett Walker has established himself as one of the nation’s most influential thinkers about transit. While you’ve probably read a lot of stories lately about ideas like autonomous Ubers replacing buses in a Jetsons future, Walker takes a very different view of transportation policy, grounding his analysis in reason and fact. He thinks about transit in terms of what it can do for cities and the people who live in them, and that mentality is reflected in the title of his blog, Human Transit (and a book by the same name).\nThe Transit Alliance, a nonprofit instrumental to passing FasTracks that has groomed regular Denverites into transit advocates with its Citizens’ Academy, brought Walker to Denver to speak at its 6th Annual Event on Thursday. Walker also plays an advisory role on the first-ever transit plan for Denver proper, which aims to produce a better transit system for current residents and to accommodate the city’s future growth.\nI spoke with Walker as we strolled the 16th Street Mall and into Uptown yesterday. We talked about the path for Denver to become a true transit city, why we need to pass a funding measure for city transit, and how electing a president with an anti-transit platform will affect cities like Denver.\nThis is the first part of the interview. Here’s part two.\nHow applicable is Denver to other cities you’ve worked in?\nWhen it comes to transit and how it relates to urban form, there’s a lot that we know that’s applicable to any city. There is the basic pattern of how a network with certain frequency is intersecting and connecting in certain ways, and how that would interact with a land use pattern to generate useful service that lots of people would use.\nThe principles are pretty well known. So it’s a matter, then, of two things: How those principles apply to your particular place, and then it’s also understanding what the community’s priorities are. Because the facts about transit require us to make a variety of judgments about competing priorities.\nThe most obvious example is, do you want a high ridership transit system or do you want a transit system that goes everywhere and serves everyone? That’s called the ridership-coverage trade-off, and you get two very different kinds of networks depending on what your objective is.\nDenver’s system is definitely about coverage more than ridership, because it’s governed by the Regional Transportation District. How can Denver become a great transit city with an agency tilted towards the suburbs?\nA regional transit agency is going to have, typically, a suburban majority and a suburban focus, because that’s what most of the region is. That’s true of most regions in North America. And so it’s very normal for an ambitious, dense, inner city — that wants to be even denser and wants to be more transit oriented — for it to discover that the level of service it gets from its regional transit agency is not doing everything it wants or needs.\n— Kayla Marie (@kaylamariewho) November 17, 2016\nAs you go up the density scale, transit demand goes up — in fact it goes up steeper than density. So, inevitably, even if you deployed service fairly per capita across a region, it would still not be enough for the inner city. Because car ownership is so much lower. It’s not just that there are more people in the city, it’s that those people are more likely to use transit. And so it’s not that anybody is doing anything wrong. It’s just that the city needs more transit than the regional consensus will ever support, and that’s totally normal for an American city.\nHow can you change that?\nPeople’s experience of transit isn’t just the result of what the transit agency does. It’s also the result of street design, land use planning, and a range of other things — but those two things primarily. Those two things are largely controlled by the city government. So when I’m talking to city officials, I will always say, look, I know you think that RTD does your transit to you or for you. But in fact, land use and street design are also transit decisions that make an enormous difference.\nSo one of the problems we have in a region like this, is that from the standpoint of the city it’s hard to plan for things that you don’t control. And the normal experience of a city public works official is, “My city council has expectations about roads and parking and bike lanes and so I focus on those things. And transit, that’s something that those people over there do.” So as a result, transit doesn’t get considered at every step of the decisions.\nHow can we make transit the business of the mayor and City Council and their city departments?\nOne of the reasons I’m here is because we worked on this in Seattle. What Seattle did was do its own plan for the city that would be adopted by council. Rather than just going with what the regional agency tells them, they did their own analysis of what kind of transit network they actually need in order to be the city they want to be. And that of course led to a vision of the much more intentional network than the regional agency was going to give them. So that was the first step.\nThe plan did two things: It told Seattle city staff to take care of transit, to do the right thing for transit, because the city council is now telling them to do that instead of just the distant transit agency. That’s actually very important.\nThat’s begun to happen here.\nIt has. The Denver transit plan will be a city adopted policy, which therefore tells city staff to take care of transit. In these specific ways. In ways that are clearly tied to the city’s interest rather than just RTD’s interest.\nBut then what?\nIf you build enough support for a higher level of transit than the region will provide, then sometimes city leaders will have to come up for a funding source for that. That was the next thing that happened in Seattle.\nSo that’s the direction Denver has to go in.\nMathematically, I don’t see how else to go. Ultimately, in the American system, in most states, citizens have to vote for the tax increases that help transit keep growing as it becomes necessary for the city. It makes all the sense in the world that those elections happen at the geographic level where people care about that.\nDoes Denver need the rest of the region to pass a transit measure?\nIt’s great if you can win these elections regionally, where there’s regional support. That’s preferable. But ultimately the central city should always expect to need more transit than the region would ever agree on.","Denver Transit Partners (DTP) and its project partners incorporate sustainability practices into the culture and daily operations on the Eagle P3 Project. To DTP, sustainability means conducting business in a socially, economically and environmentally responsible manner to the benefit of current and future generations, thereby creating value for all of our stakeholders.\nSustainability on the Eagle P3 Project\nFor a printable Sustainability Fact Sheet, click here\nTo read the Denver Transit Partners’ 2014 Sustainability Report, click here.\nAn important aspect of a community’s sustainability is the sustainability of it social capital – the skills, knowledge and health of the individuals in the community and their ability to work together to improve quality of life for all.\n- Workforce Development through the WIN Program – The regional Workforce Initiative Now (WIN) is a collaborative partnership between RTD, Community College of Denver, Denver Transit Partners (DTP) and the Urban League of Metropolitan Denver. WIN helps job seekers, companies, and local communities through the creation of career opportunities in the transportation and construction industries. Click here to learn more about RTD’s WIN Program.\n- Disadvantaged/Small Business Enterprise Contracts – DTP is committed to partnering with Disadvantaged Business Enterprises (DBE) and Small Business Enterprises (SBE) in designing, building, operating and maintaining our project. We support the Regional Transportation District (RTD) SBE Program as well as the City and County of Denver and Colorado Department of Transportation’s DBE Program. DTP joins these entities as they work to create a level playing field, remove barriers and assist in the development of small businesses. Click here for information on our DBE/SBE Programs.\n- Training – As a pre-requisite to working in the field, orientation training on safety and environmental issues has been given to more than 2,000 employees of DTP and its partners. Other training includes an in-depth environmental orientation, OSHA-10 and OSHA-30 training.\n- Community Outreach – Since the beginning of the project, DTP has conducted numerous public information and outreach meetings. As construction proceeds, DTP contacts local businesses and residents affected by the project’s work.\n- Railcar Refinements – DTP installed a railcar mock-up at Denver Union Station to solicit feedback on the railcar design. Based on this outreach, custom modifications were made to the railcars that will be delivered to the Eagle P3 system.\n- DTP Volunteer Day – Each year DTP holds a Day of Service where employees volunteer at local non-profit agencies. In 2012, more than 30 DTP employees volunteered concurrently at A Precious Child, The GrowHaus, and The Park People, representing 155 hours of community service to non-profit organizations that serve the community in our project corridor.\n- Community Service– DTP performs monthly community service that has resulted in the volunteering of over 700 hours of their time working with a variety community organizations.\n- Charitable Contributions – DTP gives to a variety of community organizations in the Denver Metro Region. DTP staff personally contributed over $5,000 to a variety of DTP-sponsored activities including a drive to provide backpacks with school supplies to 114 disadvantaged students in the Denver area in collaboration with A Precious Child, our Community Partner Agency for 2013. During the summer of 2012, DTP provided matching contributions to all employee donations to both the Aurora Victim Relief Fund and to the American Red Cross Colorado Chapter, supporting victims of the Aurora theater shooting and the Colorado wildfires respectively.\n- Waste Reduction – Through employee recycling and green kitchen supplies, DTP is reducing its carbon footprint in\n- Reductions of Employee Vehicle Miles Traveled – Through the encouragement of public transit use, carpooling, and biking, employees are reducing the number of vehicular miles traveled to and from work.\nSustainability considerations designed into the project include an employee rail station within a ½ mile of the commuter rail maintenance facility (CRMF); bike racks at all stations; preferred parking spaces at the CRMF for fuel-efficient and low-emitting vehicles; storm water management features; reflective roofing materials at the CRMF; water-efficient plumbing fixtures; native plants for landscaping to reduce water needed for irrigation; and energy-efficient lighting.\nCommitment to Sustainability through LEED Certification\nThe design and construction of the CRMF is on track for a LEED Silver certification.\nConstruction considerations relative to sustainability include hazardous material and site remediation, storm water management and water quality management, construction activity pollution prevention, dust mitigation and particulate mitigation, maintaining clean streets, noise mitigation, and light pollution mitigation.\nSoil Conservation / Habitat Preservation\nAll topsoil is conserved at jobsites, stockpiled and stored for reuse in the Eagle P3 project. DTP has had multiple efforts regarding fauna preservation for prairie dogs, owls and raptors.\nThe CRMF is on track to divert 75% of all construction and demolition waste from the landfill. Construction materials that have been or will be diverted include concrete, asphalt, and metals including rail scrap.\nSalvaged Material Procurement\n264 tons of surplus rail from the RTD West Corridor project was salvaged for reuse as yard rail in the Eagle P3 project. Also, DTP is currently in discussions with BNSF to salvage the rail that will be taken up along the Gold Line corridor, and to reuse it for guard rail on DTP bridge structures.\nRegional Material Procurement\nThe majority of the material being purchased in the Design/Build phase of the Eagle P3 project, with the exception of the Rolling Stock, is from regional sources within 500 miles. This has a sustainability impact as there is a reduction in transportation requirements and there is a sustainable benefit to the regional economy.\n- Concrete ties for Eagle P3 lines were manufactured in Denver by Rocla.\n- Rails were manufactured in Pueblo, Colorado.\n- All of the special trackwork, switches, frogs and crossovers come from a regional manufacturer in Cheyenne, Wyoming.\n- All ballast for the Eagle P3 project is regionally procured from Albert Frei and Sons quarry in Colorado.\nEfficient Material Use through Standardization\nMore than two-thirds of bridge structural elements such as girders and beams are standardized so that prefabrication is possible. Mechanically-stabilized earth (MSE) wall panels are also standardized and prefabricated. Factory manufactured pre-fabricated elements have less waste than field-constructed bridges and use reusable precast forms.\nOperations and Maintenance\n- Rolling Stock Sustainability Features include Lower Volatile Organic Compound (VOC)-emitting Materials and Products including paints, building materials, and cleaners; and regenerative braking that generate power in the braking mode and is feed back into the system’s power supply for traction power.\n- As the result of input from the public and special interest groups (ADA community, bicycle advocacy groups, service animal group) at public engagement meetings held in the Spring of 2011, a number of changes were made to the design of the rail cars including:\n- Improved maneuverability space for wheelchairs\n- Improved luggage space\n- Improved handrails, especially around door areas\n- Improved flip seat design & wheelchair securement\n- Added bicycle storage to accommodate variety of users\n- Added a handhold between seat head rests to assist with seating\n- Improved passenger emergency intercom accessibility"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:17d6dd4e-90e8-446d-89df-795bb32ef2af>","<urn:uuid:0b7b4304-ad46-425e-af1d-dca71969122c>"],"error":null}
{"question":"As a port operations student, I'm curious about how megaships are handled at ports. What are the main efficiency challenges in container handling, and what security risks exist in the process?","answer":"Container handling faces several efficiency challenges at ports. Megaships require more cranes, but traditional quayside cranes cause bay blocking issues where adjacent bays can't be worked simultaneously. While solutions like the Fastnet crane concept exist, increased crane deployment leads to higher power demands. Additionally, container stowage patterns on ships must be evenly spread for optimal crane utilization. The terminal yard must also handle increased operational peaks, as megaships create massive but less frequent throughput peaks and remain at port 20% longer. As for security risks, the vulnerable messaging system used for ship loading and container stowage plans can be exploited by hackers. The BAPLIE system, which determines container placement, can be manipulated to obscure container contents and weights, potentially disrupting load balance and gravity center calculations. This vulnerability could lead to weeks-long manual re-inventory processes and potentially dangerous imbalance situations that automatic transfer pumps may not handle.","context":["Getting megaships to berth is just half of the equation. Upgrading crane facilities dryside and then into the hinterland is another pressing challenge. Senior ports and maritime engineer Alex To outlines the key obstacles and opportunities.\nWith a greater volume of containers per call to handle and with megaships expected to remain at the port on average 20% longer, it is crucial that ports plan for improvements in the handling of containers to reduce turnaround times.\nAn obvious solution is to deploy more ship-to-shore cranes, but this is not as simple as it sounds. Most quayside cranes are designed to be wider than container bay blocks, which means that when a crane is working, bays either side are blocked off and cannot be worked on by the adjacent crane. This issue of bay blocking requires a complete rethink of how quay cranes are built and supported at the quayside.\nAn example of innovative crane design, which eliminates the issue of adjacent bay blocking, is the Fastnet crane concept developed by APM Terminals, where the cranes are individually mounted on a single elevated girder supported by automated moveable pillars. This enables cranes to work on all bays of the ship.\nHowever, the deployment of more cranes usually leads to a greater power demand for the port, which may sometimes be the key constraint.\nFinally, even if more cranes are adequately accommodated at the quayside, the full benefits may not be realised as the stowage pattern of the containers on the ship determines the actual number of cranes that can work on the vessel. An even spread of containers across the bays of the ship is required for full utilisation of quayside cranes.\nThe number of boxes handled per lift could be increased to improve the handling rate. Traditional crane arrangements consist either of two TEUs or a single FEU (40ft equivalent unit) utilising a single spreader and head-block.\nModern-day innovations have seen productivity of cranes improved by utilising two trolleys, increased hoist speeds and deploying tandem FEU lifting configurations using two spreaders and head-blocks. A tandem FEU configuration can essentially allow for the lifting of four TEUs, two FEUs or a combination in a single lift.\nAlthough the handling of more containers per lift will improve quayside productivity, there are a number of associated operational factors, which need to be considered for this option to be fully beneficial. With the utilisation of tandem FEU cranes, congestion at the yard and wharf areas is likely to be exacerbated since a greater number of tractor trailers are required for each lift. To reduce congestion a tandem chassis can be employed under the crane where four TEUs or two FEUs can be transported per tractor-trailer. The main disadvantage of a tandem chassis is that the terminal layout might have to be reconfigured to accommodate the larger and wider tracker trailer system.\nScaling the peaks\nQuayside handling efficiency doesn’t finish the jigsaw. Once the quayside matches the demands of the megaship, the efficiency of the terminal yard must also accommodate quayside operations. The biggest impact on yard side operations will be the increase in operational peaks and the duration of these peaks. The throughput may only be marginally more with megaships, but the little-and-often pace of before is replaced by massive-but-less.\nHow do operators cope with the need to perform faster in peak times of activity, and make the most of downtime? Megaships will be quayside for 20% longer, but the incentive to get them out quicker – closer to 60 TEU an hour than 40 – not least to avoid demurrage – is increasingly important.\nThe right models and solutions can help clients make better use of downtime to organise stacking so that containers are more quickly stored, registered and moved on, so that the megaships are turned around quicker. Ports will also welcome more than megaships, so it’s important not to over-focus on these giants, which may not visit every day. Port optimisation is therefore valuable exercise.\nStorage capacity is one of the key choke points of the container terminal, and should be increased where possible. Long container dwell times can also have a telling impact on yard storage capacity. However, to reduce dwell times, significant alterations in port administration and cross-organisational procedures are required, which may not be easily achievable.\nWell-established ports are sometimes unable to increase appreciably in size as they are limited by adjacent infrastructure, urban development or areas of environmental/ecological importance. Where onsite storage can’t be increased, inland depots or dry ports can provide relief to congested terminals, especially if multimodal transportation is also utilised.\nBeyond the fence line\nPorts operators should ask themselves: are local transport links sufficient to allow for the greater volume of containers passing through the port? In the UK, where ports are privatised, operators are asked to contribute to the cost of upgrading local transport links as a result of port upgrades. Working with local authorities and national transport agencies is crucial to ensuring local infrastructure can accommodate more frequent peaks in port movements such as those that occur when megaships call to port.\nBy getting the masterplan right from the start, ports can avoid bottlenecks down the line. But often, the more practical and affordable solution is to assess the limitations, adapt existing infrastructure and then integrate them to create a system that works.","Ships can be hacked and the reason is its vulnerable messaging system.\nIt is a fact that ship loading and container stowage plans are created without using a secure messaging system, and there is obviously a lengthy series of electronic messages that are exchanged between the entities responsible for the creation of vessels including shipping lines, terminals, and port authorities. Understandable this flaw can be exploited by malicious threat actors anytime at their will, and this is exactly what security firm Pen Test Partners’ security consultant Ken Munro is concerned about.\nOn a daily basis, large vessels use a system called BAPLIE to displace thousands of containers some carrying around 200,000 tons’ load. This system informs port authorities where to place every single container, and the ship’s manufacturers very regularly update it. However, if customers do not use its latest version, there is every chance of foul play since criminal hackers would obscure the real contents and weight of the container by altering the information sent to the customs.\nLaw enforcement authorities cannot examine every cargo and target shipments from countries that are categorized as high-risk. If a hacker alters this information, then investigators won’t be able to detect that a container is marked as high-risk.\nIn the official blog post, it was revealed that the threat is real and anyone can perform the hack. The vulnerability of messaging system would affect the day-to-day functioning of the ship as instead of completing the job of loading and unloading in 24 to 48 hours; the ship would take weeks for manual re-inventory. Furthermore, the load planning software, which is used for placement of heavy containers at the bottom of the stacks to ensure that gravity center stays low and balance is maintained, can be exploited to disturb this balance.\n“How about if a hacker manipulated the load plan to put a ship out of balance deliberately? Disguise the data, so that the loading cranes unintentionally put the heavy containers at the top and on one side? While some balancing actions are automatic, the transfer pumps may not be able to cope with a rapidly advancing, unanticipated out of balance situation,” read the blog post.\nPen Test Partners has warned about the use of USB devices for exchanging data between ship and terminal mainly because of the possibility of inviting malware into the system since the computer having load plan software might also be used for surfing the web or emailing. Researcher claims that interoperability is vital between shipload plan and the various ports that it visits so that the load plan is securely transmitted to the port.\n“Simple = USB = vulnerable. This is ripe for attack. The consequences are financial, environmental and possibly even fatal,” states Munro.\nPen Test Partners has urged operators, terminals and ports to conduct a thorough review of their messaging systems so that the threat of tampering is curtailed given that there is already evidence of stealing of valuable items from containers parked at the port probably via insider access."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:ad13dc51-ff98-4d4c-8177-1744c299e23c>","<urn:uuid:2332893a-537f-450c-a26c-415215e41f1a>"],"error":null}
{"question":"As someone interested in artisanal textile work, I'm curious how Mulyana and Natasha Lehrer's approaches to fiber arts differ in terms of their community engagement and artistic goals?","answer":"Mulyana and Natasha Lehrer take different approaches to community engagement and artistic goals in their fiber arts work. Mulyana collaborates specifically with the transgender community in Yogyakarta, working with them to create crocheted coral installations. His work, while not intentionally focused on gender identity, involves creating Coral Islands from yarn pom poms and knitted shapes, taking 3-4 months to complete. In contrast, Lehrer's community engagement is broader, working with 30 local producers through the Illinois Green Pastures Fiber Cooperative and hosting multigenerational groups at her studio. Her focus is on teaching various fiber arts techniques like weaving, dyeing, knitting, and spinning, with an emphasis on the process of transforming raw materials into finished products.","context":["Bandung-born, Yogyakarta-based Mulyana creates colourful Coral Islands - pieced together from yarn pom poms and knitted shapes. After coming up with a design, the artist works with members of the transgender community to execute the individual 'corals'.\nA former art teacher, the 32-year-old came up with his crocheted alter-ego in 2008: The Mogus - short for Monster, Gurita ('octopus' in Bahasa Indonesia) and Sigarantang (Mulyana's clan name) - is a cephalopod which has since gone through many incarnations and variations. When we meet Mulyana at Art Stage Singapore in January 2016, he is holding a sky-blue Mogus with pink tentacles and what look like yellow antlers.\nWhile Mulyana says he did not set out to overtly make a point about gender and sexuality with his art, it is possible to read the Coral Islands as an attempt to negotiate the fluidity of bodies and identity. Floating on invisible seas, yet anchored to one another, Mulyana's corals represent the desire to move beyond arbitrary labels that devalue us as human beings; in favour of a useful, multi-armed, multi-gendered to do something useful with the people around us.\nWhat inspired you to make the Coral Islands?\nIt's tightly tied with The Mogus. The Mogus is an octopus and octopus lives in the sea. Where would an octopus like to live in the sea? Coral reefs!\nHow long does each Coral Island take to complete?\nIt varies according to the size and shape of the corals. Usually three to four months.\nHow did you start working with the transgender community?\nI had just moved to Yogyakarta from Bandung in 2014, and was trying to find something interesting for a community-based project, for an exhibition.\nI was introduced to a transgender community in Sorogenen village. This community is very, very small, but, unlike in Bandung, its members seem to be well-accepted by other people. Unlike in other regions, where the transgender community often bask on the streets or have to resort to prostitution to survive, transgender people in Sorogenen hold steady jobs in restaurants, hair salons, etc.. That was why I was interested to work with them. I wanted to show people that the transgender community has a voice and are able to channel it.\nHow do they feel about making the art with you?\nIt was difficult at first. The first time, they were more interested in a work that could produce money instantly. They were questioning the merit of working on an art project. It needed a lot of convincing before they were finally willing to work with me.\nTell us more about the women you work with, and their stories...\nOne of them is Tamara Pertamina. I met her while helping her translate subtitles for the documentary Paris is Burning, for Makcik Project, a collaborative art project in Yogyakarta in 2013 . She moved from West Java to Yogya because she was mocked for being a transgender in her home town. In Yogya, she felt she had finally found her place - she could express herself as a transgender.\nOne of the most interesting transgender women I met was Ibu Marian, who unfortunately passed away recently. Bu Marian actually runs pesantren (Islamic boarding school in Indonesia) for young transgender women. She herself has gone to the haj as a woman.\nWhy did you decide to use knitting/crochet as a medium?\nTo put it simply , knitting and crochet are very portable. I can work on a project wherever and whenever I want. It also does not need cleaning up and the process does not take up a lot of space.\nWhat sorts of stereotypes have you come up against as \"the man who knit\" [sic], the hash tag you've given yourself? Knitting is traditionally seen as a woman's thing...\nIt is! Usually people are baffled when they see me crochet or knit, but I never feel ashamed. Instead, I feel very proud, especially when I give a workshop, and the participants are amazed to find that it’s a man conducting the workshop, but are still willing to finish the workshop. I also belong to a group called “The Men Who Knit” in Bandung. It makes me feel very proud to be working with my hands!\nHow do you negotiate the idea of bodies and gender/sexual identity in your work?\nThis is kind of hard to answer. I don’t feel that there’s an idea of bodies, gender or sexual identity in any of my work. If it’s somehow percieved, it’s mostly not intentional. The Mogus is based on my wish to be useful and give more.\n- Interview and photographs by Clara Chow","Weaving Farm Upbringing and Love of Fiber Arts at Esther’s Place\nWhen most people get dressed, they don’t think about where their clothes came from or how they were made.\nNatasha Lehrer, on the other hand, appreciates every thread and fiber running through them. And she should. After all, she’s one of few Americans today who regularly sits down at a spinning wheel.\nBusiness of a Bygone Era\nThe 22-year-old from Big Rock in Kane County is the owner of Esther’s Place Fiber Arts Studio, a business she started in a restored Victorian home when she was 18.\n“It’s nontraditional in today’s world,” admits Lehrer, who opted to start the business right after high school instead of going to college. “But fiber is a thread that runs through all cultures. Women used to take their baby on their back and their spinning wheel on a horse and go to their neighbor’s house to spin and enjoy the company. In South America, people even based a woman’s rank in society on her (fiber arts) skills.”\nYou might say Lehrer was destined to become a lover of fiber arts. When she moved to Big Rock from suburban Aurora in 2000 with her mom, dad and brother, a neighbor gave her a spinning wheel. Tucked away inside the family’s new home, she discovered a loom and several books about spinning left by the previous owner.\n“Everything fell into place,” she says. “It was meant to be.”\nSpinning Dreams Into Reality\nLehrer taught herself everything from spinning and weaving to knitting and dyeing wool sheared from her family’s small flock of sheep.\n“There’s a huge amount of change from the animal to the finished product, and you can feel every step of the process,” she says. “It’s neat to see it pass right through your hands.”\nLehrer dreamed of pursuing a career in fiber arts, but she didn’t know how to make it a reality being from a small, rural farm.\n“I prayed about it, and one day, I opened my Bible to Esther 4:14, which says ‘Who knows if you have been put in this position for such a time as this?’” she recalls. “I realized if God wants something to happen, He always makes a way for it.”\nThe same day, Lehrer found an application in her mailbox for a U.S. Department of Agriculture value-added grant.\n“We hadn’t even requested it,” she says. “But our goal was to take raw fiber and add value to it by creating something artists can work with.”\nLehrer and her mother, Donna, applied for the grant, competing with giant companies such as Del Monte and Sunkist. Six months later, they got incredible news: they were awarded a grant in the amount of $24,000.\n“It was pretty amazing,” Lehrer says. “It was a huge push forward.”\nWith the help of her parents, Lehrer opened a combination retail store, art studio and learning space with an upstairs bed-and-breakfast in a 19th-century Victorian home three miles from the family’s farm. The first-floor retail store sells raw fiber and fiber that has been spun into yarn, woven into cloth, organically dyed, knit into scarves and crocheted into sweaters. There are also classrooms where Lehrer teaches weaving, dyeing, knitting, spinning, felting and other techniques.\nUpstairs, a quaint bed-and-breakfast is an inviting retreat where guests can spend a weekend eating local produce and working with local fiber.\n“Overall, we’re a place to relax, de-stress and make memories,” Lehrer says. “We get a lot of mothers and daughters and multigenerational groups. One family came and wove a rug for one of the daughters who was getting married.”\nLehrer named her business after Esther in the Bible, who served as her inspiration, as well as for a sheep named Esther on the family’s farm.\n“It has brought about a lot of great opportunities,” she says. “I was awarded a trip to a spinning conference in 2005, and I just got a piece juried into a show in Albuquerque. I’ve met two U.S. secretaries of agriculture. We were even invited to speak at a USDA conference in (Washington) D.C.”\nIntertwined With Other Artists\nEsther’s Place has been a blessing for other Illinois producers, too. In 2006, the Lehrers formed the Illinois Green Pastures Fiber Cooperative, which allows them to sell fibers from 30 other local producers in their store.\n“We wanted more variety to offer our customers and to give other producers access to a great market,” Lehrer says.\nOne of the things Lehrer enjoys most about the business is seeing people realize how much time and effort goes into working with raw fibers.\n“I’ll say, ‘That’s a sample – you can’t buy it off the shelf. But come sit down, and I’ll teach you,” she says. “We focus more on the process than the final product. It’s a lot of blood, sweat and tears. But it’s definitely worth it.”\nIf You Go …\nEsther’s Place is located at 201 W. Galena St. (Route 30) in Big Rock, Ill.; (630) 556-9665; open 10 a.m. to 6 p.m. Tuesday through Saturday.\nTo sign up for classes or special events, visit www.esthersplacefibers.com."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:3c4070a9-b4ee-4b23-91a8-a9b41e6d84e6>","<urn:uuid:fecb7bae-f43b-40b0-baaf-aeb36ad46464>"],"error":null}