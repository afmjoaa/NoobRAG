{"question":"im studying optimization methods - whats d difference between how Master Method n Geometric Programming handle recursive problems?","answer":"The Master Method and Geometric Programming handle recursive problems quite differently. The Master Method is specifically designed for solving recurrence relations of the form T(n) = aT(n/b) + f(n) where a ≥ 1 and b > 1, and works by comparing work done at different levels of a recurrence tree. It provides direct solutions for algorithms like Merge Sort (Θ(n Logn)) and Binary Search (Θ(Logn)). On the other hand, Geometric Programming focuses on optimization of polynomials and posynomials, solving through dual problems. For polynomials, it finds stationary points that could be maxima, minima, or saddle points, requiring additional testing to determine their character. The approach is more complex and can face challenges with negative optimal weights, requiring problem reformulation.","context":["Many algorithms are recursive in nature. When we analyze them, we get a recurrence relation for time complexity. We get running time on an input of size n as a function of n and the running time on inputs of smaller sizes. For example in Merge Sort, to sort a given array, we divide it in two halves and recursively repeat the process for the two halves. Finally we merge the results. Time complexity of Merge Sort can be written as T(n) = 2T(n/2) + cn. There are many other algorithms like Binary Search, Tower of Hanoi, etc.\nThere are mainly three ways for solving recurrences.\n1) Substitution Method: We make a guess for the solution and then we use mathematical induction to prove the the guess is correct or incorrect.\n2) Recurrence Tree Method: In this method, we draw a recurrence tree and calculate the time taken by every level of tree. Finally, we sum the work done at all levels. To draw the recurrence tree, we start from the given recurrence and keep drawing till we find a pattern among levels. The pattern is typically a arithmetic or geometric series.\nTo know the value of T(n), we need to calculate sum of tree nodes level by level. If we sum the above tree level by level, we get the following series T(n) = c(n^2 + 5(n^2)/16 + 25(n^2)/256) + ....\nThe above series is geometrical progression with ratio 5/16. To get an upper bound, we can sum the infinite series. We get the sum as (n2)/(1 - 5/16) which is O(n2)\n3) Master Method:\nMaster Method is a direct way to get the solution. The master method works only for following type of recurrences or for recurrences that can be transformed to following type.\nT(n) = aT(n/b) + f(n) where a >= 1 and b > 1\nHow does this work?\nMaster method is mainly derived from recurrence tree method. If we draw recurrence tree of T(n) = aT(n/b) + f(n), we can see that the work done at root is f(n) and work done at all leaves is Θ(nc) where c is Logba. And the height of recurrence tree is Logbn\nIn recurrence tree method, we calculate total work done. If the work done at leaves is polynomially more, then leaves are the dominant part, and our result becomes the work done at leaves (Case 1). If work done at leaves and root is asymptotically same, then our result becomes height multiplied by work done at any level (Case 2). If work done at root is asymptotically more, then our result becomes work done at root (Case 3).\nExamples of some standard algorithms whose time complexity can be evaluated using Master Method\nMwrge Sort :T(n) = 2T(n/2) + Θ(n). It falls in case 2 as c is 1 and Logba] is also 1. So the solution is Θ(n Logn)\nBinary Search:T(n) = T(n/2) + Θ(1). It also falls in case 2 as c is 0 and Logba is also 0. So the solution is Θ(Logn)\n1) It is not necessary that a recurrence of the form T(n) = aT(n/b) + f(n) can be solved using Master Theorem. The given three cases have some gaps between them. For example, the recurrence T(n) = 2T(n/2) + n/Logn cannot be solved using master method.\n2) Case 2 can be extended for f(n) = Θ(ncLogkn)\nIf f(n) = Θ(ncLogkn) for some constant k >= 0 and c = Logba, then T(n) = Θ(ncLogk+1n)\nSuppose that a flock of 20 pigeons flies into a set of 19 pigeonholes to roost. Because there are\n20 pigeons but only 19 pigeonholes, a least one of these 19 pigeonholes must have at least two\npigeons in it. To see why this is true, note that if each pigeonhole had at most one pigeon in it,\nat most 19 pigeons, one per hole, could be accommodated. This illustrates a general principle\ncalled the pigeonhole principle, which states that if there are more pigeons than pigeonholes,\nthen there must be at least one pigeonhole with at least two pigeons in it.\nI) If “A” is the average number of pigeons per hole, where A is not an integer then\nII) We can say as, if n + 1 objects are put into n boxes, then at least one box contains two or more objects.\nThe abstract formulation of the principle: Let X and Y be finite sets and let f: X –> Y be a function.\nPigeonhole principle is one of the simplest but most useful ideas in mathematics. We will see more applications that proof of this theorem.\nExample 1: If (Kn+1) pigeons are kept in n pigeon holes where K is a positive integer, what is the average no. of pigeons per pigeon hole?\nSolution: average number of pigeons per hole = (Kn+1)/n\n= K + 1/n\nTherefore at least a pigeonholes contains (K+1) pigeons i.e., ceil[K +1/n] and remaining contain at most K i.e., floor[k+1/n] pigeons.\ni.e., the minimum number of pigeons required to ensure that at least one pigeon hole contains (K+1) pigeons is (Kn+1).\nExample 2: A bag contains 10 red marbles, 10 white marbles, and 10 blue marbles. What is the minimum no. of marbles you have to choose randomly from the bag to ensure that we get 4 marbles of same color?\nSolution: Apply pigeonhole principle.\nNo. of colors (pigeonholes) n = 3\nNo. of marbles (pigeons) K+1 = 4\nTherefore the minimum no. of marbles required = Kn+1\nBy simplifying we get Kn+1 = 10.\nVerification: ceil[Average] is [Kn+1/n] = 4\n[Kn+1/3] = 4\nKn+1 = 10\ni.e., 3 red + 3 white + 3 blue + 1(red or white or blue) = 10\nPigeonhole principle strong form.\nTheorem: Let q1, q2, . . . , qn be positive integers.\nIf q1+ q2+ . . . + qn − n + 1 objects are put into n boxes, then either the 1st box contains at least q1 objects, or the 2nd box contains at least q2 objects, . . ., the nth box contains at least qn objects.\nApplication of this theorem is more important, so let us see how we apply this theorem in problem solving.\nExample 1: In a computer science department, a student club can be formed with either 10 members from first year or 8 members from second year or 6 from third year or 4 from final year. What is the minimum no. of students we have to choose randomly from department to ensure that a student club is formed?\nSolution: we can directly apply from the above formula where,\nq1 =10, q2 =8, q3 =6, q4 =4 and n=4\nTherefore the minimum number of students required to ensure department club to be formed is\n10 + 8 + 6 + 4 – 4 + 1 = 25\nExample 2: A box contains 6 red, 8 green, 10 blue, 12 yellow and 15 white balls. What is the minimum no. of balls we have to choose randomly from the box to ensure that we get 9 balls of same color?\nSolution: Here in this we cannot blindly apply pigeon principle. First we will see what happens if we apply above formula directly.\nFrom the above formula we have get answer 47 because 6 + 8 + 10 + 12 + 15- 5 + 1 = 47\nBut it is not correct. In order to get the correct answer we need to include only blue, yellow and white balls because red and green balls are less than 9. But we are picking randomly so we include after we apply pigeon principle.\ni.e., 9 blue + 9 yellow + 9 white – 3 + 1 = 25\nSince we are picking randomly so we can get all the red and green balls before the above 25 balls. Therefore we add 6 red + 8 green + 25 = 39\nWe can conclude that in order to pick 9 balls of same color randomly, one has to pick 39 balls from a box.","For this case either the cost or profit can be represented by a polynomial. The same procedure employing classical methods (8) will be used to obtain the dual problem, and the techniques will be essentially the same to find the optimum. However, the main difference is that stationary points will be found, and there will be no guarantee that either a maximum or a minimum has been located. It will be necessary to use the methods of Chapter 2 or local exploration to determine their character.\nIt is convenient to group the positive terms and the negative terms together to represent a general polynomial. This is written as:\nAn example of this equation, given below, will be used to illustrate the solution technique for polynomials.\nand comparing equations (3-17) and (3-18) gives:\nfor i = 1, 2, 3, ..., N, which, after multiplying by xi /y can be written as:\nfor i = 1, 2, ..., N.\nThe definition of the optimal weights (equation 3-7) can now be used to give the orthogonality conditions for polynomial optimization, i.e.:\nfor n = 1, 2, ..., N, where the subscript n has been used in place of i for convenience.\nAlso the normality condition is obtained the same way as equation (3-6) by dividing equation (3-17) by the optimal value of y, which is known in principle from the solution of the set of equations given by equatio (3-20). The result is:\nThe only algebraic manipulations that remain are to obtain the equation comparable to equation (3-14) for a polynomial profit or cost function. The procedure is the same and uses equation (3-22) as follows:\nAgain the definition of the optimal weights, equation (3-7), is used to eliminate y from the right-hand side of equation (3-23) and introduce ct and wt as:\nand the above can be written as\nThe term in the second bracket can be written as\nUsing equation (3-21) and performing the manipulations done to obtain equation (3-13), the result is comparable to equation (3-14) and is:\nThe primal and dual problems for the method of geometric programming for polynomials can be stated as:\nThe term optimize is used for both the primal and dual problems. A polynomial can represent a cost to be minimized or a profit to be maximized, since terms of both signs are used. The results obtained from the dual problem could be a maximum, a minimum, or a saddle point since stationary points are computed. Consequently, tests from Chapter 2 or local exploration would be required to determine the character of these stationary points. Before this is discussed further let us examine the geometric programming solution or equation (3-18) to illustrate the procedure.\nObtain the geometric programming solution for equation (3-18).\nThe normality and orthogonality conditions are:\nSolving simultaneously gives:\nThe optimal of y, in this case, is a maximum.\nand the optimal value of x1, x2, and x3 can be computed from the definitions of the optimal weights by selecting the most convenient form from among the following:\nUsing the first, third and fourth, gives:\nA problem can be encountered from the formulation of a geometric programming problem where the result will be negative values for the optimal weights (3,6). The dual problem required that the optimum values of y be positive. If the economic model is formulated in such a way that the optimal value is negative when calculated from the primal problem, the result will be negative weights computed in the dual problem, and it will not be possible to compute the optimum value of the function using equation (3-27). However, the value of the weights will be correct in numerical value but incorrect in sign. The previous example will be used to illustrate this difficulty, and the proof and further discussion is given by Beightler and Phillips(6).\nIn example 3-4 a maximum was found, and the value of the profit function was 0.1067. Had the example been to find the minimum cost, i.e., - y the result would have been -0.1067. However, this value could not have been calculated using equation (3-27). Reformulating the problem of -y with the positive terms first as:\nThe solution to the equation set is:\nand unacceptable negative values of the optimal weights are obtained. Although not obvious, the cause of this is that the value of the function is negative at the stationary point, i.e., -0.1067. Reformulating the problem to find the stationary point of the negative of the function will give positive optimal weights and a positive value of the function as was illustrated in example 3-4.\nIn the illustration, example 3-4, the degree of difficulty, T - (N+1), was zero. As in posynomial optimization, the degree of difficulty must be zero or greater to be able to solve the problem by geometric programming. Also if the degree of difficulty is one or more, then the dual problem is a constrained optimization problem, which has to be solved by the procedures of Chapter 2 or other methods. However, Agognio (18) has proposed a primal-dual, normed space (PDNS) algorithm which used the primal problem and the dual problem together to locate the optimum. This algorithm consists of operations within the primal and dual programs and two sets of mappings between them which depend on a least-squares solution minimizing the two-norm of the overdetermined set of linear equations. The PDNS algorithm was tested on a number of standard problems and performed essentially equally as well as other methods. Also, the dissertation of Agogino (18) describes multiobjective optimization applications of the algorithm.\nIn this example we have covered the geometric programming optimization of unconstrained posynomials and polynomials. Posynomials represented the cost function of a process, and the procedure located the global minimum by solving the dual problem for the global maximum. Polynomials represented the cost or profit function of a process, and the procedure of solving the dual problem located stationary points which could be maxima, minima, or stationary points. Their character had to be determined by the methods of Chapter 2 or by local exploration. Also, for polynomials if the numerical value of the function being optimized was negative at the stationary point, this caused the optimal weights of the dual problem to be negative. It was then necessary to seek the optimum of the negative of the function to have a positive value at the stationary point. This gave positive optimal weights, and then numerical value of the function at the stationary point was computed using equation (3-27).\nA complete discussion of geometric programming is given by Beightler and Phillips (6), which includes extensions to equality and inequality constraints. These extensions have the same complications as associated with the degrees of difficulty that occur with the unconstrained problems presented here. Because of these limitations the lengthy details for constraints will not be summarized here, and those who are interested in exploring this subject further are referred to the texts by Beightler and Phillips (6), and Reklaitis, et. al. (17).\nThe dual problem is solved when it is less complicated than the primal problem. An exponential transformation procedure for the dual problem has been described by Reklaitis et. al. (17) to make the computational problem easier when the degree of difficulty is greater than zero and also if constraints are involved. In addition, Reklaitis et. al. (17) reported on comparisons of computer codes for geometric programming optimization based on their research and that of others, including Dembo and Sarma. The testing showed that the best results were obtained with the quotient form of the generalized geometric programming problem, and second was the generalized reduced gradient solution of the exponential form of the primal problem. Also, results by Knopf, Okos, and Reklaitis (9) for batch and semicontinuous process optimization showed that the dual problem can be solved more readily than the primal problem using the generalized reduced gradient multidimensional search technique. Moreover, Phillips (10) has reported other successful applications with non-zero degrees of difficulty requiring multidimensional search methods, which are the topic of Chapter 6. In summary, if the economic model and constraints can be formulated as polynomials, there are many advantages of extensions of geometric programming which can be used for optimization.\n1. Zener, C. M. \"A Mathematical Aid in Optimizing Engineering Designs\" Proceeding of the National Academy of Science, Vol. 47, No. 4, 537-9 (April, 1961).\n2. Duffin, R. J., E. L. Peterson and C. M. Zener, Geometric Programming, John Wiley and Sons, Inc., New York (1967).\n3. Wilde, D. J. and C. S. Beightler, Foundations of Optimization Prentice-Hall, Inc., Englewood Cliffs, N.J. (1967).\n4. Zener, C. M., Engineering Design by Geometric Programming, John Wiley and Sons, Inc., New York (1971).\n5. Nijhamp, P., Planning of Industrial Complexes by Means of Geometric Programming, Rotterdam Univ. Press, Rotterdam, Netherlands (1972).\n6. Beightler, C. S. and D. T. Phillips, Applied Geometric Programming, John Wiley and Sons, Inc., New York (1976).\n7. Avriel, M. and D. J. Wilde, \"Optimal Condenser Design by Geometric Programming\", Ind. and Engr. Chem., Process Design and Development, Vol.6, No. 2, 256 (April, 1967).\n8. Chen, N. H., \"A Simplified Approach to Optimization by Geometric Programming\" Preprint 12b, 65th National Meeting, American Institute of Chemical Engineers, Cleveland, Ohio (May 4-7,1969).\n9. Knopf, C. F., M. R. Okos, and G. V. Reklaitis, \"Optimal Design of Batch/Semicontinuous Processes\", Ind. Eng. Chem, Process Des. Dev., Vol 21, No.1, 79 (1982).\n10. Phillips, D. T. Mathematical Programming for Operations Researchers and Computer Scientists, Ed. A. G. Holtzman, Marcel Dekker, Inc., New York (1981).\n11. Stocker, W. F., Design of Thermal Systems, McGraw-Hill Book Co., New York (1971).\n12. Sherwood, T. K., A Course in Process Design, MIT Press, Cambridge, Mass. (1963).\n13. Beightler, C. S., D. T. Phillips and D. J. Wilde, Foundations of Optimization, 2nd Ed., Prentice-Hall, Inc., Englewood Cliffs, N.J. (1979).\n14. Wilde, D. J., Globally Optimal Design, John Wiley and Sons, Inc., New York, (1978).\n15. Ray, W. H. and J. Szekely, Process Optimization with Applications in Metallurgy and Chemical Engineering, John Wiley and Sons, Inc., New York (1973).\n16. Beveridge, G. S. G. and R. S. Schechter, Optimization: Theory and Practice, McGraw-Hill Book Company, New York (1970).\n17. Reklaitis, G. V., A. Ravindran and K. M. Ragsdell, Engineering Optimization: Methods and Applications, John Wiley and Sons, Inc., New York (1983).\n18. Agogino, Alice M., A Primal-Dual Algorithm for Constrained Generalized Polynomial Programming: Application to Engineering Design and Multiobjective Optimization, Ph. D. Dissertation, Stanford University, Stanford, California (1984).\n3-1. Solve the following problem by geometric programming\n3-2.(10) Solve the following problem by geometric programming.\n3-3.(13) Solve the following problem by geometric programming.\n3- 4 a. Solve the following problem by geometric programming.\n3-5. Solve the following problem by geometric programming.\n3-6.(16) Consider the following geometric programming problem.\n3-7.(11) Treatment of a waste is accomplished by chemical treatment and dilution to meet effluent code requirements. The total cost is the sum of the treatment plant, pumping power requirements, and piping cost. This cost is given by the following equation\nwhere C is in dollars, D in inches, and Q in cfs. Find the minimum cost and best vaues of D and Q by geometric programming.\n3-8. The work done by a three stage compressor is given by the following equation.\nwhere P1 is the inlet pressure to stage 1, P2 is the discharge pressure from stage 1 and inlet pressure to stage 2, P3 is the discharge pressure from stage 2 and inlet pressure to stage 3, P4 is the discharge pressure from stage 3, and e is equal to (k-1)/k where k is the ratio of specific heats, a constant.\nFor specified inlet pressure P1 and volume V1 and exit pressure P4, determine intermediate pressures P2 and P3 which minimize the work by geometric programming.\nThe installed cost in dollars is 150 D and the lifetime pumping cost in dollars is 122,500/D5. The diameter D is in inches.\n3-10. Sherwood (12) considered the optimum design of a gas transmission line, and obtained the following expression for annual charges (less fixed expenses).\nwhere L is equal to pipe length between compressors in feet, D is the diameter in inches, F = r 0.219-1, where r is the ratio of inlet to outlet pressure. Determine the minimum cost, and the optimal values of L, F, D and r.\n3-11.(15) The economic model for the annual cost is given below for a furnace in which a slag-metal reaction is to be conducted.\nIn this equation L is the characteristic length of the furnace in feet and T is the temperature in oK.\nc. If the cost function only contained the first two terms, deleting the third term, indicate the effect on the solution by geometric programming.\n3-12. The profit function for each of three chemical reactors operating in parallel with the same feed is given by the three equations below. Each reactor is operating with a different catalyst and conditions of temperature and pressure. The profit function for each reactor has the feed rates x1, x2, and x3 as the independent variable, and the parameters in the equation are determined by the catalyst and operating conditions.\n3-13. A batch process has major equipment components which are a reactor, heat exchanger, centrifuge, and dryer. The total cost in dollars per batch of feed processed is given by the following equation, and it is the sum of the costs associated with each piece of equipment.\nwhere V is the volume of feed to be processed per batch in ft3, and t1 and t2 are residence times in hours for the two sections of the process.\n3-14.(6) A total of 400 cubic yards of gravel must be ferried across a river. The gravel is to be shipped in an open box of length x1, width x2, and height x3. The ends and bottom of the box cost $20/sq.yd. to build, the sides, $5/sq.yd. Runners cost $2.50/yd., and two are required to slide the box. Each round trip on the ferry cost $0.10. The problem is to find the optimal dimensions of the box that minimized the total costs of construction and transportation. This total cost is given by:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:4e0dfba3-6d19-4718-ad67-57fdc8618717>","<urn:uuid:290a2de7-f859-4b3f-9e86-2b610eef7ddb>"],"error":null}
{"question":"How does biomechanical analysis contribute to sports performance, and what role does it play in muscle development?","answer":"Biomechanical analysis enhances sports performance by enabling comprehensive measurement of movement and external forces through medical imaging, which helps establish preventive measures and optimize training recommendations. At facilities like the Julius Wolff Institute, researchers can analyze, evaluate, and optimize procedures for performance diagnostics and motor skills analysis. Regarding muscle development, biomechanical analysis helps understand how different exercises affect muscle groups differently. For example, exercises like squats engage multiple muscles including quadriceps, hamstrings, and gluteus maximus, while machines like leg extensions isolate specific muscles for concentrated strength building. This understanding allows for intelligent exercise programming that maximizes gains in strength and muscle development while ensuring proper form and technique.","context":["One of the world's largest laboratories on movement\nJulius Wolff Institute: Where science and sport interact\nLooking at the stunning facade of the brick building at Philippstraße 13 in Berlin-Mitte, you’d never know that it houses one of the largest and most state-of-the-art laboratories on movement analysis in the world.\nFor example, in Haus 11 on Campus Nord of the Humboldt University, one finds the Julius Wolff Institute for Biomechanics and Musculoskeletal Regeneration. Prof. Dr. Georg Duda founded this institute associated with the Charité – Universitätsmedizin Berlin ten years ago. Since then, he has worked in cooperation with the HU’s Sports Sciences Department under one roof to develop a leading site of research into the interplay between medicine and sports.\nProfessor Georg Duda: Working in the traditon of Julius Wolff\nProfessor Duda completed his studies in precision engineering and biomedical engineering at Technische Universität Berlin (TU) and went on to focus on the interplay between mechanics and biology in the field of orthopedics. In 2008, he became director of the Julius Wolff Institute and professor of biomechanics and musculoskeletal regeneration.\nIn 1892, the German orthopedist Julius Wolff postulated what came to be known as “Wolff’s Law” regarding the transformation of bones. This law states that bones will adapt to the load under which they are placed; for example, if loading on a bone increases, it will remodel itself over time to become stronger in order to resist that form of loading. The inverse of this law is also true. In other words, bones build up and are strengthened when loaded, but will degrade when only slightly loaded or not at all.\nAt the two locations that make up the Julius Wolff Institute, more than 80 scientists perform basic applied research in the fields of orthopedics and trauma surgery. They explore the stresses and strains placed on the human musculoskeletal system while also examining new regeneration procedures, athletes’ performance capacities and the effectiveness of training and rehabilitation. They not only focus on the bodies of professional and amateur athletes, but also on the regeneration and rehabilitation of average patients. The kinesiological focus at Campus Charité Mitte complements the regenerative focus at the Campus Virchow-Klinikum.\nThe work done by Professor Duda and his team at the institute unites several different approaches, including methods of prevention, new insights into degeneration and athletes’ aspirations to achieve ever new records. As Duda notes, the team focuses on sports performance as well as the effectiveness of training, regeneration and rehabilitation: “At our institute, we are able to analyze, evaluate and optimize procedures that were developed under laboratory conditions – including procedures designed for the rehabilitation of top athletes, performance diagnostics and the analysis of athletes’ motor skills – only under real-life conditions.”\nCooperation with athletes\nIn order to gain a more comprehensive understanding of physical measures’ influence on movement and the loading of joints, the team at the Julius Wolff Institute is using systems designed to measure movement and external forces through medical imaging in combined form. This versatile approach is then used to establish preventive measures. As Professor Duda explains, “This is the way we work, for example, in the field of injury prevention and competitive sports, but also with regard to the early diagnosis of degenerative diseases like osteoarthritis.” For example, the team at the institute is currently supervising the German national volleyball team, looking especially at how high levels of stress during training affects the structure of cartilage. Indeed, volleyball players are at an especially high risk for joint injuries. The goal of the analysis is to identify players with more unstable joints early in order to reduce the risk of injury.\nThe team closely and continuously monitors the cartilage of athletes who are deemed to be susceptible. At the same time, they provide the athletes with training recommendations designed to maximize joint stability and thus minimize the risk of osteoarthritis and injury. The junior soccer players on Berlin’s 1. FC Union team are also being treated by the experts at the Julius Wolff Institute. Among other things, the researchers measure joint stability in players’ knees; in turn, these measurements allow them to make recommendations for holistic training concepts. “We monitor our subjects to see to what extent we can improve joint stability by means of special exercises so that we can reduce the risk of injury and arthrosis,” explains the institute director.\nRUNSCAN: Biomechanical running analysis\nIn the past two years, researchers at the Julius Wolff Institute developed RunScan, a biomechanical walking and running analysis system that allows them to determine the individual running styles and patterns of athletes and orthopedic patients. “For example, we can use RunScan to analyze the running pattern of someone who is eager to run a marathon, but who would like to avoid provoking any chronic overloading of their joints due to stress or their particular running style,” says Professor Duda. RunScan can also determine how quickly athletes will be able to return to their sport after suffering an injury.\nThe first step involves recording the athlete’s comprehensive medical history. After that, he or she runs on the instrumented treadmill, where all major stress parameters are measured to determine the load on the feet and the distribution of body weight. Every step is monitored and all force parameters are recorded, with the athlete’s movements captured by two high-speed cameras and a motion analysis system. As soon as the results have been evaluated, the most important task is to provide the athlete with a custom-made training recommendation. A full examination lasts up to one hour and costs between €150 and €170.\nThe interplay of medicine and sport\nThe Julius Wolff Institute cooperates with a number of partners, including the Olympiastützpunkt Berlin, 1. FC Union Berlin, SV Babelsberg and various ALBA baskeball players. For a while now, the team has also been working together with dancers at the Staatsballett Berlin, where the injury rate can quickly reach 80% per season. The team’s goal here is to identify dancers who might have instabilities in their knees and hips or any problems with their ankles and backs; after that, their job is to offer them support and advice. The research institute is closely connected with the Center for Musculoskeletal Surgery at Charité Hospital and the HU’s Department of Training and Movement Sciences. The Julius Wolff Institute is looking forward to creating an even closer exchange among athletes, scientists and physicians.\n“Sports are a key theme in Berlin, and so, too, is science. I’m hoping these two fields work more closely together in the future, as we would benefit tremendously from more interfaces between the two worlds.” - Prof. Georg Duda\nStay ahead of the game\nLearn more about the sports metropolis Berlin, discover our BrainCity Berlin campaign or read on about Berlin as a Science Location.\nText: Anna Knüpfing\nThe article was originally published in Berlin to go 02/2018\nImages: Julius Wolff Institute","Learn something new every day More Info... by email\nThe study of biomechanics for exercise is the analysis of the forces of physical exertion on the structures of the body. Studying the effects of these forces has two main benefits. First, it enables the exerciser to improve technique and avoid injury. It also helps to improve performance and make greater fitness gains in strength, cardiovascular fitness, and flexibility. Utilizing biomechanics for exercise enables an individual to intelligently engineer exercise sessions to make maximum gains in the shortest time and in the safest way.\nDuring exercise, the body endures physical stress in order to get stronger in some way, either by building muscle size and strength or by increasing effectiveness of the cardiovascular system. This stress is created by lifting heavy weights or moving the body in ways that increase heart rate and respiration rates. Correctly done, the movements create force on the bones and muscles in a way that causes microscopic damage to muscle fibers and stresses bone and tissue. Although this sounds undesirable, it is necessary to make muscles and body tissues stronger. Once these tiny injuries heal over the course of a few days, the body is stronger than it was before exercising.\nStudying biomechanics for exercise allows the exerciser to understand the way the body responds to different types of exercise intensities, angles, and positions. For example, there are many ways to strengthen the quadriceps, the main muscle running along the front of the thigh. Yet some methods engage different surrounding muscles to differing degrees, while other methods bypass these auxiliary muscles and isolate the quadriceps. Squatting exercises engage not only the quadriceps, but also the hamstrings, gluteus maximus, and dozens of other muscles in the body to varying degrees.\nTaking this example further, the quadriceps muscle can also be strengthened through use of a leg extension machine, which involves sitting with a weighted bar at the ankle. Straightening the leg while using this machine causes the quadriceps to work with much less help from the muscles that might normally help during the movement. This has several effects.\nFirst, the quadriceps muscle is worked more directly and may build concentrated strength. Yet this movement also increases forces upon the knee that can cause damage over time, especially in an athlete who has had a knee injury in the past. Studying biomechanics for exercise will make this effect clearer so the exerciser can choose appropriate exercises.\nAvoidance of injury is another reason for studying biomechanics for exercise. In the previous example, the exerciser performed a squatting exercise to strengthen the legs, specifically the quadriceps. Performing the exercise correctly will have many beneficial effects on strength and muscle size. If the exerciser has poor form and technique, biomechanical analysis can reveal the fact and help determine the type of forces poor technique will exert on surrounding joints and tissues.\nThrough sports and exercises biomechanical software, movements can be analyzed and corrected to make the exercise safer. If the movement feels right to the exerciser, it can be difficult to understand how to correct it. Through biomechanical observation and recording of the movement, the exerciser can see his movement as if he were an outside observer and correct it, preventing injury.\nOne of our editors will review your suggestion and make changes if warranted. Note that depending on the number of suggestions we receive, this can take anywhere from a few hours to a few days. Thank you for helping to improve wiseGEEK!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:6ce46456-a13e-4e6f-9494-66e7026d84c9>","<urn:uuid:4e9e7578-96c8-49c2-9a31-8477f8dd824a>"],"error":null}
{"question":"Having worked with both methodologies, I'd like to know: do design thinking and agile share any common principles in their approach to user feedback?","answer":"Yes, design thinking and agile share significant common ground in their approach to user feedback. Both emphasize continuous user input and iteration. Design thinking promotes 'continuous dialog and encourage feedback' from users, allowing development teams to 'continuously iterate towards the desired solution.' Similarly, Agile methodology 'promotes continuous feedback from users and stakeholders on how a product can be improved' and emphasizes personal communication over formal agreements. Both approaches prioritize early and ongoing user testing - design thinking through user interviews and shadowing, while Agile implements continuous testing and user research to ensure the product remains user-centric and meets business goals.","context":["Design thinking is quickly entering the vocabulary of IT leaders. Just a few years ago,\nthe approach was largely confined to industrial design and product design circles. However, enterprises looking to differentiate themselves through custom software development are realising they must focus intently on user experience. Design thinking helps ensure applications provide the right user experience—and thus, the intended benefits—by fostering an iterative, user-centric approach across the entire application lifecycle.\nThere are a number of aspects to taking a design thinking approach to application development including being empathetic to your users, defining insights, ideation, prototyping and evaluation. In this article I’ll do a deep-dive into the concept of prototyping.\nPrototyping is a key component of the design thinking process. The first phase of an application development process is to put yourself in your end-users’ shoes. Gather information about intended users and their needs through interviews and shadowing. Define the requirement and develop a functional protype to bring ideas to life and review with users, before embarking on the actual development. Prototypes serve to test ideas by giving end users and business stakeholders something tangible to react to. It’s likely that via prototyping you’ll discover that some initial concepts miss the mark. It’s important to engage the business in continuous dialog and encourage feedback so that the development team can continuously iterate towards the desired solution.\nGlobal design consulting firm, IDEO has a saying, “If a picture is worth a thousand words, a prototype is worth a thousand meetings!” Through the process of making prototypes, a development team is able to think deeply, ask questions, and begin to uncover the true requirements for the solution—in a way that creating abstract specification documents could never replicate.\nPrototypes don’t need to be thrown away\nMany believe that prototypes serve a temporary purpose and are ultimately meant to\nbe replaced by something better. Practices like throwaway prototyping or rapid prototyping advocate for the creation of a mockup that’s eventually discarded rather than becoming part of the final delivered software. Developing the prototype quickly is important to minimise the time and money spent on the throwaway prototype. The idea is that it’s better to focus on validating concepts and refining requirements early on, rather than investing in building software that will change significantly.\nBut what if you could have the best of both worlds: the ability to rapidly create a prototype to collect feedback from users, and the ability to continue refining it and have it ultimately become the production application? You’d still get the benefit of learning through prototyping, having spent minimal time and money in the process. On top of that, the time to value for the final solution would be accelerated, because you wouldn’t need to throw the prototype away and start from scratch.\nLow-code application development platforms progress prototyping through to production\nThis is the power a low-code development platform brings to the design thinking approach. Low-code platforms employ visual, WYSIWYG development techniques that are ideal for enabling small, cross-functional teams, and even individuals, to iteratively design and build applications; progressing them through prototyping and onwards into production.\nIn the context of design thinking, developers and even business domain experts can leverage a low-code platform to quickly construct functional prototypes for validation with users. Pulling from a variety of reusable templates, functional components, and professionally designed UI elements, they can assemble screens, and begin building the application’s logic and underlying data model, without needing to create everything from scratch. Once it’s ready for feedback, the prototype can be shared with a single click and previewed instantly across web, mobile, and tablet devices. Users can provide feedback via an embedded feedback widget, and a closed loop brings this feedback directly into the development environment, facilitating rapid iteration.\nUsing a low-code platform, it is certainly possible to create throwaway mockups with the same speed and ease you would experience using common prototyping tools. In fact, as noted above, many prototypes will simply miss the mark based on feedback from users. These can be discarded without a major investment of time and money, and the team can move on to the next one.\nBut a prototype that does resonate with users can be carried forward, forming the basis of the actual finished application. The team can extend it with complex logic, integrate it with other systems, define a fine-grained security model, and more. They can leverage built-in agile project management tools to iteratively develop the solution, continuing to solicit and adapt based on user feedback. Lastly, a cloud-native architecture with out-of-the-box high availability and failover ensures that the application can be deployed at scale.\nThere are many large organisations that pursue an iterative approach to development. ADP, a payroll technology provider is a good example. The company’s product incubator embraces design thinking, especially the principle of empathy.. Once the team develops a deep understanding of an app’s intended users, they use a low-code platform to quickly build a working application that they test with real users and iterate based on their feedback. One recent app, Compass, was so successful that after rolling it out to 50,000 employees worldwide, ADP commercialised it, selling it externally.\nCombine rapid prototyping with the ability to develop and deploy enterprise-grade apps\nWhile prototyping tools will always have a place, particularly for wireframes, a low-code platform is an ideal solution for organisations looking to bring together the benefits of prototyping with the need for accelerated delivery timeframes. In the context of a design thinking approach, low-code platforms enable teams to quickly construct functional prototypes for validation with users. Because these prototypes can evolve all the way to robust production apps, time to value is reduced significantly because you don’t have to throw the prototype away and start building the production app from scratch.","Agile is a methodology that encourages a fast and iterative process to create digital products. It also promotes continuous feedback from users and stakeholders on how a product can be improved.\nUsing Agile in product design is essential to creating high-quality digital products that resonate with customers. It also helps digital products adhere to budgets and meet customer needs.\nThe planning phase of the Agile Design Process involves defining a goal and purpose, and deciding on a strategy to achieve these objectives. It also involves making choices about how you will evaluate your progress and what successful achievement means.\nIn addition, the planning phase of the Agile Design Process needs to involve communication between all members of the team. This will help teams develop trust and promote a collaborative environment.\nA well-developed plan will prevent hasty decisions and random actions, and provide a basis for future decisions. It will also ensure that every person knows what is expected of them and how to get things done.\nThe Agile Design Process is an iterative, incremental, and flexible framework that enables teams to quickly build and test products. It is a great way to deliver product development projects on time and on budget while accommodating the needs of users and businesses.\nThe design phase of the Agile Design Process typically involves research, ideation, wireframing, prototyping, and user testing. In addition, it focuses on the development of high-quality products that are relevant to users and meet business goals.\nIt is important for Agile teams to understand that design can be challenging and requires a lot of input from stakeholders and designers alike. That’s why it is important to use an effective RACI matrix and ensure everyone knows their role and responsibilities throughout the design process.\nIt is also critical to conduct user research and testing so that the product is truly user-centric. The results of this research can help designers improve their designs and add value to users.\nAgile has been embraced by teams and organizations all over the world, especially in design. It is a management method that makes teams more adaptable to change, which can lead to better products and better experiences for users.\nUnlike waterfall-based software development methods, which follow a linear and progressive process, Agile is an iterative, collaborative, and flexible methodology that provides teams with the ability to make changes quickly and efficiently. It emphasizes a functional software system over extensive documentation, personal communication over formal agreements, and partnering with clients over sticking to a blueprint.\nTo implement the Agile Design Process effectively, it is essential to create an agile design partnership across all team members and leaders. This partnership ensures that all team members are on the same page with the goals and activities of the design process. This approach can help your team avoid confusion, miscommunication, and delays. It can also help your team achieve greater design quality and consistency.\nTesting is a key part of Agile development and should be performed early and continuously. It also saves a lot of monetary cost by catching bugs in earlier stages.\nTesters should work with developers as they write code and test it to validate that the code works as expected. This helps avoid mistakes and reduces the risk of missing a release deadline.\nThe first step is to create user stories that include acceptance criteria. This helps drive the definition of done, which is a shared understanding among the team that a user story has been completed.\nNext, create a test case for each user story and generate a test run to verify that the user story passes the acceptance criteria. This helps to drive the definition of done and ensures that both testers and developers know what’s been tested and what still needs to be resolved.\nLastly, testers conduct frequent regression cycles to detect defects that may occur during development. This reduces the likelihood of missed milestones and enables the software to be released on short cycles."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:0d7129d5-e47d-4290-bf27-c10e7070b0b3>","<urn:uuid:de79f1df-29f7-41f2-a260-9effcf24c0a0>"],"error":null}
{"question":"What is the main difference between therapy dogs and service dogs in terms of their primary purpose?","answer":"The main difference is that service dogs are trained to assist people with specific disabilities by performing tasks like guiding the visually impaired or helping with mobility limitations, while therapy dogs are trained to provide comfort and emotional support to people with mental health disorders by visiting facilities like hospitals, nursing homes, and schools.","context":["Dog training is not just about teaching our canine companions a few basic commands; it is an invaluable tool that unlocks their true potential and fosters a strong bond between humans and dogs. Training serves various purposes, ranging from obedience and behavior modification to specialized tasks and recreational activities. This article will delve into the multifaceted purposes of dog training, exploring its benefits and the impact it has on both dogs and their owners.\n- Obedience Training:\nObedience training forms the foundation of a well-behaved and socially well-adjusted dog. By teaching fundamental commands such as sit, stay, down, and come, dogs learn to respond promptly and reliably to their owners’ cues. Obedience training enhances communication, instills discipline, and ensures the safety of both the dog and those around them.\n2. Behavior Modification:\nBehavioral issues can pose challenges to dog owners and negatively impact the dog’s quality of life. Through training, these problematic behaviors can be addressed and modified. Dogs can be trained to overcome issues like excessive barking, destructive chewing, jumping on people, separation anxiety, and aggression. Positive reinforcement techniques are widely used to redirect undesirable behaviors and replace them with desirable ones, creating a more harmonious living environment.\nSocialization is a vital aspect of dog training. It involves exposing dogs to various environments, people, animals, and situations, enabling them to adapt and interact appropriately. Proper socialization reduces fear, anxiety, and aggression, making dogs more confident and well-rounded. Well-socialized dogs tend to have fewer behavioral issues and can enjoy outings, interactions, and experiences with ease.\n4. Assistance and Service Dogs:\nDog training plays a significant role in shaping assistance and service dogs. These highly skilled canines assist individuals with disabilities, providing them with the support they need to navigate daily life. Service dogs can be trained to perform tasks like guiding the visually impaired, alerting those with hearing impairments, and assisting individuals with mobility limitations. The training process involves advanced obedience, specialized tasks, and an extensive focus on reliability and precision.\n5. Search and Rescue:\nSearch and rescue dogs are trained to locate missing persons or individuals trapped in disaster-stricken areas. Their specialized training enables them to follow scents, track human trails, and locate victims, even under challenging circumstances. These highly trained dogs are instrumental in saving lives and assisting in emergency situations, working alongside their human counterparts.\n6. Therapy Dogs:\nTherapy dogs bring comfort, joy, and emotional support to individuals in hospitals, nursing homes, schools, and other settings. These dogs undergo specific training to be calm, gentle, and well-mannered in various environments. Their presence has been shown to alleviate stress, reduce anxiety, and improve emotional well-being. Therapy dog training focuses on temperament evaluation, obedience, and socialization skills.\n7. Sporting and Recreational Activities:\nDog training is not limited to functional purposes; it also extends to sports and recreational activities. Activities like agility, flyball, obedience trials, dock diving, and disc dog competitions showcase the athletic abilities, intelligence, and teamwork between dogs and their handlers. Training for these activities enhances the dog’s physical fitness, mental stimulation, and overall well-being, while also fostering a stronger bond between dog and owner.\nDog training serves a multitude of purposes, all of which contribute to the betterment of our four-legged friends and the enhancement of our relationships with them. From basic obedience and behavior modification to specialized tasks and recreational pursuits, training unlocks the full potential of dogs, allowing them to lead fulfilling and purposeful lives. Moreover, training instills discipline, promotes good behavior, and fosters a deeper understanding and connection between humans and their canine companions.","Working and service dogs are dogs that have been thoroughly trained to perform a set of specific tasks in stressful environments. Working dogs are often assisting humans in their daily jobs. Service dogs are more of a helping hand to diminish the consequences of a man disability. Lastly, therapy dogs are a newer type of working dogs helping stabilize people victim of mental health issues.\nThe label of working dogs is very broad. There are different types of service, therapy, and working dogs. Whether or not you want to make one of them a member of your family, it is interesting to note the kind of jobs that the dogs are suited to perform and also be aware of the risks, if any, associated with these jobs. This way, the utmost potential of the furry ones can be recognized without them being hurt or their health compromised in the process.\nDifferences between Working, Service and Therapy dogs?\nThese special types of breeds that are considered to be working dogs can further be divided into the following types, based on their area of specialization.\nThe types of working dogs commonly fall into three broad categories:\n- working dogs – they perform specifics tasks with excellence\n- service dogs – they assist people and children with needs\n- therapy dogs – they provide mental support to distressed people\nWorking dogs actually assist human resources in performing specific tasks. A working dog has the potential to learn and execute tasks that help in assisting its human companions. Police dogs and military dogs are all considered to be working dogs while the typical jobs that they are seen to be carrying out are detection, herding, hunting, rescue, search and so on. Breeds and types of working dogs vary widely. For instance, Beagles are known for their great tracking ability and hence, they find value in the Customs & Border related authorities.\nService dogs are meant to help and assist people with certain disabilities and the dogs are trained to do so. The disabilities can be visual difficulties, stress disorder, mental illness, ambulatory issues or hearing impairments or autism. Depending on the kind of disability for which the dog is required to provide support, there can be guide dogs, mobility dogs, autism service dogs, hearing dogs and so on.\nTherapy dogs are meant to provide psychological support for people with mental health issues and disorders such as anxiety, bipolar disorder and more. The therapy dogs find wide recognition in schools for special children, hospitals and nursing homes.\nService dogs and therapy dogs also fall under the umbrella term of working dogs as such. But the differentiation is purposefully done in order to streamline the nature of tasks and that each is capable of performing.\nWhat are the Different Types of Working Dogs?\nWorking dogs are able to perform dozens of very specialized tasks including search and rescue, cancer detection, sniffer dogs, herding, guarding, military K9, and more.\nIndeed, it is a matter of great pleasure that humans are blessed to have such great companions. Working dogs are polyvalent enough to accompany in each step by either performing intelligent, physical tasks or providing psychological support. And all this is because of the different types of breeds and their uniqueness that are expressed in the dogs.\nHere are some of the different working dogs and a quick look at the nature of work they are supposed to execute along with a few other related and relevant details, worth being aware of.\nService dogs are the most common types of working dogs and their primary job is to offer assistance to people with disabilities. Since the disabilities can vary from visual impairment to impaired mobility and so on, the service dogs are specifically trained to be of immense help to the individual. Also referred to as assistance dogs, the service dogs are very good and stable in temperament. They are required to have an altogether different type of psychological make-up, the main components of which are biddability and trainability.\nEvery service dog needs some special kind of custom training for the person the animal is going to help. This is because every person experiences a different disability and looks for assistance often in a very distinct and individualistic way. The main types of service dogs usually include psychiatric service dogs and guide and hearing service dogs.\nThere are four breeds that are considered to be the best service dogs and these are:\n- Labrador Retrievers,\n- German Shepherds,\n- Golden Retrievers, and\n- Labrador and Golden Retriever cross.\nTherapy dogs are the ones that are trained to provide comfort, love, and affection to people suffering from some kind of mental health disorders. As such, the therapy dogs’ jobs require them to visit retirement homes, hospitals, schools, nursing homes, and disaster homes. The interesting history of the therapy dogs dates back to 1976 when Elaine Smith noticed a positive response in patients who were suffering from mental problems and were brought in contact with a Golden Retriever.\nThere is not much special training that is available for therapy dogs. However, they need to be friendly, loving and caring in general. There are no potential risks to the therapy dogs except that they can be frightened by unusual sounds or the sight of canes, wheelchair, and unexpected human behaviors. That is why; some organizations provide evaluation and registration for therapy dogs. Golden Retrievers and Labradors are the main breeds of therapy dogs.\nSearch & Rescue dogs\nSearch and rescue dogs are used in tracking and locating missing people, generally in the wilderness during natural disasters and mass casualty events. They work in coordination with dedicate handlers and are worked by a small team on foot. The search and rescue dogs primarily detect a human scent that might be generated from skin cells, respiratory gases, or even perspiration.\nSAR dogs can further be categorized into air-scenting dogs and trailing or tracking dogs. They also find the application of their skills in water searches, avalanche searches, and rubble searches.\nSome of the breeds that are considered the most suitable search and rescue tasks are the:\n- American Pit Bull Terrier,\n- German Shepherd,\n- Coonhound, and\n- English Springer Spaniel.\nThe SAR dogs need rigorous training for years that begins with obedience training right at the puppy stage. Scent training is carried out last once the basic agility required has been developed by the animal.\nHerding dogs are working dogs used for herding livestock or farm animals. They are a type of pastoral dog who can help in herding either by natural instincts or through training.\nCertain herding dog breeds created by humans by means of selective breeding. The process ensures that the dog’s natural inclination to treat cattle and sheep as potential prey is reduced to a significant extent. At the same time, the dog’s hunting skills are maintained to allow them to be effective in gathering and herding every animal in the group.\nAs far as the process of herding is concerned, different breeds do it differently. While some breeds like the Australian Cattle Dog nip at the heels of the animals, others like the Border Collie use their strong eye to stare the animal down. A lot of training in the field is also required.\nThe popular breeds of herding dogs include the:\n- Border Collie,\n- Australian Cattle Dog,\n- Bearded Collie,\n- Berger Picard,\n- Black Mount Cur,\n- Croatian Sheepdog,\n- English Shepherd,\n- and a few more.\nA dog’s herding instincts, as well as trainability, are measured through non-competitive herding tests and an introduction to livestock.\nSledding dogs, or sled dogs, are working dogs used for transportation or cart-pulling in the polar or arctic environment. They were required to carry supplies to the otherwise ice cold and inaccessible areas. Sled dogs have been used for both the Arctic and the Antarctic exploration though their uses have become limited to some rural areas only these days. Sled dogs were popularly used during the Alaskan gold rush as well, as a means of fast transportation.\nA sledding dog has the natural physical potential to survive the cold. Yet, it has been found that carrying loads and transporting them has certain risks to their long-term health. Sled dogs require little training for hobby sledding and cart pulling, as they tend to work very instinctively.\nSome of the common breeds of this type of working dogs include:\n- Alaskan Husky,\n- Alaskan Malamute,\n- Canadian Eskimo Dog,\n- Siberian Husky, and\n- Greenland Dog.\nHistorically, Togo and Balto were the two famous sled dogs assisting human teams in extremely adverse weather conditions. There are plenty of friendly competitions for the Arctic and Siberian dog breeds to compete on sled dog racing, cart pulling, or even on-snow racing.\nFarm dogs are yet another type of pastoral working dogs. They are different from the herding dogs in the sense that most of them are all-purpose and execute a lot of tasks at the same time. With a combination of natural instincts and a bit of training, a farm dog can be categorized depending on the specific tasks they perform, which include:\n- guarding livestock,\n- herding farm animals,\n- pest control, and\n- engaging in multipurpose farm activities.\nThese farm dogs find wide acceptance in urban farming, which is on the rise these days. The dogs, here, are meant to serve the role of companions to the farmers and agriculture experts.\nSome of the most popular farm dog breeds include the:\n- Anatolian Shepherd,\n- Old English Sheepdog,\n- Rat Terrier,\n- Miniature Pinscher,\n- and a few others.\nFarm dogs are often mutts coming from very good working bloodlines that are sold between local farmers. Some farmers make a lot more money via their small dog breeding operations than their farming activities.\nA guard dog, also referred to as a watchdog, is a heavily trained dog used to watch for and guard against unwanted or unexpected intruders – people or animals! They are different from attack dogs as such and their main capability lies in discriminating familiar and known people, from unfamiliar and potentially-threatening intruders.\nThe breeds that work most effectively as guard dogs include:\n- the Doberman Pinscher,\n- the Miniature Schnauzer,\n- the Scottish Terrier,\n- the West Highland White Terrier,\n- the German Spitz,\n- the German Shepherd, and\n- the Rottweiler.\nA powerful and threatening bark is one of the striking characteristics of guard dogs. They bark loudly to alert their owner of the presence of an intruder. Unlike the watchdog that can only bark and make people alert, the guard dog is actually capable of restraining, stopping and attacking the intruder.\nHunting and Gun Dogs\nHunting dogs are highly skilled and able working dogs recognized widely in police and intelligence services. Different breeds and types of hunting dogs are used to perform certain specific tasks. The broad categories of hunting dogs include:\n- Spaniels – flush game out of dense wood or brush\n- Hounds – track or chase prey\n- sighthounds – gazehounds that hunt by sight and speed\n- scenthounds – hunting dogs that hunt by scent rather than sight\n- lurchers – sighthounds mated with a pastoral or terrier-type dog\n- Setters – gundog mostly hunting game (e.g. pheasant, grouse, and quail)\n- Pointers – bird dogs used to find and point game to move it into gun range\n- Terriers – selectively bred for varmint hunting and rat killing\nAll breeds of hunting dogs require a lot of training with their dedicated handler. Physical agility and a unique sense of sight and sound are quintessential requirements of for all types of gun dogs.\nThe military K9 dogs or the warfare dogs are the most historically prevalent working dogs out there. They were trained to support the military, scouts, sentries, and trackers. They continue to be extremely useful in modern times as well. The training that these already active breeds of dogs require is rigorous and often, time-consuming. It is however required to be allowed in the field during critical missions.\nThe contemporary roles of these dogs include law enforcement, intimidation, and police forces. The most common breed of the military dog is German Shepherd although smaller and more resilient breeds like the Dutch Shepherd or the Belgian Malinois are being preferred these days for activities such as patrolling. They are typically found to have a keener sense of smell.\nPolice K9 Units\nPolice dogs also referred to as K-9, are highly respected and acclaimed working dogs. They are specifically trained to assist law enforcement officials and police personnel. Their primary job includes searching for explosives and drugs, identifying pieces of evidence on crime scenes, locating missing people and attacking targeted people by the police.\nThe police dogs need to be extremely fit, both physically and mentally. Their work areas pose certain risks of hazards as well. Training these dogs is a lengthy process, usually handled by a specialized K9 handler.\nThe most common breeds of Police dogs are the:\n- Belgian Malinois,\n- Dutch Shepherd, and of course,\n- the German Shepherd.\nIn many countries, injuring or killing a police dog is a criminal offense.\nDetection dogs are also called sniffer dogs. Detecting dogs are trained to sniff out substances like drugs, explosives, blood, wildlife scat, by means of their senses. Smell is the primary sense used by most of the detection dogs that are used to assist police forces.\nIn different countries, detection dogs are used for specific tasks. The two most common types of these dogs are the bed bug detection dogs and the wildlife scat detection dogs. The former is used to identify the scent of bed bugs while the latter is used to detect wildlife scat.\nSome of the popular breeds of detection dogs include:\n- English Springer Spaniel,\n- Labrador Retriever,\n- German Shepherd,\n- Bloodhound, and\n- Basset Hound.\nDetection dog breeds have a perfect combination of sensory power, mental alertness, and physical strength.\nBio Detection Dogs\nAmong the category of detection dogs, cancer detection dogs happen to be a very special type of working dogs. Recent studies have found that the olfactory ability of the dogs in detecting the presence of alkenes and aromatic compounds in urine and breath can be used in an approach to cancer screening.\nSome research in the field has led to positive results leading to some media coverage as well. However, there has been no valid, conclusive result in many cases, which is why; the use of working dogs in cancer detection is somewhat questionable. The breeds that work as detection dogs, in general, are the same for cancer detection as well.\nDogs, the most loving and friendly animal that you ever come across prove to be a perfect member of any family. However, apart from their unconditional love, care and obedience to human beings, they do come with the potential to be an integral part of the human workforce. Not all breeds are suitable for this though. There is a special category of purebred dogs that are referred to as the working dogs. Each of them is found to be suitable for certain kinds of tasks, almost all of which require unique characteristics as well as special physical and mental strength and abilities."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:12ebbc68-31b1-4725-bcb6-335a08400ef7>","<urn:uuid:76a91d80-d071-4dbb-a157-008cc94d6482>"],"error":null}
{"question":"How do geocaching and nature scavenger hunts differ in their approach to finding items?","answer":"Nature scavenger hunts involve finding multiple natural items from a prepared list within a general area, with flexibility in exact locations - participants look for things like leaves, rocks, animal tracks, or pine needles that can be found throughout the hunting grounds. In contrast, geocaching is more precise and singular in focus - participants must locate one specific container at an exact GPS coordinate location, requiring careful navigation and searching of a very specific spot. The scavenger hunt items are typically visible natural objects, while geocaches are intentionally hidden containers that need to be discovered.","context":["Finding fun ways to get yourself and the kids outdoors together can be a tricky business. Nevertheless, science and psychology are now showing us more and more that nature-time is all but essential to our kids (and our own) mental & physical well-being. So, what to do?\nEnter the nature scavenger hunt!\nIn this article, we’ll take you through the nuts and bolts of an outdoor scavenger hunt for kids, from the initial prep and planning right down to the itty bitties of ‘treasure’ selection.\nOur aim is to introduce you to a safe, fun way, healthy way for you and your kids to get out and enjoy your time together. If you’re new to outside scavenger hunting, we’d recommend giving it a try. Trust us, both your inner child and your actual child(ren) will thank you for it!\n- Do a little bit of research to find organized scavenger hunts in your area\n- Get other friends and families involved — the more the merrier!\n- Be sure to take adequate clothing so the weather doesn’t spoil your fun\n- Brainstorm potential themes and tasks to make your ideas more entertaining\n- Don’t hold your scavenger hunt where there are too many objective dangers (wildlife, poison ivy, cliffs, etc.)\nTable of Contents\n- Key Takeaways\n- Buy a Ready-Made Scavenger Hunt List\n- Make Your Own Scavenger Hunt\n- Outdoor Scavenger Hunt Ideas\n- Scavenger Hunt List of Nature Ideas\n- Collection of Nature Scavenger Hunt Printables\nBuy a Ready-Made Scavenger Hunt List\nMake Your Own Scavenger Hunt\nIn terms of fun, making your own nature treasure hunt is second only to planning for a vacation or Christmas (without, gladly, the stress, annoying relatives, and ubiquitous Bing Crosby music!). Planning a scavenger hunt for kids or teens might take some work, sure, but can easily be broken down into stages for simplification and also allows you to customize to the needs and interest of your kid(s).\nTo help you out, we’ve outlined each of these stages below. If you’re short on time, patience or aren’t too keen on all this planning lark, you can skip ahead to the ready-made, printable scavenger hunt ideas at the end of this article.\nRecon Your Hunting Ground\nWhether you’re hopping in your car to investigate in person or browsing sites found in a Google search, scoping out your location beforehand is always a good idea for the following reasons:\n- To make sure it’s safe for you and your kids\n- To check that the items on your hunt list will be discoverable\n- To acquaint yourself with the terrain in case anyone (including you!) should get lost\n- To check the available facilities\nPro Tip: Age Appropriate\nTry to make your hunt as age-appropriate as possible — while our toddlers might be happy enough pottering about in pursuit of a few items in their vicinity, teenagers are likely to want to stretch their legs and have a bit of a challenge.\nChoose a Theme\nUsing themes can really make a difference in maintaining your kids’ interest, particularly by the time you’re onto your sixth or seventh hunt. A list of potential themes is included below.\nMake a List\nAfter you’ve done your recon and theme selection, it’s time to choose the items you want on your list. We’ve included a long list of potential items below — the items you choose will vary depending on where you are in the world and the season.\nOutdoor Scavenger Hunt Ideas\nA variety of themes can spice up your scavenger hunt and help to maintain your kids’ interest. Not only this, a few tweaks and changes will mean the idea of the scavenger hunt never goes stale. Below, we’ve included some of the best:\nThis will really depend on where you are in the world, but some more obvious examples of items you can use to make it season-specific include:\n- Winter: mistletoe, holly leaf, holly berry, a green leaf, an icicle, a frozen puddle or pond, a robin, tracks in snow, snow-laden branches, evergreen plants/trees, woodpeckers, cardinals, blue jay\n- Spring: unopened buds, forget-me-nots, snowdrops, melting snow/ice, popped buds, pine warbler, common yellowthroat, pollinating bees, frogs, salamanders, tadpoles\n- Summer: edible berries, sunburnt grass, marmot, wild lavender, lizards, flowering lime tree, chicken of the woods, other mushrooms, leaf eaten by caterpillar, bluebells, poppies, cowslips\n- Fall/Autumn: chestnuts, sycamore leaves/seeds, red leaves, a bare tree, ripe blackberries, empty seed pod, multicolored leaves, crunchy leaves\nFind the Opposites\nThis is a simple hunt that is ideal for boosting your toddler’s vocab while getting them out and about. A few examples of items to find include: big/small, rough/smooth, tiny/huge, long/short, fast/slow, wet/dry, old/new.\nPro Tip: Laminate Your List\nFor littlies in particular, printing pictures or drawings of items and then laminating your list is a good idea. Not only does it help them find what they’re looking for, but also makes the list water/chocolate/ice-cream-proof (!) and reusable. Lamination machines are great for many things involving little ones.\nThis is the most basic and common types. How long, short, difficult or easy your hunt will be will depend on the number of items on your list and also their accessibility and prevalence the hunt location.\nGiven that this task is a touch trickier than a simple list-based hunt, it’s a great addition to nature scavenger hunts for older kids. A printout of the ‘hunted’ species or a pocketbook guide is highly recommended!\nKids love hearing about cool facts so why not indulge them by being a walking encyclopaedia! Unless you are already well-versed in all things nature, bring along a few pocket guidebooks that relate to the items on your childrens list.\nScavenger Hunt – Tasks\nA variety of tasks can be used to make stand-alone scavenger hunts or to spice up standard hunts. A few possibilities include: building a shelter, determining north without a compass, taking rubbings of leaves, and drawing pictures of the hunt. Tasks are ideal if you need a break or simply want to keep everyone in one place.\nSend kids out to find various items with names that begin with each letter of the alphabet, going in order from A through Z.\nQuite simply give the kids a list of colors and have them find as many items as possible matching each color…easy!\nSensory Scavenger Hunt\nAs the title suggests, this type of hunt is more experiential in format, revolving around four of the five senses: see, feel, hear, smell. For obvious reasons, the ‘taste’ list is one best saved for well-seasoned hunt veterans! Some sensory ideas are included below.\nScavenger Hunt List of Nature Ideas\nBefore heading out to your hunt location, do a little bit of research to find out exactly what items you’re likely to find there (and, of course, not find there). Below, we’ve drawn up a collection of items you might want to add to your scavenger hunt list.\n- Pine tree\n- Seeds or seed pod\n- Exposed tree roots\n- Dead tree\n- Eroded soil\n- Smooth/shiny rock\n- Rabbit hole\n- Dark or light green leaf\n- Small pebble\n- Insects on a tree\n- Deer tracks\n- Animal hole in the ground\n- Unusual shaped leaf\n- Rocks with many colors\n- Different shades of green or brown leaves\n- Dew on a flower or leaf\n- Fungus on a tree\n- Knot in a tree\n- Poison ivy (be careful!)\n- Tree with blossoms\n- Hole in a tree\n- Animal tracks\n- Grain of sand\n- Evidence of the presence of animals (tracks, scat, burrowing)\n- Evidence of the presence of people (footprints, trash, tire marks)\n- Y-shaped twig\n- Something spiky\n- Pine needles\n- Acorn or other nuts\n- Wild Flowers\n- Docking leaves\n- Stream or creek\n- Blade of grass\n- Clover leaf\n- Pond or pool in a creek\n- Butterfly or moth\n- Bird’s nest\n- Leaf with insect holes\n- Leaf with insect eggs\nPro Tip: Adhering to LNT\nConsider using a camera to take pictures of the items on your lists. Not only does this prove your ‘success’ to other teams, but it lets you collect memories of every kids scavenger hunt while still adhering to the LNT principals.\n- Draw a picture of a tree, flower, plant, insect, animal\n- Take a leaf rubbing\n- Quiz: answer questions about items on your list playing ‘What am I?’ (i.e. “I’m small and shiny. I’m very slow and I live in a shell. My name rhymes with ‘tail’. What am I?”)\n- Write a story about your scavenge/hike\n- Hang from a branch\n- Play ‘Guess the Object’: place 10 or so items in a (non-transparent) bag and have the children identify them by touch alone (we’d recommend excusing the lizards, butterflies, and beetles from participation in this one!)\n- Skip a rock on a pond/creek/lake\n- Hike to the top of a hill\n- Build a bird’s nest\nFor Older Children\n- Start a fire without matches (ideal if you need a break and want to keep them busy for a while!)\n- Cook lunch (see above!) without utensils\n- Catch a fish/tadpole/fly/butterfly/bee\n- Build a shelter\n- Find the coin: test map-and-compass skills by hiding a coin or prize somewhere and having teams or individuals navigate to it\n- Find edible plants, berries, and nuts\n- Animals feeding\n- Lightning Bugs\n- Reflection in the water\n- Trail markers\n- Other hikers\n- Animal homes or nests\n- Something unusual\n- Something scary\n- Sunlight coming through trees\n- Sunrise or sunset\n- Squirrel climbing a tree\n- Ant carrying something\n- Wind blowing leaves\n- Fish jumping\n- Shooting star\n- Clouds passing\n- Something funny\n- Falling leaf/leaves\n- Spider in its web\n- Insect trapped in spiderweb\n- Stars in the sky\n- Prickly plant\n- Wet mud\n- Rough leaf\n- Smooth leaf\n- Slimy stone\n- Tree bark\n- Grass between your toes\n- Rotten wood\n- Wind blowing on face\n- Different rock textures\n- Leaves crunching under your feet\n- Cricket’s croaking\n- Water running in a stream/river\n- A stone plopping into water\n- A creaking branch/tree\n- Wind in the trees\n- A bee buzzing\n- Birds singing/chirping\n- Noises in the woods\n- Fresh air\n- Cedar tree\n- Pine tree\n- Spruce or fir tree\n- Different leaves\n- Wild garlic\n- Find different types of rock\n- Find different animal tracks\n- Find different types of plant, tree, flower, leaf\n- Identify different types of bird\nCollection of Nature Scavenger Hunt Printables\nJust in case all the above planning sounds like too much work, below we’ve included a selection of ready-to-go, pre-made printable scavenger hunt printables for various ages.\nFind Items For Younger Children Free Printable\nFind Items For Older Children Free Printable\nAlphabet For Older Children Free Printable\nSenses For Not so Little-uns Free Printable\nFeel free to post any of our scavenger hunts on your blog, we only ask that you please link back to this post (not the pdf) when you do!","by Dave (the turtle) Wilkes\nIn a previous article I explained how to get started Geocaching and provided some links to sites useful to new/potential cachers. In this article I will describe the process of creating your own cache.\nSoon after starting to geocache, many cachers get the idea that it would be fun to have one or more of their own caches. And they would not be wrong. Creating and maintaining a cache is a little more involved than simply going out and finding the ones that others have created, but it can also be a lot of fun.\nSo let’s say you have been caching for a while, enjoy it, and want to start your own cache. To start you have to answer two questions: Where & What.\nWhere? Where are you going to hide your cache? This is an important question. You need to find a spot that is accessible and not on private land (unless you get permission from the owner). Next, is it accessible to other cachers while unlikely to be accidentally discovered by “Geomuggles” (Geomuggle – non-geocaching people)? Caches that are accidentally found are often vandalized, so this is to be avoided. Is it a safe location? It is important that you consider the cacher’s safety when determining the location of your cache. For example if you are in an area prone to things like snakes, putting the container where people need to reach blindly into a hole may not be advisable. Next, think about weather conditions. Ideally the cache will be in this location for years. Is it likely to be washed away in spring floods? Could this location be (or become) the home of some bird or animal? Next, will it be a problem for you to periodically visit this location? It is the responsibility of the cache owner to maintain the cache, replacing items (log, container, etc) as necessary, and making sure it has not been “Muggled” (see Geomuggle above).\nOk, so you found the ideal location. Now what kind of cache will it be and what container will you use? The first and foremost thing to consider is the container it will be in. In most cases it will be exposed to weather, and so needs to be able to withstand the expected (and maybe unexpected) weather conditions for at least a few years. Military ammo boxes are a very good choice, but they can be a bit hard to come by and/or pricy. Sealable food containers (e.g. Tupperware) can be good, but they are not normally made for outdoors and so can degrade quickly. The disposable food containers are not a good choice as they tend to degrade quite quickly. A popular choice is the little ‘hide a key’ containers. Making your own container is also a good option (we built a fake birdhouse out of that synthetic decking material; it should outlast the tree it is hung on). The size of the container depends on its location and the intent of the cache. Some caches are very tiny, containing nothing more than a small rolled up piece of paper as a log (these are commonly called ‘micro caches’). If you intend your cache to hold items for trading, then obviously it must be bigger. It is also important to clearly mark your cache as a Geocache. There are preprinted labels available, or you can do something custom. Labeling the cache helps people identify it as a cache and not some random piece of trash, or worse, something dangerous or illegal, so it may be less likely to be vandalized or removed.\nNow that you have the ideal location, and an appropriate container…you need to put something in it. You need to start with an explanation of what it is. When found by non-cachers this will explain what it is and hopefully prevent them from removing or destroying it, and maybe even get them involved in geocaching! The next thing is some sort of log. This is where geocachers can log their find and maybe even leave little notes about their find. This can be anything from a folded or rolled up piece of paper, to a bound book. Finally, if you want to ‘prime’ your cache with things to trade it can be a good idea. Some folks even start their cache out with a bit of a prize for the first person who finds it. Something to remember is to make sure you only put appropriate items in the cache. No food, nothing perishable, nothing dangerous or illegal, and you should not use the cache as an advertisement for a business.\nThe next step is to register your cache on one of the geocaching web sites. The process probably varies depending on the site (I am only familiar with Geocaching.com) but at a minimum they will all need the exact location of the cache in latitude/longitude, a name for the cache, and a description. Note; it is important that you note as precise a location as possible. It can be quite frustrating to be searching for a cache, only to find out it is actually 30’ or more from where your GPS says it is. If your cache is in a location where a GPS fix is difficult (under dense trees, or in a deep narrow canyon) getting an exact lat/lon is very important. This is also true if your cache is near private property or something that could be dangerous (a cliff, abandoned well, etc). After registering your cache, it will probably need to be evaluated and verified by the folks running the site before it is officially posted.\nAt this point I should mention that some caches are intentionally easy to locate while some are devilishly hard. It is all part of the fun. Some folks like the challenge in finding caches that others have not and some delight in creating caches that many look for but few find. In addition to this, there are also caches that are designed to be puzzles. Some have multiple locations that you need to find in a specific order with each leading you to the next, and some require you to obtain some particular information before you will be able to locate the cache (e.g. “go to [some location] and use the red numbers on the side of the building to fill in the missing digits of the cache lat/lon”). I will not go into the specifics of these more ‘advanced’ caches in this article, but leave it up to the reader, if interested, to investigate the details regarding what is allowed and how to go about it.\nFinally, after you have established your cache and folks are finding it (or at least looking), it is expected that you maintain your caches. Most cachers will assist by replacing plastic bags that get ripped and sometimes even replace damaged or full logs (I carry extra zip-loc bags and small log books just for that reason). Nevertheless, it is the owner’s responsibility to maintain the cache. If cachers report that there is something wrong with the cache or if it appears to be missing, it is expected that the owner corrects the problem or posts the cache as ‘inactive’.\nSomething I forgot to mention in my previous article is that Geocaching can be a very social activity. All of the Geocachers and cache owners I have met have been very friendly and personable folks. They love to meet other cachers as well as share stories about caching and caches. As such I have created a Geocaching topic in the 4AllOutdoors.org forum section where I welcome questions, comments, and stories from cachers as well as people interested in the activity."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:1f7f8942-07e4-4c38-b42a-7050bcc9a21f>","<urn:uuid:26b026b5-cdd3-480d-a038-8baa3ec53767>"],"error":null}
{"question":"How did Bing Thom Architects and F.R. Khan approach the relationship between functionality and aesthetics in their architectural projects?","answer":"Bing Thom Architects prioritized integrating different uses within single structures, as demonstrated in Surrey Central City where they combined a shopping mall, university classrooms, and office space in ways that created practical advantages like shared parking and facilities. F.R. Khan, on the other hand, believed in letting natural structural forms define architectural expression, rather than imposing preconceived architectural designs. His approach focused on finding beauty in structural efficiency, as seen in his tubular design concept that both improved building performance and resulted in cost savings while creating distinctive architectural aesthetics.","context":["Based in Vancouver, Bing Thom Architects (BTA) is a firm of visionary architects and planners, who share a fundamental belief in the transformative power of great architecture to uplift, not only the physical, but also the economic and social conditions of a community. Their belief in this power has become the grounding philosophy for the office and has resulted in memorable architecture that consistently taps into something beyond aesthetics.\nOne project that exemplifies their adept skill is Surrey Central City, a mixed-use building that has spurred renewal for a community that had been neglected for decades. Surrey Central City is a project born of Surrey’s complex history as an edge city south of Vancouver, whose sprawl and lack of planning left it without a perceived centre.\nA regional transportation system called SkyTrain was built in the 1980s in order to link Surrey, as well as other suburban cities, in Metro Vancouver. Once the system was built, a series of planners were brought in to develop a plan for a town centre, but none of their visions took hold or shape. Plagued by high crime and challenging demographics, the area became a victim of halted proposals and failed plans.\nBing Thom Architects believed that a large mixed-use development with a significant public sector component could kick start Surrey’s missing downtown. The firm was able to broker a deal between the city and provincial governments, whereby in exchange for the city donating land in the designated city centre, the province would agree to locate the new university there.\nAt the time, BTA were also working for a large insurance company that was looking for real estate investment opportunities. BTA suggested that they option the shopping centre near the planned site for the university. The designated area of the city, known as Whalley, was arguably the bleakest part of Surrey, despite being on the transit line. Though the project was complex and could have been a tough sell, the client recognized the value of creating a city centre. They ultimately bought the shopping centre and, at the request of the province, agreed to act as the developer for both the university and a new office building.\nWith a 680,000 sq ft shopping mall receiving more than 1,400 visitors an hour, a 450,000 sq ft university that would introduce 5,000 students to the area, and an additional 2,500 employees in a new 500,000 sq ft office building, BTA decided to integrate the different uses as closely as possible. The firm took the roof off the existing shopping centre and placed the university’s classrooms above it.\nThough seemingly strange bedfellows, this disparate partnership brought many advantages. Above shoppers is the hum of activity of students changing classes every hour. The presence of the shopping centre’s food court removed the need for a student cafeteria, and similarly, instead of building their own athletic facilities, the university provides each student with membership to the city’s nearby recreation centre.\nHours of peak parking demand for the shopping mall are opposite those of school parking (evening, weekends and holidays), so parking requirements are substantially reduced. A series of atria were created to organise the various uses. Heavy timber construction — a technique historically associated with British Columbia — was used in a contemporary way to celebrate the technological focus of the university and give distinctive common identity to all the spaces. In addition, BTA created a civic plaza, the first truly urban, civic open space in Surrey. They also aligned the plaza with the mall entrance that faced SkyTrain, and it has since become the de facto entrance to the university and the complex as a whole, with thousands of commuters crossing it each day.\nSix years later, the project has accomplished its mission to create a vibrant city centre. The new campus of Simon Fraser University is very popular, and students enjoy being integrated into the community. A testament to this fact is, even though this campus is newer and has fewer resources, its waiting list is longer than the main campus of Simon Fraser University. Students actually require higher grades to gain access to this new campus. In addition, there has been a resurgence in adjacent high density development including several condominium towers being completed.\nThe downtown Surrey landscape has clearly been established. Surrey Central City is a wonderful example of the power of good architecture to transform. Not only was the client able to sell the project at a very considerable profit, but the building has changed the trajectory of the city by giving the community a new sense of confidence and pride. The city has actually changed its logo from a traditional crest to a profile of the building, and will be investing a further $500 million in Surrey City Centre, which will include a new city hall and office building, additional space for Simon Fraser University, a central library and a major civic plaza that will become the heart of Surrey’s outdoor events, celebrations and festivities.\nBuilding on an updated city centre plan prepared by BTA in 2008, new civic construction will include a Performing Arts Centre with a 1,600-seat theatre and a 250-seat studio theatre, as well as a new Central Library (designed by BTA and now under construction) and a mixed use building possibly accommodating hotel and office facilities.\nThe City believes its investment will build on the earlier success of Central City and will serve as a further catalyst to attract both public and private sector investment in additional transit improvements; the addition of thousands of jobs in a growing office and commercial core; a growing tourism industry; the presence of thousands of new residents in a high density residential neighbourhood; the creation of walk-able streets through thoughtful design guidelines and development practices and the investment in infrastructure to complete, tame and beautify the road network.\nThe urban lifestyle will be supported by the addition of entertainment and cultural facilities including performing arts venues, galleries, sports arenas, community centres and additional retail, restaurant and entertainment venues. It will be a sustainable, complete neighbourhood where people can live, work and play in an environmentally responsible community.\nClient Brief: BTA’s original client for the project, the Insurance Corporation of British Columbia (ICBC), wanted a regional headquarters with facilities that could sustain changes in technological advancement, as well as provide revenue from other tenants. BTA suggested they build in a blighted area of Surrey that would benefit from the investment, and was also the future home of a new university, an existing shopping centre and adjacent access to public transportation. The client agreed, and acted as developer of Surrey Central City to create a downtown Surrey; with space for the university, additional retail and public space.","Lest We Forget |\nF R Khan -- An architect with a difference\nSyed Ashraf Ali\nA prophet is not honoured in his own country\" -- so goes the proverb. It is painfully true in case of Dr. Fazlur Rahman Khan, the legendary Bangladesh-born structural engineer. His achievements are hailed by men of science all over the world, obituary references to him were made even in leading magazines like Time and Newsweek (perhaps the only Bangladeshi non-political personality to be so honoured), he was acclaimed the 'Construction Man of the Year' and accorded Alfred E. Lindau Award (considered to be the most precious award in the world of architecture), he headed the prestigious Council on Tall Buildings and Urban Habitat for years (till the end of his days), and yet very few in our country are aware of his monumental contribution, world-wide fame and recognition. No organisation worth its name in Bangladesh even bother to pay tribute to this great son of the soil even on his birth or death anniversary. It very cruelly reminds us of Allama Iqbal's memorable utterance : Izzat use mili jo watanse nikal gaya,\nWoh ful dalimey chara jo chaman se nikal gaya\nFortune smiles on him who leaves the motherland, The flower that leaves the garden glimmers in the garland.\nF.R. Khan was an outstanding civil engineer who was hailed all over the globe for innovations in high-rise building construction, especially tubular design. He earned international fame for inventing the \"bundled tube\" system, a structural network consisting of narrow cylinders clustered together to form a thick tower, which minimised the amount of structural steel needed for high towers and eliminated the need for internal wind bracing (since the perimeter columns carry wind loadings). He was born in Dhaka on April 3, 1929 and obtained his Bachelor of Engineering degree from Shibpur Engineering College, University of Calcutta in 1950 at the top of his class. He worked as assistant engineer for the India Highway Department, and then taught at the University of Dhaka. Qualifying for a Fulbright scholarship in 1952, he enrolled at the University of Illinois in Urbana, where he completed enough credits for two Master of Science degrees, one in applied mechanics and the other in structural engineering. He obtained a doctorate in the latter and accepted an engineering position in Skidmore, Owings and Merrill, a leading and world-renowned architectural firm in Chicago.\nHe returned briefly to his native country (then East Pakistan) and won an important position as Executive Engineer of the Karachi Development Authority. After serving Karachi Development Authority for more than three years (between 1957 and 1960), he came to the painful realisation that the environment in the then Pakistan was in no way congenial to the blossoming of a budding creative genius like him. Although he loved the motherland with all his heart, he was pained to find that the administrative demands in the Karachi Development Authority kept him from design works. He found no way out but to return to the United States where his talent and creativity would have ample opportunities to blossom in their full majesty and splendour. In 1960, he joined Skidmore, Owings and Merrill once again and remained associated with it till his last breath. He shuffled off the mortal coil while on a job-site visit to Jeddah on March 27, 1982.\nIt was during the early 1960s that he laid the groundwork for his later successes in the field of high-rise buildings. This was a time when intense urbanisation was bringing in its wake a new wave of high-rise buildings. His 1964 ASCE paper on shear-wall-frame interaction was a milestone in the development of economical high-rise buildings in both concrete and steel. With the methodology developed in this paper, the stiffness of frame buildings could be increased several times, without an increase in cost.\nIn the same period, Dr. Khan also initiated the tubular design concept, with its first application in the 43-storey reinforced concrete Chestnut-Dewitt apartment building in Chicago in 1963.\nThe next innovation, pushing still further the economically feasible height of multistorey buildings, was the application of shear-wall-frame inter-action principles to tubular structures, creating the tube-in-concept (a pharase coined by Khan), applied first to the Brunswick Building in Chicago. This concept was soon applied to many other structures, including the 52-storey One Shell Plaza, in Houston, which was the tallest reinforced concrete building in the world at the time of its completion.\nAlso in the 60s, came Khan's first steel version of the tubular structure : the diagonally braced, 100-storey John Hancock Building in Chicago. It became another milestone, particularly due to the strong expression of its dominant structural feature in the architectural facade of the building.\nThen came the Sears Roebuck Tower in Chicago in 1974, using a further innovation -- bundling nine tubes into a single structural system -- with 110 stories and 1450ft. height. It was the world's tallest building in the 1980s. Like the John Hancock, it used about half the steel needed for a conventional tubular design.\nMore innovations followed under his direction, including composite buildings, combining the advantages of the rigidity of a concrete tubular structure and the speed of erection of steel slab systems and interior columns.\nA principal feature of Khan's work was to make highly efficient exterior tubular configurations to carry the lateral loads imposed on multistory buildings, rather than assigning this role to less efficient interior frames which clutter the rentable space, as had been common. The innovations introduced by Khan not only improved the rigidity of tall buildings, resulting in their superior performance, but also resulted in substantial economies over the cost of buildings designed, using traditional schemes.\nDr. Khan's startling innovations did not, however, go unchallenged. Skeptics and high-brows in many a circle criticised his innovative theories and questioned their feasibility. But the economy and effectiveness of his massive structures (among the world's tallest) silenced the critics once and for all. As a result, most of the ultra-high buildings today are built on principles introduced by him.\nF.R. Khan became the master builder of tall structure of the 60s and 70s. His buildings provided an economic answer to the needs of the day, utilising not only advanced technology but the art of engineering as well. Dr. Khan indeed was an architect with a difference. He believed that there is beauty and simplicity in the structural form of a building that is natural to it from an engineering point of view. Instead of going for a preconceived architectural expression, he let the natural structural form be the architectural representation of a building. He was encouraged and supported in this bold effort by Bruce Graham, a dominant architectural figure at Skidmore, Owings and Merrill. Khan and Graham jointly shaped the new skyline of many of the world's larger cities.\nKhan's influence on the architecture of high-rise buildings was acknowledged by Skidmore, Owings and Merrill with his admission, in the late 1960s, as a general partner in a firm that had heretofore only architect partners. Fazlur was later instrumental in the elevation of other structural and mechanical engineers to the status of partner.\nIn the non-high-rise category as well, a number of very remarkable projects were designed by Dr. Khan. Of these mention may be made of the suspension roof of the Baxter Laboratories buildings near Deerfield, Illinois, the Hajj Terminal of the King Abdul Aziz Airport (fabric suspension roof) in Jeddah, which covers an area of 105 acres; the fabric suspension roof of the Humphrey Memorial in Minneapolis; the University in Makkah; the US Air Force Academy in Colorado Springs; and the engineering designs for the solar telescope at Kitt Peak, Arizona.\nThe honours received by Fazlur Rahman Khan during his chequered life are too numerous to be mentioned here. In 1972, he was proclaimed \"Construction's Man of the Year\" by the Engineering News Record for his many accomplishments in the field of ultra high-rise buildings. In 1973, he was the recipient of the Alfred E Lindau Award for his \"outstanding contributions in advancing the art of reinforced concrete construction in high buildings\". He also received the Wason Medal for Most Meritorious Paper in 1971 for his publication, co-authored with Mark Fintel, on \"Shock-absorbing Soft Storey Concept for Multistorey Earthquake Structures.\" He was honoured with the coveted and prestigious post of the Chairman of the Council on Tall Buildings and Urban Habitat right from its inception until his death. He was also a member of Committees 118, Use of Computers, and 442, Response of Concrete Buildings to Lateral Forces.\nF R Khan had always been both human and humane. Unlike the average run of engineers, he never found himself confined to the dull and stereotyped environment of cut and dried formulae and techniques. He began his professional career as a structural engineer and gradually developed into an engineer architect with a keen perception of aesthetics. Furthermore, his appreciation of the mechanical and electrical aspects of building design gave him an overview of the entire construction process. Even beyond his intimate understanding of the non-structural disciplines and aesthetics and his exceptional intuitive understanding of structural behaviour. Khan had a remarkable perception of the social needs of the millions who live and work in the cities. Engineering and architecture were the media through which he sought to fulfil the needs. In a way, he was a philosopher who defined the role of the architect engineer in society. In the ultimate analysis, it was the urge to respond to human needs and aspirations that enabled Dr. Khan to make the outstanding structures and innovations that brought him recognition and honour by society and profession.\nF R Khan was also an exceptional communicator very much at ease with all groups as well as large audiences. He had the remarkable ability to articulate complex concepts in simple, understandable language. In spite of the intense demand of his busy professional life, he found time to regularly teach courses at the Illinois Institute of Technology and supervise graduate students. With his untiring activities, remarkable achievements and pleasant but commanding personality he inspired countless young engineers and set a standard for them to measure up against.\nIn 1971, when the merciless military junta launched the most heinous atrocity on the innocent unarmed civilian population in Bangladesh, Dr. Khan, in spite of his heavy commitments and preoccupations, found enough time to organise the Bangalees and their friends in the United States into a Defence League which raised enormous funds for relief works. He also organised a strong lobby in Washington for months to urge the US authority to stop shipment of arms to the junta. No wonder, he was the founder-president of both the Bangladesh Foundation, which helped the grassroots non-government development projects, and the Bangladesh Association in the United States.\nThe enormous professional success did never affect the behaviour or the way of life of this great man. He remained humble, always accessible to his associates and friends, and continued his modest way of life. Never did he change his philosophy that people are the focal point of life. \"In social contacts,\" says his friend Mark-Fintel of Portland Cement Association, \"he was interesting, entertaining, and deeply and sincerely concerned with his friends, their families, their lives, joys, and sorrows. To him, friendship meant giving of himself, and that is why he had so many friends, and no enemies.\" This philosophy always remained the strongest motivation behind his actions, whether in professional or in personal life.\nSyed Asraf Ali is former D G of Islamic Foundation Bangladesh.\nSears Tower, Chicago; inset:F R Khan"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:9744fd8c-5711-4a93-9257-dce92420ac17>","<urn:uuid:790dfb3e-2b90-4c9c-89d9-4e1d7fbca707>"],"error":null}
{"question":"As a hotel manager interested in historical properties, I'm curious: what famous houses have been preserved as museums, and what eco-friendly measures could be implemented to maintain such heritage buildings?","answer":"Several famous houses have been preserved as museums, including Mark Twain's Gothic Revival home in Connecticut, Shakespeare's 16th century birthplace in Stratford-upon-Avon, and Dracula's Castle in Romania, which now houses art and period furniture. To maintain such heritage buildings sustainably, they can implement various eco-measures: installing energy management systems, using daylight where possible, conducting regular maintenance checks for leaks and equipment efficiency, switching to LED lighting, and implementing green purchasing policies for cleaning supplies and materials.","context":["There are some houses that have become so famous and iconic that we know instantly where they are and who lives or lived there. These are homes that are special for some reason, whether lived in by the rich and famous or notable for their role in the power of a country. Here we look at 10 such properties and why they are instantly recognizable.\nThe home of Elvis Presley in Tennessee has become almost as famous as the King of Rock ‘n’ Roll. This house is one of the world’s most visited attractions. It sits on a 13.8-acre estate in Memphis which also includes the world’s largest Elvis Presley museum and a luxury resort hotel.\nThe White House\nEvery leader of the United States has lived here at 1600 Pennsylvania Avenue since the second president. This impressive home was designed by James Hoban and took 8 years to construct between 1793 and 1801. Could there be a more iconic residence? To keep it looking whiter than white takes 570 gallons of white paint!\nJust like Graceland, Michael Jackson’s estate in California is equally as famous as the pop singer’s biggest hits. The huge estate contains a zoo, two railroads and a personal amusement park. Since Jackson’s death, it has become rather like a ghost town but his daughter has pledged to renovate it back to its former glory.\nSan Francisco’s Painted Ladies\nThese beauties are among the most photographed sites in San Francisco. A row of Victorian homes, brightly painted and ascending one of the city’s famous inclines. This style of home is called a painted lady if it has been painted in two or more colours that really show off its architectural details. There are many such houses in the city but this particular row is stunning and with the backdrop of skyscrapers, makes quite a contrast.\nSaid to be the inspiration for Jane Austen’s Pride & Prejudice, this glorious country home in Derbyshire, England sits in an enviable location. Set against a backdrop of rocky hills and thick woodland, the inside isn’t too shabby either! Antique paintings hang from the walls, beautiful period furniture adorns the rooms and not to mention the sculptures and libraries. It’s been used many times in TV and movies and is the UK’s favorite country home.\nHome of Mark Twain\nThis classic home in Connecticut was where the author lived with his wife when he published The Adventures of Tom Sawyer. It was almost demolished many years ago but was thankfully rescued by a friend of the family. The Gothic Revival home has been fully restored to its former glory as Mark Twain would have known it.\nA beautifully restored 16th century house in the center of Stratford-upon-Avon, England is said to be the birthplace of the world-famous playwright. The Shakespeare family home was where the young writer grew up and you can tread the same boards as the home is a museum open to the public. There are tours, actors providing live entertainment and many artefacts from Shakespeare’s time on display.\nThe Playboy Mansion\nHome of the founder of Playboy magazine, Hugh Hefner, this posh pad is instantly recognizable and highly renowned. Located in Los Angeles and famous for its epic parties, the mansion has 29 rooms, was built in 1927 and is now mainly used for charity or civic functions. It was constructed in a Gothic/Tudor style and is valued at more than $50 million.\nLocated in Romania, this castle is said to be the inspiration behind Bram Stoker’s ‘Dracula’ and has historical links to the terrifying Vlad the Impaler. It is considered a national monument in Romania and sits on the border with Transylvania. It has become known as Dracula’s Castle but this medieval landmark is now home to a museum for art and period furniture.\nThis iconic and world famous residence has been the official home to the sovereigns of the UK since 1837. Sitting grandly in Westminster, London, it is the main focus of state occasions and visits to the Queen. It was designed in a neo-classical style by a number of different architects and contains 775 rooms, both State and private. It is a grand building fit for a Head of State but is still very much home for members of the royal family. The balcony is one of the most famous in the world with the first official Royal appearance on it back in 1851 when Queen Victoria stepped out during celebrations.","Go Green Hotels: Green Ideas for Hotels and Resorts\nThis page lists sustainable ideas for green hotels and resorts. Eco hotels can keep up to date on green lodging news at Green Lodging News, Eco Green Hotel and Green Biz (tourism & hospitality). If you would like to encourage hotels you stay at to implement these ideas, consider inviting them to visit this page. (www.globalstewards.org/hotel.htm) through their hotel comment card. Find green hotels through TripAdvisor (when searching, select 'Green' from the 'Style' menu option) and the Green Hotels Association.\nBack to Top\n- Create a 'green team' at your hotel with the goal of continual improvement and scheduled re-evaluation and reporting. Apply for a green hotel certification (see also green building programs by country). Connect with other hotels and organizations to share best practices. See resources for ideas.\n- Create an incentive program to encourage your staff to participate in and improve upon environmentally-friendly practices.\n- Educate your staff to:\n- Turn off lights and turn down heating/air conditioning in unoccupied rooms or employee-only areas\n- Close/open drapes to reduce the need for heating/air conditioning\n- Continually check for and respond to leaking faucets and toilets\n- Continually check for and power down unused hotel equipment (i.e., kitchen exhaust fans) that have been left running\n- Report opportunities to reduce resource consumption\nBack to Top\nWater and Energy\n- Conduct or schedule a water audit.\n- Start a linen (both towels and sheets) reuse program in all guest rooms. One example: Project Planet Program.\n- To reduce water use, consider rainwater harvesting and/or a greywater system.\n- Switch to drought resistant native plants in garden areas and switch to WaterSense Landscape Irrigation Controllers. Replace mowed landscaping with native ground cover (also Native Gardening and Invasive Plant Guide)\n- Install low-flow showerheads (.5 to 2 gpm) and sink aerators (.25 gpm to .5 gpm for hand and face washing and 2.2 gpm for dish washing).\n- Switch to low flow or dual flush toilets/urinals or install toilet-tank fill diverters. Also, stay on top of leakage from the toilet flappers (the #1 source of leaks - a leaking or poorly fitting flapper can waste up to 200 gallons of water a day and may cost hundreds of dollars a year). Flappers typically start leaking within 1-2 years so schedule regular checks for leaks. Add a sign to your public bathrooms letting people know how to report leaks.\n- To reduce operational costs, water and energy consider installing an ozone laundry system.\n- Switch to Mercury-Free Flourescent/CFLs (called ESL (electron stimulated luminescence) lamps) or, where appropriate, LED light bulbs in guestrooms, lobbies, and hallways. With flourescent tube lamps, replacing outdated T12 models (with magnetic ballasts) with newer T5 or T8 models (with electronic ballasts) reduces energy use and improves light quality.\n- Use occupancy sensors and/or timers for areas of your hotel that are less frequently used such as hallways, outdoor areas, or public bathrooms. In some cases (i.e., staff storage areas) more energy is saved by training staff to turn off lights when a room is unoccupied.\n- If available, schedule an energy audit through your local energy provider or a local energy auditor (aka energy consultant).\n- Consult outside sources to evaluate the total system when replacing major mechanical equipment (such as chiller, water tower, etc). Often, this can lead to downsizing and other opportunities to reduce both the initial investment and operating costs.\n- Use an energy management system (EMS) to tie in air handling units, HVAC, and lighting to prevent conditioning space when it is not necessary.\n- Replace or modify HVAC heating and air conditioning units (see also guide for engineers) to increase energy efficiency. Consider using heat pumps or other geothermal technologies.\n- Replace exit signs with Energy Star exit signs (i.e., Light Emitting Diode (LED)).\n- Conduct an audit of equipment that uses \"standby power\" (the energy used while an appliance is switched off or not performing - a usage monitor can show standby watts) - plug equipment into bye bye standby or smart meters so that they are powered down completely when turned off.\n- Use daylight exclusively in your lobby, bar, and restaurant for as much of the day as possible. Consider installing Energy Star skylights if needed.\n- Purchase Energy Star appliances wherever possible (Energy Star for Hospitality provides detailed information about energy saving appliances and monitoring systems). Replace old washing machines with both water and energy conserving models.\n- If vending machines are used, learn about opportunities to reduce energy use.\n- Install window film to lower heating and cooling loads and reduce glare in guestrooms.\n- If the hotel has a pool and/or hot tub, install a solar water heating system and use pool and hot tub covers when the pool area is closed.\n- For roofs, use recommended levels of insulation or radiant barriers. Learn about other options at energy.gov(also see Cool Roof Rating Council). Consider switching to a green roof.\nBack to Top\n- Provide guestroom recycler baskets for newspaper, white paper, glass, aluminum, cardboard, and plastic.\n- Provide recycling bins both in public areas (i.e., poolside), in the kitchen, and in the back office (including one at each desk) to make recycling as easy as possible.\nBack to Top\n- Include filter changes, refrigerator coil cleaning, thermostat calibration, water leak checks, and damper adjustments in your ongoing maintenance plan.\n- Monitor, record and post rates of energy and water use. Make repairs or replace equipment when usage changes indicate problems.\nBack to Top\n- Create a green purchasing policy for cleaners, sanitizers, paints, pesticides, office supplies, etc. throughout the hotel.\n- Buy environmentally-friendly paper (copier, toilet paper, etc.):\nUse the Paper Calculator to compare the benefits of different recycled office paper products. Additional Resource: What's in Your Paper?, information on paper certification and Model Forest Resources Policy Template\n- With high post-consumer recycled content\n- Made without the use of toxic chemicals such as chlorine or mercury\n- Certified by the Forest Stewardship Council or guaranteed to contain no fiber from endangered forests\n- Minimize the amount of paper used for each guest and in the office (i.e., reduce paper size of invoices, etc.). Print with soy-based inks.\n- Buy office and guest amenity products that contain recycled material. For company listings, access the Recycled-Content Product Directory and/or the Recycled Plastics Product Directory.\n- Buy organic, fair trade, cruelty-free guest amenity products whenever possible:\nBack to Top\nGuests and Guest Rooms\n- Come up with creative ways to reward hotel guests for being green. Great example: Crowne Plaza offers a free meal vouchers to guests who generate electricity on the gym bicycle.\n- Provide your guests with bicycles, walking maps, and information on public transportation.\n- Offer discounted rates to sustainable living/environmental organizations who would like stay at and/or hold meetings at your hotel.\n- Donate leftover guest amenities, old furniture and appliances to charities.\n- Donate used soap and shampoo to people in need through Clean the World.\n- Provide glass cups and ceramic mugs (instead of plastic) for in-room beverages. Place cups and mugs upside down on paper doilies (instead of covering opening with a plastic wrapping).\n- Whenever possible, buy food and guest amenities in bulk (i.e., use refillable hair and skin care dispensers)."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:e7babadc-5525-4ee9-bc74-4ffa7910ee52>","<urn:uuid:fd75a927-c921-4124-8c9b-71f63341cb67>"],"error":null}
{"question":"What are the primary testing challenges for underwater cables, and what specific solutions exist to test their insulation integrity?","answer":"For underwater cables, two main testing challenges exist. First, when testing autonomous underwater vehicles at depths up to 6,000 meters, water can seep through microscopic defects in wire insulation under great pressure. Second, testing requires measuring between water and wiring while maintaining electrical isolation. The solution involves using an isolation transformer to power both the tester and computer, allowing successful measurements between water and wiring to verify insulation integrity. For the MARS undersea cable system, which spans 51km at 891m depth, the cable was buried in a trench to protect it, and monitoring showed little detectable impact on seabed conditions or biological assemblages.","context":["Potential Impacts of the Monterey Accelerated Research System (MARS) Cable on the Seabed and Benthic Faunal Assemblages\n- Summary to Date\n- Monitoring Trends\n- Figs. & Images\n- Jim Barry\nMonterey Bay Aquarium Research Institute\n- Linda Kuhnz\nMonterey Bay Aquarium Research Institute\n- National Science Foundation\n- Monterey Bay Aquarium Research Institute\nThe Monterey Accelerated Research System (MARS) includes an undersea cable 51 km in length that connects the Monterey Bay Aquarium Research Institute (MBARI) to a science ‘node’ at a depth of 891 m on the continental slope just outside Monterey Bay, California (Figure 1). MARS was installed in March 2007, and the cable provides power and high data bandwidth for science instruments connected to the node via additional, thin ‘extension’ cables deployed on the seabed by remotely operated vehicles (ROVs). MARS is one of a few cabled ocean-observing systems that enable continuous, long-term science capabilities for ocean science with real-time communication, control, and data capture from offshore subsea sensor systems.\nThe cable connecting MARS to MBARI does not lay on top of the sea floor where it could be snagged by other human activities. Instead, the cable was buried in a thin trench, protecting it from becoming a hazard or becoming damaged by fishing activities. Digging the trench required an environmental impact report to determine the impact of both the trench and the presence of the cable on benthic and demersal biota.\nPrior to MARS cable installation, an environmental impact report was prepared, including characterization of seabed biological communities along the cable route and initial sampling for future environmental impact assessment (2004). This survey included characterization of the megafaunal animals (organisms identifiable in video recordings) and macrofaunal organisms (worms, crustaceans, etc., captured from sieved sediment samples) along the cable route. Subsequent to MARS cable installation, a Post-Lay Inspection and Burial (PLIB) survey of the entire route was conducted (March 16 to March 22, 2007 and June 7, 2007) and environmental impact assessments are required at ~18 month to 5-year intervals, including observations of the condition of the cable and potential effects on biological communities. This monitoring project presents data from environmental impact assessment surveys performed prior to cable installation, in 2007-2008 following cable installation, and again in 2010.\nSummary to DateInspection of the MARS cable in 2010 (Fig. 2), coupled with a sampling program to evaluate changes in geological and biological conditions on local and regional scales with respect to the installation of the cable indicate little detectable influence of the cable. The most conspicuous evidence of cable installation is the cable exposed on the seabed for a short distance where it could not be buried (Fig. 3). Analyses of the geological and biological sampling program indicate the following:\n- Over most of its length, the cable remains buried, with little evidence of change since installation (Fig. 4).\n- Changes in mean grain size were undetectable in relation to the MARS cable (Fig. 5).\nThe percent organic carbon content of sediments increased near the MARS cable at some locations, possibly due to natural variation or the effects of the cable or both (Fig. 6).\n- Local variation in benthic megafaunal communities (Fig. 7A-C) within 50-100 m of the MARS cable is minor or undetectable. The abundances of most animals observed did not differ between the area over the cable route and 50 m away. Longnose skates (Raja rhina) were significantly more abundant in one area where the MARS cable is suspended over topography (~300 m depth) in 2008. These animals may have responded to weak electromagnetic fields generated by the cable. During 2010, when the cable was energized, the numbers of R. rhina were near background levels near and distant from the cable.\n- The MARS cable has little effect on the distribution and abundance of macrofaunal (Fig. 7D-F) and megafaunal assemblages on a regional scale (e.g. kilometers). Megafauna and macrofauna compared before and after cable installation among 3 control stations and 1 cable station at each of 3 depth zones (Shelf - <200 m, Neck – 200-500 m, Slope - >500 m) indicated relatively few potential changes in benthic biological patterns due to the MARS cable (Fig. 8). Natural spatial and temporal variation in the abundance and distribution of benthic macrofauna and megafauna appears to be greater than any detectable effects of the MARS cable.\nDiscussionThe sampling program was designed to: • Observe the condition of the cable or cable trench along the cable route (51 km), • Assess the potential impacts of the MARS cable on geological characteristics and biological assemblages on a local scale (0-100 m from the cable) and a regional scale (km), using remotely operated vehicle (ROV) video transects and sediment samples.\nThe major conclusion of the study is that the MARS cable has had little detectable impact on seabed geomorphology, sediment conditions, or biological assemblages.\n- Substrate characterization\n- Habitat association\nStudy MethodsCable position and condition assessed by ROV surveys\nVideo Transects used to estimate densities of seabed organisms and objects\nSediment core sampling using ROV manipulator arms and pushcores\nFigures and ImagesImage not available at this time.\n/public/regional_images/monitoring_projects/100391_fig1.jpgImage not available at this time.\n/public/regional_images/monitoring_projects/100391_fig2.jpgImage not available at this time.\n/public/regional_images/monitoring_projects/100391_fig3.jpgImage not available at this time.\n/public/regional_images/monitoring_projects/100391_fig4.jpgImage not available at this time.\n/public/regional_images/monitoring_projects/100391_fig5.jpgImage not available at this time.\n/public/regional_images/monitoring_projects/100391_fig6.jpgImage not available at this time.\n/public/regional_images/monitoring_projects/100391_fig7.jpgImage not available at this time.","Over the last 26 years, since the founding of CAMI Research Inc., customers and friends have asked why the company would focus its efforts on such a mundane, low-tech, and uninteresting device as a cable tester, particularly since a variety of such devices already existed. On the surface, testing wiring assemblies might seem to be mundane. Instead, the incredible variety and richness of applications I describe below demonstrate that it is not.\nWhile a simple LED circuit can show continuity in a wire, the technology needed to meet increasingly complex and demanding customer quality standards is as different from an LED circuit as lightning is from a lightning bug. I first learned this lesson in 1991, when we began work on the CableEye tester.\nAt that time, the USB interface did not exist. Instead, computer peripherals—including printers, scanners, modems and industrial controls—used serial ports to communicate with computers, a technology developed in the early 1960s when teletype machines were in wide use. Initially, serial port cables used DB25 connectors and followed the RS232 standard developed by the Electronic Industries Association.\nFor reasons of cost, competitive advantage and advancing technology, very few manufacturers followed this standard exactly, and different, usually incompatible, serial cable wiring proliferated. Using a wrongly wired cable having DB25 connectors that obligingly fit most serial devices could cause device malfunction, or worse, circuit damage.\nWith mass production came molded connectors, now making it impossible to remove a cable’s backshell to see which color wire was connected to which pin. This left us with the mind-wrenching task of using an ohmmeter or beeper to buzz out a cable to ascertain suspect wiring.\nTo address these problems, our company decided to develop a computer-based product to which a serial cable could be connected and its wiring instantly displayed on the computer screen. We succeeded after two years of hard work, but it quickly became apparent that customers needed to test cables with many other connector styles, and they required measurement capability beyond simple continuity to certify connection quality and search for various assembly defects like intermittent connections.\nSince the first CableEye tester was introduced in 1993, we have continually adapted to meet these ever-changing needs. The case studies that follow show some of the unusual applications we have encountered over the years.\nPlease note that in this article, “cable” refers to any wiring assembly that might be found in an automobile or aircraft: from a simple lamp cord to a complex wire harness.\nKeeping Wiring Dry Underwater\nAutonomous underwater vehicles are small, unmanned submarines that can operate without remote human intervention. Many companies, including the Woods-Hole Oceanographic Institute, Bluefin Robotics and Boeing, make such devices of wide-ranging size for various purposes. Our customer, Hydroid in Pocasset, MA, makes deep-diving vehicles for ocean exploration at depths of up to 6,000 meters.\nDuring the manufacture of these vehicles, various hull penetrations are necessary to admit wiring for external cameras, sonar, and electronic sensors. With great pressure present at normal operating depth, water may seep through microscopic defects in wire insulation that would eventually lead the electronics to malfunction. As a result, all vehicles are tested in the factory using a large tank that can be pressurized to simulate the deep ocean. While under pressure, high-voltage cable testing can reveal insulation defects that cause conductive liquid, like seawater, to penetrate copper conductors.\nTo test the quality of electrical insulation, we normally apply voltage between two unconnected wires and look for leakage between wires in fractions of a microamp. To obtain better than 1 gigohm of isolation between two wires, we would apply 1,000 VDC between the wires and expect to see leakage of less than 1 microamp.\nThe challenge in this application is that the water itself must serve as one of the conductors so that we can detect leakage under the insulation to a single wire rather than between two wires. However, the cable tester’s electronics require that all points be electrically isolated from earth ground so that each wire can be individually energized while the other wires are not.\nA body of water, unfortunately, is, of course, always at ground potential, including a water tank attached to a grounded structure. As a result, using the water itself as a test point becomes problematic. Since we could not electrically isolate the water in the tank, we instead isolated the tester and computer from ground by powering both through an isolation transformer. This permits successful measurements between the water and the wiring to confirm that the insulation remains intact.\nMedical catheters for electrical sensing or stimulation comprise small tubes containing extremely thin wires (fractions of a millimeter) that lead from an electrical connector at the near end to evenly separated electrodes along a short length of tube at the far end. Testing such an assembly can sometimes prove quite challenging.\nFor example, simply clipping a minihook connector to each electrode would not only be awkward, but could apply enough force to the electrode to deform it. The special fixture we developed, however, allows the operator to make physical contact with the electrodes without damaging them.\nWe milled a circular groove in a 0.093-inch-thick fiberglass circuit board to about half its depth. On the other side of the board, we milled slots perpendicular to the circular groove spaced at the positions of the catheter’s electrodes to just over half the depth of the board. This exposed openings along the circular groove where the electrodes would be located.\nBy soldering-in a specially designed spring pin on the back side of the board to terminate at each opening along the groove, we made electrical contact with the catheter. A circular clamp closing from above applied uniform pressure around the catheter to ensure that all electrodes touched the spring pins. Once mounted, a test with resistance can be completed in about one second.\nSlip Rings and Signal Transmission\nSlip rings provide continuous electrical contact with rotating objects such as a constantly turning radar antenna. Newly manufactured slip rings require testing to ensure that low electrical resistance exists uniformly for the full 360-degree circle of rotation. Any discontinuities in electrical resistance could cause serious equipment malfunction, or arcing if the rings transmit power. Also, because some element of friction exists at the contact point between rotor and stator, periodic testing becomes necessary for equipment taken out of service for maintenance.\nFor this application, we modified the tester’s firmware to make repetitive four-wire Kelvin resistance measurements at the rate of 500 per second. Because the sampling process is not synchronized with the rotation, several rotation cycles are needed to ensure that we collect enough resistance samples around the circumference. The cycles also provide a high degree of confidence that the rings transmit electrical signals reliably through their full span of rotation.\nDummy Data Preserved\nCrash test dummies substitute for human subjects when automotive manufacturers intentionally stage collisions to test vehicle safety systems. Unlike humans, these anthropomorphic creations contain numerous embedded sensors, including accelerometers, load sensors, strain gauges, temperature sensors, angular rate sensors, air pressure sensors and position sensors located where body stress and injury are most likely.\nNot surprisingly, these many sensors link to data acquisition modules using cables exposed to the same forces experienced by the various sensors. Following a crash experiment, these cables must be carefully inspected and electrically tested before reuse, especially considering the expense of a destroyed test vehicle and the possible loss of data from a failed cable.\nLong Cables With High Voltage\nExtremely long cables connect computers, sensors and other equipment separated by distances of thousands of feet or more, sometimes by miles. Simply connecting long cables to a modern cable test instrument without making some adjustments to the measurement process will produce spurious data. As wires increase in length, each wire begins to look more like the plates of an elongated capacitor, particularly when a shield conductor exists.\nAutomatic cable testers, unlike ohmmeters, use a series of rapid pulses to acquire connection and resistance data. Unfortunately, the parasitic capacitance of a long cable distorts these pulses. For connectivity and resistance measurements, generally widening the test pulses\naddresses the problem by permitting the tester’s drive circuit more time to pump charge into the parasitic capacitance before reading back a response.\nThe downside of this approach is it lengthens test time. As with the autonomous submarine testing cited earlier, the cable must be isolated from earth ground, or if this is not possible, the tester and computer must be.\nHigh-voltage testing of long cables presents us with the same capacitance issue, but in this specific application, the inrush current needed to bring the cable up to the test voltage may exceed the current level that would normally signify insulation breakdown (i.e., the “trip current”). In this case, we reduce the ramp rate to gradually increase the test voltage.\nTesting long cables with high voltage also introduces a serious safety concern. Unlike the low voltage test for continuity and resistance that uses 10 VDC pulses, a hipot test may apply 500 VDC or more to the cable to test insulation properties and leakage.\nWe can certainly reduce the ramp rate while raising the voltage from 0 volts to the test voltage. However, as we do so, we pump considerable charge in the cable due to its large capacitance. If, for any reason, a person should come into contact with the far end of a long cable during the hipot test, they could receive a lethal electrical shock from the cable’s stored charge discharging through their body. Therefore, this demands special care on the part of those operators performing the test.\nConnector Panels for Concerts\nMany people enjoy the live concert venue featuring rock musicians, vocalists and flashy laser light and sound effects. Few people know about the enormous effort required to set up one of these concerts, or the large and varied number of cables necessary to make everything work. These include cables for power, lighting, microphones, speakers, video projectors and cameras, laser and special effect controls, and mixer panels.\nSeveral of our customers specialize in the rental of cables and equipment needed to support these concerts. With cables connected and disconnected for each show, walked over, rolled over by forklifts, and generally manhandled during this process, testing the interconnect cables before each show becomes critically important.\nTo this end, special large connector panels have been designed for this purpose. One such panel is portable and includes an internal CableEye test system, computer, printer and mating connectors for all cables a show technician might use.\nKeeping Moisture at Bay\nWoods Hole Oceanographic Institute, and the National Oceaniac and Atmospheric Association, among others, conduct research in the deep ocean to monitor wind speed and direction, air temperature, sunlight, surface and subsurface water temperature, and water current. Moored buoys powered by solar panels collect this data, transmit it to satellites, and remain in a fixed location by cables attached to an instrument package on the ocean floor.\nCables that connect the buoy to the instrument package not only hold the buoy in position, but also transmit power to, and exchange signals with, these instruments. The cables typically use heavy rubber insulation several inches thick to keep high-pressure seawater from penetrating to the internal copper conductors.\nA tour of duty for a buoy and its cables typically lasts one year. Because it’s very expensive to replace these cables, they are made in 50-foot sections and interconnected to obtain the desired overall length, with each section individually testable and reusable. End-users regularly rely on a hipot test to reveal if any moisture penetration has reached the copper conductors. If not, the test certifies that the cable section is ready for the next deployment."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:3a5c7d99-9288-4fd7-a5cb-561c645fcf2a>","<urn:uuid:3f960cd8-68b7-4565-b392-ca61b9c2edef>"],"error":null}
{"question":"I have pets at home and wonder about air filter maintenance - how often should I change my HVAC air filters according to different manufacturer recommendations?","answer":"The frequency of air filter changes varies depending on several factors. For standard fiberglass filters, they should be changed every 90 days. However, if you have pets and multiple people in the home, filters should be changed more frequently - about once every month. More efficient filters like pleated filters or high-efficiency particulate air filters require more frequent changes. The specific schedule depends on factors such as how often your equipment runs, the number of pets and people in the home, and the particular design of the filter.","context":["Are you wondering about the basics of HVAC systems? Do you want to know how to maintain yours or what setting to put it on? If so, you’ve come to the right place. In this article, we’ll answer some frequently asked questions about HVAC systems to help you out. Keep reading to learn more.\nShould the fan be set to auto or on?\nWhen it comes to using a thermostat, there is a wide variety of settings available to choose from. One of the most popular and beneficial settings is the fan setting. This setting can be used to help regulate indoor temperature and air quality, making it an important element of any home. So, should fan be on auto or on for heat? In most cases, it’s best to keep the fan set on auto. This allows the fan to run only when it’s necessary to keep the temperature within the desired range. This also helps to conserve energy and prevents the fan from constantly running, which can be noisy and inefficient.\nWhen the fan is set to on, it will constantly run regardless of the temperature. This can lead to an increase in energy consumption and noise levels. This can also result in temperature fluctuations as the fan is constantly running and not just when it’s necessary to keep the temperature in the desired range.\nHow often should you change air filters?\nYour HVAC air filter is responsible for catching and trapping dirt, dust, pet dander, and other airborne particles. If not changed frequently, your air filter could become clogged and dirty. The frequency of air filter changes depends on several factors, such as the type of filter you are using, the size of the HVAC system, and the environment in which it is situated. Generally, most standard fiberglass filters should be changed every 90 days, depending on the environment. More efficient filters, such as pleated filters or high-efficiency particulate air filters, should be changed more frequently. Clean air filters will ensure that your HVAC system runs as efficiently as possible and that you have quality indoor air.\nIs seasonal HVAC maintenance important?\nSeasonal HVAC maintenance is an important part of ensuring your system is running at peak efficiency. It is important to have your system regularly serviced to keep it running smoothly and ensure that it is still working as it should. Regular maintenance can help to prevent costly and unexpected repairs, as well as increase the longevity of your equipment.\nSeasonal HVAC maintenance performed by a professional technician can include tasks such as checking for any signs of damage or wear and tear, lubricating moving parts, and cleaning system components. This can help to ensure that your system is running as efficiently as possible and can help to maintain its performance. Additionally, having your system maintained before the heating and cooling seasons can help to prevent any issues due to changing temperatures.\nHow can you improve the energy efficiency of your system?\nImproving the energy efficiency of your HVAC system can help reduce your monthly energy bills and help protect the environment. Given that your air conditioning can account for as much as 12 percent of your annual energy expenditures, it’s imperative that you ensure energy efficiency. One of the most important steps is making sure your system is the correct size for your home and the climate you live in. An oversized system will cause your energy bills to increase because it will be running more than necessary.\nAnother way to improve the energy efficiency of your system is to invest in a programmable thermostat, which will allow you to set temperature settings that automatically adjust according to the time of day. Finally, you should consider upgrading your system with high-efficiency components such as a more efficient compressor and fan. These components will help your system run more efficiently and reduce your energy bills over time.\nYour HVAC system is essential for the comfort and well-being of your home. Understanding how to use your equipment and maximize efficiency will ensure that your home stays comfortable all year long.","Frequently Asked Questions\nQ. How often should air filters be cleaned or replaced?\nA. Generally, they should be replaced or cleaned once every month depending on how often your equipment runs, how many pets and people are in the home, and the particular design of the filter.Many new systems are equipped with permanent filters. These should be cleaned according to the instructions supplied by the manufacturer or the contractor that installed the original equipment. When replacing disposable filters, match the type and size of the original filter.\nTip: Every time you pay your electric bill, change your\nQ. If an outdoor unit needs replacing, should the indoor unit be replaced, too?\nA. YES, It is not only a good idea to replace the entire system when an air handler or condenser needs to be replaced, ITS THE LAW in NC! The efficiency rating is based on the entire system. To gain the maximum benefit of new, highly efficient technology, the entire system should be replaced together. This ensures the system is reliable and achieving its true efficiency rating. (AHRI Matched)\nQ. How do you know what size system a house needs?\nA. There are many variables to be considered before determining which system is best for you, such as size of the house, climate at the location, the number and type of windows, insulation of the house, number of people in the house, etc. A quality contractor will consider all factors, by performing heat and cooling loads, based on The Manual J and obtaining energy use calculations for any home before making any size recommendations.\nQ. What do SEER rating numbers mean?\nA. To help consumers make informed choices, the U.S. government requires an efficiency rating of all air conditioning and heating equipment. The rating is meant to reflect the percentage of energy used efficiently. A high rating indicates high efficiency. There are various names for the efficiency ratings of varying types of equipment. Air conditioning equipment is rated by the Seasonal Energy Efficiency Rating, or SEER. Heat pump equipment is rated by the Heating Seasonal Performance Factor, or HSPF. Gas furnaces are rated according to their Annual Fuel Utilization Efficiency, or AFUE.\nTo learn more about efficiency ratings on heating and air conditioning equipment in your home or business, please visit the U.S. Department of Energy's Energy Efficiency and Renewable Energy Consumer's Guide website.\nQ. Should outdoor units be covered in winter?\nA. Air conditioners that operate seasonally are built to withstand an outdoor environment. There is no advantage to covering an air conditioning unit during off seasons. In fact, rain helps to keep the unit clean.\nQ. Should a thermostat be set to “auto” or “on”?\nA. When the thermostat is set to “auto,” the fan operates only when the temperature requires it. This is the most used setting. However, there are advantages to using the “on” setting. First of all, the air in the house is constantly filtered through the unit’s air filter. Secondly, the constantly circulating air results in an even temperature throughout the house. However, the \"on\" position will result in higher humidity since air is being blown over wet coils after the condenser shuts off.\nQ. Can shrubs and flowers be planted around an outdoor unit?\nA. Yes. However, we recommend that plants be no closer than 18 inches from the unit. This allows plenty of room for air circulation in and out of the unit. Without this room for air circulation, the unit could overheat, resulting in a premature need for service."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:2f52eaf9-b1fc-4228-8412-b85aecba6283>","<urn:uuid:61ad1c13-529a-4d1d-b778-1c660c387f3a>"],"error":null}
{"question":"¿Es verdad que both lightning strikes on planes and deforestation tienen serious environmental consequences? Me interesa saber los specific effects.","answer":"Yes, both phenomena have serious consequences. Lightning strikes on planes can cause immediate physical effects like loud bangs, white flashes, burning smells, and force planes to divert (as seen in the Air New Zealand flights that had to land in Christchurch). Regarding deforestation, it causes severe environmental impacts including species extinctions (137 species become extinct daily), soil erosion, climate change, desertification, pollution, and habitat fragmentation. At current rates, rainforests could vanish within 100 years, leading to unknown effects on global climate and eliminating most plant and animal species.","context":["About 10 minutes before landing at Queenstown Airport there was a loud bang, a flash of white light, and for a few seconds Edward Guy thought it was all over.\n\"I thought the engine had blown up and fallen off,\" said Guy, a passenger on one of two Air New Zealand flights struck by lightning just out of Queenstown yesterday.\nThere was almost 3500 lightning strikes around the country yesterday, with most in the South Island on the West Coast.\nAs flight NZ621 on route from Auckland started to descend, the plane first struck some heavy turbulence.\n\"As we came through the thick clouds there was some pretty heavy turbulence,\" Guy said.\n\"Passengers were throwing up.\"\nThen about 10 minutes from the airport the plane was struck by lightning.\n\"There was this intense white light and a bang at the same time. I think it hit somewhere around my feet. I was sitting right next to the engine and thought it had died and that was it.\n\"The feeling only lasted a few seconds though as I turned and saw the engine was still intact and the plane was flying normally.\"\nHowever, as the plane descended towards the airport it next encountered strong side winds.\n\"We must have got to about 300m before they decided to abort the landing.\"\nThe plane rerouted and landed at Christchurch Airport, much to the relief of those on board.\n\"It was the first flight I have been on where people have been clapping and cheering,\" Guy said.\n\"Everyone was pretty happy to make it on to the ground, even though it was the wrong destination.\"\nAfter departing Auckland at 12.05pm they were scheduled to arrive in Queenstown at 1.55pm, but didn't land until 2.45pm in Christchurch.\n\"The combination of pretty reasonable turbulence, lightning, strong winds on landing then having to abort the flight - it was an interesting experience,\" Guy said.\n\"I have done a lot of flying but never experienced anything like that.\"\nThe Air New Zealand staff had done a good job and sorted everyone out with accommodation in Christchurch or with transport options, Guy said.\nFlight NZ605 from Wellington to Queenstown was also struck by lightning yesterday afternoon. It too diverted to Christchurch.\nOne of the passengers on NZ605 said there was a lot of turbulence and the plane quickly gained altitude.\nAnother passenger on the plane, Ben Taylor, of Wanaka, said: \"There was a really loud bang. The whole plane shook and the skies outside the plane were brightly lit up. Afterwards, there was an acrid, burning smell for a few seconds.\"\nAnother passenger on the flight from Wellington, Jason Epps-Eades, said the head flight attendant kept spirits up as they \"climbed out\" and diverted to Christchurch.\n\"There was a loud sound and a flash of light that went through the plane, then a burnt smell for a few minutes,\" he said.\nAir New Zealand said both the A320 aircraft were undergoing standard engineering checks in Christchurch.\n\"Lightning strikes are not uncommon. Aircraft are designed with this in mind and our pilots train for this scenario.\"","What is Deforestation?\nDeforestation refers to the cutting, clearing, and removal of rainforest or related ecosystems into less bio-diverse ecosystems such as pasture, cropland, or plantations (Kricher, 1997).\nWhat are the causes of deforestation?\nIII. Oil and gas extraction\nIV. Cattle ranching\nV. Agriculture: Cash crops\nVI. Local, National, and International factors: development, land titles, government subsidies to attract corporations into developing countries, trade agreements (NAFTA, CAFTA), civil wars, debt, lack of resources, and lack of law enforcement.\nLargest rainforests worldwide listed in descending order (from largest to smallest).\n- Amazon basin of South America\n- Congo river basin of Central Africa\n- S.E. Asia\n- New Guinea\n- Did you know that tropical rainforests, which cover 6-7% of the earth's surface, contain over half of all the plant and animal species in the world!\n- Did you know that 57% of all rainforests remaining are located in the Neotropics, with 30% located in Brazil.\nOverview of deforestation around the world:\nBetween 1960 and 1990, most of the deforestation occurred globally, with an increasing trend every decade.\n- Brazil has the highest annual rate of deforestation today.\n- Atlantic coast of Brazil has lost 90-95% of its rainforest.\n- Central America has 50% of its rainforests.\n- South America has 70% of its rainforests.\n- Philipines have lost 90% of its rainforests!\n- Madagascar has lost 95% of its rainforests!\n- El Salvador has lost 70-85% of its rainforest due to heavy bombing during the civil war 1984-1985.\n- Sumatra has 15% of its rainforests left.\n- Only 6% of Central Africa's forests are protected by law.\nStatistics on Global Rates of Rainforest Destruction:\n2.4 acres (1 hectare) per second: equivalent to two U.S. football fields\n149 acres (60 hectares) per minute\n214,000 acres (86,000 hectares) per day: an area larger than New York City\n78 million acres (31 million hectares) per year: an area larger than Poland\nOn average, 137 species become extinct everyday; or 50,000 each year!\n*If the current rate of deforestation continues, the world's rain forests will vanish within 100 years- causing unknown effects on global climate and eliminating the majority of plant and animal species on the planet*\nWhat are the consequences of deforestation?\n- Extinctions (loss of biodiversity of microbes (bacteria), plants, insects, animals, indigenous peoples, etc.\n- Habitat fragmentation. This disturbes the animals' habitat and may force them to enter habitats which are already occupied. This can pose many problems such as territorial conflicts, homelessness (loss of habitat), lack of food availability, migration disturbances, etc.\n- Soil erosion occurs when trees and plants are removed; the rain water washes the nutrients in the top soil away.\n- Changes in watershed geomorphology.\n- Desertification (dry, hot, arid conditions).\n- Edge effects can change microclimates (small climates) which affect endemic species (native species which can only live in specific environmental and habitat conditions).\n- Climate change (more carbon dioxide is released into the atmosphere, thus increasing the effects of global warming).\n- Pollution (ground, water and air pollution from oil extraction and mining chemicals).\n- Loss of culture (indigenous peoples subsistence living in the rainforest). People who live in the rainforest depend on the natural environment for food, shelter, materials for cooking, clothing, etc. If the forest is cut down or if their environment becomes polluted from oil extraction and mining, they are forced to move or risk starvation and sickness.\n- Displacement of people (loss of farmland, forest resources, etc).\n- Social conflicts and struggles over land and natural resources.\n- Conflicts over racial and ethnic rights.\n- Poisoning from oil and mining waste.\n- Economic uncertainty (price fluctuations and high interest rates on outstanding international loans with The World Bank and International Monetary Fund.\nWhat can we do to STOP or at least lessen the amount of deforestation and conserve our own use of natural resources such as wood, oil and gas, electricity, minerals and elements, and water? Brainstorm...here's a start:\n- Always use both sides of paper when writing, drawing, photo-copying, faxing, etc.\n- Recycle paper, cans, glass, and plastic.\n- Read the newspaper on-line.\n- Buy paper products made from recycled paper: notebook paper, paper towels, toilet paper, books, etc.\n- Use pencils until they are stubs! Think of pencils as gold (you'll never lose them if you do).\n- Encourage your parents, relatives, and friends to buy furniture and wood that is Certified. That means the wood was legally cut-down.\n- If you buy a product and you notice they use wood chips to package it, write to the company and suggest they use another packaging material.\n- Trees get cut down for cattle to graze. Instead of eating meat, think of eating other sources of protein such as fish, soy, beans, whole-wheat, and nuts.\n- Buy organic fruits and vegetables. That means there are no insecticides or pesticides (poisonous chemicals) sprayed on the food. If these chemicals kill insects and pests that try and eat the vegetables, think about how harmful they can be to you and the environment.\n- Instead of buying gold or diamonds, which are mined and cause environmental damage, consider jewelry that is made from materials that are not mined...such as glass.\n- Encourage your parents, relatives, and friends to drive fuel efficient cars that get good gas mileage. Hybrid and bio-diesel cars get great mileage and use less or no gasoline.\n- Even better, whenever possible, walk, bike, carpool or use mass transit (bus or train).\n- Save electricity by turning off lights, t.v., radio, computer, etc when you are not using them.\n- Save water by NOT taking baths; instead take quick showers (turning off the water while you soap up) and then turning it back on to rinse quickly.\n- While washing your hands and brushing your teeth, turn off the water. You'll save gallons if you do.\n- When washing the dishes or your parent's car, turn off the water while washing it with soap. Rinse quickly after washing.\n- Hmmm, can you think of other ways to conserve wood, oil and gas, electricity, minerals and elements, and water, etc...? Brainstorm with your pen pal or a family member.\nOkay, now show YOURSELF what you have LEARNED by answering the following questions:\n- What does deforestation mean? (Hint: The prefix de- means to remove or reduce).\n2. Why does deforestation happen? For what purpose(s)?\n3. The largest rainforest in the world is located in:\na.) The Philipines\nb.) The Congo Basin in Central Africa\nd.) The Amazon Basin of South America\n4. If 2 U.S. football fields are destroyed every second, how many football fields are destroyed in 5 seconds?\n5. If 50,000 species become extinct every year, how many will become extinct in half a year?\n6. T or F: Rainforests contain over half of all plant and animal species in the world?\n7. Fill in the blank: One environmental consequence of deforestation is __________. This occurs when heavy rains wash nutrients from the soil.\n8. Name two things you can do as a global citizen to decrease deforestation.\n9. Biodiversity refers to:\na.) The loss of animals and plants\nb.) A variety, or many different kinds of living things\nc.) When animals lose their living space or habitat\nd.) An increase in the earth's temperature\n10. Fill in the blank: Indigenous people _______ in the rainforest. They depend on the forest for their food, clothing, medicine, cooking and building materials.\nAnswers are located after the references (please don't look until you have completed all 10 questions).\nPen Pal Letter: Imagine you're in class and your teacher reads an article about a U.S. company which is deforesting a rainforest in Brazil. Your teacher encourages you and your classmates to write letters to the company. Using the information you have learned in this lesson, write your letter to convince the company to STOP the deforestation. Use the facts you have learned to support and provide evidence for your position. Write your letter in the Comments Section after this lesson. You and your Pen Pal will read each other's letters and provide positive feedback to each other. Please don't forget to type your name in the Comments Section.\nKricher, J. (1997). A Neotropical Companion: An introduction to the animals, plants, & ecosystems of the New World Tropics. New Jersey: Princeton University Press.\nRainforest Action Network web-site: http://ran.org/info_center/factsheets/04b.html\nNASA web-site: http://eospso.gsfc.nasa.gov/ftp_docs/Deforestation.pdf\nWRM Briefing: This is an excellent site on deforestation! http://www.wrm.org.uy/publications/briefings/underlying.html\nAnswers to questions:\n1. Deforestation refers to the cutting, clearing, and removal of rainforest or related ecosystems into less bio-diverse ecosystems such as pasture, cropland, or plantations.\n2. Logging, mining, oil and gas extraction, cattle ranching, agriculture, and International, National, and Local reasons.\n3. d.) The Amazon Basin in South America\n4. 2 U.S. football field= 1 second, then\n? U.S. football fields= 5 seconds\nYou can set it up as a proportion: 2/1= n/5, n=10\n5. 1/2 of 50,000 or 1/2 x 50,000 or 50,000/2= 25,000 species\n8. Buy paper products made from recycled paper and become a vegetarian\n9. b.) A variety, or many different kinds of living things\nHow did you do? I bet you did great!\nNote: This integrated lesson is designed for 3rd grade students. The following California standards are addressed in this lesson:\nReading: Vocabulary and Concept Development (1.6): Use sentence and word context to find the meaning of unknown words.\nReading Comprehension: Comprehension and Analysis of Grade-Level-Appropriate Text (2.6): Extract appropriate and significant information from the text, including problems and solutions.\nWriting applications: Write personal and formal letters , thank-you notes, and invitations (2.3): Show awareness of the knowledge and interests of the audience and establish a purpose and context.\nLife Science: Students know when the environment changes, some plants and animals survive and reproduce; others die or move to new locations.\nSocial Studies: Students understand the role of rules and laws in our daily lives and the basic structure of the U.S. government (3.42): Discuss the importance of public virtue and the role of citizens, including how to participate in a classroom, in the community, and in civic life.\nAlgebra and Functions: Students select appropriate symbols, operations, and properties to represent, describe, simplify, and solve simple number relationships: (1.1): Represent relationships of quantities in the form of mathematical expressions, equations, or inequalities."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:0b4546ad-1f0e-417c-8911-a8428afc30c4>","<urn:uuid:d21a9d6b-9350-49d2-9a97-a8473ebad5dd>"],"error":null}
{"question":"What should the bedroom be associated with for better sleep?","answer":"The bed should be associated with falling asleep quickly. This is why it's important not to stay in bed when unable to sleep - you should get out of bed if you don't fall asleep immediately. The goal is to create a strong association between your bed and quickly falling asleep.","context":["Sleeping Soundly: Understanding and Treating Sleep Disorders\nDoes your partner's snoring keep you awake? Are you worried that your child seems to be struggling for the next breath at night? Perhaps you find falling asleep difficult or you doze off at inappropriate times? Sleeping Soundly is intended to help you to fully understand sleep problems and to provide solutions so that you and your family can enjoy a great night's rest every night.\nWhat people are saying - Write a review\nWe haven't found any reviews in the usual places.\nDreams and dreaming\nSleeprelated internet sites\nThe function of sleep\nabnormal airway resistance syndrome alcohol antidepressants anxiety awake bedtime benzodiazepines body bruxism cataplexy cause insomnia cent Chapter child chronic fatigue syndrome Ciguatera common daytime sleepiness delayed sleep phase difficulty initiating disturbed breathing dose driving effect emotional epilepsy example eye movements fall asleep feeling Figure hormone Hospital 03 Hospital Sleep Unit important insomnia light limb movement disorder lucid dreams maintaining sleep medical conditions melatonin milligrams modafinil mouthguards muscle naps narcolepsy narcoleptic narcoleptic syndrome night nighttime non-REM sleep obstructive sleep apnoea occur overnight sleep study oxygen parents patients periodic limb movement person psychiatric illness rapid eye movement recognised REM sleep reported restless legs risk shift side-effects sleep and wake sleep apnoea sleep deprivation sleep diary sleep disorders Sleep paralysis sleep phase syndrome sleep quality sleep starts Sleep Unit 07 sleep walking sleep-wake sleepiness tendency sleeping tablets stimulant medications stopping breathing surgery symptoms treatment usually wake function\nPage 79 - On such occasions, the instructions are to be followed afterward when you intend to go to sleep. 3. If you find yourself unable to fall asleep, get up and go into another room. Stay up as long as you wish and then return to the bedroom to sleep: Although we do not want you to watch the clock, we want you to get out of bed if you do not fall asleep immediately. Remember the goal is to associate your bed with falling asleep quickly!\nPage 21 - How likely are you to doze off or fall asleep in the following situations, in contrast to feeling just tired? This refers to your usual way of life in recent times. Even if you have not done some of these things recently, try to work out how they would have affected you.\nPage 21 - Situation Chance of dozing Sitting and reading Watching TV Sitting, inactive in a public place (eg, a theater or a meeting) As a passenger in a car for an hour without a break Lying down to rest in the afternoon when circumstances permit Sitting and talking to someone Sitting quietly after a lunch without alcohol In a car, while stopped for a few minutes in traffic Source: From Johns,25 p.\nPage 79 - Step 3. Do this as often as is necessary throughout the night. 5. Set your alarm and get up at the same time every morning irrespective of how much sleep you got during the night. This will help your body acquire a consistent sleep rhythm. 6. Do not nap during the day.\nPage 34 - ... is calculated by dividing the weight (in kilograms) by the square of the height (in meters).\nPage 80 - People who feel angry and frustrated because they cannot sleep should not try harder and harder to fall asleep but should turn on the light and do something different.\nPage 15 - Early to bed early to rise Make a man healthy, wealthy and wise.\nPage 189 - Akerstedt, T., Torsvall, L. and Gillberg, M. 1982 'Sleepiness and shift work: field studies\nPage 189 - Chronic fatigue syndrome: chronic ciguatera poisoning as a differential diagnosis' Medical Journal of Australia vol.\nPage 177 - American Sleep Disorders Association 1610 14th St NW, Suite 300 Rochester, MN 55901..."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:4771e92c-22d7-4e05-ab81-2f5958033536>"],"error":null}
{"question":"How does the approach to transparency differ between managing a social media crisis and managing fire exit security?","answer":"In social media crisis management, transparency is emphasized through immediate acknowledgment of issues, not censoring critical comments, and providing full information about the situation on the organization's website. In contrast, for fire exit security, transparency is more literal and physical - exits must be clearly marked, well-lit, and immediately accessible without ambiguity, with any electronic locking systems required to have visible green emergency release points and clear labeling. In both cases, hiding or obscuring information/access is explicitly warned against, but the implementation differs based on the context.","context":["Knowing how to maintain an online reputation is an essential component of healthcare marketing. In this blog post, I will show you how to put an effective crisis response strategy in place for your healthcare brand.\nHaving an online presence has so many advantages when it comes to healthcare marketing, but it also comes with some risks. With the click of a mouse, patients can share their experiences online – good and bad – and their comments travel at lightening-speed through their social network. A social media crisis can escalate rapidly and you must be ready to step in and remedy the situation without delay. The only way to do this is to have a crisis plan already in place.\nCrisis management involves dealing with threats before, during, and after they have occurred. Let’s look at these three stages in more detail.\nStage 1 Preparation\nProactively prepare by developing a crisis response plan. The following elements are involved.\n#1 Crisis Definition\nFirst, define what constitutes a crisis. Three elements are common to a crisis (a) a threat to the organization, (b) the element of surprise, and (c) a short decision time.\nA crisis can fall into several categories including:\n(a) Technological (eg; your website has been hacked);\n(b) Confrontation (disgruntled employee, client, or patient attacks you online);\n(c) Rumours (eg; spreading false information about you, your product or service online);\n(d) Malevolence (eg; In 1982, a murderer added cyanide to some Tylenol capsules on store shelves, killing seven people).\n#2 Monitor Online Chatter\nAn effective social media strategy requires active listening to the online chatter about your healthcare organization. Should a crisis occur, listening to the conversation will help you shape a more insightful and effective response. Responding in real time to issues strengthens public perception that your focus is firmly on patient satisfaction. In addition, use monitoring to find the healthcare conversations you can add value to. Investing in community building online now will pay dividends in the form of support should a crisis hit you.\nThere are many free and paid monitoring tools available to you. These tools vary in scope and range across a number of sites, real-time or delayed searching, the sophistication of analytics, the flexibility of data presentation, integration with other applications, and of course, price. When it comes to reputation management, choose a tool that does more than just track mentions of your name. You need to be able to evaluate the sentiment (the ratio of mentions that are positive to those that are negative) attached to the mentions. Mention is a freemium monitoring tool that includes sentiment. Tweets that include words like “not working,” “fail” or “poor experience” should be resolved immediately.\n#3 Create a Written Plan\nYour written plan should include the following:\n- Clear guidelines on how to respond to each of the different situations outlined above in #1.\n- Links to your terms of service.\n- Who should respond – establish a clear chain of command and list contact information.\n- Make sure every member of your team knows this plan is in place, how to access it, and how to put the plan into action.\nStage 2: Action\nNow’s the time to put your carefully crafted crisis plan into place. The following are key considerations:\n- Determine the exact nature of the crisis. How and where did it originate? How is it affecting your patients or clients?\n- Go to the source. Find where the complaint originated and with whom. Determine their sphere of influence. If a blogger has published something that is untrue or misrepresentative of you, ask them to remove, amend, or modify the piece if this is appropriate.\n- Be respectful, polite and engaged. Never get into a public argument or talk down to anyone.\n- Be as transparent as possible as quickly as possible. Acknowledge that you are aware of the situation and that you are dealing with it straight away.\n- Respond swiftly and appropriately. Every moment counts on social media. The longer you wait, the more the conversation will heat up. Twitter, in particular, is a place where people expect a quick response no matter what time of day.\n- Don’t lie or try to hide the truth; admit when the fault is yours.\n- Use the same channel on which you were criticized to respond.\n- Don’t censor or remove the critical comments that appear on your social media platforms. Tempting as this may appear, it will only fan the flames of the social media fire.\n- Channel communication to your own website. Develop an area on your website or blog that houses the information about the crisis and what your organization is doing about it.\n- Communicate your story. A story gets out of control when you haven’t told your side and people begin to speculate. While you can’t control the story, you can provide the facts, information, and access to key people that allow journalists and bloggers to help you frame it in the right way.\nStage 3: Review\nWhen the crisis has passed, go over what happened. Ask yourself the following questions:\n- How well did you handle the situation?\n- Did it escalate to a bigger problem than it was?\n- What could you have done differently?\n- Prepare to deliver on your word. Make changes based on feedback if those changes are warranted and if you have promised to put them in place.\nIf handled well a crisis may even turn out to be an opportunity to show your commitment to your patients and consumers. Remember the Tylenol example above? Johnson & Johnson recalled and destroyed 31 million capsules at a cost of $100 million. The CEO appeared in television ads and at news conferences informing consumers of the company’s actions. Tamper-resistant packaging was quickly introduced, and Tylenol sales bounced back to near pre-crisis levels.\nWhile you can’t control everything that happens on social media, you can control your response. The best way to handle a crisis is to have your response plan in place. If you haven’t already made one, then do it today.\n- Don’t Be Scared, Be Prepared – How to Manage a Social Media Crisis\n- 3 Steps for Communicators to Implement a Crisis Ready Culture\n- The Five Social Media Emergencies Your Next Client Could Have\n- In the Trenches with Crisis Comms: 10 Things to Prepare\n- Crisis Communication and Social Media in Healthcare\n- 5 Steps to Planning for Effective PR Crisis Management\n- 7 Tips for Taking the Stress Out of a Social Media Crisis\n- How to Use Social Media Effectively in a Crisis\n- The Secret Behind Crisis Communications","Should there be an outbreak of fire, it is essential that people can safety and quickly evacuate the building without being put at any risk. Fire exit routes need to lead as directly to a place of safety as possible and must be marked with exit signs.\nThe exact numbers of fire exits needed in any building depends on a number of factors, including the number of people who could potentially use the exit. Churches and Places of Worship that have recently been refurbished or built will comply with the current Building Regulations so the number and size of the fire exits should be adequate in these situations. In older premises, the exits will need further assessment to ensure the exits are large enough and well distributed.\nIn most Churches and Places of Worship, the time taken to evacuate the building should be about 2 minutes 30 seconds, with consideration being made to the time it takes for people to respond to a fire alarm.\nThis time can be increased by up to 30 seconds for new buildings that comply with the latest Building Regulations, or reduced as much as necessary for a high fire risk building, such as those predominantly built from wood, where the risk to life due to fire is much higher than average.\nNumbers and size of exits\nA typical single-width exit door (750mm wide) will allow about 40 people per minute to evacuate. Larger doors will typically allow more people to pass through, so a 1050mm door width can allow as many as 80 people to evacuate per minute.\nHowever, when calculating how many exits are required, it is usually assumed that the largest exit route is unavailable because it is blocked by fire. If more than one exit discharges into the same place (for example, a lobby), then consider the aggregate loss of exit capacity caused by a fire in the lobby.\nIt is often the case that the number of fire exits limits the number of people that can safely use a building although other factors can have an impact. In some cases, the number of seats is the deciding factor as to the capacity of a room or building. In others, it is more down to the practicalities of the number of people being in the room.\nIt is commonly accepted that one exit is adequate in buildings where no more than 60 people congregate, providing that the building is on ground floor level only. However, it is always recommended that more than one fire exit is available so there is always another way out if the main entrance is blocked by fire. Furthermore, many rooms of low occupancy with low fire risk need only one exit.\nThe fire exits should be distributed around the building so ensuring that people can reach a safe exit route. Ideally the two exit routes would go in opposite directions, but at the least they should be positioned so that they are far enough apart so that a fire blocking one of the doors will not block the other. In looking at the direction of the exits, also consider any furniture or obstacles that could prevent people taking a direct path to an exit.\nIn some buildings, inward opening, rotating or sliding doors are installed at main entrances. None of these types of door are usually acceptable as emergency exit doors so they might need to be fixed open using a latch or chain if the door is needed as an exit route. Note that windows and ladders are not acceptable for use as fire exits in Churches and Places of Worship because the public cannot be expected to use these in an emergency.\nAn additional assessment is needed to determine if the position of exits with a building is adequate. This is to look at the travel distances. This is the distance that a person needs to move to reach the final exit from the building, or to a storey exit within a fire protected staircase. Consideration must be given to furniture and building features that might make the travel distance longer as people move around these objects.\nA number of travel distances have been published in government guidance, and for a typical Church or Place of Worship with normal fire risks, the following would usually apply as recommendations:\n- 32 metres in areas with seating in rows where more than one exit is provided\n- 15 metres in areas with seating in rows where there is only one exit\n- 45 metres in all other areas where more than one exit is provided\n- 18 metres in all other areas where there is only one exit\nThe seating plan can also play a significant role in the safe evacuation of people. The layout should ensure people can access a safe aisle or circulation space readily.\nIdeally, seating should be secured to the floor so that seats do not move or fall over. If seating is not secured to the floor, rows should usually be no longer than 12 seats and the seats should be secured together. No fewer than four seats should be secured together otherwise the seats could be tipped over easily and block someone's exit route.\nWhere stand-alone seating is used, the layout should allow for the inevitable rearrangement of seating by people in the congregation, and enough seats should be put out to prevent people adding seats to the end of rows and in aisles which could block the exit route.\nSeating plans need to allow free and ready access to exit routes (including aisles) and allowance needs to be made for wheelchair users so that they do not need to use aisle space. Aisles should be at least 1.05 metres wide along the entire length. Seats should allow for a clear seatway of 305mm (the distance between the back of one seat to the closest part of the one behind is the seatway) to allow space for people to move along a row of seats.\nDoor locks and security\nFor the final exit door from the building, it would be usual to use panic locks (of the 'push bar to open' kind). Such doors must be opened quickly in an emergency as people might panic as they evacuate the burning building, so additional locks or mechanisms must not be fitted.\nIt must be remembered that the door should be capable of being opened by any person immediately in an emergency. Some kinds of locks and latches used on doors may be unsuitable for use by everyone - some people may not be able to operate certain kinds of lever, knob or handle due to dexterity problems.\nSome doors might be secured by locks and keys. It considered unacceptable to have a key available nearby or in a red 'break glass' key box to open a fire exit door. Instead, a thumb-turn should be fitted to the inside of such doors to allow the door to be unlocked quickly by anyone.\nAll exit doors should not be locked when the public is present and checks should be made beforehand that this is the case unless the doors have simple lock mechanisms, like a panic bar. Members of the public should expect to have to operate one simple, unambiguous and ideally labelled device to open a door in an emergency.\nElectronic locking systems are becoming more popular. These must be fitted with suitable 'emergency door release' call points (break glass units), green in colour, and positioned so the door can be unlocked if the exit is needed. These locking mechanisms should ideally be connected to the fire alarm system, if there is one present, to ensure that the doors unlock automatically and remain unlocked until reset manually whenever the fire alarm is activated.\nUnwanted use of exit doors\nUnofficial and unwanted use of fire exit doors can be a problem in some premises. It might be that people could be looking for a short cut, or want some extra ventilation in hot weather, but the use of fire exit doors can lead to other safety and security implications, not least the safety of children.\nThere are a number of ways in which the use of fire exit doors can be controlled. Breakable straps are available that seal around the panic bar, or can be looped through screwed eyelet fixings. These seals break at about 10 to 12kg force, so will break when the door is opened for legitimate reasons. Note that cable ties and other non-breakable or home-made securing methods must not be used as these will prevent people using the exit door.\nAlarms can be a useful deterrent to unwanted fire exit use. The simplest is a stick-on alarm available for a few pounds from most hardware stores. The disadvantage of these is that many models are very loud and can be turned off by a small switch on the side. However, a number of manufacturers produce specific fire exit alarms that cost more but have the advantage of having a key or code number to limit who can arm or disarm the alarm.\nInward Opening doors\nTo ensure people can evacuate the premises quickly, exit doors should open in the direction of travel.\nIn some situations, including many older / historic Parish Church buildings, the doors open inwards. While this might be acceptable for Places of Worship where only a small number congregate (60 or fewer people), it is unlikely this would be acceptable otherwise.\nIn some cases, the door can be secured open with a cabin hook, or more securely with a padlock and key to prevent unauthorised closing (which can happen in cold weather). Where this is not practical, the door can be supervised by a steward or a nominated person (such as a fire Marshal) who will have to open and secure the door in the event of a fire. Any person given this role should sit close to the door at all times and cover should be arranged for holidays etc...\nFire exit routes must be kept clear at all times.\nThere are 'rules' about what you cannot have in these areas because they might hamper evacuation. The list includes:\nPortable heating equipment (including electric heaters)\nAnything that has a naked flame, including candles and lamps\nCooking equipment (including tea urns and kettles)\nBins and rubbish bags\nStored items, such as clothes for a charity shop or bring-and-buy sale\nNotice boards (unless small and the notices are kept firmly pinned onto the board)\nIt might be acceptable to have a coat rack in the escape route as long as it does not reduce the width of the exit significantly. Fire retardant furniture might be acceptable, again providing that there is no reduction in the escape route width and consideration is given to the fact that people will often move the furniture about and this could block the exit route.\nBe mindful of the fact that people who are trying to leave a smoke-filled room will often use the walls as a guide, so obstructions on, or close to, a wall are not desirable. This includes objects that are installed close to head-height that might not be seen.\nIn the event of fire, smoke can quickly make visibility poor. Exits need to be well lit by normal mains lighting, which should be switched on whenever the building is in use. In many situations, emergency lighting might be necessary both inside the building and directly outside the final exit doors. Such lighting would be designed to provide enough light to find an exit in safety.\nExits for people with disabilities\nCurrent Equality legislation requires a provision of suitable fire exits for disabled people. In some buildings, it might be possible to have a number of dedicated exit routes for people with impaired mobility where the normal fire exits are not suitable. In situations where exit routes need to be marked as suitable for wheelchair users, special exit signs are available which show a wheelchair symbol next to the usual fire exit symbols, arrow and text.\nIn some larger buildings (on more than one storey level), it might be necessary to provide 'refuge points' for disabled or elderly people to wait for assistance. This is necessary because most lifts cannot be used in the event of fire as people might be put in greater danger if the lift doors open onto the fire itself. Refuge areas usually need to have some form of intercom or communications system and must be protected against fire by means of fire doors.\nIn any case, you should have a procedure in place to enable the safe evacuation of all people from the building. This includes those that might have impaired hearing, mobility, sight or perception. For regular attenders, employees and volunteers, consider setting up a personal emergency evacuation plan (PEEP), where a detailed plan is laid down for each person, highlighting who will provide assistance and what assistance is needed. Generic Emergency Evacuation Plans (GEEPs) can be beneficial for visitors.\nFire and Emergency Assembly Point\nA Fire Assembly Point is a place of safety where people meet if there is a fire or similar emergency. This needs to be away from the building, and the normal recommendation is for it to be a distance equal to twice the height of the building. The assembly point must be large enough to accommodate all the people that might be present and located so that people do not get in the way of the emergency services.\nIt is also wise to have a second assembly point available. If the main assembly point is being affected by a fire, people can be moved to a further place of safety. Additionally to this, if the emergency is during bad weather, arrangements should be made to accommodate people in a nearby building so they are not affected by the weather.\nBecause of the complexities of fire exit calculation, the above is only an overview. More information on exits and means of escape can be found in the guidance to the Regulatory Reform (Fire Safety) Order 2005. These guides are intended for England and Wales. The guide for Small and Medium Places of Assembly is suitable for most Churches and Places of Worship of small or moderate size.\nIf there is any doubt as to the suitability of exit routes, it is recommended that a competent person conducts a Fire Risk Assessment to determine if the size and distribution of exits, and the travel distance, is acceptable."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:03c69d2d-f6ba-4770-8c63-4c267d49ae49>","<urn:uuid:206ad08d-6f14-495a-9afa-7eea73cd2cb3>"],"error":null}
{"question":"How can I make images scale properly on different screen sizes in responsive web design?","answer":"There are two main methods to handle image scaling in responsive design: 1) Set the maximum width of the image to 100% of the screen width or browser, which will make the image scale down proportionally when the screening width diminishes. 2) Use a method that not only resizes images but also decreases their resolution on smaller devices to save space, loading time, and mobile bandwidth on smaller mobile screens.","context":["Responsive Web design is the strategy that indicates that development and design should reply to the consumer’s behavior and surroundings based on screen size, platform, and orientation.\nAs the user switches from their laptop to iPad, the website should automatically switch to accommodate for resolution, picture dimensions, and scripting abilities.\nThe concept of Responsive Web Design\nResponsive web design becomes quite clear if you take a moment to analyze and understand the intricacies of the contemporary net and mobile technology, and implementing responsive design is simpler than you may think.\nThe number of those devices has increased exponentially, which in turn has improved the variety of browsers specifications and display resolutions.\nThere is a wide variety of information on responsive web design available all over the Internet, and a lot of information is abstract, philosophical, and anecdotal. If you’re looking for a tidy, concrete understanding of the base of responsive web design, it comes down to below methods that all work in conjunction:\nThere are a couple issues that crop up with flexible media but the biggest issue is around ratio that’s a knock-on effect from defining height and width on the component itself. Since the viewport grows larger the contained image/video/iframe grows wider too, which means that it needs to grow taller also.\nWhat’s the best method to design a website that will be looked at on countless different display sizes, some of which might not even exist yet? The ideal approach is not one comprising catering to specific devices; it’s one that more generally makes your layout scalable, adaptive, and adaptable to screens of all sizes, even sizes which are yet to become popular.\nThere are few points to make it responsive –\nIf the maximum width of the image is set to 100 percent of the screen width or the browser then the overall screening width diminishes, the image will scale down proportionally.\nAnother method that aims not just to resize images, but also to decrease the resolution of images on smaller devices in order to not waste space, loading time, and expensive mobile bandwidth on more small mobile screens.\nMedia Queries are possibly the most important tool a web designer has to create their sites responsively.\nThis set of rules enables developers to create fluid layouts that accommodate without distortion or loss of quality to the viewer’s device. It is great to point out that this set of guidelines (or principles) ought to be stored in a separate CSS stylesheet from the one where there are those for the general style (which is usually named style.css).\nMedia questions help take into consideration multiple display possibilities and, as a result, the organized style rules can be modified quickly and easily.\nPrinciples of Responsive Web Design\nResponsive website design is a great solution to our multi-screen problem, but getting into it in the printing standpoint is difficult. No page size, no millimeters or inches, regardless of actual limitations to resist against. Designing in pixels for Desktop and Mobile just is also the past, as a growing number of gadgets may open up a website.\nTherefore, let’s clarify some basic principles of responsive web design here in order to adopt the fluid web, instead of fighting it.\nProper use of Breakpoints\nBreakpoints allow the layout to change at predefined factors, i.e. having 3 columns on a desktop, but just 1 column onto a mobile device. Most CSS properties could be changed from 1 breakpoint into another. Usually where you put one is determined by the content. If a sentence breaks, then you might have to add a breakpoint. But use them with caution — it may get cluttered quickly when it’s difficult to understand what’s affecting what.\nWhen a design is coded in relative proportions of the components on the webpage, this layout is known as a “fluid grid”. Traditionally, a CSS layout uses fixed width grids to place each element online page. This fixed-width approach is no longer viable today since there are many displays, from phone screens to HDTV displays. Now, each component is given a relative size or percent instead of particular heights and widths.\nMobile or Desktop First\nTechnically there isn’t a great deal of difference if a job is started from a smaller display to a bigger or vice versa. Yet it provides additional limitations and makes it possible to make decisions if you start with mobile. Frequently people start from both ends at once, so really, go and see what works best for you.\nEvery single consumer should be able to get every single piece of information on your page in the simplest manner possible. The text ought to be readable on all display sizes. The colors should be appealing, yet not harsh on the eyes. Do not let the content be overpowered by the background. The plan should have visual harmony and balance with all the color scheme as well as contrast.\nRemember the comparative position? With a lot of elements based on each other would be difficult to control. So, wrapping elements in a container retain it way more clear, tidy and clean. This is where static units like pixels can help. They are useful for articles that you don’t wish to scale, like logos and buttons.\nBitmap images vs Vectors\nIs it true that your icon possess a lot of details and some fancy effects applied? If yes, use a bitmap. If not, consider employing a vector picture. Each has some advantages and some drawbacks. However, remember the dimensions — no graphics should go online without optimization. Vectors on the other hand often are tiny, but some older browsers won’t support it. Furthermore, if it’s lots of curves, it may be heavier than a bitmap, so select wisely."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:118d33a5-55ec-4216-b1cc-70ca7da28da5>"],"error":null}
{"question":"How do vinyl and CD masters differ in their approach to dynamic range and compression?","answer":"Vinyl masters typically require less compression due to the format's physical limitations, while CDs can be mastered more aggressively for loudness. The vinyl format's restrictions naturally dictate a certain tonal balance and character. Bob Katz notably sources his vinyl masters from mixes prior to peak limiting or additional loudness processing. However, low levels of clipping and hard limiting (up to 3dB) on CDs can be surprisingly inaudible. Modern CD masters often have reduced dynamic range due to the loudness wars, though excessive dynamic range may be contrary to many people's listening habits. Once dynamic range is crushed out of the music in CD mastering, it generally cannot be restored, effectively destroying that information.","context":["- 1 Does vinyl intrinsically require a superior master than CD?\n- 2 How many different ways can a CD master differ from a vinyl master?\n- 3 How do you know if a vinyl master is audibly superior than the CD master?\n- 4 Is less compressed music always of a superior quality?\n- 5 Some known examples: Vinyl releases with a different master than the CD\n- 6 Some known counterexamples: Vinyl releases with same/similar hypercompressed master as on CD\n- 7 Vinyl releases suspected of being of different masters than the CD\n- 8 Vinyl releases suspected of being the same master as the CD\n- 9 Vinyl mastering online\nDoes vinyl intrinsically require a superior master than CD?\nThere's this idea floating around that vinyl records must have intrinsically different masterings than CDs of the same material. There's both a kernel of truth to this, and a few gigantic myths.\nCDs have only one (extremely strong) restriction on how loud they can be cut - the digital peak level, 0dbFS - and (almost) anything that doesn't violate that restriction is permissible. Vinyl manufacturing has many different restrictions, and they are all rather loose, in that the restrictions can be sometimes relaxed.\n- The grooves can actually overlap each other if they are too loud. This can be alleviated by spacing the grooves further apart.\n- If the groove \"moves\" quickly enough - its velocity is high enough - some turntable cartridges will be unable to track the groove, and a skip results.\n- Just like the voice coils in a speaker can burn up if enough energy is dumped into them, the voice coils on a cutter head can burn up if the signal is of a high enough power. (The amplifiers range into the hundreds of watts and the coils themselves are liquid- or helium-cooled, depending on who you ask, so the powers involved here really are quite substantial.) The historical solution to this is a special limiterthat squashes the high frequencies in the music if the cutting head temperature exceeded some threshold. Clearly not a high-fidelity solution (and many mastering engineers do not use them nowadays).\n- Excessive stereo bass content (bass in one channel or another) can compromise tracking or even make the cutting head jump out of the groove. This is sometimes solved with an elliptical filter, which sums bass frequencies to mono. Again, not all mastering engineers use this.\nNone of these restrictions explicitly say \"hypercompressed, distorting music cannot be cut onto vinyl\". Rather, that music may be more difficult to cut and play back than other music.\nHow many different ways can a CD master differ from a vinyl master?\n- The CD and vinyl masters might just be exactly the same: the same signal that goes on the ADC goes on the cutting head.\n- Acceleration limiting might be used on the vinyl master.\n- Elliptic filtering (bass sums to mono) might be used on the vinyl master.\n- The vinyl master may be sourced from a 24-bit version of the CD master. (However, the high noise content of vinyl generally makes this a meaningless distinction.)\n- The vinyl master may be sourced from a higher-sampling-rate version of the CD master. (However, the demonstrated inaudibility of frequencies above 20khz makes this a meaningless distinction.)\n- The vinyl master may be EQ'd differently to account for equalization differences in the cutting head, electronics, or playback devices.\n- Finally, the vinyl master might be sourced from a master with less dynamic range compression or limiting than the CD master. This is the only distinction between a vinyl an CD master that is meaningful - in the sense that information exists on the vinyl master, in terms of reduced compression, that does not exist on the CD master.\nHow do you know if a vinyl master is audibly superior than the CD master?\nYou ask the mastering engineer what he did. Other that, that, generally, you don't know. There are certainly many wrong ways to determine this, which can lead to false positives and false negatives.\n- Many people look at large-scale waveform plots, like those available in Audacity and Audition, and compare the waveforms across the entire piece of music. This does not work. The distortions present in vinyl - everything from subsample delays in the recording process to phase errors in the analog electronics to tracking and tracing distortion - ensure that even if the vinyl is cut with the exact same master as the CD, the peaks will be considerably higher, even during regions of gross clipping. Thus this technique is generally not acceptable, even though it is by far the most popular.\n- RMS loudness estimates, such as the industry standard RMS figure and ReplayGain, are ineffective because they require a reference level to compare the vinyl and CD versions against. No such reference level exists.\n- Experimental dynamic range estimators, such as pfpf and SparkleMeter, are useful in teasing out substantial differences in dynamic range, and may be quite useful in estimating when they become audible, rather than . pfpf, in particular, is designed to be immune to moderate levels of clipping distortion, under the expectation that clipping is either going to be inaudible or going to affect the timbral character of the music, not the dynamic range.\nThe one consistently accepted method of showing reduced compression is to show the individual samples in a clipped waveform against the same waveform in a different master that is not clipped. But again, this method is not foolproof: Various distortions can mask the clipping so that it is not consistently at the signal peak, yet still retains its characteristic distortion. However, clipping may not exist obviously in hypercompressed music, and even if a difference exists, it very well may not be audible.\nIs less compressed music always of a superior quality?\nSometimes. Low levels of clipping and hard limiting (perhaps up to 3db!) are surprisingly inaudible. Excessive dynamic range is generally contrary to many peoples' listening habits and situations; few people will tolerate the full dynamic range of a symphony orchestra in their living rooms, or the full loudness of a live rock band. Modern music listeners consistently perceive less compressed music as being drier in tone and less pleasing to the ear than modern mastering styles.\nThat said, just like speaking in different tones and loudnesses of voice is considered more emotional and human than speaking monotonically, music with a wider dynamic range is generally perceived as being more emotional than music with a tight dynamic range. And once the dynamic range is crushed out of the music, it generally cannot be added back in at a later time. The information representing by the dynamic range is effectively destroyed. These factors, as well as the diverse other factors mentioned in discussions of the \"Loudness War\", ultimately reduce the value of modern-mastered records in many peoples' view.\nSome known examples: Vinyl releases with a different master than the CD\nBob Katz has stated that all of his vinyl masters are sourced from a mix \"prior to any of the peak limiting or any additional loudness makers other than the ones there for esthetic purposes.\"\nSteve Hoffman's work is generally known for distinctly different masterings compared to equivalent CD releases. His mastering of ZZ Top's Tres Hombres includes diverse changes, including a much less compressed drum track. His mastering of the White Stripes's Icky Thump is also well praised for being distinctly different (and better) than the CD release.\nSome known counterexamples: Vinyl releases with same/similar hypercompressed master as on CD\nThis is a list of vinyl/CD releases, where the CD release has been considered compressed or clipped for a popular music audience, and the vinyl master has been proven to be sourced from digital audio equal in compression/clipping content to that of the CD master. This list excludes albums whose CD releases are considered relatively uncompressed or unclipped.\nIt does not mean the CD master necessarily sounds poor - some CDs referenced here have been highly commended for their sound quality - but it does mean that, for any of these records, if one considers the CD to be 'hypercompressed', one ought to also consider the LP to be hypercompressed as well, so preferring LP over CD for that reason would be foolish.\n- Foo Fighters, In Your Honor 45rpm 4LP. Steve Berson, Total Sonic Mastering: Sourced from 24/96 digital masters, which differ from CD masters only in choice of output format. Hypercompression/clipping which existed on CD also existed on LP master. Emphasis added:\nWhen I cut the vinyl DMM masters for the first \"special edition\" release of the 45rpm 4-LP set for the Foo Fighters \"In Your Honor\" album the producers made a big deal out of wanting to have an \"audiophile\" release and made sure that I could work from 24bit/96kHz source (which I was able to do at Europadisk due to SAWStudio sending to a matched pair of Lavry Blues that went to both the pitch depth computer and the cutting head) and that I did as close to a flat transfer from these as possible.\nI was disappointed to find that the high res files I received ... had been heavily clipped as they were the same files (just prior to SRC and dithering) that the CD master was made from.\n- Metallica, Death Magnetic. Visually confirmed, and strongly hinted at by Ted Jensen, who mentions that the mixes were clipped. No word from vinyl mastering engineer Kevin Gray. As the mix itself was clipped, it is highly anticipated that the vinyl release is of the same master as the CD. Jensen:\nIn this case the mixes were already brick walled before they arrived at my place. Suffice it to say I would never be pushed to overdrive things as far as they are here. Believe me I’m not proud to be associated with this one...\n- Soundgarden \"Down on the Upside\", as admitted at http://www.gearslutz.com/board/mastering-forum/502795-soundgardens-down-upside-vinyl.html. At least one other record mastered by Collins is also implied as being sourced from CD masters. Two things about Collins's work are worth pointing out:\n- He was specifically commended for the sound quality of the vinyl release before he pointed out that it was sourced from CD.\n- He claims that a Stereophile reviewer once commended him in the same way about a separate record, and refused to believe that the vinyl was in fact sourced from 16/44. (emphasis added)\nI once had a reviewer from Stereophile call and was raving, raving I tell you, about the sound of an LP done the same way: I sent my eq'd CD master and someone else cut the lacquer.\n\"The depth, the detail, the microdynamics are beyond compare, it's just more proof of the superiority of analog.\"\"Uh, I gotta get back to work.......\"\n\"But it was cut from a 16 bit digtial source.\"\n\"I was at the session.\"\n\"Don't you tell me what I'm hearing!\"\n- Pete Lyman, Infrasonic Sound, same \"Down on the Upside\" thread as above, comments on how CD sources form the majority of his business in cutting records, and points out that 16-bit digital delay lines have been used since the 80s, and quite possibly for most records released since then (emphasis added):\nI cut about 10 sides a week, most of them from a 16/44.1 source. If the source material sounds good and doesn't have excessive high end issues or phase issues, it should sound fine, if not better. I doubt most people could tell the difference between the same material cut at 16 bit and 24 bit. Most records (at lease since DAT/digital surpassed analog mixdown options)have gone thru some conversion during the cutting. I think it would shock quite a few people to know that those \"analog\" records they love so much probably passed thru a 16 bit digital delay line on their way to the cutting head. But, quite a few of us are opting to bypass that sort of delay and do it in the workstation thru a second set of converters, eliminating a stage of conversion.\nVinyl releases suspected of being of different masters than the CD\n- REM, Accelerate\n- Battles, Mirrored\n- Clips, but at a different (lower) level than the CD (!)\n- Slayer, Christ Illusion\n- Depeche Mode, All new releases since 1997.\nVinyl releases suspected of being the same master as the CD\n- The Decemberists, The Crane Wife\n- REM, Accelerate\n- Commentary is very divisive. Some people believe the vinyl is clearly superior to the CD, and some people abhor the vinyl as sounding hypercompressed.\n- Autechre: Untilted, Chiastic Slide, Gantz Graf (and probably all others)\n- Shellac: Excellent Italian Greyhound (and probably all others)\n- Of Montreal, Hissing Fauna: Are You The Destroyer?\nVinyl mastering online\nHere is the list of services where you can make online masterig for vinyl:","Listening tests are very difficult to administer. Small errors in the setup can lead to erroneous results.\nVinyl places many restrictions on the mix and mastering engineers. High-frequency content must be limited due to the mechanical limitations of cutters and playback cartridges. Low-frequency energy must also be limited for similar reasons. Stereo content must be limited and stereo separation of the system is also limited. Vinyl cannot be aggressively mastered for increased loudness, the way CDs can.\nCDs can handle a wider variety of mixing styles, and a wider variety of spectral content. However, CDs do not always deliver better results. The loudness wars have degraded the quality of many newer CD releases. Early CD releases were degraded by poor quality conversion and poor quality digital processing. The conversion and processing problems are behind us, but the loudness wars are alive and well.\nBottom line- vinyl is mastered very differently than CDs. It is possible to record any vinyl master on a CD, but many CD masters cannot be recorded on vinyl. Unfortunately (for those who like the vinyl sound), most recordings that were originally released on vinyl, are re-mastered when they are re-released on CD. The result is that vinyl and CD releases of the same album never sound the same.\nThe restrictions of the vinyl format dictate the tonal balance of the final master. This gives a certain character to all vinyl recordings. These restrictions dictated the mixing style of the 60s, 70s, and early 80s.\nYour ADC1 and DAC1 can capture and reproduce everything recorded on the vinyl and can do so with near-perfect transparency. We have produced 16th generation dubs using the ADC1 and DAC1. A group of mix and mastering engineers were unable to tell the difference between the 1st generation and the 16th generation. These converters are very transparent and will not impact the sound of the vinyl recording.\nSet your ADC1 so that the highest peak reaches -3 to -10 dB. Use the peak-hold function on the ADC1 to see if you have the input gain set properly. If you go over -3 dB you may want to reduce the input gain on the ADC1 and start over. Ideally, we want to leave 3.5 dB of headroom for digital processing. If we record at too low a level, noise will increase (although the ADC1 and DAC1 are more than 30 dB quieter than a vinyl record).\nOn playback, adjust the DAC1 volume control to match the level produced by the turntable playback path. Ideally the DAC1 should be operated in the upper half of its volume control range (to optimize the signal to noise ratio). If you are using the XLR outputs, you can set the passive attenuators to keep the volume control in the ideal range.\nTo run your test, you will have the challenge of starting the vinyl and digital recordings in sync. To avoid this issue, you may want to just pass the live vinyl feed through the ADC1 and DAC1 without recording. The only problem is that the converters add a small delay to the signal path and some listeners may be able to detect this delay if the switching is instantaneous. If the signal mutes for a fraction of a second while switching, then you should be able to get away with live A/D and D/A conversion. Make sure that switching transients sound the same when switching back and forth between the two sources. At Benchmark, we have a relay-controlled ABX switcher that does this very well. Listeners will pick up on the slightest differences in switching transient even if they cannot hear a difference in the two sources. Tests should be at least be blind (listener does not know which source is which). Ideally the test should be double-blind (neither the administrator nor the listener know which source is which). Double-blind tests usually require a computer-controlled switcher.\nIf you do decide to record the digital signal, I would not adjust the gain in the digital system unless you are sure that it is properly dithered. If you do adjust it, I would keep any gain adjustments small ( a few dB at most). Playback levels must be matched to an accuracy of 0.1 dB. A test record with a steady tone is useful for calibration. A good volt meter with a fast-responding peak hold function is useful if you do not have a test record and SPL meter.\nThe 22 kHz limitation of 44.1 should not be a problem. You should be able to run the test at 96/24 if you wish. One advantage of 96 kHz is that the delay through the ADC1 and DAC1 will be shorter. The live vinyl --> A/D --> D/A comparison to the direct vinyl should work better at 96 kHz because the delay through the converters is shorter.\nWe have only skimmed the surface. Search double-blind listening tests or ABX listening tests for more information.\nIn some cases, upsampling will improve the output of your D/A converter. The low-pass filters incorporated into the upsampling process will essentially replace the filters in your D/A converter. If the upsampling software has better filters than those built into the D/A, then you may see an improvement."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:ab336205-660b-4d7b-a3a7-1bb381ff91f6>","<urn:uuid:671f0ac1-7ed2-4292-baa3-58b61c06ba22>"],"error":null}
{"question":"How do the soil moisture requirements differ between Aeonium plants and Pansies during cold weather?","answer":"Aeoniums and Pansies have contrasting soil moisture needs in cold conditions. Aeoniums cannot tolerate soil that remains moist for long periods, as stems will soften and eventually rot when kept under wet conditions. They need soil to dry readily between waterings. In contrast, Pansies require well-draining soil but benefit from thorough watering before a freeze, as moist soil helps insulate their roots and mitigate the effects of extreme cold, though overwatering should be avoided as overly wet soil combined with dramatic temperature drops could cause root freezing.","context":["What Is the Aeonium Plant?\nSucculents can be interesting and fun plants to grow, thriving outdoors in areas with dry summers and warm winters. In regions where winters are cool, they appreciate summer sun outdoors and then can grow indoors as houseplants when weather cools. Aeonium plants (Aeonium spp.) make up a group of succulents that are native to Africa and typically have leaves radially arranged in rosettes.\nAlthough dozens of plants belong to the genus Aeonium, they share a number of common characteristics. All have leafless stems topped by leaves that are radially arranged in a structure called a rosette. Leaves in the plant's rosettes are thick and have a waxy outer covering, and stems tend to be thick and resemble branches on larger specimens. Some large aeoniums can reach a mature height of 5 feet or more, while others are compact and form mounds of short-stemmed rosettes. All aeoniums are frost-tender, although several can tolerate a few hours of freezing temperatures. Generally, they are suitable for growing outdoors in U.S. Department of Agriculture plant hardiness zones 9 through 11.\nLight, Soil and Potting\nAeoniums thrive in full sun and need at least six hours of full sun each day. They can tolerate shade for a few hours, but many types will not develop their full leaf colors without full sun. They generally prefer a sandy soil mix with good drainage appropriate for cacti. When potted, they do best in terra cotta pots that allow soil to dry readily between waterings. Aeoniums cannot tolerate soil that remains moist for long periods; stems will soften and eventually rot when plants are kept under wet or soggy conditions.\nFeeding and Maintenance\nMost aeoniums grow actively during fall and spring, and respond to fertilizing every four to six weeks with a balanced, 10-10-10 formula while growing. When plants slow their growth during winter and summer, you should withhold fertilizer and cut back on watering. Aeoniums are unusual because rosettes tend to die after they flower. You can either cut these spent rosettes back when they fade, or to prevent flowering, take cuttings of terminal stems and plant these in moist soil. This helps control the size of your plant and stops it from blooming, preventing dieback of terminal rosettes and helps increase the size of your aeonium planting.\nDozens of aeonium species exist, each containing many cultivars. Some of the best choices include the black rose aeonium (A. arboreum \"Zwartkop\"), which has nearly black rosettes and can reach a height of 5 or 6 feet in warm climates; purple aeonium (A. arborerum \"Atropurpureum\"), with purple-red rosettes on a 2- to 3-feet tall plant; and green rose buds (A. aureum \"Green Rose Buds\"), a plant that forms up to 50 rosettes that are 4- to 10-inches wide. They have an apple green color on a compact plant that tolerates partial shade. Some aeoniums are multi-colored, such as Haworth's aeonium (A. haworthii \"Variegated\"), with red-edged green leaves, and the cultivar called sunburst aeonium (A. arboreum \"Luteovariegatum\"), a cold-tolerant, bushy plant with green leaves outlined in pale yellow.\nJoanne Marie began writing professionally in 1981. Her work has appeared in health, medical and scientific publications such as Endocrinology and Journal of Cell Biology. She has also published in hobbyist offerings such as The Hobstarand The Bagpiper. Marie is a certified master gardener and has a Ph.D. in anatomy from Temple University School of Medicine.","Pansies can tolerate temperatures as low as 26°f (-3°c) before they can no longer survive. Pansies are popular cool-season bedding plants that produce colourful flowers that range from yellow to blue to purple, and sometimes even white.\nThey’re often used for gardens or pots, with some gardeners growing them as annuals and others as perennials. Pansies are known for being cold-hardy, but the amount of cold that they can tolerate varies depending on the species and variety. In this article, we’ll discuss how cold pansies can tolerate, as well as tips on how to protect them during a cold snap. Whether you’re a seasoned gardener or just starting, having the right knowledge will ensure your pansies thrive in the colder months.\nFactors Affecting Pansy Cold Tolerance\nPansies are lovely cold-hardy plants that bring a burst of color to gardens and landscapes during chilly months. They are a favorite among gardeners who want pretty flowers during winter. If you’re thinking of planting pansies, you may be wondering how much cold they can tolerate.\nThis article will explore the factors that affect pansy cold tolerance.\nGenetic Makeup Of The Plant\nThe genetic makeup of pansies plays a crucial role in their cold tolerance. Pansies that have been bred for colder climates are generally hardier than those bred for warmer areas.\n- Flower size: smaller flowers typically handle colder temperatures better than large ones.\n- Stem length: shorter stems usually cope better with cold weather than longer ones.\n- Leaf texture: thick, leathery leaves can endure colder temperatures than thin ones.\n- Variety: some pansy varieties are naturally cold-hardy and can withstand temperatures as low as 5°f (-15°c).\nApart from genetics, environmental conditions also influence pansy cold tolerance. Understanding these factors can help you create the ideal conditions for your pansies to thrive.\n- Soil moisture: pansies require well-draining soil. Soggy or waterlogged soil can make them more prone to cold damage.\n- Sun exposure: pansies need ample sunlight to photosynthesize and develop robust stems and flowers. However, too much sun exposure during winter can cause drying and scorching of foliage.\n- Air temperature: pansies can handle temperatures as low as 15°f (-9°c). However, if the temperature drops below this threshold, they may suffer cold damage. Frost, in particular, can damage pansy foliage and flowers.\n- Wind exposure: cold winds can cause dehydration and damage to pansy foliage.\nPansy cold tolerance is influenced by multiple factors, including genetics and environmental conditions. By understanding these key factors, gardeners can create an optimal environment for their pansies to thrive. When planting pansies, it’s important to consider these factors and make necessary adjustments to ensure healthy growth.\nUnderstanding Pansy Hardiness Zones\nPansies are beautiful, herbaceous garden plants that flourish in cool climates across the united states. They are known to be hardy and can tolerate some amount of cold weather, but how much cold can they actually withstand? This blog post aims to discuss pansy hardiness zones and help readers understand how to determine the hardiness zone of their garden to ensure their pansies thrive.\nExplanation Of Hardiness Zones\nHardiness zones refer to geographic regions determined by the average minimum temperature in a location during winter. The united states department of agriculture (usda) establishes these zones based on reliable observations of the climate records. Each zone has a different temperature range that is used to grow different types of plants.\nDifferent Zones Across The United States\nThe united states is divided into 13 different hardiness zones ranging from 1a to 13b. Zone 1a has the coldest temperature range compared to zone 13b, which has the warmest. Pansies can tolerate cold weather, and they grow well in zones 4 to 8, which have a temperature range that lies between minus 40 degrees fahrenheit in zone 4a to 20 degrees fahrenheit in zone 8b.\nDetermining The Hardiness Zone Of Your Garden\nTo know which hardiness zone your garden falls under, you can enter your zip code on the usda website and check the interactive map. Once you have determined your zone, it is easier to choose the type of plants that grow best in your area.\nThis way, you can avoid choosing plants that do not thrive in your location and instead select varieties perfect for your garden’s climate.\nUnderstanding hardiness zones is crucial when it comes to planting pansies. By determining your garden’s hardiness zone, you can choose the type of plants that are better suited for your area, ensuring they thrive and flourish beautifully.\nWhat low temperature can pansies tolerate?\nTips For Protecting Pansies From Cold\nPansies are a popular choice for gardeners because of their hardy nature, but they still need protection from frosty conditions. The temperature that pansies can tolerate varies, depending on the variety and growing conditions. However, generally, they can survive temperatures as low as the mid-20s for a short period.\nHere are some tips to help protect your pansies from the cold.\nCovering Pansies With Mulch Or Straw\n- Spread a layer of mulch or straw over the bed of your pansies before the onset of cold weather.\n- Using straw or mulch will provide insulation to the pansies and help to retain heat in the soil.\n- Be sure to cover the entire bed and use a thick enough layer to provide insulation for the roots.\nUsing Frost Cloths Or Blankets\n- Frost cloths or blankets can help protect pansies against frost and cold winds.\n- These cloths and blankets are made from lightweight materials that allow air and water to reach the plants while protecting them from the cold.\n- It’s best to set up the frost cloths or blankets before freezing temperatures set in as it can be challenging to protect plants after exposure to low temperatures.\nWatering Pansies Before A Freeze\n- Water your pansies thoroughly when you know a freeze is expected.\n- The moist soil will help to insulate the roots of the pansies and can mitigate the effects of extreme cold.\n- Be careful not to overwater the pansies. If the soil is too wet and the temperature drops dramatically, the roots could freeze.\nPotting Pansies To Bring Indoors During Extreme Cold\n- Consider potting your pansies to bring them inside during extreme cold.\n- Select a pot that is big enough to accommodate the root structures and provide enough space for the pansies to grow.\n- Place the pot in an area of your home with bright, indirect light, and protect it from drafts.\nBy following these tips, you can help protect your pansies from the cold and keep them thriving. Remember to monitor the weather conditions and take necessary precautions to keep your plants healthy.\nPansy Varieties With High Cold Tolerance\nPansies are gorgeous and versatile flowers that adorn gardens in various regions of the world. While pansies thrive in chilly weather, gardeners still wonder about their ability to endure extreme cold. Luckily, some pansy varieties have proven their exceptional resistance to frosty temperatures.\nLet’s explore pansy varieties with high cold tolerance.\nDescription Of Pansy Varieties Known For Their Cold Tolerance\nPansy varieties that can endure low temperatures and even frost are about as close to winter-perfect pansies as we can get.\n- delta series: these flowers are a popular choice. They typically bloom in bright hues and have a large blossom size.\n- winter flowering series: these are popular in colder climates. They are small size flowers that thrive in low light conditions.\n- crown series: they come in a wide range of colors and bloom in spring and fall.\n- hibernica series: these pansies are exceptionally cold-tolerant, with strong stems that improve their durability in harsh weather.\nBenefits And Drawbacks Of Choosing A Cold-Tolerant Variety\nChoosing a cold-tolerant variety of pansy comes with several benefits and drawbacks, including:\n- Cold-tolerant varieties are ideal for regions with long winters, as they can withstand temperatures as low as twenty degrees.\n- These pansies can continue to flower throughout the winter, which can add a beautiful touch of color to your garden.\n- They are hardy and pest-resistant.\n- Cold-tolerant pansy varieties typically produce smaller flowers with less brightness and fewer blooms.\n- Maintenance is critical; if not adequately cared for, these flowers may bloom erratically or fail to bloom at all.\nTips For Successful Growth And Care Of These Pansy Varieties\nIf you’re thinking of planting pansy varieties with high cold tolerance, consider following these tips for successful growth and care:\n- Plant in a location that receives plenty of sunlight, even during the winter months.\n- Use well-draining soil and add a slow-release fertilizer before planting.\n- Do not overwater the plants, as this can lead to root rot.\n- During snow and frost, lightly brush off any weight from the blooms to prevent the stems from breaking.\n- Remove dead flowers to encourage new growth.\n- Inject a balanced liquid fertilizer every two weeks to promote continuous blooming.\nPansy varieties with high cold tolerance are an excellent choice for gardeners in regions with long winters. They come in several color variations and are proven to thrive in intensely chilly weather. By following the tips provided above, your winter garden will burst with colorful pansies.\nAs we come to the end of our exploration on how cold can pansies tolerate, we can conclude that pansies indeed are sturdy cold-tolerant plants. With proper care and attention, they can withstand temperatures as low as 5°f (-15°c) and still bloom beautifully.\nAlthough pansies are rugged and resilient plants, gardeners should be mindful of the temperature fluctuations and exposure to extreme cold. Ensuring that your pansies are planted in well-draining soil, lightly fertilized, and adequately hydrated can enhance their cold tolerance. Remember to provide protection from harsh winter elements like frost, snow, and ice by covering them with a cloth or mulch.\nPansies are perfect for adding vibrant colors to your winter landscape, and with these tips, you can enjoy them thriving throughout the colder months. Happy planting!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:f4fef257-c053-4976-a403-cf65e0115b9b>","<urn:uuid:d60e6f0b-9b83-4d92-b5ff-c0b5ad4872dc>"],"error":null}
{"question":"What is the relationship between green jobs and sustainability careers, and how do organizations measure their progress in implementing sustainable practices?","answer":"Green jobs, as defined by the Bureau of Labor Statistics, are jobs that produce goods or provide services that benefit the environment or use fewer natural resources. These are actually a subset of sustainability jobs, which encompass the broader 'triple bottom line' of environment, equity, and economy. Organizations can measure their progress in implementing sustainable practices through various methods, from free online tools to comprehensive third-party programs. These programs can include detailed analysis and industry benchmarking to recognize different sector needs. Companies can track specific metrics like greenhouse gas emissions, and some organizations achieve carbon-balanced status through a combination of reducing emissions and purchasing carbon offsets. Progress can be documented annually to inform future sustainability goals.","context":["Green and Sustainability Jobs and Career Resources\nBy Debra Rowe\nStudent interest in creating a greener and more sustainable future has never been higher. Our urgent societal employment and environmental challenges have fueled this student interest. In response to student demand, Princeton Review now ranks colleges on their green and sustainability initiatives. Green and sustainability focused curricula are showing up as new majors, minors, degrees, certificates, and continuing education programs in higher education. Over 1800 interdisciplinary environmental and sustainability programs now exist in higher education within the United States, according to a study recently conducted by the National Council for Science and the Environment. Furthermore, according to the college graduate-focused job resource, MonsterTRAK (now MonsterCollege), 80% of young professionals would like to work in a green job. Career development professionals are uniquely well positioned to respond to this increasing demand.\nGreen and sustainability jobs go beyond the solar designer or the wind generator installer that many people characterize as a green job. Green jobs are defined by the Bureau of Labor Statistics as jobs…that produce goods or provide services that benefit the environment or use fewer natural resources. This means that any job where the people are making decisions about what resources to consume are green jobs, from simple items like ordering office supplies to complex product development.\nGreen jobs are a subset of sustainability jobs. Sustainability jobs and careers are about implementing the “triple bottom line of sustainability”. The triple bottom line is often described as “people, planet and prosperity” or “environment, equity and economy”. Whatever the choice of words, sustainability is about making smarter decisions so our society can have healthy ecosystems, improved quality of life, and vibrant economies. Sustainability jobs span all types of companies, non-profits and government entities, and encompass a very large realm of environmentally and socially responsible professional career pathways.\nStudents need information about both career pathways and job openings. Three networks, the Higher Education Associations Sustainability Consortium (HEASC), the Disciplinary Associations Network for Sustainability, and the US Partnership for Education for Sustainable Development have worked with multiple HEASC Sustainability Fellows to compile a listing of resources. These networks are made up of over 42 national higher education associations and all have been requesting quality information on green and sustainability career pathways and job openings. The Fellows have therefore compiled a listing of over five dozen sources of information for use on campuses across the country. These resources will be also updated regularly. This easy to read document includes both job boards and career pathway resources that can help students explore which green and sustainability careers they are interested in, what competencies are desired for different career pathways, and what jobs are presently available both locally and nationally.\nThis resource can be shared prominently in your career center. The logos of the above organizations can be used as graphics/poster materials to raise student awareness when they enter the career center. (Many more images are free and downloadable from the web or can be taken from the set of sites on the list.) Career professionals on campus can take the lead to share these resources with the following target audiences on campus:\n- career center staff,\n- academic advisors in all areas,\n- faculty in all departments,\n- student life and residential life staff.\nThe national trend in higher education regarding sustainability has made clear that each academic discipline has a unique and important perspective to bring to the creation of solutions to our shared sustainability challenges. There are career pathways in sustainability from each academic area so it is important that faculty and academic advisors are aware of these sustainability resources. Oftentimes they are the ones who have to first respond to a student’s interest in sustainability. Academic associations have recognized this (see Participating Associations at http://dans.aashe.org/content/participating-associations) and are increasingly posting sustainability resources on their own sites, but the career resources are often overlooked, so sharing them internally on campus with faculty and staff can be very useful.\nFurthermore, student life activities and residential life activities are often related to sustainability (see the resources page at http://heasc.aashe.org/content/heasc-resource-center), yet the career resources are often not connected, so sharing this link with student life and residential life staff could be very helpful.\nA partial list follows of the hundreds of careers/jobs where green and sustainability competencies are increasingly valued:\n- Energy Efficiency and Renewable Energy Products Financiers, Manufacturers, Distributors, Contractors, Retailers, Salespeople and Installers\n- Energy/Sustainability Policy Analysts and Advocacy Specialists\n- Non-Profit Sustainability Related Educator/Program Manager\n- Greenhouse Gas Analyst/Broker\n- Brownfields Real Estate Developer\n- Environmental Economist\n- Sustainability Entrepreneur\n- Socially Responsible Investment Advisor\n- Sustainability Related Social Media Specialist\n- Sustainable Neighborhoods Community Organizer\n- Environmental Journalist\n- Permaculture Designer/Contractor or Sustainable Landscape Architect\n- Climate Change Risk Assessor and Mitigation Expert\n- Sustainable Transportation Planner\n- Federal, state and Local Energy and Other Sustainability Related Staff (e.g. economic development, workforce development)\n- Corporate Social Responsibility Staff\n- Corporate Sustainability Staff\n- Small Business Buyer/Manager\n- Sustainability Oriented Purchasing Agent and Business VP\n- Energy Manager\n- Facilities Director/Maintenance Staff\n- Materials Scientist\n- Environmental Engineering Technician\n- Biomass Plant Designer, Manager, Technician…\n- Utility Plant Operatives\n- Heating and Air Conditioning/ Building Automation Technician Controls Specialist\n- Refuse & Recycling Worker\n- Sustainable Agriculture: farmer, distributor, marketer…\n- Risk Assessor\n- Property Assessor\n- Building Codes Inspector\n- City/Community Manager\n- Operations Manager\n- Energy Storage Specialists (e.g. Smart grid, hydrogen, batteries, and compressed air)\n- Water Reservoir and Watershed Engineer/Technician\n- Green Building Designers, Distributor, Installer …\n- Energy Statistician\n- Recycling Director\nThis listing provides just a partial introduction to the many sustainability jobs presently in the marketplace. Job searches regularly provide hundreds of openings in any given region, and some of the more exciting sustainability jobs (e.g. ride sharing platform designers, local currency and time banks, deconstruction business owners, micro-financing developer) are being created by graduates as they see the community’s needs. Career advisors are uniquely suited to help students in any academic discipline envision and find pathways to a more sustainable future for all.\nDebra Rowe, Ph.D., is the President of the U.S. Partnership for Education for Sustainable Development. She is also co-founder of the Higher Education Associations Sustainability Consortium, founder/facilitator of the Disciplinary Associations’ Network for Sustainability, Senior Fellow at Second Nature, and Senior Advisor to the Association for the Advancement of Sustainability in Higher Education. She has also been professor of energy management, renewable energy, and sustainable products for over 30 years at Oakland Community College in Michigan. Debra presently chairs the Technical Advisory Group and the Green Jobs Policy Community of Action for the American Association of Community Colleges (AACC), where she is also a U.S. Designee to the World Federation of Colleges and Polytechnics’ for their international sustainability group. She is often a keynote speaker at national and international education conferences and is the author or editor of numerous publications. Debra may be contacted at firstname.lastname@example.org.\nDisclaimer: The opinions expressed in the comments shown above are those of the\nindividual comment authors and do not reflect the opinions of this organization.\n< Back | Printer Friendly Page","Now that green design and LEED certification are household terms, architects and interior designers are, by necessity, more fluent than ever in sustainable design strategies. But how well are we walking the talk? Firms of all sizes increasingly are putting their businesses under the microscope and asking how they can do more to align their practices with their design principles. While firms entering this new territory of greening their practices can choose a variety of paths, some helpful guidelines are emerging.\nThanks to municipal programs and simple pragmatism, basic practices such as recycling and the use of energy-efficient lighting are second nature for most businesses. Many firms go a few steps further, replacing paper cups with mugs, purchasing environmentally friendly office supplies, and allowing for telecommuting. But realizing meaningful reductions in a business’ carbon footprint requires a deeper commitment and, often, outside assistance. If there is one point to make about greening your practice, however, it is that the benefits for your business—in terms of office morale, cost savings, and marketing—will clearly outweigh the investment.\nAiming Lower and Lower\nDocumenting a business’ practices is essential for setting goals and assessing progress, and while free online tools exist, third-party programs can save time and give added credibility. Programs vary from a checklist of green practices to detailed analysis, both audited and not, and many include industry benchmarking that recognizes, for example, the differing water demands of hotels and offices. In addition to considering program costs, which range from none to thousands of dollars, we were interested in obtaining solid data about our emissions since our goal was to be carbon balanced. We chose to work with Climate Smart, a Vancouver, Canada–based company that helps businesses monitor, reduce and offset their greenhouse gas (GHG) emissions.\nAs a close-knit, medium-sized firm, we already had a range of sustainable business practices in place, including making notebooks from used paper, providing bicycle facilities, holding family-style office lunches (to reduce packaging waste), and Web conferencing (to reduce meeting travel). We also own our LEED Silver–certified historic office building, which gave us much broader control over building operations.\nClimate Smart’s training and Web-based tools helped us document and calculate our GHG emissions for 2009 and to identify strategies for further reducing our carbon footprint. Not surprisingly, we found that business travel, though mostly limited to the West Coast region, comprised more than half of our emissions. We achieved carbon-balanced status by purchasing carbon offsets through San Francisco-based TerraPass, but we clearly had room for improvement. We saw this challenge as an opportunity to engage the entire office and to further cultivate the leadership of the younger staff members who had guided the initiative from the outset.\nOur sustainable design team mapped out a year-long action plan that included encouraging car-free commuting, composting, implementing sustainable purchasing practices, and further reducing our use of energy, water, office supplies and air travel. Of course, there have been some bumps in the road—for example, convincing architects to replace their preferred pens with a refillable alternative or weighing the environmental costs of tablet readers, a quickly outdated device, against printing on recycled paper. But when we recalculate our annual GHG emissions this summer, we expect to see improved performance—requiring fewer carbon offsets—that will inform our goals for the coming year.\nPositive Peer Pressure\nFor Cambridge, Mass.-based Tsoi/Kobus & Associates (TK&A), joining the Challenge for Sustainability—a green business initiative sponsored by local nonprofit A Better City—opened up a new network of like-minded businesses. The Challenge uses similar tools to reduce their members’ carbon footprints, but rather than certification, staff work with businesses to set annual goals for improvement. More uniquely, the Challenge hosts regular programs that provide a forum for networking and discussion, as well as an opportunity to learn how different industries are adapting.\nBlake Jackson, TK&A’s sustainability practice leader, hopes that this knowledge sharing will help to address the added hurdles faced by businesses in leased offices. Tenants can have limited control over cleaning, waste removal, water, and other services managed by the landlord, yet most green business programs factor these practices.\nThe apples-and-oranges nature of green business certification programs eventually may drive demand for some industry standards. Until then, some simple guidelines will help firms get started:\n• Lead by supporting leaders within. Cultivating a green business culture requires a team of smart, enthusiastic individuals and clear support from senior principals. Firms with multiple offices should establish teams in every office—and encourage some friendly competition.\n• Find a good fit. Choose a program that fits your goals, resources, and culture. In some cases, beginning informally can help build a crucial base of support for more ambitious initiatives.\n• Take a long view. Establishing achievable goals each year will help ensure your success over time.\n• Celebrate. Greening your practice truly involves every staff member. Take time to recognize your collective achievements.\nClimate Smart: climatesmartbusiness.com\nA Better City Challenge for Sustainability: www.abettercity.org/environment/challenge.html"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:fe8543f2-136e-47ae-b483-27df1910e8e8>","<urn:uuid:89bdbeae-92ec-4a0f-aa11-cfeaefa3dafa>"],"error":null}
{"question":"What types of unmanned systems are being developed for naval warfare, and how are they tested in controlled environments?","answer":"Naval unmanned systems are being developed across multiple domains - air, surface, and undersea vessels that can operate in coordinated networks under human control. These platforms can be equipped with various weapons including torpedoes, lasers, and missiles, while also conducting reconnaissance missions. For testing these systems, facilities like the National Maritime Research Institute's Actual Sea Model Basin are used to conduct controlled experiments, particularly for autonomous underwater vehicles (AUVs). These test environments allow for prototype development and validation of formation control algorithms before deploying systems in actual ocean conditions.","context":["Warrior Video Above: USS Zumwalt Commander Capt. Carlson Describes Riding the Stealthy Ship in Stormy Seas\nbyKris Osborn- Warrior Maven\n(Washington D.C.) It may be far too early for the U.S. Navy to cultivate any kind of integrated picture of what its Future Surface Combatant may ultimately look like, given that the ship is not anticipated to arrive until the 2030s. However, the Navy’s plan for new generation of destroyers invites a consideration of certain key technical trends and tactical goals likely agreed upon by Navy futurists now exploring conceptual work on the ship.\nA few things can unequivocally be understood, by virtue of Navy strategies, plans and current modernization activities. The service plans to extend its successful fleet of DDG 51 Arleigh Burke class destroyers for many years to come, and it seems clear that current near-term modernization trends are likely to shape any conceptual approach to 2030.\nMany elements of the existing Zumwalt-class and DDG 51Flight III destroyers arguably inform the conceptual foundation of what the Navy envisions for a new generation of destroyers. There are what might be considered somewhat self-evident areas of focus, such as power-scaling lasers, and newer networked command and control for the operation of ocean drones. Other considerations include advances in radar sensitivity such as the SPY6 and EW and laying the technical foundation to accommodate a new generation of emerging weapons. Perhaps of most critical importance, cyber hardening and the expansion of AI-empowered computer systems. As for stealth, that seems like very much of an open question, as it seems that it would be challenging to merge some of the stealthy characteristics of Zumwalt with the massively armed Flight III DDG 51 destroyers. After all, DDG 51s rely upon a wide range of externally-mounted sensors, antennae, deck-launched weapons and mast configurations.\nSeveral existing areas of technical examination seem to present themselves as areas of likely focus, as they offer a foundation upon which subsequent future systems could be evolved. One obvious weapon is lasers, as there is much room for growth. Current work is geared toward power-scaling and form-factor miniaturization, two trajectories intended to expand the tactical envelope for lasers. Stronger beams, involving the merging of several beams into a single weapon, can destroy larger objects, increase range and vastly improve the effects “scalability” of lasers. For instance, larger, more powerful lasers can adjust from “disrupt” or “damage” mode to “destroy” mode, depending upon the application. Strong lasers could take out enemy helicopters, surface combatants or even fighter jets. Also, the Navy is doing early work on hardened, longer-range laser weapons perhaps being able to perform advanced missile defense missions at much higher altitudes, or lasers less prone to weather-induced beam attenuation. Of course, lasers are cheaper than interceptor missiles, and advances in electrical power projection such as the Zumwalt’s electrical propulsion Integrated Power System offers itself as a potential window of exploration. In addition, AI-enabled integrated ship defenses should be a huge area of focus, as advanced algorithms are increasingly able to gather, organize, analyze and transmit near-real time combat-sensitive information. Enabled by a limitless database of historical scenarios, available weapons and specific enemy threats, computer programs could instantly perform analytics on multiple variables at one time, all while enemy attacks are incoming, to give commanders immediate information regarding which defensive or counterattack options would best fit the situation.\nAlong with lasers and on board electrical power able to accommodate more computing power and new weapons systems, autonomy and drone-focused command and control will likely figure prominently. For instance, the Navy is looking much more closely at employing small fleets of unmanned surface and undersea vessels, to be controlled by “motherships” where humans perform command and control. This is already underway with the Navy’s amphibious attack strategy and could easily be applied to destroyers and Carrier Strike Groups. Perhaps a future destroyer could command large numbers of networked, forward operating air, surface and undersea drones all performing coordinated missions with advanced levels of autonomy. Under human control, they could fire weapons such as torpedoes, lasers or even some interceptor missiles, something the Navy is already exploring. They could more safely conduct undersea, surface or aerial reconnaissance as well. Why couldn’t some kind of medium-to-large sized unmanned platform contain Vertical Launch Systems, deck-mounted guns or surface-fired long-range missiles?\nWeapons and sensor range, cross-domain networking, autonomy and especially drones will undoubtedly rule the day. This will certainly be the case when looking at current technical areas of promise, anticipated future maritime warfare environments and methods of confronting the many as-of-yet-unknown mysteries of future warfare. It would not be a stretch to recognize that AI, drones, networking and electrical power will all figure prominently.\nKris Osborn is the new Defense Editor for the National Interest. Osborn previously served at the Pentagon as a Highly Qualified Expert with the Office of the Assistant Secretary of the Army—Acquisition, Logistics & Technology. Osborn has also worked as an anchor and on-air military specialist at national TV networks. He has appeared as a guest military expert on Fox News, MSNBC, The Military Channel, and The History Channel. He also has a Masters Degree in Comparative Literature from Columbia University.","Development of Testbed AUV for Formation Control and its Fundamental Experiment in Actual Sea Model Basin\nAkihiro Okamoto, Motonobu Imasato, Shunka C. Hirao, Hidenori Sekiguchi, Takahiro Seta, Masahiko Sasano, and Toshifumi Fujiwara\nOffshore Advanced Technology Department, National Maritime Research Institute, National Institute of Maritime, Port, and Aviation Technology\n6-38-1 Shinkawa, Mitaka, Tokyo 181-0004, Japan\nThe formation control of multiple autonomous underwater vehicles (AUVs) is increasingly becoming a vital factor in enhancing the efficiency of ocean resources exploration. However, it is currently difficult to deploy such a package of AUVs for operation at sea because of their large size. The aim of our study is to create a demonstration system for formation control algorithms using actual hardware. To implement a prototype system, we developed a testbed AUV usable in a test basin and performed a simple formation control test in the Actual Sea Model Basin of the National Maritime Research Institute, Japan. Two AUVs, the simulated “virtual” leader and the developed “real” follower, communicate through an acoustic link and hence cruise to maintain a constant distance between them. Tests for more sophisticated formation control algorithms will be enabled using the system; consequently rapid implementation at sea will be realized.\n-  K. Kim and T. Ura, “A Cruising AUV r2D4: Intelligent Multirole Platform for Deep-Sea Survey,” J. Robot. Mechatron., Vol.26, No.2, pp. 262-263, 2014.\n-  Y. Nishida, T. Ura, T. Nakatani, T. Sakamaki, J. Kojima, Y. Itoh, and K. Kim, “Autonomous Underwater Vehicle “Tuna-Sand” for Image Observation of the Seafloor at a Low Altitude,” J. Robot. Mechatron., Vol.26, No.4, pp. 519-521, 2014.\n-  Y. Nishida, K. Nagahashi, T. Sato, A. Bodenmann, B. Thornton, A. Asada, and T. Ura, “Autonomous Underwater Vehicle “BOSS-A” for Acoustic and Visual Survey of Manganese Crusts,” J. Robot. Mechatron., Vol.28, No.1, pp. 91-94, 2016.\n-  T. Maki, Y. Noguchi, Y. Kuranaga, K. Masuda, T. Sakamaki, M. Humblet, and Y. Furushima, “Low-Altitude and High-Speed Terrain Tracking Method for Lightweight AUVs,” J. Robot. Mechatron., Vol.30, No.6, pp. 971-979, 2018.\n-  A. Okamoto, T. Seta, M. Sasano, S. Inoue, and T. Ura, “Visual and Autonomous Survey of Hydrothermal Vents Using a Hovering-Type AUV: Launching Hobalin Into the Western Offshore of Kumejima Island,” Geochemistry, Geophysics, Geosystems, Vol.20, No.12, pp. 6234-6243, 2019.\n-  Y. Nishida, T. Sonoda, S. Yasukawa, K. Nagano, M. Minami, K. Ishii, and T. Ura, “Underwater Platform for Intelligent Robotics and its Application in Two Visual Tracking System,” J. Robot. Mechatron., Vol.30, No.2, pp. 238-247, 2018.\n-  T. Sato, K. Kim, S. Inaba, T. Matsuda, S. Takashima, A. Oono, D. Takahashi, K. Oota, and N. Takatsuki, “Exploring Hydrothermal Deposits with Multiple Autonomous Underwater Vehicles,” Proc. of the 2019 IEEE Underwater Technology (UT), Kaohsiung, Taiwan, pp. 1-5, 2019.\n-  A. P. Aguiar and J. P. Hespanha, “Trajectory-Tracking and Path-Following of Underactuated Autonomous Vehicles with Parametric Modeling Uncertainty,” Proc. of the IEEE Trans. Automatic Control, Vol.52, No.8, pp. 1362-1379, 2007.\n-  P. Millán, L. Orihuela, I. Jurado, and F. R. Rubio, “Formation Control of Autonomous Underwater Vehicles Subject to Communication Delays,” IEEE Trans. on Control Systems Technology, Vol.22, No.2, pp. 770-777, 2014.\n-  B. Das, B. Subudhi, and B. B. Pati, “Adaptive sliding mode formation control of multiple underwater robots,” Archives of Control Sciences, Vol.24, No.4, pp. 515-543, 2014.\n-  P. Ghorbanian, S. G. Nersesov, and H. Ashrafiuon, “Obstacle avoidance in multi-vehicle coordinated motion via stabilization of time-varying sets,” Proc. of the 2011 American Control Conf., pp. 3381-3386, 2011.\n-  X. Xiang, B. Jouvencel, and O. Parodi, “Coordinated Formation Control of Multiple Autonomous Underwater Vehicles for Pipeline Inspection,” Int. J. Advanced Robotic Systems, Vol.7, No.1, pp. 75-84, 2010.\n-  S. Li, X. Wang, and L. Zhang, “Finite-Time Output Feedback Tracking Control for Autonomous Underwater Vehicles,” J. Oceanic Engineering, Vol.40, No.3, pp. 727-751, 2015.\n-  P. L. Kempker, A. C. M. Ran, and J. H. van Schuppen, “A formation flying algorithm for autonomous underwater vehicles,” Proc. of the 2011 50th IEEE Conf. on Decision and Control and European Control Conf., Orlando, FL, pp. 1293-1298, 2011.\n-  Z. Yan, D. Xu, T. Chen, W. Zhang, and Y. Liu, “Leader-Follower Formation Control of UUVs with Model Uncertainties, Current Disturbances, and Unstable Communication,” Sensors, Vol.18, No.2, 662, 2018.\n-  J. A. Neasham, G. Goodfellow, and R. Sharphouse, “Development of the “Seatrac” miniature acoustic modem and USBL positioning units for subsea robotics and diver applications,” Proc. OCEANS 2015 – Genova, pp. 1-8, 2015.\n-  T. Seta, A. Okamoto, S. Inaba, and M. Sasano, “Development of a new operating system software for a hovering-type autonomous underwater vehicle HOBALIN,” Proc. of the 2017 11th Asian Control Conf. (ASCC), Gold Coast, QLD, pp. 37-42, 2017.\n-  K. Tanizawa, M. Ueno, H. Taguchi, and T. Fujiwara, “Actual Sea Model Basin in NMRI (National Maritime Research Institute),” Marine Engineering, Vol.48, No.6, pp. 776-781, 2013 (in Japanese).\n-  M. Tsujimoto, M. Kuroda, K. Shiraishi, Y. Ichinose, and N. Sogihara, “Verification on the Resistance Test in Waves Using the Actual Sea Model Basin,” J. Japan Society of Naval Architects and Ocean Engineers, Vol.16, pp. 33-39, 2012."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:1c9eb0ca-307d-487c-a3cc-0ad0ca4685b8>","<urn:uuid:fe20762a-a227-4c73-9ad9-ab586f2d6e0c>"],"error":null}
{"question":"Which infection can spread more quickly: osteomyelitis or cellulitis?","answer":"Cellulitis spreads more quickly. While osteomyelitis is a serious bone infection that develops gradually and may take several weeks to become visible on X-rays, cellulitis is described as a quickly spreading infection of the skin and underlying tissues. Cellulitis requires immediate medical attention if spreading, as it can rapidly lead to dangerous complications like blood infection (sepsis), bone infection (osteomyelitis), or heart infection (endocarditis).","context":["People often aren’t aware of osteomyelitis meaning, therefore, we would like to answer that common question today. So, osteomyelitis refers to a bone disease that is infectious and inflammatory of bone that is often of bacterial origin and is marked by local death and separation of tissue.\nA rare but serious condition, osteomyelitis meaning refers to a bone infection. Bone can become infected, either through the spread of infection from one part of the body to the bone or through an open fracture that exposes the bone.\nIn children, osteomyelitis meaning occurs in the long bones of legs and upper arm, while adults generally develop osteomyelitis in the bones of the spinal vertebrae. Also, people with diabetes who have foot ulcers can develop osteomyelitis in their feet.\nOsteomyelitis meaning was earlier referred and associated with an untreatable condition; today it has a successful treatment plan.\nWhat are the causes of the disorder?\nAs mentioned above, osteomyelitis develops when bones develop an infection. Infection may occur in two ways:\n- The result of injury (Contiguous osteomyelitis) like during surgery, or from an animal bite or fractured bone; more common in adults.\n- Through the bloodstream (hematogenous osteomyelitis), which is more common in children. Conditions that affect the blood supply to certain parts of the body (diabetes) or weaken the immune system (rheumatoid arthritis) increase the chances of developing osteomyelitis.\nThe disorder can lead to a chronic form of osteomyelitis if it is not treated quickly because bones become permanently damaged, causing loss of function and persistent pain.\nWhat one needs to know about symptoms or signs?\nApart from Osteomyelitis Meaning, know m0re about Symptoms include:\n- High fever\n- Bone pain\n- Swelling, redness and warm sensation in the area\nThough bones in the legs are most commonly affected, other bones like those of the arms or back can also be affected.\nWhich specialist should be consulted in case of signs and symptoms?\nPeople experiencing symptoms similar to those of osteomyelitis should consult an orthopedic surgeon or a doctor who specializes in infectious diseases and who understands osteomyelitis meaning in medical terms.\nWhat are the screening tests and investigations done to confirm or rule out the disorder?\nDiagnosis comprises tests to confirm osteomyelitis and also identifying the germ causing the infection. It is done through:\n- Blood tests – Blood tests cannot confirm osteomyelitis but can check for higher-than-normal levels of white blood cells and other factors that indicate infection, and also reveal the type of germs if the infection is in blood.\n- Imaging tests – X-rays can detect any damage to the bone though this can be visible if osteomyelitis has been present for several weeks. Computerised tomography (CT) scans and magnetic resonance imaging (MRI) are more detailed tests that look at the bone and surrounding soft tissues for abnormalities.\n- Bone biopsy – This is by far the most confirmatory test for osteomyelitis as it can specify the type of organism responsible for the condition. This is useful in determining the antibiotic to be prescribed for treatment. An open biopsy is conducted under anaesthesia and surgery to reach the bone. Alternatively, a biopsy sample may be taken by inserting a long needle through the skin, using x-rays or other imaging scans to guide through the process.\nWhat treatment modalities are available for management of the disorder?\nCommon treatment options include antibiotics and surgery to remove portions of the bone that are infected or dead.\n- Medications – The right antibiotic to be prescribed is determined through the results of the bone biopsy. The antibiotics are injected through the vein in the arm, and the treatment may last for four to six weeks. More serious infections may require a supplemental course of oral antibiotics. Side effects of antibiotics include nausea, vomiting and diarrhea.\n- Surgery – In severe cases, surgery may be required. The following procedures may be followed:\n- Draining the infected area – The area around the infected bone is opened up and fluid or pus accumulated within id drained out.\n- Removal of the diseased bone and tissue (Debridement) – The surgeon removes the diseased bone part along with a small part of the healthy bone and surrounding tissue as a precautionary measure to eliminate all traces of infection.\n- Restoration of blood flow to bone – The empty space left due to the removal of bone is filled with a piece of bone or tissue like the skin or muscle from another part of the body. Alternatively, temporary fillers are placed in the area till a separate graft is placed.\n- Amputation of the limb – If the infection cannot be contained, the affected limb may need to be amputated.\n- Hyperbaric oxygen therapy – Very difficult-to-treat cases of osteomyelitis are given hyperbaric therapy in which excess oxygen is delivered to the bone using a pressure chamber. This is done to promote healing.\nWhat are the known complications in the management of the disorder?\nOsteomyelitis meaning: Complications during treatment may include:\n- People with underlying conditions like diabetes (which increases the risk of osteomyelitis) run the risk of recurrent infections.\n- Amputation of the affected limb may become necessary if the blood supply to the area is severely reduced.\nWhat precautions or steps are necessary to stay healthy and happy during the treatment?\nThe following measures can ensure that the patient receives the best possible treatment:\n- Patient must discuss his/her medical history or any current health conditions like diabetes with the doctor. For example, people with diabetes must monitor the feet closely and should contact their doctor at the first signs of infection.\n- Early treatment of osteomyelitis prevents the condition from becoming chronic. Further, it also ensures better pain management and reduces the risk of recurrent infections.\nAlso, read about : Air Quality Index Delhi\nHow can the disorder be prevented from happening or recurring?\nIt is always good to be aware of the different diseases and understand osteomyelitis meaning by visiting your nearest doctor on seeing the first symptoms of the disease. Though it is not always possible to prevent osteomyelitis, a person can reduce his/her risk of contracting the infection by\n- Cleaning all wounds thoroughly and dressing them in clean bandages.\n- Maintaining good health for a strong immune system.\n“Osteomyelitis,” Medline Plus, NLM, NIH, https://www.nlm.nih.gov/medlineplus/ency/article/000437.htm\n“Osteomyelitis,” WebMD.com, https://www.webmd.com/pain-management/osteomyeltis-treatment-diagnosis-symptoms\n“Osteomyelitis,” MayoClinic.com, Mayo Clinic Staff, https://www.mayoclinic.org/diseases-conditions/osteomyelitis/basics/treatment/con-20025518\n“Osteomyelitis,” NHS.uk, https://www.nhs.uk/conditions/osteomyelitis/Pages/Introduction.aspx","Cellulitis is one of the most painful, quickly spreading and potentially deadly types of infections that can be caused by Staph or MRSA. Because these infections are becoming more common, it’s best to know what to look for and what to do if you think you have it. MRSA cellulitis can take a long time to get rid of. And like other types of MRSA and Staph, it is prone to recurring and can be difficult to treat.\nFortunately, there are both antibiotic drug and potent alternative options for controlling cellulitis. Becoming more familiar with the symptoms, risk factors and treatment options below is a good first step if you’re struggling with an infection.\nWhat is cellulitis?\nCellulitis is a deep infection of the skin and the underlying tissues. If left unchecked, cellulitis can quickly lead to more dangerous infections of the blood (sepsis), bone (osteomyelitis) and heart (endocarditis) and in some cases tissue death (gangrene).\nCellulitis infections are most common on the lower legs, face and arms, but they can occur anywhere on the body. Children often get this infection on their bottom. The most common symptoms are red, warm and tender skin, but these infections can have many other symptoms as well. Looking a cellulitis pictures can be a helpful tool. Possible warning signs and indications of cellulitis include:\n- Redness of the skin area, which often grows in size over time\n- Swelling or inflammation of the area, which often gets bigger over time\n- Red streaks radiating outward may occur, which often spread outward over time\n- Pain and tenderness in the area\n- Skin that is warm to the touch in the infected area.\n- Skin sores, bumps and small pimples can sometimes occur\n- A rash or red patches in the area may occur\n- Skin sometimes appears stretched, tight or shiny in the area\n- Fever, chills, sweating or shaking can occur\n- Feeling ill, weak or having muscle aches can occur\n- Nausea, joint stiffness and hair loss in the area may occur in some cases\nWho’s at risk?\nThere are many risk factors for cellulitis, some more significant than others. Any of the factors below, either by themselves or in combination, can increase the risk of catching cellulitis:\n- Any cuts, scrapes, cracks, burns, blisters or open wounds on the skin provide an easy entry for bacteria.\n- Surgical wounds, catheter sites and IV injection sites are very common places to get cellulitis and other infections.\n- Dry or peeling skin, Athlete’s foot, eczema, shingles, chickenpox and other skin conditions can make your skin more vulnerable to getting infected.\n- Insect or animal bites or stings can easily become infected.\n- The elderly and people with weakened immune system have a higher risk of cellulitis and other infections.\n- A long term stay in a hospital, nursing home or other healthcare facility can increase your exposure to MRSA and many other infections.\n- Taking medications that depress the immune system, including corticosteroids and antirejection drugs, make you more prone to infections in general.\n- Ulcers caused by Diabetes or vascular disease are a risk factor.\n- Swollen legs or arms caused by either medical conditions (such as cirrhosis, scleroderma or congestive heart failure) or by medications (such as diuretics or water pills) increase the risks of cellulitis.\n- Conditions that weaken the immune system, including diabetes, HIV/AIDS, circulatory disorders, liver disease, kidney disease and leukemia can make you more prone to many types of infection.\n- IV drug use is another risk factor for many types of infection.\nWhat should you do?\nCellulitis can be quite dangerous and should be taken very seriously. If you think you have cellulitis, it’s best to get medical attention as soon as possible. If the infection is spreading or growing, or if it causes chills or fever, it’s crucial to get immediate medical attention. These infections are even more serious if they occur on your face or in other sensitive areas of the body.\nAntibiotics are the most common mainstream medical treatment for cellulitis. Mild cases are often treated with oral antibiotics that can be taken at home. Moderate and severe infections are usually treated with IV antibiotics in a hospital. Recovery can take a few days to several months, depending on the severity of the infection and other complicating health problems.\nWhile there are several natural remedies that can be used for treatment, it’s best to be under the close supervision of a doctor for these challenging infections, regardless of which treatments you use. Natural remedies of high potency taken in larger doses are usually needed. Because higher doses of any natural product can lead to sensitivity, detoxification, liver toxicity and other symptoms, it’s best if your doctor is familiar with these symptoms and the particular remedy you are using.\nShort-term use of therapeutic grade oregano oil taken internally in moderate to high doses is one natural method that has been used. Other essential oils, such as ravensara, clove and cinnamon bark, have been used internally for challenging infections, along with milder oils, such as tea tree, for external skin support. High doses of stabilized allicin and specialized medicinal herbal blends, such as Olive Leaf extract, have also been used for these infections. As with any natural remedy, treatment alone is usually not enough. Additional steps should be taken to boost the immune system and repair damage done by bacteria and drugs, especially for severe infections.\nDepending on the circumstances, both antibiotic drugs and natural remedies have been used together. Getting your mainstream medical doctor to collaborate with your Naturopathic Doctor is one of the best ways to get comprehensive holistic health care.\nTo your best health,\nMicrobiologist and Natural Health Expert"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:1fef3ef5-151d-43b5-8313-5359d58d5632>","<urn:uuid:24d53e73-3fdb-4fe8-9d31-e9814c6e055a>"],"error":null}
{"question":"Which city in Israel is known for cybersecurity innovation?","answer":"Tel Aviv in Israel is an established, successful location for cyber innovation and keeping data safe.","context":["From Sacramento to Songdo, from Dundee to your den, we’ve identified 21 places around the world where the future of work is being built right now, reveals Robert Brown, a futurist in Cognizant’s Center for the Future of Work.\nOver the last few years, we’ve published a series of reports describing what we think will be the jobs of the future in this, the age of automation, algorithms and artificial intelligence. In this report, we outline where many of these new jobs will appear, starting with the insight that jobs of the future often stem from unlikely places.\nThe cities and towns that we profile range from the large to the small, the old to the new, and the well-known to the little known. What they have in common is that they’re hotbeds of innovation and new ideas, offer vast and intriguing cultural and lifestyle amenities, and have a demonstrated propensity to create — or recreate — a future by offering their citizens the work of the future.\nThe following figure illustrates the complex methodology we used. To learn more, read our full report, “21 Places of the Future.”\nNote that the places we feature often anchor on one key technology or concept. For example:\n- Cybersecurity. Tel Aviv in Israel is an established, successful location for cyber innovation and keeping data safe.\n- Digital twins. Wellington, New Zealand, invested early in a cloud-based digital replica of its physical self and hastened its rebuild following a major 2016 earthquake.\n- Fintech. In Kenya, Nairobi’s digital advances have reinvented it as a highly adaptable financial services metropolis.\n- E-sports. Dundee, Scotland, has successfully fused the tech-heavy worlds of gaming and design.\n- Sustainability. India’s Kochi airport is powered entirely by solar energy.\n- Diversity and inclusion. Atlanta is one of the Western Hemisphere’s burgeoning innovation economies, sourcing some of the most diverse talent in the U.S.\n- Digital engineering. Shenzhen, China, showcases world-class lessons in the power of rapid prototyping at its Huaqiangbei electronics market.\n- Virtual workplaces. Remotopia (that is, our conceptual destination for remote work) features huge, cloud-based infrastructure investments that showcase the power of supporting millions of telecommuting employees with modern systems.\nThe 21 places\nThe map below reveals all 21 of our selections, and we’ve spotlighted just a few of them (along with their notable strengths and areas for enrichment according to our methodology), citing four locales from various continents — and one from much farther away.\nDa Nang, Vietnam\n- High points: Affordability, Environment\n- Need improvement: Talent pool, Culture & Entertainment\nThis melting pot of Vietnam’s old and new is morphing from a simple coastal town to a global destination, poised to become the next transformation jewel of Southeast Asia. A central coastal city that acted as the U.S. airbase during the Vietnam War, Da Nang is making strides toward becoming an eco-smart urban area, a hub for startups and innovation, and one of the most livable cities in Asia.\nMore than just a tourist hotspot, Da Nang is all about growth — massive growth — and it’s poised to become an investment destination for the world. Rampant construction and incipient skyscrapers are proof of a new Da Nang in the making. To stimulate post-COVID-19 consumer demand, Da Nang is eyeing stimulus investments to promote domestic businesses, and a restructuring of the tourism sector.\nThe next decade will also see Da Nang flooded with tech talent. The city has set up a Da Nang Business Incubator and is developing a startup training network. Its Information Technology Park is expected to generate US$1.5 billion in revenue per year and 25,000 jobs.\n- High points: Architecture, Infrastructure and more\n- Need improvement: Lifestyle, Tech Investment\nWith its rich history, Mediterranean climate and status as a world “capital of cool,” Portugal’s largest city is at once an aspirational tech innovator and a must-live city for Europe’s young workforce. From the ashes of the 2010 financial crisis, Lisbon has reinvented itself as a global hub of innovation and near-shore services for Continental Europe.\nThe growth of the city’s tech scene has been astounding. Portugal’s startup ecosystem is growing twice as fast as the European average, according to Startup Europe Partnership, and Lisbon is now one of the biggest startup hubs in Europe. The city is home to over 30 incubators and accelerators and nearly 50 co-working spaces, according to Invest Lisboa.\nTo live up to its promise, though, Lisbon must overcome structural issues stemming from previous governments, particularly its legacy bureaucracy. The city is striving to provide more public services through digital means and is investing in advanced technologies that support “digital by default” operations.\n- High points: Talent pool, Environment and more\n- Need improvement: Culture & Entertainment, Lifestyle\nWith the economic center of gravity in Africa shifting from the south to the north. Nairobi is emerging as a hub of innovation and culture — a future African superpower. This vibrant city is shaking off its colonial past and reinventing itself as a highly adaptable technology and financial services metropolis.\nThe coronavirus has served to highlight Nairobi’s adaptability; local authorities were quick to act, and local manufacturers quickly pivoted to the production of personal protective equipment.\nNairobi’s standing on the global stage is best portrayed by the amount of investment that it’s received. While foreign direct investment (FDI) is dropping globally because of the pandemic, Kenya is still one of the largest recipients of FDI in Africa, according to the UN, due largely to its technology initiatives.\nYoung Kenyans are flocking to Nairobi, and they’re infusing it with optimism, entrepreneurship and a drive to succeed. The city’s innovative and bustling ecosystem includes both established brands and tech startup incubators such as iHub and Nairobi Garage. More cars equal more traffic, so privately funded satellite cities are popping up around Nairobi, most notably Tatu City and Konza Technopolis.\n- High points: Potential!\n- Need improvement: Attainability\nThe idea of space as a place of the future may seem far-fetched, but in truth, a new age of exploration is upon us. Space is a new frontier of exploration and innovation of “new worlds” that will rival the Age of Exploration in the 15th and 16th centuries. And just as terra nova in America, Africa and Asia morphed over time into places like New York, Cape Town and Hong Kong, so too will the Moon, Mars, space stations and space hotels become “places” of the 21st century.\nCOVID-19 may be another factor encouraging some to seek pastures new. The (embryonic) space industry is already worth over $400 billion, according to the U.S. Federal Aviation Administration. Morgan Stanley forecasts a $1 trillion value worldwide by 2040. China wants an Earth-Moon space economic zone to generate $10 trillion in annual services by 2050. In 2019, the U.S. established the first new military service in over 70 years — the Space Force.\nSão Paolo, Brazil\n- High points: Infrastructure, Culture & Entertainment\n- Need improvement: Lifestyle, Environment\nFor many, Brazil conjures images of caipirinhas by the beach and dizzying displays on the soccer pitch. But while that may tell the story of nights and weekends, by day, Paulistanos (as locals self-identify) work just as hard as they play. São Paulo leads all of Brazil in GDP, scientific production, number of expatriates and artistic output. These factors have contributed to its status as the leading business hub in Latin America.\nAnd as the factors combine to spark its startup and innovation scene, São Paulo is primed to become a global leader in that arena, as well. The past 10 years in São Paulo have seen the opening of new offices for Google, Facebook, Airbnb and every other tech company looking to do business in Latin America. The creative and entrepreneurial talent cultivated at those offices has been a galvanizing force in São Paulo’s startup scene.\nThe future of work may not necessarily be in “places of the now,” like London or New York or Cupertino or Berlin or Bangalore. Increasingly, it’s going to be in places like those analyzed in this report. All of us potentially have the power to make our places fit for the future. We hope we’ve offered some ideas — perhaps controversial or at least unconventional — on how to go about doing that."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:f508d602-4299-43ff-9b4d-f2ad89374e32>"],"error":null}
{"question":"I've been studying 20th century photographers and their impact on documenting social issues. How did Margaret Bourke-White's coverage of racial segregation differ from Yousuf Karsh's role as an immigrant photographer in North America?","answer":"Bourke-White directly confronted racial issues through her photography, documenting segregation both in South Africa during Apartheid and in the American South during the 1950s. She captured contrasting images of white families representing the American way of life alongside photos of black people as secondary figures, creating visual narratives about racial inequality. Karsh, on the other hand, experienced persecution as an Armenian in Turkey before emigrating to Canada at age 16 in 1924, and rather than focusing on social issues, he became the official portrait photographer of the Canadian government in 1935 and gained citizenship in 1947, channeling his work primarily into formal portraiture of prominent figures.","context":["Exhibition dedicated to Margaret Bourke-White, one of the most representative and emblematic figures of photojournalism.\nIn honor of the life and work of Margaret Buorke-White, a woman ahead of her time, the Museo di Roma in Trastevere is hosting a retrospective exhibition with more than 100 images of the artist. Considered a pioneer in many moments of photography, Buorke-White started as a photographer dedicating herself to the world of industry and corporate projects. She was the first female photographer for the famous and influential Fortune and Life magazines. Recording important historical moments through photography, she was at the battlefront of the Second World War, and documented segregation and racial conflicts in South Africa and the United States. Buorke-White also portrayed 20th century political characters such as Stalin and Gandhi.\nThe exhibition is composed of 11 sessions that bring together photographs from each period registered by the photographer. After reading an introduction about the artist’s life, with texts available in both Italian and English, the exhibition’s first session is entitled L’incanto delle acciaierie. It shows Buorke-White’s first works, with closed images captioning the industrial process of steel mills and wide photos of cities seen from above. This phase of the artist portrays the American industrial growth in the first decades of the 20th century, and the big cities becoming the main economic centers of the country. In contrast, the next session, Conca di polvere, documents the social work done by the photographer portraying the years of the Great Depression, in the South of the United States.\nThe third section, LIFE, located in the museum’s outdoor area, is one of the only to show colourful works, documenting the work done by Buorke-White while collaborating with the famous American magazine LIFE. Filled with magazine covers and articles, and expressive photos taken by the artist, this session seems to take us back in time and experience the latest news from the past century.\nThen, the exhibition’s itinerary directs us to another wing of the museum where we can observe extremely political photographs. Being the first Western professional photographer permitted into the Soviet Union, Buorke-White provides us with photos of the soviet lifestyle in the 1930s, as well as figures of common and famous people — including the portrait of Stalin. Gradually, we reach the session where she was designated as the first female war correspondent in the Second World War. Having been on the battlefront, the photographs are intense and show the reality of the conflict.\nNei Campi, the sixth session of the exhibition, is probably the most emotional one. We witness the horrors of the Nazi concentration camps, as well as the moment of the liberation of the Buchenwald Concentration Camp in 1945. With strong images of people trapped and then reunited, putting an end to the nightmare, Buorke-White superbly captures the terror of war and its consequences.\nIn the next room, we turn to, firstly, the situation in India in the second half of the 1940s, and then a documentation on the Apartheid regime in South Africa. Having portrayed the violence that erupted at the Independence of India and its separation from Pakistan, Buorke-White was one of the last people to speak to and portray Gandhi, just hours before he was murdered in 1948. One of her most iconic photographs of the Indian popular leader at his spinning wheel is also displayed in the exhibition. No less impactful is the documentation of life in South Africa during the years of racial segregation, which we can see in the next room. Alternating photos of political leaders with photos of people’s routines, the artist builds a narrative of how life was like at the time.\nContinuing on the theme of segregation and racism, in the section entitled Voci del Sud bianco, we observe colored images that portray society in southern United States in the 1950s. Photos of white families representing the American way of life are followed by photos of black people as secondary figures during that time. Buorke-White once again manages to teach a history lesson with images only.\nThe final sessions, in addition to exhibiting some significant aerial images done by the artist throughout her life, details through a series of images of her last and strenuous fight against Parkinson’s disease.\nCompleting the cycle of the artist’s life and work history, the exhibition “PRIMA, DONNA. Margaret Bourke-White” is a lesson in 20th century history through photography. The photo collection shows her visionary and at the same time narrative ability, allowing her to compose dense and dazzling photographic stories. The female identity, having Bourke-White as a pioneer in different moments in photojournalism, at a time when the capacity of women was extremely doubted, is portrayed throughout the entire exhibition. Iconic and representative images made this artist, who dedicated her life to photography, to be marked in history forever.\nTill April 30, 2022\nMuseo di Roma in Trastevere\nPiazza S. Egidio, 1/b\nTue – Sun 10am – 8pm\nEntry fee €6,5-8,5","Portraits by Yousuf Karsh\nYousuf Karsh (1908-2002) Canadian photographer, known for portraits of the great personages of his time.\nAs an Armenian in Turkey, the young Karsh endured persecution and privation. At the age of 16 (in 1924) he emigrated to Canada, joining his uncle, who was a photographer in Sherbrooke, Que. From 1928 to 1931 he served as an apprentice to a prominent Boston painter and portrait photographer and briefly attended art school. Returning to Canada in 1932, he was employed by an Ottawa photographer, whose studio Karsh leased after his employer retired. He was appointed official portrait photographer of the Canadian government in 1935. He became a naturalized Canadian citizen in 1947.\nKarsh's portrait of Sir Winston Churchill, made in Ottawa in 1941, brilliantly conveyed\nthe dogged determination of\nBritain's wartime leader and brought Karsh his first real international fame. Karsh went on to photograph an enormous\nnumber of the world's most prominent personalities, including royalty, statesmen, artists, and writers. He also\ncontinued to make portraits of world leaders. Karsh's style as a portraitist was formal. He used subtle lighting to\nmeticulously model his subjects' faces, thereby obtaining a monumental and idealized presentation that was in accord\nwith their public image.\nHis books include Faces of Destiny (1946), Portraits of Greatness (1959), In Search of\nGreatness (1962), Karsh Portfolio (1967), Faces of Our Time (1971), Karsh Portraits (1976), Karsh Canadians\n(1978), Karsh: A Fifty-Year Retrospective (1983),\nKarsh: American Legends (1992),\nKarsh: A Sixty-Year Retrospective (1996),\nYousuf Karsh: Heroes of Light and Shadow\nHis books include Faces of Destiny (1946), Portraits of Greatness (1959), In Search of Greatness (1962), Karsh Portfolio (1967), Faces of Our Time (1971), Karsh Portraits (1976), Karsh Canadians (1978), Karsh: A Fifty-Year Retrospective (1983), Karsh: American Legends (1992), Karsh: A Sixty-Year Retrospective (1996), and Yousuf Karsh: Heroes of Light and Shadow (2001).\nModern reproductions of portraits\nThese high quality color and black and white prints were produced in Switzerland by Imprimerie Jean Genoud SA, Lausanne. They come from one of Yousuf Karsh art books. Usually, there is a short unrelated text or another portrait on the back side but it doesn't effect the main picture. Please refer to a description of the specific print for details. These prints are of poster quality or better.\nA gravure is a photomechanical intaglio process print, developed in the mid-19th Century, in which the image is transferred to the printing plate by using a light sensitized gelatin film surface on a metal plate which is then etched. The photogravure c an reproduce an original painting or photograph with an accuracy of detail and tonal depth unlikely to be surpassed in monochrome printing. The sheet-fed gravure method involves feeding each sheet individually through the printer, with printing only on one side of the page. This prevents one of the most common printing problems -- the show through of material from the other side of the page. In addition, Karsh and the publisher went to considerable length -- the use of a special soft ink and of specially produced (very heavy) paper -- to insure that the final print was as close an approximation of the original photograph as possible. The deep, velvety blacks and the low gloss finish provide a sense of texture that is totally lacking from most reproductions.\nThe magnificent portraits were produced by sheet-fed gravure, a printing process not presently used in North America for this type of work. The world-renowed printing house of Enschede in Haarlem, Holland, which has been turning out fine printing for more than two hundred and fifty years, was entrusted with making and reproduction of the gravure cylinders. The text was first printed by offset lithography on paper especially manufactured for this book in Paris. The gravure printing of the portraits followed. The results are as close to the quality of Yousuf Karsh's originals mat finish prints as has ever been obtained by any printing method A special thermoplastic binding used in gravure portfolio books allows for easy removing individual portraits for mounting and framing.\nGravure prints offered on this site come all from early portfolio books by Yousuf Karsh. They are 30 to 40+ years old. Some of these prints are very difficult to find in a good condition. Due to the soft ink they need to be handled with care. Archival matting and framing under glass is recomended."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:1743e2ce-811a-46c7-b5ce-7570ebdd566c>","<urn:uuid:2e3da5ab-cd9d-47f5-b962-ee727d1413c5>"],"error":null}
{"question":"What is the Comprehensive Care for Joint Replacement model, and how are patient demographics for joint replacement surgeries changing over time?","answer":"The Comprehensive Care for Joint Replacement (CJR) model is a Medicare initiative that promotes quality and financial accountability for lower-extremity joint replacement procedures. It involves bundled payments and quality measurements for episodes of care lasting 90 days after discharge, encouraging hospitals, physicians, and post-acute care providers to work together to improve care coordination. Regarding demographics, research shows patients are getting younger. The average age for total hip replacement patients decreased from 66.3 years in 2000 to 64.9 years in 2014, while total knee replacement patients' average age dropped from 68.0 to 65.9 years. Women make up 55-62% of patients, though there are more men undergoing these procedures than in 2000. By 2030, primary total hip replacements are projected to increase by 171% to 635,000, and primary total knee replacements by 189% to 1.28 million.","context":["Comprehensive Care for Joint Replacement Model\nWashington Hospital Healthcare System is participating in a New Care Improvement\nInitiative from Medicare\nWashington Hospital Healthcare System is participating in a Medicare initiative\ncalled the Comprehensive Care for Joint Replacement (CJR) model. The CJR\nmodel aims to promote quality and financial accountability for care surrounding\nlower-extremity joint replacement (LEJR) procedures, commonly referred\nto as hip and knee replacements and/or other major leg procedures. Washington\nHospital Healthcare System’s participation in the CJR model should\nnot restrict your access to care for your medical condition or your freedom\nto choose your health care providers and services. All existing Medicare\nbeneficiary protections continue to be available to you. These include\nthe ability to report concerns of substandard care to Quality Improvement\nOrganizations and 1-800-MEDICARE.\nThe CJR model aims to help give you better care.\nThe CJR model aims to support better and more efficient care for beneficiaries\nundergoing LEJR procedures. A CJR episode of care is typically defined\nas an admission of an eligible Medicare beneficiary to a hospital participating\nin the CJR model that eventually results in a discharge paid under Medicare\nSeverity-Diagnosis Related Groups (MS-DRG) 469 (major joint replacement\nor reattachment of lower extremity with major complications or comorbidities)\nor 470 (major joint replacement or reattachment of lower extremity without\nmajor complications or comorbidities). The CJR episode of care continues\nfor 90 days following discharge. This model tests bundled payment and\nquality measurement for an episode of care associated with LEJR procedures\nto encourage hospitals, physicians, and post-acute care providers to work\ntogether to improve the quality and coordination of care from the initial\nhospitalization through recovery. Through this bundled payment model,\nWashington Hospital Healthcare System will receive additional payments\nif quality and spending performance are strong or, if not, potentially\nhave to repay Medicare for a portion of the spending for care surrounding\na lower extremity joint replacement procedure.\nMedicare is using the CJR model to encourage Washington Hospital Healthcare\nSystem to work more closely with your doctors and other health care providers\nthat help patients recover after discharge from the hospital, including\nnursing homes (skilled nursing facilities), home health agencies, inpatient\nrehabilitation facilities, and long-term care hospitals. The goal of the\nmodel is to encourage these providers and suppliers to provide you with\nbetter, more coordinated care during and following your hospital stay.\nThe model is expected to lower the cost of care to Medicare but your costs\nfor covered care will not increase due to these changes.\nWashington Hospital Healthcare System is working closely with the doctors\nand other health care providers and suppliers who will care for you during\nand following your hospital stay and extending through the recovery period.\nBy working together, your health care providers and suppliers are planning\nmore efficient, high quality care as you undergo treatment.\nMedicare will monitor your care to ensure you and others are receiving\nhigh quality care.\nWashington Hospital Healthcare System entered into financial arrangements\nwith collaborating health care providers and suppliers who are engaged\nin care redesign with the hospital and who may furnish health care services\nto you during your episode of care. Under these agreements, Washington\nHospital Healthcare System may share payments received from Medicare as\na result of reduced episode spending and hospital internal cost savings\nwith collaborating providers and suppliers. Washington Hospital Healthcare\nSystem may also share financial accountability for increased episode spending\nwith collaborating providers and suppliers.\nThe following list includes health care providers and suppliers that have\nestablished a collaborator agreement with Washington Hospital Healthcare\nSystem in order to share in financial rewards and/or losses in the CJR model:\nDr. John Dearborn and Dr. Alexander Sah\n2000 Mowry Ave., Fremont, CA 94538","New research shows that patients undergoing total joint replacement are younger now than they were in 2000. According to a review from the National Inpatient Sample (NIS) database, the average patient undergoing a total hip replacement (THR) in 2014 was 64.9 years, while the average patient in 2000 was 66.3 years. In parallel, the average patient undergoing a total knee replacement (TKR) was 65.9 in 2014, and 68.0 in 2000.\nDr. Matthew Sloan, lead researcher and orthopaedic resident at the University of Pennsylvania in Philadelphia, tells Reuters Health by email: “These differences may not seem like much, but an average decrease of two years in a pool of 1 million people is a significant difference. It’s also a meaningful difference when you take into account the fact that these total joint replacements have a finite lifespan.”\nDr. Sloan further explains: “The technology for total hip and knee replacements continues to improve. However, at some point, the implant wears out. We believe modern implants without any unforeseen complications should last 20 years or more. The problem with an increasingly younger group of patients having these procedures, it becomes more likely that the implant will wear out during their lifetime. When this happens, a second surgery is required to revise the joint replacement. These procedures are not as successful as the initial surgeries, they are bigger operations, they take longer, and now the patient is 20 years older and not as strong as they were when they had the initial procedure.”\nBecause a second surgery is riskier and prone to complications like early failure or infection, the goal is to wait as long as possible so that a patient will undergo one surgery in their life. Other findings presented at the American Society of Orthopaedic Surgeon’s annual meeting include:\n- 55-62% of people who make up the majority of THR and TKR patients are women – this may be because women have a higher rate of arthritis compared to men\n- there are more men undergoing THR and TKR than there were in 2000\n- data from the US shows that there may be cultural or social differences among groups that cause non-Hispanic whites to pursue surgical treatments, while Asians seem to avoid surgery\n- in 2014, there were more than 370,000 THR and 680,000 TKR surgeries in the United States\nDr. Sloan and colleague Dr. Neil Sheth predicts that by 2030, primary THR is projected to reach 635,000 (171% increase) and primary TKR will reach 1.28 million (189% increase). Similar increases are expected for the second THR and TKR surgery. Dr. Sloan concluded: “These numbers are always changing. We will continue to look at new data as the numbers need to be constantly updated, especially if they are used to make predictions for future healthcare saving decisions, as the impact can be in the millions of dollars. It’s imperative to provide policy makers with high-quality data to inform decisions that will affect patient access to orthopaedic care and the financial viability of elective orthopaedic procedures.”\nTips on how to avoid knee or hip surgery\nAccording to Harvard Health, “losing weight, strengthening muscles, and increasing flexibility may help you stave off joint replacement.”\nExercise and weight loss may help prevent the knee or hip pain and prevent surgery. When exercising, focus on the exercises that strengthen the muscles that support your joints – the quadriceps in the front of the thigh and the hamstrings in the back are key to knee strength. In an interview with Harvard Health, David Nolan, a physical therapist at Massachusetts General Hospital, said: “Every time you walk or run or do anything weight-bearing, the quads absorb the shock. The stronger your quads are, the less load that gets transferred into the joint.”\nAccording to Harvard Health, the force you place on your joints can be up to six times your weight. It means that if you are 10 pounds overweight, it’s 30 to 60 pounds of pressure on every step. A dietician can help you determine the best meal plan for you – reducing your calories while ensuring you are getting what your body needs to build muscle and keep up your energy.\nExercises to build quad strength\n- Lie down on your back, tighten your quads with your leg in front of you\n- Lie down on your stomach, raise your foot into the air to strengthen your hamstrings\n- Leg curls – Hold the back of a sturdy chair with one or both hands to maintain balance, bend your right knee and lift your heel up toward your buttock, hold for 2-3 seconds, slowly lower it down, repeat and alternate between the right and left knee.\nExercises to build hip strength and flexibility\n- Leg lifts – Clamshells\n- Leg lifts – similar to the one in the previous section.\n- Stretching before and after exercising will bring more blood flow to the area and makes the muscle more adaptable to change."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:d64284e3-7eba-4939-adaa-e16de8ff09f4>","<urn:uuid:5d13f50d-1795-4bd5-a13a-a0e436aeeb76>"],"error":null}
{"question":"What ancient preservation techniques are used in both Pompeii and Caesarea, and how do modern conservation efforts differ between these historical sites?","answer":"In both sites, traditional materials and techniques play a key role in preservation. In Caesarea, conservation efforts focus on preserving exposed sections of mosaics and maintaining archaeological features in their natural place. The site receives significant resources for accessibility and visitor experience, drawing over 700,000 tourists annually. In Pompeii, conservationists specifically avoid modern materials like concrete, instead using traditional materials such as lime. However, Pompeii's preservation approach is more technologically advanced, incorporating nanotechnology to make lime more fluid for stabilizing frescos, and using seismic measurements to protect against future tremors. The Pompeii Sustainable Preservation Project also includes comprehensive strategies like drainage systems, protective structures, and serves as a training site for conservationists worldwide.","context":["A collaborative effort from Israel Antiquities Authority and the Caesarea Development Corporation has revealed a rare, multicolored Roman mosaic with Greek inscriptions. The excavation was carried out inside the perimeters of the Caesarea National Park that encompasses the ruins of the ancient Judeo-Roman port town of Caesarea (or Kesariya) in north-central Israel. According to the archaeologists involved in the project – which originally focused on the reconstruction of the impressive Crusaders-era entrance bridge to the town, the fascinating mosaic dates back to circa 2nd-3rd century AD and was found underneath the remnants of a larger building.\nDr. Peter Gendelman and Dr. Uzi ‘Ad, directors of the excavation for the Israel Antiquities Authority, talked about the artwork depictions on this 1800-year old mosaic –\nThis colorful mosaic, measuring more than 3.5 x 8 m [11.5 ft x 26.2 ft or 302 sq ft], is of a rare high quality. It features three figures, multicolored geometric patterns and a long inscription in Greek, which were damaged by the Byzantine building constructed on top of it. The figures, all males, wear togas and apparently belonged to the upper class. The central figure is frontal and the two other face him on either side. Who are they? That depends on what the building was used for, which is not yet clear. If the mosaic was part of a mansion, the figures may have been the owners. If this was a public building, they might have represented the donors of the mosaic or members of the city council.\nThe assessment of this substantially large mosaic unfolded the exquisite artisanship with the depicted images found to showcase densely placed tesserae of about 12,000 stones per square meter. This impressive artistic level matched the intricate specimens discovered in the vicinity of Antioch, Turkey. And as for the Byzantine building that enclosed this incredible artwork, the researchers have dated it back to circa 5th century AD, which coincides with the early Eastern Roman (Byzantine) period. And judging by its large size, the structure was possibly a part of an agora – the ‘composite’ area that combined commerce and public assemblies.\nWhen it comes to the realm of history, the ancient port town of Caesarea was originally built by Herod the Great in honor of Augustus Caesar (circa late 1st century BC). The settlement then subsequently served as the provincial capital of the Judaea Province of the Roman Empire, and as such was an important trading hub (bolstered by farming lands) even after the passing of the Romans, till the Fatimid period. Given this spectrum of historical legacy (especially pertaining to the Romans), back in last year, authorities had already decided to restore the once-imposing ancient Roman temple of the 2,000-year old metropolis, aided by a $27 million project.\nAnd now, the Israel Antiquities Authority Conservation Administration is also looking forward to preserving the exposed sections of the aforementioned mosaic. This project will go in conjunction with the conservation planned for the entire site, which is known for drawing in visitors. Michael Karsenty, CEO of the Caesarea Development Corporation, said –\nIn collaboration with our colleagues from the Israel Antiquities Authority and the Israel Nature and Parks Authority, we make sure to preserve every find in its natural place and are investing huge resources to make the site accessible to Israeli visitors and tourists from all over the world. Caesarea already provides one of the best and most exciting visitor experiences in the world, and as a result attracts more than 700,000 Israeli and foreign tourists every year. We are proud to note that Caesarea is one of the three most visited sites in Israel. But we have no intention of resting on our laurels. At the same time that we make possible the very intensive archeological work throughout the park, the Caesarea Development Corporation, together with the Israel Nature and Parks Authority, is working to constantly upgrade infrastructure at the site, including the establishment of a spectacular archaeological park, an advanced visitor center, visitor amenities and a delightful promenade that will begin at the ancient aqueduct (Aqueduct Beach) and link up to the promenade along the walls of Caesarea’s Old City.\nKarsenty also added –\nThe impressive mosaic joins the many other important recently unearthed archaeological finds. Among these is the altar of the temple built by Herod 2,000 years ago and mentioned by the ancient historian Josephus Flavius; a mother-of-pearl tablet etched with a seven-branched candelabrum, as well as the statue of a ram, which was a symbol of Christian congregation in the Byzantine period.\nSource: Israel Ministry of Foreign Affairs / All Images Credit (except last image): Assaf Peretz, Israel Antiquities Authority","Modern buildings are designed to have a lifespan of around 50 years. But in historical terms, that is a mere blink of an eye. We would like archeological sites like Pompeii, for example, to stand the test of time immemorial. Preserving sites such as this with the most basic materials represents a huge scientific challenge. As part of the “Pompeii Sustainable Preservation Project”, researchers from Technische Universität München (TUM), Fraunhofer-Gesellschaft and ICCROM will spend the next ten years investigating long-term solutions to prevent the UNESCO world heritage site of Pompeii from falling further into ruin.\nAlmost two thousand years ago, the city of Pompeii was buried under a shroud of ash and lava ejected by nearby Vesuvius. The eruption in 79 AD essentially froze the ancient city, preserving it for centuries. Large-scale excavations did not begin until the 18th century. Bit by bit, the city began to reveal its secrets about life in ancient times.\nAs one of the largest self-contained sites surviving from antiquity, Pompeii is a treasure trove. Each new excavation yields new knowledge, and is greeted with huge interest by the public and research community. All too often however, a lot less interest has been shown in the sustainable preservation of this unique site.\nMany of the finds, most notably Pompeii’s frescos, have been moved to museums, to protect them from the wind and weathering. But because of inadequate conservation measures, the exposed walls of the city with their lavish decorations are now visibly disintegrating.\nPompeii as a center of world-class research\nThe researchers participating in the Pompeii Sustainable Preservation Project intend to concentrate on one of Pompeii’s apartment buildings, known as an insula. From 2014, they will embark on an ambitious conservation program, taking in everything from elaborate murals to the smallest wall. “The first step will be drainage, followed by new types of protective structures. But that is just the start,” explains Professor Erwin Emmerling of TUM’s Chair of Restoration.\nAn important new approach is preventive restoration. “To date, this has not been undertaken on an adequate scale. We want to find out more about ongoing restoration,” continues Emmerling. The researchers will only use simple, traditional materials. In any case, large equipment like cranes would be of no use in the narrow streets of Pompeii. They will also have to make do without concrete because it was not used in those days. Instead, the restoration team will use lime and other traditional building materials.\nModern technology for ancient monuments\nBut the researchers will not be foregoing all high-tech aids. They will use nanotechnology to make the lime more fluid, thus stabilizing the frescos through backfilling. The experts intend to conserve the topmost layer of the paintings using lime and silicon compounds.\nResearchers from various disciplines will be working alongside restoration experts and archeologists in the Pompeii Sustainable Preservation Project. The ancient city will be accurately surveyed both on the ground and through aerial photographs. Seismic measurements will provide information on how the monument will be impacted by future seismic activity, which will help to ensure that the conserved structures will later withstand these tremors. Construction and structural engineers will be supporting activities in this area.\nLast but not least, suitable sites within Pompeii are to be re-landscaped, and the project as a whole will be a training site for conservationists from around the world.\nThe key partners in the Pompeii Sustainable Preservation Project are Technische Universität München (Chair of Restoration, Art Technology and Science of Conservation), Fraunhofer-Gesellschaft (Fraunhofer Institute for Building Physics), and the International Center for the Study of the Preservation and Restoration of Cultural Property (ICCROM), which is attached to UNESCO. These institutions will be assisted by the Soprintendenza Speciale per i Beni Archeologici di Napoli e Pompei and the Istituto Superiore per la Conservazione ed il Restauro, which is a body of the Italian Ministry of Cultural Heritage. The University of Oxford’s School of Geography and the Environment, the Department of Ancient History at Ludwig Maximilian University in Munich, the German Archeological Institute (DAI) in Rome, the University of Pisa and the Istituto per i Beni Archeologici e Monumentali of the Consiglio Nazionale delle Ricerche (CNR) are supporting the project as research partners.\nProf. Erwin Emmerling\nTechnische Universität München\nLehrstuhl für Restaurierung, Kunsttechnologie und Konservierungswissenschaft"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:6f2904ca-009f-4272-bc7c-8f5ccc1bd7c3>","<urn:uuid:8ce8cd42-3a55-4275-a07a-660d16098ec2>"],"error":null}
{"question":"How do selection tools differ between Photoshop and Illustrator when using a tablet vs. mouse?","answer":"When using a tablet, the path and lasso selection tools in Photoshop gain significant control and precision compared to mouse usage, making detailed selections much easier. The tablet provides fine-grained control for placing pen points exactly where desired. In Illustrator, the Selection Tool (black arrow) and Direct Selection Tool (white arrow) behave similarly across both input devices, with the Direct Selection Tool specifically allowing for precise anchor point adjustments. Both programs benefit from tablet use for selections, but Photoshop shows more dramatic improvement in selection capabilities compared to mouse input.","context":["I’ve now edited several pictures using the tablet, and as I began this process, I started jotting down notes of random thoughts, insights and frustrations involved in changing my workflow to make use of the new addition. As such, this section is a little disjointed – I apologize for that, but it was the best way I could think of to impart the actual learning curve involved.\nBrushes, dynamics, and why you buy the bloody thing\nThe first thought was to get down to brass tacks, and turn everything in Photoshop to use pen sensitivity that could. This meant all “Brush Dynamics” on full bore – my pressure would allow the size of my brush to grow and the opacity to increase. This seems intuitive – if you pull a paintbrush across a canvas lightly, you get a very thin line that may or may not be very opaque, depending on the paint.\nTherein lies the key, though: it depends on the paint. Photoshop does not have an intuitive way to adjust for this (it’s all hidden under the individual brushes) – so turning this feature on is like trying to watercolor on paper with a heavy bleed. There are points where this is useful, but you need to first learn to set your brush sizes appropriately, which means you better have that on the speed dial. If your brush size is set right, the dynamics will control a “bleed” around the edge of the chosen brush – push hard, the bleed is greater. This becomes incredibly useful when dodging and burning, because a harder push not only provides the appropriate level of the effect, but also makes sure it blends into the surrounding area without looking like you went nuts.\nOver time, I grew to like allowing the brush to be controlled by pen pressure, but I would in no way recommend this while you are getting used to the tablet – and I turn it off regularly depending on effect. At first, it is very hard to guess just what area you are going to affect with different sensitivity. When working with a mouse, you have zero directions of “dynamic” control – you must set your opacity and your brush size manually, and then if you want more, you have to paint over the same area multiple times (leading to streaks and artifacts).\nWhen transitioning to the tablet, it’s good to give yourself ONE degree of freedom before giving yourself two – and I’d personally start with opacity, as I find that to be less possible with the mouse. Now that I’m more used to it, there’s points where I will use brushes that have both on – but often I will find myself either allowing the tablet to choose brush size or opacity, but not both.\nPlease note that where to turn all of these individual dynamics on and off in Photoshop is dependent on the brush you choose (not all options are even possible on every brush), so it takes some hunting around and careful choosing or creating of your brushes to make the best use of this feature over time. In fact, a big part of what the tablet has given me is yet one more resource to start building on top of filters and actions – brushes. It’s funny how something that should save me work creates different work.\nThe Path Tool, and other selection tools\nIf you’ve not used the path tool before because it was too cumbersome to control with the mouse, well, I can’t blame you. If you’ve listened to or read anything on making proper selections, alpha layers, etc, you also know that you’re missing out on the best technique around. Allow me to say that if there was NO other function that the tablet provided, the path tool would make it worth a good deal of its asking price.\nThe fine-grained control that you gain from the tablet means that pen points go exactly where you want them, and tweaking their curvature becomes a breeze. My ability to draw a detailed, complex selection with a path has grown from nonexistent to well above passable nearly overnight. Good selections are a HUGE factor in good editing – this control makes that much more possible than ever before.\nThe lasso selector is another tool that receives a big function boost. Being able to properly draw is a very different feeling than trying to use your elbow and wrist to guide a mouse delicately, and again, the sensitivity comes into play. It is much easier to make a detailed selection with the tablet than it ever was with the mouse. The polygon lasso, rectangle and ellipse selection tools also function better than with a mouse, but not enough to write home about. It’s great to be able to place that rectangle right where you want it, but I never found that to be so difficult to accomplish with the mouse afte a little nudging with the keyboard arrows.\nThe Tablet outside of the picture\nMay I just say, “AAAARRGGHH!”\nThe minute you need to go outside of the boundaries of your picture with the tablet – be it for a tool, a layer change, an action, or to open menus – you begin to feel slowed down. The mouse is a speedy pointer, and I have grown used to that over the years. In fact, my sensitivity is usually dialed up pretty highly so I can get around on my desktop with the minimum of wrist motion.\nThe tablet, on the other hand, can be quite frustrating in this regard. A big part of it is the size: one wouldn’t think that 5.5 inches by 8.5 inches is that big of a tablet, and indeed there are two sizes even larger than that. However, that means your hand has to move on average at least 4 inches to get to elements on the edge of your screen, assuming that you’re in the center of the tablet. In comparison, the average person moves the mouse about 3 inches to get across the ENTIRE screen.\nMore size means more sensitivity when it comes to dealing with your actual image, and that is why you buy this in the first place. However, that comes with the sacrifice of mobility when you need to use the remaining interface elements that you couldn’t shoehorn into your tablet’s quick-keys. The included “mouse” is supposed to help with that, but as I mentioned before – it’s way easier to grab your real mouse, or just deal with the speed and motion of the hovering pen.\nOverall, this is where the tablet begins to grow very difficult for me – a lot of my editing style is “Apply a filter or action, then edit masks to control where it actually affects, then play around with blend modes, then add necessary curves layers to dodge and burn… then repeat with new filter or action, then edit where it affects, then play around with blend modes, then add necessary curves layers…”\nIn any image, I might use 5-10 different filters/actions, each with their own layer, then a mask, then a blending change, then adjustment layers. In this pattern, the tablet helps with the mask and the adjustment layers – two integral things, but not the whole workflow. The slow speed of the tablet on the whole rest of the UI means that I still spend a lot of time switching between the pen and the mouse, which is a frustration for those seemingly quick edits.","Vector graphics are to raster graphics as apples are to oranges — They share some distant similarities (both can edit and create graphics) but they are very different in form and function! Raster graphics are typically created and edited using Adobe Photoshop, and Illustrator is king in the world of vector graphic editing and creation. Seems simple enough! Once you understand the functionality of various Adobe Illustrator tools, you’ll know how to create logos and countless other vector graphics!\nTAKING A CLOSER LOOK AT THE “TOOLS OF THE TRADE”\nWe touched on the quick and dirty fundamentals of Adobe Illustrator in our previous Illustrator quick start guide post, and today we’ll be diving into the essence of what makes Illustrator the ideal choice for learning how to create logos and other scalable graphics.\nTHE SELECTION TOOLS\nThere are two Selection Tools available in the Default Tool Menu (which is typically docked on the left-hand side of your window.) These tools can be used to select parts of your document and manipulate it in several different ways.\nTHE GROUP SELECTION TOOL\n[BLACK ARROW ICON | KEYBOARD SHORTCUT: ‘V’]\nThis is the go-to, default Selection Tool for Illustrator. It behaves pretty much the same way here in Illustrator that it does in Photoshop, so you shouldn’t have too much trouble with this one. The primary function of this tool is to be able to select an entire group or layer at once versus having to select each one individually.\nTHE DIRECT SELECTION TOOL\n[WHITE ARROW ICON | KEYBOARD SHORTCUT ‘A’]\nThis is the tool you want if you need to adjust a single path or one part of a path — it is ideal for moving individual anchor points around without having to delete and then recreate them if they wind up in the wrong place. This tool also behaves in a similar fashion across the Creative Suite (this is the name of Adobe’s software package which includes Illustrator and Photoshop.)\nTHE DRAWING TOOLS\nTHE PEN TOOL(S)\n[PEN ICON, CLICK AND HOLD TO REVEAL TOOLS | PEN : ‘P’/ ADD ANCHOR: ‘+’ /DELETE ANCHOR: ‘-‘ /ANCHOR POINT: ‘SHIFT+C’]\nThe shortest distance between two points is a straight line, and with the Pen tool that line is just a couple of clicks away!\nTo create a simple line, select the Pen Tool, hover over the point you would like your line to begin, and then hover over the point you would like your line to end, clicking again to create your second anchor point. Now you have your first path!\nThe remaining anchor point specific Drawing Tools are fairly self-explanatory; you can use the Add Anchor Point tool to add an anchor point to any spot on your path. You can use the Delete Anchor Point Tool to remove any existing anchor point on your path. The Anchor Point Tool can be used to modify an anchor point by converting it to a corner point or a smooth point, and to manipulate the handles that allow the path on either side of an anchor point to create the desired curve / angle / line.\nTHE SHAPE TOOL(S)\n[RECTANGLE ICON, CLICK AND HOLD TO REVEAL TOOLS | RECTANGLE: ‘M’ / ROUNDED RECTANGLE / ELLIPSE: ‘L’ / POLYGON / STAR / FLARE]\nIf you know that you’re going to need a certain, basic shape, why reinvent the wheel by creating that shape with the Pen Tool? Use the Shape Tool instead. Several basic shapes are available by default such as squares (hold down the Shift key while dragging the rectangle tool to create a shape with equal length sides), rectangles, ellipses, circles (hold down the Shift key while dragging the Ellipse Tool to create a shape with equal diameter all around), polygons, stars, and flares.\nTHE PENCIL TOOL\n[PENCIL ICON | KEYBOARD SHORTCUT: ‘N’]\nThe Pencil Tool in Illustrator behaves much the same way that the Freeform Pen Tool does in Photoshop — users are able to draw freeform paths as though they were using a pencil and paper. Once a path has been created, Illustrator will add anchor points for you in places where your path changes shape / direction. These anchor points can then be modified using the Pen Tools mentioned above.\nThe Brush Tool behaves much like the Pen Tool in that it can be used to create freeform paths — the main distinction between the Pencil and Brush Tools lies in the appearance of the resulting path. When a path is created using the Brush Tool, it will already be styled with a brush stroke, whereas a path drawn with the Pencil tool will be a uniform path from end to end.\nOTHER IMPORTANT TOOLS\n[SQUARE WITH EXTERIOR CORNERS ICON | KEYBOARD SHORTCUT: ‘SHIFT+O’]\nThe Artboard Tool allows you to modify an existing artboard or create a new artboard anywhere within the Illustrator window. This is very useful for creating multi-page documents as well as keep elements of a design separate until time to combine them.\n[HAND ICON | KEYBOARD SHORTCUT: ‘H’]\nThe Hand Tool is very handy (<- pun intended) for panning around within the Illustrator window without having to worry about the Navigation Panel, Zoom Tool, or accidentally selecting and modifying any of your active layers.\n[MAGNIFYING GLASS ICON | KEYBOARD SHORTCUT: ‘Z’]\nThe Zoom Tool is as simple as any tool can be; select the Zoom Tool, and click on an area of your document you want to see up close. Done. If you would like to use keystrokes instead for zooming in and out, you can zoom in by holding down the ‘Control’ key and pressing the ‘+’ key until you reach your desired magnification. To zoom out, hold down the ‘Control’ key and press the ‘-‘ key until you’re back to where you want to be.\nGETTING STARTED WITH VECTORS\nSTARTING WITH SHAPES\nLet’s get started creating our new project!\nSelect your desired Shape tool. Next, either click and drag it across your main artboard to the desired dimensions (hold down the ‘Shift’ key to produce a “perfect” even-sided shape) OR click once on your active artboard and enter the desired dimensions for your chosen shape.\nOnce your shape has been created, you can choose to leave it as it is, or to modify it using the Selection Tools and / or the Anchor Point Tools.\n- The Group Selection Tool can be used to enlarge or condense your shape. Simply select your shape and then use the transformation control points that appear as a box surrounding your shape. Hold down the ‘Shift’ key while manipulating your shape to maintain its current proportions.\n- The Direct Selection Tool can be used to modify the individual anchor points of your shape. Choose the Direct Selection Tool and hover over the path of your shape (run your pointer over the edge of your shape and the path will be highlighted) and click on an anchor point. This should select that anchor point and allow you to move it wherever you’d like it to be.\n- The Add & Remove Anchor Point Tools can be used exactly as their names imply.\n- The Anchor Point Tool can be used to alter the ‘state’ of an anchor point from being part of angle or part of a curve.\nSTARTING WITH THE PEN TOOL\nAs we’ve already discussed, the Pen Tool can quickly create lines and shapes with ease — each click of your mouse or tap of your stylus creates an anchor point that can then be manipulated into an angle or a curve, or left alone until the path has been completed.\nWhen creating right-angle shapes (rectangles & squares) with the Pen Tool it is helpful to drag a few Guides onto your artboard. If you have your rulers showing on the left and top of your workspace, simply click on any spot on the ruler and drag down; a guide should appear. If not, select View > Guides > Make Guides or use the keystroke ‘Ctrl’+’5’. Once your guides are placed to create the shape you want, click once in each place the guides intersect — this will create anchor points in each corner. When you’re finished, open your Layers palette (on the right-hand docked menu) and simply delete the guides, which will appear individually under your current active layer. Ta da! You’ve just made a new custom shape with just the Pen Tool. The Pen Tool is such a multi-faceted and versatile tool, it can help you create some truly amazing things.\nOnce you have created a path, you can select it and explore the path modification options available from the File / Application menu — Select Object > [any of the items on the menu that are black / active] and you can modify your path in lots of different ways, including joining two separate paths, or simplifying a path you’ve made.\nTypography is such an enormous part of logo design and learning how to create logos that it bears special mention in this guide. In fact, many logos for top brands are created using only type.\nIllustrator doesn’t disappoint in this area, and offers a wealth of tools that give users an incredible amount of flexibility and versatility when it comes to adding type to their designs, or even starting with type and working their way through their project.\nTHE TYPE TOOLS:\n[“T” ICON | KEYBOARD SHORTCUT: ‘T’]\nTo initiate the Type Tool, you can click on your active artboard to create a Point Text Object (text with no constraints i.e. a text box) or click and drag across your active artboard until you have created your desired size text box (known as an Area Type Object.)\nAREA TYPE TOOL\n[“T” WITH SHAPE BACKGROUND ICON]\nClick on any Closed Path on your active artboard to create an Area Type object — this will enable you to constrain your text to only appear within the boundary of that path. You’re essentially creating a custom text ‘box’ for your text.\nTYPE ON A PATH TOOL\n[“T” AT AN ANGLE ABOVE A LINE ICON]\nCreate a path using any of the tools we’ve discussed so far and adjust it however you would like your text’s baseline to follow. (The baseline of your text is the “bottom” of the letter with no ascenders or descenders; letters like lowercase e’s and o’s.)\nVERTICAL TYPE TOOL\n[“T” WITH ARROW POINTING DOWN ICON | KEYSTROKE: “T” + HOLD DOWN ‘SHIFT’ KEY]\nClick anywhere on your active artboard and drag to create the shape that will contain your vertical text.\nCreating vertical text is NOT THE SAME as rotating the shape as shown in the image for the Area Type Tool.\nVERTICAL AREA TYPE TOOL\n[“T” WITH SHAPE BACKGROUND AND ARROW POINTING DOWN ICON]\nCreate a shape and click anywhere on the path with the Vertical Area Type Tool in order to transform that path into the perfect customized text box for your vertical text.\nVERTICAL TYPE ON A PATH TOOL\n[“T” AT A DIFFERENT ANGLE ON THE SAME SHAPE / LINE AS THE TYPE ON A PATH TOOL.]\nCreate a path using any of the Drawing Tools or the Line Segment Tool and click anywhere on that path with the Vertical Area Type on a Path Tool in order have your vertical text follow the angles / curves of the path you have chosen for it.\nCONVERTING TYPE TO OUTLINES\nHOW / WHEN / WHY TO TAKE YOUR TEXT BEYOND THE RESTRICTIONS OF “EDITABLE TYPE”\nThanks to some of the advances made in Illustrator CC you can keep your type “live” while still employing many techniques that used to mean converting your text to outlines first was a must.\nUsing the Appearance Panel (usually a part of the docked sidebar on the right edge of your workspace) you can now:\n- Apply multiple effects to the same character(s) including strokes\n- Use Envelope Distort [‘Object > Envelope Distort’]\n- Mask “live” editable type\nThere are still some cases where converting your type to outlines can be beneficial, so don’t completely discard the idea just yet!\nASK YOURSELF THESE QUESTIONS TO DETERMINE WHETHER OR NOT YOU WILL NEED TO TAKE THE PLUNGE AND CLICK “CONVERT TO OUTLINES”:\n- Will you need to transform or distort your type beyond Warping Effects or Envelopes?\nOutlining your type transforms each character into a path (or compound path for letters with holes such as O’s) that is composed of the same angles and curves as any other path you’ve created thus far. This means that you can grab an anchor point with your Direct Selection Tool and drag it somewhere else, modify it with the Anchor Point Tool, and so on until you create exactly the character you need. This is a very common practice when creating a logo because it allows you to start with something you know and create something truly unique for your client.\n- Will your type be printed at a small size?\nIf your answer is yes, you’re better off keeping your type “live” because even at 600 dpi, type that has been converted to outlines is not as clear as type that is kept in its original form.\n- Have you kerned your type, or otherwise altered the spacing of your characters / words?\nIf so, you may want to convert your type to outlines as one of the final things you do before saving your final file for a project. Many programs do not recognize custom kerning or tracking changes made in Illustrator, and having your type as outlines will prevent any modifications from being lost in translation.\n- Do you have permission to embed or otherwise distribute the font you’ve used?\nIf you don’t have a license to embed or share the font you’re working with, and your client / their printer / etc. does not have their own license, you will need to convert your type to outlines before sending over your final files.\nADDING STROKES & FILLS\nFills and Strokes are what give shapes and paths in Illustrator their appearance — otherwise our creations wouldn’t have any substance at all!\nA Fill is the name given to the color, pattern, or gradient that is applied to the inside of an object. Both open and closed objects can have Fills applied to them.\nA Stroke is the visible outline of a shape or path. You can control the width and color of a stroke, as well as give it a dotted or dashed appearance. You can even create stylized strokes using brushes.\nYou can apply and adjust Strokes and Fills using the Appearance Panel, which is automatically part of your standard workspace in Illustrator, and can be found docked in the sidebar on your right.\nOnce you have created a path like the one pictured above, you can select it with either of your selection tools and using either of these two sidebar options you can choose a stroke color. Only by using the Appearance Panel can you alter the width and opacity of a stroke.\nAs we’ve shown in the same image above, a fill can be applied to any path, even an open one, though this obviously creates a rather interesting effect.\nFills can be solid colors, gradients, or patterns, and can be customized to suit your project by adjusting the opacity and/or Blend Mode.\nIllustrator offers you several different ways of saving your finished project.\nTo save your document in a vector-friendly format, Select “File” from your Application Menu / File Menu, and then choose the option that reads “Save As”. You will then be able to save your file as AI (illustrator’s native file type, like PSD is for Photoshop), PDF, EPS, or SVG.\nNext you will see a screen which will give you the option of saving your file in a “legacy” format. This means that your saved file will be able to be read and edited by past versions of Illustrator. If you would like to stick to the version of Illustrator you are using, then simply leave the options as they are.\nAs you are saving your files, please consider saving a copy of your document as a native AI file. Native file formats associated with each member of the Creative Cloud are able to store large amounts of information about your document that will be lost when saving as another file type, even one that is meant to be vector-friendly.\nHopefully this post has left you with a greater understanding of just how powerful Illustrator really is, and how how relevant it is when learning how to create logos and many of the other graphics we see around us every day!\nBECOME A GRAPHIC DESIGNER AT AN ACCREDITED GRAPHIC DESIGN SCHOOL\nF.I.R.S.T. Institute’s Graphic Design and Web Development Program will give you the tools to become a graphic designer in just 11 months and skills that stretch far beyond those covered in this Illustrator quick start guide. Not only will you learn how to make logos and similar graphics, you’ll learn how to thrive in the design industry. From a faculty made up of experienced design industry professionals to a broad spectrum of courses and a dedicated Career Development department, we give students access to everything they need to succeed in the design industry, both in class and on the job.\nIf you’d like to know more about our Graphic Design and Web Development school, please give us a call or send us an email! Our admissions team are standing by to answer any questions you might have."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:afb77acd-c44f-4d3c-a13c-43d6eb258463>","<urn:uuid:dbd54f00-47f1-444b-ab72-5acf0cc665dc>"],"error":null}
{"question":"What role does Google Analytics 4 play in website analytics compared to Unity Catalog's role in data governance?","answer":"Google Analytics 4 focuses on tracking campaigns and events within websites, using artificial intelligence to learn and analyze user interactions. Meanwhile, Unity Catalog serves as a comprehensive metadata and governance layer for data assets in the Databricks environment. While GA4 primarily handles website analytics and user behavior tracking, Unity Catalog manages broader data governance aspects including access controls, sensitive data cataloging, and automated lineage tracking across all lakehouse assets. Both tools use AI capabilities, but for different purposes - GA4 for website analytics optimization and Unity Catalog for data governance and metadata management.","context":["In the same way that a person takes a blood test to know their health, websites also have tools to assess their performance. If you plan to monetize your website or would like to know how well it is performing, you should consider installing Google Analytics on your site.\nIn this article, we will teach you everything you need to know about Google Analytics, from what you need to know, to how to install it. We will guide you step by step through the process of installing it on your website with ease so that you can better understand the performance of your website with complete ease.\nWhat is Google Analytics?\nGoogle Analytics is a tool developed by Google whose purpose is to measure and analyze everything that happens on a website or mobile application.\nOne of the advantages of Google Analytics is that it has a completely free version. This version is used by millions of users around the world and, if you need access to more complex functions, it also has a commercial variant called Google Analytics 360.\nThe truth is that in the vast majority of cases it is more than enough with the standard version of Google Analytics, which is the one we are going to see in this article. Don’t be misled into thinking that a tool with free options is simple because with Google Analytics you will have a large amount of information at your disposal.\nWhat data can you get with Google Analytics?\nThanks to this tool we can get to know our audience better. This is because we can know at what time of the day they connect, if they are men or women, the age range, the content they have searched to reach our page, from where they link to our page…\nWith Google Analytics you will probably get more information than you will be able to handle. This tool is essential to knowing the number of unique users, the bounce rate, the average duration of the sessions…\nThanks to all these metrics we can know the status of our page. Know if it has more or fewer views than last month, assess how marketing campaigns are working or also know those aspects that we should improve.\nLike everything in life, Google Analytics is changing and evolving over time. In this sense, we have two versions of Analytics, Universal Analytics (UA) and Google Analytics 4 (GA4). Let’s get to know them better to see the differences.\nUniversal Analytics (UA)\nUniversal Analytics is a form of Google Analytics measurement that we could say is already part of the past. Although it has been widely used by a large number of users, the truth is that its days are numbered.\nUniversal Analytics will cease operations in July 2023\nThis is because Universal Analytics will stop working next July 2023. As of today, there is still more than a year to go, however, it is important to begin to appreciate that this is a form of measurement that will soon become obsolete so it is important to begin to value the change.\nGoogle Analytics 4 (GA4)\nThat change is Google Analytics 4. The main difference between Google Analytics 4 and Universal Analytics is that Universal Analytics focuses more on page views while Google Analytics 4 focuses more on the campaigns or actions (events) in particular that take place within the website itself.\nWhich version of Google Analytics should you install on your website?\nThis is an important decision to make and one that many users are still wondering about. From here we believe that it is a good option to install both versions.\nFirst of all, Universal Analytics is quite easy to use, so it is especially attractive for those users who are going to run their first website. Also, there’s a lot more data and, well, it’s really still used a lot.\nOn the other hand, it’s important not to lose sight of the future. Installing Google Analytics 4 will allow you to collect data to be able to better analyze your campaigns. Although Google Analytics 4 is still in its infancy, the truth is that due to the artificial intelligence it uses, it learns very quickly.\nToday, Google’s recommendation is to have both versions, Universal Analytics to avoid losing data on the website and Google Analytics 4 to collect data and let the artificial intelligence work.\nInstalling Google Analytics (UA) step by step\n- The first thing you should do is to create an Analytics account. Creating an account is simple, just access the “Manage” section, go to the “Account” column and click on “Create account”.\n- The next step is to assign a name to the property. You will reach this section after you have created your account and it will basically serve to configure options such as the type of currency and the time zone where it is located.\n- To continue you must click on “Show advanced options” which is located at the bottom of the property configuration. At this point activate the switch that says “Create a Universal Analytics property” and enter the web page where you are going to place it.\n- Here you will be given the option to create a normal Analytics account, which will be the one we will stay with or the Analytics 4. Select the account you want, hit continue and click “Create”.\n- With the account already created and configured, the next step is to add its tag to the website. Here the procedure will be different depending on whether we are going to add it to a website that is hosted on a CMS, for example, WordPress or on a normal web page.\n- For the CMS website what you should do is look for the “Global website tag (gtag.js)”. Copy and paste in that part all the custom tags that your Google Analytics account has with the add HTML function and you are done.\n- In the case of a website, it is similar. You will have to look for the global tag in the same way, but here you will have to paste the tag after the <head> tag of your web page.\nThe data collection may take about 30 minutes to start. We recommend you wait a little time and check if the process has been carried out correctly.\nInstalling Google Analytics 4 (GA4) step by step\nWhen installing Google Analytics 4 on your website the procedure is similar to the previous section. The first thing you have to do is to create an account.\nFollow the same steps as in the previous section. If you notice, at the end of the procedure when the account is already created and configured the page itself will tell us if we want normal Analytics or Analytics 4. In this case, it is clear that we will choose the Analytics 4 option as it is the one we want. Probably next year this option will disappear as Analytics 4 will be the only one left.\nThe next step is to add this Google Analytics 4 to the site to collect the data. To add it to a CMS you should go to “Manage” and, in the “Property” column, check that you have selected the new Google Analytics 4 property. Click on “Data Flows” and then on Web.\nThen you should add the ID starting with “G-” that appears on the top right. The field where you will have to put this ID will vary depending on the CMS in question so you will have to look for it.\nIf it is a website the instructions are more general. In the same way that happens with normal Analytics, you should look for the “Global website tag (gtag.js)”. When you find it, copy and paste the custom tag that Google Analytics 4 has given you in its entirety with the add HTML function.\nIn the same way that happens with CMS, this will change depending on the CMS you use. It will not be the same in Blogger as in GoDaddy, however, the procedure is quite similar and you will find it through its configuration menu, it should not cost you much to find it.\nAgain, please note that the data collection may take about 30 minutes. Again, wait a reasonable amount of time and check if it is working.\nPlugins to install Google Analytics in WordPress\nLet’s see some of the most interesting plugins that exist for WordPress with which you can insert your Google Analytics code in a very simple way.\nGoogle Analytics by Yoast\nThis is one of the most recommended plugins for WordPress. Behind its development is Yoast, so we can be sure of the quality of the plugin.\nThis plugin allows us to filter by user roles so that the visits to the page that have been caused by the administrators or authors themselves are not counted. Thanks to this advantage you will have more reliable statistics and you will have much clearer visits.\nAlso noteworthy is the option offered to authenticate in Google Analytics using the Google API with just a click of a button.\nThis plugin is developed by SumoMe and, in addition to allowing you to insert the Google Analytics code in a simple way, it also allows you to get the Analytics information in the WordPress dashboard itself.\nAlthough it does not have as many features as other plugins, one of the advantages it has is that it is very flexible. For example, you will have no problem choosing the Analytics you prefer, either the classic or the 4.\nNK Google Analytics\nThe main advantage that NK Google Analytics has over other plugins is the large amount of information displayed in the WordPress panel through the API.\nThis plugin also has the advantage of allowing you to insert the cookie warning bar and is fairly simple to configure. It is true that it is not as flexible as other plugins, but it also gives you the option to choose the Analytics you want for your page.\nSimple Universal Google Analytics\nAs its name suggests, it is one of the simplest plugins you can find. Simple Universal Google Analytics is designed only to insert the Google Analytics tracking code.\nAs it hardly has different configuration options, this plugin is basically recommended for those who do not need anything else.\nAs you can see, installing Google Analytics on your website is a simple task that in turn will provide you with a series of data necessary to control the proper functioning of your website. Thanks to GA you will know if the actions you perform on your website have a positive or negative impact on the actions that users perform on it (your sales percentage increases, the number of visits you receive from a certain country or city increases, the bounce rate decreases, etc).\nIf you have any doubts or problems when installing Google Analytics on your website, you can contact us so we can help you, we will be happy to do it!","Databricks Metadata Management — FAQs, Tools, Getting Started\nSeptember 15th, 2022\nShare this article\nDatabricks enables metadata management of its lakehouse assets using Unity Catalog — the metadata and governance layer with automated lineage.\nWhile Unity Catalog is effective for Databricks assets, it’s not enough when you must must view the metadata from Databricks along with other data tools in your stack. That’s where a catalog of catalogs like Atlan can help.\nIn this article, we’ll discuss the importance of metadata management for Databricks assets, the capabilities of Unity Catalog, and the benefits of integrating it with Atlan.\nWhy is metadata management important for Databricks assets?\nMetadata is the key to finding and analyzing every data asset (schema, column, dashboard, query, or access log) and understanding how they affect your workflows. Since warehouses are organized and structured, handling metadata within each warehouse isn’t too complex. The challenge mainly lies in making the metadata searchable and easy to understand.\nHowever, the data ingested into data lakes don’t have a specific format or structure. Without a mechanism to organize and manage metadata, data lakes can quickly transform into nightmarish data swamps (also known as data dumps or one-way data lakes). In such an environment, data gets stored but isn’t in any shape to be used.\nAs a result, there’s no context on data, leading to the chaos around questions such as:\n- When was a data set created?\n- Who’s the owner?\n- Where did it come from?\n- What’s the data type?\nThis information comes from the metadata and its effective management.\nManaging metadata is also the most effective way to ensure data quality and governance by tracking, understanding, and documenting all types of metadata — business, social, operational, and technical metadata.\nFor instance, metadata management can help you ensure that the data uploaded to a table matches its schema. Similarly, it helps you verify if someone’s allowed to view a specific table of sensitive data and logs all accesses automatically for easier audits.\nThat’s why data lakes and lakehouses (like Databricks) can only be effective with a proper metadata management system in place.\nBut before exploring metadata management in Databricks, let’s understand its history and origins.\nWhat is Databricks?\nDatabricks is a company that offers a cloud-based lakehouse platform for data engineering, analytics, and machine learning use cases.\nThe platform started as an open-source project (i.e., Apache Spark™) in academia. One engineer describes the core Databricks platform as “Spark, but with a nice GUI on top and many automated easy-to-use features.”\nToday, Databricks is valued at $38 billion and is considered to be one of the world’s most valuable startups, joining the ranks of Stripe, Nubank, Bytedance, and SpaceX.\nDatabricks: An origin story (TL;DR)\nThe official release of Hadoop in 2006 helped companies power their data processing and analytics use cases with horizontal scaling of database systems. However, Hadoop’s programming framework (MapReduce) was complex to work with and led to the development of Apache Spark™.\nAli Ghodsi and Matei Zaharia set out to solve the problem of simplifying working with data by developing Apache Spark™ — an open-source platform for processing massive volumes of data in different formats. Companies used Spark to directly read and cache data.\nAccording to Zaharia:\n“Our group was one of the first to look at how to make it easy to work with very large data sets for people whose main interest in life is not software engineering.”\nApache Spark™ was so good that it set the world record for speed of data sorting in 2014.\nHowever, setting up Apache Spark™ for clusters of servers and adjusting various parameters to ensure peak performance was tedious and challenging. This eventually brought the creators of Apache Spark™ together with other academics to collaborate on the Databricks project — a cloud data lakehouse platform that’s powered by Apache Spark™.\nTo understand the significance of the project, let’s go back to 2015 and look at the problems with the existing warehouse-lake data architecture.\nWhy did the lakehouse architecture become so important?\nIn 2015, when cloud data lakes such as Amazon S3 and Azure Data Lake Storage (ADLS) became more popular and replaced the Hadoop systems, organizations started using a two-tier data architecture:\n- A cloud data lake as the storage layer for large volumes of structured and unstructured data\n- A cloud data warehouse, like Amazon Redshift or Snowflake, as the compute layer for data analytics\nAccording to the founders of Databricks — Michael Armbrust, Ali Ghodsi, Reynold Xin, and Matei Zaharia — such an architecture was:\n- Complex: Data undergoes multiple ETL and ELT processes.\n- Expensive: Continuous ETL processes incur substantial engineering overhead. Moreover, once the data is in the warehouse, the format is already set, and migrating that data to other systems involves further costly transformations.\n- Unreliable: Each ETL step can lead to new failure modes or bugs, affecting data quality. Additionally, the data in the warehouse is stale, when compared to the lake data — constantly fresh and updated. This lag leads to increased waiting time for insightful data and introduces engineering bottlenecks as business teams wait for engineers to load new data in the warehouses.\n- Not ideal for advanced analytics: ML systems such as PyTorch and TensorFlow process large volumes of data using non-SQL queries. Warehouses aren’t equipped to handle these formats, leading to further data transformations for each use case.\nThe solution is to build a platform that combines the best of both worlds — the open data format of a lake and the high-performance and data management capabilities of a warehouse.\nThat’s how the concept of a lakehouse, pioneered by Databricks, rose to prominence.\nThe USP of the lakehouse architecture: Metadata management\nArmbrust, Ghodsi, Xin, and Zaharia define a lakehouse as:\n“A low-cost, directly-accessible storage with traditional analytical DBMS management and performance features such as ACID transactions, data versioning, auditing, indexing, caching, and query optimization.”\nA lakehouse has five layers — ingestion, storage, metadata, API, and consumption. The key difference between the lake-warehouse architecture vs. the lakehouse architecture is the metadata layer.\nThe metadata layer sits on top of the storage layer and “defines which objects are part of a table”. As a result, you can index data, ensure versioning, enable ACID transactions, and support other data management capabilities.\nThis metadata (now called metadata and governance layer) is the Databricks Unity Catalog, which indexes all data within the lakehouse architecture, such as the schemas, tables, views, and more.\nWithout this metadata layer, each Databricks workspace will be a silo.\nHere’s how Databricks describes its Unity Catalog:\n“Unity Catalog is a unified governance solution for all data and AI assets including files, tables, machine learning models, and dashboards in your lakehouse on any cloud.”\nThink of Unity Catalog as a collection of all databases, tables, and views in the Databricks lakehouse.\nA Guide to Building a Business Case for a Data Catalog\nDownload free ebook\nDatabricks metadata management capabilities: Metadata management in the Unity Catalog\nUnity Catalog is an upgrade to the old meta store within Databricks, the caveat being better permissions model and management capabilities. Using Unity Catalog, you can manage schemas, access controls, sensitive data cataloging, and generate automated views for all Databricks assets.\nThe Databricks Unity Catalog enables:\n- Easy data discovery\n- Secure data sharing using the Delta Sharing protocol — an open protocol from Databricks\n- Automated and real-time table and column-level lineage to understand data flows within the lakehouse\n- A single and unified permissions model across all Databricks assets\n- Centralized auditing and monitoring of lakehouse data\nLet’s look at the most prominent use cases powered by the metadata and governance layer of Databricks.\nWhat use cases can you power with metadata management of Databricks?\nDiscovery and Cataloging\nWith metadata management, it’s easier to discover trustworthy data and get adequate context on it as it indexes every asset inside the lakehouse — data sets, databases, ML models, and analytics artifacts.\nLineage and impact analysis\nMetadata management powers lineage capability across tables, columns, notebooks, workflows, workloads, and dashboards. As a result, you can:\n- Trace the flow of data across Databricks workspaces\n- Track the spread of sensitive data across datasets\n- Monitor the data transformation cycle\n- Analyze the impact of proposed changes to downstream reports\nCentralized governance, security, and compliance\nModern metadata management platforms automatically identify and classify PII and other sensitive data in the lakehouse. In addition, it maintains centralized audit logs on data access and use (as mentioned earlier), which helps govern usage.\nThey also centralize access control with a single and unified permissions model. As a result, access control and data privacy compliance have become simpler.\nThe Unity Catalog is sufficient for metadata management of Databricks lakehouse assets, however, the modern data stack has several tools (dashboards, CRMs, warehouses), each with a metastore acting as the tool’s data catalog. To get a 360-degree view of all data in your stack, you must set up a catalog of catalogs with active metadata management, and that’s where Atlan comes into the picture.\nAtlan + Databricks: Activating Databricks metadata\nAtlan activates your Databricks data with active metadata management — a two-way flow of metadata across data tools in your stack. Think of it as reverse ETL for metadata.\nWhy does active metadata management matter?\nThe core premises of active metadata management are:\n- As and when data changes, the associated metadata also changes automatically\n- As data is used, updated, or modified, metadata keeps getting generated and cataloged\n- The management of metadata should be automated using open, extensible APIs\nAtlan checks all three boxes to provide you with:\n- An active, living single source of truth with 360° profiles of data assets\n- Proper context on data using the tools of your choice, i.e., embedded collaboration\n- An open API architecture, empowering you to connect Unity Catalog’s REST API with Atlan to extract metadata from Databricks clusters and workspaces\nWhen you pair the Databricks metadata with metadata from the rest of your data assets, you can achieve true cross-system lineage and visibility across your data stack.\nThe benefits of activating Databricks metadata using Atlan\n- Build a home for all kinds of metadata — technical, business, personal, and custom metadata with Atlan’s no-code Custom Metadata Builder\n- See column-level data flows from your lakehouse to BI dashboards without switching apps\n- Analyze which downstream assets will get affected by changing any Databricks asset and alert the relevant data owners\n- Create Jira tickets for broken assets for action on broken assets\n- Discuss the impact of these changes using your tool of choice, like Slack\nTo see Atlan and Unity Catalog in action, enabling active metadata management, check out this live product demo:\nA Demo of Atlan and Databricks Unity Catalog in action\nRelated resources on integrating Unity Catalog with Atlan\n- How to set up Databricks\n- How to crawl Databricks\n- How to extract lineage from Databricks\n- What does Atlan crawl from Databricks?\nIf you are evaluating an enterprise metadata management solution for your business, do take Atlan for a spin — Atlan is a third-generation data catalog built on the premise of embedded collaboration that is key in today’s modern workplace, borrowing principles from GitHub, Figma, Slack, Notion, Superhuman, and other modern tools that are commonplace today.\nDatabricks metadata management: Related reads\n- What is a data lake and why it needs a data catalog?\n- What is active metadata management? Why is it a key building block of a modern data stack?\n- Enterprise metadata management and its importance in the modern data stack\n- Data catalog vs. metadata management: Understand the differences\n- Activate metadata for DataOps\n- Databricks data governance: Overview, setup, and tools\nShare this article"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:cdaf4e4f-28bb-46bc-9062-a35473746b69>","<urn:uuid:4aeea8f5-db05-47f0-983e-307110480c84>"],"error":null}
{"question":"What parallels exist between memory retrieval methods and the prophetic elements in Arthurian legend?","answer":"Memory retrieval and Arthurian prophecy operate through different but comparable mechanisms of accessing information about the future versus the past. While memory retrieval works through sequential access for short-term memory and associative connections for long-term memory, Merlin's prophecy about Arthur's destiny follows an associative pattern, linking current events to future outcomes. Just as long-term memory retrieval connects related pieces of information, Merlin's foretelling connects Arthur's present circumstances to his inevitable fate at Mordred's hands.","context":["The day of destiny in \"Le Morte d'Arthur\" is a significant event in the Arthurian legend, as it marks the end of the reign of King Arthur and the beginning of a new era. In the story, the day of destiny is foretold by the wizard Merlin, who tells Arthur that he will meet his fate at the hands of his illegitimate son, Mordred, on the battlefield.\nDespite knowing his fate, Arthur remains determined to protect his kingdom and his people, and he prepares for the final battle with Mordred. On the day of the battle, Arthur and his knights gather on the field, ready to fight for their cause. Despite their valiant efforts, the outcome of the battle is inevitable, and Arthur is fatally wounded by Mordred.\nThe day of destiny marks the end of an era and the beginning of a new one, as Arthur's death signals the end of the Arthurian golden age and the start of a time of chaos and uncertainty. However, Arthur's legacy lives on through the stories and legends that have been passed down through the ages, and he is remembered as a great and noble king who fought for justice and righteousness.\nIn conclusion, the day of destiny in \"Le Morte d'Arthur\" is a poignant and significant moment in the Arthurian legend, marking the end of an era and the beginning of a new one. Although Arthur meets his fate on the battlefield, his legacy lives on through the stories and legends that have been passed down through the ages, and he will always be remembered as a great and noble king.\nAwesome Psychology Experiment Ideas for High School Students\nIt also gives some hints why for example in the Nazi dictatorship so many ordinary people became delinquents, doing unimaginable gruesome things. Are attractive people more likely to be perceived as kind, funny, and intelligent? Developmental Psychology Research Topics Developmental Psychology explains how and why the thinking, feeling, and behaviors of humans change throughout their life. How do you convince them that you are now cured, better? Some couples do choose to take their seats however, and are rewarded with cheers from the crowd and a round of free Carlsberg beers. Don't drive, of course. To be fully prepared, take the maximum advantage from studying. Afterwards, participants filled out a survey, which asked the question, \"About how fast were the cars going when they smashed into each other? During the 1950s, psychologist Solomon Asch conducted ï»¿ï»¿ The study revealed that people are surprisingly susceptible to going along with the group, even when they know the group is wrong.\nNot only should students be exposed to varied experiments, but also be asked to conduct them as well. To get our psychology assignment writing help online, simply place your order by filling your requirements in the order form. It is an application of management principles to the people working in the organization. Realising the power of their experiment, the researchers tried to undo the damage they had done, but to no avail. As a result, whichever group was favoured by Elliott performed enthusiastically in class, answered questions quickly and accurately, and performed better in tests; those who were discriminated against felt more downcast, were hesitant and uncertain in their answers, and performed poorly in tests. Mischel was interested in learning whether the ability to In the experiments, children between the ages of 3 and 5 were placed in a room with a treat often a marshmallow or cookie.\nIn case, you are asked to bring a good psychology research topic, then during topic selection, this is what you should do. If you can do voices or at least accents, do a munchkin or Tweety or Elmer Fudd all day. Ask some participants to describe about how fast the cars were going when they. The results of the study highlight how children learn behaviors from observing others. In that case, these topics can be brilliant ideas for your paper. Using children of ages four to six as subjects, they were led into a room where a treat usually a marshmallow, but sometimes a cookie or pretzel stick , was placed on a table, by a chair. The Conformity Experiment Aim: To test the tendency of humans to conform to the opinion of a group rather than maintain their own.\n17 Social Experiments That Led To Unexpected Results\nFor example, a study in Psychological Science demonstrates that people under stress tend to eat high-calorie foods. Consider an idea from the list above or turn some of your own questions about the human mind and behavior into an experiment. You need to find something that meets the guidelines and, perhaps most importantly, is approved by your instructor. Adults considering infants stupid little thing are, well, stupid, and those who say that they do not feel pain, do not \"remember\" distress, or let them cry because otherwise they \"would learn to always get their will\" are simple cruel. The bystander effect exists, but the Kitty Genovese case is a bit more complex.\nLecture Notes in Computer Science. Spend the day role-playing as Forest Gump, or somebody at about the same IQ level. While he taught his students about Nazi Germany during his \"Contemporary World History\" class, Jones found it difficult to explain how the German people could accept the actions of the Nazis, and decided to create a social movement as a demonstration of the appeal of fascism. Zimbardo even admitted he began thinking of himself as a police superintendent rather than a psychologist. Journal of Applied Psychology.\n28 Psychological Experiments That Revealed Incredible And Uncomfortable Truths About Ourselves\nTheory Law of effect —Those actions tend to be repeated that is followed by a positive consequence and vice-versa. They were lectured about stuttering and told to take extra care not to repeat words. Of those who attempted to delay, one third deferred gratification long enough to get the second marshmallow. In one rather amusing social experiment, which actually All but two of the 150 seats were already full. In that case, you need a relatively simple topic to research and write about and still impress your educator. While it can feel intimidating to ask for help, your instructor should be more than happy to provide some guidance. The children could eat the treat, the researchers said, but if they waited for fifteen minutes without giving in to the temptation, they would be rewarded with a second treat.\nFind here, some interesting social psychology research topics. In some cases the participant was alone, in some there were three unsuspecting participants in the room, and in the final condition, there was one participant and two confederates. Just like you were there for that purpose. According to the classic tale, while multiple people may have witnessed her attack, no one called for help until it was much too late. Do you trust Facebook to look after your best interests? They were then divided into two groups. It wasn't until the second phase of the experiment that the children learned that there was another group, at which point the experimenters placed the two groups in direct competition with each other. This would certainly make evolutionary sense as other human faces hold all sorts of useful information which is vital for our survival.\nPsychology Project Ideas & Topics in 2023[For Freshers]\nYou believe in chemtrails? In some cases, you can have a tight deadline within which to submit or present your paper. How caffeine consumption affects the brain. The social media giant manipulated the news feeds of 689,003 people for one week, prioritizing either positive or negative emotional content. This article discusses some ideas you might try if you need to perform a psychology experiment or study. Results: When alone, 75% of people reported the smoke almost immediately. Use an alternative operating system. In case, you have less knowledge of psychology research paper writing, feel free to read this blog post.","Memory is the process of maintaining information over time. – Matlin, 2005\nThe general understanding of memory is storing certain information, which of course, isn’t a complete definition of the memory as a whole.\nSternberg defined memory as the means to draw past experiences in order to assess the information at the present. The complete process of structuring and processing the information involved in the storage and retrieval of such information can be defined as memory.\nMemory processes limitless amount of information every day, and information is stored in different forms like meaning, sounds and images. From storing the information to the final processing in order to produce a desired outcome, memory can be categorized into three stages.\nEncoding is the first stage of memory. As the term suggests, this is the stage of memory which accumulates all the information from the surrounding and encodes or stores it in our brain. The information we intake from the world around us is processed in three different forms.\n- Visual (picture)\n- Acoustic (sound)\n- Semantic (meaning)\nIn simple words, these different forms are how we take in the information. We either consume information as a picture, a sound, or we make it meaningful. These three different forms are termed as visual, acoustic and semantic accordingly. Encoding can also be defined as the process used to remember the information. We retrieve information in the same form we take it in because of this process.\nFor instance, consider remembering a certain telephone number. You are seeing it, means it’s visual coding. If you’re repeating the number to yourself verbally in order to remember it, that’s acoustic coding. Likewise, if you perceive that the last three numbers of the phone number are same, all 3s, you are giving meaning to it. All three forms of encoding takes place so that you can remember that particular number.\nIt has been found out that acoustic coding is the major form of encoding in Short Term Memory. Likewise, the major encoding system for the Long Term Memory seems to be semantic, with meaning. But it’s not just limited to that, and can be done in visual and acoustic forms too.\nThis stage deals with nature of the memory where the information is stored, time duration of the memory, the amount of information that can be stored, and type of memory. The manner in which information is stored directly affects the way in which information is retrieved. Information is stored in two main parts of memory.\n- Short Term Memory (STM)\n- Long Term Memory (LTM)\nMiller stated in 1956 that most adults are capable of storing within 5 to 9 items in their STM, short term memory, and he called it the magic number 7 (plus or minus 2). According to findings of Miller, there is limited number of slots in Short Term Memory; however, he didn’t specify the amount of information that can be stored in each memory slots. This gives the idea that more than one information can be stored together if information can be related and chunked, one upon another.\nTotal capacity of Short Term memory is said to be around 0 – 30 seconds.\nLong Term Memory, however, is a whole different ball game when it comes to memory. Its capacity is said to be unlimited and the information stored can last a whole life time.\nFor example, we might remember the color of someone’s dress only for a while if it’s stored in short term memory. However, we remember that the sun rises from the east, and we will probably remember it for the rest of our lives, which is the information stored in long term memory.\nAs the term suggests, retrieval refers to retrieving information out of our memory storage. Failure to retrieve information is often understood as not being able to remember or recall the particular information. Trying to retrieve information from our memory makes the differences between STM and LTM pretty apparent.\nSTM is both stored and retrieved in a sequential manner. For example, let’s say a subject is given a phone number to remember and then asked the second last number on the list. The person goes through the number from the start to retrieve the required information.\nLTM is both stored and retrieved by association. Remembering a certain action might lead to retrieval of information about some other actions. For instance, traveling the same road you were walking the day before might lead you to retrieve information about the girl you had seen the day before.\nMemory organization is one of the ways to increase your ability to retrieve memory. Information can be organized alphabetically, by time, by size, or by any other means you deem fit. This will help you recall the information in a swift manner as you’ll be more comfortable with the manner in which you organized the information.\nCritical Evaluation of Experiments Related to Memory\nOne of the major criticisms about the subject matter has been the ecological validity of the findings. Most memory experiments are conducted in a controlled environment, a laboratory, with pre-planned set of questionnaires and methods. Psychologists have raised questions about the validity as the experiments are not in general, and these findings might not be realistic representation of the every-day world."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:7c16bf4e-a55c-4a17-b8e4-15800db16c03>","<urn:uuid:5034c0c2-28a5-4074-bd97-4c463d681624>"],"error":null}
{"question":"As a landscaper specializing in vertical gardens, I need to understand both the climbing mechanisms of different vines and their training requirements. What are the main climbing methods vines use, and what support structures do they require for proper growth?","answer":"Vines climb in several distinct ways: Some use modified tendrils with circular discs (like Boston ivy), others form small rootlets along the stem (like English ivy and winter-creeper), while some climb by winding shoots or leaf-like appendages acting as tendrils (like clematis and grape). Twining vines coil their stems around vertical supports as they grow. For proper support, these vines require different structures - a trellis with galvanized 10- or 11-gauge wire is recommended, with wooden end-posts at least 6 inches in diameter and 8 to 8½ feet long, set about 3 feet deep. It's important to note that vines with rootlets or disc-like appendages should not be grown on wooden houses or wooden parts of brick houses.","context":["A Guide To The Different Types Of Vines\nVines can add a fast growing vertical element to your garden, barn, or house. Most vines will grow toward sunlight by elongating their stems and attaching themselves to support where available, making them great for walls, trellises, archways and pergolas.\nVines are usually valued for the summer shade they provide when trained over an arbor or patio roof. Other varieties add interest when trained against the sunny wall of a building or planted to frame a doorway. Vertical vines can be used to provide a needed accent to relieve the monotony of a large expanse of fencing or even hide a chain-link fence with green foliage. Vine varieties such as ivy, memorial rose, and winter-creeper euonymus can be useful on steep banks. Perhaps the greatest benefit of vines, for the gardener, is the small amount of ground space they require for growth. Many are happy to remain planted in containers throughout the garden.\nWhen selecting vines, always consider temperature hardiness and light requirements. The amount of sunlight your vines will receive can be critical to their survival, or it may reduce flowering and fruiting. Check with a qualified nursery to determine which vines are appropriate for your climate zone. Most vines are quite tolerant of a wide range of soil types.\nDifferent Types Of Vines Vines are usually described as woody or semi-woody climbing or trailing plants, however some vines are annuals and have herbaceous stems. Each species and cultivar of vine possesses distinctive characteristics making it suitable for a specific location.\n- Balloon vine Black-eyed Susan Cup and saucer Firecracker (mina lobata) Hyacinth bean Moonflower Morning glory\n- American bittersweet Fiveleaf Akebia Boston ivy Clemantis Dutchman’s pipe English ivy Gloryvine Hydrangea Oriental bittersweet Passionflower Silver fleecevine Trumpet vine Wintercreeper euonymus Wisteria\n- Bean (pole) Bitter melon Blackberry Cantaloupe (vining types) Chayote Kiwi Nasturtium (vining types) Scarlet runner bean Sweet pea Sweet potato, ornamental\nFast-growing Vines For Quick Shade\nGrowth rate and sun exposure requirements should be considered when selecting vines. Some vines, such as oak leaf hydrangea, grow slowly and are appropriate when only a small area needs covering. Virginia creeper and Boston ivy are astonishingly fast growers and should not be selected for small areas. But these are good choices when a lot of shade is needed in just several growing seasons.\nFlowering Vines For Garden Color\nVines that provide color in the garden come in an amazing range of flowering color choices. The passionflower vines are available in pink, purple, and blue. Moonflowers come in white and purple. Morning glories are available in lavender, pale yellow, pearl, strawberry cream swirl, blue, red, and a wide variety of variegated faced colors. Flowering Vines\n- Blue butterfly pea Blue crown passionflower Bush potato vine Cardinal climber Flying saucer morning glory Lil red trumpet creeper Lil violet trumpet Maypop passionflower Messina creeper morning glory Orange Noah trumpet creeper Purple passionflower Purple moonflower Spanish flag White night-blooming moonflower\nClimbing Vines Vines are generally divided into groups based on their method of climbing. Some plants climb by attaching small appendages as a means of support. Boston ivy has modified tendrils with small, circular discs at the tips; English ivy and winter-creeper form small rootlets along the stem. These types of vines should not be allowed to grow on wood houses or wooden parts of brick houses such as window frames, and eaves with wooden rafters.\nVines, such as clematis and grape climb by winding shoots or leaf-like appendages which act as tendrils around the object on which they are growing. Most vines climb by twining stems. As the growing tips elongate, the stem coils around the nearest vertical support. Avoid planting twining vines near small trees and shrubs because they may become difficult to control. Vines such as bittersweet and wisteria climb by twining.\n- Akebia Black-eyed Susan Bougainvillea Cardinal climber Cypress vine Climbing hydrangea English ivy Gloxinia Mandevilla Wintercreeper\nWhen planted in the correct location and properly trained, vines can provide a high degree of colorful visual interest, fragrance, and enjoyment in your garden. They can be used in a number of imaginative ways, such as hanging baskets, window boxes, and plant containers with trellises. Vines also make a good solution for blank and boring walls around the yard. Plus, you are not limited to the sunny side of buildings. The Boston ivy, climbing hydrangea, and other vines can be successfully grown in partial shade.","Home Fruit Production: Grape Training Systems\nMichele R. Warmund\nState Fruit Specialist\nDivision of Plant Sciences\nPruning, or training, is one of the most important and most neglected practices in home plantings of grapes. Grapes need some form of support, and pruning is necessary to develop the plant and to maintain it on the support provided. Regular, purposeful pruning is essential for controlling the number, position and vigor of fruiting canes and the yield and quality of the fruit.\nGrapes should be pruned during the dormant season, late December to March. Late-winter pruning is generally preferred because fruiting canes are likely to experience some extent of winter injury. If pruning is delayed until near bud swell, the cuts commonly ooze sap abundantly. Though not desirable, such “bleeding” seems to be of minor importance.\nSome knowledge of grapevine parts is helpful in understanding pruning details (Figure 1).\nThe main permanent stem of the plant.\nThe immature, soft stem growths of the current growing season. Shoots arise from buds on wood one or more years old and bear leaves, flowers and fruit.\nThe mature shoots; those that have become woody after growth has ceased. Fruiting cane refers to a 1-year-old cane that is capable of and suitable for bearing fruit.\nSide extensions from the main trunk that are often trained horizontally along a trellis wire for multiple growing seasons.\nOne-year-old canes originating from a cordon and shortened to two to four buds. Shoots and, later, canes develop from the spur buds.\n- Renewal spurs\nSpurs pruned to two buds used to renew the curtain in later years.\nShoots or canes usually arising from the lower part of the trunk.\nMature grapevine depicting summer vine growth on left and, on right, spurs with buds after pruning in March.\nNumerous training systems can be used for grapes. However, the bilateral cordon system is useful for many grape cultivars because it allows excellent light and spray penetration, yields high-quality fruit, is easy to prune and pick, and requires minimal tying.\nGrapes trained to a bilateral cordon system require a trellis. The end-posts should be wooden, at least 6 inches in diameter and from 8 to 81/2 feet long. When end posts are set about 3 feet deep and properly braced, the wire can be stretched fairly tight. Other posts may be smaller and need not be set as deeply as end posts. Oak posts treated with a preservative are satisfactory. Steel posts may be used for all except the end posts.\nThe trellis wire should be smooth, galvanized, 10- or 11-gauge wire. Post spacing, wire height and other details are shown in Figure 2.\nConstruction details of a two-wire trellis and alternative methods of bracing end posts.\nImmediately after planting a dormant vine, remove all but one shoot (Figure 3A). Cut the remaining shoot back, leaving only two buds. Tie a string from the base of the plant, near the soil surface, to the trellis wire. About a month later, train the strongest-growing shoot up the string and let the other shoot grow along the ground.\nWhen the vine reaches the trellis in year one or two, pinch off about an inch of the terminal shoot to force the vine to branch. Train and tie the two lateral branches to the wire to form two cordons (Figure 3B). Remove all flower clusters in year two.\nDuring the next dormant season, in early March, prune the lateral growth on the cordons (Figure 3C). Select 1-year-old canes spaced 6 to 12 inches apart along the cordon to form the spurs, and cut each back to four buds. Leave a renewal spur between each pair of spurs. Cut each renewal spur back to two buds. After the danger of frost has passed, growth from one or two buds per spur can be removed if vines are growing vigorously and were undamaged by low temperatures.\nGenerally, during the third growing season, growth from the lateral-growing cordon produces the “curtain” (Figure 3D). If vines are spaced 8 feet apart, each cordon should be maintained at a 4-foot length. Position the growth from the cordons downward during the growing season, and allow only one cluster of fruit to set during this year. In following years, eliminate the old spurs during dormant-season pruning and use renewal spurs to develop the curtain.\nThe cordons require periodic renewal, especially if one has been severely injured by low temperatures, insects or disease. Simply train another well-positioned shoot that arises near the base of the old cordon on the trunk, grow the new shoot for a year, and remove the old cordon the next year.\n(A) First growing season. (B) Second growing season. (C) Dormant spur pruning. (D) Third growing season.\nThe fan system is useful for training grapes to walls and fences or to a special trellis or arbor (Figure 4). A plant pruned and trained to this system has several upright canes branching from arms on a very short trunk. This system is ideal for plants that naturally grow upright or for weak vines.\nPrune the newly set plant to a short two-bud spur. If the plant grows well in the first season, both of the canes (one from each bud) may be left after shortening each to three or four buds. A weaker plant may be left with two spurs of two buds each.\nAt the beginning of the third season, the plant may have four or more good canes. Select three or four of the best for fruiting canes. If others are present in a fairly low position, leave two or three of them as two-bud spurs and remove any others. Shorten the fruiting canes to the height of the trellis, and tie them at the tip. Position the canes in a spreading fanlike arrangement.\nThe number of fruiting canes may be increased to six or eight in later years, depending on the length of the canes, or number of buds, and plant vigor. Select fruiting canes from renewal spurs where possible. Otherwise, select them from near the base of last year’s fruiting cane and, if possible, leave three or four spurs for use the following year.\nPlant pruned to fan sysem at the beginning of third season.\nMany people prefer to grow grapes on an arbor, thus combining the benefits of fruit production, shade and ornamental effects (Figure 5). Vines grown on arbors generally produce less low-quality fruit than traditional systems do. An almost endless number of arbor designs are suitable for training grapes. More important than design is that the arbor be constructed of durable materials requiring minimal maintenance.\nIn general, plants are placed on both sides of an arch-like structure and trained to grow up and over to about mid-point of the top. Providing this amount of foliage cover requires a larger and taller plant than is necessary for an ordinary trellis. In this situation, the tendency is to prune too lightly, if at all. Often the result is a tangled mass of multiple trunks, numerous canes, weak growth and poor fruit production. Training the plants to a single trunk and leaving relatively short horizontal fruiting canes is a suitable method for most situations.\nDevelop a portion of the trunk each year by tying an uppermost vigorous cane in a vertical position. At the same time, select fruiting canes at intervals of 2 to 3 feet. These should be limited to five or six buds to favor development of the upper trunk and canes. The selection and use of renewal spurs is also valuable for maintaining a source of fruiting wood close to the trunk.\nGrapes trained and pruned on an arbor.\nFor tying vines, use a material that will last the entire season. Binder twine or a lighter jute twine is satisfactory.\nTying should be done during relatively mild temperatures, but before buds have swollen or started growth. Buds that are quite swollen are easily broken off. Canes that are very cold or frozen are brittle and easily broken. Such breakage is particularly likely when sharply bending the canes."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:e6d571c9-a197-40ef-abd5-9b01a7aec866>","<urn:uuid:13c9a06f-bfc9-4ffe-934a-c7e2820a28f4>"],"error":null}
{"question":"As a genealogy enthusiast working on constructing a comprehensive family narrative, what methodology should I employ for conducting effective relative interviews? #FamilyHistory","answer":"When interviewing relatives, start with your parents and then proceed to interview aunts, uncles, grandparents, and family friends. Don't limit yourself to collecting dates and names - ask open-ended questions to discover surprising stories and anecdotes about family members, as these can provide valuable insights into your family history.","context":["Doing a family history search is an important thing for most people who wish to know about the legacy of the past generations of their family tree. Others simply want to know where their ancestors originated or to have a better understanding of their past. Doing a family history search can be made easier and more fruitful by following these tips.\nOne of the first things that would help you get some information about your family tree is by starting with what you currently have. Before you delve into the distant past of your family tree, you have to find out what you can about the present. You can start by rummaging through your own home for that piece of family history.\nYou would need to collect any documents that you may have with you that will help give you clues of your family history. This would include gathering family photos, heirlooms passed down to you, important family documents like birth, marriage and death certificates as well as other similar things. You can also ask from relatives who may also have other similar things and documents that may prove helpful in trying to piece together your family history search. If they won’t part with such important documents, you can have copies made of them as well as photographs taken as your own proof.\nAside from gathering documents and similar objects from the family’s past, it may also be very helpful to do some interviews with relatives and other people who might have important information about your family’s history. You can start with interviewing your own parents; you can then proceed to interviewing aunts, uncles, grandparents, family friends, etc. Don’t just limit yourself to knowing about dates and names. You can also try to gather stories and anecdotes about certain family members which might also help provide certain insights to your family history. Try to ask open ended questions more in order to discover certain surprises and story nuggets.\nOnce you have gathered certain information concerning your family’s history, try to construct a pedigree or a family tree chart. There are different ways in which a family tree chart can be made and there are usually step by step instructions that might help you make one. Even at such an early time, having a family tree chart on hand can make you keep track of your progress when it comes to finding out about your family history bit by bit. Having a starting family tree chart can also help you discover some of the certain areas or blanks in the chart where you might still need to gather more information in order to fill up.\nIf you require important documents from the past, there are certain places that you might need to know about. You might want to head to the local family history center where you might be able to find certain information about your family ancestors. There are courthouses, churches as well as cemeteries that you can visit in order to get some additional information about your ancestors that may help fill up the missing parts on the family tree chart.\nThere are local government offices and agencies that you may want to check out in order to be furnished with certain records such as wills, birth and death certificates, marriage certificates, immigration or business documents that may help you find additional information to build up your family history search information."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:c8f7d9c5-7003-4e77-9c05-4249b5ea841a>"],"error":null}
{"question":"Hey chainsaw experts! Just got my grandpa's old saw and noticed it's cutting slower than it should. How can I tell if it really needs sharpening? 🪚","answer":"A sharp saw should cut at a rate of about one inch per second. If you find yourself pushing harder to make cuts, that's typically the first sign that sharpening is needed. Make sure when sharpening that the depth gauge (or 'raker') is 25-35 thousandths of an inch below the cutting tooth for optimal performance. Simply filing down the depth gauge to improve cutting speed actually slows the chain and makes the saw overly aggressive.","context":["Here you will find tips and tricks that I think you might like from the Chainsaw Strategies archives.\nI will also post helpful information about products that I find to be helpful or ones that I would recommend.\nI also welcome your comments and will answer questions. Just send me an email with your question or comment. I'll respond and, with your permission, publish the one's I think are informative or \"interesting.\"\nProper maintenance is critical\nThis past weekend I got the pleasure to operate a chainsaw that belonged to my Grandfather. The saw is a Johnsered 535 Classic. That in it's self is not a big deal. But this saw has special meaning to me. It was given to me at the time of my Grandfathers passing and had not been run in years. My Grandfather told me before he died that the saw would not run and that the repair shop told him it was ...not worth fixing. I brought this saw home and with very little effort I got the saw running. My Grandfather passed away in 2001 and he had cut an unknown amount of wood with this saw, lets just say a lot. Now I use this saw on occasion to cut wood for my family. With a little maintenance and repair I was able to bring this saw back to life. Proper maintenance is a critical part of chainsaw safety. What's the first thing you do when a saw get's dull...(for those of you who have attended one of my classes you know the answer) nope it's not stop and sharpen it. Not usually, most often it is push harder. The second thing you do is sharpen. A sharp saw is a safe saw, if yours is not cutting well, take time to sharpen it, while you doing that take time to remember a loved one who has gone home to be with God. Every time I pick up my Grandfathers 535 I take time to remember this great man. Then I get the warmest feeling possible when I place my hands where his once were while I cut wood. This saw is not the fastest cutting saw I own but when I use it I'm not in a hurry.\nHow do you know when a saw is sharp? It is not by the size of the chip trust me. A sharp saw should cut at a rate of about one inch per second. Knowing the 5 parts of the saw tooth and their design characteristics is critical. Sometimes people file down the depth gauge or \"raker\" to improve cutting speed. This actually slows the speed of the chain and makes the saw overly aggressive. Having the depth gauge at 25-35 thousands of an inch below the cutting tooth is the correct height. Then, sharpen the tooth properly and your saw will preform at it's best and keep you more safe while cutting.\nNotch and hinge\nIf you're cutting trees you should know the proper size notch and hinge. The hinge is the last friend you have when the tree starts to fall and it controls the tree all the way to the ground if it is set properly. If you do not know how to do this please take the time to learn. The notch allows the hinge to function and also must be learned before cutting.\nMany times saw operators will form a 45 degree notch. This type of notch will likely close before the tree reaches the ground and either hang the tree or break the hinge prematurely allowing the tree to fall unpredictably. Try using an open face notch of 70-90 degrees.\nA properly sized hinge should be 10% of the trees DBH (Diamiter at breast height). So a 15 inch tree would have a hinge of 1.5 inches. The widest part of the notch should be 80% of the trees DBH\nso on a 15 inch tree it would be 13 inches. To form an open face notch place your saw against the tree trunk with the bar in an upright position. Pointing at your target make the first cut, a\ndownward cut until you obtain a 13 inch wide (15\"dbh tree) cut. Then remove the saw from the kerf, roll the saw to a horizontal position and while looking down the first saw kerf trim out the bottom\nof the notch. This will allow the hinge to work properly.\nPower Sharp is as fast as lightning\nFor all of you who prefer not to sharpen your own chain. Check out Oregon PowerSharp. I have used this chain system and it is fantastic!! This chain system is an Oregon chain, the initial purchase includes a new bar and sharpening device.\nAfter that each time you buy a new chain you get new stones to go in the sharpener. You could literally saw into concrete and then use this device to sharpen your chain to like new condition in a few seconds.\nI wouldn’t say this if I had not seen it with my own eyeballs!\n19 Sadoga Rd\nHeath, MA 01346"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:d3f66b6e-68d6-4419-80ce-db90a25f3d9c>"],"error":null}
{"question":"How do epigenetic factors regulate HSV-1 latency, and what role do post-translational modifications play in HIV chromatin regulation?","answer":"HSV-1 latency is primarily regulated by post-translational histone modifications rather than DNA methylation. The LAT region is uniquely enriched in marks indicating transcriptional permissiveness, specifically dimethyl H3 K4 and acetyl H3 K9 K14, while lytic genes show reduced levels of these marks. Additionally, lytic genes are enriched with facultative heterochromatin marks, particularly trimethyl H3 K27 and histone variant macroH2A during latency. Chromatin insulators, through CTCF protein binding, prevent heterochromatic spreading. In HIV infection, latency is established and regulated through modifications of nucleosomes bound to the viral promoter region. These modifications include histone deacetylation and methylation, which contribute to viral repression. The viral protein Vpr plays a crucial role by influencing how nucleosomes are modified, facilitating HIV replication and blocking the installation of repressive chromatin.","context":["Regardless of the eradication of smallpox several decades ago, variola and monkeypox infections possess the to be significant risks to open public wellness even now. important restorative modalities. This paper summarizes a number of the historical usage of the smallpox vaccine and immunoglobulins in the post-exposure establishing in human beings and reviews at length the newer pet research that address the usage of restorative vaccines and immunoglobulins in orthopoxvirus attacks aswell as the introduction of fresh restorative monoclonal antibodies. before starting point of symptoms), smallpox disease could possibly be altered. As the instances he gathered had smallpox, the data did not include cases that might have been fully protected by post-exposure vaccination. He reported on 19 cases exposed to smallpox that had previously never been vaccinated and were vaccinated during the incubation period. He found that two (10.5%) developed no symptoms, eight (42%) showed mild symptoms, seven (37%) showed moderate symptoms, and the vaccine did not alter the disease in two (10%) who developed severe symptoms. In contrast to vaccination before the onset of symptoms, he reported on 11 Lurasidone cases (previously never vaccinated) that were given vaccine after the Rabbit Polyclonal to NDUFB1. onset of symptoms. These symptomatic patients did not develop the characteristic vesiculopustular lesions following vaccination and three (27%) died, five (46%) developed severe disease, and three (27%) showed moderate disease. The reasons for not developing characteristic vaccine-related skin lesions could have been due to a failure of the vaccination procedure, or in these individuals, who had never been vaccinated, it is possible that the immune response towards the VARV disease may have suppressed VACV replication (just like individuals who’ve anti-vaccinia immunity from extremely latest or multiple vaccinations). Post-exposure vaccination was more lucrative in those vaccinated previously. In the organizations that got vaccination prior, he reported on 25 instances revaccinated through the incubation period and discovered that 21 (84%) demonstrated Lurasidone gentle symptoms, three (12%) demonstrated moderate symptoms, as well as the vaccine didn’t alter disease in a single (4%) who created serious symptoms. In the 19 instances revaccinated after advancement of symptoms, the forming of vesiculopustular lesions after vaccination had been poor (maybe again because of advancement of early immune system reactions to VARV), and in this group nine (47%) demonstrated gentle symptoms and 10 (53%) demonstrated moderate symptoms, most likely indicating some known degree of protection from their earlier primary vaccination . A similar changes in disease by post-exposure vaccination was recommended by Rao . He discovered that among those provided major vaccination at the time of exposure, 44 of 502 exposed (8.8%) developed modified-type smallpox, while only 15 of 1 1,453 exposed (1.0%) among unvaccinated patients developed modified-type smallpox. However, both groups developed ordinary type smallpox at similar rates: 85% (426 of 502) in the vaccinated group and 89% (1296 of 1453) in the unvaccinated group. Using slightly different outcome measures, Heiner showed a greater effect of post-exposure vaccination . In a group of 53 people who were vaccinated within seven days after smallpox exposure, only one (1.9%) developed smallpox. While in a combined group of 412 people exposed to smallpox who didn’t obtain vaccinated, 90 (21.8%) developed smallpox. Nevertheless, Lurasidone in this scholarly study, it was not yet determined how many of the smallpox open people have been previously vaccinated against smallpox. Within a more substantial epidemiologic research of smallpox outbreaks in Pakistan, Mack showed some aftereffect of post-exposure vaccination in smallpox vaccine na previously?ve sufferers . They determined 43 individuals who had been under no circumstances vaccinated and subjected to an active case of smallpox. Of those who did not get vaccinated within 10 days of the exposure, 26 of 27 (96%) developed smallpox. In those who received vaccination within 10 days of the exposure, 12 of 16 (75%) developed smallpox. Based on these types of observational studies, which are the only available data on post-exposure vaccination, it is accepted generally, that if provided early more than enough after publicity, post-exposure vaccination with live VACV vaccines may modify and stop smallpox disease potentially. That is, major vaccination completed early after publicity could at least protect people from serious disease and revaccination through the initial week of publicity of previously vaccinated people could prevent smallpox. Such conclusions have already been reached in a recently available analysis of outdated data models . 2.2. Pet Research of Post-exposure Vaccination with Replication Capable VACV Vaccines Because the eradication of smallpox, all data on post-exposure vaccinations for the treating poxvirus infections have already been produced using animal versions. In these scholarly studies, post-exposure vaccination facilitates the idea that vaccination might provide an effective methods to reduce the morbidity and mortality from poxvirus exposures in the crisis setting. However, a significant caveat to all or any of the scholarly research is that problem dosages and problem routes usually do not recapitulate the.\nLike other alpha-herpesviruses Herpes Simplex Virus Type 1 (HSV-1) possesses the capability to establish latency in sensory ganglia being a nonintegrated nucleosome-associated episome in the host cell nucleus. transcription isn’t governed by DNA methylation but most likely by post-translational histone adjustments. The LAT area is the just region from the genome enriched in marks indicative of transcriptional permissiveness particularly dimethyl H3 K4 and acetyl H3 K9 K14 as the lytic genes show up under-enriched in those same marks. Furthermore facultative heterochromatin marks particularly trimethyl H3 K27 as well as the histone variant macroH2A are enriched on lytic genes during latency. The distinctive epigenetic domains from the LAT as well as the lytic genes seem to be separated by chromatin insulators. Binding of CTCF a proteins that binds to all or any known vertebrate insulators to sites inside the HSV-1 genome most likely prevents heterochromatic dispersing Lurasidone and blocks enhancer activity. When the latent viral genome goes through stress-induced reactivation it’s possible that CTCF binding and insulator function are abrogated allowing lytic gene transcription to ensue. Within this review we summarize our current knowledge of latent HSV-1 epigenetic legislation when it comes to attacks in both rabbit and mouse versions. Lurasidone CTCF insulator legislation and function of histone tail adjustments will end up being discussed. We may also present a present-day model of the way the latent genome is certainly carefully controlled on the epigenetic level and exactly how stress-induced adjustments to it could cause reactivation. Lurasidone I. Launch to HSV-1 biology and latency The herpesviruses are huge enveloped infections that infect a broad spectral range of invertebrate and vertebrate types which range from oysters to guy. All herpesviruses talk about the defining characteristic of building a life-long latent infections of their hosts. The latent infections is certainly seen as a a shutdown of trojan replicative features and the shortcoming to identify infectious virus. Regular reactivation of Lurasidone the latent infections allows for following infections of various other hosts. During latency the 120-300 kb double-stranded DNA genomes of the viruses are preserved as multiple copies of round episomes inside the nuclei from the cells where they truly became latent. The department of herpesviruses into three sub-families alpha beta and gamma is situated largely on the sites of latency. This review shall concentrate on the latency from the alpha-herpesviruses which keep a latent infection within neurons. Particularly we will discuss the existing knowledge of epigenetic elements that regulate HERPES VIRUS (HSV) latent gene appearance within neurons. HSV establishes a life-long latent infections within sensory nerve ganglia A couple of two types of HSV: HSV-1 which is certainly associated mostly with scientific features in the orofacial area such as frosty sores or fever blisters and HSV-2 which may be the principal reason behind genital herpes attacks. While some top features of their scientific illnesses differ both infections set up a latent an infection within sensory nerve ganglia. Since HSV-1 and HSV-2 are genetically virtually identical the prototype of the group HSV-1 would be the principal focus of the review. As specified in Amount 1 HSV-1 attacks are generally obtained by direct get in touch with on the Lurasidone top of dental mucosa. The trojan then replicates inside the mucosal epithelial cells and gets into the nerve termini from the sensory neurons which innervate the website of the principal an infection. The HSV virion moves towards the cell systems Rabbit Polyclonal to ARNT. of the neurons located inside the trigeminal ganglia via fast-axonal transportation. Once in the neuron 1 of 2 processes is set up: either lytic replication or repression of lytic genes and establishment of latency (Amount 2). During lytic gene replication purchased gene appearance occurs with the merchandise of every gene class getting essential for initiation of appearance of the next class. The initial gene class to become transcribed and translated is normally that of the immediate-early (IE) genes which furthermore to offering transactivation and export/transportation functions are essential for transcription of the first genes; the first (E) genes which are essential in viral DNA replication are prerequisite towards the structural and capsid-associated later (L) genes. If the lytic genes aren’t portrayed the viral.","Latent HIV infection of resting CD4 T cells present a formidable challenge in the pursuit of treatments to purge HIV from the body. Latency is established and regulated in large part through the modifications of the nucleosomes that are bound around the viral promoter region. Drugs that reactivate latent HIV must influence these structures in order to allow HIV transcription and virus expression. However, for unknown reasons not all intact proviruses respond to latency reversing agents and thus present a particularly troublesome barrier to cure. A greater understanding of the regulation of HIV-1 chromatin, the installation of nucleosomes and their histone post translational modifications (PTM) will contribute greatly to the development of therapeutic control over HIV- 1 latency. This project seeks such an understanding and builds upon extensive preliminary studies revealing important new insights into these processes. For the first time we investigate the initial stages of HIV-1 chromatin installation, finding that it occurs either contemporaneously or soon after reverse transcription, and before integration into the cellular DNA. We find that repressive chromatin is installed in the absence of the viral protein Vpr being delivered in the virion. Virion Vpr dramatically increases the number of transcriptionally active proviruses in the first 4 days after infection of resting CD4 T cells. When Vpr is present, the nucleosomes that control HIV expression are modified in ways that facilitate HIV replication and block installation of repressive chromatin. For example, the alternate histone H2A.Z is installed only when virion Vpr is available, and H2A.Z is a central organizer of paused but responsive promoters. Without Vpr, the chromatin of latent proviruses takes on a more repressed structure, resulting in more proviruses that do not respond to latency reversing agents. Novel RNA-seq data demonstrate that virion Vpr reprograms gene expression pathways central to chromatin organization and transcriptional regulation. This project will describe both the initial steps in HIV chromatization and the long term processes that lead to reversible latency and irreversible repression.\nAim 1 will systematically analyze early proviral chromatin in resting CD4 T cells and the influence of Vpr and Tat-directed transcription on the epigenetic landscape. Our central hypothesis is that Vpr directs installation of the transcriptional pre-initiation complex for basal pre-Tat transcription in resting T cells.\nAim 2 will analyze Vpr-dependent pathways leading to these structures. RNA-seq data will be expanded and used to study novel Vpr targets of regulation, and known Vpr pathways will be investigated for their influence on early events.\nAim 3 will examine long term infection and latency under the influence the structures and pathways gleaned from Aims 1 and 2. Our hypothesis is that Vpr protects the provirus from epigenetic repression and irreversible latency. The proposed studies will provide much needed information that will assist in the development of therapeutics that can purge HIV from the body or permanently repress virus replication without continual antiviral treatments.\nHIV latency, which presents a barrier to elimination of the virus, is largely the result of repressive epigenetic modifications to the HIV-associated chromatin, including histone deacetylation and histone methylation. The search for treatments which might cure infection or permanently repress the virus depend upon a thorough un- derstanding of these processes. This project will define the initial installation of HIV-1 chromatin and the influ- ence of the viral protein Vpr on this process, providing vital information important for the design of effective means to purge virus from the body or repress virus replication."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:c07a974a-e0ed-4361-8649-1b6b52ef5998>","<urn:uuid:2d46caef-2bf7-4fb3-809b-a28a28443985>"],"error":null}
{"question":"How does fermentation preserve food and what are its benefits?","answer":"Fermentation preserves food through a process where microorganisms feed on sugars and release carbon dioxide, acids, and alcohols. This process not only preserves the food but also provides additional benefits. It creates probiotic benefits, adds complex flavors, and only requires basic ingredients like salt, water, and a dark storage space. The process can be used to make various foods including sauerkraut, kimchi, tofu, and even probiotic beverages.","context":["Canning is practiced by grandmothers and mothers all over America. The sterilization of high heat and pressurization has made it the safe and preferred preservation of Americans for generations, but some people aren’t interested in spending hours over a stove for cupboards precariously stacked with jars of heat-compromised nutrients. And we don’t have to! We can use our garden through the winter months and it won’t require us to buy a fancy pan or convert our crawl space into an apple butter pantry.\nFermentation is an age-old practice that involves little more than salt, water and a dark corner. Everything from soy sauce and sauerkraut to kimchi and tofu is thanks to a nice long probiotic bath. It’s so simple it seems too good to be true, but there’s a reason for its popularity across the world.\nIt all starts with a community of hungry microorganisms. They feed off of sugars and, in turn, release carbon dioxide, acids and alcohols which actually act to preserve our foods. So our veggies gets a new lease on life, plus a healthy dose of microflora and a punch of complex flavor. In all, fermentation offers a tasty jar of probiotic benefits that can be used to make everything from sauerkraut to medicinal wine and probiotic lemonade.\nGet to know the basics of fermentation, then find yourself a crock pot or a good glass jar to give it a go. If you live in the right growing zone, you can put off this kitchen project until late fall, when cold-loving crops like cabbage will be at their peak.\nAcetic acid, or vinegar, is itself a product of fermentation whereby microorganisms turn sugar into alcohol and then acid. Thanks to this acidic nature, vinegar provides an environment that has food spoilers turning on their heels. Garden veggies can be kept for months, and the process can be completed in a matter of minutes. If you’re a busy gardener, then this is the option for you.\nWe generally think of condiments like pickles and relish when we think of pickled foods, but vinegar offers far more versatility. There’s a number of pickle-icious fruits and vegetables, and you can even preserve herbs with vinegar. Spices are also a welcome addition that will boost flavors while imparting their health and antiseptic properties. Whatever route you choose to go, you’ve got a tangy jar of preserved goodness that will keep bacteria at bay.\nDehydrating is my personal favorite when it comes to preservation. Dried foods are lightweight and compact, so they take minimal storage space, keep for months, and they’re highly mobile. The drying process maintains the structure of your fruits and veggies so they can easily be rehydrated and put to work, but their dried form gives them an explosive concentration of flavor that taste buds love.\nDehydration comes with another huge benefit, and that’s convenience. You can make gallon jars of dried soups, instant mashed potatoes, and even sauces that will be ready with just a cup of hot water. And you can always appease the kid in all of us with fun fruit leathers, salty veggie chips or even pies and ketchup.\nElectrical dehydrators are a worthy investment because they work whether there’s rain or shine, but this convenience does come along with a whirring fan and a bump in the electrical bill. If you live in a location with sunny days ahead, then dehydrate in true off-grid style with a solar dehydrator. You can make your own with our simple DIY project so the sun can handle the energy bill.\n4. Root cellar\nWhether you have a full root cellar, or just a cool, dry space in your home, you should be thinking storage. Root crops like carrots and onions thrive in the dark conditions of soil, many even surviving through winter, so they’re easily brought indoors. It can take some trial and error to perfect the balance between moisture and temperature, but you’ll have many pounds of crunchy veggies without any extra fridge space.\nSome crops will require curing to store long-term, as is the case with potatoes, but the storing process is simple and basic. When storing foods, you’ll want to clean them simply by shaking them (no washing!). Vegetables can then be stored in plastic tubs, baskets, buckets or root-storage bins, with carrots and beets being packed in slightly damp sand. From here you just need to maintain ventilation, temperature and humidity, and be sure to sift through bins for spoilers while you’re around.\nPreservation can be done simply, and it won’t cost you stacks of mason jars or hours over a stovetop. Figure out which approach best accommodates your space, diet and cooking considerations, and give it a try.\nDo you have tips, tricks or resources that other people should know about? Share your knowledge in the comments below:"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:0b877d75-cfa1-42d5-8560-21dfbf583e06>"],"error":null}
{"question":"Given my interest in sustainable urban infrastructure, could you explain how city hydrant systems are maintained and what environmental impact considerations are involved in both water management and meat production?","answer":"City hydrant systems require comprehensive maintenance of approximately 1,600 hydrants, including pressure testing, leak checks, and strategic flushing of dead-end hydrants. Environmental protection is integrated through dechlorination during flushing to prevent waterway contamination. This focus on environmental protection parallels concerns in meat production, where livestock operations face challenges with methane emissions, manure management, and agricultural run-off. Both sectors require careful environmental stewardship - water systems through controlled discharge and dechlorination, and meat production through sustainable feed sourcing, responsible manure processing, and greenhouse gas reduction initiatives.","context":["Hydrant Flushing and Maintenance Program\nThe Fire Department, working with the Public Works – Operations, maintains approximately 1,600 hydrants throughout the city (2,065 in the Fire Department’s service area, which includes the Riverdale, Alto Park and Rivergrove districts).\n- Checking for visible leaks\n- Testing water pressure, flow and operation\n- Charging the hydrant to allow air to escape and the barrel of the hydrant to be filled with water\n- Making sure that the hydrant drains properly so that it will not have water in the barrel during freezing weather\n- Checking to make sure caps and valves are in working order\n- Clearing any brush or vegetation from around the hydrant\n- Repairing any broken or problem hydrants\nWhile all hydrants are maintained, some hydrants are also flushed. Not every hydrant needs to be flushed, only the hydrants located at the end of a waterline (dead-end hydrants). This routine maintenance is necessary to maintain the integrity of the water system and continue to deliver the highest quality water possible to customers.\nHydrant flushing - what happens?\nDuring this program, the City will be using fire hydrants to discharge a large volume of water from the water system – essentially, flushing the water system.\nWhy flush fire hydrants?\n- To clean water lines\n- To ensure that they are working properly\nBy discharging a large volume of water through a fire hydrant, water flow within the pipeline is increased. This increased flow actually scours the inside of the pipeline, removing and then transporting silt and mineral deposits out of the water system. In addition, this process provides Lake Oswego with the opportunity to assess the general condition of the hydrant and test for flow and operations.\nWill Hydrant Flushing Affect Your Water?\nDuring the flushing process, you may experience some water discoloration, especially when work is being done in your immediate neighborhood. As water is flushed through water lines and out the hydrant, silt and mineral sediment can cause water to become discolored. Once the hydrant flushing is complete, the water quality will return to normal.\nCan I drink from the tap during hydrant flushing?\nAlthough the water may not be visually appealing, it is safe to drink and continues to meet all federal and state drinking water standards. There is no health hazard associated with the discolored water.\nHow long does hydrant flushing take?\nFire hydrant flushing typically takes 15 minutes to several hours, but discolored water may last up to 4 hours.\nFlushing releases a large amount of chlorinated water, will this chlorinated water get into the waterways?\nNo. The City will use a dechlorinator when the hydrant flushing is performed.\nWhat should you do?\nHere’s what you should do if hydrant flushing is taking place in your neighborhood:\n- If possible, avoid using water while the hydrant flushing is taking place. By not taking water from the tap or running appliances that use water (dishwashers and washing machines), you can prevent discolored water from entering your household plumbing altogether.\n- Don’t do laundry while hydrant flushing is taking place. The discolored water can sometimes stain fabrics. Wait until water runs clear at the tap before using your washing machine, and wash a load of dark clothes first.\n- If you encounter discolored water following hydrant flushing, run the cold water taps throughout your home (bathroom sinks and tubs, kitchen faucets, etc.) for 5 to 10 minutes or until water clears. This allows discolored water to work its way out of your household plumbing.\n- If, after hydrant flushing, your water pressure or volume seems low, clean faucet screens to remove silt and mineral sediment that could be obstructing water flow.\nFor questions regarding the hydrant flushing program or if you experience water quality problems lasting more than four hours following hydrant flushing, contact the Public Works – Water Operations at 503-635-0280.\nFind the closest hydrant\nThe City has an interactive map available to search for information including hydrants and water lines.","Meat Guide: Health Benefits, Concerns and Profile of Different Meat Cuts\n“There are a lot of reasons to eat meat, but there are more reasons not to eat meat.”\nPork: One of the most popular meat types is pork. It’s cheap in comparison to other meat and quite versatile when it comes to it’s uses. It can be cooked in a million different ways and it can be used to make ham, bacon, jamon, prosciutto, salami, sausages and hot dogs. As far as it’s nutritional value, it is a good source of protein, B1 vitamin, selenium and zinc. However, it contains high amounts of fat, and a high omega 6:3 ratio, which doesn’t make it a top choice.\nBeef: There are many different cuts of beef, with different fat content. It has higher nutritional value than pork (better omega 6:3 ratio, similar protein content, high iron content). However, it is more expensive in comparison to pork.\nChicken: Chicken makes the top 3 in the list of the most popular meat. It is classified as poultry, while the previous 2 are defined as red meat. It has a much lower fat content, similar protein and decent vitamin content. It is also a very good source of gelatin when making soups and broth with chicken bones. It is quite versatile when it come to it’s culinary use, as it can be cooked in any way you can imagine or it can be used to make burgers or ham. It is quite cheap to buy and provides a decent source of necessary vitamins and minerals, particularly selenium, potassium, phosphorus and B vitamins.\nLamb and Mutton: They are quite similar kinds of meat, lamb is from sheep less than a year old and mutton is from adult sheep. Lamb chops is the most popular cut but there are many more. They graze on pasture thus they have a very low omega 6:3 ratio when compared to other meat. It is quite expensive to buy and not so easy to find in some places. Lamb contains high amounts of Zinc, Selenium and B Vitamins.\nTurkey: Turkey is another type of white meat such as chicken. Roasted turkey is probably the most popular form of cooking it and appears mainly during Christmas. It’s nutrition profile is quite similar to that of chicken.\nVenison: The meat that come from deer. It is possibly the most nutrient-dense form of meat but also very expensive. It is considered read meat such as beef and pork but it’s low-fat content makes it’s nutrition profile similar to chicken but packed with more vitamins and minerals than most meat. It has and excellent omega 6:3 ratio because deer mostly live in the wild.\nBison: Very lean form of red meat. As most wild meat, bison has a low omega 6:3 ratio, unless it is grain-fed. Low calorie choice, high nutritional value.\nRabbit: Very nutrient-dense, low calorie form of meat. It is mainly found in Europe but also in the US and China. It’s got one the best omega 6:3 ratios. Not as high protein content.\nDuck: Mainly used in Chinese gastronomy, duck has a similar nutritional profile with lower fat. However, it is mostly served with high fat and sodium sauces in the Asian cuisine.\nOther things to consider:\nGrain-fed vs Grass-fed:\nThe nutritional value of meat is not only impacted by the type of cut. The same cut can have a quite better omega 6:3 ratio when the animal is grown outdoor and is grass-fed. A human diet should have a ratio as close to 1:1 of omega 6:3, however nowadays it can go up to 25:1. Thus, the type of meat we consume should be carefully selected.\nAnimal welfare can have a significant impact on the nutrition profile of meat and should be taken in account for ethical reason as well.\nThe environmental impact of meat production:\nLivestock is one of the most harmful activities for the environment nowadays. Animals release high amounts of substances like methane that pollute the atmosphere, and excessive manure that contains antibiotics, bacteria, pesticides, and heavy metals. When manure is decomposed it releases more methane, ammonia and carbon dioxide into the atmosphere which further contributes to climate change.\nForests like the Amazon that are extremely important for our planet’s wellbeing are being destroyed and millions of acres replaced with monoculture crop fields dedicated to feeding livestock. Converting natural habitats to agricultural fields releases carbon pollution, contributing to climate change. Fertilizers are used to treat the crop fields in much higher amounts that the plants can absorb, thus polluting the waterways.\n1. Sustainable Feed Sourcing\na. Raise all meat on feed from suppliers verifiably implementing practices to prevent agricultural run-off pollution, soil erosion, and native ecosystem clearance across their supply chain.\nb. Enrollment in nutrient optimization plan to prevent excess fertilizer application\nc. Implementation of cover crops and conservation tillage to protect soil health and reduce run-off\nd. Policy against clearing native ecosystems\ne. Incorporation and support of diverse crop rotation to improve soil health\n2. Responsible Manure Management\na. Provide centralized processing facilities to process manure generated\nb. Policy against placement of new or expansion of CAFOs in watersheds already classified as “impaired” from nutrient pollution\n3. Greenhouse Gas Emissions Reductions\na. Time-bound goals to reduce emissions across supply chain\nb. Require meat suppliers to reduce emissions from direct and contract suppliers as well as feed production\nOriginally published at https://www.nutritionjourneys.com on November 26, 2022."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:ebd7e95c-7cf4-4f46-86aa-2eb729c7001e>","<urn:uuid:522663c0-3085-4385-ac57-aa6a0ccb6477>"],"error":null}
{"question":"I'm planning to make both flatbread and pizza tonight. Which takes longer to cook at their recommended temperatures?","answer":"The 2-ingredient flatbread cooks faster, taking only 2-3 minutes per side on a hot griddle. Pizza, on the other hand, takes 10-15 minutes when baked at 475°F, or 8-12 minutes at 450°F. Even at a lower temperature of 425°F, pizza still requires 18-22 minutes total (8-10 minutes for the crust, plus 10-12 minutes after adding toppings).","context":["If you want a quick, easy, fresh-off-the-griddle flatbread, it doesn't get much easier than this dough that uses just two ingredients. In just about 30 minutes from start to finish, you'll be tucking into tender flatbreads that can be eaten with dip, rolled up with your favorite fillings, or used to sop up something saucy that you're serving for dinner.\nOh, and guess what? There's no stand mixer or special equipment required. Here's how we do it.\nThe 2 Ingredients for Success\n- Self-rising flour: Self-rising flour might be a new ingredient to you, but you should embrace it! This flour blend has all-purpose flour, salt, and baking powder, so all you have to do is measure one ingredient instead of three. If you don't keep this as a pantry staple, it's pretty easy to make up on your own. Follow the recipe below or pick up a bag of self-rising flour from the grocery store.\nThe recipe: How To Make Self-Rising Flour\n- Full-fat plain Greek yogurt: Yogurt hydrates the dough and gives it a nice tangy flavor and a needed dose of fat. It's important to stick with full-fat, not low-fat or nonfat, yogurt here.\nCustomize Your Flatbread\nIt's easy to flavor and customize your flatbread. You can use fresh herbs or scallions, or add a big pinch of black pepper or your favorite spice. Some grated hard cheese like Parmesan would also be delicious.\nAfter the dough is mixed together in a bowl (no stand mixer needed — just elbow grease), it's kneaded together and then left to rest for 20 minutes. This lets the dough relax and makes it easier to roll out. This super-tender dough is a dream to roll out after a rest. After, all it needs is a few minutes on hot griddle or skillet with a brush of oil to get nice and golden-brown.\nUse this flatbread like you would pocketless pita, lavash, or naan. It's truly versatile, but tasty enough to just eat on its own — straight off the griddle!\nHow To Make 2-Ingredient Griddled Flatbreads\nWhat You Need\n1 1/2 cups self-rising flour, plus more for kneading and rolling\n1 1/4 cups plain whole-milk Greek yogurt\nHerb add-ins (optional, see Recipe Notes)\n2 tablespoons olive oil\nMeasuring cups and spoons\nBrush or paper towels\nGriddle, grill pan, or 12-inch cast iron skillet or nonstick frying pan\nCutting board or baking sheet\nTongs or flat spatula\n- Mix the dough: Place the flour, yogurt, and herbs if using in a large bowl and mix with a rubber spatula until a moist, shaggy dough forms.\n- Knead the dough: Dust a work surface with flour. Transfer the dough onto the work surface, sprinkle with more flour, and knead until smooth, 8 to 10 kneads.\n- Divide the dough: Divide the dough into 4 portions and roll each into a ball about 3 inches wide. Cover loosely with plastic wrap and let rest for 20 minutes.\n- Heat the pan: Heat a griddle, grill pan, 12-inch cast iron skillet, or nonstick frying pan over medium-high heat until a drop of water immediately sizzles on contact. Meanwhile, roll out the dough.\n- Roll the dough: Generously dust a work surface and rolling pin with flour. Roll 1 ball of dough into a 9-inch round about 1/8-inch thick, dusting with more flour as needed to prevent sticking.\n- Griddle the flatbread: Brush the pan with a thin layer of oil or use a paper towel to coat the pan with a thin layer of oil. Place the flatbread in the pan and brush the top with a thin layer of oil. Cook until puffed and golden-brown in spots, 2 to 3 minutes per side. Remove to a cutting board.\n- Roll and grill the remaining flatbreads: While the first flatbread is cooking, roll out the next ball of dough. Cook the remaining flatbreads, making sure to brush the pan and top of the flatbreads with oil each time.\n- Cut and serve: Cut the flatbreads into wedges if desired and serve warm.\n- Herb add-ins: Add finely chopped fresh herb leaves: 1/4 cup fresh parsley, oregano, basil, cilantro, scallions, or chives, OR 2 tablespoons finely chopped fresh rosemary or thyme.\n- Self-rising flour substitute: Measure 1 1/2 cups all-purpose flour into a bowl. Remove 2 1/2 teaspoons of the flour and place back in the flour container. Add 2 1/4 teaspoons baking powder and 1/4 teaspoon fine salt to the bowl and whisk together.\n- Storage: Store leftover flatbreads tightly wrapped in plastic wrap or aluminum foil in the refrigerator for up to 3 days. Rewarm in a 300°F oven or on a warm frying pan.","Preheat oven to 475°F (246°C). Allow it to heat for at least 15-20 minutes as you shape the pizza. Lightly grease baking sheet or pizza pan with nonstick spray or olive oil.\nWhat temperature do you cook homemade pizza on?\nBake pizza: Bake pizza in the 475°F oven, one at a time, until the crust is browned and the cheese is golden, about 10-15 minutes. If you want, toward the end of the cooking time you can sprinkle on a little more cheese.\nHow long do you cook pizza at 450 degrees?\nSet oven rack to middle position and preheat oven to 450°F. Place pizza on middle rack. Do not use a pan or cookie sheet to bake pizza. Bake for 8-12 minutes or until pizza is golden brown.\nHow long does it take to cook a pizza at 425?\nConclusion: The Oven\nPlace the pizza dough on the hot stone, reduce heat to 425 degrees and cook for 8 to 10 minutes, until crust is firm but not brown. Top with sauce, meat, cheese or your favorite toppings and return the pizza to the oven for an additional 10 to 12 minutes.\nWhat temperature should pizza dough be?\n6) A good finished dough temperature is 80 to 85F for most pizza doughs, doughs that are colder than this may exhibit bubbling tendencies.\nWhat oven setting is best for pizza?\nPreheat the oven between 450 and 500 degrees F (250 to 260 degrees C) — the stone needs heat up while the ovenheats. Large, thick-crusted pizzasmight need an oven temperature closer to 400 degrees F (200 degrees C) so the crust can cook completely before the toppings burn.\nHow long do you cook pizza at 350?\nAlthough every oven is different and needs different kinds of attention, you can make a scrumptious pizza at 350 or 400 degrees. It will take around 15-20 minutes for an oven to finish baking a pizza. To understand that it’s cooked perfectly, see if the crust is golden brown and some bits of cheese will be overcooked.\nCan you cook pizza at 450?\nSet oven rack to middle position and preheat oven to 450 °F. Place pizza on middle rack. Do not use a pan or cookie sheet to bake pizza. Bake for 8-12 minutes or until pizza is golden brown.\nHow long do you cook pizza at 400?\nTime To Bake!\nPlace your pizza into a hot preheated oven, 400 degrees F. Bake for about 15 minutes, until crust is cooked through. Look at that thick crust! If your family prefers a thin crust, press your dough into a larger round before baking.\nHow do I make the bottom of my pizza crispy?\nHow to get a crispy pizza base\n- Use a pizza stone or perforated pizza pan. …\n- Pre-heat your pizza stone or pan before placing your pizza. …\n- Why you need to use the right cheese for pizza. …\n- Don’t overload your pizza with toppings. …\n- Why your oven temperature needs to be hot for pizza. …\n- Use a concentrated pizza sauce.\nShould pizza dough be pre baked?\nIt’s absolutely essential to pre-bake the dough for 5-6 minutes before adding your toppings. Once you’ve added Pizza Sauce and all your toppings, return it to the oven to finish baking! This will result in a crust that holds on it’s own and is crispy on the outside, and soft and airy on the inside.\nWhat makes dough chewy?\nThe tough and chewy stage is set when a high protein (very strong) flour is used to make the dough. … Pizza crusts made with a high-protein flour of 13 percent or more can have a wonderfully light and cri…\nWhy is my pizza dough tough?\nThe first reason your pizza dough gets tough is that it contains too much flour. Or in baking terms, the dough has too low hydration. If the dough contains too much flour compared to water, the result will be a dry, tough pizza dough that’s hard to work with. The simple solution is therefore to add less flour.\nHow long can pizza dough rise at room temp?\nPizza dough after it has risen should not sit out for more than a 3 hours. If you aren’t going to begin to roll out the dough right away then keep it in a refrigerator. Even at room temperature the dough will begin to form a skin as it dries. This makes the dough harder to roll and will cook uneven.\nHow long should pizza dough rise refrigerated?\nCover the bowl with plastic wrap or a kitchen towel and let the dough rise until doubled in size, 1 to 1 1/2 hours. Option 3 — Store the dough in the fridge.\nDo you prove pizza dough at room temperature?\nPizza dough should proof in room temperature anywhere from 1 to 24-hours or even more. … It’s up to you how long you want toproof your dough, but generally speaking, a longer fermentation will result in a more flavorful pizza crust."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:7d6828b0-d35d-41a5-b17b-32b03ce4100e>","<urn:uuid:ef8a1ceb-0e03-45fd-9ace-67505e53f7fd>"],"error":null}
{"question":"What is the overall timeframe of Mexico's Energy Transition Support Programme (TrEM)?","answer":"The Energy Transition Support Programme in Mexico runs from 2019 to 2022.","context":["Supporting the energy transition in Mexico\nTitle: Energy Transition Support Programme in Mexico (TrEM)\nCommissioned by: Federal Ministry for Economic Cooperation and Development (BMZ)\nPartner: Mexican Secretary of Foreign Affairs (SRE), Mexican Secretary of Energy (SENER), National Commission for the Efficient Use of Energy (CONUEE), Energy Regulatory Commission (CRE), National Center for Energy Control (CENACE)\nLead executing agency: Agencia Mexicana de Cooperación Internacional para el Desarrollo (AMEXCID); Mexican Agency for International Development Cooperation\nOverall term: 2019 to 2022\nThe legislation already in place in Mexico makes a good foundation for using sustainable and renewable energy. The country would like to establish a reliable energy supply with competitive prices. The aim is to increase the proportion of renewable energy used by 2030. Mexico has committed to international agreements to tackle climate change.\nWith the General Climate Change Law and Energy Transition Law, the Mexican Government is becoming actively involved in protecting the climate. The Energy Transition Law in particular forms the basis for the country’s political activities in energy generation and use, increasing energy productivity and reducing emissions of greenhouse gases.\nBased on the 2030 Agenda and new laws, Mexico uses more renewable energy and has become more energy-efficient.\nThe project supports the Mexican Government in implementing improvements in connection with the energy supply. To this end, human resources and expertise will be strengthened in several public institutions. These institutions include the Secretariat of Energy (SENER), the Energy Regulatory Commission (CRE) and the National Center for Energy Control (CENACE). The aim is to promote an exchange of experience and expertise, including at international level. The project is also to take on an advisory role in the areas of renewable energy use and energy efficiency. Issues such as gender equality, accountability and transparency are to be incorporated into the change processes. The project supports the development of information systems and helps to monitor the energy transition.\nThe project advises important public and private institutions, scientists and ordinary citizens on the development of schedules and technologies for realising the energy transition. For this purpose, local authorities in Mexico will be recommended to introduce a municipal energy data monitoring system. Networks between representatives of federal states and local authorities are to be created to raise awareness of renewable energy use and energy efficiency.\nThe programme works closely with international partnerships which provide funding for implementing energy efficiency measures. The aim is to promote the exchange of experience.\nThe planning and control tools used in Mexico for operating the national electricity system (SEN) are to be adapted while the use of renewable energy is increased. Recommendations for legislation will be made based on analysis and prioritisation of issues involved. Where appropriate, the project will also recommend new tools for grid planning and system operation. The long-term goal is to increase the flexibility of the electricity system while at the same time ensuring the system’s reliability when using renewable energy.\nA further goal is to develop decentralised energy production from renewable sources. This will be achieved by supporting new business models and increasing participation by the private sector. To this end the project will issue advice, taking into account in particular the financial situation and the market in Mexico. The project focuses on developing and introducing new financing mechanisms or finance products, and reducing regulatory obstacles."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:7727cfc7-7c7a-46af-8021-6c9392c55e77>"],"error":null}
{"question":"How do the brown marmorated stink bug and emerald ash borer differ in their methods of destroying trees and plants, and what are the typical timeframes for damage from each pest?","answer":"The brown marmorated stink bug (BMSB) and emerald ash borer (EAB) have different destructive mechanisms. BMSB is a polyphagous pest that directly damages both ornamental and food crops through feeding, while EAB specifically attacks ash trees by laying eggs on the bark, after which the larvae bore beneath the bark and feed on the water and nutrient-conducting tissues. In terms of damage timeframes, EAB typically kills infested trees within two to four years, while BMSB causes immediate crop damage during growing seasons, as evidenced by the population explosion and significant crop destruction observed in 2010.","context":["Post by Lauren Hunt and Thomas Pike\nOn 22 November 2013, we heard MS degree candidate Ashley Jones give her exit seminar in Entomology Colloquium, describing research on indigenous natural enemies of the brown marmorated stink bug (BMSB). BMSB is an invasive species from Asia that arrived in Allentown, PA in 1996 and spread quickly. Now recorded from 40 states, BMBS is an extraordinarily destructive pest of both ornamentals and food crops primarily due to their extremely polyphagous nature. Originally considered merely a nuisance pest, in 2010 BMSB populations exploded, prompting farmers and researchers alike to take action to halt the spread. Biological control is an attractive option, as there are many egg parasitoids and predators in the stink bug’s native range. The goal of Ashley’s research was to determine baseline natural enemy behavior in the US in the hopes that natural enemy complexes could be conserved and promoted to control BMSB.\nAshley’s talk focused primarily on the native egg parasitoids of BMSB. During the 2012 and 2013 field seasons, several ornamental tree genera (Acer, Prunus and Ulmus) were checked weekly for eggs. These eggs were then monitored for mortality and parasitoid emergence, which can be seen in this video. Egg mortality was categorized as follows: no mortality, chewing mortality, sucking mortality, parasitism or unascribed mortality (cause unknown). In both seasons, parasitism was the primary source of mortality by a wide margin (32% parasitism in 2012, 44% in 2013). This parasitism increased during the season and was primarily the work of a single genus, Anastatus, which was responsible for 98% of the egg parasitism. Perhaps most intriguing was the change in sex ratio from 2012 to 2013. In 2012, Anastatus reduvii (the most prolific egg parasitoid) had a male:female ratio of 1:2.05. In 2013, this ratio jumped to 1:4.85. Ashley posits that this is most likely due to an increase in eggs available in 2013, encouraging the parasitoids to produce more females to exploit this resource.\nAshley concluded that predation by various common nursery predators did seem to augment control of the BMSB. She observed a distinct pattern in the way the predator density and the stink bug infestation were related. As the number of stink bugs decreased in the season, so did the number of predators observed. Likewise, as the number of stink bugs present increased, the number of predators rose with slight delay, compensating, it seems, for the increase in food availability. This pattern was consistent in both years she ran the study.\nIn the laboratory, Ashley used feeding trials to determine which natural enemies were the most successful at control. She conducted trials on wheel bugs, jumping spiders, lacewings, minute pirate bugs, convergent lady beetles, and multicolored Asian lady beetles. Across all life stages of BMSB, wheel bugs were most voracious by far. The lab BMSB egg trials, however, did not show what was clearly demonstrated in field observations. No predators consumed the eggs in these trials, even though lacewing larvae had been observed feeding on BMSB eggs throughout the summer. This may be attributed to a different species of lacewing used in lab trials compared to what is found in the field setting.\nRemaining unanswered and open for additional investigation is the occurrence of unascribed mortality, in which the BMSB eggs did not hatch and died, with causes unknown. Could this arise from feeding by adult parasitoids? A fungal pathogen, perhaps? A byproduct of a feeding predator? Because unascribed mortality was more common with larger egg clutch sizes (density dependent), this tends to favor a biotic hypothesis over, for instance, extreme temperature, the effects of which should not differ with egg density. The more information we have, the more possible research avenues become available to learn how to best control these pests in an environmentally safe and sustainable manner.\nAshley suggested further research is needed on the main species of parasitoid found successfully controlling BMSB in ornamentals and nurseries, Anastatus reduvii. Although we know Anastatus is a generalist parasitoid, we know relatively little about other factors that may directly influence its longevity and fecundity. By gathering more information about this promising biological control agent, it might be included as a component of an Integrated Pest Management (IPM) program to control these pesky stinkers in our nurseries, gardens, and homes.\nAbout the authors\nLauren Hunt is a first year Master’s student conducting research focused on biological control of stink bugs, with emphasis on the usage of insectary plants to increase mortality of the invasive brown marmorated stink bug (BMSB). With exponential human population growth, there is an intensifying need for all varieties of sustainable living practices, including food and grain production. She is interested in the development of techniques that will conserve natural resources and biological living systems.\nThomas Pike is a second year Master's student studying the effects of entomopathogenic fungi on the brown mamorated stink bug with regards to its use as a formulated biocontrol. He is also pursuing trap-and-kill systems utilizing BMSB pheromones and the fungus to be used as a passive control. These control methods show promise as an alternative to current pesticide use in ornamental and crop systems.","DES MOINES, Iowa – Emerald ash borer has been confirmed in Chickasaw, Franklin and Jones counties for the first time, bringing the total to 69 counties in Iowa where this invasive insect has been detected.\nInsect samples were collected from New Hampton (Chickasaw County), Hampton (Franklin County), and Anamosa (Jones County). Officials with the Animal and Plant Health and Inspection Service (APHIS) of the U.S. Department of Agriculture (USDA) confirmed these samples positive for EAB.\nEAB is a non-native beetle that attacks and kills ash tree species. Adult beetles lay eggs on the bark of ash trees. Once the eggs hatch, the larvae bore beneath the bark and begin feeding on the water and nutrient-conducting tissues. Infested trees typically die within two to four years.\nAsh trees infested with EAB can exhibit canopy thinning, water sprouts from the trunk or main branches, increased woodpecker activity, serpentine (“S”-shaped) galleries under the bark, vertical bark splitting and 1/8 inch D-shaped exit holes.\nThe spread of EAB is accelerated by transport of infested wood products such as firewood. People are reminded to use locally-sourced firewood to help reduce the spread of EAB.\nEAB was first discovered in southeastern Michigan near Detroit in 2002, and has now spread to 35 states. This exotic pest was first detected in Iowa in 2010.\nAt this calendar date, the treatment window for soil-applied preventive treatment measures (soil injection, soil drench, or granular application) and basal bark sprays has ended. Trunk injections can be done now through the end of August if a landowner is interested in protecting a valuable and healthy ash tree within 15 miles of a known infestation.\nGood soil moisture is critical for the effectiveness of any systemic insecticide movement in a tree. More details pertaining to treatment are available in Iowa State University Extension and Outreach publication PM2084, Emerald Ash Borer Management Options.\nTo find a certified applicator in your area, download publication PM3074, Finding a Certified Pesticide Applicator for Emerald Ash Borer Treatment, and follow the steps.\nThe State of Iowa monitors the spread of EAB on a county-by-county basis. Anyone who suspects an infested ash tree in a new location is encouraged to contact one of the following:\n- Iowa Department of Agriculture and Land Stewardship, State Entomologist Office: 515-725-1470\n- Iowa Department of Natural Resources: 515-725-8453\n- Iowa State University Extension and Outreach, Entomology: 515-294-1101\nTo learn more about EAB and other pests that are threatening Iowa’s tree population, please visit http://www.iowatreepests.com.\nFor more information contact any of the following members of the Iowa EAB Team:\n- Mike Kintner, IDALS EAB coordinator, 515-745-2877, Mike.Kintner@IowaAgriculture.gov\n- Robin Pruisner, IDALS state entomologist, 515-725-1470, Robin.Pruisner@IowaAgriculture.gov\n- Jeff Goerndt, DNR state forester, 515-725-8452, Jeff.Goerndt@dnr.iowa.gov\n- Mark Shour, ISU Extension and Outreach entomologist, 515-294-5963, email@example.com\n- Tivon Feeley, DNR forest health program leader, 515-725-8453, Tivon.firstname.lastname@example.org\n- Donald Lewis, ISU Extension and Outreach entomologist, 515-294-1101, email@example.com\n- Emma Hanigan, DNR urban forestry coordinator, 515-249-1732, Emma.Hanigan@dnr.iowa.gov\n- Laura Iles, ISU Extension and Outreach entomologist, ISU Plant and Insect Diagnostic Clinic, 515-294-0581, firstname.lastname@example.org\n- Jeff Iles, ISU Extension and Outreach horticulturist, 515-294-3718, email@example.com"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:3935cce2-e860-421d-b7f4-47f58a14e0f7>","<urn:uuid:d001ee5d-87e3-4819-9b98-0995cc888f65>"],"error":null}
{"question":"Hey! I'm really curious about how ichthyosaurs moved through water. How did these sea monsters swim?","answer":"Ichthyosaurs were very speedy swimmers that moved through water by rapidly propelling themselves with side-to-side tail movements, similar to how tuna and sharks swim. They could reach speeds of up to 40 kilometers per hour. Their bodies evolved to become more rigid over time and developed extra bones in their tails, which helped them maneuver effectively through water at such great speeds.","context":["From the depths of the ocean to Queensland Museum, discover the secrets of the monsters from the deep with Sea Monsters: Prehistoric Ocean Predators. Presenting the profiles of the three giant marine reptiles that ruled the sea.\nIchthyosaur (pronounced ick-thee-o-sore) remains the original marine reptile or “sea lizard” as they’re often referred to, marking their reign of the ocean for nearly 150 million years.\nIdentified as an ocean shape-shifter, their evolution saw changes in their structure. At the beginning, their appearance resembled that of a dolphin, with long bendy bodies and crescent shaped tails. Over time their bodies became more rigid and their tails developed extra bones, allowing them to manoeuvre through the water at great speeds of 40 kilometres per hour.\nThey were speedy, rapidly propelled throughout the water by moving their tails from side to side, much like the movement of tuna and shark. They had big eyes, with the ability to see clearly into the dark depths of the ocean, making them serious ocean predators and attending to their prey with a significant number of teeth.\nTaking the form of a lizard-like sea reptile, the plesiosaur (pronounced plea-zee-o-sore) were the second sea monster to evolve after the ichthyosaur.\nTogether, they shared the ocean for 100 million years, however the plesiosaurs outlived the ichthyosaur, spanning the sea, seen in the discovery of their fossils found on each and every continent.\nVaried in size from as small as 1.5 metres to 15 metres long, the plesiosaur body is described as an almond shape, consisting of a short tail, long flippers and flattened bodies, with differences seen in the shape and size of the head and neck.\nTheir movement through the ocean saw them flapping their four flippers as birds flap their wings.\nThe mosasaurs (pronounced: mow-sa-sores) were the last of the sea monsters to dive into the ocean, joining the plesiosaurs five million years after the extinction of the ichthyosaurs.\nJust like the ichthyosaurs, their presentation morphed over time from a small lizard-like species to large ocean predators, ruling as the top of the food chain.\nMosasaurs swam like crocodiles, swinging their long tails in a back and forth motion. They had goanna-like bodies, pronounced snouts and forked tongues and were covered in a scaly skin.\nTheir diet was unbiased covering anything from ammonites, fish, turtles, plesiosaurs, sea birds and even smaller mosasaurs.\nThe Tylosaurus (pronounced: Tie-low-sore-us) is the most well-known mosasaur since its starring role in the recent Jurassic World movie and is often referred to as the ‘T.rex of the sea’. That’s a cooler name than its real one – Tylosaurus means ‘knob lizard’!\nTheir extinction came at the end of the Cretaceous period, ending the sea monsters reign of the sea.\nFind out more about these ancient marine reptiles and the mysteries of the deep in Sea Monsters: Prehistoric Ocean Predators.\nAn Australian National Maritime Museum touring exhibition, developed in partnership with Queensland Museum Network, presented in Brisbane with support from BHP as part of Project DIG.\nImages courtesy of Australian National Maritime Museum ©"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:af02ed31-edfa-4c4b-bbab-ea3b87ef4ba0>"],"error":null}
{"question":"Hi! I heard about The Hermes Experiment - what makes their ensemble unique and what kind of music do they perform?","answer":"The Hermes Experiment features an unusual combination of soprano, clarinet, harp, and double bass. They perform their own commissioned compositions, arrangements, and free improvisation. They have commissioned works from more than 50 composers and also perform arrangements of existing works, such as Meredith Monk's Double Fiesta. Their repertoire includes both contemporary classical music and experimental pieces, like Stevie Wishart's Eurostar: Between Cities in Sound, which incorporates unusual aural techniques.","context":["ROSL (Royal Over-Seas League) Annual Music Competition\nMixed Ensembles Section Final\nPrincess Alexandra Hall, Over-Seas House, London. 19 March 2019\nThe Royal Over-Seas League (ROSL) has, since 1952, run an annual music competition through their ROSL ARTS, open to young Commonwealth classical musicians under the age of 30. A series of section finals for solo performers leads to a Gold Medal Final which, this year, will be held in the Queen Elizabeth Hall on 30 May. Past winners of the section finals and the Gold Medal have gone on to become well-known names. Alongside the section finals for solo performers (in wind & brass, voice, strings, and keyboard), are two ensemble finals, for strings and mixed ensembles. More than £75,000 is offered in awards, with a £15,000 first prize for solo performers and two chamber ensemble awards of £10,000. Winners of the solo sections receive £5,000 each. Details of the 2019 competition can be found here.\nAfter an initial video round in January, successful performers go through a day-long second stage selection process at Over-Seas House, the London home of the Royal Over-Seas League and, the following day, an evening public recital where each gives a 20-minute performance. I attended the last of the six Section Finals, the Mixed Ensembles Section Final, with four groups competing for the £10,000 prize, and an appearance at the Gold Medal Final. The range of music offered was wide, with composer birthdates ranging from 1540 to 1989.\nThe first group was Improviso, a period instrument quartet founded in 2017 whose performances are based around an improvisatory approach to early music. They opened with improvised divisions on the bass-line of Pauls’ Steeple, from John Playford’s 1684 Division Violin, the interplay between the recorder and violin (Fatima Lahham and Elin White, pictured below) being particularly effective, not least in the high degree of balance between the two instruments. A Chaconne by the French composer Leclair gave baroque cellist Florence Petit a chance to shine, with several flourishes and variants on the bass line. Theorbo player Johan Lõfving offered some attractive linking passages in their final piece, Castello’s gloriously anarchic Sonata decima a 3, with its contrast of different moods and textures (in the stylus phantasticus) giving the impression of a mini-opera. They proved themselves fully immersed in the performance style of the period, with its emphasis on free interpretation and improvisation, and gave a very convincing and expressive performance.\nThe second group couldn’t have been more different from the specialist period instrument ensemble Improviso. The Hermes Experiment field the unusual combination of Héloïse Werner, soprano, Oliver Pashley, clarinet, Anne Denholm, harp, and Marianne Schofield, double bass. They perform their own commissioned compositions alongside arrangements and free improvisation and have commissioned works from more than 50 composers to date. They opened with Gareth Wood’s short To Morning from his Blake Canticles, followed by a commission from the groupFreya Waley-Cohen’s Delta Song from We Phoenician Sailors, with snatches of melody over a sparse accompaniment. Another commission was Stevie Wishart’s Eurostar: Between Cities in Sound a piece based on the sounds of a Eurostar train leaving Brussels which featured an extraordinary range of aural techniques from the instrumentalist and singer. They finished with Anne Denholm’s imaginative arrangement of Meredith Monk’s 1986 Double Fiesta, originally for two pianos and voice, its rhythmically strong instrumental background supporting the extraordinarily complex and virtuosic vocalising from Héloïse Werner (pictured below). One person I overheard during the interval opined that she had “never heard anything like it.”.\nAfter a short interval, the recorder consort Palisander gave a reduced version of their Beware the Spider! programme, based on the curious 17th-century belief that the effects of a tarantula bite could be alleviated by music, resulting in several Italian dances with the title of Tarantella. This was early music on Renaissance and Baroque recorders, but presented as musical theatre, with a sequence of slickly choreographed dance routines. The four members of the group (Lydia Gosnell, Miriam Nerval, Elspeth Robertson and Caoimhe de Paor) took turns to introduce the pieces with contemporary descriptions of the effects of the spider’s bite and other medical oddities, including a suggestion that asthma could be cured by swallowing a frog greased with butter. They included a version of Vivaldi’s La Notte, under the title of ‘The Nightmare Concerto’, arranged by Miriam Nerval (pictured below left with Caoimhe de Paor) and a modern piece, Articulator V by Agnes Dorwarth, based on the vowel and consonant sounds that recorder players use as various articulation devices.\nThe final group was the Meraki Duo (Meera Maharaj, flute and James Girling, guitar) with a programme that to a certain extent filled the gap between the early and ultra-contemporary music heard so far. They started with the first two movements of Ástor Piazzolla’s Histoire du Tango: the lively Bordello 1900 representing the earliest incarnations of the Tango while Cafe 1930 features the later more melodic and mellow version, not usually intended to be danced to. They followed this with Miroslav Tadic’s Pajdushko the fourth of his Macedonian Pieces, with Meera Maharaj transferring to an alto flute (as pictured below) and featuring some busy guitar playing from James Girling. Chick Corea’s 1971 Spain (in the original Light as a Feather version) completed their set. It opens with the Adagio from Rodrigo’s Concierto de Aranjuez before setting off on a fast and furious variant of the theme and its harmonic structure. They played with a fine sense of the momentum of the music.\nThe four-strong panel of adjudicators awarded the Mixed Ensembles Prize, and £10,000, to The Hermes Experiment. In doing so, they commented that they had been looking for excellent technical performance, programming and commitment, with a focus on how the groups presented the music, adding a warning that “it is not just entertainment”. The three unsuccessful groups were encouraged to reapply in future years – apparently many past ROSL winners have appeared more than once."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:59a9b409-98e5-4662-b0fa-0297ebfcc500>"],"error":null}
{"question":"How do the cleaning needs and safety considerations compare between stove drip pans and baby bottles when it comes to using ammonia as a cleaning agent?","answer":"Both stove drip pans and baby bottles require careful consideration when using ammonia for cleaning, but with different protocols. For stove drip pans, ammonia can be used by placing them in sealed plastic bags with 1/4 cup of ammonia for 12 hours, requiring careful handling due to strong fumes and necessitating good ventilation and rubber gloves. However, for baby bottles, ammonia is not recommended as a cleaning agent at all - instead, the preferred methods are using hot water and soap, sterilizing with boiling water, steam sterilization, or in specific cases, using food-grade chlorine-based sterilizing tablets. If chemical sterilization is needed for baby bottles, the CDC approves a specific bleach solution (one teaspoon of unscented bleach with 16 cups of hot water) rather than ammonia.","context":["All stoves, no matter their make, glass, porcelain, or iron, have stove drip pans. The drip pans are the burners used for cooking.\nWithout them, grease will not be released. Instead, the grease will continue to burn all the time once the burner is on.\nDrip pans come in a variety of sizes, and they collect all the debris inside the stove. Hence, they can become very dirty and really difficult to clean.\nTable of Contents\nMaintaining Your Stove Drip Pan\nIf you do not clean your stove drip pan on a regular basis, your stove will appear filthy and unkempt.\nThe lack of appropriate maintenance of your drip pan detracts from the overall aesthetic and performance of your stove. The tips below will help you:\n- Always utilize the proper size of drip pan for your stove. Drip pans come in a variety of sizes, so make sure you know the proper size of the drip pan for your stove before you purchase any to use.\n- Try to avoid chrome-plated drip pans because they do not clean properly. A poorly cleaned drip pan will not operate efficiently.\n- Do not interchange your drip pan for your drip bowl. The drip bowl is meant to hold to warm things.\n- Replace your drip pans with new ones when they become too old and dirty. This will help to beautify the appearance of your stove. However, replace your drip pans with the ones fit.\nCleaning Your Stove Drip Pan with Dryer Sheets\nDo you have dryer sheets? If yes, do you know that you can use them to clean your stove drip pans? If you do not know, well, now you know.\nDryer sheets are very effective when cleaning the filth from your stove drip pan.\nLet’s learn how to use them in the highlighted steps below:\nStep 1: Clean the stove thoroughly and remove the dust and filth. After this, soak the sheet in water and put the sheet out on the stove so that it fits on over the drip pans.\nLeave the dryer sheets for about 30 minutes. Remove the sheets and wipe down the stove afterward with a soft cloth.\nStep 2: Soak the pans in hot soapy water. Use detergent instead of your regular dishwashing soap and soak and stir for about 10 minutes.\nYou can also use a mild cleaning solution to clean the pan. Use a towel to remove the grease and grime on the burner.\nStep 3: Next, get baking soda and form a paste with water. Make sure that the paste is very thick. Now use a brush to spread the paste all over the burner and let it sit for about 20 minutes.\nStep 4: If your burner has extreme stains, you can use wire wool to clean these affected areas. Take a ball of wire wool and scrape it on the stove.\nThis tool can effectively remove difficult stains, burnt-on food, and grease from your drip pans. After scraping the burner, use a dryer sheet to wipe the dirt on the burner.\nStep 5: After all these actions, place the dryer sheet in the mid-section of the drip pans. Pour lukewarm water over the sheet, and add a few drops of dishwashing liquid.\nLet everything sit for a few hours. Wipe down the stove drip pan with a microfiber towel. Your stove drip pan should be looking its best at this point.\nWhen Should You Clean Your Stove Drip Pans?\nDrip pans should be cleaned every time a cooktop burner is used. If spills and splatters are cleaned up quickly, it’s easier to clean the drip pans.\nFor someone that cooks every day, you should clean the drip pans on a regular basis or when they get dirty.\nIf you let food particles and grease build up on drip pans, smoke or even a fire could start.\nOther Ways to Clean Your Stove Drip Pans\nUsing dryer sheets to take off your stove drip pans is not the only way to do it. There are other methods of cleaning your stove drip pans. Let’s take a look at these methods.\nNote: Before you begin the cleaning process, allow your burners to cool down completely before taking any parts off to clean the drip pan or other parts of the stovetop.\nWatch how the parts fit together so that you can put them back in the right order.\nWhat You’ll Require\n- Rubber gloves\n- Sink or bucket\n- Microfiber cloth\n- Plastic scouring pad (optional)\n- Dishwashing liquid with a degreaser\n- Baking soda\n- Distilled white vinegar\n- Household ammonia\n- Melamine sponge\n- One-gallon re-sealable plastic bags.\nCleaning Drip Pans with Dishwashing Liquid\nThis is a great way to clean up new spills and splatters.\n- Take out the drip pans and the rings.\n- Half-fill a sink with hot water and add a few drops of a good dishwashing detergent that cuts grease.\n- Soak the drip pans and other parts that come off in soapy water. Give them at least 10 minutes to soak. Clean the surface with a sponge or a dishcloth. If there are stains that won’t come off, use a melamine sponge to scrape them away.\n- Use hot water and a microfiber cloth to clean the drip pan after soaking and scrubbing.\n- Put the drip pans back under the burners and make sure they are fastened securely.\nUsing Vinegar and Baking Soda to Clean Drip Pans\nWhat do you do when just soapy water does not work? Well, vinegar and baking soda make for an excellent alternative.\nWhen the stove is cool, take the drip pans off. To get rid of any loose bits of burned food, shake them over a trash can.\nAt this point, check to see if any food has gotten on the stovetop. It might also need to be cleaned.\nPour enough hot water and a few drops of dish soap into a sink or bucket to cover the drip pans completely.\nAllow this to soak for about 15 minutes. Drain the hot soapy water and soak the pans for about 30 minutes in distilled white vinegar.\nAdd baking soda to the mix. As the acid in the vinegar mixes with the baking soda, there will be some fizzing.\nGive the combination at least 15 minutes to work its magic. Scrub the pots and pans with a plastic scrubber and more baking soda where the stains are stubborn.\nAfter all, these, soak the drip pans in hot water and then dry them with a microfiber cloth. Assemble all of the parts back on the stovetop.\nAmmonia is an excellent cleanser if you have one. However, you should be very careful because of the fumes.\nAlways use ammonia in a well-ventilated area, and make sure to have rubber gloves on when you use it.\n- Make sure that the drip pans are completely cool, then put each one in its own one-gallon plastic bag that can be sealed.\n- Fill each bag with a quarter cup of ammonia. The fumes will cut through grease and dirt.\n- After you seal the bags, give the ammonia at least 12 hours to work. When it’s time to open the bags, make sure to do so away from your face. A lot of smoke will come out. Take off the drip pans and dispose of the ammonia and plastic bags in a safe way.\n- Add hot water and a few drops of dish soap to the sink until it is half full. Use a sponge to clean the drip pans, and use a plastic scrubber or melamine sponge to clean any tough spots.\n- Rinse each drip pan well with hot water and dry with a microfiber towel. On the stove, put it back.\nHow to Clean the Stove Rings of Your Stovetop\nYour stove rings control the flow of heat or gas, depending on your type of stove, when using your stove.\nLike drip pans, the rings can get dirty and greasy. They should be cleaned too, and ammonia makes for a good choice of cleaner.\n- Put two rings at a time in a 2-gallon Ziploc bag, plastic bag, or another storage bag that can be sealed.\n- Once the rings are in, pour ammonia into the bottom of the 2-gallon plastic bag. You don’t have to fill the bag all the way up. The muck is eaten away by the ammonia fumes.\n- Put the rings in the bag and leave them there overnight. The next day, drain the bag and take out the rings. Give them a good rub to get any crust off.\nNote: Do not inhale the fumes of ammonia. Always protect your nose and eyes from the danger of ammonia.\nCaring for your stovetop is really essential because it allows them to perform better over a long period of time.\nYou can use any of the above methods to clean your drip pans and make sure to follow the safety precautions with any of these methods, especially ammonia.","When and How to Sterilize Baby Bottles\nOnce you’ve done your research and decided on the best bottles for baby, there’s still another step to consider: how to clean them. Whether you’ve chosen glass, plastic or silicone bottles, you’ll have to keep them as pristine as possible to protect baby’s developing immune system. If you listen to advice from your mother or grandmother, you might be fretting over how to sterilize baby bottles—but nowadays sterilizing baby bottles isn’t absolutely necessary, except in certain situations. “This practice is a bit outdated now that the majority of homes in developed countries use treated municipal water,” says Caitlin Hoff, a health and safety investigator for ConsumerSafety.org. “There are, however, some cases in which you might want to sterilize a bottle.” Here, we break down when sterilizing baby bottles is a good idea and the best methods for the job.\nSterilizing baby bottles is an added step beyond traditional cleaning that provides extra protection against germs. And generally speaking, it’s a one-and-done deal. “When you first buy bottles, it’s important to sterilize them at least one time,” says Samira Armin, MD, a pediatrician at Texas Children’s Pediatrics. After all, you don’t know where that bottle was before it was packaged and sold to you, so an initial sterilization is a quick, easy way to ensure baby’s health and safety. “After that, it’s no longer necessary to sterilize bottles or their accessories,” she adds. “Many years ago, when water supplies weren’t reliably clean, baby items required sterilization, but nowadays this is thankfully not an issue.”\nThat said, there are instances when you might want to sterilize baby’s bottle beyond that first use. According to Hoff, these include:\n• If you’re using borrowed or second-hand bottles. With all the gear and supplies that babies require, some moms hit up consignment shops or borrow baby bottles from a friend. In these cases, it’s critical to sterilize pre-used bottles before giving it to your child for the first time. The same goes for bottles that have been used for older siblings in your own home.\n• If baby has been sick. It’s no fun when baby’s sick, so the last thing you want to do is risk re-infecting them by using unclean bottles. “If you’re concerned about any lingering germs or bacteria on your child’s bottles, sterilizing them will certainly put your mind at ease,” Hoff says.\n• If baby was premature or has health issues. According to the Centers for Disease Control and Prevention (CDC), sterilization is particularly important if baby was born prematurely or has a weakened immune system.\n• If you don’t have access to clean drinking water. If your home isn’t part of a municipality with clean drinking water, you use well water or you’re traveling in a country with questionable water, you may need to sterilize baby’s bottles often; once daily or even after each use would be prudent to avoid buildup of harmful microbes.\nAs long as you have good quality municipal drinking water that isn’t coming from a well, it’s not necessary (or even recommended) to sterilize baby’s bottles too often. “Regular sterilization can potentially damage the bottle and allow chemicals to leach into the milk, especially if the bottle has BPA in it,” says Daniel Ganjian, MD, a pediatrician at St. John’s Health Center in Santa Monica, California. The Food and Drug Administration (FDA) banned the use of bisphenol A, or BPA, in baby bottles back in 2012 due to concerns over the chemical’s impact on infant development—but if you’re using older plastic bottles, make sure they don’t have the recycling number 7 imprinted on the bottom.\nHow often to sterilize baby bottles is really up to you, so do what feels right for your family. If you use a dishwasher with hot water and a heated drying cycle to clean your child’s feeding items, sanitizing baby bottles by hand isn’t called for. Otherwise, for extra germ removal beyond standard washing, the CDC says you can sanitize bottles at least once daily.\nWhen to stop sterilizing baby bottles\nIf you do decide to sterilize baby’s bottles regularly, it’s okay to stop once baby is older than 3 months, according to CDC guidelines, since baby’s immune system isn’t quite so fragile anymore.\nYou should also stop sterilizing baby bottles and accessories if you notice any damage, Ganjian says. Glass bottles with cracks or chips should be tossed out, as should plastic bottles with splits, cracks, strong odors or any warping. Bottle nipples that have sustained noticeable wear and tear should always be replaced, since they can be a choking hazard.\nSterilization kills bacteria in bottles through the use of high temperatures or chemicals, Armin says, and one method isn’t superior to another. So when it comes to deciding how to sterilize baby bottles, choose an approach that works best for you and your budget. Read on for step-by-step instructions for how to sterilize baby bottles using various techniques.\nSterilizing baby bottles with boiling water\nNo special equipment required here! To sterilize baby bottles using boiling water, all you need is water and a pot. And don’t worry—it’s fine to sanitize plastic bottles using this method.\n- Fill a large, clean pot with enough water to cover the bottles.\n- Submerge the freshly washed bottles in the water upside down, making sure there aren’t any air bubbles at the bottom.\n- Bring the water to a boil.\n- Boil the bottles for five minutes (check manufacturer guidelines for variations).\n- Turn the heat off and remove the bottles using tongs.\n- Place them on a clean, dry dishcloth and allow them to air dry.\nSterilizing baby bottles in the microwave\nAnother super-easy approach to sterilizing baby bottles? Using your microwave’s steam power! Here’s how to sterilize baby bottles in the microwave without any other special equipment:\n- Start with a clean microwave.\n- Fill bottles about halfway with water.\n- Microwave on high for one to two minutes.\n- Using oven mitts, remove bottles from the microwave, dump remaining water out and let the bottles air dry.\nAnother option is to purchase a microwave baby bottle sterilizer. This type of sterilizer also harnesses the power of steam, but it encloses the bottles in a plastic casing to give them a more thorough cleansing. These handy sterilizers are widely available and typically cost about half as much as the better-known electric baby bottle sterilizers.\nSterilizing baby bottles with electric steam\nIf you know you’ll sleep easier if baby’s bottles are sterilized regularly, you may want to spring for a countertop bottle sterilizer. Steam sterilization can reach higher temperatures than boiling water, so it can kill more bacteria and mold, Ganjian says.\nThough they’re a bit more pricey than any of the other options, electric baby bottle sterilizers are probably the quickest, easiest option if you want (or need) to sanitize bottles frequently. Simply follow the instructions provided by the manufacturer. Plus, they can be used to sanitize bottles, bottle parts, nipples and more. Many moms even use these for small plastic toys and teething rings once baby outgrows the bottle stage. That’s knowing how to stretch a dollar!\nSterilizing baby bottles with bleach\nIf you’re in a pinch and don’t have access to boiling water, steam or a dishwasher, the CDC condones the use of bleach to clean baby bottles. Here’s how to sterilize baby bottles with this method:\n- Combine one teaspoon of unscented bleach with 16 cups of hot water.\n- Submerge bottles in the solution, taking care to avoid any air bubbles in the bottom of the bottles.\n- Soak bottles for two to five minutes, then remove with clean tongs.\n- Place bottles on a clean dish towel to air dry. There’s no need to rinse, Armin says: “Any remaining bleach will break down quickly during the air-drying process and will not harm baby.”\nSterilizing baby bottles using sterilizing tablets\nWondering how to sterilize baby bottles when you’re away from home and don’t have access to your normal equipment? Food-grade, chlorine-based sterilizing tablets are just as effective at removing all the same microbes as the other sterilization techniques above. Be sure to follow the instructions on the packaging to ensure proper sterilization.\nRegardless of whether you decide to sterilize baby’s bottles, you’ll still have to thoroughly clean them after every feeding. “Newborns and infants have underdeveloped immune systems are vulnerable to infections by viruses, bacteria, parasites and fungi, which can all lead to illness. These germs can grow quickly if breast milk or formula is added to a partially used bottle that hasn’t been well cleaned,” Armin says. “Washing items thoroughly with hot water and soap is all that’s required to remove most harmful germs from bottles.” You can choose to wash bottles and their parts by hand or in the dishwasher. Here’s how:\nCleaning baby bottles in the dishwasher\nAre your baby bottles dishwasher safe? Good news: Using your dishwasher’s hottest water setting and a heated drying cycle effectively sterilizes the bottles!\n- Separate all bottle parts.\n- Rinse the bottles and parts with clean water to remove any milk particles.\n- Place all small parts (including rings, valves and nipples) in a dishwasher-safe basket to prevent them from falling to the bottom of the dishwasher.\n- When possible, run the bottles on a hot-water cycle and heated drying cycle or select the sanitizing setting.\n- Remove the bottles and parts from the dishwasher and allow to air dry on a clean dishcloth.\nCleaning baby bottles by hand\nWhen cleaning by hand, the CDC recommends washing the bottles and their parts in a special container that’s only used for bottles, rather than having bottles come in contact with the sink, to prevent cross-contamination. You should also use a bottle brush or other cleaning utensil that’s set aside just for baby’s bottles.\n- Start with clean hands.\n- Separate the bottles and their parts and rinse each piece under running hot or cold water to remove any milk particles. Don’t set the bottles down in the sink.\n- Fill a clean basin with hot water and soap.\n- Scrub the bottles and parts with a bottle brush, taking care to thoroughly clean all the way to the bottom of the bottle.\n- Clean inside the nipples, making sure to flush water through the tiny holes at the tips.\n- Rinse again under running water.\n- Air dry on a clean dishcloth.\nUpdated February 2020\nAs a health and safety investigator for ConsumerSafety.org, Caitlin Hoff educates families about important consumer topics that impact the general public’s health and safety. She holds a certification in CDC Health Literacy for Public Health Professionals.\nSamira Armin, MD, FAAP, is a pediatrician at Texas Children’s Pediatrics. She earned her medical degree from St. George’s University in 2009 and specializes in newborns and healthy eating.\nDaniel Ganjian, MD, is a pediatrician at Providence St. John’s Health Center in Santa Monica, California. He earned his medical degree from the University of California, Irvine, in 2004 and is a member of the Alpha Omega Alpha honor society, a designation given to just 10 percent of American physicians.\nPlus, more from The Bump:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:29fd56f9-96bf-4e24-8ef8-ef3bdedf5b04>","<urn:uuid:b8eeccf6-a062-47f9-a4af-4164dd376408>"],"error":null}
{"question":"How do nanoparticles contribute to sustainable energy development, and what are their current applications in dental materials?","answer":"In sustainable energy, nanoparticles are advancing solar cell technology with research focusing on nanowires and nanostructured materials to create more efficient and cost-effective cells than conventional planar silicon ones. They're also crucial in hydrogen fuel cells, where 1-5 nm noble metal particles serve as catalysts, and in developing batteries with higher energy content. In dentistry, nanoparticles are intentionally added to dental products to enhance material properties and can also be present as byproducts from filler milling processes. They are commonly found in various dental materials, though the general risk of titanium nanoparticles from dental implants in alveolar bone is considered low.","context":["Nanoparticles are present in nature or can be purposely manufactured and are used to a large extent in everyday life, e.g. in cosmetics like sunscreen containing zinc oxide nanoparticles.\nNanoparticles are intentionally added/embedded into dental products to improve material properties. In addition, nanoparticles can be byproducts from milling processes for fillers and thus get embedded in many dental materials.\nIn the dental laboratory, dental technicians are exposed to nanoparticles as dust.\nIn the dental practice, dental personnel are mainly exposed to nanoparticle dust produced by grinding and polishing dental materials, irrespective of nanoparticles being present in the material. The lungs are the prime target organ. Recent risk assessment has shown that the health risk for dental personnel after inhalation of nanoparticles as dust is likely to be low. No data are available regarding the effects of long-term exposure of dental nanoparticles for dental personnel. Despite exposure to dental nanoparticles for many decades, there are no indications of an increased rate of lung disease for dental personnel.\nPatients are exposed to dental nanoparticle dust or debris but to a much lesser extent than dental personnel. Recent risk assessment has shown that the health risk for patients for both inhalation of nanoparticles or ingestion from wear is likely to be low. Available information is limited, especially regarding the effect of dental material nanoparticles on vulnerable patient groups, such as those with asthma or chronic obstructive pulmonary disease.\nCurrent evidence suggests that the general risk of titanium nanoparticles from dental implants in the alveolar bone is likely to be low.\nRecently, nanoparticles have become a matter of public and scientific concern. National and international agencies are dealing with nanoparticles and their safety as they may cause adverse effects due to their size and possibly their chemical composition.\nThis FDI Policy Statement covers the effects of nanoparticles in and from dental materials on the health of patients and dental personnel, and on the environment.\nFor the purpose of this document, a nanoparticle is defined as a particle having one or more external dimensions in the size range from 1 nm-100 nm.\nEffective oral healthcare must be based on high quality and safety. As nanoparticles in dentistry have become a matter of concern, FDI has analyzed the most recent data on the matter to advise and protect patients, dental personnel, and the environment.\nFDI supports the following statements:\n- FDI agrees to promote research on the health effects of ingestion/inhalation, and cell and tissue exposure to nanoparticles from dental materials.\n- In the dental laboratory, dental personnel must follow available relevant national/international occupational health safety regulations. In countries where no regulation is available, efforts should be made to reduce the risks by wearing filtering masks and providing effective local ventilation in the laboratory. Encapsulated powder/liquid systems may further reduce the dust exposure.\n- In order to minimize any possible risk to dental personnel in practice and patients, the amount of generated dental nanoparticle dust should be kept to a minimum and the following measures are recommended:\n- Proper sculpting of the restorations before setting/curing may reduce the amount of material that is cut during the finishing and polishing step.\n- Adequate amount of water coolant and effective suction when grinding and polishing intraorally whenever possible.\n- Effective local ventilation in the treatment area and the installation of ventilation devices designed for air purification purposes could also be considered.\n- Encapsulated powder/liquid systems may further reduce the dust exposure.\n- Common surgical face masks and FFP3 (FFP=Filtering Face Piece) reduce exposure to nanoparticles. Attention should be given to ensuring a close fit of the mask.\n- Available data on possible adverse reactions derived from nanoparticles in and from dental materials by manufacturing and processing dental materials and environmental exposure are sparse, and more research is necessary. When developing dental materials and application methods, emphasis should be given to minimizing nanoparticle exposure.\nThe information in this Policy Statement was based on the best scientific evidence available at the time. It may be interpreted to reflect prevailing cultural sensitivities and socio-economic constraints.\n- Schmalz G, Hickel R, van Landuyt KL, Reichl FX Nanoparticles in Dentistry. Int Dent J 2018 2018 May 22. doi: 10.1111/idj.12394.\n- Schmalz G, Hickel R, van Landuyt KL, Reichl FX Nanoparticles in Dentistry. Dent Mater 2017 Nov;33(11):1298-1314.","Environmental impact of nanotechnology\n|Part of a series of articles on the|\nThe environmental impact of nanotechnology is the possible effects that the use of nanotechnological materials and devices will have on the environment. As nanotechnology is an emerging field, there is great debate regarding to what extent industrial and commercial use of nanomaterials will affect organisms and ecosystems.\nNanotechnology's environmental impact can be split into two aspects: the potential for nanotechnological innovations to help improve the environment, and the possibly novel type of pollution that nanotechnological materials might cause if released into the environment.\nNanopollution is a generic name for waste generated by nanodevices or during the nanomaterials manufacturing process. Ecotoxicological impacts of nanoparticles and the potential for bioaccumulation in plants and microorganisms is a subject of current research, as nanoparticles are considered to present novel environmental impacts. Of the US$710 million spent in 2002 by the U.S. government on nanotechnology research, $500,000 was spent on environmental impact assessments.\nThe capacity for nanoparticles to function as a transport mechanism also raises concern about the transport of heavy metals and other environmental contaminants. Two areas of concern can be identified. First, in their free form nanoparticles can be released into the air or water during production, or production accidents, or as waste by-product of production, and ultimately accumulate in the soil, water, or plant life. Second, in fixed form, where they are part of a manufactured substance or product, they will ultimately have to be recycled or disposed of as waste.\nScrinis raises concerns about nano-pollution, and argues that it is not currently possible to “precisely predict or control the ecological impacts of the release of these nano-products into the environment.” A May 2007 Report to the UK Department for Environment, Food and Rural Affairs noted concerns about the toxicological impacts of nanoparticles in relation to both hazard and exposure. The report recommended comprehensive toxicological testing and independent performance tests of fuel additives. Risks have been identified by Uskokovic in 2007. Concerns have also been raised about Silver Nano technology used by Samsung in a range of appliances such as washing machines and air purifiers.\nLife cycle responsibility \nTo properly assess the health hazards of engineered nanoparticles the whole life cycle of these particles needs to be evaluated, including their fabrication, storage and distribution, application and potential abuse, and disposal. The impact on humans or the environment may vary at different stages of the life cycle.\nThe Royal Society report identified a risk of nanoparticles or nanotubes being released during disposal, destruction and recycling, and recommended that “manufacturers of products that fall under extended producer responsibility regimes such as end-of-life regulations publish procedures outlining how these materials will be managed to minimize possible human and environmental exposure” (p.xiii). Reflecting the challenges for ensuring responsible life cycle regulation, the Institute for Food and Agricultural Standards has proposed standards for nanotechnology research and development should be integrated across consumer, worker and environmental standards. They also propose that NGOs and other citizen groups play a meaningful role in the development of these standards.\nEnvironmental benefits of nanotechnology \nNanotechnology could potentially have a great impact on clean energy production. Research is underway to use nanomaterials for purposes including more efficient solar cells, practical fuel cells, and environmentally friendly batteries. The most advanced nanotechnology projects related to energy are: storage, conversion, manufacturing improvements by reducing materials and process rates, energy saving (by better thermal insulation for example), and enhanced renewable energy sources.\nCurrent commercially available solar cells have low efficiencies of 15-20%. Research is ongoing to use nanowires and other nanostructured materials with the hope of to create cheaper and more efficient solar cells than are possible with conventional planar silicon solar cells. It is believed that these nanoelectronics-based devices will enable more efficient solar cells, and would have a great effect on satisfying global energy needs.\nAnother example for an environmentally friendly form of energy is the use of fuel cells powered by hydrogen. Probably the most prominent nanostructured material in fuel cells is the catalyst consisting of carbon supported noble metal particles with diameters of 1-5 nm. Suitable materials for hydrogen storage contain a large number of small nanosized pores.\nNanotechnology may also find applications in batteries. Because of the relatively low energy density of conventional batteries the operating time is limited and a replacement or recharging is needed, and the huge number of spent batteries represent a disposal problem. The use of nanomaterials may enable batteries with higher energy content or supercapacitors with a higher rate of recharging, which could be helpful for the battery disposal problem.\nWater filtration and remediation \nA strong influence of nanochemistry on waste-water treatment, air purification and energy storage devices is to be expected.\nMechanical or chemical methods can be used for effective filtration techniques. One class of filtration techniques is based on the use of membranes with suitable hole sizes, whereby the liquid is pressed through the membrane. Nanoporous membranes are suitable for a mechanical filtration with extremely small pores smaller than 10 nm (“nanofiltration”) and may be composed of nanotubes. Nanofiltration is mainly used for the removal of ions or the separation of different fluids.\nMagnetic nanoparticles offer an effective and reliable method to remove heavy metal contaminants from waste water by making use of magnetic separation techniques. Using nanoscale particles increases the efficiency to absorb the contaminants and is comparatively inexpensive compared to traditional precipitation and filtration methods.\nSome water-treatment devices incorporating nanotechnology are already on the market, with more in development. Low-cost nanostructured separation membranes methods have been shown to be effective in producing potable water in a recent study.\nSee also \n- Gyorgy Scrinis (2007). \"Nanotechnology and the Environment: The Nano-Atomic reconstruction of Nature\". Chain Reaction 97: 23–26.\n- Vuk Uskokovic (2007). \"Nanotechnologies: What we do not know\". Technology in Society 29: 43–61. doi:10.1016/j.techsoc.2006.10.005.\n- Royal Society and Royal Academy of Engineering (2004). Nanoscience and nanotechnologies: opportunities and uncertainties. Retrieved 2008-05-18.\n- Tian, Bozhi; Zheng, Xiaolin; Kempa, Thomas J.; Fang, Ying;Yu, Nanfang; Yu, Guihua; Huang, Jinlin & Lieber, Charles M. (2007). \"Coaxial silicon nanowires as solar cells and nanoelectronic power sources\". Nature 449 (7164): 885–889. Bibcode:2007Natur.449..885T. doi:10.1038/nature06181. PMID 17943126.\n- Hillie, Thembela; Hlophe, Mbhuti (2007). \"Nanotechnology and the challenge of clean water\". Nature Nanotechnology 2 (11): 663–664. Bibcode:2007NatNa...2..663H. doi:10.1038/nnano.2007.350. PMID 18654395.\n- Zhang, Wei-xian (2003). \"Nanoscale iron particles for environmental remediation: an overview.\". Journal of Nanoparticle Research 5: 323–332. doi:10.1023/A:1025520116015.\nFurther reading \n- Colvin VL (Oct 2003). \"The potential environmental impact of engineered nanomaterials\". Nat Biotechnol. 21 (10): 1166–70. doi:10.1038/nbt875. PMID 14520401.\n- University of California Center for Environmental Implications of Nanotechnology\n- Duke University Center for the Environmental Implications of NanoTechnology"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:505b57b4-b4de-45b5-b7b1-a54e022667bd>","<urn:uuid:070cf2a9-bbb6-4bca-8984-96222ad21c86>"],"error":null}
{"question":"As someone working in refugee healthcare, I am interested to know: what are the educational barriers faced by refugees in UK universities, and what evidence exists about their healthcare access and mental health status?","answer":"Refugees face significant educational barriers in UK universities, primarily due to financial constraints. They are not entitled to domestic fees due to their immigration status and must pay higher international fees, making higher education often unaffordable. Currently, only 3% of refugees globally can access higher education. However, initiatives like Sanctuary Scholarships help address this by providing tuition fee waivers and living bursaries. Regarding healthcare and mental health, research shows that Syrian refugees particularly struggle with chronic illnesses, low quality of life, and anxiety. A clinical trial demonstrated that pharmaceutical care services can significantly improve refugees' drug-related problems and anxiety scores, though quality of life improvements were seen regardless of whether medication-related issues were resolved.","context":["By Joel Bates, Intern at What Works\nA hidden imbalance of access to education may very well impact someone you know. A classmate, colleague, or friend who has been barred the right to an affordable education because they, or their family had to flee danger in the search for a better and safer life in the UK. According to the United Nations High Commission for Refugees (UNHCR), only 3% of refugees are able to access higher education (HE) globally[i]. In recent years, a series of global refugee crises has highlighted an ever-growing need to offer sustainable and expansive support for: refugees, forced migrants, and their families. Since 2008, Sanctuary Scholarships have helped to mitigate against the systematic injustices faced by this specific group of students.\nFor context, the term ‘Sanctuary Scholarship’ is used to generally describe funding initiatives which exclusively support forced migrants without access to student finance or other private funding, to study at a UK university. As it stands, students from forced migrant backgrounds that are born, or brought up in the UK are not entitled to domestic fees due to their parents’ citizenship status and their own temporary immigration status. As a result, students are left to either pay higher international level university fees or abandon hopes of progressing to higher education. This goes against the mission of widening participation at King’s to work with “underrepresented learners and their supporters, empowering them to access, thrive and succeed in higher education.”[ii]\nSanctuary Scholarships at King’s: what have they achieved so far?\nFor each Scholar tuition fees are waived, and a means-tested living bursary is provided to ensure that recipients have the financial security to thrive at university. So far, Sanctuary Scholarships have enabled 25 King’s students that were otherwise locked out of education to make great strides and achieve personal and professional successes.\nIn March 2021, King’s Service Team commissioned a project to interview ten Scholars from past and present cohorts to explore how the students felt about their academic success and confidence levels (self-efficacy), as well as the wider benefits of the bursaries on Scholars’ university experiences. Through unlocking access to HE, the Scholars were able to achieve a deeper sense of self-belief, gain a greater sense of power and autonomy, along with improved levels of confidence. Highlighted below are some of the key outcomes that students shared as part of their interviews.\nFinding one: Increased self-confidence was a key takeaway from these interviews:\n“When I started the course, it just boosted my confidence, in terms of thinking that it is not too late to start university.” – Sanctuary Scholar.\nThis supports literature which has highlighted that refugee and forced migrants merely need guidance and support to vastly increase feelings of self-efficacy and confidence[iii].\nFinding two: In interviews, scholars reported a reduction in feelings of self-defeat, and lack of self-determination:\n“It [the bursary] has helped me a lot because since I got the scholarship, then I thought whoever chose me, they must have believed in me. Now I know that I have potential.” – Sanctuary Scholar.\nFinding three: The interviews further highlighted that scholars possessed an abundance of drive and determination but were previously lacking opportunities to display their impressive work ethic:\n“I would take a bus and a train for two and a half hours before I got to work. I would do a night shift and then I would go to lessons in the morning… it becomes so engrained within you, you’re always operating” – Sanctuary Scholar.\nThe idea that forced migrants and refugees have the tenacity and drive to succeed but are simply held back by the systems in place is not a new realisation. Research has well established that unjust policies and systems are a significant factor to explain the uncertainty and chaos faced by forced migrants globally[iv].\nFinding four: Scholars felt systemic barriers limited their opportunities. This created tension and frustration from not being able to reach their true potential:\n“There is nothing more self-defeating, knowing that you can offer more but you’re always limited” – Sanctuary Scholar.\nThis finding is mirrored in wider literature[v], which outlines the issue with access and social mobility, caused by excluding refugees and forced migrants from entering HE\nFinding five: Sanctuary Scholarships provide immeasurable support to vulnerable families, freeing them from the financial burden of funding HE for their children, siblings and grandchildren:\n“When I got that scholarship, it was like a giant weight had been lifted off my shoulders.” –Sanctuary Scholar.\n“My dad didn’t have to worry about where he’s going to find the next £30,000 to pay for my second year. Yes, so it’s a whole wide network sense of relief that you get.” –Sanctuary Scholar.\nFinding six: Unfortunately, students reported a potential lack of understanding among HE staff:\n“I told her then that I was applying for the King’s scholarship, and she said, “Oh, why?” And I just explained to her what was going on. And she looked at me, so amazed” – Sanctuary Scholar.\nAll students deserve informed and reliable advice from staff, no matter their background. As the refugee crisis continues, HE staff must stay informed of common struggles and burdens that are unique to forced migrant students in order to offer helpful and accessible guidance to their students[vi].\nThis project carried out by What Works reflects how beneficial and important Sanctuary Scholarships, and other similar initiatives are for: the beneficiaries, their families and wider society. Research from the 2016 Syrian refugee crisis[vii] highlighted that not only did individuals and families gain access to education and social mobility, but society gained tangible benefits from the introduction of such dedicated young professionals into a wide array of competitive industries.\nAs such it is clear that many of our Scholars will go on to achieve academic and professional excellence throughout their lives. Something which will no doubt benefit the economy and society in the long term. With this in mind, it is more important than ever before to commit to supporting and creating opportunities for Sanctuary Scholars of the future.\nLooking forward – Sanctuary Scholars of the future\nWith the socio-political upheaval currently taking place in Afghanistan it is foreseeable that many forced migrants will arrive in the UK searching for safety for themselves and their families. The unfolding crisis in Afghanistan serves to reinforce the value and need for initiatives such as the Sanctuary Scholarship programme. We must take action now. And double down our efforts to enable forced migrants to access HE and truly flourish.\nSecuring funding to support forced migrants in HE relies on generous donations and successful fundraising campaigns to provide the well-deserved support that Scholars would not ordinarily receive. Given the current crisis in Afghanistan, it is increasingly important to secure extra funds to maintain our Scholarship programme, and expand to meet the growing demand for fair and equitable access to education that all learners deserve.\nTo hear more about the Sanctuary Scholars project, you can contact What Works at firstname.lastname@example.org.\nTo support the Sanctuary Scholarship programme and the vital assistance it provides, contact Sarah Cook – Head of Major Gifts at email@example.com.\nTo find out more about King’s work with refugees and forced migrants, visit the King’s Sanctuary Programme webpages.\nClick here to join our mailing list.\nFollow us on Twitter: @KCLWhatWorks\n[i] United Nations High Commissioner for. (n.d.). Climate change and disaster displacement. UNHCR. Retrieved 24 August 2021, from https://www.unhcr.org/climate-change-and-disasters.html\n[iii] Tip, L. K., Brown, R., Morrice, L., Collyer, M., & Easterbrook, M. J. (2020). Believing is achieving: a longitudinal study of self-efficacy and positive affect in resettled refugees. Journal of Ethnic and Migration Studies, 46(15), 3174-3190.\n[iv] Sobane, K., Momani, F. A., Bislimi, F., Nouns, I., & Lunga, W. (2018). Barriers to access to education for migrant children.\n[v] Dryden-Peterson, S., & Giles, W. (2010). Higher education for refugees. Refuge: Canada’s Journal on Refugees, 27(2), 3-9.\n[vi] Dunwoodie, K., Webb, S., & Wilkinson, J. (2019). Embracing Social Inclusion?: The Asylum Seeker Experience of Applying for Admission to Tertiary Education in Australia. In Power and Possibility (pp. 143-153). Brill Sense.\n[vii] Wit, H. de, & Altbach, P. (2016). The Syrian Refugee Crisis and Higher Education. International Higher Education, 84, 9–10. https://doi.org/10.6017/ihe.2016.84.9109","A randomized control trial assessing the effect of a pharmaceutical care service on Syrian refugees’ quality of life and anxiety\nBackground: Syrian refugees residing in Jordan suffer from chronic illnesses, low quality of life (QoL) and anxiety. Pharmacists delivering the medication review service can have a role in improving this growing worldwide problem.\nObjectives: To assess the effect of the medication review service on QoL and anxiety scores for Syrian refugees living with chronic medical conditions.\nMethods: This randomized single-blinded intervention control study was conducted in Jordan. Syrian refugees were recruited and randomized into intervention and control groups. Two home visits were organized with each participant, at baseline and three months later. The medication review service was delivered to the participants and questionnaires regarding QoL and anxiety were completed by all participants. As a part of the medication review service, drug-related problems (DRPs) were identified by a clinical pharmacist for all patients, but recommendations to resolve these DRPs were delivered to intervention group refugees’ physicians only (control group patients did not receive this part of the service till the end of the study); DRPs were corrected and pharmacist-delivered counseling and education were provided as well. At follow-up, DRPs assessment, QoL and anxiety scores were assessed for refugees in the intervention and control groups.\nResults: Syrian refugees (n=106) were recruited and randomized into intervention (n=53) and control (n=53) groups with no significant difference between both groups at baseline. The number of medications and diagnosed chronic diseases per participant was 5.8 (SD 2.1) and 2.97 (SD 1.16), respectively. At follow-up, a significant decrease in the number of DRPs for refugees in the intervention group was found (from 600 to 182, p<0.001), but not for the control group (number stayed at 541 DRPs, p=0.116). Although no significant difference between the groups was found with regards to QoL at follow-up (p=0.266), a significant difference was found in the anxiety scores between the groups (p<0.001).\nConclusion: The medication review service delivered by clinical pharmacists can significantly improve refugees’ DRPs and anxiety scores. As for QoL, significant improvements can be seen for all refugee patients, regardless of whether the DRPs identified were resolved or not.\nTurrini G, Purgato M, Ballette F, Nosè M, Ostuzzi G, Barbui C. Common mental disorders in asylum seekers and refugees: umbrella review of prevalence and intervention studies. Int J Ment Health Syst. 2017;11:51. https://doi.org/10.1186/s13033-017-0156-0\nGeorgiadou E, Morawa E, Erim Y. High manifestations of mental distress in arabic asylum seekers accommodated in collective centers for refugees in Germany. Int J Environ Res Public Health. 2017;14(6);612. https://doi.org/10.3390/ijerph14060612\nAl-Smadi AM, Tawalbeh LI, Gammoh OS, Ashour AF, Alshraifeen A, Gougazeh YM. Anxiety, stress, and quality of life among Iraqi refugees in Jordan: A cross sectional survey. Nurs Health Sci. 2017;19(1):100-104. https://doi.org/10.1111/nhs.12323\nLindencrona F, Ekblad S, Hauff E. Mental health of recently resettled refugees from the Middle East in Sweden: the impact of pre-resettlement trauma, resettlement stress and capacity to handle stress. Soc Psychiatry Psychiatr Epidemiol. 2008;43(2):121-131. https://doi.org/10.1007/s00127-007-0280-2\nGlobal Trends forced displacement in 2017. Available at: https://www.unhcr.org/5b27be547.pdf (accesed Feb 3, 2019).\nUnited Nations population Fund regional situation report for the Syrian crisis. Available at: https://www.unfpa.org/sites/default/files/resource-pdf/UNFPA_Regional_Situation_Report_for_the_Syria_Crisis_-_Issue_72_-_August.pdf (accesed Sep 3, 2018).\nWorld Vision Team. Syrian refugee crisis: Facts, FAQs, and how to help. Available at: https://www.worldvision.org/refugees-news-stories/syrian-refugee-crisis-facts (accesed Feb 4, 2020).\nAy M, Arcos González P, Castro Delgado R. The Perceived Barriers of Access to Health Care Among a Group of Non-camp Syrian Refugees in Jordan. Int J Health Serv. 2016;46(3):566-589. https://doi.org/10.1177/0020731416636831\nEl-Khatib Z, Scales D, Vearey J, Forsberg BC. Syrian refugees, between rocky crisis in Syria and hard inaccessibility to healthcare services in Lebanon and Jordan. Confl Health. 2013;7(1):18. https://doi.org/10.1186/1752-1505-7-18\nJokanovic N, Tan EC, Sudhakaran S, Kirkpatrick CM, Dooley MJ, Ryan-Atwood TE, Bell JS. Pharmacist-led medication review in community settings: An overview of systematic reviews. Res Social Adm Pharm. 2017;13(4):661-685. https://doi.org/10.1016/j.sapharm.2016.08.005\nBasheti IA, Ayasrah SM, Basheti MM, Mahfuz J, Chaar B. The Syrian refugee crisis in Jordan: a cross sectional pharmacist-led study assessing post-traumatic stress disorder. Pharm Pract (Granada). 2019;17(3):1475. https://doi.org/10.18549/PharmPract.2019.3.1475\nMahdikhani S, Dabaghzadeh F. Benefits of Pharmacist’s Participation on Hospitalist Team. Acta Med Iran. 2016;54(2):140-145.\nBasheti IA, Rizik M, Bulatova NR. Home medication management review in outpatients with alarming health issues in Jordan: a randomized control trial. J Pharm Health Serv Res. 2018;9(2):91-100.\nBrandt M, Hallas J, Graabæk T, Pottegård A. Description of a practice model for pharmacist medication review in a general practice setting. Pharm Pract (Granada). 2014;12(3):420-443. https://doi.org/10.4321/s1886-36552014000300005\nBasheti IA, Tadros OK, Alnajjar MS, Aburuz S. Assessing patient satisfaction with the medication management review service delivered in Jordan. J Pharm Health Serv Res. 2019;10(1):49-55. https://doi.org/10.1111/jphs.12233\nPellegrino AN, Martin MT, Tilton JJ, Touchette DR. Medication therapy management services. Drugs. 2009;69(4):393-406. https://doi.org/10.2165/00003495-200969040-00001\nCastelino RL, Bajorek BV, Chen TF. Are interventions recommended by pharmacists during Home Medicines Review evidence‐based? J Eval Clin Pract. 2011;17(1):104-110. https://doi.org/10.1111/j.1365-2753.2010.01375.x\nAl-Qudah RA, Tuza O, Tawfiek H, Chaar B, Basheti IA. Community pharmacy ethical practice in Jordan: assessing attitude, needs and barriers. Pharm Pract (Granada). 2019;17(1):1386. https://doi.org/10.18549/PharmPract.2019.1.1386\nAlkoudsi KT, Basheti IA. Prevalence of anxiety and depression among women with Polycystic Ovary Syndrome living in war versus non-war zone countries: A randomized controlled trial assessing a pharmacist intervention. Res Social Adm Pharm. 2019 [Ahead of print]. https://doi.org/10.1016/j.sapharm.2019.08.027\nBasheti IA, Tadros OK, Aburuz S. Value of a community‐based medication management review service in Jordan: a prospective randomized controlled study. Pharmacotherapy. 2016;36(10):1075-1086. https://doi.org/10.1002/phar.1833\nBasheti IA, Al-Qudah RA, Obeidat NM, Bulatova NR. Home medication management review in outpatients with chronic diseases in Jordan: a randomized control trial. Int J Clin Pharm. 2016;38(2):404-413. https://doi.org/10.1007/s11096-016-0266-9\nAbuNaba'a Y, Basheti IA. Assessing the impact of medication management review service for females diagnosed with depression and anxiety: A randomized control trial. J Eval Clin Pract. 2019 [Ahead of print]. https://doi.org/10.1111/jep.13314\nBasheti IA, Qunaibi EA, Bulatova NR, Samara S, AbuRuz S. Treatment related problems for outpatients with chronic diseases in Jordan: the value of home medication reviews. Int J Clin Pharm. 2013;35(1):92-100. https://doi.org/10.1007/s11096-012-9713-4\nAburuz SM, Bulatova NR, Yousef AM, Al-Ghazawi MA, Alawwa IA, Al-Saleh A. Comprehensive assessment of treatment related problems in hospitalized medicine patients in Jordan. Int J Clin Pharm. 2011;33(3):501-511. https://doi.org/10.1007/s11096-011-9497-y\nErnst FR, Grizzle AJ. Drug-related morbidity and mortality: updating the cost-of-illness model. J Am Pharm Assoc (Wash). 2001;41(2):192-199. https://doi.org/10.1016/s1086-5802(16)31229-3\nKothari D, Gupta S, Sharma C, Kothari S. Medication error in anaesthesia and critical care: A cause for concern. Indian J Anaesth. 2010;54(3):187-192. https://doi.org/10.4103/0019-5049.65351\nPinilla J, Murillo C, Carrasco G, Humet C. Case-control analysis of the financial cost of medication errors in hospitalized patients. Eur J Health Econ. 2006;7(1):66-71. https://doi.org/10.1007/s10198-005-0332-z\nAl Alawneh M, Nuaimi N, Basheti IA. Pharmacists in humanitarian crisis settings: Assessing the impact of pharmacist-delivered home medication management review service to Syrian refugees in Jordan. Res Social Adm Pharm. 2019;15(2):164-172. https://doi.org/10.1016/j.sapharm.2018.04.008\nSluggett JK, Ilomäki J, Seaman KL, Corlis M, Bell JS. Medication management policy, practice and research in Australian residential aged care: current and future directions. Pharmacol Res. 2017;116:20-28. https://doi.org/10.1016/j.phrs.2016.12.011\nFreeman CR, Cottrell WN, Kyle G, Williams ID, Nissen L. An evaluation of medication review reports across different settings. Int J Clin Pharm. 2013;35(1):5-13. https://doi.org/10.1007/s11096-012-9701-8\nGusi N, Olivares PR, Rajendram R. The EQ-5D health-related quality of life questionnaire. In: Preedy VR, Watson RR, eds. Handbook of disease burdens and quality of life measures. New York, NY:Springer;2010. ISBN: 978-0-387-78664-3\nZigmond AS, Snaith RP. The hospital anxiety and depression scale. Acta Psychiatr Scand. 1983;67(6):361-370. https://doi.org/10.1111/j.1600-0447.1983.tb09716.x\nJohnston M, Pollard B, Hennessey P. Construct validation of the hospital anxiety and depression scale with clinical populations. J Psychosom Res. 2000;48(6):579-584. https://doi.org/10.1016/s0022-3999(00)00102-1\nMolino Cde G, Carnevale RC, Rodrigues AT, Visacri MB, Moriel P, Mazzola PG. Impact of pharmacist interventions on drug-related problems and laboratory markers in outpatients with human immunodeficiency virus infection. Ther Clin Risk Manag. 2014;10:631-639. https://doi.org/10.2147/TCRM.S61821\nCoutts A, Fouad FM. Response to Syria's health crisis—poor and uncoordinated. Lancet. 2013;381(9885):2242-2243. https://doi.org/10.1016/s0140-6736(13)61421-x\nAbughosh SM, Wang X, Serna O, Henges C, Masilamani S, Essien EJ, Chung N, Fleming M. A pharmacist telephone intervention to identify adherence barriers and improve adherence among nonadherent patients with comorbid hypertension and diabetes in a Medicare Advantage Plan. J Manag Care Spec Pharm. 2016;22(1):63-73. https://doi.org/10.18553/jmcp.2016.22.1.63\nMossialos E, Courtin E, Naci H, Benrimoj SI, Bouvy ML, Farris KB, Noyce P, Sketris I. From “retailers” to health care providers: transforming the role of community pharmacists in chronic disease management. Health Policy. 2015;119(5):628-639. https://doi.org/10.1016/j.healthpol.2015.02.007\nBruhn H, Bond CM, Elliott AM, Hannaford PC, Lee AJ, McNamee P, Smith BH, Watson MC, Holland R, Wright D. Pharmacist-led management of chronic pain in primary care: results from a randomised controlled exploratory trial. BMJ Open. 2013;3(4): e002361. https://doi.org/10.1136/bmjopen-2012-002361\nObradovic M, Lal A, Liedgens H. Validity and responsiveness of EuroQol-5 dimension (EQ-5D) versus Short Form-6 dimension (SF-6D) questionnaire in chronic pain. Health Qual Life Outcomes. 2013;11:110. https://doi.org/10.1186/1477-7525-11-110\nSteel Z, Chey T, Silove D, Marnane C, Bryant RA, van Ommeren M. Association of torture and other potentially traumatic events with mental health outcomes among populations exposed to mass conflict and displacement: a systematic review and meta-analysis. JAMA. 2009;302(5):537-549. https://doi.org/10.1001/jama.2009.1132\nWorld Health Organization. Global strategy on human resources for health: workforce 2030. Available at: https://apps.who.int/gb/ebwha/pdf_files/WHA72/A72_24-en.pdf (accessed Feb 4, 2020).\nWorld Health Organization. High-Level Commission on Health Employment and Economic Growth, Working for health and growth: investing in the health workforce. Report of the High-Level Commission on Health Employment and Economic Growth. Available at: http://apps.who.int/iris/bitstream/10665/250047/1/9789241511308-eng.pdf (accesssed Feb 4, 2018.\nElwyn G, Edwards A, Kinnersley P. Shared decision-making in primary care: the neglected second half of the consultation. Br J Gen Pract. 1999;49(443):477-482.\nCopyright (c) 2020 Pharmacy Practice and the Authors\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:39cf8325-fd82-4441-9b46-3e2ff53179e9>","<urn:uuid:fb9ea682-4e47-4d97-8b64-2c1734ba22a0>"],"error":null}
{"question":"As a maintenance supervisor, I need to understand warning signs of battery problems. What should I look for in terms of car battery starting issues, and what inspection routines should be followed for industrial batteries?","answer":"For car batteries, if the car won't start, check headlight brightness - if lights dim during cranking, the battery is likely low or dead. Bright lights during cranking usually indicate a bad starter or starter solenoid. Also check for loose or corroded battery cables. For industrial batteries, implement regular inspection routines: examine battery leads and plugs for damage before each charge; ensure battery covers are open during charging for proper ventilation; verify vent caps are properly installed to prevent acid spillage; and maintain clean, dry battery tops. Watch for signs of overheating during charging, and report any equipment faults immediately as damaged charging equipment can lead to improper charging.","context":["FREQUENTLY ASKED QUESTIONS\nMy car won’t start and won’t crank over. What do I do?\nIf your car won’t crank over, check the battery voltage by turning on the headlights. If the lights come on bright, have someone watch the headlights while turning the ignition key to the cranking position. If the lights dim or go out, your battery is low or dead. Also check your car’s battery cables. They might be loose or corroded.\nIf the headlights stay bright, you probably have a bad starter or starter solenoid. If your car has an aftermarket alarm system, you might have an alarm system problem.\nRemember if you try to use Jumper cables, Red is the positive terminal and Black is the negative terminal (Ground). Hook up both the Red terminals, then the Ground terminals, then start the car you are using to jump the broken car. Rev up the engine on the running car, this will give you extra power to start the broken car and protect the electrical of this car. Then have someone start the broken car. If possible, remember to wear glasses when working around batteries.\nMy Check Engine light is on! What does it mean?\nWhen the yellow or orange Check Engine (or Service Engine Soon) light on, the car’s computer is monitoring the fuel injection, smog system and some ignition systems. If the car is running normally, don’t worry too much. Look at the other gauges and warning lights. If they are normal, you probably have a sensor or emissions problem. Bring it to your repair shop at your convenience. You are probably using extra gas, but you will not get stuck with your car.\nIf your car is sluggish or missing and the Check Engine light is on and the rest of the gauges look normal, you need to drive to a shop as soon as possible. Your car’s ignition system or injection system maybe malfunctioning and you could get stuck!\nIf your Check Engine light is on and the other gauges are not normal, TURN OFF YOUR CAR! Tow your car in to prevent further damage and to avoid getting stuck on the way to a shop!\nHow do I keep my car from breaking down?\nMost people driving today’s cars do not understand how to repair their own car. Regular, routine maintenance from a high-tech auto repair shop will help avoid a breakdown. During routine maintenance, the shop can catch problems early before they become major repairs. Routine maintenance will also keep your car in better shape for a longer period of time.\nWe would like to see our customers every 3,000 miles for a Lube, Oil & Filter. At that service interval, we can get a good overall look at you car. Depending on your driving habits, rotate your tire every second or third service.\nEvery 15,000 miles change your air filter. Every 30,000 miles service the automatic transmission and replace fuel filters. 60,000 miles is usually the biggest service interval. Tune ups, cooling system and timing belts are all done at 60,000 miles. This service is expensive, so remember to budget for it in advance.\nWe can always estimate the price to you in advance.","12 Safety Tips When Working With Industrial Batteries\nFiled under: Batteries\nFollowing on from our previous blog post looking at considerations for the safe design of a battery charging station or room, this blog post will look at 12 safety tips when working with industrial batteries.\n1. No smoking, sparks, naked flames or welding in close proximity to battery charging\nIt may seem slightly obvious but having any naked flame or spark near a battery charging station is an immense ignition risk, which could result in a major explosion.\n2. Ensure battery cover is open during charging\nAs batteries emit hydrogen gas during charging adequate ventilation is needed to disperse the gas otherwise it can build up in high concentration and become an ignition risk. Opening the battery cover will provide sufficient ventilation.\n3. Keep vent caps on during charging\nThe vent caps have their own vents in them for allowing gas to escape. If the caps are open or removed, droplets of acid and water will form on the top of the battery causing electrical shorts to the case and frame of the battery. This can lead to hard to trace problems with your lift truck.\n4. Always switch charger off before disconnecting battery\nIt is imperative that all chargers are switched off before you disconnect a battery as live electricity can spark and become a source of ignition.\n5. Never unplug battery by pulling leads, always hold plug\nPulling leads out of a battery will eventually damage the leads.\n6. Always allow charger to complete charge cycle\nDisconnecting a battery before a charge cycle is finished when using a standard type charger can damage battery performance, thus reducing overall battery life. Opportunity charging is now possible when the appropriate charger is supplied with your battery. Speak to our sales team about more info.\n7. Ensure battery top is clean and dry at all times\nIt is imperative that the top of the battery is clean and dry at all times as if it wet when connected to charge it becomes an electrocution hazard. The battery will also self-discharge due to voltage tracking. This can lead to an over discharged battery condition when return from extended shutdown periods. Having clean charging points also ensure a proper connection is maintained and that the battery charges correctly.\n8. Never discharge battery below 80% of its rated capacity\nYou lead acid traction battery is rated to either 1200 or 1500 cycle to 80% depth of discharge. When the battery is discharged beyond 80% the life cycle expectancy is significantly reduced, instead of lasting at least 5 years (single shift, 5 day a week operation), your battery may only last 2. Over discharging also causes electrical issues with your lift truck, overheating electrical circuits, bowing fuses etc. In some instances, the charger will not recognise the battery for a recharge and you will need expensive battery repairs.\n9. Always allow appropriate cooling period for battery\nA proposed cooling down period should be observed as the battery temperature increases under charging. It becomes a potential hazard if it overheats; it also significantly reduces the life of the battery.\n10. Inspect battery, leads and plugs for damage, report any faults immediately\nA thorough inspection of all charging equipment should be conducted before each charge to ensure a proper charge is completed. Damaged equipment can lead to faults within the charging process.\n11. If no auto equalise charge, ensure battery is equalised at least once a month\nYou battery is made up of multiple 2V cells linked together to form a required voltage, 24, 36, 48 etc. Each of these cells are individuals, they charge and discharge at different rates. After a period of time, you will end up with varied voltages where some cells are fly charge, some half charged and some flat. If this is not controlled, your battery will fail early. This is where equalise charging comes in. Depending on your work application, the equalise requirements will be different. An equalise charge instigates at a timed period after a full charge has completed, usually on a weekend. A low current charge will occur for a set period of time. A low current is utilised so the fully charged cells do not get overcharged too much whilst the lower cells are catching up.\n12. Only top up once battery is fully charged\nDuring discharge the electrolyte is absorbed into the plates, this effect is reversed when the battery is charged. If the battery is topped up at any stage other than fully charged you risk the battery spilling acid out during the next charge/charges. The reason we top up batteries is to replace the liquid that has evaporated during the charge cycles, not replace the liquid that has been absorbed into the plates during discharge.\nDid you find this blog post useful?\nPrevious entry: Designing a safe forklift battery charging room"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:d9cda14d-b532-449c-97f9-3905f2f24a15>","<urn:uuid:d5fca8b4-5629-43a7-9296-9f42ab0dd605>"],"error":null}
{"question":"What role do beneficial bacteria play in both oral health and infant gut development?","answer":"Beneficial bacteria are crucial in both oral and gut health. In the mouth, a healthy person maintains a stable ecosystem of beneficial bacteria that helps keep harmful bacteria away, similar to a healthy lawn preventing weed growth. In infant gut development, beneficial bacteria, particularly bifidobacteria, form a dominant and prevalent microbial group that plays a pivotal role in maturing and modulating the host immune system, establishing mucosal integrity, and forming the mucus gel layer. These beneficial bacterial communities in both locations help prevent colonization by pathogens and contribute to overall health maintenance.","context":["According to a new study, smoking causes the body to turn against its own helpful bacteria, leaving smokers more vulnerable to disease.\nDespite the daily disturbance of brushing and flossing, the mouth of a healthy person contains a stable ecosystem of healthy bacteria. New research shows that the mouth of a smoker is a much more chaotic, diverse ecosystem -- and is much more susceptible to invasion by harmful bacteria.\nAs a group, smokers suffer from higher rates of oral diseases -- especially gum disease -- than do nonsmokers, which is a challenge for dentists, according to Purnima Kumar, assistant professor of periodontology at Ohio State University. She and her colleagues are involved in a multi-study investigation of the role the body's microbial communities play in preventing oral disease.\n\"The smoker's mouth kicks out the good bacteria, and the pathogens are called in,\" said Kumar. \"So they're allowed to proliferate much more quickly than they would in a non-smoking environment.\"\nThe results suggest that dentists may have to offer more aggressive treatment for smokers and would have good reason to suggest quitting smoking, Kumar said.\n\"A few hours after you're born, bacteria start forming communities called biofilms in your mouth,\" said Kumar. \"Your body learns to live with them, because for most people, healthy biofilms keep the bad bacteria away.\"\nShe likens a healthy biofilm to a lush, green lawn of grass. \"When you change the dynamics of what goes into the lawn, like too much water or too little fertilizer,\" she said, \"you get some of the grass dying, and weeds moving in.\" For smokers, the \"weeds\" are problem bacteria known to cause disease.\nIn a new study, Kumar's team looked at how these bacterial ecosystems regrow after being wiped away. For 15 healthy nonsmokers and 15 healthy smokers, the researchers took samples of oral biofilms one, two, four and seven days after professional cleaning.\nThe researchers were looking for two things when they swabbed subjects' gums. First, they wanted to see which bacteria were present by analyzing DNA signatures found in dental plaque. They also monitored whether the subjects' bodies were treating the bacteria as a threat. If so, the swab would show higher levels of cytokines, compounds the body produces to fight infection.\nThe results of the study were published in the journal Infection and Immunity.\n\"When you compare a smoker and nonsmoker, there's a distinct difference,\" said Kumar. \"The first thing you notice is that the basic 'lawn,' which would normally contain thriving populations made of a just few types of helpful bacteria, is absent in smokers.\"\nThe team found that for nonsmokers, bacterial communities regain a similar balance of species to the communities that were scraped away during cleaning. Disease-associated bacteria are largely absent, and low levels of cytokines show that the body is not treating the helpful biofilms as a threat.\n\"By contrast,\" said Kumar, \"smokers start getting colonized by pathogens -- bacteria that we know are harmful -- within 24 hours. It takes longer for smokers to form a stable microbial community, and when they do, it's a pathogen-rich community.\"\nSmokers also have higher levels of cytokines, indicating that the body is mounting defenses against infection. Clinically, this immune response takes the form of red, swollen gums -- called gingivitis -- that can lead to the irreversible bone loss of periodontitis.\nIn smokers, however, the body is not just trying to fight off harmful bacteria. The types of cytokines in smokers' gum swabs showed the researchers that smokers' bodies were treating even healthy bacteria as threatening.\nAlthough they do not yet understand the mechanisms behind these results, Kumar and her team suspect that smoking is confusing the normal communication that goes on between healthy bacterial communities and their human hosts.\nPractically speaking, these findings have clear implications for patient care, according to Kumar.\n\"It has to drive how we treat the smoking population,\" she said. \"They need a more aggressive form of treatment, because even after a professional cleaning, they're still at a very high risk for getting these pathogens back in their mouths right away.\n\"Dentists don't often talk to their patients about smoking cessation,\" she continued. \"These results show that dentists should take a really active role in helping patients to get the support they need to quit.\"\nFor Kumar, who is a practicing periodontist as well as a teaching professor, doing research has changed how she treats her patients. \"I tell them about our studies, about the bacteria and the host response, and I say, 'Hey -- I'm really scared for you.' Patients have been more willing to listen, and two actually quit.\"\nKumar's collaborators include Chad Matthews and Vinayak Joshi of Ohio State's College of Dentistry as well as Marko de Jager and Marcelo Aspiras of Philips Oral Healthcare. The research was sponsored by a grant from Philips Oral Healthcare.\nCite This Page:","Selected aspects of the human gut microbiota\nThe gut microbiota represents a highly complex assembly of microbes, which interact with each other and with their host. These interactions have various implications in terms of health and disease, and this multi-author review issue will address a number of selected aspects pertaining to gut microbiota research.\nKeywordsMicrobiota Microbiome Probiotic Gut commensals Diet & health\nHuman beings are colonized by complex microbial communities that influence and govern various biological processes during their entire lifespan. Within the human body, microbes reach their highest density in the intestinal tract, where they form a complex microbial community known as the gut microbiota . This gut microbiota develops during infancy of the host to reach its mature form following weaning [2, 3, 4]. The human gut microbiota in the early stages of life plays a pivotal role in the maturation and modulation of the host immune system, as well as in promoting various physiological processes in the human gut, such as the establishment of mucosal integrity and the mucus gel layer [5, 6].\nFurthermore, later in life the abundance and prevalence of particular members of the human gut microbiota continue to play a variety of roles in the maintenance of human health, for example, by (i) assisting in the breakdown of food components and liberation of nutrients that would otherwise be inaccessible to the host, (ii) promoting the differentiation of particular host tissues, (iii) reducing the risk of gut colonization by pathogens, and (iv), as already mentioned above, maturation and modulation of the immune system. As underlined by several studies, the development of the complex microbial assemblage within a specific ecological niche reaches a so-called climax status represented by the establishment of a balanced equilibrium of its microbial components . Numerous factors are known to cause shifts in the composition of the microbiota, thereby disrupting this microbial homeostasis and causing a state of dysbiosis. Dysbiosis is typically associated with having a negative impact on host health with long-term consequences, being associated with various disorders or diseases, such as obesity, diabetes, inflammatory bowel disease (IBD), and metabolic syndrome.\nHowever, despite the wealth of publications that support the key roles played by gut microbiota in host health maintenance and promotion, there are still substantial knowledge gaps related to the associated mechanisms of action. Another intriguing finding of the metagenomic era is represented by the identification of numerous, but as yet uncharacterized microbial taxa that have overwhelmed bacterial taxonomy. Such newly discovered taxa represent a challenging frontier for microbiology research that focuses on the development of methodologies for the isolation and cultivation of these novel microbes.\nThis multi-author review will cover a variety of topics, including the development of the gut microbiota following birth and the impact of antibiotics either prenatally or postnatally on these early microbial communities (see Nogacka et al.). In the context of early life microbiota, one particular group of gut commensals, the bifidobacteria, represents a dominant and prevalent microbial group. The contribution by Turroni et al. provides an overview of the ecological and biological role of bifidobacteria in the infant gut.\nThe multi-author review is counter-balanced by three articles focusing on gut microbiota composition in aging organisms, from flies to humans. The contribution by Clark and Walker presents a detailed overview of the role of gut microbiota in age-associated health decline as determined by the use of invertebrate models. In addition, in this context, the contribution by O’Toole and Jefferey provides insights into the composition of the gut microbiota of elderly, with particular emphasis on the role of microbial effector metabolites in host-signaling and impact on host health status. The contribution by Santoro et al. documents our current knowledge on gut microbiota composition at the extreme end of human life, namely in centenarians, providing relevant findings on the role of gut microbiota in neurodegenerative diseases, such as Parkinson’s and Alzheimer’s diseases.\nFinally, comparative analysis of gut commensals found in two different mammalian species, humans and mice, revealed that they had a relatively small number of microorganisms in common (Hugenholtz and de Vos), and reminds us that extreme caution should be taken when findings from murine analyses are being extrapolated to humans.\n(1) Early microbiota, antibiotics and health. Alicja M. Nogacka, Nuria Salazar, Silvia Arboleya, Marta Suárez, Nuria Fernández, Gonzalo Solís, Clara G. de los Reyes-Gavilán, and Miguel Gueimonde. (2) Role of gut microbiota in aging-related health decline; insights from invertebrate models. Rebecca I. Clark and David W. Walker. (3) Bifidobacteria and the infant gut: an example of co-evolution and natural selection. Francesca Turroni, Christian Milani, Sabrina Duranti, Chiara Ferrario, Gabriele Andrea Lugli, Leonardo Mancabelli, Douwe van Sinderen, and Marco Ventura. (4) Microbiome–health interactions in older people. Paul O’Toole and Ian B. Jeffery. (5) Gut microbiota changes in the extreme decades of human life: a focus on centenarians. Aurelia Santoro, Rita Ostan, Marco Candela, Elena Biagi, Patrizia Brigidi, Miriam Capri, and Claudio Franceschi. (6) Mouse models for human intestinal microbiota research: a critical evaluation. Floor Hugenholtz and Willem M de Vos.\n- 2.Bokulich NA, Chung J, Battaglia T, Henderson N, Jay M, Li HL, Lieber AD, Wu F, Perez-Perez GI, Chen Y, Schweizer W, Zheng XH, Contreras M, Dominguez-Bello MG, Blaser MJ (2016) Antibiotics, birth mode, and diet shape microbiome maturation during early life. Sci Transl Med 8:343ra82CrossRefPubMedPubMedCentralGoogle Scholar\n- 3.Yassour M, Vatanen T, Siljander H, Hamalainen AM, Harkonen T, Ryhanen SJ, Franzosa EA, Vlamakis H, Huttenhower C, Gevers D, Lander ES, Knip M, Group DS, Xavier RJ (2016) Natural history of the infant gut microbiome and impact of antibiotic treatment on bacterial strain diversity and stability. Sci Transl Med 8:343ra81CrossRefPubMedPubMedCentralGoogle Scholar\n- 4.Yatsunenko T, Rey FE, Manary MJ, Trehan I, Dominguez-Bello MG, Contreras M, Magris M, Hidalgo G, Baldassano RN, Anokhin AP, Heath AC, Warner B, Reeder J, Kuczynski J, Caporaso JG, Lozupone CA, Lauber C, Clemente JC, Knights D, Knight R, Gordon JI (2012) Human gut microbiome viewed across age and geography. Nature. 486:222–227PubMedPubMedCentralGoogle Scholar"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:46f906d6-9a2a-4628-9f74-c24031956c9f>","<urn:uuid:b7865a5a-0171-408a-85a1-a35217f39739>"],"error":null}
{"question":"As a legal professional, I'm curious about the specific sentence length given in the recent laser-pointing case in San Antonio. What was the prison term imposed?","answer":"The federal judge sentenced Justin John Shorey to 51 months in federal prison, followed by three years of supervised release after completing his prison term.","context":["Schertz Man Sentenced to Federal Prison for Pointing Laser at San Antonio Police Helicopter\nIn San Antonio today, a federal judge sentenced 39-year-old Justin John Shorey of Schertz, TX, to 51 months in federal prison after he pleaded guilty to aiming a laser pointer at a San Antonio Police Department helicopter, announced U.S. Attorney Gregg N. Sofer, FBI Special Agent in Charge Christopher Combs, San Antonio Division, and San Antonio Police Chief William McManus.\nIn addition to the prison term, U.S. District Judge David A. Ezra ordered that Shorey be placed on supervised release for a period of three years after completing his prison term.\n“Pointing lasers at law enforcement is extremely dangerous and can cause serious injury. This is particularly true when the pilots of an aircraft are involved,” stated U.S. Attorney Sofer. “Today’s 51-month prison sentence demonstrates the seriousness of this offense. We will aggressively prosecute anyone who purposely points a laser at an aircraft, endangering both people in the air and those in our communities on the ground.”\nOn November 20, 2019, Shorey pleaded guilty to aiming a laser pointer at an aircraft. According to the factual basis filed in this case, on February 17, 2019, Shorey knowingly aimed the beam of a laser pointer at an aircraft in flight. The San Antonio Police Department helicopter was flying just north of Highway 90 West, assisting in the search of a shooting suspect. When the laser beam made contact with the helicopter, it hit the pilot in the eyes affecting his ability to see and read his gauges.\nAt the time, the helicopter was flying in the path of the San Antonio International Airport, and Shorey’s actions endangered both civilian flights and the public on the ground. The pilot and his tactical officer onboard began a search for the laser suspect. Shorey admitted to aiming the laser at the aircraft once as it approached his location in the 2100 block of Hays Street in San Antonio and twice as it circled above him.\nThe pilot managed to land safely at the San Antonio International Airport. The injury to the pilot’s eyes caused by the defendant’s actions resulted in the pilot being unable to fly for a week.\n“When aimed at an aircraft, the powerful beam of light from a hand-held laser can travel more than a mile and illuminate a cockpit, disorienting and temporarily blinding pilots. Lasing an aircraft represents a significant public safety threat, which endangers pilots, aircrew, passengers, and individuals on the ground, should an aircraft crash or require an emergency landing,” stated FBI Special Agent in Charge Combs. “This case should serve as a warning to others who engage in this dangerous criminal activity.”\n“Actions such as lasering law enforcement helicopters are dangerous for the pilots assisting officers on the ground. I was glad to hear that the Department of Justice does not tolerate this behavior and held Mr. Shorey accountable for his actions,” stated San Antonio Police Chief McManus.\nIf you have information about a lasing incident, contact the San Antonio FBI at 210-225-6741. If you see someone pointing a laser at an aircraft, call the nearest local law enforcement agency immediately by dialing 911. Tips can also be submitted online at https://tips.fbi.gov.\nThe FBI and San Antonio Police Department conducted this investigation. Assistant U.S. Attorneys Mark Roomberg and William R. Harris prosecuted this case on behalf of the government.\nThe year 2020 marks the 150th anniversary of the Department of Justice. Learn more about the history of our agency at www.Justice.gov/Celebrating150Years."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:c91baea9-2880-46b3-b4a9-34e1e597e1a0>"],"error":null}
{"question":"How do the design requirements differ between sports wheelchairs and everyday wheelchairs when considering stability and maneuverability?","answer":"Sports wheelchairs and everyday wheelchairs have distinct design requirements that reflect their different uses. Sports wheelchairs are typically non-folding to increase rigidity and feature a pronounced negative camber (angled wheels) for improved stability and sharp turns. They are usually made of lightweight composite materials and may have radically different seating positions, such as kneeling positions for racing. In contrast, everyday wheelchairs prioritize general mobility and must balance different factors affecting propulsion effort, including inertia and friction. They need to handle multiple daily activities involving starts, stops, and turns, with research showing that 85% of mobility bouts last less than 60 seconds. The everyday chair must be efficient for these frequent direction changes, which require more effort than maintaining straight-line motion.","context":["Ultralight Research, Part I\nManual Chair Propulsion: New Insight Into Maneuvering Efficiency\n- By Stephen Sprigle\n- Nov 01, 2014\nManual chairs are mechanical systems just like bicycles. Wheelchairs and bikes have several things in common. Both are propelled by the occupant or user, who weighs more than the mechanical system. This means that the occupant has a great influence on the forces imparted onto the vehicle and on the forces required to propel the vehicle.\nConversely, many significant differences also exist between bicycles and wheelchairs.\nA wheelchair is much less efficient than a bicycle. This means that the occupant must expend more effort to perform the same task. This requirement of greater propulsion effort can have consequences for wheelchair users.\nGreater effort can lead to difficulty in achieving desired speeds, a higher probability of fatigue over long bouts of mobility, and difficulty negotiating inclines. Over time, the accumulation of this greater effort can increase the potential for injury in the upper extremities.\nOne of the reasons for this difference in effort lies in the means of propulsion. Bikes use cranks and often have gears to increase the mechanical advantage. Another reason is due to turning. A bike is turned using handlebars that the operator can easily turn while pedaling. In distinction, a wheelchair user turns by propelling forward on one wheel while slowing down the other wheel. This is obviously inefficient because the user is expending energy and slowing down simply to change direction. However, this type of differential steering makes the wheelchair much more maneuverable than a bike. You could never maneuver a bike with the same precision as a wheelchair.\nInertia, Friction & Wheelchairs\nPropulsion effort, whether on a bike or a wheelchair, is dictated by two principals of physics: inertia and friction.\nInertia is inherent to all things with mass and reflects the resistance to any change in motion, including changes in speed or direction. Your body weight is representative of your translational inertia. For a wheelchair, this translational inertia is due to the occupant’s mass and the mass of the entire wheelchair system comprised of the base (frame, two drive wheels, two casters and two caster forks), seating system (cushions), and accessories (headrest, backpack, supplies).\nManeuvering a wheelchair also requires overcoming the inertia due to turning and the rotational inertia of the drive wheels and casters. Therefore, one can think of a wheelchair as a system with translational, rotational and turning inertia.\nYou’ve probably heard of Newton’s First Law of Motion: A body at rest stays at rest, while a body in motion stays in motion with the same velocity — until acted on by an external force. Well, for wheelchairs (and bikes) the external forces that influence (or impact) the ability to change velocity are the propelling forces and friction. Friction is the force that slows down a wheelchair. It is ever present and is dependent on a number of factors, most significantly the weight on the wheels, the type of wheels and tires, and the surface upon which one is rolling.\nGiven this background, one can deduce that many choices that are made about wheelchair components and configuration affect both friction and inertia.\nFrictional forces are influenced by the mass of the occupied wheelchair, the mass distribution (the amount of weight on the drive wheels and casters), and the types of wheels and tires that impact rolling resistance and tire scrub. Rolling resistance is the frictional force that slows down a rolling wheel. Without this friction, the wheels spin on the surface, so we need friction to move about.\nTire scrub is the frictional force that occurs at the tire-surface interface during turning. Front casters will scrub as they swivel into alignment with the path of travel. In distinction, the drive wheels will scrub throughout the turn so, in fact, may have a greater frictional influence on the effort of propulsion.\nInertia of the wheelchair system is also influenced by the weight of the occupied wheelchair, the weight distribution and the rotational inertia of the drive wheels and casters. When initiating wheelchair movement, one must exert greater force (i.e., perform more work) compared to that required to keep the wheelchair moving.\nYou have experienced this every time you ride a bike. Think about how hard you have to pedal while starting compared to how hard you pedal to maintain a certain speed.\nThis explanation should leave you with the first two important points of this article:\n1) Decisions made about a wheelchair and its configuration will impact either inertia or friction, and most often, will impact both. So, by extension, decisions made about a wheelchair or its configuration impact propulsion effort.\n2) Maneuvering a wheelchair throughout the day involves a series of starts, stops and turns, and these changes in velocity require more effort than that needed to keep moving in a straight line.\nWe measured how full-time wheelchair users move about during everyday life. The data from 68 wheelchair users consists of about 60,000 bouts of activity and shows that people move in short bouts with 85 percent of bouts lasting less than 60 seconds. Just as importantly, wheelchair users perform, on average, 90 bouts of mobility every day, with each bout involving accelerating the wheelchair. Some users perform over 200 bouts of mobility a day. This fact served as our motivation for developing a system that was capable of measuring forces during changes in momentum — a change in momentum is a change in the speed or direction of travel.\nTwo Manual Wheelchairs, One Robot\nMeasuring propulsion effort of wheelchairs during maneuvers is not a trivial task. One must measure the torque applied to the wheels during freewheeling maneuvers, something that treadmills and rollers will not allow. One must also measure a wheelchair without changing its inertia or frictional parameters, so adding mass to the chair or inertia to the wheels will not suffice.\nTherefore, a wheelchair-propelling robot was designed at the Rehabilitation Engineering Research Center on Wheeled Mobility (mobilityRERC), a project funded by the National Institute of Disability and Rehabilitation Research.\nThe robot was designed with anthropomorphic characteristics in order to replicate the loads imparted by the occupant onto the wheelchair. It was designed to mimic the 50-percent male in height and the 95-percent male in mass or about 100 kg/200 lbs. The robot propels the wheelchair by applying forces to a special geared handrim using motors. We are able to program certain maneuvers and have very precise control over the acceleration, speed and direction. Therefore, the robot offers the ability to measure propulsion torque during wheelchair maneuvers in a very repeatable and accurate manner.\nTo illustrate the influences of wheelchair design and configuration on the effort to maneuver a wheelchair, the robot was programmed to perform both straight and turning maneuvers. These maneuvers were designed to highlight the varying inertial and frictional influences that govern propulsion effort. The straight maneuver consisted of accelerating the chair to 0.7 meters/second (m/s) in 2.5 seconds and then continuing at that speed for another 2.5 seconds. The turning maneuver was a fixed-wheel turn in which one wheel is driven forward while the other wheel is fixed in place. This maneuver accelerated the wheelchair to 0.5 m/s in 2.5 seconds and then maintained the trajectory for another 2.5 seconds. Therefore, both maneuvers had an acceleration phase and a steady-state phase. Furthermore, these maneuvers were performed on tile and low-pile carpet surfaces.\nWe selected two wheelchairs that represent disparate designs and configurations.\nThe standard folding-frame wheelchair with 7\" front casters, 24\" drive wheels and 1\" solid tires had a mass of 17.6 kg (i.e., about 39 lbs.), and when loaded with the robot, had about 55 percent of the system mass loaded onto the drive wheels.\nThe second chair was an ultralightweight rigid-frame wheelchair with 5\" casters and 24\" wheels with 1 3/8\"-wide pneumatic tires. It had a mass of 12.1 kg/26-plus lbs. with 70 percent of the system mass on the drive wheels. The specific configurations, including the type of tires and mass distribution, are very important to consider when comparing systems. Our methods include complete measurement of the inertias of all components as well as the frictional parameters as measured using a simple coast-down test. These parameters will be discussed as we review the results of the study.\nThe results are shown in Figures 1 and 2. The values in the figures are the torques required to either accelerate the chair or to maintain the respective speed during the maneuver. Each graph depicts the torques for both phases on tile and carpet. One graph illustrates results from the straight maneuver, while to the other is for turning. Torque is reported in N-m and reflects measured work.\nEditor’s Note: See the January issue for Part II of this series.\nThis article originally appeared in the November 2014 issue of Mobility Management.\nStephen Sprigle, Ph.D., PT, is a professor of Applied Physiology, Bioengineering & Industrial Design at the Georgia Institute of Technology in Atlanta.","Types of Wheelchair\nOriginal Editor - Naomi O'Reilly as part of the Wheelchair Service Provision Content Development Project\nThere are a wide variety of types of wheelchair, differing by propulsion method, mechanisms of control, and technology used. Some wheelchairs are designed for general everyday use, others for single activities, or to address specific access needs. No single model or size of wheelchair can meet the needs of all users, and the diversity among users creates a need for different types of wheelchair. Those selecting wheelchairs, in consultation with the user, need to understand the physical needs of the intended user and how they intend to use the wheelchair, as well as knowledge of the reasons for different wheelchair designs.\nThe composition of the frame is a key factor in the functionality of the wheelchair. The steel being the most usual, is the heaviest but also the cheapest. A wheelchair with aluminium frame is much lighter and then much easier to propel, but also more expensive. You can also find frames made of very light materials like specific aluminium, titanium and carbon. They are commonly used in rigid frame wheelchairs and have a very high price.\nThe frame of a wheelchair can also be either rigid (fixed), or foldable. The energy efficiency for the user to propel is double in a wheelchair with a rigid frame, taking an average of 15-20% of the impulse, while a folding wheelchair only uses on average of 5 - 8% of the impulse.\nBoth rigid frame and folding wheelchairs have advantages and disadvantages. The best choice will depend on the wheelchair users lifestyle, how they transfer in and out of transport and their personal preference.\nThe folding frame types of wheelchairs consist of a folding X Style Frame. Most frames fold when the locking mechanism is released for folding and tend to include removable foot rests which allow for easy folding. Most frames are made from aluminum or titanium and are heavier than the rigid frame. As with anything with moveable parts, the folding wheelchair is not as durable as the rigid frame wheelchair. Therefore maintenance is required more frequently to keep all parts in good working order. Similarily in a folding wheelchair, part of the propulsion energy is lost in movements within its structure and all points of articulation.\nRigid FrameA rigid frame wheelchair consists of a single welded frame on which the individual sits. This can incorporate either a fixed back rest or a folding back rest where the back of the chair is able to fold down. It also incorporates wheels can be removed with a quick release mechanism to enable easy storage and transportation of the wheelchair. In most cases these frames are lightweight and are made from either aluminum or titanium, and in some cases from carbon fiber, A lightweight rigid frame wheelchair can weigh as little as 10 lbs without the wheels. The advantage of these types of wheelchairs are that they have fewer moving parts, which means they are generally stronger and last longer than the folding wheelchair. Other advantages for the rigid frame is that it is easy to handle and is somewhat lighter than one similar folding wheelchair. However, the folding wheelchair is generally more comfortable for transport and storage by occupying less space.\nJust as other sports require participants to use certain equipment and skills, wheelchair sports require a certain set of equipment and skills. There are a wide range of sports developed for wheelchair users including Archery, basketball, boccia, dancing, racing, rugby, and tennis. Whether a beginner looking to start playing a wheelchair sport, those with advanced skills, or professional athletes, there are a huge range of sports wheelchairs available depending on the sport and the level at which the user plays. The wheelchairs used for each sport have evolved to suit the specific needs of that sport and often no longer resemble their everyday cousins. They are usually non-folding (in order to increase rigidity), with a pronounced negative camber for the wheels (which provides stability and is helpful for making sharp turns), and often are made of composite, lightweight materials. Even seating position may be radically different, with racing wheelchairs generally used in a kneeling position. Sport wheelchairs are rarely suited for everyday use, and are often a 'second' chair specifically for sport use, although some users prefer the sport options for everyday. Some disabled people, for instance lower-limb amputees, may use a wheelchair for sports, but not for everyday activities.\nMotivation’s Multisports Wheelchair, has been designed for a wide range of users, from school children to adults. The affordable Multisport Wheelchair, is a fraction of the price of ‘Professional’ or 'Specialist Sport Specific' Wheelchairs and yet can compete almost at the same level and is designed to make sport accessible to people with a disability. There are two models, which suit basketball, tennis and no doubt other sports and activities.\nThe main design features of the chair are;\n- adjustable backrest, ensuring that it fits a range of sizes;\n- quick release wheels;\n- inclined / angled wheels, add to the stability of the wheelchair, especially during sporting activities;\n- adjustable back wheel, also called the caster, adding stability;\n- lightweight frame manufactured from common tubular steel;\n- highly manoeuvrable, allowing it to used successfully in sporting activities.\n- robust and strong, able to withstand the type of rough treatment, that occurs in wheelchair sports.\n- removable toe guard at the front, keeps the users feet out of harms way.\nSport Specific Wheelchair\nWheelchair basketball, played by two teams of five players, is a fast-paced game where the object is to shoot the ball into the opposing team’s basket. Each team is comprised of five players and up to seven substitutes. A game consists of four periods of ten minutes. The basic rules of wheelchair basketball are very similar to running basketball with minor adjustments to meet the needs of the game in a wheelchair. The primary piece of equipment in the game of wheelchair basketball is the wheelchair, which has evolved over time alongside the development of the sport and technology. When wheelchair basketball was first played, players just used their everyday wheelchair which were often very heavy, while today wheelchair basketball chairs are specifically developed for the sport, are constructued with aluminium or titanium making them lightweight and streamlined to allow for speed and agility with many cutting-edge, sport-enhancing design features. Basketball Wheelchairs are designed for enhanced stability and mobility and now include an additional fifth and sixth wheel anti-tip devices. The front bumper is designed so that it will not lock with or be held by the opponent. The angle of the wheel base or “camber” is optimal for each athlete to allow for stability and quick turns. Guards have lower seats and therefore greater stability for ball handling and getting down the court as quickly as possible in comparison to Centers and Forwards whose chairs have higher seats and therefore less mobility, but the height increases the player's reach for shots at the hoop and for rebounds and meets their role which is typically underneath the net.\nWheelchair racing includes both track and road races. At Paralympic level there are wheelchair racing track events in: 100m, 200m, 400m, 800m, 1500m, 5000m, 10000m and relay races 4x100m and 4x400m. There is also the marathon out on the road. Track Tech Explained - Racing Wheelchairs\nWatch this video about the technology used by wheelchair paralympians\nWheelchair Rugby is a mixed team sport for male and female quadriplegic athletes. A unique sport created by athletes with a disability, it combines elements of rugby, basketball and handball. Players compete in teams of four to carry the ball across the opposing team's goal line. Contact between wheelchairs is permitted, and is in fact an integral part of the sport as players use their chairs to block and hold opponents. Wheelchair Rugby players compete in manual wheelchairs specifically designed for the sport. The rules include detailed specifications for wheelchairs to ensure safety and fairness. In international competition, all wheelchairs must meet these requirements.\nWheelchair tennis, founded in 1976 when Brad Parks, follows the same rules as running tennis, as endorsed by the International Tennis Federation, with the only exception being that the wheelchair tennis player is allowed two bounces of the ball. The 'two-bounce rule' is written into the Rules of Tennis. A Tennis wheelchair is lighter than everyday chairs to allow the athlete flexibility of movement, making the game more spectacular. The wheelchair is considered part of the player - therefore, general rules of contact apply. To keep the player stable on the chair, a positioning strap across the waist and/or thighs is used. Wheelchair tennis has improved dramatically in recent years due in large part to technological advancements.\n- Wheels should be light and rigid.\n- Radial spoking wheels are best for wheelchair tennis.\n- Some of the top players are playing in wheelchairs with two small wheels at the front and an anti tip bar at the rear.\n- Camber, or the angling of the chairs wheels, has also been introduced to the tennis wheelchair. Camber aids the wheelchair tennis player by enhancing side to side balance and stability, increasing security, and enhancing turn speed.\n- Newer chairs frames are manufactured with lightweight and more durable materials such as aluminium, and titanium.\n- Today’s tennis wheelchair can weigh anywhere between 8-15 kilograms.\n- Wheelchair tennis chairs can be either custom built or fully adjustable. Adjustable chairs are recommended, especially for beginner players.\n- Tyres are crucial for wheelchair tennis. Weight in the tyres, especially the push rim, directly relates to the power required to set that wheel in motion.\n- Lighter materials are currently being tested in the push rim as well as the spokes.\n- NHS Choices. Looking Beyond the Standard Wheelchair. Available from: https://youtu.be/GHQo1xBphMY [last accessed 6/6/2018]\n- Gold Pictures. The Manual Wheelchair Comparision: Chair Shopping. Available from: https://www.youtube.com/watch?v=Zk30X1oujao&t=331s [last accessed 6/6/2018]\n- Gold Pictures. The Manual Wheelchair Comparison: Pediatric Chairs. Available from: https://youtu.be/qH3TxciH-og?list=PLv0O9I4DD6_X0WBOnMg-pl4GomOZfE8Pt [last accessed 6/6/2018]\n- Ottobock. Sport and Everyday Life in a Wheelchair. Available from: https://www.youtube.com/watch?v=Qpfa0rbscdQ&t=32s [last accessed 6/6/2018]\n- Motivation Charity. Assembling your Motivation Multisport Wheelchair. Available from: https://youtu.be/Ip3RYO1WW_I [last accessed 6/6/2018]\n- Paralympics Games. Paralympic Sports A-Z: Wheelchair basketball. Available from: https://www.youtube.com/watch?v=0RWvXzYKoHM&t=14s [last accessed 6/6/2018]\n- MA. Wheelchair Basketball Equipment at Invictus Orlando 2016. Available from: https://www.youtube.com/watch?v=TnLhUJ1uRj0 [last accessed 6/6/2018]\n- Paralympics Games. Paralympic Sports A-Z: Wheelchair Rugby. Available from: https://www.youtube.com/watch?v=tSzFmlWgVsM&t=9s [last accessed 6/6/2018]\n- MA. Wheelchair Rugby Equipment at Invictus Orlando 2016. Available from: https://www.youtube.com/watch?v=dAMxv08Ki4U [last accessed 6/6/2018]\n- Paralympics Games. Paralympic Sports A-Z: Wheelchair Tennis. Available from: https://www.youtube.com/watch?v=uKTdxygcat4 [last accessed 6/6/2018]\n- United States Tennis Association (USTA). Introduction to USTA Wheelchair Tennis: Equipment. Available from: https://www.youtube.com/watch?v=OktRBQzUccA&t=9s [last accessed 6/6/2018]"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:caa68a9a-e152-44e4-87ce-9f6ce626a84f>","<urn:uuid:0e904388-5379-42b5-9bf6-7499a89e8dcd>"],"error":null}
{"question":"What makes furniture harder to move after staying in the same place for years?","answer":"When furniture stays in the same place for a long time, it becomes harder to move due to frictional aging. This happens through several processes: surfaces can get squashed closer together through material creep, water vapor can condense in tiny crevices between surfaces, and chemical bonds can gradually form between the contacting surfaces.","context":["Have you ever had the impression that heavy items of furniture start to take root – that after years standing in the same place, they’re harder to slide to a new position? Do your best wine glasses, after standing many months unused in the cabinet, seem slightly stuck to the shelf? Has the fine sand in the kids’ play tray set into a lump?\nIf so, you’re not just imagining it. The friction between two surfaces in contact with each other does slowly increase over time. But why? A paper by two materials scientists at the University of Wisconsin in Madison, USA, suggests that the surfaces could actually be slowly chemically bonding together.\nThere are already several other explanations for this so-called “frictional ageing” effect. One is simply that two surfaces get squashed closer together. But a curious thing about friction is that the frictional force opposing sliding doesn’t depend on the area of the contacting surfaces. You’d expect the opposite to be the case: more contact should create more friction. But in fact two surfaces in apparent contact are mostly not touching at all, because little bumps and irregularities, called asperities, prop them apart. That’s true even for apparently smooth surfaces like glass, which are still rough at the microscopic scale. It’s only the contacts between these asperities that cause friction.\nIf two surfaces are pushed more firmly together, the asperities can get deformed and flattened so that more of the surfaces touch one another, and the friction then gets stronger. This sort of squashing and increased contact can also happen over time without any extra pressure, because the materials slowly sag, a process called creep that occurs for most materials.\nAnother cause of frictional ageing is that water vapour can condense into liquid in all the tiny crevices at the interface. Even if air is not humid enough for water droplets to form on a flat surface, they can condense in very small gaps, a phenomenon called capillary condensation. The surface tension of the water then holds the surfaces in place and increases friction. This effect may make powders clump together and resist flowing, because there are lots of small gaps between grains where water can condense.\nYet these effects can’t explain all kinds of frictional ageing. For example, last year a team of US researchers showed that it can happen between two surfaces of silicon dioxide (silica, which is basically sand or window glass), even when the pressure pushing them together is too low to induce creep. They figured that the surfaces must be gradually adhering because of the formation of chemical bonds between them.\nIt is this process that Yun Liu and Izabela Szlufarska at Wisconsin have set out to understand. For one thing, they wondered how such bonds might form, given that physical contact between materials doesn’t necessarily lead to chemical bonding. And they wanted to know why the friction increases over time according to a very specific mathematical relationship, namely that the frictional force is proportional to the logarithm of the time elapsed (which means that the increase is very gradual).\nLiu and Szlufarska focused on silica too, and for good reason. One of the most important contexts for frictional ageing is in earthquake zones, where rock surfaces deep in the earth are pressed against one another along a geological fault. The likelihood of a fault slipping, creating a quake, depends on the friction there, and so it’s important to understand how this friction changes over time. Most rocks are silicate minerals, which are rather similar to silica.\nThe slow adhesion of minerals is also important for the stability of historical stone buildings, which are held together as much by friction as by mortar. And yet another reason to be interested specifically in the stickiness of silica is that bonding between very thin films of this material is important in the manufacture of silicon chips for microelectronics.\nUsing computer simulations of silica’s chemical behaviour, the Wisconsin pair studied how two surfaces held very close together change over time. In other words, they calculated how atoms at the surface may react with one another. If there is a little water vapour around, the silicon atoms at the surfaces react with water molecules to become ‘capped’ with hydroxyl chemical groups, composed of an oxygen and hydrogen atom. But if two of these hydroxyl-capped silicon atoms come close together, they can react further, spitting out a water molecule and leaving a lone oxygen atom bridging the two silicons. In other words, these oxygen bridges can bind the two surfaces together.\nThe more bonds that form, the greater the frictional force, and Liu and Szlufarska showed that the chance of a new bond forming at a particular silicon atom depends on whether there are already bonds bridging neighbouring atoms. It is this inter-dependence of bond formation that leads to the number of bridging bonds increasing in proportion to the logarithm of the time passed – just as is seen experimentally. So this bonding alone is enough to explain what is seen experimentally for frictional ageing – even if other processes, such as creep and capillary condensation, might also occur."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:59843dce-ecb5-420a-824f-fec290f39c41>"],"error":null}
{"question":"How do film archives serve cultural preservation and monetization purposes?","answer":"Film archives serve cultural preservation by maintaining audiovisual records that capture historical and political value, such as student films dating back to 1966 that explore themes like love, relationships, and personal passions. For monetization, digital archives enable content to be repurposed across emerging distribution channels, supporting home viewing, internet downloads, streaming, and multi-casting. The digitization of archives allows for instant retrieval and seamless integration into production workflows, transforming unusable content into accessible and monetizable files.","context":["VCA Digital Archive: Love and joy\nThe VCA Digital Archive is a living audiovisual record of student films that date back to 1966. The articles in this series respond thematically to the depth and breadth of the collection, which will be available for research from mid-2019. Enjoy!\nBy Isabelle Rudolph\nIn my search for love in the VCA Film Archive, I found I was spoiled for choice. Love in the broadest sense is hard to define. Does it include infatuation? Lust? The passion one feels for a particular activity?\nI decided to answer “yes” to all of these questions. The films I have chosen to review are laced with pain, hope, envy, loneliness and joy – themes that converge with each other and, although highly varied, all concern themselves with the subject of love.\nThe films vary in tone. Some are loaded with gravitas while others take a whimsical and light-hearted approach, but all possess nuance and subtlety, qualities well-suited to exploring a subject as complex and varied as love.\nLove Me Stupid… A Story of Blood. Aleksi Vellis. 1984. Experimental.\nLove Me Stupid… A Story of Blood (Aleksi Vellis. 1984) follows two protagonists, Robert and Trish, whose experiences of intimacy form the basis of the narrative. In the context of an emotionally abusive and constraining family life, a longing for love and care leads Trish to seek comfort in romantic connections.\nRobert is also plagued by the absence of love in his childhood, as a result of abandonment and neglect. As the story unfolds, the consequences of these early traumas create the drama in the narrative. While familial and romantic love are definite themes, what I found really moving and interesting about this film is the way love within friendship is portrayed.\nCharacters peripheral to the main story arc provide some of the most poignant exchanges by caring for and supporting the two main characters as they navigate their life circumstances and consequential loss. It’s a simple story told with subtlety and care.\nFlesh on Glass. Ann Turner. 1981. Drama.\nThe surreal Flesh on Glass (Ann Turner. 1981) is evocative and memorable in ways that are hard to define. What could have been a simple, fairly mundane film about a love triangle between three characters – a sister, brother and his wife – is made compelling by the underpinning strangeness. Spooky séances and dream sequences, the ghost of a Catholic nun, the presence of the ocean and a collection of glass ornaments, flesh out the narrative and give the film a disorder that is deeply engaging.\nLove and Reality. Julianne Negri. 2000. Comedy.\nHiro and Miko. Adam Pietrzak. 1997. Comedy.\nLove and Other Red Spot Specials. Lauren Anderson. 2008. Comedy.\nDuring my travels through the Archive, I really enjoyed films that twisted the romantic comedy genre into something quirky and surprising. Among the highlights were: Love and Reality (Julianne Negri. 2000), a story of real estate, misfits and collections; Hiro and Miko (Adam Pietrzak. 1997), a crime comedy; and Love and Other Red Spot Specials (Lauren Anderson. 2008), about a transvestite living in a small town.\nExoskeleton. Michael Dean. 2006. Drama.\nFor a serious take on the search for love or the sparked beginnings of romance, the film Exoskeleton (Michael Dean. 2006) stood out. Across the film’s 12 minutes, Exoskeleton manages to invoke the challenges of self-acceptance, the tension between longing for and fear of intimacy, and the raw vulnerabilities that come with love and the human condition.\nFilled with Water. Elka Kerkhofs. 2006. Animation.\nSurreal and dream-like, Filled with Water (Elka Kerkhofs. 2006) and Pigeon Island (Clare Davies. 2002) explore the theme of love via drawings and music. The absence of words and the use of evocative imagery gives focus to the mood and raw emotion of both films.\nPigeon Island. Clare Davies. 2002. Animation.\nFinally, there are several documentaries that look into how a deep passion has driven and shaped a subject’s life experiences. I empathise and find this genre of love beautiful and fascinating, where a person is doing something they really love in life that brings them joy and fulfilment.\nIn Cold Blooded Love (Jacqui Davis, 2002), a man has made a living from caring for venomous snakes and other reptiles. He discusses his unshakable devotion and love for snakes in spite of being hospitalised on multiple occasions from snakebites. In Against the Odds (Sare Aminian. 2004), an Iranian man tells the story of his love of music and how his refusal to give it up resulted in his incarceration and, ultimately, drove him into exile.\nCold Blooded Love. Jacqui Davis. 2002. Documentary.\nAgainst the Odds. Sareh Aminian. 2004. Documentary.\nI wonder why it is that I’m drawn to seek and connect with others endeavouring to understand love and its place in our lives, and what draws storytellers time and again to the subject of love.\nOne gets a sense in watching these films that love is inseparable from joy. That, although often illusive, love must be the key to feeling a sense of belonging, of completeness and contentment. This isn’t only romantic love, but love in all its complex diversity. I learn best through relating to others, through narrative and empathy, and I suspect many of us who love film do, too.\nMy understanding of love, its power, diversity and importance, has been enhanced by engaging with these films. With films as tender, funny and insightful as these, it is hard not to fall in love with the VCA’s Film and Television archive.\nIsabelle Rudolph is a Melbourne-based artist. She completed a Bachelor of Fine Art (Honours) at the VCA, Melbourne University, in 2018. Her practice engages sculpture to explore issues of social connection and desire.\nThe FTV Digital Archive series of articles were commissioned as part of a grant from the University of Melbourne, Student Services Amenities Fee. University of Melbourne staff and students and some industry people dipped into the FTV archive and watched films based on themes. The idea was to use the archive as stimulus in which to curate and create. Some responses are completely creative, others are reviews, others are word art pieces.\nThe full collection will be available for research from mid-2019. In the meantime you can find a selection of more than 100 films live on our YouTube page. To find out more, visit the VCA Digital Archive Project Page.\nBanner image: Still from Exoskeleton. Credit: filmmaker Michael Dean.","Digitization in the age of broadcasting\nBroadcasting is indeed proliferating with its multiple screen adaptation, home videos, streaming, OTT and instant content generation. Yet are the broadcasters preserving and monetizing the extensive and instant content warehouse? “Any physical artifact is just that, a physical artifact,” says Mike Mashon, Library of Congress. “These things can shrink, they can fade, they can crumble to dust in less than a lifetime.” The issues related to broadcast archives/content are wide ranging and include technical obsolescence, media degradation, carrier deterioration, physical and chemical decay and loss to fire and floods. An active digital preservation strategy adapts content to the requisites of flexibility, scalability and sustainability.\nAccording to Unesco, “historically audiovisual resources have been recorded on fragile and unstable media, including glass plates, nitrate and acetate based film negatives and magnetic audio and videotape.” Digital Archiving enables keeping valued image assets preserved for posterity. An equally imperative aspect is renewing the archives for continued monetization across emerging distribution channels.\nTechnical obsolescence and media degradation\nOne key aspect of content loss and deterioration is technical obsolescence. Data and information is fragile in the present times as the technological evolution in ‘carriers and media’ is rapid and transformative. Once a new format is ushered in, the older formats struggle to survive. Hence, the indispensability of digital archiving and digitization. The very format that data/information is stored on, is not designed to last into infinity and can quickly degrade. This media can include magnetic tapes, floppy discs, optical discs and more. Obsolescence also affects the media that digital data is stored on. It is quite difficult now to find a computer with a 3 ½” floppy drive, much less one for 5 ¼” floppies. Digital materials, regardless of whether they are created initially in digital form or converted to digital form, are threatened by techno logy obsolescence and physical deterioration (Hedstrom and Montgomery 1998). According to the Library of Congress, “Digital preservation is the active management of digital content over time to ensure ongoing access.” Organizations, including, museums and cultural heritage, broadcasters, libraries, production studios, governments sectors are increasingly transforming their analogue and digital born into digital surrogates of the rare, unique and valuable collections. The primary objective of these projects is invariably access, which in the long run translates into monetization. Preservation considerations tend to be primarily related to the preservation of the object being digitized, not to the digital surrogate. However, a logical consequence is the question of how long access to the digital surrogate can be maintained.\nLifecycle Media Preservation approach facilitates asset digitization, storage, distribution and monetization capabilities. Digital Asset Management is about preservation and conservation. Experts define preservation as interventionist, while conservation as sustenance. Digitization and digital migration are preservation tools. Both collectively make life cycle monetization of your content viable.\nMonetization of digital files and library is a crucial aspect of the digitization value chain. From instant retrieval to asset management, digitization renders unusable content into accessible and usable files. Early television was frequently programmed live because there was limited access to recording technology and content was not always deemed to hold cultural or historical value. Even when Kinescope made recording more viable, shows were recorded far and between. Television tapes were still being largely subjected to disposal, wiping and reusing. Many of the tapes and videos were often reused to record new shows and broadcasts or dumped to free storage space. In recent times, the value of archives has transformed in the content industry with “the rise of home viewing of videos and DVDs, Internet downloads, streaming video and audio, and the trend towards multi-casting” [TAPE]. The archives aren’t just warehoused, post broadcasting, but the recycle market enables the reuse of the audiovisual assets. Content generators are tapping digital libraries, “where the archive is seamlessly integrated in the production workflow”. [TAPE] Digitization enables broadcasters to capture, pluralize and promote content in ways that facilitate the application of archival assets to support business objectives.\nSeamless navigation and content management:\nCatalogue backlogs and insufficient documentation becomes a major stumbling block in preservation of content. The report, Tracking the reel world, by TAPE, it says “around 40% of respondents report cataloguing backlogs, and on average this concerns a third of their collections. With expansive digitization and digital libraries, “accurate and extensive metadata” is crucial to preservation. Lack of metadata tagging “on content, carrier, size”, hinders long term preservation and accessibility. With preservation moving to digital and cloud platforms, the lack of metadata, make digital libraries highly vulnerable. Modern Media Asset Management enables integration of the archives and digital content into the production workflow. Many broadcasters are yet to tap it . Digitization allows you to seamlessly navigate your audio-video collection. With digital library of the content converged with organizational workflows, it facilitates both accessibility and monetization. Once content is a part of a digital library, it improves interaction and digital access. The emergence of a digital library brings its own set of opportunities and challenges. Paul Conway in his book, Preservation in the age of Google, underlines that digitization enables us to capture the authenticity and integrity of deteriorating formats and media. Creating descriptive metadata on item level for digitized objects and their integration into digital collections removes barriers to access and enables discovery of those unique visual resources in the digital environment. Digitization is the best media asset management tool.\nFlexible and customized digitization models:\nBroadcasters having chock a bloc stores with legacy tapes and footage, must take that crucial step towards digital archiving. Digitization assures both preservation and monetization of your content and hence is an investment that can neither be ignored nor delayed. Whether it is legacy or contemporary, the content held in those analogue tapes or hard drives, is not going to be available unhindered. The scratches and blurs, the dark patches and missing frames haunt audiovisual content in every corner of the world. While there is no escaping either technical obsolescence or media degradation, there is a way to restore and preserve the content. The requisites of content sensitivity, privacy, security and copyright alongside the technical steps of tagging, cleaning, digital enhancement, ingestion, storage and quality control, makes digitization an intricate and challenging process. Outsourcing digitization ensures the handling of the complex workflows by the experts. There are three Digitization Models, suited to disparate organizational capacities. With flexible and customized digitization models, such as On-Site, Off-Site and Managed services, digitization is compatible with diverse organizational requirements and budgetary outlines. While the Off-Site model is best suited for short-term and brief projects, the On-site digitization model is the perfect option, with the management of the project remains in the hands of the organization, whiles the process and the technical workflow, lies with the experts. A managed service is compatible with heavy generation and ownership of content. A service contract that outsources operations, infrastructure, equipment and technical workflows, the model is a blend of the previous two options.\nPreservation and digitization have become a crucial part of the broadcasting industry, ensuring long term usability, accessibility and monetization. And yet, audio visual preservation is not at the forefront of the broadcasting industry. Richard Wright, Research Engineer, BBC says, “there is nothing left of broadcasting unless it is recorded, collected and preserved, and of course accessed.” Broadcast content beside their intrinsic value, are the embodiment of indispensable historical and political value, and also account for historical and research source, 20th century onwards."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:9e932676-dff8-4373-8074-1e68bd65a39c>","<urn:uuid:048540d7-2872-492e-9533-42aa22a1aa8b>"],"error":null}
{"question":"Which cutting method requires more setup time: plasma cutting or laser cutting with focusing?","answer":"Plasma cutting requires less setup time as it does not need 'heat-up' time, making it time efficient and convenient. In contrast, laser cutting requires a more complex setup process involving precise focusing of the laser beam through focal lenses, which includes careful measurement and adjustment of the focal distance, and may require multiple test spots to find the true focal point of the lens.","context":["Plasma cutting is a method applied to cut highly durable and thick materials with high precision. Cutting materials such as steel is an intricate process that requires the professional application of precision cutting equipment and can only be handled by trained and capable persons.\nWhat is Plasma Cutting?\nWhile the majority of the world recognizes the term plasma as applied to their high definition television screens, they rarely take the time to consider what plasma actually is. Plasma is the fourth state of matter, as it begins as a gas and then becomes ionized. As such, it is capable of conducting electricity.\nBy adding energy to an electrically neutral gas, plasma is created. In terms of plasma cutting, the plasma gas is applied as a compressed air. When electrical energy is applied to the plasma gas, its heat increases, creating a stronger cutting force to be projected from the plasma arc.\nThe plasma arc is the actual device used to apply the flow of this forceful energy to the materials to be cut. In laymen's terms, it can be thought of as the nozzle from which the ionized gas projects and is applied strategically to the material.\nThe plasma arc cutting machine controls this intense energy effectively. The high-intensity pressure is forced through the nozzle, a small concentrated outlet for the massive energy source. In turn, this force of energy and ionized gas can be used to cut the most durable and resistant materials.\nWhy Plasma Cutting?\nThere are a number of practical reasons that make plasma cutting a first choice amongst the various other metal cutting options. Not only is it one of the most tidy methods of intense cutting processes, it can also be applied to a wider range of materials than other methods.\nSome of the advantages of selecting a plasma cutting service over other cutting services include:\nQuicker cutting time\nDoes not require \"heat-up” time, making it time efficient and convenient\nThe width of the cut is extremely precise\nUnlike the application of other metal cutting methods, plasma cutting will not warp the edges of the metals from overheating them, leaving a smooth, clean edge\nWhat Metals Can Plasma Cut?\nAs it is an electrically conducive substance, plasma can cut a wider range of materials more effectively than other methods of metal cutting. It can be applied to a range of electrically conductive metals and materials:\nAs with the majority of technological and scientific advancements, the world has seen improvements in plasma cutting over the past 50 years. It developed out of plasma welding in the 1960s and professionals were quick to see its advantages over other metal cutting methods. Previously, other metal sources were used to cut metal, which was labor-intensive and not very precise. This left large metal chips and produced inaccurate lines and cuts.\nPlasma cutting methods gave clean results and soon, professionals took to plasma-cutting services over any other. Since then, the world has seen plasma cutting evolve to a dependable and heavy-duty metal cutting service.","You can find the normal solution about laser cutting machine and laser cutting process.\nLaser cutting is a technology that uses a laser to cut materials, and is typically used for industrial manufacturing applications, but is also starting to be used by schools, small businesses, and hobbyists. Laser cutting works by directing the output of a high-power laser most commonly through optics. The laser optics and CNC (computer numerical control) are used to direct the material or the laser beam generated. A typical commercial laser for cutting materials would involve a motion control system to follow a CNC or G-code of the pattern to be cut onto the material. The focused laser beam is directed at the material, which then either melts, burns, vaporizes away, or is blown away by a jet of gas, leaving an edge with a high-quality surface finish. Industrial laser cutters are used to cut flat-sheet material as well as structural and piping materials.\nFactors Influencing on the Dimensions Accuracy in Laser Cutting\nWe confirm a laser cutting machine manufacturer is excellent, the cutting precision is the first standard. Therefore, how to confirm the cutting precision whether qualified will be from the following four factors considered\n1. the size of the laser coagulation of the laser generator. If the spot is very small, the cutting accuracy is very high, and if the gap is very small after cutting. It shows that the precision of laser cutting machine is very high, and the quality is very high.\n2. the accuracy of the working table. If the accuracy of the working table is very high, then the accuracy of the cutting will be improved. Therefore, the precision of the working table is also a very important factor to measure the accuracy of the laser generator.\n3. laser beam condensed into a cone. When cutting, the laser beam is to taper down, when the thickness of the workpiece cutting is very large, cutting accuracy will be reduced, cut out the gap will be very large.\n4. cutting the material is different, also will affect the precision of the laser cutting machine. In the same case, the cutting of stainless steel and aluminum will be very different accuracy, stainless steel cutting accuracy will be higher, and the section will be smooth.\nHow to focus the laser\nThe laser beam is focused through the focal lens. The focal lens acts like a magnifying glass and sunlight. For a 55mm lens, the laser beam passes through the lens and converges to the smallest point at about 55mm from the edge of the lens. The laser beam is concentrated to the smallest size at this “spot”. Given that the lens is mounted in the focal tube, the question is how to put the material at the optimum location to engrave or cut.\nFirst, think of what results are desired. Whenever we want to engrave, we want to have the laser beam focused to the smallest spot and that spot located at the top surface of the material. Having the smallest spot size will give us the best resolution. the best DPI (dots per inch). The laser machine should have come with a manual height measurement tool. Some machine come with a square piece or acrylic to match to a marker on the side of the focal tube. Other machines come with a feeler gauge that snuggly fits between the focal tube nozzle and the top surface of the material.\nThe normal adjustment method is to place the material on the worktable and then to move the worktable height such that the top surface of the material is at the focus spot of the laser beam. Use the measurement tool while moving the table to the proper height. Make sure not to move the table too far. You will not want to damage the table surface, material, or the focal assembly.\nMost laser machines have a movable table height. If the table will not move or is already moved to the top, then the focal tube has some adjustment to move/slide up and down about 1.5 inches. First, loosen the focal tube nut (or screw). Second, move the focal tube to the desired height above the material surface. Last, tighten the focal tube nut (or screw).\nYou may be concerned that you are using the provided tool to place the focus at the prescribed distance, but the focus just doesn’t seem right. Please remember that the Chinese optics are not the best. The optimum focal distance might be slightly closer or farther away from the lens. Place a piece of flat scrap material (wood) under the focal assembly. Adjust the focus such that the material is slightly too close to the focal lens. Use the “laser” button to make a test spot on the wood. The spot size will be larger than desire for engraving. Move the table away from the lens just a small distance. Move the wood to a clean target location. Make another test spot using the “laser” button. The spot size should have gotten smaller. Continue moving the table and making test spots on the wood surface. When the spot starts getting bigger, then you have just passed the focal point. This is the easiest way to find the true focal distance of your lens.\nTo get the best engraving….\n1. Make sure that your laser is focused on the material.\n2. If your target material is an uneven surface, then it may find some areas where the laser is out of focus.\n3. If your target material is a dowel rod and you are not using a rotary attachment. The laser will be out of focus at some portions of the image.\n4. If your image seems fuzzy at the edges of the laser cut, but is focused, then you might be trying to engrave at too high of a speed. Set the engraving speed to a slower rate. You will also need to reduce the laser power percentage as to not over-burn the material.\n5. If your material shows (Scan) lines in the engraved areas, then the “scan gap” may need to be reduced. The “scan gap” is the amount of space that the rail moves in the Y direction between engraver scanning passes. Setting the “scan gap” to a lower number will give a better resolution. With some materials (anodized aluminum, hard plastics, and hard wood), a scan gap of 0.05 may give excellent results. A good setting for glass is 0.07. In soft plastics, a scan gap of 0.1 will be needed to ensure the plastic does not glob. A setting of 0.1 is good for soft woods.\nIf you are regularly engraving materials that vary in distance from the focal point, then it may be a good idea to purchase a focal lens with longer focal length. The longer focal length will stay tighter to focus for more distance.\nThere are three main types of lasers used in laser cutting. The CO2 laser is suited for cutting, boring, and engraving. The neodymium (ND) and neodymium yttrium-aluminum-garnet (ND-YAG) lasers are identical in style and differ only in application. ND is used for boring and where high energy but low repetition are required. The ND-YAG laser is used where very high power is needed and for boring and engraving. Both CO2 and ND/ ND-YAG lasers can be used for welding.\nCommon variants of CO2 lasers include fast axial flow, slow axial flow, transverse flow, and slab.\nCO2 lasers are commonly “pumped” by passing a current through the gas mix (DC-excited) or using radio frequency energy (RF-excited). The RF method is newer and has become more popular. Since DC designs require electrodes inside the cavity, they can encounter electrode erosion and plating of electrode material on glassware and optics. Since RF resonators have external electrodes they are not prone to those problems.\nCO2 lasers are used for industrial cutting of many materials including mild steel, aluminum, stainless steel, titanium, task board, paper, wax, plastics, wood, and fabrics. YAG lasers are primarily used for cutting and scribing metals and ceramics.\nIn addition to the power source, the type of gas flow can affect performance as well. In a fast axial flow resonator, the mixture of carbon dioxide, helium and nitrogen is circulated at high velocity by a turbine or blower. Transverse flow lasers circulate the gas mix at a lower velocity, requiring a simpler blower. Slab or diffusion cooled resonators have a static gas field that requires no pressurization or glassware, leading to savings on replacement turbines and glassware.\nThe laser generator and external optics (including the focus lens) require cooling. Depending on system size and configuration, waste heat may be transferred by a coolant or directly to air. Water is a commonly used coolant, usually circulated through a chiller or heat transfer system.\nA laser microjet is a water-jet guided laser in which a pulsed laser beam is coupled into a low-pressure water jet. This is used to perform laser cutting functions while using the water jet to guide the laser beam, much like an optical fiber, through total internal reflection. The advantages of this are that the water also removes debris and cools the material. Additional advantages over traditional “dry” laser cutting are high dicing speeds, parallel kerf, and omnidirectional cutting.\nFiber lasers are a type of solid state laser that is rapidly growing within the metal cutting industry. Unlike CO2, Fiber technology utilizes a solid gain medium, as opposed to a gas or liquid. The “seed laser” produces the laser beam and is then amplified within a glass fiber. With a wavelength of only 1.064 micrometers fiber lasers produce an extremely small spot size (up to 100 times smaller compared to the CO2) making it ideal for cutting reflective metal material. This is one of the main advantages of Fiber compared to CO2\nProduction and cutting rates\nLaser cutting and laser fine cutting are applied for different kinds of materials where complex contours demand precise, fast and force-free processing. Lasers create narrow kerfs and thus achieve high-precision cuts. This method does not show any distortion and in many cases post-processing is not necessary as the component is subject to only little heat input and can mostly be cut dross-free.\nAlmost all kinds of metals can be laser cut: mild steel, stainless steel and aluminum are the most common applications. Other laser cut parts are made from wood, plastics, glass and ceramics. Compared to alternative techniques like die cutting, laser cutting is cost-efficient already for small-batch production. The big benefit of laser cutting is the localized laser energy input providing small focal diameters, small kerf widths, high feed rate and minimal heat input.\n- Mild steelThe cutting in two dimensions is a domain of the CO2 laser. Typical cutting speeds for mild steel are for example at 18m/min for 1 mm, 4.5 m/min for 3 mm and 1.5 m/min for 8 mm material strength. Basically, the cutting of metals with lasers happens through the local heating of the material above its melting point in the focal point of the focused laser. The resulting molten material is ejected by a gas flow oriented coaxially to the laser beam so that a kerf is formed.For low-alloyed steels in particular, oxygen is typically used as cutting gas. At the moment, the maximal processable counter thickness for laser flame cutting of steel is approximately at 25 mm.\n- Stainless SteelStainless steel is processed with laser fusion cutting. CO2 and also solid-state lasers are suited for this kind of applications, with CO2 lasers being preferred for cutting of thicker materials. CO2 lasers cut stainless steel and construction steel at a cutting speed of 18m/min at a material strength of 1 mm.In micro material processing solid-state lasers (fiber laser, pulsed ND: YAG) are commonly applied for laser cutting stainless steel, providing cut widths down to 20 microns depending on the steel thickness.\n- Non-Ferrous Heavy Metal (Processing of Highly Reflective Materials)Aluminum, magnesium, brass, copper, bronze, titanium, zirconium, nickel, silver, gold, platinum, tantalum, zinc or tin are examples for non-ferrous metals that can be cut with a laser. Depending on the requirements of the workpiece and material thickness, you can use either removal cutting processes with pulsed lasers or fusion cutting processes with CW lasers. Micro or Macro?\nThin workpieces can be cut with either pulsed lasers or continuous wave lasers. With these processes, an assist gas is used to expel molten material out of the kerf, resulting in a burr-free cutting edge. The width of the kerf will be between 50-300 micrometer depending on the material and the type of laser. The laser power determines the cutting speed, which can range from 0.5m/min to over 100m/min. Using these cutting processes on thin material, pulsed lasers achieve high accuracy, high quality, and the thermal impact is low. Continuous wave lasers (macro) using the same process will be able to achieve very high cutting speeds.Sublimation cutting is done without any cutting gas. The material evaporates directly, and the kerf is created by gradual ablation. This can be realized using single mode fiber lasers (macro) or by short pulse lasers with high peak power (micro). With both processes, mirror deflection systems are the preferred option for the beam movement.\nMacro: which is the right laser?\nBoth CO2 and fiber lasers can be used for cutting the most non-ferrous metals. Some non-ferrous metals reflect the CO2 laser beam so strongly that fiber laser cutting is preferred. This applies for copper (cu), gold (au) and silver (ag). For all other non-ferrous metals, you can say: fiber lasers mainly have their advantages in thin sheet, but CO2 lasers mostly have a better quality for thicker material.\n- Noble Metals\nPerfect Cutting Results for Metals with High Heat Conductivity Precious metals, like gold, silver and platinum metals can be laser cut. Platinum metals are cut by the CO2 laser as well as solid-state lasers, whereas for gold and silver, pulsed or q-switched YAG lasers are the tool of first choice as wavelengths of solid-state lasers are better absorbed.\nSend your message to us:\nPost time: Dec-14-2017"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:96625a96-c562-4372-8aa1-9f11c00ffb1a>","<urn:uuid:10af8520-6a72-43d1-9dcf-79eb017691bc>"],"error":null}
{"question":"I'm planning cultural visits in Oxford. What are the architectural highlights of the Sheldonian Theatre, and how does its cultural significance compare to educational landscape designs?","answer":"The Sheldonian Theatre, built between 1664-1669, features impressive architectural elements including Robert Streater's painted ceiling and a Cupola offering 360° panoramic views of Oxford. It serves as a venue for music concerts, lectures, and University ceremonies. In terms of cultural significance, landscape designs similarly reflect cultural and political belief systems - this is evident in how different eras and cultures have varying ideals of landscape beauty. This can be explored through various examples like Chinese Scholar's Gardens and historical estate gardens, which express different cosmological views and cultural values through their design.","context":["Situated in the focal southern piece of England, Oxford, a city of somewhat more than 150,000, is incredibly famous for its college. In any case, the city has a great deal more to offer than simply the University of Oxford.\nPitt Rivers Museum\nSituated toward the east of the Oxford University Museum of Natural History, the Pitt Rivers Museum highlights archeological and anthropological shows. Established in 1884, Augustus Pitt Rivers gave his gathering with the condition that a changeless anthropological speaker be named. Indeed, even in present day, the staff of the exhibition hall are included in showing paleontology and human studies at the University. The accumulations are entrancing and frightful. You will see skulls, contracted heads, a time by time fossil science show, a Dodo and progressively that you can’t simply observe anyplace. The exhibition hall obliges all ages notwithstanding offering a paper and pencil to guests to discover diverse types of dinosaur to make it much additionally intriguing and a test to keep more youthful individuals eager to investigate promote. You’ll require a few hours to peruse this astounding, stand-out and immense gathering.\nOxford University Museum of Natural History\nGiving the main access to Pitt Rivers Museum, the Oxford University Museum of Natural History, additionally referred to just as Oxford University Museum, displays normal history examples and contains an address theater. On the off chance that you need to visit Pitt Rivers Museum, you will likewise need to work an appearance of this historical center into your motivation. Established in 1860, the accumulation comprises of land and zoological examples. The building itself is an incredible sight with its neo-Gothic engineering. Some of its more well known shows are the Oxfordshire dinosaurs, the Dodo and swifts in the tower. With more than 250,000 examples, the zoological accumulation contains various imperiled and wiped out species. The Dodo show is the most total stays of that species on the planet. Different accumulations worth saying incorporate those of Thomas Bell, William Burchell and Charles Darwin. Between the two galleries, you will require a whole day to take everything in.\nWorked somewhere around 1664 and 1669, and named after chancellor of the college at the time, Gilbert Sheldon, who was additionally the monetary sponsor, the Sheldonian Theater has music shows, addresses and University functions. When you enter the theater, you will be hypnotized by the amazing roof painted by Robert Streater. You can then dare to the Cupola where you have a fantastic 360° all encompassing perspective of Oxford. You can wander through the theater all alone or book a guided visit and catch wind of the history as you take in the sights. In case you’re truly fortunate, you can go to a show when on your excursion. Simply make a point to check their timetable before booking your excursion and you can buy tickets with awesome seats before you go.\nChrist Church Picture Gallery\nSituated at Christ Church, the Christ Church Picture Gallery is a workmanship historical center highlighting around 300 Old Master depictions and just about 2,000 drawings. It is viewed as a standout amongst the most critical accumulations in the United Kingdom with a noteworthy part of the gathering being given by General John Guise upon his passing in 1765. From that point forward, there have been different endowments and estates by names, for example, W.T.H. Fox-Strangways, Walter Savage Landor, Sir Richard Nosworthy and C.R. Patterson. The chips away at show here have no equivalent anyplace on the planet. You will be completely excited in the radiance of the gathering and the otherworldly messages contained in them. It’s a shrouded treasure inside Christ Church offering an awesome display, a peaceful environment and recollections to endure forever.\nKeep running by the University of Oxford, Harcourt Arboretum is an arboretum and a satellite of the college’s natural garden and covers roughly 130 sections of land (.61 km2). Around 10 sections of land (40,000 m2) comprises of normal English forest and another 37 sections of land (150,000 m2) is a late spring blossoming knoll. The center of the arboretum comprises of the Pinetum including goliath redwoods and monkey-astound trees. It was initially intended to make an amazing access to the Nuneham House situated around 1.5 miles (2.4 km) away. It’s a wonderful normal perfect world definitely justified even despite the visit. They offer guided visits, knapsacks and occasional trails for you to appreciate. When you obtain a knapsack, it is packed with exercises to center your stroll around the 130 sections of land. The arboretum is interested in the overall population throughout the entire year.","A school garden allows children to learn in a hands-on, synthesizing manner.\nExploring knowledge and skills through a garden allows students to learn, to create and to experiment, as well as to work collaboratively.\nsome program possibilities for school gardens\n1. Edible schoolyard & sustainability cycle. In this program, school children work with the garden through a full cycle : plant, harvest, provide food from garden to cafeteria, learn to prepare meals, celebrate a harvest feast, compost the leftovers, use the compost to fertilize new seeds; as well as rainwater harvesting.\n2. Sustainability : native plant selection to support local food web.\n3. Principles of biophilic design : environmental psychology research from E. O. Wilson till today, and exploring how these principles can be applied to design of spaces.\n4. Horticultural techniques : growing from seed, root cuttings, per-square-foot planting calculations, composition, fertilizing, pruning, etc.\n5. Cultural : in this program, the history of garden design is explored to discover how cosmological or political belief systems are reflected in the ideals of landscape beauty. From one era to the next and one culture to the next, these ideals vary greatly.\n6. History of North America through horticulture : starting with American Indian cultures' botanical knowledge and practices, followed by different horticultural practices brought to the continent by various cultures, cultural cross-pollination, etc.\nareas of study\n1. History - this subject finds its way into all of the programs in one way or another. For example, the history of sustainability would include a review of wisdom traditions of American Indians, the legacy of John Muir and Teddy Roosevelt, and the call-to-action of 1960s environmental leaders like Rachel Carson and Ian McHarg.\n2. Social studies - this subject finds fascinating material in the collective wisdom of the many cultures and nationalities with their particular sets of skills, knowledge, aesthetics and how these have woven together in the American landscape.\n3. Math - this subject overlaps with science, design, planning, and can also be explored in the arts. For example, an examination of fractals as found in nature provides an exciting exploration of both math and art.\n4. Sciences - studies of botany, soil science, climate, microclimates, irrigation, sustainable methods, materials, photosynthesis, carbon sequestration, oxygen production, the chemistry of compost and of soil and of plant matter.\n5. Fine Arts - imagination is the limit for the arts as relates to landscape. Cross-pollination with the other areas of study may produce the most exciting ideas. Practiced methods of creative inspiration used by famous artists throughout history might be employed (e.g., growing yarrow and using dried stalks to throw the I ching, resulting in a hexagram upon which the class then bases a design). Also, the landscape as expressing a culture's cosmological view or political system could be explored through field trips to NYC landscapes such as the Chinese Scholar's Garden, the Highline, the Cloisters, and area estate gardens on the Hudson and Long Island. A trip to Olana and to Central Park, as well as a review of the Hudson River School could explore 18th century Americans' view of nature.\n6. Performance Arts - Cross-pollinating the garden and other fields of study with performance could result in some fun and expressive works by and for school kids. For example, grades K-1 could might write and stage on a play about the garden's life cycle over 4 seasons. Some could play the role of seeds, some could be earthworms, some could be fallen leaves or blueberries or the sun or Winter.\n7. Environment - in addition to a review of the history of the environmental movement in North America, field trips could be made to NYC's various ecosystem treasures such as the primeval forests in Inwood and at New York Botanical Garden, the wetlands at Alley Pond Environmental Center and in Brooklyn, the area beaches, and to the green roof facility at Randalls Island."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:a35f6412-8faf-4520-ad56-1d2b2738957e>","<urn:uuid:06a6904d-4fab-4e44-8970-5c975fcdd32d>"],"error":null}
{"question":"As a student of comparative religion, I'd like to know how Advaita Vedanta and Madhva's Dvaita differ in their views on the relationship between God and reality?","answer":"Advaita Vedanta and Madhva's Dvaita present opposing views on reality and God. Advaita teaches that Brahman (God) is the only truth and reality, while everything else including the universe and individuals is false. It states that ultimately there is no difference between Brahman and individual self (Atman). In contrast, Dvaita philosophy maintains that reality consists of two distinct principles: the independent (God) and the dependent (souls, matter, time, etc.). While Madhva acknowledges these dependent realities as eternal and distinct, they exist only through God's consent. Madhva specifically describes five fundamental differences: between soul and God, matter and God, between souls, between matter and soul, and between material elements.","context":["|Part of a series on\n|Samkhya · Yoga|\n|Nyaya · Vaisheshika|\n|Purva Mimamsa · Vedanta|\n|Schools of Vedanta|\n|Advaita · Vishishtadvaita|\n|Dvaita · Shuddhadvaita|\n|Dvaitadvaita · Achintya Bheda Abheda|\n|Kapila · Patañjali|\n|Gotama · Kanada|\n|Jaimini · Vyasa|\n|Adi Shankara · Ramanuja|\n|Madhva · Madhusudana|\n|Tukaram · Namadeva|\n|Vedanta Desika · Jayatirtha|\n|Vallabha · Nimbarka|\n|Ramakrishna · Ramana Maharshi|\n|Vivekananda · Narayana Guru|\n|A.C. Bhaktivedanta Swami Prabhupada|\n|N.C. Yati · Coomaraswamy|\n|Satyananda · Chinmayananda|\nAdvaita Vedanta (IAST Advaita Vedānta; Sanskrit अद्वैत वेदान्त; IPA: [əd̪vait̪ə veːd̪ɑːnt̪ə]), a sub-school of the Vedānta (literally, end or the goal of the Vedas, Sanskrit) school of Hindu philosophy, numbers with Dvaita and Viśishṭādvaita as major sub-schools of Vedānta. Advaita (literally, non-duality) often has been called a monistic system of thought. The word \"Advaita\" essentially refers to the identity of the Self (Atman) and the Whole (Brahman). The key source texts for all schools of Vedānta include the Prasthanatrayi—the canonical texts consisting of the Upanishads, the Bhagavad Gita and the Brahma Sutras. Adi Shankara first explicitly consolidated the principles of Advaita Vedanta.\nVedānta, based on the most ancient Hindu scriptures, the Vedas, is one of the oldest branches of Hinduism. Advaita Vedanta arose in the 700s C.E. through the thinking of Adi Shankara (700-750 C.E.). He created Advaita Vedanta through reflection on the basic Hindu texts, Upanishads, the Bhagavad Gita and the Brahma Sutras. Shankara's founding of Advaita Vedanta upon classical Hindu texts accounts, in part, for the longevity of his branch of Hinduism. Another reason for the longevity and vitality of Advaita Vedanta lay in the need fulfilled by the theology and philosophy.\nShankara introduced a monistic thought, referred to as non-dualistic. Basically, he contented, based upon Hindu scriptures, that Brahmin (Whole) and Self (Atman) are the same. No difference or distinction exists between Atman and Brahmin. That is a difficult, and profound, position to defend. Yet Shankara set forth a reasonable system that has stood the test of time. He argued that Brahmin is the only truth, the world is illusion, and that reality is three-tiered. At the third tier, all existence is one. Advaita's greatest contribution is serving as a bridge between the rationalistic (jnana) yoga and the devotional (bhakti) yoga, the yoga of ordinary people.\n|This article contains Indic text. Without proper rendering support, you may see question marks or boxes, misplaced vowels or missing conjuncts instead of Indic text.|\nAdi Shankara consolidated the Advaita Vedanta, an interpretation of the Vedic scriptures approved and accepted by Gaudapada and Govinda Bhagavatpada siddhānta (system). Continuing the line of thought of some of the Upanishadic teachers, and also that of his own teacher's teacher Gaudapada, (Ajativada), Adi Shankara expounded the doctrine of Advaita—a nondualistic reality.\nBrahma satyaṃ jagat mithyā, jīvo brahmaiva nāparah — Brahman is the only truth, the world is illusion, and there is ultimately no difference between Brahman and individual self\nIn his metaphysics, three tiers of reality exist with each one negating the previous. The category illusion in that system amounts to unreal only from the viewpoint of the absolutely real, different from the category of the Absolutely unreal. His system of vedanta introduced the method of critical study on the accepted metaphysics of the Upanishads, all the later vedanta schools adopting that style. His refusal to literally use scriptural statements, rather adopting symbolic interpretation where he considered it appropriate, represents another distinctive feature of his work.\nAdi Shankara made crucial contributions to Advaita, especially the commentaries on the Prasthanatrayi (Brahma Sūtras, Bhagavad Gītā, the Upanişads) and the Gaudapadiya Karikas. He also wrote a major independent treatise, called Upadeśa Sāhasrī, expounding his philosophy.\nAdvaita vedānta requires anyone seeking to study advaita vedānta to learn from a Guru (teacher). The Guru must have the following qualities (see Gambhirananda and Andre van den Brink, Mundaka upanishad (Den Haag: Van den Brink, 2001, 1.2.12): Śrotriya, must have expert knowledge in the Vedic scriptures and sampradaya. Brahmaniṣṭha, literally meaning established in Brahman; must have realized the oneness of Brahman in everything and in himself\nThe seeker must serve the Guru and submit questions with all humility to remove all doubts (see Bhagavad Gita 4.34). By doing so, advaita says, the seeker will attain moksha (liberation from the cycle of births and deaths).\nAny mumukṣu (one seeking moksha) has to have the following four sampattis (qualifications), collectively called Sādhana Chatuṣṭaya Sampatti (the fourfold qualifications): 1) Nityānitya vastu viveka — The ability (viveka) to correctly discriminate between the eternal (nitya) substance (Brahman) and transitory existence (anitya). 2) Ihāmutrārtha phala bhoga virāga — The renunciation (virāga) of enjoyments of objects (artha phala bhoga) in this world (iha) and the other worlds (amutra) like heaven. 3) Śamādi ṣatka sampatti — the sixfold qualities of śama (control of the antahkaraṇa), dama (the control of external sense organs), uparati (the refraining from actions; instead concentrating on meditation), titikṣa (the tolerating of tāpatraya), śraddha (the faith in Guru and Vedas), samādhāna (the concentrating of the mind on God and Guru). 4) Mumukṣutva — The firm conviction that misery and the intense longing for moksha (release from the cycle of births and deaths) represents the nature of the world.\nAdvaita vedānta teaches that moksha, or liberation, comes only to those fourfold qualifications. Any seeker wishing to study advaita vedānta from a teacher must possess them.\nAdvaita Vedanta also teaches that the Self has the capacity of knowing itself without those conditions. Knowing the Self or Atman in relation to Brahman simply requires knowing that you know, which may be realized in an instant without a guru. Advaita Vedanta teaches that you, physical manifestations, the universe and beyond are who you are, that you are your own Guru. You are the source of all knowledge, because you are knowledge itself. Teachers or Gurus may help but each person is their own guru. Purity and trueness, as stated in the Prashna Upanishad, \"The bright world of Brahman can be attained only by those that are pure and true,\" represent the only prerequisites.\nPramāṇas. Pramā, in Sanskrit, refers to the correct knowledge of any thing, derived thorough reasoning. Pramāṇa (sources of knowledge, Sanskrit) forms one part of a tripuṭi (trio), namely: 1) Pramātṛ, the subject; the knower of the knowledge. 2) Pramāṇa, the cause or the means of the knowledge. And 3) Prameya, the object of knowledge.\nIn Advaita Vedānta, the following pramāṇas prevail: 1) Pratyakṣa — the knowledge gained by means of the senses. 2) Anumāna — the knowledge gained by means of inference. 3) Upamāna — the knowledge gained by means of analogy. 4) Arthāpatti — knowledge gained by superimposing the what is known on what is apparently knowledge. And 5) Āgama — the knowledge gained by through studying texts such as Vedas (also known as Āptavākya, Śabda pramāṇa).\nKārya and kāraṇa. Vedanta places in highlight the kārya (effect) and kāraṇa (cause), recognizing two kāraṇatvas (ways of being the cause): 1) Nimitta kāraṇatva — Being the instrumental cause. 2) Upādāna kāraṇatva — Being the material cause. Advaita concludes that Brahman serves as both the instrumental cause and the material cause.\nKārya-kāraṇa ananyatva. Advaita states that kārya (effect) is similar kāraṇa (cause), yet they have differences or Kārya-kāraṇa ananyatva (the non-difference of the effect from the cause). Kārya is not different from kāraṇa; however kāraṇa is different from kārya. In the context of Advaita Vedanta, Jagat (the world) is not different from Brahman; however Brahman is different from Jagat.\nThree levels of truth. According to Advaita Vedanta, three levels of truth exist: 1) The transcendental or the Pāramārthika level with Brahman as the only reality and nothing else. 2) The pragmatic or the Vyāvahārika level where both Jiva (living creatures or individual souls) and Ishvara are true. The material world is completely true. And, 3) The apparent or the Prāthibhāsika level where even material world reality is actually false, like illusion of a snake over a rope or a dream.\nBrahman. According to Adi Shankara, God, the Supreme Cosmic Spirit or Brahman is the One, the whole and the only reality. Other than Brahman, everything else, including the universe, material objects and individuals, are false. Brahman is at best described as that infinite, omnipresent, omnipotent, incorporeal, impersonal, transcendent reality, the divine ground of all Being.\nBrahman is the origin of this and that, the origin of forces, substances, all of existence, the undefined, the basis of all, unborn, the essential truth, unchanging, eternal, the absolute and beyond the senses. Brahman dwells in the purest knowledge itself, illuminant like a source of infinite light. Due to ignorance (avidyā), the Brahman is visible as the material world and its objects. The actual Brahman is attributeless and formless (see Nirguna Brahman), the Self-existent, the Absolute and the Imperishable, indescribable.\nMāyā. Māyā (/mɑːjɑː/) According to Adi Shankara, Māyā constitutes the illusionary power of Brahman that brings people to see the Brahman the material world of separate forms. It has two main functions; to \"hide\" Brahman from ordinary human perception and to present the material world in its stead.\nStatus of the world. Adi Shankara says that the world is an illusion because of some logical reasons. Consider the following logical argument. A pen is placed in front of a mirror. One can see its reflection. To one's eyes, the image of the pen is perceived. Now, what should the image be called? It cannot be true, because it is an image. The truth is the pen. It cannot be false, because it is seen by our eyes.\nĪshvara (literally, the Supreme Lord). According to Advaita Vedanta, when man tries to know the attributeless Brahman with his mind, under the influence of Maya, Brahman becomes the Lord. Ishvara is Brahman with Maya—the manifested form of Brahman. The Supreme Lord's actual form in the transcendental level is the Cosmic Spirit.\nIshvara is Saguna Brahman or Brahman with innumerable auspicious qualities. All-perfect, omniscient, omnipresent, incorporeal, independent, Creator of the world, Brahman acts as its ruler and also destroyer. Eternal and unchangeable, the material and the instrumental cause of the world, both immanent and transcendent, he may even have a personality.\nBrahman is the source morality and giver of the fruits of one's Karma. He himself is beyond sin and merit. He rules the world with his Maya. (His divine power). There is no place for a Satan or devil in Hinduism, unlike Abrahamic religions. Advaitins explain the misery because of ignorance.\nStatus of God. To think that there is no place for a personal God (Ishvara) in Advaita Vedanta is a misunderstanding of the philosophy. Ishvara is, in an ultimate sense, described as \"false\" because Brahman appears as Ishvara only due to the curtain of Maya. However, as described earlier, just as the world is true in the pragmatic level, similarly, Ishvara is also pragmatically true. Just as the world is not absolutely false, Ishvara is also not absolutely false. He is the distributor of the fruits of one's Karma. See, Karma in Hinduism for more information. In order to make the pragmatic life successful, it is very important to believe in God and worship him. In the pragmatic level, whenever we talk about Brahman, we are in fact talking about God. God is the highest knowledge theoretically possible in that level. Devotion (Bhakti) will cancel the effects of bad Karma and will make a person closer to the true knowledge by purifying his mind. Slowly, the difference between the worshiper and the worshiped decreases and upon true knowledge, liberation occurs.\nĀtman. The soul or the self (Atman) is identical with Brahman, not a part of Brahman that ultimately dissolves into Brahman, but the whole Brahman itself. Atman, the silent witness of all the modifications, stands free and beyond sin and merit, experiencing neither happiness nor pain because it is beyond the triad of Experiencer, Experienced and Experiencing, incorporeal and independent. When the reflection of atman falls on Avidya (ignorance), atman becomes jīva—a living being with a body and senses. Each jiva feels as if he has his own, unique and distinct Atman, called jivatman. The concept of jiva has truth only in the pragmatic level. In the transcendental level, only the one Atman, equal to Brahman, is true.\nSalvation. Liberation or Moksha (akin to Nirvana of the Buddhists)—Advaitins also believe in the theory of reincarnation of souls (Atman) into plants, animals and humans according to their karma. They believe that suffering arises from Maya, and only knowledge (called Jnana) of Brahman can destroy Maya. Maya removed, ultimately Jiva-Atman and the Brahman are the same. Such a state of bliss, when achieved while living, goes by the term Jivan mukti.\nTheory of creation. Adi Shankara believes in the Creation of the world through Satkaryavada. Samkhya teaches a sub-form of Satkaryavada called Parinamavada (evolution) whereby the cause really becomes an effect. The Supreme Lord Ishvara created the universe from a viewpoint of the sense. Maya represents Ishvara divine magic, with the help of which Ishvara creates the world.\nThe Upanishads sets for the order of Creation. First of all, Ishvara creates the five subtle elements (ether, air, fire, water and earth). Maya creates Ether. Air arises from ether. Fire, arises from air. Water arises from fire, earth from water. From a proportional combination of all five subtle elements, the five gross elements come into creation. From those elements, the universe and life derive. Destruction follows the reverse order.\nStatus of ethics. Ethics has a firm place in Advaita; the same place as the world and God. Ethics, which implies doing good Karma, indirectly helps in attaining true knowledge. The Shruti (the Vedas and the Upanishads) constitute the basis of merit and sin. Dharma infuses truth, non-violence, service of others, and pity while adharma (sin) infuses lies, violence, cheating, selfishness, and greed.\nAdvaita rejuvenated much of Hindu thought and also spurred debate with the two main theistic schools of Vedanta philosophy formalized later: Vishishtadvaita (qualified nondualism), and Dvaita (dualism). Advaita further helped to merge the old Vedic religion with popular south-Asian cults/deities, thus making a bridge between higher types of practice (such as jnana yoga) and devotional religion of ordinary people.\n|Topics||Logic · Idealism · Monotheism · Atheism · Problem of evil|\n|Āstika||Samkhya · Nyaya · Vaisheshika · Yoga · Mimamsa · Vedanta (Advaita · Vishishtadvaita · Dvaita)|\n|Nāstika||Carvaka · Jaina (Anekantavada) · Bauddha (Shunyata · Madhyamaka · Yogacara · Sautrantika · Svatantrika)|\nAll links retrieved February 13, 2016.\nNew World Encyclopedia writers and editors rewrote and completed the Wikipedia article in accordance with New World Encyclopedia standards. This article abides by terms of the Creative Commons CC-by-sa 3.0 License (CC-by-sa), which may be used and disseminated with proper attribution. Credit is due under the terms of this license that can reference both the New World Encyclopedia contributors and the selfless volunteer contributors of the Wikimedia Foundation. To cite this article click here for a list of acceptable citing formats.The history of earlier contributions by wikipedians is accessible to researchers here:\nNote: Some restrictions may apply to use of individual images which are separately licensed.","The Great Madhva Acarya\nPage 2 – Writings and Theology\nThe writings of Madhvacarya comprise thirty-seven works, collectively called the sarva-mula. They are divided into four groups. The first group includes his commentaries on the Upanisads, Bhagavad-gita and Vedanta-sutra. In this group there are ten Upanisad commentaries, two Gita commentaries and fourVedanta-sutra commentaries. The second group includes ten short works called the Dasa-prakaranas that outline the basic principles of Madhva’s theology and demonstrates his refutation of key aspects of advaita theology. The third group is Madhva’s commentaries on the Bhagavata-purana, theMahabharata and the Rg-veda. The fourth group is his miscellaneous works that includes important poems, writings on rituals, image worship and rules for the ascetic order.\nMadhva’s writing style is straightforward, unembellished and terse. Were it not for the explanations of his later commentators, especially Jayatirtha in the 14th century, Madhva’s theology may have remained obscure due to its extreme brevity. Never does Madhva engage in long discussions like his predecessors,Sankara or Ramanuja. It was left to the work of his followers to bring forth the subtlety of his thoughts.\nThere is a controversy that hangs over Madhva’s writings. His works are filled with a great number of corroborating sources that are no longer extant. Consequently, the authenticity of his sources has been called into question. Madhva has even been accused of inventing many of his references. Over the centuries this has been an important issue for Madhva scholars. It is known that Madhvacarya had an extensive library of manuscripts and it has been argued that his references have been drawn from this collection of manuscripts. B. N. K. Sharma has elaborately discussed this criticism.\nA Brief Synopsis of Madhva’s Theology\nThe school of theology that Madhva founded is commonly called the DvaitaSchool. It is also known as tattva-vada, the doctrine of categories. The worddvaita means duality. According to this view, reality is composed of only two basic principles: the independent (sva-tantra) and the dependent (para-tantra). God or the Supreme Being is the only independent reality. Everything else, soul (jiva), matter (prakrti), time (kala), action (karma), etc. are dependent realities. Although these dependent realities are eternal and distinct in their own right, they only exist through the consent and sanction of God.\nStated as tattva-vada, Madhva says that reality is composed of three basic categories (tattvas): God (isvara), soul (jiva) and matter (prakrti). All three of these categories are real and distinct, but with one essential qualification, soul and matter are dependent on God.\nThe idea of two orders of reality, one independent and the dependent, and the real differences that exist between the various categories of reality are the hallmarks of Madhva’s dvaita. Madhvacarya is often depicted in a sitting posture with his hand raised showing two fingers. The gesture of two fingers indicates duality (dvaita). Madhva’s theology is based on a strict realism. For Madhva the differences that we see in this world are real and not due to illusion (maya). Madhva’s duality, therefore, greatly contrasts Sankara’s theology of oneness, advaita.\nIn fact Madhva describes five basic differences: the difference between the soul and God, the difference between matter and God, the difference between one soul and another, the difference between matter and the soul, and finally, the difference between one element of matter and another. Suffering in this world is the result of improperly understanding these differences. One who correctly understands these five differences has attained knowledge and is fit for moksa (liberation).\nAnother notable feature of Madhva’s theology is his tripartite classification of souls. According to Madhva there are an infinite number of souls that can be divided into three groups. Some of them qualify for liberation, some are condemned to eternal hell, and others are subject to eternal rebirth. Madhva’s tripartite classification of the soul is unique in Hindu theology, but one that he and his followers maintain can be substantiated from Vedic scripture.\nHierarchy of Devatas According to Madhvas\n1. Sri Visnu (Read Sri before all names)\n2. Laksmi devi\n(Please note that all of the following represent ‘posts’ and not\n3. Brahma, Mukhya-prana , All Rjus [200 in number in every creation]\n4. Sarasvati, Bharati, Rjus-patnis\n5. Garuda, Sesa, Rudra\n6. Krsna’s San-mahisis:\n7. dharmapatnis of #5\nSauparani, Varuni, and Parvati\n8. Indra, and Kama (includes their avataras like\nArjuna, Vali, Pradyumna, Bharata, etc.)\n9. Ahankarika prana\n10. Svayambhu Manu, Daksaprajapati, Brhaspatyacarya, dharmapatnis of # 8 ÐSachidevi, Rati and Aniruddha (Son of Kama).\n11. Pravaha Vayu.\n12. Vivasvan nama surya, Candra and Yama\nSatarupi (dharmapatni of Svayambhu Manu).\n15. Bhrgu, Agni, and Prasuti (dharmapatni of Daksaprajapati)\n16. a. Brahma-putras: Marici, Atri, Angirasa, Pulastya, Pulaha, Kratu, Vasista, and b. Vaivasvata Manu and Visvamitra\n17. Mitra Nama Surya, Nirrti, Pravahi (dharmapatni of Pravaha Vayu) Tara(dharmapatni of Brhaspatyacarya).\n18. Visvaksena, Ganapati, Asvini devatas, Kubera, and Sesa satasta devas (6 Adityas (out of 12) except Devasarma, Urukrama, Varuna, Mitra, Vivasvan and Parjanya), 47 Maruts (out of 50), 7 Vasus (except Agni), 10 Rudras (except Parvati pati), 10 Visvadevas and Asvini Devatas\n19. Karmaja Devatas: a long list including Prahlada, Dhruva, Jayanta, Kasyapa, 11 Manus, 7 Indras including Bali, great Cakravartis like Dusyanta, Prthu, Mandata, Haricandra, Bharata, etc.\n20. a. Parjanya Nama Surya (Meghabhimani),\nb. Ganga (dharmapatni of Varuna)\nc. Sajj-a (dharmapatni of Vivasvan Surya).\nd. Rohini (dharmapatni of Candra).\ne. Usa (dharmapatni of Vayu’s son Aniruddha).\nf. Samala (dharmapatni of Yama)\n21. Kurmadi Devatas\n22. Svaha (dharmapatni of Agni. (Mantrabhimanini)\n23. Budha (Jalabhimani).\n24. Devaki, Yasoda and Usa, the Namabhimanini (dharmapatni of AsviniDevata‘s).\n[I am not sure of the numbering below, but the order is the right one]\n25. Sanaiscara and Dhara Devi\n26. Puskara (karmabhimani)\n27. Ajanaja devatas\nDasgupta, Surendranath. A History of Indian Philosophy. 4 Vols. Delhi: Motilal Banarsidass, 1975.\nTapasyananda, Svami. Sri Madhvacarya, His Life, Religion and Philosophy. Madras: Sri Ramakrishna Math, 1981.\nSharma, B. N. K. History of the Dvaita School of Vedanta and its Literature.Delhi: Motilal Banarsidass, 1981\nCopyright © SRI Publications 2002\nMadhvacarya is generally shown with two fingers upraised. This indicates his philosophy of dvaita or duality.\n|Murti of Madhvacarya located outside of the Krsna Deity at the Mutt in Udupi."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:d156e368-f5b0-4cfd-9e9c-7d3ac7efd350>","<urn:uuid:79163aed-39fe-47b8-a4ef-f6c79e215e8c>"],"error":null}
{"question":"How long does it take to kill Salmonella bacteria at different cooking temperatures?","answer":"Salmonella can be killed at different temperatures depending on cooking duration: at 131°F for one hour, at 140°F for a half-hour, or at 167°F for 10 minutes. Temperatures above 150°F will destroy Salmonella.","context":["Salmonella are destroyed at cooking temperatures above 150 degrees F. The major causes of salmonellosis are contamination of cooked foods and insufficient cooking. Contamination of cooked foods occurs from contact with surfaces or utensils that were not properly washed after use with raw products.\nCan Salmonella be killed by cooking?\nDoes cooking kill salmonella? Thorough cooking can kill salmonella. But when health officials warn people not to eat potentially contaminated food, or when a food is recalled because of salmonella risk, that means don’t eat that food, cooked or not, rinsed or not. The stakes are too high.\nHow long do you have to cook to kill Salmonella?\nThese bacteria reproduce very slowly, if at all, below 40 F and above 140 F. But note that the temperatures at which bacteria are killed vary according to the microbe. For example, salmonella is killed by heating it to 131 F for one hour, 140 F for a half-hour, or by heating it to 167 F for 10 minutes.\nCan Salmonella survive boiling?\nBoiling does kill any bacteria active at the time, including E. coli and salmonella. … And the spores can survive boiling temperatures. After a food is cooked and its temperature drops below 130 degrees, these spores germinate and begin to grow, multiply and produce toxins.\nCan Salmonella survive frying?\n“Any process in which the whites or yolks are insufficiently cooked — yielding whites or yolks that are still liquid — provides the potential for Salmonella to survive. … For instance, he said, eggs fried sunny-side-up or over-easy are only partially cooked and still can harbor bacteria.\nWhat disinfectant kills Salmonella?\nBleach-based cleaners kill bacteria in the most germ-contaminated sites, including sponges, dishcloths, kitchen and bathroom sinks and the kitchen sink drain area. Use bleach-based spray or a solution of bleach and water on cutting boards after every use to kill harmful bacteria like E. coli and Salmonella.\nCan you get food poisoning from frozen food?\nFrozen and raw produce may also carry germs that can cause foodborne illness. It is important to handle produce properly to prevent the spread of germs to your food and kitchen.\nDoes dish soap kill salmonella?\n“Soap is not a sanitizer. It’s not intended to kill microorganisms,” Claudia Narvaez, food safety specialist and professor at the University of Manitoba, explained to CTVNews.ca. “It will kill some bacteria, but not the ones that are more resistant to environmental conditions, like salmonella or E. coli.”\nDoes cooking kill salmonella in onion?\nWhat if onions are cooked? Cooking an onion will kill the salmonella bacteria, Warriner said. The real risk is that the bacteria could be on the outside of the onion, which could spread to kitchen surfaces and other ingredients when it’s chopped, he added.\nCan you kill salmonella in the microwave?\nCan microwaving or re-heating these foods kill the bacteria? If properly and thoroughly reheated, yes. That said, we know heat doesn’t help kill salmonella — it helps breed it — so when microwaving, you must be sure everything is re-heated to the same, proper internal temperature.\nHow long does Salmonella live on food?\nMost Salmonella bacteria live on dry surfaces for up to four hours before they’re no longer infectious. But Salmonella’s survival rate also depends on its species. A 2003 study found that Salmonella enteritidis can survive for four days in high enough amounts to still lead to illness.\nHow long does Salmonella live in sink?\nFoodborne-illness causing bacteria can remain on surfaces for a very long time. Campylobacter can survive in your kitchen for up to 4 hours, and Salmonella can last for up to 32 hours (and both can be found on raw poultry).\nHow quick is Salmonella?\nWhat Is Salmonella? Symptoms of Salmonella usually appear within six hours to six days after eating food (or touching an animal) contaminated with the bacteria and include. Nausea, vomiting, fever and diarrhea are all hallmark symptoms."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:a52e87cc-a1b3-4e09-8673-b087b4f987e3>"],"error":null}
{"question":"Which dessert requires a longer baking time - the Triple Coconut Cream pie or the Succot Date and Nut Pudding?","answer":"The Triple Coconut Cream pie requires longer baking time. For the pie crust alone, it needs 20-25 minutes initial baking plus another 10-12 minutes after removing the parchment and beans, totaling about 35 minutes. In contrast, the Succot Date and Nut Pudding only needs to be baked for about 40 minutes total until firm.","context":["Everyone loves turkey, mashed potatoes and stuffing on Thanksgiving. But let's be honest, the real star of the show is the dessert! We're talking mouthwatering pies. If you want to impress your family and friends this Thanksgiving then try these awesome recipes from the one and only Tom Douglas. We got a personal baking lesson from top pastry chef Stacy Fortner. She let us know how they make their top pies at Dahlia Bakery: The world famous Triple Coconut Cream pie and the Hot Buttered Rum Apple pie. Watch and enjoy!\nThe recipes are below and you can also get them in Tom Douglas’ The Dahlia Bakery Cookbook\nTriple Coconut Cream Pie\n- Coconut Pastry Cream\n- 1 cup (8 ounces/230 grams) milk\n- 1 cup (8 ounces/230 grams) canned unsweetened coconut milk, stirred\n- 2 cups (6 ounces/170 grams) shredded sweetened coconut\n- 1 vanilla bean, split in half lengthwise\n- 2 large eggs\n- 1/2 cup plus 2 tablespoons (4 1/4 ounces/125 grams) sugar\n- 3 tablespoons (7/8 ounce/26 grams) all-purpose flour\n- 4 tablespoons (1/2 stick/2 ounces/57 grams) unsalted butter, room temperature\n- One 9-inch blind-baked and cooled coconut pastry shell (see below)\n- Whipped Cream Topping\n- 2 1/2 cups (20 ounces/600 grams) heavy cream, chilled\n- 1/3 cup (2 1/4 ounces/63 grams) sugar\n- 1 teaspoon pure vanilla extract\n- 2 ounces (57 grams) unsweetened chip or large-shred coconut (about 1 1/2 cups), or sweetened shredded coconut (about 2/3 cup)\n- Chunk of white chocolate (4 to 6 ounces, to make 2 ounces of curls)\n- 1.To make the coconut pastry cream, combine the milk, coconut milk, and shredded coconut in a heavy-bottomed medium saucepan. Use a paring knife to scrape the seeds from the vanilla bean and add both the scrapings and the pod to the milk mixture. Place the saucepan over medium-high heat and stir occasionally until the mixture almost comes to a boil.\n- 2.In a bowl, whisk together the eggs, sugar, and flour until well combined. Temper the eggs by pouring a small amount (about 1/3 cup) of the scalded milk into the egg mixture while whisking. Then add the warmed egg mixture to the saucepan of milk and coconut. Whisk until the mixture is very thick, 4 to 5 minutes more. Remove the saucepan from the heat. Add the butter and whisk until it melts. Remove and discard the vanilla pod. Transfer the pastry cream to a bowl and place it over another bowl of ice water. Stir occasionally until it is cool. Place a piece of plastic wrap directly on the surface of the pastry cream to prevent a skin from forming and refrigerate until completely cold. The pastry cream will thicken as it cools.\n- 3.When the pastry cream is cold, fill the pastry shell, smoothing the surface with a rubber spatula.\n- 4.In an electric mixer with the whisk attachment, whip the heavy cream with the sugar and vanilla extract to peaks that are firm enough to hold their shape. Fill a pastry bag fitted with a star tip with the whipped cream and pipe it all over the surface of the pie.\n- 5.For the garnish, preheat the oven to 350F. Spread the coconut chips on a baking sheet. Toast in the oven for 7 to 8 minutes, watching carefully and stirring once or twice until lightly browned, since coconut burns easily. Remove the coconut from the oven and allow to cool, then sprinkle it over the top of the pie. Use a vegetable peeler to scrape about 2 ounces of the white chocolate into curls on top of the pie. If you prefer, you can cut the pie into wedges and put the wedges on plates, then garnish each wedge individually with coconut and white chocolate curls.\nCoconut Pastry Dough\n- 1 cup plus 2 tablespoons (5 3/4 ounces/165 grams) all-purpose flour\n- 1/2 cup (1 3/4 ounces/50 grams) shredded sweetened coconut\n- 1/2 cup (1 stick/4ounces/113 grams) cold unsalted butter, cut into 1/2-inch dice\n- 2 teaspoons sugar\n- 1/4 teaspoon kosher salt\n- 1/3 cup (2 5/8 ounces/75 grams) ice-cold water, or more as needed\n- 1.In the bowl of a food processor, combine the flour, coconut, diced butter, sugar, and salt. Pulse to form coarse crumbs. Gradually add the water, a tablespoon at a time, pulsing each time. Use only as much water as needed for the dough to hold together when pressed gently between your fingers; don’t work the dough with your hands – just test to see if it’s holding. (The dough will not form a ball or even clump together in the processor – it will still be quite loose.)\n- 2.Place a large sheet of plastic wrap on the counter and dump the coconut dough onto it. Pull the plastic wrap around the dough, forcing it into a rough flattened round with the pressure of the plastic wrap. Chill for 30 to 60 minutes before rolling.\n- 3.To roll the dough, unwrap the round of coconut dough and put it on a lightly floured board. Flour the rolling pin and your hands. Roll the dough out into a circle about 1/8 inch thick. Occasionally lift the dough with a bench knife or scraper to check that it is not sticking and add more flour if it seems like it’s about to stick. Trim to a 12- to 13-inch round.\n- 4.Transfer the rolled dough to a 9-inch pie pan. Ease the dough loosely and gently into the pan. You don’t want to stretch the dough at this point because it will shrink when it is baked. Trim any excess dough to a 1- to 1 1/2-inch overhang. Turn the dough under along the rim of the pie pan and use your fingers and thumb to flute the edge. Chill the unbaked pie shell for at least an hour before baking. (This step prevents the dough from shrinking in the oven.)\n- 5.When you are ready to bake the piecrust, preheat the oven to 400F. Place a piece of parchment in the pie shell, with sides overhanging the pan, and fill with dried beans. (This step prevents the bottom of the shell from puffing up during baking.) Bake the piecrust for 20-25 minutes, or until the pastry rim is golden. Remove the pie pan from the oven. Remove the paper and beans and return the piecrust to the oven. Bake for another 10 to 12 minutes or until the bottom of the crust has golden brown patches. Remove from the oven and allow the pie shell to cool completely.\nHot Buttered Rum Apple Pie\n- 6 to 8 apples, such as Gravenstein (about 3 3/4 pounds/1.7 kilograms)\n- 1/3 cup (2 1/2 ounces/70 grams) granulated sugar\n- 1/4 cup (1 3/8 ounces/40 grams) packed brown sugar\n- 1/4 cup (2 ounces/58 grams) dark rum\n- 4 tablespoons (1/2 stick/2 ounces/60grams) cold unsalted butter, cut into 1/4-inch dice\n- 1 tablespoon plus 1 teaspoon cornstarch\n- 2 teaspoons pure vanilla extract\n- 1 teaspoon dry pectin or an extra 1/2 teaspoon cornstarch\n- 1/2 teaspoon ground cinnamon\n- 1/4 teaspoon freshly grated nutmeg\n- 1/4 teaspoon kosher salt\n- 2 tablespoons heavy cream\n- 2 tablespoons granulated sugar\nFlaky but Tender Pastry Dough for double-crust 9-inch pie (see below for recipe)\n- 1.Peel and core the apples and slice them 1/4 to 1/2 inch thick. You should have about 8 cups of apple slices.\n- 2.Place 2 large (at least 10-inch) sauté pans over medium-high heat and divide the 1/3 cup granulated sugar evenly between them. Cook the sugar, without stirring, until it melts and then caramelizes and turns amber in color, tilting the pans a little to swirl and distribute the color, adjusting the heat as needed. (As soon as it melts, it will quickly start caramelizing, so be ready to add the apples as soon as the color of the sugar turns amber.)\n- 3.Add the apples, dividing them between the 2 pans, and sauté until they are about half-cooked and the juices that are released boil away and reduce until no liquid remains, 8 to 10 minutes. Toss and stir the apples regularly while they are cooking so they cook evenly on both sides. When the apples are done, they should have some give but should not fall apart when you press one between your fingers.\n- 4.Transfer the apples to a bowl and allow them to cool completely to room temperature.\n- 5.Meanwhile, preheat the oven to 350F\n- 6.When the apples are cooled, add the brown sugar, rum, butter, cornstarch, vanilla extract, pectin (if using), spices, and salt and toss to combine.\n- 7.Put the apple filling in the pastry-lined pie plate. Place the pastry circle on top, roll the overhang up and over, and seal with starch water. Press or crimp the edge, then use a paring knife to cut a few vents in the top. If using pastry decorations, attached them to the top crust of the pie with starch water. Brush the pie (and the decorations, if using) with the heavy cream and sprinkle with the 2 tablespoons sugar.\n- 8.Put the pie on a baking sheet to catch any drips, place in the oven, and bake for 30 minutes. Tent the pie with foil and continue to bake for 1 hour. Remove the foil and bake for 30 minutes or until the pie is evenly golden brown. (The total baking time is 2 hours.) Remove the pie from the oven and allow to cool for at least 1 hour on a wire rack before slicing. The pie will still be warm, or you can cool it to room temperature, then slice and serve.\nFlaky but Tender Pastry Dough\n- *For a 9- or 9 1/2-inch single crust pie or a 10-inch tart shell\n- 1 1/3 cups (6 1/8 ounces/175 grams) pastry or cake flour\n- 1/3 cup (1 3/4 ounces/50 grams) all-purpose flour\n- 1 tablespoon sugar\n- 1 1/2 teaspoons kosher salt\n- 1/2 cup (1 stick/4 ounces/113 grams) unsalted butter, freezer-cold, cut into 1/2-inch dice\n- 2 tablespoons vegetable shortening (we use “trans-fat-free” shortening), freezer-cold\n- 1/4 cup (2 ounces/57 grams) ice-cold water, or more as needed\n- 1 teaspoon distilled white vinegar\n- *For a 9-inch double crust pie or 8 individual rustic pies\n- 2 2/3 cups (12 1/4 ounces/350 grams) pastry or cake flour\n- 2/3 cup (3 1/2 ounces/100 grams) all-purpose flour\n- 2 tablespoon sugar\n- 1 tablespoon kosher salt\n- 1 cup (2 sticks/8 ounces/226 grams) unsalted butter, freezer-cold, cut into 1/2-inch dice\n- 1/4 cup vegetable shortening (we use “trans-fat-free” shortening), freezer-cold\n- 1/2 cup (4 ounces/114 grams) ice-cold water, or more as needed\n- 2 teaspoons distilled white vinegar\nElectric Mixer Instructions\n- 1.In the bowl of an electric mixer with the paddle, combine the flours, sugar, and salt. Add the cold butter and shortening, mixing on low speed until the mixture looks shaggy and the pieces of butter are slightly smaller than peas. Stop the mixer and check the size of the butter, sifting through the mixture with your hands. If you find a few bigger chunks, quickly smear them between your fingers.\n- 2.Put the ice-cold water and vinegar into a measuring cup or small container and stir to combine.\n- 3.Add the water-vinegar mixture to the flour-fat mixture in the electric mixer on low speed and mix briefly with a few rotations of the paddle, but do not let the dough come together.\n- 4.Turn off the mixer and scrape around the sides and the bottom of the mixer bowl to make sure there are no pockets of dry ingredients, rotating the paddle a few more times if needed, then squeeze a small amount of dough in your hand. The dough should come together as a clump. If the dough seems too dry, add a little more water a few teaspoons at a time and rotate the paddle a few more times.\n- 5.Remove the dough from the mixer and shape, wrap, and chill as directed.\nFood Processor Instructions\n- 1.Put the flours, sugar, and salt in the bowl of a food processor and pulse a few times to combine. Add the cold butter and shortening to the dry ingredients. Use your hands to break up the shortening into several small clumps and get them coated with flour.\n- 2.Pulse 9 to 12 times. Turn off the machine and take the lid off. The butter should be in pieces a little smaller than the size of a pea. If needed, put the lid back on and pulse a couple more times.\n- 3.Put the ice-cold water and vinegar into a measuring cup or small container and stir to combine.\n- 4.Gradually pour the water-vinegar mixture through the feed tube while pulsing 10 to 12 times. Take the lid off. Use your fingers to see if you can clump the mixture together to form a dough. (The dough should not come together to form a ball while you are pulsing it in the food processor, but it should form a clump pressed between your fingers.) Use a rubber spatula to scrape around the sides of the food processor bowl and the bottom of the bowl to see if there are any dry pockets of flour. If the dough seems too dry, you can add more water a few teaspoons at a time and pulse a few more times.\n- 5.Remove the dough from the food processor and shape, wrap, and chill as directed.","For the sweet side of the Succot menu, sometimes the best choice is a simple\nhomemade fruit dessert.\nA longtime favorite of ours is apple flan, made\nof sauteed apples baked in an easy-to-make custard. In France, chefs enhance the\napple taste by flavoring the custard with Calvados, a Norman apple brandy. (See\nAnother easy but delicious treat is a fruit parfait with yogurt and\ngranola. Betty Rosbottom, author of Sunday Brunch, makes lightly spiced plum\nparfaits by cooking diced plums in a hot frying pan with a little sugar and\nsweet spices – ground ginger and cinnamon. After a few minutes of cooking, the\nsugar turns syrupy and the plums become tender. At serving time, she layers the\nplums in wine glasses with lightly sweetened Greek yogurt (which is similar to\nlabaneh) flavored with a bit of vanilla, and tops each layer with a sprinkling\nRosbottom uses apples and dates to make another fruit dessert\nwith a creamy topping. For her apple and date compote with maple cream, she\nsautees apple wedges in butter with a sprinkling of brown sugar. After removing\nthe apple pieces, she simmers apple juice in the pan with honey, lemon juice and\nsweet spices, and then adds quartered dates and heats the apples in the light\nsauce. She serves the fruit with a topping of whipped cream combined with sour\ncream and maple syrup, and a garnish of toasted walnuts.\nIf you’re in the\nmood for a more substantial sweet, consider baking a pudding. Stephen and Ethel\nLongstreet, authors of The Joys of Jewish Cooking, make Succot date and walnut\npudding with very little sugar, as the dates naturally contribute sweetness. The\ningredients for their pudding are similar to those for a cake – flour, sugar and\neggs – and the batter can be stirred together in a few minutes. (See recipe.)\nThe Longstreets’ apple bread pudding is another simple combination, made of\nbeaten eggs, sugar and milk flavored with vanilla, nutmeg and a little salt. To\nthis batter they add thin apple slices and bread cubes, and bake the pudding in\na buttered dish set in a pan of hot water. They serve the pudding with heavy\nHomey fruit cakes are perfect Succot desserts. To make a\nfruit-topped country cake, Jeffrey Alford and Naomi Duguid, authors of Home\nBaking, bake pears or apples in a yeast dough. The cake is enriched with a bit\nof butter, and for its sweetness depends mainly on the fruit. (See recipe.)\nMaking such a cake, wrote the authors, is “easy as pie...easier than\nFaye Levy is the author of 1,000 Jewish Recipes.\nFRENCH APPLE FLAN\nThis dessert is from Fresh from France: Dessert Sensations by\nTo make the dessert parve, substitute vegetable oil or\nmargarine for the butter, and use 11⁄2 cups soy milk or other nondairy milk\ninstead of the combination of milk and cream.\n350 gr. (3⁄4 lb.) apples –\nsweet, semi-tart or tart\n1 Tbsp. butter\n5 to 7 Tbsp. sugar, depending on the\nsweetness of the apples\n1⁄2 cup creme fraiche, sour cream or whipping cream\n1 to 11⁄2 cups milk\n2 large eggs\n2 large egg yolks\n2 Tbsp. Calvados\n(apple brandy), other brandy or 1 tsp. pure vanilla extract\nPreheat oven to\n175ºC (350ºF). Peel, core and thinly slice apples. Heat butter in a large\nskillet, add apples and stir until coated with butter. Cover and cook over\nmedium-low heat, stirring often, until apples are very tender, about 15 minutes.\nAdd 2 or 3 Tbsp. sugar, depending on the sweetness of the apples, and cook\nuncovered over medium-high heat, stirring often, until the liquid that comes out\nof the apples evaporates.\nGenerously butter four 2⁄3-cup ramekins. Spoon\napple mixture into ramekins. If you will be adding creme fraiche or other cream,\nmeasure 1 cup milk; if not adding cream, measure 11⁄2 cups milk. Bring milk to a\nboil. Remove from heat and cool a few minutes.\nWhisk eggs and yolks\nlightly in large bowl. Add 4 Tbsp. sugar and whisk just to blend. Stir in creme\nfraiche, if using, and whisk it in lightly. Gradually pour in about 1⁄2 cup\nmilk, stirring with whisk. Using wooden spoon, gradually stir in\nremaining 1⁄2 cup milk, or 1 cup milk if you are not adding cream. Stir in\nCalvados. Pour into measuring cup. Skim foam from surface of\nSet ramekins in a roasting pan or large shallow baking dish.\nDivide custard mixture evenly among ramekins. Place pan with ramekins in oven.\nAdd enough very hot water to pan to come halfway up sides of ramekins. Set a\nsheet of foil on top to cover ramekins. Bake until the point of a small\nthin-bladed knife inserted gently in center of each ramekin comes out clean,\nabout 28 minutes.\nServe dessert hot or cold, in the\nMakes 4 servings\nSUCCOT DATE AND NUT PUDDING\nold-fashioned Russian baked pudding is from The Joys of Jewish Cooking. For\ndairy meals, authors Stephen and Ethel Longstreet recommend whipping 2 cups\nheavy cream to serve on top of the pudding.\n3⁄4 cup all-purpose flour\n1 1⁄2 tsp. baking powder\n1⁄2 tsp. salt\n2 cups chopped pitted dates\n1 cup walnut\nhalves 3 eggs\n1 Tbsp. sugar\nPreheat oven to 160ºC (325ºF). Grease a deep\n23-cm. (9-in.) square pan. Sift flour, baking powder and salt into a\nbowl. Lightly mix in the dates and nuts.\nBeat the eggs lightly in another\nbowl and add the sugar. Stir in the date, nut and flour mixture.\nbatter into prepared pan. Bake until pudding is firm, about 40 minutes. Cut\npudding in wedges while it is still warm.\nMakes 8 servings\nThis recipe is from Home Baking. Authors Jeffrey Alford and Naomi\nDuguid write that the cake probably has its origins in the country habit of\nbaking a cake from extra yeast dough left after making bread.\nsimple bread dough needs only a few minutes of kneading, and has a quick topping\nof sliced fruit, sugar, cinnamon and small pieces of butter.\n1⁄4 cup warm water\n2 tsp. active dry yeast\n1 large egg\n1⁄4 tsp. salt\n1 Tbsp. unsalted butter, softened\n1 1⁄2 cups all purpose flour\nor 2 small pears or apples\nabout 1⁄4 cup sugar\n1 to 1 1⁄2 tsp. cinnamon\ntsp. unsalted butter, cut into small chunks\nTo make the dough, stir the sugar\ninto the warm water, then stir in the yeast. Beat the egg in a medium bowl. Stir\nin the salt and butter, then 1⁄2 cup of the flour. Stir briefly, then add the\nyeast mixture and stir it in. Add 3⁄4 cup more flour and stir in. Turn the dough\nout onto a floured surface and knead until firm and smooth, about 4\nPlace the dough in a clean bowl and cover well with plastic\nwrap. Let rise until doubled in volume, about 11⁄2 hours.\nWhen ready to\nbake, place a rack in the upper third of the oven and preheat the oven to 200ºC\nButter a 23-cm. (9-in.) square or 25-cm. (10-in.) round, shallow\ncake pan or 25-cm. (10-in.) tart pan. Peel, core and thinly slice the\nFlatten the dough out in the prepared pan. Sprinkle with half the\nsugar and half the cinnamon, then arrange the fruit slices decoratively on top,\nto cover the whole surface. Sprinkle on the remaining sugar and cinnamon and\nthen dot with the small pieces of butter. Let stand, covered, for 10\nBake for about 20 minutes, until golden. Serve hot or at room\nMakes 8 servings"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:68c015f5-f752-4090-84f9-725af6c8ccc0>","<urn:uuid:68687eff-7a79-4cf6-b4ff-7f5b758c3bba>"],"error":null}